---
ver: rpa2
title: Exploring the Potential of Large Language Models to Generate Formative Programming
  Feedback
arxiv_id: '2309.00029'
source_url: https://arxiv.org/abs/2309.00029
tags:
- feedback
- chatgpt
- programming
- tasks
- education
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the potential of large language models (LLMs)
  to generate formative programming feedback for novice programmers. The authors analyze
  99 responses from ChatGPT to incorrect student solutions in introductory programming
  tasks, characterizing the feedback along dimensions such as content, quality, and
  feedback type.
---

# Exploring the Potential of Large Language Models to Generate Formative Programming Feedback

## Quick Facts
- arXiv ID: 2309.00029
- Source URL: https://arxiv.org/abs/2309.00029
- Reference count: 32
- Primary result: LLM-generated feedback provides textual explanations of errors and fixes unique compared to existing tools, but quality varies and can contain misleading information for novices

## Executive Summary
This paper explores the potential of large language models, specifically ChatGPT, to generate formative programming feedback for novice programmers. The authors analyze 99 responses to incorrect student solutions across four introductory programming tasks, characterizing feedback along dimensions of content, quality, and type. Results show that while ChatGPT provides unique textual explanations alongside improved code, the feedback quality varies significantly and can contain misleading information. The authors conclude that LLMs show promise for addressing students' need for formative feedback, but require guidance on how to use such feedback effectively due to limitations for novice learners.

## Method Summary
The study used ChatGPT to generate feedback on 99 student submissions to introductory programming exercises (TEOS, TTBS, TTBA, NEGF) with various errors. Researchers used open-ended prompts asking "What's wrong with my code?" and analyzed responses based on predefined criteria including content, quality, and feedback type classifications. The analysis compared feedback consistency across three regenerations of the same input and examined how well the feedback addressed specific error types. The study did not use task constraints or explicit context in prompts, representing a general-use scenario.

## Key Results
- ChatGPT provides textual explanations of errors and fixes along with improved code, a unique feature compared to existing automated feedback tools
- Generated feedback quality varies significantly, with 61 out of 99 cases containing misleading information
- Feedback consistency across three regenerations varies, with 116 out of 363 triples showing different content, making it challenging to rely on responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated feedback provides textual explanations of errors and fixes that are unique compared to existing automated feedback tools
- Mechanism: The model leverages its training on code and natural language to generate contextual explanations alongside improved code snippets
- Core assumption: The LLM has sufficient training data on programming errors and corrections to produce meaningful feedback
- Evidence anchors:
  - [abstract] states "ChatGPT provides textual explanations of errors and fixes along with improved code, which is a unique feature compared to existing tools"
  - [section] IV.A notes "ChatGPT's responses usually contained textual explanations of the cause and fix of errors (88, and 83 responses each)"
- Break condition: The model encounters novel error patterns or task constraints not represented in its training data

### Mechanism 2
- Claim: LLM feedback variability across generations creates challenges for consistent learning outcomes
- Mechanism: Each regeneration produces different content, some containing misleading information
- Core assumption: The stochastic nature of LLM generation is inherent and cannot be eliminated
- Evidence anchors:
  - [section] IV.A states "the three generated responses from ChatGPT to the same student input vary greatly" and "Due to this degree of randomness, it seems challenging to rely on responses"
  - Table II shows variations in 116 out of 363 triples across three regenerations
- Break condition: The model is run in deterministic mode or with strict sampling parameters

### Mechanism 3
- Claim: LLM feedback quality depends heavily on prompt engineering and task context
- Mechanism: Without explicit task constraints or context, the model may provide incomplete or misleading feedback
- Core assumption: The model cannot infer task-specific constraints without explicit input
- Evidence anchors:
  - [section] V notes "ChatGPT may provide misleading information, and it lacks information on task constraints"
  - [section] IV.A mentions "some of the student submissions included package imports, which are prohibited... errors due to incorrect imports... were not addressed and caused some of the misleading information"
- Break condition: Task constraints are explicitly provided in the prompt or context window

## Foundational Learning

- Concept: Large Language Models (LLMs) and their training paradigm
  - Why needed here: Understanding how LLMs generate code feedback requires knowledge of their training on both natural language and code
  - Quick check question: What distinguishes a general-purpose LLM from a code-specialized model like Codex?

- Concept: Feedback typology in programming education
  - Why needed here: The study applies existing feedback classifications to analyze LLM output, requiring understanding of different feedback types
  - Quick check question: How does "Knowledge about mistakes" feedback differ from "Knowledge about how to proceed" feedback?

- Concept: Novice programming error patterns
  - Why needed here: The study analyzes feedback quality for common student errors, requiring familiarity with typical CS1 mistakes
  - Quick check question: What distinguishes a compiler error from a semantic error in programming?

## Architecture Onboarding

- Component map: Student code submissions -> Prompt template -> ChatGPT API interface -> Response parsing -> Criteria matching -> Feedback classification -> Analysis
- Critical path: Submission → Prompt generation → ChatGPT API call → Response parsing → Criteria matching → Feedback classification → Analysis
- Design tradeoffs: Open-ended prompts maximize flexibility but increase variability; task constraints improve accuracy but require additional input management
- Failure signatures: Misleading information (61/99 cases), uncertainty expressions (21/99 cases), code compilation failures when code is provided
- First 3 experiments:
  1. Compare feedback consistency across 3 generations for identical prompts
  2. Test feedback quality difference between open prompts vs prompts with explicit task constraints
  3. Evaluate whether including test cases in prompts reduces misleading feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and type of feedback from LLMs vary across different programming tasks and student error types?
- Basis in paper: [explicit] The authors note that feedback quality varies and can contain misleading information for novices, particularly for logic and semantic errors or when multiple errors are present.
- Why unresolved: The study only explored four specific tasks and a limited number of student submissions, which may not be representative of the broader range of programming tasks and error types.
- What evidence would resolve it: Conducting a comprehensive analysis of LLM-generated feedback across a diverse set of programming tasks and error types, comparing the feedback quality and type to expert feedback.

### Open Question 2
- Question: Can the design and engineering of prompts improve the accuracy and usefulness of LLM-generated feedback for novice programmers?
- Basis in paper: [explicit] The authors mention that future work will include investigating how to engineer prompts to generate specific types of feedback and correct solutions from student input.
- Why unresolved: The study only explored one general, open prompt, and the impact of different prompt designs on feedback quality is not yet known.
- What evidence would resolve it: Experimenting with various prompt designs and evaluating their impact on the accuracy, relevance, and helpfulness of LLM-generated feedback for novice programmers.

### Open Question 3
- Question: How can educators effectively guide students in using LLM-generated feedback to enhance their learning and programming skills?
- Basis in paper: [explicit] The authors emphasize the need for educators to provide guidance on how to use LLM-generated feedback due to its limitations for novices.
- Why unresolved: The study does not provide specific strategies or methods for educators to guide students in effectively using LLM-generated feedback.
- What evidence would resolve it: Developing and evaluating pedagogical methods and strategies that help students critically analyze, interpret, and apply LLM-generated feedback to improve their programming skills and learning outcomes.

## Limitations

- Analysis covers only 99 responses across four introductory programming tasks, which may not represent the full spectrum of novice programming errors or complex scenarios
- The study focuses on ChatGPT specifically without comparing performance across different LLM architectures or versions
- Feedback quality analysis lacks systematic comparison with existing automated feedback tools or expert-generated feedback

## Confidence

High confidence: The finding that LLM-generated feedback provides textual explanations of errors and fixes is well-supported by the data showing 88-83 responses containing such explanations. The observation about variability across regenerations (116 out of 363 triples showing variation) is also strongly evidenced.

Medium confidence: The claim about misleading information appearing in 61 out of 99 cases is supported but may depend on prompt formulation and task complexity. The relationship between prompt engineering and feedback quality is plausible but not systematically tested.

Low confidence: Claims about the uniqueness of LLM feedback compared to existing tools lack systematic comparison. The recommendation for guidance on using LLM feedback is reasonable but not empirically validated through student outcomes.

## Next Checks

1. **Systematic prompt engineering study**: Design a controlled experiment varying prompt specificity, task constraints, and context window content to measure their impact on feedback accuracy and consistency across multiple LLM generations.

2. **Cross-model comparison**: Test the same student submissions across multiple LLM architectures (ChatGPT, Codex, Claude, etc.) to determine whether feedback patterns are model-specific or represent broader LLM characteristics.

3. **Novice student impact study**: Conduct a small-scale experiment where actual novice programmers receive both LLM-generated and traditional automated feedback, measuring learning outcomes and error correction rates to validate the practical utility of the feedback types identified.