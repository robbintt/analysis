---
ver: rpa2
title: '(S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes
  and Edge of Stability'
arxiv_id: '2302.08982'
source_url: https://arxiv.org/abs/2302.08982
tags:
- have
- gradient
- sparse
- stepsizes
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the effect of stochasticity and large step sizes
  on the implicit regularization of gradient descent (GD) and stochastic gradient
  descent (SGD) over diagonal linear networks. The authors prove the convergence of
  GD and SGD with macroscopic step sizes in an overparameterized regression setting
  and characterize their solutions through an implicit regularization problem.
---

# (S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability

## Quick Facts
- **arXiv ID**: 2302.08982
- **Source URL**: https://arxiv.org/abs/2302.08982
- **Reference count**: 40
- **Primary result**: SGD and GD with macroscopic stepsizes converge to zero-loss solutions that minimize the same potential function but with different effective initializations, affecting sparse recovery.

## Executive Summary
This paper analyzes the implicit regularization properties of gradient descent (GD) and stochastic gradient descent (SGD) on diagonal linear networks when using large step sizes. The authors prove convergence for both algorithms with macroscopic stepsizes in an overparameterized regression setting and characterize their solutions through an implicit regularization framework. They show that both methods minimize the same hyperbolic entropy potential, but with an effective initialization that is strictly smaller than the original initialization in a component-wise sense. The study reveals that large stepsizes can either benefit or hinder sparse recovery depending on whether SGD or GD is used, with these effects being magnified in the "edge of stability" regime where stepsizes approach the divergence threshold.

## Method Summary
The paper studies optimization on diagonal linear networks where the model parameters β = u ⊙ v (element-wise product of two vectors). The authors analyze both full-batch GD and mini-batch SGD with constant step sizes, proving convergence for stepsizes satisfying γ_k ≤ c/LB for a numerical constant c. They establish that both algorithms converge to zero-training-loss solutions that minimize a hyperbolic entropy potential, but with different effective initializations derived from the cumulative gain vector Gain_γ = ∑_k q(γ∇_L_B_k(β_k)). The analysis leverages a mirror descent interpretation with time-varying potentials, showing that SGD follows a mirror descent recursion while GD has a different behavior in the edge of stability regime.

## Key Results
- SGD and GD with macroscopic stepsizes converge to zero-loss solutions minimizing the same hyperbolic entropy potential
- The effective initialization α∞ for both methods is strictly smaller than the original initialization α in a component-wise sense
- Large stepsizes benefit SGD for sparse recovery problems but can hinder GD recovery of sparse solutions
- In the edge of stability regime, SGD maintains homogeneous fluctuations across coordinates while GD exhibits heterogeneous oscillations concentrated on the support of the sparse solution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SGD with large stepsizes converges to a zero-training-loss solution minimizing hyperbolic entropy with an effective initialization smaller than the original initialization.
- **Mechanism**: SGD on diagonal linear networks follows a mirror descent recursion with time-varying potentials. The effective initialization α∞ derives from the cumulative gain vector Gain_γ = ∑_k q(γ∇_L_B_k(β_k)), which shrinks each coordinate of α multiplicatively based on encountered squared stochastic gradients.
- **Core assumption**: Stepsizes satisfy γ_k ≤ c/LB for a numerical constant c, ensuring iterates remain bounded and gradients don't explode.
- **Evidence anchors**: Theorem 1 establishes convergence and characterizes the solution through D_ψ_α_∞(β_⋆, β̃_0). The weak corpus evidence suggests related work exists on implicit bias and mirror descent.

### Mechanism 2
- **Claim**: Large stepsizes magnify SGD-GD solution differences by altering the gain vector shape, affecting implicit regularization.
- **Mechanism**: The gain vector captures both scale and shape of effective initialization. For GD, heterogeneous gain vector shape with higher magnitude on sparse solution support leads to weighted ℓ1-norm penalizing support coordinates. For SGD, more homogeneous shape due to stochasticity enables better sparse recovery.
- **Core assumption**: Inputs sampled from N(0, σ²I_d) with minimum ℓ1-norm interpolator also being sparsest interpolator.
- **Evidence anchors**: Propositions 3 and 4 characterize gradient statistics at initialization. Weak corpus evidence exists on stochasticity and implicit bias, but not specifically this heterogeneous gain vector mechanism.

### Mechanism 3
- **Claim**: In edge of stability regime, SGD benefits sparse recovery while GD hinders it due to different oscillation patterns.
- **Mechanism**: For GD in edge of stability, iterates oscillate primarily on support coordinates, creating higher gain vector magnitude on these coordinates and weighted ℓ1-norm penalizing them. For SGD, more uniform fluctuations maintain balanced gain vector shape.
- **Core assumption**: Small enough initialization for gradient flow to converge close to sparse interpolator, stepsize in [γ_max, γ̃_max] range.
- **Evidence anchors**: Section 5 describes trajectory decomposition into three phases and explains convergence behavior. Fig. 3 illustrates different fluctuation patterns. Weak corpus evidence on edge of stability, but not specifically neuron-level oscillation mechanism.

## Foundational Learning

- **Concept**: Mirror descent with time-varying potentials
  - **Why needed here**: Establishes that SGD on diagonal linear networks follows a mirror descent recursion with varying potentials, crucial for proving convergence and characterizing implicit regularization.
  - **Quick check question**: What is the key difference between standard mirror descent and mirror descent with time-varying potentials in this context?

- **Concept**: Implicit regularization in overparameterized models
  - **Why needed here**: Main result characterizes solutions recovered by SGD and GD through implicit regularization problem, showing how effective initialization affects recovered solution.
  - **Quick check question**: How does the effective initialization α∞ relate to the original initialization α in terms of scale and shape?

- **Concept**: Edge of stability regime in optimization
  - **Why needed here**: Explains how edge of stability regime affects SGD and GD behavior differently, particularly for sparse recovery problems.
  - **Quick check question**: What distinguishes oscillation patterns of GD from fluctuation patterns of SGD in edge of stability regime?

## Architecture Onboarding

- **Component map**: Diagonal Linear Network β = u ⊙ v where w = (u, v) ∈ R²ᵈ -> Loss function F(w) = L(u ⊙ v) = 1/(2n) ∑_i(y_i - ⟨u ⊙ v, x_i⟩)² -> Optimization algorithm: Mini-batch SGD with stepsize γ_k and batch B_k -> Effective initialization: α∞ derived from cumulative gain vector -> Implicit regularization: Minimization of hyperbolic entropy with effective initialization

- **Critical path**: 1. Initialize u_0 = √2α, v_0 = 0; 2. For each iteration k: Sample batch B_k, compute gradient ∇_F_B_k(β_k), update w_k+1 = w_k - γ_k∇_F_B_k(w_k); 3. Track β_k = u_k ⊙ v_k and compute cumulative gain vector; 4. Derive effective initialization α∞ and solve implicit regularization problem; 5. Analyze solution properties (sparsity, generalization)

- **Design tradeoffs**: Stepsize selection: Larger stepsizes benefit SGD for sparse recovery but can hinder GD; must stay below divergence threshold. Batch size: Affects magnitude of gain vector (linear scaling rule applies). Initialization scale: Smaller scales push solution toward ℓ1-norm but can slow training.

- **Failure signatures**: Divergence: Stepsize too large, iterates escape bounded region. Poor sparse recovery: GD with large stepsizes or inappropriate initialization shape. Convergence to wrong solution: Stepsize too small, behaves like gradient flow.

- **First 3 experiments**: 1. Verify convergence of SGD with macroscopic stepsizes for simple overparameterized regression problem. 2. Compare solutions recovered by SGD vs GD for different stepsizes and batch sizes on sparse recovery task. 3. Analyze shape of gain vector for SGD and GD to understand differences in implicit regularization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise upper bound for the stepsize γ_max in terms of problem constants for SGD and GD to converge?
- **Basis in paper**: Explicit - The paper states convergence holds for stepsizes satisfying γ_k ≤ c/LB, where c is a numerical constant, but mentions empirically loss still converges for larger stepsizes, and defines γ_max as largest stepsize before iterates stop converging.
- **Why unresolved**: Paper provides conservative bound but does not derive exact value of γ_max or explain why it differs between SGD and GD.
- **What evidence would resolve it**: Rigorous derivation of γ_max as function of problem constants, along with empirical validation showing exact threshold for both SGD and GD.

### Open Question 2
- **Question**: How does the shape of the gain vector Gain_γ affect the recovery of the minimum ℓ1-norm solution for different initializations?
- **Basis in paper**: Explicit - Paper discusses how shape of Gain_γ, influenced by batch size and input data, determines which coordinates of β are penalized in implicit regularization problem, affecting sparse recovery.
- **Why unresolved**: While paper provides insights into relationship between shape of Gain_γ and sparse recovery, does not offer complete characterization or quantitative measure of this effect for arbitrary initializations.
- **What evidence would resolve it**: Theoretical analysis of relationship between shape of Gain_γ and implicit regularization problem, along with empirical studies demonstrating impact on sparse recovery for various initializations.

### Open Question 3
- **Question**: How does the Edge of Stability (EoS) regime affect the implicit bias of GD and SGD in more complex architectures beyond diagonal linear networks?
- **Basis in paper**: Explicit - Paper discusses EoS regime for GD and SGD in diagonal linear networks, explaining how large stepsizes lead to oscillations in GD and fluctuations in SGD, affecting recovery of sparse solutions.
- **Why unresolved**: Paper focuses on diagonal linear networks and does not explore implications of EoS regime for more complex architectures, such as deep neural networks.
- **What evidence would resolve it**: Empirical studies and theoretical analyses of EoS regime in deep neural networks, demonstrating how large stepsizes affect implicit bias and generalization properties of learned models.

## Limitations
- Results are proven for diagonal linear networks, which is a highly simplified architecture that may not capture the behavior of more complex neural networks
- The analysis assumes Gaussian input distributions and specific initialization schemes that may not hold in practical scenarios
- The paper relies on several technical conditions and bounds that require verification in practice and may be conservative

## Confidence
- **High**: The convergence results for SGD and GD with macroscopic stepsizes under the stated conditions
- **Medium**: The characterization of implicit regularization through effective initialization
- **Low**: The specific mechanisms explaining differences between SGD and GD in the edge-of-stability regime

## Next Checks
1. Verify the bounds on stepsizes experimentally by training with increasing step sizes until divergence occurs, comparing with the theoretical thresholds c/LB and γ_max
2. Test sparse recovery performance on non-Gaussian data distributions to assess the robustness of the findings beyond the assumed input model
3. Implement the edge-of-stability experiments with different network sizes and sparsity levels to confirm that the observed oscillation patterns generalize beyond the specific examples shown