---
ver: rpa2
title: 'From Empirical Measurements to Augmented Data Rates: A Machine Learning Approach
  for MCS Adaptation in Sidelink Communication'
arxiv_id: '2309.17086'
source_url: https://arxiv.org/abs/2309.17086
tags:
- data
- sidelink
- goodput
- features
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of selecting optimal modulation
  and coding schemes (MCS) for C-V2X sidelink communication without feedback. A machine
  learning approach is proposed to predict the MCS level with the highest achievable
  data rate using quantile regression.
---

# From Empirical Measurements to Augmented Data Rates: A Machine Learning Approach for MCS Adaptation in Sidelink Communication

## Quick Facts
- arXiv ID: 2309.17086
- Source URL: https://arxiv.org/abs/2309.17086
- Reference count: 22
- Key outcome: Gradient boosting with quantile regression achieved 12.295 Mbit/s goodput, outperforming static MCS baselines by 16.7% using real-world C-V2X sidelink data from Berlin

## Executive Summary
This paper addresses the challenge of selecting optimal modulation and coding schemes (MCS) for C-V2X sidelink communication without feedback by proposing a machine learning approach using quantile regression. The authors collected extensive real-world data through drive tests in Berlin and trained four algorithms (random forest, gradient boosting, neural network, linear regression) to predict the MCS level with highest achievable data rate. By using quantile prediction to minimize overestimation risk, they achieved significantly higher goodput compared to non-adaptive methods, with gradient boosting performing best at 12.295 Mbit/s versus 10.541 Mbit/s for static MCS.

## Method Summary
The authors collected real-world data via extensive drive tests in Berlin using SDR measurements with high time resolution, extracting features including SNR, RSRP, RSSI, noise power, Rx power, and GPS information. They implemented four machine learning algorithms with quantile prediction using pinball loss or quantile regression forests. Hyperparameter optimization was performed using randomized search with leave-one-group-out cross-validation, and feature importance was determined through correlation analysis and permutation methods. The models were trained to predict the optimal MCS level that maximizes goodput, with performance evaluated on test data.

## Key Results
- Gradient boosting achieved highest goodput of 12.295 Mbit/s compared to 10.541 Mbit/s for static best MCS
- Quantile prediction reduced overestimation risk by using lower quantiles of MCS distribution
- Feature reduction from 13 to 4 features maintained performance while reducing computational complexity
- Neural networks required more training samples than gradient boosting to reach similar performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantile regression reduces overestimation risk in MCS prediction
- Mechanism: By minimizing pinball loss instead of MSE, the model is trained to predict a lower quantile of the MCS distribution, which biases predictions toward safer (lower) MCS levels
- Core assumption: The asymmetric cost of over-predicting MCS (packet loss) outweighs under-predicting MCS (slightly lower throughput)
- Evidence anchors:
  - [abstract] "By using quantile prediction, we achieved significantly higher goodput"
  - [section] "we use quantile prediction, which we implemented by using the pinball loss function"
  - [corpus] No direct evidence found
- Break condition: If channel conditions are highly stable and overestimation risk is minimal, quantile regression may be unnecessarily conservative

### Mechanism 2
- Claim: Feature importance ranking enables effective feature selection
- Mechanism: Permutation feature importance method identifies which features most affect prediction accuracy, allowing reduction to 4 features without significant performance loss
- Core assumption: The top features capture most predictive signal and interactions between features are minimal
- Evidence anchors:
  - [section] "we employ the more powerful permutation feature importance method"
  - [section] "sorted the features based on importance...performance increases with an increasing number of features for all algorithms except linear regression"
  - [corpus] No direct evidence found
- Break condition: If feature interactions are significant, reducing features may discard valuable predictive information

### Mechanism 3
- Claim: Gradient boosting achieves superior performance through ensemble learning
- Mechanism: GB combines multiple weak learners (decision trees) to create a strong predictor that generalizes well to diverse driving conditions
- Core assumption: The data distribution in different driving environments is sufficiently similar for a single model to generalize
- Evidence anchors:
  - [section] "we achieved significantly higher goodput (12.295 Mbit/s with gradient boosting)"
  - [section] "gradient boosting performed best"
  - [corpus] No direct evidence found
- Break condition: If driving conditions are too diverse, a single model may fail to capture all relevant patterns

## Foundational Learning

- Concept: Quantile regression
  - Why needed here: Standard regression would optimize for average performance but ignores the asymmetric cost of overestimation in MCS prediction
  - Quick check question: Why is minimizing pinball loss more appropriate than MSE for MCS prediction?

- Concept: Feature importance methods
  - Why needed here: With 13 features, selecting the most informative subset reduces computational complexity without sacrificing accuracy
  - Quick check question: What is the difference between Pearson correlation and permutation feature importance?

- Concept: Gradient boosting fundamentals
  - Why needed here: GB's ensemble approach provides better generalization across diverse driving conditions than single models
  - Quick check question: How does gradient boosting differ from random forests in handling feature interactions?

## Architecture Onboarding

- Component map: Data collection (SDR measurements) -> Feature extraction (SNR, RSRP, GPS) -> Model training (GB/RF/NN with quantile regression) -> Prediction engine (MCS selection) -> Performance evaluation (goodput calculation)

- Critical path: Data collection → Feature extraction → Model training → Prediction → Goodput calculation

- Design tradeoffs:
  - Computational complexity vs. prediction accuracy (4 features vs. all features)
  - Model complexity vs. training data requirements (NN needs more samples than GB)
  - Safety margin vs. throughput (quantile level selection)

- Failure signatures:
  - Consistently low goodput indicates model is too conservative
  - Frequent packet loss indicates overestimation in MCS prediction
  - Model degradation over time suggests data distribution shift

- First 3 experiments:
  1. Compare goodput using different quantile levels (0.5, 0.6, 0.7) to find optimal balance
  2. Test feature reduction from 13 to 4 features and measure performance impact
  3. Evaluate model performance on individual geographical areas (tunnel, highway, etc.) to identify weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of including additional context features (e.g., weather conditions, road type, or time of day) on the performance of MCS prediction algorithms?
- Basis in paper: [inferred] The paper discusses the use of GPS and physical layer measurements as features for prediction, but does not explore the inclusion of other contextual information that could affect signal propagation.
- Why unresolved: The authors did not conduct experiments with additional contextual features beyond those already mentioned.
- What evidence would resolve it: Conducting experiments that incorporate various contextual features and comparing their impact on prediction performance.

### Open Question 2
- Question: How do different machine learning algorithms compare in terms of computational complexity and real-time applicability for MCS prediction in C-V2X sidelink communication?
- Basis in paper: [explicit] The paper evaluates several algorithms (random forest, gradient boosting, neural network, linear regression) but does not discuss their computational complexity or suitability for real-time applications.
- Why unresolved: The authors focused on prediction accuracy rather than computational efficiency or real-time constraints.
- What evidence would resolve it: Analyzing the computational requirements and latency of each algorithm in a real-time setting.

### Open Question 3
- Question: How does the performance of the proposed ML-based MCS adaptation approach scale with increasing vehicle density and network load?
- Basis in paper: [inferred] The paper does not address scenarios with varying vehicle density or network load, which could affect the applicability of the proposed approach in more congested environments.
- Why unresolved: The authors did not conduct experiments or simulations with varying vehicle density or network load.
- What evidence would resolve it: Evaluating the performance of the ML-based approach under different vehicle density and network load conditions.

## Limitations

- Limited dataset diversity: The evaluation relies on a single real-world dataset collected in Berlin, which may not fully represent all driving conditions and environments encountered in practice.
- Algorithm-specific tuning: While hyperparameter optimization was performed, the optimal configurations may be sensitive to specific implementation details not fully disclosed in the paper.
- Quantile level sensitivity: The choice of quantile level for prediction could significantly impact the tradeoff between throughput and packet loss, but the sensitivity analysis is limited.

## Confidence

- High confidence: The fundamental mechanism of using quantile regression to reduce overestimation risk in MCS prediction is well-established and supported by the experimental results.
- Medium confidence: The superiority of gradient boosting over other algorithms is demonstrated but may be dataset-specific and not generalizable to all scenarios.
- Medium confidence: The feature importance analysis provides useful insights, but the potential for significant feature interactions is not thoroughly explored.

## Next Checks

1. **Dataset diversity validation**: Evaluate the models on additional datasets collected in different geographical locations and under various environmental conditions to assess generalizability.
2. **Quantile sensitivity analysis**: Conduct a comprehensive analysis of how different quantile levels affect the throughput-packet loss tradeoff across various driving scenarios.
3. **Algorithm comparison under diverse conditions**: Test the four algorithms (GB, RF, NN, LR) across a wider range of simulated driving conditions to identify performance boundaries and failure modes.