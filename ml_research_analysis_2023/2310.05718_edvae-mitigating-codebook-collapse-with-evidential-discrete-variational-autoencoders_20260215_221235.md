---
ver: rpa2
title: 'EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders'
arxiv_id: '2310.05718'
source_url: https://arxiv.org/abs/2310.05718
tags:
- codebook
- distribution
- conference
- embeddings
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses codebook collapse in discrete VAEs, where
  only a few codebook embeddings are used. The authors propose EdVAE, which replaces
  softmax with an evidential deep learning approach to better capture uncertainty
  and reduce overconfident probability assignments.
---

# EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders

## Quick Facts
- arXiv ID: 2310.05718
- Source URL: https://arxiv.org/abs/2310.05718
- Reference count: 23
- Primary result: Replaces softmax with evidential deep learning to reduce overconfident codebook embedding selection, improving codebook usage and reconstruction.

## Executive Summary
EdVAE addresses codebook collapse in discrete VAEs by replacing the softmax function with an evidential deep learning approach that models embedding selection uncertainty using Dirichlet distributions. This substitution leads to higher entropy in the probability distributions over codebook embeddings, resulting in more diverse usage of the codebook and better reconstruction performance. The method demonstrates superior performance compared to VQ-VAE and dVAE baselines across multiple datasets including CIFAR10, CelebA, and LSUN Church, with improved perplexity, reconstruction error, and image generation quality as measured by FID scores.

## Method Summary
EdVAE modifies the standard dVAE architecture by replacing softmax with evidential deep learning that outputs concentration parameters for Dirichlet distributions over codebook embeddings. The encoder produces logits that are converted to Dirichlet concentration parameters (αθ), which are then used to sample probability distributions over embeddings. These distributions are relaxed using Gumbel-Softmax for differentiable training. The model is trained using an ELBO loss that combines reconstruction error with a KL divergence term between the learned Dirichlet posterior and a uniform Dirichlet prior, encouraging diverse codebook usage while maintaining reconstruction quality.

## Key Results
- Achieves higher entropy in codebook probability distributions compared to dVAE during training
- Improves codebook usage as measured by perplexity across CIFAR10, CelebA, and LSUN Church datasets
- Demonstrates better reconstruction error and FID scores than VQ-VAE and dVAE baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using evidential deep learning (EDL) with Dirichlet-distributed concentration parameters reduces overconfidence in codebook embedding selection.
- **Mechanism:** The softmax in dVAE produces highly peaked probability distributions, leading to a few dominant embeddings being selected repeatedly (confirmation bias). EDL replaces softmax with a Dirichlet distribution whose concentration parameters are learned from data. This yields smoother, more entropic distributions, increasing the chance that multiple embeddings are sampled and reducing the collapse.
- **Core assumption:** The root cause of codebook collapse is the softmax's tendency to assign overconfident probabilities to the "best" codebook element, rather than a problem in the quantization or training procedure itself.
- **Evidence anchors:**
  - [abstract] "We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements."
  - [section 5.1] "Figure 2a visualizes the average entropy of the probabilities during the training... EdVAE's mean values of the entropy are higher than those of dVAE's... higher standard deviation for EdVAE... aiding a relatively liberal codebook usage."
  - [corpus] Weak/no direct evidence; only distantly related papers on vector quantization and posterior collapse, no specific EDL analysis.

### Mechanism 2
- **Claim:** The hierarchical Dirichlet-Categorical structure in EdVAE captures uncertainty about which codebook embedding best represents a spatial position.
- **Mechanism:** The encoder outputs concentration parameters α for a Dirichlet over categorical distributions over codebook embeddings. Sampling from this Dirichlet produces a distribution over embeddings rather than a single hard choice. This stochasticity is learned from data, so the model can adapt its "confidence" per sample.
- **Core assumption:** Modeling the embedding selection as a distribution over distributions is more expressive than a single categorical distribution, enabling better exploration of codebook space.
- **Evidence anchors:**
  - [section 4.1] "EDL incorporation models a second-order uncertainty... αθ represents how confident this prediction is, and π predicts which codebook element can best reconstruct the input."
  - [section 4.1] "Unlike dVAE which has a single level of distribution... we employ an additional level in EdVAE... to incorporate randomness with well quantified uncertainty over the selection process of the codebook embeddings."
  - [corpus] No direct evidence; the cited corpus does not address second-order uncertainty modeling.

### Mechanism 3
- **Claim:** The ELBO loss in EdVAE balances reconstruction accuracy with codebook diversity through a KL term between the amortized Dirichlet posterior and a uniform Dirichlet prior.
- **Mechanism:** The reconstruction term encourages accurate reconstruction using chosen embeddings. The KL divergence term regularizes the posterior to stay close to a uniform Dirichlet prior, which maximizes entropy over the categorical distribution and thus promotes using many embeddings.
- **Core assumption:** A uniform Dirichlet prior encourages uniform usage of codebook embeddings, preventing dominance by a few.
- **Evidence anchors:**
  - [section 4.1] "Equation 3 demonstrates our prior design as a Dirichlet distribution that generates uniform distributions over the codebook embeddings on average... Equation 8 shows that the second term in Equation 8 indirectly supports that goal via a diversified codebook usage."
  - [section 5.1] "We emphasize that while prior works induce high entropy via regularizers, the feature introduced by our ELBO formulation inherently achieves the same effect as a result of our modeling assumptions that harness the power of the Dirichlet distribution."
  - [corpus] No direct evidence; corpus papers focus on other regularization methods but not Dirichlet KL in this context.

## Foundational Learning

- **Concept:** Dirichlet distribution and its KL divergence with uniform prior.
  - **Why needed here:** The EDL layer in EdVAE uses a Dirichlet distribution to model uncertainty over categorical embedding choices. Understanding its properties (entropy, KL to uniform) is essential to see how it encourages diverse codebook usage.
  - **Quick check question:** What is the KL divergence between Dir(π|α₁,...,αₖ) and Dir(π|1,...,1), and how does it behave as αₖ increase or decrease?

- **Concept:** Gumbel-Softmax relaxation for differentiable sampling from categorical distributions.
  - **Why needed here:** Both dVAE and EdVAE sample discrete codebook indices during training. Gumbel-Softmax provides a differentiable approximation so gradients can flow back to the encoder.
  - **Quick check question:** How does the temperature parameter in Gumbel-Softmax affect the smoothness of the relaxed samples, and what happens as it approaches zero?

- **Concept:** Vector quantization and codebook collapse.
  - **Why needed here:** The target problem is codebook collapse, where only a few embeddings are used. Knowing how VQ-VAEs quantize and why collapse occurs (deterministic nearest-neighbor vs. stochastic selection) is crucial.
  - **Quick check question:** In a VQ-VAE with K=512 embeddings, what does a perplexity of 190 imply about embedding usage?

## Architecture Onboarding

- **Component map:** Encoder -> ze(x) -> exp(ze(x)) + 1 -> αθ -> Dirichlet(αθ) -> π -> RelaxedOneHotCategorical(π) -> z -> M[z] -> Decoder -> reconstruction
- **Critical path:** ze(x) → αθ → π → z → M[z] → reconstruction
- **Design tradeoffs:**
  - Using Dirichlet instead of softmax increases entropy but adds sampling variance
  - Gumbel-Softmax with low temperature yields near-hard samples but can be unstable
  - β balances reconstruction fidelity vs. codebook diversity; too high hurts reconstruction
- **Failure signatures:**
  - Perplexity stays low → codebook collapse persists
  - Training instability → high variance in αθ or numerical overflow in exp(ze)
  - Poor reconstruction → β too high or temperature too low
- **First 3 experiments:**
  1. Run dVAE and EdVAE on CIFAR10 with same architecture, compare perplexity and MSE after 50k steps
  2. Visualize entropy maps of π over spatial positions for both models on a fixed sample
  3. Sweep β from 1e-7 to 1e-5 on CIFAR10, observe impact on perplexity vs. MSE

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the codebook collapse problem manifest in dVAE when using higher temperature values during training?
  - **Basis in paper:** [explicit] The paper mentions that using higher temperature values with dVAE on CIFAR10 and CelebA datasets does not consistently improve perplexity, indicating sensitivity to this hyperparameter.
  - **Why unresolved:** The paper only briefly mentions the temperature sensitivity and does not provide detailed analysis or experimental results for different temperature settings.
  - **What evidence would resolve it:** Detailed experiments showing the impact of varying temperature values on codebook usage and reconstruction performance for dVAE across multiple datasets.

- **Open Question 2:** What are the computational trade-offs between EdVAE and other VQ-VAE based methods in terms of training time and resource usage?
  - **Basis in paper:** [inferred] The paper compares EdVAE to other methods in terms of performance metrics but does not discuss computational efficiency or resource requirements.
  - **Why unresolved:** The paper focuses on performance metrics like perplexity and reconstruction error but does not address the practical considerations of implementing EdVAE at scale.
  - **What evidence would resolve it:** Comparative analysis of training time, memory usage, and GPU requirements for EdVAE versus other VQ-VAE based methods on the same hardware configurations.

- **Open Question 3:** How does EdVAE perform on larger and more diverse datasets like ImageNet compared to its performance on CIFAR10, CelebA, and LSUN Church?
  - **Basis in paper:** [explicit] The paper acknowledges that it evaluates EdVAE on small to medium-sized datasets and suggests exploring its performance on more diverse datasets like ImageNet as a future work.
  - **Why unresolved:** The paper's experiments are limited to smaller datasets, and the authors explicitly state that evaluating EdVAE on larger datasets is a potential direction for future research.
  - **What evidence would resolve it:** Comprehensive experiments evaluating EdVAE's performance on ImageNet or other large-scale datasets, comparing metrics like perplexity, reconstruction error, and image generation quality to other state-of-the-art methods.

## Limitations
- The uniform Dirichlet prior assumes uniform usage is optimal, which may not hold for all datasets or codebook sizes
- The paper does not test whether softmax-only models with entropy regularization would achieve similar improvements
- Architectural details for encoder/decoder beyond basic ResNet blocks are underspecified

## Confidence
**High Confidence:**
- EdVAE achieves higher entropy in codebook probability distributions than dVAE during training, as measured empirically
- EdVAE outperforms dVAE and VQ-VAE baselines on CIFAR10, CelebA, and LSUN Church in terms of perplexity, MSE, and FID scores

**Medium Confidence:**
- The Dirichlet-based EDL approach is the primary driver of improved codebook usage rather than other factors like KL regularization or sampling noise
- The uniform Dirichlet prior is the optimal choice for encouraging codebook diversity across different datasets

**Low Confidence:**
- Softmax overconfidence is the dominant cause of codebook collapse in discrete VAEs, rather than issues in quantization or training dynamics
- The hierarchical Dirichlet-Categorical structure provides significant benefits over simpler entropy regularization approaches

## Next Checks
1. **Ablation on Softmax vs. EDL:** Train a dVAE variant with entropy regularization (KL to uniform categorical) and compare perplexity/MSE progression to EdVAE. This isolates whether the Dirichlet hierarchy or entropy regularization drives improvements.

2. **Prior Sensitivity Analysis:** Train EdVAE with non-uniform Dirichlet priors (e.g., α=2 instead of α=1) on CIFAR10 and measure impact on codebook usage and reconstruction. This tests whether the uniform prior assumption is critical.

3. **Temperature Sweep Diagnostic:** For both dVAE and EdVAE, sweep Gumbel-Softmax temperature from 0.1 to 2.0 and record final perplexity and MSE. This determines whether temperature alone can mitigate collapse without EDL.