---
ver: rpa2
title: Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural
  Language Processing
arxiv_id: '2310.12664'
source_url: https://arxiv.org/abs/2310.12664
tags:
- financial
- language
- sentence
- tasks
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of large language
  models (LLMs) on financial natural language processing (NLP) tasks through a framework
  called FinLMEval. The authors collected nine datasets, including five public and
  four proprietary datasets, to assess the performance of both encoder-only and decoder-only
  models.
---

# Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing

## Quick Facts
- **arXiv ID**: 2310.12664
- **Source URL**: https://arxiv.org/abs/2310.12664
- **Reference count**: 7
- **Primary result**: Fine-tuned encoder-only models generally outperform zero-shot decoder-only models on financial NLP tasks

## Executive Summary
This paper presents FinLMEval, a comprehensive evaluation framework for assessing large language models on financial natural language processing tasks. The authors collected nine datasets spanning five public benchmarks and four proprietary datasets to evaluate both encoder-only models (BERT, RoBERTa, FinBERT, FLANG) through fine-tuning and decoder-only models (ChatGPT, GPT-4, PIXIU, LLAMA2-7B, Bloomberg-GPT) through zero-shot prompting and in-context learning. The results demonstrate that while zero-shot decoder-only models show notable performance across most financial tasks, they generally lag behind fine-tuned expert models, particularly on proprietary datasets where the performance gap widens significantly.

## Method Summary
The study employs a two-pronged approach: encoder-only models are fine-tuned on financial datasets using standard hyperparameters (learning rate 2Ã—10^-5, weight decay 0.01, batch size 48 for 3 epochs), while decoder-only models are evaluated through zero-shot prompting with carefully crafted prompts and in-context learning using both random and similar sample selection strategies. The evaluation spans nine financial NLP datasets covering sentiment analysis, named entity recognition, question answering, and other financial text understanding tasks, with micro-F1 score as the primary metric (accuracy for NER).

## Key Results
- Fine-tuned encoder-only models outperform zero-shot decoder-only models on most financial tasks
- Performance gaps between fine-tuned and zero-shot models are larger on proprietary datasets than public ones
- Zero-shot decoder-only models show notable but inconsistent performance across different financial NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned encoder-only models outperform decoder-only models on financial tasks due to domain-specific pretraining and task-specific fine-tuning. Encoder-only models like FinBERT and FLANG are pretrained on financial corpora, giving them inherent financial knowledge that is effectively leveraged through fine-tuning.

### Mechanism 2
Zero-shot decoder-only models perform better on public datasets than proprietary datasets due to potential data leakage and overfitting. Public datasets may have been used in pretraining or instruction tuning of decoder-only models, giving them an unfair advantage over truly novel proprietary datasets.

### Mechanism 3
In-context learning is only effective under certain circumstances, such as when zero-shot prompting is ineffective. In-context learning provides additional task-specific demonstrations that can clarify the task for the model, but for well-defined tasks, zero-shot prompts are already sufficient.

## Foundational Learning

- **Transformer architecture and pretraining objectives**: Understanding the differences between encoder-only and decoder-only models, and how pretraining objectives affect downstream task performance. Quick check: What are the key differences between BERT's masked language modeling objective and GPT's causal language modeling objective?

- **Domain adaptation and transfer learning**: Understanding how pretraining on a large corpus and fine-tuning on a specific task can lead to improved performance, especially in specialized domains like finance. Quick check: How does pretraining on a financial corpus and then fine-tuning on a sentiment analysis task differ from training a model from scratch on the sentiment analysis task?

- **Prompt engineering and in-context learning**: Understanding how to effectively prompt decoder-only models and leverage in-context learning to improve performance on downstream tasks. Quick check: How does the choice of in-context examples (random vs. similar) affect the performance of in-context learning?

## Architecture Onboarding

- **Component map**: Datasets (9 financial NLP datasets) -> Models (4 encoder-only, 5 decoder-only) -> Evaluation (Fine-tuning and zero-shot/in-context learning experiments)

- **Critical path**: 
  1. Preprocess and prepare the datasets
  2. Fine-tune encoder-only models on the training sets
  3. Evaluate encoder-only models on the test sets
  4. Prompt decoder-only models with zero-shot and in-context learning
  5. Evaluate decoder-only models on the test sets
  6. Analyze and compare the results

- **Design tradeoffs**: Using encoder-only vs. decoder-only models (fine-tuning required vs. zero-shot capability), using public vs. proprietary datasets (potential data leakage vs. novelty), using random vs. similar in-context examples (ease of generation vs. effectiveness)

- **Failure signatures**: Encoder-only models showing poor performance on test sets despite good validation performance (overfitting), decoder-only models showing poor performance on proprietary datasets (lack of domain expertise), in-context learning showing no improvement for well-defined tasks

- **First 3 experiments**:
  1. Fine-tune BERT on the FinSent dataset and evaluate on the test set to establish a baseline for encoder-only models
  2. Prompt ChatGPT with zero-shot and in-context learning on the FinSent dataset to compare with the fine-tuned BERT model
  3. Repeat experiment 2 with GPT-4 to see if the larger model size leads to improved performance

## Open Questions the Paper Calls Out

### Open Question 1
How do more advanced large language models (LLMs) perform on the proprietary datasets compared to the public datasets in the FinLMEval framework? The paper only evaluates a limited number of LLMs and does not explore the performance of more advanced models on the proprietary datasets.

### Open Question 2
Can the financial expertise of decoder-only LLMs be improved through fine-tuning on domain-specific financial data? The paper mentions that fine-tuned encoder-only models generally outperform decoder-only models, suggesting potential for improvement, but does not investigate fine-tuning decoder-only models.

### Open Question 3
How does the performance of encoder-only and decoder-only models vary across different financial NLP tasks within the FinLMEval framework? The paper provides an overall comparison but does not delve into specific variations across different financial tasks.

## Limitations

- The study's reliance on proprietary datasets limits reproducibility and external validation
- Potential data leakage in public datasets affecting zero-shot model performance is concerning but difficult to quantify
- Limited exploration of fine-tuning decoder-only models for financial tasks

## Confidence

- **High Confidence**: Fine-tuned encoder-only models generally outperforming zero-shot decoder-only models is well-supported by experimental results
- **Medium Confidence**: In-context learning effectiveness being task-dependent is supported but could benefit from more extensive testing
- **Low Confidence**: Speculation about data leakage in public datasets affecting zero-shot model performance remains an educated guess without direct testing

## Next Checks

1. **External Validation Study**: Conduct the same evaluation using only publicly available datasets and models to verify if performance patterns hold without proprietary data dependencies

2. **Pretraining Data Audit**: Analyze the pretraining corpora of major decoder-only models to determine actual overlap with public financial NLP benchmarks, quantifying the data leakage concern

3. **In-Context Learning Parameter Sweep**: Systematically vary the number, diversity, and quality of in-context examples across a wider range of financial NLP tasks to better characterize when this approach is most effective