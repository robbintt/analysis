---
ver: rpa2
title: Pretraining Language Models with Text-Attributed Heterogeneous Graphs
arxiv_id: '2310.12580'
source_url: https://arxiv.org/abs/2310.12580
tags:
- graph
- nodes
- node
- pretraining
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces THLM, a pretraining framework for Language
  Models (LMs) on Text-Attributed Heterogeneous Graphs (TAHGs). It tackles two key
  challenges: 1) capturing both first- and high-order topological connections among
  heterogeneous nodes, and 2) handling imbalanced textual descriptions across node
  types.'
---

# Pretraining Language Models with Text-Attributed Heterogeneous Graphs

## Quick Facts
- **arXiv ID**: 2310.12580
- **Source URL**: https://arxiv.org/abs/2310.12580
- **Reference count**: 40
- **Primary result**: THLM improves LM pretraining on TAHGs by capturing multi-order topological connections and handling text imbalance through context graph prediction and text augmentation

## Executive Summary
This paper introduces THLM, a pretraining framework for Language Models on Text-Attributed Heterogeneous Graphs (TAHGs). THLM addresses two key challenges: capturing multi-order topological connections among heterogeneous nodes and handling imbalanced textual descriptions across node types. The framework uses a context graph prediction task that jointly optimizes an LM and an auxiliary heterogeneous graph neural network, along with a text augmentation strategy that enriches textless nodes by combining neighbor texts. Experiments on three datasets demonstrate consistent improvements over state-of-the-art baselines in link prediction and node classification tasks.

## Method Summary
THLM pretrains LMs on TAHGs using a context graph prediction task that captures multi-order topological information. The method extracts neighborhoods of target nodes within K orders to form context graphs, then jointly optimizes an LM (BERT/RoBERTa) and an auxiliary heterogeneous graph neural network (R-HGNN) to predict which nodes are present in these context graphs. To handle text imbalance, THLM employs a text augmentation strategy that enriches textless nodes by concatenating their own text with texts from sampled neighbors. The model is pretrained using AdamW optimizer with learning rates 6e-5 (LM) and 1e-4 (HGNN), then fine-tuned on downstream tasks including link prediction and node classification.

## Key Results
- THLM consistently outperforms state-of-the-art baselines on link prediction (RMSE/MAE) and node classification (micro/macro precision/recall, NDCG) across three TAHG datasets
- Context graph prediction task effectively captures multi-order topological connections, improving performance on both link prediction and node classification
- Text augmentation strategy successfully handles text imbalance, enriching textless nodes with neighbor texts to improve downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context graph prediction task enables the model to capture multi-order topological information in TAHGs.
- Mechanism: The model extracts neighborhoods of a target node within K orders to form a context graph. It then jointly optimizes a language model and a heterogeneous graph neural network to predict which nodes are present in this context graph. This forces the model to learn both first-order and higher-order structural relationships.
- Core assumption: Multi-order structural relationships in TAHGs contain valuable information for downstream tasks.
- Evidence anchors:
  - [abstract] "We define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph"
  - [section] "We propose a topology-aware pretraining task (namely, context graph prediction) for pretraining LMs on TAHGs to capture multi-order connections among different nodes"
  - [corpus] "Weak evidence - the paper focuses on multi-order topology but doesn't explicitly validate this mechanism against single-order baselines"
- Break condition: If multi-order structural information provides no advantage over first-order information, or if the context graph extraction fails to capture meaningful neighborhood relationships.

### Mechanism 2
- Claim: Text augmentation strategy enriches semantics of textless nodes by combining neighbor texts.
- Mechanism: For textless nodes, the model concatenates the node's own brief text with texts from k sampled neighbors, creating augmented input. This provides richer semantic context for nodes that lack sufficient textual descriptions.
- Core assumption: Textless nodes benefit from incorporating semantic information from their neighbors.
- Evidence anchors:
  - [abstract] "we devise a text augmentation strategy to enrich textless nodes with their neighbors' texts for handling the imbalance issue"
  - [section] "we devise a text augmentation strategy to tackle the imbalanced textual descriptions of nodes, which enriches the semantics of textless nodes by combining the textual descriptions of their neighbors"
  - [corpus] "Moderate evidence - the paper shows improved performance with text augmentation but doesn't isolate the specific contribution of neighbor text incorporation"
- Break condition: If neighbor texts are irrelevant or noisy, or if the concatenation approach fails to produce coherent semantic representations.

### Mechanism 3
- Claim: Joint optimization of language model and heterogeneous graph neural network enables transfer of graph learning ability to the language model.
- Mechanism: The heterogeneous graph neural network computes structural representations of nodes, while the language model computes semantic representations. The joint optimization task requires the language model to learn to use structural information effectively.
- Core assumption: Graph neural networks can effectively compute structural representations that are useful for language model pretraining.
- Evidence anchors:
  - [abstract] "predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network"
  - [section] "we present the Context Graph Prediction (CGP) for pretraining LMs on TAHGs to capture the rich information... aims to inject the graph learning ability of graph neural networks (Bing et al., 2022) into LMs"
  - [corpus] "Moderate evidence - the paper shows improved performance but doesn't directly compare against using only the language model without the auxiliary GNN"
- Break condition: If the auxiliary GNN doesn't provide meaningful structural information, or if the joint optimization fails to transfer learning to the language model.

## Foundational Learning

- Concept: Heterogeneous graphs with multiple node types and edge types
  - Why needed here: TAHGs contain different types of entities (papers, authors, keywords) with varying textual descriptions, requiring the model to handle type-specific patterns
  - Quick check question: Can you explain the difference between a homogeneous graph and a heterogeneous graph?

- Concept: Graph neural networks and their ability to capture structural information
  - Why needed here: The auxiliary heterogeneous graph neural network computes structural representations that are essential for the context graph prediction task
  - Quick check question: What is the key difference between message passing in GNNs and attention mechanisms in transformers?

- Concept: Pretraining tasks for language models (masked language modeling, next-token prediction)
  - Why needed here: THLM builds upon existing pretraining frameworks and adds the context graph prediction task alongside traditional tasks
  - Quick check question: How does masked language modeling help language models learn contextual representations?

## Architecture Onboarding

- Component map: Input layer (text sequences with special tokens) -> BERT/RoBERTa (semantic representations) -> Context graph extraction (neighborhoods within K orders) -> R-HGNN (structural representations) -> Prediction layer (binary classification) -> Text augmentation (neighbor text concatenation for textless nodes)

- Critical path: Text input → LM encoding → Context graph extraction → GNN encoding → Joint prediction → Loss computation

- Design tradeoffs:
  - Choice of K (context graph order): Higher K captures more structural information but increases computational cost and may introduce noise
  - Number of neighbors k for text augmentation: More neighbors provide richer context but increase input length and computational requirements
  - GNN architecture choice: R-HGNN provides type-awareness but adds complexity compared to simpler GNN variants

- Failure signatures:
  - Poor performance on node classification: May indicate insufficient semantic understanding or structural information
  - Overfitting to pretraining task: May require more regularization or data augmentation
  - Slow convergence: May indicate suboptimal hyperparameter choices or learning rate issues

- First 3 experiments:
  1. Verify context graph extraction works correctly by visualizing extracted neighborhoods for sample nodes
  2. Test text augmentation by comparing representations of textless nodes with and without neighbor text concatenation
  3. Validate joint optimization by monitoring both LM and GNN losses during training and checking for balanced learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of THLM scale with the number of node types and edge types in TAHGs?
- Basis in paper: [inferred] The paper evaluates THLM on three datasets with varying numbers of node types (2-4) and edge types (2-4), but does not explore how performance changes with more complex TAHGs.
- Why unresolved: The datasets used have a limited number of node and edge types, preventing conclusions about performance on more complex TAHGs.
- What evidence would resolve it: Experiments on TAHGs with a larger variety and number of node types and edge types, measuring performance on link prediction and node classification tasks.

### Open Question 2
- Question: How does the performance of THLM compare to other methods on larger-scale TAHGs?
- Basis in paper: [inferred] The paper only evaluates THLM on three relatively small-scale datasets, leaving uncertainty about its performance on larger TAHGs.
- Why unresolved: The datasets used are not large-scale, limiting the generalizability of the results.
- What evidence would resolve it: Experiments on larger-scale TAHGs, comparing THLM's performance to other methods on link prediction and node classification tasks.

### Open Question 3
- Question: How does the performance of THLM change with different levels of text imbalance in TAHGs?
- Basis in paper: [inferred] The paper introduces a text augmentation strategy to handle text imbalance, but does not explore how performance changes with varying levels of imbalance.
- Why unresolved: The datasets used have a fixed level of text imbalance, preventing conclusions about performance on TAHGs with different levels of imbalance.
- What evidence would resolve it: Experiments on TAHGs with different levels of text imbalance, measuring performance on link prediction and node classification tasks with and without the text augmentation strategy.

## Limitations

- The evaluation focuses on only three specific domains (academic, book, and patent networks), leaving uncertainty about performance on other TAHG types
- The model requires substantial computational resources due to joint optimization of both LM and GNN components
- The text augmentation strategy assumes neighbor texts are relevant and non-noisy, which may not hold in all TAHG scenarios

## Confidence

- **High Confidence**: The context graph prediction task improves LM performance on TAHGs, as evidenced by consistent improvements across multiple datasets and tasks
- **Medium Confidence**: The text augmentation strategy effectively enriches textless nodes, though the paper doesn't isolate its contribution from other components
- **Medium Confidence**: Joint optimization transfers structural knowledge to LMs, but direct comparisons against using only the LM component are lacking

## Next Checks

1. Test THLM on additional TAHG domains (e.g., social networks, biological networks) to assess domain generalizability
2. Conduct ablation studies isolating the contribution of neighbor text incorporation versus other text augmentation approaches
3. Compare R-HGNN against simpler GNN variants (RGCN, GAT) to validate the architectural choice