---
ver: rpa2
title: 'Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant
  Transformers'
arxiv_id: '2311.04744'
source_url: https://arxiv.org/abs/2311.04744
tags:
- algebra
- equivariant
- geometric
- product
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to build equivariant transformers for 3D
  geometry using different geometric algebras. The authors compare Euclidean, projective,
  and conformal geometric algebras, constructing corresponding GATr variants.
---

# Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers

## Quick Facts
- arXiv ID: 2311.04744
- Source URL: https://arxiv.org/abs/2311.04744
- Authors: 
- Reference count: 26
- Primary result: Euclidean and conformal geometric algebras are sufficiently expressive for universal equivariant approximation, while projective algebra is not unless the join operation is included.

## Executive Summary
This paper studies how to build equivariant transformers for 3D geometry using different geometric algebras (GA). The authors compare Euclidean, projective, and conformal GAs, constructing corresponding GATr variants and evaluating their expressiveness and performance. Theoretically, E-GA and C-GA are sufficiently expressive for universal equivariant approximation, while P-GA is not unless the join operation is included. Experimentally, conformal and improved projective GATrs achieve the best performance on n-body and arterial wall-shear-stress prediction tasks, while Euclidean GATr is less sample-efficient due to its smaller symmetry group.

## Method Summary
The paper constructs GATr variants based on three geometric algebras: Euclidean (EGA), Projective (PGA), and Conformal (CGA). Each variant implements equivariant linear layers using grade projections and geometric products specific to the algebra. The models use GA inner product for attention, with normalization layers adapted to each algebra's properties. The theoretical analysis proves that EGA and CGA can express any non-equivariant multilinear map, while PGA cannot without the join operation. The experimental evaluation compares these variants on n-body gravitational simulation and arterial wall shear stress prediction tasks, measuring performance and sample efficiency.

## Key Results
- E-GATr is computationally cheapest but has reduced sample efficiency due to smaller symmetry group
- C-GATr and iP-GATr (improved projective) achieve best performance on tested tasks
- Distance-based attention is naturally available in C-GATr but requires additional machinery in PGA
- E-GA and C-GA are sufficiently expressive for universal equivariant approximation, while P-GA is not without join operation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Euclidean and conformal geometric algebras are sufficiently expressive to construct universal equivariant transformers for 3D geometry, while projective algebra is not unless the join operation is included.
- Mechanism: The paper proves that Euclidean (EGA) and conformal (CGA) algebras can express any non-equivariant multilinear map using only geometric products and constant multivectors, due to their non-degenerate inner products. Projective algebra (PGA) has a degenerate inner product, making it insufficient unless the join operation is added.
- Core assumption: Multilinear maps form a sufficient basis for universal approximation in this context, and the join operation in PGA restores full expressivity.
- Evidence anchors:
  - [abstract] states that "Euclidean and conformal algebras are sufficiently expressive for universal equivariant approximation, while projective algebra is not unless the join operation is included."
  - [section 4] provides the formal proposition that EGA and CGA can express any non-equivariant multilinear map with geometric products and constant multivectors, while PGA cannot due to inner product degeneracy.
- Break condition: If the multilinear map assumption fails or if join operation in PGA is computationally prohibitive, this mechanism breaks.

### Mechanism 2
- Claim: The symmetry group of the geometric algebra representation determines sample efficiency in learning equivariant transformers.
- Mechanism: E-GATr only has equivariance under rotations around the center of mass (reducing the symmetry group from E(3) to a subgroup), requiring more training samples to learn translation invariance implicitly. P-GATr and C-GATr have full E(3) equivariance, making them more sample-efficient.
- Core assumption: Larger symmetry groups in the representation space lead to better sample efficiency because the model doesn't need to learn symmetry transformations from data.
- Evidence anchors:
  - [abstract] notes that "Euclidean architecture is computationally cheap, but has a smaller symmetry group and is not as sample-efficient."
  - [section 4] explains that E-GATr "can not represent translations or absolute positions, and thus must rely on centering to be E(3) equivariant. This reduces the symmetry group to rotations around the center, and thus sample efficiency."
- Break condition: If centering-based approaches can efficiently learn translation invariance despite the smaller symmetry group, or if computational advantages of E-GATr outweigh sample efficiency costs.

### Mechanism 3
- Claim: Distance-based attention is naturally available in conformal algebra but not in projective algebra without additional machinery.
- Mechanism: In CGA, Euclidean position vectors have direct inner product relationships that compute distances. In PGA, inner products between transformed point representations are constant in position, preventing distance computation. iP-GATr solves this by mapping PGA points to CGA points before computing attention.
- Core assumption: The ability to compute distances through inner products is crucial for attention-based transformers to effectively model geometric relationships.
- Evidence anchors:
  - [section 4] states that "Distance-based attention appears most naturally in the C-GATr architecture" and "In the CGA, a Euclidean position vector p ∈ R3 is represented as p = o + p + ∥p∥2∞/2, and inner products between points directly compute the Euclidean distance."
  - [section 4] proves that "in P-GATr, dot-product attention cannot compute distances" and iP-GATr addresses this "by computing CGA points from PGA points, and using the CGA inner product in the attention."
- Break condition: If alternative attention mechanisms that don't rely on distance computation can achieve similar performance, or if the additional complexity of iP-GATr's point mapping becomes a bottleneck.

## Foundational Learning

- Concept: Geometric Algebra (GA) and Clifford algebras
  - Why needed here: The entire paper builds GATr variants based on different geometric algebras (EGA, PGA, CGA). Understanding how multivectors, geometric products, inner products, and grade projections work is essential to grasp why different algebras have different expressive powers and computational properties.
  - Quick check question: What is the geometric interpretation of a 2-vector in PGA versus CGA?

- Concept: Equivariance and symmetry groups
  - Why needed here: The paper's core argument about sample efficiency and expressiveness depends on understanding how different algebras represent symmetry groups (E(3), SO(3), etc.) and what transformations leave certain representations invariant.
  - Quick check question: Why does E-GATr only have equivariance under rotations around the center of mass, while P-GATr and C-GATr have full E(3) equivariance?

- Concept: Multilinear maps and universal approximation
  - Why needed here: The paper's theoretical analysis of expressivity hinges on whether algebras can represent multilinear maps, which relates to universal approximation capabilities of the resulting transformers.
  - Quick check question: Why does the degeneracy of PGA's inner product prevent it from expressing all multilinear maps without the join operation?

## Architecture Onboarding

- Component map:
  GATr backbone -> Equivariant linear layers -> GA inner product attention -> Normalization layers -> Output

- Critical path: Choose algebra → Implement equivariant linear layers → Implement GA inner product attention → Add normalization layers appropriate to algebra → Test with simple equivariance checks

- Design tradeoffs:
  - Expressivity vs computational cost: E-GATr is cheapest but least expressive; C-GATr is most expressive but requires float32 and has stability issues
  - Sample efficiency vs implementation complexity: Full E(3) equivariance (P-GATr, C-GATr) requires fewer samples but is more complex to implement
  - Distance awareness vs algebraic simplicity: C-GATr naturally computes distances; P-GATr needs iP-GATr's point mapping machinery

- Failure signatures:
  - Training instability with C-GATr (likely due to negative inner products and null multivectors)
  - Poor performance with P-GATr without join operation (insufficient expressivity)
  - Overfitting with E-GATr on small datasets (reduced symmetry group)

- First 3 experiments:
  1. Verify equivariance: Apply random SE(3) transformations to inputs and check that outputs transform consistently
  2. Test inner product behavior: Compute inner products between point representations and verify distance computation in CGA vs PGA
  3. Compare normalization: Test different normalization schemes (standard LayerNorm vs grade-separated) and observe training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of the join operation affect the expressiveness of the projective geometric algebra (PGA) in GATr?
- Basis in paper: [explicit] The paper states that PGA can only express non-equivariant multilinear maps with the join operation.
- Why unresolved: While the paper discusses the theoretical necessity of the join operation for PGA, it does not provide experimental evidence on how its inclusion affects model performance or expressiveness in practice.
- What evidence would resolve it: Comparative experiments showing the performance of PGA with and without the join operation in GATr would clarify its practical impact.

### Open Question 2
- Question: What are the specific stability issues encountered with C-GATr during training, and how do they affect the model's performance?
- Basis in paper: [explicit] The paper mentions that C-GATr can suffer from instabilities, but does not detail the nature or impact of these instabilities.
- Why unresolved: The paper suggests using float32 instead of bfloat16 for C-GATr to address stability, but it does not explore the root causes of these instabilities or their effects on model accuracy and training efficiency.
- What evidence would resolve it: A detailed analysis of the stability issues, including their causes and effects on training and performance, would provide insights into potential solutions or alternative approaches.

### Open Question 3
- Question: How does the choice of normalization layers affect the training stability and performance of different GATr variants?
- Basis in paper: [explicit] The paper discusses the adaptation of normalization layers for different algebras, particularly highlighting challenges with the CGA.
- Why unresolved: While the paper suggests modifications to normalization layers for stability, it does not empirically evaluate how these changes impact the overall training dynamics and performance across different GATr variants.
- What evidence would resolve it: Experiments comparing the performance of GATr variants with different normalization strategies would clarify the impact of these choices on training stability and model accuracy.

## Limitations

- The experimental evaluation is limited to only two datasets (n-body simulation and arterial wall shear stress), which may not represent the diversity of 3D geometric learning tasks.
- The paper doesn't provide detailed computational complexity analysis or runtime comparisons between GATr variants.
- The theoretical expressivity results, while formally proven, may not directly translate to practical learning scenarios.

## Confidence

- Expressivity Results: Medium confidence. The theoretical proofs are sound, but their practical relevance is uncertain.
- Sample Efficiency Findings: Low-Medium confidence. The results align with theoretical expectations, but the limited experimental scope raises concerns about generalizability.
- Distance Computation Claims: Medium confidence. The algebraic derivations are correct, but the practical impact on downstream performance needs more validation.
- Overall Architecture Comparisons: Low-Medium confidence. While the experimental results show clear trends, the limited number of tasks and the specific dataset characteristics may influence the conclusions.

## Next Checks

1. **Broader Task Evaluation**: Test GATr variants on additional 3D geometry tasks including molecular conformation prediction, point cloud classification, and 3D shape matching to assess generalizability.

2. **Computational Complexity Analysis**: Perform detailed profiling of training time, inference time, and memory usage across all GATr variants on standardized hardware to verify the claimed computational advantages of E-GATr.

3. **Ablation Studies on Attention Mechanisms**: Conduct controlled experiments isolating the impact of distance-based attention by comparing different attention variants within each algebra, particularly testing whether iP-GATr's performance advantage comes primarily from distance awareness or other factors.