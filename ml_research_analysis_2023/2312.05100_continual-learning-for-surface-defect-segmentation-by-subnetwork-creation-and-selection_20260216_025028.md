---
ver: rpa2
title: Continual learning for surface defect segmentation by subnetwork creation and
  selection
arxiv_id: '2312.05100'
source_url: https://arxiv.org/abs/2312.05100
tags:
- learning
- defect
- task
- defects
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a continual learning algorithm called LDA-CP&S
  for surface defect segmentation without catastrophic forgetting. The method creates
  defect-specific subnetworks via iterative pruning and trains a classifier based
  on linear discriminant analysis (LDA).
---

# Continual learning for surface defect segmentation by subnetwork creation and selection

## Quick Facts
- arXiv ID: 2312.05100
- Source URL: https://arxiv.org/abs/2312.05100
- Reference count: 13
- Key outcome: LDA-CP&S achieves mean IoU scores two times better than existing continual learning methods for surface defect segmentation

## Executive Summary
This paper introduces LDA-CP&S, a continual learning algorithm for surface defect segmentation that addresses catastrophic forgetting through subnetwork creation and selection. The method creates defect-specific subnetworks via iterative pruning and trains a classifier based on linear discriminant analysis (LDA). At inference, it predicts the defect type using LDA and then segments using the corresponding subnetwork. Evaluated on two surface defect datasets, LDA-CP&S significantly outperforms existing continual learning methods, achieving mean Intersection over Union scores two times better. The approach shows comparable results to joint training where all data is available simultaneously, making it the first work developing a continual learning approach for surface defect segmentation.

## Method Summary
LDA-CP&S is a continual learning algorithm for surface defect segmentation that creates defect-specific subnetworks via iterative pruning and uses LDA for task prediction. The method trains a U-Net architecture incrementally on each defect type separately, with iterative pruning creating a unique subnetwork for each defect type that is frozen after creation. At inference, LDA predicts the defect type from feature embeddings extracted using EfficientNet-B5, and the corresponding subnetwork performs segmentation. The approach addresses catastrophic forgetting by never updating frozen subnetworks while creating new subnetworks with free parameters from iterative pruning.

## Key Results
- LDA-CP&S achieves mean IoU scores two times better than existing continual learning methods on surface defect segmentation tasks
- The approach performs comparably to joint training where all data is available simultaneously
- Ablation study confirms iterative pruning is essential, with simple finetuning achieving only 32.6% IoU compared to 71.1% with LDA-CP&S

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific subnetworks prevent catastrophic forgetting.
- Mechanism: During training, iterative pruning creates a unique subnetwork for each defect type. Once created, these subnetworks are frozen and never updated, preserving performance on previous tasks while allowing new subnetworks to learn new defects.
- Core assumption: Each defect type is sufficiently distinct that separate subnetworks can be learned without interference.
- Evidence anchors:
  - [abstract]: "Our method creates a defect-related subnetwork for each defect type via iterative pruning"
  - [section 3.1]: "Overall, NNrelief finds kernels that propagate on average the lowest signal... we obtain a subnetwork (sub-U-Net) that predicts the defect for only one type of defects. Then we fix all parameters that are assigned to this subnetwork and do not update them anymore."
- Break condition: If defects share significant visual features or if the subnetwork pruning removes critical parameters needed for future tasks, performance will degrade.

### Mechanism 2
- Claim: LDA accurately selects the correct subnetwork at inference time.
- Mechanism: LDA is trained on feature embeddings extracted from a pretrained EfficientNet-B5 to learn class means and shared covariance for each defect type. At inference, it predicts the defect type from a single test sample, routing to the corresponding subnetwork.
- Core assumption: Defect types form well-separated distributions in the feature space that LDA can discriminate.
- Evidence anchors:
  - [abstract]: "At the inference stage, we first predict the defect type with LDA and then predict the surface defects using the selected subnetwork"
  - [section 3.2]: "In contrast to image classification, every task in defect segmentation problems consists of defects of only one type. This represents an opportunity for architectural continual learning methods because we can train a model that learns the distribution of each defect separately."
- Break condition: If defect types have overlapping distributions or if the feature extractor fails to capture distinguishing characteristics, LDA will misclassify and select the wrong subnetwork.

### Mechanism 3
- Claim: Fixed subnetworks maintain knowledge while new subnetworks learn with available parameters.
- Mechanism: The base U-Net architecture is pruned to create space for new subnetworks. Each new task is allocated free parameters from the original network, allowing learning without affecting frozen subnetworks.
- Core assumption: Sufficient free parameters remain after pruning to learn new tasks without network saturation.
- Evidence anchors:
  - [section 3.1]: "CP&S finds a subnetwork for this task within the main U-Net, using the parameters assigned to the previous tasks, but without updating them."
  - [section 4.3]: "The results on the Magnetic tile dataset show the trade-off between learning the first tasks and the last ones: if we prune the network twice, α = 0.9 leads to better performance if there are no more than three tasks, while α = 0.85 is better suitable for longer task sequences."
- Break condition: If pruning is too aggressive or too many tasks are learned, network saturation occurs and new tasks cannot be learned effectively.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why traditional finetuning fails is crucial for appreciating the need for this approach
  - Quick check question: What happens to performance on previous tasks when you finetune a network on new data?

- Concept: Continual learning categories (regularization, replay, architectural)
  - Why needed here: LDA-CP&S falls into the architectural category, which is important for understanding its design choices
  - Quick check question: How do architectural approaches to continual learning differ from regularization-based approaches?

- Concept: Linear discriminant analysis (LDA)
  - Why needed here: LDA is the key component for subnetwork selection, so understanding its assumptions and mechanics is essential
  - Quick check question: What assumption does LDA make about class distributions that makes it suitable for this application?

## Architecture Onboarding

- Component map: EfficientNet-B5 feature extractor -> LDA classifier -> U-Net with iterative pruning -> defect-specific subnetworks
- Critical path:
  1. Pretrain feature extractor on ImageNet
  2. Train base U-Net on first defect type with iterative pruning
  3. For each subsequent defect type:
     - Update LDA with new class mean and covariance
     - Prune base U-Net to create new subnetwork
     - Train new subnetwork on current defect type
  4. At inference: use LDA to select subnetwork, then segment
- Design tradeoffs:
  - Aggressive pruning (low α) creates more free parameters but may reduce segmentation quality
  - Number of pruning iterations affects sparsity and expressivity
  - Feature extractor choice impacts LDA accuracy
- Failure signatures:
  - Performance degradation on early tasks indicates forgetting (should not occur with frozen subnetworks)
  - Poor segmentation on any task suggests incorrect subnetwork selection or insufficient pruning
  - Network saturation manifests as inability to learn new tasks
- First 3 experiments:
  1. Verify subnetwork creation: Train on first defect type, then check if performance drops to near-zero on second defect type before subnetwork creation
  2. Test LDA selection: Use feature extractor to extract embeddings from all defect types, verify LDA can correctly classify them
  3. Validate no forgetting: After learning multiple tasks, verify performance on first task remains at initial levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed LDA-CP&S method perform on datasets with more than five types of defects, and what are the limits of scalability in terms of the number of defect types?
- Basis in paper: [inferred] The paper evaluates the method on datasets with three and five types of defects, but does not explore datasets with more defect types or discuss scalability limits.
- Why unresolved: The paper does not provide experimental results or analysis for datasets with more than five defect types, leaving the scalability of the method unexplored.
- What evidence would resolve it: Experimental results demonstrating the performance of LDA-CP&S on datasets with varying numbers of defect types, especially those with more than five types, would clarify the method's scalability limits.

### Open Question 2
- Question: How does the choice of the feature extractor (EfficientNet-B5) affect the performance of LDA-CP&S, and are there alternative feature extractors that could potentially improve the method's accuracy?
- Basis in paper: [explicit] The paper mentions the use of EfficientNet-B5 as the feature extractor and notes its importance for LDA classification accuracy, but does not explore alternative feature extractors.
- Why unresolved: The paper does not compare the performance of LDA-CP&S using different feature extractors, leaving the impact of this choice on the method's accuracy unclear.
- What evidence would resolve it: Comparative experiments using various feature extractors (e.g., ResNet, VGG) would reveal the impact of the feature extractor choice on LDA-CP&S performance and identify potential alternatives for improvement.

### Open Question 3
- Question: How does the performance of LDA-CP&S compare to other continual learning methods when applied to non-surface defect segmentation tasks, such as medical image segmentation or satellite image analysis?
- Basis in paper: [inferred] The paper focuses on surface defect segmentation and does not explore the method's applicability or performance on other types of segmentation tasks.
- Why unresolved: The paper does not provide experimental results or analysis for LDA-CP&S applied to non-surface defect segmentation tasks, leaving its generalizability and performance on other tasks unknown.
- What evidence would resolve it: Experimental results demonstrating the performance of LDA-CP&S on various non-surface defect segmentation tasks (e.g., medical image segmentation, satellite image analysis) would clarify its generalizability and effectiveness across different domains.

## Limitations

- Network saturation risk: The approach relies on iterative pruning to create free parameters for new tasks, but there's no analysis of theoretical limits on the number of tasks that can be learned before network capacity is exhausted
- Generalization across defect types: The method assumes defect types are sufficiently distinct to warrant separate subnetworks, but performance may degrade when defect types share significant visual features
- Hyperparameter sensitivity: Critical parameters like pruning intensity and feature extractor choices significantly impact performance, but the paper doesn't explore sensitivity to these choices or provide selection guidelines

## Confidence

- High Confidence: The core mechanism of creating frozen subnetworks to prevent catastrophic forgetting is well-supported by experimental results
- Medium Confidence: The claim of "two times better" performance than existing methods is based on comparisons with specific baselines
- Low Confidence: The assertion that this is the "first work" in continual learning for surface defect segmentation is difficult to verify

## Next Checks

1. **Network Saturation Test**: Systematically vary the number of tasks and pruning intensity to identify the point at which network saturation occurs
2. **Cross-Dataset Generalization**: Evaluate the approach on additional surface defect datasets with varying numbers of defect types and different visual characteristics
3. **LDA Robustness Analysis**: Test LDA performance with varying numbers of samples per defect type and under conditions of overlapping defect distributions