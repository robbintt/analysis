---
ver: rpa2
title: Click-aware Structure Transfer with Sample Weight Assignment for Post-Click
  Conversion Rate Estimation
arxiv_id: '2304.01169'
source_url: https://arxiv.org/abs/2304.01169
tags:
- knowledge
- task
- information
- which
- click
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the data sparsity problem in post-click conversion
  rate (CVR) prediction by proposing a multi-task learning framework with three key
  innovations. First, it mines task-independent structure information from click-through
  rate (CTR) data to filter relevant CVR knowledge instead of directly sharing feature
  representations.
---

# Click-aware Structure Transfer with Sample Weight Assignment for Post-Click Conversion Rate Estimation

## Quick Facts
- arXiv ID: 2304.01169
- Source URL: https://arxiv.org/abs/2304.01169
- Reference count: 10
- Key outcome: Proposed CSTWA model achieves up to 7.43% improvement in purchase AUC on public datasets and 1.47% on industrial datasets for post-click conversion rate estimation

## Executive Summary
This paper addresses the data sparsity problem in post-click conversion rate (CVR) prediction by proposing a multi-task learning framework that transfers knowledge from click-through rate (CTR) data. The proposed Click-aware Structure Transfer with Sample Weight Assignment (CSTWA) model introduces three key innovations: mining task-independent structure information from CTR data through collaborative filtering relationships, modeling click bias information to distinguish clicked from non-clicked samples, and implementing a sample weight assignment algorithm that emphasizes contradictory samples. The model significantly outperforms existing methods on both public and industrial datasets, achieving state-of-the-art performance in CVR prediction.

## Method Summary
The CSTWA framework consists of a multi-task learning architecture that leverages pre-trained CTR embeddings to improve CVR prediction. The method extracts collaborative filtering relationships (user-user and item-item similarities) from CTR embeddings, constructs graphs based on these relationships, and uses message passing to propagate structural knowledge to the CVR task. A Click Perceptron component injects CTR bias information into CVR modeling through multiplicative scaling, while the Curse Escaper algorithm assigns higher weights to samples where CTR and CVR predictions disagree. The model is trained end-to-end with L2 regularization and Adam optimizer, using the entire impression space for training to address sample selection bias.

## Key Results
- Achieved up to 7.43% improvement in purchase AUC on Ali-CCP public dataset
- Obtained 1.47% improvement in purchase AUC on industrial dataset
- Outperformed existing state-of-the-art models including ESMM, DISN, and other multi-task learning approaches
- Demonstrated effectiveness of sample weight assignment in emphasizing contradictory samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mining latent task-independent structure information from CTR can filter out CTR-related noise while preserving CVR-relevant signals.
- **Mechanism:** The model extracts collaborative filtering relationships (item-item and user-user similarities) from pre-trained CTR embeddings, constructs graphs based on these relationships, and uses them to guide CVR feature learning through message passing. This transfers structural knowledge that is orthogonal to task-specific objectives.
- **Core assumption:** The collaborative filtering patterns learned from CTR data (what items/users are similar) are independent of the task (click vs conversion) and thus transferable to CVR modeling.
- **Evidence anchors:**
  - [abstract]: "We mine the task-independent structure information (i.e., latent structure information) from the CTR and construct the Structure Migrator to inject it into the CVR feature representations."
  - [section]: "users are more likely to interact with items that are similar to those they have interacted with before, rather than different ones. This collaborative filtering relationship, also known as structure information, is independent of the task."
- **Break condition:** If collaborative filtering patterns are task-dependent (e.g., clicked items are systematically different from purchased items), transferring CTR structure would harm CVR performance.

### Mechanism 2
- **Claim:** Modeling click bias information enables the model to distinguish between clicked and non-clicked samples, preventing CTR knowledge from overwhelming CVR signals.
- **Mechanism:** The Click Perceptron component uses the hidden representation from the CTR tower as bias information, injecting it into CVR modeling through multiplicative scaling. This creates sample-specific modulation that captures the difference between clicked and non-clicked impressions.
- **Core assumption:** Clicked and non-clicked samples have fundamentally different characteristics that must be explicitly modeled to avoid conflating CTR and CVR objectives.
- **Evidence anchors:**
  - [abstract]: "we calibrate the representation layer and reweight the discriminant layer to excavate the click bias information from the CTR tower."
  - [section]: "we incorporate the hidden representation of the CTR auxiliary task into CVR modeling as a bias information for the samples."
- **Break condition:** If click bias is negligible or if the multiplicative injection creates instability in training, the performance benefit would diminish.

### Mechanism 3
- **Claim:** Sample weight assignment that emphasizes contradictory samples (high CTR prediction but no conversion, or low CTR prediction but conversion) strengthens the model's ability to capture true conversion behavior.
- **Mechanism:** The Curse Escaper algorithm assigns higher weights to samples where CTR and CVR predictions disagree, making the loss function focus on these informative cases rather than being dominated by the majority of samples where CTR and CVR align.
- **Core assumption:** Samples with contradictory CTR/CVR signals contain the most valuable information for learning true conversion behavior and are underrepresented in standard training.
- **Evidence anchors:**
  - [abstract]: "Moreover, it incorporates a sample weight assignment algorithm biased towards CVR modeling, to make the knowledge from CTR would not mislead the CVR."
  - [section]: "we increase the weights of the false positive samples (i.e., samples where the CTR predictive value is high but no conversion behavior actually occurs) and the false negative samples (i.e., samples where the CTR predictive value is low but conversion behavior actually occurs) in the loss function."
- **Break condition:** If the weight assignment becomes too aggressive (γ too high), it may overfit to noisy contradictory samples or create training instability.

## Foundational Learning

- **Concept:** Multi-task learning and knowledge transfer between related tasks
  - **Why needed here:** The paper builds on multi-task learning framework but addresses its limitations when tasks are contradictory rather than complementary
  - **Quick check question:** Why can't we simply share all feature representations between CTR and CVR tasks?

- **Concept:** Graph neural networks and message passing for structure propagation
  - **Why needed here:** The Structure Migrator uses graph-based message passing to propagate collaborative filtering information through user-item networks
  - **Quick check question:** How does message passing differ from direct feature sharing in transferring knowledge between tasks?

- **Concept:** Sample selection bias and its impact on conversion rate estimation
  - **Why needed here:** The paper addresses the fundamental problem that CVR data is sparse because conversion only occurs after clicks, creating biased training data
  - **Quick check question:** What is the difference between sample selection bias and data sparsity in CVR prediction?

## Architecture Onboarding

- **Component map:** Input Layer → Embedding Layer (CVR-specific) → Structure Migrator (graph message passing) → Click Perceptron (bias injection) → Conversion Tower → Conversion Layer (CVR prediction) + Click Tower → Click Layer (CTR prediction) + Curse Escaper (weighted loss)
- **Critical path:** The flow from pre-trained CTR embeddings through Structure Migrator to CVR tower is the core innovation that distinguishes this from standard multi-task learning
- **Design tradeoffs:** The model trades computational complexity (graph construction and message passing) for improved CVR performance by better handling the CTR-CVR contradiction
- **Failure signatures:** If the Structure Migrator is ineffective, CTR and CVR AUC scores will be similar to baseline multi-task models; if Curse Escaper is too aggressive, training loss may become unstable
- **First 3 experiments:**
  1. Compare CSTWA with and without Structure Migrator on a small dataset to verify the structural transfer mechanism works
  2. Test different K values (top-K neighbors) in graph construction to find optimal sparsity level
  3. Validate that the weight assignment in Curse Escaper actually increases focus on contradictory samples by examining weighted loss distributions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed CSTWA model handle the trade-off between introducing auxiliary knowledge and protecting valuable CVR-related information, and what is the optimal balance for different datasets?
- **Basis in paper:** [explicit] The paper mentions that a trade-off should be achieved between the introduction of large amounts of auxiliary information and the protection of valuable information related to CVR.
- **Why unresolved:** The paper does not provide specific guidelines on how to determine the optimal balance between auxiliary knowledge and CVR-related information for different datasets.
- **What evidence would resolve it:** Conducting experiments with different datasets and varying the amount of auxiliary knowledge introduced to find the optimal balance for each dataset.

### Open Question 2
- **Question:** How does the sample weight assignment algorithm, Curse Escaper, impact the performance of the CSTWA model in different scenarios, and what are the potential limitations of this approach?
- **Basis in paper:** [explicit] The paper introduces the Curse Escaper algorithm, which emphasizes the role of false negative and false positive samples in the training process.
- **Why unresolved:** The paper does not provide a comprehensive analysis of the algorithm's impact on performance in various scenarios or discuss potential limitations.
- **What evidence would resolve it:** Conducting experiments with different datasets and scenarios to evaluate the algorithm's performance and identify potential limitations.

### Open Question 3
- **Question:** How does the CSTWA model compare to other state-of-the-art models in terms of computational efficiency and scalability, especially for large-scale industrial applications?
- **Basis in paper:** [inferred] The paper mentions that the proposed model achieves state-of-the-art performance on industrial and public datasets, but does not provide a detailed comparison of computational efficiency and scalability.
- **Why unresolved:** The paper does not provide a comprehensive comparison of the CSTWA model's computational efficiency and scalability with other state-of-the-art models.
- **What evidence would resolve it:** Conducting experiments to compare the computational efficiency and scalability of the CSTWA model with other state-of-the-art models on large-scale industrial datasets.

## Limitations

- The effectiveness of structure transfer depends critically on the assumption that collaborative filtering patterns are truly task-independent, which may not hold across all e-commerce domains
- The multiplicative formulation of click bias injection through Click Perceptron may introduce training instability that isn't fully characterized
- The comparison against baselines could be more comprehensive, missing some recent ESMM variants and other advanced multi-task learning approaches

## Confidence

- **High confidence:** The sample weight assignment mechanism (Curse Escaper) is well-grounded in addressing sample selection bias and has clear empirical support
- **Medium confidence:** The CTR bias injection through Click Perceptron shows promise but the multiplicative formulation may introduce training instability that isn't fully characterized
- **Medium confidence:** The overall performance improvements are significant, though the comparison against baselines could be more comprehensive

## Next Checks

1. **Task-dependence ablation:** Systematically vary the correlation between CTR and CVR objectives across different product categories to test the robustness of structure transfer when collaborative filtering patterns become task-dependent

2. **Weight sensitivity analysis:** Conduct a grid search over the γ parameter in Curse Escaper to identify the optimal weighting scheme and characterize the stability region where performance gains are maintained

3. **Cross-domain transfer:** Evaluate the model on datasets from different e-commerce platforms to verify that the CTR-to-CVR knowledge transfer generalizes beyond the specific industrial dataset used in the study