---
ver: rpa2
title: 'ADELT: Transpilation Between Deep Learning Frameworks'
arxiv_id: '2303.03593'
source_url: https://arxiv.org/abs/2303.03593
tags:
- code
- adelt
- layers
- keyword
- placeholder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ADELT is a transpilation method between deep learning frameworks
  that combines few-shot prompting on large language models for code skeleton translation
  with a domain-adversarial approach for API keyword mapping. By learning aligned
  embeddings from unlabeled code corpora and generating keyword dictionaries via cosine
  similarity, ADELT avoids the need for parallel training data.
---

# ADELT: Transpilation Between Deep Learning Frameworks

## Quick Facts
- arXiv ID: 2303.03593
- Source URL: https://arxiv.org/abs/2303.03593
- Reference count: 30
- Key outcome: ADELT achieves 91.29 F1 score on PyTorch-Keras transpilation, 22.76 points higher than prior methods

## Executive Summary
ADELT is a transpilation method between deep learning frameworks that combines few-shot prompting on large language models for code skeleton translation with domain-adversarial training for API keyword mapping. By learning aligned embeddings from unlabeled code corpora and generating keyword dictionaries via cosine similarity, ADELT avoids the need for parallel training data. The method achieves state-of-the-art performance on PyTorch-Keras and PyTorch-MXNet benchmarks while preserving syntactic correctness and enabling easy manual refinement of its keyword dictionary.

## Method Summary
ADELT combines few-shot prompting on large language models for code skeleton translation with domain-adversarial training for API keyword mapping. The method first extracts code skeletons using few-shot prompting on Codex, then trains aligned API embeddings using PyBERT in a domain-adversarial setup. A hierarchical dictionary generation algorithm computes cosine similarity scores between embeddings to create keyword mappings, which are then applied to substitute keywords in the transpiled skeleton. The entire process operates on unlabeled code corpora, eliminating the need for parallel training data.

## Key Results
- Achieves 91.29 F1 score on PyTorch-Keras transpilation benchmark
- Outperforms prior methods by 22.76 points on the same benchmark
- Achieves 100% exact match score for skeletal code transpilation using few-shot prompting

## Why This Works (Mechanism)

### Mechanism 1: Domain-adversarial training for embedding alignment
The domain-adversarial training aligns API keyword embeddings across frameworks without parallel data. A generator maps contextual embeddings to a shared space while a discriminator is trained to distinguish embeddings from different frameworks, forcing embeddings from different frameworks to occupy similar regions in the aligned space. The core assumption is that contextual embeddings capture enough semantic information about API keywords for adversarial alignment to be meaningful. Break condition: If contextual embeddings are too sparse or noisy, adversarial alignment fails to produce meaningful correspondences.

### Mechanism 2: Few-shot prompting for code skeleton transpilation
Few-shot prompting on large language models reliably transpiles code skeletons because skeletal structure is highly predictable in DL programs. Code skeletons retain overall program structure but remove API-specific details, allowing Codex to generalize from few examples due to repetitive and constrained DL code patterns. The core assumption is that DL code skeletons have enough regularity that LM generalization works reliably without task-specific fine-tuning. Break condition: If code skeletons become too diverse or complex, few-shot prompting may fail to capture necessary patterns.

### Mechanism 3: Hierarchical dictionary generation
The hierarchical dictionary generation algorithm leverages the structured nature of API keywords to improve translation quality. API keywords are grouped by callable/parameter type, similarity scores are computed at both group and individual keyword levels, and greedy matching is applied hierarchically to preserve semantic relationships. The core assumption is that API keywords have inherent structure (callables vs parameters) that can be exploited for better matching than flat word-by-word translation. Break condition: If API keywords lack clear structural boundaries or if the threshold for parameter-callable translation is poorly chosen.

## Foundational Learning

- **Domain-adversarial training in representation learning**
  - Why needed: Enables alignment of embeddings from different frameworks without requiring parallel training data
  - Quick check: What is the role of the discriminator in domain-adversarial training?

- **Few-shot prompting in large language models**
  - Why needed: Allows code skeleton transpilation without task-specific fine-tuning of the LM
  - Quick check: Why might few-shot prompting be sufficient for DL code skeleton transpilation?

- **Contextual embeddings from pretrained language models**
  - Why needed: Captures semantic information about API keywords in their usage context
  - Quick check: How do contextual embeddings differ from static word embeddings in this application?

## Architecture Onboarding

- **Component map**: PyBERT (contextual embedder) → Generator (hidden state mapping) → Discriminator (framework classifier) → Output embeddings (API keyword representations) → Dictionary generation (cosine similarity + hierarchical matching) → Skeletal code transpilation (Codex few-shot prompting)
- **Critical path**: Code skeleton extraction → Few-shot prompting → Keyword embedding extraction → Domain-adversarial training → Dictionary generation → Keyword substitution
- **Design tradeoffs**: Decoupling skeleton transpilation from keyword mapping allows using specialized methods for each, but requires careful coordination between components
- **Failure signatures**: Low F1 scores suggest keyword mapping problems; high BLEU but low exact match suggests skeleton transpilation issues
- **First 3 experiments**:
  1. Verify few-shot prompting correctly transpiles a simple skeleton (e.g., Linear layer)
  2. Test domain-adversarial training produces aligned embeddings (check cosine similarity between matched keywords)
  3. Validate hierarchical dictionary generation preserves parameter-callable relationships

## Open Questions the Paper Calls Out

### Open Question 1
How does ADELT perform when transpiling between deep learning frameworks with significantly different architectural paradigms (e.g., PyTorch to TensorFlow graphs)? The paper focuses on transpiling between frameworks with similar paradigms but does not explore scenarios with fundamentally different architectures.

### Open Question 2
What is the impact of varying the size and quality of the unlabeled DL corpus on ADELT's performance? The paper mentions using a large-scale DL corpus but does not explore how different corpus sizes or qualities affect the model's performance.

### Open Question 3
How does ADELT handle transpilation of complex, non-standard API usages or custom layers that are not well-represented in the training data? The paper demonstrates effectiveness on standard API usages but does not address its performance on non-standard or custom layer transpilation.

## Limitations

- The 100% exact match for code skeleton transpilation is reported without showing generalization beyond the specific dataset used
- Domain-adversarial training assumes contextual embeddings capture sufficient semantic information without empirical validation
- Hierarchical dictionary generation shows effectiveness on the paper's dataset but lacks comparison against unstructured baselines
- Performance on more complex code patterns beyond the tested frameworks remains unknown

## Confidence

- **High confidence**: Domain-adversarial training mechanism for embedding alignment is well-established in literature
- **Medium confidence**: Few-shot prompting approach for code skeleton transpilation shows strong results on benchmark
- **Medium confidence**: Hierarchical dictionary generation improves translation quality for structured API keywords

## Next Checks

1. Test few-shot prompting generalization by evaluating on code skeletons not present in the training corpus, including more complex architectural patterns
2. Validate that contextual embeddings contain sufficient semantic information by measuring cosine similarity between manually verified matching API keywords before and after domain-adversarial training
3. Compare hierarchical dictionary generation against flat word-by-word translation on datasets with varying levels of API keyword structure to determine when the hierarchical approach provides benefits