---
ver: rpa2
title: Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals
arxiv_id: '2310.00603'
source_url: https://arxiv.org/abs/2310.00603
tags:
- causal
- matching
- methods
- explanation
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes two approaches for model-agnostic explanations
  of black-box NLP models using counterfactuals. The first approach generates counterfactuals
  using LLMs, while the second learns a causal embedding space to match examples to
  counterfactuals.
---

# Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals

## Quick Facts
- **arXiv ID:** 2310.00603
- **Source URL:** https://arxiv.org/abs/2310.00603
- **Reference count:** 40
- **Primary result:** Two novel approaches for model-agnostic explanations using LLM-generated counterfactuals and causal representation matching

## Executive Summary
This paper addresses the challenge of explaining black-box NLP models by introducing two model-agnostic approaches that leverage counterfactuals. The first approach generates counterfactuals using large language models (LLMs), while the second learns a causal embedding space to match examples to counterfactuals. The authors theoretically prove that their approximated counterfactual methods are order-faithful, providing a new criterion for faithful explanations. Empirically, they demonstrate that their approaches outperform existing methods on the CEBaB benchmark and a new stance detection dataset, with the counterfactual generation approach providing the strongest explanations and the matching approach offering competitive performance with greater efficiency.

## Method Summary
The paper proposes two complementary approaches for explaining black-box NLP models. The first approach uses LLMs to generate counterfactual examples that intervene on specified concepts, creating pairs of original and counterfactual texts. The second approach learns a causal representation space using a contrastive objective that pulls query examples toward their valid counterfactuals and matches while pushing them away from misspecified examples. Both methods estimate causal effects by comparing model predictions on original and counterfactual/matched texts, providing explanations that are theoretically guaranteed to be order-faithful under certain assumptions.

## Key Results
- The causal representation matching approach outperforms all matching baselines, including the best-performing method from the CEBaB paper
- Top-K matching universally improves the explanation capabilities of every tested method
- The counterfactual generation approach provides the strongest explanations overall, though the matching approach is more computationally efficient
- A new stance detection dataset is introduced for evaluating explanation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Approximated counterfactual explanation methods are always order-faithful
- Mechanism: The method estimates causal effect by comparing model predictions on original input with approximated counterfactuals. The expected prediction of an approximated counterfactual equals the interventional prediction (conditioned on the do operator), making the method an unbiased estimator of the causal effect
- Core assumption: The approximation error has zero mean
- Evidence anchors:
  - [abstract]: "we propose a simple, intuitive, and essential criterion: Order-faithfulness...we prove the following theorem, which elucidates why approximated CF methods are always order-faithful, unlike non-causal ones."
  - [section 3.2]: "From Lemma 1 we know that SCF is order-faithful for any DGP and (1) holds...Thus, SCF is an unbiased estimator of CaCEf, and it holds that..."
  - [corpus]: Weak - no direct citations found, but this is a novel theoretical contribution
- Break condition: If the approximation error has non-zero mean or is systematically biased, the order-faithfulness property may be violated

### Mechanism 2
- Claim: Top-K matching universally improves the explanation capabilities of every method
- Mechanism: By considering multiple counterfactuals instead of a single one, the variance of the approximation error is reduced, making the causal effect estimator more robust
- Core assumption: The multiple counterfactuals generated or matched are diverse and representative of the intervention
- Evidence anchors:
  - [abstract]: "We also find that Top-K techniques universally improve every tested method."
  - [section 5]: "When comparing the top rows of Table 2 (k = 1) to the bottom rows (k = 10), it is easy to observe that Top-K matching lowers the Err of every examined explanation method."
  - [corpus]: Weak - no direct citations found, but this is an empirical observation from the paper's experiments
- Break condition: If the multiple counterfactuals are not diverse or representative, or if the number of counterfactuals (K) is too large, the improvement may diminish or even degrade the explanation quality

### Mechanism 3
- Claim: The causal representation model for matching outperforms all matching baselines
- Mechanism: The model learns to encode texts in a space faithful to the causal graph by minimizing an objective that attracts the query example to its counterfactuals and matches while repelling it from misspecified counterfactuals and matches
- Core assumption: The LLM-generated counterfactuals and matches are valid and representative of the intervention and adjustment variables
- Evidence anchors:
  - [abstract]: "Our method for learning causal representations for matching outperforms all the matching baselines, including the best-performing method from the CEBaB paper."
  - [section 5.2]: "Table 2 also sheds light on the performance of our novel representation method in comparison to various explainers and matching baselines. Notably, our causal model consistently outperforms all the matching baselines, achieving substantially lower errors across five explained models."
  - [corpus]: Weak - no direct citations found, but this is an empirical observation from the paper's experiments
- Break condition: If the LLM-generated counterfactuals or matches are invalid or not representative, or if the objective function is not properly designed, the model may not learn the desired representation space

## Foundational Learning

- Concept: Causal graphs and the back-door criterion
  - Why needed here: To identify the adjustment variables (confounders) that need to be controlled for when estimating the causal effect of a concept on the model's prediction
  - Quick check question: In the causal graph shown in Figure 1, which variables should be adjusted for when estimating the causal effect of the "service" concept on the model's prediction?

- Concept: Counterfactuals and the do-calculus
  - Why needed here: To understand how to estimate the causal effect by comparing the model's prediction on the original input with its prediction on a counterfactual where the intervention has been applied
  - Quick check question: What is the difference between a counterfactual and an interventional distribution, and how are they related in the context of estimating causal effects?

- Concept: Representation learning and contrastive objectives
  - Why needed here: To understand how the causal representation model learns to encode texts in a space faithful to the causal graph by minimizing a contrastive objective that attracts the query example to its counterfactuals and matches while repelling it from misspecified ones
  - Quick check question: How does the contrastive objective in Equation 6 encourage the model to learn the desired representation space, and what are the roles of the different sets (XCF, XM, X¬CF, X¬M) in this objective?

## Architecture Onboarding

- Component map: Input text -> Concept intervention -> Counterfactual generation/matching -> Model prediction -> Causal effect estimation -> Explanation
- Critical path: Input text → Concept intervention → Counterfactual generation/matching → Model prediction → Causal effect estimation → Explanation
- Design tradeoffs:
  - Counterfactual generation: High quality but computationally expensive, especially for large models or batch processing
  - Matching: Computationally efficient but depends on the quality and representativeness of the candidate set
  - Top-K: Improves explanation quality but increases computational cost and may require tuning the K parameter
- Failure signatures:
  - Low explanation quality: Counterfactuals/matches are not diverse or representative, or the causal representation model is not properly trained
  - High computational cost: Using large generative models for counterfactual generation or a large candidate set for matching
  - Incorrect causal effect estimation: Failing to control for the right adjustment variables or using a non-faithful explanation method
- First 3 experiments:
  1. Implement the causal representation model and train it on a small dataset to verify that it can learn the desired representation space
  2. Use the trained model to find matches for a given input text and intervention, and verify that the matches are diverse and representative
  3. Estimate the causal effect of a concept on a simple model's prediction using the matched counterfactuals, and compare the result with a ground truth or baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed causal representation model compare to other model-agnostic explanation methods that do not rely on LLMs?
- Basis in paper: Inferred - The paper mentions that the proposed causal representation model outperforms other matching baselines and is competitive with generative models that use LLMs
- Why unresolved: The paper does not provide a direct comparison between the causal representation model and other model-agnostic explanation methods that do not utilize LLMs
- What evidence would resolve it: Conducting experiments comparing the proposed causal representation model to other model-agnostic explanation methods that do not rely on LLMs, such as feature importance methods or local interpretable model-agnostic explanations (LIME)

### Open Question 2
- Question: How sensitive is the proposed causal representation model to the choice of hyperparameters, such as the temperature parameter τ and the learning rate?
- Basis in paper: Inferred - The paper mentions that the hyperparameter τ is set to 0.1 and the learning rate is set to 5e-6, but does not provide an analysis of the sensitivity of the model to these hyperparameters
- Why unresolved: The paper does not provide an ablation study or sensitivity analysis of the proposed causal representation model to the choice of hyperparameters
- What evidence would resolve it: Conducting experiments to analyze the sensitivity of the proposed causal representation model to the choice of hyperparameters, such as varying the temperature parameter τ and the learning rate, and measuring the impact on the model's performance

### Open Question 3
- Question: How does the proposed causal representation model perform when explaining models on datasets with different characteristics, such as different languages or domains?
- Basis in paper: Inferred - The paper demonstrates the effectiveness of the proposed causal representation model on the CEBaB benchmark and a new stance detection dataset, but does not provide an analysis of its performance on datasets with different characteristics
- Why unresolved: The paper does not provide experiments or analysis of the proposed causal representation model's performance on datasets with different languages or domains
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed causal representation model on datasets with different languages or domains, such as text classification datasets in different languages or sentiment analysis datasets in different domains

## Limitations
- The order-faithfulness proof relies on the approximation error having zero mean, which may not hold in practice for imperfect LLM-generated counterfactuals
- The causal representation model's performance is contingent on the quality and representativeness of LLM-generated counterfactuals and matches
- Top-K matching's universal improvement claim is empirically observed but lacks theoretical justification
- The evaluation focuses on binary classification tasks, limiting generalizability to other prediction types

## Confidence
- **High**: The theoretical framework connecting counterfactuals to causal effect estimation
- **Medium**: The empirical superiority of the causal representation matching approach over baselines
- **Low**: The universality of Top-K improvements across all explanation methods

## Next Checks
1. Test the causal representation model's robustness when given LLM-generated counterfactuals with varying quality levels
2. Evaluate the approach on multi-class classification tasks to assess generalizability
3. Conduct ablation studies to quantify the impact of different LLM prompts on counterfactual quality and downstream explanation performance