---
ver: rpa2
title: 'EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level
  Planning'
arxiv_id: '2312.06722'
source_url: https://arxiv.org/abs/2312.06722
tags:
- task
- arxiv
- embodied
- planning
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EgoPlan-Bench addresses the gap in evaluating Multimodal Large
  Language Models (MLLMs) for real-world embodied planning. It provides a comprehensive
  benchmark based on egocentric videos, featuring realistic tasks, diverse actions,
  and complex visual observations.
---

# EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning

## Quick Facts
- arXiv ID: 2312.06722
- Source URL: https://arxiv.org/abs/2312.06722
- Reference count: 40
- Primary result: Current MLLMs achieve only 34.40% accuracy on embodied planning tasks

## Executive Summary
EgoPlan-Bench addresses a critical gap in evaluating Multimodal Large Language Models (MLLMs) for real-world embodied planning capabilities. The benchmark leverages egocentric videos from kitchen and household environments to create realistic planning scenarios requiring models to predict the next executable action based on task progress, visual observations, and language instructions. Through comprehensive evaluation of various open-source MLLMs, the benchmark reveals significant limitations in current models' ability to act as effective real-world embodied planners, with the highest-performing model achieving only 34.40% accuracy.

To advance this field, the authors construct EgoPlan-IT, an instruction-tuning dataset derived from the same video sources, enabling MLLMs to learn embodied planning experience from real-world demonstrations. Models fine-tuned on this dataset show significant performance improvements on the benchmark and demonstrate potential as task planners for embodied agents in simulated environments. The benchmark includes 3.4K high-quality human-verified questions and employs a multiple-choice evaluation format to provide objective assessment of planning capabilities.

## Method Summary
EgoPlan-Bench is constructed through hierarchical task decomposition of egocentric videos from Epic-Kitchens and Ego4D datasets, creating realistic planning scenarios. The process involves automatically generating multiple-choice questions based on task goals and actions, aligning visual inputs with questions, and human verification of QA pairs. For evaluation, models must predict the next executable action given task progress, current visual observation, and language instructions. The instruction-tuning dataset EgoPlan-IT is constructed using the same pipeline and fine-tuned on a Video-LLaMA baseline using contrastive loss and LoRA fine-tuning. The benchmark employs a ranking strategy where models select the choice with the highest likelihood as their prediction.

## Key Results
- Current MLLMs achieve only 34.40% accuracy on EgoPlan-Bench, indicating significant limitations in embodied planning
- Models fine-tuned on EgoPlan-IT show substantial performance improvements on both in-domain and out-of-domain subsets
- The enhanced model demonstrates potential as an effective task planner in simulated environments
- MLLMs struggle with detecting subtle visual state changes and often neglect crucial visual observations when making planning decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's effectiveness stems from using real-world egocentric videos with hierarchical task decomposition.
- Mechanism: By extracting tasks from actual human activity videos, the benchmark creates realistic planning scenarios that require understanding complex visual observations and temporal task progress. The hierarchical decomposition (overall goals → subgoals → secondary subgoals) provides appropriate task granularity for evaluation.
- Core assumption: Real-world video tasks capture the complexity needed to evaluate embodied planning capabilities better than artificial tasks.
- Evidence anchors:
  - [abstract] "EgoPlan-Bench emphasizes the evaluation of planning capabilities of MLLMs, featuring realistic tasks derived from real-world videos, diverse action plans, and intricate visual observations."
  - [section] "Unscripted egocentric videos captured in kitchen scenes often present a dense sequence of diverse manipulation actions in daily household tasks, which is suitable and challenging for evaluating embodied planning abilities."
- Break condition: If the hierarchical decomposition process introduces artifacts or misses crucial task relationships, the benchmark would not accurately reflect real-world planning challenges.

### Mechanism 2
- Claim: The instruction-tuning dataset improves planning performance by providing experience-based learning from real-world videos.
- Mechanism: The EgoPlan-IT dataset is constructed from real-world videos using the same pipeline as the benchmark, allowing models to learn planning patterns through exposure to diverse, realistic scenarios. The contrastive loss and LoRA fine-tuning ensure the model learns to align visual context with language instructions.
- Core assumption: Learning from real-world video demonstrations can effectively transfer to planning capabilities in new environments.
- Evidence anchors:
  - [abstract] "The model tuned on EgoPlan-IT not only significantly improves performance on our benchmark, but also effectively acts as embodied planner in simulations."
  - [section] "Using this dataset, we refine and enhance an MLLM. The experimental outcomes show that the model's performance is significantly improved through instruction-tuning."
- Break condition: If the model overfits to the specific video patterns in the training data without learning generalizable planning principles, transfer to new environments would fail.

### Mechanism 3
- Claim: The multiple-choice question format provides more objective evaluation than open-form prediction.
- Mechanism: By presenting models with discrete action choices, the evaluation eliminates ambiguity in scoring and focuses assessment on the model's ability to select the correct next action given visual and task context. The negative choices are sampled from the same task to ensure semantic similarity.
- Core assumption: Multiple-choice format better isolates the planning capability from language generation abilities.
- Evidence anchors:
  - [section] "We evaluate Egocentric Embodied Planning with multiple-choice questions, which provides a more objective assessment of models compared with open-form prediction."
- Break condition: If the negative choices are too easily distinguishable from the correct answer through superficial features rather than genuine planning understanding, the evaluation loses validity.

## Foundational Learning

- Concept: Multimodal integration in large language models
  - Why needed here: MLLMs must combine visual perception with language understanding to make planning decisions
  - Quick check question: What architectural components allow MLLMs to process both video frames and text instructions simultaneously?

- Concept: Hierarchical task decomposition
  - Why needed here: Breaking down complex real-world activities into manageable planning subgoals is essential for evaluating planning capabilities
  - Quick check question: How does hierarchical task decomposition help in creating appropriate evaluation scenarios for embodied planning?

- Concept: Contrastive learning for instruction tuning
  - Why needed here: The contrastive loss ensures the model learns to distinguish correct planning actions from semantically similar but incorrect options
  - Quick check question: Why is contrastive loss particularly effective for instruction tuning in planning tasks compared to standard cross-entropy loss?

## Architecture Onboarding

- Component map: Video-LLaMA baseline → Video Q-Former (video processing) + Linear Layer (embedding matching) + LoRA-modified LLM (planning). Instruction tuning adds contrastive loss and diverse instruction mixing.
- Critical path: Video frames → Video Q-Former → Linear layer → LLM input → Action prediction. The critical decision point is the LLM's selection of the next action.
- Design tradeoffs: Hierarchical video processing (K keyframes per clip × N clips) vs. computational cost; multiple-choice evaluation vs. open-form flexibility; LoRA fine-tuning vs. full fine-tuning.
- Failure signatures: Models selecting actions based on semantic similarity to instructions rather than visual context; inability to track object state changes across video frames; poor transfer to new environments despite in-domain success.
- First 3 experiments:
  1. Evaluate baseline Video-LLaMA on EgoPlan-Bench to establish performance floor
  2. Test enhanced model on in-domain vs. out-of-domain subsets to measure transfer capability
  3. Ablation study removing contrastive loss to quantify its contribution to performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve MLLMs' ability to detect subtle state changes in visual inputs during task progress?
- Basis in paper: [explicit] The paper mentions that GPT-4V, considered as an upper bound for MLLMs, fails to recognize subtle state changes like yogurt being added to meat, which are critical for task completion.
- Why unresolved: The paper highlights this as a key area for improvement but does not provide a solution or detailed approach to address this issue.
- What evidence would resolve it: Developing and testing new algorithms or training datasets that focus on detecting subtle visual changes, and evaluating their impact on MLLM performance in Egocentric Embodied Planning tasks.

### Open Question 2
- Question: How can MLLMs be enhanced to make more informed plans based on comprehensive visual input analysis relevant to specific task goals?
- Basis in paper: [explicit] The paper suggests that current MLLMs tend to neglect crucial visual observations, impacting their overall performance in Egocentric Embodied Planning.
- Why unresolved: The paper identifies this as a challenge but does not propose specific methods to improve contextual understanding of MLLMs in relation to task goals.
- What evidence would resolve it: Implementing and testing methods that integrate real-time feedback mechanisms and evaluating their effectiveness in improving MLLM planning accuracy in dynamic environments.

### Open Question 3
- Question: How effective is the instruction-tuning dataset EgoPlan-IT in improving MLLM performance on out-of-domain tasks?
- Basis in paper: [explicit] The paper constructs EgoPlan-IT and shows improved performance on both in-domain (Epic-Kitchens) and out-of-domain (Ego4D) subsets, but the long-term effectiveness and limitations are not fully explored.
- Why unresolved: The paper provides initial results but does not explore the long-term effectiveness or potential limitations of using EgoPlan-IT for out-of-domain tasks.
- What evidence would resolve it: Conducting extended studies to evaluate the performance of MLLMs fine-tuned with EgoPlan-IT on a broader range of out-of-domain tasks, and analyzing the factors that contribute to or hinder their effectiveness.

## Limitations

- The benchmark's focus on kitchen and household environments may limit generalizability to other real-world domains
- Reliance on human-verified QA pairs introduces potential subjectivity in the verification process without quantitative measures of inter-annotator agreement
- The performance gap between in-domain and out-of-domain evaluation remains an important uncertainty for assessing true planning capabilities

## Confidence

- **High Confidence**: The benchmark's construction methodology and evaluation framework are clearly specified and internally consistent. The claim that current MLLMs struggle with embodied planning is well-supported by the empirical results showing maximum 34.40% accuracy.
- **Medium Confidence**: The assertion that EgoPlan-IT significantly improves planning performance is supported but could benefit from more detailed ablation studies isolating the contribution of different training components.
- **Medium Confidence**: The claim about effective transfer to simulated environments is promising but based on limited demonstration rather than comprehensive evaluation across diverse simulation scenarios.

## Next Checks

1. Conduct inter-annotator reliability analysis on the human-verified QA pairs to quantify subjectivity and establish quality thresholds for benchmark inclusion.
2. Perform systematic evaluation of model performance across different environmental domains (kitchen, workshop, outdoor) to assess domain generalization capabilities.
3. Implement controlled ablation studies removing individual components of the instruction-tuning pipeline (contrastive loss, LoRA fine-tuning, auxiliary datasets) to quantify their specific contributions to performance gains.