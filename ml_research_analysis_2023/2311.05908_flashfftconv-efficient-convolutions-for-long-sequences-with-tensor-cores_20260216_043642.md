---
ver: rpa2
title: 'FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores'
arxiv_id: '2311.05908'
source_url: https://arxiv.org/abs/2311.05908
tags:
- flashfftconv
- convolution
- sequence
- convolutions
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlashFFTConv optimizes FFT convolutions for long sequences by adapting
  the Monarch decomposition to broadcast matrix multiplies across the input sequence
  instead of batch/hidden dimensions. This enables kernel fusion for sequences up
  to 32K on modern GPUs and exploits tensor cores for matrix operations.
---

# FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores

## Quick Facts
- arXiv ID: 2311.05908
- Source URL: https://arxiv.org/abs/2311.05908
- Reference count: 40
- Key outcome: Speeds up FFT convolutions by up to 7.93x over PyTorch, achieves 4.4x end-to-end speedup for convolutional models

## Executive Summary
FlashFFTConv optimizes FFT convolutions for long sequences by adapting the Monarch decomposition to broadcast matrix multiplies across the input sequence instead of batch/hidden dimensions. This enables kernel fusion for sequences up to 32K on modern GPUs and exploits tensor cores for matrix operations. The system introduces two sparse convolution algorithms - partial convolutions and frequency-sparse convolutions - which can be implemented simply by skipping blocks in the matrix decomposition. FlashFFTConv enables solving previously intractable tasks like high-resolution image classification on Path-512 (sequence length 256K) and embedding the longest human genes at single nucleotide resolution (2.3M base pairs).

## Method Summary
FlashFFTConv adapts the Monarch FFT decomposition to convolutional workloads by broadcasting matrix operations across the input sequence dimension rather than batch/hidden dimensions. This reformulation enables kernel fusion by keeping intermediate results in SRAM instead of spilling to HBM. The system exploits tensor cores for matrix operations and introduces domain-specific optimizations including real-valued FFT computation for half-length computation and zero-padding skipping. Two sparse convolution algorithms are implemented through simple block skipping in the decomposition: partial convolutions (zeroing out later portions of the kernel) and frequency-sparse convolutions (zeroing out portions of the kernel in frequency space).

## Key Results
- 7.93x speedup over PyTorch FFT convolutions for long sequences
- 4.4x end-to-end speedup for convolutional models
- Up to 2.3 points better perplexity on language modeling and 3.3 points higher GLUE score given same compute budget
- Enables previously intractable tasks: high-resolution image classification on Path-512 (256K sequence length) and human gene embedding at single nucleotide resolution (2.3M base pairs)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Monarch decomposition enables efficient FFT convolution by replacing permutations with simple matrix transposes in SRAM.
- Mechanism: Traditional FFT implementations require expensive HBM permutations between butterfly stages. The Monarch decomposition reformulates these permutations as transposes on intermediate matrices that fit in SRAM, drastically reducing I/O.
- Core assumption: Transposing small matrices in SRAM is orders of magnitude faster than permuting large sequences in HBM.
- Evidence anchors:
  - [abstract] "FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O."
  - [section] "Instead, we broadcast the matrix operation across the entire sequence... This reduces the SRAM requirements for kernel fusion, since we only need to load a single sequence into SRAM at a time"
  - [corpus] No direct evidence; assumption based on memory hierarchy knowledge.
- Break condition: When sequences become too long to fit even the intermediate matrices in SRAM, requiring HBM permutations again.

### Mechanism 2
- Claim: Broadcasting matrix multiplies across the sequence dimension instead of batch/hidden dimensions enables kernel fusion for longer sequences.
- Mechanism: By treating the entire sequence as the matrix multiply dimension, intermediate results stay in SRAM rather than spilling to HBM, allowing the entire FFT convolution kernel to be fused.
- Core assumption: The sequence dimension is typically much larger than batch/hidden dimensions in long-sequence models, making this decomposition viable.
- Evidence anchors:
  - [abstract] "FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences"
  - [section] "This decomposition introduces a second benefit: a reduction in the amount of the sequence that needs to be kept in SRAM, which makes kernel fusion viable at longer sequence lengths."
  - [corpus] No direct evidence; assumption based on typical sequence modeling workloads.
- Break condition: When the sequence length becomes so large that even the decomposed intermediate matrices exceed SRAM capacity.

### Mechanism 3
- Claim: Partial convolutions and frequency-sparse convolutions map naturally to the Monarch decomposition structure.
- Mechanism: Both techniques can be implemented by simply skipping blocks in the matrix decomposition, reducing memory footprint and compute without requiring separate sparse algorithms.
- Core assumption: The Monarch decomposition structure is flexible enough to accommodate these sparsity patterns through simple block skipping.
- Evidence anchors:
  - [abstract] "We also present two sparse convolution algorithms—1) partial convolutions and 2) frequency-sparse convolutions—which can be implemented simply by skipping blocks in the matrix decomposition"
  - [section] "These can be viewed as convolutional analogues to sparse/approximate attention in Transformers [ 8, 50, 51, 62, 92], and map naturally on to FlashFFTConv: both algorithms can be implemented simply by skipping portions of the matrix decomposition"
  - [corpus] No direct evidence; assumption based on decomposition structure analysis.
- Break condition: When sparsity patterns become too complex to express as simple block skips in the decomposition.

## Foundational Learning

- Concept: Fast Fourier Transform (FFT) convolution
  - Why needed here: Understanding the standard FFT convolution algorithm is essential to grasp why FlashFFTConv's decomposition is beneficial
  - Quick check question: What is the computational complexity of a standard FFT convolution compared to direct convolution?

- Concept: GPU memory hierarchy and tensor cores
  - Why needed here: FlashFFTConv's optimizations specifically target the memory hierarchy bottlenecks and leverage tensor cores for matrix operations
  - Quick check question: What is the approximate speedup of tensor cores over general compute units for matrix multiplication on modern GPUs?

- Concept: Kernel fusion and memory-bound vs compute-bound operations
  - Why needed here: Understanding when kernel fusion is beneficial and the tradeoffs between memory and compute bottlenecks is crucial for FlashFFTConv's design
  - Quick check question: When does kernel fusion typically provide the most benefit in GPU computations?

## Architecture Onboarding

- Component map: Monarch decomposition module -> kernel fusion manager -> domain-specific optimizers (real-valued FFT, causal convolutions) -> sparse convolution handlers (partial, frequency-sparse)

- Critical path: The critical path is the sequence of matrix multiply operations in the Monarch decomposition. Optimizing these operations (using tensor cores, minimizing I/O) provides the most significant performance gains.

- Design tradeoffs: The primary tradeoff is between FLOP count and I/O cost. Higher-order decompositions reduce FLOP count but increase I/O. The system must balance these based on sequence length and hardware characteristics.

- Failure signatures: Out-of-memory errors indicate sequence lengths exceeding SRAM capacity. Low speedup compared to PyTorch suggests the decomposition order is suboptimal for the given sequence length.

- First 3 experiments:
  1. Benchmark a standard FFT convolution against FlashFFTConv for a moderate sequence length (e.g., 4K) to verify the speedup claim.
  2. Vary the Monarch decomposition order (p=2,3,4) for a fixed sequence length to find the optimal configuration.
  3. Implement and benchmark partial convolutions to verify the quality/compute tradeoff and memory savings.

## Open Questions the Paper Calls Out
- Question: How does the order-p Monarch decomposition scale to even longer sequences beyond 4 million? What are the theoretical limits and practical bottlenecks?
  - Basis in paper: [explicit] The paper evaluates FlashFFTConv up to 4 million sequence length and mentions future work on extending support to non-GPU accelerators.
  - Why unresolved: The paper does not explore sequence lengths beyond 4 million or analyze the theoretical scaling limits of the Monarch decomposition.
  - What evidence would resolve it: Benchmarking FlashFFTConv on sequence lengths exceeding 4 million, analyzing memory usage and runtime, and theoretically modeling the scaling behavior of the Monarch decomposition.

- Question: Can the frequency-sparse convolution technique be extended to learn the optimal sparsity pattern during training, rather than using a fixed pattern?
  - Basis in paper: [inferred] The paper presents frequency-sparse convolutions as a static technique but does not explore learning the sparsity pattern.
  - Why unresolved: The paper focuses on the computational benefits of frequency-sparse convolutions but does not investigate whether the sparsity pattern can be optimized for downstream task performance.
  - What evidence would resolve it: Experiments comparing learned vs. fixed sparsity patterns on various tasks, analyzing the impact on model quality and computational efficiency.

- Question: How does FlashFFTConv perform on different hardware architectures beyond GPUs, such as TPUs or custom accelerators?
  - Basis in paper: [explicit] The paper mentions future work on extending support to non-GPU accelerators.
  - Why unresolved: The paper primarily focuses on GPU performance and does not explore the generalizability of FlashFFTConv to other hardware platforms.
  - What evidence would resolve it: Porting FlashFFTConv to TPUs or custom accelerators, benchmarking performance, and comparing against existing implementations on those platforms.

## Limitations
- The Monarch FFT decomposition's correctness and optimality for convolutions specifically requires independent verification
- Experimental evaluation focuses primarily on synthetic benchmarks and two specific models (M2-BERT and Hyena), limiting generalizability
- Quality improvements depend on specific models and tasks tested, may not generalize across all language modeling tasks

## Confidence
**High Confidence**: The general approach of using matrix decomposition to enable kernel fusion for FFT convolutions is sound and well-established. The memory hierarchy benefits and tensor core utilization are theoretically justified.

**Medium Confidence**: The specific implementation details of the Monarch decomposition for convolutions and the claimed speedups are likely correct but require independent verification. The end-to-end quality improvements depend on the specific models and tasks tested.

**Low Confidence**: The exact performance characteristics across different GPU architectures, sequence lengths, and convolutional models cannot be determined without additional experimentation. The scalability to extremely long sequences (millions of elements) may face practical limitations not fully explored.

## Next Checks
1. **Decomposition Verification**: Implement the Monarch FFT decomposition independently and verify that it correctly computes the same results as standard FFT for various convolution sizes and kernel shapes.

2. **Architecture Portability**: Test FlashFFTConv across multiple GPU architectures (different NVIDIA generations, AMD GPUs) to validate that the memory hierarchy assumptions and performance benefits generalize beyond the tested hardware.

3. **Model Generalization**: Evaluate FlashFFTConv on a diverse set of convolutional models beyond M2-BERT and Hyena, including different sequence modeling tasks, image processing applications, and audio processing models to assess the breadth of the claimed improvements.