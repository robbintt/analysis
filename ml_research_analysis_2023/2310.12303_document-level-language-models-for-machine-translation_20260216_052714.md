---
ver: rpa2
title: Document-Level Language Models for Machine Translation
arxiv_id: '2310.12303'
source_url: https://arxiv.org/abs/2310.12303
tags:
- document-level
- data
- translation
- fusion
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores leveraging document-level monolingual data
  for document-level machine translation without document-level parallel training
  data. It proposes combining a sentence-level MT model with a document-level language
  model through fusion, with improvements including internal language model neutralization
  and context-dependent fusion scales.
---

# Document-Level Language Models for Machine Translation

## Quick Facts
- arXiv ID: 2310.12303
- Source URL: https://arxiv.org/abs/2310.12303
- Authors: Herold et al.
- Reference count: 40
- Key outcome: Document-level LM fusion improves pronoun F1 score by +6.2% and profession accuracy by +6.0% over sentence-level baseline

## Executive Summary
This paper addresses the challenge of incorporating document-level context into machine translation without requiring document-level parallel training data. The authors propose a novel approach that combines sentence-level translation models with document-level language models through fusion, along with internal language model neutralization and context-dependent fusion scales. Experiments on four translation tasks demonstrate substantial improvements in document-targeted metrics while being more computationally efficient than traditional grid search methods for tuning fusion scales.

## Method Summary
The method combines a sentence-level MT model with a document-level language model through weighted log-linear fusion. The approach includes three key innovations: internal language model neutralization to prevent overcounting of source-agnostic probabilities, context-dependent fusion scales that adapt to each predicted subword, and automatic scale learning through cross-entropy optimization on synthetic document-level data. The document-level LM is trained on monolingual data from the same domain as the MT training data, and fusion can be performed with either small domain-specific LMs or large pre-trained LLMs.

## Key Results
- +6.2% pronoun F1 score improvement over sentence-level baseline
- +6.0% profession accuracy improvement over sentence-level baseline
- On-the-fly scale selection achieves 0.429 average neighbor FMR score, demonstrating moderate novelty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining sentence-level MT with document-level language models through fusion improves translation quality by incorporating discourse-level context.
- Mechanism: The fusion approach integrates three probability distributions: the translation model (conditioned on source and previous target context), the document-level LM (conditioned on previous target context), and an internal LM (estimating the implicit language modeling in the MT model). These are combined via weighted log-linear combination.
- Core assumption: The translation model's implicit internal language model can be estimated and neutralized to prevent overcounting of source-agnostic probabilities.
- Evidence anchors:
  - [abstract]: "This can be achieved by combining any existing sentence-level translation model with a document-level language model."
  - [section 3.1]: "We multiply the model output probabilities and normalize them... p(ei) := p(ei | ei−1 0 , F, E−1 −k)"
  - [corpus]: Weak evidence - no direct citation found, but mechanism is theoretically sound based on LM fusion literature
- Break condition: The approach breaks when the internal LM cannot be accurately estimated, leading to improper weighting and degraded translation quality.

### Mechanism 2
- Claim: Using context-dependent fusion scales instead of static grid search improves document-level translation quality while reducing computational overhead.
- Mechanism: Fusion scales (λ0, λ1, λ2) are either chosen on-the-fly during decoding by maximizing the fused model scores, or learned automatically through cross-entropy optimization on synthetic document-level data.
- Core assumption: Document-level context is not uniformly useful for all predicted subwords, so adaptive scaling is beneficial.
- Evidence anchors:
  - [section 3.2.1]: "We propose to also choose the fusion scales in a similar fashion and define them to maximize the fused model scores"
  - [section 3.2.2]: "Alternatively, we propose to learn the fusion scales automatically using a small amount of training examples"
  - [corpus]: Moderate evidence - corpus shows "average neighbor FMR=0.429" suggesting moderate relevance of adaptive approaches
- Break condition: The approach breaks when the automatic learning overfits to the validation set or when on-the-fly scaling becomes computationally prohibitive.

### Mechanism 3
- Claim: Large language models can be effectively fused with sentence-level MT systems to improve translation quality through increased data and context.
- Mechanism: The document-level LM is replaced with a large language model (e.g., LLaMA) which has been pre-trained on vastly more data and context. The same fusion framework is applied.
- Core assumption: The LLM's superior language modeling capabilities will transfer to improved translation through fusion.
- Evidence anchors:
  - [section 6.4.4]: "First findings demonstrate that fusion with an LLM outperforms a small LM trained on in-domain data"
  - [section 6.4.4]: "Table 5 shows the perplexities of both LMs and their contrastive scores"
  - [corpus]: Weak evidence - only one LLM experiment mentioned with limited results
- Break condition: The approach breaks when the LLM's general knowledge doesn't align with translation-specific needs, or when the fusion scales cannot properly balance the contributions.

## Foundational Learning

- Concept: Language model fusion
  - Why needed here: The core innovation relies on combining multiple probabilistic models through weighted combination
  - Quick check question: What are the three probability distributions being combined in the document-level LM fusion approach?

- Concept: Neural machine translation architecture
  - Why needed here: Understanding the base MT system and its implicit language modeling is crucial for grasping the internal LM neutralization
  - Quick check question: How does the translation model implicitly learn language modeling during training?

- Concept: Document-level context in translation
  - Why needed here: The entire approach is predicated on the importance of document-level context for resolving translation ambiguities
  - Quick check question: What types of translation ambiguities are specifically addressed by incorporating document-level context?

## Architecture Onboarding

- Component map:
  - Sentence-level MT model (base system) -> Document-level language model (monolingual training) -> Internal language model (trained on MT target-side data) -> Fusion mechanism (weighted combination) -> Scale learning/tuning system (grid search, on-the-fly, or automatic) -> Translation output

- Critical path: MT model → LM fusion → Context-dependent scaling → Translation output

- Design tradeoffs:
  - Static vs. context-dependent fusion scales (computational cost vs. quality)
  - Internal LM estimation method (accuracy vs. complexity)
  - Document-level vs. sentence-level back-translation (quality vs. retraining requirement)

- Failure signatures:
  - Degraded translation quality when fusion scales are poorly chosen
  - Overfitting when automatically learning scales on small validation sets
  - Poor performance when internal LM estimation is inaccurate

- First 3 experiments:
  1. Implement basic LM fusion with static scales on a small dataset to verify the framework works
  2. Test internal LM neutralization by comparing fusion with and without ILM subtraction
  3. Evaluate on-the-fly scale selection by measuring translation quality and computational overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed non-static fusion scales compare to static scales in terms of computational efficiency and translation quality?
- Basis in paper: [explicit] The paper compares static fusion scales tuned via grid search with on-the-fly and automatically learned scales, showing improvements in document-targeted metrics and computational efficiency.
- Why unresolved: The paper provides some comparison but does not conduct a detailed analysis of the computational efficiency and translation quality trade-offs between different fusion scale approaches.
- What evidence would resolve it: A comprehensive study comparing the computational time and memory usage of static, on-the-fly, and automatically learned fusion scales, along with their impact on translation quality across various tasks and languages.

### Open Question 2
- Question: Can the proposed document-level LM fusion approach be effectively extended to other language pairs beyond English to German?
- Basis in paper: [inferred] The paper focuses on English to German translation tasks, but the approach is based on general principles of document-level LM fusion that could potentially be applied to other language pairs.
- Why unresolved: The paper does not provide any experiments or analysis on the applicability of the approach to other language pairs.
- What evidence would resolve it: Experiments applying the proposed approach to different language pairs, along with an analysis of the challenges and adaptations required for each pair.

### Open Question 3
- Question: How does the proposed document-level LM fusion approach compare to other methods of utilizing monolingual data, such as multi-task learning or pre-training?
- Basis in paper: [explicit] The paper compares the approach to back-translation and a task-specific LM re-ranking method, but does not explore other methods like multi-task learning or pre-training.
- Why unresolved: The paper focuses on comparing the proposed approach to specific baselines, but does not provide a comprehensive comparison with other methods of utilizing monolingual data.
- What evidence would resolve it: Experiments comparing the proposed approach to other methods of utilizing monolingual data, such as multi-task learning or pre-training, in terms of translation quality and computational efficiency.

## Limitations

- The effectiveness of the approach for language pairs beyond English to German remains uncertain
- The LLM fusion results are based on limited experimentation with a single LLM model
- The automatic scale learning method raises concerns about potential overfitting on validation sets

## Confidence

**High confidence**: The basic premise that document-level context improves translation quality, particularly for pronoun resolution and discourse phenomena, is well-supported by the experimental results (+6.2% pronoun F1 score improvement). The mathematical framework for LM fusion is theoretically sound and properly implemented.

**Medium confidence**: The effectiveness of internal LM neutralization and the specific benefits of context-dependent fusion scales are supported by experimental evidence but rely on assumptions about the internal workings of MT models that are difficult to verify independently. The computational efficiency claims are plausible but not exhaustively validated.

**Low confidence**: The LLM fusion results are based on limited experimentation and should be treated as preliminary findings rather than robust conclusions. The generalizability of the approach across different language pairs, domains, and model architectures remains uncertain.

## Next Checks

1. **Internal LM estimation validation**: Conduct ablation studies comparing different methods for estimating the internal language model of the MT system, including both the proposed method and alternative approaches from the literature.

2. **Scale learning robustness**: Evaluate the automatic scale learning method on multiple validation sets and test for overfitting by measuring performance degradation on held-out data not used during scale optimization.

3. **LLM fusion scalability study**: Test the large language model fusion approach with multiple LLMs of varying sizes and training data characteristics to determine the relationship between LLM quality and translation improvements.