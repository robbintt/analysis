---
ver: rpa2
title: Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization
arxiv_id: '2309.01107'
source_url: https://arxiv.org/abs/2309.01107
tags:
- robust
- reward
- policy
- uncertainty
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies reward-robust MDPs, where the transition model
  is known but the reward is uncertain and coupled across states. The authors show
  that non-rectangular reward uncertainty leads to overly conservative solutions if
  solved using robust Bellman operators.
---

# Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization

## Quick Facts
- arXiv ID: 2309.01107
- Source URL: https://arxiv.org/abs/2309.01107
- Reference count: 40
- One-line primary result: Policy gradient method with convergence guarantees for non-rectangular reward-robust MDPs via frequency regularization

## Executive Summary
This work addresses reward-robust MDPs where the transition model is known but the reward is uncertain and coupled across states. The authors show that standard robust Bellman operators lead to overly conservative solutions when applied to non-rectangular uncertainty sets. By establishing a connection between such RMDPs and policy visitation frequency regularization, they derive the worst-case reward function in closed form for Lp-norm uncertainty sets. This leads to a policy gradient method with convergence guarantees that scales to high-dimensional continuous control problems.

## Method Summary
The method formulates reward-robust MDPs with non-rectangular uncertainty sets and derives the worst-case reward function for Lp-norm balls centered around a nominal reward. This worst-case reward is expressed as RπRp(s,a) = R0(s,a) - α(dπ(s,a)/∥dπ∥q)^(q-1), revealing a direct connection to policy visitation frequency regularization. An actor-critic algorithm is developed where the policy update uses gradients from the robust Q-function, and the Q-function is updated with a temporal difference error that includes a frequency penalty term. The occupancy measure is tracked online to compute the regularization term.

## Key Results
- Non-rectangular reward uncertainty leads to overly conservative solutions with standard robust Bellman operators
- Worst-case reward function derived in closed form for Lp-norm uncertainty sets
- Policy gradient method achieves O(1/ε) convergence rate
- Learned policies are robust to reward perturbations and less conservative than rectangular uncertainty approaches
- Method scales to high-dimensional continuous control tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard robust Bellman operators assume rectangular uncertainty sets, making them overly conservative for non-rectangular sets
- Core assumption: Uncertainty sets are convex and compact
- Evidence anchors:
  - [abstract]: "they instead establish a connection between such RMDPs and policy visitation frequency regularization"
  - [section]: "the robust Bellman evaluation operator does not admit the robust value function as a fixed point"
  - [corpus]: Weak evidence - related papers discuss non-rectangular uncertainty sets

### Mechanism 2
- Claim: For Lp-norm uncertainty sets centered around nominal reward, worst-case reward has closed-form solution
- Core assumption: Uncertainty set is Lp-norm ball centered around nominal reward
- Evidence anchors:
  - [abstract]: "deriving the worst-case reward function in closed form"
  - [section]: "Theorem 5 (Worst-case reward)... RπRp(s, a) = R0(s, a) - α(dπ(s, a)/∥dπ∥q)^(q-1)"
  - [corpus]: Weak evidence - related papers discuss Lp norms but not this specific derivation

### Mechanism 3
- Claim: Robust policy gradient method converges globally with rate O(1/ε)
- Core assumption: Uncertainty set is Lp-norm ball with p ∈ (1, ∞)
- Evidence anchors:
  - [abstract]: "This leads to a policy gradient method with convergence guarantees"
  - [section]: "Lemma 10 (Smoothness)... robust return ρπRp is β-smooth in π"
  - [corpus]: Weak evidence - related papers discuss policy gradients for robust MDPs

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire paper builds upon the MDP framework to formalize sequential decision-making problems with uncertainty
  - Quick check question: What is the Bellman optimality equation for an MDP?

- Concept: Robust Optimization
  - Why needed here: The paper addresses model uncertainty in MDPs by framing it as a robust optimization problem
  - Quick check question: What is the difference between a rectangular and non-rectangular uncertainty set in robust optimization?

- Concept: Policy Gradient Methods
  - Why needed here: The paper proposes a policy gradient method to solve the robust MDP with non-rectangular reward uncertainty
  - Quick check question: What is the policy gradient theorem and how is it used to derive the policy update rule?

## Architecture Onboarding

- Component map: Robust MDP formulation -> Worst-case reward derivation -> Frequency regularization connection -> Policy gradient method -> Actor-critic algorithm

- Critical path:
  1. Define robust MDP with non-rectangular reward uncertainty
  2. Derive worst-case reward function in closed form
  3. Establish connection to policy visitation frequency regularization
  4. Develop policy gradient method and prove convergence
  5. Implement scalable actor-critic algorithm
  6. Validate through experiments

- Design tradeoffs:
  - Non-rectangular vs. rectangular uncertainty sets: Non-rectangular sets capture correlations between states but are computationally harder to solve
  - Lp-norm vs. other uncertainty set structures: Lp-norms provide balance between expressiveness and tractability
  - Policy gradient vs. value iteration: Policy gradient methods handle non-rectangular uncertainty sets but may have slower convergence

- Failure signatures:
  - Non-convergence of policy gradient method: May indicate issues with smoothness assumption or step size choice
  - Overly conservative policies: May suggest uncertainty set is too large or rectangularity assumption is being used inadvertently
  - Poor performance on high-dimensional tasks: May indicate scalability issues with actor-critic algorithm

- First 3 experiments:
  1. Implement and validate worst-case reward function derivation on simple tabular MDP with known uncertainty set
  2. Compare policy gradient method performance on non-rectangular vs. rectangular reward robust MDP
  3. Test scalability of actor-critic algorithm on high-dimensional continuous control task like Ant environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does convergence rate change when reward uncertainty set is not a norm ball but a more general convex set?
- Basis in paper: [inferred] The paper focuses on Lp norm balls for p > 1 but acknowledges framework could extend to general convex sets
- Why unresolved: Paper does not explore convergence properties for general convex uncertainty sets
- What evidence would resolve it: Theoretical analysis of convergence rate for policy gradient methods under general convex uncertainty sets

### Open Question 2
- Question: Can policy gradient method be extended to handle time-varying reward uncertainties or non-stationary environments?
- Basis in paper: [inferred] Paper assumes fixed reward uncertainty set but real-world scenarios often involve time-varying uncertainties
- Why unresolved: Paper does not address challenges of adapting policy gradient method to handle time-varying uncertainties
- What evidence would resolve it: Theoretical analysis of convergence rate for policy gradient methods under time-varying reward uncertainties

### Open Question 3
- Question: How does performance of policy gradient method compare to other robust RL methods like adversarial training?
- Basis in paper: [inferred] Paper focuses on policy gradient methods but other robust RL methods exist
- Why unresolved: Paper does not provide comprehensive comparison to other robust RL methods
- What evidence would resolve it: Empirical results comparing policy gradient method to other robust RL methods on range of tasks

## Limitations
- Theoretical results assume convex, compact uncertainty sets which may not hold in practice
- Closed-form worst-case reward derivation only proven for Lp-norm balls
- Convergence guarantees rely on smoothness assumptions that may not hold for all problem instances

## Confidence

- **High Confidence**: Derivation of worst-case reward function for Lp-norm uncertainty sets and its connection to frequency regularization
- **Medium Confidence**: Policy gradient method's convergence guarantees and actor-critic algorithm's scalability to high-dimensional tasks
- **Low Confidence**: Generality of approach beyond Lp-norm uncertainty sets and applicability to partially observable environments

## Next Checks
1. Test policy gradient method on synthetic MDPs with ellipsoidal and polytope uncertainty sets to verify convergence beyond Lp-norm balls
2. Implement actor-critic algorithm on Walker2d and Humanoid tasks, comparing against robust adversarial reinforcement learning
3. Conduct ablation study on impact of hyperparameters (α, p, learning rates) across tasks with varying uncertainty magnitudes