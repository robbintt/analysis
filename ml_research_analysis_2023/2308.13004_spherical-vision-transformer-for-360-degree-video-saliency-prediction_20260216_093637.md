---
ver: rpa2
title: Spherical Vision Transformer for 360-degree Video Saliency Prediction
arxiv_id: '2308.13004'
source_url: https://arxiv.org/abs/2308.13004
tags:
- saliency
- tangent
- spherical
- video
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses 360-degree video saliency prediction, which
  is challenging due to spherical distortion, high resolution, and limited labelled
  data. The proposed method, SalViT360, is the first to use tangent image representations
  for omnidirectional saliency prediction.
---

# Spherical Vision Transformer for 360-degree Video Saliency Prediction

## Quick Facts
- arXiv ID: 2308.13004
- Source URL: https://arxiv.org/abs/2308.13004
- Reference count: 37
- Primary result: SalViT360 outperforms state-of-the-art on three ODV saliency datasets using tangent image representations and spherical geometry-aware attention

## Executive Summary
This paper introduces SalViT360, the first tangent image-based approach for 360-degree video saliency prediction. The method addresses the spherical distortion problem in equirectangular projections by using gnomonic projection to extract undistorted local viewports (tangent images), which are processed by a transformer with spherical geometry-aware attention. The model also introduces a consistency-based unsupervised regularization term to reduce artefacts in overlapping regions after inverse projection. Extensive experiments demonstrate state-of-the-art performance across three datasets with significant improvements in saliency prediction metrics.

## Method Summary
SalViT360 processes 360-degree videos by first projecting them into multiple tangent images using gnomonic projection with a 120° field of view. Each tangent image is encoded using a frozen ResNet-18 backbone, and the resulting features are processed by a transformer with Viewport Spatio-Temporal Attention (VSTA) that performs temporal attention across frames followed by spatial attention within frames. Spherical geometry-aware position embeddings encode the angular coordinates of each pixel. A CNN decoder generates saliency predictions on tangent images, which are then inverse-projected to the equirectangular format. The training objective combines supervised losses (KLD, CC, Selective-MSE) with an unsupervised Viewport Augmentation Consistency (VAC) loss that enforces consistency between predictions from different tangent configurations.

## Key Results
- Achieves state-of-the-art performance on VR-EyeTracking, PVS-HMEM, and 360A V-HM datasets
- Outperforms existing methods across all evaluation metrics (NSS, KLD, CC, SIM)
- VAC loss effectively reduces artefacts in overlapping regions of tangent predictions
- VSTA maintains computational efficiency while capturing global temporal dependencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tangent images eliminate spherical distortion for local feature extraction while preserving global context through spherical geometry-aware attention
- Mechanism: Gnomonic projection maps undistorted local patches (tangent images) from the sphere to a plane, enabling standard 2D backbones to extract high-quality local features. These features are then processed by a transformer with spherical geometry-aware position embeddings that aggregate global context across tangent viewports
- Core assumption: The distortion in equirectangular projection significantly degrades local feature quality, and this degradation can be mitigated by processing undistorted local patches
- Evidence anchors:
  - [abstract] "We use gnomonic projection to obtain multiple undistorted tangent images, which enables us to extract rich local spatial features using any pre-trained and fixed 2D backbone"
  - [section] "Our extensive experiments demonstrate the effectiveness of our proposed SalViT360 model against the state-of-the-art on VR-EyeTracking [28], PVS-HMEM [29], and 360A V-HM [30] datasets"
  - [corpus] Weak evidence - related papers focus on different aspects of 360° video processing, not specifically on tangent image representations for saliency prediction

### Mechanism 2
- Claim: Viewport Spatio-Temporal Attention (VSTA) effectively captures global temporal dependencies in 360° videos while maintaining computational efficiency
- Mechanism: VSTA approximates joint spatio-temporal attention by performing temporal attention among consecutive frames for the same tangent viewport, followed by spatial attention among tangent viewports within each frame, reducing complexity from O(F² × T²) to O(F² + T²)
- Core assumption: The temporal information in 360° videos can be effectively captured by processing the same tangent viewports across consecutive frames
- Evidence anchors:
  - [abstract] "We introduce a spherical geometry-aware spatiotemporal self-attention mechanism that is capable of effective omnidirectional video understanding"
  - [section] "Since incorporating the temporal dimension of the videos increases the number of tokens and thus the computational complexity, we approximate spatio-temporal attention with two stages"
  - [corpus] Weak evidence - while related papers mention spatio-temporal modeling, none specifically address the efficiency trade-off in the context of tangent image representations

### Mechanism 3
- Claim: Viewport Augmentation Consistency (VAC) loss reduces artefacts in overlapping regions of tangent predictions by enforcing consistency between different tangent projections
- Mechanism: VAC computes a consistency loss between saliency predictions from two tangent image sets with different configurations (e.g., shifted viewports, different FOV), encouraging the model to produce consistent predictions across different projections
- Core assumption: Artefacts in overlapping regions of tangent predictions can be reduced by training the model to produce consistent outputs for different tangent configurations of the same scene
- Evidence anchors:
  - [abstract] "Furthermore, we present a consistency-based unsupervised regularization term for projection-based 360° dense-prediction models to reduce artefacts in the predictions that occur after inverse projection"
  - [section] "To tackle this issue, we propose an unsupervised loss strategy, called Viewport Augmentation Consistency (VAC), for improving the consistency between the saliency predictions P and P′ from two tangent projection sets"
  - [corpus] No direct evidence - related papers don't address the specific artefact problem in tangent image predictions

## Foundational Learning

- Concept: Spherical geometry and projections (equirectangular, cube-map, gnomonic)
  - Why needed here: Understanding how different projections handle spherical distortion is crucial for appreciating why tangent images are advantageous for 360° video processing
  - Quick check question: What are the main disadvantages of equirectangular projection for computer vision tasks, and how does gnomonic projection address them?

- Concept: Vision Transformer architecture and self-attention mechanisms
  - Why needed here: The paper builds on standard ViT components but modifies them for spherical geometry; understanding the baseline is essential for grasping the innovations
  - Quick check question: How does the computational complexity of self-attention scale with sequence length, and why is this particularly problematic for 360° video with temporal dimensions?

- Concept: Saliency prediction metrics (NSS, KLD, CC, SIM) and their interpretation
  - Why needed here: The paper uses multiple evaluation metrics to demonstrate effectiveness; understanding what each metric measures helps interpret the results
  - Quick check question: What is the key difference between NSS (Normalized Scanpath Saliency) and CC (Correlation Coefficient) in evaluating saliency predictions?

## Architecture Onboarding

- Component map: Input video → Gnomonic projection (tangent images) → ResNet-18 encoder (frozen) → Spherical geometry-aware position embeddings → Viewport Spatio-Temporal Attention (VSTA) transformer → CNN decoder → Inverse gnomonic projection → Output saliency map; VAC loss computed between predictions from different tangent configurations
- Critical path: Tangent image generation → Feature extraction → Global context aggregation → Prediction generation → Artefact reduction (VAC)
- Design tradeoffs: Using pre-trained 2D backbone on tangent images vs. training specialized spherical convolutions; computational efficiency of VSTA vs. full joint attention; multiple tangent sets for VAC vs. single prediction at inference
- Failure signatures: High KLD values indicating poor distribution match; visible artefacts in overlapping tangent regions; poor generalization across datasets; excessive computational overhead
- First 3 experiments:
  1. Compare NSS, KLD, CC, SIM scores with baseline models on VR-EyeTracking dataset
  2. Ablation study removing VAC loss to measure artefact reduction impact
  3. Test different tangent image configurations (number of patches, FOV) to find optimal balance between local detail and global context

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit questions arise from the methodology and results presented.

## Limitations

- The computational overhead of multiple tangent projections and inverse projections is not thoroughly analyzed
- Comparison is limited to relatively few state-of-the-art methods in the field
- Specific configurations for VAC (FOV, shifts, number of tangent images) are not detailed
- The VSTA approximation may miss important cross-viewport temporal dependencies

## Confidence

**High confidence**: The core mechanism of using tangent images with gnomonic projection to eliminate spherical distortion for local feature extraction is well-supported by the experimental results showing improved NSS, KLD, CC, and SIM scores across three datasets.

**Medium confidence**: The effectiveness of VSTA in capturing global temporal dependencies while maintaining computational efficiency is supported by the results, but the approximation may have limitations that are not fully explored.

**Medium confidence**: The VAC loss appears to reduce artefacts in overlapping regions, but the specific configurations and their impact on the final predictions require further validation.

## Next Checks

1. **Computational Overhead Analysis**: Measure and compare the computational cost of SalViT360 (including tangent projections and inverse projections) against baseline methods to determine if the performance gains justify the additional complexity.

2. **VAC Configuration Sensitivity**: Systematically vary the configurations used in VAC (e.g., FOV, number of tangent images, shifts) to determine how sensitive the artefact reduction is to these parameters and identify optimal settings.

3. **Cross-Viewport Temporal Dependency Test**: Compare SalViT360 with a variant that uses full joint spatio-temporal attention on a subset of data to quantify the impact of the VSTA approximation on capturing important cross-viewport temporal relationships.