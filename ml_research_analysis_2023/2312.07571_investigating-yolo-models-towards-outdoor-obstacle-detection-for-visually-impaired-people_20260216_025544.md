---
ver: rpa2
title: Investigating YOLO Models Towards Outdoor Obstacle Detection For Visually Impaired
  People
arxiv_id: '2312.07571'
source_url: https://arxiv.org/abs/2312.07571
tags:
- detection
- precision
- object
- recall
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates seven YOLO models for outdoor obstacle
  detection to assist visually impaired individuals. The models evaluated include
  YOLO-NAS (small, medium, large), YOLOv8, YOLOv7, YOLOv6, and YOLOv5, all trained
  on an obstacle dataset containing images from VOC, COCO, TT100K, and field-collected
  data.
---

# Investigating YOLO Models Towards Outdoor Obstacle Detection For Visually Impaired People

## Quick Facts
- arXiv ID: 2312.07571
- Source URL: https://arxiv.org/abs/2312.07571
- Reference count: 40
- Primary result: YOLOv8 achieved 80% precision and 68.2% recall on outdoor obstacle detection for visually impaired individuals

## Executive Summary
This paper investigates seven YOLO models for outdoor obstacle detection to assist visually impaired individuals. The study evaluates YOLO-NAS (small, medium, large), YOLOv8, YOLOv7, YOLOv6, and YOLOv5, all trained on an obstacle dataset containing images from VOC, COCO, TT100K, and field-collected data. YOLOv8 achieved the best performance with a precision of 80% and a recall of 68.2%, making it the most effective model for detecting obstacles like stop signs, vehicles, and pedestrians. Despite being the latest model, YOLO-NAS showed suboptimal performance. Ablation studies revealed that YOLO-NAS is sensitive to threshold scores, impacting the precision-recall trade-off. This study highlights YOLOv8 as a promising solution for real-time obstacle detection in assistive devices.

## Method Summary
The study trained seven YOLO models on a combined obstacle dataset containing 5066 training images from VOC, COCO, TT100K datasets plus field-collected data. Models were trained for 25 epochs with batch size 8 on NVIDIA Tesla T4 GPU using Adam optimizer with learning rate 5e-4, weight decay 0.0001, warmup, cosine decay, and mixed precision. Performance was evaluated using precision, recall, mAP@0.5, mAP@0.5-0.95, and F1 score metrics.

## Key Results
- YOLOv8 achieved best performance with 80% precision and 68.2% recall on obstacle detection
- YOLO-NAS showed suboptimal performance despite being the latest model
- YOLO-NAS demonstrated sensitivity to threshold scores affecting precision-recall trade-off
- Combined dataset approach provided diverse representation of outdoor obstacles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** YOLOv8 outperforms other YOLO models in outdoor obstacle detection for visually impaired people due to its improved architecture and training strategies.
- **Mechanism:** YOLOv8 introduces a new backbone network and anchor-free detection head, which enhances feature extraction and eliminates the need for predefined anchor boxes. This leads to faster inference speeds and higher accuracy compared to previous YOLO versions.
- **Core assumption:** The improvements in YOLOv8's architecture directly translate to better performance on the obstacle detection task.
- **Evidence anchors:**
  - [abstract]: "YOLOv8 achieved the best performance with a precision of 80% and a recall of 68.2%"
  - [section]: "YOLOv8 achieves faster inference speeds compared to other object detection models, while also maintaining a high level of accuracy."
  - [corpus]: Weak - No direct evidence found in corpus neighbors about YOLOv8's performance on outdoor obstacle detection.
- **Break condition:** If the dataset characteristics differ significantly from the training data used for YOLOv8, or if the model's computational requirements exceed the available hardware resources.

### Mechanism 2
- **Claim:** The trade-off between precision and recall in YOLO-NAS models is influenced by the threshold score parameter.
- **Mechanism:** Increasing the threshold score in YOLO-NAS models leads to higher precision but lower recall, as the model becomes more selective in its predictions. This relationship allows for fine-tuning the model's performance based on the specific requirements of the obstacle detection task.
- **Core assumption:** The threshold score parameter directly controls the balance between precision and recall in YOLO-NAS models.
- **Evidence anchors:**
  - [abstract]: "Ablation studies revealed that YOLO-NAS is sensitive to threshold scores, impacting the precision-recall trade-off."
  - [section]: "When threshold score increases (from 0.5 to 0.7), the precision increases, while the recall decreases and vice-versa."
  - [corpus]: Weak - No direct evidence found in corpus neighbors about the relationship between threshold score and precision-recall trade-off in YOLO-NAS models.
- **Break condition:** If the dataset contains a high proportion of ambiguous or overlapping objects, or if the optimal threshold score varies significantly across different classes of obstacles.

### Mechanism 3
- **Claim:** The combination of multiple datasets (VOC, COCO, TT100K, and field-collected data) in the Obstacle Dataset provides a diverse and comprehensive representation of outdoor obstacles, leading to better generalization of YOLO models.
- **Mechanism:** By training on a diverse dataset that includes images from various sources, YOLO models learn to recognize a wide range of obstacle types and scenarios, improving their ability to detect obstacles in real-world conditions.
- **Core assumption:** The diversity and representativeness of the training data directly impact the generalization performance of YOLO models on outdoor obstacle detection.
- **Evidence anchors:**
  - [abstract]: "The models evaluated include YOLO-NAS (small, medium, large), YOLOv8, YOLOv7, YOLOv6, and YOLOv5, all trained on an obstacle dataset containing images from VOC, COCO, TT100K, and field-collected data."
  - [section]: "We particularly choose this dataset as it is a comprehensive dataset also containing images from the VOC dataset, COCO dataset, and TT100K dataset."
  - [corpus]: Weak - No direct evidence found in corpus neighbors about the impact of dataset diversity on the generalization performance of YOLO models.
- **Break condition:** If the field-collected data is not representative of the target deployment environment, or if the model overfits to specific characteristics of the combined dataset.

## Foundational Learning

- **Concept:** Object Detection
  - **Why needed here:** Understanding the basics of object detection is crucial for interpreting the performance of YOLO models and their application to outdoor obstacle detection.
  - **Quick check question:** What are the key components of an object detection pipeline, and how do they contribute to the overall performance?

- **Concept:** Convolutional Neural Networks (CNNs)
  - **Why needed here:** YOLO models are based on CNNs, so having a solid understanding of CNN architectures, layers, and training processes is essential for analyzing and optimizing the models.
  - **Quick check question:** How do convolutional layers, pooling layers, and fully connected layers work together in a CNN to extract features and make predictions?

- **Concept:** Precision, Recall, and mAP
  - **Why needed here:** These metrics are used to evaluate the performance of YOLO models on the obstacle detection task. Understanding their definitions and trade-offs is crucial for interpreting the results and making informed decisions.
  - **Quick check question:** How do precision, recall, and mAP differ, and what are the implications of prioritizing one metric over the others in the context of outdoor obstacle detection for visually impaired people?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Model training -> Model evaluation -> Ablation studies
- **Critical path:** The most critical steps in the pipeline are data preprocessing, model training, and evaluation. Ensuring high-quality data, appropriate model architecture, and robust evaluation metrics are essential for achieving reliable results.
- **Design tradeoffs:** The choice of YOLO model version involves tradeoffs between model complexity, inference speed, and accuracy. For example, YOLOv8 offers higher accuracy but may require more computational resources compared to YOLOv5. Similarly, adjusting the threshold score in YOLO-NAS models involves a tradeoff between precision and recall.
- **Failure signatures:** Potential failure modes include overfitting to the training data, poor generalization to unseen scenarios, and suboptimal hyperparameter choices. Monitoring training and validation metrics, as well as conducting thorough evaluation on diverse test sets, can help identify and mitigate these issues.
- **First 3 experiments:**
  1. Implement data preprocessing pipeline and verify the quality and diversity of the preprocessed dataset.
  2. Train a baseline YOLOv5 model and evaluate its performance on a held-out validation set to establish a performance benchmark.
  3. Conduct ablation studies by varying the threshold score in YOLO-NAS models and analyzing the impact on precision, recall, and mAP to identify optimal hyperparameter settings.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does YOLO-NAS show suboptimal performance on the obstacle detection task despite being the latest model and demonstrating better performance in many other applications?
- **Basis in paper:** [explicit] The paper states that YOLO-NAS was found to be suboptimal for the obstacle detection task despite being the latest model and demonstrating better performance in many other applications.
- **Why unresolved:** The paper does not provide a detailed explanation for the suboptimal performance of YOLO-NAS on the obstacle detection task. It only mentions that ablation studies revealed that YOLO-NAS is sensitive to threshold scores, which impacts the precision-recall trade-off.
- **What evidence would resolve it:** Further investigation into the specific reasons for YOLO-NAS's suboptimal performance on the obstacle detection task, such as analyzing the model's architecture, training process, or data preprocessing methods, would help resolve this question.

### Open Question 2
- **Question:** How does the performance of YOLO-NAS compare to other object detection models when applied to outdoor obstacle detection for visually impaired individuals?
- **Basis in paper:** [inferred] The paper focuses on evaluating the performance of seven YOLO models, including YOLO-NAS, on the obstacle detection task. However, it does not provide a direct comparison of YOLO-NAS's performance with other object detection models.
- **Why unresolved:** The paper does not include a comparative analysis of YOLO-NAS's performance with other object detection models on the obstacle detection task.
- **What evidence would resolve it:** Conducting experiments to compare the performance of YOLO-NAS with other object detection models, such as Faster R-CNN or SSD, on the obstacle detection task would help resolve this question.

### Open Question 3
- **Question:** What are the potential applications and limitations of using YOLO models for real-time obstacle detection in assistive devices for visually impaired individuals?
- **Basis in paper:** [explicit] The paper highlights YOLOv8 as a promising solution for real-time obstacle detection in assistive devices for visually impaired individuals. However, it does not discuss the potential applications and limitations of using YOLO models in such devices.
- **Why unresolved:** The paper does not provide a comprehensive discussion on the potential applications and limitations of using YOLO models for real-time obstacle detection in assistive devices.
- **What evidence would resolve it:** Conducting a thorough analysis of the advantages and limitations of using YOLO models for real-time obstacle detection in assistive devices, including factors such as computational efficiency, accuracy, and adaptability to different environments, would help resolve this question.

## Limitations
- Dataset size (5066 training images) is relatively small compared to standard computer vision benchmarks, which may limit model generalization
- Field-collected data characteristics are not detailed, making it difficult to assess how representative the dataset is of real-world deployment scenarios
- Study focuses on daytime outdoor scenarios with unclear performance in varying lighting conditions or adverse weather

## Confidence
- **High Confidence**: YOLOv8 achieving best performance metrics (80% precision, 68.2% recall) - directly measured and reported with specific values
- **Medium Confidence**: YOLO-NAS sensitivity to threshold scores - supported by ablation study but limited by dataset size
- **Medium Confidence**: Combined dataset approach improving generalization - logical assumption but not empirically validated through ablation studies

## Next Checks
1. Conduct k-fold cross-validation to establish confidence intervals for all reported metrics and assess model stability across different data splits
2. Test model performance on an independent, held-out dataset collected from different geographical locations or seasons to verify generalization claims
3. Implement a threshold sweep analysis across all YOLO models to quantify the precision-recall trade-off and identify optimal operating points for assistive applications