---
ver: rpa2
title: 'MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General
  Time Series Forecasting'
arxiv_id: '2311.18780'
source_url: https://arxiv.org/abs/2311.18780
tags:
- series
- time
- forecasting
- multiresformer
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiResFormer introduces adaptive multi-resolution modeling for
  time series forecasting by detecting salient periodicities to dynamically set patch
  lengths within each Transformer block. This allows effective modeling of both interperiod
  and intraperiod variations using shared Multi-head Attention and Feed-Forward Networks
  across different resolutions.
---

# MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting

## Quick Facts
- arXiv ID: 2311.18780
- Source URL: https://arxiv.org/abs/2311.18780
- Authors: 
- Reference count: 31
- MultiResFormer achieves state-of-the-art performance on long-term and short-term forecasting benchmarks

## Executive Summary
MultiResFormer introduces adaptive multi-resolution modeling for time series forecasting by detecting salient periodicities to dynamically set patch lengths within each Transformer block. This allows effective modeling of both interperiod and intraperiod variations using shared Multi-head Attention and Feed-Forward Networks across different resolutions. An interpolation scheme enables parameter sharing while resolution embeddings maintain scale-awareness. The model eliminates the need for an embedding layer, reducing parameter count. MultiResFormer achieves state-of-the-art performance on long-term and short-term forecasting benchmarks, significantly outperforming CNN and patch-based Transformer baselines while using fewer parameters.

## Method Summary
MultiResFormer processes time series data through adaptive multi-resolution modeling. At the start of each Transformer block, FFT detects salient periodicities and converts them to patch lengths, creating non-overlapping patches. These patches form multiple resolution branches that are interpolated to a common length for shared processing by a Transformer block. Resolution embeddings maintain scale-awareness across branches. The model outputs predictions of the same shape as input, reducing parameter burden. Instance normalization is applied at the beginning and de-normalization at the end to handle temporal distribution shift.

## Key Results
- Achieves state-of-the-art performance on long-term forecasting benchmarks (ETT, Weather, Electricity, Traffic, ILI datasets)
- Significantly outperforms CNN and patch-based Transformer baselines while using fewer parameters
- Demonstrates strong performance on short-term forecasting with M4 datasets across multiple subsets
- Eliminates the need for an embedding layer, reducing parameter count without sacrificing performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MultiResFormer improves forecasting by adaptively detecting salient periodicities and forming resolution branches accordingly.
- Mechanism: FFT identifies top-k frequencies in the input, which are converted to patch lengths that segment the series into non-overlapping patches. Each patch length defines a resolution branch capturing patterns at a specific scale.
- Core assumption: Detected periodicities correspond to meaningful temporal patterns that should be modeled separately.
- Evidence anchors: [abstract] "dynamically models temporal variations by adaptively choosing optimal patch lengths"; [section 3.1] "we leverage the Fast Fourier Transform (FFT) to decompose the input X...to identify salient periodicities based on the amplitude of each frequency"

### Mechanism 2
- Claim: Shared Transformer blocks across resolution branches enable parameter-efficient multi-resolution modeling.
- Mechanism: Instead of separate Transformer blocks per resolution, MultiResFormer interpolates patches from all branches to a common length d and applies the same Transformer block, processing multiple resolutions without proportional parameter increases.
- Core assumption: The same Transformer architecture can effectively model both interperiod and intraperiod variations across different resolutions when patch representations are properly aligned.
- Evidence anchors: [section 3.2] "The original patch lengths are set according to the salient periodicities...we adopt non-overlapping patching...we leverage a shared Transformer block to jointly capture the two types of variations within each resolution branch"; [section 2.2] "Instead of relying on a pre-defined hierarchy of resolutions, each MultiResFormer block constructs multiple resolution branches based on the salient periodicites"

### Mechanism 3
- Claim: Resolution embeddings enhance scale-awareness and improve representation learning across branches.
- Mechanism: Each resolution branch receives an additive embedding inversely proportional to its patch length. This embedding is shared across the model and helps the Transformer distinguish between different resolutions during processing.
- Core assumption: The model can learn to associate specific embedding values with particular scales, improving its ability to process multi-resolution inputs coherently.
- Evidence anchors: [section 3.2] "we propose to use a resolution embedding that consists of a learnable embedding scaled inversely proportional to the corresponding periodicity so as to incur smoothness over different resolution branches"; [section 3.3] "We aggregate them and perform a weighted sum to produce the final output of this MultiResFormer block. The weights are based on normalized amplitudes of the corresponding frequencies"

## Foundational Learning

- Concept: Fast Fourier Transform for periodicity detection
  - Why needed here: FFT provides an efficient way to identify dominant frequencies in time series data, which are then converted to patch lengths for multi-resolution modeling
  - Quick check question: How does FFT decompose a time series signal into its frequency components?

- Concept: Non-overlapping patching and interpolation
  - Why needed here: Non-overlapping patches preserve all information while reducing computational complexity; interpolation aligns patches to a common length for shared processing
  - Quick check question: What happens to the temporal relationships within a patch when we interpolate it to a different length?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Multi-headed attention captures global dependencies between patches (interperiod variations), while FFN layers process individual patches (intraperiod variations)
  - Quick check question: How does multi-headed attention enable the model to attend to different types of relationships simultaneously?

## Architecture Onboarding

- Component map: Input normalization (RevIN) → Periodicity detection (FFT) → Patching and interpolation → Resolution embeddings → Shared Transformer block (MHA + FFN) → Interpolation back → Truncation → Weighted aggregation → Output normalization → Linear prediction head

- Critical path: Periodicity detection → Patching and interpolation → Shared Transformer processing → Aggregation
  - The FFT computation and patching must complete before the Transformer block can process the data

- Design tradeoffs:
  - Using FFT for periodicity detection provides interpretability but may miss non-sinusoidal patterns
  - Shared Transformer blocks reduce parameters but may not capture resolution-specific nuances as well as dedicated blocks
  - Resolution embeddings add minimal parameters but provide crucial scale-awareness

- Failure signatures:
  - Poor performance with highly irregular or non-periodic time series (FFT detects irrelevant frequencies)
  - Degraded results when patch lengths vary drastically across branches (interpolation distorts patterns)
  - Overfitting with too many resolution branches relative to dataset size

- First 3 experiments:
  1. Test on synthetic periodic data with known frequencies to verify FFT detection and patching accuracy
  2. Compare single-resolution vs multi-resolution performance on datasets with clear multi-scale patterns
  3. Evaluate sensitivity to the number of resolution branches (k parameter) on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MultiResFormer's performance compare to hybrid models that combine both convolutional and transformer approaches for time series forecasting?
- Basis in paper: [inferred] The paper extensively compares MultiResFormer to pure transformer and CNN baselines, but doesn't explore hybrid architectures.
- Why unresolved: The authors focused on pure transformer approaches and didn't investigate whether combining CNNs with transformers could yield even better results.
- What evidence would resolve it: Direct comparison experiments between MultiResFormer and hybrid CNN-Transformer models on the same benchmark datasets.

### Open Question 2
- Question: What is the optimal number of salient periodicities (k) to detect for different types of time series data, and how sensitive is MultiResFormer's performance to this hyperparameter?
- Basis in paper: [explicit] The authors state they use "k detected periodicities" but don't provide systematic analysis of how performance varies with different k values.
- Why unresolved: The paper uses k without exploring its sensitivity or providing guidelines for choosing it across different datasets.
- What evidence would resolve it: Ablation studies showing performance across different k values for various dataset types, and any correlation between dataset characteristics and optimal k.

### Open Question 3
- Question: How does MultiResFormer's adaptive multi-resolution approach perform on non-periodic time series data, such as chaotic or stochastic processes?
- Basis in paper: [inferred] The paper focuses on periodic time series and demonstrates effectiveness on datasets with clear periodic patterns, but doesn't address non-periodic cases.
- Why unresolved: The method relies on periodicity detection, and its effectiveness on non-periodic data remains unexplored.
- What evidence would resolve it: Experiments testing MultiResFormer on datasets known to have non-periodic characteristics, such as chaotic systems or financial data without clear periodic patterns.

## Limitations

- Performance may degrade on non-stationary or noise-dominated time series where periodic components are weak or transient
- The interpolation scheme could introduce artifacts when aligning patches of vastly different lengths, potentially distorting scale-specific patterns
- Lack of specified hyperparameter values (model size d, number of layers N, number of resolution branches k) impacts reproducibility

## Confidence

**High Confidence**: The architectural design choices (adaptive multi-resolution modeling, shared Transformer blocks, resolution embeddings) are well-specified and logically coherent. The experimental methodology using standard benchmarks (ETT, M4) follows established practices.

**Medium Confidence**: The performance improvements over baselines are substantial and statistically significant, but the exact hyperparameter configurations remain unspecified. The claim that FFT reliably detects "salient periodicities" assumes stationary signals with clear frequency components.

**Low Confidence**: The paper's assertion that eliminating the embedding layer significantly reduces parameters while maintaining performance requires empirical verification across diverse datasets. The weighted aggregation scheme's sensitivity to frequency amplitude estimation errors is not thoroughly evaluated.

## Next Checks

1. **Sensitivity Analysis**: Systematically vary the number of resolution branches (k) and measure performance degradation on validation sets to determine the optimal trade-off between model complexity and forecasting accuracy.

2. **Robustness Testing**: Evaluate MultiResFormer on synthetic time series with injected noise and non-stationary components to assess how FFT-based periodicity detection performs under challenging conditions.

3. **Ablation Study**: Compare full MultiResFormer against versions with fixed resolution branches, separate Transformer blocks per resolution, and no resolution embeddings to quantify the contribution of each design element.