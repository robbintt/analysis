---
ver: rpa2
title: Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval
arxiv_id: '2309.12158'
source_url: https://arxiv.org/abs/2309.12158
tags:
- music
- retrieval
- audio
- sheet
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of robust and truly large-scale
  audio-sheet music retrieval, a fundamental problem in Music Information Retrieval
  (MIR). The core method involves using cross-modal deep learning architectures to
  learn joint embedding spaces that link audio and sheet music images.
---

# Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval

## Quick Facts
- **arXiv ID**: 2309.12158
- **Source URL**: https://arxiv.org/abs/2309.12158
- **Reference count**: 14
- **Primary result**: Introduces soft-attention and self-supervised contrastive pre-training to improve cross-modal audio-sheet music retrieval, achieving MRR of 0.75 on synthetic data

## Executive Summary
This paper addresses the challenge of robust and truly large-scale audio-sheet music retrieval using cross-modal deep learning architectures. The core method involves learning joint embedding spaces that link audio spectrograms and sheet music images through convolutional neural networks with pairwise ranking loss. Key innovations include a soft-attention mechanism to handle variable tempo and context discrepancies, self-supervised contrastive pre-training on real-world data to improve generalization, and dynamic time warping (DTW) for exploiting temporal dependencies between subsequent snippets. While substantial progress has been made, challenges remain in developing efficient retrieval structures for large-scale collections and handling diverse instrumentation and genres.

## Method Summary
The approach uses cross-modal deep learning to learn joint embedding spaces linking audio and sheet music. The architecture consists of two CNN pathways (one for audio spectrograms, one for sheet music images) that map inputs to a shared embedding space using pairwise ranking loss with a CCA layer. A soft-attention mechanism is introduced on the audio pathway to mask irrelevant temporal regions, improving robustness to tempo variations. Self-supervised contrastive pre-training on real-world data (MAESTRO for audio, IMSLP for sheet music) enhances generalization beyond synthetic training data. For piece identification, DTW is applied to sequences of embeddings to exploit temporal dependencies. The system is trained on synthetic data from the MSMD dataset and evaluated using MRR, R@K, and MR metrics.

## Key Results
- Soft-attention mechanism significantly improved retrieval performance, with MRR increasing from 0.63 to 0.75 on synthetic data
- Self-supervised contrastive pre-training on real-world data improved generalization, with pre-trained models outperforming baseline in all evaluation scenarios
- DTW-based matching strategy improved cross-modal piece identification by exploiting temporal dependencies between subsequent snippets
- The approach achieved strong performance on synthetic data but showed substantial degradation when applied to real-world noisy data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft-attention improves robustness to variable tempo and context discrepancies by learning to mask irrelevant frames in audio snippets
- Mechanism: The attention mechanism generates a 1-D probability density function that acts as a mask over expanded audio spectrogram frames, allowing the network to focus on musically relevant content regardless of tempo
- Core assumption: The attention pathway can learn to distinguish between musically relevant and irrelevant temporal regions within an audio snippet
- Evidence anchors: Boost in retrieval performance with MRR increasing from 0.63 to 0.75 when attention is added to baseline

### Mechanism 2
- Claim: Self-supervised contrastive pre-training on real-world data improves generalization from synthetic to real audio-sheet music pairs
- Mechanism: The network learns invariant representations by contrasting augmented versions of real audio and sheet music snippets, without requiring alignment annotations
- Core assumption: Data augmentations that preserve musical content can create effective positive pairs for contrastive learning
- Evidence anchors: Pre-trained models outperform baseline in all scenarios across synthetic, partially real, and fully real datasets

### Mechanism 3
- Claim: Dynamic Time Warping (DTW) on embedding sequences improves piece identification by exploiting temporal dependencies between subsequent snippets
- Mechanism: DTW aligns the sequence of embeddings from the query with each candidate piece, using the alignment cost as a matching score
- Core assumption: Temporal coherence between consecutive snippets is preserved in the embedding space and can be exploited for matching
- Evidence anchors: DTW-based matching strategy improves identification results by a large margin on real and noisy music data

## Foundational Learning

- **Cross-modal embedding spaces**: Why needed here: Core task requires comparing fundamentally different data types (audio waveforms/spectrograms vs. sheet music images) in a shared representation. Quick check: How would you evaluate whether two modalities are well-aligned in a joint embedding space?

- **Data augmentation for self-supervised learning**: Why needed here: Without labeled alignment data, the system must create meaningful training pairs from unlabeled real-world data. Quick check: What properties must augmentations have to be useful for contrastive learning in music?

- **Attention mechanisms in sequence modeling**: Why needed here: Variable tempo and note durations create context discrepancies that fixed-size snippets cannot handle. Quick check: How does soft-attention differ from simply using longer fixed-size snippets?

## Architecture Onboarding

- **Component map**: Data augmentation → Pre-training (contrastive) → Fine-tuning (supervised ranking) → Inference (nearest neighbor/DTW)
- **Critical path**: Audio pathway (CNN + attention) → Embedding space (cosine similarity) → Retrieval layer (nearest neighbor) → Matching layer (DTW for piece identification)
- **Design tradeoffs**: Fixed vs. variable snippet sizes (flexibility vs. computational cost), Supervised vs. self-supervised pre-training (annotation cost vs. generalization), Attention mechanism complexity vs. retrieval speed
- **Failure signatures**: Poor retrieval performance on real data despite good synthetic results (overfitting), High variance in attention masks across similar queries (unstable learning), DTW alignment costs that don't correlate with perceptual similarity (embedding space issues)
- **First 3 experiments**:
  1. Compare MRR on synthetic vs. real data for baseline vs. attention models
  2. Ablation study: pre-training with different augmentation sets and their impact on real data performance
  3. DTW alignment visualization: show how alignment costs change with different snippet embedding strategies

## Open Questions the Paper Calls Out

- **Efficient retrieval algorithms**: How can we develop efficient and scalable retrieval algorithms for cross-modal music retrieval in large and heterogeneous music collections? The paper identifies the need for efficient structures for fast retrieval but doesn't provide concrete solutions.

- **Generalization to real-world data**: How can we improve the generalization of audio-sheet music retrieval systems to real-world data, considering the limitations of synthetic data? The paper acknowledges limitations of synthetic data but doesn't provide comprehensive solutions.

- **Unified methodology for diverse music**: How can we develop a unified and robust methodology for cross-modal music retrieval that handles diverse instrumentation, genres, and structural differences between audio performances and sheet music? The paper discusses these challenges but doesn't provide comprehensive solutions.

## Limitations
- Performance evaluation relies heavily on synthetic data with perfect alignment, limiting confidence in real-world applicability
- Attention mechanism effectiveness on complex polyphonic textures and highly variable tempos remains unproven
- Scalability to truly large-scale collections (millions of pairs) and efficient retrieval structures not fully demonstrated

## Confidence

**High Confidence Claims:**
- Cross-modal embedding framework using deep learning is technically sound and reproducible
- Pairwise ranking loss with CCA layer effectively aligns audio and sheet music representations
- DTW for temporal sequence alignment is a valid approach for piece identification

**Medium Confidence Claims:**
- Soft-attention mechanism improves robustness to tempo variations (based on controlled synthetic experiments)
- Self-supervised contrastive pre-training on real-world data improves generalization (tested on limited real datasets)
- Proposed architecture outperforms baseline models in most evaluation scenarios

**Low Confidence Claims:**
- System performance on truly large-scale, diverse real-world collections without clean alignment annotations
- Attention mechanism effectiveness on complex musical passages with multiple instruments and voices
- Scalability of approach to handle millions of audio-sheet music pairs efficiently

## Next Checks

1. **Ablation Study on Real Data**: Conduct controlled experiments comparing baseline, attention-only, and pre-training-only variants on the same real-world dataset (MAESTRO and IMSLP) to isolate component contributions, reporting MRR and R@K across multiple genres and instrumentation types.

2. **Attention Mechanism Robustness Test**: Design benchmark with synthetic data including extreme tempo variations, polyphonic passages, and structural variations (repeats, variations) to test whether attention consistently identifies musically relevant regions without suppressing important content or amplifying noise.

3. **Scalability and Retrieval Efficiency**: Implement and evaluate proposed retrieval structure (approximate nearest neighbor indexing) on dataset scaled to 100,000+ audio-sheet music pairs, measuring both retrieval accuracy and computational efficiency to verify large-scale applicability claims.