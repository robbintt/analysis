---
ver: rpa2
title: Rethinking Evaluation Metric for Probability Estimation Models Using Esports
  Data
arxiv_id: '2309.06248'
source_url: https://arxiv.org/abs/2309.06248
tags:
- score
- probability
- balance
- estimation
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates metrics for evaluating probability estimation
  models in esports, where traditional accuracy measures fail due to inherent uncertainty
  in match outcomes. The authors analyze the Brier score and Expected Calibration
  Error (ECE), identifying limitations such as sensitivity to operating conditions
  and reliance on binning techniques.
---

# Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data

## Quick Facts
- arXiv ID: 2309.06248
- Source URL: https://arxiv.org/abs/2309.06248
- Reference count: 20
- One-line primary result: Proposes a novel Balance score metric for evaluating probability estimation models that addresses limitations of Brier score and ECE, particularly for esports applications where uncertainty in match outcomes makes traditional accuracy measures inadequate.

## Executive Summary
This paper addresses the challenge of evaluating probability estimation models in esports contexts where traditional accuracy measures fail due to inherent uncertainty in match outcomes. The authors identify key limitations in existing metrics like Brier score and Expected Calibration Error (ECE), particularly their sensitivity to operating conditions and reliance on binning techniques. They propose a novel Balance score that evaluates calibration performance without hyperparameters, maintains interpretability, and approximates true expected calibration error effectively. Through simulation and real game data from League of Legends, the Balance score demonstrates superior performance in reflecting model reliability compared to accuracy and Brier score, especially under varying operating conditions.

## Method Summary
The paper analyzes probability estimation models for esports using logistic regression on game snapshot data from League of Legends. The authors collect 100,000 matches with feature vectors at 5, 10, and 15 minutes (gold difference, experience difference for 5 roles, dragons killed, towers destroyed). They train logistic regression models on 60,000 matches and evaluate on 40,000 test matches. The evaluation compares traditional metrics (accuracy, Brier score) with the proposed Balance score and ECE, using both real data and synthetic beta distribution simulations to study operating condition effects.

## Key Results
- Balance score effectively approximates true expected calibration error without requiring binning hyperparameters
- Balance score maintains consistent performance across varying operating conditions while ECE shows sensitivity to bin count
- The proposed metric demonstrates superior reliability evaluation compared to accuracy and Brier score in esports contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Balance score avoids the hyperparameter tuning problem of ECE by eliminating the binning step.
- Mechanism: Instead of dividing predictions into M bins and computing errors per bin, the Balance score computes expected error pointwise for each prediction. This removes the dependency on the bin count M and avoids the bias introduced when bins contain too few samples.
- Core assumption: The model's calibration error can be accurately estimated by averaging pointwise expected errors without grouping predictions.
- Evidence anchors:
  - [abstract]: "the Balance score can be an effective approximation of the true expected calibration error which has been imperfectly approximated by ECE using the binning technique"
  - [section E]: "However, ECE has some limitations... there is no criterion for determining how many bins to divide... the bias occurs because the number of ˆp in each bin decreases"
  - [corpus]: No direct empirical comparison in corpus; claim is based on theoretical analysis in the paper.
- Break condition: If the model's error is highly non-linear with respect to predicted probability, pointwise averaging might not capture the full calibration error structure without grouping.

### Mechanism 2
- Claim: The Balance score remains interpretable and actionable under varying operating conditions.
- Mechanism: The scoring function in the Balance score rewards correct predictions proportionally to the confidence in the prediction, while penalizing incorrect ones inversely. This means the metric naturally adapts to whether the model is underconfident or overconfident without requiring recalibration for different datasets.
- Core assumption: Calibration error can be meaningfully expressed as a single expected score that balances gains and losses across the full probability range.
- Evidence anchors:
  - [section E]: "Equation (11) simply shows that model gets 0 score only if model estimates p under p, and the balance is broken in proportion to the difference"
  - [abstract]: "Balance score that can address the shortcomings identified in other metrics"
  - [corpus]: No direct empirical calibration comparisons; interpretation is derived from the scoring formula.
- Break condition: If the model's calibration error is not symmetric between over- and underconfidence, the balance assumption may not hold.

### Mechanism 3
- Claim: The Balance score can serve as a direct target for model optimization.
- Mechanism: Since the optimal Balance score is zero regardless of operating condition, model training can be guided directly by minimizing the Balance score without needing to estimate or approximate a separate calibration error.
- Core assumption: The scoring function is differentiable (or can be made so) and its expected value under the true distribution can be optimized via gradient-based methods.
- Evidence anchors:
  - [section E]: "Since we know that the optimal value of both metrics is 0, we can directly understand the performance of the logistic regression model itself and thus can target to update the model targeting the values to be 0"
  - [abstract]: "the Balance score can be an effective approximation of the true expected calibration error"
  - [corpus]: No explicit training loop or loss-function derivation in corpus; optimization claim is inferred from theoretical form.
- Break condition: If the Balance score is non-differentiable or noisy in practice, gradient-based optimization may fail.

## Foundational Learning

- Concept: Calibration vs discrimination in probability estimation
  - Why needed here: The paper contrasts accuracy (discrimination) with calibration metrics. Understanding the difference is essential to see why accuracy fails for win probability estimation.
  - Quick check question: What is the difference between a well-calibrated model and a model with high accuracy?

- Concept: Scoring rules and their properness
  - Why needed here: The Balance score is derived as a proper scoring rule. Understanding scoring rules is essential to grasp why the metric works.
  - Quick check question: What makes a scoring rule "proper" and why is this important for probability estimation?

- Concept: Beta distribution modeling for win probability uncertainty
  - Why needed here: The paper uses symmetric beta distributions to simulate operating conditions in esports matches.
  - Quick check question: How does changing the alpha and beta parameters of a beta distribution affect the concentration of win probabilities?

## Architecture Onboarding

- Component map:
  Data ingestion -> Feature extraction -> Model training -> Prediction generation -> Metric computation -> Evaluation analysis

- Critical path:
  1. Load dataset → vectorize features → split train/test
  2. Train probability model on training set
  3. Generate predictions on test set
  4. Compute all three metrics (Brier, ECE, Balance)
  5. Analyze trends across operating conditions

- Design tradeoffs:
  - ECE vs Balance score: ECE gives per-bin error insight but is sensitive to bin count; Balance score is single scalar but less granular.
  - Simulation vs real data: Simulations isolate operating condition effects; real data validates practical relevance.

- Failure signatures:
  - ECE values change drastically with M → binning instability
  - Balance score remains stable but high → calibration issue
  - High Brier score with low accuracy → model is overconfident

- First 3 experiments:
  1. Compare Balance score and ECE on synthetic overconfident model across varying M.
  2. Train logistic regression on 5-min game snapshots, evaluate all metrics.
  3. Simulate three beta distributions (Beta(0.5,0.5), Beta(1,1), Beta(2,2)), compute optimal model scores.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the analysis, several important questions remain:

### Open Question 1
- Question: How does the Balance score perform compared to ECE when applied to models with underconfidence tendencies?
- Basis in paper: [explicit] The paper states "If model has an underconfident property, True ECE ≈ Balance score" but does not provide empirical evidence for this claim.
- Why unresolved: The paper only empirically demonstrates the relationship between ECE and Balance score for overconfident models, leaving the underconfidence case unverified.
- What evidence would resolve it: Empirical experiments comparing ECE and Balance score on models with known underconfidence tendencies would provide validation.

### Open Question 2
- Question: Can the Balance score be extended to multi-class probability estimation problems beyond binary classification?
- Basis in paper: [inferred] The paper focuses exclusively on binary probability estimation in esports contexts, but the general concept of scoring functions could theoretically extend to multi-class scenarios.
- Why unresolved: The paper does not explore or test the Balance score on multi-class problems, nor does it discuss potential modifications needed for such applications.
- What evidence would resolve it: Implementation and testing of the Balance score on multi-class probability estimation tasks with various calibration properties would demonstrate its generalizability.

### Open Question 3
- Question: How does the Balance score perform in comparison to other calibration metrics like Maximum Calibration Error (MCE) or adaptive calibration error (ACE)?
- Basis in paper: [explicit] The paper only compares Balance score with Brier score and ECE, mentioning MCE in passing but not including it in the empirical evaluation.
- Why unresolved: The paper establishes Balance score as superior to Brier score and ECE in certain aspects but does not benchmark it against the broader ecosystem of calibration metrics.
- What evidence would resolve it: Comprehensive experiments comparing Balance score performance against MCE, ACE, and other modern calibration metrics across diverse datasets and model types would clarify its relative advantages.

## Limitations

- The paper lacks extensive empirical validation across diverse datasets beyond the esports domain
- The claim that Balance score is superior for optimization purposes remains largely theoretical without proof-of-concept implementations
- The relationship between Balance score and ECE for underconfident models is stated but not empirically verified

## Confidence

- **High confidence**: The identification of limitations in existing metrics (ECE's sensitivity to binning hyperparameters, Brier score's context-dependence)
- **Medium confidence**: The theoretical construction of the Balance score as a hyperparameter-free alternative
- **Low confidence**: The claim that the Balance score is broadly superior in practice, particularly for optimization purposes

## Next Checks

1. Conduct a systematic ablation study varying the binning hyperparameter M in ECE across multiple datasets to quantify the stability advantage of the Balance score
2. Implement a proof-of-concept training loop using the Balance score as a differentiable loss function and compare convergence properties against standard calibration-aware losses
3. Test the Balance score on additional probability estimation tasks beyond esports (e.g., medical diagnosis, weather forecasting) to assess generalizability