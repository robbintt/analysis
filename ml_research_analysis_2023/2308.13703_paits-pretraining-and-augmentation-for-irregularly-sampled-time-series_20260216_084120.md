---
ver: rpa2
title: 'PAITS: Pretraining and Augmentation for Irregularly-Sampled Time Series'
arxiv_id: '2308.13703'
source_url: https://arxiv.org/abs/2308.13703
tags:
- time
- data
- pretraining
- series
- paits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PAITS introduces a framework for self-supervised pretraining of
  irregularly sampled time series data by combining NLP-inspired pretext tasks (forecasting
  and masked reconstruction) with data augmentations (noise addition and masking).
  It employs a random search to identify the best pretraining strategy for each dataset.
---

# PAITS: Pretraining and Augmentation for Irregularly-Sampled Time Series

## Quick Facts
- arXiv ID: 2308.13703
- Source URL: https://arxiv.org/abs/2308.13703
- Reference count: 38
- Primary result: AUROC of 0.8967 on MIMIC III mortality prediction with 100% labeled data

## Executive Summary
PAITS introduces a framework for self-supervised pretraining of irregularly sampled time series data by combining NLP-inspired pretext tasks (forecasting and masked reconstruction) with data augmentations (noise addition and masking). It employs a random search to identify the best pretraining strategy for each dataset. Experiments on healthcare and retail datasets show consistent improvements over prior methods, especially when labeled data is limited. For example, on MIMIC III mortality prediction, PAITS achieves AUROC of 0.8967 (100% labeled data) compared to 0.8883 for no pretraining, and on H&M purchase prediction, it reaches MAP@12 of 0.0161 (100% labeled data) versus 0.0132 for no pretraining. Different datasets benefit from different strategies, highlighting the need for systematic selection.

## Method Summary
PAITS uses NLP-inspired pretraining tasks (forecasting and masked reconstruction) combined with data augmentations (noise and masking) to learn robust representations from irregularly sampled time series. The framework represents time series as sequences of (time, feature, value) triplets and employs a random search to identify the optimal pretraining strategy for each dataset. The encoder uses triplet embeddings, a transformer, and fusion self-attention, followed by static feature fusion. Two pretext tasks are used: forecasting (predicting future values) and reconstruction (reconstructing masked sequences). The method is evaluated on healthcare (MIMIC III, Physionet 2012, eICU) and retail (H&M) datasets, showing consistent improvements over baseline methods.

## Key Results
- PAITS achieves AUROC of 0.8967 on MIMIC III mortality prediction (100% labeled data) vs 0.8883 for no pretraining
- On H&M purchase prediction, PAITS reaches MAP@12 of 0.0161 (100% labeled data) vs 0.0132 for no pretraining
- Different datasets benefit from different pretraining strategies, demonstrating the need for systematic selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random search over pretraining strategies consistently finds better configurations for each dataset than fixed methods.
- Mechanism: Different datasets have different sparsity patterns, noise characteristics, and temporal correlations; random search explores the strategy space to match each dataset's specific needs.
- Core assumption: The best pretraining strategy is dataset-dependent and cannot be predetermined.
- Evidence anchors:
  - [abstract] "different datasets benefit from different pretraining choices"
  - [section] "we found that there was not a one-size-fits-all approach"
  - [corpus] No direct evidence; this is a methodological design choice
- Break condition: If pretraining strategy impact becomes negligible compared to finetuning or dataset quality, random search offers little benefit.

### Mechanism 2
- Claim: Combining NLP-inspired pretraining tasks (forecasting and masked reconstruction) is more effective than contrastive learning for irregularly sampled time series.
- Mechanism: Sequence-based representations allow direct application of NLP tasks; these tasks explicitly model missingness and irregularity rather than assuming regular intervals.
- Core assumption: Irregular sampling patterns are better captured by NLP-style sequence modeling than by contrastive methods designed for regular signals.
- Evidence anchors:
  - [abstract] "NLP-inspired pretraining tasks and augmentations"
  - [section] "inspired by masked language modeling, is to reconstruct the original time series"
  - [corpus] No direct evidence; this is the core contribution
- Break condition: If future contrastive methods adapt to irregularity without strong regularity assumptions, the advantage may diminish.

### Mechanism 3
- Claim: Augmentations (noise and masking) improve robustness by simulating realistic variations in irregularly sampled data.
- Mechanism: Adding Gaussian noise to times/values and masking random elements forces the model to learn representations invariant to common irregularities like sensor failures or missing observations.
- Core assumption: Real-world irregularities can be approximated by synthetic noise and masking patterns during pretraining.
- Evidence anchors:
  - [abstract] "we propose combining two classes of augmentations (which we illustrate in toy examples)"
  - [section] "we apply additional augmentations to further down-sample the time series"
  - [corpus] No direct evidence; augmentation design is heuristic
- Break condition: If augmentations introduce biases that do not reflect real data patterns, model generalization may suffer.

## Foundational Learning

- Concept: Sequence-based representation of time series (events as triplets)
  - Why needed here: Avoids discretization-induced sparsity and missingness; aligns with NLP-style sequence modeling
  - Quick check question: Can you explain why representing a time series as a sequence of (time, feature, value) triplets is more flexible than a fixed-grid matrix?

- Concept: Pretraining tasks (forecasting vs. reconstruction)
  - Why needed here: Forecasting predicts future values given past observations; reconstruction predicts masked values from context—both leverage self-supervision
  - Quick check question: What is the difference between the forecasting and reconstruction pretext tasks in PAITS, and when might each be more useful?

- Concept: Random search vs. grid search for hyperparameter tuning
  - Why needed here: The strategy space is large and likely sparse; random sampling is more efficient at finding good configurations than exhaustive grid search
  - Quick check question: Why might random search be preferred over grid search when tuning pretraining strategies across multiple datasets?

## Architecture Onboarding

- Component map: Triplet embeddings -> Transformer -> Fusion self-attention -> Static feature fusion -> Forecasting module (dense layer) -> Reconstruction module (three dense layers) -> Prediction module (two dense layers) -> Augmentation pipeline (noise -> masking)

- Critical path:
  1. Load unlabeled data -> generate pretraining windows
  2. Sample strategy (λF, λR, augmentations, finetune aug)
  3. Pretrain with combined loss (λF·LF + λR·LR)
  4. Fine-tune on labeled data with selected strategy
  5. Evaluate and select best validation strategy

- Design tradeoffs:
  - Fixed vs. adaptive max sequence length: Longer allows more context but increases compute
  - Joint vs. separate pretraining tasks: Joint training is efficient but may require tuning λF, λR
  - Augmentation strength: Too much can distort patterns; too little may not help

- Failure signatures:
  - Training instability: Check learning rates and gradient norms
  - Poor generalization: Verify augmentations are realistic and not over-corrupting
  - Strategy search ineffective: Confirm search space covers meaningful variations

- First 3 experiments:
  1. Run baseline (no pretraining) to establish floor performance
  2. Test one fixed strategy (e.g., STraTS: forecasting only, no augmentations) to confirm improvement
  3. Run PAITS strategy search and compare best-found strategy to fixed approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PAITS' performance scale with increasingly large unlabeled pretraining datasets, and is there a point of diminishing returns?
- Basis in paper: [inferred] The paper uses varying amounts of unlabeled data across datasets (e.g., MIMIC III: 422K pretraining vs. 29K labeled), but does not systematically explore scaling effects or saturation points.
- Why unresolved: The experiments use fixed pretraining dataset sizes per domain, so the relationship between pretraining data size and downstream performance gains remains unclear.
- What evidence would resolve it: Controlled experiments varying unlabeled dataset size while holding other factors constant, measuring performance on downstream tasks to identify scaling trends or saturation.

### Open Question 2
- Question: Would PAITS benefit from dynamically adapting pretraining strategies during the search process based on intermediate validation performance?
- Basis in paper: [inferred] PAITS uses random search to sample strategies, but the paper does not explore adaptive or Bayesian optimization approaches that could potentially converge faster or find better strategies.
- Why unresolved: The current random search approach treats all strategies equally without leveraging information from previous trials to guide future sampling.
- What evidence would resolve it: Comparative experiments between random search and adaptive search methods (e.g., Bayesian optimization) using the same strategy space and evaluation metrics.

### Open Question 3
- Question: How sensitive is PAITS to the choice of hyperparameters for the neural network architecture (e.g., number of transformer layers, embedding dimensions)?
- Basis in paper: [explicit] The paper states "While the architecture is flexible, we held it constant for our PAITS strategy search experiments" and uses a fixed architecture across all datasets.
- Why unresolved: The fixed architecture assumption means the contribution of architectural choices versus pretraining strategy choices cannot be disentangled.
- What evidence would resolve it: Ablation studies varying architectural hyperparameters while keeping pretraining strategies constant, or jointly optimizing both architectural and pretraining hyperparameters.

## Limitations
- Random search methodology may find dataset-specific strategies that don't transfer well to new data distributions
- The claim that NLP-inspired tasks are inherently better than contrastive methods lacks comparative evidence against state-of-the-art contrastive approaches adapted for irregularity
- Extensive computational resources required for random search may limit real-world applicability

## Confidence
- **High confidence**: The empirical improvements over baseline methods (AUROC of 0.8967 vs 0.8883 on MIMIC III) are well-documented and reproducible
- **Medium confidence**: The claim that different datasets require different pretraining strategies is supported by results but could reflect limited search space rather than fundamental dataset differences
- **Low confidence**: The assertion that NLP-inspired tasks are inherently better than contrastive methods for irregular time series lacks comparative evidence against state-of-the-art contrastive approaches adapted for irregularity

## Next Checks
1. Test PAITS strategy transfer by applying the best-found strategy from one dataset to another to measure cross-dataset generalization
2. Compare PAITS against contrastive learning methods specifically designed for irregular time series to validate the NLP-inspired approach advantage
3. Measure the computational cost of random search versus fixed strategies to assess practical scalability for real-world deployment