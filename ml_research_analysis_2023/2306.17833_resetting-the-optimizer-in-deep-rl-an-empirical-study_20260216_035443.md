---
ver: rpa2
title: 'Resetting the Optimizer in Deep RL: An Empirical Study'
arxiv_id: '2306.17833'
source_url: https://arxiv.org/abs/2306.17833
tags:
- training
- frames
- million
- rainbow
- reset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes resetting the internal parameters of deep RL
  optimizers like Adam at the start of each training iteration. The authors argue
  that not resetting these parameters causes "contamination" when the optimization
  landscape changes significantly between iterations.
---

# Resetting the Optimizer in Deep RL: An Empirical Study

## Quick Facts
- arXiv ID: 2306.17833
- Source URL: https://arxiv.org/abs/2306.17833
- Reference count: 40
- Key outcome: Resetting Adam's moment estimates at each RL iteration improves Rainbow performance on Atari games, especially for smaller K values, with benefits extending to RMSProp, Rectified Adam, and proximal updates.

## Executive Summary
This paper proposes resetting the internal parameters (first and second moment estimates) of deep RL optimizers like Adam at the start of each training iteration. The authors argue that not resetting these parameters causes "contamination" when the optimization landscape changes significantly between iterations, which is common in RL with forward bootstrapping. They demonstrate that resetting Adam's moments improves Rainbow's performance on Atari games, particularly when using smaller K values. The benefit extends to other optimizers like RMSProp and Rectified Adam, and even helps with proximal updates. On 55 Atari games with 10 seeds, resetting Adam and Rectified Adam both outperform the baseline Rainbow.

## Method Summary
The method involves modifying deep RL training to reset optimizer internal state (first moment m=0, second moment v=0, timestep counter i=0) at the start of each iteration, before any gradient updates. This is applied to Rainbow on Atari games and SAC on MuJoCo tasks. Experiments compare performance with and without resetting across different K values (updates per iteration), different optimizers (Adam, RMSProp, Rectified Adam), and proximal updates. The evaluation uses human-normalized median and mean scores for Atari games and average return for MuJoCo tasks.

## Key Results
- Resetting Adam improves Rainbow's median human-normalized score on 55 Atari games from 112% to 125%
- The benefit is most pronounced for smaller K values (e.g., K=1000), where fewer gradient steps make contamination more costly
- Resetting works for multiple optimizers including Adam, RMSProp, Rectified Adam, and proximal updates
- On MuJoCo tasks, SAC shows smaller improvements from resetting, likely due to Polyak averaging reducing landscape changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Resetting prevents contamination of moment estimates by gradients from different loss landscapes.
- Mechanism: In RL with forward bootstrapping, the loss function changes per iteration because the target network is frozen while the online network updates. Adam accumulates first/second moment estimates across these different landscapes. Resetting forces Adam to start with zero moments, avoiding carry-over from unrelated optimization surfaces.
- Core assumption: The loss landscape at iteration t is sufficiently different from iteration t-1 that moment estimates from t-1 become detrimental rather than helpful.
- Evidence anchors:
  - [abstract] "we demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one."
  - [section 3] "If the counter i is not reset, then the debiasing quantities 1 − power(β1, i) and 1 − power(β2, i) quickly go to 1 and so the debiasing steps will have minimal effect on the overall update."

### Mechanism 2
- Claim: Resetting improves performance especially when K is small, because the agent spends fewer steps unlearning outdated moment estimates.
- Mechanism: With small K, Adam has limited gradient steps to adapt to the new landscape. If moment estimates carry over, the first few steps are wasted "unlearning" old information. Resetting allows all K steps to be used for adapting to the current landscape.
- Core assumption: Fewer gradient steps per iteration increase the relative cost of contamination versus adaptation.
- Evidence anchors:
  - [section 4.1] "Observe that with K = 1000, resetting the Adam optimizer under the Rainbow agent results in a significantly better performance on most of the 12 randomly-chosen games."
  - [section 4.1] "With resetting, however, the agent actually works even better with small K because fewer number of steps are needed to reasonably solve each iteration."

### Mechanism 3
- Claim: Resetting works for multiple optimizers because the contamination effect is a general property of optimizers that maintain internal state across changing loss functions.
- Mechanism: Any optimizer that tracks running statistics (first/second moments, running averages) can suffer from cross-iteration contamination. Resetting removes stale state, making the optimizer's internal assumptions valid again.
- Core assumption: The internal state accumulation assumption (stationarity of the optimization problem) is violated in RL with changing targets.
- Evidence anchors:
  - [section 4.2] "we still observe that resetting the optimizer per iteration can provide a positive effect in terms of reward performance."
  - [section 4.2] "Notice that while overall we see a performance degradation when we move from Adam to RMSProp, we still observe that resetting the optimizer at each iteration can provide a positive effect in terms of reward performance."

## Foundational Learning

- Concept: Forward bootstrapping in RL
  - Why needed here: It's the source of the non-stationary loss landscape that causes optimizer contamination.
  - Quick check question: In DQN, what two networks are updated, and how often does the target network get synchronized with the online network?

- Concept: Adam's bias correction mechanism
  - Why needed here: Understanding debiasing explains why resetting to zero is safe and why not resetting leads to near-zero debiasing over time.
  - Quick check question: What happens to the debiasing factors 1 − β^t as t grows large, and why does this matter if we never reset?

- Concept: Hyperparameter K in Rainbow/DQN
  - Why needed here: K controls how many gradient steps are taken per target update, directly affecting how much contamination can accumulate and how much resetting helps.
  - Quick check question: If K = 8000 by default, what is the practical implication of reducing K to 1000 on the optimizer's state?

## Architecture Onboarding

- Component map:
  - environment interaction -> replay buffer -> mini-batch sampling -> gradient computation -> optimizer update -> periodic target network sync

- Critical path:
  1. Sample mini-batch from replay buffer
  2. Compute gradients
  3. Before applying optimizer update, reset m=0, v=0, i=0
  4. Apply Adam update
  5. Repeat for k=0..K-1
  6. Sync target network

- Design tradeoffs:
  - Reset every iteration vs. every N iterations: more frequent resets reduce contamination but may lose momentum benefits; less frequent resets preserve momentum but risk more contamination.
  - Reset vs. decay: resetting is a hard reset; decaying moments gradually could be a softer alternative but requires tuning.
  - Reset for all optimizers vs. selective: resetting is cheap and safe for Adam/RMSProp; for Polyak-updated methods, resetting may be less beneficial.

- Failure signatures:
  - Performance drops after reset: likely due to too-frequent resets or small K where momentum is needed.
  - No improvement from reset: landscape changes are minimal (e.g., Polyak averaging smooths target changes), or K is large enough to unlearn contamination within the iteration.
  - Worse than no reset: optimizer was benefiting from accumulated state (rare in RL with changing targets, more common in stationary supervised learning).

- First 3 experiments:
  1. Run Rainbow with Adam, K=8000, no reset vs. reset every iteration; compare median human-normalized score over 55 games.
  2. Vary K ∈ {1, 500, 1000, 2000, 4000, 6000, 8000} with reset vs. no reset; plot area under learning curve to find optimal K.
  3. Replace Adam with RMSProp and Rectified Adam; repeat experiment 1 to confirm cross-optimizer benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does resetting optimizers provide similar benefits across different deep RL algorithms beyond Rainbow and SAC, such as DQN, PPO, or A3C?
- Basis in paper: [explicit] The paper shows resetting benefits Rainbow and SAC, but suggests broader implications for deep RL optimization.
- Why unresolved: The study primarily focused on Rainbow and SAC, leaving open whether the benefits generalize to other algorithms with different update rules and architectures.
- What evidence would resolve it: Systematic experiments applying optimizer resetting to a diverse set of deep RL algorithms (e.g., DQN, PPO, A3C) across various benchmarks, measuring performance changes and consistency of benefits.

### Open Question 2
- Question: How does the frequency of optimizer resetting affect learning performance and stability in deep RL?
- Basis in paper: [explicit] The paper resets optimizers at the start of each iteration, but also explores random resetting with probability 1/K.
- Why unresolved: While the paper shows benefits of per-iteration resetting, it does not explore the full spectrum of resetting frequencies (e.g., every 100, 500, 1000 steps) or their impact on learning dynamics.
- What evidence would resolve it: Experiments varying the resetting frequency (e.g., every 100, 500, 1000 steps) across multiple environments and algorithms, analyzing performance curves, convergence speed, and stability metrics.

### Open Question 3
- Question: What is the theoretical explanation for why optimizer resetting mitigates contamination in deep RL optimization?
- Basis in paper: [inferred] The paper argues that optimizer internal parameters become contaminated when the optimization landscape changes significantly between iterations, and resetting prevents this.
- Why unresolved: The paper provides empirical evidence of benefits but does not offer a rigorous theoretical analysis of the underlying mechanisms or conditions under which contamination occurs.
- What evidence would resolve it: Theoretical analysis deriving conditions for contamination in deep RL optimization, potentially using techniques from optimization theory or dynamical systems, validated by controlled experiments isolating key factors.

## Limitations
- Limited evaluation beyond Atari games - results may not generalize to all RL domains
- Smaller benefits observed in continuous control tasks with SAC
- Does not explore partial resetting (e.g., only first or second moment) or adaptive resetting frequencies

## Confidence
- High confidence in the mechanism and empirical results for Atari Rainbow: The paper provides clear ablation studies across multiple games and seeds, with consistent improvements when resetting.
- Medium confidence in cross-optimizer benefits: While results show positive effects for RMSProp and Rectified Adam, the magnitude varies significantly, and the underlying reasons for different optimizer responses warrant further investigation.
- Low confidence in SAC results: The MuJoCo experiments show minimal benefit, and the authors attribute this to Polyak averaging, but more rigorous analysis of when resetting helps versus hurts would strengthen these claims.

## Next Checks
1. Test the interaction between resetting and learning rate schedules - does resetting interfere with warm-up or decay schedules commonly used in RL?
2. Evaluate the effect of partial resetting (e.g., only first moment, only second moment) to determine which components drive the improvements.
3. Conduct a more systematic study of K values across diverse task families to identify regimes where resetting is most/least beneficial.