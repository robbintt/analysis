---
ver: rpa2
title: 'Don''t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers'
arxiv_id: '2308.12661'
source_url: https://arxiv.org/abs/2308.12661
tags:
- attack
- solarization
- accuracy
- image
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adversarial solarization, a new adversarial
  attack based on image solarization that is conceptually simple yet effective at
  degrading accuracy of ImageNet classifiers, especially when not included in training
  augmentations. The attack is cheap to compute and does not destroy global image
  structure.
---

# Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers

## Quick Facts
- arXiv ID: 2308.12661
- Source URL: https://arxiv.org/abs/2308.12661
- Reference count: 0
- Primary result: Adversarial solarization degrades ImageNet classifier accuracy, especially when not in training augmentations, but robustness can be improved with solarization-aware training.

## Executive Summary
This paper introduces adversarial solarization, a new adversarial attack based on image solarization that is conceptually simple yet effective at degrading accuracy of ImageNet classifiers, especially when not included in training augmentations. The attack is cheap to compute and does not destroy global image structure. Evaluations show significant accuracy drops for common ImageNet models under the attack. While incorporating solarization into training augmentations increases robustness, no full immunity is achieved. Universal weak spots also exist that can be exploited in black-box attacks. The results highlight the need for holistic robustness evaluations beyond standard corruptions benchmarks.

## Method Summary
The paper proposes a gradient-free adversarial attack based on image solarization, where pixel values above a threshold α are inverted. A greedy random search strategy is used to find effective α values that cause misclassification, avoiding the pitfalls of noisy, non-convex loss landscapes under solarization. The attack is evaluated on multiple ImageNet models with and without solarization augmentations in training, and universal weak spots are identified that enable black-box attacks.

## Key Results
- Solarization significantly degrades ImageNet classifier accuracy, especially for models without solarization augmentations in training.
- Incorporating solarization into training augmentations (e.g., RandAug) improves robustness but does not provide full immunity.
- Universal solarization thresholds exist that can be exploited in black-box attacks across multiple models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Solarization alters pixel intensities in a way that disproportionately affects the model's reliance on texture over shape.
- Mechanism: The solarization function inverts pixel values above a threshold α, changing color/texture information while preserving overall shape structure. Models trained on natural images often rely more heavily on texture cues, making them susceptible to this form of attack.
- Core assumption: The model's decision boundary is more sensitive to texture variations than to shape preservation under solarization.
- Evidence anchors:
  - [abstract] "This paper introduces adversarial solarization, a new adversarial attack based on image solarization that is conceptually simple yet effective at degrading accuracy of ImageNet classifiers, especially when not included in training augmentations."
  - [section] "Observing the attack, we notice its primary impact on color information. In specific instances (binder) it even introduces new edges. Nevertheless, across all cases, the global shape remains consistent."
  - [corpus] Weak evidence; corpus neighbors focus on different attack types and defenses, not specifically on solarization-based attacks.
- Break condition: If a model is explicitly trained with solarization augmentations (e.g., RandAug), the attack's effectiveness diminishes but does not disappear entirely.

### Mechanism 2
- Claim: Solarization creates adversarial examples without requiring gradient-based optimization, avoiding pitfalls of loss landscape non-convexity.
- Mechanism: A greedy random search is used to find a threshold α that causes misclassification, bypassing the need for gradient computation which may fail due to noisy, non-convex loss landscapes under solarization.
- Core assumption: Random search can efficiently find effective α values despite the complex loss landscape.
- Evidence anchors:
  - [section] "Given these landscapes, gradient-based searches for optimal α values, as utilized in adversarial attacks [1], are less likely to succeed. As a result, we deviate to a greedy random search strategy for α - an approach conceptually simpler and insensitive to the loss landscape due to being gradient-free."
  - [corpus] No direct evidence; corpus focuses on different attack strategies.
- Break condition: If the loss landscape becomes more predictable or if more efficient search strategies are developed, this mechanism's advantage may erode.

### Mechanism 3
- Claim: Universal solarization thresholds exist that can serve as black-box attacks across multiple models.
- Mechanism: Certain α values consistently degrade performance across different models, especially those not trained with solarization augmentations, enabling effective black-box attacks without model-specific tuning.
- Core assumption: Models share common vulnerabilities to specific solarization intensities.
- Evidence anchors:
  - [section] "In other settings, the attack can often be simplified into a black-box attack with model-independent parameters."
  - [section] "Interestingly, the optimal attack parameters tend to correlate with the specific training schemes as the models display a higher degree of variance in trends..."
  - [corpus] Weak evidence; corpus neighbors do not discuss universal attack parameters for solarization.
- Break condition: If models are trained with diverse augmentation strategies, universal α values become less effective.

## Foundational Learning

- Concept: Image solarization transformation
  - Why needed here: Understanding the exact mathematical transformation (pixel inversion above threshold α) is critical to implementing the attack and predicting its effects.
  - Quick check question: What happens to pixel values when α = 0 versus α = 1?

- Concept: Adversarial attack evaluation metrics
  - Why needed here: Measuring robustness requires knowledge of top-1/top-5 accuracy, clean vs. attacked performance, and how to interpret degradation.
  - Quick check question: How does top-1 accuracy differ from top-5 accuracy in the context of adversarial attacks?

- Concept: Gradient-free optimization strategies
  - Why needed here: The paper uses random search instead of gradient-based methods due to noisy loss landscapes; understanding this choice is key to replicating the attack.
  - Quick check question: Why might gradient-based searches fail on solarization loss landscapes?

## Architecture Onboarding

- Component map:
  Image preprocessing module -> Model inference pipeline -> Search algorithm -> Evaluation harness

- Critical path:
  1. Load image and model.
  2. For each sample, iterate n times: apply solarization with random α, evaluate model output.
  3. Select α that causes misclassification (if any).
  4. Report accuracy drop.

- Design tradeoffs:
  - Search budget (n) vs. attack success rate: Higher n increases success but also computation time.
  - Attack granularity (top-1 vs top-5) vs. defense evasion: Top-5 attacks are stronger but less stealthy.
  - Universal α vs. sample-specific α: Universal is faster but weaker.

- Failure signatures:
  - No accuracy drop: Model may be trained with solarization augmentations.
  - High variance in α effectiveness: Indicates model-specific defenses or architecture differences.
  - Consistent misclassification to same wrong class: May indicate dataset bias or shortcut learning.

- First 3 experiments:
  1. Apply solarization with α=0 (full inversion) to a pretrained ResNet-50 and measure top-1 accuracy drop.
  2. Run RandSol-Top1-10 on a model trained with RandAug and compare to baseline.
  3. Test universal α values (e.g., 0.1, 0.5, 0.9) across multiple models and record accuracy trends.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating solarization into training augmentations affect generalization to other out-of-distribution scenarios beyond solarization itself?
- Basis in paper: [explicit] The paper states that models using RandAug training with solarization augmentation are more robust but still show accuracy impairment, suggesting potential trade-offs.
- Why unresolved: The study focuses on solarization robustness but does not explore transfer to other OOD scenarios, leaving the generalization effect unclear.
- What evidence would resolve it: Experiments comparing models trained with and without solarization augmentation across multiple diverse OOD benchmarks (e.g., ImageNet-C, adversarial attacks, real-world distribution shifts) would clarify if solarization training provides broad or narrow robustness gains.

### Open Question 2
- Question: Can the universal weak spots in models be characterized in terms of specific image features or semantic categories that are consistently vulnerable to solarization?
- Basis in paper: [explicit] The paper mentions that model-dependent universal weak spots exist that may be exploited by malicious actors, but does not characterize them.
- Why unresolved: While the existence of weak spots is demonstrated, their nature and the types of images most susceptible are not analyzed.
- What evidence would resolve it: Systematic analysis of which object categories, textures, or semantic concepts are most frequently misclassified under solarization across models would reveal the underlying vulnerabilities.

### Open Question 3
- Question: How do human perception and model predictions diverge under solarization, and can this divergence be quantified or predicted?
- Basis in paper: [explicit] The paper notes that solarization preserves global shape information for humans while degrading model accuracy, highlighting a perceptual gap.
- Why unresolved: The paper does not provide quantitative comparisons between human and model performance or models that better align with human perception under solarization.
- What evidence would resolve it: Human subject studies measuring accuracy and confidence under varying solarization intensities, compared with model predictions, would quantify the divergence and inform more human-aligned architectures.

## Limitations
- Generalizability across diverse architectures and datasets is uncertain, as the attack is only tested on ImageNet models.
- The random search strategy's efficiency and scalability to larger search spaces or higher-resolution images are unclear.
- Adaptive defenses specifically designed to counter solarization are not explored, leaving a gap in understanding the full robustness landscape.

## Confidence
- High Confidence: The claim that solarization degrades ImageNet classifier accuracy is well-supported by empirical results, especially for models without solarization augmentations in training.
- Medium Confidence: The assertion that universal α values can serve as effective black-box attacks is plausible but may overfit to the specific models and datasets tested.
- Low Confidence: The mechanism by which solarization disproportionately affects texture-based models is hypothesized but not rigorously validated through ablation studies or alternative explanations.

## Next Checks
1. Test the solarization attack on non-ImageNet datasets (e.g., CIFAR-10, medical imaging) to assess generalizability.
2. Implement and evaluate defenses specifically designed to counter solarization (e.g., frequency-domain filtering, texture-invariant training).
3. Compare the greedy random search to other gradient-free methods (e.g., Bayesian optimization, evolutionary strategies) to determine if more efficient strategies exist.