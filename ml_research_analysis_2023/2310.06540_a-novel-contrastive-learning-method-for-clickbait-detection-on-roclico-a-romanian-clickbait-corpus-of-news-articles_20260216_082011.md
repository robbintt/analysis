---
ver: rpa2
title: 'A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A
  Romanian Clickbait Corpus of News Articles'
arxiv_id: '2310.06540'
source_url: https://arxiv.org/abs/2310.06540
tags:
- clickbait
- news
- detection
- romanian
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoCliCo, the first publicly available corpus
  for Romanian clickbait detection, containing 8,313 manually annotated news articles.
  The authors propose a novel BERT-based contrastive learning model that learns to
  encode news titles and contents into a deep metric space where non-clickbait pairs
  have high cosine similarity and clickbait pairs have low cosine similarity.
---

# A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles

## Quick Facts
- arXiv ID: 2310.06540
- Source URL: https://arxiv.org/abs/2310.06540
- Reference count: 15
- Primary result: Contrastive RoBERTa achieves F1 scores of 0.8852 (clickbait) and 0.9546 (non-clickbait) on Romanian clickbait detection

## Executive Summary
This paper introduces RoCliCo, the first publicly available corpus for Romanian clickbait detection, containing 8,313 manually annotated news articles. The authors propose a novel BERT-based contrastive learning model that learns to encode news titles and contents into a deep metric space where non-clickbait pairs have high cosine similarity and clickbait pairs have low cosine similarity. The model achieves superior performance compared to traditional fine-tuned models and handcrafted feature approaches, demonstrating the effectiveness of contrastive learning for detecting semantic inconsistency between titles and content.

## Method Summary
The method employs a Siamese RoBERTa architecture where separate encoders process news titles and contents, producing embeddings that are compared using cosine similarity. A contrastive loss function with a fixed margin (m=1) penalizes clickbait pairs with high similarity while encouraging low similarity for non-clickbait pairs. The model is trained on 6,806 articles from three Romanian news sources, with source separation ensuring test articles come from different publications. During inference, cosine similarity scores are thresholded at 0.75 to classify articles as clickbait or non-clickbait.

## Key Results
- Contrastive RoBERTa achieves F1 score of 0.8852 on clickbait articles and 0.9546 on non-clickbait articles
- Outperforms fine-tuned Ro-BERT, BiLSTM, and SVM/RF models based on handcrafted features
- Weighted ensemble combining five models achieves the best overall performance
- Source separation between training and test sets prevents overfitting to publication-specific patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The contrastive learning approach leverages the inherent inconsistency between clickbait titles and their actual content to improve detection accuracy.
- Mechanism: By encoding title-content pairs into a shared embedding space where non-clickbait pairs have high cosine similarity and clickbait pairs have low similarity, the model learns to distinguish clickbait based on semantic coherence.
- Core assumption: The degree of semantic alignment between a title and its content is a reliable indicator of clickbait status.
- Evidence anchors:
  - [abstract] "learns to encode news titles and contents into a deep metric space where non-clickbait pairs have high cosine similarity and clickbait pairs have low cosine similarity"
  - [section] "The embedding space is constructed such that titles and contents of non-clickbait news have high cosine similarity, while titles and contents of clickbait news have low cosine similarity"
- Break condition: If clickbait articles happen to have coherent titles and contents by chance, or if non-clickbait articles have deliberately misleading but accurate titles, the semantic similarity metric would fail.

### Mechanism 2
- Claim: The Siamese RoBERTa architecture enables effective learning of title-content relationships without requiring explicit negative mining.
- Mechanism: By treating each news article as a single pair (title, content) and optimizing the contrastive loss directly on these pairs, the model avoids the computational overhead of finding hard negative examples.
- Core assumption: Each article provides a naturally contrasting pair without needing external negative sampling.
- Evidence anchors:
  - [abstract] "learns to jointly encode news titles and contents into a deep metric space"
  - [section] "Unlike our approach, related contrastive methods use (ti, tj) pairs... require hard example mining"
- Break condition: If the dataset contains many articles where titles and contents are accidentally similar regardless of clickbait status, the model may learn spurious correlations.

### Mechanism 3
- Claim: Source separation between training and test sets prevents overfitting to publication-specific patterns unrelated to clickbait detection.
- Mechanism: By excluding news from the same sources in both training and test sets, the model is forced to learn genuine clickbait indicators rather than source-specific writing styles or formatting cues.
- Core assumption: Publication source characteristics are independent of clickbait status and could otherwise bias the model.
- Evidence anchors:
  - [section] "We separate the news platforms between the training and test sets, keeping the news from Cancan, ProTV and WowBiz for training and those from Libertatea, Viva and Digi24 for testing"
  - [section] "Without source separation... a model that overfits to certain source-specific aspects... will reach high scores on the test"
- Break condition: If clickbait detection patterns are actually correlated with publication sources (e.g., certain publishers consistently use clickbait), this separation could reduce performance unfairly.

## Foundational Learning

- Concept: Contrastive learning and metric learning
  - Why needed here: The model needs to learn a representation space where the distance between embeddings reflects clickbait status based on title-content semantic alignment.
  - Quick check question: How does contrastive loss differ from standard classification loss, and why is it appropriate for learning similarity relationships?

- Concept: Siamese neural networks
  - Why needed here: The architecture processes title and content through identical networks to produce comparable embeddings in the same space.
  - Quick check question: What architectural constraints must be satisfied for a Siamese network to produce comparable embeddings?

- Concept: Cosine similarity as a similarity metric
  - Why needed here: The contrastive loss uses cosine dissimilarity to measure alignment between title and content embeddings.
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for this task, and how does normalization affect the metric?

## Architecture Onboarding

- Component map: Romanian news articles → RoBERTa tokenizer → Siamese RoBERTa encoders → normalized embeddings → cosine similarity → contrastive loss → parameter update

- Critical path:
  Title → RoBERTa encoder → normalized embedding
  Content → RoBERTa encoder → normalized embedding
  Cosine similarity → Contrastive loss → Parameter update

- Design tradeoffs:
  - Using separate RoBERTa encoders for title and content vs. shared encoder with sequence separation
  - Fixed margin value (m=1) vs. learnable margin
  - 256 sequence length limit vs. longer sequences for comprehensive content representation
  - 5 training epochs vs. longer training with potential overfitting risk

- Failure signatures:
  - High similarity scores for clickbait articles indicate the model fails to capture semantic inconsistency
  - Low similarity scores for non-clickbait articles suggest the model misses genuine alignment
  - Poor generalization to new sources indicates overfitting to source-specific patterns
  - Similar performance to standard fine-tuned RoBERTa suggests contrastive learning provides no benefit

- First 3 experiments:
  1. Train with standard fine-tuned RoBERTa (concatenate title and content) as baseline
  2. Train Siamese RoBERTa with contrastive loss but without source separation
  3. Train full contrastive model with source separation and margin-based loss

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the methodology and results presented.

## Limitations
- Limited evaluation scope: Model tested only on Romanian news articles, limiting generalizability to other languages or domains
- Contrastive loss sensitivity: Performance depends heavily on fixed margin value and similarity threshold without sensitivity analysis
- Baseline comparison limitations: Does not explore simpler approaches like concatenation to isolate benefits of contrastive learning

## Confidence
- High confidence: Dataset construction methodology and source separation strategy are well-documented and reproducible
- Medium confidence: Contrastive learning mechanism is theoretically sound but empirical evidence limited to single dataset
- Low confidence: Claims about contrastive learning superiority not well-supported by ablation studies or controlled experiments

## Next Checks
1. Perform ablation study comparing contrastive Siamese model against standard fine-tuned RoBERTa with concatenated inputs to isolate architectural benefits
2. Conduct threshold optimization analysis to determine if the fixed cosine similarity threshold of 0.75 is optimal
3. Test models with and without source separation on held-out sources to verify the separation strategy improves generalization