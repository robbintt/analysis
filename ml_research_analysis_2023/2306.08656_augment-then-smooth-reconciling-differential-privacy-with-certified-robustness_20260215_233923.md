---
ver: rpa2
title: 'Augment then Smooth: Reconciling Differential Privacy with Certified Robustness'
arxiv_id: '2306.08656'
source_url: https://arxiv.org/abs/2306.08656
tags:
- training
- certified
- robustness
- privacy
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DP-CERT, a framework that achieves differential
  privacy (DP) and certified robustness (CR) simultaneously by integrating randomized
  smoothing into differentially private model training. DP-CERT uses augmentation
  multiplicity, which involves averaging the gradients of multiple augmentations of
  the same training sample before clipping, to manage the additional privacy risks
  incurred by data augmentations.
---

# Augment then Smooth: Reconciling Differential Privacy with Certified Robustness

## Quick Facts
- arXiv ID: 2306.08656
- Source URL: https://arxiv.org/abs/2306.08656
- Reference count: 40
- Primary result: DP-CERT framework achieves both differential privacy and certified robustness, showing up to 2.5% increase in certified accuracy on CIFAR10 for the same DP guarantee.

## Executive Summary
This paper presents DP-CERT, a framework that achieves differential privacy (DP) and certified robustness (CR) simultaneously by integrating randomized smoothing into differentially private model training. The key innovation is augmentation multiplicity, which involves averaging the gradients of multiple augmentations of the same training sample before clipping, allowing data augmentations to be incorporated without incurring additional privacy cost. Experimental results on MNIST, Fashion-MNIST, and CIFAR10 demonstrate that DP-CERT significantly outperforms baseline methods in terms of certified accuracy and average certified radius, with up to a 2.5% increase in certified accuracy for the same DP guarantee on CIFAR10.

## Method Summary
DP-CERT is a framework that achieves differential privacy and certified robustness simultaneously. The method uses augmentation multiplicity, which involves averaging the gradients of multiple augmentations of the same training sample before clipping, to manage the additional privacy risks incurred by data augmentations. The framework also incorporates regularization and adversarial training techniques to enhance robustness. The training procedure consists of DPSGD with augmentation multiplicity, randomized smoothing at inference, and optional regularization/adversarial training stages. The model variants include DP-Gaussian, DP-Stability, and DP-MACER, trained on MNIST/Fashion-MNIST using a 4-layer CNN and fine-tuned on CIFAR10 with CrossViT-Tiny.

## Key Results
- DP-CERT significantly outperforms baseline methods in terms of certified accuracy and average certified radius
- Up to 2.5% increase in certified accuracy for the same DP guarantee on CIFAR10
- Larger certifiable radii correlate with smaller local Lipschitz constants
- DP-CERT effectively reduces Lipschitz constants compared to other DP training methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmentation multiplicity enables DP-CERT to incorporate data augmentations without incurring additional privacy cost.
- Mechanism: The gradients of multiple augmentations of the same training sample are averaged before clipping. Since all downstream impact to the model weights from a sample is contained in this averaged gradient, clipping it provides a finite sensitivity as required for the Sampled Gaussian Mechanism used in DPSGD.
- Core assumption: Averaging gradients across augmentations of the same sample preserves the sensitivity properties needed for DP.
- Evidence anchors:
  - [section] "An important component of our DP-CERT is how we handle training with augmented data points. We adopt augmentation multiplicity, introduced in [11] and previously unused in studies of CR for DP, which involves averaging the gradients of multiple augmentations of the same training sample before clipping."
  - [abstract] "DP-CERT uses augmentation multiplicity, which involves averaging the gradients of multiple augmentations of the same training sample before clipping, to manage the additional privacy risks incurred by data augmentations."
- Break condition: If the averaging process does not adequately bound the sensitivity, or if the number of augmentations becomes too large, the privacy guarantee may be violated.

### Mechanism 2
- Claim: Integrating randomized smoothing into DP training provides certified robustness without sacrificing privacy guarantees.
- Mechanism: Gaussian noise is added to inputs during training and inference, and the smoothed classifier's predictions are averaged over this noise distribution. This provides probabilistic robustness verification with a certifiable radius.
- Core assumption: The smoothed classifier's robustness properties transfer to the base classifier when Gaussian noise is used.
- Evidence anchors:
  - [abstract] "DP-CERT, a simple and effective method that achieves both privacy guarantees and certified robustness simultaneously by integrating randomized smoothing into standard differentially private model training."
  - [section] "We aim to make CR feasible within the standard training procedure of DPSGD, with state-of-the-art convergence and proper accounting for additional privacy risks by introducing the DP-CERT framework."
- Break condition: If the smoothing distribution does not adequately capture the input space, or if the base classifier is not sufficiently robust under Gaussian perturbations, the certified radius may be too small.

### Mechanism 3
- Claim: Regularization and adversarial training techniques enhance certified robustness when integrated into DP-CERT.
- Mechanism: Stability training minimizes the distance between the output probability of the original and augmented examples, while consistency regularization minimizes the KL divergence between the average output probability of all smoothed samples and the output probability of each individual sample. Adversarial training uses SmoothAdv to find adversarial examples that maximize the loss in an l2 ball around the input.
- Core assumption: These regularization and adversarial training techniques improve the base classifier's robustness to input perturbations, which translates to higher certified accuracy for the smoothed classifier.
- Evidence anchors:
  - [section] "We propose adapting stability and consistency regularization to private training in order to minimize the distance between the output probability of the original and augmented examples, hereby improving the robustness to input noise."
  - [section] "To achieve better certified accuracy, we incorporate adversarial training by deploying existing attacks to create adversarial examples."
- Break condition: If the regularization or adversarial training techniques are not properly tuned, they may harm convergence or introduce bias, reducing the overall effectiveness of DP-CERT.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP-CERT relies on DP training methods to provide privacy guarantees for the model. Understanding the core principles of DP, such as the privacy budget (epsilon, delta) and the use of gradient clipping and noise addition, is crucial for implementing and analyzing DP-CERT.
  - Quick check question: What is the main difference between the privacy guarantees provided by DP and those provided by randomized smoothing?

- Concept: Randomized Smoothing
  - Why needed here: Randomized smoothing is a key component of DP-CERT, as it provides the certified robustness guarantees. Understanding how randomized smoothing works, including the choice of smoothing distribution and the calculation of the certified radius, is essential for implementing and evaluating DP-CERT.
  - Quick check question: How does the choice of smoothing distribution (e.g., Gaussian) affect the certified radius and the overall performance of the smoothed classifier?

- Concept: Data Augmentation
  - Why needed here: Data augmentation is used in DP-CERT to improve the base classifier's robustness to input perturbations. Understanding the different types of data augmentation techniques (e.g., Gaussian noise, adversarial examples) and their effects on the model's performance is important for optimizing DP-CERT.
  - Quick check question: What is the main challenge in using data augmentation with DP training, and how does DP-CERT address this challenge?

## Architecture Onboarding

- Component map: DP training (DPSGD with gradient clipping and noise addition) -> Data augmentation (augmentation multiplicity) -> Regularization (stability and consistency regularization) -> Adversarial training (SmoothAdv) -> Randomized smoothing (Gaussian noise addition during training and inference)

- Critical path: The critical path in DP-CERT is the training loop, which consists of the following steps:
  1. Sample a mini-batch of data points
  2. Generate augmentations of the data points using augmentation multiplicity
  3. Compute the gradients of the loss function with respect to the model parameters
  4. Clip the gradients to bound the sensitivity
  5. Add Gaussian noise to the aggregated gradients
  6. Update the model parameters using the noisy gradients
  7. Apply regularization and adversarial training techniques as specified

- Design tradeoffs:
  - Privacy vs. robustness: Increasing the number of augmentations or the strength of regularization may improve robustness but also increase the privacy cost.
  - Computation vs. performance: Using more augmentations or more complex regularization techniques may improve performance but also increase the computational cost of training.
  - Model size vs. performance: Using a larger model may improve performance but also increase the privacy cost and computational requirements.

- Failure signatures:
  - Low certified accuracy: This may indicate that the smoothing distribution is not capturing the input space well, or that the base classifier is not sufficiently robust under Gaussian perturbations.
  - High privacy cost: This may indicate that too many augmentations are being used, or that the regularization or adversarial training techniques are too strong.
  - Poor convergence: This may indicate that the regularization or adversarial training techniques are harming the convergence of the model, or that the privacy budget is too restrictive.

- First 3 experiments:
  1. Implement DP-CERT with a simple base classifier (e.g., a small CNN) on a simple dataset (e.g., MNIST) without any regularization or adversarial training. Evaluate the certified accuracy and privacy cost.
  2. Add Gaussian noise augmentation to the base classifier and evaluate the impact on certified accuracy and privacy cost.
  3. Implement stability regularization and evaluate its impact on certified accuracy and privacy cost.

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- Hyperparameter sensitivity: The exact configurations for gradient clipping norms, noise multipliers, and augmentation multiplicity values are not fully specified, affecting reproducibility.
- Computational overhead: The paper does not explicitly quantify the computational cost of augmentation multiplicity, especially for larger models and datasets.
- Privacy-robustness trade-off: The interaction between multiple augmentation types and their combined effect on both privacy guarantees and certified robustness remains underexplored.

## Confidence
- Augmentation Multiplicity Mechanism: High confidence - The theoretical foundation for averaging gradients across augmentations is well-established in the DP literature, and the paper provides clear implementation details.
- Certified Robustness Claims: Medium confidence - While the experimental results demonstrate significant improvements over baselines, the variance across runs and the sensitivity to hyperparameter choices are not fully characterized.
- Lipschitz Constant Analysis: Medium confidence - The correlation between smaller Lipschitz constants and larger certifiable radii is supported by experiments, but the causal relationship and generalizability to other architectures require further validation.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary gradient clipping norms (C), noise multipliers (Ïƒ), and augmentation multiplicity (K) across a grid search to map the privacy-accuracy-robustness Pareto frontier for each dataset and model variant.

2. **Computational Overhead Benchmarking**: Measure and compare training times, memory usage, and communication costs (for federated settings) between DP-CERT and baseline methods across different augmentation multiplicity values and batch sizes.

3. **Cross-Architecture Generalization Study**: Implement DP-CERT on additional model architectures (e.g., ResNet, Vision Transformer variants) and datasets (e.g., SVHN, CIFAR100) to evaluate the framework's robustness to architectural changes and dataset complexity.