---
ver: rpa2
title: Leveraging Word Guessing Games to Assess the Intelligence of Large Language
  Models
arxiv_id: '2310.20499'
source_url: https://arxiv.org/abs/2310.20499
tags:
- word
- llms
- agent
- agents
- spygame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to assess the intelligence
  of Large Language Models (LLMs) through word guessing games. The proposed frameworks,
  DEEP and SpyGame, evaluate LLMs' expression, disguising abilities, and strategic
  thinking.
---

# Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models

## Quick Facts
- arXiv ID: 2310.20499
- Source URL: https://arxiv.org/abs/2310.20499
- Reference count: 40
- One-line primary result: Novel frameworks DEEP and SpyGame effectively evaluate LLMs' intelligence through word guessing games, with GPT-4 outperforming other models in both accuracy and disguise capabilities.

## Executive Summary
This paper introduces DEEP and SpyGame, two novel frameworks that assess Large Language Models' (LLMs) intelligence through word guessing games. DEEP evaluates LLMs' ability to generate both clear and deliberately ambiguous descriptions of words, while SpyGame creates a multi-agent environment to test strategic thinking and theory of mind. Experiments show that these frameworks can effectively distinguish between open-source and closed-source LLMs, with GPT-4 demonstrating superior performance in both accuracy and disguise capabilities. The study also addresses potential biases in multi-agent interactions, ensuring fair evaluation across different models.

## Method Summary
The paper proposes two frameworks for evaluating LLM intelligence through word guessing games. DEEP uses aggressive and conservative description modes to test LLMs' expression and disguising abilities, with GPT-4 judging the accuracy of descriptions against target and distractor words. SpyGame is an interactive multi-agent framework that simulates real-world scenarios through a "Who is Spy" game, requiring LLMs to demonstrate linguistic skills and strategic thinking. Both frameworks incorporate bias mitigation strategies, particularly in SpyGame where name, speaking order, and option order biases are addressed through randomization and content-free experiments.

## Key Results
- DEEP and SpyGame frameworks successfully distinguish between open-source and closed-source LLMs
- GPT-4 performs best in both accuracy and disguise capabilities across all evaluation metrics
- Bias mitigation strategies in SpyGame effectively reduce systematic influences on evaluation outcomes
- LLMs demonstrate varying degrees of strategic thinking and theory of mind capabilities in multi-agent interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DEEP evaluates LLMs' ability to generate both informative and deliberately ambiguous descriptions.
- Mechanism: The framework uses two distinct prompting modes—aggressive (clear, comprehensive description) and conservative (ambiguous, disguised description). GPT-4 judges the accuracy of these descriptions against target and distractor words, creating a two-dimensional evaluation space.
- Core assumption: GPT-4 can reliably assess the semantic match between generated descriptions and target words.
- Evidence anchors:
  - [abstract]: "DEEP requires LLM to describe a word in aggressive and conservative modes"
  - [section 2.1]: "We employ the chain-of-thought (CoT) prompting for the LLM to perform the conservative description"
  - [corpus]: Weak—no direct corpus evidence that GPT-4 is consistently reliable as an evaluator across diverse domains
- Break condition: If GPT-4's assessment is inconsistent with human judgment or exhibits bias toward certain word types.

### Mechanism 2
- Claim: SpyGame creates a realistic multi-agent environment to evaluate strategic reasoning and theory of mind.
- Mechanism: Agents play "Who is Spy" with roles (spy/villager) and keywords. Through speaking, reasoning, and voting actions, the framework tests LLMs' ability to understand context, infer others' intentions, and strategically communicate.
- Core assumption: The game mechanics create meaningful differentiation between LLMs' strategic and reasoning capabilities.
- Evidence anchors:
  - [abstract]: "SpyGame requires the target LLM to possess linguistic skills and strategic thinking"
  - [section 3.2]: "Reasoning In the real-world game playing scenario, human participants infer the identities of their counterparts by scrutinizing verbal and non-verbal cues"
  - [corpus]: Weak—no corpus evidence that the specific game mechanics are optimal for evaluating theory of mind
- Break condition: If the game mechanics fail to create meaningful strategic differentiation or if biases dominate outcomes.

### Mechanism 3
- Claim: Randomization and bias mitigation make SpyGame evaluations fair and robust.
- Mechanism: The framework addresses name, speaking order, and option order biases through randomization and content-free experiments to isolate bias effects.
- Core assumption: Randomization effectively neutralizes systematic biases in LLM decision-making.
- Evidence anchors:
  - [section 3.3]: "we observe the bias issue in SpyGame and identify three main bias issues, i.e., name, speaking order, and option order bias"
  - [section 3.3]: "we randomize the speaking order in SpyGame to ensure that agents consider all players' responses equally"
  - [corpus]: Weak—no corpus evidence that the proposed randomization strategies fully eliminate bias
- Break condition: If remaining biases still significantly influence evaluation outcomes despite randomization.

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: The framework evaluates LLMs' ability to understand others' mental states and intentions, which is central to both game mechanics and strategic reasoning.
  - Quick check question: What's the difference between first-order and second-order ToM, and how does SpyGame test each?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT is used in conservative mode prompting to help LLMs generate ambiguous descriptions by first identifying conceptually similar words.
  - Quick check question: How does CoT prompting help LLMs generate more effective disguised descriptions?

- Concept: Multi-Agent System Bias
  - Why needed here: Understanding various bias types (name, order, option) is crucial for designing fair evaluations and interpreting results correctly.
  - Quick check question: What are the three main bias types identified in SpyGame, and how does each affect evaluation outcomes?

## Architecture Onboarding

- Component map: DEEP consists of prompting engine (aggressive/conservative modes) -> GPT-4 judge -> evaluation metrics. SpyGame consists of game environment -> host agents (GPT-3.5-Turbo) -> guest agent -> action modules (word guessing, speaking, reasoning, voting) -> victory conditions.
- Critical path: For DEEP: prompt generation -> description generation -> GPT-4 evaluation -> metric calculation. For SpyGame: game initialization -> speaking phase -> reasoning phase -> voting phase -> outcome determination.
- Design tradeoffs: DEEP prioritizes simplicity and automation but relies heavily on GPT-4 as judge. SpyGame prioritizes comprehensive evaluation but requires more complex multi-agent coordination and bias mitigation.
- Failure signatures: DEEP failures show as inconsistent scores between target and distractor words or poor correlation with human evaluation. SpyGame failures manifest as high bias scores, low variance between different LLMs, or failure to distinguish between spy and villager strategies.
- First 3 experiments:
  1. Test DEEP with a small set of common words (cat, dog, apple) across all models to verify basic functionality
  2. Run SpyGame with a single keyword pair and observe agent interactions to validate game mechanics
  3. Perform bias analysis by running content-free experiments (agents outputting only "...") to measure baseline biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in word guessing games translate to their performance in real-world applications that require similar skills?
- Basis in paper: [inferred] The paper discusses the use of word guessing games to assess LLMs' intelligence, focusing on their ability to describe words accurately and disguise descriptions.
- Why unresolved: The paper does not explore the correlation between game performance and real-world task performance.
- What evidence would resolve it: Conducting experiments where LLMs trained or evaluated using word guessing games are then tested on real-world tasks that require similar skills, such as chatbots or virtual assistants.

### Open Question 2
- Question: Can the DEEP and SpyGame frameworks be effectively adapted for languages other than English and Chinese?
- Basis in paper: [explicit] The paper mentions that the keyword set used in experiments covers both Chinese and English languages.
- Why unresolved: The paper does not discuss the adaptability of the frameworks to other languages.
- What evidence would resolve it: Implementing the DEEP and SpyGame frameworks for a diverse set of languages and evaluating their performance across these languages.

### Open Question 3
- Question: How do the biases identified in the SpyGame framework (name bias, speaking order bias, and option order bias) affect the evaluation results, and what are the best strategies to mitigate these biases?
- Basis in paper: [explicit] The paper identifies three main bias issues in the SpyGame framework and proposes strategies to mitigate them.
- Why unresolved: The paper does not provide a comprehensive analysis of how these biases impact the evaluation results or the effectiveness of the proposed mitigation strategies.
- What evidence would resolve it: Conducting experiments with and without the proposed mitigation strategies and comparing the evaluation results to assess the impact of biases and the effectiveness of the strategies.

## Limitations
- Heavy reliance on GPT-4 as both participant and judge creates potential circular validation issues
- Complete elimination of systematic biases in multi-agent interactions remains uncertain despite proposed mitigation strategies
- Generalizability of results across different cultural contexts and languages requires further validation

## Confidence
- High confidence: The framework's ability to distinguish between open-source and closed-source LLMs, as evidenced by consistent performance gaps across experiments
- Medium confidence: The effectiveness of bias mitigation strategies in SpyGame, pending more extensive cross-cultural validation
- Medium confidence: The correlation between game-based performance metrics and real-world intelligence measures, requiring human evaluation benchmarks

## Next Checks
1. Conduct human evaluation studies to validate GPT-4's assessments across diverse word categories and cultural contexts
2. Test the framework with additional LLM models, particularly those with different architectural approaches, to verify consistent performance patterns
3. Implement cross-linguistic validation using non-English word sets to assess the framework's language-agnostic capabilities