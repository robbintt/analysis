---
ver: rpa2
title: How susceptible are LLMs to Logical Fallacies?
arxiv_id: '2308.09853'
source_url: https://arxiv.org/abs/2308.09853
tags:
- agent
- reasoning
- logical
- debater
- persuader
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the susceptibility of large language models
  (LLMs) to logical fallacies in multi-round argumentative debates. The authors introduce
  LOGICOM, a novel benchmark that assesses LLMs' logical reasoning performance by
  contrasting debates where the persuader employs logical reasoning versus fallacious
  arguments.
---

# How susceptible are LLMs to Logical Fallacies?

## Quick Facts
- arXiv ID: 2308.09853
- Source URL: https://arxiv.org/abs/2308.09853
- Authors: 
- Reference count: 11
- Key outcome: LLMs are 41-69% more susceptible to fallacious arguments than logical ones in multi-round debates

## Executive Summary
This paper introduces LOGICOM, a benchmark that evaluates how susceptible large language models are to logical fallacies in argumentative debates. The authors find that both GPT-3.5 and GPT-4 are significantly more likely to be convinced by fallacious arguments than by logical reasoning, with GPT-4 showing the highest susceptibility at 69%. The study also introduces a new dataset containing over 5,000 pairs of logical and fallacious arguments extracted from debates on controversial topics.

## Method Summary
LOGICOM uses a multi-round debate framework where a persuader agent attempts to convince a debater agent using either logical reasoning or fallacious arguments. The system employs separate helper agents to generate either logical or fallacious responses, with moderator agents controlling debate flow and assessing whether the debater changes their opinion. The benchmark was tested with 200 controversial topics, using GPT-3.5 as the persuader and GPT-4 as the debater, with each debate limited to 10 rounds and repeated three times per claim.

## Key Results
- GPT-3.5 is 41% more likely to be convinced by fallacious arguments compared to logical reasoning
- GPT-4 is 69% more likely to be convinced by fallacious arguments compared to logical reasoning
- The debater agent shows varying susceptibility to different types of logical fallacies, with Ad Hominem being particularly effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are more likely to change their opinions when presented with fallacious arguments than with logical ones.
- Mechanism: The persuader exploits emotional appeals and false information through fallacious arguments, which the debater LLM accepts more readily than valid reasoning.
- Core assumption: The debater LLM's reasoning capability is weaker against fallacies than against valid arguments.
- Evidence anchors:
  - [abstract] "when presented with logical fallacies, GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often, respectively, compared to when logical reasoning is used."
  - [section] "Our findings indicate that GPT-3.5 and GPT-4 can adjust their opinion through reasoning. However, when presented with logical fallacies, GPT-3.5 and GPT-4 are 41% and 69% more likely, respectively, to be convinced when exposed to logical fallacies compared to when they encounter logical reasoning."
- Break condition: If the debater LLM is trained to detect or resist fallacies.

### Mechanism 2
- Claim: Using a separate fallacious helper agent prevents the persuader agent from being distracted or underperforming due to the complexity of generating fallacious arguments.
- Mechanism: The persuader focuses on debating while the helper specializes in crafting fallacious arguments, maintaining persuader performance.
- Core assumption: The persuader agent's performance is negatively impacted by the complexity of generating fallacious arguments.
- Evidence anchors:
  - [section] "To prevent the effects of lengthy prompts on the agent's performance, we've separated crafting fallacious arguments from the persuader agent's tasks."
- Break condition: If the helper agent's fallacious arguments are not effective.

### Mechanism 3
- Claim: Moderator agents ensure the debate remains focused and that the debater's final stance is accurately assessed.
- Mechanism: Three subordinate moderators handle specific tasks: checking if the debater is convinced, maintaining focus on the topic, and preventing pleasantries loops.
- Core assumption: Without moderation, the debate could become unfocused or the debater might not be accurately assessed.
- Evidence anchors:
  - [section] "The discussion flow is controlled by a master moderator LLM agent and three subordinate moderators."
- Break condition: If moderator agents make incorrect assessments or fail to maintain focus.

## Foundational Learning

- Concept: Logical fallacies
  - Why needed here: The benchmark aims to assess LLMs' susceptibility to logical fallacies, so understanding what constitutes a logical fallacy is crucial.
  - Quick check question: Can you define and provide examples of common logical fallacies such as ad hominem, appeal to emotion, and false dilemma?

- Concept: Multi-round debates
  - Why needed here: The benchmark uses a multi-round debate format to assess the debater LLM's ability to change its opinion through reasoning over time.
  - Quick check question: How might a debater's opinion change over multiple rounds of debate, and what factors could influence this change?

- Concept: LLM prompting and fine-tuning
  - Why needed here: The experiment uses carefully crafted prompts to guide the behavior of the LLM agents.
  - Quick check question: How might the specific wording and structure of a prompt influence an LLM's response?

## Architecture Onboarding

- Component map:
  - Persuader agent -> Fallacious/Logical helper agent -> Debater agent
  - Moderator agents (3 subordinates + 1 master) -> Debate control and assessment
  - Memory agent -> Debate history management

- Critical path:
  1. Persuader agent initiates the debate with its claim and reasons
  2. Debater agent responds, stating its initial stance
  3. Persuader agent, with helper assistance, generates its next argument
  4. Memory agent manages the debate history and passes it to the debater agent
  5. Moderator agents assess the debater's stance and decide whether to continue
  6. Steps 3-5 repeat for up to 10 rounds or until moderator terminates
  7. Moderator assesses the debater's final stance

- Design tradeoffs:
  - Using separate agents for persuasion and fallacy generation vs. a single agent handling both tasks
  - Limiting the debate to 10 rounds vs. allowing longer debates
  - Repeating the experiment 3 times per claim vs. more repetitions for increased statistical significance

- Failure signatures:
  - Debater agent consistently agrees or disagrees with all claims, regardless of arguments
  - Persuader agent fails to generate coherent or relevant arguments
  - Moderator agents incorrectly assess the debater's stance

- First 3 experiments:
  1. Run the benchmark with a simple claim and a known logical fallacy, and verify that the debater agent is more likely to be convinced by the fallacy than by a logical argument.
  2. Run the ablation study with the logical helper agent and verify that the persuader's persuasiveness does not significantly increase compared to using the fallacious helper agent.
  3. Run the benchmark with a debater agent that has been fine-tuned to resist fallacies, and verify that it is less susceptible to fallacious arguments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the susceptibility of LLMs to logical fallacies vary significantly across different types of fallacies (e.g., Ad Hominem vs. appeal to emotion)?
- Basis in paper: [explicit] The paper notes that "the debater agent is more susceptible to certain types of logical fallacies, such as Ad Hominem, which requires further investigation in future."
- Why unresolved: The paper acknowledges this as an area for future research but does not provide detailed analysis of susceptibility variations across different fallacy types.
- What evidence would resolve it: Comparative analysis of debater agent susceptibility rates across multiple categories of logical fallacies with statistical significance testing.

### Open Question 2
- Question: How does the performance of LLMs in multi-round debates compare to human debaters when facing fallacious arguments?
- Basis in paper: [inferred] The paper focuses on LLM-to-LLM debates but notes that "a potentially more efficient method would involve employing humans as the persuader and helper agents."
- Why unresolved: The experiments use only LLM agents; no comparison with human debaters is conducted.
- What evidence would resolve it: Head-to-head debate experiments with human debaters versus LLM debaters facing identical fallacious arguments.

### Open Question 3
- Question: What is the long-term consistency of LLM opinion changes after exposure to fallacious arguments?
- Basis in paper: [inferred] The paper mentions that "GPT-3.5 and GPT-4 are 41% and 69% more likely to be convinced, respectively, compared to being persuaded by non-fallacious arguments" but does not track changes over time.
- Why unresolved: The experiments only examine immediate responses in single debate sessions without longitudinal follow-up.
- What evidence would resolve it: Repeated exposure experiments measuring how LLM opinions evolve over multiple interactions with the same fallacious arguments over time.

## Limitations
- The moderator agent system may introduce biases in determining when a debater is "convinced"
- The 10-round limit may not capture longer-term persuasion dynamics
- The specific prompts used may significantly influence debate outcomes

## Confidence
- High confidence: LLMs show measurably higher susceptibility to fallacies than logical reasoning in controlled debates
- Medium confidence: Task separation between persuader and helper agents improves fallacy generation
- Low confidence: Moderator agents reliably detect persuasion and maintain debate quality

## Next Checks
1. Test debater agents with explicit fallacy-detection fine-tuning to measure resistance to fallacious arguments
2. Conduct ablation studies removing moderator agents to assess their impact on persuasion outcomes
3. Extend debate length beyond 10 rounds to evaluate whether persuasion effects strengthen or weaken over time