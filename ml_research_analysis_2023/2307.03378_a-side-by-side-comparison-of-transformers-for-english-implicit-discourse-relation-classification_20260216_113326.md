---
ver: rpa2
title: A Side-by-side Comparison of Transformers for English Implicit Discourse Relation
  Classification
arxiv_id: '2307.03378'
source_url: https://arxiv.org/abs/2307.03378
tags:
- discourse
- relation
- language
- implicit
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates seven pre-trained language models on the PDTB-3
  dataset for implicit discourse relation classification, achieving a new state-of-the-art
  accuracy of 0.671. The study finds that models using masked language modeling with
  full attention (e.g., RoBERTa, DeBERTa) outperform those with sentence-level pre-training
  objectives.
---

# A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification

## Quick Facts
- arXiv ID: 2307.03378
- Source URL: https://arxiv.org/abs/2307.03378
- Reference count: 22
- Key outcome: New SOTA accuracy of 0.671 on PDTB-3 implicit discourse relation classification

## Executive Summary
This study evaluates seven pre-trained language models on the PDTB-3 dataset for implicit discourse relation classification, establishing a new state-of-the-art accuracy of 0.671. The research systematically compares models with different pre-training objectives (MLM vs. sentence-level) and attention mechanisms (full vs. sparse), revealing that MLM with full attention consistently outperforms alternatives. Surprisingly, using full sentences as input rather than annotated argument spans decreases performance, suggesting that argument spans provide more focused and effective input for this task. The study also finds that long-document modifications, while beneficial for other NLP tasks, negatively impact performance on implicit discourse relation classification.

## Method Summary
The study fine-tunes seven pre-trained language models (ALBERT-large, BART-large, BigBird-RoBERTa-large, DeBERTa-large, Longformer-large, RoBERTa-large, and SpanBERT-large) on the PDTB-3 dataset using 12-fold cross-validation with sliding window. Models are fine-tuned for 10 epochs with AdamW optimizer, learning rate 5e-6 (2e-6 for DeBERTa and RoBERTa), batch size 8, and max input length 256. Input consists of concatenated argument spans (Arg1 and Arg2) or full sentences, with evaluation based on 14-way discourse sense classification accuracy. All experiments are conducted using HuggingFace transformers, and results are averaged across folds.

## Key Results
- Masked language modeling with full attention (RoBERTa, DeBERTa) outperforms sentence-level pre-training objectives
- Long-document modifications (sparse/block attention) negatively impact performance
- Using full sentences instead of annotated argument spans decreases accuracy, contrary to expectations
- Achieved new state-of-the-art accuracy of 0.671 on PDTB-3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked language modeling with full attention consistently outperforms sentence-level pre-training objectives for implicit discourse relation classification.
- Mechanism: Full attention captures fine-grained token-level relationships within short argument spans, while MLM forces models to learn contextual dependencies without losing low-level semantic cues.
- Core assumption: Implicit discourse relations rely on dense, subtle cues rather than explicit sentence-level structure.
- Evidence anchors: Abstract states "Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance"; Section 3.1 shows sentence-level objectives performed worse than MLM-only models of similar size.

### Mechanism 2
- Claim: Long-document modifications that alter attention mechanisms degrade performance on implicit discourse relation classification.
- Mechanism: Sparse or block attention schemes reduce the model's ability to attend to all tokens within short argument spans, leading to loss of crucial low-level semantic information.
- Core assumption: Implicit discourse relations are signaled by local, dense token interactions rather than long-range dependencies.
- Evidence anchors: Section 3.1 notes that both BigBird-RoBERTa-large and Longformer-large performed worse than RoBERTa-large; abstract states "long-document modifications negatively impact performance."

### Mechanism 3
- Claim: Using annotated argument spans as input yields better performance than using full sentences for implicit discourse relation classification.
- Mechanism: Argument spans provide focused context that excludes extraneous tokens, allowing the model to concentrate on the most informative discourse cues.
- Core assumption: Not all tokens in a sentence contribute equally to discourse relation inference; annotated spans are curated to maximize relevant information.
- Evidence anchors: Section 3.1 states "Opposed to our postulation, using full sentence(s) as input decreased performance on the test set"; abstract notes this suggests argument spans provide more focused and effective input.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper compares models with different attention schemes (full, sparse, global+window), so understanding how attention operates is essential to interpret performance differences.
  - Quick check question: What is the difference between full attention and sparse attention in transformer models, and how might this affect the model's ability to capture discourse relations?

- Concept: Pre-training objectives (MLM, NSP, SBO, etc.)
  - Why needed here: The study contrasts models with different pre-training objectives to determine their impact on discourse relation classification, requiring knowledge of what each objective optimizes for.
  - Quick check question: How does masked language modeling (MLM) differ from next sentence prediction (NSP) in terms of the linguistic knowledge they encourage models to learn?

- Concept: Implicit vs. explicit discourse relations
  - Why needed here: The task focuses on implicit discourse relations, where connectives are absent, making it crucial to understand why this is more challenging than explicit relation classification.
  - Quick check question: Why is classifying implicit discourse relations more difficult than explicit ones, and what kind of linguistic cues must the model rely on in the absence of connectives?

## Architecture Onboarding

- Component map: Input (Arg1+Arg2 spans or full sentences) -> Encoder (PLM: RoBERTa, DeBERTa, etc.) -> Output (14-way discourse sense classification) -> Special tokens ([CLS], [SEP], [EOS] for segmentation)

- Critical path: Load pre-trained model from Hugging Face repository → Prepare input by concatenating argument spans with special tokens → Fine-tune on PDTB-3 using cross-validation → Evaluate using 14-way classification accuracy

- Design tradeoffs:
  - Full attention vs. sparse attention: Full attention captures all token interactions but is computationally expensive; sparse attention scales to longer documents but may miss crucial local cues.
  - Argument spans vs. full sentences: Spans are focused but require accurate span identification; full sentences provide context but include noise.
  - Pre-training objectives: MLM-only models perform better here, but sentence-level objectives might help in other discourse tasks.

- Failure signatures:
  - Low accuracy on development set: Likely hyperparameter issues (learning rate, batch size) or insufficient training epochs.
  - Large variance across folds: Possible data leakage or inconsistent preprocessing between folds.
  - Performance drop with long-document models: Expected — these models sacrifice local attention for scalability.

- First 3 experiments:
  1. Fine-tune RoBERTalarge on argument spans with learning rate 5e-6, batch size 8, max length 256; observe baseline performance.
  2. Replace RoBERTalarge with DeBERTalarge (same hyperparameters) to test if disentangled attention improves results.
  3. Switch input from argument spans to full sentences while keeping DeBERTalarge to verify the span advantage empirically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does using full sentence(s) instead of argument spans generally decrease performance in implicit discourse relation classification?
- Basis in paper: The paper found that using full sentence(s) as input decreased performance on the test set compared to using argument spans.
- Why unresolved: While the authors hypothesized that textual cues might be spread throughout sentences, they did not conduct a detailed analysis of why this approach is less effective.
- What evidence would resolve it: A detailed error analysis comparing model predictions on full sentences versus argument spans, along with ablation studies on different types of discourse relations and sentence lengths.

### Open Question 2
- Question: Why do long-document modifications (sparse or block attention mechanisms) decrease the original model performance for implicit discourse relation classification?
- Basis in paper: The authors observed that models with sparse or block attention mechanisms (BigBird-RoBERTalarge and Longformerlarge) performed worse than the original RoBERTalarge model.
- Why unresolved: The authors only speculated that long-document modifications might be a drawback for this task, but did not investigate the underlying reasons.
- What evidence would resolve it: An in-depth analysis of the attention patterns in these models, along with experiments on different types of discourse relations and argument span lengths.

### Open Question 3
- Question: Why are sentence-level pre-training objectives (NSP, SBO, SOP) generally not necessary for creating the best-performing models for implicit discourse relation classification?
- Basis in paper: The authors found that models with sentence-level objectives (ALBERT, SpanBERT) performed worse than MLM-only models (RoBERTa, DeBERTa) with similar model sizes.
- Why unresolved: While the authors argued that MLM with full attention is better suited for this task, they did not explore the specific reasons why sentence-level objectives are less effective.
- What evidence would resolve it: A comprehensive analysis of the learned representations in models with and without sentence-level objectives, along with experiments on different types of discourse relations and pre-training datasets.

## Limitations

- Generalizability Across Languages and Domains: Findings are based exclusively on the English PDTB-3 corpus, leaving questions about applicability to other languages and domains.
- Model Architecture Specificity: All tested models are based on BERT-large architecture, potentially limiting generalizability to other architectures like T5, GPT-style models, or newer approaches.
- Hyperparameter Sensitivity: Fixed hyperparameters across all models may not be optimal for each architecture, potentially affecting relative performance comparisons.

## Confidence

- High Confidence: MLM with full attention outperforms sentence-level pre-training objectives for implicit discourse relation classification within tested model families.
- Medium Confidence: Long-document modifications negatively impact performance on implicit discourse relation classification.
- Medium Confidence: Using argument spans outperforms full sentences as input for this task.
- Low Confidence: These findings represent universal principles for discourse relation classification across all languages and domains.

## Next Checks

**Validation Check 1**: Test the same seven models on a non-English discourse corpus (such as Chinese CDTB or German TIGER-Discourse) using identical hyperparameters to assess cross-linguistic generalizability of the findings.

**Validation Check 2**: Implement and evaluate a broader range of architectures including encoder-decoder models (T5, BART), decoder-only models (GPT-style), and newer architectures with different attention mechanisms (RWKV, Mamba) on the same PDTB-3 task.

**Validation Check 3**: Conduct an ablation study varying the input length and composition systematically (argument spans of different granularities, full sentences with varying context windows, and combinations thereof) across all seven models.