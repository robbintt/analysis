---
ver: rpa2
title: Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder's
  Perspective
arxiv_id: '2312.01398'
source_url: https://arxiv.org/abs/2312.01398
tags:
- unfair
- sentences
- contracts
- stakeholders
- contract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores unfairness in commercial contracts from the
  perspective of non-legal stakeholders. Through an empirical study, the authors identify
  scenarios of unfairness and propose a novel ternary classification task to automatically
  detect fair, potentially unfair, and clearly unfair sentences in contracts.
---

# Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder's Perspective

## Quick Facts
- arXiv ID: 2312.01398
- Source URL: https://arxiv.org/abs/2312.01398
- Reference count: 21
- Primary result: Self-trained BERT achieves 84% accuracy on contract fairness classification, outperforming few-shot prompting by 9%

## Executive Summary
This paper addresses the challenge of detecting perceived unfairness in commercial contracts from the perspective of non-legal stakeholders. The authors introduce a novel ternary classification task to distinguish between fair, potentially unfair, and clearly unfair sentences in contracts. Through empirical experiments using data augmentation, self-training, and chain-of-thought prompting, they demonstrate that self-trained BERT models outperform few-shot prompting approaches. The work highlights the importance of considering non-legal stakeholder perspectives in contract fairness and provides a promising approach for automating the detection of potentially unfair clauses.

## Method Summary
The approach combines BERT-based fine-tuning with semi-supervised self-training and data augmentation to classify contract sentences. The method involves extracting sentences from proprietary HTML contracts, manual labeling by legal experts, generating additional minority class examples using ChatGPT, and iteratively training BERT models with pseudo-labels from unlabeled data. The classification task is evaluated against few-shot Chain-of-Thought prompting using large language models, with experiments measuring accuracy and macro F1 score across multiple iterations.

## Key Results
- Self-trained BERT achieves 84% accuracy on the contract fairness classification task
- Data augmentation addresses class imbalance by generating synthetic examples for the minority "clearly unfair" class
- Chain-of-Thought prompting with LLMs provides competitive but lower performance than self-training
- The approach effectively identifies potentially unfair clauses that non-legal stakeholders may perceive as problematic

## Why This Works (Mechanism)

### Mechanism 1: Self-training with pseudo-labels
- Claim: Self-training improves classification accuracy by iteratively leveraging pseudo-labels from unlabeled data
- Mechanism: Teacher model predicts on unlabeled data, confident predictions become pseudo-labels, student model trained on augmented dataset, repeat until convergence
- Core assumption: Confidence correlates with label accuracy and unlabeled data contains useful patterns
- Evidence anchors: [abstract] "Using BERT-based fine-tuning, we achieved an accuracy of 84%"; [section] "We performed a set of experiments to examine the ability of PLMs"
- Break condition: Confidence thresholding removes too many examples or introduces noisy pseudo-labels

### Mechanism 2: Data augmentation for class imbalance
- Claim: Synthetic generation addresses minority class scarcity in "clearly unfair" examples
- Mechanism: ChatGPT generates sentences for minority class based on few-shot prompts, annotators verify and add to training data
- Core assumption: Generated examples are representative and maintain label consistency
- Evidence anchors: [abstract] "We introduce a novel classification task tailored specifically for commercial contracts"; [section] "We performed data augmentation of the training set"
- Break condition: Synthetic examples fail to capture real unfairness patterns or introduce noise

### Mechanism 3: Chain-of-Thought prompting for fairness reasoning
- Claim: LLMs can reason about fairness through multi-step analysis when given explicit instructions
- Mechanism: Prompts guide LLMs through analyzing rights/obligations, detecting imbalances, and identifying ambiguous terms before final classification
- Core assumption: LLMs can follow sequential reasoning when explicitly instructed
- Evidence anchors: [abstract] "We investigate the ability of PLMs to identify unfairness in contractual sentences by comparing Chain of Thought prompting"; [section] "The prompting strategy is fundamentally anchored on the principle of giving the model time to think"
- Break condition: LLM reasoning fails to capture domain-specific fairness criteria

## Foundational Learning

- Concept: Contract fairness definitions
  - Why needed here: Classification task relies on understanding what makes clauses fair, potentially unfair, or clearly unfair from non-legal stakeholders' perspectives
  - Quick check question: Can you explain the difference between potentially unfair (ambiguous language) and clearly unfair (imbalance in rights/obligations) clauses?

- Concept: Semi-supervised learning via self-training
  - Why needed here: Approach uses unlabeled contract sentences to improve model performance beyond what's possible with limited labeled data
  - Quick check question: What happens in each iteration of the self-training loop, and when does it stop?

- Concept: Data augmentation for imbalanced classification
  - Why needed here: Dataset has very few "clearly unfair" examples, requiring synthetic generation to train balanced classifier
  - Quick check question: How do you ensure generated examples maintain label consistency and don't introduce noise?

## Architecture Onboarding

- Component map: Data pipeline → Text extraction → Sentence tokenization → Manual labeling → Data augmentation → Self-training loop → BERT classifier → Evaluation
- Critical path: Text extraction → Labeling → Data augmentation → Self-training → Classification → Evaluation
- Design tradeoffs: Manual labeling provides high-quality labels but is time-consuming; self-training reduces labeling effort but introduces pseudo-label noise
- Failure signatures: Low inter-annotator agreement indicates unclear fairness definitions; overfitting during self-training suggests pseudo-labels are too noisy
- First 3 experiments:
  1. Train vanilla BERT on labeled data only to establish baseline
  2. Apply data augmentation and retrain to measure class imbalance impact
  3. Implement self-training with confidence thresholding to evaluate iterative improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed fairness classification task perform on contract sentences from different domains beyond the proprietary dataset?
- Basis in paper: [inferred] Study used nine business domains but annotators did not observe labeling differences across domains
- Why unresolved: Dataset limited to specific domains, no testing on contracts from other domains
- What evidence would resolve it: Experiments on larger, more diverse dataset from various domains

### Open Question 2
- Question: How does the approach handle contracts written in languages other than English?
- Basis in paper: [explicit] Authors state work focuses on English contracts and does not consider other languages
- Why unresolved: Findings limited to English contracts, no exploration of cross-lingual effectiveness
- What evidence would resolve it: Testing classification task on contracts in different languages

### Open Question 3
- Question: What is the impact of incorporating project-specific and human-level attributes on classification accuracy?
- Basis in paper: [explicit] Project-specific details and human-level attributes are out of scope but planned for future work
- Why unresolved: Study does not explore influence of these attributes on classification performance
- What evidence would resolve it: Experiments incorporating project-specific and human-level attributes

## Limitations

- Proprietary dataset restricts reproducibility and external validation of findings
- Limited generalizability to contracts outside the vendor organization's domain
- Potential bias from annotator perspectives on fairness definitions

## Confidence

- Self-training mechanism: Medium confidence based on empirical results but proprietary data limitations
- Data augmentation approach: Medium confidence with concerns about synthetic example quality
- Chain-of-Thought prompting: Low confidence due to limited comparative results and weaker performance

## Next Checks

1. Test model performance on contracts from different industries to assess domain transferability
2. Conduct inter-annotator reliability analysis to quantify subjectivity in fairness labeling
3. Perform adversarial testing with intentionally crafted unfair clauses to evaluate classifier robustness