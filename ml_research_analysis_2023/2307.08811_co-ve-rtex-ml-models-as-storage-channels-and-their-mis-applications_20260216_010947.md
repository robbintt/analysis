---
ver: rpa2
title: 'Co(ve)rtex: ML Models as storage channels and their (mis-)applications'
arxiv_id: '2307.08811'
source_url: https://arxiv.org/abs/2307.08811
tags:
- data
- baseline
- accuracy
- figure
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel information-theoretic framework for
  exploiting overparameterization in ML models as a covert communication channel.
  By treating unused parameters as a storage medium, the authors demonstrate black-box
  techniques for embedding arbitrary data within trained models during the training
  phase, which can later be extracted by querying the model.
---

# Co(ve)rtex: ML Models as storage channels and their (mis-)applications

## Quick Facts
- arXiv ID: 2307.08811
- Source URL: https://arxiv.org/abs/2307.08811
- Reference count: 40
- Primary result: Introduces a framework for exploiting overparameterization in ML models as covert communication channels, achieving up to 97% accuracy in transferring data while minimally affecting baseline model performance.

## Executive Summary
This paper presents a novel information-theoretic framework that treats overparameterized machine learning models as covert storage channels by leveraging unused parameters as a medium for embedding arbitrary data during training. The authors demonstrate that by augmenting training data with patched inputs, arbitrary data can be stored within the model's parameters and later retrieved through inference queries. They introduce optimizations including dynamic sample allocation and a combinatorial error correction protocol tailored to the stochastic nature of the channel. Experimental results show successful transfer of text, random data, and images through both covert and non-covert channels across different model architectures, with accuracies reaching up to 97% and minimal baseline model degradation.

## Method Summary
The method treats ML models as storage channels by exploiting unused parameters during training. It involves generating patched inputs (single-pixel patches for MNIST, 8-location patches for CIFAR-10), augmenting training data with these samples, and training models with a 1:1 ratio of baseline to patched data. Dynamic allocation optimizes the number of samples per address to minimize baseline accuracy degradation, while combinatorial error correction improves reliability by leveraging class probability distributions from multiple reads. The approach is tested across different architectures (Lenet-5, AlexNet, ResNet50) on standard computer vision datasets.

## Key Results
- Achieved up to 97% accuracy in data transfer through covert channels
- Demonstrated successful embedding of text, random data, and images across multiple model architectures
- Showed minimal baseline model accuracy degradation (0.1-0.3%) while maintaining high channel capacity
- Validated the lottery ticket hypothesis as justification for exploiting unused parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ML model can be treated as a storage channel by leveraging unused parameters (UPs) that do not affect the primary task.
- Mechanism: During training, patched inputs are used to store arbitrary data by forcing the model to learn mappings from these patterns to specific output labels. Later, querying with the same patterns retrieves the stored data.
- Core assumption: The winning ticket hypothesis holds—pruning reveals subnetworks sufficient for the primary task, leaving other parameters as don't-care states.
- Evidence anchors:
  - [abstract] "By treating unused parameters as a storage medium, the authors demonstrate black-box techniques for embedding arbitrary data within trained models during the training phase, which can later be extracted by querying the model."
  - [section] "The state of these parameters is essentially a 'don't-care' with respect to the primary model provided that this state does not interfere with the primary model."
- Break condition: If the model is fine-tuned or pruned post-deployment, the stored data may be lost or corrupted.

### Mechanism 2
- Claim: Dynamic allocation of training samples per address improves channel efficiency compared to static allocation.
- Mechanism: Initially train with a small number of patched samples per address, then incrementally add samples only for addresses that fail to store their values correctly.
- Core assumption: Different addresses require different numbers of samples to achieve reliable storage; some patterns are easier to embed than others.
- Evidence anchors:
  - [abstract] "Experimental results show successful transfer of text, random data, and images... with accuracies reaching up to 97% and minimal baseline model degradation."
  - [section] "We discovered that the number of samples needed for each address increases with the size of the message; as we demand more of the network, it needs more examples to learn the stored value."
- Break condition: If the upper threshold for sample addition is reached or training accuracy plateaus, the optimization stops and remaining errors persist.

### Mechanism 3
- Claim: A combinatorial error correction protocol tailored to ML models outperforms conventional error correction.
- Mechanism: After multiple reads per address, use the frequency distribution of outputs to substitute the most likely classes until checksums match, leveraging the model's probabilistic output structure.
- Core assumption: The model's output probabilities are stable across repeated queries, and the correct class is most likely to appear frequently.
- Evidence anchors:
  - [abstract] "Experimental results show successful transfer... and a novel combinatorial error correction protocol tailored to the stochastic nature of the channel."
  - [section] "To further improve the capacity, we develop a novel error correction code that takes advantage of the nature of the model by multiple read operations and with this extra information it outperforms the optimal Reed Solomon error correction method in a highly noisy channel."
- Break condition: If aliasing occurs (checksum matches but data is incorrect) or the permutation search exceeds computational limits, correction fails.

## Foundational Learning

- Concept: Lottery Ticket Hypothesis (LTH)
  - Why needed here: LTH justifies treating pruned parameters as unused and available for covert storage without harming the primary task.
  - Quick check question: If a pruned subnetwork can achieve the same accuracy as the full network, what does that imply about the remaining parameters?

- Concept: Shannon's Channel Capacity
  - Why needed here: Provides the theoretical upper bound on how much data can be embedded based on the number of unused parameters.
  - Quick check question: How does increasing the number of unused parameters affect the maximum achievable capacity of the storage channel?

- Concept: Error Correction Codes (ECC)
  - Why needed here: Ensures reliable data retrieval in the presence of noise inherent in the ML model's stochastic outputs.
  - Quick check question: Why might a combinatorial ECC based on class likelihoods outperform Reed-Solomon coding in this context?

## Architecture Onboarding

- Component map: Training pipeline -> Address space generator -> Dynamic sample allocator -> Error correction module -> Query interface
- Critical path: 1. Generate patched inputs → 2. Augment training data → 3. Train model with dynamic allocation → 4. Deploy model → 5. Query with patched patterns → 6. Apply combinatorial ECC → 7. Retrieve data
- Design tradeoffs:
  - Higher capacity vs. baseline accuracy degradation
  - Covertness vs. channel reliability
  - Computational overhead of dynamic allocation and ECC
- Failure signatures:
  - Baseline accuracy drops unexpectedly (model capacity exceeded)
  - Stored data retrieval accuracy low (insufficient training samples)
  - ECC fails frequently (aliasing or permutation limit reached)
- First 3 experiments:
  1. Test static DeepMem on MNIST with Lenet-5; measure baseline vs. patch accuracy.
  2. Implement dynamic allocation; compare sample efficiency and accuracy to static.
  3. Add combinatorial ECC; evaluate correction performance against Reed-Solomon under controlled noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the achievable capacity of DeepMem scale with model size beyond the tested architectures, particularly for trillion-parameter models like GPT-4?
- Basis in paper: [explicit] The paper mentions that overparameterization capacity increases with model size and references GPT-4's rumored trillion parameters, but only tests relatively small models (LeNet-5, ResNet50, AlexNet)
- Why unresolved: The paper's empirical results are limited to models with parameters ranging from 61K to 23.5M, which is several orders of magnitude smaller than state-of-the-art models
- What evidence would resolve it: Experimental results showing DeepMem performance across a wider range of model sizes, particularly testing on models with hundreds of billions to trillions of parameters

### Open Question 2
- Question: What is the practical limit of covertness in DeepMem-C, and can detection rates be reduced below 1% while maintaining reasonable channel capacity?
- Basis in paper: [explicit] The paper discusses detection methods including Local Outlier Factor and cosine similarity, showing that covert DeepMem still has detectable patterns in both input and feature spaces
- Why unresolved: The paper demonstrates that covert patterns are harder to detect than non-covert ones, but doesn't establish the fundamental limits of how covert the channel can be while remaining functional
- What evidence would resolve it: Systematic evaluation of detection accuracy across varying levels of covertness, identifying the trade-off curve between detection probability and channel capacity

### Open Question 3
- Question: How effective are current mitigation strategies (pruning and fine-tuning) against advanced DeepMem variants that use more sophisticated encoding schemes or distributed training?
- Basis in paper: [explicit] The paper tests basic pruning and fine-tuning defenses against current DeepMem implementations, but acknowledges these could be circumvented with distributed training or more advanced encoding
- Why unresolved: The evaluated defenses show promise but are tested against relatively simple attack variants; the paper suggests more sophisticated attacks could bypass these defenses
- What evidence would resolve it: Comparative evaluation of mitigation strategies against DeepMem variants using distributed training, advanced encoding, and other proposed countermeasures

## Limitations
- Limited to standard computer vision datasets (MNIST, CIFAR-10, CelebA) and CNN architectures
- Channel capacity and reliability heavily depend on the winning ticket hypothesis and specific pruning patterns
- Covert channel detectability depends on implementation details with limited evaluation methods

## Confidence
- High confidence: The theoretical framework treating ML models as storage channels is sound and well-supported by the lottery ticket hypothesis.
- Medium confidence: The dynamic sample allocation optimization shows improved efficiency in experiments, but generalizability across different model architectures needs further validation.
- Low confidence: The combinatorial error correction protocol's superiority over conventional methods relies on specific assumptions about class probability distributions that may not hold universally.

## Next Checks
1. Test the covert channel mechanism on transformer-based models (BERT, ViT) and NLP datasets to assess cross-architecture applicability.
2. Implement and evaluate additional detection methods (adversarial detection, statistical fingerprinting) to comprehensively assess the covert channel's stealth capabilities.
3. Systematically vary key parameters (patch size, allocation ratio, training epochs) to map the operational boundaries and failure modes of the storage channel.