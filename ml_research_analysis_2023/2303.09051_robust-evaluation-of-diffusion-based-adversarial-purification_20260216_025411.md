---
ver: rpa2
title: Robust Evaluation of Diffusion-Based Adversarial Purification
arxiv_id: '2303.09051'
source_url: https://arxiv.org/abs/2303.09051
tags:
- steps
- puri
- cation
- robust
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the evaluation of diffusion-based adversarial
  purification methods, which aim to remove adversarial effects from input data at
  test time. The authors question whether current evaluation practices, particularly
  the use of white-box attacks like AutoAttack, are appropriate for measuring the
  robustness of these purification methods.
---

# Robust Evaluation of Diffusion-Based Adversarial Purification

## Quick Facts
- arXiv ID: 2303.09051
- Source URL: https://arxiv.org/abs/2303.09051
- Reference count: 40
- Key outcome: This work addresses the evaluation of diffusion-based adversarial purification methods, questioning whether current evaluation practices are appropriate for measuring robustness of these methods.

## Executive Summary
This paper critically examines how diffusion-based adversarial purification methods are evaluated, finding that current white-box attack approaches like AutoAttack may not adequately assess these defenses. The authors demonstrate that using direct backpropagation or surrogate processes for gradient computation provides more reliable evaluation than the commonly used adjoint method. They propose PGD+EOT as a more effective attack method for these defenses and analyze hyperparameter impacts on purification robustness. Their gradual noise-scheduling strategy for multi-step purification achieves competitive results compared to state-of-the-art adversarial training approaches on CIFAR-10, ImageNet, and SVHN datasets.

## Method Summary
The authors propose a framework for evaluating diffusion-based adversarial purification methods that involves computing gradients through the full defense process using either direct backpropagation or a surrogate process. They introduce a gradual noise-scheduling strategy for multi-step purification where forward noise addition and denoising steps are varied across purification iterations. The evaluation uses PGD+EOT attacks to handle the stochasticity and iterative nature of the purification process, comparing robust accuracy against traditional AutoAttack and BPDA methods. Experiments are conducted on CIFAR-10, ImageNet, and SVHN datasets using pretrained diffusion models and classifiers.

## Key Results
- PGD+EOT achieves 12.35% and 22.26% higher attack success rates than AutoAttack against DiffPure and ADP respectively
- Direct backpropagation yields 26.55% lower robust accuracy than adjoint method, making it a more reliable evaluation approach
- Gradual noise-scheduling strategy with multi-step purification achieves competitive results compared to state-of-the-art adversarial training
- BPDA shows lower attack success rates than attacks using direct or surrogate gradients for diffusion-based purification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct backpropagation of the full defense process yields lower robust accuracy than the adjoint method, making it a more reliable evaluation method.
- **Mechanism:** When evaluating diffusion-based purification defenses, the adjoint method approximates gradients via a numerical solver, introducing potential inaccuracies. Direct backpropagation computes exact gradients through the entire denoising pipeline, including multiple denoising steps, which better exposes vulnerabilities to white-box attacks.
- **Core assumption:** Memory constraints are manageable or a surrogate process can approximate the defense well enough for gradient computation.
- **Evidence anchors:**
  - [abstract] "We find that the adjoint method... may not be the most effective approach. Instead, we propose using direct back-propagation..."
  - [section] "Table 1 shows that the robust accuracy of DiffPure with the direct back-propagation is 51.25%... which is 26.55% lower than the reported accuracy with the adjoint method."
- **Break condition:** If memory constraints prevent backpropagation through the full denoising steps and the surrogate process fails to adequately approximate gradients, the evaluation might revert to less accurate methods like BPDA or adjoint.

### Mechanism 2
- **Claim:** PGD+EOT is more effective than AutoAttack for evaluating diffusion-based purification defenses.
- **Mechanism:** AutoAttack is designed for adversarial training scenarios and may not effectively exploit the iterative, multi-step nature of diffusion purification. PGD+EOT explicitly handles the stochasticity and iterative calls by optimizing over the expectation of the purification process, making it better suited to reveal weaknesses in these defenses.
- **Core assumption:** The purification process introduces sufficient stochasticity and iteration depth that standard white-box attacks like AutoAttack fail to fully exploit.
- **Evidence anchors:**
  - [abstract] "Our experiments show that PGD+EOT is more effective than AutoAttack for evaluating diffusion-based purification..."
  - [section] "For the ℓ∞ threat model... PGD+EOT shows 12.35% and 22.26% more attack success rate than AutoAttack against DiffPure and ADP, respectively."
- **Break condition:** If the purification process is made deterministic or simplified such that AutoAttack can fully optimize over it, the advantage of PGD+EOT may diminish.

### Mechanism 3
- **Claim:** BPDA is less effective than gradient-based attacks using direct or surrogate gradients for diffusion-based purification.
- **Mechanism:** BPDA approximates gradients of non-differentiable functions by assuming a differentiable proxy. However, diffusion-based purification processes are differentiable, so BPDA's approximations are unnecessary and less precise than direct or surrogate gradient computations. Using exact or well-approximated gradients allows more powerful white-box attacks.
- **Core assumption:** The purification process remains differentiable or can be well-approximated by a differentiable surrogate.
- **Evidence anchors:**
  - [abstract] "we propose using direct back-propagation or a surrogate process to approximate gradients..."
  - [section] "BPDA has a lower attack success rate than the attacks using direct gradients of the defense process... Against PGD+EOT using direct gradients of defense process, ADP and GDMP show robust accuracy of 37.27% and 30.82%..."
- **Break condition:** If the purification process introduces non-differentiable operations that cannot be well-approximated by a surrogate, BPDA might regain relevance.

## Foundational Learning

- **Concept:** Diffusion models and denoising processes
  - Why needed here: The core defense mechanism relies on understanding how diffusion models gradually denoise adversarial examples; evaluating robustness requires knowledge of how forward and reverse processes interact with adversarial perturbations.
  - Quick check question: Can you explain the difference between the forward process (adding noise) and reverse denoising process in diffusion models?

- **Concept:** Adversarial attack evaluation metrics (robust accuracy, attack success rate)
  - Why needed here: The paper compares different attack methods by measuring robust accuracy; understanding these metrics is essential to interpret results and recommendations.
  - Quick check question: What is the difference between standard accuracy and robust accuracy in the context of adversarial attacks?

- **Concept:** Gradient-based attack methods (PGD, AutoAttack, BPDA, EOT)
  - Why needed here: The paper evaluates purification robustness using these attacks; understanding their mechanisms and assumptions is critical to follow the analysis and recommendations.
  - Quick check question: Why does BPDA exist, and when is it typically used in adversarial defense evaluation?

## Architecture Onboarding

- **Component map:**
  Input image → Forward noise addition (t* steps) → Denoising process (multiple steps) → Cleaned image → Classifier

- **Critical path:**
  1. Noise scheduling (t* forward steps)
  2. Denoising steps (τ schedule)
  3. Gradient computation (direct or surrogate)
  4. Attack optimization (PGD+EOT)
  5. Robust accuracy calculation

- **Design tradeoffs:**
  - Memory vs. gradient accuracy: Direct backpropagation gives exact gradients but may exceed memory limits; surrogate process approximates gradients but may be less precise.
  - Forward noise amount vs. robustness: Too little noise leaves adversarial signal; too much destroys semantic content.
  - Number of purification steps vs. efficiency: More steps improve robustness but increase computation and memory cost.

- **Failure signatures:**
  - High robust accuracy under direct gradient attacks but low under BPDA → Indicates differentiable defense correctly evaluated.
  - Low robust accuracy under BPDA but high under direct gradients → Suggests evaluation using wrong attack method.
  - No improvement with more denoising steps → Suggests purification process saturation or suboptimal noise scheduling.

- **First 3 experiments:**
  1. **Validate gradient computation method:** Compare robust accuracy using adjoint, direct backpropagation, and surrogate processes on a simple diffusion purification model to confirm mechanism 1.
  2. **Attack method comparison:** Evaluate the same purification model using PGD+EOT vs. AutoAttack to confirm mechanism 2.
  3. **Hyperparameter sensitivity:** Vary t* and number of denoising steps to observe robustness trends and validate claims in mechanism 3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of denoising model (e.g., DDPM vs DDIM) affect the robustness of diffusion-based purification methods under different threat models and datasets?
- **Basis in paper:** [explicit] The paper compares DDPM and DDIM samplers in diffusion-based purification and shows that DDIM is more robust than DDPM after a certain number of forward steps.
- **Why unresolved:** While the paper provides experimental results on CIFAR-10 and ImageNet, it does not comprehensively analyze the impact of denoising models across various threat models (e.g., ℓ∞ vs ℓ2) and datasets (e.g., SVHN). The results may vary depending on the specific characteristics of the dataset and threat model.
- **What evidence would resolve it:** Conducting a systematic study that evaluates the robustness of diffusion-based purification methods using different denoising models (e.g., DDPM, DDIM) across multiple threat models (e.g., ℓ∞, ℓ2) and datasets (e.g., CIFAR-10, ImageNet, SVHN). This study should include a detailed analysis of the trade-offs between robustness and accuracy for each combination of denoising model, threat model, and dataset.

### Open Question 2
- **Question:** What is the optimal strategy for scheduling the number of forward steps and denoising steps in multi-step diffusion-based purification to maximize robustness against adversarial attacks?
- **Basis in paper:** [explicit] The paper proposes a gradual noise-scheduling strategy for multi-step purification and shows that fewer forward steps in the initial purification steps can improve robustness.
- **Why unresolved:** While the paper provides a specific scheduling strategy and shows its effectiveness on CIFAR-10, ImageNet, and SVHN, it does not explore the full space of possible scheduling strategies or provide a theoretical justification for the proposed strategy. The optimal scheduling strategy may depend on factors such as the dataset, threat model, and underlying classifier.
- **What evidence would resolve it:** Conducting a comprehensive search over the space of possible scheduling strategies, including different combinations of forward steps and denoising steps across multiple purification steps. This search should be guided by theoretical insights into the properties of diffusion-based purification and evaluated on a wide range of datasets and threat models. The optimal strategy should be validated through extensive experimentation and compared against other state-of-the-art adversarial defense methods.

### Open Question 3
- **Question:** How does the combination of diffusion-based purification with adversarial training affect the overall robustness of the system, and what is the optimal way to integrate these two approaches?
- **Basis in paper:** [explicit] The paper evaluates the combination of diffusion-based purification with adversarial training and shows that the combination performs worse than adversarial training alone in some cases.
- **Why unresolved:** While the paper provides some experimental results on the combination of diffusion-based purification with adversarial training, it does not explore the full space of possible integration strategies or provide a theoretical analysis of the interaction between these two approaches. The optimal integration strategy may depend on factors such as the dataset, threat model, and the specific techniques used for adversarial training and diffusion-based purification.
- **What evidence would resolve it:** Conducting a systematic study that explores different integration strategies for combining diffusion-based purification with adversarial training, such as using diffusion-based purification as a pre-processing step, post-processing step, or as part of the training process itself. This study should include a theoretical analysis of the interaction between these two approaches and evaluate their performance on a wide range of datasets and threat models. The optimal integration strategy should be validated through extensive experimentation and compared against other state-of-the-art adversarial defense methods.

## Limitations
- Memory constraints for direct gradient computation through multi-step denoising processes remain a practical limitation
- Effectiveness of proposed evaluation methods depends on specific diffusion model implementations and hyperparameter choices
- Limited analysis of computational overhead and real-world deployment feasibility of multi-step purification strategies

## Confidence
- **High confidence**: The superiority of PGD+EOT over AutoAttack for evaluating diffusion-based purification (supported by specific attack success rate differences of 12.35% and 22.26% on DiffPure and ADP respectively)
- **Medium confidence**: The recommendation to use direct backpropagation over adjoint methods for gradient computation (results show significant differences in robust accuracy, but memory constraints may limit practical application)
- **Medium confidence**: The effectiveness of gradual noise-scheduling strategy for multi-step purification (achieves competitive results but depends heavily on hyperparameter tuning)

## Next Checks
1. **Memory constraint validation**: Systematically test the memory requirements of direct backpropagation through varying numbers of denoising steps (e.g., 10, 25, 50 steps) on representative models to quantify the practical limits and identify the break-even point where surrogate methods become necessary.

2. **Cross-architecture generalization**: Evaluate the proposed evaluation methodology (PGD+EOT with direct gradients) on diffusion-based purification methods built on different backbone architectures (ViT, ConvNeXt) and datasets (ImageNet-21k, CIFAR-100) to assess generalizability beyond the reported experiments.

3. **Surrogate process fidelity analysis**: Conduct ablation studies comparing the robust accuracy results from direct backpropagation versus surrogate process approximations across multiple purification methods, measuring the correlation and identifying scenarios where surrogate gradients fail to capture true vulnerabilities.