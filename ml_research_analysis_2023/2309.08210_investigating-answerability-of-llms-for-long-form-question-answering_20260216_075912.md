---
ver: rpa2
title: Investigating Answerability of LLMs for Long-Form Question Answering
arxiv_id: '2309.08210'
source_url: https://arxiv.org/abs/2309.08210
tags:
- llms
- context
- evaluation
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a scalable evaluation method for long-form
  question answering (LFQA) by generating complex questions from abstractive summaries
  of long documents. The approach aims to create challenging scenarios for large language
  models (LLMs) to test their reasoning and inference capabilities over extended contexts.
---

# Investigating Answerability of LLMs for Long-Form Question Answering

## Quick Facts
- arXiv ID: 2309.08210
- Source URL: https://arxiv.org/abs/2309.08210
- Reference count: 8
- Primary result: Questions generated from abstractive summaries require deeper reasoning, and ChatGPT significantly outperforms open-source models on long-form question answering tasks.

## Executive Summary
This study proposes a scalable evaluation method for long-form question answering (LFQA) by generating complex questions from abstractive summaries of long documents. The approach aims to create challenging scenarios for large language models (LLMs) to test their reasoning and inference capabilities over extended contexts. Experiments compare ChatGPT with open-source models like Alpaca and Llama, showing that questions generated from summaries require deeper understanding and multi-pass reasoning. Results indicate that distilled LLMs perform better than base models but struggle with longer contexts (>1024 tokens). ChatGPT significantly outperforms other models across all evaluation metrics, while open-source LLMs show decreased reliance on context but generate less accurate and relevant answers for summary-based questions.

## Method Summary
The study collects Wikipedia articles from 9 domains, filters sections â‰¥256 tokens, and uses ChatGPT to generate abstractive summaries. Follow-up questions are then generated from these summaries and original passages using ChatGPT prompts. Various LLMs (ChatGPT, Alpaca-7B/13B, Llama-7B/13B) generate answers to these questions in a zero-shot setting. Answer quality is evaluated using GPT-4 on coherence, relevance, factual consistency, and accuracy metrics. Human evaluation is also performed for validation.

## Key Results
- Questions generated from abstractive summaries require deeper reasoning and multi-pass inference compared to questions from original passages
- ChatGPT significantly outperforms open-source models across all evaluation metrics for long-form question answering
- Distilled LLMs (Alpaca) show decreased reliance on context for simple questions but struggle significantly with complex, summary-generated questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Questions generated from abstractive summaries require deeper reasoning and multi-pass inference across longer contexts.
- Mechanism: Abstractive summaries condense information, forcing LLMs to infer relationships not explicitly stated and reference dispersed information in the original document.
- Core assumption: Summaries retain enough semantic content to generate questions that necessitate cross-referencing the full document.
- Evidence anchors:
  - [abstract]: "generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts."
  - [section]: "We hypothesize that follow-up questions from these summaries would require a deeper understanding of the topics that would link different parts of the source document."

### Mechanism 2
- Claim: Distilled LLMs (Alpaca) rely less on context for questions generated directly from the original document but degrade significantly on summary-generated questions.
- Mechanism: Distilled models capture patterns from base models but may lack depth for complex inference, leading to reliance on memorized knowledge for simple questions and failure on harder, summary-based questions.
- Core assumption: Distillation preserves answer patterns for straightforward questions but not the reasoning needed for complex, inference-heavy questions.
- Evidence anchors:
  - [abstract]: "open-source LLMs exhibit decreased reliance on context for generated questions from the original document, but their generation capabilities drop significantly on generated questions from document summaries."
  - [section]: "Distilled LLMs (Alpaca-7B, 13B) tend to rely less on context for generated questions from the original document, but their generation capabilities drop significantly on generated questions from document summaries."

### Mechanism 3
- Claim: GPT-4 is a reliable evaluator for answer quality due to high correlation with human evaluation.
- Mechanism: GPT-4's advanced reasoning capabilities allow it to assess answer coherence, relevance, factual consistency, and accuracy effectively, mimicking human judgment.
- Core assumption: GPT-4's evaluation aligns closely with human preferences and captures nuanced aspects of answer quality.
- Evidence anchors:
  - [abstract]: "GPT-4 has the highest correlation with humans and surpasses all other auto-evaluation methods on summarization tasks."
  - [section]: "We take inspiration from prior works on long-form text generation metrics... and adopt them in our evaluation... GPT-4 as evaluator has shown high correlation with human evaluation in long form text generation tasks like summarization."

## Foundational Learning

- Concept: Abstractive summarization
  - Why needed here: The study relies on generating questions from summaries, so understanding how summaries condense information is crucial.
  - Quick check question: How does abstractive summarization differ from extractive summarization in terms of information retention and inference requirements?

- Concept: Long-form question answering (LFQA)
  - Why needed here: The study focuses on LFQA, requiring knowledge of how LLMs handle extended contexts and complex reasoning.
  - Quick check question: What challenges do LLMs face when answering questions that require synthesizing information across long documents?

- Concept: Evaluation metrics for text generation
  - Why needed here: The study uses GPT-4 to evaluate answers based on coherence, relevance, factual consistency, and accuracy, so understanding these metrics is essential.
  - Quick check question: How do coherence, relevance, factual consistency, and accuracy differ in evaluating long-form answers?

## Architecture Onboarding

- Component map: Wikipedia articles -> ChatGPT summaries -> ChatGPT questions -> LLM answers -> GPT-4 evaluation -> Human validation
- Critical path: 1) Collect and preprocess Wikipedia articles, 2) Generate summaries using ChatGPT, 3) Generate questions from summaries and original passages, 4) Filter out unanswerable questions, 5) Generate answers using LLMs, 6) Evaluate answers using GPT-4, 7) Perform human evaluation for validation
- Design tradeoffs: Using ChatGPT for question generation may bias results favorably toward ChatGPT but is necessary due to limitations of open-source models; GPT-4 as evaluator is efficient but may have unknown biases; limiting context length to 2k tokens for fair comparison but potentially missing longer context effects
- Failure signatures: If open-source LLMs consistently fail to generate meaningful questions; if GPT-4's evaluation significantly diverges from human judgment; if questions generated from summaries are consistently unanswerable or too ambiguous
- First 3 experiments: 1) Compare question generation quality between ChatGPT and open-source LLMs on a small dataset, 2) Evaluate answer quality for questions generated directly from passages vs. summaries, 3) Analyze the effect of context length on answer quality across different LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on long-form question answering tasks vary with the complexity of the generated questions, and what specific characteristics of the questions contribute to this variation?
- Basis in paper: [explicit]
- Why unresolved: The paper discusses the complexity of generated questions and their impact on LLM performance but does not provide a detailed analysis of how specific question characteristics affect LLM performance.
- What evidence would resolve it: Conducting experiments that systematically vary question characteristics (e.g., length, domain specificity, required reasoning type) and measure their impact on LLM performance would provide insights into the relationship between question complexity and LLM capabilities.

### Open Question 2
- Question: What are the limitations of using GPT-4 as an evaluator for answer quality in long-form question answering tasks, and how do these limitations affect the reliability of the evaluation results?
- Basis in paper: [inferred]
- Why unresolved: The paper acknowledges that using GPT-4 as an evaluator has limitations but does not explore these limitations in depth or discuss their potential impact on the evaluation results.
- What evidence would resolve it: Investigating the potential biases and limitations of GPT-4 as an evaluator through comparative studies with human evaluators and other evaluation methods would help assess the reliability of the evaluation results.

### Open Question 3
- Question: How do open-source LLMs like Alpaca and Llama perform on long-form question answering tasks compared to massive LLMs like ChatGPT, and what specific aspects of their performance differentiate them?
- Basis in paper: [explicit]
- Why unresolved: The paper provides a comparison of open-source LLMs and ChatGPT but does not delve into the specific aspects of their performance that differentiate them, such as their ability to handle longer contexts or generate more accurate and relevant answers.
- What evidence would resolve it: Conducting detailed performance analyses that focus on specific aspects of LLM capabilities, such as context handling, answer accuracy, and relevance, would provide a clearer understanding of the differences between open-source and massive LLMs.

## Limitations

- Heavy reliance on ChatGPT for both question generation and evaluation creates potential circularity in assessing model performance
- Abstractive summary approach may introduce artifacts or omissions that affect question quality and answerability
- GPT-4 evaluation, while correlated with human judgment, may not fully capture nuanced aspects of answer quality for complex long-form questions

## Confidence

- High confidence: The observation that ChatGPT outperforms open-source models across all evaluation metrics is well-supported by the experimental results and evaluation methodology
- Medium confidence: The claim that questions generated from summaries require deeper reasoning is supported by the mechanism but relies on assumptions about summary quality and question generation that could introduce bias
- Medium confidence: The finding that distilled LLMs show decreased reliance on context for simple questions but struggle with summary-generated questions is supported, though the effect may be influenced by the question generation method

## Next Checks

1. Conduct a thorough human evaluation comparing GPT-4 scores with independent human judgments on a subset of questions to verify the correlation and identify any systematic biases in automated evaluation
2. Test open-source LLMs' ability to generate questions from both summaries and original passages to determine if the performance gap is due to model capabilities or the question generation process itself
3. Systematically evaluate model performance across different context lengths (256, 512, 1024, 2048 tokens) to identify the precise point where performance degrades and whether this is consistent across question types