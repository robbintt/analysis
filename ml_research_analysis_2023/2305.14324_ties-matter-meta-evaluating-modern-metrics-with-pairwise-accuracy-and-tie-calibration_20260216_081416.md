---
ver: rpa2
title: 'Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie
  Calibration'
arxiv_id: '2305.14324'
source_url: https://arxiv.org/abs/2305.14324
tags:
- ties
- metric
- metrics
- scores
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper highlights issues with existing Kendall\u2019s tau\
  \ variants in the context of machine translation metric meta-evaluation, particularly\
  \ their handling of tied scores. The authors demonstrate that current variants either\
  \ ignore ties or penalize metrics for predicting them, leading to unfair comparisons."
---

# Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration

## Quick Facts
- arXiv ID: 2305.14324
- Source URL: https://arxiv.org/abs/2305.14324
- Reference count: 23
- One-line primary result: New τ23 and acc23 variants reward tie predictions in metric meta-evaluation, improving fairness and accuracy over existing Kendall's tau methods.

## Executive Summary
This paper addresses a critical flaw in existing Kendall's tau variants used for meta-evaluating machine translation metrics: their inconsistent and often unfair handling of tied scores. The authors demonstrate that current methods either ignore ties or penalize metrics for predicting them, leading to misleading metric rankings. To resolve this, they propose τ23, a novel Kendall variant that rewards correct tie predictions, and acc23, a pairwise accuracy measure that never returns NaN values. They also introduce a τ optimization algorithm that automatically introduces ties into metric scores, enabling fair comparison between metrics that do and do not predict ties. Experiments on WMT22 data show that these methods produce more reliable and interpretable metric rankings.

## Method Summary
The authors propose τ23 and acc23 as new variants of Kendall's tau that explicitly reward metrics for correctly predicting ties in human scores. τ23 is defined using concordant, discordant, and tie counts, while acc23 is a simpler accuracy measure that ranges from 0 to 1. They also introduce a τ optimization algorithm that searches over possible tie-introducing margins to maximize correlation, allowing fair comparison between regression-style metrics (which rarely predict exact ties) and classification-style metrics (which frequently predict ties). The methods are evaluated on WMT22 MQM human scores and metric predictions across three language pairs, comparing against existing τ variants and demonstrating improved fairness and accuracy.

## Key Results
- τ23 and acc23 correctly reward metrics for predicting ties in human MQM scores, unlike existing τ variants which either ignore or penalize ties.
- The τ optimization algorithm produces optimal tie thresholds (ϵ*) that enable fair comparison between metrics with different tie-predicting behaviors.
- On WMT22 data, the proposed methods lead to more reliable metric rankings, with significant improvements in fairness (avoiding NaN issues) and accuracy (better alignment with true metric quality).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correctly rewarding tie predictions in metric scores improves meta-evaluation accuracy by aligning correlation with true metric quality.
- Mechanism: The τ23 and acc23 variants assign credit for concordant pairs (C) and correctly predicted ties (Thm), while penalizing discordant pairs (D) and incorrect tie predictions (Th, Tm). This ensures that metrics which accurately predict tied human scores are not unfairly penalized.
- Core assumption: Human MQM scores that are tied represent true equivalence in translation quality, and metrics that predict such ties are more reliable.
- Evidence anchors:
  - [abstract]: "We propose a novel variant that gives metrics credit for correctly predicting ties, as well as an optimization procedure that automatically introduces ties into metric scores, enabling fair comparison between metrics that do and do not predict ties."
  - [section 3]: "ties in MQM scores are very common. For instance, up to 53% of possible pairs in en-de have tied MQM scores... If ties in the MQM scores are removed from τ, we throw away a valuable metric quality signal."
  - [corpus]: Weak evidence; no directly relevant citations, but FMR score is moderate (0.544), suggesting moderate relatedness to tie handling in ranking.
- Break condition: If ties in human scores are not reliable indicators of true equivalence, rewarding tie predictions would degrade rather than improve evaluation quality.

### Mechanism 2
- Claim: The τ optimization algorithm (Algorithm 1) enables fair comparison between metrics by introducing optimal tie thresholds based on score differences.
- Mechanism: The algorithm searches over possible ϵ values to maximize a τ statistic, treating any two translations with a score difference ≤ ϵ as tied. This allows regression metrics that rarely predict exact ties to still be evaluated fairly against classification-style metrics that frequently predict ties.
- Core assumption: Small absolute differences in metric scores reflect genuine uncertainty about relative translation quality, and there exists an optimal threshold ϵ that captures this.
- Evidence anchors:
  - [section 5.1]: "Because τ optimization introduces an optimal number of tie predictions, metrics are not penalized for under predicting ties, and therefore metrics that do and do not predict ties can be fairly compared."
  - [section 6.2]: "The optimization procedure also produces an ϵ∗ value for each metric... These values are also included in Table 5. Although its value is dataset-dependent, the ϵ∗ for several metrics on the WMT'22 en-de dataset is rather high."
  - [corpus]: No direct evidence; corpus neighbors do not address optimization procedures for tie thresholds.
- Break condition: If the assumption that small score differences indicate true ties is violated (e.g., if metric scores are not on a continuous scale), the optimization may introduce spurious ties and degrade evaluation.

### Mechanism 3
- Claim: Avoiding NaN values by defining correlation behavior for constant score vectors eliminates gaming opportunities via tie introduction.
- Mechanism: Existing τ variants return NaN when either score vector is constant (all pairs tied), and practical implementations drop NaN results from averages. The proposed acc23 variant never produces NaN, ensuring all data is used and preventing metrics from improving scores by forcing NaNs through tie introduction.
- Core assumption: Including all pairs in correlation computation, even when constant, provides a more stable and fair evaluation than dropping NaN results.
- Evidence anchors:
  - [section 4.2]: "Another consequence of how the τ correlations handle ties is what we refer to as the 'NaN problem.'... A metric could take advantage of this property... resulting in higher correlations... Indeed, we find that this is possible."
  - [section 5]: "τ23 does not suffer from the same issues as the other τs... Because its value is never NaN, it does not suffer from the NaN problem."
  - [corpus]: No direct evidence; corpus does not contain material on NaN handling in correlation metrics.
- Break condition: If constant-score cases are truly uninformative and should be excluded, forcing inclusion could dilute meaningful comparisons.

## Foundational Learning

- Concept: Kendall's tau variants and their handling of ties
  - Why needed here: Understanding how τa, τb, τc, τ10, τ13, τ14 differ in tie treatment is essential to grasp why existing variants fail and how τ23/acc23 fix them.
  - Quick check question: In τ10, what happens to pairs that are tied only in the metric scores? (Answer: They are penalized as if they were discordant pairs.)

- Concept: Segment-level vs. system-level correlation in MT evaluation
  - Why needed here: The paper focuses on segment-level correlations (no-grouping, group-by-item, group-by-system), which are more discriminative and relevant for fine-grained metric comparison.
  - Quick check question: Which segment-level correlation method evaluates each translation individually rather than aggregating by system or source segment? (Answer: No-Grouping.)

- Concept: Multi-dimensional Quality Metrics (MQM) and tie prevalence
  - Why needed here: MQM scores are the human gold standard in recent WMT tasks, and their integer nature leads to many ties, motivating the need for tie-aware evaluation.
  - Quick check question: What percentage of en-de pairs in WMT'22 had tied MQM scores when grouping by source segment? (Answer: 53%.)

## Architecture Onboarding

- Component map:
  - Data layer: WMT'22 MQM human scores and metric predictions across three language pairs.
  - Core algorithm: τ optimization (Algorithm 1) that searches ϵ values to maximize correlation.
  - Evaluation metrics: acc23 (pairwise accuracy with ties) and τ23 (Kendall variant with ties).
  - Comparison layer: Existing τ variants (τa, τb, τc, τ10, τ13, τ14) for benchmarking.
  - Reporting layer: Tables showing metric rankings under each τ variant and the optimized version.

- Critical path:
  1. Load human and metric score vectors.
  2. Compute existing τ variants and acc23/τ23.
  3. Run τ optimization to find ϵ∗ and optimal correlation.
  4. Compare metric rankings across variants.
  5. Report results with interpretability metrics (e.g., proportion of correctly ranked pairs).

- Design tradeoffs:
  - τ optimization is O(n² log n) due to sorting all pairs; for large n (no-grouping mode), this is prohibitive, so random subsampling is used, trading precision for tractability.
  - acc23 is more interpretable (0-1 scale) but may be less sensitive to fine-grained ranking differences than τ23.
  - Including all pairs (even constant vectors) avoids NaN issues but may dilute comparisons if constant cases are uninformative.

- Failure signatures:
  - Sudden rank reversals between τ variants without clear justification (e.g., MaTESe dropping from 1st to 14th under τ10).
  - Large differences in number of non-NaN segments across metrics, indicating NaN problem exploitation.
  - Optimization producing extreme ϵ∗ values (e.g., 15.00 for GEMBA-Dav3-DA), suggesting dataset-specific artifacts.

- First 3 experiments:
  1. Run all existing τ variants on a small synthetic dataset with known ties to verify that τ23/acc23 correctly reward tie predictions while others do not.
  2. Apply τ optimization to a regression-style metric (e.g., COMET-22) and confirm that its ranking improves relative to tie-predicting metrics without changing its underlying scores.
  3. Intentionally create a constant-score vector for a metric and confirm that acc23 returns a defined value while existing τ variants return NaN.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the analysis, the following unresolved issues emerge:

### Open Question 1
- Question: How can we develop a universal method for handling ties in Kendall's tau that works across different NLP tasks beyond machine translation, such as summarization or image captioning?
- Basis in paper: [explicit] The authors state that "Although our experiments were specific to MT, the methods proposed are generally applicable to any metric meta-evaluation in NLP."
- Why unresolved: The paper focuses exclusively on machine translation metrics, leaving the applicability and effectiveness of the proposed methods in other NLP domains unexplored.
- What evidence would resolve it: Experiments applying τ23, acc23, and τ optimization to other NLP tasks (e.g., summarization, image captioning) with different types of human judgments and metric outputs would demonstrate their broader utility.

### Open Question 2
- Question: What are the computational trade-offs of using the proposed τ optimization algorithm in large-scale meta-evaluation scenarios?
- Basis in paper: [inferred] The authors mention that the no-grouping variant of segment-level correlation can involve over 200 million pairs, making the algorithm's O(n² log n) complexity potentially problematic.
- Why unresolved: The paper does not analyze the practical runtime implications or explore optimization strategies for large datasets.
- What evidence would resolve it: Empirical runtime comparisons between the proposed algorithm and alternative tie-handling methods on large-scale datasets would quantify computational costs and identify bottlenecks.

### Open Question 3
- Question: How does the proposed method handle ties in cases where human judgments are inherently noisy or subjective, such as in open-ended generation tasks?
- Basis in paper: [explicit] The authors discuss the reliability of MQM scores in WMT but do not address scenarios with subjective or noisy human judgments.
- Why unresolved: The paper assumes human scores are reliable and does not explore the method's robustness to noisy or inconsistent human judgments.
- What evidence would resolve it: Experiments evaluating τ23 and acc23 on datasets with known levels of human judgment noise would assess their robustness and reliability.

### Open Question 4
- Question: Can the proposed method be extended to handle ties in other correlation measures, such as Pearson's r or Spearman's ρ, and what would be the implications for meta-evaluation?
- Basis in paper: [explicit] The authors mention that Pearson's r has its own issues with sensitivity to outliers but do not explore tie-handling in other correlation measures.
- Why unresolved: The paper focuses exclusively on Kendall's tau variants, leaving the potential for tie-handling improvements in other correlation measures unexplored.
- What evidence would resolve it: Theoretical analysis and experimental validation of tie-handling modifications to other correlation measures would demonstrate their feasibility and impact on meta-evaluation.

## Limitations
- The proposed methods are evaluated only on WMT22 data, limiting generalizability to other datasets or NLP tasks.
- τ optimization introduces additional hyperparameters (ϵ values) that may overfit to specific datasets, and the paper does not thoroughly explore cross-dataset stability.
- The assumption that tie prevalence in MQM scores reflects true equivalence is not thoroughly validated, and the impact of τ optimization on long-term metric development is unclear.

## Confidence
- **High confidence**: The identification of NaN problems in existing τ variants and the mathematical formulation of τ23/acc23 are well-supported.
- **Medium confidence**: The empirical improvements in fairness and accuracy on WMT22 data are convincing, but limited to a single dataset.
- **Low confidence**: The assumption that tie prevalence in MQM scores reflects true equivalence is not thoroughly validated, and the impact of τ optimization on long-term metric development is unclear.

## Next Checks
1. **Cross-dataset stability test**: Apply τ optimization to WMT21 and WMT20 data to verify that ϵ* values and metric rankings remain stable across years, checking for overfitting to WMT22 patterns.
2. **Human score quality validation**: Conduct a small-scale study where human raters resolve ties in MQM scores and measure how this affects τ23/acc23 correlations, validating the assumption that ties represent true equivalence.
3. **Metric development impact study**: Compare metric development trajectories when using τ23/acc23 versus existing variants on a held-out validation set, measuring whether tie-aware evaluation leads to measurably better metrics over multiple development cycles.