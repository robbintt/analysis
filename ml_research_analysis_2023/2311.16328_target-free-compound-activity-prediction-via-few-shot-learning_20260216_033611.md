---
ver: rpa2
title: Target-Free Compound Activity Prediction via Few-Shot Learning
arxiv_id: '2311.16328'
source_url: https://arxiv.org/abs/2311.16328
tags:
- activity
- context
- compounds
- learning
- compound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of few-shot regression for continuous
  compound activity prediction, where models must predict the activity of a new compound
  in an unseen assay given only a few known compounds and their activity values. The
  authors propose FS-CAP, a novel model-based few-shot learning approach that learns
  to aggregate assay-specific information from context compounds while separately
  encoding the unknown compound.
---

# Target-Free Compound Activity Prediction via Few-Shot Learning

## Quick Facts
- arXiv ID: 2311.16328
- Source URL: https://arxiv.org/abs/2311.16328
- Authors: 
- Reference count: 18
- One-line primary result: FS-CAP achieves up to 0.54 Pearson correlation on compound activity prediction in PubChem BioAssay using few-shot learning.

## Executive Summary
This paper tackles the problem of few-shot regression for continuous compound activity prediction, where models must predict the activity of a new compound in an unseen assay given only a few known compounds and their activity values. The authors propose FS-CAP, a novel model-based few-shot learning approach that learns to aggregate assay-specific information from context compounds while separately encoding the unknown compound. Key innovations include a multiplication-based featurization of compound fingerprints and activities, and a dedicated encoder for the query compound.

## Method Summary
FS-CAP uses two separate neural encoders - one for the query compound (representing assay-independent binding characteristics) and one for context compounds (combining fingerprints with activity values via multiplication). Context encodings are aggregated via averaging to create a fixed-length assay representation, which is then combined with the query encoding to predict activity. The model is trained end-to-end using MSE loss on datasets like PubChem BioAssay and BindingDB, and evaluated on tasks including anti-cancer drug activity prediction.

## Key Results
- FS-CAP achieves up to 0.54 Pearson correlation on compound activity prediction in PubChem BioAssay
- Outperforms traditional chemical similarity methods and state-of-the-art few-shot learning baselines
- Demonstrates generalization to anti-cancer drug activity prediction (CCLE dataset), showing ability to capture biologically meaningful assay representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multiplication-based featurization of compound fingerprints and activities enables the model to directly capture substructure-activity relationships.
- Mechanism: By multiplying the binary Morgan fingerprint vector with the scalar activity value, substructures that contribute to activity are weighted higher, allowing the context encoder to learn which features are most relevant for predicting activity in a given assay.
- Core assumption: Each bit in the Morgan fingerprint corresponds to a specific molecular substructure that has a direct impact on biological activity.
- Evidence anchors:
  - [abstract] "We use a deterministic neural encoder to represent context compounds and their activities via a new multiplication-based featurization."
  - [section] "Since Morgan fingerprints are substructure-based, i.e. each element in the vector has a 1 bit if there is a certain substructure present and 0 otherwise, and substructures are known to contribute directly to binding characteristics, this featurization may make it easier for the model to learn which substructures contribute how much to activity."
  - [corpus] Weak - no direct corpus support found for this specific multiplication approach in few-shot learning.
- Break condition: If substructures in the fingerprint do not have linear or additive contributions to activity, or if activity depends on complex 3D interactions not captured by 2D fingerprints.

### Mechanism 2
- Claim: Separate encoding of the query compound allows the model to capture assay-independent binding characteristics before receiving assay-specific context information.
- Mechanism: The query encoder processes the compound fingerprint independently of any activity data, learning a representation that encodes intrinsic molecular properties relevant to binding, which is then combined with assay-specific context encoding.
- Core assumption: Molecular properties that influence binding (like shape, polarity) are largely independent of the specific assay and can be learned separately.
- Evidence anchors:
  - [abstract] "We also introduce a separate encoder for the unknown compound to represent its assay-independent binding characteristics."
  - [section] "However, in drug discovery, there are useful query features that may be extracted entirely independently of any assay, such as compound shape and electrostatics."
  - [corpus] Weak - no direct corpus support found for separate query encoding in few-shot compound activity prediction.
- Break condition: If assay-independent features are not predictive of activity, or if the assay context is so dominant that pre-encoding provides no benefit.

### Mechanism 3
- Claim: Deterministic aggregation of context encodings via averaging maintains permutation invariance while capturing assay-specific patterns from limited examples.
- Mechanism: By averaging the individual context encodings, the model creates a fixed-length representation of the assay that is invariant to the order of context compounds and robust to noise from individual examples.
- Core assumption: The average of context encodings contains sufficient information to represent the assay, and the order of compounds in the context set is irrelevant.
- Evidence anchors:
  - [section] "To aggregate each individual context encoding ri into a single real-valued vector xc that represents the context set as a whole, we take the average across each ri: This maintains permutation invariance, as desired, since the order of the contexts should not affect their encoding."
  - [section] "More complex aggregation techniques, such as self-attention, did not lead to improved performance (Table 6)."
  - [corpus] Weak - no direct corpus support found for averaging in few-shot compound activity prediction; attention methods are more commonly discussed.
- Break condition: If the assay is better represented by non-linear combinations of context compounds, or if certain context compounds are much more informative than others.

## Foundational Learning

- Concept: Few-shot learning and meta-learning
  - Why needed here: The problem requires predicting activity in unseen assays with only a few examples, which is the defining challenge of few-shot learning.
  - Quick check question: What is the difference between few-shot classification and few-shot regression in the context of drug discovery?

- Concept: Molecular fingerprints and featurization
  - Why needed here: The model uses Morgan fingerprints as input, so understanding how these encode molecular structure is essential for interpreting the model's behavior.
  - Quick check question: How does a Morgan fingerprint represent molecular substructures, and why is this representation useful for activity prediction?

- Concept: Assay-specific vs. compound-specific features
  - Why needed here: The architecture separates assay-independent (query) and assay-dependent (context) information, so understanding this distinction is crucial.
  - Quick check question: Why might it be beneficial to encode a compound's features separately from assay information in drug discovery?

## Architecture Onboarding

- Component map: Input → Query encoder (compound fingerprint → embedding) + Context encoder (fingerprint × activity → embedding) → Context aggregation (average) → Predictor (concatenated embeddings → activity prediction)
- Critical path: Compound fingerprint → query encoder → predictor; Compound fingerprint × activity → context encoder → context aggregation → predictor
- Design tradeoffs: Simple averaging vs. attention for context aggregation; separate query encoder vs. direct concatenation; multiplication vs. concatenation for context featurization
- Failure signatures: Poor correlation on test assays suggests issues with context encoding or aggregation; failure to generalize to CCLE suggests limited biological relevance of learned representations
- First 3 experiments:
  1. Test model performance with and without the query encoder to validate its contribution
  2. Compare multiplication vs. concatenation featurization for context compounds
  3. Evaluate different context aggregation methods (mean vs. attention vs. learned pooling)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do more complex molecular representations like graphs or sequences compare to Morgan fingerprints in the few-shot regression setting for compound activity prediction?
- Basis in paper: [inferred] The authors mention that a limitation of their current implementation is the use of relatively simple molecular representations (Morgan fingerprints) and suggest that exploring more complex representations could be a future development.
- Why unresolved: The paper focuses on Morgan fingerprints as the molecular representation, and while it acknowledges the potential for more complex representations, it does not directly compare them to other methods like graph neural networks or sequence-based representations.
- What evidence would resolve it: Direct comparison of FS-CAP with different molecular representations (e.g., Morgan fingerprints, graph neural networks, sequence-based models) on the same tasks and datasets would provide evidence of which representation is most effective for few-shot regression in compound activity prediction.

### Open Question 2
- Question: What is the impact of different context aggregation techniques beyond the mean on the performance of FS-CAP?
- Basis in paper: [explicit] The authors mention that they use a simple mean aggregation technique and note that more complex aggregation techniques like self-attention did not lead to improved performance in their experiments. They also suggest exploring more complex context aggregation methods as a future development.
- Why unresolved: While the authors tested self-attention as a context aggregation method and found it did not improve performance, they do not explore other potentially more expressive aggregation techniques that could capture complex relationships between context compounds.
- What evidence would resolve it: Experimenting with different context aggregation techniques (e.g., attention mechanisms, graph neural networks, transformers) and comparing their performance to the mean aggregation method on the same tasks and datasets would provide evidence of the impact of aggregation techniques on FS-CAP's performance.

### Open Question 3
- Question: How does FS-CAP perform on datasets with a larger number of assays and compounds, and what are the computational limitations of scaling up the model?
- Basis in paper: [inferred] The paper mentions that the authors used datasets like PubChemBA and BindingDB, which contain a large number of assays and compounds, but does not explicitly discuss the computational limitations of scaling up the model to even larger datasets.
- Why unresolved: The paper does not provide information on the computational resources required to train and test FS-CAP on datasets larger than those used in the experiments, nor does it discuss potential limitations in scaling up the model.
- What evidence would resolve it: Conducting experiments with FS-CAP on larger datasets and reporting the computational resources required (e.g., training time, memory usage) would provide evidence of the model's scalability and potential limitations. Additionally, exploring techniques to improve computational efficiency (e.g., model parallelism, distributed training) would provide insights into how to scale up FS-CAP for larger datasets.

## Limitations
- Multiplication-based featurization lacks strong empirical validation against alternative methods
- Model's performance on extremely small context sets (k=1 or k=2) is not thoroughly evaluated
- Computational scalability to larger datasets is not discussed or tested

## Confidence

- High confidence: The overall problem formulation and the need for few-shot learning approaches in drug discovery are well-established
- Medium confidence: The comparative performance against baseline methods is robust, though hyperparameter sensitivity is not fully explored
- Low confidence: The specific architectural choices (multiplication featurization, separate query encoding) lack direct empirical validation against alternatives

## Next Checks

1. **Ablation study on featurization**: Systematically compare multiplication-based featurization against concatenation and other alternatives on the same benchmark tasks
2. **Context set size sensitivity**: Evaluate model performance across a wider range of context set sizes (k=1, 2, 3, 5) to understand performance scaling
3. **Transfer learning validation**: Test the model's ability to transfer knowledge from one compound class to another (e.g., kinase inhibitors to GPCR modulators) to validate biological relevance claims