---
ver: rpa2
title: Self-attention Dual Embedding for Graphs with Heterophily
arxiv_id: '2305.18385'
source_url: https://arxiv.org/abs/2305.18385
tags:
- topology
- graph
- graphs
- feature
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SADE-GCN, a Graph Neural Network (GNN) designed
  to perform well on both homophilic and heterophilic graphs. The authors observe
  that node features and graph topology provide different amounts of informativeness
  in different graphs, and that allowing negative attention weights when propagating
  graph topology information improves accuracy.
---

# Self-attention Dual Embedding for Graphs with Heterophily

## Quick Facts
- arXiv ID: 2305.18385
- Source URL: https://arxiv.org/abs/2305.18385
- Reference count: 40
- Achieves state-of-the-art results on heterophilic graphs with 13 real-world datasets

## Executive Summary
This paper introduces SADE-GCN, a Graph Neural Network designed to handle both homophilic and heterophilic graphs effectively. The key insight is that node features and graph topology provide different amounts of informativeness across different graphs, and that allowing negative attention weights when propagating topology information improves accuracy on heterophilic graphs. SADE-GCN employs a novel self-attention mechanism with dual embedding paths that maintain independence between feature and topology information during message passing, combining them only at the final layer. The model achieves state-of-the-art performance on heterophilic graphs while maintaining competitive results on homophilic graphs across 13 datasets ranging from thousands to millions of nodes.

## Method Summary
SADE-GCN implements a self-attention dual embedding architecture that separates feature and topology information into two independent paths (Pf and Pt) during message passing. Each path uses self-attention to compute asymmetric attention weights, with the topology path additionally employing a scaling function that maps attention values to [-1,1] to enable negative weights. The model stacks multiple Self-Attention Graph Convolution (SAGC) modules, each containing self-attention, heterophilic graph convolution, and adaptive combination layers. After message passing through L layers, the feature and topology embeddings are adaptively combined using learnable weights before the final classification layer. The model is trained using Adam optimizer with cross-entropy loss on node classification tasks.

## Key Results
- Achieves state-of-the-art accuracy on heterophilic graphs compared to existing GNNs
- Maintains competitive performance on homophilic graphs while excelling at heterophily
- Ablation studies confirm the effectiveness of dual embedding, negative attention weights, and asymmetric attention mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly separating feature and topology embeddings prevents premature mixing that degrades performance when one type of information is more informative.
- Mechanism: SADE-GCN uses two parallel paths (Pf for features, Pt for topology) that remain independent during message passing and only combine at the final layer.
- Core assumption: Feature and topology information have different informativeness across graphs, and early mixing introduces noise when one signal dominates.
- Evidence anchors:
  - [abstract]: "we show that node features and graph topology provide different amounts of informativeness in different graphs"
  - [section 3.1.1]: "we propose keeping the independence of feature and topology embeddings during message passing as a way to enhance the model's robustness to noise"
- Break condition: If both feature and topology carry roughly equal signal strength, the dual-path design may add unnecessary complexity without accuracy gains.

### Mechanism 2
- Claim: Negative attention weights improve heterophily extraction by enabling high-frequency filtering of topology information.
- Mechanism: The scaling function maps attention values from [0,1] to [-1,1], allowing the model to push embeddings of differently-labeled neighbors apart.
- Core assumption: Heterophilic graphs require high-pass filtering to capture class differences, while homophilic graphs benefit from low-pass filtering.
- Evidence anchors:
  - [abstract]: "we show that allowing negative attention weights when propagating graph topology information improves accuracy"
  - [section 3.1.2]: "To account for heterophily, GNNs leverage high-frequency filters by introducing negative edge weights"
- Break condition: If the graph is strongly homophilic, negative weights may reduce signal quality by weakening beneficial smoothing.

### Mechanism 3
- Claim: Asymmetric attention weights between nodes better model real-world influence patterns where neighbor effects are not reciprocal.
- Mechanism: SADE-GCN uses self-attention without enforcing symmetry, allowing i→j influence to differ from j→i influence.
- Core assumption: In heterophilic graphs, influence flows are directional (e.g., fraudsters influence regular users more than vice versa).
- Evidence anchors:
  - [abstract]: "we show that asymmetric attention weights between nodes are helpful"
  - [section 3.1.3]: "neighbors in a heterophilic graph often possess varying influences over each other"
- Break condition: If the graph structure is nearly symmetric in influence patterns, asymmetric weights may overfit noise.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: SADE-GCN builds on GNN foundations but modifies aggregation to handle heterophily
  - Quick check question: What is the difference between low-pass and high-pass filtering in graph signal processing?
- Concept: Self-attention mechanism from Transformers
  - Why needed here: SADE-GCN applies self-attention to both features and topology to compute asymmetric weights
  - Quick check question: How does softmax over query and key separately differ from softmax over the attention matrix in standard Transformers?
- Concept: Heterophily vs homophily in graphs
  - Why needed here: The core motivation for SADE-GCN is handling graphs where connected nodes often have different labels
  - Quick check question: What metric quantifies the fraction of edges connecting same-class nodes?

## Architecture Onboarding

- Component map:
  Input layer (X, A) -> Pf and Pt parallel paths -> SAGC modules (self-attention + heterophilic GCN + adaptive combination) -> Combine Pf(L) and Pt(L) -> Output layer
- Critical path: X → Pf → SAGC → ... → Pf(L) → Combine → Output, similarly for Pt
- Design tradeoffs:
  - Dual embedding increases parameter count and memory usage but improves robustness to noise
  - Asymmetric attention adds expressiveness but requires careful normalization to avoid instability
  - Negative weights help heterophily but may hurt homophilic graphs if not gated properly
- Failure signatures:
  - Degraded accuracy on homophilic graphs: likely from excessive negative weights or over-asymmetric attention
  - Memory OOM on large graphs: check if efficient self-attention is enabled; large N and H create quadratic cost
  - Poor convergence: check learning rate, scaling function implementation, and gradient flow through residual connections
- First 3 experiments:
  1. Compare SADE-GCN with and without dual embedding on a heterophilic graph (e.g., Chameleon) to confirm the independence benefit.
  2. Validate negative attention weights by comparing SADE-GCN with and without the scaling function on a heterophilic graph.
  3. Test asymmetric vs symmetric attention by running SADE-GCN-sym on the same dataset to confirm the asymmetry gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance scale with increasingly large and sparse graphs beyond the tested datasets?
- Basis in paper: [inferred] The paper tests on graphs with up to millions of nodes and tens of millions of edges, but does not explore extreme sparsity or very large graphs beyond these benchmarks.
- Why unresolved: The authors only tested on a limited set of large graphs, leaving open the question of performance on graphs with even higher node-to-edge ratios or orders of magnitude larger.
- What evidence would resolve it: Experimental results on synthetic or real-world graphs with sparsity levels and sizes significantly exceeding the current benchmarks would clarify the model's scalability limits.

### Open Question 2
- Question: What is the impact of using different self-attention mechanisms (e.g., multi-head attention) on the model's ability to capture heterophily and asymmetry?
- Basis in paper: [explicit] The authors use a single-head self-attention mechanism but mention the possibility of using multi-head attention in the related work section.
- Why unresolved: The paper only explores a single self-attention configuration, leaving open the question of whether alternative attention mechanisms could improve performance.
- What evidence would resolve it: Ablation studies comparing SADE-GCN's performance using different self-attention mechanisms (e.g., single-head vs. multi-head) would reveal the optimal attention configuration for capturing heterophily.

### Open Question 3
- Question: How does the model's performance change when applied to directed graphs with asymmetric edge weights?
- Basis in paper: [inferred] The paper focuses on undirected graphs, but mentions the importance of capturing asymmetry in heterophilic graphs.
- Why unresolved: The authors do not test the model on directed graphs, leaving open the question of how it would handle inherently asymmetric edge weights.
- What evidence would resolve it: Experiments applying SADE-GCN to directed graphs with varying levels of edge weight asymmetry would reveal its effectiveness in capturing directed relationships.

## Limitations

- Dual embedding design may add unnecessary complexity for homophilic graphs where feature and topology signals are balanced
- Negative attention weights could degrade performance on strongly homophilic graphs if not properly gated
- Asymmetric attention mechanism assumes real-world influence flows are directional, which may not hold for all graph types

## Confidence

- **High** confidence in empirical results and ablation studies demonstrating effectiveness on heterophilic graphs
- **Medium** confidence in generalizability across all graph types given limited testing on strongly homophilic networks
- **Low** confidence in scalability claims beyond the tested million-node datasets

## Next Validation Checks

1. Test SADE-GCN on strongly homophilic graphs (e.g., protein-protein interaction networks) to verify that negative attention weights and asymmetric mechanisms don't degrade performance.
2. Evaluate the model's performance on graphs with more than 1 million nodes to confirm the claimed scalability and efficiency of the self-attention mechanism.
3. Implement a gating mechanism that adaptively controls the use of negative attention weights based on local graph homophily metrics, and compare against the fixed negative weight approach.