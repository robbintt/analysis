---
ver: rpa2
title: 'AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments'
arxiv_id: '2310.06514'
source_url: https://arxiv.org/abs/2310.06514
tags:
- attribution
- methods
- pixels
- figure
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AttributionLab, a controllable synthetic
  environment designed to rigorously evaluate the faithfulness of feature attribution
  methods in deep neural networks. The core idea is to manually design both the network
  and the data, ensuring complete control over the ground truth features that influence
  model outputs.
---

# AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments

## Quick Facts
- **arXiv ID**: 2310.06514
- **Source URL**: https://arxiv.org/abs/2310.06514
- **Reference count**: 40
- **Key outcome**: Introduces a synthetic environment to rigorously evaluate feature attribution faithfulness by manually designing both network and data to ensure ground truth alignment.

## Executive Summary
AttributionLab addresses a fundamental challenge in evaluating feature attribution methods: the inability to verify if attribution maps align with actual feature importance when ground truth is unknown. The authors solve this by creating a synthetic environment where they manually design both the neural network architecture and dataset, guaranteeing that only predetermined features influence predictions. This controlled setting enables rigorous testing of attribution methods under various conditions including different baselines and segmentation techniques. Experiments reveal significant performance differences among methods, with some struggling to identify negative contributions or being sensitive to baseline choices. The framework also exposes vulnerabilities in perturbation-based evaluation metrics when models encounter out-of-distribution inputs.

## Method Summary
The method involves creating a synthetic environment where the authors manually design both the neural network and dataset to ensure complete control over ground truth features. They construct simple CNN architectures (color detector, accumulator, and MLP) that respond only to predetermined colors in synthetic images. The data generator creates images with colored patches on backgrounds, where the network output depends exclusively on counting specific colors. Attribution methods are then applied to this environment and evaluated against ground truth masks using precision, recall, and F1-score metrics. The framework also tests methods under different baselines and with out-of-distribution simulation through redundant channels.

## Key Results
- Attribution methods show significant performance variation, with some failing to identify negative contributions or being baseline-sensitive
- Perturbation-based evaluation metrics can produce unreliable results when models encounter out-of-distribution inputs
- The synthetic environment successfully exposes fundamental limitations in common attribution methods before real-world deployment
- Ground truth alignment through manual design reveals that some methods struggle even on simple, well-defined tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic environment ensures ground truth alignment by manually designing both model weights and dataset
- Mechanism: By manually constructing the neural network and its weights, the authors guarantee that the model only uses intended features for predictions, eliminating the Rashomon effect where multiple models could achieve the same accuracy using different features
- Core assumption: Complete control over network weights allows deterministic feature attribution
- Evidence anchors:
  - [abstract] "We solve this issue by designing the network and manually setting its weights, along with designing data."
  - [section] "By explicitly designing the data and the neural network according to specific properties, we know the ground truth features in our synthetic setting."
  - [corpus] "Average neighbor FMR=0.449" (weak signal, no direct evidence of synthetic control)
- Break condition: If network complexity increases such that manual weight design becomes infeasible or if unintended feature interactions emerge from weight interactions

### Mechanism 2
- Claim: AttributionLab can isolate method sensitivity to baseline choices by providing known ground truth baseline
- Mechanism: The synthetic environment allows testing attribution methods with their ideal baseline versus arbitrary baselines, revealing baseline sensitivity that would be difficult to detect in real data
- Core assumption: Ground truth baseline is accessible and meaningful for attribution evaluation
- Evidence anchors:
  - [abstract] "The environment is also a laboratory for controlled experiments by which we can analyze attribution methods and suggest improvements."
  - [section] "We introduce this baseline-accurate variant of IG as IG*, which uses the true baseline corresponding to background color (20, 20, 20) in synthetic images."
  - [corpus] "Weak corpus evidence" (no specific mention of baseline testing)
- Break condition: If baseline sensitivity is method-specific and not generalizable across different synthetic settings

### Mechanism 3
- Claim: Synthetic environment exposes perturbation-based metric vulnerabilities through controlled "unseen data" simulation
- Mechanism: By adding redundant channels that activate on out-of-distribution inputs, the environment can test whether perturbation-based faithfulness metrics are measuring feature importance or just model sensitivity to novel inputs
- Core assumption: Out-of-distribution behavior can be reliably simulated through redundant channels
- Evidence anchors:
  - [abstract] "The synthetic environment also exposes issues with perturbation-based evaluation metrics, which can be unreliable when models encounter out-of-distribution inputs."
  - [section] "Specifically, the network exclusively counts the number of pixels of predetermined colors, and other input values (colors) do not affect the network output."
  - [corpus] "Weak corpus evidence" (no specific mention of OOD simulation)
- Break condition: If perturbation metrics remain reliable despite controlled OOD simulation, suggesting the issue is more fundamental than anticipated

## Foundational Learning

- Concept: Rashomon effect in machine learning
  - Why needed here: Understanding why synthetic environments are necessary for attribution evaluation
  - Quick check question: Can you explain why two models with identical accuracy might use completely different features to make predictions?

- Concept: Feature attribution axioms (sensitivity and symmetry)
  - Why needed here: The synthetic design explicitly enforces these axioms to ensure ground truth quality
  - Quick check question: How would you verify that a synthetic dataset satisfies the sensitivity and symmetry properties?

- Concept: Perturbation-based evaluation metrics limitations
  - Why needed here: Understanding why the synthetic environment can expose weaknesses in common evaluation approaches
  - Quick check question: What could cause a perturbation-based metric to incorrectly rank attribution methods?

## Architecture Onboarding

- Component map: Color detector CNN -> Accumulator CNN -> MLP -> Attribution method application -> Ground truth comparison -> Metric calculation
- Critical path: Data generation → Model evaluation → Attribution method application → Ground truth comparison → Metric calculation
- Design tradeoffs: Simple synthetic design vs. realism; controlled evaluation vs. generalizability to real-world scenarios
- Failure signatures: 
  - Attribution methods fail on simple synthetic tasks → Likely fundamental issues
  - Attribution methods succeed on synthetic but fail on real → Evaluation framework may be too restrictive
  - OOD simulation doesn't affect perturbation metrics → Problem may be more fundamental than anticipated
- First 3 experiments:
  1. Run attribution methods on synthetic data with known ground truth, compare F1 scores
  2. Test same methods with incorrect baselines to measure sensitivity
  3. Apply perturbation-based metrics to synthetic data with OOD simulation active, compare rankings to ground truth-based evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Unseen Data Effect be effectively mitigated in real-world applications of feature attribution methods?
- Basis in paper: [explicit] The paper discusses the Unseen Data Effect as a significant issue in perturbation-based evaluation metrics, showing that it can lead to unreliable results when models encounter out-of-distribution inputs
- Why unresolved: While the paper highlights the problem, it does not provide a comprehensive solution for mitigating this effect in practical scenarios
- What evidence would resolve it: Developing and testing methods that can handle out-of-distribution inputs effectively, or creating evaluation metrics that are robust to such effects

### Open Question 2
- Question: What are the potential improvements for attribution methods that struggle with identifying negative contributions to the target class?
- Basis in paper: [explicit] The paper notes that methods like GradCAM and IBA fail to identify negatively contributing features, which is inherent to their design limitations
- Why unresolved: The paper identifies the issue but does not propose specific enhancements to address this limitation
- What evidence would resolve it: Designing and testing new attribution methods or modifying existing ones to better capture both positive and negative contributions

### Open Question 3
- Question: How can the sensitivity of attribution methods to baseline choices be reduced to improve their reliability?
- Basis in paper: [explicit] The paper demonstrates that methods like Integrated Gradients (IG) and Occlusion are sensitive to baseline choices, which can significantly affect their performance
- Why unresolved: While the paper shows the impact of baseline sensitivity, it does not provide a definitive approach to mitigate this issue
- What evidence would resolve it: Experimenting with different baseline selection strategies or developing methods that are less dependent on baseline choices

## Limitations
- Synthetic environment may oversimplify real-world feature interactions and complexity
- Manual weight design approach may not scale to more complex architectures
- Results may be specific to the controlled synthetic setting and not fully generalizable to real-world scenarios

## Confidence

**High Confidence**: The mechanism for ground truth alignment through manual network and data design is well-supported by the authors' explicit construction methodology.

**Medium Confidence**: The claims about baseline sensitivity and OOD simulation effects are plausible given the controlled environment, but may require additional validation on more complex synthetic tasks.

**Low Confidence**: The extrapolation of synthetic findings to real-world attribution method performance should be treated cautiously until validated on more realistic datasets.

## Next Checks

1. Test attribution methods on a more complex synthetic environment with multiple interacting features to assess scalability of the framework.

2. Apply the same attribution methods to a real-world dataset with semi-synthetic ground truth (e.g., segmentation masks) to compare performance across environments.

3. Investigate whether the perturbation-based metric vulnerabilities persist when using attribution methods that incorporate uncertainty estimation or robustness mechanisms.