---
ver: rpa2
title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization
arxiv_id: '2305.11095'
source_url: https://arxiv.org/abs/2305.11095
tags:
- whisper
- speech
- language
- prompt
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates Whisper''s ability to perform three unseen
  tasks using prompt engineering: audio-visual speech recognition (AVSR), code-switched
  speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs.
  The authors propose task-specific prompts by leveraging large-scale models or manipulating
  special tokens in the default prompts.'
---

# Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization

## Quick Facts
- **arXiv ID**: 2305.11095
- **Source URL**: https://arxiv.org/abs/2305.11095
- **Reference count**: 0
- **Primary result**: Task-specific prompts improve Whisper's zero-shot performance on AVSR, CS-ASR, and ST by 10-45% over default prompts

## Executive Summary
This paper investigates Whisper's capability to perform three unseen tasks - audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) - using only prompt engineering without model fine-tuning. The authors propose task-specific prompts by leveraging large-scale models like CLIP or manipulating special tokens in default prompts. Experiments show that these prompts significantly improve performance across all three tasks, with some configurations even outperforming state-of-the-art supervised models. The study reveals interesting properties of Whisper including its robustness to prompts, accent biases, and multilingual understanding in its latent space.

## Method Summary
The authors adapt Whisper to three zero-shot tasks by modifying only the prompt tokens sent to the decoder, without changing model weights or architecture. For AVSR, they incorporate CLIP-retrieved objects as visual prompts. For CS-ASR, they modify prompts to include both language tokens. For ST, they discovered that the <|asr|> task token enables En→X translation while <|st|> only produces English. The approach leverages Whisper's transformer-based encoder-decoder architecture where prompt tokens interact with encoder features and positional embeddings to guide output generation.

## Key Results
- Proposed prompts improve performance by 10-45% compared to default prompts across all three zero-shot tasks
- Visual prompting helps English models but hurts multilingual models on AVSR
- The <|asr|> token outperforms <|st|> for English-to-X speech translation
- Whisper shows accent bias in CS-ASR, performing better on Chinese-accented Mandarin than Singaporean/Malaysian-accented speech

## Why This Works (Mechanism)

### Mechanism 1
Whisper can be adapted to unseen tasks by modifying only its prompt without changing model weights or architecture. The decoder consumes a prompt token sequence along with encoder features and positional embeddings to produce outputs. By engineering these prompt tokens, the model can be directed toward different tasks. This relies on the assumption that prompt tokens interact with the model's latent representations in a way that allows task steering without fine-tuning.

### Mechanism 2
Whisper's multilingual understanding in its latent space allows it to perform zero-shot speech translation from English to other languages using the <|asr|> task token instead of <|st|>. The model's training on multilingual ASR and X→En translation creates latent representations where semantically related words and phrases from different languages are close, enabling translation without explicit training. This assumes the model's latent space captures cross-lingual semantic relationships despite not being explicitly trained on En→X translation.

### Mechanism 3
Whisper can leverage visual information for audio-visual speech recognition by incorporating CLIP-retrieved objects into its prompt. CLIP's vision-language embeddings can retrieve semantically relevant objects from video frames, and inserting these as prompt tokens guides Whisper to produce more accurate transcriptions by providing visual context. This assumes the visual context provided by CLIP-retrieved objects is semantically related to the speech content and can improve ASR performance.

## Foundational Learning

- **Transformer-based encoder-decoder architecture**: Understanding how Whisper processes input and generates output is crucial for comprehending how prompt engineering can steer the model. *Quick check: What are the main components of Whisper's architecture and how do they interact during inference?*

- **Prompt engineering and zero-shot learning**: The paper's core contribution is demonstrating how task-specific prompts can adapt Whisper to unseen tasks without fine-tuning. *Quick check: How does modifying the prompt tokens influence the model's output, and what are the limitations of this approach?*

- **Multilingual speech processing and cross-lingual transfer**: Whisper's ability to perform zero-shot speech translation and code-switched ASR relies on its multilingual understanding, which is a key aspect of the paper's findings. *Quick check: How does training on multilingual data enable zero-shot performance on language pairs not seen during training?*

## Architecture Onboarding

- **Component map**: Log Mel spectrogram -> Transformer encoder -> Encoder features + Prompt tokens + Positional embeddings -> Transformer decoder -> Text output
- **Critical path**: During inference, the encoder processes the input audio to produce features, which are then combined with the prompt tokens and positional embeddings in the decoder to generate the output transcription or translation
- **Design tradeoffs**: Whisper trades off model size and multilingual coverage for performance on specific tasks. The paper shows that larger models generally perform better, but there are exceptions (e.g., visual prompting not helping multilingual models)
- **Failure signatures**: Poor performance on tasks not well-represented in the training data, sensitivity to prompt engineering (e.g., visual prompting hurting multilingual models), and biases (e.g., accent bias in code-switched ASR)
- **First 3 experiments**:
  1. Test the effectiveness of the proposed prompts on the three zero-shot tasks (AVSR, CS-ASR, ST) compared to default prompts
  2. Analyze the impact of prompt length and noise on AVSR performance across different Whisper model sizes
  3. Investigate the role of accent and language detection in CS-ASR performance and how the concat approach improves over the default prompt

## Open Questions the Paper Calls Out

### Open Question 1
What specific properties of Whisper models (e.g., size, multilinguality) affect their performance on audio-visual speech recognition (AVSR)? The authors observed that visual prompting improves the performance of English models and smaller multilingual models but hurts the performance of larger multilingual models, but cannot draw definitive conclusions because the larger English models are not available.

### Open Question 2
How does Whisper's language identification (LID) performance vary across different accents, and how does this affect its performance on code-switched speech recognition (CS-ASR)? The authors observed that Whisper's LID performance for detecting English is much worse on the SEAME dataset (Singaporean and Malaysian accented speech) than on the ASCEND dataset (Chinese accented speech), but did not conduct a comprehensive investigation of Whisper's LID performance across different accents.

### Open Question 3
What is the underlying mechanism that allows Whisper to perform English-to-X (En→X) speech translation (ST) using the <|asr|> task token instead of the <|st|> task token? The authors found that using the <|st|> task token leads Whisper to only output English text, while using the <|asr|> task token allows it to perform En→X ST, but did not investigate the latent space structure of Whisper to confirm their hypothesis.

## Limitations
- Lack of ablation studies and theoretical analysis to validate the proposed mechanisms
- Accent bias in CS-ASR not fully explained - unclear whether it reflects training data bias or fundamental limitations
- Only examines three specific tasks, leaving open whether findings generalize to other zero-shot applications

## Confidence
**High Confidence (90%+):**
- Whisper can be adapted to perform unseen tasks through prompt engineering without model fine-tuning
- Task-specific prompts improve performance on AVSR, CS-ASR, and ST compared to default prompts
- Larger Whisper models generally outperform smaller ones on these zero-shot tasks
- Visual prompting is effective for AVSR on English models but not multilingual models

**Medium Confidence (70-90%):**
- The concat approach improves CS-ASR performance by better handling accent and language identification
- Whisper's latent space contains cross-lingual semantic relationships enabling zero-shot ST
- CLIP-retrieved objects provide semantically relevant visual context for AVSR

**Low Confidence (50-70%):**
- The specific mechanisms by which prompt tokens influence decoder output distributions
- The exact nature of cross-lingual relationships in Whisper's latent space
- Why visual prompting hurts multilingual models specifically

## Next Checks
1. Conduct ablation studies varying prompt token positions, lengths, and noise levels systematically across all three tasks to identify which prompt components are essential versus redundant
2. Perform latent space analysis using techniques like similarity search, clustering, or probing classifiers to empirically verify whether semantically related cross-lingual representations exist in Whisper's encoder outputs
3. Test the proposed prompts on additional zero-shot tasks beyond AVSR, CS-ASR, and ST (such as speaker identification or emotion recognition) to determine whether the findings generalize or are task-specific