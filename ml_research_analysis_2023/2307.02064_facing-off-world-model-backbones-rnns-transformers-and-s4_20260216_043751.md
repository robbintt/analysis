---
ver: rpa2
title: 'Facing Off World Model Backbones: RNNs, Transformers, and S4'
arxiv_id: '2307.02064'
source_url: https://arxiv.org/abs/2307.02064
tags:
- latexit
- sha1
- base64
- s4wm
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares RNN, Transformer, and S4 backbones for world
  models in model-based reinforcement learning. The authors introduce S4WM, the first
  S4-based world model, and evaluate it against RNN and Transformer baselines on memory-demanding
  tasks including long-term imagination, context-dependent recall, reward prediction,
  and memory-based reasoning.
---

# Facing Off World Model Backbones: RNNs, Transformers, and S4

## Quick Facts
- arXiv ID: 2307.02064
- Source URL: https://arxiv.org/abs/2307.02064
- Reference count: 40
- Key outcome: S4WM outperforms Transformer-based world models in long-term memory tasks while being faster to train and more efficient at imagination

## Executive Summary
This paper compares RNN, Transformer, and S4 backbones for world models in model-based reinforcement learning. The authors introduce S4WM, the first S4-based world model, and evaluate it against RNN and Transformer baselines on memory-demanding tasks including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. S4WM outperforms Transformer-based models in long-term memory tasks while being faster to train and more efficient at imagination. The study provides insights into the strengths and weaknesses of different backbone architectures for world model learning.

## Method Summary
S4WM uses a stack of S4 blocks to encode history into embedding vectors for each time step, leveraging S4's structured state space modeling to capture long-range dependencies in latent space. The model employs a probabilistic latent variable framework with variational inference to efficiently generate high-dimensional image sequences through latent imagination. Compared to baselines RSSM-TBTT (RNN-based) and TSSM-XL (Transformer-based), S4WM is trained on the same datasets using AdamW optimizer with different learning rates and trained for 57-100 epochs depending on environment complexity.

## Key Results
- S4WM achieves lower mean squared error in long-term imagination compared to Transformer-based models
- S4WM trains faster than both RNN and Transformer baselines due to parallel computation capabilities
- S4WM demonstrates superior context-dependent recall accuracy in teleport environments
- RSSM-TBTT shows highest imagination throughput but struggles with long-term memory tasks
- TSSM-XL performs well on context-dependent operations but suffers from quadratic complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S4WM leverages S4's structured state space modeling to capture long-range dependencies in latent space, avoiding the quadratic complexity of Transformers.
- Mechanism: S4WM uses a stack of S4 blocks to encode the history (z<t, a≤t) into an embedding vector ht for each time step. This encoding is computed in parallel during training and sequentially during imagination, enabling efficient modeling of long-term dependencies.
- Core assumption: The latent space representation captures the essential information from high-dimensional image sequences, allowing S4 to effectively model long-range dependencies.
- Evidence anchors:
  - [abstract]: "S4WM outperforms Transformer-based world models in terms of long-term memory, while exhibiting greater efficiency during training and imagination."
  - [section]: "We propose S4WM, the first S4-based world model that can generate high-dimensional image sequences through latent imagination."
- Break condition: If the latent space representation fails to capture essential information, S4WM's performance will degrade, especially in tasks requiring long-term memory.

### Mechanism 2
- Claim: The probabilistic latent variable modeling framework enables S4WM to efficiently generate high-dimensional image sequences through latent imagination.
- Mechanism: S4WM models the observations and state transitions through a probabilistic generative process, using variational inference to learn the latent states. This allows the model to generate future observations by sampling from the learned latent space.
- Core assumption: The variational inference framework accurately captures the underlying distribution of the observations and state transitions.
- Evidence anchors:
  - [abstract]: "We propose S4WM, the first S4-based world model that can generate high-dimensional image sequences through latent imagination."
  - [section]: "We aim to model p(x1:T | x0, a1:T ), the distribution of future observations given the action sequence."
- Break condition: If the variational inference framework fails to accurately capture the underlying distribution, S4WM's generation quality will suffer.

### Mechanism 3
- Claim: The MLP added to each S4 block improves generation quality by refining the encoded history information.
- Mechanism: After the S4 blocks encode the history into an embedding vector ht, an MLP is applied to further process this information before using it to compute the sufficient statistics of the prior and likelihood.
- Core assumption: The MLP can effectively refine the encoded history information to improve generation quality.
- Evidence anchors:
  - [section]: "We find that adding the final MLP in each S4 block can improve generation quality."
  - [corpus]: Weak - no direct evidence in corpus neighbors.
- Break condition: If the MLP does not effectively refine the encoded history information, S4WM's generation quality may not improve or could even degrade.

## Foundational Learning

- Concept: State Space Models (SSMs) and their discretization
  - Why needed here: Understanding SSMs is crucial for grasping the S4 model's core mechanism and how it differs from RNNs and Transformers.
  - Quick check question: What is the key difference between continuous-time and discrete-time SSMs in terms of computation?

- Concept: Variational inference and evidence lower bound (ELBO)
  - Why needed here: S4WM uses variational inference to learn the latent states and maximize the ELBO, which is essential for its probabilistic latent variable modeling framework.
  - Quick check question: What is the purpose of the ELBO in variational inference, and how does it relate to the model's training objective?

- Concept: Transformer-XL and its cache mechanism
  - Why needed here: TSSM-XL, one of the baseline models, uses Transformer-XL to handle long sequences. Understanding its cache mechanism is important for comparing its performance with S4WM.
  - Quick check question: How does the cache mechanism in Transformer-XL enable it to model longer-term dependencies compared to the vanilla Transformer?

## Architecture Onboarding

- Component map:
  - CNN encoder: Encodes image observations into latent states
  - S4 blocks: Encode history (z<t, a≤t) into embedding vectors ht
  - MLP: Refines encoded history information in each S4 block
  - CNN decoder: Decodes latent states and embedding vectors into image observations
  - Variational inference framework: Learns latent states and maximizes ELBO

- Critical path:
  1. Encode image observation using CNN encoder
  2. Compute input to S4 blocks (gt = MLP(concat[zt-1, at]))
  3. Encode history using S4 blocks (h1:T, sT = S4Blocks(g1:T, s0))
  4. Compute prior and likelihood using MLP and CNN decoder
  5. Maximize ELBO using variational inference

- Design tradeoffs:
  - S4WM vs. TSSM-XL: S4WM offers better long-term memory and efficiency, but TSSM-XL may be better at context-dependent operations with a large cache.
  - S4WM vs. RSSM-TBTT: S4WM trains faster due to parallel computation, but RSSM-TBTT is more memory-efficient during training and has higher throughput during imagination.

- Failure signatures:
  - Poor generation quality: Check the latent space representation and variational inference framework
  - Slow training: Check the S4 block implementation and parallel computation
  - High memory usage: Check the CNN encoder/decoder and latent state size

- First 3 experiments:
  1. Train S4WM on a simple environment (e.g., Two Rooms) and evaluate long-term imagination quality
  2. Compare S4WM's performance with TSSM-XL and RSSM-TBTT on the Four Rooms environment
  3. Analyze S4WM's context-dependent recall ability on the teleport environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can S4WM be scaled to handle more complex and stochastic environments?
- Basis in paper: [explicit] The authors mention that all models struggle in the Ten Rooms environment and note this likely reaches the sequence modeling limits of the S4 model. They state "we leave the investigation of more sophisticated model architectures to future work."
- Why unresolved: The paper focuses on relatively simple and deterministic environments to simplify evaluation. The performance degradation in more complex environments suggests current architectures may be insufficient.
- What evidence would resolve it: Successful application of S4WM or its variants to benchmark environments with high visual complexity and stochastic dynamics, such as Atari games or continuous control tasks with visual observations.

### Open Question 2
- Question: What is the optimal architectural design for combining the strengths of S4 and Transformers in world models?
- Basis in paper: [explicit] The authors note that S4 and Transformers have complementary strengths - S4 excels at long-range dependencies while Transformers are better at context-dependent operations. They reference hybrid architectures proposed in other domains.
- Why unresolved: The paper compares pure S4WM, pure Transformer (TSSM-XL), and RNN (RSSM-TBTT) architectures but doesn't explore hybrid approaches specifically for world modeling.
- What evidence would resolve it: Performance comparison showing a hybrid S4-Transformer architecture outperforming both pure S4WM and TSSM-XL on the memory-demanding tasks evaluated in this paper.

### Open Question 3
- Question: How does the trade-off between training speed and imagination speed vary across different world model backbones?
- Basis in paper: [explicit] The authors provide detailed speed comparisons showing S4WM trains fastest while RSSM-TBTT has highest imagination throughput. They note S4WM's multi-layered recurrence structure slows imagination performance.
- Why unresolved: While speed comparisons are provided, the paper doesn't systematically explore how different architectural choices (number of S4 layers, Transformer depth, RNN truncation length) affect this trade-off.
- What evidence would resolve it: A comprehensive ablation study varying architectural parameters and measuring both training and imagination speeds across multiple environment complexities, identifying optimal configurations for different use cases.

## Limitations
- Claims are primarily supported by controlled experiments on synthetic memory tasks rather than real-world applications
- Performance comparisons may be influenced by implementation details, hyperparameters, and specific environment choices
- Paper doesn't extensively explore the impact of varying latent space dimensions or trade-offs between memory capacity and computational efficiency

## Confidence
- High confidence in S4WM's mechanism for leveraging S4's structured state space modeling for long-range dependency capture
- Medium confidence in claims about S4WM's efficiency advantages during training and imagination
- Low confidence in generalizability of S4WM's performance to real-world MBRL applications beyond controlled synthetic environments

## Next Checks
1. Evaluate S4WM on a real-world robotics or control task (e.g., simulated robotic manipulation or navigation) to assess its performance beyond synthetic memory tasks
2. Conduct an ablation study to quantify the contribution of each component (S4 blocks, MLP refinements, variational inference) to S4WM's overall performance
3. Perform a comprehensive sensitivity analysis on key hyperparameters (e.g., latent space dimensions, learning rates, KL balancing) to understand their impact on S4WM's training stability and performance