---
ver: rpa2
title: Bandit-Driven Batch Selection for Robust Learning under Label Noise
arxiv_id: '2311.00096'
source_url: https://arxiv.org/abs/2311.00096
tags:
- learning
- selection
- instances
- batch
- exp3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a combinatorial bandit-based approach for
  batch selection in SGD training under label noise. It adapts the Follow-the-Perturbed-Leader
  (FPL) algorithm for semi-bandit feedback to dynamically select informative batches,
  improving upon instance-level methods like Exp3 and Active Bias.
---

# Bandit-Driven Batch Selection for Robust Learning under Label Noise

## Quick Facts
- arXiv ID: 2311.00096
- Source URL: https://arxiv.org/abs/2311.00096
- Reference count: 40
- Key outcome: FPL achieves 8.37% test error at 10% label noise on CIFAR-10, outperforming Active Bias (9.17%) and Exp3.

## Executive Summary
This paper introduces a combinatorial bandit-based approach for batch selection in SGD training under label noise. The authors adapt the Follow-the-Perturbed-Leader (FPL) algorithm with semi-bandit feedback to dynamically select informative batches, improving upon instance-level methods like Exp3 and Active Bias. Experiments on CIFAR-10 with DenseNet show FPL achieves significantly lower test error than baselines across label corruption levels, demonstrating superior ability to balance exploration and exploitation at the batch level while avoiding overfitting to a narrow subset of instances.

## Method Summary
The paper proposes FPL for combinatorial bandit-based batch selection in noisy deep learning. At each iteration, FPL adds perturbation noise to instance weights, selects the top-m scoring instances as a batch, and updates weights based on semi-bandit feedback using geometric re-sampling. The method employs prediction uncertainty (variance over recent history) as a weight metric to identify clean vs. noisy instances. FPL operates with minimal computational overhead and demonstrates strong scalability and hyperparameter stability across different label noise levels on CIFAR-10.

## Key Results
- FPL achieves 8.37% test error at 10% label noise on CIFAR-10, outperforming Active Bias (9.17%) and Exp3.
- The method maintains performance across label corruption levels from 0% to 50% with minimal hyperparameter tuning.
- FPL adds only 20-40% computational overhead through geometric re-sampling while improving sample efficiency.

## Why This Works (Mechanism)

### Mechanism 1
FPL dynamically balances exploration and exploitation by perturbing weights, enabling effective batch-level selection in noisy settings. At each iteration, FPL adds perturbation noise to instance weights, selects the top-m scoring instances as a batch, and updates weights based on semi-bandit feedback. This avoids the combinatorial explosion of monitoring all possible m-sets while still leveraging batch-level information. The core assumption is that instance-level reward estimates (via semi-bandit feedback) are sufficient to guide batch selection toward informative, low-noise samples.

### Mechanism 2
The uncertainty-based weight metric (prediction variance over recent history) isolates clean instances and suppresses mislabeled ones during selection. For each instance, prediction variance is computed over the last 10 predictions; lower variance indicates confident, likely correct labels. FPL uses these weights to favor uncertain (potentially mislabeled) instances early, then shifts focus to stable, clean ones as training progresses. The core assumption is that prediction uncertainty is correlated with label correctness and remains stable across epochs when averaged over recent history.

### Mechanism 3
Geometric re-sampling (GR) provides unbiased importance-weighted reward estimates without full combinatorial enumeration, enabling scalable semi-bandit learning. After selecting a batch, GR samples each chosen instance according to a geometric distribution to estimate 1/p_i, then bounds the reward estimate by M to trade accuracy for efficiency. This approximates the full semi-bandit update without evaluating all m-sets. The core assumption is that the geometric re-sampling estimator converges quickly enough that bounded rewards still capture relative instance utility.

## Foundational Learning

- **Combinatorial bandits and semi-bandit feedback**: Why needed here? Standard multi-armed bandits select single instances; batch selection requires choosing m instances jointly without evaluating all combinations. Quick check question: What is the computational complexity of enumerating all m-subsets from n instances, and why is semi-bandit feedback preferable?

- **Prediction uncertainty as a proxy for label noise**: Why needed here? Label noise corrupts gradients; uncertainty metrics identify likely clean instances to stabilize training. Quick check question: How does prediction variance over a short history correlate with label correctness, and what assumptions underlie this link?

- **Follow-the-Perturbed-Leader (FPL) algorithm**: Why needed here? FPL efficiently approximates Exp3 for combinatorial actions via weight perturbations and geometric re-sampling, avoiding explicit probability storage. Quick check question: How does the Fréchet(2) perturbation distribution affect exploration-exploitation balance compared to Exp(1)?

## Architecture Onboarding

- **Component map**: Uncertainty metric calculator -> Weight vector -> Perturbed argmax -> Batch sampler -> DenseNet trainer -> Reward estimator -> Geometric re-sampling -> Weight update
- **Critical path**: Uncertainty metric → weight vector → perturbed argmax → batch sampling → model forward/backward → reward estimation → weight update
- **Design tradeoffs**: FPL trades off exploration (via perturbations) for exploitation (via high weights); GR trades off accuracy (bounded rewards) for efficiency (fewer samples)
- **Failure signatures**: High variance in test error across runs suggests poor weight estimation; persistent overfitting to a small subset indicates over-exploration
- **First 3 experiments**:
  1. Run FPL with η=0.3, β=20, shape=0.45 on CIFAR-10 (10% noise) and compare final test error to uniform sampling
  2. Vary M in GR (100, 500, 1000) and measure trade-off between wall-clock time and test accuracy
  3. Replace uncertainty metric with loss-based metric and evaluate impact on clean vs. noisy instance selection

## Open Questions the Paper Calls Out
- How does the choice of weight metric affect the performance of bandit-based batch selection methods in deep learning under label noise?
- How does the computational overhead of the geometric re-sampling step in FPL scale with increasing batch size and dataset size?

## Limitations
- Empirical evaluation limited to DenseNet-40 architecture, may not generalize to other architectures
- FPL's sensitivity to hyperparameters (η, β, shape) is acknowledged but not fully characterized across diverse datasets
- Bounded rewards (M=1) in geometric re-sampling could introduce bias in reward estimation, particularly in early training phases

## Confidence
- **High confidence**: FPL's mechanism for balancing exploration/exploitation at the batch level is well-supported by the described algorithm and theoretical foundations
- **Medium confidence**: The claim that FPL outperforms Exp3 and Active Bias is supported by CIFAR-10 experiments, but results on other datasets or architectures are not provided
- **Low confidence**: The assertion that geometric re-sampling provides unbiased estimates requires more rigorous validation, as the bounded reward mechanism may introduce systematic bias

## Next Checks
1. Replicate experiments using ResNet-18 on CIFAR-10 and CIFAR-100 to assess FPL's performance across architectures
2. Evaluate FPL on SVHN and Tiny ImageNet to verify robustness beyond CIFAR-10
3. Compare test error rates using exact semi-bandit updates (full combinatorial enumeration) versus GR to quantify bias from bounded rewards