---
ver: rpa2
title: 'Towards Effective Human-AI Decision-Making: The Role of Human Learning in
  Appropriate Reliance on AI Advice'
arxiv_id: '2310.02108'
source_url: https://arxiv.org/abs/2310.02108
tags:
- learning
- explanations
- human
- reliance
- human-ai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the role of human learning in achieving
  appropriate reliance on AI advice during decision-making. Through a behavioral experiment
  with 100 participants, researchers examined how example-based explanations influence
  learning and, in turn, appropriateness of reliance (AoR).
---

# Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice

## Quick Facts
- **arXiv ID**: 2310.02108
- **Source URL**: https://arxiv.org/abs/2310.02108
- **Reference count**: 0
- **Primary result**: Example-based explanations significantly increase learning, which strongly influences relative self-reliance (RSR) in human-AI decision-making

## Executive Summary
This study investigates how human learning mediates appropriate reliance on AI advice during decision-making. Through a behavioral experiment with 100 participants, researchers found that example-based explanations significantly increased learning compared to baseline AI advice. Learning had a strong positive effect on relative self-reliance (RSR), enabling humans to correctly override incorrect AI advice. However, sufficient learning was necessary for learning to positively influence relative AI-reliance (RAIR), the ability to correctly follow accurate AI advice. These findings extend our understanding of appropriate reliance by incorporating learning as a key mediator and provide insights for designing effective human-AI decision-making systems.

## Method Summary
The study used a behavioral experiment with 100 participants randomly assigned to baseline (AI advice only) or XAI (AI advice + example-based explanations) conditions. Participants completed two knowledge tests and a main task of 16 sequential image classifications using the Caltech-UCSD Birds-200-2011 dataset filtered to 216 black-colored bird images across 4 classes. A pre-trained ResNet50 model was fine-tuned to 87.96% accuracy and used to generate AI predictions and example-based explanations. Appropriateness of Reliance (AoR) was measured through Relative Self-Reliance (RSR) and Relative AI-Reliance (RAIR) metrics, with learning measured as the difference in classification accuracy between knowledge tests.

## Key Results
- Example-based explanations significantly increased learning compared to baseline condition
- Learning had a strong positive effect on relative self-reliance (RSR), enabling correct overrides of incorrect AI advice
- Sufficient learning was necessary for learning to positively influence relative AI-reliance (RAIR), indicating a threshold effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Example-based explanations increase human learning during AI-assisted decision-making.
- Mechanism: Example-based explanations provide concrete, labeled examples from training data that illustrate patterns the AI uses, helping humans infer causal relationships and improve task-specific knowledge.
- Core assumption: Humans can derive generalizable patterns from individual examples when they are contrastive and grounded in ground-truth labels.
- Evidence anchors:
  - [abstract]: "example-based explanations significantly increased learning compared to the baseline condition"
  - [section]: "Example-based explanations have the potential to increase knowledge even if the AI is wrong"

### Mechanism 2
- Claim: Learning mediates the effect of explanations on relative self-reliance (RSR).
- Mechanism: As humans learn task-specific patterns through explanations, they become better at identifying when AI predictions are incorrect and can override them appropriately.
- Core assumption: Improved task knowledge directly translates to better validation capability against incorrect AI advice.
- Evidence anchors:
  - [abstract]: "Learning was found to have a strong positive effect on relative self-reliance (RSR)"

### Mechanism 3
- Claim: Learning only improves relative AI-reliance (RAIR) when sufficient learning threshold is reached.
- Mechanism: Humans need adequate domain knowledge to recognize when they lack expertise and should follow correct AI advice; without sufficient learning, automation bias dominates.
- Core assumption: There exists a critical learning threshold below which humans over-rely on their own judgment even when incorrect.
- Evidence anchors:
  - [abstract]: "sufficient learning is necessary for learning to positively influence relative AI-reliance (RAIR)"

## Foundational Learning

- Concept: Mediation analysis in structural equation modeling
  - Why needed here: To test whether learning mediates the relationship between explanations and appropriateness of reliance
  - Quick check question: What does it mean if the direct path is not significant but the indirect path through learning is significant?

- Concept: Sequential task design for measuring appropriateness of reliance
  - Why needed here: Required to capture initial human decisions, AI advice, and final decisions to compute RSR and RAIR metrics
  - Quick check question: Why can't we measure AoR in a single-step decision task?

- Concept: Contrastive vs. normative examples in XAI
  - Why needed here: Example-based explanations use both types to help humans understand prediction boundaries
  - Quick check question: How do comparative examples differ from normative examples in their impact on learning?

## Architecture Onboarding

- Component map: Participant interface → Knowledge test 1 → Tutorial → Main task (16 instances) → Knowledge test 2 → Demographic survey
- Critical path: Ensure sequential decision flow where humans make initial classification, receive AI advice, then can revise decision
- Design tradeoffs: Tutorial provides minimal training to simulate real-world onboarding vs. full training that might inflate performance
- Failure signatures: No cases where human was initially wrong and AI was correct → undefined RAIR; negative learning values indicating unlearning
- First 3 experiments:
  1. Test different example-based explanation frequencies (1 vs. 2 vs. 3 examples per instance)
  2. Vary the complexity of bird classes (simple color-based vs. subtle morphological differences)
  3. Compare example-based explanations against feature-based or rule-based explanations for learning impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we disentangle the effects of learning and automation bias on appropriateness of reliance (AoR) in human-AI decision-making?
- Basis in paper: The paper mentions that "the positive effects of learning we observe could be diminished by automation bias, resulting in a non-significant net impact on AoR" and that "Future research needs to disentangle the effects."
- Why unresolved: The study found a significant effect of learning on relative self-reliance (RSR) but not on relative AI-reliance (RAIR), which the authors attribute to potential confounding effects of automation bias.
- What evidence would resolve it: Controlled experiments that manipulate both learning opportunities and automation bias tendencies would help isolate these effects.

### Open Question 2
- Question: What design features can effectively improve human learning during human-AI decision-making beyond example-based explanations?
- Basis in paper: The paper states that "Future research should evaluate these potential design features to provide practitioners with a toolkit for the effective use of AI" and notes that "Future work should evaluate different example techniques."
- Why unresolved: While the study shows that example-based explanations can improve learning and RSR, it only examined one type of explanation technique.
- What evidence would resolve it: Comparative studies testing multiple explanation types across different tasks and domains would identify which design features most effectively promote learning.

### Open Question 3
- Question: Does learning during human-AI decision-making lead to an equilibrium where humans and AI have aligned knowledge, or is continuous learning necessary to maintain complementary capabilities?
- Basis in paper: The discussion section raises this question: "The question now is whether humans and AIs can reach an equilibrium where the knowledge of both has aligned such that no further learning is possible."
- Why unresolved: The paper presents this as a theoretical consideration but does not empirically investigate whether learning reaches a plateau or continues indefinitely.
- What evidence would resolve it: Longitudinal studies tracking learning curves and knowledge alignment over extended periods would reveal whether learning stabilizes or continues.

## Limitations

- Limited generalization due to single task (bird classification) and explanation type (example-based)
- Small sample size (100 participants) may lack power to detect smaller effects or interactions
- Threshold effect for RAIR improvements is based on subgroup analysis with potential alternative explanations

## Confidence

- **High Confidence**: Example-based explanations increase learning compared to baseline
- **Medium Confidence**: Learning mediates the effect on relative self-reliance (RSR)
- **Low Confidence**: Threshold effect for learning's impact on relative AI-reliance (RAIR)

## Next Checks

1. **Cross-Domain Replication**: Test whether example-based explanations improve learning and AoR in a different domain (e.g., medical diagnosis or legal document review) to assess generalizability.

2. **Learning Threshold Validation**: Conduct a larger study with finer-grained learning measurements to determine if there's a true threshold effect or if the relationship between learning and RAIR is more continuous.

3. **Longitudinal Learning Assessment**: Measure learning immediately after the experiment versus after a delay to determine if the learning effects are durable and translate to real-world decision-making scenarios.