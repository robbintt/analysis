---
ver: rpa2
title: 'SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive
  Noise'
arxiv_id: '2305.16491'
source_url: https://arxiv.org/abs/2305.16491
tags:
- page
- first
- bound
- then
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of decomposing multivariate time
  series into deterministic non-stationary components and stationary stochastic components,
  where the latter is modeled as an autoregressive (AR) process. The proposed SAMoSSA
  algorithm extends multivariate singular spectrum analysis (mSSA) by incorporating
  AR noise structure in a two-stage procedure: first, mSSA estimates the non-stationary
  components despite the presence of AR noise; second, an AR model is learned from
  the residuals.'
---

# SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise

## Quick Facts
- arXiv ID: 2305.16491
- Source URL: https://arxiv.org/abs/2305.16491
- Reference count: 40
- This paper proposes SAMoSSA, a two-stage algorithm that extends mSSA to handle AR noise, achieving 5-37% improvement in forecasting accuracy over existing methods.

## Executive Summary
This paper addresses the challenge of decomposing multivariate time series into deterministic non-stationary components and stationary stochastic components modeled as autoregressive (AR) processes. The authors propose SAMoSSA, a two-stage algorithm that first applies mSSA to estimate non-stationary components despite AR noise presence, then learns AR parameters from residuals. Theoretical guarantees are established for estimation and forecasting errors, with the out-of-sample forecasting error scaling as 1/T + 1/√(NT). Empirical results demonstrate consistent improvements over mSSA, ARIMA, and Prophet across various benchmark datasets.

## Method Summary
SAMoSSA is a two-stage algorithm that extends multivariate singular spectrum analysis (mSSA) to handle autoregressive noise structure. First, it constructs a stacked Page matrix representation of the multivariate time series and applies Hard Singular Value Thresholding (HSVT) to extract low-rank non-stationary components. Second, it estimates AR parameters from the residuals using ordinary least squares (OLS). The method leverages the spatio-temporal factor model assumption, where non-stationary components have finite-rank Page matrix representations, enabling separation of non-stationary and stationary components. Theoretical guarantees are provided for estimation error, AR parameter identification, and forecasting accuracy.

## Key Results
- SAMoSSA achieves 5-37% improvement in forecasting accuracy over mSSA, ARIMA, and Prophet across benchmark datasets
- Theoretical error bounds show estimation error scales as 1/√(NT) and forecasting error scales as 1/T + 1/√(NT)
- The algorithm successfully handles AR noise structure that confounds traditional mSSA methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage SAMoSSA algorithm accurately estimates non-stationary components in the presence of AR noise by exploiting the low-rank structure of the Page matrix representation.
- Mechanism: SAMoSSA first applies mSSA to estimate the non-stationary components despite AR noise, then learns the AR parameters from the residuals. The low-rank structure of the stacked Page matrix under the spatio-temporal factor model enables accurate separation of non-stationary and stationary components.
- Core assumption: The deterministic non-stationary components can be modeled as linear combinations of a small number of fundamental time series with finite-rank Page matrix representations (Assumptions 2.1 and 2.2).
- Evidence anchors:
  - [abstract]: "we establish desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correlated stationary AR component"
  - [section]: "Proposition 2.1 (Proposition 2 in [3] ). Let Assumptions 2.1 and 2.2 hold. Then for any L ≤ ⌊√T ⌋ with any T ≥ 1, t0 > 0, the rank of the Page matrix Z(fn, L, T, t0) for n ∈ [N] is at most R × G"
- Break condition: If the non-stationary components violate the spatio-temporal factor model assumptions or the AR noise structure is non-stationary, the low-rank property fails and SAMoSSA's accuracy degrades.

### Mechanism 2
- Claim: SAMoSSA provides finite-sample consistency for forecasting by combining accurate non-stationary component estimation with AR parameter identification under bounded perturbations.
- Mechanism: After estimating non-stationary components, SAMoSSA uses OLS to learn AR parameters from residuals. The error bounds show the forecasting error scales as ~1/T + 1/√(NT), combining estimation errors from both stages.
- Core assumption: The AR processes are stable and stationary (Assumption 2.3), and the per-step noise is sub-gaussian (Assumption 2.4).
- Evidence anchors:
  - [abstract]: "we provide a finite-sample forecasting consistency bound for the proposed algorithm, SAMoSSA... the out-of-sample forecasting error for SAMoSSA for the T time-steps ahead scales as ~ 1/T + 1/√(NT) with high probability"
  - [section]: "Theorem 4.4. Let the conditions of Theorem 4.2 and Assumption 4.2 hold. Then, with probability of at least 1 − c/T 10 ForErr(N, T ) ≤ ˜CG3R3p2σ6 x pσ2 log (T ) T + GRσ6 x min {σ2, σ4} log(N T)2 N T !"
- Break condition: If the AR processes are unstable or the noise is heavy-tailed rather than sub-gaussian, the finite-sample bounds no longer hold and forecasting performance deteriorates.

### Mechanism 3
- Claim: SAMoSSA outperforms existing methods by explicitly modeling the AR noise structure rather than treating residuals as i.i.d.
- Mechanism: By learning the AR parameters from residuals after non-stationary component estimation, SAMoSSA captures the temporal dependencies in the noise, leading to improved forecasting accuracy compared to methods like mSSA that ignore AR structure.
- Core assumption: The stationary component can be well-approximated by a finite-order AR process.
- Evidence anchors:
  - [abstract]: "SAMoSSA's ability to account for AR noise structure yields improvements ranging from 5% to 37% across various benchmark datasets"
  - [section]: "Through representative empirical studies, we validate the superior performance of SAMoSSA compared to existing baselines"
- Break condition: If the stationary component has complex structure that cannot be well-approximated by a finite-order AR process, or if the AR parameters are difficult to estimate accurately, the performance gains may be limited.

## Foundational Learning

- Concept: Spatio-temporal factor model and finite-rank Page matrix representation
  - Why needed here: This is the core structural assumption that enables SAMoSSA to separate non-stationary and stationary components using matrix methods
  - Quick check question: What are the two key assumptions (spatial and temporal structure) required for the low-rank property of the Page matrix?

- Concept: Sub-gaussian random variables and concentration inequalities
  - Why needed here: Used to establish high-probability bounds on estimation and forecasting errors
  - Quick check question: What is the key property of sub-gaussian random variables that makes them useful for concentration inequalities?

- Concept: Singular Value Decomposition (SVD) and Hard Singular Value Thresholding (HSVT)
  - Why needed here: Core algorithmic tools for separating low-rank non-stationary components from the observed time series
  - Quick check question: How does HSVT with parameter k work to extract the top k singular components?

## Architecture Onboarding

- Component map: Observations → Page matrix construction → SVD/HSVT → Non-stationary estimation → Residual computation → AR parameter estimation → Forecasting
- Critical path: Data → Page matrix construction → SVD/HSVT → Non-stationary estimation → Residual computation → AR parameter estimation → Forecasting
- Design tradeoffs:
  - Choice of Page matrix width L: Larger L provides more data for estimation but may violate theoretical assumptions
  - Choice of rank k for HSVT: Higher k captures more non-stationary components but risks including noise
  - Choice of AR order p: Higher p can model more complex noise but requires more parameters and data
- Failure signatures:
  - Poor forecasting performance despite high R² on training data: May indicate overfitting or inappropriate AR order
  - Large residuals after Stage 1: May indicate violation of spatio-temporal factor model assumptions
  - Unstable AR parameter estimates: May indicate insufficient data or inappropriate model specification
- First 3 experiments:
  1. Generate synthetic data with known non-stationary components and AR noise, apply SAMoSSA, compare estimated vs true components
  2. Vary the Page matrix width L and rank k, measure impact on estimation accuracy and computational cost
  3. Apply SAMoSSA to real-world dataset, compare forecasting performance against baseline methods (mSSA, ARIMA, Prophet) using R² score

## Open Questions the Paper Calls Out

- Can the SAMoSSA framework be extended to handle non-stationary stochastic processes, such as random walks or unit root processes?
  - Basis in paper: [inferred] The paper explicitly states that its model only considers stationary stochastic processes and acknowledges this as a limitation.
  - Why unresolved: The authors note that investigating the inclusion of non-stationary stochastic trends and their interplay with SSA literature represents a compelling direction for future work, but do not provide a solution.
  - What evidence would resolve it: Developing a modified version of SAMoSSA that can handle non-stationary stochastic components and demonstrating its effectiveness through theoretical analysis and empirical validation.

- How would SAMoSSA perform when there are interactions among the N stationary AR processes, potentially through a vector AR (VAR) model?
  - Basis in paper: [inferred] The authors mention that their model assumes non-interaction between the N stationary processes and suggest that examining a setting with interactions through a VAR model is a compelling direction for future work.
  - Why unresolved: The paper does not explore or analyze the case where the N AR processes interact with each other.
  - What evidence would resolve it: Extending SAMoSSA to handle VAR models and comparing its performance to the current version on datasets where interactions among time series are present.

- What are the theoretical guarantees for SAMoSSA in approximate low-rank settings or scenarios with incomplete data?
  - Basis in paper: [explicit] The authors mention that their results can be readily adapted to accommodate approximate low-rank settings and scenarios with incomplete data, but do not discuss these settings in detail.
  - Why unresolved: The paper focuses on the core contributions and does not provide detailed analysis for these more general settings.
  - What evidence would resolve it: Providing theoretical guarantees and empirical results for SAMoSSA in approximate low-rank settings and with incomplete data, comparing its performance to existing methods in these scenarios.

## Limitations
- The spatio-temporal factor model assumption may not hold for complex real-world systems with non-linear relationships
- The requirement for stable and stationary AR processes excludes applications with regime changes or non-stationary noise
- The sub-gaussian noise assumption may not capture heavy-tailed phenomena common in financial or network traffic data

## Confidence
- **High confidence**: The two-stage algorithmic framework is well-specified and the theoretical error bounds are rigorously derived under stated assumptions. The mechanism by which SAMoSSA separates non-stationary and stationary components through low-rank matrix decomposition is mathematically sound.
- **Medium confidence**: The empirical validation shows consistent improvements over baselines, but the relative performance gains may be dataset-dependent. The comparison against only three baseline methods (mSSA, ARIMA, Prophet) provides limited context for the state of the art in multivariate time series forecasting.
- **Low confidence**: The paper does not thoroughly investigate hyperparameter sensitivity or provide guidance for hyperparameter selection in scenarios where theoretical assumptions are violated. The practical performance when assumptions are moderately violated remains unclear.

## Next Checks
1. **Assumption violation testing**: Systematically evaluate SAMoSSA performance when key assumptions are violated (e.g., unstable AR processes, non-linear relationships between components, heavy-tailed noise) to establish robustness boundaries.
2. **Hyperparameter sensitivity analysis**: Conduct comprehensive experiments varying Page matrix width L, rank k, and AR order p to identify stable operating regions and provide practical guidelines for hyperparameter selection.
3. **Real-world stress testing**: Apply SAMoSSA to datasets with known structural breaks, regime changes, or non-linear dynamics to assess performance in realistic scenarios where theoretical assumptions are likely violated.