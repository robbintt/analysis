---
ver: rpa2
title: 'Counterfactual Situation Testing: Uncovering Discrimination under Fairness
  given the Difference'
arxiv_id: '2302.11944'
source_url: https://arxiv.org/abs/2302.11944
tags:
- discrimination
- counterfactual
- individual
- protected
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents counterfactual situation testing (CST), a\
  \ framework for detecting discrimination in classifiers that uses causal knowledge\
  \ to construct test groups from a complainant\u2019s counterfactual. CST extends\
  \ situation testing by operationalizing fairness given the difference\u2014accounting\
  \ for how protected attributes affect seemingly neutral features\u2014rather than\
  \ assuming ceteris paribus comparisons."
---

# Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference

## Quick Facts
- arXiv ID: 2302.11944
- Source URL: https://arxiv.org/abs/2302.11944
- Reference count: 40
- One-line primary result: CST detects more individual discrimination cases than situation testing by operationalizing fairness given the difference using counterfactual reasoning.

## Executive Summary
This paper presents counterfactual situation testing (CST), a framework for detecting discrimination in classifiers that uses causal knowledge to construct test groups from a complainant's counterfactual. CST extends situation testing by operationalizing fairness given the differenceâ€”accounting for how protected attributes affect seemingly neutral featuresâ€”rather than assuming ceteris paribus comparisons. Experiments on synthetic and real datasets (loan applications and law school admissions) show CST uncovers more individual discrimination cases than situation testing, even when the classifier satisfies counterfactual fairness. CST also equips counterfactual fairness with confidence intervals and detects counterfactually fair yet discriminatory cases.

## Method Summary
CST constructs control and test groups using a k-NN algorithm with a distance function, where the test group is built on the complainant's counterfactual generated using causal knowledge from a structural causal model. The framework compares decision outcomes between groups using the difference in proportion of negative outcomes, providing statistical inference through Wald confidence intervals. CST operationalizes fairness given the difference by accounting for how protected attributes causally influence seemingly neutral features, rather than assuming these features remain constant during comparison.

## Key Results
- CST detects more individual discrimination cases than standard situation testing in both synthetic and real-world datasets
- CST identifies counterfactually fair yet discriminatory cases that traditional counterfactual fairness methods miss
- The framework successfully handles multiple and intersectional discrimination through separate counterfactual datasets for each protected attribute

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CST detects more individual discrimination cases than standard situation testing by operationalizing fairness given the difference.
- Mechanism: CST builds the test group using a complainant's counterfactual instead of the complainant itself, accounting for how protected attributes affect seemingly neutral features.
- Core assumption: The structural causal model accurately captures the causal relationships between protected attributes and other features.
- Evidence anchors:
  - [abstract] "CST extends the legally-grounded situation testing of Thanh et al. [62] by operationalizing the notion of fairness given the difference using counterfactual reasoning."
  - [section] "The choice of the test search centers is what sets CST apart from ST. ST performs an idealized comparison... CST, conversely, performs a more flexible comparison under fairness given the difference."
- Break condition: If the SCM does not accurately represent the true causal relationships, the counterfactuals will not properly reflect the effects of protected attributes.

### Mechanism 2
- Claim: CST equips counterfactual fairness with confidence intervals and detects counterfactually fair yet discriminatory cases.
- Mechanism: By comparing multiple similar instances around the complainant, CST provides statistical inference on the difference in decision outcomes between control and test groups.
- Core assumption: The differences in decision outcomes between control and test groups follow an asymptotically normal distribution.
- Evidence anchors:
  - [abstract] "CST also equips counterfactual fairness with confidence intervals and detects counterfactually fair yet discriminatory cases."
  - [section] "The confidence interval (5) responds to the hypothesis that there is individual discrimination, providing a measure of certainty on Î”ð‘ through a range of possible values."
- Break condition: If the distribution of differences is not normal or the sample size is too small, the confidence intervals may not be valid.

### Mechanism 3
- Claim: CST addresses multiple and intersectional discrimination by operationalizing separate counterfactual datasets for each protected attribute.
- Mechanism: For multiple discrimination, CST runs separate tests for each protected attribute and looks for cases where discrimination is detected across all runs. For intersectional discrimination, CST creates a new protected attribute representing the conjunction of all protected attributes and runs a single test.
- Core assumption: The counterfactuals for intersectional discrimination can be accurately generated by setting all protected attributes to their non-protected values simultaneously.
- Evidence anchors:
  - [abstract] "CST can account operationally for both scenarios" (multiple and intersectional discrimination).
  - [section] "Under intersectional discrimination, we work as with|ð´| = 1 but look at the intersectional protected status."
- Break condition: If the causal relationships between protected attributes are not properly captured, the intersectional counterfactuals may not accurately represent the counterfactual world.

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: SCMs are required to generate counterfactuals that reflect how changing protected attributes affects other features.
  - Quick check question: What are the three key assumptions required for generating valid counterfactuals from an SCM?

- Concept: Counterfactual Fairness
  - Why needed here: CST builds upon the concept of counterfactual fairness by providing an actionable extension with confidence intervals.
  - Quick check question: How does CST differ from counterfactual fairness in terms of the instances used for comparison?

- Concept: Wald Confidence Intervals
  - Why needed here: Wald confidence intervals are used to provide statistical inference on the difference in decision outcomes between control and test groups.
  - Quick check question: What is the formula for the Wald confidence interval used in CST?

## Architecture Onboarding

- Component map: Dataset D -> SCM M -> Counterfactual generator -> k-NN algorithm -> Control and test groups -> Decision outcome comparison -> Confidence intervals
- Critical path: Generate counterfactual dataset â†’ Construct control and test groups â†’ Compare decision outcomes â†’ Calculate confidence intervals
- Design tradeoffs:
  - Distance function: Different distance functions may lead to different results in terms of detected discrimination cases.
  - k size: Larger k sizes may provide more stable results but may also dilute the similarity between instances.
  - Confidence level: Higher confidence levels provide more certainty but may also lead to fewer detected cases.
- Failure signatures:
  - If the SCM is incorrect, the counterfactuals will not properly reflect the effects of protected attributes.
  - If the dataset is biased, the detected discrimination cases may not be representative of the true population.
  - If the classifier is highly complex, the similarity between instances may not be well-captured by the distance function.
- First 3 experiments:
  1. Generate counterfactual dataset for a simple scenario with one protected attribute and verify that the counterfactuals reflect the expected changes in other features.
  2. Run CST on a synthetic dataset with known discrimination and verify that the detected cases match the expected results.
  3. Compare the results of CST with standard situation testing on a real-world dataset and analyze the differences in terms of detected discrimination cases.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the work:

1. How does the choice of distance function (d) impact the number of detected discrimination cases in CST?
2. How does CST perform when the structural causal model (SCM) contains hidden confounders?
3. How does CST scale to high-dimensional datasets with many attributes?

## Limitations
- The effectiveness of CST depends heavily on the accuracy of the structural causal model, which is difficult to validate in real-world scenarios.
- The framework assumes causal sufficiency, meaning no hidden confounders, which may not hold in practice.
- The computational complexity of generating counterfactuals for large datasets may limit practical applicability.

## Confidence
- Confidence in detecting more discrimination cases: Medium
- Confidence in statistical inference (confidence intervals): Medium-High
- Confidence in handling multiple and intersectional discrimination: Medium

Key uncertainties include generalizability across domains, robustness when causal assumptions are violated, computational complexity, and sensitivity to hyperparameter choices.

## Next Checks
1. Cross-dataset validation: Apply CST to multiple real-world datasets with known discrimination patterns to assess consistency of results and compare against ground truth when available.

2. Causal model sensitivity analysis: Systematically vary the structural causal model assumptions and measure the impact on detected discrimination cases to quantify robustness to model misspecification.

3. Scalability benchmarking: Evaluate CST's computational performance on increasingly large datasets to identify practical limits and optimize the counterfactual generation process for real-world deployment scenarios.