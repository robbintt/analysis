---
ver: rpa2
title: 'ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective
  Evaluation Model'
arxiv_id: '2311.01149'
source_url: https://arxiv.org/abs/2311.01149
tags:
- data
- text
- quality
- chinese
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new pipeline approach for extracting high-quality
  Chinese pre-training data from the web. The approach filters raw crawled web data
  using both handcrafted rules and a well-designed quality evaluation model.
---

# ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model

## Quick Facts
- arXiv ID: 2311.01149
- Source URL: https://arxiv.org/abs/2311.01149
- Reference count: 38
- This paper introduces a new pipeline approach for extracting high-quality Chinese pre-training data from the web, releasing a 1.4 TB dataset with quality scores and a 600 GB subset exceeding 90% quality by human evaluation.

## Executive Summary
This paper presents a comprehensive pipeline for extracting high-quality Chinese web text from CommonCrawl snapshots, addressing the challenge of preparing large-scale pre-training data for language models. The approach combines rule-based filtering with a BERT-based quality evaluation model to produce a dataset of 1.4 TB of Chinese text, each entry associated with a quality score. The authors also release a much cleaner 600 GB subset with quality exceeding 90% by human evaluation. The pipeline includes language identification, deduplication, sensitive content filtering, and a two-stage quality evaluation process using both BERT and FastText models.

## Method Summary
The method employs a two-stage pipeline: first, rule-based filtering removes explicit noise including non-Chinese text, duplicates, toxic content, and advertisements. Second, a quality evaluation model assigns scores to remaining texts using a BERT-based architecture trained on curated high-quality corpora and sampled low-quality web texts. The model is refined through self-training and then distilled into a FastText classifier for efficient inference. The final dataset includes quality scores enabling researchers to re-filter data according to desired quality thresholds.

## Key Results
- Released 1.4 TB Chinese web text dataset with quality scores
- Produced 600 GB subset with quality exceeding 90% by human evaluation
- Demonstrated effective combination of rule-based and ML-based filtering approaches
- Achieved efficient inference through BERT-to-FastText distillation while maintaining classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
Rule-based filtering alone is insufficient for extracting high-quality Chinese web text; an ML-based quality evaluation model is needed. The pipeline first removes explicit noise (e.g., toxic, ads, duplicates) using handcrafted rules, then applies a BERT-based model to assign a quality score to each remaining text. Texts are filtered based on this score. The core assumption is that after rule-based filtering, the dataset still contains a large proportion of low-quality but not explicitly noisy text.

### Mechanism 2
A two-stage BERT training process (pre-training + self-training) improves the quality evaluation model's robustness and reduces bias toward specific text types. Stage 1 trains BERTEval on curated high-quality texts and sampled low-quality web texts. Stage 2 uses self-training with pseudo-labels to refine the model on the CommonCrawl corpus, mitigating coarse-grained supervision limitations. The core assumption is that the CommonCrawl corpus contains both high-quality and low-quality texts, and the model can learn to distinguish them even with noisy labels.

### Mechanism 3
Knowledge distillation from BERTEval to FastText yields a faster, more resource-efficient classifier with near-equal performance. BERTEval is used to classify a large set of web texts into high-quality and low-quality groups. These labels train FastText, leveraging its efficiency for inference and reduced hardware requirements. The core assumption is that the FastText classifier can approximate BERTEval's performance on quality classification when trained on BERTEval's labeled outputs.

## Foundational Learning

- **Language Identification (LID)**: Used to extract monolingual Chinese data from multilingual web sources. Why needed: CommonCrawl contains text in many languages; only Chinese text is relevant. Quick check: How does the LID model distinguish between Chinese text and similar scripts (e.g., Japanese, Korean)?
- **Deduplication via hash-based inter-string comparison**: Removes duplicate pages or content that can bias model training. Why needed: Web data often contains duplicate pages or content. Quick check: What hash function or string similarity metric is used to detect duplicates at scale?
- **Quality scoring and thresholding**: Enables control over the trade-off between dataset size and quality. Why needed: The final dataset must be both large and high-quality. Quick check: How does changing the quality threshold affect dataset size and downstream model performance?

## Architecture Onboarding

- **Component map**: Data Ingestion -> Preparation Module (LID + deduplication) -> Preprocessing Module -> Quality Evaluation Module (BERTEval + FastText) -> Quality Control -> Final Dataset
- **Critical path**: Data Ingestion → Preparation → Preprocessing → Quality Evaluation → Quality Control → Final Dataset
- **Design tradeoffs**: Precision vs. recall in rule-based filtering; Model complexity vs. inference speed (BERTEval vs. FastText); Dataset size vs. quality (threshold choice)
- **Failure signatures**: Rule-based filters removing too much or too little data; Quality evaluation model mislabeling large portions of dataset; Human evaluators rejecting majority of sampled texts
- **First 3 experiments**: 1) Run pipeline on small CommonCrawl snapshot and measure data reduction at each stage; 2) Evaluate BERTEval on manually labeled validation set to confirm classification accuracy; 3) Compare inference speed and memory usage of BERTEval vs. FastText on same data batch

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality evaluation model's performance vary across different Chinese language variants (e.g., Simplified vs Traditional Chinese) within the dataset? The paper mentions filtering out traditional Chinese characters and using simplified Chinese as the target, but does not analyze the model's performance on different variants. Detailed breakdown of evaluation model performance metrics for Simplified vs Traditional Chinese texts, and analysis of any systematic quality score differences between variants would resolve this.

### Open Question 2
What is the impact of varying quality thresholds on downstream LLM performance metrics (e.g., perplexity, accuracy on downstream tasks)? The paper releases data with quality scores and a cleaner subset but does not evaluate how different quality thresholds affect model training outcomes. Comparative studies training LLMs on datasets filtered with different quality thresholds, measuring downstream task performance and efficiency would resolve this.

### Open Question 3
How does the proposed EvalWeb tool-chain compare to domain-specific filtering approaches (e.g., legal, medical, or technical text extraction)? The paper introduces a general-purpose Chinese text extraction tool-chain but does not compare it to specialized domain filtering methods. Comparative analysis of EvalWeb versus domain-specific filtering tools on specialized corpora, measuring precision, recall, and relevance to specific domains would resolve this.

## Limitations

- The quality evaluation model's generalization across diverse Chinese web text domains is assumed but not explicitly validated
- The 90% quality threshold is based on human evaluation without detailed statistics on rater consistency or quality score distribution
- The approach may not perform optimally on domain-specific text types not well-represented in the training data

## Confidence

- **High confidence**: Effectiveness of rule-based filtering pipeline for removing explicit noise (ads, toxic content, duplicates)
- **Medium confidence**: BERT-based quality evaluation model's ability to distinguish high-quality from low-quality text given the self-training approach
- **Low confidence**: Generalization capability of the distilled FastText model without cross-domain validation results

## Next Checks

1. Conduct cross-domain evaluation of the quality evaluation model on Chinese web text from domains not represented in the training data (e.g., academic papers, social media, news articles)
2. Perform ablation studies to quantify the contribution of each preprocessing rule and determine if the current thresholds are optimal
3. Analyze the distribution of quality scores in the final dataset and verify the 90% quality threshold through blind human evaluation on multiple random samples