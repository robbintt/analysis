---
ver: rpa2
title: Do Multi-Document Summarization Models Synthesize?
arxiv_id: '2301.13844'
source_url: https://arxiv.org/abs/2301.13844
tags:
- reviews
- sentiment
- summarization
- primera
- pegasus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We investigate whether multi-document summarization models can\
  \ implicitly synthesize inputs with respect to a key aspect, like sentiment or treatment\
  \ effect. We find existing models partially perform synthesis but imperfectly\u2014\
  they are over-sensitive to input ordering and under-sensitive to input composition."
---

# Do Multi-Document Summarization Models Synthesize?

## Quick Facts
- **arXiv ID**: 2301.13844
- **Source URL**: https://arxiv.org/abs/2301.13844
- **Reference count**: 40
- **Key outcome**: Existing multi-document summarization models partially synthesize inputs regarding properties like sentiment or treatment effect, but are over-sensitive to input ordering and under-sensitive to input composition. A diverse beam search approach improves synthesis performance.

## Executive Summary
This paper investigates whether multi-document summarization models can implicitly synthesize inputs with respect to key properties like sentiment or treatment effect. Through experiments on movie reviews and biomedical systematic reviews, the authors find that existing models exhibit imperfect synthesis behavior - they are overly sensitive to input ordering while being under-sensitive to changes in input composition. To address these limitations, the authors propose a simple method: generate diverse candidate outputs and select the one best aligned with the expected aggregate measure for the inputs. This approach improves model synthesis performance, particularly when models abstain from outputting when no good candidate exists.

## Method Summary
The authors evaluate existing transformer-based summarization models (Longformer, Pegasus, PRIMERA, T5) on two tasks: movie reviews meta-synthesis and biomedical systematic review synthesis. They measure synthesis capability by examining model sensitivity to input ordering and composition changes. Their proposed method uses diverse beam search to generate multiple candidate summaries, then selects the candidate whose predicted property (sentiment or effect significance) best matches the expected aggregate derived from input documents. When no candidate sufficiently matches the expected aggregate, the model abstains from outputting.

## Key Results
- Existing models are over-sensitive to input ordering changes but under-sensitive to input composition changes
- Diverse beam search with candidate selection improves synthesis calibration
- Abstaining when no good candidate exists further improves synthesis quality
- The approach works across both movie review sentiment and biomedical treatment effect synthesis tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse beam search improves synthesis by generating candidate outputs that vary in their conveyed property of interest
- Mechanism: DBS penalizes similarity between beam groups, ensuring candidates span a wider range of possible properties, increasing likelihood of matching expected aggregate
- Core assumption: Model has capacity to generate meaningful variation in synthesized property without compromising fluency
- Evidence anchors: Abstract mentions generating diverse candidates and selecting the one best aligned with expected aggregate measure

### Mechanism 2
- Claim: Selecting candidates matching predicted aggregate property improves synthesis calibration
- Mechanism: External model measures property in each candidate, choosing the one closest to expected aggregate from inputs
- Core assumption: External measurement model accurately estimates property of interest
- Evidence anchors: Section 5 describes using measurement model to select candidate best aligned with anticipated aggregated property

### Mechanism 3
- Claim: Abstaining when no candidate matches expected aggregate prevents misleading synthesis
- Mechanism: If no diverse candidate's measured property is close enough to expected aggregate, model abstains
- Core assumption: Better to abstain than generate misleading output when model cannot accurately reflect synthesis target
- Evidence anchors: Abstract mentions abstaining when model produces no good candidate

## Foundational Learning

- Concept: Text regression/classification for property measurement
  - Why needed here: External model measures properties like sentiment or significance in text candidates
  - Quick check question: How would you train a model to predict the sentiment of a movie review given a dataset of reviews with sentiment scores?

- Concept: Diverse beam search
  - Why needed here: Generates candidate outputs varying in conveyed property of interest
  - Quick check question: What is the role of the diversity penalty hyperparameter λ in DBS, and how does it affect the diversity of generated candidates?

- Concept: Aggregation functions for synthesis
  - Why needed here: Expected aggregate property derived from inputs using aggregation function G
  - Quick check question: Why might a weighted mean be more appropriate than a simple mean for aggregating sentiment scores from reviews of varying lengths or credibility?

## Architecture Onboarding

- Component map:
  Input documents -> Linearization -> Diverse Beam Search generation -> Property measurement for each candidate -> Expected aggregate computation -> Selection logic (choose closest or abstain) -> Output summary or abstention

- Critical path:
  1. Linearize input documents into single input
  2. Generate diverse candidate outputs using summarization model + DBS
  3. Use measurement model to estimate property for each candidate
  4. Compute expected aggregate property from input documents
  5. Select candidate with minimal distance to expected aggregate, or abstain
  6. Output selected candidate or abstention

- Design tradeoffs:
  - Diversity vs. fluency: Increasing diversity may reduce overall coherence
  - Measurement accuracy vs. abstention rate: Stricter thresholds increase abstention but reduce misleading outputs
  - Aggregation method: Choice of G affects expected aggregate and selection
  - External model dependency: Approach relies on availability/accuracy of measurement model

- Failure signatures:
  - High abstention rate: Measurement model too strict or model cannot generate matching candidates
  - Low correlation between selected outputs and expected aggregate: Measurement model inaccurate or diversity insufficient
  - Poor fluency in selected outputs: Diversity penalty too high or base model quality low

- First 3 experiments:
  1. Implement DBS with varying diversity penalty λ to assess impact on candidate diversity and fluency
  2. Train/evaluate measurement model (sentiment classifier) on held-out set to ensure accuracy
  3. Run end-to-end synthesis with selection and abstention, measuring correlation with expected aggregate and abstention rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does over-sensitivity to input ordering persist across different model architectures and scales?
- Basis in paper: [inferred] The paper notes ordering sensitivity but doesn't systematically investigate across architectures or model sizes
- Why unresolved: Experiments limited to specific transformer models without exploring architectural sources of ordering sensitivity
- What evidence would resolve it: Experiments comparing ordering sensitivity across diverse model architectures and sizes

### Open Question 2
- Question: How do diversity-based synthesis improvements generalize to other aspects beyond sentiment and treatment effect significance?
- Basis in paper: [explicit] Paper acknowledges neither measure covers topicality, fluency, or other quality measures
- Why unresolved: Evaluation narrowly focused on two specific aspects using proxy metrics
- What evidence would resolve it: Systematic evaluation across multiple dimensions of summary quality using comprehensive metrics

### Open Question 3
- Question: What are the computational and practical trade-offs of the generate-diverse-then-select approach in real-world systems?
- Basis in paper: [inferred] Paper proposes post-hoc method but doesn't address practical implications for deployment
- Why unresolved: Focuses on demonstrating effectiveness without discussing computational costs or user experience
- What evidence would resolve it: Empirical measurements of inference latency, resource usage, and user satisfaction studies

## Limitations
- Evaluation relies on synthetic test scenarios that may not fully capture real-world synthesis challenges
- Proposed approach lacks ablation studies showing whether diversity or selection drives improvements
- Abstention mechanism is theoretically justified but empirically under-validated
- Reliance on external measurement models introduces additional uncertainty not fully addressed

## Confidence
- **High confidence**: Core finding that existing models are over-sensitive to input ordering and under-sensitive to input composition, demonstrated across multiple models and datasets
- **Medium confidence**: Claim that diverse beam search with selection improves synthesis calibration, though ablation is incomplete
- **Low confidence**: Abstention mechanism's practical utility and generalizability to other synthesis properties beyond sentiment and treatment effect

## Next Checks
1. **Ablation study of diversity vs. selection**: Compare standard beam search with selection, diverse beam search without selection, and full proposed approach to isolate which component drives improvements

2. **Measurement model uncertainty analysis**: Quantify how errors in sentiment/effect classifiers affect candidate selection by injecting controlled noise into measurement model and measuring impact on synthesis performance

3. **Cross-domain generalization test**: Apply evaluation framework to a third domain (e.g., product reviews) with different synthesis property (e.g., urgency) to test whether input sensitivity patterns generalize beyond sentiment and treatment effect