---
ver: rpa2
title: 'More than Correlation: Do Large Language Models Learn Causal Representations
  of Space?'
arxiv_id: '2312.16257'
source_url: https://arxiv.org/abs/2312.16257
tags:
- representations
- city
- spatial
- accuracy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) like
  DeBERTa and GPT-Neo have an internal model of space that causally influences their
  behavior. The authors use representational similarity analysis and probing classifiers
  to show that these models implicitly learn spatial representations of city names.
---

# More than Correlation: Do Large Language Models Learn Causal Representations of Space?

## Quick Facts
- arXiv ID: 2312.16257
- Source URL: https://arxiv.org/abs/2312.16257
- Authors: 
- Reference count: 10
- Primary result: LLM hidden states encode spatial representations that causally influence geospatial task performance

## Executive Summary
This paper investigates whether large language models like DeBERTa and GPT-Neo develop an internal spatial world model beyond simple memorization. Using representational similarity analysis and probing classifiers, the authors demonstrate that these models implicitly learn spatial representations of city names. Through intervention experiments that modify intermediate hidden states, they show that perturbing these spatial representations causally affects the models' performance on country prediction and next-word prediction tasks, suggesting LLMs learn and utilize an internal model of space in geospatial-related tasks.

## Method Summary
The authors analyze LLM representations of city names using representational similarity analysis (RSA) to measure alignment between learned and real-world spatial relationships. They train linear and non-linear probing classifiers on hidden states to decode geographical coordinates. Causal intervention experiments are conducted by modifying intermediate hidden states using gradients from probing classifiers, and the effects on downstream geospatial tasks (country prediction and next-word prediction) are measured to establish causality.

## Key Results
- RSA shows high correlation (0.82 Pearson) between hidden state distances of city names and real-world geospatial distances
- Non-linear probes outperform linear probes in decoding latitude/longitude from hidden states
- Causal intervention experiments demonstrate significant performance degradation on geospatial tasks when spatial representations are perturbed (Z-test p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs develop an internal spatial world model that causally influences output behavior
- Mechanism: Hidden states encode spatial representations used in downstream geospatial tasks
- Core assumption: Spatial information in hidden states is causally linked to geospatial task performance
- Evidence anchors:
  - [abstract]: "Our experiments suggested that the LLMs learn and use an internal model of space in solving geospatial related tasks."
  - [section]: "Our causal intervention experiments showed that the spatial representations influenced the model's performance on next word prediction and a downstream task that relies on geospatial information."
- Break condition: If perturbing spatial representations doesn't affect geospatial task performance

### Mechanism 2
- Claim: LLM spatial representations align with real-world geographical relationships
- Mechanism: RSA shows correlation between hidden state distances and real-world geospatial distances
- Core assumption: Alignment between learned and real-world spatial representations is meaningful
- Evidence anchors:
  - [abstract]: "Using representational similarity analysis and probing classifiers, we showed that these models implicitly learn spatial representations of city names."
  - [section]: "Our experiments showed that the distance between the hidden states of city names in the De-BERTa's learned representation space has high correlation with the real-world geospatial distance between cities."
- Break condition: If correlation between learned and real-world spatial representations is not statistically significant

### Mechanism 3
- Claim: Probing classifiers can extract meaningful spatial information from LLM hidden states
- Mechanism: Linear and non-linear probes predict geographical coordinates from hidden states
- Core assumption: Hidden states contain sufficient spatial information that can be decoded
- Evidence anchors:
  - [abstract]: "Through linear and non-linear probing regressors, we further demonstrated that the hidden states of city names can be mapped linearly and non-linearly to the cities' actual latitudes and longitudes."
  - [section]: "To study whether the LLM's learned representations encode the spatial property of its input, we trained probing regressors on the hidden states after each transformer layer of DeBERTa-v2 and GPT-Neo."
- Break condition: If probes cannot accurately predict geographical coordinates from hidden states

## Foundational Learning

- Concept: Representational Similarity Analysis (RSA)
  - Why needed here: Measures alignment between LLM spatial representations and real-world geographical relationships
  - Quick check question: How does RSA measure the alignment between two different representations of the same data?

- Concept: Probing classifiers
  - Why needed here: Extract and decode spatial information from LLM hidden states
  - Quick check question: What is the difference between linear and non-linear probing in LLM interpretability?

- Concept: Causal intervention
  - Why needed here: Establish causal link between learned spatial representations and model behavior
  - Quick check question: How does perturbing hidden states with probing classifier gradients demonstrate causality?

## Architecture Onboarding

- Component map: LLM (DeBERTa-v2, GPT-Neo) -> Hidden state extraction -> RSA analysis -> Probing classifiers -> Causal intervention -> Task performance measurement
- Critical path: Extract hidden states → Perform RSA and probing → Conduct causal intervention → Measure task performance changes
- Design tradeoffs: Linear probes (simpler, less expressive) vs. non-linear probes (more expressive, risk overfitting)
- Failure signatures: Poor probe performance, insignificant RSA alignment, no causal effect from interventions
- First 3 experiments:
  1. Perform RSA on hidden states of city names to measure alignment with real-world geographical relationships
  2. Train linear and non-linear probing classifiers on hidden states to predict latitude and longitude
  3. Conduct causal intervention experiments by perturbing hidden states with probing classifier gradients and measure impact on country prediction and next-word prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs learn spatial representations for words without inherent geographical properties?
- Basis in paper: [explicit] The paper mentions that certain words like "human" are present almost anywhere on Earth and cannot be mapped to a specific location naturally. The authors propose this as a future work question.
- Why unresolved: Current study only focused on location words with inherent geographical properties
- What evidence would resolve it: Conduct similar experiments using non-geographical words and examine effects on non-spatial tasks

### Open Question 2
- Question: How do different distance metrics in RSA affect alignment between LLM representations and real geographical distances?
- Basis in paper: [explicit] The paper uses three distance metrics with varying results and mentions that different metrics may yield different outcomes
- Why unresolved: Paper doesn't provide comprehensive analysis of how metrics affect alignment conclusions
- What evidence would resolve it: In-depth analysis comparing RSA results across different distance metrics

### Open Question 3
- Question: How do non-linear probes compare to linear probes in decoding spatial information, and what limits predictive power?
- Basis in paper: [explicit] Non-linear probes outperform linear probes but show a ceiling in predictive power
- Why unresolved: Paper doesn't explain reasons behind the ceiling in predictive power
- What evidence would resolve it: Experiments investigating factors contributing to predictive power ceiling and exploring alternative probe designs

## Limitations

- Generalization uncertainty: Results may not extend to other geospatial concepts or non-geospatial domains
- Causal inference constraints: Findings show emergent pattern matching rather than true spatial reasoning
- Model and dataset specificity: Results are specific to DeBERTa-v2, GPT-Neo, and web training data

## Confidence

- High confidence: The existence of spatial representations in hidden states
- Medium confidence: The causal influence of spatial representations on geospatial tasks
- Medium-Low confidence: The claim that this constitutes a "world model"

## Next Checks

1. Cross-domain generalization test: Apply methodology to non-geographic relational data (family trees, organizational hierarchies) to test if spatial encoding is domain-specific

2. Zero-shot spatial reasoning evaluation: Design novel geospatial tasks not present in training data to test genuine reasoning versus pattern matching

3. Architectural ablation study: Compare spatial representation development across different transformer variants and with attention mechanisms disabled to isolate essential components