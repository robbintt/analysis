---
ver: rpa2
title: Reward Engineering for Generating Semi-structured Explanation
arxiv_id: '2309.08347'
source_url: https://arxiv.org/abs/2309.08347
tags:
- reward
- explanation
- graph
- language
- semi-structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating semi-structured
  explanations using moderate-sized language models (FLAN-T5-XXL, 13B parameters).
  The authors demonstrate that supervised fine-tuning (SFT) alone is insufficient
  for this task and propose a reward engineering method using reinforcement learning
  (RL) to improve performance.
---

# Reward Engineering for Generating Semi-structured Explanation

## Quick Facts
- arXiv ID: 2309.08347
- Source URL: https://arxiv.org/abs/2309.08347
- Authors: 
- Reference count: 9
- Key outcome: Reward engineering using RL with multiple aggregation methods improves semi-structured explanation generation beyond SFT alone, achieving state-of-the-art results on ExplaGraph and COPA-SSE benchmarks.

## Executive Summary
This paper addresses the challenge of generating semi-structured explanations using moderate-sized language models (FLAN-T5-XXL, 13B parameters). The authors demonstrate that supervised fine-tuning (SFT) alone is insufficient for this task and propose a reward engineering method using reinforcement learning (RL) to improve performance. Their approach involves designing multiple reward aggregation methods, combining reward model outputs with evaluation metric-based rewards (e.g., Graph-BERTScore). The proposed RL-based method achieves new state-of-the-art results on two benchmarks (ExplaGraph and COPA-SSE), outperforming existing methods on multiple metrics including stance accuracy, structural correctness, and semantic correctness.

## Method Summary
The method employs a three-phase approach: first, supervised fine-tuning of FLAN-T5-XXL with LoRA on the task data; second, training a reward model (LLaMA-7B) on paired preference data generated from SFT outputs; third, RL fine-tuning using PPO with reward aggregation combining the reward model output (R_ϕ) and metric-based rewards (R_m) like Graph-BERTScore. The reward aggregation uses different weight factors for different tasks (α=0.9 for ExplaGraph, α=0.5 for COPA-SSE), and KL regularization with coefficient β=0.3 balances exploration and exploitation during RL optimization.

## Key Results
- SFT+RL with reward engineering achieves state-of-the-art performance on ExplaGraph and COPA-SSE benchmarks
- Aggregation of reward model output and Graph-BERTScore without weights performs best, while weighted aggregation decreases effectiveness
- KL coefficient β=0.3 provides optimal balance between exploration and preserving SFT knowledge
- Graph-BERTScore is more effective than n-gram metrics (BLEU, ROUGE) for evaluating semi-structured explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward engineering addresses the insufficiency of SFT for semi-structured explanation generation by providing targeted feedback during RL fine-tuning.
- Mechanism: The method combines a reward model (R_ϕ) trained on paired preference data with metric-based rewards (R_m) that directly evaluate graph quality using semantic matching metrics like Graph-BERTScore. These rewards are aggregated to guide the model toward generating structurally and semantically correct explanation graphs.
- Core assumption: The reward aggregation preserves the relative importance of different reward signals without washing out their effects.
- Evidence anchors:
  - [abstract] "We investigate multiple reward aggregation methods and provide a detailed discussion which sheds light on the promising potential of RL for future research."
  - [section 3.3] "To aggregate two rewards, an important premise is that the order of magnitude of two rewards should not have too much difference (e.g., 0.01 vs 100), otherwise the effect of one reward could be washed away."

### Mechanism 2
- Claim: The KL coefficient (β) in RL fine-tuning controls the balance between exploration and exploitation, preventing the model from deviating too far from the SFT baseline while still allowing improvement.
- Mechanism: During RL optimization, the KL divergence term D_KL[π_θ(y|x)||π_ref(y|x)] is weighted by β, which constrains the new policy to stay close to the reference policy (initial SFT model). This prevents catastrophic forgetting while enabling the model to learn from reward signals.
- Core assumption: The initial SFT model captures useful inductive biases that should be preserved during RL fine-tuning.
- Evidence anchors:
  - [section 4.5] "As the β increases from 0.1, the performance becomes better until β is over 0.3. From 0.3 to 1.0, the performance goes down gradually, although they achieve the highest SA."
  - [section 3.3] "In practice, the language model policy π_θ is also initialised to the initial SFT model."

### Mechanism 3
- Claim: Graph-BERTScore is more effective than n-gram based metrics (BLEU, ROUGE) for evaluating semi-structured explanations because it captures semantic similarity rather than exact string matching.
- Mechanism: Graph-BERTScore treats each triple as a sentence and uses BERT embeddings to calculate semantic similarity between generated and reference triples. This is particularly effective for graph-structured data where exact n-gram matching is less meaningful.
- Core assumption: Semantic similarity is more important than exact string matching for evaluating the quality of explanation graphs.
- Evidence anchors:
  - [section 4.4] "Graph-BERTScore is a semantic evaluation metric which is still useful in graph-structured data, thus leading better performance in R_m."
  - [section 3.2] "Since the semi-structured explanation is represented in format of a set of triples (i.e., [head, relation, tail]), following the previous work (Saha et al., 2021), we consider each triple as a sentence and use the existing text matching metrics to calculate the graph matching score."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF provides a framework for fine-tuning language models using reward signals that capture human preferences, which is essential for improving the quality of generated explanations beyond what SFT can achieve.
  - Quick check question: What are the three main phases of RLHF and how do they relate to the semi-structured explanation generation task?

- Concept: Reward modeling and preference learning
  - Why needed here: The reward model needs to learn to distinguish between good and bad explanations based on paired preference data, which is crucial for providing effective feedback during RL fine-tuning.
  - Quick check question: How is the paired preference data generated and filtered to ensure high-quality training signals for the reward model?

- Concept: Graph-based evaluation metrics
  - Why needed here: Semi-structured explanations are represented as graphs of triples, requiring specialized evaluation metrics that can handle graph structure and semantic relationships between nodes.
  - Quick check question: What is the difference between Graph-BERTScore and traditional BLEU/ROUGE metrics, and why is this difference important for evaluating explanation graphs?

## Architecture Onboarding

- Component map: SFT model (FLAN-T5-XXL) → Reward model training (LLaMA-7B) → RL fine-tuning (PPO) → Evaluation pipeline
- Critical path: SFT model → Reward model training → RL fine-tuning → Evaluation
- Design tradeoffs:
  - Using moderate-sized models (13B) instead of LLMs for efficiency vs. potential performance limitations
  - Combining multiple reward sources vs. simplicity of single reward signal
  - Preserving SFT knowledge through KL regularization vs. allowing more aggressive exploration
- Failure signatures:
  - Reward hacking: model learns to optimize rewards without improving actual explanation quality
  - Mode collapse: generated explanations become too similar to training examples
  - Semantic drift: explanations satisfy reward metrics but fail to capture correct reasoning
- First 3 experiments:
  1. Compare SFT-only performance against SFT+RL with different reward configurations to establish baseline improvements
  2. Ablation study of reward aggregation methods (with vs. without weights, different weight factors)
  3. Sensitivity analysis of KL coefficient β to find optimal balance between exploration and exploitation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed reward engineering method perform on other graph-based generative tasks beyond semi-structured explanation generation?
- Basis in paper: [inferred] The authors mention that preliminary experiments using RL for other Text-to-Graph generation tasks (i.e., WebNLG) indicated that RL may not be an effective method when SFT reaches the task performance ceiling.
- Why unresolved: The paper does not provide detailed results or analysis of the RL method's performance on other graph-based generative tasks, leaving the generalizability of the approach unclear.
- What evidence would resolve it: Conducting experiments on a diverse set of graph-based generative tasks and comparing the performance of the proposed RL method with other state-of-the-art approaches would provide insights into its effectiveness and generalizability.

### Open Question 2
- Question: What is the impact of using different reward aggregation methods on the performance of the RL-based approach for semi-structured explanation generation?
- Basis in paper: [explicit] The authors investigate multiple reward aggregation methods and find that the aggregation of Rϕ and Rm without using weights performs best, while using weights decreases the effect of two rewards.
- Why unresolved: The paper does not explore the reasons behind the performance differences between different reward aggregation methods or provide a theoretical explanation for the observed results.
- What evidence would resolve it: Conducting a detailed analysis of the reward aggregation methods, including their mathematical properties and the impact of different hyperparameters, would help in understanding the reasons behind the performance differences and guide the design of more effective reward aggregation strategies.

### Open Question 3
- Question: How does the choice of the reward model architecture and training data affect the performance of the RL-based approach for semi-structured explanation generation?
- Basis in paper: [explicit] The authors use LLaMA-7B as the reward model and fine-tune it on the task data to improve its understanding of the input.
- Why unresolved: The paper does not investigate the impact of using different reward model architectures or training data on the performance of the RL-based approach, leaving the optimal design of the reward model unclear.
- What evidence would resolve it: Conducting experiments with different reward model architectures (e.g., different model sizes, architectures) and training data (e.g., different amounts of preference data, data augmentation techniques) would provide insights into the factors that contribute to the effectiveness of the reward model and guide the design of more effective reward models for semi-structured explanation generation tasks.

## Limitations
- The stability of reward aggregation methods across different domains remains unclear
- KL coefficient optimization was empirical without systematic hyperparameter exploration
- Effectiveness of Graph-BERTScore depends on BERT embedding quality and may not generalize to all semantic structures

## Confidence
- **High confidence**: Core experimental results showing SFT+RL improvements over SFT-only baselines, basic RLHF framework implementation, and qualitative observations about reward aggregation effects
- **Medium confidence**: Specific choice of hyperparameters (β=0.3, α weights), claim that Graph-BERTScore is superior to n-gram metrics for this task, and generalizability to other semi-structured explanation tasks
- **Low confidence**: Stability of reward model training process, robustness to different reward model architectures, and long-term effectiveness beyond evaluated benchmarks

## Next Checks
1. **Reward Stability Analysis**: Evaluate the variance in RL performance across multiple runs with different random seeds to assess the stability of the reward aggregation approach and identify conditions under which the method may fail.

2. **Cross-domain Generalization**: Apply the SFT+RL pipeline to a different semi-structured explanation task (e.g., abductive reasoning or scientific explanation generation) to test whether the reward engineering approach generalizes beyond the two evaluated benchmarks.

3. **Reward Model Ablation**: Systematically vary the reward model architecture (e.g., different model sizes, different training objectives) and measure the impact on final RL performance to determine whether the LLaMA-7B choice is optimal or merely sufficient.