---
ver: rpa2
title: Efficient Hyperdimensional Computing
arxiv_id: '2301.10902'
source_url: https://arxiv.org/abs/2301.10902
tags:
- accuracy
- dimension
- hypervectors
- binary
- dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that hypervector dimensions in HDC can be significantly
  reduced without sacrificing accuracy by analyzing the relationship between hypervector
  dimensions and classification accuracy. Using a novel binary kernel-based encoder
  with majority rule, the authors achieve 96.88% accuracy on MNIST with only 32 dimensions,
  reducing computation operations to 0.35% of traditional HDC models using 10,000
  dimensions.
---

# Efficient Hyperdimensional Computing

## Quick Facts
- arXiv ID: 2301.10902
- Source URL: https://arxiv.org/abs/2301.10902
- Reference count: 31
- Primary result: Achieves 96.88% MNIST accuracy with only 32 dimensions, reducing computation to 0.35% of traditional HDC

## Executive Summary
This paper demonstrates that hypervector dimensions in Hyperdimensional Computing (HDC) can be significantly reduced without sacrificing accuracy by analyzing the relationship between hypervector dimensions and classification accuracy. Using a novel binary kernel-based encoder with majority rule, the authors achieve state-of-the-art results on MNIST with only 32 dimensions, reducing computation operations to 0.35% of traditional HDC models using 10,000 dimensions. The work also shows theoretical bounds proving that strict orthogonality can be achieved at much lower dimensions (2⌈log₂n⌉ for n vectors), and that higher dimensions actually decrease accuracy upper bounds.

## Method Summary
The authors introduce a binary kernel-based encoder with majority rule to maintain state-of-the-art accuracy with much lower dimensions. The approach uses a three-layer binary encoder to extract features, replacing the fully connected layer with majority rule aggregation. Two HDC retraining algorithms are applied to improve representations. The method leverages Hadamard matrices to construct strictly orthogonal hypervectors in lower dimensional spaces, and uses straight-through estimator for training binary neural networks. The key innovation is demonstrating that dimension reduction below 64 can maintain accuracy through careful encoder design and retraining procedures.

## Key Results
- Achieves 96.88% accuracy on MNIST with only 32 dimensions
- Reduces computation operations to 0.35% of traditional HDC models using 10,000 dimensions
- Demonstrates 46.18% accuracy on CIFAR-10 with 128 dimensions
- Proves theoretical bounds showing strict orthogonality achievable at 2⌈log₂n⌉ dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strict orthogonality can be achieved with hypervector dimensions of 2⌈log₂n⌉ for n vectors
- Mechanism: Using Hadamard matrices to construct n orthogonal vectors in a 2⌈log₂n⌉-dimensional space
- Core assumption: Hadamard matrices exist for the required dimensions
- Evidence anchors: Statement 3.3 shows dimension d of only 2⌈log₂n⌉ is needed to find n strictly orthogonal hypervectors; definition of Hadamard matrix supports construction

### Mechanism 2
- Claim: Higher dimensions actually decrease accuracy upper bounds due to misrepresentation error
- Mechanism: When projecting high-dimensional linearly separable data into low dimensions, small dimensions suffer from misrepresentation error that dominates misclassification error
- Core assumption: Data is linearly separable in the original high-dimensional space
- Evidence anchors: Statements 3.1 and 3.2 show as dimension increases, worst-case and average-case prediction accuracy decreases; numerical experiments show breakdown for dimensions d ≤ 64

### Mechanism 3
- Claim: Binary kernel-based encoder with majority rule can maintain state-of-the-art accuracy with much lower dimensions
- Mechanism: The encoder learns to extract relevant features that can be represented in lower dimensions while maintaining class separability, and the majority rule aggregates binary representations effectively
- Core assumption: The encoder can learn useful representations despite the binary constraint
- Evidence anchors: MNIST results show 96.88/97.23% accuracy with 32/64 dimensions using the proposed encoder; three-layer binary kernel-based encoder is specifically designed for this purpose

## Foundational Learning

- Concept: Hyperdimensional Computing (HDC) basics
  - Why needed here: Understanding HDC is fundamental to grasping why reducing dimensions works
  - Quick check question: What are the three main operations in HDC and how do they work?

- Concept: Orthogonality and its role in HDC
  - Why needed here: The paper's core argument relies on understanding orthogonality requirements
  - Quick check question: Why is orthogonality important in HDC and how does it affect accuracy?

- Concept: Binary neural networks and straight-through estimator
  - Why needed here: The proposed encoder uses binary activations and requires understanding of STE
  - Quick check question: How does the straight-through estimator work and why is it needed for binary networks?

## Architecture Onboarding

- Component map: Input data -> Binary kernel-based encoder (3 layers) -> Binary representation -> Majority rule aggregation -> Class representation -> Similarity checker (inner product)

- Critical path:
  1. Input data → Binary encoder → Binary representation
  2. Binary representation → Majority rule aggregation → Class representation
  3. Test data → Binary encoder → Similarity comparison → Class prediction

- Design tradeoffs:
  - Dimension vs. accuracy: Lower dimensions reduce computation but may decrease accuracy if too small
  - Binary vs. floating point: Binary operations are faster but may limit representational capacity
  - Encoder complexity vs. retraining effectiveness: More complex encoders may need less retraining

- Failure signatures:
  - Accuracy drops significantly below theoretical bounds
  - Orthogonality degrades rapidly with dimension reduction
  - Retraining fails to improve accuracy beyond baseline

- First 3 experiments:
  1. Verify orthogonality preservation with dimension reduction using Hadamard matrices
  2. Test accuracy degradation curve with increasing dimension on MNIST
  3. Validate binary encoder training with STE on a small dataset before scaling to MNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical relationship between hypervector dimension and accuracy for low dimensions (d ≤ 64) in HDC models?
- Basis in paper: The paper notes that current theoretical statements (3.1 and 3.2) do not predict the low accuracy for dimension d ≤ 64 in numerical experiments
- Why unresolved: The paper identifies the issue but does not provide a complete theoretical framework to explain or predict the behavior of HDC models at low dimensions
- What evidence would resolve it: A detailed mathematical model that accounts for the misrepresentation error when projecting high-dimensional data to a low-dimensional space, and its impact on the accuracy of HDC models

### Open Question 2
- Question: How does the choice of encoder architecture and its parameters affect the performance of HDC models at low dimensions?
- Basis in paper: The paper introduces a novel binary kernel-based encoder with majority rule but does not explore the impact of different encoder architectures or parameters
- Why unresolved: While the paper demonstrates improved performance with a specific encoder architecture, it does not investigate the broader space of possible architectures and parameters
- What evidence would resolve it: Systematic experiments varying encoder architectures (e.g., number of layers, kernel sizes, strides) and parameters (e.g., threshold values, learning rates) to identify the most effective configurations for HDC models at low dimensions

### Open Question 3
- Question: What is the impact of data quantization on the performance of HDC models, and how does it interact with the choice of hypervector dimension?
- Basis in paper: The paper mentions that quantization is applied to input data in state-of-the-art works to reduce the number of classes K
- Why unresolved: The paper does not provide a detailed analysis of how different quantization schemes affect the performance of HDC models or how this interacts with the choice of hypervector dimension
- What evidence would resolve it: Experiments comparing the performance of HDC models using different quantization schemes and their interaction with hypervector dimensions, to determine the optimal quantization strategy for a given task and dimension

## Limitations
- Theoretical bounds show significant deviation from empirical results for dimensions below 64
- Binary encoder's effectiveness on CIFAR-10 (46.18%) remains substantially below traditional HDC approaches
- Reliance on Hadamard matrices assumes the Hadamard conjecture holds for all required dimensions

## Confidence
- High confidence in the orthogonal dimension reduction theory and basic MNIST results
- Medium confidence in the binary encoder's generalizability and CIFAR-10 performance
- Low confidence in the theoretical bounds' applicability to all dimension ranges

## Next Checks
1. Verify orthogonality preservation across the full range of tested dimensions (32-10,000) using empirical measurements rather than theoretical bounds
2. Conduct ablation studies isolating the contribution of the binary encoder versus dimension reduction to overall accuracy improvements
3. Test the approach on additional datasets with varying complexity (e.g., Fashion-MNIST, SVHN) to assess generalizability limits