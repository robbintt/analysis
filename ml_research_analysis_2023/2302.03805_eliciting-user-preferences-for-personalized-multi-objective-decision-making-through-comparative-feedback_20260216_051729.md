---
ver: rpa2
title: Eliciting User Preferences for Personalized Multi-Objective Decision Making
  through Comparative Feedback
arxiv_id: '2302.03805'
source_url: https://arxiv.org/abs/2302.03805
tags:
- policy
- have
- algorithm
- value
- span
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper considers the problem of learning user preferences over
  multiple objectives for personalized multi-objective decision making. The authors
  propose a framework where each user is characterized by an unknown preference vector,
  and the goal is to efficiently compute a near-optimal policy for a given user using
  policy comparisons as feedback.
---

# Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback

## Quick Facts
- **arXiv ID:** 2302.03805
- **Source URL:** https://arxiv.org/abs/2302.03805
- **Reference count:** 40
- **Key outcome:** This paper considers the problem of learning user preferences over multiple objectives for personalized multi-objective decision making using policy comparisons as feedback.

## Executive Summary
This paper addresses the challenge of learning user preferences for personalized multi-objective decision making through comparative feedback. The authors propose a framework where each user is characterized by an unknown preference vector, and the goal is to efficiently compute a near-optimal policy for a given user using policy comparisons as feedback. They consider two feedback models: comparing explicit policies and comparing weighted sets of representative trajectories. The algorithms have polynomial time complexity and logarithmic query complexity in the error parameter, providing near-optimal personalized policies.

## Method Summary
The method involves finding a minimal set of basis policies with independent value vectors, estimating the user's preference vector via comparison queries, and computing a nearly optimal personalized policy. For the first feedback model, the algorithm sequentially finds policies that maximize individual objectives and are orthogonal to current value vectors. For the second model, it uses Carathéodory's theorem to compress trajectory representations. The preference vector is estimated by comparing policies using a "do nothing" baseline and binary search to find ratios between basis policies.

## Key Results
- Algorithm finds minimal set of basis policies with independent value vectors in polynomial time
- Preference vector estimated via comparison queries with logarithmic query complexity in error parameter
- Trajectory-based representation preserves policy value information while being more interpretable than explicit policies

## Why This Works (Mechanism)

### Mechanism 1
The user's preference vector can be learned efficiently by finding a basis of linearly independent value vectors and estimating the ratios of personalized values between basis policies. A benchmark policy is found by comparing policies that maximize individual objectives, then sequentially finding d-1 additional orthogonal policies. The ratios αi = ⟨w*,Vπi+1⟩/⟨w*,Vπ1⟩ are estimated using binary search with the "do nothing" policy as comparator. This works under the assumption that the MDP has finite rank d and comparison queries are noiseless.

### Mechanism 2
A weighted trajectory set representation can preserve all information about policy values while being more interpretable than explicit policy mappings. Given a policy π, k+1 trajectories and weights are found such that their weighted sum equals the policy's value vector. This uses Carathéodory's theorem to compress the exponentially large set of possible trajectories into a polynomial-sized representation. The assumption is that policy values are convex combinations of trajectory returns.

### Mechanism 3
The "do nothing" policy can be used as a comparator to estimate ratios between personalized values of basis policies. For each basis policy πi+1, binary search on a mixing parameter α̂i finds where α̂iπ1 + (1-α̂i)π0 becomes indistinguishable from πi+1 under user comparison. This gives an estimate of the true ratio αi up to additive error, assuming the "do nothing" policy has value zero and is always available.

## Foundational Learning

- **Concept:** Linear independence and spanning sets in vector spaces
  - Why needed here: To ensure basis policies span the entire space of possible policy values, allowing unique determination of any user's preference vector
  - Quick check question: If you have k linearly independent vectors in a k-dimensional space, can they span the entire space? Why or why not?

- **Concept:** Carathéodory's theorem and convex combinations
  - Why needed here: To compress the exponentially large set of possible trajectories into a polynomial-sized representation that preserves all policy value information
  - Quick check question: Carathéodory's theorem states that any point in the convex hull of a set in R^k can be expressed as a convex combination of at most k+1 points. How does this apply to trajectory representations?

- **Concept:** Binary search and precision parameters in active learning
  - Why needed here: To efficiently estimate the ratio between personalized values using comparison queries with bounded precision ǫ
  - Quick check question: If each binary search step reduces the search interval by half, how many steps are needed to achieve precision δ when starting with interval [0,C]?

## Architecture Onboarding

- **Component map:** MDP model -> Basis policy finder -> Ratio estimator -> Preference vector solver -> Optimal policy computation

- **Critical path:**
  1. Find basis policies π1,...,πd (Algorithm 1)
  2. Estimate ratios α̂1,...,α̂d-1 (Algorithm 4)
  3. Solve for preference vector ŵ (Eq 4)
  4. Compute optimal policy for ŵ (using standard RL solver)
  Alternative: Replace steps 1-3 with trajectory-based representation

- **Design tradeoffs:**
  - Explicit policy vs. trajectory representation: explicit is computationally lighter but less interpretable; trajectory is more interpretable but requires O(k⁴H|S|) preprocessing
  - Number of basis policies: more policies give better approximation but require more queries
  - Precision parameter ǫ: smaller ǫ gives better approximation but requires more queries

- **Failure signatures:**
  - If basis policies have very similar values, ratio estimation becomes noisy
  - If MDP has high rank d, the exponential dependence on d in error bounds becomes problematic
  - If user comparisons are noisy (not captured in current model), the noiseless comparison assumption breaks

- **First 3 experiments:**
  1. Test basis policy finder on a simple gridworld with 2 objectives to verify linear independence and orthogonality properties
  2. Validate ratio estimation accuracy by comparing α̂i to ground truth αi on a synthetic MDP with known w*
  3. Compare trajectory compression quality by checking if weighted trajectory sets preserve policy values within tolerance ǫ

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal dependency on epsilon for the query complexity and suboptimality bounds in the explicit policy representation setting? The authors note that the current bounds have a factor of k^(d+14/3) * epsilon^(1/3) which may not be tight. A proof showing either the current bounds are tight or a new algorithm achieving better bounds would resolve this.

### Open Question 2
How does the algorithm performance change when the MDP is unknown and must be learned from data? The current theoretical analysis assumes full knowledge of the MDP structure. An analysis showing how model estimation error propagates to preference estimation error and final policy quality would resolve this.

### Open Question 3
Can the algorithms be extended to handle non-linear utility functions over objectives? The current framework assumes linear preferences but many real-world scenarios involve non-linear tradeoffs. A demonstration of either an extension to specific non-linear utility classes with provable guarantees or a proof that certain non-linear classes make the problem computationally intractable would resolve this.

## Limitations
- The noiseless comparison assumption may not hold for real human feedback, potentially increasing query complexity
- Exponential dependence on the rank d in error bounds becomes problematic for MDPs with high-dimensional value spaces
- The O(k⁴H|S|) complexity for trajectory compression may be prohibitive for large MDPs in practice

## Confidence

**High confidence:** The theoretical framework for basis policy finding and preference vector estimation is well-established with solid mathematical foundations.

**Medium confidence:** The computational complexity analysis assumes idealized conditions and may not reflect practical limitations.

**Low confidence:** The generalization to noisy human feedback is not addressed, and the "do nothing" baseline assumption may not hold in all MDPs.

## Next Checks

1. **Empirical noise tolerance test:** Implement the framework with simulated noisy comparisons and measure how query complexity scales with noise level.

2. **Real-user study:** Conduct a user study with human participants comparing policies in a simple gridworld environment to validate the practical applicability of the comparison model.

3. **Scalability benchmark:** Test the trajectory compression algorithm on MDPs with varying state and action space sizes to empirically verify the O(k⁴H|S|) complexity claim and identify practical limits.