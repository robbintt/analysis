---
ver: rpa2
title: Earning Extra Performance from Restrictive Feedbacks
arxiv_id: '2304.14831'
source_url: https://arxiv.org/abs/2304.14831
tags:
- tuning
- data
- which
- expected
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model tuning without access
  to target data, introducing the EXPECTED framework where model providers refine
  models based on restrictive feedbacks. The proposed approach, Performance-guided
  Parameter Search (PPS), optimizes the distribution of model parameters using gradient
  estimation, enabling tuning from feedback signals like accuracy or usage rates.
---

# Earning Extra Performance from Restrictive Feedbacks

## Quick Facts
- arXiv ID: 2304.14831
- Source URL: https://arxiv.org/abs/2304.14831
- Authors: 
- Reference count: 40
- Achieves comparable or superior performance to unsupervised tuning methods using significantly fewer queries by optimizing model parameters based on restrictive feedback signals.

## Executive Summary
This paper addresses the challenge of model tuning when target data is inaccessible, proposing the EXPECTED framework that refines models using only restrictive feedback signals like accuracy or usage rates. The framework introduces Performance-guided Parameter Search (PPS) for simple models and Layerwise Coordinate Parameter Search (LCPS) for complex deep neural networks, enabling efficient parameter optimization through gradient estimation without requiring explicit target data access. The approach demonstrates significant query efficiency improvements while maintaining or exceeding the performance of traditional unsupervised tuning methods.

## Method Summary
The EXPECTED framework treats model parameters as samples from a distribution and optimizes this distribution using gradient estimation based on feedback signals. For simple models, PPS uses Monte Carlo sampling with antithetic sampling and feedback normalization to estimate gradients and update parameters. For complex deep neural networks, LCPS extends this approach by partitioning parameters layer-wise and using a multi-armed bandit strategy to dynamically allocate queries based on layer importance. Both algorithms rely on the assumption that performance can be characterized as an expectation over the parameter distribution, enabling optimization without direct target data access.

## Key Results
- PPS and LCPS achieve comparable or superior performance to unsupervised tuning methods requiring entire target features
- LCPS demonstrates significant query efficiency, requiring fewer queries than global parameter updates
- The methods show particular effectiveness in handling distribution shifts and customized evaluation metrics including fairness and fault-intolerant scenarios
- Experimental results across multiple applications validate the framework's effectiveness and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can be optimized without direct access to target data by characterizing the geometry of model performance with regard to model parameters.
- Mechanism: The framework treats model parameters as samples from a distribution, and optimizes this distribution using gradient estimation based on feedback signals. This allows the model to learn the relationship between parameter changes and performance improvements without needing explicit target data.
- Core assumption: The evaluation function E(·) can be approximated as an expectation over the parameter distribution π(θ), allowing gradient-based optimization through Monte Carlo sampling.
- Evidence anchors:
  - [abstract]: "To enable tuning in this restrictive circumstance, we propose to characterize the geometry of the model performance with regard to model parameters through exploring the parameters' distribution."
  - [section]: "Problem Formulation" section establishes that the evaluation function E(·) measures performance over target data D and returns a score si, which becomes the basis for optimization.
  - [corpus]: Weak - no direct corpus evidence found, but related to black-box optimization concepts.
- Break condition: If the evaluation function is too noisy or non-smooth, the gradient estimation may become unreliable and optimization will fail.

### Mechanism 2
- Claim: Layerwise parameter updates with dynamic query allocation improve efficiency compared to global parameter updates.
- Mechanism: The framework partitions parameters by layer and uses a multi-armed bandit approach to dynamically allocate queries to layers based on their performance improvements. This focuses computational resources on the most impactful layers.
- Core assumption: Different layers contribute differently to model performance, and this contribution can be measured through layerwise updates and used to guide future query allocation.
- Evidence anchors:
  - [abstract]: "For complex deep neural networks, Layerwise Coordinate Parameter Search (LCPS) efficiently updates parameters layer-by-layer with dynamic query allocation based on layer importance."
  - [section]: "Extension to Complex Models" section explains the limitations of global updates and introduces layerwise tuning with query budget reassignment.
  - [corpus]: Weak - related to multi-armed bandit approaches but no direct evidence for this specific layerwise dynamic allocation mechanism.
- Break condition: If all layers contribute equally or if the layer importance signal is too noisy, the dynamic allocation strategy may not provide benefits over uniform allocation.

### Mechanism 3
- Claim: Antithetic sampling and normalization of feedbacks stabilize gradient estimation and improve optimization convergence.
- Mechanism: The framework uses antithetic sampling (sampling symmetric points) to reduce variance in gradient estimates, and normalizes feedbacks to maintain constant learning rates and provide conditions for theoretical analysis.
- Core assumption: Antithetic sampling provides lower-variance gradient estimates, and feedback normalization allows for consistent learning rates across different performance scales.
- Evidence anchors:
  - [section]: "Implementation with Gaussian Prior" section describes using antithetic sampling and feedback normalization in the PPS algorithm.
  - [section]: "Quality Analysis of the Estimated Gradient" section provides theoretical justification for why antithetic sampling preserves gradient information.
  - [corpus]: Weak - related to variance reduction techniques in optimization but no direct evidence for this specific combination in the context of restrictive feedback tuning.
- Break condition: If the performance landscape is highly non-smooth or if feedbacks are extremely noisy, even these stabilization techniques may not prevent gradient estimation from becoming unreliable.

## Foundational Learning

- Concept: Distribution-based optimization
  - Why needed here: The framework needs to optimize model parameters without direct access to target data, so it treats parameters as samples from a distribution and optimizes this distribution instead of individual parameter values.
  - Quick check question: How does treating model parameters as samples from a distribution allow optimization without direct access to target data?

- Concept: Multi-armed bandit problem
  - Why needed here: When tuning complex models with many layers, the framework needs to decide which layers to update with limited queries, which is formulated as a multi-armed bandit problem where each layer is an "arm" and the goal is to maximize cumulative reward (performance improvement).
  - Quick check question: How does formulating layer selection as a multi-armed bandit problem help with efficient query allocation in complex models?

- Concept: Gradient estimation through Monte Carlo sampling
  - Why needed here: The framework needs to estimate gradients of the performance function without analytical access, so it uses Monte Carlo sampling to approximate these gradients based on feedback from sampled models.
  - Quick check question: Why is Monte Carlo sampling necessary for gradient estimation in this framework, and what are its limitations?

## Architecture Onboarding

- Component map: Pre-trained model -> PPS/LCPS parameter optimization -> Feedback collection -> Gradient estimation -> Parameter distribution update -> Performance evaluation
- Critical path: Sample candidate models → query feedbacks → estimate gradients → update distribution parameters → repeat until query budget exhausted or performance satisfactory
- Design tradeoffs: The framework trades off between exploration (sampling diverse models to learn the performance landscape) and exploitation (focusing on promising regions). It also trades off between computational efficiency (layerwise updates) and potential suboptimal convergence (not updating all parameters simultaneously).
- Failure signatures: The framework may fail if feedbacks are too noisy, if the evaluation function is non-smooth, if the query budget is too small to learn the performance landscape, or if the parameter distribution cannot adequately represent the optimal model parameters.
- First 3 experiments:
  1. Implement PPS on a simple MLP with known performance landscape to verify gradient estimation and convergence properties.
  2. Apply LCPS to a multi-layer network with synthetic data to test layerwise update dynamics and query allocation strategies.
  3. Test the framework on a real-world dataset with limited target data to validate performance improvements compared to baseline methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PPS and LCPS scale with increasingly complex neural network architectures beyond the tested residual networks and BERT variants?
- Basis in paper: [explicit] The paper mentions testing on residual networks and BERT variants but does not explore more complex architectures like transformers with more layers or architectures like Vision Transformers.
- Why unresolved: The experiments were limited to specific architectures, and the paper does not provide theoretical or empirical evidence for how the methods would perform on more complex or different architectures.
- What evidence would resolve it: Conducting experiments on a variety of more complex architectures and comparing the performance to the current methods would provide evidence for or against the scalability of PPS and LCPS.

### Open Question 2
- Question: What is the impact of using different types of feedback signals (e.g., F1 score, precision-recall curves) on the performance of the PPS and LCPS algorithms?
- Basis in paper: [explicit] The paper mentions using accuracy and usage rate as feedback signals but does not explore other types of feedback.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on the impact of different feedback signals on the algorithms' performance.
- What evidence would resolve it: Conducting experiments using different types of feedback signals and comparing the performance to the current methods would provide evidence for or against the robustness of PPS and LCPS to different feedback types.

### Open Question 3
- Question: How does the query efficiency of PPS and LCPS compare to other gradient-free optimization methods like evolutionary strategies or Bayesian optimization when applied to model tuning?
- Basis in paper: [explicit] The paper mentions that other gradient-free optimization methods like CMA-ES and Bayesian optimization could be used but prefers PPS for its scalability and performance.
- Why unresolved: The paper does not provide a direct comparison of the query efficiency of PPS and LCPS to other gradient-free optimization methods.
- What evidence would resolve it: Conducting experiments comparing the query efficiency of PPS and LCPS to other gradient-free optimization methods would provide evidence for or against the relative efficiency of the proposed methods.

## Limitations
- Distribution assumption validity: The framework assumes model parameters follow a Gaussian distribution that may not hold for all model architectures or optimization landscapes
- Feedback signal quality: Success heavily depends on the quality and informativeness of restrictive feedback signals
- Query budget sensitivity: Optimal query budget allocation between PPS and LCPS is not thoroughly explored and may be highly sensitive to hyperparameters

## Confidence
- High Confidence: The basic premise of optimizing model parameters through feedback signals without direct access to target data is theoretically sound and well-established in the black-box optimization literature
- Medium Confidence: The layerwise coordinate parameter search with dynamic query allocation shows promise but needs more empirical validation across diverse architectures
- Low Confidence: The specific combination of antithetic sampling and feedback normalization for stabilizing gradient estimation lacks strong empirical validation in this context

## Next Checks
1. **Ablation Study on Feedback Mechanisms**: Conduct controlled experiments removing antithetic sampling and normalization to quantify their individual contributions to performance. Test with synthetic feedback signals of varying noise levels to establish robustness boundaries.

2. **Cross-Architecture Generalization**: Apply LCPS to fundamentally different model architectures (RNNs, Transformers, Graph Neural Networks) beyond standard CNNs to validate whether the layerwise dynamic allocation strategy generalizes across architectural families.

3. **Feedback Dimensionality Scaling**: Systematically vary the dimensionality of feedback signals (from single scalar to multi-dimensional metrics) and measure the impact on gradient estimation quality and optimization convergence. This would establish practical limits on the framework's applicability to complex evaluation scenarios.