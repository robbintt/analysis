---
ver: rpa2
title: A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression
arxiv_id: '2310.00987'
source_url: https://arxiv.org/abs/2310.00987
tags:
- error
- test
- kernel
- bound
- upper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives sharp non-asymptotic upper and lower bounds
  on the test error of finite-rank kernel ridge regression (KRR) in the under-parameterized
  regime (where the rank M of the kernel is less than the sample size N). The key
  idea is to carefully analyze the finite-rank KRR test error using algebraic manipulations
  and concentration results for sub-Gaussian random covariance matrices.
---

# A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression

## Quick Facts
- arXiv ID: 2310.00987
- Source URL: https://arxiv.org/abs/2310.00987
- Reference count: 40
- This paper derives sharp non-asymptotic upper and lower bounds on the test error of finite-rank kernel ridge regression (KRR) in the under-parameterized regime.

## Executive Summary
This paper provides a theoretical analysis of the test error for finite-rank kernel ridge regression in the under-parameterized regime (M < N). The authors derive sharp non-asymptotic bounds on the bias and variance components of the test error that are tighter than previous results and remain valid for any regularization parameter λ. The key innovation is analyzing the fluctuation matrix in the L2(ρ) basis rather than the RKHS basis, which allows decoupling the kernel's spectral decay from sampling randomness. The bounds are validated through experiments on truncated neural tangent and Legendre kernels, showing significant improvements over previous bounds.

## Method Summary
The method analyzes finite-rank kernel ridge regression by decomposing the test error into bias and variance components. The key technical innovation is working in the L2(ρ) basis rather than the RKHS basis, which allows expressing the test error in terms of a fluctuation matrix δ and its powers. The authors use Neumann series expansion and concentration inequalities for sub-Gaussian random covariance matrices to bound the operator norm of δ and control the randomness in the KRR solution. This approach yields tighter bounds than previous work while remaining valid for any regularization parameter.

## Key Results
- Derived upper bounds on bias that improve upon previous work by a factor of 1/4 and include non-asymptotic decay terms depending on N
- Obtained variance bounds with a much faster 1/N decay rate compared to the previous log N/N rate
- Validated bounds through experiments on truncated neural tangent and Legendre kernels, showing close tracking of true test error
- The bounds remain valid for any regularization parameter λ, unlike comparable previous results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The key improvement comes from analyzing the fluctuation matrix in the L2(ρ) basis instead of the RKHS basis.
- Mechanism: By working in the L2(ρ) basis, the analysis decouples the effect of the kernel's spectral decay from the sampling randomness, allowing sharper bounds that don't depend on the ridge parameter λ.
- Core assumption: The eigenfunctions ψk are well-behaved (sub-Gaussian) under the sampling distribution ρ.
- Evidence anchors:
  - [abstract]: "Our bounds are tighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters."
  - [section]: "The main technical difference from prior works is that we consider the basis {ψk}M k=1 in the function space L2 ρ instead of {λ−1/2 k ψk}M k=1 in the RKHS H."
- Break condition: If the eigenfunctions are not sub-Gaussian or have heavy tails, the concentration results fail.

### Mechanism 2
- Claim: The Neumann series expansion of the resolvent matrix provides tight control over the bias and variance components.
- Mechanism: The expansion allows expressing the test error in terms of the fluctuation matrix δ and its powers, which can be bounded using concentration inequalities for random matrices.
- Core assumption: The operator norm of the fluctuation matrix δ is bounded by a constant less than 1.
- Evidence anchors:
  - [section]: "We first bound the operator norm δ of the fluctuation matrix ∆" and the use of concentration results from Vershynin [2010].
  - [section]: "Lemma C.9 (B-Expansion). Given that δ = ∥∆∥op < 1. It holds that lim n↑∞ ∥B − Pn s=0(− ¯P∆)s ¯P ∥op = 0"
- Break condition: If N is too small relative to M, the fluctuation matrix δ may not be well-controlled.

### Mechanism 3
- Claim: The bias-variance decomposition provides a natural framework for obtaining sharp non-asymptotic bounds.
- Mechanism: By separately bounding the bias (fitting error + finite rank error) and variance (noise term), the analysis can exploit different concentration properties for each component.
- Core assumption: The target function can be decomposed into a component in the RKHS and an orthogonal complement.
- Evidence anchors:
  - [section]: "The analysis of this paper also follows the classical bias-variance decomposition, thus we write RZ,λ = bias + variance"
  - [section]: "Proposition C.5 (Bias). Recall that B = (IM + ∆ + λΛ−1)−1. The bias Ex f λ X(x) − ˜f(x)2 has the following expression"
- Break condition: If the target function has significant components outside the RKHS, the bias bounds may be loose.

## Foundational Learning

- Concept: Sub-Gaussian random variables and concentration inequalities
  - Why needed here: To bound the operator norm of the fluctuation matrix and control the randomness in the KRR solution.
  - Quick check question: What is the sub-Gaussian norm of a bounded random variable?

- Concept: Mercer decomposition and reproducing kernel Hilbert spaces (RKHS)
  - Why needed here: To express the finite-rank kernel as a sum of eigenfunctions and eigenvalues, and to define the hypothesis space for KRR.
  - Quick check question: What is the relationship between the eigenvalues of a kernel and the norm of a function in the corresponding RKHS?

- Concept: Bias-variance tradeoff and decomposition
  - Why needed here: To separate the approximation error (bias) from the estimation error (variance) in the KRR test error.
  - Quick check question: How does the ridge parameter λ affect the bias and variance components of the test error?

## Architecture Onboarding

- Component map: Data generation -> Kernel definition -> KRR training -> Test error computation -> Bound computation

- Critical path:
  1. Sample training data from ρ
  2. Compute the kernel matrix and its eigendecomposition
  3. Solve for the KRR solution using the resolvent matrix
  4. Evaluate the test error on held-out data
  5. Compute the theoretical bounds using the fluctuation matrix and concentration results

- Design tradeoffs:
  - Choosing M (rank of kernel) vs N (sample size): Affects the bias-variance tradeoff and the validity of the bounds
  - Selecting λ (ridge parameter): Balances stability and approximation accuracy
  - Using L2(ρ) vs RKHS basis: Impacts the tightness of the bounds and the assumptions required

- Failure signatures:
  - Large gap between upper and lower bounds: Indicates looseness in the analysis or violation of assumptions
  - Bounds not improving with N: Suggests issues with concentration results or eigenfunction behavior
  - Numerical instability in computing the resolvent: May indicate ill-conditioning or insufficient regularization

- First 3 experiments:
  1. Verify the concentration of the fluctuation matrix δ for increasing N and fixed M
  2. Compare the theoretical bounds to empirical test error for varying λ and N
  3. Test the sensitivity of the bounds to the choice of eigenfunctions (e.g., Fourier vs Legendre)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the derived bounds extend to the over-parameterized regime (M > N) and can they explain the double descent phenomenon?
- Basis in paper: [explicit] The authors mention this as a future research direction, noting they treated under-parameterized and over-parameterized regimes separately for sharpest bounds
- Why unresolved: The paper explicitly focuses on the under-parameterized regime and leaves the over-parameterized case for future work
- What evidence would resolve it: Derivation and validation of bounds in the M > N regime, showing their behavior near the interpolation threshold and ability to capture double descent curves

### Open Question 2
- Question: What is the optimal choice of finite-rank kernel that minimizes the test error bounds for a given target function?
- Basis in paper: [inferred] The bounds depend on the kernel eigenvalues λk and the alignment between the target function and kernel eigenfunctions, suggesting there's an optimal kernel design
- Why unresolved: The paper provides bounds for any finite-rank kernel but doesn't address which kernel choice minimizes these bounds for a specific problem
- What evidence would resolve it: A systematic study comparing different kernel constructions (random features, Nyström, NTK truncations) and their impact on the derived bounds

### Open Question 3
- Question: How do the bounds change when relaxing the sub-Gaussian assumption on the input distribution?
- Basis in paper: [explicit] The bounds rely on Assumption 4.1 requiring sub-Gaussian inputs, but the paper doesn't explore what happens when this is violated
- Why unresolved: The assumption is stated but not tested against heavier-tailed distributions or bounded but non-sub-Gaussian cases
- What evidence would resolve it: Experiments with non-sub-Gaussian input distributions (e.g., heavy-tailed or discrete) showing how the bounds degrade or require modification

## Limitations
- The analysis relies heavily on the assumption that eigenfunctions are sub-Gaussian under the sampling distribution, which may not hold for all kernels
- Results are derived only for the under-parameterized regime (M < N), with over-parameterized case left unexplored
- Numerical experiments are limited to two specific finite-rank kernels, limiting generalizability

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Derivation of theoretical bounds and bias-variance decomposition framework | High |
| Improvement in bounds compared to previous work | Medium |
| Numerical validation of bounds across diverse kernels | Low |

## Next Checks

1. Test the bounds for a wider range of kernels and hyperparameters: Conduct experiments with a diverse set of finite-rank kernels (e.g., polynomial, Gaussian) and varying values of M, N, and λ to assess the generality of the bounds.

2. Analyze the sensitivity to eigenfunction properties: Investigate how the bounds are affected by different assumptions on the eigenfunction behavior (e.g., sub-Gaussian vs sub-exponential) and explore the impact of heavy-tailed eigenfunctions on the concentration results.

3. Validate the bounds in the over-parameterized regime: Extend the analysis to the case where M > N and derive corresponding bounds on the test error. Compare these bounds to the under-parameterized regime and assess the transition behavior.