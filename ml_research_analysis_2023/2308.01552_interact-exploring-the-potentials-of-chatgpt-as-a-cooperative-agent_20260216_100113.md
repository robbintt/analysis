---
ver: rpa2
title: 'InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent'
arxiv_id: '2308.01552'
source_url: https://arxiv.org/abs/2308.01552
tags:
- arxiv
- chatgpt
- tasks
- language
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores integrating OpenAI's ChatGPT into embodied agent
  systems for interactive decision-making. InterAct, the proposed approach, assigns
  ChatGPT various roles like checker and sorter, integrating them with the original
  language model.
---

# InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent

## Quick Facts
- **arXiv ID**: 2308.01552
- **Source URL**: https://arxiv.org/abs/2308.01552
- **Reference count**: 40
- **Primary result**: InterAct achieves 98% success rate across 6 household tasks in AlfWorld, outperforming ReAct's 75% accuracy

## Executive Summary
InterAct integrates OpenAI's ChatGPT into embodied agent systems for interactive decision-making in text-based environments. By assigning ChatGPT specialized roles like checker and sorter, and modifying the original ReAct model's prompt for trajectory planning, InterAct significantly improves performance on household tasks. The approach demonstrates ChatGPT's ability to comprehend and perform complex tasks effectively, achieving a 98% success rate compared to ReAct's 75% in the AlfWorld benchmark. This work highlights the potential of role specialization and prompt engineering in enhancing LLM-based embodied agents.

## Method Summary
InterAct is an approach that integrates ChatGPT into embodied agent systems by assigning it various roles and modifying the original ReAct model's prompt. The method involves using ChatGPT as a checker to verify object identification, a sorter to rank likely locations for objects, and modifying the ReAct prompt to include trajectory planning for multi-object tasks. This specialization allows each component to handle specific reasoning tasks, reducing confusion and errors. The approach is tested on the AlfWorld benchmark, a suite of text-based environments with 6 different household tasks, demonstrating significant performance improvements over the base ReAct agent.

## Key Results
- InterAct achieves a 98% success rate across 6 household tasks in AlfWorld
- Significant improvement over ReAct's 75% accuracy
- Demonstrates ChatGPT's ability to comprehend and perform complex tasks effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InterAct's performance gains stem from role specialization—delegating object identification to a dedicated checker and ranking tasks to a dedicated sorter.
- Mechanism: By splitting responsibility among ChatGPT, InstructGPT, and specialized modules, each agent handles the type of reasoning it is most competent at, reducing confusion and errors in the original monolithic prompt.
- Core assumption: InstructGPT's lack of commonsense reasoning causes it to misidentify objects, while ChatGPT's RLHF training improves contextual understanding and ranking logic.
- Evidence anchors:
  - [abstract] "We feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model."
  - [section III-B] "We employ ChatGPT as a checker by providing it with appropriate prompts... ChatGPT can successfully distinguish between similar objects."
  - [corpus] No direct evidence in neighbors; this claim is unique to the InterAct paper.
- Break condition: If either checker or sorter is unavailable, the system reverts to ReAct-level performance.

### Mechanism 2
- Claim: The new trajectory planning prevents redundant exploration and mitigates "hallucinations" in multi-step pick tasks.
- Mechanism: After placing the first object, the model explicitly plans a second search trajectory that revisits previously promising locations before exploring new ones, reducing the chance of missing the second item.
- Core assumption: Without explicit trajectory planning, the agent forgets earlier search locations and wastes steps revisiting irrelevant areas.
- Evidence anchors:
  - [section III-B] "We made changes to the original model's prompt... the model to autonomously generate a trajectory while ensuring that this path does not overlook areas where the second object might be present."
  - [section IV] "In the 'pick2' task... ReAct alone tends to forget its previous locations, resulting in inefficient trajectories characterized by frequent revisits to the same place."
  - [corpus] No evidence in neighbors; unique to InterAct.
- Break condition: If the second object is not in any previously searched location, the benefit of revisiting is lost.

### Mechanism 3
- Claim: Using ChatGPT's stronger reasoning ability to generate sorted search lists outperforms InstructGPT's naive ranking.
- Mechanism: ChatGPT generates a ranked list of likely locations for an object, which the agent then uses to plan an efficient search path.
- Core assumption: InstructGPT's ranking is "often falls short" because it lacks sufficient training in factual knowledge and commonsense reasoning.
- Evidence anchors:
  - [section III-B] "ChatGPT has been fine-tuned using Reinforcement Learning with Human Feedback (RLHF) and has demonstrated a more nuanced understanding of various situations... to improve the efficiency of predicting object locations."
  - [section IV] "ReAct, without any helper, faces difficulties in accurately determining the presence of items in a specific location or employing ineffective search strategies."
  - [corpus] No direct evidence in neighbors; unique claim.
- Break condition: If the environment changes so that prior knowledge is irrelevant, ranking becomes less useful.

## Foundational Learning

- Concept: Embodied agent systems in text-based environments
  - Why needed here: InterAct operates in AlfWorld, a simulated household requiring navigation and object interaction via text commands.
  - Quick check question: In AlfWorld, what are the two main types of actions an agent can take at each step?
    - Answer: `<tool_call>` (verbalize internal reasoning) and `<action>` (issue a command to the environment).

- Concept: Prompt engineering with few-shot examples
  - Why needed here: ReAct and InterAct rely on few-shot trajectories to guide the agent's reasoning and action selection.
  - Quick check question: How does InterAct modify the original ReAct prompt to handle multi-object tasks?
    - Answer: It adds trajectory planning logic that explicitly considers revisiting previous locations for the second object.

- Concept: LLM role specialization
  - Why needed here: InterAct assigns different responsibilities (checker, sorter) to specialized prompts to leverage each model's strengths.
  - Quick check question: What problem does the checker module solve that ReAct's single model cannot?
    - Answer: It prevents object misidentification by distinguishing between similar objects (e.g., pan vs. pot).

## Architecture Onboarding

- Component map:
  Main agent (ChatGPT with modified prompt) -> Checker module (ChatGPT with object verification prompt) -> Sorter module (ChatGPT with location ranking prompt) -> Environment (AlfWorld simulator) -> Original base (ReAct agent for comparison)

- Critical path:
  1. Parse task goal → generate search target list
  2. Feed search target to sorter → get ranked location list
  3. Sequentially visit locations, using checker to verify object presence
  4. Upon finding object, take it and plan next steps
  5. For multi-object tasks, use trajectory planning to revisit prior locations

- Design tradeoffs:
  - **Pro**: Specialized roles reduce confusion and improve accuracy
  - **Con**: Increased latency due to multiple LLM calls
  - **Pro**: Can swap modules independently for experimentation
  - **Con**: Heavy reliance on prompt quality and completeness

- Failure signatures:
  - Repeated identical actions → missing or malformed prompt components
  - Object misidentification → checker prompt not properly specified
  - Inefficient search → sorter prompt not generating correct ranking

- First 3 experiments:
  1. Run InterAct on a single "heat" task without sorter or checker to confirm baseline improvement from trajectory planning alone
  2. Add checker module to a "pick2" task to measure reduction in object misidentification errors
  3. Add sorter module to a "clean" task to measure improvement in search efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does InterAct's performance scale when applied to more complex tasks or larger environments beyond the AlfWorld benchmark?
- Basis in paper: [inferred] The paper mentions that InterAct achieves a 98% success rate in AlfWorld, but also notes that the dataset has limitations in terms of task variety and quantity. It suggests that the model would need improvement to maintain accuracy in more diverse environments.
- Why unresolved: The current evaluation is limited to a specific dataset with a finite set of tasks and environments. The paper does not provide evidence of InterAct's performance in more complex or varied settings.
- What evidence would resolve it: Testing InterAct on larger and more diverse datasets with a wider range of tasks and environments would provide insights into its scalability and performance in real-world applications.

### Open Question 2
- Question: How does the accuracy of the checker and sorter components in InterAct affect the overall performance of the model?
- Basis in paper: [explicit] The paper introduces the checker and sorter components to address issues like object misidentification and inefficient planning. It mentions that ChatGPT is used as a checker and decision-making component for sorting.
- Why unresolved: While the paper demonstrates the integration of these components, it does not provide a detailed analysis of their individual contributions to the model's success or how their accuracy impacts overall performance.
- What evidence would resolve it: Conducting experiments to isolate and evaluate the performance of the checker and sorter components separately would help understand their individual impact on the model's success rate.

### Open Question 3
- Question: What are the limitations of using GPT-4 as a supervisor for error detection in InterAct, and how can these be addressed?
- Basis in paper: [explicit] The paper discusses the potential use of GPT-4 as a supervisor for error detection, noting its significant improvement in error detection performance compared to ChatGPT. However, it also mentions that GPT-4 is not available as an open-source model and requires funding for extensive simulations.
- Why unresolved: The paper highlights the potential benefits of using GPT-4 for error detection but does not explore the limitations or propose solutions to overcome the accessibility and cost barriers.
- What evidence would resolve it: Investigating alternative models or methods for error detection that are more accessible and cost-effective could provide solutions to the limitations mentioned. Additionally, conducting simulations with GPT-4, if possible, would offer insights into its effectiveness as a supervisor.

## Limitations

- The exact prompt formulations for the checker and sorter modules are not provided, making precise reproduction difficult
- The paper does not present ablation studies needed to isolate the effects of role specialization and trajectory planning on performance improvements
- Results may not generalize to future model versions or alternative LLMs due to heavy reliance on ChatGPT's current capabilities

## Confidence

- **High Confidence**: The overall success rate improvement (98% vs 75%) is well-documented and reproducible given the public AlfWorld benchmark
- **Medium Confidence**: The general concept of role specialization and trajectory planning is clearly described, though implementation details are missing
- **Low Confidence**: Claims about why InstructGPT performs worse than ChatGPT (lack of commonsense reasoning) are asserted without comparative empirical validation

## Next Checks

1. **Ablation Study**: Implement InterAct without the checker module and without the sorter module separately to quantify each component's contribution to the 23% performance gain

2. **Prompt Sensitivity Analysis**: Test whether the performance improvements persist with variations in the trajectory planning prompt, particularly the logic for revisiting previously searched locations

3. **Model Generalization Test**: Evaluate InterAct's performance using GPT-4 instead of ChatGPT to determine whether improvements stem from model capabilities or the InterAct architecture itself