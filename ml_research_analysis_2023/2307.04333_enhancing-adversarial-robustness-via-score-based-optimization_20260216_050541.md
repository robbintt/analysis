---
ver: rpa2
title: Enhancing Adversarial Robustness via Score-Based Optimization
arxiv_id: '2307.04333'
source_url: https://arxiv.org/abs/2307.04333
tags:
- adversarial
- diffusion
- attacks
- optimization
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ScoreOpt, a novel adversarial defense scheme
  that optimizes adversarial samples at test-time using score-based priors from pre-trained
  diffusion models. The key idea is to derive the posterior distribution of clean
  samples given an adversarial example, and then optimize the adversarial sample towards
  the points with locally maximum confidence of this posterior distribution.
---

# Enhancing Adversarial Robustness via Score-Based Optimization

## Quick Facts
- **arXiv ID**: 2307.04333
- **Source URL**: https://arxiv.org/abs/2307.04333
- **Reference count**: 40
- **Primary result**: ScoreOpt achieves state-of-the-art robust accuracy against various adversarial attacks while improving inference speed compared to sequential denoising methods.

## Executive Summary
ScoreOpt is a novel adversarial defense scheme that optimizes adversarial samples at test-time using score-based priors from pre-trained diffusion models. The key innovation is deriving the posterior distribution of clean samples given an adversarial example and then optimizing toward points with locally maximum confidence of this posterior distribution. The method introduces effective loss functions including a novel score regularizer and proposes practical algorithms (ScoreOpt-O and ScoreOpt-N) that outperform existing adversarial defenses in both robustness performance and inference speed across multiple datasets.

## Method Summary
ScoreOpt formulates adversarial defense as an optimization problem where the goal is to recover the original clean sample from an adversarial example. The method uses pre-trained diffusion models to provide score-based priors that guide the optimization process. Unlike sequential denoising approaches, ScoreOpt employs a single optimization step with gradient-based algorithms to move adversarial samples toward regions of high posterior confidence. The framework introduces a score regularizer loss that encourages consistency between denoised versions of clean and adversarial samples, improving both standard and robust accuracy. The method is evaluated on CIFAR-10, CIFAR-100, and ImageNet using various attack types including transfer-based, BPDA+EOT, and adaptive white-box attacks.

## Key Results
- Achieves state-of-the-art robust accuracy against Transfer-PGD, BPDA+EOT, and PGD+EOT attacks
- Outperforms existing adversarial defenses in both robustness performance and inference speed
- Demonstrates effectiveness across multiple datasets including CIFAR-10, CIFAR-100, and ImageNet
- Shows insensitivity to hyperparameter selection through the use of score regularizer loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ScoreOpt achieves adversarial robustness by optimizing adversarial samples toward the maximum posterior distribution of clean samples using score-based priors from pre-trained diffusion models.
- **Mechanism**: The method computes the posterior distribution of clean samples given an adversarial example, then uses gradient-based optimization guided by diffusion model scores to move the adversarial sample toward regions of high posterior confidence.
- **Core assumption**: The posterior distribution of clean samples given adversarial examples can be effectively approximated using score-based priors from diffusion models.
- **Evidence anchors**:
  - [abstract] "Our key intuition is to derive the posterior distribution of clean samples given a specific adversarial example. Then adversarial samples can be optimized towards the points that maximize the posterior distribution with gradient-based algorithms at test time."
  - [section] "Our main idea is to formulate the adversarial defense as an optimization problem given the perturbed sample and the pre-trained prior, in which the solution to the optimization problem is the recovered original sample that we want."
- **Break condition**: If the pre-trained diffusion model does not capture the true data distribution well, the score guidance will lead to incorrect optimization directions.

### Mechanism 2
- **Claim**: The score regularizer loss function improves both standard and robust accuracy by encouraging consistency between denoised versions of clean and adversarial samples.
- **Mechanism**: The score regularizer adds a constraint term that minimizes the pixel-level distance between denoised versions of the current sample and the initial adversarial sample, promoting semantic preservation while removing adversarial perturbations.
- **Core assumption**: Adversarial perturbations and clean samples share similar semantic content that can be preserved through score-based regularization.
- **Evidence anchors**:
  - [section] "We propose to introduce a hyperparameter-free score regularizer (SR) loss: LSR(x, xa, θ) = Et∼U(0,1),ϵ1,ϵ2∼N(0,I) h ∥Dθ(x + σtϵ1; t) − x∥22 + ∥Dθ(x + σtϵ1; t) − Dθ(xa + σtϵ2; t)∥22 i"
  - [section] "Figure 2b and 2c demonstrate the effectiveness of the SR loss. As the number of optimization steps increases, both the standard and robust accuracy converge to stable values, with the latter remaining close to the optimal."
- **Break condition**: If adversarial perturbations significantly alter semantic content beyond what the diffusion model can recognize, the regularization may fail to preserve meaningful information.

### Mechanism 3
- **Claim**: ScoreOpt achieves state-of-the-art robustness while improving inference speed compared to sequential denoising methods.
- **Mechanism**: By using a single optimization step with score-based priors rather than sequential denoising steps, ScoreOpt reduces computational overhead while maintaining or improving robustness performance.
- **Core assumption**: The score-based optimization can effectively denoise adversarial examples in fewer steps than sequential denoising approaches.
- **Evidence anchors**:
  - [section] "In contrast to previous diffusion-based purification methods, our optimization framework departs from the sequential step-by-step denoising procedure."
  - [section] "As shown in Table 7, our time cost is about twice under the same steps. However, ScoreOpt needs only a few steps (about 5) to obtain better results than the multi-step denoising method."
- **Break condition**: If the optimization landscape becomes too complex for simple gradient-based methods to navigate effectively, the single-step approach may underperform sequential methods.

## Foundational Learning

- **Concept**: Diffusion models and score matching
  - Why needed here: ScoreOpt relies on pre-trained diffusion models to provide score-based priors for optimization guidance.
  - Quick check question: What is the relationship between the score function and the gradient of the log probability density in diffusion models?

- **Concept**: Bayesian inference and posterior distribution
  - Why needed here: The method formulates adversarial defense as finding the maximum a posteriori estimate of clean samples given adversarial examples.
  - Quick check question: How does the posterior distribution p(x|xa) relate to the prior p(x) and likelihood p(xa|x) in the context of adversarial purification?

- **Concept**: Adversarial attacks and threat models
  - Why needed here: Understanding different attack types (transfer-based, white-box, BPDA+EOT) is crucial for evaluating the effectiveness of ScoreOpt.
  - Quick check question: What distinguishes transfer-based attacks from adaptive white-box attacks in terms of the information available to the attacker?

## Architecture Onboarding

- **Component map**: Base classifier -> ScoreOpt optimization module (using diffusion model scores) -> Purified sample -> Classification
- **Critical path**: Adversarial example → ScoreOpt optimization (using diffusion model scores) → Purified sample → Classification
- **Design tradeoffs**:
  - Single optimization step vs. sequential denoising: Faster inference but potentially less thorough denoising
  - Score regularizer vs. MSE loss: Better semantic preservation but potentially higher computational cost
  - Fixed noise schedule vs. adaptive selection: Simpler implementation but may not optimize for all sample types
- **Failure signatures**:
  - Robust accuracy decreases while standard accuracy remains high: Score regularizer may be over-constraining
  - Both accuracies drop significantly: Diffusion model may not capture true data distribution well
  - Optimization fails to converge: Learning rate or noise level schedule may need adjustment
- **First 3 experiments**:
  1. Implement ScoreOpt with Diff loss on CIFAR-10 using a pre-trained EDM model and evaluate against Transfer-PGD attack
  2. Compare ScoreOpt with SR loss against MSE loss on the same dataset and attack type
  3. Test ScoreOpt against BPDA+EOT attack on CIFAR-100 to verify robustness against strong adaptive attacks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of ScoreOpt scale with the complexity and diversity of the adversarial attack?
- **Basis in paper**: [inferred] The paper evaluates ScoreOpt against transfer-based attacks, BPDA+EOT attacks, and adaptive white-box attacks, but does not explore a wide range of attack methods or attack complexities.
- **Why unresolved**: The paper does not provide a comprehensive evaluation of ScoreOpt's performance against a diverse set of adversarial attacks, including more complex and sophisticated attack methods.
- **What evidence would resolve it**: Conducting experiments with a wider range of adversarial attacks, including more complex and sophisticated methods, would provide evidence for how well ScoreOpt scales with attack complexity and diversity.

### Open Question 2
- **Question**: What is the impact of the optimization hyperparameters on the performance of ScoreOpt, and how sensitive is the method to their selection?
- **Basis in paper**: [explicit] The paper mentions that the performance of MSE optimization is highly dependent on the weighting hyperparameter, and that the SR loss shows insensitivity to the weighting hyperparameter.
- **Why unresolved**: The paper does not provide a comprehensive analysis of the impact of optimization hyperparameters on ScoreOpt's performance, or how sensitive the method is to their selection.
- **What evidence would resolve it**: Conducting experiments with different optimization hyperparameters and analyzing their impact on ScoreOpt's performance would provide evidence for the sensitivity of the method to hyperparameter selection.

### Open Question 3
- **Question**: How does the performance of ScoreOpt compare to other state-of-the-art adversarial defense methods, particularly those that do not rely on diffusion models?
- **Basis in paper**: [inferred] The paper compares ScoreOpt to other diffusion-based adversarial defense methods, but does not provide a comprehensive comparison to other state-of-the-art methods that do not rely on diffusion models.
- **Why unresolved**: The paper does not provide a comprehensive comparison of ScoreOpt to other state-of-the-art adversarial defense methods, particularly those that do not rely on diffusion models.
- **What evidence would resolve it**: Conducting experiments comparing ScoreOpt to other state-of-the-art adversarial defense methods, including those that do not rely on diffusion models, would provide evidence for how ScoreOpt compares to other methods in terms of performance and robustness.

## Limitations

- Performance heavily relies on the quality and generalization capability of pre-trained diffusion models
- Computational overhead at inference time, while reduced, still represents a significant increase over standard classification
- Limited evaluation across diverse attack types and complexities, particularly query-based attacks

## Confidence

- **High confidence**: The theoretical framework connecting diffusion model scores to posterior distribution estimation is sound and well-established in the literature.
- **Medium confidence**: The empirical results showing improved robust accuracy are compelling, but the sample size across different architectures and datasets is limited.
- **Medium confidence**: The computational efficiency claims are supported by timing experiments, but the practical implications for deployment scenarios require further investigation.

## Next Checks

1. Test ScoreOpt's performance when using diffusion models trained on different datasets (e.g., using CIFAR-trained models for ImageNet data) to assess generalization capabilities.
2. Conduct ablation studies varying the number of optimization steps and noise schedule parameters to identify the minimum viable configuration.
3. Evaluate ScoreOpt's performance against a broader range of attack types, including query-based attacks and attacks that specifically target diffusion-based defenses.