---
ver: rpa2
title: 'StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language
  Models'
arxiv_id: '2310.13673'
source_url: https://arxiv.org/abs/2310.13673
tags:
- people
- warmth
- competence
- groups
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes STEREO MAP, a framework grounded in the Stereotype
  Content Model (SCM) from psychology, to quantify and analyze the awareness of human-like
  stereotypes in large language models (LLMs). STEREO MAP employs the dimensions of
  Warmth and Competence to map LLMs' perceptions of social groups, while also investigating
  associated emotions, behavioral tendencies, and reasoning verbalizations.
---

# StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models

## Quick Facts
- arXiv ID: 2310.13673
- Source URL: https://arxiv.org/abs/2310.13673
- Reference count: 40
- Primary result: STEREO MAP framework reveals that LLMs exhibit mixed-dimension clusters along Warmth and Competence, with awareness of social disparities and systematic biases.

## Executive Summary
This study introduces STEREO MAP, a framework grounded in the Stereotype Content Model (SCM) from psychology, to quantify and analyze the awareness of human-like stereotypes in large language models (LLMs). The framework uses Warmth and Competence dimensions to map LLMs' perceptions of social groups, while also investigating associated emotions, behavioral tendencies, and reasoning verbalizations. Applied to three LLMs (BARD, GPT-3.5, and DAVINCI), the framework reveals that all models exhibit mixed-dimension clusters consistent with human stereotype patterns, and demonstrate awareness of social disparities through statistical reasoning and research citations.

## Method Summary
The STEREO MAP framework prompts LLMs to rate social groups on Warmth and Competence dimensions derived from SCM theory. The framework collects ratings, clusters groups using K-means, maps clusters to known emotional and behavioral patterns, extracts reasoning keywords, and analyzes semantic content using the Stereotype Content Dictionary. The method employs both original and extended prompt versions to elicit comprehensive responses, then correlates dimensions with emotions and behaviors to validate the semantic grounding of LLM outputs.

## Key Results
- All three models (BARD, GPT-3.5, DAVINCI) exhibited mixed-dimension clusters along Warmth and Competence, with consistent clustering of groups like Asians and Jews in high-competence-low-warmth clusters.
- LLMs demonstrated awareness of social disparities, often citing statistical data and research findings to support their reasoning.
- Keyword analysis showed semantic grounding of LLM outputs, with clusters exhibiting expected associations (e.g., low competence/warmth clusters showing negative Morality, Status, and Health directions).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The STEREO MAP framework captures nuanced stereotype perceptions in LLMs by mapping outputs onto the Warmth-Competence dimensions derived from SCM theory.
- Mechanism: The framework prompts LLMs to rate social groups on Warmth and Competence, clusters the ratings using K-means, and maps clusters to known emotional and behavioral patterns. This structured mapping reveals which social groups are perceived similarly to human stereotypes.
- Core assumption: LLM outputs on Warmth-Competence correlate with how humans perceive stereotypes, so grouping LLMs' perceptions along these dimensions produces valid, interpretable clusters.
- Evidence anchors:
  - [abstract] "STEREO MAP maps LLMs' perceptions of social groups... using the dimensions of Warmth and Competence."
  - [section 5.1] Results show that all three models exhibited mixed-dimension clusters, (e.g. High Competence- Low Warmth), with consistent clustering of groups like Asians and Jews in high-competence-low-warmth clusters.
  - [corpus] Found 25 related papers; neighbor titles include studies on attribution bias and measuring bias in LLMs.
- Break condition: If LLM responses on Warmth-Competence diverge from SCM-predicted human patterns, cluster interpretation becomes unreliable.

### Mechanism 2
- Claim: The framework leverages reasoning analysis to reveal LLMs' awareness of systematic social disparities.
- Mechanism: By extracting keywords from LLM reasoning verbalizations and analyzing their association with social factors (e.g., status, discrimination), the framework uncovers how LLMs reason about social group success or failure.
- Core assumption: LLM reasoning verbalizations contain structured, extractable signals about social disparities that align with real-world patterns.
- Evidence anchors:
  - [abstract] "LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning."
  - [section 5.3] Analysis shows BARD citing sources like "median salary, according to the U.S. Census Bureau," while DAVINCI and GPT-3.5 emphasize "public perception and media influence."
  - [corpus] Related works include "Ask LLMs Directly, 'What shapes your bias?': Measuring Social Bias in Large Language Models."
- Break condition: If LLM reasoning becomes vague or refuses to answer (as seen with BARD on Poor Blacks), keyword extraction yields weak or biased signals.

### Mechanism 3
- Claim: Keyword mapping to stereotype content dimensions validates the semantic grounding of LLM outputs.
- Mechanism: LLM-generated keywords are mapped to dimensions (e.g., Morality, Status, Ability) using the Stereotype Content Dictionary, allowing comparison of group-level semantic patterns across clusters.
- Core assumption: The Stereotype Content Dictionary reliably links natural language keywords to underlying stereotype dimensions.
- Evidence anchors:
  - [section 3.2] "We utilized the Stereotype Content Dictionary... which offers a comprehensive linkage between words (e.g., confident) to specific dimensions (e.g., assertiveness)."
  - [section 5.2] Keywords analysis shows Cluster 0 (low competence/warmth) exhibiting negative Morality, Status, and Health directions, matching expectations.
  - [corpus] Related studies use content dictionaries to measure stereotype dimensions; neighbor titles include "Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4."
- Break condition: If keywords are too sparse or the dictionary mapping fails to capture domain-specific usage, semantic grounding weakens.

## Foundational Learning

- Concept: Stereotype Content Model (SCM)
  - Why needed here: SCM provides the theoretical grounding for defining and interpreting stereotype dimensions (Warmth, Competence) in LLM outputs.
  - Quick check question: What are the two fundamental dimensions in SCM, and how do they relate to stereotype perception?

- Concept: K-means clustering
  - Why needed here: Clustering LLM ratings on Warmth and Competence reveals natural groupings that align with SCM-predicted stereotype clusters.
  - Quick check question: How does K-means clustering help identify groups with similar stereotype profiles?

- Concept: Correlation analysis
  - Why needed here: Spearman's ρ correlation tests associations between dimensions, emotions, and behaviors, validating the link between stereotype content and predicted outcomes.
  - Quick check question: Why use Spearman's ρ instead of Pearson's r when correlating stereotype dimensions with emotions?

## Architecture Onboarding

- Component map: Prompt configuration (ORIGINAL/EXTENDED) -> LLM inference -> Rating extraction -> K-means clustering -> Emotion/behavior correlation -> Keyword extraction -> Dictionary mapping -> Reasoning analysis
- Critical path: 1. Prompt -> LLM output, 2. Rating extraction -> clustering, 3. Keyword extraction -> dictionary mapping, 4. Reasoning analysis -> insight generation
- Design tradeoffs:
  - Prompt length vs. response quality: Longer prompts improve depth but may increase refusal rates.
  - Dictionary coverage vs. manual annotation: Relying solely on the dictionary may miss domain-specific terms but ensures consistency.
- Failure signatures:
  - High refusal rates (e.g., BARD on Poor Blacks) -> incomplete coverage
  - Low keyword coverage (<60%) -> weak semantic grounding
  - Inconsistent cluster patterns across models -> unreliable SCM mapping
- First 3 experiments:
  1. Run prompt ablation to compare w/o CoT vs. full prompt impact on consistency and refusal rate.
  2. Test keyword lemmatization pipeline to improve dictionary coverage.
  3. Validate clustering stability by rerunning K-means with different seeds and checking cluster assignment consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models exhibit different stereotype patterns across cultures, and if so, how can these differences be systematically measured and compared?
- Basis in paper: [explicit] The paper acknowledges that stereotypes can vary across cultures and regions, and their findings may not generalize to other contexts. It also mentions that the analysis focused on stereotypes prevalent in the US.
- Why unresolved: The study only examined stereotypes in the context of the US, leaving open the question of how these patterns might differ in other cultural settings.
- What evidence would resolve it: Conducting similar analyses using social groups and stereotypes relevant to other cultures, then comparing the results across different cultural contexts to identify systematic differences in LLM stereotype patterns.

### Open Question 2
- Question: How do different prompting strategies (e.g., Chain-of-Thought vs. direct questioning) impact the consistency and depth of LLM responses regarding social stereotypes?
- Basis in paper: [explicit] The paper mentions using a Chain-of-Thought inspired prompt design and conducting sensitivity checks on prompts. It also discusses different versions of prompts (ORIGINAL vs. EXTENDED).
- Why unresolved: While the paper briefly touches on prompt variations, it doesn't provide a comprehensive analysis of how different prompting strategies affect LLM responses to stereotype-related questions.
- What evidence would resolve it: Systematically comparing LLM responses across various prompting strategies (including CoT, direct questioning, and other approaches) using metrics like consistency, depth of reasoning, and diversity of responses.

### Open Question 3
- Question: Can the STEREO MAP framework be effectively adapted to identify and measure implicit stereotypes that are not captured by predefined social groups and dimensions?
- Basis in paper: [inferred] The paper acknowledges limitations in focusing on explicit understanding of stereotypes using predefined groups and dimensions, which may overlook implicitly held and nuanced stereotypes.
- Why unresolved: The current framework relies on predefined social groups and dimensions, potentially missing more subtle or implicit stereotypes that exist in language models.
- What evidence would resolve it: Developing and testing extensions to the STEREO MAP framework that can capture implicit stereotypes, perhaps through unsupervised methods or by incorporating more diverse and nuanced social categorizations, then validating these extensions against known implicit bias datasets.

## Limitations
- The study relies on predefined social groups and may not capture intersectional identities or emerging social categories.
- LLM responses may be influenced by training data biases and prompt sensitivity, potentially limiting generalizability.
- The framework depends on the Stereotype Content Dictionary, which may not fully cover domain-specific or evolving language use.

## Confidence
- High: The clustering of social groups along Warmth-Competence dimensions aligns with established SCM theory and produces interpretable patterns.
- Medium: The reasoning analysis reveals awareness of social disparities, but the depth and reliability of LLM-generated explanations vary across models.
- Low: The semantic grounding of keywords via the Stereotype Content Dictionary may miss nuanced or context-specific language, reducing coverage accuracy.

## Next Checks
1. Conduct prompt ablation studies to measure the impact of prompt structure on response consistency and refusal rates.
2. Validate clustering stability by rerunning K-means with different seeds and comparing cluster assignments across models.
3. Perform manual annotation of a subset of keywords to assess dictionary coverage and improve semantic grounding accuracy.