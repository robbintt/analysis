---
ver: rpa2
title: Disinformation Capabilities of Large Language Models
arxiv_id: '2311.08838'
source_url: https://arxiv.org/abs/2311.08838
tags:
- llms
- disinformation
- text
- texts
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive study of the disinformation
  generation capabilities of current large language models (LLMs). The authors evaluate
  10 LLMs using 20 disinformation narratives across five categories, generating 1,200
  "news articles".
---

# Disinformation Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2311.08838
- Source URL: https://arxiv.org/abs/2311.08838
- Reference count: 14
- This paper comprehensively evaluates the disinformation generation capabilities of 10 LLMs using 20 narratives, finding that some models can produce convincing fake news articles.

## Executive Summary
This paper presents a systematic study of the disinformation generation capabilities of current large language models (LLMs). The authors evaluate 10 different LLMs by generating 1,200 news articles based on 20 disinformation narratives across five categories. They find significant variation in model performance, with some LLMs like Vicuna and GPT-3 Davinci readily generating convincing disinformation, while others like Falcon and ChatGPT exhibit stronger safety filters. The study demonstrates that LLMs can be steered via prompts to increase alignment with disinformation narratives, and that current detection models can identify machine-generated disinformation with high precision.

## Method Summary
The researchers generated 1,200 news articles using 10 different LLMs (including GPT-3 variants, GPT-4, Vicuna, Falcon, and others) with two prompt types: title-only and title-with-abstract. They evaluated 840 articles using human annotators with a 6-question framework, and used GPT-4 to automatically evaluate an additional 360 articles. The study also tested 5 state-of-the-art detection models on all 1,200 generated articles plus 73 human-written fake news articles. Narratives were drawn from fact-checking sources across five categories: COVID-19, Politics, Climate Change, Immigration, and Other.

## Key Results
- Vicuna and GPT-3 Davinci readily generate convincing disinformation articles that agree with dangerous narratives, while Falcon and ChatGPT show stronger safety filters
- Larger LLMs generally produce higher quality disinformation articles that more closely align with provided narratives
- Prompt engineering with narrative abstracts significantly increases LLM alignment with disinformation content
- Detection models achieve high precision in identifying machine-generated disinformation articles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate convincing disinformation articles when prompted with disinformation narratives.
- Mechanism: LLMs are instruction-tuned to follow prompts and generate coherent text, which allows them to produce news articles that align with provided disinformation narratives, including novel arguments and supporting evidence.
- Core assumption: The LLM's internal knowledge and generation capabilities are sufficient to create believable disinformation content when given appropriate prompts.
- Evidence anchors:
  - [abstract]: "We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives."
  - [section]: "Vicuna and Davinci are LLMs that rarely disagree with the prompted narratives, and they are capable of generating convincing news-like articles with novel arguments."
  - [corpus]: Found 25 related papers; average neighbor FMR=0.337, suggesting moderate relevance but no direct evidence of this mechanism.
- Break condition: If the LLM's internal knowledge is insufficient or if the prompt is too vague or contradictory, the generated text may not align with the disinformation narrative.

### Mechanism 2
- Claim: The tendency of LLMs to agree with disinformation narratives increases with model size.
- Mechanism: Larger LLMs have better text generation capabilities and more comprehensive training data, leading to higher quality and more convincing disinformation articles that align with the provided narratives.
- Core assumption: Model size and training data quality directly correlate with the ability to generate convincing disinformation content.
- Evidence anchors:
  - [section]: "The model capacity impacts the quality and agreement. The results for three GPT-3 versions (Davinci, Curie, and Babbage) show a discernible trend. The larger the model, the better-formed the text is, and the more likely it is to resemble a news article."
  - [section]: "More worryingly, the tendency to agree with the disinformation narratives and to support them with novel arguments increases as well."
  - [corpus]: No direct evidence in corpus, but related work on model size and capabilities suggests a correlation.
- Break condition: If the larger models have stronger safety filters or if the training data does not include relevant disinformation narratives, the tendency to agree may not increase with size.

### Mechanism 3
- Claim: LLMs can be steered via prompts to increase alignment with disinformation narratives.
- Mechanism: By providing narrative abstracts in the prompts, LLMs can be guided to generate articles that are more closely aligned with the specific disinformation narratives, incorporating appropriate facts and arguments.
- Core assumption: LLMs are capable of understanding and incorporating contextual information from prompts to generate more aligned content.
- Evidence anchors:
  - [section]: "Based on the significantly improved score for Q3 (Agree), it is evident that the provided abstracts affect the level of alignment with the narrative."
  - [section]: "With further prompt-tuning, the quality of the generated disinformation could probably be increased even more."
  - [corpus]: No direct evidence in corpus, but related work on prompt engineering suggests its effectiveness.
- Break condition: If the LLM's understanding of the prompt is limited or if the prompt is not specific enough, the steering effect may not be significant.

## Foundational Learning

- Concept: Instruction-tuned LLMs
  - Why needed here: Understanding that these LLMs are trained to follow instructions is crucial for comprehending how they can be prompted to generate disinformation.
  - Quick check question: What distinguishes instruction-tuned LLMs from standard generative LLMs in terms of their response to user prompts?

- Concept: Prompt engineering
  - Why needed here: The ability to craft effective prompts is essential for steering LLM behavior and generating desired content, including disinformation.
  - Quick check question: How can the inclusion of narrative abstracts in prompts influence the alignment of generated articles with specific disinformation narratives?

- Concept: Text detection methods
  - Why needed here: Understanding the capabilities and limitations of existing text detection methods is important for assessing the risks associated with LLM-generated disinformation.
  - Quick check question: What are the key factors that contribute to the high precision of detection models in identifying machine-generated disinformation articles?

## Architecture Onboarding

- Component map:
  LLM generators (10 different models) -> Prompt templates (title and title-abstract) -> Human annotators and GPT-4 for evaluation -> Detection models (5 SOTA detectors) -> Safety filters (built-in to some LLMs)

- Critical path:
  1. Generate articles using LLM generators and prompt templates
  2. Evaluate generated articles using human annotators and GPT-4
  3. Detect machine-generated articles using detection models
  4. Analyze results to assess disinformation capabilities and risks

- Design tradeoffs:
  - Using multiple LLM models vs. focusing on a few representative models
  - Manual evaluation vs. automated evaluation with GPT-4
  - Inclusion of safety filters in some LLMs vs. their absence in others

- Failure signatures:
  - Low-quality or incoherent generated articles
  - Inability to detect machine-generated articles with high precision
  - Inconsistent evaluation results between human annotators and GPT-4

- First 3 experiments:
  1. Generate articles using a single LLM model with both title and title-abstract prompts, and evaluate the difference in alignment with the disinformation narratives.
  2. Test the effectiveness of different prompt engineering techniques in steering LLM behavior towards generating more convincing disinformation content.
  3. Assess the performance of various detection models in identifying machine-generated articles across different LLM models and prompt types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of LLMs in automatically evaluating disinformation generation capabilities be further improved?
- Basis in paper: [inferred] The paper suggests that GPT-4 can partially automate the evaluation process, but its results are not reliable on a per-sample level. It mentions that further prompt tuning or even fine-tuning could potentially improve the performance.
- Why unresolved: The paper only presents initial steps towards fully automatic assessment and does not explore the extent to which performance can be improved through prompt tuning or fine-tuning.
- What evidence would resolve it: Conducting experiments to systematically test the impact of different prompt engineering techniques and fine-tuning approaches on the accuracy of LLM-based evaluation. Measuring improvements in correlation with human evaluations.

### Open Question 2
- Question: How do LLMs perform in generating disinformation in languages other than English?
- Basis in paper: [explicit] The paper explicitly states that its analysis focuses on the English language only and acknowledges the need to understand LLM behavior in other languages.
- Why unresolved: The study did not include an evaluation of disinformation generation capabilities in languages other than English. It mentions that text generation capabilities and safety mechanisms may vary across languages.
- What evidence would resolve it: Conducting a similar comprehensive evaluation of LLM disinformation generation capabilities across multiple languages. Comparing the performance, safety filters, and detectability of generated texts across different languages.

### Open Question 3
- Question: What are the limitations of current machine-generated text detection methods and how can they be improved?
- Basis in paper: [inferred] The paper discusses the performance of various commercial and research detectors in identifying machine-generated disinformation texts. It mentions that some detectors exhibited miscalibration and that adversarial attacks could potentially decrease detection accuracy.
- Why unresolved: The study only evaluated a limited set of existing detectors and did not explore the limitations or potential improvements in depth. It did not consider adversarial attacks or other advanced techniques that bad actors might employ.
- What evidence would resolve it: Conducting a comprehensive analysis of the strengths and weaknesses of current detection methods. Exploring the impact of adversarial attacks and developing more robust detection techniques that can withstand such attacks. Evaluating the performance of detectors on a wider range of machine-generated text samples.

## Limitations

- The study evaluates only 20 disinformation narratives across 5 categories, which may not represent the full diversity of potential disinformation campaigns
- The effectiveness of built-in safety filters varies significantly across models, but the study doesn't fully explore the underlying mechanisms or potential workarounds
- While the study demonstrates high precision in detecting machine-generated disinformation, it's unclear how well these detection models would perform against adversarially optimized generations or in real-world deployment scenarios

## Confidence

- **High Confidence**: The core finding that LLMs can generate convincing disinformation articles when prompted appropriately is well-supported by both human and automated evaluations
- **Medium Confidence**: The claim that model size correlates with disinformation generation quality is supported by the GPT-3 variant analysis, but this relationship may not hold across all model architectures
- **Medium Confidence**: The effectiveness of prompt engineering in steering LLM alignment with narratives is demonstrated, but the study doesn't explore the full space of potential prompt variations

## Next Checks

1. **Adversarial Testing**: Design a systematic evaluation of prompt engineering techniques specifically aimed at circumventing safety filters, testing both successful and failed attempts to generate harmful content across all 10 models.

2. **Cross-Domain Transferability**: Evaluate whether detection models trained on one set of disinformation narratives can effectively identify machine-generated content from completely different narrative domains, testing the generalizability of current detection approaches.

3. **Long-Form Generation Analysis**: Generate extended articles (1,000+ words) to assess whether the quality and coherence of disinformation content scales with length, and whether this affects detection accuracy or human perception of credibility.