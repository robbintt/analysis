---
ver: rpa2
title: Invariance Measures for Neural Networks
arxiv_id: '2310.17404'
source_url: https://arxiv.org/abs/2310.17404
tags:
- invariance
- variance
- measure
- measures
- transformations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes measures to quantify the invariance of neural
  networks with respect to transformations of their inputs, by analyzing the variance
  and distances between activations. The measures are efficient, interpretable, and
  applicable to any network model and transformation set.
---

# Invariance Measures for Neural Networks

## Quick Facts
- arXiv ID: 2310.17404
- Source URL: https://arxiv.org/abs/2310.17404
- Authors: 
- Reference count: 40
- The paper proposes efficient and interpretable measures to quantify neural network invariance with respect to input transformations, validated on CNNs with CIFAR10 and MNIST datasets.

## Executive Summary
This paper introduces a set of measures to quantify the invariance of neural networks to transformations of their inputs. The measures are based on the variance and distances between activations across samples and transformations, and are designed to be efficient, interpretable, and applicable to any network model and transformation set. Experiments on CNN models trained on CIFAR10 and MNIST datasets demonstrate the measures' ability to detect invariance, stability to weight initialization, and sensitivity to data augmentation. The results show that invariance is remarkably stable to random weight initializations but not to changes in dataset or transformation.

## Method Summary
The paper proposes variance-based and distance-based measures to quantify invariance. Variance measures include Transformation Variance (TV), Sample Variance (SV), and Normalized Variance (NV), computed using Welford's running mean and variance equations for efficiency. Distance measures (Transformation Distance, Sample Distance, Normalized Distance) use pairwise distances between activations, approximated efficiently in batches. The measures are applied to individual activations or aggregated over layers. ANOVA is used to detect statistically significant invariance. The measures are evaluated on CNN models trained on MNIST and CIFAR10 with data augmentation using affine transformations.

## Key Results
- The Normalized Variance measure effectively captures the increasing invariance of models trained with data augmentation.
- Invariance is stable to random weight initializations but not to changes in dataset or transformation.
- The measures are efficient and interpretable, applicable to any neural network model and transformation set.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformation Variance measures decrease monotonically when a model is trained with data augmentation, reflecting increased invariance.
- Mechanism: During training with data augmentation, the loss function pushes the model to produce similar outputs for transformed inputs, reducing the variance of activations across transformations. The Transformation Variance (TV) captures this by averaging the variance of each activation over all transformations for each sample, so as invariance increases, TV decreases.
- Core assumption: Data augmentation directly correlates with the model's learned invariance, and the variance of activations is a reliable proxy for this property.
- Evidence anchors:
  - [abstract]: "we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation."
  - [section]: "Figure 18 shows the results for the different training transformations complexity for the Normalized Variance measure. The measure was always evaluated with the most complex of the transformation sets. We can observe that the Normalized Variance measure captures the increasing invariance of the modelâ€™s activations."
- Break condition: If the dataset naturally contains transformed samples or the model is trained without sufficient diversity, the assumption breaks, and variance may not reflect true invariance.

### Mechanism 2
- Claim: Normalized Variance provides a scale-invariant comparison of invariance across layers, datasets, and transformations.
- Mechanism: By dividing the Transformation Variance (TV) by the Sample Variance (SV), the Normalized Variance (NV) removes the dependence on the absolute scale of activations. This allows fair comparison between layers with different activation magnitudes, different datasets with varying pixel distributions, or different transformation sets with different intensity levels.
- Core assumption: The ratio of transformation-induced variance to sample-induced variance is a meaningful indicator of invariance that is independent of scale.
- Evidence anchors:
  - [abstract]: "The measures are efficient and interpretable, and can be applied to any neural network model."
  - [section]: "Figure 16 shows the results of the Transformation Variance and Sample Variance measures. The models were trained with data augmentation with a set of transformations T and then the TransformationVariance and SampleVariance were also calculated with respect to T. The magnitudes of both measures are similar for the same layer but quite different between datasets and mildly different between transformations."
- Break condition: If both TV and SV are near zero (dead activations), NV defaults to 1, which may not reflect meaningful invariance. Also, if the sample variance is artificially low due to preprocessing, the normalization may be misleading.

### Mechanism 3
- Claim: Distance-based measures (Normalized Distance) approximate variance-based measures when using squared Euclidean distance, allowing flexible distance functions for different activation types.
- Mechanism: The squared Euclidean distance between activations is mathematically equivalent to twice the variance, so the Normalized Distance with this metric mirrors the Normalized Variance. However, other distance functions (e.g., Frechet-Inception for feature maps) can capture invariance in ways variance cannot, especially for multimodal activation distributions.
- Core assumption: The choice of distance function is appropriate for the activation type, and pairwise distances can be approximated efficiently in batches.
- Evidence anchors:
  - [section]: "In the case of the squared euclidean distance measure, it is well known that the variance and distance computations are equivalent, and so are the measures, as per equation [12]."
  - [corpus]: "We can compare entire feature maps with a semantic distance measure such as the Frechet-Inception distance [41] and obtain pairwise distances between them." (Weak corpus support; no direct evidence of Frechet-Inception usage in the paper.)
- Break condition: If the distance function does not align with the notion of invariance for the activation type, or if batch size is too small to approximate pairwise distances accurately.

## Foundational Learning

- Concept: Analysis of Variance (ANOVA) hypothesis testing
  - Why needed here: The ANOVA measure uses the null hypothesis of invariance (equal means across transformations) to detect invariant activations. Understanding ANOVA is crucial for interpreting the measure and its significance.
  - Quick check question: In a one-way ANOVA with transformations as treatments, what does rejecting the null hypothesis imply about an activation's invariance?

- Concept: Variance and standard deviation as measures of spread
  - Why needed here: Variance-based measures rely on computing the variance of activations over transformations and samples. A solid grasp of variance properties (e.g., scale dependence, interpretation of zero variance) is essential for understanding and implementing the measures.
  - Quick check question: If an activation has zero variance over a set of transformations, what does that imply about its invariance?

- Concept: Distance metrics and their relationship to variance
  - Why needed here: Distance-based measures use pairwise distances between activations to quantify invariance, especially when variance assumptions (e.g., unimodality) do not hold. Understanding different distance functions and their properties is key to extending the measures.
  - Quick check question: How does the squared Euclidean distance between a set of values relate to their variance?

## Architecture Onboarding

- Component map: ST matrix construction -> Variance module (TV, SV, NV) / Distance module (TD, SD, ND) -> ANOVA module -> Feature map specialization
- Critical path:
  1. Forward pass through model for each sample and transformation to populate ST matrix online
  2. Compute variance-based or distance-based measures for each activation
  3. Aggregate results (e.g., by layer) for analysis and visualization
- Design tradeoffs:
  - Online ST computation vs. storing all activations: Online is memory efficient but requires careful iteration order; storing is simpler but prohibitive for large models.
  - Variance vs. distance measures: Variance is efficient and interpretable for unimodal distributions; distance is more general but computationally heavier due to pairwise comparisons.
  - Normalization via SV: Removes scale dependence but can be unstable for dead activations or low-variance datasets.
- Failure signatures:
  - High variance in activations that should be invariant: May indicate poor training, insufficient data augmentation, or inappropriate distance function.
  - NaN or infinite NV values: Often due to zero sample variance; check for dead activations or preprocessing issues.
  - ANOVA always rejecting null: May indicate overly strict significance threshold or Bonferroni correction issues.
- First 3 experiments:
  1. Train SimpleConv on MNIST with rotation augmentation; measure invariance with NV and compare to untrained model.
  2. Vary the size of the transformation set (e.g., 4, 16, 64 rotations) and dataset (e.g., 100, 1000, 10000 samples); plot relative error in NV.
  3. Compare NV, GF, and ANOVA measures on the same trained model; analyze differences in layer-wise invariance patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the invariance of neural networks change when using infinite transformation sets instead of finite ones?
- Basis in paper: [inferred] The paper mentions that using an infinite set of transformations is possible but unnecessarily complicates other definitions. It also suggests that the number of transformations in a set can be increased as needed for an approximately infinite set.
- Why unresolved: The paper focuses on finite transformation sets for simplicity and practicality, leaving the behavior with infinite sets unexplored.
- What evidence would resolve it: Experiments comparing invariance measures with finite and infinite transformation sets, showing any differences in invariance detection and stability.

### Open Question 2
- Question: Can the proposed invariance measures be extended to automatically detect transformational structures in terms of groups of activations rather than individual activations?
- Basis in paper: [explicit] The paper suggests that it would be interesting to extend the measures to automatically detect the transformational structures of the network in terms of groups of activations, allowing analysis with intermediate granularities.
- Why unresolved: The current measures focus on individual activations and aggregate results for global analysis, lacking the ability to detect group-level transformational structures.
- What evidence would resolve it: Development and validation of new measures that can identify and quantify invariance at the group level, demonstrating their effectiveness compared to individual activation analysis.

### Open Question 3
- Question: How do the proposed invariance measures perform with non-affine transformations and outside image classification domains?
- Basis in paper: [explicit] The paper mentions interest in widening the set of models, transformations, and domains to apply the measures, ranging outside affine transformations and image classification problems.
- Why unresolved: The current experiments are limited to affine transformations and image classification, leaving the performance with other transformation types and domains unexplored.
- What evidence would resolve it: Experiments applying the invariance measures to various non-affine transformations and different domains, showing their effectiveness and any limitations compared to affine transformations in image classification.

## Limitations

- The measures are primarily validated on image datasets and affine transformations. Their effectiveness for other data types (e.g., text, graphs) or transformation types (e.g., color jitter, adversarial attacks) is not explored.
- The paper focuses on invariance as a static property measured post-training. The dynamics of invariance learning during training and the impact of different optimization algorithms are not investigated.
- The measures assume that variance and distance are appropriate proxies for invariance, but this may not hold for all activation distributions or network architectures.

## Confidence

- High confidence: The core claims about the measures' efficiency, interpretability, and applicability to any network model are well-supported by the paper's methodology and experiments.
- Medium confidence: The claims about the measures' ability to detect invariance and their sensitivity to data augmentation are supported by experiments, but the generalizability to other datasets and transformations is uncertain.
- Low confidence: The claims about the measures' ability to reveal new insights into invariance representation and understanding are largely speculative and not directly supported by the paper's results.

## Next Checks

1. Apply the measures to a non-image dataset (e.g., text, graphs) with appropriate transformations and evaluate their effectiveness in detecting invariance.
2. Conduct a longitudinal study of invariance measures during training to understand the dynamics of invariance learning and the impact of different optimization algorithms.
3. Compare the measures' performance to alternative invariance quantification methods (e.g., information bottleneck, mutual information) on a variety of datasets and network architectures.