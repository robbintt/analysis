---
ver: rpa2
title: Decomposed Prompt Tuning via Low-Rank Reparameterization
arxiv_id: '2310.10094'
source_url: https://arxiv.org/abs/2310.10094
tags:
- prompt
- tuning
- soft
- rank
- bottleneck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors show that soft prompts in prompt tuning have a low
  intrinsic rank and propose a low-rank reparameterization (DPT) to reduce the number
  of trainable parameters. DPT decomposes the soft prompt into two compact matrices
  and maintains effectiveness while requiring significantly fewer trainable parameters
  than vanilla prompt tuning (e.g., 11.2K vs.
---

# Decomposed Prompt Tuning via Low-Rank Reparameterization

## Quick Facts
- arXiv ID: 2310.10094
- Source URL: https://arxiv.org/abs/2310.10094
- Reference count: 36
- Key result: DPT reduces trainable parameters from 102K to 11.2K for T5-Large while outperforming vanilla prompt tuning on SuperGLUE

## Executive Summary
This paper introduces Decomposed Prompt Tuning (DPT), a parameter-efficient fine-tuning method that leverages the low intrinsic rank of soft prompts. By decomposing the soft prompt matrix into two lower-rank matrices, DPT significantly reduces the number of trainable parameters while maintaining competitive performance. The method demonstrates effectiveness across T5 model sizes and in both high-resource and low-resource scenarios on the SuperGLUE benchmark.

## Method Summary
DPT reparameterizes soft prompts by decomposing the embedding matrix Pemb into two compact matrices A and B, where Pemb = AB. This decomposition constrains the soft prompt to a low-rank matrix by setting the intermediate dimension b to a small value, reducing trainable parameters from ec to eb + bc. The method is applied to frozen T5 models, with training performed using AdamW optimizer at learning rate 0.3 for 100 epochs.

## Key Results
- DPT requires significantly fewer trainable parameters than vanilla prompt tuning (11.2K vs 102K for T5-Large)
- Consistently outperforms vanilla prompt tuning and residual prompt tuning on SuperGLUE benchmark
- Maintains effectiveness in both high-resource and low-resource scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft prompts exhibit low intrinsic rank, enabling compression without significant performance loss
- Mechanism: Soft prompt matrix Pemb can be decomposed into two lower-rank matrices A and B, reducing parameters from ec to eb + bc
- Core assumption: Soft prompt's parameter space contains redundant dimensions that can be eliminated while preserving task-relevant information
- Evidence anchors: Empirical studies show soft prompts exhibit low intrinsic rank behavior; no direct corpus evidence found

### Mechanism 2
- Claim: Low-rank reparameterization maintains competitive performance while reducing parameters
- Mechanism: Constraining soft prompt to low-rank matrix through decomposition learns efficient representations that generalize well
- Core assumption: Essential information for task adaptation can be captured in lower-dimensional subspace
- Evidence anchors: Experimental results on SuperGLUE demonstrate effectiveness; no direct corpus evidence found

### Mechanism 3
- Claim: Bottleneck size b controls tradeoff between parameter efficiency and model capacity
- Mechanism: Intermediate dimension b acts as compression factor - smaller b reduces parameters more but may limit expressiveness
- Core assumption: Optimal bottleneck size balances parameter efficiency with sufficient representational capacity
- Evidence anchors: Performance fluctuations observed with different bottleneck sizes; no direct corpus evidence found

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: Understanding rank and matrix decomposition is crucial for grasping why soft prompts can be compressed
  - Quick check question: If a matrix has rank r, how many non-zero singular values does it have?

- Concept: Low-rank approximation
  - Why needed here: Core idea of DPT relies on approximating soft prompt with lower-rank matrix
  - Quick check question: What is the relationship between matrix rank and number of parameters needed to represent it?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: DPT is PEFT method, so understanding broader context of PEFT approaches is important
  - Quick check question: What are main categories of PEFT methods, and how does DPT fit into this landscape?

## Architecture Onboarding

- Component map: Input -> Concatenation of soft prompt P and original input X -> Frozen T5 model -> Output

- Critical path:
  1. Initialize matrices A and B randomly
  2. Compute soft prompt embedding: Pemb = AB
  3. Concatenate with input embedding
  4. Feed through frozen T5 model
  5. Generate predictions and compute loss
  6. Backpropagate through A and B only

- Design tradeoffs:
  - Bottleneck size b vs. parameter efficiency and performance
  - Prompt length c vs. expressiveness and computational cost
  - Random initialization vs. pre-trained initialization of A and B

- Failure signatures:
  - Underfitting: Poor performance with small bottleneck sizes
  - Overparameterization: No performance gain with large bottleneck sizes
  - Instability: High variance in performance across runs

- First 3 experiments:
  1. Compare DPT with vanilla prompt tuning on single SuperGLUE task using T5-Small
  2. Vary bottleneck size b while keeping prompt length fixed to find optimal compression ratio
  3. Test DPT in few-shot setting (8, 16, 32 samples) to validate parameter efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DPT performance compare to LoRA when both applied to same pre-trained model and task?
- Basis in paper: [inferred] Paper discusses LoRA as related PEFT method sharing low-rank principle but differing in approach
- Why unresolved: No direct comparison between DPT and LoRA on identical tasks or models
- What evidence would resolve it: Experimental results comparing DPT and LoRA on identical tasks, models, and evaluation metrics

### Open Question 2
- Question: How does rank of soft prompt change during training process, and what factors influence this change?
- Basis in paper: [explicit] Investigates "intrinsic rank" of soft prompt but lacks detailed analysis of rank changes during training
- Why unresolved: Only provides high-level observation of rank change without comprehensive analysis of training dynamics
- What evidence would resolve it: Detailed tracking of soft prompt matrix rank during training with analysis of influencing factors

### Open Question 3
- Question: Can low-rank property of soft prompts be exploited in other NLP areas like text generation or machine translation?
- Basis in paper: [inferred] Focuses on natural language understanding tasks without exploring applicability in other NLP domains
- Why unresolved: Does not investigate whether low-rank property generalizes to other NLP tasks or domains
- What evidence would resolve it: Experiments applying DPT or similar methods to text generation or machine translation tasks

## Limitations
- Limited evaluation scope to SuperGLUE benchmark without testing on diverse task types
- Rank analysis lacks systematic validation across different tasks and model scales
- Insufficient ablation studies examining initialization strategies and hyperparameter impacts
- No theoretical guarantees about approximation quality or performance degradation conditions

## Confidence
- High confidence: Core technical contribution and mathematical formulation are well-defined and reproducible
- Medium confidence: Empirical performance claims on SuperGLUE are supported but generalizability and robustness remain uncertain
- Low confidence: Fundamental claim about soft prompts' "low intrinsic rank" lacks systematic validation

## Next Checks
1. Conduct systematic singular value decomposition analysis of soft prompts across diverse task types to empirically validate low-rank claim
2. Perform comprehensive hyperparameter sweeps over bottleneck dimension, prompt length, and initialization strategies
3. Evaluate DPT performance on non-SuperGLUE benchmarks including structured prediction, long-form generation, and multilingual datasets