---
ver: rpa2
title: Making Language Models Better Tool Learners with Execution Feedback
arxiv_id: '2305.13068'
source_url: https://arxiv.org/abs/2305.13068
tags:
- tools
- tool
- learning
- training
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TRICE, a two-stage end-to-end framework that
  teaches language models to selectively use tools by leveraging execution feedback.
  The method addresses the problem of models using tools indiscriminately, which can
  lead to errors.
---

# Making Language Models Better Tool Learners with Execution Feedback

## Quick Facts
- arXiv ID: 2305.13068
- Source URL: https://arxiv.org/abs/2305.13068
- Reference count: 12
- Key result: TRICE significantly improves mathematical reasoning accuracy while reducing tool dependency compared to previous methods

## Executive Summary
This paper introduces TRICE, a two-stage end-to-end framework that teaches language models to selectively use tools by leveraging execution feedback. The method addresses the problem of models using tools indiscriminately, which can lead to errors. TRICE first uses behavior cloning to teach the model how to call tools, then applies reinforcement learning with execution feedback (RLEF) to enhance selective tool usage and improve accuracy. Experiments on two mathematical reasoning datasets show substantial performance gains over previous fine-tuning-based tool learning methods.

## Method Summary
TRICE is a two-stage training framework that first teaches a language model basic tool usage through behavior cloning, then refines its selective usage through reinforcement learning with execution feedback. The method uses execution results as rewards to train the model to score responses from different generation strategies. The framework is trained end-to-end, starting from a pretrained language model (Alpaca-7B), and evaluates performance on mathematical reasoning tasks using calculator tools.

## Key Results
- Significant improvement in accuracy on ASDiv and SVAMP test sets compared to previous methods
- Reduced tool dependency while maintaining or improving answer accuracy
- Effective learning of when to use tools versus solving problems directly

## Why This Works (Mechanism)

### Mechanism 1
The two-stage training approach addresses both the "when" and "how" of tool usage by first teaching the model to imitate tool use and then refining its selection process through execution feedback. Stage I uses behavior cloning to learn the general structure of tool API calls. Stage II applies reinforcement learning with execution feedback to improve selective usage and accuracy. The core assumption is that a model that can generate tool calls can be improved through feedback about whether those calls actually help solve the problem.

### Mechanism 2
Using execution feedback as a reward signal allows the model to learn when tools actually improve performance versus when they introduce errors. The reward function compares the predicted answer after tool execution to the gold answer, assigning higher scores to responses that yield closer results. The core assumption is that the numerical answer produced by executing a tool API call is a reliable proxy for whether that tool use was appropriate.

### Mechanism 3
The ranking loss in RLEF helps the model learn to score responses correctly by comparing different generation strategies. The model scores multiple responses from different sources and uses a ranking loss to align its scores with the execution-based rewards. The core assumption is that diverse response generation from multiple models provides a good basis for learning relative quality.

## Foundational Learning

- Concept: Behavior Cloning
  - Why needed here: To teach the model the basic structure of tool API calls before introducing more complex learning signals
  - Quick check question: What is the loss function used in the behavior cloning stage?

- Concept: Reinforcement Learning with Execution Feedback
  - Why needed here: To refine the model's tool selection by providing feedback about whether tool usage actually improved performance
  - Quick check question: How is the reward score calculated in the RLEF stage?

- Concept: Ranking Loss for Response Scoring
  - Why needed here: To help the model learn to distinguish between good and bad tool usage strategies by comparing multiple responses
  - Quick check question: What is the purpose of the ranking loss in the policy loss?

## Architecture Onboarding

- Component map: Pretrained LLM (Alpaca-7B) -> Data Construction Pipeline -> Behavior Cloning Stage -> Response Collection -> RLEF Stage -> Evaluation
- Critical path: Data construction → Behavior Cloning (Stage I) → Response Collection → RLEF (Stage II) → Evaluation
- Design tradeoffs: Using execution feedback provides direct performance signals but requires reliable tool execution. The two-stage approach ensures basic tool usage is learned before refinement.
- Failure signatures: Poor initial tool generation leads to ineffective RLEF training. Imbalanced tool/no-tool data in Stage I causes over-reliance on tools. Noisy execution feedback corrupts the reward signal.
- First 3 experiments:
  1. Train only Stage I (behavior cloning) and measure tool usage rate and accuracy to establish baseline imitation capability
  2. Train only Stage II (RLEF) starting from a randomly initialized model to test whether execution feedback alone is sufficient
  3. Train the full two-stage approach and compare accuracy and tool dependency against both baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively scale the TRICE framework to handle multiple tools or tool compositions?
- Basis in paper: The paper mentions that the current method is unable to learn the usage of multiple tools or tool compositions and suggests that more sophisticated trial-and-error processes and feedback mechanisms will be necessary
- Why unresolved: The paper does not provide specific methods or experiments for handling multiple tools or tool compositions
- What evidence would resolve it: Developing and testing a version of TRICE that can handle multiple tools or tool compositions, and comparing its performance to the single-tool version

### Open Question 2
- Question: How can we design more complex reward mechanisms that integrate execution feedback from a range of tasks across various scenarios?
- Basis in paper: The paper mentions that more complex reward mechanisms will be necessary to cater to various tasks and to enhance the approach
- Why unresolved: The paper does not provide specific methods or experiments for designing complex reward mechanisms
- What evidence would resolve it: Developing and testing new reward mechanisms that integrate execution feedback from various tasks and scenarios, and evaluating their effectiveness in improving the model's performance

### Open Question 3
- Question: How can we address the problem of knowledge conflicts in tool learning, especially in highly specialized fields such as biomedical research and legal assistance?
- Basis in paper: The paper mentions that knowledge conflicts may arise from the conflicts between model knowledge and augmented knowledge from tools, and among augmented knowledge from different tools
- Why unresolved: The paper does not provide specific methods or experiments for addressing knowledge conflicts in tool learning
- What evidence would resolve it: Developing and testing methods that can help the model differentiate and prioritize knowledge from various sources, and evaluating their effectiveness in reducing knowledge conflicts and improving the model's performance in specialized fields

## Limitations

- The framework is limited to single-tool usage and cannot handle tool compositions or multiple tools
- Critical implementation details are missing, including exact prompt formats and hyperparameter settings
- Evaluation is limited to mathematical reasoning tasks with calculator tools, leaving generalization to other domains unclear

## Confidence

- **High confidence** in the overall methodological framework: The two-stage approach is well-motivated and technically coherent
- **Medium confidence** in the empirical results: The reported improvements are substantial, but limited scope and lack of ablation studies make it difficult to isolate which components drive the gains
- **Low confidence** in reproducibility: Critical implementation details are missing, particularly around data construction and the exact reward function implementation

## Next Checks

1. **Ablation study validation**: Run experiments comparing the full two-stage approach against only Stage I (behavior cloning) and only Stage II (RLEF) to quantify the contribution of each component

2. **Reward signal robustness test**: Evaluate model performance when tool execution is noisy or unreliable to assess sensitivity to execution feedback quality

3. **Tool dependency analysis**: Measure and compare the frequency of tool usage across different models and training stages to verify the claim of reduced tool dependency