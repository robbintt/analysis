---
ver: rpa2
title: Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation
arxiv_id: '2307.05476'
source_url: https://arxiv.org/abs/2307.05476
tags:
- learning
- fisher
- sampling
- merging
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fisher-weighted parameter merging for sequential
  recommendation models, addressing the challenge of combining multiple contrastive
  learning approaches. The method aggregates parameters from models trained with different
  contrastive loss functions by optimizing a joint likelihood based on Gaussian posteriors,
  with variances derived from Fisher information matrices.
---

# Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation

## Quick Facts
- arXiv ID: 2307.05476
- Source URL: https://arxiv.org/abs/2307.05476
- Reference count: 14
- One-line primary result: Fisher-weighted parameter merging of contrastive learning models improves sequential recommendation performance, with NDCG@10 scores reaching up to 0.1386 on MovieLens-1M.

## Executive Summary
This paper introduces Fisher-weighted parameter merging for sequential recommendation models, addressing the challenge of combining multiple contrastive learning approaches. The method aggregates parameters from models trained with different contrastive loss functions by optimizing a joint likelihood based on Gaussian posteriors, with variances derived from Fisher information matrices. Experiments on the MovieLens-1M dataset show that Fisher merging in fine-tuning settings improves performance compared to individual models, with NDCG@10 scores reaching up to 0.1386, and demonstrates robustness by improving results even when merging with underperforming models.

## Method Summary
The Fisher-weighted merging method combines parameters from multiple sequential recommendation models trained with different contrastive learning techniques (BERT4Rec, CL4SRec, DuoRec). It assumes each model's parameters follow a Gaussian posterior distribution with variances derived from Fisher information matrices. The merged parameters are obtained by maximizing the joint likelihood across all models, weighted by their respective Fisher information. To improve computational efficiency, the method uses batch-wise computation and sampling strategies (random, top-k, model-based, and target-item-only) to approximate Fisher matrix calculations. The approach is evaluated on the MovieLens-1M dataset for next-item prediction tasks.

## Key Results
- Fisher merging improves sequential recommendation performance in fine-tuning settings, with NDCG@10 scores reaching up to 0.1386 on MovieLens-1M
- The method demonstrates robustness by improving results even when merging with underperforming models
- Computational efficiency is achieved through batch-wise computation and sampling strategies, reducing item consideration from thousands to hundreds for Fisher matrix calculations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fisher merging improves sequential recommendation by combining multiple contrastive learning models through a weighted average based on posterior distributions.
- Mechanism: The Fisher merging approach assumes each model's parameters follow a Gaussian posterior distribution with variances derived from the Fisher information matrix. The merged parameters are obtained by maximizing the joint likelihood across all models, weighted by their respective Fisher information.
- Core assumption: Model parameters follow Gaussian posteriors, and the Fisher information matrix captures the uncertainty in parameter estimates effectively.
- Evidence anchors:
  - [abstract]: "The method aggregates parameters from models trained with different contrastive loss functions by optimizing a joint likelihood based on Gaussian posterals, with variances derived from Fisher information matrices."
  - [section]: "The posterior of θm can be represented as p (θ|θm). Since obtaining this posterior directly is generally challenging, it can employ approximation methods such as Laplace approximation... Let us interpret the process of finding θ∗ as maximizing the joint likelihood,P m log p (θ | θm)."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If the Gaussian assumption for parameter posteriors is violated, or if Fisher information matrices don't accurately capture model uncertainty.

### Mechanism 2
- Claim: Batch-wise computation and sampling strategies reduce computational cost while maintaining performance in Fisher merging for large-scale recommendation systems.
- Mechanism: Instead of computing Fisher matrices for all users and items individually, the method processes data in batches and samples subsets of items (randomly, top-k, or model-based) to approximate the Fisher information matrix calculations.
- Core assumption: Batch-wise computation with sorted probabilities minimizes approximation error, and sampling representative items captures sufficient information for Fisher matrix estimation.
- Evidence anchors:
  - [section]: "To address the first challenge of performing computations on individual samples, we reinterpet the equation and carry out the calculations on a batch basis... To alleviate the computational burden associated with iterating over all j values, which scales with |V|, we employ a sampling-based approach."
  - [section]: "The figure demonstrates that within a batch containing 10 samples... it is possible to minimize the error described by the eq.6. Furthermore, the figure illustrates the rationale behind top-k sampling."
  - [corpus]: Weak - no direct corpus evidence supporting this specific computational optimization.
- Break condition: If batch sizes are too small to capture meaningful patterns, or if sampled items miss critical information for recommendation quality.

### Mechanism 3
- Claim: Ensemble methods combining models with different contrastive learning frameworks reduce error inconsistency and improve recommendation robustness.
- Mechanism: Models trained with different contrastive learning techniques (BERT4Rec, CL4SRec, DuoRec) have uncorrelated errors due to different generalization behaviors. Merging these models through Fisher weighting creates a more robust ensemble that performs better than individual models, especially in fine-tuning settings.
- Core assumption: Different contrastive learning frameworks produce models with complementary strengths and uncorrelated errors.
- Evidence anchors:
  - [section]: "Previous research has shown that ensemble methods yield significant benefits when multiple learning frameworks are employed... We propose a practical and feasible method to ensemble the parameters of models trained with different contrastive learning techniques."
  - [appendix]: "Previous research (Gontijo-Lopes et al., 2021; Yosinski et al., 2015) demonstrated the increased effectiveness of ensemble methods as error inconsistency grows."
  - [corpus]: Moderate - related papers on contrastive learning for sequential recommendation support the general approach.
- Break condition: If models trained with different frameworks have highly correlated errors, reducing the benefit of ensemble methods.

## Foundational Learning

- Concept: Gaussian posterior distributions and Laplace approximation
  - Why needed here: The Fisher merging method relies on approximating model parameter posteriors as Gaussian distributions using Laplace approximation to compute the joint likelihood.
  - Quick check question: What is the relationship between the Fisher information matrix and the Hessian matrix in the context of Laplace approximation?

- Concept: Contrastive learning and data augmentation in sequential recommendation
  - Why needed here: The method combines models trained with different contrastive learning techniques (item cropping, masking, reordering, dropout-based augmentation), so understanding these techniques is crucial for grasping the approach.
  - Quick check question: How do different data augmentation strategies affect the construction of positive and negative pairs in contrastive learning for sequential recommendation?

- Concept: Recommendation evaluation metrics (NDCG@10)
  - Why needed here: The paper uses NDCG@10 to evaluate recommendation performance, which measures the quality of ranked item recommendations by considering the position of relevant items.
  - Quick check question: What is the difference between NDCG@10 and precision@k in evaluating recommendation systems?

## Architecture Onboarding

- Component map: Multiple BERT4Rec-based models (BERT4Rec, CL4SRec, DuoRec) -> Fisher information matrix computation (batch-wise with sampling) -> Parameter merging (weighted averaging based on posteriors)
- Critical path: The most critical path is the Fisher matrix computation followed by parameter merging, as these steps directly impact the final recommendation performance. Batch-wise computation and sampling are optimizations that affect computational efficiency.
- Design tradeoffs: The method trades off computational efficiency (through sampling and batching) against potential loss of information from not using all items in Fisher matrix calculations. It also trades off model diversity (using different contrastive techniques) against the complexity of merging multiple models.
- Failure signatures: Poor performance may indicate: (1) Insufficient sampling leading to inaccurate Fisher matrices, (2) Models with highly correlated errors reducing ensemble benefits, (3) Batch sizes too small to capture meaningful patterns, or (4) Gaussian posterior assumption violated.
- First 3 experiments:
  1. Implement and compare basic parameter averaging vs. Fisher-weighted merging on a small subset of MovieLens data.
  2. Test different sampling strategies (random, top-k, model-based) on a single model to evaluate their impact on Fisher matrix accuracy.
  3. Compare ensemble performance when merging models with similar vs. dissimilar contrastive learning frameworks to verify the error inconsistency hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Fisher merging perform when combining models with more diverse architectures beyond the BERT4Rec-based models used in this study?
- Basis in paper: [inferred] The paper only tested Fisher merging on models sharing the same BERT4Rec architecture with different contrastive learning approaches. The authors mention that diverse training methodologies exhibit different generalization capabilities, but don't test architectural diversity.
- Why unresolved: The paper focuses exclusively on models with identical architectures but different loss functions. The effectiveness of Fisher merging across fundamentally different model architectures remains untested.
- What evidence would resolve it: Experimental results comparing Fisher merging performance across models with varying architectures (e.g., CNN-based, RNN-based, or different transformer variants) would clarify whether the method generalizes beyond architectural similarity.

### Open Question 2
- Question: What is the optimal sampling strategy for Fisher matrix computation across different recommendation domains with varying sparsity levels?
- Basis in paper: [explicit] The paper experiments with random, top-k, model-based, and target-item-only sampling strategies, finding top-k performs best for MovieLens-1M, but notes that computational efficiency varies with sampling size.
- Why unresolved: The paper only evaluates sampling strategies on MovieLens-1M dataset. Different recommendation domains (e.g., books, music, e-commerce) may have different item distributions and sparsity patterns that could affect optimal sampling.
- What evidence would resolve it: Comparative experiments across multiple recommendation datasets with varying sparsity levels and item distributions would identify whether a universal optimal sampling strategy exists or if domain-specific tuning is required.

### Open Question 3
- Question: How does the performance of Fisher merging scale with the number of models being merged, and is there a diminishing return point?
- Basis in paper: [inferred] The paper merges three models (BERT4Rec, CL4SRec, DuoRec) and demonstrates performance improvements, but doesn't explore merging larger ensembles or analyze the relationship between ensemble size and performance gains.
- Why unresolved: The experimental setup only considers three models. The theoretical framework suggests merging could work with any number of models, but practical limitations or performance plateaus at larger ensemble sizes are unexplored.
- What evidence would resolve it: Systematic experiments merging increasing numbers of models (4, 5, 6+) with the same Fisher-weighted approach would reveal whether performance continues improving or plateaus, and whether computational costs become prohibitive at scale.

## Limitations
- The method relies on the Gaussian posterior assumption for parameter distributions, which may not hold for all model architectures or training scenarios
- Experimental validation is limited to a single dataset (MovieLens-1M), raising questions about generalization to other recommendation domains
- The trade-off between computational efficiency and Fisher matrix accuracy through sampling strategies isn't fully characterized

## Confidence
- Performance improvement claims: Medium
- Computational efficiency claims: Medium
- Ensemble method effectiveness: Medium

## Next Checks
1. **Dataset Generalization Test**: Validate the Fisher merging approach on at least two additional sequential recommendation datasets (e.g., Amazon, Yelp) with varying sparsity levels and user behavior patterns to assess robustness beyond MovieLens-1M.

2. **Sampling Strategy Analysis**: Conduct controlled experiments comparing different sampling strategies (random, top-k, model-based) across various batch sizes to quantify the trade-off between computational efficiency and recommendation accuracy, establishing guidelines for optimal parameter selection.

3. **Gaussian Assumption Validation**: Perform empirical analysis of parameter posterior distributions across the three base models to verify the Gaussian assumption, testing alternative approximation methods (e.g., variational inference) when the assumption is violated.