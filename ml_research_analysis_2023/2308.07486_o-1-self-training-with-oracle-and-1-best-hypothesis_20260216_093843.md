---
ver: rpa2
title: 'O-1: Self-training with Oracle and 1-best Hypothesis'
arxiv_id: '2308.07486'
source_url: https://arxiv.org/abs/2308.07486
tags:
- training
- embr
- oracle
- hypothesis
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces O-1, a new self-training objective that improves
  speech recognition by reducing the gap between 1-best and oracle word error rates.
  O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR) that boosts the oracle
  hypothesis while suppressing the 1-best hypothesis, and can be applied to both supervised
  and unsupervised data.
---

# O-1: Self-training with Oracle and 1-best Hypothesis

## Quick Facts
- arXiv ID: 2308.07486
- Source URL: https://arxiv.org/abs/2308.07486
- Authors: 
- Reference count: 0
- O-1 achieves 13-25% relative WER improvements on SpeechStew datasets and 12% relative gap reduction on in-house data compared to EMBR baseline

## Executive Summary
O-1 introduces a novel self-training objective for speech recognition that reduces the gap between 1-best and oracle Word Error Rates (WER). The method is a faster variant of Expected Minimum Bayes Risk (EMBR) that boosts the oracle hypothesis while suppressing the 1-best hypothesis. O-1 can be applied to both supervised and unsupervised data, making it scalable for large-scale speech recognition tasks. The proposed method achieves consistent relative improvements over EMBR across various public and in-house datasets.

## Method Summary
O-1 is a self-training objective that improves speech recognition by reducing the gap between 1-best and oracle WERs. It is a faster variant of Expected Minimum Bayes Risk (EMBR) that computes the difference between oracle and 1-best hypothesis scores, scaled by their respective WERs. The method uses RNN-T models with conformer encoders and LSTM decoders, employing beam search decoding to generate hypotheses. O-1 can be applied to both supervised and unsupervised data, with unsupervised training using hard distillation. The objective is combined with the standard RNN-T loss using scaling factors λ and γ.

## Key Results
- O-1 achieves 13-25% relative WER improvements over EMBR on SpeechStew datasets
- 12% relative gap reduction between 1-best and oracle WER on in-house data
- Overall 9% relative WER improvement compared to EMBR baseline
- Consistent improvements across multiple public and in-house datasets

## Why This Works (Mechanism)

### Mechanism 1
O-1 reduces the gap between 1-best and oracle WER by directly boosting the oracle hypothesis score. The method computes the difference between oracle and 1-best hypothesis scores, scaled by their respective WERs, encouraging the model to prioritize hypotheses with lower error rates rather than just high probability. The core assumption is that the oracle hypothesis (best WER in the beam) is a better target than the 1-best hypothesis for improving recognition performance.

### Mechanism 2
O-1 training is computationally more efficient than EMBR because it only uses two hypotheses instead of the full n-best list. By eliminating the need to compute softmax over all hypotheses and expected word error, O-1 reduces per-batch computation cost. The computational savings from using only oracle and 1-best hypotheses is assumed to outweigh any potential loss in modeling the full distribution of hypotheses.

### Mechanism 3
O-1 addresses label bias by encouraging the model to re-learn from the oracle hypothesis rather than being biased towards the 1-best hypothesis. By boosting the oracle hypothesis score and suppressing the 1-best, O-1 forces the model to update its predictions based on lower-error hypotheses. The core assumption is that the oracle hypothesis provides a more accurate signal for model updates than the 1-best hypothesis, especially in cases where the 1-best is erroneous.

## Foundational Learning

- Concept: Self-training and co-training in speech and language processing
  - Why needed here: O-1 is a form of self-training that iteratively improves model predictions using both ground-truth and model-generated hypotheses
  - Quick check question: What is the key difference between self-training and co-training, and how does O-1 fit into these paradigms?

- Concept: Expected Minimum Bayes Risk (EMBR) and Minimum Bayes Risk (MBR) training
  - Why needed here: O-1 is a variant of EMBR, so understanding the core principles of MBR training is essential to grasp the motivation and mechanics of O-1
  - Quick check question: How does MBR training differ from maximum likelihood estimation (MLE) in terms of the optimization objective and the impact on model performance?

- Concept: Exposure bias and scheduled sampling in autoregressive models
  - Why needed here: O-1 addresses exposure bias by allowing the model to learn from its own predictions, similar to scheduled sampling but with a different mechanism
  - Quick check question: What is exposure bias, and how does scheduled sampling mitigate it? How does O-1's approach to exposure bias differ from scheduled sampling?

## Architecture Onboarding

- Component map: RNN-T model with conformer encoder -> LSTM decoder -> Beam search decoder -> O-1 loss computation -> Backpropagation

- Critical path: 1) Forward pass through RNN-T model to generate alignment probabilities 2) Beam search decoding to generate n-best hypotheses 3) WER computation between each hypothesis and ground-truth/pseudo-label 4) O-1 loss computation using oracle and 1-best hypotheses 5) Backpropagation and model update

- Design tradeoffs: Using larger beam size during O-1 training can improve oracle hypothesis selection but increases computational cost; choice of scaling factors (λ, γ) for RNN-T and O-1 losses impacts balance between sequence-level and token-level objectives; incorporating unsupervised data via distillation can improve model generalization but requires pre-trained teacher model

- Failure signatures: Degraded performance on test sets where EMBR also fails to improve; inconsistent improvements across different test sets or domains; increased computational cost without corresponding performance gains

- First 3 experiments: 1) Compare O-1 performance against baseline and EMBR on single test set to verify oracle hypothesis boosting mechanism 2) Vary beam size during O-1 training to identify optimal trade-off between oracle hypothesis selection and computational cost 3) Evaluate impact of RNN-T loss scaling factor (λ) on balance between sequence-level and token-level objectives

## Open Questions the Paper Calls Out

- Question: How does the performance of O-1 compare to other self-training methods like scheduled sampling or beam search optimization in terms of both accuracy and computational efficiency?
  - Basis in paper: [explicit] The paper mentions that O-1 differs from scheduled sampling in two aspects and provides a comparison on Librispeech test set
  - Why unresolved: While the paper provides a comparison with scheduled sampling, it does not provide a comprehensive comparison with other self-training methods or direct comparison of computational efficiency
  - What evidence would resolve it: A detailed comparative study of O-1 against other self-training methods in terms of both accuracy and computational efficiency

- Question: Can the O-1 objective be further improved by using multiple hypotheses from the teacher model and applying advanced distillation approaches?
  - Basis in paper: [explicit] The paper mentions in the conclusion that O-1 can be further improved in the future using multiple hypotheses from the teacher model and applying advanced distillation approaches
  - Why unresolved: The paper does not explore the potential improvements that could be achieved by using multiple hypotheses from the teacher model or advanced distillation approaches
  - What evidence would resolve it: Conducting experiments using multiple hypotheses from the teacher model and advanced distillation approaches

- Question: How does the performance of O-1 scale with the size of the training dataset and the complexity of the model architecture?
  - Basis in paper: [explicit] The paper demonstrates the effectiveness of O-1 on publicly available SpeechStew datasets and a large-scale, in-house data set
  - Why unresolved: While the paper shows the performance of O-1 on different datasets and model architectures, it does not provide a systematic analysis of how the performance scales with the size of the training dataset and the complexity of the model architecture
  - What evidence would resolve it: Conducting experiments with varying sizes of training datasets and model architectures

## Limitations

- O-1's effectiveness depends on the oracle hypothesis being reliably better than the 1-best hypothesis; may fail when oracle is consistently ranked low in beam
- Computational efficiency gains over EMBR may vary depending on implementation details and hardware configurations
- Performance improvements may be inconsistent across different test sets or domains where EMBR also fails to improve

## Confidence

- **High Confidence**: The core mechanism of O-1 (boosting oracle hypothesis while suppressing 1-best) is well-defined and reported improvements on SpeechStew and in-house datasets are consistent with the stated objective
- **Medium Confidence**: The computational efficiency claims relative to EMBR are plausible but would benefit from direct benchmarking on the same hardware and implementation
- **Medium Confidence**: The scalability claims for large-scale ASR tasks are supported by results but would be strengthened by ablation studies on beam size and hyperparameter sensitivity

## Next Checks

1. Conduct experiments to quantify the frequency and magnitude of cases where the oracle hypothesis differs from the 1-best hypothesis across different datasets and model configurations to validate the core assumption underlying O-1

2. Implement and benchmark O-1 against EMBR on the same hardware using identical datasets and model architectures to verify the claimed computational efficiency gains

3. Perform systematic ablation studies on the scaling factors (λ, γ) and beam size to determine the optimal configuration for different datasets and to assess the robustness of O-1 to hyperparameter choices