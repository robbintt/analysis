---
ver: rpa2
title: 'Squared Neural Families: A New Class of Tractable Density Models'
arxiv_id: '2305.13552'
source_url: https://arxiv.org/abs/2305.13552
tags:
- kernel
- neural
- which
- snefy
- normalising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Squared Neural Families (SNEFY), a new class
  of tractable density models formed by squaring the 2-norm of a neural network and
  normalizing it with respect to a base measure. The authors show that SNEFYs admit
  closed-form normalizing constants in many cases of interest, making them flexible
  yet fully tractable density models.
---

# Squared Neural Families: A New Class of Tractable Density Models

## Quick Facts
- arXiv ID: 2305.13552
- Source URL: https://arxiv.org/abs/2305.13552
- Reference count: 40
- Primary result: SNEFY achieves competitive performance on 2D synthetic unconditional density estimation tasks and outperforms other methods on astronomy data for photometric redshift estimation

## Executive Summary
This paper introduces Squared Neural Families (SNEFY), a new class of tractable density models formed by squaring the 2-norm of a neural network and normalizing it with respect to a base measure. The authors show that SNEFYs admit closed-form normalizing constants in many cases of interest, making them flexible yet fully tractable density models. SNEFYs strictly generalize classical exponential families, are closed under conditioning, and have tractable marginal distributions. The utility of SNEFYs is illustrated on a variety of density estimation, conditional density estimation, and density estimation with missing data tasks.

## Method Summary
SNEFY models are constructed by taking the squared 2-norm of a neural network output and normalizing it using a base measure. The key innovation is that the normalizing constant can be computed in closed form using neural network Gaussian process (NNGP) kernels. This is achieved by expressing the squared norm as a trace inner product involving readout parameters V and a kernel matrix KΘ, where the entries of KΘ are computed via NNKs that evaluate expected products of activation functions over the base measure. The models generalize classical exponential families, support tractable conditioning and marginalization under certain conditions, and can be trained using standard gradient-based optimization.

## Key Results
- SNEFYs achieve competitive performance on 2D synthetic unconditional density estimation tasks compared to normalizing flows
- On astronomy data for photometric redshift estimation, SNEFY outperforms other methods including conditional kernel density estimators and conditional normalizing flows
- SNEFYs maintain tractable normalization constants across multiple activation functions (cos, Snakea, exp) and base measures (Gaussian, sphere)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SNEFYs admit closed-form normalizing constants by leveraging neural network Gaussian process (NNGP) kernels in a reversed role.
- **Mechanism**: The squared 2-norm of the neural network evaluation, which forms the unnormalized density, can be expressed as a trace inner product involving the readout parameters V and a kernel matrix KΘ. The entries of KΘ are computed via neural network kernels (NNKs), which evaluate the expected product of activation functions over the base measure.
- **Core assumption**: The NNK admits a closed-form expression for specific combinations of activation function σ, sufficient statistic t, and base measure µ.
- **Evidence anchors**:
  - [abstract]: "Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit closed form normalising constants in many cases of interest"
  - [section]: "The coefficients of the quadratic depend on Θ = (W, b). We now characterise these coefficients of the quadratic in terms of the NNK evaluated at rows θi, θj of Θ"
  - [corpus]: Weak or missing - no direct mention of kernel forms or closed-form calculations in related papers
- **Break condition**: If the NNK for a chosen (σ, t, µ) does not admit a closed form, the normalizing constant cannot be computed exactly, breaking the tractability of SNEFY.

### Mechanism 2
- **Claim**: SNEFYs strictly generalize classical exponential families by allowing negative mixture weights via the readout parameters V.
- **Mechanism**: When σ(u) = exp(u/2), the unnormalized density can be written as a mixture of exponential family distributions with coefficients v⊤·,iv·,j, some of which may be negative. This allows for more flexible modeling compared to standard exponential family mixtures.
- **Core assumption**: The exponential family log-partition function is tractable for the chosen sufficient statistic t and base measure µ.
- **Evidence anchors**:
  - [abstract]: "SNEFYs strictly generalise classical exponential families"
  - [section]: "When m > 1 and n > 1, we obtain a type of exponential family mixture model with coefficients V ⊤V, some of which may be negative"
  - [corpus]: Weak or missing - related papers discuss squared families but do not mention negative weights or exponential family generalization
- **Break condition**: If the exponential family log-partition function is not tractable, the kernel matrix KΘ cannot be computed, and the generalization to exponential families fails.

### Mechanism 3
- **Claim**: SNEFYs are closed under conditioning and marginalization when the base measure µ factorizes and the sufficient statistic t has a specific structure.
- **Mechanism**: For conditional distributions, the conditioning variable can be absorbed into the bias term of the neural network, leading to a new SNEFY with modified parameters. For marginalization, the marginal density can be expressed as another SNEFY with a kernel matrix derived from the joint kernel.
- **Core assumption**: The base measure µ factorizes as µ(dx) = µ1(dx1)µ2(dx2) and the sufficient statistic t(x) splits as t(x) = [t1(x1), t2(x2)].
- **Evidence anchors**:
  - [abstract]: "SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions"
  - [section]: "Theorem 1... the conditional distribution of x1 given x2 = x2 is SNEFYX1,t1,σ,µ1 with parameters V and Θ1|2"
  - [corpus]: Weak or missing - no direct mention of conditioning or marginalization properties in related papers
- **Break condition**: If the base measure does not factorize or the sufficient statistic does not split appropriately, the conditional or marginal distributions may not be tractable SNEFYs.

## Foundational Learning

- **Concept**: Neural network Gaussian process (NNGP) kernels
  - Why needed here: NNGP kernels provide the closed-form expressions for the kernel matrix KΘ, which is crucial for computing the normalizing constant of SNEFY.
  - Quick check question: What is the form of the NNGP kernel for a ReLU activation function with identity sufficient statistic and Gaussian base measure?

- **Concept**: Exponential family distributions and their log-partition functions
  - Why needed here: SNEFYs generalize exponential families, and understanding the log-partition function is key to computing the kernel matrix KΘ and leveraging the tractability of exponential families.
  - Quick check question: What is the log-partition function for a Gaussian distribution with mean 0 and variance 1?

- **Concept**: Conditioning and marginalization in probability theory
  - Why needed here: SNEFYs are closed under conditioning and marginalization under certain conditions, which is important for building flexible models for conditional and marginal densities.
  - Quick check question: If (X, Y) is jointly Gaussian with mean 0 and covariance matrix [[1, ρ], [ρ, 1]], what is the conditional distribution of X given Y = y?

## Architecture Onboarding

- **Component map**: Input (t(x), µ) -> Hidden layer (W, b, σ) -> Readout (V) -> Output (SNEFY density)
- **Critical path**: Compute the kernel matrix KΘ using the chosen (σ, t, µ), then compute the normalizing constant as Tr(V ⊤V KΘ), and finally evaluate the density as the unnormalized density divided by the normalizing constant.
- **Design tradeoffs**:
  - Expressivity vs. tractability: More expressive activation functions and sufficient statistics may lead to intractable kernel matrices.
  - Constrained vs. unconstrained V: Constrained V (e.g., diagonal) may lead to simpler models but less expressive, while unconstrained V allows for more flexible mixture weights but may be harder to optimize.
  - Base measure choice: Different base measures can lead to different tractability properties and modeling assumptions.
- **Failure signatures**:
  - Non-positive normalizing constant: Indicates numerical issues or inappropriate choice of (σ, t, µ).
  - Ill-conditioned kernel matrix: May lead to optimization difficulties or numerical instability.
  - Poor density fit: May indicate that the chosen (σ, t, µ) is not expressive enough for the data.
- **First 3 experiments**:
  1. Verify the closed-form kernel matrix KΘ for a simple case (e.g., σ = exp, t = identity, µ = Gaussian).
  2. Compute the normalizing constant and evaluate the SNEFY density on a small synthetic dataset.
  3. Compare the performance of SNEFY with a standard exponential family model on a density estimation task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of activation function affect the tractability and performance of SNEFY models?
- Basis in paper: [explicit] The paper discusses various activation functions like cos, exp, and Snakea, and their impact on kernel computations and model performance.
- Why unresolved: While the paper provides examples of tractable activation functions, it does not systematically explore the full space of activation functions or their relative performance across different tasks.
- What evidence would resolve it: A comprehensive study comparing different activation functions on a range of tasks, analyzing both tractability and empirical performance.

### Open Question 2
- Question: What are the limitations of SNEFY models in terms of dimensionality and data complexity?
- Basis in paper: [inferred] The paper demonstrates SNEFY on 2D synthetic data and a 6D astronomy dataset, but does not extensively explore performance on higher-dimensional or more complex real-world data.
- Why unresolved: The experiments are limited in scope, and the theoretical analysis does not provide clear bounds on dimensionality or complexity.
- What evidence would resolve it: Experiments on high-dimensional datasets (e.g., image data) and analysis of model performance as a function of data complexity.

### Open Question 3
- Question: How can sampling from SNEFY models be made more efficient?
- Basis in paper: [explicit] The paper mentions that sampling using rejection sampling is computationally expensive compared to other models like normalizing flows.
- Why unresolved: The paper does not propose or investigate alternative sampling methods for SNEFY models.
- What evidence would resolve it: Development and comparison of efficient sampling algorithms for SNEFY models, potentially leveraging connections to other probabilistic models.

### Open Question 4
- Question: What is the relationship between SNEFY and other tractable density models, such as normalizing flows or kernel-based methods?
- Basis in paper: [explicit] The paper discusses connections to neural network Gaussian process kernels and kernel-based methods for nonnegative functions.
- Why unresolved: The paper does not provide a comprehensive comparison or theoretical analysis of the relationship between SNEFY and other tractable density models.
- What evidence would resolve it: A detailed study comparing SNEFY to other tractable density models in terms of expressiveness, computational efficiency, and theoretical properties.

## Limitations
- Closed-form normalizing constant relies critically on tractable neural network Gaussian process kernels for specific combinations of activation functions, sufficient statistics, and base measures
- Conditioning and marginalization properties require specific factorization assumptions on the base measure and sufficient statistic structure
- The paper demonstrates SNEFY on 2D synthetic data and a 6D astronomy dataset, but does not extensively explore performance on higher-dimensional or more complex real-world data

## Confidence
- **High**: The core mathematical framework connecting SNEFYs to NNGP kernels and the proof of closed-form normalizing constants for demonstrated cases
- **Medium**: Generalization claims to exponential families and the experimental results on 2D synthetic datasets
- **Low**: The broad applicability of conditioning/marginalization properties and the scalability to high-dimensional problems

## Next Checks
1. **Kernel Verification**: For the ReLU activation with identity sufficient statistic and Gaussian base measure, verify the closed-form expression for the NNGP kernel matrix KΘ against numerical integration to ensure correctness of the tractable normalizing constant computation.

2. **Negative Weight Behavior**: Construct a controlled experiment where the exponential family generalization mechanism (negative mixture weights via V⊤V) is essential for capturing a bimodal distribution, demonstrating that standard exponential family mixtures cannot fit this data while SNEFY can.

3. **Conditioning Robustness**: Test the conditioning property on a synthetic dataset where the base measure factorizes appropriately, verifying that the conditional distribution computed via the paper's method matches the empirical conditional density obtained through direct sampling and kernel density estimation.