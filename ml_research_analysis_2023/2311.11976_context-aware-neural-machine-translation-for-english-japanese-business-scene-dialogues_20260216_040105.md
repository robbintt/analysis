---
ver: rpa2
title: Context-aware Neural Machine Translation for English-Japanese Business Scene
  Dialogues
arxiv_id: '2311.11976'
source_url: https://arxiv.org/abs/2311.11976
tags:
- context
- translation
- speaker
- information
- context-aware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates context-aware neural machine translation
  for English-Japanese business dialogue scenarios. The authors adapt a pretrained
  mBART model and propose a novel attention mechanism (CoAttMask) to leverage context,
  particularly source-side context, for improved translation.
---

# Context-aware Neural Machine Translation for English-Japanese Business Scene Dialogues

## Quick Facts
- arXiv ID: 2311.11976
- Source URL: https://arxiv.org/abs/2311.11976
- Reference count: 8
- Key outcome: CoAttMask attention with source-side context and speaker/scene tokens improves BLEU/COMET and CXMI for English-Japanese business dialogue translation, especially for honorifics.

## Executive Summary
This paper investigates context-aware neural machine translation for English-Japanese business dialogue scenarios. The authors adapt a pretrained mBART model and propose a novel attention mechanism (CoAttMask) to leverage context, particularly source-side context, for improved translation. They also explore the use of extra-sentential information such as speaker turn and scene type as additional context. The results show that increased source-side context paired with scene and speaker information improves translation quality compared to context-agnostic baselines and previous work, measured in BLEU and COMET metrics. The models also show increased usage of context as measured by CXMI. The analysis reveals that context is particularly beneficial for translating honorifics in Japanese.

## Method Summary
The paper adapts a pretrained mBART model for context-aware English-Japanese translation. The proposed CoAttMask architecture masks context representations during decoder cross-attention while retaining them for encoder self-attention. Extra-sentential information such as speaker turn and scene type is added as special tokens. The model is fine-tuned on business dialogue data with varying context sizes, and evaluated using BLEU, COMET, and CXMI metrics.

## Key Results
- CoAttMask with source-side context and speaker/scene tokens outperforms context-agnostic baselines on BLEU and COMET metrics.
- CXMI scores increase with larger context sizes, indicating greater context usage.
- Context is particularly beneficial for translating Japanese honorifics, with significant improvements in CXMI for honorific tokens.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CoAttMask architecture improves translation quality by masking context representations during decoder cross-attention while retaining them for encoder self-attention.
- Mechanism: Context sentences are fed through the encoder, allowing self-attention to learn richer input representations. During decoding, only the source sentence's encoder outputs are passed to the decoder for cross-attention, preventing context confusion.
- Core assumption: Larger pretrained models (mBART) can benefit from context-enhanced encoder representations even when context is excluded from decoder attention.
- Evidence anchors:
  - [abstract]: "We adapted a pretrained mBART model, finetuning on multi-sentence dialogue data, which allows us to experiment with different contexts."
  - [section]: "We pass the context-extended input to the encoder part of the model but mask the encoder outputs that correspond to the context when passed to the decoder."
  - [corpus]: Weak evidence—CoAttMask performance improvements are only shown for source-side context; no explicit corpus-level comparison is provided.
- Break condition: If context representations are critical for decoder inference (e.g., resolving pronoun coreference), masking may degrade performance.

### Mechanism 2
- Claim: Adding extra-sentential information such as speaker turn and scene type as special tokens provides useful context for Japanese honorific translation.
- Mechanism: Speaker and scene tokens are prepended or interleaved with context sentences, allowing the model to learn associations between speaker formality/scene type and honorific usage.
- Core assumption: Japanese honorifics are tightly linked to speaker role and situational context, so encoding these explicitly helps the model choose appropriate forms.
- Evidence anchors:
  - [abstract]: "We propose novel extra-sentential information elements such as speaker turn and scene type, to be used as additional source-side context."
  - [section]: "In a dialogue dataset with multiple speakers... using a fixed context window implies potentially including multiple speakers in the context."
  - [corpus]: Moderate—CXMI scores increase when speaker/scene tokens are added, but gains are mostly in BLEU/COMET for smaller context windows.
- Break condition: If the dataset contains mostly informal dialogue or speakers with similar formality levels, these tokens may add noise rather than signal.

### Mechanism 3
- Claim: Increasing source-side context size improves CXMI and translation quality for honorifics, especially for tokens like "伺(ukaga)".
- Mechanism: More preceding sentences increase the chance of capturing speaker-addressee relationships, which are necessary to choose the correct honorific level.
- Core assumption: Honorifics depend on explicit relational cues in the dialogue history; these cues become more reliable as context expands.
- Evidence anchors:
  - [abstract]: "increased source-side context paired with scene and speaker information improves the model performance... measured in BLEU and COMET metrics."
  - [section]: "Honorifics CXMI scores for all context sizes show positive score, indicating that the provision of additional context helps the model to attribute higher density to the correct honorific translation."
  - [corpus]: Weak—CXMI rises monotonically with context size, but BLEU/COMET gains plateau or degrade beyond context size 3-4.
- Break condition: If honorific usage is independent of speaker role (e.g., fixed politeness level), longer context may dilute relevant signals.

## Foundational Learning

- Concept: Transformer encoder-decoder architecture
  - Why needed here: Understanding how self-attention in the encoder and cross-attention in the decoder operate is critical to grasping why CoAttMask masks context during decoding.
  - Quick check question: What is the difference between self-attention in the encoder and cross-attention between encoder and decoder?

- Concept: Pretraining and fine-tuning paradigm (e.g., mBART)
  - Why needed here: The paper relies on adapting a large multilingual model; understanding pretraining objectives and fine-tuning strategies is essential.
  - Quick check question: Why is fine-tuning a pretrained model preferable to training from scratch for small dialogue datasets?

- Concept: Mutual information and CXMI metrics
  - Why needed here: CXMI is used to measure how much context the model actually leverages; understanding its calculation and interpretation is key to evaluating context-aware models.
  - Quick check question: How does CXMI differ from BLEU in measuring the impact of context on translation quality?

## Architecture Onboarding

- Component map: mBART encoder (context + source sentences) → CoAttMask layer → masked source encoder outputs → decoder → target output
- Critical path:
  1. Input preprocessing: concatenate context sentences + special tokens
  2. Encoder: self-attention over full input
  3. CoAttMask: zero out context part of encoder outputs
  4. Decoder: cross-attention only over source encoder outputs
  5. Loss: computed only on target sentence (not context)
- Design tradeoffs:
  - Masking context in decoder reduces computational load but may miss some context-dependent translation cues.
  - Using special tokens for speaker/scene simplifies input representation but may not capture complex speaker dynamics.
  - Larger context sizes improve CXMI but may hurt BLEU/COMET if context becomes too noisy.
- Failure signatures:
  - BLEU/COMET degrades as context size increases → context overload or model confusion
  - CXMI increases but BLEU/COMET flatlines → context is being used but not effectively for final translation
  - Model predicts honorifics incorrectly in speaker-change scenarios → speaker tags not informative enough
- First 3 experiments:
  1. Ablation: Compare CoAttMask vs standard context-aware (no mask) on BLEU/CXMI to confirm mask helps.
  2. Token analysis: Inspect honorific translations for "伺(ukaga)" with/without speaker tags.
  3. Context window sweep: Vary source context size (1-5) and record BLEU, COMET, CXMI to find optimal context length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing context size beyond 4 sentences continue to improve translation quality for English-Japanese business dialogue translation, or does the benefit plateau or even decrease?
- Basis in paper: [explicit] The paper experiments with context sizes up to 5 sentences and observes increased CXMI scores, but only notes significant BLEU/COMET improvements for context sizes 3 and 4. The paper mentions that "it would be interesting to consider further adapting target-side context or explore pre-training on larger corpora as a way to mitigate this in future work."
- Why unresolved: The experiments only tested context sizes up to 5 sentences, and the results suggest that very large context sizes might not be beneficial for this task. However, the paper doesn't explore context sizes beyond 5 sentences.
- What evidence would resolve it: Further experiments with context sizes larger than 5 sentences, measuring both CXMI and translation quality metrics (BLEU, COMET) to determine if there's a point of diminishing returns or if the benefits continue to increase.

### Open Question 2
- Question: How does the proposed CoAttMask architecture compare to other context-aware NMT architectures (e.g., multi-encoder models, cross-attention mechanisms) for English-Japanese dialogue translation, especially in low-resource settings?
- Basis in paper: [explicit] The paper proposes the CoAttMask architecture as an improvement over the Tiedemann and Scherrer (2017) approach, but doesn't compare it to other context-aware architectures. The paper states "we found that directly using the extended source inputs resulted in significantly lower performance for all context sizes, when compared to the original context-agnostic model" and proposes CoAttMask to address this issue.
- Why unresolved: The paper only compares the CoAttMask architecture to the baseline Tiedemann and Scherrer (2017) approach and a context-agnostic model. It doesn't explore how CoAttMask performs compared to other state-of-the-art context-aware architectures.
- What evidence would resolve it: Experiments comparing CoAttMask to other context-aware NMT architectures (e.g., multi-encoder models, cross-attention mechanisms) on the same English-Japanese dialogue dataset, measuring both translation quality and context usage.

### Open Question 3
- Question: How does the performance of context-aware models for English-Japanese dialogue translation change when using different types of extra-sentential context, such as speaker personality traits, dialogue act tags, or topic information?
- Basis in paper: [explicit] The paper explores the use of speaker turn information and scene type as extra-sentential context, finding that combining both improves performance. The paper states "Depending on the scene the speakers may change more or less frequently signifying a necessary change of style (e.g. compare a presentation scene versus the phone call one)."
- Why unresolved: The paper only experiments with two types of extra-sentential context (speaker turn and scene type). There are many other types of contextual information that could potentially be useful for dialogue translation, such as speaker personality traits, dialogue act tags, or topic information.
- What evidence would resolve it: Experiments using different types of extra-sentential context (e.g., speaker personality traits, dialogue act tags, topic information) in combination with source-side context, measuring both translation quality and context usage to determine which types of extra-sentential context are most beneficial for English-Japanese dialogue translation.

## Limitations
- The CoAttMask mechanism's benefits are only demonstrated for source-side context; target-side context improvements remain unexplored.
- The use of speaker and scene tokens may not generalize beyond business dialogue scenarios with predictable speaker formality levels.
- CXMI, while useful for measuring context usage, does not directly correlate with translation quality improvements.

## Confidence
- High Confidence: The core mechanism of CoAttMask (masking context during decoder cross-attention) is clearly described and theoretically justified.
- Medium Confidence: The claim that speaker and scene tokens improve honorific translation is supported by CXMI scores and token-level analysis, but lacks robust evidence across diverse dialogue scenarios.
- Low Confidence: The assertion that CoAttMask outperforms context-aware baselines in all scenarios is questionable, as BLEU/COMET gains plateau or degrade beyond context size 3-4.

## Next Checks
1. Conduct an ablation study with varying context masking strategies to isolate the specific benefits of the masking mechanism.
2. Test the model on non-business dialogue datasets to assess generalizability of speaker/scene tokens and CoAttMask architecture.
3. Perform human evaluation of honorific translations to determine if context usage translates to meaningful quality improvements from a human perspective.