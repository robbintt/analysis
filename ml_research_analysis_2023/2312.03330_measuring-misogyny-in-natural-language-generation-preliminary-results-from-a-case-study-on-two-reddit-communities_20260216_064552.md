---
ver: rpa2
title: 'Measuring Misogyny in Natural Language Generation: Preliminary Results from
  a Case Study on two Reddit Communities'
arxiv_id: '2312.03330'
source_url: https://arxiv.org/abs/2312.03330
tags:
- toxicity
- language
- misogyny
- classifier
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers demonstrated that generic toxicity classifiers fail
  to distinguish between language models fine-tuned on misogynistic versus less misogynistic
  Reddit communities. Using data from r/Incels and r/ForeverAlone, they fine-tuned
  two Pythia language models and evaluated generations using both an open-source toxicity
  classifier and a feminist-designed misogyny lexicon.
---

# Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities

## Quick Facts
- arXiv ID: 2312.03330
- Source URL: https://arxiv.org/abs/2312.03330
- Reference count: 17
- Generic toxicity classifiers fail to distinguish misogynistic content from less misogynistic content in language model evaluations

## Executive Summary
This paper demonstrates that generic toxicity classifiers are inadequate for measuring specific ideological harms like misogyny in language model generations. Using data from r/Incels and r/ForeverAlone Reddit communities, researchers fine-tuned two Pythia language models and evaluated their outputs using both a standard toxicity classifier and a feminist-designed misogyny lexicon. While the toxicity classifier showed no meaningful difference between models, the lexicon successfully identified more misogynistic content from the r/Incels model, aligning with prior qualitative research. The study highlights the need for subject-matter expert lexicons as benchmarks for evaluating language model harms.

## Method Summary
Researchers collected Reddit data from r/Incels and r/ForeverAlone communities (2011-2016), filtered for posts ≥100 characters, and removed duplicates with cosine similarity ≥0.9. They fine-tuned 12B parameter Pythia models on these datasets using DeepSpeed ZeRO-3 with 4× A100 GPUs and 512GB RAM. The models generated 25 completions per prompt from the RealToxicityPrompts dataset using nucleus sampling. Evaluations were conducted using both Detoxify-multilingual toxicity classifier and the Farrell lexicon for misogyny detection, computing Expected Maximum Likelihood (EML) and Binary Classifier Frequency (BCF) metrics.

## Key Results
- Generic toxicity classifiers showed no meaningful difference between models trained on misogynistic versus less misogynistic data
- Feminist-designed lexicon successfully identified more misogynistic content from r/Incels model compared to r/ForeverAlone model
- Lexicon-based approach aligned with prior qualitative research on these communities
- Standard toxicity evaluation metrics would have missed important ideological differences in model behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generic toxicity classifiers conflate various forms of harmful language, making them ineffective for detecting specific ideological harms like misogyny.
- Mechanism: The classifier aggregates signals from diverse toxic behaviors (profanity, threats, insults) without distinguishing between content-specific ideologies, leading to poor signal-to-noise ratios when targeting specific harms.
- Core assumption: Different harmful language patterns produce distinct statistical signatures that generic classifiers cannot disentangle.
- Evidence anchors:
  - [abstract]: "Generic ‘toxicity’ classifiers continue to be used for evaluating the potential for harm in natural language generation, despite mounting evidence of their shortcomings."
  - [section]: "Using only a generic ‘toxicity’ classifier, looking at the EML metric, the r/Incels fine-tuned model receives low ‘toxicity’ scores."
  - [corpus]: Found 25 related papers with average neighbor FMR=0.485, suggesting significant research interest in toxicity and bias detection challenges.

### Mechanism 2
- Claim: Lexicon-based approaches with subject-matter expertise can better capture specific ideological harms by targeting precise linguistic patterns.
- Mechanism: Domain experts create lexicons that map to specific harmful behaviors, allowing classifiers to detect nuanced patterns that generic models miss due to their broad training objectives.
- Core assumption: Subject-matter experts can identify linguistic markers of specific harms that are statistically distinguishable from general toxicity.
- Evidence anchors:
  - [abstract]: "We contrast this with a misogyny-specific lexicon recently proposed by feminist subject-matter experts, demonstrating that... this shows promise as a benchmark to evaluate language models for misogyny."
  - [section]: "When considered using the Farrell lexicon though, the result better aligns with the prior literature studying these communities – r/Incels is seen to be more misogynistic than r/ForeverAlone."
  - [corpus]: Neighbor paper "PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets" suggests lexicon approaches are being explored for nuanced harm detection.

### Mechanism 3
- Claim: Comparing models trained on corpora with known differences in harmful behavior can reveal classifier sensitivity to specific ideological harms.
- Mechanism: By fine-tuning models on datasets with documented differences in misogyny levels, we create a controlled experiment where classifier performance can be measured against ground truth.
- Core assumption: The training corpora differences will be reflected in model generations, providing a measurable signal for classifier evaluation.
- Evidence anchors:
  - [abstract]: "We use data from two well-characterised ‘Incel’ communities on Reddit that differ primarily in their degrees of misogyny to construct a pair of training corpora."
  - [section]: "r/Incels and r/ForeverAlone... differ markedly in the degree of misogynistic ideology they express (see Appendix B)."
  - [corpus]: Neighbor paper "Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large Language Models" indicates active research in auditing models for specific biases.

## Foundational Learning

- Concept: Toxic language detection limitations
  - Why needed here: Understanding why generic toxicity classifiers fail is crucial for developing better evaluation methods.
  - Quick check question: What are the documented shortcomings of generic toxicity classifiers when applied to specific ideological harms?

- Concept: Lexicon-based evaluation approaches
  - Why needed here: Lexicon methods offer an alternative to statistical classifiers by using expert-curated terms for specific harms.
  - Quick check question: How do lexicon-based approaches differ from statistical classifiers in handling context and nuance?

- Concept: Model fine-tuning for behavior analysis
  - Why needed here: Fine-tuning on ideologically distinct datasets allows controlled comparison of classifier performance.
  - Quick check question: What considerations are important when selecting training corpora for analyzing specific harmful behaviors in language models?

## Architecture Onboarding

- Component map: Data collection pipeline (Reddit API) -> Text preprocessing and filtering -> Language model fine-tuning system -> Generation system with prompt dataset -> Toxicity classifier (Detoxify-multilingual) -> Misogyny lexicon evaluator (Farrell lexicon) -> Evaluation metrics calculator

- Critical path:
  1. Collect and preprocess Reddit data
  2. Fine-tune language models on different datasets
  3. Generate completions using RealToxicityPrompts
  4. Run both toxicity classifier and lexicon evaluation
  5. Compare results against ground truth from literature

- Design tradeoffs:
  - Using open-source classifier vs. Perspective API (speed vs. potentially better performance)
  - Simple lexicon matching vs. more sophisticated NLP approaches for lexicon evaluation
  - Single lexicon vs. multiple specialized lexicons for different harm types

- Failure signatures:
  - Similar scores from toxicity classifier across models with known behavioral differences
  - High variance in lexicon evaluation results indicating poor coverage
  - Model generations that do not reflect training data characteristics

- First 3 experiments:
  1. Compare a single model's generations against both the toxicity classifier and lexicon to establish baseline correlation
  2. Run the evaluation pipeline on a small sample of prompts with manual verification to check for implementation errors
  3. Test the sensitivity of the lexicon approach by evaluating generations known to contain specific misogynistic terms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would lexicon-based approaches perform better than toxicity classifiers for detecting other specific harmful behaviors beyond misogyny?
- Basis in paper: [explicit] The paper demonstrates that a feminist-designed lexicon outperformed generic toxicity classifiers specifically for measuring misogyny
- Why unresolved: The study only tested lexicon-based approaches for misogyny. It's unclear if this approach would generalize to other types of harmful content like racism, ableism, or homophobia
- What evidence would resolve it: Testing lexicon-based approaches designed by subject-matter experts against toxicity classifiers for measuring other specific harmful behaviors

### Open Question 2
- Question: How do different toxicity classifiers compare in their ability to detect misogynistic content versus the Farrell lexicon?
- Basis in paper: [explicit] The paper only tested one open-source toxicity classifier (Detoxify-multilingual) against the Farrell lexicon
- Why unresolved: There are multiple toxicity classifiers available, including the Perspective API which the paper couldn't access due to rate limits. Different classifiers may have varying levels of success
- What evidence would resolve it: Systematic comparison of multiple toxicity classifiers against the Farrell lexicon for detecting misogynistic content

### Open Question 3
- Question: What is the relationship between the specific training data used to fine-tune language models and their tendency to generate misogynistic content?
- Basis in paper: [inferred] The paper found that models fine-tuned on r/Incels data generated more misogynistic content than those fine-tuned on r/ForeverAlone, despite these communities having similar overall topics
- Why unresolved: The study only tested two communities. It's unclear how different types or degrees of misogynistic content in training data affect model outputs
- What evidence would resolve it: Systematic testing of language models fine-tuned on various datasets with different levels and types of misogynistic content

## Limitations
- Lexicon-based approach relies on a single feminist-designed misogyny lexicon that may not capture all forms of misogynistic expression
- Evaluation used a specific set of prompts (RealToxicityPrompts) which may not represent all contexts where misogynistic content could emerge
- Study focused on only two Reddit communities, limiting generalizability to other online spaces or forms of harmful content

## Confidence
- High Confidence: Generic toxicity classifiers fail to distinguish misogynistic content from less misogynistic content in language model evaluations
- Medium Confidence: Lexicon-based approach shows promise but requires further validation across different contexts and harm types
- Low Confidence: Broader applicability of findings to other forms of harmful content beyond misogyny and other evaluation scenarios

## Next Checks
1. Test the lexicon-based evaluation approach on language models fine-tuned on different types of harmful content (e.g., racism, homophobia) to assess its generalizability across various ideological harms.

2. Conduct human evaluation studies to validate the lexicon-based scores against human judgments of misogynistic content, particularly for edge cases where automated detection might struggle.

3. Experiment with combining multiple specialized lexicons and domain-specific classifiers to create a more robust evaluation framework that can detect various forms of harmful content while maintaining sensitivity to specific ideological harms.