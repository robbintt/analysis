---
ver: rpa2
title: 'Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset'
arxiv_id: '2311.05113'
source_url: https://arxiv.org/abs/2311.05113
tags:
- reasoning
- problems
- language
- dataset
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose CONIC 10K, a dataset for evaluating mathematical understanding
  and reasoning ability. Our dataset contains 10,861 math problems about conic sections
  in Chinese senior high school education.
---

# Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset

## Quick Facts
- arXiv ID: 2311.05113
- Source URL: https://arxiv.org/abs/2311.05113
- Reference count: 40
- Key outcome: Dataset reveals LLMs struggle with complex mathematical reasoning despite narrow domain focus

## Executive Summary
Conic10K presents a dataset of 10,861 Chinese math problems focused on conic sections to evaluate large language models' mathematical understanding and reasoning capabilities. The dataset includes formal representations using Assertional Logic, step-by-step reasoning, and final solutions. Experiments demonstrate that existing models, including GPT-4, achieve only 15.5% accuracy on complex reasoning tasks, revealing significant limitations in current approaches to mathematical reasoning. The narrow domain enables clear separation of knowledge from reasoning ability, making it an effective benchmark for evaluating true reasoning versus pattern matching.

## Method Summary
The Conic10K dataset construction involves collecting Chinese math problems about conic sections, annotating them with high-quality formal representations using Assertional Logic, and providing complete reasoning steps and solutions. Models are evaluated on two tasks: semantic parsing (translating problems to formal representations) and math question answering (generating solutions). The evaluation pipeline includes finetuning on semantic parsing, zero-shot chain-of-thought inference for mathQA, and human evaluation of model outputs. The narrow domain focus enables isolation of reasoning ability from background knowledge requirements.

## Key Results
- GPT-4 achieves only 15.5% accuracy on mathQA task using human evaluation
- Best finetuned model (Vicuna) achieves 39.9% accuracy on semantic parsing
- Problems requiring 10+ reasoning steps show significantly worse model performance
- Models frequently fail to find shortcut solutions, using unnecessarily complex approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Focused narrow-domain knowledge enables clear separation of reasoning vs. knowledge gaps
- Mechanism: By restricting problems to only conic sections, any model failure can be attributed specifically to reasoning ability rather than missing background knowledge
- Core assumption: The dataset contains sufficient problem variety within conic sections to test different reasoning depths while keeping the required knowledge constant
- Evidence anchors: [abstract] "Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has."

### Mechanism 2
- Claim: High-quality formal representations enable precise semantic parsing evaluation
- Mechanism: The formal representation using Assertional Logic provides unambiguous, structured descriptions that can be used to evaluate whether models truly understand mathematical concepts
- Core assumption: The formal representation is both comprehensive enough to capture all problem nuances and simple enough for models to learn
- Evidence anchors: [section 3.1] "Our representation is built upon Assertional Logic (Zhou, 2017)...In this work, we use a variant of AL with three components: declarations, facts and queries."

### Mechanism 3
- Claim: Long reasoning chains expose limitations in current language models
- Mechanism: Problems requiring multiple reasoning steps reveal that current models struggle with maintaining correctness throughout extended logical derivations
- Core assumption: The dataset contains problems with sufficiently long reasoning chains to distinguish between shallow pattern matching and true reasoning
- Evidence anchors: [section 3.4] "We also estimate the number of reasoning steps...Results show that CONIC 10K is the dataset with the second largest number of reasoning steps."

## Foundational Learning

- Concept: Assertional Logic formal representation
  - Why needed here: Provides unambiguous, structured way to represent mathematical problems that avoids the ambiguity of natural language
  - Quick check question: Can you write a formal representation for "Find the distance between two foci of an ellipse with eccentricity 1/2 and major axis length 10"?

- Concept: Chain-of-thought reasoning evaluation
  - Why needed here: The dataset's design requires understanding how models handle multi-step mathematical reasoning rather than just pattern matching
  - Quick check question: What distinguishes a model that memorizes common problem patterns from one that can reason through novel problems of similar structure?

- Concept: Semantic parsing vs. math question answering
  - Why needed here: The paper evaluates two different aspects of mathematical ability - understanding vs. reasoning - using separate tasks
  - Quick check question: Why might a model perform well on semantic parsing (translating to formal representations) but poorly on math question answering (generating answers)?

## Architecture Onboarding

- Component map: Dataset construction → Formal representation annotation → Model evaluation pipeline → Analysis framework
- Critical path: Data collection → Annotation quality control → Model training/fine-tuning → Evaluation with human judgment → Analysis of failure modes
- Design tradeoffs: Narrower domain knowledge (conic sections only) vs. broader applicability; complex formal representations vs. model learning difficulty; Chinese language vs. English language evaluation
- Failure signatures: Models that generate plausible-looking but incorrect reasoning; models that fail on long LaTeX expressions; models that cannot find shortcut solutions; models that hallucinate intermediate steps
- First 3 experiments:
  1. Evaluate baseline models on semantic parsing task to establish understanding capability
  2. Test zero-shot chain-of-thought reasoning on mathQA task to assess reasoning ability
  3. Analyze error patterns in formal representation generation to identify specific model limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of large language models on mathematical reasoning tasks vary when using different formal representations, such as propositional logic or first-order logic, compared to the assertional logic used in Conic10K?
- Basis in paper: [inferred] The paper mentions that the formal representation used in Conic10K is designed to be accurate, unambiguous, and close to natural language, but it does not fit any existing symbolic reasoners. The authors suggest that their conclusions may not apply to other formal representations such as propositional logic and first-order logic.
- Why unresolved: The paper does not provide any direct comparison of model performance using different formal representations. The authors only speculate that their conclusions may not apply to other formal representations without providing evidence.
- What evidence would resolve it: Conducting experiments to evaluate the performance of large language models on mathematical reasoning tasks using different formal representations, such as propositional logic or first-order logic, and comparing the results with those obtained using the assertional logic in Conic10K.

### Open Question 2
- Question: How does the ability of large language models to find shortcut solutions in mathematical reasoning tasks relate to their overall performance on complex reasoning tasks?
- Basis in paper: [explicit] The paper mentions that large language models tend to employ naive approaches to solve problems and fail to find shortcut solutions, which leads to more complicated computation and longer reasoning steps. The authors provide examples of naive solutions from GPT-4 and the corresponding shortcut solutions.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the ability to find shortcut solutions and overall performance on complex reasoning tasks. It is unclear whether the inability to find shortcuts is a major factor contributing to poor performance on complex reasoning tasks.
- What evidence would resolve it: Conducting experiments to measure the correlation between the ability to find shortcut solutions and overall performance on complex reasoning tasks. Analyzing the reasoning steps and computation complexity of model solutions to identify patterns and potential improvements.

### Open Question 3
- Question: How does the performance of large language models on mathematical reasoning tasks differ when the problems are presented in different languages, such as Chinese and English?
- Basis in paper: [explicit] The paper mentions that translating problems into English improves the performance of GPT-4 on mathematical reasoning tasks, but the accuracy is still significantly lower than that of human experts. The authors conclude that the primary challenge of mathQA in Conic10K lies in how to do mathematical reasoning correctly, rather than the language used.
- Why unresolved: The paper only provides a comparison between Chinese and English, but does not explore other languages or investigate the factors that contribute to the differences in performance. It is unclear whether the improvement in performance is due to the language itself or other factors such as the availability of training data or the complexity of the problems.
- What evidence would resolve it: Conducting experiments to evaluate the performance of large language models on mathematical reasoning tasks using problems presented in different languages. Analyzing the factors that contribute to the differences in performance, such as the availability of training data, the complexity of the problems, and the linguistic features of the languages.

## Limitations
- Narrow domain focus (conic sections only) may limit generalizability to broader mathematical reasoning
- Human evaluation of 200 samples represents small fraction of full test set, raising representativeness concerns
- Translation to English for GPT-4 evaluation introduces potential semantic drift affecting performance comparisons

## Confidence
- High confidence: Existing LLMs show weak performance on complex mathematical reasoning (supported by quantitative results)
- Medium confidence: Narrow domain restriction effectively isolates reasoning ability from knowledge gaps (mechanistically sound but depends on dataset diversity)
- Low confidence: Assertional Logic formal representations are optimal for this task (no comparison to alternative representation schemes provided)

## Next Checks
1. Evaluate model performance on broader mathematics dataset (GSM8K or MATH) to assess domain generalization
2. Implement and compare simpler formal representation scheme against Assertional Logic approach
3. Systematically analyze model performance across different problem complexity levels to identify reasoning capability breakpoints