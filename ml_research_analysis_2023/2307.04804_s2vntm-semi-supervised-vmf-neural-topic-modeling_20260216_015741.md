---
ver: rpa2
title: 'S2vNTM: Semi-supervised vMF Neural Topic Modeling'
arxiv_id: '2307.04804'
source_url: https://arxiv.org/abs/2307.04804
tags:
- topic
- topics
- keywords
- s2vntm
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S2vNTM, a semi-supervised neural topic model
  that uses von Mises-Fisher (vMF) distribution and negative sampling to incorporate
  seed keywords for topic modeling. The model takes a small set of seed keywords for
  a subset of topics and leverages the semantic patterns in the keywords to discover
  additional topics and refine the keyword sets.
---

# S2vNTM: Semi-supervised vMF Neural Topic Modeling

## Quick Facts
- arXiv ID: 2307.04804
- Source URL: https://arxiv.org/abs/2307.04804
- Authors: 
- Reference count: 24
- This paper introduces S2vNTM, a semi-supervised neural topic model that uses von Mises-Fisher (vMF) distribution and negative sampling to incorporate seed keywords for topic modeling.

## Executive Summary
This paper introduces S2vNTM, a semi-supervised neural topic model that uses von Mises-Fisher (vMF) distribution and negative sampling to incorporate seed keywords for topic modeling. The model takes a small set of seed keywords for a subset of topics and leverages the semantic patterns in the keywords to discover additional topics and refine the keyword sets. S2vNTM uses vMF distribution for its clustering properties and spherical word embeddings trained on the dataset. It also employs negative sampling to exclude unrelated keywords and improve topic coherence. The model is trained end-to-end with a loss function that combines reconstruction error, KL divergence, and keyword-based regularization. Experimental results on three datasets (AG News, R8, and DBLP) show that S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy, topic diversity, and runtime efficiency.

## Method Summary
S2vNTM is a semi-supervised neural topic model that uses vMF distribution as latent distribution, negative sampling, and spherical word embeddings. The model takes bag-of-words input and passes it through a neural network encoder (2 hidden layers [256,64], ReLU, dropout 0.5) to produce vMF parameters (μ, κ). A sample from the vMF distribution is scaled by a temperature function and used to compute topic proportions. The decoder uses softmax over topic-word matrix (fixed spherical embeddings) to reconstruct the input. The loss combines reconstruction loss, KL divergence, keyword-based regularization (LCE), and negative sampling penalty (LNS). The model is trained using Adam optimizer (lr=0.002, batch=256) and Smith & Topin scheduler (lr=0.01, max_iter=50) with 80% test split.

## Key Results
- S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy on AG News, R8, and DBLP datasets
- The model achieves higher topic diversity (percentage of unique words in top 25 words across all topics) compared to baselines
- S2vNTM demonstrates runtime efficiency advantages over existing methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The vMF distribution provides better clustering properties than Gaussian priors in low-dimensional latent spaces.
- **Mechanism**: vMF places probability mass uniformly on a hypersphere, avoiding concentration at the origin and encouraging topic vectors to form distinct clusters on the sphere surface.
- **Core assumption**: Topics in the document space correspond to distinct directions in the latent embedding space.
- **Evidence anchors**:
  - [abstract]: "We use von Mises-Fisher distribution because it captures distributions on unit sphere and induces better clustering properties."
  - [section]: "vMF based V AE has better clusterability of data points especially in low dimensions Guu et al. (2018)."
  - [corpus]: Weak—only mentions related work on vMF-based methods, no direct quantitative comparison provided.
- **Break condition**: If the true topic distribution is not well represented by spherical clustering (e.g., topics are not naturally directional), the vMF assumption will degrade performance.

### Mechanism 2
- **Claim**: Negative sampling pushes away unrelated keywords, improving topic coherence.
- **Mechanism**: For each matched keyword set, the model samples unrelated words from the top-N topic vocabulary and applies a penalty that reduces their probability, effectively separating topics semantically.
- **Core assumption**: Words that are semantically distant from the seed keywords should have reduced association with the topic.
- **Evidence anchors**:
  - [abstract]: "S2vNTM leverages negative sampling to create topics that match the pattern of selected keywords."
  - [section]: "When a keyword set is matched with a topic, we want the topic to be less correlated with words that are unrelated to the matched keyword set."
  - [corpus]: Weak—only mentions negative sampling in related work, no ablation on negative sampling alone.
- **Break condition**: If seed keywords are too generic or ambiguous, negative sampling may incorrectly suppress valid topic words.

### Mechanism 3
- **Claim**: Temperature scaling increases expressivity by spreading topic probabilities more uniformly across the vocabulary.
- **Mechanism**: Multiplying the sampled vMF direction by a temperature factor before softmax increases the maximum probability assigned to a single topic, allowing more polarized topic distributions.
- **Core assumption**: Higher temperature allows the model to assign more probability mass to dominant topics, improving discrimination.
- **Evidence anchors**:
  - [section]: "To overcome the expressibility concern... temperature function τtemp is used to increase expressibility."
  - [section]: "If we let τtemp(ηd) = 10 ∗ ηd, the highest topic proportion of the above example becomes 0.99."
  - [corpus]: Weak—only shows empirical tuning, no theoretical justification provided.
- **Break condition**: Excessively high temperature may collapse topics into single words or cause instability in learning.

## Foundational Learning

- **Concept**: von Mises-Fisher (vMF) distribution
  - Why needed here: Provides a natural prior for directional data, enabling spherical clustering of topics.
  - Quick check question: What property of the vMF distribution makes it suitable for topic modeling compared to Gaussian?
- **Concept**: Temperature scaling in softmax
  - Why needed here: Adjusts the sharpness of the topic distribution, balancing between uniform coverage and topic-specific focus.
  - Quick check question: How does increasing temperature affect the entropy of the resulting probability distribution?
- **Concept**: Negative sampling for word embeddings
  - Why needed here: Efficiently approximates the softmax normalization by contrasting target words with randomly sampled negatives.
  - Quick check question: In what way does negative sampling differ from standard softmax training in terms of computational complexity?

## Architecture Onboarding

- **Component map**: Document → Encoder → vMF sample → Temperature scaling → Topic distribution → Decoder → Reconstruction loss
- **Critical path**: Document → Encoder → vMF sample → Temperature scaling → Topic distribution → Decoder → Reconstruction loss
- **Design tradeoffs**:
  - Fixed embeddings vs. trainable: Fixed reduces parameters and stabilizes training but limits adaptation.
  - vMF vs. Gaussian: vMF improves clustering but may underperform in high-dimensional spaces.
  - Negative sampling vs. full softmax: Faster but introduces sampling variance.
- **Failure signatures**:
  - Low topic diversity: Temperature too low or negative sampling γ too small.
  - Poor classification accuracy: Seed keywords too sparse or unrelated; vMF dimension mismatch.
  - Unstable training: High variance in negative sampling or inappropriate κ initialization.
- **First 3 experiments**:
  1. Vary temperature (τtemp) from 5 to 25 on AG News and measure accuracy/diversity.
  2. Toggle negative sampling (γ=0 vs γ>0) and observe changes in topic coherence.
  3. Replace vMF with Gaussian prior and compare clustering and classification metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of S2vNTM vary with different levels of keyword sparsity or when keywords are completely absent?
- Basis in paper: [explicit] The paper mentions S2vNTM's ability to handle limited keywords and its performance with less common keywords, but does not explore the extreme case of no keywords.
- Why unresolved: The paper focuses on the benefits of incorporating seed keywords but does not test the model's robustness or performance in the absence of any keywords, which is a critical edge case for real-world applications.
- What evidence would resolve it: Experiments comparing S2vNTM's performance on datasets with varying levels of keyword sparsity, including a complete absence of keywords, would clarify its effectiveness in fully unsupervised scenarios.

### Open Question 2
- Question: What is the impact of using different word embedding techniques (e.g., contextual embeddings like BERT) on the performance of S2vNTM?
- Basis in paper: [explicit] The paper uses spherical word embeddings trained on the dataset, but does not explore the use of pre-trained contextual embeddings like BERT or other transformer-based models.
- Why unresolved: The choice of word embeddings can significantly affect the model's ability to capture semantic relationships, especially in domains with specialized terminology. The paper does not investigate how alternative embeddings might improve or hinder performance.
- What evidence would resolve it: Comparative experiments using different embedding techniques, such as BERT, GloVe, or domain-specific embeddings, would demonstrate the impact of embedding choice on S2vNTM's topic modeling and classification accuracy.

### Open Question 3
- Question: How does S2vNTM scale with very large datasets or high-dimensional topic spaces?
- Basis in paper: [inferred] The paper mentions that vMF distribution may have higher reconstruction loss in high dimensions and discusses the model's performance on datasets with varying numbers of topics, but does not address scalability to extremely large datasets or high-dimensional topic spaces.
- Why unresolved: While the paper demonstrates S2vNTM's effectiveness on moderate-sized datasets, it does not explore its scalability or computational efficiency when applied to massive datasets or when the number of topics is very large.
- What evidence would resolve it: Scalability tests involving large-scale datasets (e.g., millions of documents) and high-dimensional topic spaces would provide insights into S2vNTM's practical limitations and computational requirements.

## Limitations
- The experimental validation is limited to three datasets, with no ablation studies on individual components (vMF vs Gaussian, negative sampling alone, temperature effects)
- The negative sampling implementation details are underspecified, making it difficult to verify whether the reported improvements are due to this mechanism or other factors
- The temperature scaling mechanism lacks theoretical justification for why the specific function form is optimal

## Confidence
- **High confidence**: The basic architecture (vMF latent space + spherical embeddings) is well-specified and reproducible
- **Medium confidence**: Classification results are likely reliable, though the exact keyword sets used are not fully specified
- **Low confidence**: The relative contribution of negative sampling and temperature scaling to the reported improvements cannot be independently verified

## Next Checks
1. Implement ablation studies comparing vMF vs Gaussian priors, with and without negative sampling, and varying temperature parameters
2. Verify negative sampling implementation details by reproducing the sampling probability formula and number of samples used
3. Test the model on additional text classification datasets with different characteristics (document length, class balance, vocabulary size) to assess generalizability