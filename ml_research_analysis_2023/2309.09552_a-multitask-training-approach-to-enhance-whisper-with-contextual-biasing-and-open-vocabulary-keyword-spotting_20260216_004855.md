---
ver: rpa2
title: A Multitask Training Approach to Enhance Whisper with Contextual Biasing and
  Open-Vocabulary Keyword Spotting
arxiv_id: '2309.09552'
source_url: https://arxiv.org/abs/2309.09552
tags:
- words
- whisper
- entity
- speech
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KWS-Whisper, a novel ASR system that leverages
  the Whisper model and performs open-vocabulary keyword spotting (OV-KWS) on the
  hidden states of the Whisper encoder to recognize user-defined named entities. The
  system uses a multitask training approach to learn OV-KWS and contextual-ASR tasks.
---

# A Multitask Training Approach to Enhance Whisper with Contextual Biasing and Open-Vocabulary Keyword Spotting

## Quick Facts
- arXiv ID: 2309.09552
- Source URL: https://arxiv.org/abs/2309.09552
- Reference count: 23
- Key outcome: KWS-Whisper improves entity recall on Chinese Aishell and code-switching test sets by combining Whisper with open-vocabulary keyword spotting and contextual biasing

## Executive Summary
This paper introduces KWS-Whisper, a novel approach to enhance Whisper ASR systems with contextual biasing and open-vocabulary keyword spotting. The system leverages the Whisper encoder's hidden states to detect user-defined named entities through a multitask training approach that simultaneously learns keyword spotting and contextual ASR tasks. By incorporating predicted entities as prompts for the Whisper decoder, the method significantly improves recognition of rare named entities like personal names and technical terms, particularly in code-switching scenarios. The approach demonstrates substantial performance gains in entity recall while maintaining overall ASR accuracy.

## Method Summary
KWS-Whisper employs a multitask training approach that integrates open-vocabulary keyword spotting (OV-KWS) with contextual ASR. The method extracts hidden states from the Whisper encoder and computes cosine similarity matrices between these states and pre-computed features of entity words synthesized via TTS. A CNN classifier detects diagonal patterns in these similarity matrices to identify entity presence in utterances. The predicted entities are then incorporated as prompts into the Whisper decoder, guiding it to generate transcriptions that include these entities more accurately. The system is trained end-to-end to optimize both the KWS and contextual-ASR objectives simultaneously.

## Key Results
- Significant improvement in entity recall compared to original Whisper model on Chinese Aishell hot word subsets
- Enhanced performance on two internal code-switching test sets
- OV-KWS module functions as a plug-and-play enhancement for frozen Whisper models
- Maintains overall ASR accuracy while improving recognition of rare named entities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The KWS module improves entity recall by filtering entity words before Whisper decoder prompting
- Mechanism: Cosine similarity between hidden states of synthetic TTS speech and input utterance is computed, creating a similarity matrix that highlights diagonal patterns for matching entities. A CNN classifier detects these patterns and filters entities to include only relevant ones in the prompt
- Core assumption: Hidden state similarity between TTS-synthesized entity speech and actual utterance is preserved across different speakers and acoustic conditions
- Evidence anchors:
  - [abstract] "performs open-vocabulary keyword spotting (OV-KWS) on the hidden states of the Whisper encoder to recognize user-defined named entities"
  - [section] "The cosine similarity matrix is computed between the hidden representations of the input utterance and the stored features of each entity word"
  - [corpus] "Average neighbor FMR=0.313" - indicates moderate relatedness but no direct evidence for this specific mechanism
- Break condition: If TTS synthesis quality degrades or acoustic conditions differ significantly from training data, similarity patterns may not be reliably detected

### Mechanism 2
- Claim: Incorporating predicted entities as prompts improves ASR accuracy by providing contextual guidance to the decoder
- Mechanism: The Whisper decoder uses predicted entity words as part of a "spoken form prompt" that mimics conversational speech patterns, guiding the decoder to generate transcriptions that include these entities
- Core assumption: Whisper's decoder can effectively use external prompts to bias generation without catastrophic forgetting or hallucinations
- Evidence anchors:
  - [abstract] "These entities serve as prompts for the Whisper decoder"
  - [section] "The predicted entity words are used as prompts to guide the whisper decoder so that these words can be recognized more accurately"
  - [corpus] "Found 25 related papers" - suggests this is an active research area but doesn't confirm this specific mechanism
- Break condition: If prompts become too long or contain too many false positives, the decoder may produce hallucinations or fail to decode accurately

### Mechanism 3
- Claim: The multitask training approach enables simultaneous learning of OV-KWS and contextual-ASR tasks, improving overall system performance
- Mechanism: The system is trained to perform both keyword spotting and contextual biasing in parallel, allowing the KWS module to learn robust entity detection while the Whisper model maintains ASR capabilities
- Core assumption: Joint training of KWS and ASR tasks does not degrade performance of either task compared to separate training
- Evidence anchors:
  - [abstract] "The system uses a multitask training approach to learn OV-KWS and contextual-ASR tasks"
  - [section] "We evaluate our approach on Chinese Aishell hot word subsets and two internal code-switching test sets and show that it significantly improves the entity recall compared to the original Whisper model"
  - [corpus] "Lightweight Prompt Biasing for Contextualized End-to-End ASR Systems" - related work but doesn't confirm multitask effectiveness
- Break condition: If the training objectives conflict or the model capacity is insufficient, performance on one or both tasks may degrade

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Whisper is based on a Transformer sequence-to-sequence model, and understanding attention is crucial for comprehending how prompts influence decoding
  - Quick check question: How does the self-attention mechanism in the decoder allow it to incorporate prompt information during generation?

- Concept: Cosine similarity and feature matching
  - Why needed here: The KWS module relies on computing cosine similarity between hidden states to detect entity presence in utterances
  - Quick check question: Why is cosine similarity an appropriate metric for comparing hidden state representations in this context?

- Concept: Convolutional neural networks for pattern recognition
  - Why needed here: The CNN classifier detects diagonal patterns in the similarity matrix to determine if entities are present in the utterance
  - Quick check question: How does a CNN's ability to recognize local patterns make it suitable for detecting diagonal structures in similarity matrices?

## Architecture Onboarding

- Component map: Whisper encoder → KWS module (TTS-synthesized entity features + CNN classifier) → Whisper decoder with entity prompts
- Critical path: Audio input → Whisper encoder hidden states → KWS module → filtered entities → Whisper decoder with prompts → final transcription
- Design tradeoffs:
  - Using TTS-synthesized speech for entity features allows open-vocabulary operation but may introduce mismatch with real speech conditions
  - The CNN classifier adds computational overhead but improves entity detection accuracy
  - Incorporating prompts improves entity recall but may increase computational cost and hallucination risk
- Failure signatures:
  - Low entity recall despite high KWS precision suggests prompt construction issues
  - Increased hallucinations or repetitions when using prompts indicates decoder confusion
  - Poor KWS performance on new entity types suggests insufficient generalization of the CNN classifier
- First 3 experiments:
  1. Test KWS module performance on synthetic data with known entities to verify pattern detection
  2. Evaluate impact of prompt length and content on decoder performance using ground-truth entities
  3. Measure end-to-end performance improvement on a small, controlled dataset with diverse entity types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CB-Whisper vary across different Whisper model sizes (e.g., small, base, large) in terms of both entity recall and MER?
- Basis in paper: [inferred] The paper mentions that the Whisper-medium model was chosen due to its superior performance in code-switching situations compared to Whisper-large, but does not explore other model sizes
- Why unresolved: The paper does not provide a comparative analysis of CB-Whisper's performance across different Whisper model sizes, leaving the impact of model size on performance unclear
- What evidence would resolve it: Conducting experiments with CB-Whisper on Whisper models of varying sizes (small, base, large) and comparing their performance metrics (entity recall and MER) across different test sets would provide insights into the optimal model size for CB-Whisper

### Open Question 2
- Question: Can the TTS-based KWS module be further optimized to reduce computational cost without compromising its performance?
- Basis in paper: [explicit] The paper mentions that the TTS-based KWS module is computationally expensive due to the need to generate synthetic speech for each entity word and compute similarity matrices with the input utterance
- Why unresolved: The paper does not explore techniques to optimize the TTS-based KWS module's computational efficiency, such as using more efficient TTS models or approximate similarity computation methods
- What evidence would resolve it: Implementing and evaluating different optimization techniques for the TTS-based KWS module, such as using faster TTS models, dimensionality reduction for hidden states, or approximate nearest neighbor search for similarity computation, and comparing their performance and computational cost would provide insights into potential optimizations

### Open Question 3
- Question: How does the choice of entity words (e.g., frequency, length, language) affect the performance of CB-Whisper?
- Basis in paper: [inferred] The paper uses entity words from various sources, including technical terms and names, but does not analyze the impact of entity characteristics on CB-Whisper's performance
- Why unresolved: The paper does not provide an analysis of how different entity word characteristics, such as frequency, length, or language, influence the performance of CB-Whisper in terms of entity recall and MER
- What evidence would resolve it: Conducting experiments with CB-Whisper using entity word lists with varying characteristics (e.g., high vs. low frequency, short vs. long words, mono- vs. multilingual) and analyzing the performance metrics (entity recall and MER) would provide insights into the impact of entity word characteristics on CB-Whisper's performance

## Limitations

- The approach relies heavily on TTS synthesis quality, which may not match real speech conditions across diverse acoustic environments
- Demonstrated improvements are primarily on Chinese datasets with code-switching, raising questions about generalizability to other languages
- The multitask training assumes sufficient model capacity to learn both KWS and contextual-ASR tasks simultaneously without interference

## Confidence

**High Confidence Claims:**
- The KWS module can successfully detect entities in utterances using similarity matrices between hidden states of synthetic and real speech
- Incorporating entity words as prompts improves Whisper decoder performance for those specific entities
- The multitask training approach is feasible and can be implemented to train both KWS and contextual-ASR tasks

**Medium Confidence Claims:**
- The improvements in entity recall and mixed-error-rate are primarily due to the proposed multitask training approach rather than other factors
- The approach generalizes well to unseen entity words and different acoustic conditions beyond the tested datasets
- The KWS module's pattern detection using CNN classifiers is robust to variations in speech speed, accent, and background noise

**Low Confidence Claims:**
- The proposed method is universally applicable to all ASR tasks and languages without significant modifications
- The multitask training approach consistently outperforms separate training of KWS and contextual-ASR tasks across all scenarios
- The computational overhead of the KWS module and prompt generation is negligible compared to overall performance gains

## Next Checks

1. **Generalization Testing**: Evaluate the KWS module's performance on diverse datasets including different languages, accents, and acoustic conditions to assess robustness and generalizability beyond the Chinese Aishell and internal code-switching test sets.

2. **Ablation Study**: Conduct a thorough ablation study to isolate the contributions of the KWS module, prompt generation, and multitask training approach to the overall performance improvements, helping to identify the most critical components.

3. **Error Analysis**: Perform detailed error analysis on false positives and false negatives in entity recognition to understand the limitations of the similarity-based KWS approach and identify potential improvements or alternative methods for pattern detection.