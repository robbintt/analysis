---
ver: rpa2
title: 'Markovian Sliced Wasserstein Distances: Beyond Independent Projections'
arxiv_id: '2301.03749'
source_url: https://arxiv.org/abs/2301.03749
tags:
- wasserstein
- distribution
- distance
- sliced
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Markovian sliced Wasserstein distances (MSW),
  which address the limitation of independent projections in traditional sliced Wasserstein
  distances by imposing a first-order Markov structure on the projecting directions.
  The proposed method generates a sequence of projecting directions where each new
  direction depends on the previous one, using three types of transition distributions:
  random walk, orthogonal-based, and input-aware.'
---

# Markovian Sliced Wasserstein Distances: Beyond Independent Projections

## Quick Facts
- arXiv ID: 2301.03749
- Source URL: https://arxiv.org/abs/2301.03749
- Reference count: 40
- Introduces Markovian Sliced Wasserstein distances with Markovian dependency between projecting directions

## Executive Summary
This paper introduces Markovian sliced Wasserstein distances (MSW) that address the limitation of independent projections in traditional sliced Wasserstein distances by imposing a first-order Markov structure on projecting directions. The proposed method generates a sequence of projecting directions where each new direction depends on the previous one, using three types of transition distributions: random walk, orthogonal-based, and input-aware. The input-aware variant uses the gradient of the projected Wasserstein distance to create informative transitions. The method demonstrates improved performance in gradient flows and color transfer tasks while being more computationally efficient than existing sliced Wasserstein variants, and achieves competitive results in deep generative modeling on CIFAR10 and CelebA datasets.

## Method Summary
The Markovian Sliced Wasserstein distance introduces a first-order Markov chain to model dependencies between projecting directions, replacing the independent random projections used in traditional sliced Wasserstein distances. The method uses a prior distribution to generate the first projecting direction, then generates subsequent directions using one of three transition distributions: random walk (von Mises-Fisher), orthogonal-based, or input-aware (using gradient information). The final distance is computed as the average of projected Wasserstein distances over the sequence. The method also introduces burning and thinning techniques from MCMC methods to reduce computational complexity while maintaining performance. The approach preserves metricity, weak convergence, and sample complexity properties while potentially reducing computational overhead.

## Key Results
- MSW with input-aware transitions outperforms existing sliced Wasserstein variants in gradient flows and color transfer tasks
- Input-aware MSW variants achieve competitive Fréchet Inception Distance (FID) and Inception Score (IS) in deep generative modeling on CIFAR10 and CelebA
- Burning and thinning technique reduces computational complexity and memory usage while maintaining or improving performance
- Random walk transition (rMSW) provides a computationally efficient alternative that still outperforms standard SW

## Why This Works (Mechanism)

### Mechanism 1
The Markovian structure on projecting directions reduces redundancy and improves discrimination between probability measures compared to independent projections. By imposing a first-order Markov structure, each new direction depends on the previous one, creating a sequence of informative projections rather than independent random ones. This sequential dependency allows the method to adaptively explore directions that better discriminate between the two input measures. The transition distribution from one projecting direction to the next can be designed to capture information about the input measures and create more discriminative projections.

### Mechanism 2
The input-aware transition distribution uses gradient information to create more discriminative projecting directions. The input-aware transition uses the sub-gradient with respect to the previous projecting direction of the corresponding projected Wasserstein distance between the two measures to design the transition. This creates a new direction that moves toward maximizing the projected Wasserstein distance, effectively seeking the most discriminative projection. The gradient of the projected Wasserstein distance with respect to the projecting direction contains information about which directions are most discriminative between the two input measures.

### Mechanism 3
The burning and thinning technique reduces computational complexity while maintaining or improving performance. By discarding initial projections (burning) and subsampling later projections (thinning), the method reduces the number of projections needed while preserving the Markovian dependency structure. This reduces both computational time and memory usage. Early projections in the Markov chain may be less informative as they haven't yet converged to discriminative directions, and subsequent projections may be correlated enough that subsampling doesn't lose critical information.

## Foundational Learning

- Concept: First-order Markov chains
  - Why needed here: The core innovation relies on understanding how to model dependencies between sequential random variables, where each projecting direction depends only on the immediately preceding one.
  - Quick check question: In a first-order Markov chain, what is the conditional probability of the current state given the entire history of previous states?

- Concept: Von Mises-Fisher distribution
  - Why needed here: This distribution is used for the random walk transition, requiring understanding of directional statistics and distributions on hyperspheres.
  - Quick check question: How does the concentration parameter κ in the von Mises-Fisher distribution control the spread of probability mass around the mean direction?

- Concept: Wasserstein distance properties
  - Why needed here: The method builds upon Wasserstein distance theory, requiring understanding of metric properties, weak convergence, and sample complexity.
  - Quick check question: What is the sample complexity of Wasserstein distance in high dimensions, and why does it suffer from the curse of dimensionality?

## Architecture Onboarding

- Component map:
  Prior distribution (σ₁) -> Transition distributions (σₜ: random walk, orthogonal-based, input-aware) -> Monte Carlo estimation -> Burning and thinning -> Gradient computation

- Critical path:
  1. Sample initial direction from prior distribution
  2. Generate sequence of projecting directions using chosen transition distribution
  3. Compute projected Wasserstein distances for each direction
  4. Average results to obtain final distance
  5. (Optional) Apply burning and thinning to reduce computation

- Design tradeoffs:
  - Random walk vs. input-aware transitions: Random walk is simpler and faster but may be less discriminative; input-aware is more computationally intensive but potentially more informative
  - Number of time steps (T): Higher T may improve performance but increases computation linearly
  - Burning and thinning parameters: Balance between computational savings and information retention

- Failure signatures:
  - Poor performance on simple distributions where independent projections would suffice
  - Degenerate behavior if transition distribution concentrates too quickly or cycles
  - Computational inefficiency if input-aware transitions are used without sufficient hardware acceleration

- First 3 experiments:
  1. Compare MSW with random walk transition to standard SW on simple Gaussian distributions to verify no degradation in basic cases
  2. Test input-aware MSW on distributions with known optimal projecting directions to verify improved discrimination
  3. Evaluate burning and thinning parameters on a gradient flow task to find optimal balance between speed and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Markovian structure in MSW always lead to better computational efficiency compared to traditional SW methods?
- Basis in paper: The paper claims MSW can be more computationally efficient, but doesn't provide comprehensive comparisons across all scenarios.
- Why unresolved: The paper only provides limited computational comparisons in specific applications (gradient flows, color transfer) and doesn't explore the full parameter space.
- What evidence would resolve it: A systematic study comparing computational complexity across different datasets, dimensions, and parameter settings (L, T, κ) would clarify when MSW is more efficient.

### Open Question 2
- Question: What is the theoretical relationship between the concentration parameter κ in vMF transition and the quality of the generated projecting directions?
- Basis in paper: The paper mentions that κ controls the trade-off between heading to the "max" projecting direction and exploring the space, but doesn't provide theoretical analysis of this relationship.
- Why unresolved: The paper only provides empirical observations about κ's effect on performance, without theoretical justification for why certain values are optimal.
- What evidence would resolve it: A theoretical analysis linking κ to the convergence rate of the Markov chain and its impact on the quality of the projected distances would provide insight.

### Open Question 3
- Question: Can the burning and thinning technique be theoretically justified beyond its empirical success?
- Basis in paper: The paper introduces burning and thinning as practical techniques to reduce complexity, but doesn't provide theoretical guarantees about their impact on the distance properties.
- Why unresolved: While the paper claims burning and thinning preserves metricity under certain conditions, it doesn't analyze how these techniques affect the statistical properties (sample complexity, Monte Carlo error) of MSW.
- What evidence would resolve it: A theoretical analysis showing how burning and thinning affect the convergence rate and statistical guarantees of MSW would strengthen the justification for these techniques.

### Open Question 4
- Question: Is there an optimal way to design the prior distribution σ₁(θ₁) for MSW beyond the uniform distribution used in experiments?
- Basis in paper: The paper mentions that σ₁(θ₁) could be any distribution on the unit hypersphere, including von Mises-Fisher or estimated distributions, but doesn't explore this systematically.
- Why unresolved: The paper only uses the uniform distribution in experiments and doesn't investigate whether problem-specific priors could improve performance.
- What evidence would resolve it: Comparative experiments using different prior distributions (e.g., von Mises-Fisher, learned priors) across various applications would reveal whether problem-specific priors offer advantages.

### Open Question 5
- Question: What is the theoretical limit of MSW in terms of dimensionality? Does it suffer from curse of dimensionality?
- Basis in paper: The paper claims MSW doesn't suffer from curse of dimensionality based on sample complexity results, but doesn't explore high-dimensional settings.
- Why unresolved: The experiments are limited to relatively low-dimensional image datasets (CIFAR10, CelebA), and the theoretical analysis doesn't extend to very high dimensions.
- What evidence would resolve it: Experiments on high-dimensional datasets (d > 100) and theoretical analysis of the concentration of measure phenomenon in high dimensions would clarify MSW's limitations.

## Limitations
- Computational overhead of maintaining and sampling from Markov chain of projecting directions, especially for input-aware variant
- Theoretical guarantees don't fully characterize convergence rate or provide finite-sample error bounds for Markovian structure
- Choice of transition distribution lacks principled guidance, requiring extensive hyperparameter tuning
- Limited exploration of high-dimensional settings to verify claims about mitigating curse of dimensionality

## Confidence
- **High Confidence**: Theoretical framework for metricity preservation and basic algorithmic implementation; computational experiments showing improved performance in gradient flows and color transfer
- **Medium Confidence**: Deep generative modeling results due to incomplete neural network architecture details
- **Low Confidence**: Theoretical convergence properties of input-aware transition, particularly regarding gradient quality dependence

## Next Checks
1. Implement ablation studies comparing the three MSW variants (rMSW, oMSW, iMSW) across multiple tasks to determine which transition distribution performs best under different data characteristics and dimensionalities
2. Conduct sensitivity analysis on the Markov chain parameters (T, burning, thinning) to establish guidelines for optimal parameter selection and verify that computational savings from burning/thinning don't compromise accuracy
3. Test MSW performance on high-dimensional datasets (d > 100) to empirically evaluate the claim that MSW mitigates the curse of dimensionality better than standard SW, particularly for the input-aware variant