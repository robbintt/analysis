---
ver: rpa2
title: Learning to Rank in Generative Retrieval
arxiv_id: '2306.15222'
source_url: https://arxiv.org/abs/2306.15222
tags:
- retrieval
- rank
- generative
- loss
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a learning-to-rank framework for generative
  retrieval, named LTRGR, to address the gap between the learning objective of autoregressive
  models and the desired passage ranking target. LTRGR enables generative retrieval
  to directly learn to rank passages by optimizing the autoregressive model towards
  the final passage ranking target using a rank loss.
---

# Learning to Rank in Generative Retrieval

## Quick Facts
- arXiv ID: 2306.15222
- Source URL: https://arxiv.org/abs/2306.15222
- Reference count: 14
- Key outcome: LTRGR achieves state-of-the-art performance among generative retrieval methods by optimizing autoregressive models toward passage ranking targets using rank loss.

## Executive Summary
This paper introduces LTRGR, a learning-to-rank framework that addresses the gap between autoregressive model training objectives and desired passage ranking targets in generative retrieval. The framework continues training existing generative retrieval models with a rank loss that directly optimizes passage ranking while preserving identifier generation capability through multi-task learning. LTRGR requires only an additional training step without adding inference burden, making it a practical enhancement to current generative retrieval systems.

## Method Summary
LTRGR extends generative retrieval by continuing training with a rank loss alongside the generation loss. The method uses a margin-based ranking objective that compares scores of positive and negative passages sampled from the top retrieved passages. During the learning-to-rank phase, the model optimizes both identifier generation (to maintain retrieval capability) and passage ranking (to improve relevance ordering) through a combined loss function. The framework is compatible with various generative retrieval models and only requires training data consisting of queries and passages.

## Key Results
- LTRGR achieves state-of-the-art performance among generative retrieval methods on NQ, TriviaQA, and MSMARCO datasets
- The method demonstrates consistent improvements across hits@5, hits@20, and hits@100 metrics
- Experimental results show that both the generation loss and rank loss are essential components for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
The rank loss directly optimizes the autoregressive model toward the final passage ranking target. By continuing training with a margin-based rank loss that compares scores of positive vs. negative passages, the model learns to assign higher scores to relevant passages. This bridges the gap between generating identifiers and ranking passages.

### Mechanism 2
The generation loss preserves the ability to generate valid identifiers while learning to rank. During the learning-to-rank phase, the generation loss is combined with the rank loss in a multi-task format, ensuring the autoregressive model maintains its identifier generation capability, which is essential for passage retrieval.

### Mechanism 3
The margin-based rank loss effectively differentiates between positive and negative passages. The rank loss uses a margin to ensure positive passages are scored higher than negative passages, with two variants using different sampling strategies to provide diverse supervision.

## Foundational Learning

- **Concept: Autoregressive language modeling**
  - Why needed here: The core generative retrieval model is an autoregressive model that generates identifier strings token by token
  - Quick check question: How does teacher forcing during training differ from autoregressive generation at inference?

- **Concept: Learning to rank (LTR) paradigm**
  - Why needed here: The paper leverages classical LTR techniques to optimize generative retrieval toward passage ranking rather than just identifier generation
  - Quick check question: What is the difference between point-wise, pair-wise, and list-wise LTR approaches?

- **Concept: Margin-based ranking loss**
  - Why needed here: The rank loss uses a margin to ensure positive passages are scored higher than negative passages
  - Quick check question: How does the margin value affect the optimization of the ranking loss?

## Architecture Onboarding

- **Component map:** Autoregressive model (AM) -> FM-index -> Heuristic function -> Rank loss module -> Multi-task loss combiner
- **Critical path:** Query input → Autoregressive model → Identifier generation → Identifiers + FM-index → Constrained generation → Generated identifiers → Heuristic function → Passage scores → Ranked list output
- **Design tradeoffs:** Training time vs. performance (learning-to-rank adds training time but significantly improves retrieval performance); Model complexity vs. inference speed (no additional burden on inference, maintaining fast generation); Generation quality vs. ranking quality (multi-task loss balances these potentially competing objectives)
- **Failure signatures:** Low recall (model fails to generate correct identifiers or heuristic mapping is ineffective); High recall but poor ranking (rank loss not properly optimizing passage scores); Slow inference (unexpected computational overhead)
- **First 3 experiments:** 1) Ablation study removing the rank loss to verify its contribution; 2) Hyperparameter sweep on margin values and generation loss weight λ; 3) Comparison of margin-based vs. list-wise rank loss variants

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the proposed LTRGR framework be generalized to other generative retrieval models beyond MINDER and SEAL? The paper mentions this natural question but only demonstrates effectiveness on two specific models.

- **Open Question 2:** What is the impact of different list-wise rank loss functions on the performance of LTRGR? The paper explores only one list-wise loss function (infoNCE) and does not investigate other potential list-wise losses.

- **Open Question 3:** How does the proposed learning-to-rank approach affect the exposure bias problem in generative retrieval? The paper mentions exposure bias as an issue but does not explicitly analyze the impact of learning-to-rank on this problem.

## Limitations

- The effectiveness heavily depends on the quality of the heuristic function mapping identifiers to passage rankings, which is treated as a black box
- The multi-task learning setup requires careful tuning of the weight balancing generation and rank loss contributions
- The theoretical justification for why margin-based ranking losses work well for this specific problem is underdeveloped

## Confidence

**Claim Cluster 1: Learning-to-rank improves generative retrieval performance** - Confidence: High
- Strong experimental evidence across three datasets
- Clear ablation showing rank loss contribution
- Consistent improvements across multiple metrics

**Claim Cluster 2: The approach is efficient and doesn't burden inference** - Confidence: High
- Explicitly stated that only training step is modified
- No additional parameters or computation added to inference
- Supported by experimental design and results

**Claim Cluster 3: Margin-based rank loss effectively learns to rank** - Confidence: Medium
- Empirical success demonstrated
- Limited theoretical justification
- No comparison to alternative ranking objectives

## Next Checks

1. **Heuristic Function Robustness Test:** Systematically evaluate the sensitivity of LTRGR performance to heuristic function quality by introducing controlled noise or using alternative heuristics to quantify dependency on perfect heuristic.

2. **Hyperparameter Sensitivity Analysis:** Conduct comprehensive grid search over rank loss margin (m) and generation loss weight (λ) parameters, plotting performance surfaces to identify stable performance regions.

3. **Alternative Ranking Loss Comparison:** Implement and compare against list-wise ranking losses (e.g., ListNet, ListMLE) to determine whether margin-based approach is optimal or if more sophisticated ranking objectives could yield further improvements.