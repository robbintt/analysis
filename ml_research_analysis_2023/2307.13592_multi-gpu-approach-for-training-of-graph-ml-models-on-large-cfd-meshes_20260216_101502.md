---
ver: rpa2
title: Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes
arxiv_id: '2307.13592'
source_url: https://arxiv.org/abs/2307.13592
tags:
- flow
- step
- training
- error
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-GPU approach for training graph neural
  network models on large CFD meshes, enabling the use of machine learning surrogates
  for accelerating CFD simulations. The key idea is to partition the mesh across multiple
  GPUs and perform halo exchanges between partitions during training to ensure information
  flow.
---

# Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes

## Quick Facts
- arXiv ID: 2307.13592
- Source URL: https://arxiv.org/abs/2307.13592
- Reference count: 33
- Primary result: Multi-GPU training with halo exchanges enables graph neural network models to be trained on large CFD meshes with up to one million cells, achieving comparable performance to single GPU training on small datasets.

## Executive Summary
This paper presents a multi-GPU approach for training graph neural network models on large CFD meshes, enabling the use of machine learning surrogates for accelerating CFD simulations. The key idea is to partition the mesh across multiple GPUs and perform halo exchanges between partitions during training to ensure information flow. The approach is applied to the MeshGraphNets model, training it on meshes with up to one million cells. Experiments on a cylinder flow dataset show the approach produces comparable results to single GPU training. On a large turbine dataset, the proposed method is outperformed by traditional distributed training without halo exchanges. Analysis reveals the model fails to capture temporal variability in the flow, likely due to vanishing gradients during synchronization. The paper outlines potential improvements including halo exchanges for edge features, multiscale architectures, and physics-based loss functions.

## Method Summary
The method involves partitioning large CFD meshes across multiple GPUs and using halo exchanges to ensure information flow between partitions during training. The MeshGraphNets architecture is employed, consisting of an encoder, processor blocks (message passing), and decoder. The approach uses PyTorch's Distributed Data Parallel framework for synchronized gradient updates across GPUs. Gradient accumulation is used to handle memory constraints when processing large batches. The halo exchange mechanism involves sharing boundary nodes between partitions at each message passing step to maintain spatial coherence. The method is tested on both small cylinder flow datasets and larger turbine datasets, comparing performance against single GPU and traditional distributed training approaches.

## Key Results
- Multi-GPU training with halo exchanges achieves comparable performance to single GPU training on small cylinder flow datasets (RMSE around 0.03-0.04 for various rollout lengths)
- On large turbine datasets, traditional distributed training without halo exchanges outperforms the proposed approach
- The model fails to capture temporal variability in the flow field, likely due to vanishing gradients during synchronization
- Increasing message passing steps and implementing halo exchanges for edge features are identified as potential improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Halo exchanges enable information propagation across GPU partitions, maintaining spatial coherence of the graph neural network predictions.
- Mechanism: By exchanging boundary node features between neighboring partitions at each message-passing step, the model ensures that nodes at partition edges have access to information from adjacent regions, preventing discontinuities in the learned flow field.
- Core assumption: The spatial relationships between nodes are preserved through halo exchanges, and that local message passing can capture long-range dependencies when augmented with halo exchanges.
- Evidence anchors:
  - [abstract] "The key idea is to partition the mesh across multiple GPUs and perform halo exchanges between partitions during training to ensure information flow."
  - [section] "In order to ensure information flow across the whole domain, each partition additionally shares all nodes that are directly connected to other partitions. These cells are typically called halo cells."

### Mechanism 2
- Claim: Distributed data parallel training with synchronized gradients maintains model consistency across GPU replicas.
- Mechanism: By using PyTorch's Distributed Data Parallel framework, gradients are synchronized across all GPUs after each backward pass, ensuring that all replicas update their parameters in a coordinated manner.
- Core assumption: Synchronized gradient updates lead to convergence of all model replicas to a consistent solution.
- Evidence anchors:
  - [section] "The parameters of ML optimizer and model are kept separate but use synchronized gradients utilizing the distributed data parallel (DDP) framework of PyTorch."
  - [section] "The optimizers are expected to be synchronized as well."

### Mechanism 3
- Claim: Gradient accumulation allows training on larger datasets when batch size is limited by GPU memory constraints.
- Mechanism: By accumulating gradients over multiple forward passes before performing an optimizer step, the effective batch size can be increased without exceeding GPU memory limits.
- Core assumption: Accumulated gradients approximate true gradients computed from a larger batch size.
- Evidence anchors:
  - [section] "So only a single sample per batch can be processed and the gradients are accumulated over two consecutive samples."
  - [section] "In order to have a fair comparison and explore the impacts of this batching strategy on the problem at hand, a single GPU version with gradient accumulation called MGN-gradAcc was trained additionally."

## Foundational Learning

- Concept: Graph Neural Networks
  - Why needed here: Mesh-based CFD simulations can be naturally represented as graphs where nodes correspond to mesh points and edges represent connectivity.
  - Quick check question: How does a graph neural network differ from a traditional convolutional neural network in handling irregular domains?

- Concept: Message Passing
  - Why needed here: Message passing allows information to propagate through the graph structure, enabling the model to capture local and potentially long-range dependencies in the flow field.
  - Quick check question: What is the role of the number of message passing steps in a graph neural network?

- Concept: Distributed Training
  - Why needed here: Large CFD meshes with millions of cells exceed the memory capacity of a single GPU, necessitating distribution across multiple devices.
  - Quick check question: What are the trade-offs between data parallelism and model parallelism in distributed training?

## Architecture Onboarding

- Component map: Encoder -> Processor (message passing blocks) -> Decoder
- Critical path:
  1. Data partitioning and halo identification
  2. Forward pass with halo exchanges
  3. Loss computation and gradient synchronization
  4. Backward pass with halo exchanges
  5. Parameter update

- Design tradeoffs:
  - Memory vs. communication: Larger halo sizes improve information flow but increase communication overhead
  - Message passing steps vs. accuracy: More steps enable longer-range information propagation but increase computational cost
  - Model size vs. scalability: Larger models may capture more complex patterns but are harder to distribute

- Failure signatures:
  - High communication overhead relative to computation
  - Poor generalization on test sets despite good training performance
  - Vanishing or exploding gradients during training
  - Discontinuities or artifacts in predicted flow fields

- First 3 experiments:
  1. Validate single-GPU performance on small meshes to establish baseline accuracy
  2. Test halo exchange implementation on small multi-GPU setup with synthetic data
  3. Compare distributed training with and without halo exchanges on a medium-sized mesh

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific edge feature exchange mechanism would enable the halo approach to capture temporal variability in the flow field?
- Basis in paper: [explicit] The authors identify that the current halo exchange only transfers node features, not edge features, and propose extending the approach to exchange edge features as a key improvement.
- Why unresolved: The paper does not implement or test edge feature exchange, only proposing it as a future direction. The specific mechanism for how edge features should be exchanged between partitions is not detailed.
- What evidence would resolve it: Experimental results showing that extending the halo exchange to include edge features enables the model to capture temporal variability in the flow field, as measured by metrics like the temporal deviation.

### Open Question 2
- Question: Would increasing the number of message passing steps beyond the current limit enable the model to capture long-range dependencies in the flow field?
- Basis in paper: [explicit] The authors state that memory and resource constraints forced a lower number of message passing steps than in the original implementation, and suggest that this may have contributed to the model's inability to capture temporal variability.
- Why unresolved: The paper does not test the impact of increasing the number of message passing steps due to hardware limitations. The relationship between message passing steps and model accuracy is not empirically established.
- What evidence would resolve it: Experiments varying the number of message passing steps and measuring the impact on model accuracy, particularly the ability to capture temporal variability. Results showing that increasing message passing steps improves temporal prediction accuracy would support this hypothesis.

### Open Question 3
- Question: How would multiscale graph neural network architectures improve the model's ability to capture flow features at different scales?
- Basis in paper: [explicit] The authors propose exploring multiscale graph neural network approaches as a potential solution to the model's inability to capture temporal variability, citing recent advances in this area.
- Why unresolved: The paper does not implement or test multiscale architectures, only proposing them as a future direction. The specific benefits and mechanisms of multiscale architectures for this problem are not explored.
- What evidence would resolve it: Experiments comparing the performance of multiscale graph neural network architectures to the current single-scale approach on the turbine dataset, measuring metrics like temporal prediction accuracy and long-range dependency capture. Results demonstrating improved performance with multiscale architectures would support their adoption.

## Limitations

- The halo exchange mechanism introduces communication overhead that may not always justify accuracy benefits, particularly for larger datasets
- The model's inability to capture temporal variability in the flow suggests fundamental limitations in the current architecture
- Performance on very large industrial-scale meshes remains unproven, as experiments were limited to meshes with up to one million cells

## Confidence

Medium. The methodology is well-grounded in established techniques (graph neural networks, distributed training), and the experimental setup is clearly described. However, the comparison between different training approaches is limited to specific datasets and model configurations, and the failure analysis relies on qualitative observations rather than systematic ablation studies.

## Next Checks

1. **Ablation study on halo exchange frequency and size**: Systematically vary the halo exchange frequency and layer size to quantify the trade-off between communication overhead and accuracy improvement.

2. **Comparison with alternative distributed strategies**: Test other distributed training approaches (e.g., model parallelism, different partitioning schemes) on the turbine dataset to determine if halo exchanges are inherently suboptimal or if implementation details are responsible for the performance gap.

3. **Temporal learning analysis**: Implement gradient clipping, learning rate scheduling, or alternative architectures (e.g., recurrent connections) to test whether the temporal learning limitations can be overcome without sacrificing spatial accuracy.