---
ver: rpa2
title: 'Exploring the Relationship between LLM Hallucinations and Prompt Linguistic
  Nuances: Readability, Formality, and Concreteness'
arxiv_id: '2309.11064'
source_url: https://arxiv.org/abs/2309.11064
tags:
- hallucination
- llms
- prompts
- language
- readability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how linguistic factors in prompts influence
  hallucinations in Large Language Models (LLMs). It categorizes four types of hallucinations:
  Person, Location, Number, and Acronym.'
---

# Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness

## Quick Facts
- arXiv ID: 2309.11064
- Source URL: https://arxiv.org/abs/2309.11064
- Reference count: 10
- Primary result: More formal and concrete prompts reduce hallucinations, particularly for Person/Location (formality) and Number/Acronym (concreteness) categories.

## Executive Summary
This paper investigates how linguistic properties of prompts—specifically readability, formality, and concreteness—affect hallucination rates in Large Language Models (LLMs). Using 2,500 New York Times tweets as prompts and 15 different LLMs, the study systematically examines hallucination patterns across four categories: Person, Location, Number, and Acronym. The research finds that increased formality and concreteness in prompts generally reduce hallucination rates, with advanced models like GPT-4 showing stronger sensitivity to these linguistic nuances. Readability effects prove more variable and model-dependent.

## Method Summary
The study employs a dataset of 2,500 NYT tweets as prompts, which are annotated for four hallucination categories (Person, Location, Number, Acronym) by human annotators. Linguistic properties are computed using established metrics: Flesch Reading Ease for readability, a formal-informal word formula for formality, and concreteness ratings from the Brysbaert et al. (2014) dataset. The prompts are then processed through 15 LLMs without fine-tuning, and hallucination frequencies are measured across different linguistic score ranges (Low/Mid/High) for each category.

## Key Results
- Formal prompts significantly reduce hallucinations in Person and Location categories
- Concrete prompts effectively reduce hallucinations in Number and Acronym categories
- Readability effects show mixed patterns across different models and hallucination types
- Advanced LLMs like GPT-4 demonstrate stronger sensitivity to linguistic nuances in prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: More formal prompts reduce hallucinations in LLMs.
- Mechanism: Formal language cues align the model's generation patterns with structured, factual outputs by implicitly constraining the model's output space.
- Core assumption: LLMs are sensitive to stylistic shifts in input that indicate higher expected precision or authority.
- Evidence anchors:
  - [abstract]: "prompts characterized by greater formality and concreteness tend to result in reduced hallucination"
  - [section]: "Our findings demonstrate how utilizing more formal prompts can address hallucinations in the Name and Location categories."
  - [corpus]: Weak; no neighbor paper directly addresses formality effects on hallucinations.
- Break Condition: If the model interprets formality as stylistic flourish rather than constraint, hallucinations may persist or shift to different categories.

### Mechanism 2
- Claim: Increased concreteness in prompts reduces hallucinations, especially for numeric and acronym categories.
- Mechanism: Concrete terms provide clear semantic anchors, reducing the model's need to interpolate or fabricate information.
- Core assumption: LLMs generate more reliably when semantic content is grounded in tangible, verifiable concepts.
- Evidence anchors:
  - [abstract]: "prompts characterized by greater formality and concreteness tend to result in reduced hallucination"
  - [section]: "incorporating more specific and concrete terms into the prompts effectively reduces hallucinations in the Number and Acronym categories."
  - [corpus]: No direct corpus support; most neighbor studies focus on paraphrasing or tone rather than concreteness.
- Break Condition: If concreteness is misinterpreted as verbosity, model performance may degrade.

### Mechanism 3
- Claim: Readability effects on hallucination are mixed and depend on model architecture.
- Mechanism: Lower readability (complex vocabulary) may confuse some models, but overly simplified prompts can reduce precision, leading to variable hallucination rates.
- Core assumption: Model architecture and training data influence how prompts of varying readability are processed.
- Evidence anchors:
  - [abstract]: "outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern."
  - [section]: "Some difficult-to-read prompts, but more formal also hallucinate less. Hence, the results regarding readability are somewhat uncertain, displaying a combination of findings."
  - [corpus]: No neighbor paper directly supports readability findings.
- Break Condition: If prompts are too simple, they may fail to trigger the model's factual reasoning, leading to hallucinations.

## Foundational Learning

- Concept: Hallucination categories (Person, Location, Number, Acronym)
  - Why needed here: The paper explicitly studies how linguistic nuances affect these four types of hallucination.
  - Quick check question: Which hallucination type would be most affected by prompts using highly concrete numeric values?

- Concept: Linguistic metrics (Readability, Formality, Concreteness)
  - Why needed here: These are the three prompt properties being manipulated to test hallucination reduction.
  - Quick check question: How is formality calculated in the paper?

- Concept: Dataset annotation and labeling
  - Why needed here: The study relies on human-annotated labels from AMT to identify hallucinations in the NYT tweet-derived dataset.
  - Quick check question: At what granularity (sentence vs. tweet) were hallucinations annotated?

## Architecture Onboarding

- Component map:
  Prompt generator → Linguistic scorer → LLM → Hallucination detector → Annotation pipeline
  Hallucination detector relies on AMT-labeled dataset

- Critical path:
  1. Generate prompt variants (readability, formality, concreteness)
  2. Run through LLMs
  3. Detect and classify hallucinations
  4. Aggregate results by prompt type

- Design tradeoffs:
  - Formal vs. informal: Precision vs. naturalness
  - Concrete vs. abstract: Grounding vs. flexibility
  - Readability: Clarity vs. complexity

- Failure signatures:
  - Uniform hallucination rates across prompt types → no linguistic effect
  - Increased hallucinations in specific categories after modification → over-constraining

- First 3 experiments:
  1. Test formality effects using NYT prompts reformulated to be more formal vs. informal.
  2. Vary concreteness levels by substituting abstract terms with concrete counterparts.
  3. Generate readability variants by adjusting sentence complexity and measure hallucination variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do linguistic nuances in prompts affect the performance of different LLM architectures in terms of hallucination frequency?
- Basis in paper: [explicit] The paper mentions that "The linguistic impacts of the prompts become more evident in LLMs such as GPT-4, OPT, and subsequent versions" and "the impact of concrete prompts becomes increasingly apparent in advanced LLMs like GPT-4, OPT, and their later iterations."
- Why unresolved: While the paper suggests that advanced LLMs like GPT-4 and OPT are more sensitive to linguistic nuances, it does not provide a detailed comparative analysis of how different LLM architectures respond to prompts of varying linguistic readability, formality, and concreteness.
- What evidence would resolve it: A comprehensive comparative study analyzing the performance of various LLM architectures (e.g., GPT-3, GPT-4, OPT) on prompts with different levels of readability, formality, and concreteness, and their corresponding hallucination frequencies.

### Open Question 2
- Question: Are there specific categories of hallucination that are more prevalent in responses prompted with formal versus informal language?
- Basis in paper: [explicit] The paper states that "Our findings demonstrate how utilizing more formal prompts can address hallucinations in the Name and Location categories."
- Why unresolved: The paper identifies that formal language prompts tend to reduce hallucinations in certain categories (Person and Location), but it does not provide a detailed breakdown of how different categories of hallucination are affected by the level of formality in prompts.
- What evidence would resolve it: An in-depth analysis of the prevalence of different categories of hallucination (Person, Location, Number, Acronym) in responses to prompts with varying levels of formality.

### Open Question 3
- Question: How does the level of linguistic concreteness in a prompt impact the probability of hallucination in LLMs, and are there specific categories of hallucination that are more affected by concreteness?
- Basis in paper: [explicit] The paper mentions that "Our results show that incorporating more specific and concrete terms into the prompts effectively reduces hallucinations in the Number and Acronym categories."
- Why unresolved: While the paper indicates that concreteness in prompts can reduce hallucinations in certain categories (Number and Acronym), it does not provide a comprehensive analysis of how different levels of concreteness affect the probability of hallucination across all categories.
- What evidence would resolve it: A detailed study examining the relationship between the level of linguistic concreteness in prompts and the probability of hallucination across all categories (Person, Location, Number, Acronym) in LLMs.

## Limitations

- Relies on human-annotated labels from AMT without detailed quality control measures
- Concreteness scoring methodology lacks specificity
- Readability effects show mixed patterns that may be confounded by model-specific behaviors
- Uses a fixed dataset of NYT tweets, limiting generalizability

## Confidence

- **High confidence**: Formality reduces hallucinations in Person and Location categories
- **High confidence**: Concreteness reduces hallucinations in Number and Acronym categories  
- **Medium confidence**: Mixed readability effects with model-dependent outcomes
- **Low confidence**: Exact mechanism by which formality constrains output space

## Next Checks

1. Conduct inter-annotator agreement analysis on a subset of annotated sentences to quantify labeling consistency
2. Test concreteness scoring on a held-out validation set to verify scoring implementation accuracy
3. Run prompts through a subset of LLMs with identical linguistic modifications to confirm reproducibility of formality effects