---
ver: rpa2
title: 'AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators'
arxiv_id: '2303.16854'
source_url: https://arxiv.org/abs/2303.16854
tags:
- answer
- data
- query
- elder
- scrolls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AnnoLLM, a system that leverages large language
  models (LLMs) such as GPT-3.5 to perform data annotation tasks traditionally done
  by human annotators. The proposed two-step "explain-then-annotate" approach uses
  LLMs to generate rationales for demonstrated examples, then constructs few-shot
  chain-of-thought prompts to annotate unlabeled data.
---

# AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators

## Quick Facts
- arXiv ID: 2303.16854
- Source URL: https://arxiv.org/abs/2303.16854
- Authors: 
- Reference count: 13
- Primary result: AnnoLLM achieves comparable or superior performance to crowdsourced annotators on three NLP tasks using a two-step "explain-then-annotate" approach

## Executive Summary
This paper introduces AnnoLLM, a system that leverages large language models (LLMs) such as GPT-3.5 to perform data annotation tasks traditionally done by human annotators. The proposed two-step "explain-then-annotate" approach uses LLMs to generate rationales for demonstrated examples, then constructs few-shot chain-of-thought prompts to annotate unlabeled data. Experiments on three tasks—user query and keyword relevance assessment, BoolQ, and WiC—show that AnnoLLM achieves comparable or superior performance to crowdsourced annotators, demonstrating the feasibility of using LLMs for data annotation in various NLP tasks.

## Method Summary
AnnoLLM employs a two-step "explain-then-annotate" approach where LLMs first generate explanations for demonstrated examples, then use these explanations to construct few-shot chain-of-thought prompts for annotating unlabeled data. The system takes task descriptions and demonstrated examples as input, generates self-explanations for the demonstrated examples, constructs few-shot CoT prompts with these explanations, and uses them to annotate new data. The method differs from traditional few-shot prompting by incorporating self-generated explanations that capture the reasoning behind correct labels.

## Key Results
- AnnoLLM achieves comparable or superior performance to crowdsourced annotators on three different NLP tasks
- The explain-then-annotate approach outperforms both zero-shot and standard few-shot prompting methods
- Performance is highly dependent on the quality of generated explanations, with ablation studies showing significant impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can function as effective crowdsourced annotators when provided with sufficient guidance and demonstrated examples
- Mechanism: The "explain-then-annotate" approach leverages LLMs to generate rationales for demonstrated examples, then constructs few-shot chain-of-thought prompts to annotate unlabeled data
- Core assumption: LLMs can generate high-quality explanations for demonstrated examples that capture the reasoning behind correct labels
- Evidence anchors:
  - [abstract] "we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate"
  - [section] "we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example"
  - [corpus] Weak - only 5 related papers found, none directly addressing LLM-based annotation systems
- Break condition: If LLMs fail to generate consistent or accurate explanations for demonstrated examples, the quality of the constructed few-shot prompts would degrade, leading to poor annotation performance

### Mechanism 2
- Claim: Few-shot chain-of-thought prompts constructed with self-generated explanations improve annotation quality
- Mechanism: Explanations generated by LLMs are appended to demonstrated examples to create prompts that elicit better reasoning and performance
- Core assumption: Explanations that include the correct answer followed by reasoning are more effective than explanations that only contain reasoning
- Evidence anchors:
  - [abstract] "we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data"
  - [section] "we found that GPT-3.5 is a good reasoner who can automatically generate reasonable explanations for demonstrated examples"
  - [corpus] Weak - no direct evidence about the effectiveness of self-generated vs human-written explanations
- Break condition: If the self-generated explanations are inconsistent or incorrect, the few-shot prompts would contain misleading information that could confuse the model

### Mechanism 3
- Claim: The explain-then-annotate approach is effective across different task types beyond traditional reasoning tasks
- Mechanism: The approach works for binary classification tasks like user query-keyword relevance assessment, BoolQ, and WiC
- Core assumption: The reasoning capabilities demonstrated in chain-of-thought prompting can generalize to other task types
- Evidence anchors:
  - [abstract] "We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC"
  - [section] "Previous studies have shown that the few-shot CoT, constructed using human-written explanations, can enhance the model's reasoning ability on reasoning tasks. However, our approach differs from previous methods in that we utilize explanations generated by the large model itself"
  - [corpus] Weak - only 5 related papers found, none specifically testing cross-task generalization of CoT methods
- Break condition: If the approach fails to generalize beyond the tested tasks, it would suggest limitations in the method's applicability

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: This technique is central to the "explain-then-annotate" approach, where explanations guide the model's reasoning process
  - Quick check question: What is the difference between zero-shot, few-shot, and chain-of-thought prompting?

- Concept: Few-shot learning
  - Why needed here: The system relies on demonstrating examples to teach the LLM how to annotate, rather than requiring extensive training
  - Quick check question: How does few-shot learning differ from traditional supervised learning in terms of data requirements?

- Concept: Crowdsourcing annotation processes
  - Why needed here: Understanding how human annotators are guided helps design effective prompts for LLMs
  - Quick check question: What are the three main components typically provided to human annotators when labeling data?

## Architecture Onboarding

- Component map: Task description and category definitions -> Demonstrated examples with ground truth labels -> LLM explanation generator (e.g., ChatGPT) -> Few-shot CoT prompt constructor -> Annotation executor (GPT-3.5)

- Critical path:
  1. Provide task description and category definitions to LLM
  2. Generate explanations for each demonstrated example
  3. Construct few-shot CoT prompts with explanations
  4. Use prompts to annotate unlabeled data

- Design tradeoffs:
  - Using self-generated explanations vs. human-written explanations (potentially lower quality but more scalable)
  - Including vs. excluding ground truth labels in explanation prompts (affects consistency)
  - Template flexibility vs. performance stability (few-shot methods are sensitive to templates)

- Failure signatures:
  - Inconsistent explanations across multiple generations for the same example
  - Performance degradation when switching between different prompt templates
  - Large performance gaps between few-shot and few-shot CoT approaches

- First 3 experiments:
  1. Compare zero-shot, few-shot, and few-shot CoT performance on a small dataset
  2. Test the effect of including vs. excluding ground truth labels in explanation prompts
  3. Measure consistency of explanations generated across multiple runs for the same example

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AnnoLLM vary when using different LLM models (e.g., GPT-3.5 vs. GPT-4) for generating explanations?
- Basis in paper: [explicit] The paper focuses on using GPT-3.5 series models, but does not compare their performance with other LLM models.
- Why unresolved: The paper does not provide a comparison of AnnoLLM's performance using different LLM models for generating explanations.
- What evidence would resolve it: Conducting experiments using different LLM models (e.g., GPT-3.5 vs. GPT-4) to generate explanations and comparing their performance on the same tasks.

### Open Question 2
- Question: How does the quality of AnnoLLM's generated explanations impact the performance of the system on different tasks?
- Basis in paper: [explicit] The paper mentions that the performance of AnnoLLM is highly dependent on the generated explanations and performs an ablation study on the effect of different methods used to generate explanations.
- Why unresolved: The paper does not provide a detailed analysis of how the quality of generated explanations affects AnnoLLM's performance on different tasks.
- What evidence would resolve it: Analyzing the relationship between the quality of generated explanations and AnnoLLM's performance on various tasks, possibly by using human evaluation of the explanations or by comparing the performance of AnnoLLM with explanations of different quality levels.

### Open Question 3
- Question: Can AnnoLLM be extended to handle multi-class classification tasks, and how would its performance compare to human annotators in such cases?
- Basis in paper: [inferred] The paper focuses on binary classification tasks, but does not explore the applicability of AnnoLLM to multi-class classification tasks.
- Why unresolved: The paper does not investigate the potential of AnnoLLM for handling multi-class classification tasks or compare its performance with human annotators in such cases.
- What evidence would resolve it: Extending AnnoLLM to handle multi-class classification tasks and comparing its performance with human annotators on a variety of multi-class tasks.

## Limitations
- The experimental scope is narrow, testing only three binary classification tasks rather than a broader range of NLP tasks
- The comparison primarily focuses on accuracy metrics without addressing other important factors like cost, speed, or annotation consistency over time
- The paper doesn't address potential biases in LLM-generated explanations or how these might affect annotation quality

## Confidence
- High confidence: The basic feasibility of using LLMs for annotation tasks has been demonstrated through empirical results
- Medium confidence: The explain-then-annotate approach improves performance compared to simple few-shot prompting
- Low confidence: The generalizability of this approach to more complex, multi-class, or open-ended annotation tasks

## Next Checks
1. Test the approach on multi-class classification tasks to evaluate scalability beyond binary decisions
2. Conduct ablation studies removing the explanation generation step to quantify its contribution to performance
3. Measure annotation consistency across multiple LLM generations for the same examples to assess reliability