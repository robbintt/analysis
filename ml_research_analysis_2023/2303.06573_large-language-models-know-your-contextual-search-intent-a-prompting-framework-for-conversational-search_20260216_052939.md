---
ver: rpa2
title: 'Large Language Models Know Your Contextual Search Intent: A Prompting Framework
  for Conversational Search'
arxiv_id: '2303.06573'
source_url: https://arxiv.org/abs/2303.06573
tags:
- search
- conversational
- language
- prompting
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a few-shot prompting framework for conversational
  search that leverages large language models to perform conversational query rewriting.
  The proposed framework, called LLMCS, explores three prompting methods to generate
  multiple query rewrites and hypothetical responses, and proposes to aggregate them
  into an integrated representation that can robustly represent the user's real contextual
  search intent.
---

# Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search

## Quick Facts
- arXiv ID: 2303.06573
- Source URL: https://arxiv.org/abs/2303.06573
- Reference count: 40
- Key outcome: LLMCS achieves up to +5.9% and +32.9% relative improvements in NDCG@3 on CAsT-19 and CAsT-20 datasets respectively

## Executive Summary
This paper presents LLMCS, a few-shot prompting framework that leverages large language models for conversational search through query rewriting. The framework explores three prompting methods (REW, RTR, RAR) to generate multiple query rewrites and hypothetical responses, then aggregates them into an integrated representation of the user's contextual search intent. Experiments on CAsT-19 and CAsT-20 demonstrate significant improvements over existing methods, including those using human rewrites, highlighting the potential of LLMs for conversational search tasks.

## Method Summary
LLMCS uses GPT-3's code-davinci-002 for few-shot in-context learning with 4 exemplar conversations from CAsT-22. The framework implements three prompting methods: REW (rewrite only), RTR (rewrite plus hypothetical response), and RAR (rewrite, hypothetical response, and aggregation). It incorporates chain-of-thought reasoning to improve intent understanding and uses MaxProb, Self-Consistency, or Mean aggregation to combine multiple generated results. A dual ad-hoc retriever (ANCE) encodes the rewrites and responses into high-dimensional vectors, and similarity scores are computed for retrieval. The system is evaluated using NDCG@3 and MRR on CAsT-19 and CAsT-20 datasets.

## Key Results
- LLMCS significantly outperforms existing methods and even using human rewrites on CAsT-19 and CAsT-20
- Up to +5.9% relative improvement in NDCG@3 on CAsT-19 and +32.9% on CAsT-20
- RAR prompting method with hypothetical responses shows the best performance
- Self-Consistency aggregation method provides the most robust search intent representation

## Why This Works (Mechanism)

### Mechanism 1
- Chain-of-thought prompting decomposes reasoning into intermediate steps, improving contextual search intent understanding by guiding the model through explicit reasoning about the user's real search intent before generating rewrites
- Core assumption: Explicitly showing the reasoning process for exemplars transfers to better understanding of current conversation context
- Evidence: Incorporation of chain-of-thought reasoning in prompts improves quality of rewriting
- Break condition: Chain-of-thought reasoning becomes unreliable when conversation context is too complex or exemplars don't adequately represent needed reasoning patterns

### Mechanism 2
- Generating hypothetical responses alongside query rewrites provides additional context that improves search performance by supplementing short query rewrites with relevant information
- Core assumption: Hypothetical responses contain complementary information that enhances search intent representation beyond query rewrites alone
- Evidence: RAR method with both rewrites and responses significantly outperforms REW method with rewrites only
- Break condition: Generation of irrelevant or incorrect hypothetical responses that mislead rather than enhance search intent understanding

### Mechanism 3
- Aggregating multiple generated results through self-consistency or mean approaches creates more robust search intent representations by filtering out incorrect interpretations while reinforcing correct ones
- Core assumption: Multiple generated results capture diverse perspectives of search intent, and aggregation can create integrated representation
- Evidence: Self-Consistency aggregation method provides more reasonable and consistent search intent representation than MaxProb or Mean
- Break condition: Aggregation methods fail when generated results are too diverse or when majority of interpretations are incorrect

## Foundational Learning

- **In-context learning**: Why needed - framework uses few-shot learning with GPT-3 without fine-tuning, relying on exemplars in prompt to guide model behavior. Quick check: How many exemplars are used in the prompt and from which dataset are they drawn?

- **Dense retrieval and embedding similarity**: Why needed - framework uses dual ad-hoc retriever to encode rewrites and hypothetical responses into high-dimensional vectors, then calculates similarity scores for retrieval. Quick check: What similarity measure is used to compare final search intent vector with passage vectors?

- **Chain-of-thought reasoning**: Why needed - framework incorporates chain-of-thought prompting to guide language model through intermediate reasoning steps for better intent understanding. Quick check: What specific instruction format is used to incorporate chain-of-thought into the prompt?

## Architecture Onboarding

- **Component map**: Current query and conversation context → Prompt construction → GPT-3 generation → Aggregation → ANCE encoding → Similarity calculation → Top-K retrieval

- **Critical path**: Conversation context → Prompt construction → GPT-3 generation → Aggregation → ANCE encoding → Similarity calculation → Top-K retrieval

- **Design tradeoffs**: Generation cost vs. performance (multiple generations improve robustness but increase computational cost); Prompt complexity vs. effectiveness (chain-of-thought improves understanding but requires manual exemplar creation); Aggregation method choice (Self-Consistency is more reliable but computationally expensive compared to MaxProb)

- **Failure signatures**: Poor performance on turns with complex references or topic shifts; Degradation when exemplars don't match conversation patterns; Aggregation failures when generated results are too diverse or incorrect

- **First 3 experiments**: 1) Compare REW vs RTR vs RAR prompting methods on CAsT-19 to validate impact of hypothetical responses; 2) Test chain-of-thought incorporation on subset of turns to measure improvement in understanding complex references; 3) Evaluate MaxProb vs Self-Consistency vs Mean aggregation methods to determine optimal balance between performance and computational cost

## Open Questions the Paper Calls Out

- **Question**: How does performance of LLMCS scale with different model sizes and types (e.g., text-davinci-002 vs. GPT-4, or other LLMs like PaLM)?
- **Question**: What is quality of hypothetical responses generated by LLMCS, and how does this quality correlate with retrieval performance?
- **Question**: Can information generated by LLMCS be effectively used to improve training of conversational dense retrievers?

## Limitations
- Framework effectiveness depends heavily on quality and representativeness of few-shot exemplars used for in-context learning
- Requires multiple expensive GPT-3 API calls, making it computationally intensive for real-world deployment
- Manual effort required for chain-of-thought exemplar creation may not scale well across different domains or languages

## Confidence
- **High confidence**: Core finding that LLMs can effectively perform conversational search through few-shot prompting is well-supported by experimental results showing consistent improvements on CAsT-19 and CAsT-20
- **Medium confidence**: Superiority of RAR prompting method and specific aggregation techniques are supported by experiments but based on limited dataset diversity
- **Low confidence**: Scalability and practical deployment implications are not thoroughly explored; manual effort for chain-of-thought exemplar creation is mentioned but not quantified

## Next Checks
1. Evaluate LLMCS on additional conversational search datasets beyond CAsT to verify generalization across different domains and conversation styles
2. Measure actual computational cost and latency under different configurations to determine practical deployment threshold
3. Conduct user study to measure time and expertise required to create effective chain-of-thought exemplars and explore automated exemplar generation methods