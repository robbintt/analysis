---
ver: rpa2
title: 'Toward Generalizable Machine Learning Models in Speech, Language, and Hearing
  Sciences: Estimating Sample Size and Reducing Overfitting'
arxiv_id: '2308.11197'
source_url: https://arxiv.org/abs/2308.11197
tags:
- features
- sample
- size
- statistical
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides a rigorous statistical power analysis of different
  cross-validation methods in machine learning for speech, language, and hearing sciences.
  Through Monte Carlo simulations, the authors demonstrate that single holdout and
  train-validation-test methods significantly overestimate model performance and have
  low statistical power, especially at small sample sizes.
---

# Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Estimating Sample Size and Reducing Overfitting

## Quick Facts
- arXiv ID: 2308.11197
- Source URL: https://arxiv.org/abs/2308.11197
- Reference count: 40
- Key outcome: Nested 10-fold cross-validation provides unbiased accuracy estimates, higher statistical power, and up to four times higher model confidence compared to single holdout and train-validation-test methods.

## Executive Summary
This study provides a rigorous statistical power analysis of different cross-validation methods in machine learning for speech, language, and hearing sciences. Through Monte Carlo simulations, the authors demonstrate that single holdout and train-validation-test methods significantly overestimate model performance and have low statistical power, especially at small sample sizes. In contrast, nested 10-fold cross-validation provides unbiased accuracy estimates, higher statistical power, and up to four times higher model confidence. The study derives a mathematical model to estimate the minimum required sample size for nested cross-validation and provides lookup tables for determining the recommended sample size to achieve a target model confidence. The key recommendation is to adopt nested k-fold cross-validation for robust, generalizable machine learning models in this field.

## Method Summary
The study uses Monte Carlo simulations to compare four cross-validation methods (single holdout, 10-fold, train-validation-test, and nested 10-fold) in supervised machine learning. Simulated datasets are generated from multivariate Gaussian distributions with specified parameters (m: dimensionality of feature space, l: number of discriminative features, D: Cohen's D effect size, n: sample size per class). Forward feature selection with logistic regression is applied, and statistical power, statistical confidence (probability of correct feature selection), and bias in accuracy estimation are evaluated. The study derives equations for estimating required sample size for nested 10-fold cross-validation and provides lookup tables for sample size recommendations.

## Key Results
- Nested 10-fold cross-validation provides unbiased accuracy estimates while single holdout and train-validation-test methods significantly overestimate performance.
- Nested cross-validation achieves up to four times higher model confidence and higher statistical power compared to alternative methods.
- The study derives mathematical equations (equations 4-7) to estimate minimum required sample size for nested cross-validation.
- Lookup tables are provided for determining recommended sample size to achieve target model confidence.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nested k-fold cross-validation produces unbiased accuracy estimates while avoiding data leakage.
- Mechanism: The inner k-fold loop performs feature selection on training data, while the outer loop uses separate testing folds to evaluate performance. This separation prevents information from the testing set from influencing model selection.
- Core assumption: The outer and inner folds are independent, and feature selection is performed solely on training data within each inner fold.
- Evidence anchors:
  - [abstract]: "Nested 10-fold cross-validation provides unbiased accuracy estimates, higher statistical power, and up to four times higher model confidence."
  - [section]: "Nested k-fold cross-validation method consists of an outer k-fold cross-validation loop... the inner loop takes one training-validation set from the outer loop at a time and performs another k-fold cross-validation on it."
  - [corpus]: Weak - no direct evidence in corpus about nested cross-validation properties.
- Break condition: If the outer and inner folds are not properly separated, or if feature selection uses information from the testing data.

### Mechanism 2
- Claim: Nested cross-validation increases statistical power by providing more robust estimates of model performance.
- Mechanism: By using multiple testing sets and averaging performance, nested cross-validation reduces variance in accuracy estimates compared to single holdout methods. This allows detection of true effects with smaller sample sizes.
- Core assumption: The averaging of multiple test set performances provides a more stable estimate than a single test set.
- Evidence anchors:
  - [abstract]: "Nested 10-fold cross-validation resulted in the highest statistical confidence and the highest statistical power, while providing an unbiased estimate of the accuracy."
  - [section]: "The distribution of Ha and the target probability of type II error (β) are the factors determining the power of a test, where power is defined as 1 -β."
  - [corpus]: Weak - no direct evidence in corpus about statistical power improvements.
- Break condition: If the number of folds is too small to provide stable estimates, or if the dataset is too small relative to the number of folds.

### Mechanism 3
- Claim: Nested cross-validation provides higher statistical confidence by ensuring correct features are selected with higher probability.
- Mechanism: The voting mechanism aggregates feature selection results across outer folds, increasing the probability that truly discriminative features are consistently selected.
- Core assumption: Features that are truly discriminative will be selected consistently across different training-validation splits.
- Evidence anchors:
  - [abstract]: "Statistical confidence of the model was defined as the probability of correct features being selected and hence being included in the final model."
  - [section]: "In this study, we employed k=10 and used the set of features that had the highest probability of joint occurrence among the k different estimates of the best feature sets."
  - [corpus]: Weak - no direct evidence in corpus about confidence calculations.
- Break condition: If the features are not truly discriminative, or if the number of outer folds is too small to provide stable voting.

## Foundational Learning

- Concept: Statistical power analysis
  - Why needed here: To determine the minimum sample size required to detect a true effect with a specified probability (1-β)
  - Quick check question: What is the relationship between effect size, sample size, and statistical power?

- Concept: Cross-validation methods
  - Why needed here: Different cross-validation methods have different properties regarding bias, variance, and data leakage
  - Quick check question: What is the key difference between two-split and three-split cross-validation methods?

- Concept: Feature selection and model complexity
  - Why needed here: The number of features selected affects both model performance and statistical confidence
  - Quick check question: How does increasing the dimensionality of the feature space affect the statistical power of a model?

## Architecture Onboarding

- Component map:
  Data generation module (Monte Carlo simulations) -> Cross-validation implementation (single holdout, k-fold, train-validation-test, nested k-fold) -> Feature selection algorithm (wrapper-forward method) -> Statistical analysis module (hypothesis testing, power calculations) -> Result visualization and reporting

- Critical path:
  1. Generate simulated data with specified parameters (m, n, l, D)
  2. Apply feature selection using nested k-fold cross-validation
  3. Calculate statistical power and confidence metrics
  4. Generate lookup tables and equations for sample size estimation

- Design tradeoffs:
  Computational cost vs. statistical robustness (nested k-fold is more robust but computationally expensive)
  Number of folds vs. stability of estimates (more folds provide more stable estimates but require more computation)
  Dimensionality of feature space vs. required sample size (higher dimensionality requires larger sample sizes)

- Failure signatures:
  Overestimation of model performance (indicates data leakage)
  Low statistical power (indicates insufficient sample size or weak effect size)
  Unstable feature selection (indicates high variance in estimates)

- First 3 experiments:
  1. Generate a balanced dataset with m=20, n=100, l=2, D=0.8 and apply nested k-fold cross-validation to verify unbiased performance estimates
  2. Vary the number of extracted features (m) from 10 to 40 while keeping other parameters constant to observe the effect on statistical confidence
  3. Test the equations provided in the paper for estimating required sample size by generating datasets with different effect sizes and comparing the predicted vs. actual required sample sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend power analysis methods to handle multi-class classification problems?
- Basis in paper: [inferred] The paper explicitly states that the derived equations assume a binary classification problem and that they cannot be directly applied to multi-class scenarios.
- Why unresolved: The authors note this as a limitation but do not provide a methodology for extending the approach to multi-class problems.
- What evidence would resolve it: A study that derives similar equations for multi-class problems, or empirical validation showing how the binary classification approach can be adapted.

### Open Question 2
- Question: How does the proposed power analysis method perform with non-linear feature spaces and non-linear classifiers?
- Basis in paper: [explicit] The authors investigate this using "equivalent Cohen's D" for a bull's eye dataset with SVM, finding an 8.6% MPE, but note this needs further investigation.
- Why unresolved: The authors acknowledge this is a preliminary investigation that requires more rigorous treatment in a separate study.
- What evidence would resolve it: A comprehensive study evaluating the method across diverse non-linear feature spaces and classifiers, with comparison to true required sample sizes.

### Open Question 3
- Question: What is the optimal value of k in nested k-fold cross-validation for maximizing statistical power while minimizing computational cost?
- Basis in paper: [explicit] The authors used k=10 based on common practice but state this is an open question that deserves future investigation.
- Why unresolved: The paper only investigates k=10 and does not explore the interaction between k, sample size, and statistical power.
- What evidence would resolve it: A systematic study varying k values across different sample sizes and problem dimensions, determining optimal k for different scenarios.

## Limitations
- Simulation-based approach may not fully capture the complexity and noise characteristics of real-world speech, language, and hearing datasets.
- The study focuses on a specific feature selection method (forward wrapper) and classifier (logistic regression), which may limit generalizability.
- The power analysis framework assumes fixed type I error rate (α=0.05) and power threshold (1-β=0.8), which may not be appropriate for all research contexts.

## Confidence
- **High**: Core claims about nested cross-validation's superiority are theoretically consistent and well-supported by simulation results.
- **Medium**: Statistical power and confidence claims are demonstrated in simulations but require further empirical validation in real-world applications.

## Next Checks
1. **Cross-method comparison**: Validate the findings using alternative feature selection methods (e.g., LASSO, random forest feature importance) and classifiers (e.g., support vector machines, random forests) to assess the robustness of nested cross-validation's advantages across different ML pipelines.

2. **Real-world dataset application**: Apply the recommended nested cross-validation approach and sample size estimation equations to multiple real speech, language, and hearing datasets with varying sample sizes, feature dimensionalities, and effect sizes to confirm that the theoretical benefits translate to practical improvements in model generalization.

3. **Clinical impact assessment**: Evaluate whether the increased statistical confidence from nested cross-validation translates to improved clinical decision-making or diagnostic accuracy in a real-world speech and hearing application, moving beyond pure classification accuracy to assess practical utility.