---
ver: rpa2
title: 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations'
arxiv_id: '2312.08935'
source_url: https://arxiv.org/abs/2312.08935
tags:
- arxiv
- step
- reasoning
- reward
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Math-Shepherd, a process reward model for
  mathematical reasoning that assigns reward scores to each step of solutions. The
  key innovation is an automatic process supervision framework that constructs training
  data without human annotations by leveraging Monte Carlo Tree Search principles
  - evaluating intermediate steps based on their potential to deduce correct final
  answers.
---

# Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations

## Quick Facts
- arXiv ID: 2312.08935
- Source URL: https://arxiv.org/abs/2312.08935
- Reference count: 11
- Key outcome: Automatic process supervision achieves 93.3% accuracy on GSM8K and 48.1% on MATH, surpassing human-annotated methods

## Executive Summary
This paper introduces Math-Shepherd, an automatic process supervision framework for mathematical reasoning that evaluates intermediate steps in LLM solutions without human annotations. The method leverages Monte Carlo Tree Search principles to construct training data by analyzing the potential of each step to lead to correct final answers. Math-Shepherd demonstrates exceptional performance across multiple open-source LLMs, achieving state-of-the-art results on both GSM8K (93.3%) and MATH (48.1%) datasets while eliminating the need for costly human annotations.

## Method Summary
The method constructs training data by sampling solutions from fine-tuned generators, then uses a completer model to generate multiple subsequent reasoning paths from each intermediate step. Each step is annotated as having potential to lead to correct answers based on the number of correct completions it generates. The process reward model is trained using these automatic annotations with binary cross-entropy loss to distinguish between steps with and without potential to deduce correct answers. The trained PRM is then used to verify and reinforce LLM solutions through step-wise evaluation.

## Key Results
- Achieves 93.3% accuracy on GSM8K and 48.1% on MATH datasets
- Outperforms human-annotated process reward models (PRM800K) and GPT-4 early versions
- Demonstrates superior performance across LLMs ranging from 7B to 70B parameters
- Eliminates need for human annotations while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatic process supervision can replace human annotations while maintaining high performance in mathematical reasoning.
- Mechanism: Uses Monte Carlo Tree Search principles to evaluate intermediate reasoning steps based on their potential to deduce correct final answers, constructing training data without human input.
- Core assumption: A step that leads to more correct answers through subsequent reasoning paths is inherently better than one that leads to fewer correct answers.
- Evidence anchors:
  - [abstract] "The key innovation is an automatic process supervision framework that constructs training data without human annotations by leveraging Monte Carlo Tree Search principles"
  - [section] "Inspired by Monte Carlo Tree Search... we define the quality of an intermediate step as its potential to deduce the correct final answer"
- Break condition: If the completion model consistently fails to generate correct subsequent reasoning paths from intermediate steps, the automatic annotation quality degrades significantly.

### Mechanism 2
- Claim: Process reward models (PRM) outperform outcome reward models (ORM) on complex mathematical reasoning tasks.
- Mechanism: PRM evaluates each reasoning step individually, providing more granular feedback than ORM which only evaluates final outcomes, allowing identification of specific error locations.
- Core assumption: Mathematical reasoning errors are localized to specific steps rather than distributed throughout the solution.
- Evidence anchors:
  - [abstract] "PRM is advantageous due to several compelling reasons... Its ability to offer precise feedback by identifying the specific location of any errors"
  - [section] "Compared to ORM, PRM can provide more detailed and reliable feedback"
- Break condition: If mathematical problems have non-localized errors where multiple steps interact to create the final error, PRM's step-by-step approach becomes less effective.

### Mechanism 3
- Claim: Larger base models serve as better completers for automatic process annotation, improving data quality.
- Mechanism: A stronger completer generates more diverse and accurate subsequent reasoning paths from intermediate steps, leading to higher-quality automatic labels.
- Core assumption: Model capability directly correlates with the quality of generated reasoning paths.
- Evidence anchors:
  - [section] "Figure 4(b) shows the cross-entropy loss across diverse completers... a larger completer is adept at generating superior-quality datasets"
  - [section] "These results substantiate that we should utilize a more potent reward model for validating or supervising the generator"
- Break condition: If the relationship between model size and completion quality plateaus or if other factors (like training data quality) become limiting, simply increasing model size may not improve annotation quality.

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS) principles
  - Why needed here: The method leverages MCTS concepts to evaluate intermediate steps based on their potential to lead to correct answers through multiple subsequent reasoning paths
  - Quick check question: How does MCTS evaluate the quality of a game position, and how is this analogous to evaluating a reasoning step's potential to lead to a correct answer?

- Concept: Reinforcement learning with reward models
  - Why needed here: The PRM is used to guide reinforcement learning of the base models through step-by-step feedback
  - Quick check question: What is the difference between outcome-based and process-based reward models in the context of reinforcement learning?

- Concept: Cross-entropy loss for binary classification
  - Why needed here: The PRM is trained using cross-entropy loss to distinguish between steps that have potential to lead to correct answers versus those that don't
  - Quick check question: How does the binary cross-entropy loss function work for training a classifier that distinguishes between two classes?

## Architecture Onboarding

- Component map:
  - Generator models (LLaMA2-7B/13B/70B, LLemma-7B/34B, DeepSeek-67B) -> produce candidate solutions
  - Completer model (LLemma-7B) -> generates subsequent reasoning paths from intermediate steps
  - PRM models (LLaMA2-70B, LLemma-34B) -> assign reward scores to individual steps
  - Verification pipeline -> combines PRM scores with self-consistency for final answer selection

- Critical path: Generator → Solution decomposition → Step-wise evaluation via PRM → Answer selection (with or without self-consistency)
- Design tradeoffs:
  - N value (number of subsequent paths): Higher N improves annotation quality but increases computational cost
  - Model size for completer: Larger models generate better completions but are more expensive to run
  - Hard vs. soft estimation: Hard estimation is simpler but soft estimation may capture more nuanced quality differences
- Failure signatures:
  - PRM accuracy drops significantly when tested on out-of-distribution problems
  - Performance degrades when N is too small (insufficient diversity in completions)
  - Model overfits to specific reasoning patterns if training data is not diverse enough
- First 3 experiments:
  1. Test PRM performance with different N values (1, 4, 16) on GSM8K to find optimal balance between quality and cost
  2. Compare PRM performance using hard estimation vs. soft estimation on a manually annotated validation set
  3. Evaluate PRM generalization by testing on Hungarian national exam dataset after training on GSM8K and MATH

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of N (number of subsequent reasoning paths to decode) for automatic process annotation to balance quality and computational cost?
- Basis in paper: [explicit] The paper states "we observe that as N increases, so does the quality of automatic annotations. However, this completion process demands a lot of computing resources, potentially imposing a limitation on the usage of our method."
- Why unresolved: The paper shows that accuracy improves with N but does not determine the point of diminishing returns or optimal trade-off point.
- What evidence would resolve it: A systematic study varying N across different problem types and measuring both annotation quality (compared to human annotations) and computational costs.

### Open Question 2
- Question: How does the performance of automatic process supervision compare to human annotation when dealing with more complex mathematical domains beyond GSM8K and MATH?
- Basis in paper: [inferred] The paper shows automatic process supervision outperforms human-annotated PRM800K on GSM8K and MATH, but acknowledges "a noticeable gap remains between PRM800K and the candidate responses generated by the open-source models used in this study."
- Why unresolved: The evaluation is limited to specific datasets, and the paper notes limitations when handling more complex reasoning tasks.
- What evidence would resolve it: Head-to-head comparison of automatic vs human annotation on advanced mathematical domains like IMO problems or university-level proofs.

### Open Question 3
- Question: What is the impact of integrating automatic process supervision with reinforcement learning fine-tuning of the generator model?
- Basis in paper: [explicit] The paper states "Although fine-tuning the generator with the trained reward models is a straightforward next step, this paper primarily investigates the reward models themselves."
- Why unresolved: The paper deliberately leaves this as future work, focusing only on verification performance rather than generator improvement.
- What evidence would resolve it: Experiments comparing generator performance with and without RL fine-tuning using the automatically constructed process reward models.

## Limitations

- Computational overhead: The method requires generating multiple completions from intermediate steps (N=8) and running large models for both generation and reward scoring, which may become prohibitive for real-time applications.
- Domain generalization uncertainty: While strong on GSM8K and MATH, the framework's performance on advanced mathematical domains with different solution patterns remains untested.
- Annotation quality dependence: The automatic annotation process quality heavily depends on the completer model's capability and may degrade when dealing with non-standard problem-solving approaches.

## Confidence

**High Confidence Claims**:
- The automatic process supervision framework successfully constructs training data without human annotations
- Process reward models outperform outcome reward models on GSM8K and MATH datasets
- Larger completer models generate higher-quality automatic annotations

**Medium Confidence Claims**:
- The optimal N value of 8 for subsequent reasoning paths balances quality and computational cost
- Hard estimation labels perform comparably to soft estimation for PRM training
- The method generalizes well to Hungarian national exam problems

**Low Confidence Claims**:
- The approach will scale effectively to mathematical domains beyond the tested datasets
- The computational efficiency will remain practical for real-time applications
- The automatic supervision quality matches human-annotated supervision across all problem types

## Next Checks

1. **Annotation Quality Validation**: Manually evaluate 100 randomly sampled intermediate steps from the automatic annotation process to measure precision and recall compared to human annotations. This would quantify how often the automatic supervision correctly identifies high-quality steps versus incorrectly rewarding or penalizing steps.

2. **Generalization Testing**: Test the trained PRM on a diverse set of mathematical problem domains including calculus problems, abstract algebra proofs, and real-world application problems from STEM fields. Measure performance degradation compared to GSM8K/MATH datasets to establish domain boundaries.

3. **Computational Cost Analysis**: Measure wall-clock time and computational resources required for automatic supervision versus traditional outcome-based approaches. Calculate the break-even point where automatic supervision becomes cost-effective based on the performance improvement it provides.