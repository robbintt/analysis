---
ver: rpa2
title: Distribution Shift Matters for Knowledge Distillation with Webly Collected
  Images
arxiv_id: '2307.11469'
source_url: https://arxiv.org/abs/2307.11469
tags:
- data
- network
- student
- knowledge
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of knowledge distillation when the
  original training data is unavailable, and the teacher network is instead used to
  guide the learning of a student network using webly collected images. The core method
  idea is to address the distribution shift between the webly collected data and the
  original data by dynamically selecting instances that are similar to the original
  data, aligning the features and classifier parameters of the teacher and student
  networks, and using a new contrastive learning block called MixDistribution to generate
  perturbed data with a new distribution.
---

# Distribution Shift Matters for Knowledge Distillation with Webly Collected Images

## Quick Facts
- arXiv ID: 2307.11469
- Source URL: https://arxiv.org/abs/2307.11469
- Reference count: 40
- Key outcome: KD3 achieves up to 5.11% accuracy improvement on TinyImageNet over state-of-the-art data-free KD methods

## Executive Summary
This paper addresses knowledge distillation when original training data is unavailable, using webly collected images instead. The key challenge is distribution shift between webly collected data and original data. The authors propose KD3, a method that combines dynamic instance selection, classifier sharing with feature alignment, and a new MixDistribution contrastive learning block. Experimental results show KD3 outperforms state-of-the-art data-free knowledge distillation approaches across multiple benchmark datasets.

## Method Summary
KD3 tackles data-free knowledge distillation by addressing distribution shift between webly collected images and original data. The method has three components: (1) dynamic instance selection using combined predictions from teacher and student networks to filter relevant data, (2) classifier sharing where the student uses the teacher's fixed classifier while aligning features, and (3) MixDistribution contrastive learning that generates perturbed versions of selected instances to build robustness to distribution shift. The student network is trained with a weighted combination of feature alignment loss and MixDistribution contrastive loss.

## Key Results
- KD3 achieves up to 5.11% accuracy improvement on TinyImageNet compared to best baseline methods
- Outperforms state-of-the-art data-free knowledge distillation approaches on CIFAR10, CIFAR100, CINIC, and TinyImageNet
- Demonstrates effectiveness across multiple teacher-student network pairs (ResNet34→ResNet18, VGGNet16→VGGNet13)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teacher-student dynamic instance selection reduces distribution shift by filtering webly collected data that matches original data distribution.
- Mechanism: Combines predictions from both teacher and student networks to dynamically assign confidence scores to each webly collected instance, filtering out low-confidence ones.
- Core assumption: Teacher network trained on original data retains class knowledge, and its high confidence on similar webly collected instances indicates distribution similarity.
- Evidence anchors:
  - [abstract] "we first dynamically select useful training instances from the webly collected data according to the combined predictions of teacher network and student network."
  - [section 3.2] "we propose to select useful instances from ¯D based on the output probabilities of teacher network NT and student network NS."
- Break condition: If teacher network loses class knowledge or is heavily biased, confidence scores no longer reflect true distribution similarity.

### Mechanism 2
- Claim: Sharing classifier and aligning features preserves original data information in student network.
- Mechanism: Student network uses teacher's classifier directly (fixed), aligning its feature extractor outputs to teacher's via weighted loss.
- Core assumption: Classifier parameters encode task-specific information; fixing them transfers this knowledge while preventing noise from webly collected data.
- Evidence anchors:
  - [abstract] "we align both the weighted features and classifier parameters of the two networks for knowledge memorization."
  - [section 3.3] "we share the classifier gT (learned by NT ) with NS, so that the critical information of unseen original data...can be transferred from NT to NS."
- Break condition: If feature alignment fails or weights become unstable, student network will diverge from teacher behavior.

### Mechanism 3
- Claim: MixDistribution contrastive learning makes student network robust to distribution shift by learning invariant representations.
- Mechanism: Generates perturbed versions of selected webly collected instances with altered statistics, encouraging student to produce consistent features for both original and perturbed versions.
- Core assumption: Distribution shift manifests mainly in image statistics; learning consistency across perturbed versions builds invariance.
- Evidence anchors:
  - [abstract] "we also build a new contrastive learning block called MixDistribution to generate perturbed data with a new distribution for instance alignment."
  - [section 3.4] "we propose MixDistribution to construct the perturbed data with new distribution, which disturbs statistics of images...the student network is encouraged to produce consistent features."
- Break condition: If perturbations are too extreme or too mild, student network cannot learn meaningful invariance.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: Core task is transferring teacher knowledge to student without original data; understanding KD basics is prerequisite.
  - Quick check question: What is the typical objective used in standard KD, and how does it differ when teacher and student share a classifier?

- Concept: Distribution shift and domain adaptation
  - Why needed here: Problem setting explicitly involves distribution shift between webly collected and original data; need to understand shift detection and mitigation.
  - Quick check question: What are the two main types of distribution shift discussed in the paper, and how does each affect KD performance?

- Concept: Contrastive learning
  - Why needed here: MixDistribution contrastive learning is a key mechanism; understanding how contrastive losses enforce consistency is critical.
  - Quick check question: In standard contrastive learning, what is the role of the temperature parameter τ, and how is it used here?

## Architecture Onboarding

- Component map: Webly collected dataset -> Dynamic instance selection -> Selected subset -> MixDistribution perturbations -> Student network (feature extractor + shared classifier) -> Loss computation

- Critical path:
  1. Load webly collected data
  2. Run dynamic selection using teacher+student predictions
  3. Align features via weighted loss
  4. Apply MixDistribution perturbations
  5. Optimize student with combined loss

- Design tradeoffs:
  - Classifier sharing: avoids retraining classifier but requires careful feature alignment
  - Dynamic selection: balances teacher reliability and student improvement over time
  - MixDistribution: improves robustness but adds computation and hyper-parameter tuning

- Failure signatures:
  - Student accuracy plateaus early: likely feature alignment not working or selection too strict
  - High variance in metrics: MixDistribution perturbations may be too extreme
  - Student underfits: dynamic selection may be filtering too much data

- First 3 experiments:
  1. Run KD3 on CIFAR10 with ResNet34→ResNet18; verify accuracy gain over DFND baseline
  2. Test classifier sharing alone by disabling MixDistribution; check accuracy drop
  3. Vary MixDistribution perturbation strength (βmix/γmix scaling); measure impact on robustness to distribution shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KD3 compare to data-free knowledge distillation methods when the webly collected data has a more severe distribution shift than the original data?
- Basis in paper: [explicit] The paper acknowledges that the webly collected data and original data may have different distributions, but it does not explore the impact of varying degrees of distribution shift on the performance of KD3.
- Why unresolved: The experiments conducted in the paper only compare KD3 to other data-free knowledge distillation methods on datasets with relatively small distribution shifts.
- What evidence would resolve it: Conducting experiments with webly collected data that has a more severe distribution shift than the original data and comparing the performance of KD3 to other data-free knowledge distillation methods would provide insights into the robustness of KD3 in handling extreme distribution shifts.

### Open Question 2
- Question: How does the performance of KD3 vary with different amounts of webly collected data?
- Basis in paper: [inferred] The paper mentions that KD3 dynamically selects useful training instances from the webly collected data, but it does not explore the impact of varying the amount of webly collected data on the performance of KD3.
- Why unresolved: The experiments conducted in the paper only use a fixed amount of webly collected data.
- What evidence would resolve it: Conducting experiments with different amounts of webly collected data and comparing the performance of KD3 would provide insights into the scalability of KD3 with respect to the amount of webly collected data.

### Open Question 3
- Question: How does the performance of KD3 vary with different types of distribution shifts (e.g., image style, image category)?
- Basis in paper: [explicit] The paper acknowledges that the webly collected data and original data may have different distributions in terms of image style and image category, but it does not explore the impact of different types of distribution shifts on the performance of KD3.
- Why unresolved: The experiments conducted in the paper only use webly collected data with a combination of image style and image category shifts.
- What evidence would resolve it: Conducting experiments with webly collected data that has different types of distribution shifts (e.g., only image style shifts, only image category shifts) and comparing the performance of KD3 would provide insights into the effectiveness of KD3 in handling different types of distribution shifts.

## Limitations

- Limited ablation studies showing individual contribution of each component to final performance
- No theoretical justification for why specific MixDistribution perturbations work
- Does not address potential biases in webly collected data or provide robustness analysis across different data collection strategies

## Confidence

- Confidence in core claim (KD3 outperforms SOTA): High
- Confidence in dynamic instance selection mechanism: Medium
- Confidence in MixDistribution contrastive learning mechanism: Low

## Next Checks

1. **Component ablation**: Run KD3 with only dynamic instance selection (no MixDistribution, no classifier sharing) and only MixDistribution (no dynamic selection, no classifier sharing) to isolate their individual contributions.

2. **Distribution shift quantification**: Measure the actual distribution shift between webly collected data and original data using statistical divergence metrics (e.g., Wasserstein distance) and correlate with performance gains.

3. **Perturbation sensitivity analysis**: Systematically vary the MixDistribution perturbation parameters (βmix, γmix) across a wide range and measure impact on both accuracy and robustness to different types of distribution shifts.