---
ver: rpa2
title: 'Now It Sounds Like You: Learning Personalized Vocabulary On Device'
arxiv_id: '2305.03584'
source_url: https://arxiv.org/abs/2305.03584
tags:
- learning
- federated
- vocabulary
- language
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of out-of-vocabulary (OOV) words\
  \ in on-device language models trained with federated learning, where memory and\
  \ latency constraints prevent the use of subword tokenization or beam search decoding.\
  \ The authors propose OOV Expansion, a method that introduces a personalized OOV\
  \ adapter\u2014a small MLP with residual connections\u2014to learn embeddings for\
  \ user-specific vocabulary by transferring knowledge from a central model."
---

# Now It Sounds Like You: Learning Personalized Vocabulary On Device

## Quick Facts
- arXiv ID: 2305.03584
- Source URL: https://arxiv.org/abs/2305.03584
- Reference count: 6
- Outperforms standard FL personalization methods with up to 5.6% relative improvements in next-word prediction accuracy

## Executive Summary
This work addresses the challenge of out-of-vocabulary (OOV) words in on-device language models trained with federated learning, where memory and latency constraints prevent the use of subword tokenization or beam search decoding. The authors propose OOV Expansion, a method that introduces a personalized OOV adapter—a small MLP with residual connections—to learn embeddings for user-specific vocabulary by transferring knowledge from a central model. The adapter processes OOV word representations from CharCNN and combines them with LSTM encoder outputs to generate OOV scores, which are concatenated with standard vocabulary scores for next-word prediction. Experiments on Reddit and Stack Overflow datasets show OOV Expansion outperforms standard FL personalization methods, achieving up to 5.6% relative improvements in next-word prediction accuracy (EMR3) and reducing the averaged unknown-word-rate by at least 97%.

## Method Summary
OOV Expansion introduces a personalized OOV adapter to handle out-of-vocabulary words in on-device language models trained with federated learning. The method uses a CharCNN to extract character-level features for OOV words, which are then transformed by an MLP adapter with residual connections to produce adapted embeddings. These embeddings are combined with LSTM encoder outputs to compute OOV scores for next-word prediction. The adapter learns user-specific vocabulary embeddings by transferring knowledge from the central model's CharCNN, which was trained on general vocabulary during pretraining. The approach achieves significant OOV rate reduction (up to 97.7%) while being 36% more parameter-efficient than the OOV-oracle baseline, without requiring additional training parameters during pretraining or federated learning phases.

## Key Results
- Achieves up to 5.6% relative improvements in next-word prediction accuracy (EMR3) compared to standard FL personalization methods
- Reduces averaged unknown-word-rate by at least 97% compared to OOV-as-UNK baseline
- 36% more parameter-efficient than OOV-oracle baseline while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The OOV adapter learns user-specific embeddings by transferring knowledge from the CharCNN, which was trained on general vocabulary during pretraining.
- Mechanism: CharCNN extracts character-level features for OOV words, which are then transformed by the adapter (MLP with residual connections) to produce adapted embeddings. These embeddings are combined with LSTM outputs to compute OOV scores for next-word prediction.
- Core assumption: The character-level features learned by CharCNN for general vocabulary can be effectively transferred to represent OOV words when passed through an adapter.
- Evidence anchors:
  - [abstract] "This method introduces a personalized 'OOV adapter' that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary."
  - [section] "The CharCNN module multitasks on (1) providing inputs for LSTM encoder and (2) computing OOV embeddings."
  - [corpus] Weak - corpus mentions OOV adaptation but doesn't specifically validate CharCNN transfer.
- Break condition: If OOV words are too dissimilar from general vocabulary, the CharCNN features may not transfer well, limiting adapter effectiveness.

### Mechanism 2
- Claim: Adapter prevents interference between CharCNN's dual tasks (general vocabulary processing and OOV embedding computation) while adapting prior knowledge to the new OOV task.
- Mechanism: The residual MLP adapter takes CharCNN's OOV representations as input and outputs adapted embeddings, isolating the OOV-specific computation from the main CharCNN operations.
- Core assumption: Separating OOV embedding computation via an adapter reduces interference with CharCNN's primary role in general vocabulary processing.
- Evidence anchors:
  - [section] "Morally speaking, OOV adapter alleviates the interference between the two tasks as well as adapting the prior knowledge of CharCNN to the new 'OOV task'."
  - [abstract] "OOV expansion uses an adapter... to compute the embedding output of the OOV words."
  - [corpus] Missing - corpus doesn't provide specific evidence about interference reduction.
- Break condition: If adapter parameters are not properly initialized or if the residual connections are not effective, interference may persist.

### Mechanism 3
- Claim: The method achieves significant OOV rate reduction (up to 97.7%) by personalizing vocabulary without increasing model size during pretraining or FL.
- Mechanism: By computing OOV scores only during personalization and concatenating them with standard vocabulary scores, the model can handle personalized OOV words without expanding the global vocabulary.
- Core assumption: OOV words can be effectively scored and combined with standard vocabulary scores without requiring additional training parameters during earlier stages.
- Evidence anchors:
  - [section] "Our approach does not require any extra training parameters during pretraining or FL."
  - [section] "The OOV expansion approach reduces the OOV rate of OOV-as-UNK... by more than 97.7%."
  - [corpus] Weak - corpus mentions OOV rate reduction but doesn't specifically validate the parameter efficiency claim.
- Break condition: If the personalized OOV vocabulary becomes too large, the concatenated score computation may become inefficient or exceed device constraints.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: The method operates in a federated learning context where models are trained across decentralized devices without sharing raw data.
  - Quick check question: What is the primary benefit of using FL for on-device language modeling?

- Concept: Character-level CNNs for language modeling
  - Why needed here: CharCNN is used to extract word embeddings at the character level, which is crucial for handling OOV words that weren't seen during pretraining.
  - Quick check question: Why is a character-level approach preferred over word-level for OOV handling?

- Concept: Adapter modules in neural networks
  - Why needed here: Adapters are used to adapt the model to new knowledge domains (OOV words) without modifying the original model parameters.
  - Quick check question: How do residual connections in adapters help with knowledge transfer?

## Architecture Onboarding

- Component map:
  CharCNN -> LSTM encoder -> (standard path to decoder) + (OOV path through adapter) -> vocabulary scores + OOV scores -> concatenated scores -> next-word prediction

- Critical path: Input text → CharCNN → (standard path to decoder) + (OOV path through adapter) → vocabulary scores + OOV scores → concatenated scores → next-word prediction

- Design tradeoffs:
  - Memory vs. coverage: Using closed vocabulary limits memory usage but requires OOV handling mechanism
  - Complexity vs. latency: Subword tokenization would handle OOV better but increases latency
  - Personalization vs. privacy: Adapter-based approach personalizes without sharing OOV data

- Failure signatures:
  - Poor OOV performance: Adapter not properly initialized or OOV words too dissimilar from general vocabulary
  - Increased latency: Concatenated score computation becomes inefficient with large OOV vocabulary
  - Memory issues: Model size exceeds device constraints despite parameter efficiency claims

- First 3 experiments:
  1. Verify CharCNN can extract meaningful features for OOV words by testing on a small held-out set
  2. Test adapter initialization strategies (random vs. pretrained) on a single client
  3. Measure OOV rate reduction with different adapter architectures (number of layers, hidden dimensions)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed OOV Expansion method perform with subword tokenization or beam search decoding, given the paper's focus on closed-vocabulary models due to memory and latency constraints?
- Basis in paper: [explicit] The paper states that subword tokenization or beam search decoding cannot be supported due to memory and latency constraints, but it does not explore the performance of OOV Expansion with these techniques.
- Why unresolved: The paper focuses on closed-vocabulary models and does not provide experimental results or theoretical analysis of how OOV Expansion would perform with subword tokenization or beam search decoding.
- What evidence would resolve it: Experiments comparing the performance of OOV Expansion with subword tokenization or beam search decoding to the current closed-vocabulary approach would provide evidence of its effectiveness in different settings.

### Open Question 2
- Question: How does the performance of OOV Expansion vary with different adapter architectures or configurations, such as varying the number of layers or hidden dimensions?
- Basis in paper: [explicit] The paper mentions that the adapter is an MLP with residual connections and discusses the impact of different configurations, but it does not provide a comprehensive analysis of how different adapter architectures affect performance.
- Why unresolved: The paper only briefly mentions the adapter's role and does not explore the impact of different architectures or configurations on the model's performance.
- What evidence would resolve it: A detailed study comparing the performance of OOV Expansion with various adapter architectures or configurations would provide insights into the optimal design choices.

### Open Question 3
- Question: How does the proposed method handle the cold-start problem, where a user's on-device historical data is insufficient or unavailable?
- Basis in paper: [explicit] The paper acknowledges the cold-start problem as a limitation but does not provide a solution or discuss how OOV Expansion would perform in such scenarios.
- Why unresolved: The paper assumes that all possible OOVs are known on-device, which may not be the case for new users or users with limited data.
- What evidence would resolve it: Experiments or theoretical analysis demonstrating the performance of OOV Expansion in cold-start scenarios would provide insights into its robustness and applicability in real-world situations.

## Limitations
- The exact adapter architecture details are not fully specified, making exact reproduction challenging
- Performance on languages with different morphological structures or character sets is untested
- Scalability with larger personalized vocabularies and its impact on latency and memory efficiency is not thoroughly explored

## Confidence
**High Confidence**:
- Significant OOV rate reduction (up to 97.7%) compared to OOV-as-UNK baseline
- Parameter efficiency without additional training parameters during pretraining or FL
- Performance improvement in EMR3 by up to 5.6% over standard FL personalization

**Medium Confidence**:
- Effective knowledge transfer from CharCNN to OOV representations through adapter
- Successful isolation of OOV computation from CharCNN's primary role
- Maintenance of device constraints while providing personalized vocabulary support

**Low Confidence**:
- Consistent performance across different types of user-generated content beyond Reddit and Stack Overflow
- Efficient scaling to handle very large personalized vocabularies
- Equal effectiveness for languages with significantly different linguistic characteristics from English

## Next Checks
1. Conduct ablation studies varying the adapter architecture (number of layers, hidden dimensions, activation functions) to determine the minimum viable configuration that maintains performance while maximizing parameter efficiency.

2. Evaluate the OOV Expansion method on a diverse set of user-generated content datasets (emails, chat messages, technical documentation) to assess the robustness of personalized vocabulary learning across different domains and writing styles.

3. Systematically test the method's performance and efficiency as the size of personalized OOV vocabulary increases, measuring both prediction accuracy and computational overhead to identify potential scalability limits.