---
ver: rpa2
title: Batch Selection and Communication for Active Learning with Edge Labeling
arxiv_id: '2311.08053'
source_url: https://arxiv.org/abs/2311.08053
tags:
- learner
- batch
- teacher
- communication
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of selecting the most informative
  unlabeled data batches to transmit to a teacher model for labeling, while accounting
  for limited communication resources. The authors propose CC-BAKD, which integrates
  Bayesian active learning with a linear mix-up compression mechanism.
---

# Batch Selection and Communication for Active Learning with Edge Labeling

## Quick Facts
- arXiv ID: 2311.08053
- Source URL: https://arxiv.org/abs/2311.08053
- Reference count: 19
- One-line primary result: CC-BAKD achieves high classification accuracy with fewer communication symbols compared to non-compressed approaches, demonstrating robustness to noise.

## Executive Summary
This paper addresses the challenge of selecting informative data batches for labeling in active learning scenarios with limited communication resources. The authors propose CC-BAKD, which integrates Bayesian active learning with a linear mix-up compression mechanism to reduce communication overhead while maintaining information content. Experiments on MNIST demonstrate that the method achieves high accuracy with fewer communication symbols compared to uncompressed approaches, and shows robustness to channel noise.

## Method Summary
CC-BAKD combines Bayesian active learning (BAKD) with batch compression via linear mix-up encoding. The learner uses BatchBALD acquisition to select batches based on epistemic uncertainty, then compresses these batches using a PCA-derived compression matrix Z before transmission. The teacher decodes the received batch, generates soft labels, and transmits them back to the learner. The learner updates its model using both initial labels and teacher feedback, with a free energy criterion balancing their contributions.

## Key Results
- CC-BAKD achieves competitive classification accuracy on MNIST with significantly reduced communication symbols
- The method demonstrates robustness to additive Gaussian noise in the communication channel
- Compression-aware batch selection outperforms naive selection followed by compression

## Why This Works (Mechanism)

### Mechanism 1
Epistemic uncertainty-based batch selection focuses on inputs where the learner's model is most uncertain, avoiding confirmation bias from selecting inherently hard examples. The BatchBALD acquisition function quantifies information gain by measuring mutual information between labels and model parameters. Break condition: If teacher and learner have different uncertainty patterns, suboptimal batches may be selected.

### Mechanism 2
Linear mix-up compression reduces communication overhead while preserving information content. The compression matrix Z transforms batches into compressed vectors that can be reconstructed at the teacher side. Break condition: When compression ratio is too high, reconstruction noise dominates and teacher feedback becomes unreliable.

### Mechanism 3
Compression-aware active batch selection accounts for distortion by selecting batches from the decoded space rather than original space. This better captures which inputs will remain informative after compression. Break condition: Severe quantization noise can make even compression-aware selection suboptimal.

## Foundational Learning

- Concept: Bayesian active learning with variational inference
  - Why needed here: Enables uncertainty estimation over model parameters for informative batch selection
  - Quick check question: How does the variational posterior q(θ|L) differ from a point estimate in batch selection?

- Concept: Epistemic vs aleatoric uncertainty
  - Why needed here: Distinguishes reducible uncertainty from inherent label noise to avoid confirmation bias
  - Quick check question: Why might selecting based on aleatoric uncertainty lead to confirmation bias?

- Concept: Mix-up data augmentation and compression
  - Why needed here: Provides framework for linear compression that reduces communication overhead
  - Quick check question: How does this mix-up approach differ from standard data augmentation?

## Architecture Onboarding

- Component map: Learner -> Batch Selection -> Compression Encoder -> Communication Channel -> Compression Decoder -> Teacher -> Soft Label Generator -> Communication Channel -> Learner Training Loop

- Critical path:
  1. Learner selects batch using compression-aware acquisition function
  2. Learner compresses batch using Zᵀx*
  3. Compressed batch transmitted over noisy channel
  4. Teacher decodes using ZZᵀx* + noise
  5. Teacher generates soft labels p_t(ˆx*)
  6. Teacher sends soft labels back (assumed negligible overhead)
  7. Learner updates training set and retrains model

- Design tradeoffs:
  - Higher compression ratio reduces communication rounds but increases reconstruction noise
  - Larger batch size improves teacher efficiency but requires more symbols per round
  - Weight τ balances teacher feedback vs initial labels

- Failure signatures:
  - Accuracy plateaus despite many communication rounds: Likely compression noise dominates
  - Accuracy decreases over time: Possible confirmation bias or poor batch selection
  - No improvement from compression: Compression ratio may be too conservative

- First 3 experiments:
  1. Baseline: Run BAKD without compression (R=0) to establish performance floor
  2. Compression sweep: Test R ∈ {0.95, 0.97, 0.99} with fixed N=784 to find optimal tradeoff
  3. Noise sensitivity: Vary γ⁻¹ to evaluate robustness and compare with uncompressed methods

## Open Questions the Paper Calls Out
- How does CC-BAKD perform when extended to multiple learners and/or teachers?
- How does CC-BAKD perform on datasets other than MNIST, especially those with higher dimensionality or more complex structures?
- How sensitive is CC-BAKD to the choice of hyperparameters, particularly the compression ratio and the weight hyperparameter τ?
- How does CC-BAKD compare to other active learning strategies that also address communication constraints?

## Limitations
- Experimental validation limited to MNIST dataset with relatively simple image data
- Theoretical guarantees for mix-up compression mechanism not rigorously established
- Hyperparameter sensitivity not fully characterized across different datasets and scenarios

## Confidence

**Epistemic uncertainty selection mechanism**: High confidence - Well-established in Bayesian active learning literature with clear integration into the proposed framework.

**Mix-up compression effectiveness**: Medium confidence - Mathematically sound but limited experimental validation beyond MNIST with small batch sizes.

**Overall system performance**: Medium confidence - Demonstrates improved communication efficiency but lacks comparison with state-of-the-art communication-constrained methods.

## Next Checks

1. **Compression-accuracy tradeoff analysis**: Systematically vary compression ratio R and noise level γ⁻¹ to map the Pareto frontier between communication efficiency and accuracy.

2. **Dataset generalization**: Test CC-BAKD on CIFAR-10/100 or other complex image datasets to evaluate scalability beyond MNIST.

3. **Alternative compression schemes**: Compare mix-up compression with other communication-efficient approaches like sparsification or quantization to establish unique advantages.