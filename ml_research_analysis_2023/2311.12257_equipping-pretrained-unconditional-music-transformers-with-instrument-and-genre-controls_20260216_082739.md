---
ver: rpa2
title: Equipping Pretrained Unconditional Music Transformers with Instrument and Genre
  Controls
arxiv_id: '2311.12257'
source_url: https://arxiv.org/abs/2311.12257
tags:
- music
- genre
- dataset
- generation
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the pretraining-and-finetuning paradigm for
  symbolic music generation using the largest-ever MuseScore dataset containing 1.5
  million songs. The authors first pretrain an unconditional transformer model on
  1.5 million songs, then propose a simple technique to equip it with instrument and
  genre controls by finetuning with additional control tokens.
---

# Equipping Pretrained Unconditional Music Transformers with Instrument and Genre Controls

## Quick Facts
- arXiv ID: 2311.12257
- Source URL: https://arxiv.org/abs/2311.12257
- Reference count: 23
- Primary result: Simple finetuning technique enables pretrained unconditional music transformers to generate controllable music with instrument and genre conditions

## Executive Summary
This paper presents a method to equip pretrained unconditional music transformers with instrument and genre controls through a simple finetuning approach. The authors first pretrain an unconditional transformer model on 1.5 million songs from MuseScore, then introduce control tokens during finetuning to enable controllable generation. The approach leverages the autoregressive nature of transformers to integrate control tokens during inference without requiring architectural changes. Subjective listening tests show the proposed models outperform the unconditional baseline in coherence, harmony, arrangement, and overall quality while successfully adhering to specified genre and instrument conditions.

## Method Summary
The method involves pretraining an unconditional transformer model on 1.5 million songs from MuseScore, then finetuning with additional control tokens to enable instrument and genre conditioning. The model uses a one-dimensional event-based representation adapted from REMI+ and MMT, where note sequences are prefixed with structural tokens including start-of-tags, start-of-program, and start-of-notes. During finetuning, new control tokens are initialized with random weights while existing token embeddings are initialized from the pretrained model. Three model variants are trained: MMT-I (instrument control), MMT-G (genre control), and MMT-GI (both controls). The approach progressively conditions the model on metadata and genre information through three training subsets of increasing specificity.

## Key Results
- The proposed models successfully generate music matching specified genre and instrument conditions
- Subjective listening tests show MMT-GI outperforms unconditional baseline across coherence, harmony, arrangement, and overall quality metrics
- The one-dimensional event-based representation with control tokens provides improved high-level controllability compared to existing approaches

## Why This Works (Mechanism)

### Mechanism 1
Pretraining an unconditional transformer on a large corpus and finetuning with control tokens enables controllable generation without retraining from scratch. The pretrained model learns general musical structure and patterns from 1.5 million songs. By adding control tokens during finetuning, the model learns to condition its output on these signals while preserving its ability to generate musically coherent sequences. The autoregressive nature of transformers allows new tokens to influence generation during inference without requiring architectural changes.

### Mechanism 2
Using a one-dimensional event-based representation with explicit control tokens provides better high-level controllability than multi-dimensional or fragmented representations. By prefixing note sequences with structural tokens, the model learns to parse and condition on high-level metadata before generating note events. This preserves expressiveness while adding controllability. The model can learn to attend to and condition on metadata tokens without disrupting its ability to model musical structure.

### Mechanism 3
Finetuning on subsets with progressively more metadata allows the model to specialize in controllability while retaining learned musical knowledge. The three-stage training (unconditional → instrument-conditioned → genre-instrument-conditioned) progressively introduces more control information, allowing the model to specialize without catastrophic forgetting. The model can leverage its pretrained knowledge while adapting to new control signals without losing previously learned capabilities.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper uses transformer models for sequence modeling, and understanding self-attention is crucial for understanding how control tokens influence generation.
  - Quick check question: How does the autoregressive nature of transformers enable the use of control tokens during inference without architectural changes?

- Concept: Event-based music representation and tokenization
  - Why needed here: The paper uses a one-dimensional event-based representation adapted from REMI+ and MMT. Understanding how musical information is encoded as tokens is essential for grasping the control mechanism.
  - Quick check question: Why does the representation use separate tokens for beat, position, pitch, duration, instrument, and time-signature rather than encoding them together?

- Concept: Pretraining and finetuning paradigms
  - Why needed here: The paper's core approach is to pretrain on a large corpus and then finetune with control tokens. Understanding when and why to use this approach is key to applying it to other domains.
  - Quick check question: What are the benefits and risks of initializing embeddings from a pretrained model vs training from scratch when adding new control tokens?

## Architecture Onboarding

- Component map: Pretrained unconditional transformer (768 dim, 12 heads, 1024 max seq length) -> Control token embeddings (randomly initialized for new tokens) -> Three model variants: MMT-I (instrument), MMT-G (genre), MMT-GI (both) -> Three training subsets: MuseScore-full (unconditional), MuseScore-metadata (instrument), MuseScore-genre (genre)

- Critical path: 1. Pretrain unconditional model on full dataset 2. Initialize new token embeddings (pretrained for existing, random for new) 3. Finetune on appropriate subset with control tokens 4. Generate with control tokens during inference

- Design tradeoffs:
  - One-dimensional vs multi-dimensional representation: simpler but may lose some explicit interdependencies
  - Progressive conditioning vs single-stage training: better specialization but more training steps
  - Large pretraining corpus vs smaller domain-specific corpus: better generalization but more compute

- Failure signatures:
  - Control tokens ignored: generated music doesn't match specified genre/instruments
  - Overfitting to control tokens: poor quality when no control tokens provided
  - Catastrophic forgetting: loss of musical coherence or style diversity

- First 3 experiments:
  1. Generate music with MMT-I using a common instrument (e.g., "piano") and check if generated tracks contain that instrument
  2. Generate music with MMT-G using a common genre (e.g., "classical") and check if generated music matches that style
  3. Generate music with MMT-GI using both genre and instrument and check if both conditions are satisfied simultaneously

## Open Questions the Paper Calls Out

### Open Question 1
How can the MuseScore dataset be further improved to reduce noise and low-quality entries? The authors mention the presence of noisy and low-quality entries in MuseScore, including practice sessions, incomplete songs, and songs without enough metadata. The paper acknowledges this as a future work direction but does not provide specific methods for dataset cleaning.

### Open Question 2
What sampling strategies could be employed to address the long-tailed distribution of genres and instruments in MuseScore? The authors identify the long-tailed nature of the dataset as a challenge for generating music in less common categories and suggest downsampling common genres/instruments and using advanced sampling methods. The paper proposes this as future work but does not implement or evaluate specific sampling strategies.

### Open Question 3
How would incorporating additional metadata attributes (e.g., composer, key signature, popularity) as conditioning signals affect the model's controllability and performance? The authors mention that metadata contains other valuable attributes beyond genre and instrument, which could serve as powerful conditioning signals, and propose extending the model to include these controls.

### Open Question 4
How does the proposed model compare to state-of-the-art models like MMT in terms of music quality, particularly for classical music generation? The authors note that their model does not perform as well as MMT for classical music generation and suggest this may be due to dataset quality issues.

## Limitations
- The quality distribution and genre/instrument coverage of the MuseScore dataset are not characterized, creating uncertainty about generalizability
- Claims about improved "high-level controllability and expressiveness" lack rigorous quantitative comparison against specific prior representations
- Subjective evaluation involved only 11 graduate students without professional musicians or inter-rater reliability metrics

## Confidence

**High Confidence**: The pretraining→finetuning paradigm with control tokens is technically sound and follows established transformer practices. The architectural modifications (adding token embeddings) are standard and well-documented.

**Medium Confidence**: The three-stage training strategy (full→metadata→genre) is plausible and shows empirical improvement over unconditional baselines. However, ablation studies comparing this progressive approach against direct conditioning or other training strategies are absent.

**Low Confidence**: Claims about improved controllability over existing representations lack rigorous quantitative comparison. The paper doesn't benchmark against specific prior representations or models, making this comparative claim difficult to verify.

## Next Checks

1. **Control Token Effectiveness Test**: Generate 50 samples each with MMT-I using instrument tokens and MMT-G using genre tokens, then measure genre/instrument classification accuracy using a pretrained music classifier. This would provide objective evidence that control tokens actually influence the generated music's characteristics rather than just being decorative.

2. **Representation Comparison Test**: Implement a simplified multi-dimensional representation (e.g., MMT-style) and train a comparable model on the same MuseScore-genre subset. Compare controllability metrics (condition adherence) and musical quality metrics (harmony, groove) to test whether the one-dimensional representation truly offers advantages.

3. **Generalization Test**: Use MMT-GI to generate music for instrument-genre combinations that appear rarely or not at all in the training data (e.g., "electric guitar" + "baroque"). Evaluate whether the model can still produce musically coherent outputs that respect both conditions, testing the learned generalization capability beyond memorized patterns.