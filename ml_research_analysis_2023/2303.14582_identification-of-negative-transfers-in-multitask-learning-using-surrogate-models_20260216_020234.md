---
ver: rpa2
title: Identification of Negative Transfers in Multitask Learning Using Surrogate
  Models
arxiv_id: '2303.14582'
source_url: https://arxiv.org/abs/2303.14582
tags:
- tasks
- task
- learning
- source
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of negative transfer in multitask
  learning by introducing a surrogate modeling approach. The core method involves
  sampling random subsets of source tasks, precomputing their multitask learning performances,
  and fitting a linear regression model to approximate these performances.
---

# Identification of Negative Transfers in Multitask Learning Using Surrogate Models

## Quick Facts
- arXiv ID: 2303.14582
- Source URL: https://arxiv.org/abs/2303.14582
- Reference count: 40
- Primary result: 4x higher accuracy than existing methods at identifying negative transfers in multitask learning

## Executive Summary
This paper addresses the problem of negative transfer in multitask learning by introducing a surrogate modeling approach that predicts when adding source tasks will harm target task performance. The method samples random subsets of source tasks, precomputes their multitask learning performances, and fits a linear regression model to predict performances of unseen task subsets. This approach achieves 4 times higher accuracy than existing task affinity measures at identifying negative transfers while improving multitask learning performance on weak supervision, NLP, and multi-group fairness datasets with up to 3.6% absolute accuracy lift.

## Method Summary
The method samples random subsets of source tasks, trains multitask models for each subset, and fits a linear regression model to predict the performance of unseen task subsets. The linear model provides relevance scores between each source and target task, which are used to select beneficial source tasks through thresholding. The approach assumes multitask learning performance is approximately additive across tasks, allowing linear combination of task relevance scores. The method scales linearly with the number of source tasks and consistently outperforms existing optimization methods for multitask learning.

## Key Results
- Surrogate model predicts negative transfers with 4x higher accuracy than existing task affinity measures
- Improves multitask learning performance with up to 3.6% absolute accuracy lift on weak supervision, NLP, and multi-group fairness datasets
- Runtime scales linearly with number of source tasks (O(k) complexity)
- Linear surrogate models accurately approximate MTL performance of task subsets with Spearman correlation of 0.8

## Why This Works (Mechanism)

### Mechanism 1
Linear surrogate models can accurately approximate MTL performance of task subsets by sampling random subsets of source tasks and precomputing their MTL losses. The linear regression model fits these values and predicts unseen subset performances with high accuracy, based on the assumption that MTL loss function is approximately additive across tasks.

### Mechanism 2
Task relevance scores preserve relative distances between tasks and the target task. The relevance score for each task is proportional to the average MTL performance of all subsets containing that task, maintaining distance gaps from source tasks to the target task under the assumption that MTL performance is monotonically related to task similarity.

### Mechanism 3
Efficient subset selection can be achieved through thresholding relevance scores. By examining the minimum of the surrogate model over all possible subsets, a threshold can be derived that optimally separates beneficial and harmful source tasks, assuming the optimal subset corresponds to selecting all tasks with relevance scores below a certain threshold.

## Foundational Learning

- Concept: Multitask learning with hard parameter sharing
  - Why needed here: The paper uses hard parameter sharing where source and target tasks share the same encoder while having separate prediction layers, which is the standard MTL architecture being optimized.
  - Quick check question: In hard parameter sharing MTL, which components are shared across tasks and which are task-specific?

- Concept: Negative transfer in multitask learning
  - Why needed here: Understanding negative transfer is fundamental to why subset selection matters - some source tasks can actually harm target task performance.
  - Quick check question: What distinguishes positive transfer from negative transfer in the context of multitask learning?

- Concept: Linear regression and feature importance
  - Why needed here: The surrogate model uses linear regression to estimate task relevance scores, analogous to feature importance in random forests, which is crucial for understanding how the method works.
  - Quick check question: How does the coefficient of a feature in linear regression relate to its importance in predicting the target variable?

## Architecture Onboarding

- Component map: Subset sampler -> MTL trainer -> Linear regression model -> Threshold-based selector
- Critical path: Training the n MTL models for random subsets dominates runtime and directly affects prediction accuracy
- Design tradeoffs: Linear surrogate models offer computational efficiency (O(k) scaling) but may miss nonlinear interactions; random sampling provides unbiased estimates but requires sufficient coverage of the subset space
- Failure signatures: Poor prediction accuracy (low F1-score for transfer classification), runtime that scales superlinearly with k, or selected task subsets that don't improve MTL performance over baselines
- First 3 experiments:
  1. Validate the linear surrogate model on a small MTL problem with known task relationships to confirm correlation between predicted and actual performances
  2. Test the subset selection threshold on a dataset where some tasks are known to cause negative transfer to verify the separation effect
  3. Measure runtime scaling with increasing numbers of source tasks to confirm the O(k) complexity claim

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical analysis of surrogate models be tightened to explain the few-shot learning scenario where the validation set size is small? The paper notes that the validation set size does not need to be very large for the approach to perform well, but this is not explained by the current Rademacher complexity-based bound.

### Open Question 2
Can more advanced sampling techniques, such as adaptive sampling, be used to speed up the training of surrogate models and enable the training of more powerful models? The current approach uses uniform sampling and a linear model for efficiency, but this may limit the model's ability to capture complex task relationships.

### Open Question 3
How can the linear surrogate model be theoretically justified for multitask learning, similar to how harmonic analysis was used to explain data models in single-task learning? The paper empirically demonstrates the effectiveness of linear models for multitask learning but lacks a rigorous explanation for this phenomenon.

## Limitations
- Linear surrogate model may not capture complex nonlinear interactions between tasks, particularly in scenarios with synergistic or antagonistic task relationships
- The threshold-based selection assumes a simple binary classification of beneficial vs harmful tasks, which may oversimplify more nuanced task relationships
- Sample complexity requirements for accurate surrogate fitting are not fully characterized across different problem domains
- Limited evaluation on highly heterogeneous task spaces where task relationships may be non-hierarchical

## Confidence

- Mechanism 1 (Linear approximation): Medium - Strong theoretical basis but limited empirical validation across diverse task types
- Mechanism 2 (Distance preservation): Low - Core assumption lacks direct supporting evidence in the paper
- Mechanism 3 (Threshold optimality): Medium - Practical effectiveness demonstrated but theoretical guarantees not established

## Next Checks

1. Test surrogate model performance on a controlled synthetic MTL problem where task relationships are known to be nonlinear
2. Evaluate subset selection robustness when task relationships form multiple distinct clusters rather than a simple hierarchy
3. Characterize sample complexity empirically by measuring prediction accuracy as a function of n across different values of k