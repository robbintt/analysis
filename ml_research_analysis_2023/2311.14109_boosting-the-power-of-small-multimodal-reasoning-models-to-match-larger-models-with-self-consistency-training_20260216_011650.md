---
ver: rpa2
title: Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models
  with Self-Consistency Training
arxiv_id: '2311.14109'
source_url: https://arxiv.org/abs/2311.14109
tags:
- arxiv
- reasoning
- multimodal
- language
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multimodal reasoning
  in visual question answering tasks. The authors propose MC-CoT, a self-consistency
  training strategy that generates multiple rationales and answers, then selects the
  most accurate through a voting process.
---

# Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training

## Quick Facts
- arXiv ID: 2311.14109
- Source URL: https://arxiv.org/abs/2311.14109
- Reference count: 40
- Key outcome: MC-CoT Base (223M parameters) achieves 90.64% average accuracy on ScienceQA, approaching performance of much larger models

## Executive Summary
This paper addresses the challenge of improving multimodal reasoning in visual question answering tasks through a self-consistency training strategy called MC-CoT. The approach generates multiple rationales and answers during training, then uses majority voting to select the most consistent and accurate outputs. By enhancing rationale quality, MC-CoT leads to more accurate and robust answers, enabling smaller models to achieve performance comparable to larger models. The method was evaluated on ScienceQA and A-OKVQA datasets, showing significant improvements over existing baselines.

## Method Summary
MC-CoT is a multimodal consistent chain-of-thought training strategy that improves visual question answering by generating multiple rationales and answers through dropout-based sampling, then applying voting mechanisms to select the most consistent outputs. During training, the model generates multiple rationales (Nr times) and answers (Na times), with word-level voting for rationales and majority voting for answers. The final prediction combines mean and weighted mean logits using a parameter α. The approach leverages Jensen's inequality to theoretically justify the aggregation process and improve bias-variance tradeoff through weighted logit combination.

## Key Results
- MC-CoT Base (223M parameters) achieves 90.64% average accuracy on ScienceQA
- Smaller models equipped with MC-CoT achieve results comparable to much larger models
- Rationale quality improvements directly translate to answer accuracy gains
- Significant improvements over existing baselines on both ScienceQA and A-OKVQA datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dropout-induced randomness during training creates diverse rationale samples that, when aggregated, reduce expected loss via Jensen's inequality
- Mechanism: Each training step samples multiple rationales from the same model with different dropout masks, then applies majority voting to select the most consistent word at each position
- Core assumption: Cross-entropy loss is convex in the logit space, allowing Jensen's inequality to guarantee that averaging logits yields lower expected loss than any single sample
- Evidence anchors:
  - [abstract] "Our approach involves having the model generate multiple rationales and then voting for the most consistent words across these rationales to yield a more refined and accurate rationale."
  - [section 4.1] "By applying Jensen's inequality to our model, we deduce that the aggregation process leads to a lower expected loss: L(E(L*), Y) ≤ E(L(L*, Y))."
  - [corpus] Weak: Related papers mention semantic weighting and rationale selection but don't directly support Jensen-based aggregation theory
- Break condition: If dropout masks become correlated or if the model's logits become non-convex in the loss landscape, Jensen's inequality no longer guarantees improvement

### Mechanism 2
- Claim: Weighted averaging of logits based on their variability improves bias-variance tradeoff in multimodal reasoning
- Mechanism: For each logit dimension, compute standard deviation across samples; assign weights inversely proportional to variability, then combine mean and weighted mean logits
- Core assumption: Lower variance predictions are more reliable and should be trusted more than high variance ones
- Evidence anchors:
  - [section 4.2] "The weighted mean logits, denoted by L̂, aim to reduce variability by assigning greater significance to consistent (low-variance) predictions."
  - [section 3.2] "The weight for each logit's dimension is inversely proportional to its variability, assigning higher confidence to predictions with lower variability."
  - [corpus] Missing: No direct evidence in corpus about variance-weighted logit combination for multimodal reasoning
- Break condition: If model predictions are systematically biased but low-variance, this weighting amplifies errors instead of reducing them

### Mechanism 3
- Claim: Rationale quality improvement directly translates to answer accuracy gains through the two-stage reasoning pipeline
- Mechanism: Better rationales provide stronger conditioning for the answer generation stage, reducing error propagation
- Core assumption: The answer generation model can effectively leverage improved rationales to produce more accurate answers
- Evidence anchors:
  - [abstract] "We observe that when rationales are completely accurate, the model's accuracy significantly improves, highlighting the need for high-quality rationale generation."
  - [section 5.2.2] "This demonstrates that even slight improvements in the quality of rationales can significantly impact the answer inference of the multimodal reasoning models."
  - [section 3.2] "The final answer is selected by the majority voting: Y* = Vote({Yi}Na)."
- Break condition: If answer generation stage is weak or ignores rationale context, improved rationales won't translate to better answers

## Foundational Learning

- Concept: Jensen's inequality in convex loss landscapes
  - Why needed here: Provides theoretical foundation for why averaging multiple dropout samples reduces expected loss
  - Quick check question: If cross-entropy loss is convex in logits, does Jensen's inequality guarantee that E[L(x)] ≥ L(E[x])?

- Concept: Bias-variance tradeoff in ensemble predictions
  - Why needed here: Explains why combining mean and weighted mean logits can outperform either alone
  - Quick check question: In what scenario would weighting by inverse variance increase bias without reducing overall error?

- Concept: Chain-of-thought rationale conditioning
  - Why needed here: Clarifies how improved intermediate reasoning steps affect downstream answer generation
  - Quick check question: If rationale quality improves by 1% but answer accuracy improves by 6%, what does this imply about the answer generation model's sensitivity?

## Architecture Onboarding

- Component map: T5 encoder-decoder backbone → Rationale generation (dropout sampling + voting) → Answer generation (dropout sampling + voting) → Final prediction (weighted logit combination)
- Critical path: Input → Dropout-based sampling → Rationale voting → Answer voting → Weighted logit combination → Output
- Design tradeoffs: Training phase voting vs inference phase voting (MC-CoT trains with voting but infers without)
- Failure signatures: Poor rationale voting → answer quality collapse; high variance logits → unstable weighted combination; dropout correlation → sampling redundancy
- First 3 experiments:
  1. Ablation: Remove rationale voting, keep answer voting - expect significant accuracy drop
  2. Ablation: Remove answer voting, keep rationale voting - expect moderate accuracy drop
  3. Hyperparameter sweep: α (0.0 to 1.0) in weighted logit combination - expect peak performance around 0.5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of rationales (Nr) and answers (Na) to generate for the voting mechanism to achieve the best balance between computational efficiency and accuracy?
- Basis in paper: [explicit] The paper mentions that the model generates multiple rationales and answers through sampling from the model Nr and Na times respectively, but does not specify an optimal number for these parameters
- Why unresolved: The paper does not provide a detailed analysis or experimentation to determine the optimal values for Nr and Na. The balance between computational cost and accuracy improvement needs further investigation
- What evidence would resolve it: A systematic study varying Nr and Na and measuring their impact on both accuracy and computational efficiency would provide insights into the optimal values

### Open Question 2
- Question: How does the MC-CoT approach perform on other multimodal reasoning tasks beyond visual question answering, such as multimodal sentiment analysis or multimodal document understanding?
- Basis in paper: [inferred] The paper focuses on visual question answering tasks and shows improvements in these areas. However, it does not explore the generalizability of the approach to other multimodal reasoning tasks
- Why unresolved: The current evaluation is limited to specific datasets (ScienceQA and A-OKVQA), and there is no evidence of the approach's effectiveness on other multimodal reasoning tasks
- What evidence would resolve it: Testing MC-CoT on a diverse set of multimodal reasoning tasks and datasets would demonstrate its generalizability and potential limitations

### Open Question 3
- Question: What is the impact of the dropout rate on the quality of generated rationales and the overall performance of the MC-CoT model?
- Basis in paper: [explicit] The paper mentions leveraging the inherent randomness introduced by dropout operations for generating diverse rationales, but does not explore the effect of varying dropout rates on performance
- Why unresolved: The relationship between dropout rate and the quality of rationales or model performance is not investigated, leaving the optimal dropout rate unknown
- What evidence would resolve it: Conducting experiments with different dropout rates and analyzing their impact on rationale quality and model accuracy would clarify the optimal dropout settings for MC-CoT

## Limitations

- Limited ablation studies to isolate contributions of rationale voting versus answer voting
- Theoretical foundation using Jensen's inequality not fully validated in multimodal reasoning context
- Claims about matching larger models need stronger statistical validation through multiple runs and variance reporting

## Confidence

- High confidence: Performance gains on ScienceQA and A-OKVQA datasets with MC-CoT training
- Medium confidence: Theoretical justification using Jensen's inequality for rationale voting mechanism
- Medium confidence: Weighted logit combination improving over simple averaging
- Low confidence: Claims about self-consistency reducing expected loss in multimodal reasoning

## Next Checks

1. **Ablation study**: Compare MC-CoT performance with and without rationale voting to quantify its contribution to accuracy gains
2. **Statistical validation**: Run MC-CoT training 5 times with different random seeds and report mean accuracy with standard deviation to establish statistical significance
3. **Mechanism verification**: Analyze correlation between rationale quality (RougeL scores) and answer accuracy to validate the claim that better rationales lead to better answers