---
ver: rpa2
title: 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding'
arxiv_id: '2309.08868'
source_url: https://arxiv.org/abs/2309.08868
tags:
- attention
- label
- multi-hop
- label-wise
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses automatic ICD coding for clinical notes, which
  involves assigning medical diagnosis codes to lengthy clinical text with a large
  number of possible labels (nearly 9,000). The main challenges are the long input
  sequences (up to 8,000 tokens) and the large label space with long-tail distribution.
---

# MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding

## Quick Facts
- arXiv ID: 2309.08868
- Source URL: https://arxiv.org/abs/2309.08868
- Reference count: 0
- Primary result: State-of-the-art performance on MIMIC datasets with fewer parameters using multi-hop label-wise attention

## Executive Summary
This paper addresses automatic ICD coding for clinical notes, which involves assigning medical diagnosis codes to lengthy clinical text with nearly 9,000 possible labels. The main challenges are the long input sequences (up to 8,000 tokens) and the large label space with long-tail distribution. The authors propose a Multi-Hop Label-wise Attention (MHLAT) model that uses a multi-hop label-wise attention mechanism to obtain more precise and informative representations. The model first chunks the input sequence into smaller segments, encodes them using a pre-trained language model (XLNet), and then performs multi-hop label-wise attention to extract label-specific features.

## Method Summary
The MHLAT model employs a three-stage architecture: chunking long clinical notes into fixed-length segments, encoding with XLNet using BitFit parameter-efficient fine-tuning, and applying multi-hop label-wise attention to refine label representations through iterative context-label interactions. The model processes extremely long sequences by dividing them into manageable chunks, uses BitFit to fine-tune only bias terms in the pre-trained XLNet, and performs sequential attention hops where label embeddings inform context representation and vice versa. This approach significantly reduces the number of trainable parameters while maintaining or improving performance compared to full fine-tuning methods.

## Key Results
- Achieves state-of-the-art performance on all seven metrics (macro F1, micro F1, macro AUC, micro AUC, Precision@5, Precision@8, Precision@15) across three benchmark MIMIC datasets
- Demonstrates significant parameter efficiency with BitFit fine-tuning compared to full fine-tuning methods
- Shows that multi-hop attention (2 hops) outperforms single-hop attention on most metrics, though 3 hops lead to overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-hop label-wise attention mimics human reading by refining label representations through iterative context-label interaction
- Mechanism: The model performs sequential attention hops where label embeddings inform context representation and vice versa, progressively refining both
- Core assumption: Information flows bidirectionally between context and labels, and multiple passes improve representation quality
- Evidence anchors:
  - [abstract] "multi-hop label-wise attention is deployed to get more precise and informative representations"
  - [section] "we propose a multi-hop label-wise attention mechanism to refine label representations and capture more informative features"
  - [corpus] Weak evidence - corpus doesn't contain specific details about multi-hop mechanisms
- Break condition: If attention weights become unstable or degenerate after multiple hops, or if model converges poorly with shared parameters

### Mechanism 2
- Claim: Bias-Terms Fine-Tuning (BitFit) achieves strong performance while tuning only a small subset of parameters
- Mechanism: Only bias terms in the pre-trained XLNet are fine-tuned during training, drastically reducing the number of trainable parameters
- Core assumption: Bias terms capture task-specific information sufficiently without full fine-tuning, avoiding catastrophic forgetting
- Evidence anchors:
  - [abstract] "we deploy Bias-Terms Fine-Tuning (BitFit) to reduce the number of parameters in PLMs to tune"
  - [section] "the BitFit algorithm [11] is deployed on XLNet backbone for parameter-efficient training"
  - [section] "our experiments reveal that BitFit achieves even better performance than fine-tuning"
- Break condition: If full fine-tuning becomes necessary for the specific medical domain or if BitFit underperforms on specialized tasks

### Mechanism 3
- Claim: Chunking long sequences enables efficient processing with standard pre-trained models
- Mechanism: Input sequences are divided into fixed-length chunks, processed independently, then concatenated to reconstruct global representation
- Core assumption: Local context within chunks preserves enough information for accurate coding, and concatenation recovers global dependencies
- Evidence anchors:
  - [section] "We propose the Chunk and Input layer to handle this problem instead" (referencing long input sequences)
  - [section] "For an extremely long sequence x = {x1, x2, ..., xn}, we limit the maximum length L for each chunk"
  - [corpus] No direct corpus evidence available
- Break condition: If critical information spans chunk boundaries or if concatenation loses important sequential dependencies

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: ICD coding assigns multiple diagnosis codes to a single clinical note
  - Quick check question: How does the model handle mutually exclusive vs. independent labels in multi-label classification?

- Concept: Attention mechanisms
  - Why needed here: Label-wise attention allows the model to focus on relevant text portions for each specific label
  - Quick check question: What's the difference between standard self-attention and label-wise attention in this context?

- Concept: Pre-trained language models and fine-tuning
  - Why needed here: XLNet provides strong language understanding that's adapted to the medical domain
  - Quick check question: What are the trade-offs between full fine-tuning vs. parameter-efficient methods like BitFit?

## Architecture Onboarding

- Component map: Chunk -> Encode -> Multi-hop Attention -> Classify
- Critical path: Chunk → Encode → Multi-hop Attention → Classify
- Design tradeoffs:
  - Memory vs. performance: Chunking enables handling long sequences but may lose cross-chunk dependencies
  - Parameter efficiency vs. expressiveness: BitFit reduces parameters but may limit adaptation
  - Hop count: More hops improve representation but increase training complexity and risk overfitting
- Failure signatures:
  - Poor performance on long-tail labels suggests attention mechanism isn't capturing rare code indicators
  - Degradation with shared parameters indicates model capacity is insufficient
  - Slow convergence with multiple hops suggests optimization difficulties
- First 3 experiments:
  1. Ablation study: Compare 1-hop vs. 2-hop vs. 3-hop attention to find optimal hop count
  2. Parameter efficiency test: Compare BitFit vs. full fine-tuning on same model architecture
  3. Chunk size sensitivity: Evaluate performance across different chunk lengths to find optimal balance between efficiency and information preservation

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several unresolved issues emerge from the analysis:

### Open Question 1
- Question: What is the optimal number of hops for the multi-hop label-wise attention mechanism across different dataset sizes and label distributions?
- Basis in paper: [explicit] The authors explore different hop configurations (1-3 hops) and find that 2 hops work best by default, but three hops lead to overfitting and slower convergence.
- Why unresolved: The optimal number of hops likely depends on dataset characteristics like size, label distribution, and text length. The paper only tests a limited range on three specific datasets.
- What evidence would resolve it: Systematic experiments varying hop numbers across diverse datasets with different sizes, label distributions, and text lengths to identify patterns or guidelines for hop selection.

### Open Question 2
- Question: How does the multi-hop attention mechanism specifically improve performance on rare or long-tail labels compared to single-pass attention?
- Basis in paper: [inferred] The authors note that their model performs worse on macro-F1 than some baselines, suggesting issues with rare labels, but don't explicitly analyze the mechanism's effect on tail labels.
- Why unresolved: The paper doesn't provide detailed analysis of how multi-hop attention affects different segments of the label distribution, particularly rare labels.
- What evidence would resolve it: Detailed ablation studies comparing performance on head vs. tail labels between single-pass and multi-hop attention, along with attention weight visualizations showing how rare labels benefit from multiple passes.

### Open Question 3
- Question: Can the BitFit technique be effectively combined with other parameter-efficient fine-tuning methods like adapters or LoRA for further performance gains?
- Basis in paper: [explicit] The authors demonstrate that BitFit outperforms both full fine-tuning and freezing the PLM, but don't explore combinations with other parameter-efficient methods.
- Why unresolved: The paper only tests BitFit in isolation against full fine-tuning and freezing, without exploring synergistic combinations with other PEFT methods.
- What evidence would resolve it: Experiments comparing BitFit alone vs. BitFit combined with adapters, LoRA, or other PEFT methods across multiple datasets and model sizes to quantify potential complementary benefits.

## Limitations

- Lack of detailed implementation specifics for the multi-hop label-wise attention mechanism makes faithful reproduction challenging
- Absence of ablation studies definitively proving the necessity of multi-hop attention versus single-hop alternatives
- Limited exploration of scenarios where full fine-tuning might be necessary despite BitFit's demonstrated effectiveness

## Confidence

- **High confidence**: The core architectural framework (chunking + XLNet + multi-hop attention + BitFit) is well-specified and the experimental results showing state-of-the-art performance across multiple metrics are robust and verifiable against the MIMIC datasets
- **Medium confidence**: The claimed benefits of multi-hop attention over single-hop alternatives are supported by results but lack detailed mechanistic explanation or ablation studies that would definitively prove their superiority
- **Medium confidence**: The BitFit parameter-efficient fine-tuning approach is well-established in the literature, and the paper's claim that it achieves better performance than full fine-tuning is supported, though domain-specific validation is limited

## Next Checks

1. **Hop Count Ablation Study**: Systematically compare MHLAT performance with 1-hop, 2-hop, and 3-hop configurations on the MIMIC-III full dataset to identify the optimal number of attention hops and determine whether multi-hop attention provides statistically significant improvements over single-hop alternatives

2. **Parameter Efficiency Comparison**: Conduct a controlled experiment comparing BitFit fine-tuning against full fine-tuning on the same MHLAT architecture, measuring both performance differences and parameter count reductions to quantify the trade-offs between parameter efficiency and model capacity

3. **Cross-Dataset Generalization Test**: Evaluate the trained MHLAT model on an external medical coding dataset (if available) or perform cross-validation across MIMIC-II and MIMIC-III subsets to assess whether the model's performance generalizes beyond its training distribution and whether the multi-hop mechanism provides consistent benefits across different data sources