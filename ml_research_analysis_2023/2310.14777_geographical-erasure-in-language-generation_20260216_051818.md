---
ver: rpa2
title: Geographical Erasure in Language Generation
arxiv_id: '2310.14777'
source_url: https://arxiv.org/abs/2310.14777
tags:
- erasure
- ptrue
- language
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies a form of geographical erasure in large language
  models, where certain countries are underpredicted in generated text relative to
  their actual English-speaking populations. The authors introduce a metric called
  ERr to quantify this underrepresentation, and show that erasure is consistent across
  a range of models and prompt phrasings.
---

# Geographical Erasure in Language Generation

## Quick Facts
- **arXiv ID**: 2310.14777
- **Source URL**: https://arxiv.org/abs/2310.14777
- **Reference count**: 36
- **Primary result**: Geographical erasure occurs in LLMs where countries are underpredicted relative to their English-speaking populations, correlating with low training corpus frequencies.

## Executive Summary
This work identifies a form of geographical erasure in large language models, where certain countries are underpredicted in generated text relative to their actual English-speaking populations. The authors introduce a metric called ERr to quantify this underrepresentation, and show that erasure is consistent across a range of models and prompt phrasings. They find that erasure strongly correlates with low frequencies of country mentions in the training corpus. To mitigate erasure, the authors propose supervised finetuning using a custom objective, which effectively reduces erasure while having only a small impact on generation quality as measured by perplexity.

## Method Summary
The authors analyze geographical erasure by prompting language models with various phrasings of "I live in" followed by country names. They compute model predictions, normalize over country candidates, and calculate an ERr metric that measures underrepresentation. The metric is based on weighted log-ratios comparing predicted distributions to ground truth English-speaking populations. To mitigate erasure, they propose supervised finetuning using ERr as a loss function, updating only bias terms while keeping dropout at 0. Training uses AdamW optimizer with learning rate 3e-5 for 5 epochs.

## Key Results
- ERr metric successfully quantifies geographical erasure, showing consistent underprediction of countries relative to their English-speaking populations
- Erasure strongly correlates with low frequencies of country mentions in the training corpus (FMR score of 0.517)
- Supervised finetuning with custom ERr objective effectively reduces erasure while only increasing perplexity by approximately 5%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Geographical erasure occurs because the frequency of country mentions in the training corpus directly influences model predictions.
- **Mechanism**: The model learns to predict country names based on their prevalence in the training data. Countries mentioned less frequently are underpredicted relative to their actual English-speaking populations.
- **Core assumption**: The model's predictions are primarily driven by the unigram frequency distribution of country mentions in the training data.
- **Evidence anchors**:
  - [abstract]: "We discover that erasure strongly correlates with low frequencies of country mentions in the training corpus."
  - [section]: "The high correlation between data bias and model bias suggests the composition of training data is a key source of erasure in the investigated LLMs." (Section 4.4)
  - [corpus]: The corpus analysis shows a high FMR score (0.517) and direct comparison of model predictions to training data frequencies in Figure 4.
- **Break condition**: If the model architecture introduces mechanisms that actively counteract frequency-based predictions (e.g., through explicit debiasing layers or loss functions), this mechanism would be mitigated.

### Mechanism 2
- **Claim**: Erasure is consistent across different model sizes because even smaller models closely mimic the frequency distribution in the training corpus.
- **Mechanism**: All model sizes, from small to large, learn similar frequency-based patterns from the training data, leading to consistent erasure patterns regardless of model scale.
- **Core assumption**: Model size does not significantly impact the ability to capture frequency-based biases in the training data.
- **Evidence anchors**:
  - [abstract]: "We find that erasure strongly correlates with low frequencies of country mentions in the training corpus."
  - [section]: "Like Rae et al. (2021), we do not find model size to have a big impact." (Section 4.3)
  - [corpus]: The corpus analysis includes models of varying sizes (GPT2-small to GPT-NeoX-20B) showing consistent erasure patterns.
- **Break condition**: If larger models develop more sophisticated reasoning capabilities that allow them to override frequency-based biases, this mechanism would be less effective for larger models.

### Mechanism 3
- **Claim**: Supervised finetuning with a custom ERr loss function can effectively mitigate geographical erasure while maintaining generation quality.
- **Mechanism**: By directly optimizing the model to reduce the ERr loss, which measures the discrepancy between predicted and ground truth country distributions, the model learns to assign more accurate probabilities to underrepresented countries.
- **Core assumption**: The ERr loss function is differentiable and can be effectively optimized through gradient descent.
- **Evidence anchors**:
  - [abstract]: "Lastly, we mitigate erasure by finetuning using a custom objective."
  - [section]: "We use the AdamW (Loshchilov and Hutter, 2017) optimiser with learning rate 3e âˆ’ 5 and train for an additional 5 epochs..." (Section 4.5)
  - [corpus]: The corpus analysis includes perplexity measurements showing only a small impact on generation quality (approximately 5% increase).
- **Break condition**: If the finetuning process overfits to the training prompts or if the ERr loss function is not properly formulated, the mitigation may not generalize or may degrade generation quality.

## Foundational Learning

- **Concept**: KL-divergence and its relationship to ERr
  - Why needed here: Understanding the mathematical foundation of the ERr metric and its connection to a well-defined divergence measure is crucial for both measuring and mitigating erasure.
  - Quick check question: How does the ERr metric relate to the KL-divergence, and why is this relationship important for the finetuning process?

- **Concept**: Prompt rephrasing and marginal distributions
  - Why needed here: The analysis requires aggregating model predictions across different prompt phrasings to obtain a more general measure of erasure that is not dependent on a specific prompt.
  - Quick check question: Why is it important to consider multiple prompt phrasings when measuring geographical erasure, and how does this relate to the concept of marginal distributions?

- **Concept**: Supervised finetuning and parameter-efficient methods
  - Why needed here: The mitigation strategy involves updating only the bias terms of the model to reduce erasure, which requires understanding the principles of parameter-efficient finetuning.
  - Quick check question: What are the advantages of updating only the bias terms during finetuning, and how does this approach balance mitigation effectiveness with maintaining generation quality?

## Architecture Onboarding

- **Component map**: Language model (GPT2, GPT-Neo, GPT-NeoX, OpenLLaMA) -> Prompt generator -> Model prediction -> ERr metric calculation -> Analysis/Fine-tuning
- **Critical path**: The critical path for measuring erasure is: prompt generation -> model prediction -> ERr calculation -> analysis. For mitigation, the critical path is: prompt generation -> model prediction -> ERr loss calculation -> gradient update -> model update.
- **Design tradeoffs**: The tradeoff between mitigation effectiveness and generation quality is a key consideration. More aggressive mitigation (e.g., ER0 instead of ER3) may lead to larger drops in perplexity. Another tradeoff is between the granularity of the ERr metric (choice of r) and the computational cost of optimization.
- **Failure signatures**: If the model predictions do not closely follow the frequency distribution of country mentions in the training data, the correlation between data bias and model bias may be weaker. If the finetuning process overfits to the training prompts, the mitigation may not generalize to new prompts.
- **First 3 experiments**:
  1. Verify that the model predictions closely follow the frequency distribution of country mentions in the training corpus by comparing p(xi|c) to ptrain(xi) for countries in the erasure set.
  2. Measure the ERr metric for different values of r (e.g., r=1, r=3, r=5) to understand the sensitivity of the metric and choose an appropriate value for finetuning.
  3. Perform finetuning with the ER3 loss function and evaluate the mitigation effectiveness by comparing the ER3 metric before and after finetuning on both training and test prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the training data bias directly cause geographical erasure, or are there other contributing factors like model architecture?
- Basis in paper: [explicit] "we find that erasure score of the training data compared to ground truth, ERr(ptrue, ptrain), is itself 0.46, which closely matches the erasure for models trained on this data. The high correlation between data bias and model bias suggests the composition of training data is a key source of erasure in the investigated LLMs."
- Why unresolved: The authors acknowledge they haven't experimentally established causality, only correlation between data bias and model bias. Model architecture and training paradigm could also play a role.
- What evidence would resolve it: Controlled experiments varying training data while keeping model architecture constant, and vice versa, to isolate the impact of each factor on geographical erasure.

### Open Question 2
- Question: How does geographical erasure manifest in the actual generated text output, beyond just the probability distributions?
- Basis in paper: [inferred] The authors work directly on p(xi|c) instead of generated text, noting this is common practice but acknowledging that sampling/decoding strategies can further amplify erasure.
- Why unresolved: The paper focuses on measuring erasure in the model's probability distributions rather than analyzing the final generated text, which could be more impactful.
- What evidence would resolve it: Empirical studies comparing the frequency of country mentions in generated text versus ground truth, across different sampling/decoding strategies.

### Open Question 3
- Question: What is the optimal value of r for defining the erasure set in different contexts or applications?
- Basis in paper: [explicit] The authors choose r=3 based on a mathematical heuristic that ERr approximates KL-divergence, but note this could vary based on legal or ethical constraints.
- Why unresolved: The choice of r is somewhat arbitrary and context-dependent. Different applications or domains might warrant different thresholds for defining erasure.
- What evidence would resolve it: Empirical studies correlating different r values with real-world harms or benefits, or legal/ethical guidelines that specify acceptable thresholds for representational harms.

## Limitations
- ERr metric depends heavily on the choice of country name variants and prompt phrasings
- Correlation between training data frequency and model predictions leaves considerable unexplained variance
- Mitigation approach evaluated on limited prompts may not generalize to more diverse geographical contexts

## Confidence
- **High**: Core observation that geographical erasure exists and correlates with training data frequency
- **Medium**: ERr metric as a comprehensive measure of erasure given its dependence on threshold selection
- **Low**: Long-term effectiveness of bias-only finetuning approach due to limited evaluation scope

## Next Checks
1. Test ERr metric sensitivity by systematically varying r from 1 to 10 and analyzing how erasure rankings change across different prompt phrasings
2. Evaluate finetuned models on geographically diverse prompts beyond Wikitext-2-v1 to assess transfer to broader contexts
3. Analyze training dynamics of finetuning by monitoring ERr and perplexity across epochs to identify potential overfitting