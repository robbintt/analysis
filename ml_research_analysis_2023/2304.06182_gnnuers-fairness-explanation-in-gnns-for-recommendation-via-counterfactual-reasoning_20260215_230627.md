---
ver: rpa2
title: 'GNNUERS: Fairness Explanation in GNNs for Recommendation via Counterfactual
  Reasoning'
arxiv_id: '2304.06182'
source_url: https://arxiv.org/abs/2304.06182
tags:
- gnnuers
- user
- edges
- unfairness
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GNNUERS introduces a counterfactual approach to explain unfairness
  in GNN-based recommender systems by identifying the user-item interactions responsible
  for recommendation utility disparities. The method perturbs a bipartite graph to
  minimize the gap in utility between protected and unprotected demographic groups
  while maintaining minimal changes to the original graph.
---

# GNNUERS: Fairness Explanation in GNNs for Recommendation via Counterfactual Reasoning

## Quick Facts
- arXiv ID: 2304.06182
- Source URL: https://arxiv.org/abs/2304.06182
- Reference count: 40
- Key outcome: GNNUERS reduces recommendation utility disparities between demographic groups by perturbing only 1% of edges.

## Executive Summary
GNNUERS introduces a counterfactual approach to explain unfairness in GNN-based recommender systems by identifying the user-item interactions responsible for recommendation utility disparities. The method perturbs a bipartite graph to minimize the gap in utility between protected and unprotected demographic groups while maintaining minimal changes to the original graph. Experiments on four real-world datasets with three GNN-based models demonstrate that GNNUERS significantly reduces utility disparities, with improvements in fairness (ΔNDCG) achieved by perturbing only 1% of edges.

## Method Summary
GNNUERS is a counterfactual reasoning framework that explains unfairness in GNN-based recommender systems by identifying and perturbing user-item edges responsible for utility disparities between demographic groups. The method uses a binary perturbation vector to selectively delete edges, optimizing a combined loss function that balances fairness (L_fair) and perturbation magnitude (L_dist). The optimization process includes gradient deactivation for the protected group to focus on reducing the utility gap. The approach is evaluated on four real-world datasets (MovieLens 1M, FENG, LFM-1K, INS) using three GNN models (GCMC, NGCF, LightGCN).

## Key Results
- GNNUERS reduces utility disparities between demographic groups by perturbing only 1% of edges.
- The edge deletion process is guided by user node properties (sparsity, reachability) and item node degree.
- NDCG decreases more for the unprotected group, with minimal loss for the protected group.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing user-item edges based on demographic group utility disparities reduces unfairness while preserving recommendation utility.
- Mechanism: The GNNUERS algorithm learns a perturbation vector that selectively deletes edges to minimize the NDCG disparity between protected and unprotected groups, guided by a combined loss of fairness (L_fair) and perturbation magnitude (L_dist).
- Core assumption: Unfairness in recommendations stems from specific user-item interactions that systematically favor one demographic group over another.
- Evidence anchors:
  - [abstract]: "GNNUERS introduces a counterfactual approach to explain unfairness in GNN-based recommender systems by identifying the user-item interactions responsible for recommendation utility disparities."
  - [section 4.3]: Defines L_fair as the mean of absolute pair-wise utility differences across demographic groups, optimized via approximated NDCG loss.
  - [corpus]: No direct evidence; corpus shows related work on fairness explanations but not this specific perturbation-based approach.
- Break condition: If the utility gap is not driven by a small set of influential edges, or if the approximated NDCG loss fails to accurately capture utility changes.

### Mechanism 2
- Claim: The edge deletion process is guided by user node properties (sparsity, reachability) and item node degree, targeting popularity bias and network connectivity patterns.
- Mechanism: After perturbations, KL divergence analysis shows the number of deleted edges correlates with user sparsity (SP), user reachability (RB), and item degree (DEG), indicating these properties drive edge selection.
- Core assumption: Unfairness arises when users with high reachability and low sparsity interact with high-degree (popular) items, creating systematic recommendation advantages.
- Evidence anchors:
  - [section 5.7]: "GNNUERS edges perturbation process can be described by the dependency from user nodes SP, RB, and item nodes DEG."
  - [section 4.5]: Defines SP as the tendency to interact with niche items and RB as closeness to other nodes of the same type.
  - [corpus]: No direct evidence; corpus neighbors discuss fairness in GNNs but not this specific dependency analysis.
- Break condition: If the unfairness is not rooted in popularity bias or network structure, or if the KL divergence measure does not capture the true dependency.

### Mechanism 3
- Claim: Gradient deactivation on the protected group ensures the perturbation focuses on reducing the utility gap rather than improving protected group utility.
- Mechanism: During backpropagation, the gradient for the protected group is deactivated, so the perturbation vector is updated only from the unprotected group's viewpoint, aiming to delete edges that create the disparity.
- Core assumption: Reducing the utility of the unprotected group while minimally affecting the protected group will close the fairness gap.
- Evidence anchors:
  - [section 4.4]: "Deactivating the gradient does not limit the group of edges that can be perturbed because the optimization does not involve only the user nodes, but also the item ones."
  - [section 5.6]: Confirms that NDCG decreases more for the unprotected group, with minimal loss for the protected group.
  - [corpus]: No direct evidence; corpus neighbors do not discuss gradient deactivation for fairness.
- Break condition: If the protected group's utility is inadvertently harmed, or if the gradient deactivation does not effectively target the disparity-causing edges.

## Foundational Learning

- Concept: Bipartite graph representation of recommender systems.
  - Why needed here: GNNUERS operates on user-item interaction graphs; understanding the bipartite structure is essential for grasping how edges are perturbed and how utility is measured.
  - Quick check question: In a bipartite graph for recommendations, can there be edges between two user nodes or two item nodes?
- Concept: Graph neural networks (GNNs) for collaborative filtering.
  - Why needed here: GNNUERS is applied to GNN-based recommender systems; understanding how GNNs propagate embeddings and make predictions is crucial for understanding the perturbation impact.
  - Quick check question: How does a GNN-based recommender system use the adjacency matrix to predict user-item relevance?
- Concept: Counterfactual reasoning in machine learning.
  - Why needed here: GNNUERS uses counterfactual explanations to identify the minimal edge perturbations that change the fairness outcome; understanding counterfactual reasoning is key to grasping the method's goal.
  - Quick check question: What is the difference between an explanation that identifies important features and a counterfactual explanation that identifies minimal changes to alter an outcome?

## Architecture Onboarding

- Component map: Input bipartite graph (user-item interactions) → GNN-based recommender (e.g., GCMC, NGCF, LightGCN) → GNNUERS perturbation module (loss function: L_fair + L_dist, gradient deactivation) → Perturbed graph → Recommender inference → Utility and fairness metrics (NDCG, ΔNDCG)
- Critical path: Adjacency matrix → GNN prediction → Utility calculation per group → Loss computation (L_fair, L_dist) → Gradient update (with protected group gradient deactivated) → Perturbation vector update → Edge deletion → Repeat until convergence
- Design tradeoffs: Using a binary perturbation vector (0 or 1) simplifies the model but may lose nuance; approximating NDCG loss enables differentiability but may not perfectly capture utility; gradient deactivation focuses perturbation but may miss some relevant edges.
- Failure signatures: If ΔNDCG does not decrease after many iterations, the perturbation may not be targeting the right edges; if NDCG drops significantly for the protected group, the gradient deactivation may not be working as intended; if the number of perturbed edges is very high, the perturbation may be too aggressive.
- First 3 experiments:
  1. Run GNNUERS on a small, synthetic bipartite graph with known unfair edges and verify that the perturbed graph reduces the utility gap.
  2. Apply GNNUERS to a real dataset (e.g., MovieLens) with gender-based groups and measure the change in ΔNDCG and NDCG for each group.
  3. Analyze the KL divergence between the number of deleted edges and user/item node properties to confirm the dependency pattern described in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GNNUERS performance scale with larger datasets beyond the current evaluation scope?
- Basis in paper: [inferred] The authors acknowledge that working with GNNs requires high computational resources and mention Last.FM 2B and BookCrossing as potential datasets for future evaluation, but these were not included in the current study.
- Why unresolved: The evaluation was limited to four datasets of varying sizes, with the largest (ML-1M) still relatively small compared to modern recommendation systems. The computational constraints mentioned by the authors prevented testing on larger datasets.
- What evidence would resolve it: Experimental results showing GNNUERS performance and fairness improvements on significantly larger datasets (millions of users and items) with varying sparsity levels.

### Open Question 2
- Question: How effective would GNNUERS be when extended to handle multiple demographic groups and sensitive attributes simultaneously?
- Basis in paper: [explicit] The authors state "GNNUERS could be extended to generate unfairness explanations for a higher number of demographic groups and sensitive attributes at once" and acknowledge this as a future work direction.
- Why unresolved: The current implementation focuses on binary settings with two demographic groups and two sensitive attributes (gender and age). The effectiveness of handling more complex, multi-dimensional fairness scenarios remains untested.
- What evidence would resolve it: Experimental results demonstrating GNNUERS performance on datasets with multiple sensitive attributes (e.g., gender, age, and location) and more than two demographic groups per attribute.

### Open Question 3
- Question: What is the trade-off between fairness improvement and recommendation utility degradation when applying GNNUERS in real-world scenarios?
- Basis in paper: [explicit] The authors note that while GNNUERS reduces utility for the unprotected group, it also affects the protected group utility in most experiments, and they mention that "GNNUERS learning process could be stopped once a desired level of fairness or utility is reached."
- Why unresolved: The experiments show varying levels of utility loss across different models and datasets, but the optimal stopping criteria and real-world impact of these trade-offs were not explored. The authors suggest this depends on application requirements but don't provide concrete guidelines.
- What evidence would resolve it: A systematic analysis of the fairness-utility trade-off across multiple real-world scenarios, including user studies to evaluate the perceived quality of recommendations after GNNUERS application and guidelines for balancing fairness and utility based on application context.

## Limitations
- The method relies on approximating NDCG loss for differentiability, which may not perfectly capture true utility changes.
- The gradient deactivation mechanism could potentially overlook some relevant edges or inadvertently harm the protected group's utility.
- The binary perturbation vector (0 or 1) may oversimplify the complex nature of edge importance in recommendation fairness.

## Confidence
- High Confidence: The core mechanism of using counterfactual reasoning to identify edges responsible for fairness disparities is well-established and theoretically sound.
- Medium Confidence: The experimental results showing significant reduction in ΔNDCG and the correlation between edge deletion and user/item properties are promising but require further validation on diverse datasets and models.
- Low Confidence: The long-term impact of edge deletions on recommendation quality and the potential for unintended consequences on user experience are not fully explored.

## Next Checks
1. Conduct ablation studies to isolate the impact of each component (gradient deactivation, binary perturbation, NDCG approximation) on fairness improvement and recommendation utility.
2. Test GNNUERS on a wider range of datasets with different demographic attributes and recommendation models to assess generalizability.
3. Perform a user study to evaluate the perceived fairness and recommendation quality after applying GNNUERS, ensuring that the method's benefits outweigh any potential drawbacks.