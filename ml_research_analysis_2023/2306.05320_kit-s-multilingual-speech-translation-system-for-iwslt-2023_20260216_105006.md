---
ver: rpa2
title: KIT's Multilingual Speech Translation System for IWSLT 2023
arxiv_id: '2306.05320'
source_url: https://arxiv.org/abs/2306.05320
tags:
- data
- translation
- speech
- table
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper describes KIT\u2019s system for the IWSLT 2023 multilingual\
  \ speech translation task, which focused on translating scientific conference talks\
  \ in English into 10 languages. The task posed challenges including accented input\
  \ speech, domain-specific terminology, and varied recording conditions."
---

# KIT's Multilingual Speech Translation System for IWSLT 2023

## Quick Facts
- **arXiv ID**: 2306.05320
- **Source URL**: https://arxiv.org/abs/2306.05320
- **Reference count**: 22
- **Primary result**: +0.8 BLEU average improvement using kNN-MT for domain adaptation in scientific speech translation

## Executive Summary
This paper describes KIT's system for the IWSLT 2023 multilingual speech translation task, which focused on translating scientific conference talks in English into 10 languages. The task posed challenges including accented input speech, domain-specific terminology, and varied recording conditions. In absence of training data from the target domain, the team used retrieval-based adaptation (kNN-MT) to effectively adapt to the scientific talk domain, achieving an average improvement of +0.8 BLEU across all target languages. The system also employed adapters to enable incremental learning from data augmentation, achieving performance on par with full retraining. Their cascaded speech translation system substantially outperformed its end-to-end counterpart on scientific talks, though their performance was similar on TED talks.

## Method Summary
The system uses WavLM for audio encoding, mBART50 for machine translation, and DeltaLM for casing/punctuation restoration. For domain adaptation to scientific talks, kNN-MT is employed at inference time by interpolating retrieved target token distributions from a datastore of domain-specific sentence pairs with the base MT model's predictions. Adapters are added after each encoder and decoder layer to enable efficient incremental learning from augmented data without full model retraining. The cascaded approach separates ASR and MT components, allowing targeted adaptation to the scientific domain, while the end-to-end system trains WavLM and mBART50 jointly. Data diversification techniques are used to improve robustness to accented speech.

## Key Results
- kNN-MT achieved an average +0.8 BLEU improvement across all target languages for speech translation
- Adapters enabled incremental learning that matched the performance of full retraining on augmented data
- Cascaded systems substantially outperformed end-to-end systems on scientific talk translation (though similar on TED talks)
- Ensembling models trained with and without TTS data improved performance by +0.7 BLEU for ACL and +0.4 for TED on average

## Why This Works (Mechanism)

### Mechanism 1: kNN-MT Domain Adaptation
- Claim: kNN-MT enables effective domain adaptation with minimal domain-specific data by interpolating retrieved target token distributions with the base MT model's predictions
- Mechanism: At inference time, the model retrieves k nearest neighbor target tokens from a datastore built from domain-specific sentence pairs. These retrieved tokens are combined with the original model's output distribution using a weighted interpolation, biasing the model toward domain-appropriate vocabulary and phrasing
- Core assumption: The decoder hidden states from domain-specific parallel sentences are sufficiently representative of the target domain's translation patterns to guide generation
- Evidence anchors:
  - [abstract]: "In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation)"
  - [section]: "Comparing the inference speed of system (4) and (5), with the same batch size of 64 sentences4, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory"
  - [corpus]: Weak - corpus neighbors don't directly address kNN-MT effectiveness
- Break condition: If the datastore contains insufficient or unrepresentative examples, the retrieved tokens won't meaningfully shift the distribution toward the target domain

### Mechanism 2: Adapter-Based Incremental Learning
- Claim: Adapters enable efficient incremental learning by adding small task-specific parameters while freezing the base model
- Mechanism: Adapter modules are inserted after each encoder and decoder layer. These modules are trained on new data while keeping the pretrained parameters frozen, allowing the model to adapt to new domains or data without full retraining
- Core assumption: The base model has learned general representations that can be effectively specialized through small adapter modules without catastrophic forgetting
- Evidence anchors:
  - [abstract]: "We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training"
  - [section]: "Empirically, we show it matches the performance of re-training on all new data"
  - [corpus]: Weak - corpus neighbors don't directly address adapter effectiveness
- Break condition: If the adapter modules are too small to capture domain-specific nuances, or if the base model's representations are too far from the target domain, performance will degrade

### Mechanism 3: Cascaded System Advantage
- Claim: Cascaded systems outperform end-to-end systems on scientific talks due to their modular architecture allowing domain-specific adaptation of individual components
- Mechanism: The cascaded approach separates ASR and MT, enabling targeted adaptation of each component to the scientific domain. The ASR can be adapted to accented speech and technical terminology, while the MT can be adapted to domain-specific translation patterns
- Core assumption: The benefits of independently adapting ASR and MT components outweigh the potential information loss from not having an end-to-end training signal
- Evidence anchors:
  - [abstract]: "Our cascaded speech system substantially outperforms its end-to-end counterpart on scientific talk translation"
  - [section]: "We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules"
  - [corpus]: Weak - corpus neighbors don't directly address cascaded vs end-to-end performance
- Break condition: If the ASR outputs are too noisy or the MT component cannot recover from ASR errors, the cascaded system's performance will degrade relative to end-to-end approaches

## Foundational Learning

- **Domain adaptation through retrieval-based methods**
  - Why needed here: The task involves translating scientific conference talks with domain-specific terminology and accented speech, for which there is no training data available
  - Quick check question: How does kNN-MT use domain-specific data at inference time without requiring model retraining?

- **Parameter-efficient fine-tuning with adapters**
  - Why needed here: The need to incorporate augmented data (data diversification) efficiently without the computational cost of full retraining
  - Quick check question: What are the trade-offs between adapter size and adaptation effectiveness in multilingual settings?

- **Multilinguality in speech translation**
  - Why needed here: The task requires translating into 10 languages of varying resource levels, making it important to understand how multilingual models perform in X-to-many scenarios
  - Quick check question: How does multilinguality impact translation quality differently for high-resource vs low-resource languages in speech translation?

## Architecture Onboarding

- **Component map**: WavLM audio encoder → mBART50 decoder (end-to-end) or WavLM + mBART50 (cascaded ASR) → DeltaLM-based MT → optional kNN-MT adaptation → output
- **Critical path**: Speech input → WavLM → (ASR output) → (casing/punctuation restoration) → DeltaLM MT → (kNN-MT) → translation output
- **Design tradeoffs**: Cascaded systems allow modular adaptation but introduce error propagation; end-to-end systems maintain end-to-end supervision but are harder to adapt to specific domains
- **Failure signatures**: kNN-MT degradation (insufficient domain coverage), adapter underfitting (too small adapters), cascaded performance drop (ASR quality issues)
- **First 3 experiments**:
  1. Train cascaded system without kNN-MT or adapters on ACL dev set, measure baseline BLEU
  2. Add kNN-MT with datastore from ACL dev bitext, measure adaptation gain
  3. Add adapters for incremental learning on diversified data, compare to full retraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size and composition of the datastore for kNN-MT adaptation in speech translation?
- Basis in paper: [explicit] The paper states "Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets" and "To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj̸=i ∈ [n] talks' bitext as datastore, where n is the total number of talks."
- Why unresolved: The paper uses a specific approach to avoid overfitting to oracle targets, but doesn't explore different datastore sizes or compositions to find the optimal configuration for kNN-MT adaptation.
- What evidence would resolve it: Systematic experiments varying the number of talks included in the datastore and measuring the resulting BLEU scores across different test sets.

### Open Question 2
- Question: Why does ensembling models trained with and without TTS data improve performance, despite having identical architecture?
- Basis in paper: [explicit] The paper notes "However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture."
- Why unresolved: The paper observes this improvement but doesn't investigate the underlying reasons for why the representations learned by these models are sufficiently different to benefit from ensembling.
- What evidence would resolve it: Analysis of the learned representations (e.g., through probing tasks or similarity metrics) to identify what aspects differ between models trained with and without TTS data.

### Open Question 3
- Question: How does multilingual pretraining impact speech translation quality for X-to-many translation compared to bilingual models?
- Basis in paper: [explicit] The paper states "In contrast, for X-to-many translation, it remains unclear whether incorporating more target languages improves translation quality" and shows preliminary results comparing multilingual vs bilingual models.
- Why unresolved: The paper provides initial comparisons showing multilingual models lag behind bilingual models on higher-resource languages, but doesn't conduct comprehensive experiments across different resource levels and language pairs.
- What evidence would resolve it: Large-scale experiments comparing multilingual and bilingual models across a wide range of language pairs with varying resource levels, controlling for other factors like model size and training data.

## Limitations

- The kNN-MT implementation details (k values, interpolation weights) are not fully specified, limiting reproducibility and assessment of generalizability
- The adapter-based incremental learning claim lacks ablation studies showing the minimum adapter size needed for parity with full retraining
- The cascaded vs. end-to-end comparison is limited to only two domains (scientific talks and TED talks), which may not generalize to other speech types
- The paper doesn't report inference latency comparisons between systems, despite mentioning that kNN-MT increases inference time by 50%

## Confidence

- **High confidence**: The cascaded system outperforms end-to-end on scientific talks (supported by IWSLT 2023 evaluation results)
- **Medium confidence**: kNN-MT provides +0.8 BLEU average improvement (result reported but implementation details unclear)
- **Medium confidence**: Adapters match full retraining performance (claim supported but lacks detailed ablation)
- **Low confidence**: Generalizability to other domains/languages (only tested on IWSLT 2023 task with specific languages)

## Next Checks

1. **kNN-MT sensitivity analysis**: Test different k values (1, 5, 10, 20) and interpolation weights (w=0.1, 0.3, 0.5) on the ACL dev set to determine optimal hyperparameters and assess robustness to datastore size variations.

2. **Adapter ablation study**: Train adapter modules of varying sizes (0.1%, 1%, 3%, 5% of base model parameters) on incremental data and measure BLEU degradation when adapters are frozen versus when the full model is fine-tuned, to establish the minimum effective adapter size.

3. **Cross-domain robustness test**: Evaluate both cascaded and end-to-end systems on out-of-domain speech (e.g., conversational speech or news broadcasts) to verify whether the cascaded advantage on scientific talks extends to other speech types.