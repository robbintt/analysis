---
ver: rpa2
title: 'Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance'
arxiv_id: '2307.07636'
source_url: https://arxiv.org/abs/2307.07636
tags:
- explanations
- explanation
- dissenting
- predictions
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose using "dissenting explanations" to address
  overreliance on AI predictions. The idea is to show both the explanation for the
  AI's prediction and an explanation for a disagreeing model's prediction.
---

# Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance

## Quick Facts
- arXiv ID: 2307.07636
- Source URL: https://arxiv.org/abs/2307.07636
- Reference count: 5
- One-line primary result: Dissenting explanations - showing explanations for both an AI prediction and a disagreeing model's prediction - reduce human overreliance on AI without reducing accuracy.

## Executive Summary
This paper introduces "dissenting explanations" as a method to reduce human overreliance on AI predictions. The approach presents users with both the explanation for an AI's prediction and an explanation from a disagreeing model. In a pilot study on deceptive review classification, dissenting explanations successfully reduced overreliance compared to single explanations, without decreasing overall accuracy. The authors also present methods to generate model disagreement through regularization and reweighting approaches.

## Method Summary
The method trains two models (a reference model and a dissenting model) on the same dataset, then uses LIME to generate feature attribution explanations for both. Dissenting explanations are created by identifying examples where models disagree and presenting both explanations to users. The paper presents regularization and reweighting approaches to maximize predictive disagreement between models, as well as methods to retrain models on specific test instances to flip predictions.

## Key Results
- Dissenting explanations reduced human overreliance on AI predictions in a deceptive review classification task
- The intervention maintained overall accuracy compared to single explanations
- Models disagreed on approximately 10% of test examples, creating opportunities for dissenting explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dissenting explanations reduce overreliance by providing alternative arguments against an incorrect model prediction
- Core assumption: Users treat explanations as arguments supporting predictions rather than as verifiable proofs, and will engage with conflicting explanations
- Evidence anchors: Abstract mentions "dissenting explanations: conflicting predictions with accompanying explanations" and demonstrates reduction in overreliance

### Mechanism 2
- Claim: Dissenting explanations work by increasing the perceived uncertainty of the model's prediction
- Core assumption: Users interpret conflicting explanations as a signal of uncertainty in the model's prediction
- Evidence anchors: Abstract shows "dissenting explanations reduce overreliance on model predictions, without reducing overall accuracy"

### Mechanism 3
- Claim: Dissenting explanations reduce overreliance by correcting the model's mistakes
- Core assumption: The dissenting model often makes the correct prediction when the reference model is incorrect, and users can verify the features
- Evidence anchors: Section 4.2 shows models disagreed on about 10% of examples, with balance of correct/incorrect predictions

## Foundational Learning

- Concept: Feature attribution explanations
  - Why needed here: The paper uses LIME to generate feature attribution explanations that highlight important words supporting or opposing predictions
  - Quick check question: What are the two types of features highlighted in a feature attribution explanation for a binary classification task?

- Concept: Model multiplicity and the Rashomon set
  - Why needed here: The paper relies on multiple models with similar performance but different predictions
  - Quick check question: What is the Rashomon set, and why is it relevant to dissenting explanations?

- Concept: Human-AI collaboration and overreliance
  - Why needed here: The paper addresses the problem of human overreliance on AI predictions when AI is incorrect
  - Quick check question: What is overreliance in the context of human-AI decision-making, and why is it a problem?

## Architecture Onboarding

- Component map:
  - Reference model (f) -> Dissenting model (g) -> Explanation generator (LIME) -> User interface -> User study platform

- Critical path:
  1. Train reference and dissenting models on same dataset
  2. Generate feature attribution explanations for both models using LIME
  3. Identify examples where models disagree
  4. Present explanations to user in interface
  5. Collect user responses and evaluate effectiveness

- Design tradeoffs:
  - Accuracy vs. disagreement: Increasing disagreement may reduce overall accuracy
  - Explanation complexity: Multiple explanations increase cognitive load
  - Model selection: Choosing models that disagree sufficiently is crucial

- Failure signatures:
  - Low disagreement: Models rarely disagree, limiting dissenting explanation generation
  - User confusion: Conflicting explanations may cause users to default to random decisions
  - Model errors: If dissenting model is also incorrect, it may mislead users

- First 3 experiments:
  1. Train linear SVM and neural network on deceptive reviews dataset, generate LIME explanations, identify disagreements, measure correction rate
  2. Conduct user study comparing no explanation, single explanation, and dissenting explanations conditions, measuring accuracy and overreliance
  3. Vary disagreement levels by adjusting training (different architectures/regularization), measure impact on explanation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of dissenting explanations vary across different types of tasks or domains?
- Basis in paper: The study only examined deceptive review classification
- Why unresolved: Limited to single task, generalizability unknown
- What evidence would resolve it: Similar experiments across diverse tasks and domains

### Open Question 2
- Question: How do dissenting explanations impact human trust in AI systems compared to single explanations?
- Basis in paper: Trust was measured through self-reported scales but not deeply analyzed
- Why unresolved: Study focused on overreliance and accuracy, not trust as primary outcome
- What evidence would resolve it: Including trust as primary outcome in future experiments

### Open Question 3
- Question: Can dissenting explanations be generated effectively for complex, high-dimensional data like images or text beyond simple feature attribution?
- Basis in paper: Only demonstrated methods for text data using feature attribution
- Why unresolved: Techniques may not scale to more complex data types
- What evidence would resolve it: Developing and testing methods for other data types

### Open Question 4
- Question: What is the optimal number of dissenting explanations to present to users?
- Basis in paper: Only presented one dissenting explanation alongside original
- Why unresolved: Study did not explore varying number of dissenting explanations
- What evidence would resolve it: Experimenting with different numbers and measuring impact

## Limitations

- Limited to a single task domain (deceptive review classification), raising questions about generalizability
- Small user study sample size (15 participants per condition) may limit statistical power
- Does not explore long-term effects of dissenting explanations on user trust and reliance

## Confidence

- **High Confidence**: Core claim that dissenting explanations reduce overreliance is supported by user study results
- **Medium Confidence**: Mechanisms (alternative arguments, uncertainty, mistake correction) are plausible but need further validation
- **Low Confidence**: Generalizability to other domains and optimal methods for generating dissenting explanations need more evaluation

## Next Checks

1. **Cross-Domain Evaluation**: Conduct user studies in different high-stakes domains (medical diagnosis, financial risk assessment) to assess generalizability

2. **Longitudinal Study**: Design study examining long-term effects of dissenting explanations on user trust, reliance, and accuracy over multiple interactions

3. **Ablation Study on Mechanism**: Perform study to isolate and test each proposed mechanism by manipulating presentation of dissenting explanations and measuring individual contributions to reducing overreliance