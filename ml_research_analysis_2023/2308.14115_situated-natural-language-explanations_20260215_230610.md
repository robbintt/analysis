---
ver: rpa2
title: Situated Natural Language Explanations
arxiv_id: '2308.14115'
source_url: https://arxiv.org/abs/2308.14115
tags:
- abstract
- explanations
- detailed
- prompts
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating natural language
  explanations that adapt to different audience situations. While previous work focused
  on improving textual quality, it ignored that explanation effectiveness depends
  on audience needs and preferences.
---

# Situated Natural Language Explanations

## Quick Facts
- arXiv ID: 2308.14115
- Source URL: https://arxiv.org/abs/2308.14115
- Authors: 
- Reference count: 16
- Primary result: The situated NLE framework generates audience-aware explanations that improve human-AI communication through prompt engineering and multi-dimensional evaluation

## Executive Summary
This paper addresses the problem of generating natural language explanations that adapt to different audience situations. While previous work focused on improving textual quality, it ignored that explanation effectiveness depends on audience needs and preferences. The authors propose "situated natural language explanations" (NLEs) as a framework that considers audience context during both generation and evaluation.

They introduce situated prompts, a prompt engineering method that adapts explanations to situations by including hints, audience details, or problem specifics. To evaluate these situated NLEs, they define automated metrics across lexical (length, self-BLEU, TTR), semantic (BLEU, concreteness), and pragmatic (convincingness, acceptability) dimensions. Human studies show that annotators prefer NLEs generated using situated prompts over unsituated ones, especially when time constraints differ.

## Method Summary
The situated NLE framework combines prompt engineering with multi-dimensional evaluation. The method generates various prompt variations based on situational parameters, then uses different sized PLMs (OPT-175B, GPT-3, GPT-2 variants) to produce explanations. An automated evaluation framework computes metrics across lexical, semantic, and pragmatic dimensions, and a selection algorithm chooses optimal prompts based on situational priorities. The critical path follows: Prompt generation → PLM inference → Evaluation → Selection → Output.

## Key Results
- Human annotators significantly prefer situated NLEs over unsituated ones, especially under different time constraints
- Smaller models like GPT-2 can generate coherent explanations but score lower on convincingness and acceptability compared to larger models
- The automated evaluation framework effectively selects optimal prompts for each situation based on situational priorities
- Prompt engineering with situational hints successfully adapts explanation characteristics (length, concreteness) to match audience needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering can effectively adapt natural language explanations to different audience situations
- Mechanism: By including hints about desired properties, audience details, or problem specifics in prompts, PLMs generate explanations with systematically different characteristics (length, concreteness, convincingness) that better match situational needs
- Core assumption: PLMs can meaningfully interpret and respond to natural language cues about situational context
- Evidence anchors:
  - [abstract] "We propose situated prompts, a prompt engineering method that adapts explanations to situations by including hints, audience details, or problem specifics"
  - [section 4.3] "We can adapt the PLM-generated explanations to the situations through prompt engineering"
  - [corpus] Weak evidence - only 1 related paper mentions "situated" in the context of NLEs, suggesting this is a novel approach

### Mechanism 2
- Claim: Automated evaluation metrics across multiple dimensions can effectively select optimal prompts for specific situations
- Mechanism: The authors establish a multi-dimensional evaluation framework (lexical, semantic, pragmatic) where different metrics become more or less important depending on the situation, enabling systematic prompt selection
- Core assumption: The relationship between explanation properties and situational appropriateness can be quantified and optimized
- Evidence anchors:
  - [abstract] "To evaluate these situated NLEs, they define automated metrics across lexical (length, self-BLEU, TTR), semantic (BLEU, concreteness), and pragmatic (convincingness, acceptability) dimensions"
  - [section 6.1] "With proper choices of prompt construction methods, the generated NLEs can significantly differ from the unsituated NLEs in the desired directions"
  - [corpus] No direct evidence - this represents a novel multi-dimensional evaluation approach

### Mechanism 3
- Claim: Smaller PLMs can generate coherent explanations but struggle with pragmatic quality compared to larger models
- Mechanism: While GPT-2 models can produce syntactically correct and semantically relevant explanations, they consistently score lower on convincingness and acceptability metrics compared to OPT-175B and GPT-3
- Core assumption: Model size correlates with the ability to generate pragmatically appropriate explanations
- Evidence anchors:
  - [section 6.5] "GPT2-{base,medium,large} models are able to generate coherent and meaningful explanations... However, as shown in Table 10, their NLEs have less convincingness scores than the NLEs generated by OPT"
  - [section 6.4] "OPT has higher acceptability, while GPT-3 has higher convincingness and lexical richness"
  - [corpus] Weak evidence - no corpus data on smaller model performance in situated NLE context

## Foundational Learning

- Concept: Prompt engineering for controlled generation
  - Why needed here: The entire situated NLE framework relies on modifying prompts to influence explanation characteristics
  - Quick check question: If you want to generate shorter explanations for busy users, which prompt modification would you try first based on the paper's findings?

- Concept: Automated evaluation metrics for natural language generation
  - Why needed here: The paper establishes multiple metrics (length, self-BLEU, TTR, BLEU, concreteness, convincingness, acceptability) to evaluate and select explanations
  - Quick check question: Which metric would you prioritize if your goal is to generate explanations that are more diverse in vocabulary usage?

- Concept: Situational adaptation in human-AI communication
  - Why needed here: The core contribution is recognizing that explanation effectiveness depends on audience context and needs
  - Quick check question: Based on the paper's findings, what would be the key difference between an explanation for a detail-oriented vs. time-constrained user?

## Architecture Onboarding

- Component map: Prompt generation -> PLM inference -> Evaluation -> Selection -> Output
- Critical path: Prompt generation → PLM inference → Evaluation → Selection → Output
  - This sequence must complete for each problem instance and situation
- Design tradeoffs:
  - Model size vs. pragmatic quality: Larger models perform better but are more expensive to run
  - Prompt complexity vs. interpretability: More detailed prompts may be more effective but harder to understand
  - Number of evaluation metrics vs. computational cost: More metrics provide better selection but increase computation time
- Failure signatures:
  - Consistent low convincingness scores across all prompts may indicate the PLM cannot understand the domain
  - High self-BLEU scores suggest the model is generating repetitive or template-based explanations
  - Disconnection between automated metrics and human preferences suggests the evaluation framework needs refinement
- First 3 experiments:
  1. Generate explanations using the baseline (unsituated) prompt across all three PLM sizes to establish performance baselines
  2. Test the effect of adding "in short" or "on a high level" to prompts and measure changes in length and concreteness
  3. Compare automated metric rankings with human preference rankings on a small subset to validate the evaluation framework

## Open Questions the Paper Calls Out
- Open Question 1: How do different "dummy explanations" compare to the "it is what it is" baseline in terms of convincingness scores?
- Open Question 2: What is the optimal prompt selection strategy for generating situated NLEs when the desired property is unknown?
- Open Question 3: How do situated NLEs perform in more diverse and complex situations beyond the two simulated scenarios tested?

## Limitations
- The automated evaluation metrics show only moderate correlation with human preferences (κ ≈ 0.2-0.3)
- The framework relies heavily on prompt engineering, which may not generalize well across different domains or explanation types
- The study focuses on relatively simple commonsense and shopping scenarios, leaving uncertainty about performance on more complex technical or specialized domains

## Confidence

- **High Confidence**: The core mechanism of prompt engineering for adapting explanation characteristics to situational needs is well-supported by both automated and human evaluation results
- **Medium Confidence**: The multi-dimensional automated evaluation framework effectively captures important aspects of explanation quality, though the moderate human correlation suggests room for improvement
- **Medium Confidence**: The performance gap between larger and smaller models on pragmatic metrics is consistent, but the absolute performance levels and generalizability across domains remain uncertain

## Next Checks

1. Apply the situated NLE framework to a different domain (e.g., technical support or medical explanations) to test generalizability of prompt engineering techniques and automated metrics

2. Conduct an ablation study of evaluation metrics to determine which dimensions contribute most to human preference correlation and which may be redundant

3. Test long-term stability by generating explanations for the same situations across multiple time periods to assess consistency of both automated metrics and human preferences