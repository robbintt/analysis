---
ver: rpa2
title: 'PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation'
arxiv_id: '2308.12604'
source_url: https://arxiv.org/abs/2308.12604
tags:
- report
- diseases
- learning
- generation
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Automatic medical report generation is challenging due to the need
  for precise clinical understanding and disease identification, compounded by imbalanced
  disease distributions in training data. The authors propose diagnosis-driven prompts
  for medical report generation (PromptMRG), a framework that uses a disease classification
  branch to convert diagnostic results into token prompts to guide the report generation
  process.
---

# PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation

## Quick Facts
- **arXiv ID:** 2308.12604
- **Source URL:** https://arxiv.org/abs/2308.12604
- **Authors:** Multiple
- **Reference count:** 20
- **Key outcome:** Automatic medical report generation is challenging due to the need for precise clinical understanding and disease identification, compounded by imbalanced disease distributions in training data. The authors propose diagnosis-driven prompts for medical report generation (PromptMRG), a framework that uses a disease classification branch to convert diagnostic results into token prompts to guide the report generation process. They also introduce cross-modal feature enhancement to improve diagnostic accuracy by retrieving similar reports from a database using a pre-trained CLIP model, and self-adaptive disease-balanced learning to address imbalanced disease learning by applying an adaptive logit-adjusted loss. Experiments on two medical report generation benchmarks demonstrate state-of-the-art clinical efficacy performance, with F1 scores of 0.476 on MIMIC and 0.211 on IU X-Ray.

## Executive Summary
Automatic medical report generation is challenging due to the need for precise clinical understanding and disease identification, compounded by imbalanced disease distributions in training data. The authors propose diagnosis-driven prompts for medical report generation (PromptMRG), a framework that uses a disease classification branch to convert diagnostic results into token prompts to guide the report generation process. They also introduce cross-modal feature enhancement to improve diagnostic accuracy by retrieving similar reports from a database using a pre-trained CLIP model, and self-adaptive disease-balanced learning to address imbalanced disease learning by applying an adaptive logit-adjusted loss. Experiments on two medical report generation benchmarks demonstrate state-of-the-art clinical efficacy performance, with F1 scores of 0.476 on MIMIC and 0.211 on IU X-Ray.

## Method Summary
The PromptMRG framework combines diagnosis-driven prompts (DDP) with cross-modal feature enhancement (CFE) and self-adaptive disease-balanced learning (SDL) to improve medical report generation. A disease classification branch predicts 4-class disease states, which are converted to token prompts ([BLA], [POS], [NEG], [UNC]) and concatenated to the decoder's input. CLIP-based retrieval finds similar reports, whose features are dynamically aggregated with visual features before classification. SDL applies logit-adjusted loss based on individual disease learning status to address class imbalance. The model is trained end-to-end with LLM loss, classification loss, and SDL-adjusted loss.

## Key Results
- State-of-the-art clinical efficacy with F1 scores of 0.476 on MIMIC and 0.211 on IU X-Ray
- Cross-modal feature enhancement and self-adaptive disease-balanced learning effectively address imbalanced disease distributions
- Diagnosis-driven prompts improve clinical accuracy by explicitly injecting diagnostic information into the decoder's attention process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagnosis-driven prompts (DDP) improve clinical efficacy by explicitly injecting diagnostic information into the decoder's attention process.
- Mechanism: Diagnostic results from a disease classification branch are converted into token prompts ([BLA], [POS], [NEG], [UNC]) and concatenated to the decoder's input sequence. The decoder attends to both visual features and these prompts during generation, biasing word selection toward clinically accurate terminology.
- Core assumption: The text decoder can effectively leverage discrete token prompts as context when predicting each word, and the prompt embeddings are sufficiently discriminative to reflect disease states.
- Evidence anchors:
  - [abstract] "When generating reports, the diagnostic results from the classification branch are converted into token prompts to explicitly guide the generation process."
  - [section] "When predicting certain words, the attention on the relevant disease token is much larger than the remaining tokens, indicating that the token prompts indeed convey useful diagnostic information to the decoder during the generation process."
  - [corpus] Weak - no direct corpus evidence provided.
- Break condition: If the prompt embeddings are not distinct enough, or if the decoder's attention mechanism cannot prioritize prompts over other tokens, the guidance will fail and clinical efficacy gains will not materialize.

### Mechanism 2
- Claim: Cross-modal feature enhancement (CFE) improves diagnostic accuracy by retrieving semantically similar reports and aggregating their features with the query image's features.
- Mechanism: A CLIP model retrieves top-k report features from a database based on image similarity. These features are dynamically aggregated using self- and cross-attention, then concatenated with the visual feature before disease classification. This enriches the input representation with relevant clinical context from similar cases.
- Core assumption: Reports of similar images contain useful diagnostic cues that, when combined with the current image's features, improve classification performance. The CLIP model's cross-modal retrieval is accurate enough to find truly relevant cases.
- Evidence anchors:
  - [abstract] "cross-modal feature enhancement, which retrieves similar reports from the database to assist the diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP."
  - [section] "we also resort to the report database of training data to obtain more robust features for disease classification."
  - [corpus] Weak - no direct corpus evidence provided.
- Break condition: If retrieved reports are not diagnostically relevant, or if the aggregation mechanism cannot effectively fuse visual and text features, CFE will add noise and degrade classification performance.

### Mechanism 3
- Claim: Self-adaptive disease-balanced learning (SDL) addresses imbalanced disease learning by adjusting classification logits based on individual disease learning status.
- Mechanism: A logit-adjusted loss is applied to each disease's classification head, where the adjustment is based on the disease's average prediction score on a validation set. This encourages rarer or harder-to-learn diseases to receive larger gradients, improving their recall without hurting common diseases.
- Core assumption: The average prediction score is a reliable proxy for a disease's learning status, and logit adjustment can effectively rebalance the training signal without destabilizing optimization.
- Evidence anchors:
  - [abstract] "the disease imbalanced issue is addressed by applying an adaptive logit-adjusted loss to the classification branch based on the individual learning status of each disease."
  - [section] "we propose to utilize prediction scores as an assessment of the learning status for different diseases: a large score indicates a well-learned disease while a small one implies that the disease is not sufficiently learned."
  - [corpus] Weak - no direct corpus evidence provided.
- Break condition: If prediction scores do not correlate with actual learning needs, or if logit adjustment causes overfitting to rare classes, SDL will fail to improve balanced performance and may degrade overall accuracy.

## Foundational Learning

- Concept: Encoder-decoder architecture with attention mechanisms
  - Why needed here: The report generation task requires mapping variable-length image features to variable-length text sequences, which is naturally handled by encoder-decoder models. Attention allows the decoder to focus on relevant image regions and disease prompts during word prediction.
  - Quick check question: In the PromptMRG framework, what two sources of information does the decoder attend to when generating each word?

- Concept: Multi-task learning and auxiliary tasks
  - Why needed here: The disease classification branch serves as an auxiliary task that encourages the model to learn discriminative visual features, which are then leveraged via prompts for better clinical accuracy.
  - Quick check question: How does the disease classification branch contribute to the report generation process in PromptMRG?

- Concept: Imbalanced learning and class rebalancing strategies
  - Why needed here: Medical datasets often have skewed disease distributions, leading to poor performance on rare diseases. SDL explicitly rebalances learning per disease using logit adjustment based on learning status.
  - Quick check question: What metric does SDL use to assess each disease's learning status, and how is it used to adjust training?

## Architecture Onboarding

- Component map: Image encoder (ResNet-101) → visual features → CLIP model → cross-modal retrieval of report features → dynamic aggregation module → fused features → disease classification branch → token prompt generator → text decoder (BERT-base) → report

- Critical path: Image → Encoder → CFE → Classification → Prompts → Decoder → Report

- Design tradeoffs:
  - Using prompts vs. direct feature injection: Prompts are discrete and interpretable, but may limit flexibility compared to continuous feature vectors.
  - Fixed vs. adaptive logit adjustment: Adaptive adjustment accounts for individual disease learning status but adds complexity and hyperparameter tuning.

- Failure signatures:
  - Low clinical efficacy despite high language modeling performance → prompts not effective or classification branch weak.
  - Degraded performance on common diseases after SDL → overcorrection on rare diseases.
  - Unstable training → aggressive logit adjustment or poor CLIP retrieval quality.

- First 3 experiments:
  1. Train baseline encoder-decoder without DDP, CFE, or SDL; measure CE metrics to establish reference.
  2. Add DDP only; verify prompt attention weights are meaningful and CE improves.
  3. Add CFE only; check if retrieval improves classification features and CE metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the pre-trained CLIP model affect the performance of cross-modal feature enhancement in PromptMRG?
- Basis in paper: [explicit] The paper mentions using a CLIP model pre-trained on MIMIC training set for cross-modal retrieval.
- Why unresolved: The paper does not explore the impact of using different pre-trained CLIP models or training the CLIP model from scratch on the overall performance of the framework.
- What evidence would resolve it: Experiments comparing the performance of PromptMRG using different pre-trained CLIP models or training the CLIP model from scratch would provide insights into the importance of the choice of CLIP model.

### Open Question 2
- Question: What is the impact of the number of retrieved reports (k) on the performance of cross-modal feature enhancement in PromptMRG?
- Basis in paper: [explicit] The paper sets k = 21 for cross-modal feature enhancement and provides a sensitivity analysis of k in the appendix.
- Why unresolved: While the paper provides a sensitivity analysis, it does not explore the optimal value of k or the impact of using a larger or smaller number of retrieved reports on the performance of the framework.
- What evidence would resolve it: Experiments comparing the performance of PromptMRG using different values of k would provide insights into the optimal number of retrieved reports for cross-modal feature enhancement.

### Open Question 3
- Question: How does the proposed self-adaptive disease-balanced learning (SDL) compare to other methods for handling imbalanced disease distributions in medical report generation?
- Basis in paper: [explicit] The paper introduces SDL as a method to address the imbalanced learning among diseases by adaptively adjusting the learning objectives of each disease based on its unique learning status.
- Why unresolved: The paper does not compare SDL to other methods for handling imbalanced disease distributions, such as class weighting or oversampling techniques.
- What evidence would resolve it: Experiments comparing the performance of PromptMRG with SDL to other methods for handling imbalanced disease distributions would provide insights into the effectiveness of SDL in addressing this issue.

### Open Question 4
- Question: How does the proposed diagnosis-driven prompts (DDP) compare to other methods for incorporating diagnostic information into the report generation process?
- Basis in paper: [explicit] The paper introduces DDP as a method to guide the report generation process using diagnostic results from a disease classification branch.
- Why unresolved: The paper does not compare DDP to other methods for incorporating diagnostic information into the report generation process, such as multi-task learning or knowledge graph integration.
- What evidence would resolve it: Experiments comparing the performance of PromptMRG with DDP to other methods for incorporating diagnostic information into the report generation process would provide insights into the effectiveness of DDP in generating diagnostically accurate reports.

## Limitations

- Several critical implementation details are underspecified in the paper, creating uncertainty in faithful reproduction, including the CLIP-based cross-modal retrieval mechanism, dynamic aggregation module, and Vicuna-13B integration for auxiliary labeling.
- The evaluation approach introduces a layer of indirection by converting generated text to disease labels using CheXbert, which may not fully capture the quality of generated reports.
- The relatively low F1 scores (0.476 on MIMIC, 0.211 on IU X-Ray) suggest that significant room for enhancement remains in clinical accuracy despite the proposed improvements.

## Confidence

**High Confidence:** The core architectural framework (encoder-decoder with disease classification branch, DDP conversion, and BERT decoder) is clearly specified and follows established practices in medical report generation. The conceptual mechanisms for each component are well-articulated and theoretically sound.

**Medium Confidence:** The adaptive logit-adjusted loss for SDL is clearly described, but the practical implementation details and hyperparameter sensitivity are not fully explored. The paper mentions tuning λ=4 and k=21, but doesn't provide guidance on how these were selected or their stability across different runs.

**Low Confidence:** The cross-modal feature enhancement via CLIP retrieval and the dynamic aggregation module lack sufficient technical detail for exact reproduction. The integration of Vicuna-13B for auxiliary labeling is mentioned but not specified, creating significant uncertainty about the experimental conditions.

## Next Checks

1. **Diagnostic accuracy validation:** Implement the disease classification branch independently and measure its 4-class disease prediction accuracy on held-out validation data. This will verify whether the classification foundation for DDP generation is reliable before proceeding to full model training.

2. **Prompt effectiveness verification:** During early training iterations, monitor the attention weights between the decoder and disease token prompts ([BLA], [POS], [NEG], [UNC]). Confirm that attention scores show meaningful patterns where disease-relevant tokens receive higher attention during generation of disease-related words, validating Mechanism 1.

3. **Cross-modal retrieval quality assessment:** Evaluate the CLIP-based retrieval system's precision@k on a validation set by checking whether retrieved reports are semantically and diagnostically relevant to the query images. This will determine if CFE is providing useful information rather than noise, validating Mechanism 2 before full integration.