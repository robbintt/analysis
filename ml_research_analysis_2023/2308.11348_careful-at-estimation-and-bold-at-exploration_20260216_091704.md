---
ver: rpa2
title: Careful at Estimation and Bold at Exploration
arxiv_id: '2308.11348'
source_url: https://arxiv.org/abs/2308.11348
tags:
- policy
- exploration
- value
- function
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel exploration strategy for continuous
  action space reinforcement learning that addresses two key issues: aimless exploration
  and policy divergence. The core idea involves using a greedy Q softmax update scheme
  based on double Q functions, where exploration actions are sampled according to
  a weighted distribution derived from the maximum of two Q functions.'
---

# Careful at Estimation and Bold at Exploration

## Quick Facts
- arXiv ID: 2308.11348
- Source URL: https://arxiv.org/abs/2308.11348
- Reference count: 40
- Primary result: Achieved ~8k score on Mujoco Humanoid in 3M steps, outperforming state-of-the-art methods

## Executive Summary
This paper introduces Greedy Actor-Critic (GAC), a novel exploration strategy for continuous action space reinforcement learning that addresses aimless exploration and policy divergence. The method leverages double Q functions to construct both a greedy Q value (maximum of two Q functions) and a conservative Q value (minimum of two Q functions). Exploration actions are sampled according to a softmax-weighted distribution derived from the greedy Q, while the policy is learned by minimizing KL divergence to a conservative target. Experiments on Mujoco benchmarks demonstrate superior performance, particularly achieving state-of-the-art results on the complex Humanoid environment.

## Method Summary
GAC uses two Q networks to compute both greedy Q (max of Q1, Q2) and conservative Q (min of Q1, Q2). The exploration policy is generated by sampling actions around the current mean action within a range determined by standard deviation, then weighting these samples by softmax probabilities computed from the greedy Q values. The policy is updated by minimizing KL divergence between itself and a target distribution derived from the conservative Q. This approach combines the exploration benefits of Q-driven action selection with the stability of conservative value estimation, breaking the correlation between policy and Q updates while maintaining effective exploration throughout training.

## Key Results
- Achieved ~8k average return on Mujoco Humanoid-v2 in 3 million steps, outperforming SAC, TD3, and RRS
- Demonstrated consistent improvement across multiple Mujoco environments including Ant, HalfCheetah, and Walker2d
- Showed that dynamic increase of β parameter during training improves exploration and learning stability
- Minimal performance gain in low-dimensional action spaces (e.g., Swimmer-v2), suggesting method benefits from higher complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy Q softmax update with double Q functions mitigates overestimation while providing a richer exploration distribution.
- Mechanism: The method constructs greedy Q as max of two Q functions and uses softmax-weighted average of conservative Q to form the target. The softmax weighting shapes the exploration distribution, giving higher probability to actions with larger greedy Q values while sampling from a broader set.
- Core assumption: Double Q framework ensures greedy Q overestimates less than single Q network, and conservative Q provides a lower bound that anchors target estimate.
- Evidence anchors:
  - [abstract] "Greedy Q takes the maximum value of the two Q functions, and conservative Q takes the minimum value of the two different Q functions."
  - [section 4.1] "We first define the greedy Q function, Qmax(s, a) = max{Q1(s, a), Q2(s, a)}, Qmin(s, a) = min{Q1(s, a), Q2(s, a)}..."
  - [corpus] Weak: related works mention double Q learning and overestimation, but not this specific softmax weighting scheme.
- Break condition: If two Q networks converge to same (bad) estimate, greedy Q will be biased and exploration distribution will be poor.

### Mechanism 2
- Claim: Learning surrogate policy via KL divergence to conservative Q distribution ensures exploration policy matches Q-driven distribution.
- Mechanism: Policy is optimized to minimize KL divergence between policy and exponential of conservative Q (normalized), aligning policy with Q function's assessment of good actions without being overly greedy.
- Core assumption: Conservative Q (min of two) is reasonable target for policy learning because it avoids overestimation bias while providing useful gradients.
- Evidence anchors:
  - [abstract] "we learn such a surrogate policy by minimizing the KL divergence between the policy and the policy constructed by the conservative Q."
  - [section 4.3] "Then we can make the policy directly learn from the target policy like the soft policy learning as follows: π′ = arg min π DKL (π( · |st) ∥ πO( · |st))"
  - [corpus] Weak: SAC also uses KL divergence to Q, but here it is to conservative Q; no direct evidence in corpus about this specific combination.
- Break condition: If conservative Q is too pessimistic, policy may under-explore or converge slowly.

### Mechanism 3
- Claim: Using exploration policy (based on greedy Q) to sample actions for Q updates provides better off-policy correction and breaks correlation with current policy.
- Mechanism: Instead of sampling from current policy for Bellman backup, actions are sampled from exploration distribution derived from greedy Q, broadening state-action visitation and improving learning stability.
- Core assumption: Exploration policy's distribution is sufficiently close to behavior policy's distribution that off-policy correction remains stable.
- Evidence anchors:
  - [abstract] "we make the policy learn from the Q function by minimizing the KL divergence between the policy and the distribution constructed by the conservative Q."
  - [section 4.2] "Refer to the SARSA method, we can sample two consecutive (s,a) pairs to estimate the expectation of the Q value..."
  - [corpus] Weak: SARSA and double Q are known, but combining exploration policy sampling for Q update is novel here; no direct corpus evidence.
- Break condition: If exploration policy deviates too far from behavior policy, Q updates may become unstable or biased.

## Foundational Learning

### Concept: Double Q learning and overestimation bias
- Why needed here: Algorithm relies on maintaining two Q networks to compute greedy and conservative Q; understanding overestimation explains why min operator is used for targets.
- Quick check question: Why does taking min{Q1, Q2} reduce overestimation compared to single Q network?

### Concept: KL divergence minimization for policy learning
- Why needed here: Policy update is driven by KL divergence to Q-based distribution; familiarity with this objective is essential to grasp how policy is shaped.
- Quick check question: What happens to policy if KL divergence target is too flat (uniform)?

### Concept: Softmax weighting and temperature (β) scheduling
- Why needed here: Exploration distribution is derived from softmax over greedy Q with dynamic β; understanding how β controls exploration/exploitation balance is critical.
- Quick check question: How does increasing β over training affect exploration distribution?

## Architecture Onboarding

### Component map
Environment -> Store transition -> Sample batch -> Compute greedy Q and exploration policy -> Sample actions for Q update -> Update Q networks -> Update policy via KL -> Update target networks

### Critical path
Environment interaction → Replay buffer storage → Batch sampling → Greedy Q computation → Exploration policy generation → Q network updates → Policy KL minimization → Target network updates

### Design tradeoffs
Sampling n actions for exploration vs. computational cost; dynamic β vs. fixed; double Q vs. single Q with target network

### Failure signatures
Poor exploration if sr (sample range) too small; unstable Q if β grows too fast; policy collapse if KL target too sharp

### First 3 experiments
1. Run with β=0 (uniform exploration) to confirm performance drop vs. dynamic β
2. Disable KL minimization (keep policy fixed) to see if Q learning still improves
3. Increase sr beyond 9 to observe diminishing returns or instability

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section implicitly raises several unresolved issues regarding hyperparameter sensitivity, computational cost, and scalability to high-dimensional action spaces.

## Limitations
- Double Q framework's effectiveness depends on two Q networks not converging to identical (potentially poor) estimates, but this assumption is not empirically validated
- Computational cost of sampling multiple actions for exploration policy (O(sn) per update) is noted but not thoroughly benchmarked or compared to alternatives
- Claims of state-of-the-art performance lack statistical significance testing with multiple random seeds

## Confidence
- **High**: Double Q functions to compute greedy Q (max) and conservative Q (min) for target estimation is well-grounded in prior work and paper's derivations
- **Medium**: KL divergence policy update to conservative Q-based target is theoretically sound, but empirical benefits over existing methods like SAC are not conclusively demonstrated
- **Low**: Claim that method achieves state-of-the-art performance on Mujoco benchmarks, particularly 8k score on Humanoid, is based on single run and lacks statistical significance testing

## Next Checks
1. Run ablation study with varying sample range (sr) and sample number (sn) to determine sensitivity of performance to these hyperparameters and identify optimal settings
2. Compare KL divergence policy update to standard SAC-style update on same benchmarks to isolate contribution of conservative Q target
3. Implement multi-run evaluation (e.g., 5 seeds) on Humanoid environment to establish statistical significance of reported 8k score and assess robustness