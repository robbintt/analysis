---
ver: rpa2
title: 'AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes'
arxiv_id: '2312.06644'
source_url: https://arxiv.org/abs/2312.06644
tags:
- room
- generation
- scene
- furniture
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnyHome is a framework that translates open-vocabulary textual
  descriptions into well-structured and textured 3D indoor scenes at house-scale.
  It employs an amodal structured representation to capture 3D spatial cues from textual
  narratives and uses egocentric inpainting to enrich these scenes.
---

# AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes

## Quick Facts
- **arXiv ID**: 2312.06644
- **Source URL**: https://arxiv.org/abs/2312.06644
- **Reference count**: 40
- **Primary result**: AnyHome translates open-vocabulary textual descriptions into well-structured and textured 3D indoor scenes at house-scale, achieving lower Out-Of-Boundary (OOB) rates and higher content generation scores compared to baseline methods.

## Executive Summary
AnyHome is a framework that translates open-vocabulary textual descriptions into well-structured and textured 3D indoor scenes at house-scale. It employs an amodal structured representation to capture 3D spatial cues from textual narratives and uses egocentric inpainting to enrich these scenes. The approach utilizes Large Language Models (LLMs) with designed prompts to comprehend and elaborate on user-provided text, generating diverse room types and layouts. It then employs graph-based intermediate representations to maintain a customizable geometry structure and refines object placement using Score Distillation Sampling (SDS) loss. Finally, it applies depth-conditioned texture inpainting to enhance visual realism. AnyHome achieves lower Out-Of-Boundary (OOB) rates and higher content generation scores compared to baseline methods, demonstrating its effectiveness in generating realistic and diverse house-scale scenes.

## Method Summary
AnyHome translates open-vocabulary textual descriptions into structured 3D scenes by first using LLM prompts to generate JSON-structured outputs for floorplans, room layouts, object retrieval, and texturing. These structured descriptions are then converted into graph-based intermediate representations: a bubble-diagram for the floorplan and a constrained layout graph for room layouts. The graphs are used to synthesize base meshes and populate rooms with retrieved 3D object meshes. Object placements are refined using Score Distillation Sampling (SDS) loss with differentiable rendering. Finally, an egocentric camera trajectory is generated to traverse the structured geometry, and depth-conditioned texture inpainting guided by this trajectory produces realistic, geometry-consistent textures.

## Key Results
- Lower Out-Of-Boundary (OOB) rates compared to baseline methods
- Higher content generation scores for generated house-scale scenes
- Achieves realistic and diverse indoor scene generation from open-vocabulary text descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can transform open-vocabulary text into structured scene representations (floorplans, room layouts, object placements) using specially designed prompts.
- Mechanism: Prompt engineering leverages the LLM's commonsense reasoning and ability to elaborate on user inputs, generating JSON-structured outputs that feed directly into downstream geometry synthesis.
- Core assumption: LLM comprehension of spatial and functional relationships in text is sufficiently reliable to guide scene generation without fine-tuning on 3D datasets.
- Evidence anchors:
  - [abstract] "specially designed template prompts for Large Language Models (LLMs), which enable precise control over the textual input."
  - [section 3.1] "users can design scenes using free-form text descriptions, which are then structured into modular descriptions for floorplans, room layouts, object retrieval and placement, and appearance."
- Break condition: LLM fails to adhere to user constraints or generates implausible object relations; JSON parsing errors occur.

### Mechanism 2
- Claim: Combining graph-based intermediate representations with LLM-generated rules enables structured geometry synthesis that aligns with textual descriptions.
- Mechanism: House floorplans are represented as bubble-diagrams (graphs), converted into base meshes; room layouts use constrained layout graphs with object bounding boxes placed according to LLM-generated rules; object retrieval aligns meshes with descriptions via CLIP and Sentence Transformers.
- Core assumption: Graph representations preserve spatial consistency and allow precise control over layout while remaining compatible with pretrained 3D generation networks.
- Evidence anchors:
  - [abstract] "graph-based intermediate representations to maintain a customizable geometry structure."
  - [section 3.2] "We use graph-based intermediate representations: one dedicated to the floorplan and the other to the room layout."
- Break condition: Generated graphs lead to intersecting or out-of-bound objects; retrieval fails to find matching meshes.

### Mechanism 3
- Claim: Depth-conditioned texture inpainting guided by an egocentric trajectory produces realistic, geometry-consistent textures that match textual prompts.
- Mechanism: Egocentric camera trajectories traverse the structured geometry; depth maps condition texture generation via diffusion models; differentiable rendering backprojects textures onto mesh vertices for multi-view consistency.
- Core assumption: Text-to-image diffusion models can generate realistic textures conditioned on depth and text, and differentiable rendering can maintain consistency across viewpoints.
- Evidence anchors:
  - [abstract] "depth-conditioned texture inpainting to enhance visual realism" and "incorporates an egocentric inpainting process."
  - [section 3.3] "The texture painting on the structured geometry adheres to the existing geometry using a depth-aware, text-conditioned inpainting model."
- Break condition: Textures fail to adhere to geometry; view inconsistencies persist despite backprojection.

## Foundational Learning

- **Graph-based representations for structured scene generation**
  - Why needed here: Enables precise control over spatial layout, object relationships, and room types while preserving consistency with textual input.
  - Quick check question: What advantages do bubble-diagrams and constrained layout graphs provide over free-form text-to-3D approaches?

- **Prompt engineering for LLM control in 3D scene synthesis**
  - Why needed here: Allows open-vocabulary input to be systematically converted into structured, machine-readable outputs (JSON) for geometry and layout generation.
  - Quick check question: How do the modular prompt templates (pfloorplan, pfurniture, pornament, ptexture) each contribute to the overall pipeline?

- **Depth-conditioned inpainting and differentiable rendering for texture synthesis**
  - Why needed here: Ensures generated textures align with geometry and maintain consistency across multiple viewpoints in the 3D scene.
  - Quick check question: Why is depth conditioning necessary for texture generation on structured 3D meshes?

## Architecture Onboarding

- **Component map**: Text input → LLM prompt parsing → graph generation → mesh synthesis → trajectory generation → texture inpainting → final textured scene
- **Critical path**: Text input → LLM prompt parsing → graph generation → mesh synthesis → trajectory generation → texture inpainting → final textured scene
- **Design tradeoffs**:
  - LLM reliance vs. deterministic rule-based layout
  - Graph complexity vs. generalization to diverse room types
  - Texture realism vs. geometry preservation
  - Egocentric view coverage vs. rendering efficiency
- **Failure signatures**:
  - Out-of-bound or intersecting objects (layout generation)
  - Inconsistent or implausible textures (inpainting)
  - JSON parsing errors (LLM output formatting)
  - Slow texture convergence (diffusion model optimization)
- **First 3 experiments**:
  1. Test LLM prompt parsing: Input simple text, verify JSON structure and plausibility.
  2. Test graph-based layout: Generate bubble-diagram from text, check floorplan mesh for intersections.
  3. Test texture inpainting: Apply depth-conditioned inpainting on a simple mesh, check multi-view consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AnyHome's approach to understanding 3D space be further improved to enhance its versatility?
- Basis in paper: [explicit] The paper states that "The understanding of 3D space by current Large Language Models (LLMs) sets a limit on the versatility of our system."
- Why unresolved: Current LLMs have limitations in understanding 3D space, which restricts the system's ability to generate diverse and complex scenes.
- What evidence would resolve it: Developing or integrating LLMs with improved 3D spatial understanding capabilities, or combining LLMs with other AI models specialized in 3D space comprehension.

### Open Question 2
- Question: What methods can be used to address the consistency issues in multi-view inpainting across different views?
- Basis in paper: [explicit] The paper mentions that "the present multi-view inpainting techniques face challenges with consistency across multiple views."
- Why unresolved: Ensuring texture consistency across multiple views in 3D scene generation remains a challenge, affecting the realism and quality of the generated scenes.
- What evidence would resolve it: Implementing advanced inpainting algorithms that maintain consistency across multiple views, or developing new techniques that integrate 3D information more effectively during the inpainting process.

### Open Question 3
- Question: How can AnyHome be extended to generate outdoor scenes or larger-scale environments beyond house-scale?
- Basis in paper: [inferred] While the paper focuses on house-scale scene generation, it does not explore the system's capabilities for larger or outdoor environments.
- Why unresolved: The current framework is optimized for house-scale indoor scenes, and its applicability to larger or outdoor environments is not explored.
- What evidence would resolve it: Adapting the framework to handle the complexities of outdoor environments, such as varying scales, lighting conditions, and natural elements, and testing its effectiveness in generating such scenes.

## Limitations

- **LLM Dependence and Output Reliability**: The framework relies heavily on LLM-generated JSON structures for scene layout and object placement. No empirical validation of prompt effectiveness is provided, and no error handling for malformed or implausible LLM outputs is described.
- **Graph Representation Assumptions**: While graph-based floorplans and room layouts are used, there is no evidence that these representations generalize well to highly irregular or complex house geometries.
- **Texture Inpainting Generalization**: Depth-conditioned texture inpainting is proposed but lacks empirical support in 3D contexts. The quality and consistency of textures under varying lighting, object occlusions, or complex surface geometries remain unproven.

## Confidence

- **High Confidence**: The conceptual pipeline (text → LLM → graph → geometry → texture) is coherent and aligns with recent advances in structured scene generation and text-to-3D synthesis.
- **Medium Confidence**: The integration of graph-based representations and LLM prompts is plausible but under-supported by empirical evidence. Success depends on the robustness of the LLM outputs and the flexibility of the graph representations.
- **Low Confidence**: The depth-conditioned inpainting mechanism, while innovative, lacks validation in 3D scenarios. The multi-view consistency guarantees and texture realism are not empirically demonstrated.

## Next Checks

1. **LLM Output Validation**: Test the LLM prompt engine with a diverse set of textual inputs (from simple labels to detailed narratives) and verify the structure, plausibility, and completeness of the generated JSON outputs.
2. **Graph Layout Robustness**: Generate floorplans and room layouts for complex or irregular house descriptions. Check for intersecting objects, out-of-bound placements, and adherence to user constraints.
3. **Texture Consistency Evaluation**: Apply depth-conditioned inpainting on a variety of 3D geometries with differing surface complexities. Evaluate texture realism and multi-view consistency through rendered frames and user studies.