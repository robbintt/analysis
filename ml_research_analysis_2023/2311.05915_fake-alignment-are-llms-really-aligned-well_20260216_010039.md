---
ver: rpa2
title: 'Fake Alignment: Are LLMs Really Aligned Well?'
arxiv_id: '2311.05915'
source_url: https://arxiv.org/abs/2311.05915
tags:
- llms
- questions
- safety
- alignment
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of fake alignment in large
  language models (LLMs), where models exhibit inconsistent performance between open-ended
  and multiple-choice safety questions. The authors argue that this is due to mismatched
  generalization, where models merely memorize answers to open-ended safety questions
  without truly understanding safety concepts.
---

# Fake Alignment: Are LLMs Really Aligned Well?

## Quick Facts
- **arXiv ID:** 2311.05915
- **Source URL:** https://arxiv.org/abs/2311.05915
- **Reference count:** 8
- **Key outcome:** This paper investigates the problem of fake alignment in large language models (LLMs), where models exhibit inconsistent performance between open-ended and multiple-choice safety questions. The authors argue that this is due to mismatched generalization, where models merely memorize answers to open-ended safety questions without truly understanding safety concepts. To address this, they propose the Fake Alignment Evaluation Framework (FAEF) with two novel metrics: Consistency Score (CS) and Consistent Safety Score (CSS). FAEF quantitatively measures fake alignment by comparing model consistency across open-ended and multiple-choice formats. Experiments on 14 widely-used LLMs reveal several models with purported safety are poorly aligned in practice. For instance, some models achieve near-perfect performance on open-ended questions but significantly lower accuracy on corresponding multiple-choice questions, indicating fake alignment. The results highlight limitations in current alignment methodologies and emphasize the need for more rigorous evaluation protocols.

## Executive Summary
This paper introduces the concept of "fake alignment" in large language models (LLMs), where models appear well-aligned on open-ended safety questions but fail on corresponding multiple-choice formats. The authors argue this inconsistency stems from mismatched generalization - models memorize response patterns for open-ended safety questions without truly understanding safety concepts. To quantify this phenomenon, they propose the Fake Alignment Evaluation Framework (FAEF) with two novel metrics: Consistency Score (CS) and Consistent Safety Score (CSS). Through extensive experiments on 14 widely-used LLMs, the study reveals that several models exhibit significant fake alignment, highlighting critical limitations in current alignment methodologies and emphasizing the need for more rigorous evaluation protocols.

## Method Summary
The paper proposes the Fake Alignment Evaluation Framework (FAEF) to detect and quantify fake alignment in LLMs. The method involves constructing parallel test questions in both open-ended and multiple-choice formats across five safety categories (fairness, personal harm, legality, privacy, civic virtue). For each question, models are evaluated on both formats, and consistency metrics (CS and CSS) are calculated by comparing performance across formats. The study tests 14 widely-used LLMs, including both open and closed-source models of various sizes. Additionally, the authors demonstrate that multiple-choice format data can be used as high-quality contrast distillation-based fine-tuning data to improve alignment consistency with minimal overhead.

## Key Results
- Several LLMs achieve near-perfect performance on open-ended safety questions but significantly lower accuracy on corresponding multiple-choice questions, indicating fake alignment
- Consistency Score (CS) and Consistent Safety Score (CSS) effectively quantify the degree of fake alignment by measuring performance discrepancies across question formats
- Multiple-choice format data serves as high-quality contrast distillation-based fine-tuning data, strongly improving alignment consistency with minimal fine-tuning overhead
- Vicuna series models show particularly high fake alignment, with large performance gaps between open-ended and multiple-choice formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can achieve high open-ended safety performance while failing on multiple-choice safety questions due to mismatched generalization.
- Mechanism: During safety training, models learn response patterns for open-ended safety questions without developing a deep understanding of safety concepts. This leads to memorization of "what to say" rather than "why to say it," making them unable to generalize to multiple-choice formats.
- Core assumption: The model's pre-training capabilities include the ability to answer multiple-choice questions, but safety training doesn't effectively transfer this capability to safety contexts.
- Evidence anchors:
  - [abstract]: "This is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests."
  - [section]: "Instead, it only remembers what to answer for open-ended safety questions, which makes it unable to solve other forms of safety tests."
  - [corpus]: Weak evidence - only 5 related papers found, none directly addressing mismatched generalization in safety evaluation.
- Break condition: If safety training data sufficiently covers diverse question formats and reasoning patterns, or if the model truly understands safety concepts rather than memorizing responses.

### Mechanism 2
- Claim: The FAEF framework can detect fake alignment by comparing model consistency across open-ended and multiple-choice formats.
- Mechanism: By constructing parallel test questions in both formats and measuring consistency, FAEF quantifies the degree to which a model's safety performance is superficial rather than genuine.
- Core assumption: Genuine safety understanding should produce consistent responses across different question formats, while fake alignment will show significant discrepancies.
- Evidence anchors:
  - [abstract]: "We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment."
  - [section]: "By comparing its consistency in answering the two types of questions."
  - [corpus]: Weak evidence - only 5 related papers found, none directly addressing consistency measurement across question formats.
- Break condition: If the consistency metrics don't correlate with actual safety behavior in real-world scenarios, or if models can game the consistency measurement.

### Mechanism 3
- Claim: Multiple-choice format data can be used as high-quality contrast distillation-based fine-tuning data to improve alignment consistency.
- Mechanism: Using multiple-choice questions with correct options as supervision during fine-tuning helps models learn the underlying safety concepts rather than just response patterns.
- Core assumption: The contrastive nature of multiple-choice questions (positive vs. negative options) provides richer supervision signals than open-ended questions alone.
- Evidence anchors:
  - [abstract]: "we found that multiple-choice format data can also be used as high-quality contrast distillation-based fine-tuning data, which can strongly improve the alignment consistency of LLMs with minimal fine-tuning overhead."
  - [section]: "This further substantiates that such consistency tests can effectively uncover the fake alignment."
  - [corpus]: Weak evidence - only 5 related papers found, none directly addressing contrast distillation for alignment improvement.
- Break condition: If models still show poor consistency after fine-tuning with multiple-choice data, or if the improvement is superficial and doesn't generalize to new safety scenarios.

## Foundational Learning

- Concept: Mismatched generalization
  - Why needed here: Understanding how pre-training capabilities can be misaligned with safety training objectives is crucial for diagnosing fake alignment.
  - Quick check question: If a model can answer multiple-choice questions well in general but poorly in safety contexts, what does this suggest about its safety training?

- Concept: Consistency measurement
  - Why needed here: Quantifying the discrepancy between open-ended and multiple-choice performance is essential for detecting fake alignment.
  - Quick check question: What would perfect consistency (CS=1) between open-ended and multiple-choice safety performance indicate about a model's alignment?

- Concept: Contrastive learning
  - Why needed here: Understanding how multiple-choice format provides contrastive supervision is important for using it as fine-tuning data.
  - Quick check question: How does the presence of both positive and negative options in multiple-choice questions create richer supervision signals than open-ended questions?

## Architecture Onboarding

- Component map:
  - Data collection module: Gathers open-ended questions around safety topics
  - Option construction module: Converts open-ended to multiple-choice format
  - Response judgment module: Evaluates model responses using LLMs or humans
  - Consistency measurement module: Calculates CS and CSS metrics
  - Fine-tuning module: Uses multiple-choice data for contrast distillation

- Critical path:
  1. Collect open-ended safety questions
  2. Convert to multiple-choice format using aligned LLM
  3. Evaluate model on both formats
  4. Calculate consistency metrics
  5. Use results to identify fake alignment

- Design tradeoffs:
  - Open-ended vs. multiple-choice: Open-ended questions are more subjective and harder for humans to evaluate but easier for models; multiple-choice is more objective and easier for humans but harder for models.
  - LLM vs. human judgment: LLM judgment is faster and more scalable but may have its own biases; human judgment is more reliable but slower and more expensive.
  - Few-shot vs. zero-shot: Few-shot evaluation can improve performance but may mask fake alignment issues.

- Failure signatures:
  - High open-ended performance with low multiple-choice performance indicates fake alignment
  - Low consistency scores (CS << 1) suggest superficial safety understanding
  - Minimal improvement from contrast distillation fine-tuning suggests deep-rooted fake alignment

- First 3 experiments:
  1. Test a known well-aligned model (e.g., GPT-4) on both formats to establish baseline consistency
  2. Test a poorly-aligned model (e.g., Vicuna) to verify fake alignment detection
  3. Fine-tune a model with multiple-choice data and measure improvement in consistency metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate and mitigate the risk of fake alignment in large language models (LLMs) without relying solely on multiple-choice questions?
- Basis in paper: [explicit] The paper highlights the existence of fake alignment in LLMs and proposes the Fake Alignment Evaluation Framework (FAEF) to quantitatively measure it. However, it acknowledges that FAEF requires a small amount of human assistance and is compatible with existing open-source datasets.
- Why unresolved: While FAEF is a step towards evaluating fake alignment, it still relies on human involvement and existing datasets. A more automated and comprehensive approach to evaluate and mitigate fake alignment is needed.
- What evidence would resolve it: Development and validation of a fully automated framework that can effectively detect and mitigate fake alignment in LLMs across various domains and tasks.

### Open Question 2
- Question: How does the size of the model and the amount of training data impact the severity of fake alignment in LLMs?
- Basis in paper: [explicit] The paper mentions that larger models and more comprehensive training data can lead to better alignment performance. However, it does not delve into the specific relationship between model size, training data, and fake alignment.
- Why unresolved: Understanding the impact of model size and training data on fake alignment is crucial for designing more effective alignment strategies and evaluating the safety of different LLM architectures.
- What evidence would resolve it: Empirical studies comparing the fake alignment performance of LLMs with varying sizes and training data, controlling for other factors such as alignment methodology and task complexity.

### Open Question 3
- Question: Can we develop more robust alignment methodologies that are less susceptible to the pitfalls of fake alignment?
- Basis in paper: [explicit] The paper identifies fake alignment as a limitation of current alignment methodologies and suggests the need for more rigorous evaluation protocols. However, it does not propose specific solutions to address this issue.
- Why unresolved: Developing more robust alignment methodologies is essential to ensure the safety and reliability of LLMs in real-world applications.
- What evidence would resolve it: Proposals and experimental validation of novel alignment methodologies that demonstrate improved resistance to fake alignment and better generalization across different evaluation formats.

## Limitations

- The study relies on a relatively small sample size of test questions (100 total across 5 categories), which may not comprehensively capture all safety domains
- The construction process for multiple-choice distractors using jailbreak methods may introduce bias into the evaluation framework
- The contrast distillation fine-tuning approach was tested on only a subset of models, limiting generalizability of the improvement claims

## Confidence

- High confidence: Fake alignment exists and can be detected via consistency metrics
- Medium confidence: Mismatched generalization is the primary mechanism driving fake alignment
- Medium confidence: Contrast distillation with multiple-choice data improves alignment consistency

## Next Checks

1. Conduct a human evaluation study on a subset of open-ended responses to validate LLM judge reliability and quantify any systematic biases
2. Test the FAEF framework on a larger, more diverse set of safety questions (n>500) to ensure robustness across broader safety domains
3. Perform ablation studies on the fine-tuning approach, varying the ratio of multiple-choice to open-ended data and testing different fine-tuning durations