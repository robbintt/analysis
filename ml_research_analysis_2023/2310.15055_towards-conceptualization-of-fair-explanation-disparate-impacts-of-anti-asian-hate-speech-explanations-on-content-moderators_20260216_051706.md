---
ver: rpa2
title: 'Towards Conceptualization of "Fair Explanation": Disparate Impacts of anti-Asian
  Hate Speech Explanations on Content Moderators'
arxiv_id: '2310.15055'
source_url: https://arxiv.org/abs/2310.15055
tags:
- explanation
- counterfactual
- hate
- explanations
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new concept of "fair explanations" by evaluating
  the disparate impact of AI explanations on different user groups. It presents a
  human study on hate speech detection, comparing saliency maps and counterfactual
  explanations for Asian vs.
---

# Towards Conceptualization of "Fair Explanation": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators

## Quick Facts
- arXiv ID: 2310.15055
- Source URL: https://arxiv.org/abs/2310.15055
- Reference count: 35
- Primary result: Saliency maps outperform counterfactual explanations in accuracy and mental discomfort for hate speech detection, while counterfactuals show disparate impact on Asian moderators

## Executive Summary
This paper introduces the concept of "fair explanations" by evaluating how different AI explanation methods impact diverse user groups. Through a human study with content moderators, the research compares saliency maps and counterfactual explanations for hate speech detection, measuring their effects on accuracy, mental discomfort, stereotype activation, perceived workload, and label time. The study reveals that saliency maps reduce mental discomfort more effectively than counterfactual explanations, while counterfactuals show evidence of disparate impact on mental discomfort for Asian moderators and label time for non-Asian moderators. The findings suggest that explanation choice should consider potential disparate impacts on different user groups.

## Method Summary
The study uses the COVID-HATE dataset with 2,290 tweets and a RoBERTa model for hate speech classification. Two explanation methods are compared: saliency maps via LIME and manually-generated counterfactual explanations. A between-subjects human study with 283 participants evaluates three conditions (baseline, saliency, counterfactual) across 12 tweets each. The study measures five metrics: accuracy, label time, mental discomfort (SPANE), stereotype activation, and perceived workload (NASA-TLX). Analysis includes statistical testing, individual fairness evaluation using pairwise distance, and heterogeneous treatment effects analysis via Double Machine Learning.

## Key Results
- Saliency maps yield significantly lower mental discomfort than counterfactual explanations (M = 1.7010 vs M = 3.4194)
- Counterfactual explanations show evidence of disparate impact on mental discomfort for Asian moderators and label time for non-Asian moderators
- Counterfactual explanations are more individually unfair than saliency maps in terms of mental discomfort and stereotype activation
- Saliency maps outperform counterfactual explanations in accuracy and reduce exposure to full hateful content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Saliency maps reduce mental discomfort by allowing participants to focus on key hateful tokens rather than full context.
- Mechanism: By highlighting only the most important tokens for hate classification, saliency maps reduce cognitive load and exposure to full hateful content.
- Core assumption: Participants experience less mental discomfort when they don't need to fully process discriminatory intent in context.
- Evidence anchors:
  - [abstract] "saliency map (M = 1.7010, SD = 5.0076) yields significantly lower mental discomfort than the no explanation baseline (M = 3.4194, SD = 5.9121)"
  - [section 5.1] "A plausible mechanism which may explain this observation is that saliency map gives participants the option to just look at the top few most 'hateful' tokens without necessarily putting them into context of a whole sentence"

### Mechanism 2
- Claim: Counterfactual explanations increase stereotype activation by presenting modified versions of hateful content that remain implicitly biased.
- Mechanism: Even when counterfactuals flip AI predictions from hate to non-hate, the modified content may still contain discriminatory elements, normalizing implicit bias.
- Core assumption: Exposure to content that is still biased but AI-classified as non-hateful increases acceptance of stereotypes.
- Evidence anchors:
  - [abstract] "counterfactual explanation (M = 0.1900, SD = 0.3365) yields significantly higher stereotype activation than the baseline (M = 0.0968, SD = 0.1990)"
  - [section 5.1] "the counterfactual explanation for the tweet '@USER Pussies.. That's what the Chinese are known for... retreat Losers!!! #ChineseVirus' is still arguably hateful even after removing the '#ChineseVirus' hashtag"

### Mechanism 3
- Claim: Individual fairness decreases with counterfactual explanations because similar individuals receive different mental discomfort scores.
- Mechanism: Counterfactual explanations create variable interpretations and emotional responses among similar individuals based on their background and experiences.
- Core assumption: Similar individuals (as defined by task-relevant questions) should have similar responses to the same explanation type.
- Evidence anchors:
  - [abstract] "counterfactual explanations were more individually unfair than saliency maps" in terms of mental discomfort and stereotype activation
  - [section 5.5] "counterfactual explanation is more individually unfair than saliency map" regarding mental discomfort and stereotype activation

## Foundational Learning

- Concept: Mental discomfort measurement using SPANE scale
  - Why needed here: The study needs a validated psychological metric to quantify the emotional impact of exposure to hate speech
  - Quick check question: What are the six dimensions of negative affect measured by SPAFE that are relevant to hate speech exposure?

- Concept: Individual fairness evaluation using pairwise distance
  - Why needed here: To measure whether similar individuals receive similar treatment across different explanation conditions
  - Quick check question: How does the study define "similar individuals" for the individual fairness evaluation?

- Concept: Heterogeneous treatment effects analysis
  - Why needed here: To understand how explanation effects vary across different demographic subgroups beyond simple group comparisons
  - Quick check question: What machine learning approach does the study use to estimate heterogeneous treatment effects?

## Architecture Onboarding

- Component map: COVID-HATE dataset -> RoBERTa classifier -> explanation generation -> Qualtrics survey -> statistical analysis -> fairness evaluation
- Critical path: Data collection -> model training -> explanation generation -> human study -> statistical analysis -> fairness evaluation
- Design tradeoffs:
  - Human-generated counterfactuals vs. automated generation (quality vs. scalability)
  - Between-subjects design (reduces learning effects but requires more participants)
  - Simplified three-way classification (improves participant understanding vs. capturing nuance)
- Failure signatures:
  - No significant differences between conditions (indicates explanations have no impact)
  - Unexpectedly high accuracy across all conditions (suggests task is too easy)
  - Very low completion rates (indicates survey fatigue or ethical concerns)
- First 3 experiments:
  1. Compare mental discomfort scores between saliency and counterfactual conditions using t-test
  2. Test individual fairness by calculating pairwise distances and output differences
  3. Analyze heterogeneous treatment effects using Double Machine Learning approach

## Open Questions the Paper Calls Out

- What are the long-term psychological effects of exposure to AI-generated explanations for hate speech on content moderators?
- How do AI explanations for hate speech detection impact different cultural or ethnic groups beyond the Asian/non-Asian binary studied?
- What mechanisms explain why saliency maps reduce mental discomfort compared to counterfactual explanations?
- How do individual differences in moderators' prior experience with hate speech affect their response to AI explanations?

## Limitations

- Manual generation of counterfactual explanations introduces inconsistency and scalability concerns
- Between-subjects design requires larger sample size for adequate statistical power
- Focus on specific demographic context (Asian vs. non-Asian moderators) limits generalizability

## Confidence

- High Confidence: Superiority of saliency maps over counterfactual explanations for mental discomfort and accuracy metrics
- Medium Confidence: Heterogeneous treatment effects analysis and individual fairness evaluation
- Low Confidence: Generalizability of the "fair explanation" concept to other domains and explanation methods

## Next Checks

1. Cross-Domain Validation: Replicate the study with different types of content moderation tasks to test generalizability of the fair explanation framework.

2. Automated Counterfactual Generation: Implement and evaluate automated methods for generating counterfactual explanations to address scalability concerns.

3. Longitudinal Impact Assessment: Conduct follow-up study to measure long-term effects of different explanation types on moderator well-being and performance.