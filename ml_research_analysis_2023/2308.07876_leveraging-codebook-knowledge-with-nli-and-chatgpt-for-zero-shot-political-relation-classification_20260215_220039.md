---
ver: rpa2
title: Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot Political
  Relation Classification
arxiv_id: '2308.07876'
source_url: https://arxiv.org/abs/2308.07876
tags:
- event
- protest
- conflict
- table
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores zero-shot learning for political relation classification,
  leveraging expert knowledge from annotation codebooks and advanced language models
  like ChatGPT and ZSP (a natural language inference-based approach). ZSP employs
  a tree-query framework to classify relations based on context, modality, and class
  disambiguation, enhancing interpretability and adaptability.
---

# Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot Political Relation Classification

## Quick Facts
- arXiv ID: 2308.07876
- Source URL: https://arxiv.org/abs/2308.07876
- Reference count: 40
- Primary result: ZSP significantly outperforms dictionary-based methods and achieves competitive performance compared to supervised models in zero-shot political relation classification.

## Executive Summary
This study explores zero-shot learning for political relation classification, leveraging expert knowledge from annotation codebooks and advanced language models like ChatGPT and ZSP (a natural language inference-based approach). ZSP employs a tree-query framework to classify relations based on context, modality, and class disambiguation, enhancing interpretability and adaptability. Experiments show that ZSP significantly outperforms dictionary-based methods and achieves competitive performance compared to supervised models, particularly in fine-grained Rootcode classification. The findings highlight the potential of leveraging transfer learning and existing expertise to improve research efficiency and scalability in political event coding.

## Method Summary
ZSP uses natural language inference with a tree-query framework that deconstructs the classification task into three levels: context, modality, and class disambiguation. The method generates modality-aware hypotheses from PLOVER codebooks and applies filtering rules at each level to improve precision. It incorporates expert knowledge through modality mapping tables and class disambiguation rules like Conflict Override and Consult Penalty. The framework processes source-target sentence pairs through NLI queries, progressively narrowing candidate relations before producing final classifications.

## Key Results
- ZSP achieves macro F1 scores of 0.73 (Binary), 0.58 (Quadcode), and 0.46 (Rootcode) on the PLV dataset, significantly outperforming dictionary-based methods.
- The tree-query framework demonstrates 2-4% higher accuracy than flat NLI approaches in fine-grained classification tasks.
- ZSP shows competitive performance against supervised models despite requiring no labeled training data, highlighting the effectiveness of codebook knowledge transfer.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-query framework improves precision by reducing hypothesis scope at each classification level.
- Mechanism: By filtering candidates contextually in Level 1 (Past modality), then narrowing further by modality in Level 2, and finally resolving ambiguity in Level 3, the model minimizes incorrect entailment scores from irrelevant hypotheses.
- Core assumption: NLI scores are more accurate when comparing a smaller, contextually relevant set of hypotheses rather than all hypotheses at once.
- Evidence anchors: [abstract] "This decomposition enhances interpretability, efficiency, and adaptability to schema changes." [section 3.4] "This framework improves interpretability, efficiency, and precision by filtering candidates contextually..."
- Break condition: If class disambiguation rules become too complex or ambiguous, the filtering benefits could be outweighed by rule conflicts.

### Mechanism 2
- Claim: Modality-aware hypotheses enable NLI to distinguish event temporality, improving classification accuracy.
- Mechanism: Adding modality tags (Past, Future, Negated Past, Negated Future) to hypotheses allows NLI to differentiate between similar semantic relations with different temporal contexts, e.g., "agreed to reduce protests" vs. "increased protests."
- Core assumption: NLI models can generalize semantic differences across modalities even without exact token matching.
- Evidence anchors: [abstract] "ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels." [section 3.2] "We observe that NLI correctly identifies our Past hypothesis as not entailed...NLI assigns the highest scores to the correct modality NF..."
- Break condition: If the modality mapping table is incomplete or inconsistent, NLI may misclassify events with subtle temporal shifts.

### Mechanism 3
- Claim: Incorporating codebook knowledge reduces reliance on large annotated datasets by reusing existing domain expertise.
- Mechanism: By converting label descriptions from the PLOVER codebook into modality-aware hypotheses and disambiguation rules, ZSP leverages prior expert knowledge instead of requiring new annotations.
- Core assumption: The existing codebook contains sufficient and accurate guidance for fine-grained classification tasks.
- Evidence anchors: [abstract] "This study explores zero-shot learning methods that use expert knowledge from existing annotation codebook..." [section 2.1] "CAMEO/PLOVER incorporates fine-grained modalities for the linguistic features of events..."
- Break condition: If the codebook is outdated or misaligned with the target ontology, performance will degrade significantly.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI determines entailment between premise and hypothesis, forming the basis for zero-shot classification without labels.
  - Quick check question: How does NLI distinguish between "request" and "protest" when both are semantically similar?

- Concept: Modality in linguistic events
  - Why needed here: Modality captures the temporal or hypothetical status of events, critical for distinguishing between past actions and future intentions in political coding.
  - Quick check question: Why does "agreed to reduce protests" require a different classification than "increased protests"?

- Concept: Class disambiguation rules
  - Why needed here: Political ontologies often have overlapping labels; rules help prioritize one label over another based on context (e.g., Conflict Override).
  - Quick check question: When should a "request" be overridden by a "protest" in the classification output?

## Architecture Onboarding

- Component map: Input sentence -> Context classification (Past modality hypotheses) -> Modality refinement (Future/Past/Negated variants) -> Class disambiguation (Conflict Override, Consult Penalty) -> PLOVER code output

- Critical path:
  1. Tokenize and parse sentence
  2. Generate modality-aware hypotheses from codebook
  3. Apply Level 1 NLI queries
  4. Filter top candidates
  5. Apply Level 2 modality queries
  6. Apply Level 3 disambiguation rules
  7. Return final classification

- Design tradeoffs:
  - Simplicity vs. coverage: Fewer hypotheses improve precision but may miss rare cases.
  - Static vs. dynamic rules: Fixed codebook rules are efficient but less adaptable to ontology changes.
  - Modality granularity: More modalities improve accuracy but increase hypothesis space.

- Failure signatures:
  - Low recall: Too aggressive filtering in early levels removes correct candidates.
  - High false positives: Overly general hypotheses pass through multiple levels.
  - Inconsistent outputs: Class disambiguation rules conflict or are ambiguous.

- First 3 experiments:
  1. Run ZSP on a small subset of PLV data with manual label verification to test tree-query accuracy.
  2. Compare Flat vs. Tree-query performance on a balanced binary task to validate filtering efficiency.
  3. Test Consult Penalty impact by toggling c on/off in a controlled setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ChatGPT be effectively adapted for zero-shot relation classification tasks with fine-grained labels?
- Basis in paper: Explicit - The paper highlights ChatGPT's instability and limitations in handling fine-grained labels and suggests the need for further investigation.
- Why unresolved: ChatGPT struggles with generating formatted results, incorporating complex task descriptions, and retaining information across interactions, making it challenging to apply in real-world scenarios.
- What evidence would resolve it: Developing improved prompt designs, exploring hybrid methods with few-shot learning, or investigating alternative zero-shot approaches that address ChatGPT's limitations.

### Open Question 2
- Question: Can the ZSP model be extended to handle tasks with semantically non-mutually exclusive labels or tasks lacking accessible knowledge bases?
- Basis in paper: Explicit - The paper acknowledges challenges in classifying semantically non-mutually exclusive fine-grained labels and suggests exploring hybrid methods for tasks without accessible knowledge bases.
- Why unresolved: The current ZSP model relies on expert knowledge from codebooks, which may not be available for all tasks, and struggles with nuanced labels that lack clear distinctions.
- What evidence would resolve it: Experimenting with hybrid approaches that combine ZSP with few-shot learning, pattern-matching, or in-context learning, and evaluating their performance on tasks with diverse label characteristics.

### Open Question 3
- Question: How can the tree-query framework in ZSP be optimized to further improve efficiency and precision?
- Basis in paper: Explicit - The paper mentions the tree-query framework's ability to improve efficiency and precision but does not explore potential optimizations.
- Why unresolved: The current framework effectively delimits the scope of candidate hypotheses but may have room for improvement in query optimization or hypothesis selection strategies.
- What evidence would resolve it: Conducting experiments to compare different tree-query configurations, analyzing the impact of various hyperparameter settings, and exploring alternative hypothesis selection methods.

## Limitations
- Reliance on static codebook knowledge may limit generalization to evolving ontologies or novel linguistic patterns.
- Evaluation focuses on macro F1 scores without per-class performance, potentially masking poor results on minority classes.
- Comparison with supervised models is limited by small dataset sizes (PLV: 1,200 instances; A/W: 5,616 instances).

## Confidence
- High confidence: The zero-shot classification mechanism using NLI and modality-aware hypotheses is technically sound and well-grounded in the literature.
- Medium confidence: The reported performance improvements over dictionary-based methods are plausible but not independently verifiable due to missing implementation details.
- Low confidence: The generalizability of results to other political ontologies or languages is uncertain, as the evaluation is restricted to PLOVER ontology and English news text.

## Next Checks
1. Test the tree-query framework on a held-out validation set with manual annotation to verify that class disambiguation rules (e.g., Conflict Override, Consult Penalty) are applied correctly and consistently across diverse examples.
2. Evaluate ZSP on a different political ontology or language to assess whether the modality-aware hypothesis generation and tree-query framework can adapt to new schemas without manual rule updates.
3. Perform an ablation study by removing or modifying class disambiguation rules to quantify their impact on macro F1 scores, particularly for fine-grained Rootcode classification.