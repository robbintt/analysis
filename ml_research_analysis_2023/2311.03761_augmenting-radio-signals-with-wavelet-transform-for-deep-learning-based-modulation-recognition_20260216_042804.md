---
ver: rpa2
title: Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation
  Recognition
arxiv_id: '2311.03761'
source_url: https://arxiv.org/abs/2311.03761
tags:
- uni00000013
- uni00000011
- uni00000016
- uni00000015
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes data augmentation methods for deep learning-based
  modulation recognition using wavelet transform. The methods replace detail coefficients
  decomposed by discrete wavelet transform to generate new samples and expand the
  training set.
---

# Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation Recognition

## Quick Facts
- **arXiv ID**: 2311.03761
- **Source URL**: https://arxiv.org/abs/2311.03761
- **Reference count**: 40
- **Primary result**: RNSR-MW augmentation method achieves 60.574% accuracy on RML1024 dataset, outperforming baseline methods across multiple CNN architectures.

## Executive Summary
This paper addresses the challenge of limited training data in deep learning-based radio modulation recognition by proposing wavelet transform-based data augmentation methods. The approach uses Discrete Wavelet Transform to decompose IQ signals into approximation and detail coefficients, then generates new training samples by replacing detail coefficients with various sequences (all zero, random zero, or random noise). The RNSR-MW method, which combines multiple wavelet bases, demonstrates significant improvements over existing augmentation techniques across different CNN architectures including ResNet8, AlexNet, DenseNet, and ResNeXt.

## Method Summary
The proposed augmentation methods decompose IQ signals using DWT, replace detail coefficients with different sequences (AZSR, RZSR, RNSR), then reconstruct new samples via inverse DWT. RNSR-MW applies RNSR across multiple wavelet bases (haar, db5, sym5, coif3, rbio1.1) and combines results. The augmented dataset is used to train CNNs on RML1024 (12 modulation types, 2×1024 IQ samples) and HKDD AMC36 datasets under AWGN channel conditions with SNR range -20 to 30 dB. The method maintains detail coefficient power during replacement to preserve signal characteristics while introducing variability.

## Key Results
- RNSR-MW achieves 60.574% accuracy on RML1024 dataset, highest among all tested methods
- Wavelet-based augmentation consistently outperforms baseline methods across ResNet8, AlexNet, DenseNet, and ResNeXt architectures
- Performance improvements are maintained across SNR range from -20 to 30 dB
- Coif3 and db5 wavelet bases yield notably higher accuracy compared to other wavelet families

## Why This Works (Mechanism)

### Mechanism 1
- Detail coefficients contain high-frequency components that act as noise-like perturbations; random replacement preserves signal power while introducing variability
- Power matching ensures augmented samples maintain signal integrity without destroying modulation structure
- Break condition: If detail coefficients contain critical modulation features, random replacement will destroy classification-relevant structure

### Mechanism 2
- Multiple wavelet bases capture signal features at different time-frequency resolutions, providing complementary local features
- Each wavelet basis decomposes IQ sequences differently, and combining results spans a richer feature space
- Break condition: If the neural network cannot learn from mixed features across wavelet bases, accuracy may degrade

### Mechanism 3
- Replacing detail coefficients expands training set diversity without collecting more real data
- Generated samples with subtle variations prevent overfitting in few-shot scenarios
- Break condition: If generated samples are too dissimilar from real data, the network may learn spurious patterns

## Foundational Learning

- **Discrete Wavelet Transform (DWT)**: Decomposes signals into approximation and detail coefficients at different scales; needed to understand the augmentation process
  - Quick check: What is the difference between approximation and detail coefficients in DWT?

- **Power normalization**: Ensuring replacement sequences match the power of original detail coefficients; critical for maintaining signal integrity during augmentation
  - Quick check: How do you compute the power of a sequence and why is it important for this augmentation?

- **CNN architectures for modulation recognition**: Understanding ResNet, AlexNet, DenseNet, and ResNeXt differences helps interpret performance results
  - Quick check: What is the key architectural difference between ResNet and DenseNet?

## Architecture Onboarding

- **Component map**: Data generation → DWT decomposition → Replacement method (AZSR/RZSR/RNSR) → IDWT reconstruction → CNN training → Evaluation
- **Critical path**: Generating augmented samples correctly (DWT → replacement → reconstruction) is the core innovation; CNN performance depends on this pipeline
- **Design tradeoffs**: Single wavelet vs. multiple wavelets (RNSR-MW) trades off computational cost for potential accuracy gains
- **Failure signatures**: Low accuracy may indicate incorrect power normalization, inappropriate wavelet choice, or over-augmentation causing noise corruption
- **First 3 experiments**:
  1. Implement RNSR on a small dataset with Haar wavelet, verify power normalization matches detail coefficients
  2. Compare single wavelet vs. RNSR-MW with 2 wavelet bases on a simple CNN model
  3. Measure accuracy degradation when replacing too many detail coefficients (E too high)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of wavelet basis function affect the performance of RNSR-MW augmentation method across different modulation types and SNR ranges?
- Basis in paper: [explicit] The paper mentions that different wavelet bases (haar, db5, sym5, coif3, rbio1.1) are used in RNSR-MW method and compares their performance, noting that coif3 and db5 yield notably higher accuracy
- Why unresolved: The paper does not provide a detailed analysis of which specific modulation types benefit most from each wavelet basis or how the optimal wavelet choice varies with SNR
- What evidence would resolve it: Comprehensive testing of RNSR-MW with all wavelet bases across all modulation types and SNR values, identifying optimal combinations

### Open Question 2
- Question: What is the theoretical limit of performance improvement achievable through wavelet-based augmentation methods compared to other augmentation techniques?
- Basis in paper: [inferred] The paper demonstrates significant improvements over existing methods but does not establish theoretical bounds or compare against optimal augmentation strategies
- Why unresolved: The study focuses on empirical comparisons without exploring fundamental limits of data augmentation in modulation recognition tasks
- What evidence would resolve it: Theoretical analysis of information-theoretic limits of augmentation, or empirical comparison against an optimal (but computationally infeasible) augmentation method

### Open Question 3
- Question: How do the proposed augmentation methods perform on real-world datasets with non-ideal channel conditions and hardware imperfections?
- Basis in paper: [explicit] The paper uses simulated datasets (RML1024 and HKDD AMC36) with AWGN channels but does not test on real-world captured signals or include realistic hardware impairments
- Why unresolved: The evaluation is limited to controlled simulations, leaving uncertainty about practical effectiveness in operational environments
- What evidence would resolve it: Testing the augmentation methods on datasets of real captured signals, including effects of hardware impairments, multi-path fading, and interference

## Limitations

- Performance heavily depends on the assumption that detail coefficients contain non-essential information for modulation recognition
- RNSR-MW's performance gain from multiple wavelet bases needs verification across different modulation types and SNR ranges
- Paper does not explore edge cases where augmentation might fail, such as extremely low SNR conditions or highly similar modulation types

## Confidence

- **High confidence**: Wavelet-based augmentation works better than baseline methods across multiple CNN architectures
- **Medium confidence**: RNSR-MW superiority over single wavelet methods is demonstrated, but exact contribution of each wavelet basis remains unclear
- **Low confidence**: Paper does not fully explore edge cases where augmentation might fail

## Next Checks

1. Test RNSR-MW method on HKDD AMC36 dataset with 36 modulation types to verify scalability beyond 12-modulation RML1024 dataset
2. Perform ablation studies to determine which detail coefficients (approximation vs. detail) are most critical for maintaining classification accuracy during augmentation
3. Evaluate impact of varying number of augmentation operations (E) on both accuracy and computational cost to find optimal trade-offs