---
ver: rpa2
title: Elucidating the Exposure Bias in Diffusion Models
arxiv_id: '2308.15321'
source_url: https://arxiv.org/abs/2308.15321
tags:
- xxxt
- sampling
- bias
- exposure
- xxx0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the exposure bias problem in diffusion
  models, where the training input differs from the sampling input, leading to prediction
  errors and sampling drift. The authors analytically model the sampling distribution,
  attributing prediction error as the root cause of exposure bias.
---

# Elucidating the Exposure Bias in Diffusion Models

## Quick Facts
- arXiv ID: 2308.15321
- Source URL: https://arxiv.org/abs/2308.15321
- Reference count: 40
- Key outcome: 2.17 FID improvement on CIFAR-10 under 100-step unconditional generation

## Executive Summary
This paper investigates the exposure bias problem in diffusion models, where the training input differs from the sampling input, leading to prediction errors and sampling drift. The authors analytically model the sampling distribution, attributing prediction error as the root cause of exposure bias. They propose Epsilon Scaling, a training-free method that scales down the network output to mitigate the input mismatch between training and sampling. The method effectively reduces exposure bias by moving the sampling trajectory closer to the learned vector field.

## Method Summary
The paper proposes Epsilon Scaling, a training-free method that addresses exposure bias in diffusion models by scaling down the network output (epsilon) during sampling. The method applies a scaling factor λ_t to the predicted epsilon value at each timestep, effectively reducing the magnitude of the predicted correction. The scaling factor follows a linear schedule λ_t = k*t + b, where k decreases as the total sampling steps increase. This approach moves the sampling trajectory closer to the learned vector field without requiring any model retraining.

## Key Results
- 2.17 FID improvement on CIFAR-10 with 100-step unconditional generation
- Significant FID improvements across multiple diffusion frameworks (ADM, DDIM, EDM, LDM)
- Reduction in exposure bias metric δt across all tested models and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Epsilon Scaling reduces exposure bias by scaling down the network output (epsilon) during sampling, moving the sampling trajectory closer to the learned vector field.
- Mechanism: During training, the network sees clean inputs with ground truth targets, but during sampling it sees progressively noisier inputs that lead to biased predictions. Epsilon Scaling multiplies the predicted epsilon by a scaling factor λ_t < 1 at each timestep, effectively reducing the magnitude of the predicted correction and counteracting the accumulated drift.
- Core assumption: The prediction error of ε_θ(·) is the root cause of exposure bias, and the magnitude of the prediction error grows monotonically with each sampling step.
- Evidence anchors: [abstract] "Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon)" and [section] "We point out that the prediction error of xxt_θ − x0 is the root cause of exposure bias"
- Break condition: If the prediction error does not grow monotonically, or if the scaling factor λ_t becomes too aggressive and destabilizes the sampling.

### Mechanism 2
- Claim: The scaling factor λ_t should be smaller for longer sampling chains to counteract the accumulated prediction error.
- Mechanism: The design principle is that longer sampling steps lead to larger accumulated prediction errors, so the scaling factor λ_t must decrease with longer chains. The authors propose a linear schedule λ_t = k*t + b, where k decreases as T' increases.
- Core assumption: The prediction error grows linearly with the sampling step, and the optimal scaling factor is inversely related to the sampling chain length.
- Evidence anchors: [section] "the longer the sampling step, the smaller k we should use" and [section] "∆N(t) can be fitted by a quadratic function in the interval t ~ (5, T)"
- Break condition: If the prediction error does not grow linearly, or if the linear schedule λ_t = k*t + b is suboptimal.

### Mechanism 3
- Claim: Epsilon Scaling does not affect the precision and recall of the base model.
- Mechanism: By scaling down the network output, Epsilon Scaling corrects the magnitude error of ε_θ(·) without changing the direction of the prediction. This preserves the model's ability to generate diverse and high-quality samples.
- Core assumption: The direction of the prediction error is less important than the magnitude error, and correcting the magnitude does not affect the precision and recall.
- Evidence anchors: [section] "Epsilon Scaling corrects the magnitude error of ε_θ(·), but not the direction error" and [section] "Epsilon Scaling does not affect the precision and recall, and we report these results in Appendix"
- Break condition: If the direction of the prediction error becomes significant, or if correcting the magnitude affects the precision and recall.

## Foundational Learning

- Concept: Diffusion models and the exposure bias problem
  - Why needed here: Understanding the exposure bias problem is crucial for grasping the motivation behind Epsilon Scaling and its effectiveness.
  - Quick check question: What is the exposure bias problem in diffusion models, and how does it affect the sampling process?

- Concept: Vector fields and gradient descent
  - Why needed here: Epsilon Scaling relies on the concept of vector fields and gradient descent to correct the sampling trajectory.
  - Quick check question: How do vector fields and gradient descent relate to the sampling process in diffusion models?

- Concept: Parameter scaling and its effects on model behavior
  - Why needed here: Epsilon Scaling involves scaling the network output by a factor λ_t, which requires an understanding of how parameter scaling affects model behavior.
  - Quick check question: What are the potential effects of scaling the network output on the sampling process and the generated samples?

## Architecture Onboarding

- Component map: Compute ε_θ(·) -> Apply scaling factor λ_t -> Use scaled output for sampling
- Critical path: The critical path for Epsilon Scaling is: compute ε_θ(·) → scale by λ_t → use scaled output for sampling
- Design tradeoffs: The main tradeoff is between the aggressiveness of the scaling factor λ_t and the stability of the sampling process. A more aggressive scaling factor may correct the exposure bias more effectively but could also destabilize the sampling.
- Failure signatures: If the scaling factor λ_t is too aggressive, the sampling process may become unstable or produce low-quality samples. If the scaling factor is too conservative, the exposure bias may not be effectively corrected.
- First 3 experiments:
  1. Implement Epsilon Scaling on a simple diffusion model (e.g., DDPM) and compare the sampling quality with and without Epsilon Scaling.
  2. Vary the scaling factor λ_t and observe its effects on the sampling process and the generated samples.
  3. Apply Epsilon Scaling to a more complex diffusion model (e.g., ADM) and evaluate its effectiveness on a challenging dataset (e.g., FFHQ).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical relationship between the prediction error (xxxt^θ - xxx0) and the variance error (δt) in diffusion models?
- Basis in paper: [explicit] The paper analytically models the sampling distribution and attributes prediction error as the root cause of exposure bias, showing that variance of sampling distribution is always larger than training distribution by a magnitude related to prediction error.
- Why unresolved: While the paper establishes a connection between prediction error and variance error, it doesn't provide a precise mathematical formula linking the two. The relationship is shown qualitatively through variance comparison but not quantified.
- What evidence would resolve it: A formal mathematical derivation showing the exact relationship between prediction error magnitude and variance error δt, potentially including sensitivity analysis of how prediction error scales affect δt.

### Open Question 2
- Question: How does Epsilon Scaling perform on diffusion models for modalities beyond images, such as audio or video generation?
- Basis in paper: [inferred] The paper focuses exclusively on image datasets (CIFAR-10, LSUN, FFHQ, ImageNet) and doesn't test Epsilon Scaling on other data types like audio or video.
- Why unresolved: The effectiveness of Epsilon Scaling on non-image data is unknown. Different modalities may have different noise characteristics and prediction error patterns that could affect Epsilon Scaling's performance.
- What evidence would resolve it: Experiments applying Epsilon Scaling to diffusion models for audio synthesis or video generation, measuring FID or equivalent metrics, and comparing against baseline models.

### Open Question 3
- Question: What is the theoretical limit of Epsilon Scaling's effectiveness as the number of sampling steps approaches infinity?
- Basis in paper: [explicit] The paper shows that Epsilon Scaling's parameter k approaches 0 as sampling steps increase, suggesting diminishing returns for longer sampling chains.
- Why unresolved: The paper demonstrates this trend empirically but doesn't establish a theoretical framework for predicting Epsilon Scaling's effectiveness at extreme step counts or explain why the parameter k behaves this way.
- What evidence would resolve it: A theoretical analysis of Epsilon Scaling's scaling schedule λt in the limit of infinite sampling steps, potentially including asymptotic analysis of prediction error accumulation and its relationship to λt.

## Limitations
- The analysis of prediction error growth relies on empirical observations rather than rigorous theoretical guarantees
- The linear scheduling assumption for λ_t lacks strong validation across diverse model architectures and datasets
- The method's effectiveness on non-image modalities remains untested

## Confidence
- High: The core observation that input mismatch between training and sampling causes exposure bias
- Medium: The effectiveness of Epsilon Scaling in reducing exposure bias on tested datasets
- Low: The theoretical justification for the linear scaling schedule and its universality

## Next Checks
1. **Cross-architecture validation**: Apply Epsilon Scaling to diffusion models with different noise schedules (e.g., variance-exploding vs. variance-preserving) and verify consistent improvements in FID scores across all variants.

2. **Ablation study on scaling schedules**: Systematically compare the proposed linear schedule against alternative scheduling functions (quadratic, exponential decay) to determine if the linear form is optimal or merely sufficient.

3. **Long-chain stability analysis**: Test Epsilon Scaling on sampling chains with 1000+ steps to verify that the method maintains stability and continues to reduce exposure bias without introducing new artifacts or mode collapse.