---
ver: rpa2
title: 'Physical Adversarial Attacks for Surveillance: A Survey'
arxiv_id: '2305.01074'
source_url: https://arxiv.org/abs/2305.01074
tags:
- adversarial
- attacks
- physical
- recognition
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey paper comprehensively reviews recent advances in physical
  adversarial attacks targeting surveillance systems. The paper proposes a unified
  framework to analyze these attacks across four key surveillance tasks: detection,
  identification, tracking, and action recognition.'
---

# Physical Adversarial Attacks for Surveillance: A Survey

## Quick Facts
- arXiv ID: 2305.01074
- Source URL: https://arxiv.org/abs/2305.01074
- Reference count: 40
- Key outcome: Comprehensive survey proposing unified framework for physical adversarial attacks on surveillance systems across four key tasks

## Executive Summary
This survey paper provides a systematic review of physical adversarial attacks targeting surveillance systems, covering human detection, identification, tracking, and action recognition. The paper proposes a unified framework that analyzes these attacks across different physical accessories (t-shirts, glasses, hats, stickers) and sensing modalities (visible, infrared, LiDAR, multispectral). Physical adversarial attacks involve learning perceptible patterns that can be printed on physical items to mislead surveillance systems in real-world conditions. The survey covers both conventional human-designed methods and modern machine-learned approaches, examining attacks across various spectra while also reviewing defense mechanisms and identifying key challenges in the field.

## Method Summary
The paper proposes a unified framework for analyzing physical adversarial attacks on surveillance systems. The framework involves learning perceptible patterns through iterative optimization that maximizes task loss (detection, identification, tracking, or action recognition failure) while enforcing physical constraints (printability, smoothness via total variation) to ensure the adversarial pattern remains effective when transferred from digital to physical domain. The learned patterns are mapped onto physical accessories like t-shirts, glasses, hats, or stickers, which are then worn or carried by subjects to mislead surveillance systems in real-world conditions. The survey examines attacks across visible, infrared, LiDAR, and multispectral spectra, covering both human-designed conventional methods and modern machine-learned approaches.

## Key Results
- Physical adversarial patches can mislead detection systems by maximizing task loss while ensuring printability through physical loss constraints
- Adversarial patterns can exploit transferability property to mislead unknown surveillance models across different architectures
- Physical ensemble attacks can be designed to be effective across multiple surveillance tasks through min-max optimization

## Why This Works (Mechanism)

### Mechanism 1
Physical adversarial patches mislead detection systems by maximizing task loss while ensuring printability through physical loss constraints. The adversarial patch is learned through iterative optimization, minimizing detection loss while enforcing constraints like total variation to ensure smoothness and printability. The core assumption is that the learned adversarial pattern remains effective under real-world imaging conditions including non-rigid body deformation and varying lighting.

### Mechanism 2
Adversarial patterns exploit the transferability property, allowing them to mislead unknown surveillance models. Adversarial examples learned to attack one model can be effective against different models due to shared vulnerabilities in deep learning architectures. The core assumption is that underlying vulnerabilities in deep learning models are consistent across different architectures and training datasets.

### Mechanism 3
Physical adversarial attacks can be designed to be effective across multiple surveillance tasks by optimizing for worst-case scenarios. Ensemble attacks optimize adversarial patterns to maximize failure across multiple detectors, ensuring robustness against different surveillance systems. The core assumption is that the adversarial pattern that works for one task can be generalized to work across multiple tasks with minimal loss in effectiveness.

## Foundational Learning

- **Deep learning vulnerabilities to adversarial examples**: Understanding why deep learning models are susceptible to adversarial attacks is crucial for designing effective physical adversarial patterns. Quick check: What property of deep learning models makes them vulnerable to adversarial examples, even when the perturbations are imperceptible?

- **Physical imaging conditions and their impact on adversarial patterns**: Real-world conditions like lighting, camera angle, and subject movement can degrade the effectiveness of adversarial patterns, so understanding these factors is essential. Quick check: How do factors like non-rigid body deformation and varying lighting conditions affect the performance of physical adversarial patches?

- **Optimization techniques for generating adversarial examples**: The effectiveness of physical adversarial attacks depends on the optimization method used to generate the adversarial patterns. Quick check: What optimization techniques are commonly used to generate adversarial examples, and how do they differ in their approach to minimizing task loss and enforcing physical constraints?

## Architecture Onboarding

- **Component map**: Adversarial pattern generation -> Physical mapping -> Real-world deployment -> Surveillance task evaluation
- **Critical path**: Adversarial pattern generation → Physical mapping → Real-world deployment → Surveillance task evaluation
- **Design tradeoffs**: Pattern complexity vs. printability (more complex patterns may be more effective but harder to print accurately); Task-specific vs. multi-task optimization (optimizing for single task may yield better results, but multi-task provides broader coverage); Digital vs. physical constraints (digital attacks can be more precise, but physical attacks must account for real-world limitations)
- **Failure signatures**: Pattern degradation (printed pattern loses detail or clarity, failing to mislead surveillance system); Lighting sensitivity (adversarial pattern highly sensitive to lighting changes, failing under different lighting conditions); Camera angle dependence (pattern only effective from certain angles, failing when subject viewed from different perspectives)
- **First 3 experiments**: 1) Generate simple adversarial patch using proposed framework and test against basic object detector like YOLOv3; 2) Print adversarial patch and evaluate performance in controlled environment with varying lighting conditions; 3) Extend adversarial patch to t-shirt design and test effectiveness in real-world scenarios with multiple camera angles

## Open Questions the Paper Calls Out

1. **How effective are adversarial attacks against multimodal surveillance systems that combine visible spectrum, infrared, and LiDAR data?** Current research primarily focuses on single-modality attacks, and there is no established methodology for creating adversarial examples that remain effective across multiple sensing modalities simultaneously.

2. **What are the most effective defense mechanisms against physical adversarial attacks in real-world surveillance scenarios?** Most existing defense mechanisms have only been tested in controlled digital environments rather than against actual physical adversarial examples in realistic surveillance conditions.

3. **How can adversarial attacks be designed to remain effective against video-based surveillance tasks like gait recognition and action recognition in the physical world?** Most physical adversarial attacks focus on static images, but video-based surveillance tasks require understanding both spatial and temporal information, presenting additional challenges for maintaining attack effectiveness.

## Limitations

- Effectiveness of physical adversarial patterns heavily dependent on real-world conditions difficult to model accurately
- Most existing work focuses on single-image attacks rather than video sequences, leaving gaps in understanding temporal performance
- Transferability claims across different surveillance architectures need more empirical validation

## Confidence

**High Confidence**: The survey's framework for categorizing physical adversarial attacks by task and attack type is well-supported by literature
**Medium Confidence**: Claims about optimization methods and loss functions are generally accurate but lack sufficient detail on relative performance
**Low Confidence**: Transferability of adversarial patterns across architectures and effectiveness of multi-task optimization strategies lack comprehensive empirical validation

## Next Checks

1. **Print Quality Impact Test**: Conduct controlled experiments varying printer resolution, material types, and printing methods to quantify how these factors affect the effectiveness of adversarial patterns in real-world conditions

2. **Temporal Attack Robustness**: Design experiments to evaluate how physical adversarial attacks perform over extended video sequences, measuring degradation in effectiveness due to subject movement, camera motion, and temporal variations

3. **Cross-Architecture Transferability**: Systematically test adversarial patterns learned on one surveillance architecture against multiple different architectures (including newer models like EfficientDet, CenterNet, and transformer-based detectors) to validate transferability claims across diverse implementations