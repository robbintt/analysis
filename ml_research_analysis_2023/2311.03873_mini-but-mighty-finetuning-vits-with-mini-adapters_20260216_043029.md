---
ver: rpa2
title: 'Mini but Mighty: Finetuning ViTs with Mini Adapters'
arxiv_id: '2311.03873'
source_url: https://arxiv.org/abs/2311.03873
tags:
- adapters
- mimi
- adapter
- parameters
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiMi, a training framework designed to efficiently
  fine-tune Vision Transformers (ViTs) with small adapters. The key idea is to iteratively
  reduce the size of adapters by identifying and removing less important neurons.
---

# Mini but Mighty: Finetuning ViTs with Mini Adapters

## Quick Facts
- arXiv ID: 2311.03873
- Source URL: https://arxiv.org/abs/2311.03873
- Authors: [Not specified in source]
- Reference count: 40
- One-line primary result: Introduces MiMi, an iterative adapter pruning framework that outperforms existing parameter-efficient transfer learning methods across 29 datasets while using significantly fewer trainable parameters

## Executive Summary
This paper introduces MiMi, a training framework for efficiently fine-tuning Vision Transformers (ViTs) using small adapters. The key innovation is an iterative approach that starts with large adapters and progressively prunes less important neurons, addressing the optimization challenges of directly training tiny adapters. MiMi achieves state-of-the-art performance across multiple benchmarks while significantly reducing the number of trainable parameters, demonstrating superior parameter efficiency for adapting pre-trained ViTs to new tasks.

## Method Summary
MiMi is a parameter-efficient transfer learning method that fine-tunes Vision Transformers through iteratively pruned adapter modules. The framework starts with large adapters and progressively reduces their size by identifying and removing less important neurons using a novel importance scoring function. This approach addresses the optimization difficulties of directly training small adapters by first optimizing larger adapters before pruning. The method automatically estimates the optimal hidden dimension for each adapter and can completely remove adapters from certain layers when beneficial, redistributing parameters to other adapters as needed.

## Key Results
- Outperforms existing parameter-efficient transfer learning methods across three dataset benchmarks (DomainNet, VTAB, Multi-task)
- Achieves better accuracy-parameter trade-offs across 29 datasets total
- Significantly reduces trainable parameters while maintaining high performance, demonstrating effectiveness in adapting ViTs to new tasks with minimal storage and computational costs
- Shows gains of more than 3% and 2% in average accuracy over BitFit for Multi-Task and DomainNet benchmarks respectively, while using fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Starting from large adapters and iteratively pruning neurons improves performance over directly training small adapters
- Mechanism: Large adapters can be optimized more effectively in early training cycles, avoiding the optimization difficulty of directly training tiny adapters. Gradual pruning preserves useful neurons while removing less important ones
- Core assumption: Sequentially training and pruning a large network finds small networks with better performance than directly training small networks from scratch
- Evidence anchors: The abstract and section explicitly state that sequential training and pruning is successful while direct training of small networks suffers from optimization issues

### Mechanism 2
- Claim: The proposed scoring function I_ij effectively compares neuron importance across adapters with different dimensions
- Mechanism: The score normalizes by the total number of parameters associated with a specific adapter dimension, enabling fair comparison across adapters with different input and hidden layer sizes
- Core assumption: Normalizing the sum of L1 norms of corresponding row and column by total parameters provides a meaningful comparison across adapters
- Evidence anchors: The abstract and section describe the scoring function as specifically designed for adapters to compare neuron importance across layers

### Mechanism 3
- Claim: Global neuron selection outperforms local neuron selection by removing entire adapters when necessary and redistributing parameters
- Mechanism: Global selection can completely remove adapters from certain layers while adding more parameters to other adapters, allowing task-specific adaptation across layers
- Core assumption: Different layers require different amounts of adaptation, and some adapters can be completely removed for certain tasks
- Evidence anchors: The section shows that MiMi outperforms BitFit with similar parameters, and visualization demonstrates complete removal of certain adapters while redistributing parameters

## Foundational Learning

- Concept: Vision Transformer architecture and adapter modules
  - Why needed here: Understanding how adapters are injected into ViT layers and how they function is crucial for implementing MiMi
  - Quick check question: How do adapter modules modify the input to a ViT layer?

- Concept: Pruning strategies and their impact on neural network performance
  - Why needed here: MiMi relies on iterative pruning of adapter neurons, so understanding pruning mechanics and potential pitfalls is essential
  - Quick check question: What are the potential risks of pruning neurons too aggressively in a neural network?

- Concept: Importance scoring functions for neuron selection
  - Why needed here: The effectiveness of MiMi depends on the scoring function's ability to identify unimportant neurons for removal
  - Quick check question: How does normalizing the importance score by total parameters enable comparison across adapters with different dimensions?

## Architecture Onboarding

- Component map: Pre-trained ViT model -> Frozen backbone weights -> Adapter modules (MLPs) -> Importance scoring function -> Iterative pruning -> Fine-tuned model

- Critical path: 1. Initialize large adapters with σ0 compression rate 2. Train adapters for 100 epochs 3. Compute importance scores for all neurons 4. Remove bottom ρ fraction of neurons 5. Update adapter dimensions 6. Repeat steps 2-5 until target compression rate achieved

- Design tradeoffs:
  - Initial adapter size vs. number of pruning iterations
  - Compression rate per iteration (ρ) vs. training stability
  - Global vs. local neuron selection strategy
  - Parameter magnitude vs. activation-based scoring

- Failure signatures:
  - Performance degradation after pruning iterations
  - Inconsistent performance across different datasets
  - Difficulty converging during initial adapter training

- First 3 experiments:
  1. Train MiMi with different initial compression rates (σ0) on a single dataset and compare final performance
  2. Vary the pruning fraction (ρ) and observe its effect on convergence and final accuracy
  3. Compare global vs. local neuron selection on a multi-task benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MiMi perform on different ViT backbones beyond Swin-T?
- Basis in paper: The authors mention evaluating MiMi on Swin-S/L, ViT, and CVT, but do not provide detailed results for these backbones
- Why unresolved: The paper only presents a summary of results for these backbones in Table 3, lacking detailed analysis or comparison
- What evidence would resolve it: Detailed performance metrics and comparisons for MiMi on Swin-S/L, ViT, and CVT backbones across various datasets

### Open Question 2
- Question: What is the impact of different neuron selection strategies on MiMi's performance?
- Basis in paper: The authors discuss various neuron selection strategies (local, global, random, gradient-based) in the ablation study but do not provide a comprehensive comparison
- Why unresolved: The ablation study focuses on a subset of strategies and does not explore the full range of possible neuron selection methods
- What evidence would resolve it: A thorough comparison of MiMi's performance using all discussed neuron selection strategies across multiple datasets

### Open Question 3
- Question: How does MiMi's performance scale with the size of the target dataset?
- Basis in paper: The authors evaluate MiMi on datasets with varying sizes, but do not explicitly analyze the relationship between dataset size and performance
- Why unresolved: The paper does not provide a systematic analysis of how MiMi's performance changes as the target dataset size increases or decreases
- What evidence would resolve it: A study that evaluates MiMi's performance on datasets with a wide range of sizes, from small to large, to identify any scaling trends

## Limitations

- Limited to Vision Transformer architectures, primarily tested on Swin-T backbone
- Performance evaluation focused on specific benchmark datasets (DomainNet, VTAB, Multi-task) with 29 datasets total
- Key implementation details like exact normalization in scoring function are not fully specified

## Confidence

**High Confidence**: The iterative pruning approach outperforms direct training of small adapters. This is supported by extensive experimental results across 29 datasets and multiple compression ranges, showing consistent improvements over baseline methods.

**Medium Confidence**: The global neuron selection strategy provides benefits over local selection. While experimental results show advantages, the mechanism could be task-dependent and may not generalize universally across all vision tasks.

**Low Confidence**: The specific formulation of the importance scoring function is optimal. The paper introduces a novel scoring function but lacks comparative analysis with alternative scoring approaches, making it difficult to assess whether this is the best possible formulation.

## Next Checks

1. **Cross-Architecture Validation**: Test MiMi on different ViT architectures (DeiT, ConvNeXt) to verify generalization beyond Swin-T and identify any architecture-specific behaviors.

2. **Ablation on Scoring Function**: Implement alternative scoring functions based on activation magnitudes and compare performance to validate whether parameter-based scoring is optimal or if other formulations could work better.

3. **Extreme Compression Analysis**: Evaluate MiMi's performance at very high compression rates (>1000×) to identify the breaking point where the iterative approach fails and determine the practical limits of the method.