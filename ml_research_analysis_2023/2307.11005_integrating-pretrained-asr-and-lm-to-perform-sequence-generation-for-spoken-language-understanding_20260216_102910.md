---
ver: rpa2
title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken
  Language Understanding
arxiv_id: '2307.11005'
source_url: https://arxiv.org/abs/2307.11005
tags:
- decoder
- pass
- sasr
- spoken
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a three-pass end-to-end SLU system that integrates ASR
  and LM subnetworks for sequence generation tasks. Our model conditions on ASR and
  LM representations in a deliberation subnetwork to make final predictions.
---

# Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding

## Quick Facts
- **arXiv ID:** 2307.11005
- **Source URL:** https://arxiv.org/abs/2307.11005
- **Reference count:** 0
- **Primary result:** Three-pass SLU system integrating pretrained ASR and LM subnetworks improves performance on sequence generation tasks, especially for acoustically challenging utterances.

## Executive Summary
This paper proposes a three-pass end-to-end spoken language understanding (SLU) system that effectively integrates pretrained automatic speech recognition (ASR) and language model (LM) subnetworks for sequence generation tasks. The system uses a deliberation subnetwork that conditions on both ASR and LM representations to make final predictions, helping to recover from ASR errors and improve overall SLU performance. The approach is evaluated on two benchmark datasets (SLURP and SLUE) and demonstrates improved performance over traditional cascaded and end-to-end SLU models, particularly on acoustically challenging utterances.

## Method Summary
The three-pass SLU system consists of an ASR subnetwork, an LM subnetwork, and a deliberation subnetwork. The ASR subnetwork generates a transcript from input speech, while the LM subnetwork generates initial SLU predictions from the ASR transcript. The deliberation subnetwork makes final predictions by conditioning on both acoustic embeddings from the ASR encoder and joint embeddings produced by concatenating ASR and LM representations. The system uses a probabilistic formulation based on maximum a posteriori (MAP) theory to model the posterior distribution P(Y|X), effectively incorporating both pretrained ASR and LM models. Training occurs in three steps: training the ASR subnetwork, training the LM subnetwork with ground truth transcripts, and training the deliberation subnetwork using outputs from the first two steps.

## Key Results
- The three-pass SLU system shows improved performance over cascaded and end-to-end SLU models on SLURP and SLUE datasets.
- The model demonstrates particularly strong performance on acoustically challenging utterances, recovering from ASR errors through deliberation.
- Cross attention integration in the deliberation decoder proves more effective than simple concatenation for conditioning on ASR and LM representations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-pass SLU system improves performance on acoustically challenging utterances by recovering from ASR errors through deliberation.
- Mechanism: The deliberation subnetwork conditions on both ASR and LM representations to make final predictions, allowing it to correct errors in the ASR transcript.
- Core assumption: Errors in the ASR transcript can be recovered by conditioning on both acoustic and semantic representations during deliberation.
- Evidence anchors:
  - [abstract]: "Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances."
  - [section]: "Our approach of E2E integration of ASR and LM subnetworks helps avoid cascading errors from the ASR transcript."
  - [corpus]: Weak evidence - related papers focus on general SLU improvements but don't specifically address error recovery through deliberation.
- Break condition: If the deliberation subnetwork cannot effectively combine ASR and LM representations, or if the errors in ASR transcript are too severe to recover from.

### Mechanism 2
- Claim: Integrating pretrained ASR and LM subnetworks into the SLU formulation using a probabilistic approach improves sequence generation performance.
- Mechanism: The three-pass architecture models the posterior distribution P(Y|X) by maximizing P(Y|X, Sasr, Y_lm)P(Y_lm|Sasr)P(Sasr|X), effectively incorporating both pretrained ASR and LM models.
- Core assumption: Pretrained ASR and LM models can be effectively integrated into the SLU framework using a probabilistic formulation.
- Evidence anchors:
  - [abstract]: "Our proposed three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks."
  - [section]: "Our formulation incorporates the ASR encoder embedding, ASR decoder embedding, and pretrained LM decoder embedding using an encoder-decoder architecture."
  - [corpus]: Moderate evidence - related work on integrating pretrained models for SLU, but specific probabilistic formulation is novel.
- Break condition: If the probabilistic formulation cannot be effectively computed or if the pretrained models are not compatible with the SLU task.

### Mechanism 3
- Claim: The use of cross attention integration in the deliberation decoder improves performance by better conditioning on ASR and LM representations.
- Mechanism: The deliberation decoder uses multi-sequence cross attention to condition on acoustic embedding casr and joint embedding cdel, which is produced by concatenating ASR decoder and LM representations.
- Core assumption: Cross attention is a more effective way of conditioning on ASR and LM representations than simple concatenation.
- Evidence anchors:
  - [abstract]: "Inspired by prior work on compositional speech processing models [8, 35], we also experiment with incorporating acoustic embedding casr using the multi-sequence cross attention in our deliberation decoder."
  - [section]: "Our experiments show that cross attention integration (Eq. 17) is a better way of conditioning on ASR and LM output than concatenation integration (Eq. 11)."
  - [corpus]: Weak evidence - related papers mention cross attention but don't specifically address its use in SLU deliberation.
- Break condition: If the cross attention mechanism cannot effectively capture the relationships between ASR and LM representations, or if it introduces too much computational overhead.

## Foundational Learning

- Concept: Probabilistic formulation of SLU using maximum a posteriori (MAP) theory
  - Why needed here: The three-pass SLU system is based on modeling the posterior distribution P(Y|X) using MAP theory, which requires understanding of probabilistic modeling in NLP tasks.
  - Quick check question: How does the MAP theory relate to the three-pass SLU system's formulation of P(Y|X)?

- Concept: Sequence-to-sequence modeling in SLU
  - Why needed here: The three-pass SLU system uses sequence-to-sequence modeling, where the input is a speech sequence and the output is a sequence of labels. Understanding this concept is crucial for implementing and improving the system.
  - Quick check question: What are the key components of a sequence-to-sequence model, and how are they used in the three-pass SLU system?

- Concept: Pretrained models and transfer learning
  - Why needed here: The three-pass SLU system leverages pretrained ASR and LM models. Understanding how to effectively use and fine-tune pretrained models is essential for implementing this system.
  - Quick check question: What are the benefits and challenges of using pretrained models in the context of the three-pass SLU system?

## Architecture Onboarding

- Component map: Input speech -> ASR subnetwork -> LM subnetwork -> Deliberation subnetwork -> Final SLU predictions
- Critical path: Input speech → ASR subnetwork → LM subnetwork → Deliberation subnetwork → Final SLU predictions
- Design tradeoffs:
  - Complexity vs. performance: The three-pass architecture is more complex but potentially offers better performance
  - Pretrained models vs. training from scratch: Using pretrained models can save training time but may limit customization
  - Cross attention vs. concatenation: Cross attention integration may offer better performance but at the cost of increased complexity
- Failure signatures:
  - Poor ASR performance leading to incorrect ASR transcripts
  - LM subnetwork failing to generate meaningful initial SLU predictions
  - Deliberation subnetwork unable to effectively combine ASR and LM representations
  - Pretrained models not generalizing well to the specific SLU task
- First 3 experiments:
  1. Implement the three-pass SLU system with concatenation integration and evaluate on a small dataset
  2. Replace concatenation with cross attention integration and compare performance
  3. Conduct an ablation study by removing the residual connection between LM posteriors and deliberation predictions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions arise:

### Open Question 1
- Question: How does the proposed 3-pass SLU model perform on languages other than English, especially low-resource languages?
- Basis in paper: [inferred] The paper evaluates the model on English datasets (SLURP and SLUE) but does not explore its performance on other languages or low-resource scenarios.
- Why unresolved: The study focuses on benchmark datasets in English, leaving the generalizability of the approach to other languages untested.
- What evidence would resolve it: Experiments on multilingual datasets or low-resource language benchmarks would demonstrate the model's cross-linguistic applicability.

### Open Question 2
- Question: Can the 3-pass SLU model be extended to handle streaming or real-time applications effectively?
- Basis in paper: [inferred] The paper does not address latency or streaming capabilities, focusing instead on batch processing and performance metrics.
- Why unresolved: The architecture and training process described are not explicitly optimized for low-latency or streaming scenarios, which are critical for real-world applications.
- What evidence would resolve it: Testing the model in streaming settings with metrics like real-time factor (RTF) and latency would clarify its suitability for real-time use cases.

### Open Question 3
- Question: How does the model handle out-of-vocabulary (OOV) words or rare entities in spoken utterances?
- Basis in paper: [explicit] The paper mentions vocabulary mismatch as a challenge for pretrained models but does not explicitly address OOV handling in the 3-pass SLU model.
- Why unresolved: The study does not provide details on how the model deals with OOV words or rare entities, which are common in spontaneous speech.
- What evidence would resolve it: Experiments with datasets containing OOV words or rare entities, along with analysis of the model's robustness, would provide insights into its handling of such cases.

### Open Question 4
- Question: What is the impact of varying the size of the pretrained LM (e.g., BART-large) on the model's performance?
- Basis in paper: [explicit] The paper uses BART-large as the pretrained LM but does not explore the effect of using smaller or larger LMs.
- Why unresolved: The choice of BART-large is not justified or compared against other LM sizes, leaving the impact of LM scale on performance unexplored.
- What evidence would resolve it: Ablation studies with different LM sizes (e.g., BART-base, BART-large, BART-3B) would clarify the trade-off between model size and performance.

## Limitations
- The evaluation is limited to two relatively small datasets (SLURP with 40.2 hours and SLUE with 14.5 hours), raising questions about scalability.
- The paper lacks detailed analysis of specific acoustic conditions that benefit from the three-pass approach.
- No quantitative analysis of computational overhead and inference latency is provided.
- The error recovery mechanism through deliberation is largely theoretical without granular empirical evidence.

## Confidence
- **High Confidence**: The core architectural contribution of integrating pretrained ASR and LM subnetworks into a deliberation framework is technically sound and well-supported by experimental results.
- **Medium Confidence**: The claim about superior performance on acoustically challenging utterances is supported by overall F1 improvements but lacks granular analysis.
- **Low Confidence**: The paper's claims about error recovery through deliberation are largely theoretical without sufficient empirical evidence showing exactly how and when the deliberation subnetwork successfully recovers from specific ASR errors.

## Next Checks
1. **Error Analysis Validation**: Conduct detailed error analysis on the SLURP and SLUE datasets to identify specific types of ASR errors (deletions, insertions, substitutions) that the deliberation subnetwork successfully recovers from versus those it fails to correct. This should include examples of both successful and failed recovery cases with qualitative analysis.

2. **Scaling Study**: Evaluate the three-pass SLU system on larger, more diverse SLU datasets (e.g., SNIPS, Fluent Speech Commands, or newer larger-scale datasets) to assess whether the performance gains observed on SLURP and SLUE scale with dataset size and diversity. Include analysis of training stability and convergence speed.

3. **Computational Overhead Analysis**: Measure and compare the inference latency and computational requirements of the three-pass system against baseline cascaded and end-to-end approaches across different hardware configurations. Quantify the trade-off between the performance gains and the increased computational cost to determine practical deployment viability.