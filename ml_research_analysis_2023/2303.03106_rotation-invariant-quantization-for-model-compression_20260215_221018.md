---
ver: rpa2
title: Rotation Invariant Quantization for Model Compression
arxiv_id: '2303.03106'
source_url: https://arxiv.org/abs/2303.03106
tags:
- quantization
- compression
- rate
- layer
- distortion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Rotation-Invariant Quantization (RIQ), a
  post-training model compression method that minimizes model entropy while satisfying
  a deviation constraint. RIQ quantizes each layer proportionally to its norm, yielding
  a mixed-precision solution optimal in terms of rate-distortion.
---

# Rotation Invariant Quantization for Model Compression

## Quick Facts
- arXiv ID: 2303.03106
- Source URL: https://arxiv.org/abs/2303.03106
- Reference count: 40
- Key outcome: Achieves compression ratios up to ×52.9 with <0.4% accuracy loss using rotation-invariant quantization

## Executive Summary
This paper introduces Rotation-Invariant Quantization (RIQ), a post-training model compression method that minimizes model entropy while satisfying a deviation constraint. RIQ quantizes each layer proportionally to its norm, yielding a mixed-precision solution optimal in terms of rate-distortion. Theoretical analysis shows the minimizing distribution is a product of spherical distributions, which RIQ searches over efficiently. Experiments demonstrate RIQ achieves state-of-the-art compression ratios with minimal accuracy degradation across various models and tasks.

## Method Summary
RIQ is a post-training quantization method that compresses neural network models by minimizing entropy under a deviation constraint. The method quantizes each layer proportionally to its norm using a single parameter k, ensuring rotation-invariant quantization. After quantization, Adaptive Number System (ANS) entropy coding is applied to achieve compression close to the theoretical entropy limit. The approach involves an iterative search over k to find the optimal solution that satisfies the deviation constraint while maximizing compression.

## Key Results
- Achieves compression ratios up to ×52.9 with <0.4% accuracy loss
- Outperforms recent quantization methods on diverse models (VGG, ResNet, ViT, YOLOv5, DistilBERT)
- Particularly effective on sparse models, achieving remarkable compression with negligible degradation
- Maintains cosine distance deviation within specified bounds while minimizing rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RIQ quantizes each layer proportionally to its norm, achieving rotation-invariant quantization.
- Mechanism: The quantization bin width Δℓ is set to ‖wℓ‖ / k, where k is a single parameter. This ensures the quantization is invariant to the orientation of the weight vectors, as the bin width scales with the norm, which is rotation-invariant.
- Core assumption: The deviation constraint can be satisfied by optimizing a single parameter k that governs the quantization bin width for all layers.
- Evidence anchors:
  - [abstract] "RIQ quantizes each layer proportionally to its norm, yielding a mixed-precision solution optimal in terms of rate-distortion."
  - [section] "Since both norm and cosine distance are invariant to rotations, this yields the optimal solution in terms of rate distortion."

### Mechanism 2
- Claim: The optimal rate-distortion minimizing distribution is a product of spherical (rotation-invariant) distributions.
- Mechanism: By modeling the quantization as a random rotation of the weights, the analysis reveals that the distribution of the quantized weights in each layer is spherical, and the joint distribution is a product of these spherical distributions.
- Core assumption: The rate-distortion function can be minimized by considering a product distribution constructed from the layers' spherical distributions.
- Evidence anchors:
  - [section] "The analysis reveals that the rate-distortion minimizing distribution for NN models is a spherical (rotation invariant) distribution constructed by the product of layers' spherical distribution."
  - [section] "Due to convexity, the rate achieved under this product distribution is bounded by a rate achieved under the layers' average spherical distribution."

### Mechanism 3
- Claim: The deviation constraint can be efficiently searched using a single parameter k that scales the quantization bin widths.
- Mechanism: The deviation is shown to scale as O(1/k²), allowing for an efficient search over k values to find the smallest k that satisfies the deviation constraint.
- Core assumption: The deviation decreases monotonically with increasing k, enabling a binary search approach to find the optimal k.
- Evidence anchors:
  - [section] "Interestingly, the monotonicity allows to search efficiently the optimal solution."
  - [section] "Since Δℓ(k) and εℓ are monotonically decreasing with k, then by Equation (6), the entropy increases with k."

## Foundational Learning

- Concept: Rate-distortion theory
  - Why needed here: Understanding the theoretical foundation for optimal quantization and compression, which guides the design of RIQ.
  - Quick check question: What is the rate-distortion function, and how does it relate to the optimal quantization of a source?

- Concept: Spherical distributions and rotation invariance
  - Why needed here: The analysis of RIQ relies on the properties of spherical distributions and their invariance to rotations, which are crucial for proving the optimality of the approach.
  - Quick check question: What is a spherical distribution, and why is it rotation-invariant?

- Concept: Entropy and its relation to compression
  - Why needed here: The goal of RIQ is to minimize the entropy of the quantized weights, which directly translates to higher compression ratios.
  - Quick check question: How does the entropy of a quantized source relate to its compressibility using entropy coding techniques like ANS?

## Architecture Onboarding

- Component map: Pre-trained model -> RIQ quantization with parameter k -> ANS compression -> Evaluation of deviation and accuracy
- Critical path: 1. Pre-trained model and calibration data 2. RIQ quantization with parameter k 3. ANS compression of quantized weights 4. Evaluation of deviation and accuracy
- Design tradeoffs: Single parameter k vs. per-layer optimization (simplicity and efficiency vs. potential for better optimization); Rotation-invariant quantization vs. non-invariant methods (optimality in terms of rate-distortion vs. potential for better performance in specific scenarios); ANS compression vs. other entropy coding methods (asymptotic optimality vs. computational complexity)
- Failure signatures: Deviation constraint not satisfied (chosen k is too small, leading to higher distortion than allowed); Suboptimal compression ratio (single parameter k does not adequately capture the optimal quantization for all layers); High computational cost (iterative search over k takes too long, especially for large models)
- First 3 experiments: 1. Verify the rotation-invariant property of RIQ by applying random rotations to the weight vectors and measuring the deviation; 2. Evaluate the efficiency of the search over k by measuring the number of iterations required to find the optimal solution for different model sizes; 3. Compare the compression ratio and accuracy of RIQ with other post-training quantization methods on a standard benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of compression achievable using RIQ when considering the entropy of the original model parameters?
- Basis in paper: [explicit] The paper states that RIQ aims to minimize model entropy and reach the entropy limit through compression while satisfying a distortion requirement.
- Why unresolved: While the paper demonstrates RIQ achieves impressive compression ratios, it doesn't explicitly calculate or compare the achieved entropy to the theoretical limit of the original model.
- What evidence would resolve it: A calculation of the entropy of the original model parameters and a comparison to the entropy achieved by RIQ after compression would provide insight into how close RIQ gets to the theoretical limit.

### Open Question 2
- Question: How does the choice of deviation constraint (D) impact the trade-off between compression ratio and model accuracy?
- Basis in paper: [explicit] The paper formulates the model compression problem as minimizing rate subject to a deviation constraint D. It also presents rate-distortion curves showing the relationship between deviation and compression ratio.
- Why unresolved: The paper demonstrates the impact of different deviation constraints on compression ratio and accuracy, but doesn't provide a systematic analysis of how varying D affects the trade-off across different models and tasks.
- What evidence would resolve it: A comprehensive study varying the deviation constraint D across a range of values for different models and tasks, analyzing the resulting compression ratios and accuracy drops, would reveal the sensitivity of the trade-off to the choice of D.

### Open Question 3
- Question: How does RIQ perform compared to other post-training quantization methods when applied to extremely sparse models?
- Basis in paper: [explicit] The paper mentions that RIQ achieves remarkable compression ratios (up to 52.9×) with sparse models, but doesn't provide a detailed comparison to other methods in this specific scenario.
- Why unresolved: While the paper demonstrates RIQ's effectiveness on sparse models, it doesn't compare its performance to other post-training quantization methods specifically designed for sparse models.
- What evidence would resolve it: A comparative study applying RIQ and other post-training quantization methods to extremely sparse models, evaluating their compression ratios and accuracy drops, would reveal RIQ's relative performance in this challenging scenario.

## Limitations
- Theoretical assumptions about weight distributions may not fully capture real-world neural network characteristics
- Single-parameter search may sacrifice some optimality compared to per-layer optimization
- Sparse model comparisons are less comprehensive than for dense models

## Confidence
- High confidence: The rotation-invariant quantization mechanism and its implementation
- Medium confidence: The theoretical optimality claims based on product distribution assumptions
- Medium confidence: The efficiency of the single-parameter search

## Next Checks
1. Analyze the actual weight distributions of quantized layers across different model architectures to verify how closely they follow the assumed spherical distributions
2. Systematically vary the parameter k across multiple orders of magnitude and measure the resulting accuracy-compression tradeoff curve to identify potential discontinuities
3. Test RIQ on models trained on datasets very different from ImageNet to evaluate whether the rotation-invariant property provides consistent benefits across diverse domains