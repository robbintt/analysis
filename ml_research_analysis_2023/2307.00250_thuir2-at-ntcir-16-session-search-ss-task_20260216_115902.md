---
ver: rpa2
title: THUIR2 at NTCIR-16 Session Search (SS) Task
arxiv_id: '2307.00250'
source_url: https://arxiv.org/abs/2307.00250
tags:
- session
- information
- search
- pre-trained
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Our team participated in both FOSS and POSS subtasks of the NTCIR-16
  Session Search Task, submitting five runs in total. We applied learning-to-rank
  models and fine-tuned pre-trained language models to leverage both ad-hoc data and
  session information.
---

# THUIR2 at NTCIR-16 Session Search (SS) Task

## Quick Facts
- arXiv ID: 2307.00250
- Source URL: https://arxiv.org/abs/2307.00250
- Reference count: 8
- Our assembled learning-to-rank model achieved the best performance in the preliminary evaluation, ranking first among all participants

## Executive Summary
Our team participated in both FOSS and POSS subtasks of the NTCIR-16 Session Search Task, submitting five runs in total. We applied learning-to-rank models and fine-tuned pre-trained language models to leverage both ad-hoc data and session information. Our assembled learning-to-rank model achieved the best performance in the preliminary evaluation, ranking first among all participants. In the final evaluation, our model using assembled traditional methods outperformed many complex pre-trained models, highlighting the effectiveness of traditional methods in document ranking tasks.

## Method Summary
The approach combined traditional IR methods (BM25, TF-IDF, F1-EXP) with fine-tuned pre-trained language models (BERT) using a learning-to-rank framework (LambdaMART). We implemented two main models: (1) Assembled Traditional Methods (ATM) using only traditional IR scores, and (2) Assembled Pre-trained Models and Traditional Methods (PMTM) using BERT fine-tuned on both ad-hoc data and session information. Session information was incorporated through Assembled Session Queries that concatenated previous queries and clicked document titles. The learning-to-rank model optimized NDCG@10 to combine features from multiple ranking methods.

## Key Results
- Assembled learning-to-rank model achieved first place in preliminary evaluation
- Traditional methods (BM25, TF-IDF, F1-EXP) assembled model outperformed many complex pre-trained models in final evaluation
- Session information fine-tuning provided small performance improvements when user intent remained consistent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning-to-rank models combining traditional IR methods with pre-trained language model scores can outperform single-model approaches in document ranking tasks.
- Mechanism: The learning-to-rank framework aggregates multiple feature scores (traditional IR scores like BM25, TF-IDF, F1-EXP, and pre-trained model relevance scores) into a unified ranking function optimized for NDCG@10, leveraging complementary strengths of different ranking approaches.
- Core assumption: The combined features from different ranking methods contain complementary information that a learning-to-rank model can effectively exploit.
- Evidence anchors:
  - [abstract]: "Our assembled learning-to-rank model achieved the best performance in the preliminary evaluation, ranking first among all participants."
  - [section 2.5]: "This run achieves the best performance among all participants in the preliminary evaluation. The good performance shows that the scores of fine-tuned pre-trained language model can improve the performance of the LambdaMART model."
- Break condition: When the features from different methods are highly correlated or when the learning-to-rank model cannot effectively learn to combine them due to insufficient training data or inappropriate model choice.

### Mechanism 2
- Claim: Traditional IR methods like BM25, TF-IDF, and F1-EXP remain competitive with advanced pre-trained models for document ranking, particularly when evaluation relies on manually labeled data.
- Mechanism: Traditional methods effectively capture keyword-based relevance matching, which aligns well with manual annotation practices that often focus on keyword presence in documents.
- Core assumption: Manual relevance judgments are primarily based on keyword matching rather than semantic understanding.
- Evidence anchors:
  - [abstract]: "In the final evaluation, our model using assembled traditional methods outperformed many complex pre-trained models, highlighting the effectiveness of traditional methods in document ranking tasks."
  - [section 2.3]: "To our surprise, this method gets the third-highest score in the final evaluation. It's better than many runs which use large-scale pre-trained language model such as BERT and Hierarchical Behavior Aware Transformers. We guess the reason is that the final evaluation results are based on manually labeled data. When labeling, it is mainly judged according to the keywords in the query and its appearance in the document, which is very similar to the traditional method."
- Break condition: When evaluation shifts to more semantic or user satisfaction-based metrics, or when queries require deeper semantic understanding beyond keyword matching.

### Mechanism 3
- Claim: Incorporating session information during fine-tuning of pre-trained language models can improve ranking performance by capturing user search intent evolution within sessions.
- Mechanism: The Assembled Session Query (ASQ) approach concatenates current query, previous queries, and clicked documents' titles to provide richer context about user information needs, which the fine-tuned BERT model can use to make more informed relevance judgments.
- Core assumption: User search intent can be inferred from their session behavior (previous queries and clicked documents), and this information improves ranking quality.
- Evidence anchors:
  - [section 2.4.3]: "The result of preliminary evaluation shows that the utilization of session information can improve the performance of ranking models to a small degree. We can conclude that fine-tuning with user behavior information can improve the document ranking model's performance because the Assembled Session Query(ASQ) contains more information of user's search intention compared to a single query."
- Break condition: When session information is noisy, when user intent changes frequently within a session, or when the model cannot effectively process the longer input sequences required by ASQ.

## Foundational Learning

- Concept: Learning-to-rank framework
  - Why needed here: The team used LambdaMART from Ranklib to assemble scores from different ranking methods (traditional IR and pre-trained models) into a unified ranking function optimized for NDCG.
  - Quick check question: What is the primary optimization metric used when training the LambdaMART model in this approach?

- Concept: Pre-trained language model fine-tuning for ranking
  - Why needed here: The team fine-tuned BERT on both ad-hoc data (query-document pairs with clicks) and session data (using Assembled Session Queries) to create ranking-specific models.
  - Quick check question: What loss function was used to optimize the BERT ranking model during fine-tuning?

- Concept: Feature engineering for learning-to-rank
  - Why needed here: The team extracted multiple features including scores from different IR methods, document/query lengths, and ranks, which were then used as input to the learning-to-rank model.
  - Quick check question: How many features were used in the Assembled Pre-trained Models and Traditional Methods (PMTM) model?

## Architecture Onboarding

- Component map: Data preprocessing -> Traditional IR scoring (BM25, TF-IDF, F1-EXP) -> Pre-trained model fine-tuning (BERT on ad-hoc data and session data) -> Feature extraction -> Learning-to-rank assembly (LambdaMART) -> Final ranking
- Critical path: The learning-to-rank assembly stage is critical as it combines all other components' outputs into the final ranking. Any failure in feature extraction or model scoring will propagate to this stage.
- Design tradeoffs: The team chose to use traditional IR methods alongside pre-trained models, trading potential gains from more sophisticated neural architectures for the interpretability and efficiency of traditional methods.
- Failure signatures: Poor performance on either FOSS or POSS subtasks while the other performs well, or degradation when session information is included despite theoretical benefits.
- First 3 experiments:
  1. Run baseline with only traditional IR methods (BM25 + TF-IDF + F1-EXP) to establish performance floor.
  2. Run with only pre-trained model scores (BERT fine-tuned on ad-hoc data) to evaluate standalone model performance.
  3. Run with session information included in fine-tuning to measure impact of behavioral signals.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of traditional IR methods like BM25, TF-IDF, and F1-EXP compare to fine-tuned pre-trained language models like BERT when evaluated on manually labeled data versus automatically generated relevance judgments?
- Basis in paper: [explicit] The authors found that their assembled traditional methods model outperformed many runs using large-scale pre-trained language models like BERT in the final evaluation, which was based on manually labeled data. They attribute this to the manual labeling process focusing on keyword matching, which aligns with traditional IR methods.
- Why unresolved: The paper does not provide a direct comparison of the performance of individual traditional IR methods versus fine-tuned pre-trained language models on both manually labeled and automatically generated relevance judgments. It only compares the overall performance of their assembled models.
- What evidence would resolve it: A direct comparison of the performance of individual traditional IR methods (BM25, TF-IDF, F1-EXP) and fine-tuned pre-trained language models (BERT) on both manually labeled and automatically generated relevance judgments, using metrics like NDCG@3 and NDCG@10.

### Open Question 2
- Question: What is the impact of incorporating user behavior information, such as click-through data and session information, on the performance of pre-trained language models in document ranking tasks?
- Basis in paper: [explicit] The authors fine-tuned BERT with both ad-hoc data and session information. They found that utilizing session information improved the performance of ranking models to a small degree, but the improvement was limited because user search intent may change during a search session.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different types of user behavior information (e.g., click-through data vs. session information) on the performance of pre-trained language models. It also does not explore the optimal way to incorporate this information during the fine-tuning process.
- What evidence would resolve it: A comprehensive study comparing the performance of pre-trained language models fine-tuned with different types and combinations of user behavior information, using metrics like NDCG@3 and NDCG@10, and analyzing the impact on model performance.

### Open Question 3
- Question: How do different learning-to-rank algorithms, such as LambdaMART, compare in terms of their ability to assemble the scores of traditional IR methods and fine-tuned pre-trained language models for document ranking tasks?
- Basis in paper: [inferred] The authors used LambdaMART as their learning-to-rank algorithm to assemble the scores of traditional IR methods and fine-tuned pre-trained language models. They found that the assembled model achieved the best performance among all participants in the preliminary evaluation.
- Why unresolved: The paper does not compare the performance of LambdaMART with other learning-to-rank algorithms, such as ListNet or Coordinate Ascent, in assembling the scores of traditional IR methods and fine-tuned pre-trained language models.
- What evidence would resolve it: A comparison of the performance of different learning-to-rank algorithms (e.g., LambdaMART, ListNet, Coordinate Ascent) in assembling the scores of traditional IR methods and fine-tuned pre-trained language models, using metrics like NDCG@3 and NDCG@10, and analyzing the impact on model performance.

## Limitations

- Performance claims rely heavily on manual evaluation results with unspecified relevance judgment criteria
- Session information incorporation showed only small performance improvements, suggesting limited effectiveness
- Success of traditional methods may be task-specific, favoring keyword-based relevance matching

## Confidence

- **High Confidence**: Traditional IR methods (BM25, TF-IDF, F1-EXP) combined with learning-to-rank can achieve competitive performance in document ranking tasks, particularly when evaluation relies on keyword-based relevance judgments.
- **Medium Confidence**: Pre-trained language models fine-tuned on ad-hoc and session data can improve ranking performance when properly assembled with traditional methods, though the magnitude of improvement depends heavily on task characteristics.
- **Low Confidence**: Session information incorporation during fine-tuning provides consistent and significant performance gains across different ranking scenarios.

## Next Checks

1. Test the assembled traditional methods approach on a dataset with longer, more semantically complex queries to determine if the observed performance advantage persists when keyword matching becomes less effective.

2. Conduct ablation studies on the number and type of features used in the learning-to-rank model to quantify the marginal contribution of each IR method and pre-trained model score.

3. Compare the performance of session-informed fine-tuning against alternative approaches like adding session context as additional input tokens without fine-tuning, to isolate the effect of the fine-tuning process itself.