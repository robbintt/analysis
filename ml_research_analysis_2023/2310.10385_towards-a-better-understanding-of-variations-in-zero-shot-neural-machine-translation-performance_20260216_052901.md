---
ver: rpa2
title: Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation
  Performance
arxiv_id: '2310.10385'
source_url: https://arxiv.org/abs/2310.10385
tags:
- zero-shot
- translation
- performance
- language
- directions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates variations in zero-shot neural machine
  translation (NMT) performance across different language pairs. The authors conduct
  a comprehensive analysis of 1,560 language directions spanning 40 languages using
  three different NMT models.
---

# Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance

## Quick Facts
- **arXiv ID**: 2310.10385
- **Source URL**: https://arxiv.org/abs/2310.10385
- **Reference count**: 23
- **Primary result**: This paper investigates variations in zero-shot NMT performance across 1,560 language directions spanning 40 languages, finding that target side translation quality is the most influential factor.

## Executive Summary
This paper conducts a comprehensive analysis of zero-shot neural machine translation (NMT) performance variations across different language pairs. The authors evaluate 1,560 language directions using three different NMT models and identify key factors influencing performance. Their findings reveal that target-side translation quality has the strongest impact on zero-shot performance, followed by vocabulary overlap and linguistic properties like language family and writing system. The study provides insights for improving zero-shot NMT by focusing on enhancing target translation quality, especially for distant language pairs, and encouraging cross-lingual transfer through better vocabulary sharing.

## Method Summary
The authors conduct a comprehensive analysis of zero-shot NMT performance using the EC40 dataset with 1,560 language directions spanning 40 languages. They train three models (mTransformer-big, mTransformer-large, and fine-tuned mBART50) using Fairseq with Adam optimizer and temperature sampling for SentencePiece vocabulary. The models are evaluated on zero-shot directions using multiple metrics (Sacrebleu, Chrf++, SpBleu, COMET) to identify factors contributing to performance variations, including target translation quality, vocabulary overlap, and linguistic properties.

## Key Results
- Target side translation quality is the dominant factor influencing zero-shot performance
- Vocabulary overlap between source and target languages consistently predicts zero-shot performance
- Linguistic properties (language family and writing system) have a more pronounced effect on smaller models compared to larger models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Target-side translation quality is the dominant factor influencing zero-shot performance
- **Mechanism**: Zero-shot direction Src→Tgt can be decomposed into Src→En and En→Tgt, with En→Tgt quality directly constraining final output
- **Core assumption**: Translation quality bottleneck occurs primarily on target side
- **Evidence anchors**: Abstract states target side quality is most influential; section 5.1 shows target resource level has stronger effect than source
- **Break condition**: Source-side translation quality becomes bottleneck, or decomposition model assumption fails due to complex multilingual interactions

### Mechanism 2
- **Claim**: Vocabulary overlap between source and target languages consistently predicts zero-shot performance
- **Mechanism**: Higher vocabulary overlap enables better cross-lingual transfer by sharing more subword units, reducing learning burden for unseen pairs
- **Core assumption**: Subword-level overlap captures meaningful linguistic similarity that model can exploit
- **Evidence anchors**: Section 5.2 shows language pairs with higher vocabulary overlap yield better zero-shot capabilities
- **Break condition**: Model learns to rely more on semantic/contextual cues than surface form overlap, or vocabulary overlap becomes less predictive with advanced multilingual embeddings

### Mechanism 3
- **Claim**: Linguistic properties (language family and writing system) explain performance variations, especially for smaller models
- **Mechanism**: Shared linguistic properties reduce learning burden by providing structural similarities that facilitate transfer learning between related languages
- **Core assumption**: Models can exploit structural similarities between languages sharing family or writing system characteristics
- **Evidence anchors**: Section 5.3 reveals interesting findings regarding effect of linguistic properties considering model size, with smaller models more susceptible to linguistic features
- **Break condition**: Model capacity increases to point where linguistic properties no longer significantly impact performance, or model learns to normalize across linguistic differences

## Foundational Learning

- **Concept**: Zero-shot translation in MNMT
  - **Why needed here**: Understanding core challenge of translating between language pairs without direct training data is essential for grasping why variations exist
  - **Quick check question**: Can you explain how a model translates between languages it has never seen paired together during training?

- **Concept**: Model decomposition (Src→En + En→Tgt)
  - **Why needed here**: Paper's analysis relies on decomposing zero-shot directions into supervised components to isolate factors affecting performance
  - **Quick check question**: Given zero-shot direction from French to German, what are two supervised directions used in decomposition analysis?

- **Concept**: Vocabulary overlap measurement
  - **Why needed here**: Paper quantifies vocabulary overlap as |VSrc ∩ VTgt| / |VTgt| to measure potential cross-lingual transfer
  - **Quick check question**: If language A has 10,000 subwords and language B has 8,000 subwords with 2,000 shared, what is their vocabulary overlap score?

## Architecture Onboarding

- **Component map**: English-centric MNMT model with language tags, trained on parallel data where English is source or target, evaluated on zero-shot directions
- **Critical path**: Data preprocessing → Vocabulary learning → Model training → Evaluation on zero-shot directions → Analysis of performance variations
- **Design tradeoffs**: English-centric vs non-English-centric data collection (computational cost vs coverage), model capacity vs performance on distant pairs
- **Failure signatures**: High off-target rates (>5%), large performance gaps between supervised and zero-shot directions, inconsistent performance across metrics
- **First 3 experiments**:
  1. Measure correlation between En→Tgt performance and zero-shot performance across all directions
  2. Calculate vocabulary overlap for each language pair and correlate with zero-shot scores
  3. Group directions by language family/writing system and compare zero-shot performance distributions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does pre-training on a large multilingual corpus improve zero-shot performance more than increasing model capacity?
- **Basis in paper**: [explicit] Paper discusses impact of pre-training (mBART50) vs increasing model size (mT-big vs mT-large) on zero-shot performance, noting pre-training benefits diminish when model sizes match
- **Why unresolved**: Study compares pre-trained vs larger models but does not isolate effect of pre-training itself
- **What evidence would resolve it**: Controlled experiment varying only pre-training status (with fixed model capacity) while measuring zero-shot performance across multiple language pairs

### Open Question 2
- **Question**: How do linguistic properties like language family and writing system impact zero-shot performance differently for smaller vs larger models?
- **Basis in paper**: [explicit] Paper finds linguistic properties have more pronounced effect on smaller models (mT-big) compared to larger models (mT-large), but does not provide detailed analysis
- **Why unresolved**: While study observes trend, it does not explain underlying reasons for differential impact based on model size
- **What evidence would resolve it**: Detailed analysis of how linguistic features influence model predictions and representations at different scales

### Open Question 3
- **Question**: Can encouraging greater cross-lingual transfer via vocabulary sharing significantly improve zero-shot translation performance?
- **Basis in paper**: [explicit] Paper identifies vocabulary overlap as consistent factor influencing zero-shot performance and suggests encouraging cross-lingual transfer via better vocabulary sharing could be promising
- **Why unresolved**: While paper suggests this direction, it does not empirically test impact of explicitly engineered vocabulary overlap on zero-shot performance
- **What evidence would resolve it**: Experiments comparing zero-shot performance with and without engineered vocabulary overlap

## Limitations

- **Decomposition Assumption Validity**: Core analysis assumes zero-shot performance can be decomposed into source-to-English and English-to-target components, which may not hold due to non-linear interactions between languages
- **Dataset Construction Transparency**: Critical details about preprocessing, filtering thresholds, and domain distribution are not fully specified, limiting reproducibility
- **Off-Target Translation Impact**: Analysis acknowledges off-target translations as confounding factor but does not fully account for their systematic impact on performance metrics

## Confidence

- **High Confidence**: Target-side translation quality is dominant factor influencing zero-shot performance, supported by multiple correlation analyses across different metrics and model sizes
- **Medium Confidence**: Relationship between vocabulary overlap and zero-shot performance, though causal mechanism is not fully established
- **Low Confidence**: Claim that linguistic properties explain performance variations, particularly differential effects across model sizes, as evidence is observational and correlational

## Next Checks

1. **Decomposition Validation**: Conduct controlled experiments where artificially degrade source-to-English translation quality while holding target-to-English quality constant, and vice versa, to directly test whether target-side quality is indeed the bottleneck

2. **Vocabulary Overlap Causality**: Design experiments that manipulate vocabulary overlap through controlled subword segmentation strategies while keeping linguistic properties constant, to isolate causal effect of surface form sharing

3. **Off-Target Rate Analysis**: Perform systematic analysis correlating off-target rates with performance metrics across all directions, and implement direction-specific metrics that exclude off-target cases to determine whether observed performance gaps are measurement artifacts or genuine quality differences