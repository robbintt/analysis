---
ver: rpa2
title: Unpaired MRI Super Resolution with Contrastive Learning
arxiv_id: '2310.15767'
source_url: https://arxiv.org/abs/2310.15767
tags:
- images
- learning
- loss
- contrastive
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unpaired MRI super-resolution
  (SR), where high-resolution (HR) and low-resolution (LR) images are not aligned.
  The authors propose an unpaired MRI SR approach that employs contrastive learning
  to enhance SR performance with limited HR training data.
---

# Unpaired MRI Super Resolution with Contrastive Learning

## Quick Facts
- arXiv ID: 2310.15767
- Source URL: https://arxiv.org/abs/2310.15767
- Reference count: 0
- One-line primary result: Proposed unpaired MRI SR approach using contrastive learning achieves 0.0239 dB improvement in SSIM and 1.1802 dB enhancement in PSNR compared to baseline when using only 10% of training data.

## Executive Summary
This paper addresses the challenge of unpaired MRI super-resolution (SR), where high-resolution (HR) and low-resolution (LR) images are not aligned. The authors propose an unpaired MRI SR approach that employs contrastive learning to enhance SR performance with limited HR training data. The core idea is to use contrastive learning to extract supervisory information from a small amount of unpaired MRI images and train the network with this self-generated supervisory information. This allows the model to learn discriminative features and achieve better SR performance even when a paucity of HR images is available. The proposed method outperforms state-of-the-art unsupervised MRI SR models in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).

## Method Summary
The method employs a generator with LR encoder and SR decoder components, along with three VGG-based discriminators assessing authenticity across source, feature, and target domains. Contrastive learning is implemented using a modified InfoNCE loss that calculates inter-embedding and intra-embedding terms, which helps maintain stable feature learning even with limited HR samples. The model is trained on T1w images from the Human Connectome Project dataset, with target images down-sampled using 3D K-space truncation with a scale factor of 2x2x2.

## Key Results
- Proposed model exhibits 0.0239 dB improvement in SSIM compared to baseline when using only 10% of training data
- Proposed model shows 1.1802 dB enhancement in PSNR compared to baseline with limited HR training data
- Model demonstrates superior consistency with the smallest standard deviation in both SSIM and PSNR metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning enables feature learning from limited unpaired HR images by constructing positive and negative sample pairs in feature space.
- Mechanism: The model constructs positive pairs between generated SR features and ground truth features, and negative pairs by pairing samples within a batch. This contrastive loss encourages the model to learn discriminative features even without paired training data.
- Core assumption: The feature representations learned through contrastive loss are sufficient to guide SR reconstruction quality.
- Evidence anchors:
  - [abstract] "The core idea is to use contrastive learning to extract supervisory information from a small amount of unpaired MRI images and train the network with this self-generated supervisory information."
  - [section] "We present an efficient strategy grounded in CL for unpaired MRI SRR. This strategy involves the construction of positive sample pairs between generated SR images and ground truth images, while negative samples are fashioned within a batch by pairing samples with each other."
- Break condition: If the feature space becomes too high-dimensional relative to the batch size, the negative sampling strategy may fail to provide meaningful contrastive signals.

### Mechanism 2
- Claim: The InfoNCE-based contrastive loss improves stability when HR training data is scarce.
- Mechanism: The model incorporates a modified InfoNCE loss that calculates inter-embedding and intra-embedding terms, which helps maintain stable feature learning even with limited HR samples.
- Core assumption: The temperature parameter τ in the contrastive loss appropriately scales the penalty on hard negative samples.
- Evidence anchors:
  - [section] "CL is implemented using a contrastive loss, with one of the classical variants being infoNCE. The work modified infoNCE by introducing inter-embedding and intra-embedding terms in graph learning."
  - [section] "Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR images is available."
- Break condition: If τ is set too low, the model may become overconfident in incorrect feature representations; if too high, learning may become too slow.

### Mechanism 3
- Claim: The domain adaptation component in the discrimination network improves SR image quality by aligning source and target domains.
- Mechanism: Three VGG-based discriminator networks assess authenticity across source, feature, and target domains, with domain adaptation integrated to improve reconstruction quality.
- Core assumption: The adversarial loss from multiple discriminators provides sufficient guidance for domain alignment.
- Evidence anchors:
  - [section] "The discrimination loss module employs three discriminant networks, each responsible for determining the authenticity of the source domain, feature domain, and target domain. Notably, domain adaptation is integrated into this process, significantly improving the quality of the reconstructed SR image."
  - [section] "Our model exhibits superior consistency, with the most minor standard deviation observed in both SSIM and PSNR metrics."
- Break condition: If the discriminators become too strong relative to the generator, the adversarial training may collapse or produce artifacts.

## Foundational Learning

- Concept: Contrastive learning fundamentals (positive/negative pairs, InfoNCE loss)
  - Why needed here: The entire method relies on extracting supervisory information from unpaired data through contrastive learning, making understanding of these fundamentals essential.
  - Quick check question: What is the role of the temperature parameter τ in the InfoNCE loss formulation?

- Concept: MRI physics and acquisition constraints
  - Why needed here: Understanding why paired HR/LR MRI data is difficult to obtain (patient movement, geometric distortions) is crucial for appreciating the unpaired approach.
  - Quick check question: Why does patient movement during MRI acquisition make it difficult to obtain aligned HR/LR image pairs?

- Concept: Super-resolution evaluation metrics (PSNR, SSIM)
  - Why needed here: The paper's performance claims are based on these metrics, and understanding their limitations is important for proper interpretation.
  - Quick check question: What is the key difference between PSNR and SSIM in evaluating image reconstruction quality?

## Architecture Onboarding

- Component map:
  LR encoder (6 conv modules) -> LR decoder -> LR reconstruction
  SR encoder (modified RCAN) -> SR decoder -> SR reconstruction
  Three VGG discriminators (source, feature, target domains)
  Contrastive loss module (feature and reconstruction contrastive losses)
  Generator loss module (L1 + SSIM losses)

- Critical path:
  LR image -> LR encoder -> LR decoder -> reconstructed LR -> LR encoder (for feature contrastive loss) -> contrastive loss -> backprop to LR decoder
  HR image -> SR encoder -> SR decoder -> SR image -> discriminators + contrastive loss -> backprop to SR encoder/decoder

- Design tradeoffs:
  - Memory vs. performance: Using separate calculations for contrastive loss saves memory but may reduce feature alignment quality
  - Discriminator strength: Three discriminators provide comprehensive feedback but increase training instability risk
  - Batch size: Larger batches provide better negative sampling but require more memory

- Failure signatures:
  - Training collapse: Discriminators overpower generator, leading to mode collapse
  - Feature collapse: Contrastive loss becomes ineffective if positive pairs are too similar
  - Mode oscillation: Alternating between different reconstruction qualities across epochs

- First 3 experiments:
  1. Ablation study: Remove contrastive loss and compare PSNR/SSIM on validation set
  2. Batch size sensitivity: Test different batch sizes and measure impact on contrastive loss effectiveness
  3. Temperature parameter sweep: Vary τ in InfoNCE loss and evaluate stability and performance tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed contrastive learning approach for unpaired MRI super-resolution compare to supervised methods in terms of computational efficiency and training time?
- Basis in paper: [inferred] The paper mentions that the proposed method outperforms state-of-the-art unsupervised MRI SR models, but it does not provide a direct comparison with supervised methods in terms of computational efficiency and training time.
- Why unresolved: The paper focuses on the effectiveness of the proposed method in terms of PSNR and SSIM, but does not provide a detailed comparison with supervised methods regarding computational efficiency and training time.
- What evidence would resolve it: Conducting experiments to compare the computational efficiency and training time of the proposed method with supervised methods would provide evidence to resolve this question.

### Open Question 2
- Question: How does the proposed method handle different types of MRI images, such as T1-weighted, T2-weighted, and FLAIR images?
- Basis in paper: [inferred] The paper mentions that the dataset used in the experiments is derived from T1-weighted images, but it does not provide information on how the proposed method handles other types of MRI images.
- Why unresolved: The paper does not discuss the generalizability of the proposed method to different types of MRI images, which is an important aspect to consider in real-world applications.
- What evidence would resolve it: Conducting experiments using different types of MRI images, such as T2-weighted and FLAIR images, and evaluating the performance of the proposed method on these images would provide evidence to resolve this question.

### Open Question 3
- Question: How does the proposed method handle different levels of noise and artifacts in the low-resolution MRI images?
- Basis in paper: [inferred] The paper does not explicitly discuss how the proposed method handles different levels of noise and artifacts in the low-resolution MRI images.
- Why unresolved: The paper focuses on the effectiveness of the proposed method in terms of PSNR and SSIM, but does not provide information on its robustness to noise and artifacts.
- What evidence would resolve it: Conducting experiments using low-resolution MRI images with different levels of noise and artifacts, and evaluating the performance of the proposed method on these images would provide evidence to resolve this question.

### Open Question 4
- Question: How does the proposed method handle different anatomical structures and regions in the MRI images?
- Basis in paper: [inferred] The paper does not explicitly discuss how the proposed method handles different anatomical structures and regions in the MRI images.
- Why unresolved: The paper focuses on the effectiveness of the proposed method in terms of PSNR and SSIM, but does not provide information on its ability to handle different anatomical structures and regions.
- What evidence would resolve it: Conducting experiments using MRI images of different anatomical structures and regions, and evaluating the performance of the proposed method on these images would provide evidence to resolve this question.

## Limitations
- Architecture details of LR encoder and SR decoder layers are not fully specified, making exact reproduction challenging
- Temperature parameter τ in InfoNCE loss and its impact on training stability are not fully explored
- Evaluation focuses on PSNR and SSIM metrics, which may not fully capture perceptual quality or clinical utility

## Confidence
- **High confidence**: The core mechanism of using contrastive learning for unpaired MRI SR is well-supported by the evidence provided, including the improvement in PSNR and SSIM metrics when using limited HR training data.
- **Medium confidence**: The claim that the proposed method outperforms state-of-the-art unsupervised MRI SR models is supported by quantitative results, but the exact baseline methods and their implementations are not fully specified, making direct comparison challenging.
- **Low confidence**: The assertion that the domain adaptation component significantly improves reconstruction quality is based on the integration of three VGG discriminators, but the specific contribution of each discriminator to the final performance is not clearly delineated.

## Next Checks
1. **Ablation Study**: Conduct an ablation study by removing the contrastive loss component and comparing the PSNR/SSIM scores on a validation set to isolate the impact of contrastive learning on performance.
2. **Parameter Sensitivity Analysis**: Perform a sensitivity analysis on the temperature parameter τ in the InfoNCE loss to determine its optimal range for balancing stability and performance.
3. **Perceptual Evaluation**: Supplement quantitative metrics (PSNR/SSIM) with a perceptual study or alternative quality assessment methods to validate the clinical relevance and visual quality of the reconstructed images.