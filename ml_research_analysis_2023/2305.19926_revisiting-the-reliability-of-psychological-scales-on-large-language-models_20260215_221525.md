---
ver: rpa2
title: Revisiting the Reliability of Psychological Scales on Large Language Models
arxiv_id: '2305.19926'
source_url: https://arxiv.org/abs/2305.19926
tags:
- chatgpt
- personality
- arxiv
- llms
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of using psychological
  scales, specifically the Big Five Inventory, on Large Language Models (LLMs). Through
  analysis of 2,500 settings across multiple models including GPT-3.5, GPT-4, Gemini-Pro,
  and LLaMA-3.1, the research finds that LLMs demonstrate consistent responses to
  personality assessments, indicating satisfactory reliability.
---

# Revisiting the Reliability of Psychological Scales on Large Language Models

## Quick Facts
- arXiv ID: 2305.19926
- Source URL: https://arxiv.org/abs/2305.19926
- Reference count: 13
- Primary result: LLMs demonstrate consistent responses to personality assessments, with satisfactory reliability across multiple models and settings

## Executive Summary
This study investigates the reliability of using psychological scales, specifically the Big Five Inventory, on Large Language Models (LLMs). Through analysis of 2,500 settings across multiple models including GPT-3.5, GPT-4, Gemini-Pro, and LLaMA-3.1, the research finds that LLMs demonstrate consistent responses to personality assessments, indicating satisfactory reliability. The study further explores GPT-3.5's ability to emulate diverse personalities and represent various groups through specific prompt instructions. Results show that LLMs can indeed represent different personalities when given appropriate prompts, which has implications for their potential use in social science research as cost-effective alternatives to human participants.

## Method Summary
The study administered the Big Five Inventory (60 questions) to multiple LLMs (GPT-3.5, GPT-4, Gemini-Pro, LLaMA-3.1) across 2,500 different settings per model. Researchers tested various prompt instructions to evaluate personality emulation capabilities and consistency of responses. The analysis examined cross-lingual consistency and the ability to represent different personalities through specific prompt engineering. Response patterns were analyzed for reliability and compared across different models and prompt variations.

## Key Results
- LLMs show consistent responses to Big Five Inventory across different prompt variations, orders, and rephrases
- GPT-3.5 can represent different personalities when given specific prompt instructions, though effectiveness varies by target personality
- Cross-lingual personality assessments yield consistent results, with LLMs maintaining personality traits across languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce consistent personality responses across different prompts, orders, and rephrases
- Mechanism: The LLM's internal representation of personality traits is stable and not easily influenced by surface-level variations in question phrasing or presentation order
- Core assumption: Personality assessment relies on stable latent representations rather than specific lexical patterns
- Evidence anchors:
  - [abstract] "Analysis of 2,500 settings per model... reveals that various LLMs show consistency in responses to the Big Five Inventory"
  - [section 2.1] "The figure demonstrates the consistent robustness of the results regardless of prompt selection"
  - [corpus] Weak evidence - corpus shows related work but no direct measurement of consistency mechanisms
- Break condition: If the LLM's personality representation is based on specific training data patterns rather than stable traits, rephrasing or reordering could produce different results

### Mechanism 2
- Claim: LLMs can represent different personalities when given specific prompt instructions
- Mechanism: The model can access different personality profiles through explicit instruction, but this requires careful prompt engineering to overcome default personality tendencies
- Core assumption: LLMs contain multiple personality representations that can be activated through targeted instructions
- Evidence anchors:
  - [abstract] "LLMs have the potential to represent different personalities with specific prompt instructions"
  - [section 4.1] "Modifying ChatGPT's inherent ENFJ personality is an unresolved challenge, and that transitioning from extroverted to introverted is relatively feasible"
  - [corpus] Moderate evidence - related work shows instruction tuning can influence behavior but full personality control remains challenging
- Break condition: If personality is deeply embedded in the model's weights during training, it may resist modification through prompts alone

### Mechanism 3
- Claim: Cross-lingual personality assessments yield consistent results across different languages
- Mechanism: The model's personality representation is language-agnostic, with core traits preserved across linguistic variations
- Core assumption: Personality is encoded in the model's semantic space rather than language-specific representations
- Evidence anchors:
  - [abstract] "Our findings reveal that LLMs have the potential to represent different personalities with specific prompt instructions"
  - [section 3.1] "The personalities of ChatGPT across different languages are consistent, maintaining an ENFJ personality type in line with the English version"
  - [corpus] Weak evidence - cross-lingual studies exist but specific validation of personality consistency is limited
- Break condition: If personality representation is influenced by language-specific training data patterns, cross-lingual consistency would break down

## Foundational Learning

- Concept: Big Five Inventory (BFI) personality assessment
  - Why needed here: This study uses BFI to measure LLM personality traits, understanding the assessment framework is crucial for interpreting results
  - Quick check question: What are the five dimensions measured by the Big Five Inventory, and what do high/low scores represent for each dimension?

- Concept: Prompt engineering and instruction tuning
  - Why needed here: The study tests various prompt designs to control LLM personality expression, requiring understanding of how different prompt structures affect model behavior
  - Quick check question: How do different prompt formats (QA, BIO, PORTRAY) potentially influence the LLM's ability to adopt different personalities?

- Concept: Cross-lingual model consistency
  - Why needed here: The study evaluates personality assessment across multiple languages, requiring understanding of how language affects model behavior and representation
  - Quick check question: What factors might cause an LLM to produce different personality assessments when tested in different languages?

## Architecture Onboarding

- Component map: LLM inference engine -> Big Five Inventory framework -> Prompt engineering module -> Response parsing -> Result aggregation
- Critical path: Question generation → Prompt formatting → LLM inference → Response parsing → BFI scoring → Result aggregation. The most time-sensitive components are LLM inference and response parsing, as these involve API calls and text processing.
- Design tradeoffs: Using the BFI provides standardized personality assessment but may not capture the full range of LLM personality expression. Testing multiple languages increases generalizability but requires careful translation and cultural adaptation of questions.
- Failure signatures: Inconsistent responses across prompt variations suggest prompt sensitivity, while language-dependent results indicate cultural or linguistic bias. Inability to modify personality through instructions suggests deep embedding of default traits.
- First 3 experiments:
  1. Test BFI consistency across 10 different prompt variations for a single language/model pair
  2. Compare BFI results across 3 languages for the same model using translated questions
  3. Attempt personality modification using the three prompt formats (QA, BIO, PORTRAY) and measure success rates for different target personalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively modify an LLM's inherent personality traits through instruction tuning?
- Basis in paper: Explicit - The paper discusses challenges in modifying ChatGPT's ENFJ personality and suggests instruction tuning as a potential solution.
- Why unresolved: Current methods like direct assignment, context manipulation, and persona impersonation have limited success in changing core personality traits.
- What evidence would resolve it: Demonstrating successful personality modification in LLMs through instruction tuning using synthesized data with personality features.

### Open Question 2
- Question: Do personality traits of LLMs generalize across different task domains beyond personality assessments?
- Basis in paper: Explicit - The study focuses on personality assessment reliability but doesn't explore cross-domain consistency.
- Why unresolved: The paper only examines personality consistency in MBTI testing contexts, not in other behavioral or task-based scenarios.
- What evidence would resolve it: Testing LLM personality consistency across multiple task domains and comparing results with personality assessment outcomes.

### Open Question 3
- Question: What is the relationship between an LLM's training data and its exhibited personality traits?
- Basis in paper: Inferred - The paper notes that ChatGPT's training data might influence responses and observes differences between various LLMs' personalities.
- Why unresolved: The study doesn't analyze the connection between training data characteristics and resulting personality traits.
- What evidence would resolve it: Comparative analysis of training data composition and corresponding personality traits across multiple LLMs.

## Limitations

- Limited cross-lingual testing beyond English, with insufficient evidence for personality preservation across languages
- Reliance on Big Five Inventory may not capture the full spectrum of personality traits that LLMs could potentially express
- Static personality questionnaires rather than dynamic interaction patterns that might reveal different aspects of LLM behavior

## Confidence

**High Confidence**: The finding that LLMs demonstrate consistent responses to personality assessments across different prompt variations and models. This is supported by extensive testing (2,500 settings per model) and multiple model comparisons.

**Medium Confidence**: The claim that LLMs can represent different personalities through specific prompt instructions. While the study shows this is possible, the effectiveness varies significantly depending on the target personality and prompt format used.

**Low Confidence**: Cross-lingual consistency claims, as the study provides limited evidence for personality preservation across languages beyond English. The mechanisms for how personality traits translate across linguistic boundaries remain unclear.

## Next Checks

1. **Prompt Sensitivity Analysis**: Conduct a systematic test of how minor variations in prompt wording affect personality assessment results, using controlled A/B testing across multiple prompt formats.

2. **Cross-Cultural Validation**: Test the same personality assessment across multiple cultural contexts using region-specific versions of the same language to identify potential cultural bias in LLM personality expression.

3. **Longitudinal Stability Test**: Evaluate whether LLM personality assessments remain consistent over extended periods and across model updates, measuring potential drift in personality representations.