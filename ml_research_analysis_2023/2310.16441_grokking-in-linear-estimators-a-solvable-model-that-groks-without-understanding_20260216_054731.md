---
ver: rpa2
title: Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding
arxiv_id: '2310.16441'
source_url: https://arxiv.org/abs/2310.16441
tags:
- grokking
- training
- generalization
- loss
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analytically solves gradient-flow training dynamics for
  linear teacher-student models in a Gaussian input setting, deriving exact predictions
  for grokking behavior. The key insight is that grokking arises naturally when the
  generalization loss decays slower than the training loss due to their different
  dependence on the training and generalization covariance matrices.
---

# Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding

## Quick Facts
- **arXiv ID**: 2310.16441
- **Source URL**: https://arxiv.org/abs/2310.16441
- **Reference count**: 40
- **Key outcome**: Analytically solves gradient-flow training dynamics for linear teacher-student models, deriving exact predictions for grokking behavior that depends critically on input dimension to training samples ratio

## Executive Summary
This work provides an analytical framework for understanding grokking in linear teacher-student models with Gaussian inputs. By solving the gradient-flow dynamics exactly, the authors show that grokking arises naturally when generalization loss decays slower than training loss due to their different dependence on training and generalization covariance matrices. The analysis reveals that grokking time depends critically on the ratio of input dimension to training samples, with maximal grokking occurring near the interpolation threshold. The framework is verified numerically and extended to 2-layer networks with both linear and nonlinear activations.

## Method Summary
The authors analyze gradient-flow training dynamics in a linear teacher-student setup with Gaussian inputs. They derive exact solutions for training and generalization loss evolution using random matrix theory, particularly the Marchenko-Pastur distribution for the training covariance matrix eigenvalues. The model considers weight decay regularization and examines how different parameterizations affect grokking behavior. Numerical experiments validate the analytical predictions across various settings including different output dimensions and network architectures.

## Key Results
- Grokking time depends quadratically on the ratio λ = din/Ntr, diverging as λ approaches 1 from either direction
- Weight decay can suppress or enhance grokking depending on whether the network is underparameterized (λ<1) or overparameterized (λ>1)
- Grokking is an artifact of the accuracy measure rather than indicating a fundamental transition from memorization to understanding
- The phenomenon persists in 2-layer networks with both linear and nonlinear activations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grokking arises because generalization loss decays slower than training loss due to their different dependence on the training and generalization covariance matrices.
- Mechanism: The training loss is computed over a fixed dataset with empirical covariance Σtr, while generalization loss is the expectation over the input distribution with identity covariance. This causes the training loss to reach the threshold ϵ faster than the generalization loss, creating the appearance of delayed generalization.
- Core assumption: The input data follows a standard Gaussian distribution and the network performs a linear transformation task.
- Evidence anchors:
  - [abstract] "The key insight is that grokking arises naturally when the generalization loss decays slower than the training loss due to their different dependence on the training and generalization covariance matrices."
  - [section] "While the generalization covariance is the identity by construction, the train covariance only approaches the identity in the limit Ntr≫din, and otherwise follows the Marchenko-Pastur distribution."
  - [corpus] Found 25 related papers with average neighbor FMR=0.431, suggesting this mechanism is recognized in the broader literature.
- Break condition: If the input distribution is non-Gaussian or the network architecture introduces nonlinearities that significantly alter the covariance structure, this mechanism may not hold.

### Mechanism 2
- Claim: Weight decay can either suppress or enhance grokking depending on the parameterization regime.
- Mechanism: In the underparameterized regime (λ<1), weight decay increases the asymptotic generalization loss, preventing perfect generalization below a certain threshold. In the overparameterized regime (λ>1), weight decay shifts the zero eigenvalues of the MP distribution, improving generalization performance and potentially suppressing grokking.
- Core assumption: The Marchenko-Pastur distribution accurately describes the eigenvalue spectrum of the training covariance matrix.
- Evidence anchors:
  - [section] "L2 regularization suppresses grokking in overparameterized networks as expected, while having a subtle effect on the grokking time in underparameterized settings."
  - [section] "We consider first the case of nonzero WD in the simpler case of dout = 1. Incorporating weight decay amounts to adding a regularization term at each gradient descent timestep, modifying Eq. (3) to Dt+1 = Dt−2η(Σ tr + 1/2γI)Dt−ηγT, where γ∈R+ is the weight decay parameter."
  - [corpus] Limited direct evidence in corpus, but this aligns with general understanding of regularization effects in ML.
- Break condition: If the weight decay parameter is too large, it may dominate the learning dynamics and prevent the network from fitting the training data adequately.

### Mechanism 3
- Claim: The grokking time depends critically on the ratio of input dimension to training samples (λ = din/Ntr), with maximal grokking occurring near the interpolation threshold.
- Mechanism: The eigenvalue distribution of the training covariance matrix, which follows the Marchenko-Pastur distribution, determines the decay rates of training and generalization losses. The grokking time diverges quadratically as λ approaches 1 from either direction.
- Core assumption: The input dimension and number of training samples are large, allowing the use of random matrix theory.
- Evidence anchors:
  - [abstract] "The analysis shows grokking time depends critically on the ratio of input dimension to training samples, with maximal grokking occurring near the interpolation threshold."
  - [section] "Eq. (12) indicates that the maximal grokking time difference occurs near λ≃1, where the grokking time diverges quadratically as ∆tgrok(λ→1)∼ 1/η0(λ−1)2 log(4/(1−λ)2)."
  - [corpus] Found 25 related papers, suggesting this is a well-studied aspect of grokking phenomena.
- Break condition: If the number of training samples is not large enough to justify the random matrix theory approximation, or if the input dimension is not sufficiently large, the predicted behavior may not hold.

## Foundational Learning

- Concept: Random Matrix Theory (RMT) and Marchenko-Pastur distribution
  - Why needed here: To characterize the eigenvalue spectrum of the training covariance matrix, which determines the decay rates of training and generalization losses.
  - Quick check question: What is the limiting eigenvalue distribution of the sample covariance matrix of a high-dimensional Gaussian random vector?

- Concept: Gradient flow dynamics
  - Why needed here: To derive the exact solutions for the training and generalization loss evolution in the linear teacher-student setup.
  - Quick check question: How does the gradient flow equation differ from standard gradient descent in terms of time discretization?

- Concept: Linear teacher-student models
  - Why needed here: To establish a solvable framework for studying grokking, where the teacher network generates perfect labels and the student network learns to mimic the teacher.
  - Quick check question: In a linear teacher-student model, what is the relationship between the student's weights and the training loss?

## Architecture Onboarding

- Component map:
  - Linear teacher network (T) -> Gaussian input data (x) -> Linear student network (S) -> MSE loss function -> Gradient descent optimizer with weight decay -> Training and generalization data splits

- Critical path:
  1. Initialize teacher and student weights
  2. Generate training data from standard Gaussian distribution
  3. Compute training loss using empirical covariance matrix
  4. Update student weights using gradient descent with weight decay
  5. Monitor training and generalization losses
  6. Identify grokking time as the difference between training and generalization accuracy reaching 95%

- Design tradeoffs:
  - Larger input dimension (din) relative to training samples (Ntr) increases grokking time but may lead to overfitting
  - Higher weight decay suppresses grokking in overparameterized networks but may prevent learning in underparameterized networks
  - Choice of learning rate affects the speed of convergence but not the qualitative behavior

- Failure signatures:
  - If training and generalization losses decrease at similar rates, grokking will not occur
  - If the input data is not Gaussian or has significant correlations, the Marchenko-Pastur distribution may not accurately describe the covariance matrix eigenvalues
  - If the network architecture introduces significant nonlinearities, the linear analysis may not apply

- First 3 experiments:
  1. Reproduce the basic grokking phenomenon in a linear teacher-student model with din=1000, Ntr=100, and dout=1. Verify that the training loss reaches the threshold ϵ faster than the generalization loss.
  2. Investigate the effect of weight decay on grokking by varying γ from 0 to 0.1 in the same setup. Observe how grokking time changes in the underparameterized and overparameterized regimes.
  3. Explore the impact of output dimension on grokking by setting dout=1, 10, 100, and 1000 while keeping din=1000 and Ntr=100. Confirm the non-monotonic dependence of grokking time on dout.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the grokking dynamics for deeper networks (beyond 2 layers) with linear and nonlinear activations?
- Basis in paper: [explicit] The authors state they performed preliminary numerical experiments with 2-layer networks and mention this as an area for future work
- Why unresolved: The analysis is currently limited to 1-layer and 2-layer networks; extending to deeper architectures requires new mathematical techniques
- What evidence would resolve it: Analytical solutions for training dynamics of 3+ layer networks under gradient flow, validated against numerical experiments

### Open Question 2
- Question: How do different optimizers (beyond gradient descent) affect grokking behavior?
- Basis in paper: [explicit] The authors suggest studying the effect of different optimizers on grokking as future work
- Why unresolved: All current analysis is performed using full-batch gradient descent; other optimizers may introduce different dynamics
- What evidence would resolve it: Analytical solutions for training dynamics using SGD, Adam, or other optimizers, showing how they modify grokking time and behavior

### Open Question 3
- Question: What is the impact of correlated or non-Gaussian input distributions on grokking?
- Basis in paper: [explicit] The authors note that extending analysis to non-Gaussian data or correlated inputs could reveal how data structure affects grokking
- Why unresolved: Current analysis assumes iid Gaussian inputs; real-world data often has correlations and non-Gaussian structure
- What evidence would resolve it: Analytical solutions for training dynamics with correlated inputs, showing how the covariance structure affects grokking behavior

### Open Question 4
- Question: How do more realistic accuracy measures (softmax, cross-entropy) affect grokking compared to MSE loss?
- Basis in paper: [explicit] The authors mention ongoing work connecting theoretical studies to practical deep learning settings using more realistic accuracy measures
- Why unresolved: Current analysis uses MSE loss with simple accuracy threshold; real classification tasks use cross-entropy loss with softmax
- What evidence would resolve it: Analytical solutions for training dynamics using cross-entropy loss, showing how the accuracy measure affects grokking time and behavior

## Limitations

- The analysis assumes standard Gaussian inputs and may not generalize to real-world correlated or non-Gaussian data distributions
- Weight decay effects are only analytically derived for dout=1, with multi-output behavior inferred from numerical experiments
- The gradient-flow approximation may not capture finite learning rate effects that could modify grokking behavior

## Confidence

- **High**: The linear teacher-student framework is mathematically tractable and the gradient-flow solutions are exact within this setting
- **Medium**: The Marchenko-Pastur distribution accurately predicts covariance eigenvalue spectra for large, uncorrelated Gaussian inputs
- **Low**: Extrapolation of weight decay effects from dout=1 to general dout, and extension to nonlinear activations

## Next Checks

1. **Finite Sample Validation**: Test the analytical predictions against numerical simulations for modest din, Ntr values (e.g., 50-500) to quantify finite-size corrections to the asymptotic theory.

2. **Covariance Structure Sensitivity**: Repeat the analysis with correlated Gaussian inputs (non-identity covariance) to verify whether grokking persists when the training and generalization covariance matrices become similar.

3. **Nonlinearity Impact**: Extend the analytical framework to handle ReLU activation in the student network while keeping the teacher linear, to assess whether the core mechanism survives the introduction of nonlinearity.