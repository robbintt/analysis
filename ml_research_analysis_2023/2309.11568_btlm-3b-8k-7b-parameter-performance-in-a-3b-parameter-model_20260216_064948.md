---
ver: rpa2
title: 'BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model'
arxiv_id: '2309.11568'
source_url: https://arxiv.org/abs/2309.11568
tags:
- btlm-3b-8k
- parameter
- training
- context
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces BTLM-3B-8K, a 3 billion parameter language\
  \ model that achieves 7 billion parameter-level performance. The model is trained\
  \ on 627B tokens from a cleaned dataset using a variable context length schedule\
  \ (2K \u2192 8K) and key architectural changes: SwiGLU activation, ALiBi position\
  \ embeddings, and maximal update parameterization."
---

# BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model

## Quick Facts
- arXiv ID: 2309.11568
- Source URL: https://arxiv.org/abs/2309.11568
- Reference count: 37
- Primary result: 3B parameter model achieving 7B parameter performance on downstream tasks

## Executive Summary
BTLM-3B-8K is a 3 billion parameter language model that achieves performance comparable to 7 billion parameter models through architectural innovations and training optimizations. The model uses SwiGLU activation, ALiBi position embeddings, and maximal update parameterization (μP) to deliver strong results across downstream tasks while maintaining efficient inference. Trained on 627B tokens from the SlimPajama dataset with variable context lengths, BTLM-3B-8K outperforms all other 3B models by 2-5.5% and even surpasses some 7B models on specific benchmarks.

## Method Summary
BTLM-3B-8K uses a GPT-3-style transformer architecture with 32 layers, SwiGLU activation, and ALiBi position embeddings. The model is trained on SlimPajama (627B tokens) with a variable context length schedule transitioning from 2K to 8K tokens. Training employs maximal update parameterization (μP) with specific hyperparameters: learning rate 1.2e-2, initialization std 0.073, embedding multiplier 14.6, and output logit multiplier 2.22. The training proceeds in two phases: 470B tokens with 2K context (batch size 1920) followed by 157B tokens with 8K context (batch size 480).

## Key Results
- Achieves 7B parameter-level performance while using only 3B parameters
- Outperforms all other 3B models by 2-5.5% across downstream tasks
- Excels at long-context tasks (8K), outperforming 7B models like MPT-7B-8K and XGen-7B-8K
- Requires only 3GB RAM (4-bit), enabling edge device deployment
- 5.36% loss reduction over baseline through architectural improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SwiGLU activation function improves training efficiency and model performance over GELU
- Mechanism: SwiGLU uses a gated linear unit structure that allows the model to learn more complex non-linear transformations, leading to better feature extraction and representation
- Core assumption: The gating mechanism in SwiGLU is more effective at capturing relevant patterns in the data compared to the simpler GELU activation
- Evidence anchors: Table 8 shows SwiGLU decreases loss by 1.37% relative to GELU

### Mechanism 2
- Claim: ALiBi position embeddings enable better long-context performance and extrapolation
- Mechanism: ALiBi uses linear biases to encode position information, allowing the model to generalize to longer sequences than seen during training without the need for additional positional encoding parameters
- Core assumption: Linear biases are sufficient to capture the relative position information needed for long-range dependencies
- Evidence anchors: Figure 3 shows ALiBi grants impressive extrapolation properties with a 255M parameter model trained on 103M tokens

### Mechanism 3
- Claim: Maximal update parameterization (μP) improves training stability and enables better hyperparameter transfer
- Mechanism: μP introduces scalar multipliers to the learning rate, output, and initialization of certain layers, counteracting activation scales growing with width and allowing for more stable training
- Core assumption: The scalar multipliers effectively normalize the activation scales across different model widths
- Evidence anchors: The maximal update parameterization (μP) enables the transfer of optimal hyperparameters (HPs) from a small proxy model up to a very large target model

## Foundational Learning

- Concept: Tokens per parameter (TPP) and its impact on training efficiency and model quality
  - Why needed here: BTLM-3B-8K is trained with a high TPP (236.4) compared to the compute-optimal 20 TPP, which affects the training dynamics and final model performance
  - Quick check question: What is the trade-off between training with a higher TPP (e.g., 236.4) versus a lower TPP (e.g., 20) in terms of training compute and inference efficiency?

- Concept: Learning rate decay schedules and their impact on training stability and final model performance
  - Why needed here: BTLM-3B-8K uses a specific learning rate decay schedule (linear warmup followed by linear decay) that is crucial for stable training and achieving good performance
  - Quick check question: How does the learning rate decay fraction (αdecay) scale with the number of tokens per parameter (TPP) to ensure optimal training dynamics?

- Concept: Position embeddings and their role in capturing long-range dependencies
  - Why needed here: BTLM-3B-8K uses ALiBi position embeddings, which are designed to enable better extrapolation to longer sequences compared to learned or rotary position embeddings
  - Quick check question: What are the key differences between ALiBi, learned, and rotary position embeddings in terms of their ability to capture long-range dependencies and extrapolate to unseen sequence lengths?

## Architecture Onboarding

- Component map: Transformer decoder -> SwiGLU activation -> ALiBi position embeddings -> Maximal update parameterization (μP) -> SlimPajama dataset

- Critical path: Data preprocessing → Model training with variable context lengths → Evaluation on downstream tasks

- Design tradeoffs:
  - High TPP (236.4) vs. compute-optimal TPP (20): Higher TPP leads to better inference efficiency but requires more training compute
  - ALiBi vs. learned/rotary position embeddings: ALiBi enables better extrapolation but may not be as effective for shorter sequences
  - SwiGLU vs. GELU activation: SwiGLU improves training efficiency but introduces additional complexity

- Failure signatures:
  - Training instability or divergence: Check learning rate, batch size, and μP hyperparameters
  - Poor long-context performance: Verify ALiBi position embeddings and variable context length training schedule
  - Suboptimal performance on downstream tasks: Examine data preprocessing, model architecture, and training hyperparameters

- First 3 experiments:
  1. Train a small proxy model (e.g., 40M parameters) with different μP hyperparameters and learning rate decay fractions to find the optimal settings
  2. Evaluate the impact of ALiBi position embeddings on long-context performance by comparing models trained with and without ALiBi
  3. Assess the trade-off between training with different TPPs (e.g., 20, 100, 236.4) by measuring training compute and inference efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BTLM-3B-8K's performance scale with context length beyond 8,192 tokens?
- Basis in paper: The paper shows BTLM-3B-8K can extrapolate to approximately 9,216 tokens but experiences loss degradation beyond that
- Why unresolved: The paper only evaluates BTLM-3B-8K up to 8,192 context length during training and testing, with extrapolation tests limited to 32,768 tokens
- What evidence would resolve it: Systematic evaluation of BTLM-3B-8K on tasks requiring context lengths greater than 32,768 tokens, comparing performance to other models trained with even longer context windows

### Open Question 2
- Question: What is the optimal tokens-per-parameter (TPP) ratio for different model sizes and tasks?
- Basis in paper: The paper discusses increasing TPP from 20 to 236.4, showing significant improvements in loss and compute efficiency
- Why unresolved: While the paper demonstrates benefits of high TPP, it doesn't explore the full spectrum of TPP ratios across different model sizes or task types
- What evidence would resolve it: Comprehensive ablation studies varying TPP across multiple model sizes (e.g., 1B, 3B, 7B) and evaluating performance on diverse task categories

### Open Question 3
- Question: How do different position embedding methods (ALiBi vs RoPE) compare for long-context tasks beyond 8,192 tokens?
- Basis in paper: The paper notes that RoPE outperformed ALiBi at 2,048 context length during training, but ALiBi was chosen for its superior extrapolation capabilities
- Why unresolved: The paper's ablation study only compares position embeddings at 2,048 context length
- What evidence would resolve it: Head-to-head comparison of ALiBi and RoPE position embeddings in models trained with variable context lengths up to and beyond 32,768 tokens

## Limitations

- The model's long-context extrapolation capabilities beyond 8K tokens remain unverified through systematic evaluation
- Dataset quality improvements over standard datasets are asserted rather than empirically validated through ablation studies
- The claim of outperforming all other 3B models is difficult to verify without access to exact evaluation protocols and random seeds

## Confidence

**High confidence**: The core architectural innovations (SwiGLU, ALiBi, μP) are well-established in the literature, and the training methodology follows standard practices

**Medium confidence**: The 2-5.5% improvement over other 3B models and the claim of "7B performance" in a 3B model are supported by benchmark results, but the evaluation is limited to specific tasks

**Low confidence**: The claim that BTLM-3B-8K "outperforms all other 3B models" is difficult to verify without access to the exact evaluation protocols and random seeds used

## Next Checks

1. **Independent replication of μP hyperparameter transfer**: Train a 40M proxy model on a subset of SlimPajama, tune μP hyperparameters, and verify whether these same settings transfer effectively to a 3B model

2. **Systematic long-context evaluation**: Evaluate BTLM-3B-8K on context lengths beyond 8K (e.g., 16K, 32K) to empirically test the limits of ALiBi extrapolation

3. **Dataset ablation study**: Train identical models using different versions of the training data to quantify the contribution of dataset quality to the reported performance gains