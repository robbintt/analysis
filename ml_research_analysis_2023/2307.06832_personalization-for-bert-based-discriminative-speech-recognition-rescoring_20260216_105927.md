---
ver: rpa2
title: Personalization for BERT-based Discriminative Speech Recognition Rescoring
arxiv_id: '2307.06832'
source_url: https://arxiv.org/abs/2307.06832
tags:
- personalized
- bert
- test
- rescoring
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores three novel approaches for personalized speech
  recognition rescoring using BERT-based models: gazetteers, natural language prompts,
  and a cross-attention encoder-decoder architecture. The study uses internal en-US
  data supplemented with personalized named entities to evaluate these methods against
  a neural rescoring baseline.'
---

# Personalization for BERT-based Discriminative Speech Recognition Rescoring

## Quick Facts
- arXiv ID: 2307.06832
- Source URL: https://arxiv.org/abs/2307.06832
- Reference count: 0
- Three novel approaches for personalized speech recognition rescoring using BERT-based models

## Executive Summary
This paper presents three approaches for improving personalized speech recognition using BERT-based rescoring models: gazetteers with slot embeddings, natural language prompts, and a cross-attention encoder-decoder architecture. The methods aim to enhance recognition of personalized named entities like contact names while maintaining performance on general speech. Using internal English data, the study demonstrates that each approach improves word error rate by over 10% on personalized test sets, with gazetteers achieving the best overall results and prompts providing 7% improvement without requiring training.

## Method Summary
The study explores three personalization approaches for BERT-based discriminative rescoring in two-pass ASR systems. First, gazetteer methods add trainable slot embeddings to mark named entities at either input or last layer. Second, a natural language prompt approach appends contextual phrases when entities are detected. Third, a cross-attention encoder-decoder architecture combines hypothesis and entity information through attention mechanisms. All models are trained using MWER loss on n-best hypotheses from a first-pass RNN-T system, with varying data fractions (0-100% personalized content) to assess generalization performance.

## Key Results
- Gazetteers achieve 10% WER improvement on personalized test set while improving WER by 1% on general test set
- Natural language prompts improve WER by 7% without any training, with only marginal loss in generalization
- Cross-attention model shows promising results but requires careful data mixing to maintain general test performance
- Larger model sizes (big BERT) consistently outperform smaller variants across all approaches

## Why This Works (Mechanism)

### Mechanism 1: Cross-Attention with Encoded Named Entities
The cross-attention encoder-decoder architecture improves recognition by allowing the rescoring model to directly attend to relevant named entities. The model encodes both the ASR hypothesis and matched named entities, then uses a cross-attention layer to let the hypothesis attend to relevant entities, providing explicit semantic context for distinguishing homophones.

### Mechanism 2: Gazetteer Embeddings for Explicit Named Entity Marking
Adding slot embeddings for named entities at either input or last layer allows the model to learn distinct representations for named entities versus regular words. Each token is tagged as matching a named entity or not, converted to embeddings that provide a learned signal about which tokens are named entities.

### Mechanism 3: Natural Language Prompts as Contextual Augmentation
Appending natural language prompts that reference relevant named entities provides additional context that helps the rescoring model choose the correct hypothesis without requiring additional training. The prompt provides semantic context that can help distinguish between homophones or similar-sounding names.

## Foundational Learning

- **Transformer architectures and attention mechanisms**
  - Why needed: Understanding how transformers work is crucial for grasping how the cross-attention model combines hypothesis and named entity information
  - Quick check: How does the cross-attention layer in the encoder-decoder model allow the hypothesis to attend to the named entities?

- **Named entity recognition and matching**
  - Why needed: The gazetteer and prompt approaches rely on accurately identifying and matching named entities in the hypotheses
  - Quick check: What is the difference between the early fusion and late fusion gazetteer approaches in terms of where the slot embeddings are added?

- **Discriminative training objectives (MWER)**
  - Why needed: The rescoring models are trained using a minimum word error rate (MWER) loss, which directly optimizes for word error rate rather than log-likelihood
  - Quick check: How does the MWER loss differ from a standard cross-entropy loss in terms of what it optimizes for?

## Architecture Onboarding

- **Component map**: First-pass RNN-T model -> RescoreBERT model -> Gazetteer/Prompt/Cross-attention model -> Combined scoring -> Final hypothesis selection

- **Critical path**: 1) Generate n-best hypotheses from first-pass RNN-T 2) Extract and match named entities from hypotheses 3) Apply chosen personalization approach 4) Compute rescoring scores and combine with first-pass scores 5) Select final hypothesis based on combined scores

- **Design tradeoffs**: Gazetteer approaches require additional training but provide explicit entity marking; Prompt approach requires no training but may introduce noise; Cross-attention approach is most complex but potentially most effective; Larger models perform better but are more computationally expensive

- **Failure signatures**: Poor named entity extraction leading to missed or incorrect matches; Slot embeddings not effectively learned in gazetteer approaches; Prompts introducing noise rather than useful context; Cross-attention model overfitting to named entities and degrading general performance

- **First 3 experiments**: 1) Implement the prompt approach and evaluate its performance on a small dataset to verify the 7% improvement claim without training 2) Implement the early fusion gazetteer approach and compare its performance to the baseline RescoreBERT model on a dataset with known named entities 3) Implement the cross-attention encoder-decoder approach and evaluate its sensitivity to the amount of general training data by varying the data fraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data mixing ratio of personalized and general training data to achieve the best performance on both test sets for the gazetteer and prompting approaches?
- Basis in paper: The paper discusses the effectiveness of mixing general data into the training data to mitigate degradation on the general test set while maintaining performance on the personalized test set
- Why unresolved: The paper indicates that a suitable data mixing ratio exists but does not specify the exact optimal ratio for gazetteer and prompting approaches
- What evidence would resolve it: Conducting experiments with varying data mixing ratios and analyzing the performance on both test sets to identify the ratio that yields the best results for both gazetteer and prompting approaches

### Open Question 2
- Question: How do the gazetteer and prompting approaches scale with even larger transformer models, beyond the tiny and big BERT models tested in this study?
- Basis in paper: The paper suggests that both gazetteer and prompting approaches improve with larger model sizes but does not explore the scalability beyond the tested models
- Why unresolved: The study only tested two model sizes (tiny and big BERT), leaving the scalability of these approaches to even larger models unexplored
- What evidence would resolve it: Testing the gazetteer and prompting approaches with transformer models larger than the big BERT model used in the study and evaluating their performance on both test sets

### Open Question 3
- Question: What is the impact of prompt tuning and prompt engineering on the effectiveness of the prompting approach in improving WER for personalized content?
- Basis in paper: The paper mentions that no prompt tuning or prompt engineering was performed and used a simple phrase as the augmented prompt, suggesting potential for improvement
- Why unresolved: The study did not explore the effects of prompt tuning or engineering, leaving the potential benefits of these techniques unexplored
- What evidence would resolve it: Conducting experiments with different prompt tuning and engineering techniques and comparing their impact on WER for personalized content against the simple phrase used in the study

## Limitations

- Proprietary evaluation data limits external benchmarking and reproducibility
- Lack of detailed architectural specifications for the cross-attention model
- No exploration of prompt tuning or engineering techniques that could improve performance
- Limited model size testing (only tiny and big BERT variants)

## Confidence

- **High Confidence**: The core finding that gazetteers outperform other personalization approaches (10% WER improvement on personalized test set) is well-supported by experimental results and methodology
- **Medium Confidence**: The claim that natural language prompts can achieve 7% WER improvement without training is promising but requires careful validation; generalization performance claims are supported but warrant further testing
- **Low Confidence**: The specific implementation details of the cross-attention encoder-decoder architecture and exact slot token processing mechanisms are not fully specified, making exact reproduction challenging

## Next Checks

1. **Cross-Attention Architecture Implementation**: Implement the cross-attention encoder-decoder model with detailed slot token processing as described in Section 4.3, and verify that it achieves the reported WER improvements on both personalized and general test sets

2. **Prompt Approach Generalization**: Test the natural language prompt approach with multiple prompt variants beyond the single "as i need to contact [entity]" phrase, and evaluate performance across different types of named entities (contacts, locations, organizations) to assess generalizability

3. **Data Dependency Analysis**: Conduct a systematic study of how the personalization approaches perform with varying amounts of general training data (as suggested in Figure 2), particularly examining the cross-attention model's performance at different data fraction points to identify potential overfitting risks