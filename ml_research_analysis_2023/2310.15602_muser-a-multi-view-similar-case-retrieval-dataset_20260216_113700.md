---
ver: rpa2
title: 'MUSER: A Multi-View Similar Case Retrieval Dataset'
arxiv_id: '2310.15602'
source_url: https://arxiv.org/abs/2310.15602
tags:
- legal
- case
- cases
- retrieval
- focus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MUSER is a new legal AI dataset for similar case retrieval (SCR)
  from Chinese civil law. It improves on prior datasets by incorporating multi-view
  case similarity based on legal facts, dispute focus, and law statutory dimensions,
  plus sentence-level legal element annotations to encode deep legal knowledge.
---

# MUSER: A Multi-View Similar Case Retrieval Dataset

## Quick Facts
- arXiv ID: 2310.15602
- Source URL: https://arxiv.org/abs/2310.15602
- Reference count: 33
- Primary result: MUSER introduces a Chinese civil law dataset with multi-view similarity measurement and sentence-level legal element annotations, showing that legal element-aware retrieval models significantly outperform traditional IR approaches.

## Executive Summary
MUSER is a new dataset for similar case retrieval (SCR) in Chinese civil law that advances the field by incorporating multi-view similarity assessment across legal fact, dispute focus, and law statutory dimensions. The dataset includes 100 query cases and 4,024 candidate cases with comprehensive sentence-level legal element annotations spanning 22 first-level, 190 second-level, and 505 third-level legal elements. Experimental results demonstrate that retrieval models incorporating legal elements significantly outperform traditional text-based IR methods, though legal element prediction remains challenging due to label imbalance and complexity. The work highlights both the potential of legal knowledge integration in SCR and the unique difficulties of civil case retrieval compared to criminal cases.

## Method Summary
The MUSER dataset construction involved three main phases: (1) selecting 100 query cases and 4,024 candidate cases from Chinese civil judgments, (2) designing a three-level hierarchical legal element label schema across three perspectives (legal fact, dispute focus, law statutory), and (3) annotating each sentence with relevant legal elements. The dataset was then used to evaluate baseline retrieval models including traditional IR approaches (BM25, TF-IDF, LMIR) and legal element-aware models. Legal element prediction was performed using BERT and Lawformer fine-tuned as multi-label classifiers, while retrieval was implemented through both text-based similarity and cosine similarity of legal element vectors with dimension-specific weighting (legal fact 0.5, dispute focus 0.4, law statutory 0.1).

## Key Results
- Legal element-aware retrieval models achieved significantly higher precision (P@5, P@10, MAP) and ranking (NDCG@10, NDCG@20, NDCG@30) metrics compared to traditional text-based IR models
- The multi-view similarity approach incorporating legal fact, dispute focus, and law statutory dimensions outperformed single-view alternatives
- Legal element prediction remains challenging due to severe label imbalance, with some elements having significantly fewer instances than others
- Civil case retrieval proved more difficult than criminal case retrieval, highlighting the complexity of civil legal relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-view similarity measurement improves case relevance assessment by incorporating legal fact, dispute focus, and law statutory dimensions.
- Mechanism: By dividing case evaluation into three distinct perspectives, the system captures different aspects of case similarity that would be missed by single-view approaches. This allows for more nuanced matching between query and candidate cases.
- Core assumption: Legal cases can be meaningfully decomposed into these three dimensions, and each dimension provides complementary information about case similarity.
- Evidence anchors:
  - [abstract]: "we select three perspectives (legal fact, dispute focus, and law statutory) and build a comprehensive and structured label schema of legal elements for each of them"
  - [section]: "The 'multi-view' similarity between query and candidate cases is judged by the following three dimensions: legal fact, dispute focus, and law statutory"
  - [corpus]: Weak evidence - related papers mention similar multi-view approaches but don't provide direct validation of this specific mechanism
- Break condition: If the three dimensions don't capture the full complexity of legal cases, or if the weighting between dimensions is suboptimal, the multi-view approach could underperform single-view methods.

### Mechanism 2
- Claim: Sentence-level legal element annotations enable more precise retrieval by focusing on specific legal concepts rather than just text semantics.
- Mechanism: By annotating individual sentences with detailed legal elements, the retrieval system can match cases based on their underlying legal structure rather than surface-level textual similarity.
- Core assumption: Legal concepts are best represented at the sentence level and can be accurately annotated with a hierarchical label schema.
- Evidence anchors:
  - [abstract]: "sentence-level legal element annotations to encode deep legal knowledge"
  - [section]: "we propose a sentence-level legal element label system for the legal fact and dispute focus to represent the deep legal knowledge implied in cases"
  - [corpus]: Moderate evidence - the dataset construction shows this approach was implemented and tested
- Break condition: If legal elements cannot be accurately annotated at the sentence level, or if the hierarchical schema is too complex to be practical, this mechanism would fail.

### Mechanism 3
- Claim: Incorporating structured legal knowledge through annotated elements outperforms traditional text-based IR models.
- Mechanism: Legal element-aware models use the annotated legal concepts as features for matching, rather than relying solely on bag-of-words or semantic similarity.
- Core assumption: Legal cases have underlying structural patterns that can be captured by the legal element labels and used for better matching.
- Evidence anchors:
  - [abstract]: "experimental results indicate that incorporating legal elements can benefit the performance of SCR models"
  - [section]: "We implement several text classification algorithms for legal element prediction and various retrieval methods for retrieving similar cases on MUSER"
  - [corpus]: Strong evidence - baseline experiments show legal element-aware models outperforming traditional IR models
- Break condition: If the legal element annotations are too sparse or imbalanced, or if the model cannot effectively use these features, the performance advantage would disappear.

## Foundational Learning

- Concept: Legal document structure and terminology
  - Why needed here: Understanding the different sections of legal documents (court's findings of fact, court's opinions, law statutory) is crucial for interpreting the dataset and the multi-view approach
  - Quick check question: What are the three main sections of a Chinese civil judgment document according to this work?

- Concept: Hierarchical classification and multi-label classification
  - Why needed here: The legal element annotation system uses a three-level hierarchical structure, and each sentence can have multiple labels assigned
  - Quick check question: How many levels deep is the legal element label schema in this dataset?

- Concept: Information retrieval evaluation metrics
  - Why needed here: Understanding metrics like precision@k, MAP, and NDCG is essential for interpreting the experimental results and comparing different retrieval models
  - Quick check question: What does P@5 measure in the context of similar case retrieval?

## Architecture Onboarding

- Component map: Data collection → Label schema design → Annotation → Model training → Evaluation pipeline
- Critical path: Query and candidate case selection → Annotation process → Model training and evaluation
- Design tradeoffs: The dataset prioritizes comprehensiveness and real-world complexity over balance and simplicity
- Failure signatures: Poor performance on ranking metrics suggests issues with the multi-view approach or legal element annotations
- First 3 experiments:
  1. Compare multi-view vs single-view retrieval performance using the same underlying model
  2. Test different weighting schemes for combining the three dimensions
  3. Evaluate the impact of different levels of the legal element hierarchy on retrieval performance

## Open Questions the Paper Calls Out

- How can the hierarchical relationships between legal elements be effectively incorporated into legal element prediction models?
- What specific challenges make civil case retrieval more difficult than criminal case retrieval, and how can these be addressed?
- How can the data imbalance in the MUSER dataset be addressed to improve legal element prediction and similar case retrieval?

## Limitations
- The multi-view similarity framework assumes equal relevance of legal fact, dispute focus, and law statutory dimensions across all case types
- Dataset focus on Chinese civil cases limits generalizability to other legal domains and jurisdictions
- Severe label imbalance in the legal element hierarchy affects model performance and may introduce retrieval bias
- The optimal weighting scheme for combining the three dimensions remains unexplored

## Confidence
- High confidence: Dataset construction methodology, legal element annotation system, and baseline experimental results showing legal element-aware models outperform traditional IR approaches
- Medium confidence: Generalizability of multi-view similarity measurement across different types of legal cases and legal systems
- Low confidence: Optimal weighting scheme for combining the three dimensions and sensitivity of performance to these weights

## Next Checks
1. Conduct ablation studies to determine the individual contribution of each dimension (legal fact, dispute focus, law statutory) to overall retrieval performance
2. Test the transferability of the multi-view approach on a different legal domain (e.g., criminal cases) using the same methodology
3. Evaluate the impact of different legal element hierarchy depths on retrieval performance by comparing models trained on only first-level elements versus full three-level annotations