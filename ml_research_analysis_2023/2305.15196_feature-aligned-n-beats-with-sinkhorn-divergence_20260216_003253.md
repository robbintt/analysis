---
ver: rpa2
title: Feature-aligned N-BEATS with Sinkhorn divergence
arxiv_id: '2305.15196'
source_url: https://arxiv.org/abs/2305.15196
tags:
- feature
- forecasting
- domain
- alignment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Feature-aligned N-BEATS with Sinkhorn divergence addresses the
  challenge of domain generalization in time series forecasting. The method extends
  N-BEATS by incorporating stack-wise feature alignment using the Sinkhorn divergence.
---

# Feature-aligned N-BEATS with Sinkhorn divergence

## Quick Facts
- arXiv ID: 2305.15196
- Source URL: https://arxiv.org/abs/2305.15196
- Reference count: 40
- Method extends N-BEATS with stack-wise feature alignment using Sinkhorn divergence for domain generalization in time series forecasting

## Executive Summary
This paper addresses the challenge of domain generalization in time series forecasting by proposing Feature-aligned N-BEATS with Sinkhorn divergence. The method extends the original N-BEATS architecture by incorporating stack-wise feature alignment across multiple source domains, using the Sinkhorn divergence to align marginal feature probability measures induced by residual operators. This approach enables the model to learn invariant features across domains while maintaining the interpretability and forecasting power of the original N-BEATS. Comprehensive experiments on financial and weather data demonstrate improved performance in cross-domain and out-domain generalization scenarios compared to standard N-BEATS.

## Method Summary
The method builds upon the N-BEATS architecture by adding stack-wise feature alignment through Sinkhorn divergence. It trains on multiple source domains while aligning marginal feature probability measures induced by residual operators and feature extractors across stacks. The training combines empirical risk minimization from multiple source domains with an alignment loss calculated using the Sinkhorn divergence. Alternate optimization is used to balance forecasting accuracy with domain generalization, allowing the model to maintain strong forecasting performance while learning invariant features.

## Key Results
- Feature-aligned N-BEATS outperforms standard N-BEATS in most cross-domain and out-domain generalization scenarios
- Stack-wise feature alignment successfully mitigates performance degradation when testing on unseen domains
- The model maintains interpretability while achieving improved generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stack-wise feature alignment enables domain-invariant learning while preserving interpretability
- Mechanism: The model aligns marginal feature probability measures induced by the composition of residual operators and feature extractors across stacks using Sinkhorn divergence. This sparse alignment propagates gradients only stack-wise rather than block-wise, avoiding exploding/vanishing gradients in long-term forecasting.
- Core assumption: The residual operators and feature extractors have Lipschitz continuity, ensuring the feature maps are well-behaved under optimal transport distances
- Evidence anchors:
  - [abstract]: "marginal feature probability measures induced by the composition of residual and feature extracting operators...are aligned stack-wise via an approximate of an optimal transport distance referred to as the Sinkhorn divergence"
  - [section 4.1]: "we devise a stack-wise alignment approach that aligns feature measures induced by the composition of residual operators and a normalization operator"
  - [corpus]: Weak - no direct mentions of "stack-wise" or "residual operators" alignment in related papers

### Mechanism 2
- Claim: Sinkhorn divergence provides computational efficiency for high-dimensional sequential data
- Mechanism: The entropic regularized Wasserstein distance (Sinkhorn divergence) enables efficient calculation of divergences between high-dimensional feature distributions while maintaining theoretical properties of optimal transport
- Core assumption: The entropic regularization term improves computational stability without significantly biasing the distance estimates
- Evidence anchors:
  - [section 4.2]: "we adopt the following debiased version of the regularized distance (8), which is referred to as the Sinkhorn divergence"
  - [section 4.2]: "Given that our feature measures are defined as pushforward measures and our objective is time series forecasting, we adopt the Sinkhorn divergence as the distance metric"
  - [corpus]: Weak - only one related paper mentions Sinkhorn divergence in the context of GANs, not time series forecasting

### Mechanism 3
- Claim: Alternate optimization balances forecasting accuracy with domain generalization
- Mechanism: The training alternates between optimizing parameters for forecasting loss and parameters for feature alignment loss, allowing the model to maintain strong forecasting performance while learning invariant features
- Core assumption: The two optimization objectives (forecasting and alignment) can be effectively decoupled without interfering with each other's convergence
- Evidence anchors:
  - [section 4.3]: "we adopt the following alternate optimization inspired from [11, Section 3.1]: Θ*↓, Θ*↑ := arg minΘ↓,Θ↑ L(F(Φ*, Θ↓, Θ↑)) and Φ* := arg minΦ Lλ(Φ, Θ*↓, Θ*↑)"
  - [section 4.3]: "To bring these two components together, we adopt the following alternate optimization"
  - [corpus]: Weak - no direct mentions of alternate optimization in related papers

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distances
  - Why needed here: The model uses Sinkhorn divergence (entropic regularized Wasserstein distance) as the metric for aligning feature distributions across domains
  - Quick check question: What is the key difference between Wasserstein distance and other divergence measures like KL divergence when comparing probability distributions?

- Concept: Pushforward Measures and Representation Learning
  - Why needed here: The model defines feature alignment in terms of pushforward measures induced by the residual architecture's feature extractors
  - Quick check question: How does the pushforward measure σ∘gm#PkX differ from simply comparing raw feature distributions across domains?

- Concept: Lipschitz Continuity and Gradient Stability
  - Why needed here: The residual operators and normalization functions must be Lipschitz continuous to ensure stable gradient flow during training and to satisfy theoretical bounds
  - Quick check question: Why is Lipschitz continuity particularly important for the residual operators in a doubly residual stacking architecture?

## Architecture Onboarding

- Component map:
  Input -> Residual blocks (M stacks, L blocks each) -> Normalization -> Marginal feature measures -> Sinkhorn divergence -> Forecast

- Critical path:
  1. Forward pass through residual blocks to extract features
  2. Calculate marginal feature measures for each stack
  3. Compute Sinkhorn divergence between source domains
  4. Alternate optimization between forecasting and alignment losses
  5. Update parameters and repeat

- Design tradeoffs:
  - Stack-wise vs block-wise alignment: Stack-wise reduces gradient flow complexity but may miss finer-grained invariances
  - Normalization function choice: Softmax provides probabilistic interpretation but tanh may preserve more information
  - Regularization strength λ: Higher values emphasize domain generalization but may hurt forecasting accuracy

- Failure signatures:
  - Training instability: Check Lipschitz constants and normalization scaling
  - Poor generalization: Verify alignment loss is actually reducing domain gaps
  - Degraded forecasting: Ensure alternate optimization isn't causing catastrophic forgetting

- First 3 experiments:
  1. Compare stack-wise vs block-wise alignment on a simple synthetic dataset with known domain shifts
  2. Test different normalization functions (softmax, tanh, none) on a small real-world dataset
  3. Evaluate the effect of regularization strength λ on the bias-variance tradeoff between forecasting accuracy and generalization performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of stacks (M) and blocks per stack (L) for Feature-aligned N-BEATS in different domain generalization scenarios?
- Basis in paper: [inferred] The paper uses M=3 and L=4 in experiments but does not explore variations systematically.
- Why unresolved: The choice of architecture depth and width could significantly impact performance, especially across different domain shift scenarios.
- What evidence would resolve it: A comprehensive ablation study varying M and L values across all three generalization scenarios (IDG, CDG, ODG) to identify optimal configurations.

### Open Question 2
- Question: How does the choice of normalizing function (softmax, tanh, or identity) interact with different domain characteristics?
- Basis in paper: [explicit] The paper tests different normalizing functions but only reports aggregate results.
- Why unresolved: The paper notes that domain-dependent scale characteristics might influence which normalizing function works best, but doesn't explore this systematically.
- What evidence would resolve it: Analysis correlating normalizing function performance with domain characteristics like scale variation, frequency content, or noise levels across the tested domains.

### Open Question 3
- Question: Can spectral normalization be effectively integrated into the residual operators to improve stability and performance?
- Basis in paper: [explicit] The paper mentions spectral normalization as a potential future direction in the conclusion.
- Why unresolved: While the paper establishes theoretical bounds, it doesn't implement or evaluate spectral normalization in practice.
- What evidence would resolve it: Experimental results comparing Feature-aligned N-BEATS with and without spectral normalization applied to the residual operators, measuring both stability during training and final performance.

### Open Question 4
- Question: How does Feature-aligned N-BEATS perform when the domain shift is subtle versus dramatic?
- Basis in paper: [explicit] The paper includes a "subtle domain shift" experiment in Appendix E.4 but provides limited analysis.
- Why unresolved: The paper only provides aggregate performance metrics without investigating when feature alignment helps versus hurts.
- What evidence would resolve it: Detailed analysis identifying specific domain characteristics or shift magnitudes where feature alignment provides benefits versus cases where it degrades performance.

## Limitations
- The stack-wise alignment mechanism may miss higher-order interactions between features across different levels of the architecture
- The effectiveness depends on the assumption that marginal feature distributions contain sufficient information for domain generalization
- The choice of softmax normalization may introduce information loss compared to other normalization schemes

## Confidence

**High Confidence**: The empirical results showing improved cross-domain generalization performance over baseline N-BEATS models

**Medium Confidence**: The theoretical framework connecting optimal transport distances to feature alignment in residual architectures, pending verification of Lipschitz continuity assumptions

**Low Confidence**: The claim that stack-wise alignment is superior to block-wise or global alignment strategies, as this was not systematically compared in experiments

## Next Checks

1. Test the sensitivity of performance to different normalization functions (softmax vs tanh vs no normalization) to verify that softmax is optimal for this alignment task

2. Compare stack-wise alignment against block-wise and global alignment strategies on synthetic datasets with known domain shift patterns to determine the most effective granularity

3. Analyze the learned feature representations using visualization techniques to confirm that aligned features actually capture domain-invariant information rather than coincidental patterns