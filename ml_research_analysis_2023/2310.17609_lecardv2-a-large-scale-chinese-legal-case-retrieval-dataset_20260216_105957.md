---
ver: rpa2
title: 'LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset'
arxiv_id: '2310.17609'
source_url: https://arxiv.org/abs/2310.17609
tags:
- cases
- case
- legal
- retrieval
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LeCaRDv2, a large-scale Chinese legal case
  retrieval dataset designed to address limitations in existing datasets. It contains
  800 queries and 55,192 candidate cases covering 50 criminal charges, significantly
  larger than previous datasets.
---

# LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset

## Quick Facts
- arXiv ID: 2310.17609
- Source URL: https://arxiv.org/abs/2310.17609
- Reference count: 36
- Primary result: Largest Chinese legal case retrieval dataset with 800 queries and 55,192 candidates annotated by 41 legal experts

## Executive Summary
This paper presents LeCaRDv2, a large-scale Chinese legal case retrieval dataset designed to address limitations in existing datasets. The dataset contains 800 queries and 55,192 candidate cases covering 50 criminal charges, significantly larger than previous datasets. The authors introduce comprehensive relevance criteria considering characterization, penalty, and procedure aspects, guided by official Chinese Supreme People's Court documents. They propose a two-level candidate pooling strategy combining sparse lexical matching, dense semantic retrieval, and law article similarity to identify diverse potential cases. The dataset is annotated by 41 legal experts, achieving a Kappa value of 0.5190 for overall relevance. Experiments with state-of-the-art retrieval models show significant room for improvement, with traditional methods like BM25 achieving strong performance.

## Method Summary
LeCaRDv2 employs a comprehensive methodology for legal case retrieval dataset construction. The process begins with corpus preprocessing of 4.3 million criminal case documents, extracting fact, reason, and decision sections. Queries are sampled to cover 50 criminal charges using a strategy that balances common, controversial, and procedural cases. The two-level candidate pooling strategy combines three distinct methods: sparse lexical matching (BM25), dense semantic retrieval using Chinese legal language models, and law article similarity with Inverse Provision Frequency. This produces diverse candidate sets that are then refined through ranking pooling. The resulting candidates undergo annotation by 41 legal experts using comprehensive relevance criteria covering characterization, penalty, and procedure aspects. The dataset is evaluated using recall@k metrics and tested with state-of-the-art retrieval models.

## Key Results
- LeCaRDv2 is the largest Chinese legal case retrieval dataset with 800 queries and 55,192 annotated candidates
- Expert annotation achieves a Kappa value of 0.5190 for overall relevance assessment
- Traditional retrieval methods like BM25 outperform state-of-the-art dense retrieval models on this dataset
- The dataset covers 50 criminal charges with diverse query types including common, controversial, and procedural cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-level candidate pooling strategy improves retrieval diversity by combining different similarity metrics
- Mechanism: Retrieval pooling uses three distinct methods (sparse lexical matching, dense semantic retrieval, law article similarity) to create diverse candidate sets, then rank pooling refines these with multiple ranking runs
- Core assumption: Different retrieval methods capture complementary aspects of legal relevance that individual methods miss
- Evidence anchors:
  - [abstract] "propose a two-level candidate pooling strategy combining sparse lexical matching, dense semantic retrieval, and law article similarity"
  - [section 3.4.1] "we employ three distinct methods with different properties: sparse lexical matching, dense semantic retrieval, and the proposed law article similarity"
  - [corpus] Weak - corpus only shows related legal datasets but no direct evidence of this specific pooling strategy
- Break condition: If candidate sets become too large to annotate effectively, or if different methods produce highly overlapping results

### Mechanism 2
- Claim: Comprehensive relevance criteria improve annotation quality and model evaluation
- Mechanism: Three-aspect relevance criteria (characterization, penalty, procedure) provide more nuanced evaluation than single-dimension approaches, leading to better quality data
- Core assumption: Legal relevance requires consideration of multiple dimensions beyond basic fact similarity
- Evidence anchors:
  - [abstract] "enrich the existing relevance criteria by considering three key aspects: characterization, penalty, procedure"
  - [section 3.5] "we design a more comprehensive relevance criteria, which follow the official guidance better"
  - [corpus] Weak - corpus shows related legal datasets but no direct evidence of multi-dimensional relevance criteria effectiveness
- Break condition: If annotators struggle to consistently apply multiple criteria, or if criteria become too complex to use effectively

### Mechanism 3
- Claim: Large-scale dataset enables better model training and evaluation than smaller alternatives
- Mechanism: 800 queries and 55,192 candidate cases provide sufficient data for training and reliable evaluation of legal case retrieval models
- Core assumption: Model performance improves with larger, more diverse training data
- Evidence anchors:
  - [abstract] "consists of 800 queries and 55,192 candidates extracted from 4.3 million criminal case documents"
  - [section 3.7.1] "LeCaRDv2 is the largest Chinese legal case retrieval dataset with tens of thousands of annotated data"
  - [corpus] Weak - corpus only shows related datasets but no direct evidence of scale benefits
- Break condition: If additional data doesn't improve model performance, or if annotation quality decreases with scale

## Foundational Learning

- Concept: Legal case retrieval vs. general document retrieval
  - Why needed here: Legal case retrieval has unique relevance criteria and document structures that differ from general IR
  - Quick check question: What are the three main aspects of legal relevance according to this paper?

- Concept: Chinese legal system characteristics
  - Why needed here: Understanding statutory law vs. case law systems affects relevance criteria and dataset design
  - Quick check question: How does the Chinese legal system differ from the Canadian case law system mentioned in COLIEE?

- Concept: Candidate pooling strategies
  - Why needed here: Different pooling approaches affect the diversity and quality of retrieved candidates
  - Quick check question: What are the two levels of pooling proposed in this paper?

## Architecture Onboarding

- Component map: Corpus → Query Selection → Candidate Pooling → Annotation → Evaluation
- Critical path: Query selection → candidate pooling → annotation → model evaluation
- Design tradeoffs: Scale vs. annotation quality, comprehensive criteria vs. annotation complexity, diversity vs. precision
- Failure signatures: Low kappa values indicating annotation disagreement, poor model performance despite large dataset, high overlap between pooling methods
- First 3 experiments:
  1. Compare single-method vs. two-level pooling performance on sample queries
  2. Test different relevance criteria combinations on annotation quality
  3. Evaluate model performance on different query types (common, controversial, procedural)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design retrieval models that effectively leverage the three aspects of legal relevance (characterization, penalty, procedure) simultaneously?
- Basis in paper: [explicit] The authors note that traditional methods like BM25 achieve strong performance on LeCaRDv2, suggesting that existing pre-trained language models struggle to capture the nuanced legal relevance criteria
- Why unresolved: The paper demonstrates that even state-of-the-art retrieval models underperform traditional methods, indicating that current approaches fail to adequately incorporate the multi-faceted legal relevance criteria
- What evidence would resolve it: Comparative experiments showing significant performance improvements when retrieval models are explicitly designed to consider characterization, penalty, and procedure aspects together, validated on LeCaRDv2

### Open Question 2
- Question: Can the proposed Inverse Provision Frequency (IPF) method be extended to other legal systems beyond Chinese criminal law?
- Basis in paper: [explicit] The authors propose IPF specifically for measuring law article similarity in Chinese criminal cases, noting that legal relevance differs across jurisdictions
- Why unresolved: While IPF shows promise for Chinese criminal law, its effectiveness in other legal systems (common law, civil law, etc.) with different legal structures and citation practices remains unexplored
- What evidence would resolve it: Cross-jurisdictional experiments applying IPF to legal case retrieval datasets from different legal systems, demonstrating consistent performance improvements

### Open Question 3
- Question: What is the optimal candidate pooling strategy that balances diversity and relevance for legal case retrieval?
- Basis in paper: [explicit] The authors propose a two-level candidate pooling strategy but acknowledge it as a "novel" approach that may not be optimal
- Why unresolved: The paper introduces a pooling strategy combining sparse lexical matching, dense semantic retrieval, and law article similarity, but doesn't explore alternative strategies or conduct ablation studies to determine the relative importance of each component
- What evidence would resolve it: Systematic ablation studies and comparison with alternative pooling strategies on LeCaRDv2, identifying the most effective combination of methods for identifying diverse yet relevant candidate cases

## Limitations
- The effectiveness of the two-level candidate pooling strategy lacks empirical comparison against simpler pooling methods
- Comprehensive relevance criteria may introduce annotation complexity that affects consistency
- Real-world impact on legal case retrieval performance and model generalizability remains unproven

## Confidence

- **High confidence**: Dataset scale and construction methodology (800 queries, 55,192 candidates, expert annotation process)
- **Medium confidence**: Two-level candidate pooling effectiveness and comprehensive relevance criteria improvements
- **Low confidence**: Real-world impact on legal case retrieval performance and model generalizability

## Next Checks

1. Compare two-level pooling performance against single-method pooling on a subset of queries to quantify diversity benefits
2. Conduct inter-annotator reliability analysis across different charge categories to identify potential bias or difficulty patterns
3. Test dataset performance with legal domain-specific models versus general IR models to validate domain adaptation effectiveness