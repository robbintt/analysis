---
ver: rpa2
title: Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning
arxiv_id: '2311.00865'
source_url: https://arxiv.org/abs/2311.00865
tags:
- agents
- ddqn
- performance
- super
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method called SUPER (Selective Multi-Agent
  Prioritized Experience Relay) that improves multi-agent reinforcement learning by
  selectively sharing a limited number of high-value experiences between agents during
  training. The approach uses temporal difference error to prioritize which experiences
  to share, allowing for largely decentralized training with only limited communication
  bandwidth.
---

# Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.00865
- Source URL: https://arxiv.org/abs/2311.00865
- Reference count: 40
- Key outcome: SUPER improves multi-agent RL by selectively sharing high-TD-error experiences between agents during decentralized training

## Executive Summary
This paper introduces SUPER (Selective Multi-Agent Prioritized Experience Relay), a method that improves multi-agent reinforcement learning by selectively sharing a limited number of high-value experiences between agents during training. The approach uses temporal difference error to prioritize which experiences to share, allowing for largely decentralized training with only limited communication bandwidth. Experiments across multiple benchmark domains show that SUPER significantly outperforms baseline no-sharing decentralized training and state-of-the-art multi-agent RL algorithms like MADDPG and SEAC.

## Method Summary
SUPER implements selective experience sharing in multi-agent RL by having each agent maintain a buffer of recent temporal difference errors and share only experiences whose TD-error exceeds a quantile or threshold. The algorithm uses DQN variants for individual agent learning while exchanging a small fraction of high-value experiences with other agents through a limited communication channel. Three selection modes are implemented: deterministic quantile, deterministic Gaussian, and stochastic weighted. The bandwidth parameter β controls the fraction of experiences shared (typically 0.01-0.1).

## Key Results
- SUPER outperforms baseline DDQN and no-sharing variants across all tested environments
- Performance gains are robust across different hyperparameters and DQN variants
- Sharing only a small number of highly relevant experiences proves more effective than sharing all experiences
- Deterministic experience selection performs slightly better than stochastic selection
- Optimal bandwidth for sharing is between 0.01-0.1 in tested environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective sharing of high-TD-error experiences accelerates learning by prioritizing the most informative transitions
- Core assumption: High TD-error correlates with high learning value for other agents in symmetric environments
- Evidence anchors: Theoretical foundation from Prioritized Experience Replay; experimental validation across multiple domains
- Break condition: In asymmetric environments where TD-error of sender doesn't reflect receiver's potential learning

### Mechanism 2
- Claim: Experience sharing with small communication bandwidth enables decentralized training while gaining centralized-training benefits
- Core assumption: A small number of relevant experiences can provide similar learning acceleration as full centralized training
- Evidence anchors: Low bandwidth requirements demonstrated experimentally; scalability benefits over centralized approaches
- Break condition: When communication bandwidth is too constrained to transmit even the most relevant experiences

### Mechanism 3
- Claim: Deterministic experience selection based on quantiles outperforms stochastic selection
- Core assumption: In experience relay, each experience has only one chance to be shared, making deterministic selection more reliable
- Evidence anchors: Ablation studies showing deterministic selection performs slightly better
- Break condition: If experiences are sampled multiple times or if stochastic selection is modified

## Foundational Learning

- Concept: Temporal Difference (TD) Error
  - Why needed here: TD error measures how "surprised" the agent is by an experience, serving as the heuristic for selecting which experiences to share
  - Quick check question: If an agent observes a transition that matches its Q-value predictions exactly, what is the TD-error for that experience?

- Concept: Prioritized Experience Replay (PER)
  - Why needed here: Provides the theoretical foundation for using TD-error as a relevance metric and demonstrates that selective sampling improves learning efficiency
  - Quick check question: In standard PER, what probability distribution is used to sample experiences from the replay buffer?

- Concept: Markov Games
  - Why needed here: The formal framework for multi-agent RL where environment dynamics depend on joint actions of all agents
  - Quick check question: In a Markov game with n agents, how is the joint action space defined?

## Architecture Onboarding

- Component map: Environment → Experience collection → Replay buffer → TD error calculation → Experience selection → Communication → Other agents' replay buffers → Training
- Critical path: 1) Collect experience from environment 2) Calculate TD-error and update sliding window 3) Select experiences to share based on threshold/quantile 4) Send selected experiences to other agents 5) Insert received experiences into replay buffer 6) Sample training batch (potentially with PER) 7) Perform gradient update
- Design tradeoffs: Bandwidth vs. learning speed; Deterministic vs. stochastic selection; Window size vs. responsiveness
- Failure signatures: Learning plateaus early (bandwidth too low); Degraded performance vs. baseline (indiscriminate sharing); High variance in performance (unstable TD-error distribution)
- First 3 experiments: 1) Run baseline DDQN vs SUPER with target bandwidth 0.1 on Pursuit environment 2) Test different bandwidth settings (0.01, 0.1, 1.0) 3) Compare deterministic vs. stochastic experience selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SUPER perform in environments with asymmetric agent capabilities or roles?
- Basis in paper: The paper mentions that SUPER assumes "anonymous" environments where experiences from one agent are meaningful to another as-is, but acknowledges this assumption may not hold in entirely asymmetric domains
- Why unresolved: Experiments focused exclusively on symmetric domains with identical agent capabilities
- What evidence would resolve it: Experiments in asymmetric multi-agent environments comparing SUPER to baselines

### Open Question 2
- Question: What is the optimal bandwidth parameter for SUPER across different environment types?
- Basis in paper: Analysis was limited to Pursuit environment, finding optimal performance around 0.01-0.1 bandwidth
- Why unresolved: Limited sensitivity analysis across diverse environments
- What evidence would resolve it: Systematic bandwidth sensitivity analysis across multiple environments

### Open Question 3
- Question: How does SUPER perform when combined with other multi-agent RL techniques like QMIX or counterfactual reasoning approaches?
- Basis in paper: The paper mentions potential for combining SUPER with credit assignment techniques
- Why unresolved: Experiments focused on comparing SUPER against baseline algorithms rather than combinations
- What evidence would resolve it: Experiments integrating SUPER with other multi-agent RL methods

## Limitations
- Performance in asymmetric environments with heterogeneous agents remains untested
- Bandwidth sensitivity analysis limited to one environment (Pursuit)
- No exploration of combining SUPER with other multi-agent RL techniques like QMIX or COMA

## Confidence
- Mechanism 1 (TD-error prioritization): High confidence
- Mechanism 2 (Bandwidth-limited communication): Medium confidence
- Mechanism 3 (Deterministic selection superiority): Medium confidence

## Next Checks
1. Test SUPER in asymmetric environments where agents have different observation/action spaces to verify the TD-error correlation assumption
2. Measure the actual communication bandwidth consumption during training to validate the "limited bandwidth" claim quantitatively
3. Conduct a systematic study varying the experience selection window size to determine optimal temporal horizon for TD-error tracking