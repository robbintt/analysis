---
ver: rpa2
title: Scalable Extraction of Training Data from (Production) Language Models
arxiv_id: '2311.17035'
source_url: https://arxiv.org/abs/2311.17035
tags:
- book
- football
- data
- training
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models memorize training data, but existing attacks
  recover far less data than is actually memorized. We develop a scalable methodology
  to extract training data from open, semi-open, and closed models.
---

# Scalable Extraction of Training Data from (Production) Language Models

## Quick Facts
- arXiv ID: 2311.17035
- Source URL: https://arxiv.org/abs/2311.17035
- Reference count: 40
- One-line primary result: We develop a scalable methodology to extract gigabytes of training data from open, semi-open, and closed language models, finding that alignment does not prevent memorization and larger models leak more data.

## Executive Summary
We present a scalable methodology for extracting training data from large language models, demonstrating that models memorize significant portions of their training data that can be recovered through specialized attacks. Our approach works across 17 diverse models including open, semi-open, and closed systems, with particular success against ChatGPT despite its alignment. We show that prompting with random strings suffices for unaligned models, while a novel "divergence attack" causes ChatGPT to emit training data at 150x the normal rate. Across all models tested, larger models leak more data, and current alignment techniques do not eliminate memorization vulnerabilities.

## Method Summary
We develop a scalable extraction methodology that combines efficient substring search using suffix arrays, membership inference via likelihood-ratio perplexity scoring, and extrapolation techniques using Good-Turing frequency estimation. For unaligned models, we use random 5-token prompts from Internet text. For ChatGPT, we developed a divergence attack that involves prompting with a single token repeated many times, which causes the model to bypass alignment constraints and emit training data at dramatically higher rates. We construct a 9TB auxiliary dataset (AUXDATASET) from multiple sources to serve as ground truth for identifying memorized content, and apply these techniques to extract gigabytes of training data across 17 models ranging from 160M to 175B parameters.

## Key Results
- Across 17 models, larger models leak more training data and are more vulnerable to extraction attacks
- ChatGPT is the most vulnerable model despite alignment, leaking data at 150x the normal rate when subjected to the divergence attack
- Current alignment techniques do not eliminate memorization - models still emit gigabytes of verbatim training data
- Our attacks recover gigabytes of training data including personally identifiable information and copyrighted material

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeating a single token indefinitely causes ChatGPT to "diverge" from its aligned chat persona and emit training data at rates up to 150× higher than normal.
- Mechanism: The model is trained with a special "end-of-text" token that signals the start of a new document. When a single token is repeated many times, the model's internal attention patterns begin to resemble those of the "beginning-of-sequence" token, causing it to "reset" and start generating as if it were beginning a new document, thereby bypassing alignment constraints.
- Core assumption: The alignment fine-tuning did not remove the underlying document-boundary behavior encoded during pretraining.
- Evidence anchors:
  - [abstract] "For aligned ChatGPT, a divergence attack that causes the model to emit training data at 150x the normal rate is effective."
  - [section] "We suspect that our attack works because it creates an effect similar to the <|endoftext|> token."
  - [corpus] Weak: No explicit corpus evidence of this mechanism; based on experimental observation and comparison to LLaMA behavior.
- Break condition: If the fine-tuning completely overwrites or masks the document-boundary attention patterns, the attack would fail.

### Mechanism 2
- Claim: Larger models leak more training data because they have greater capacity to memorize verbatim examples, and over-training amplifies this effect.
- Mechanism: Model scaling laws show that larger models can store more unique sequences. Over-training on many epochs over the same data increases the probability of exact memorization of training examples.
- Core assumption: Memorization is a monotonic function of model size and number of training epochs.
- Evidence anchors:
  - [abstract] "Across 17 models, larger models leak more data, and ChatGPT is the most vulnerable despite alignment."
  - [section] "We find that over-training increases privacy leakage" and "ChatGPT may have been pre-trained for many epochs."
  - [corpus] Moderate: Supporting evidence from scaling experiments with GPT-Neo 6B and Pythia models.
- Break condition: If regularization or architectural constraints prevent verbatim memorization regardless of size.

### Mechanism 3
- Claim: Aligned models appear more private under standard extraction attacks but are actually more vulnerable to specialized divergence attacks.
- Mechanism: Alignment fine-tuning trains the model to refuse or avoid generating certain types of content, making standard random-prompt attacks less effective. However, the underlying memorization remains intact and can be triggered by causing the model to diverge from its aligned behavior.
- Core assumption: Alignment modifies generation behavior but does not eliminate memorization of training data.
- Evidence anchors:
  - [abstract] "Current alignment techniques do not eliminate memorization" and "ChatGPT is the most vulnerable despite alignment."
  - [section] "Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack."
  - [corpus] Moderate: Experimental comparison between ChatGPT and unaligned models shows this pattern.
- Break condition: If alignment fine-tuning included specific mechanisms to detect and prevent extraction of memorized content.

## Foundational Learning

- Concept: Suffix arrays for efficient substring search in large datasets
  - Why needed here: The attack needs to efficiently check if generated text matches any substring in terabyte-scale training datasets
  - Quick check question: How does a suffix array enable O(log n) substring search instead of O(n) linear scan?

- Concept: Membership inference attacks using likelihood-ratio perplexity
  - Why needed here: To distinguish memorized training data from hallucinated text with high precision during extraction
  - Quick check question: What is the formula for the likelihood-ratio perplexity used to score potential memorization?

- Concept: Good-Turing frequency estimation for population extrapolation
  - Why needed here: To estimate total extractable memorization from limited generations when the full training set is unknown
  - Quick check question: How does Good-Turing smoothing help predict the probability of observing new unique memorized sequences?

## Architecture Onboarding

- Component map:
  - Prompt generation module: Creates diverse attack prompts (random strings, single-token repetition)
  - Model interaction layer: Interfaces with target LLM APIs (ChatGPT, GPT-3.5-turbo-instruct, etc.)
  - Generation storage: Captures and stores model outputs for analysis
  - Ground truth matching: Uses suffix arrays to efficiently match outputs against AUXDATASET
  - PII detection: Applies regex and LLM-based classification to identify sensitive information
  - Extrapolation engine: Applies Good-Turing and other estimators to project total memorization

- Critical path: Prompt generation → Model interaction → Generation storage → Ground truth matching → PII detection → Analysis

- Design tradeoffs:
  - Auxiliary dataset size vs. false negative rate: Larger AUXDATASET reduces false negatives but increases memory and I/O costs
  - Prompt diversity vs. attack effectiveness: More diverse prompts increase coverage but may reduce success rate per prompt type
  - String length threshold vs. false positive rate: Longer required matches reduce false positives but may miss shorter memorized examples

- Failure signatures:
  - No memorized examples found: Could indicate effective alignment, insufficient AUXDATASET coverage, or attack methodology issues
  - Extremely high false positive rate: May indicate need to adjust entropy threshold or string length requirements
  - Memory exhaustion: Suggests need to shard suffix arrays or reduce AUXDATASET size

- First 3 experiments:
  1. Test divergence attack on ChatGPT with single-token repetition and measure extraction rate vs baseline
  2. Vary string length threshold (50 tokens vs 30 vs 70) and measure impact on false positive/negative rates
  3. Apply Good-Turing extrapolation to estimate total memorization from limited generations and validate against known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does the vulnerability in ChatGPT generalize to other proprietary language models that use similar alignment techniques?
- Basis in paper: [explicit] The paper shows that ChatGPT can be manipulated to emit training data at 150x the normal rate, but notes this attack is "specific to this model and, to the best of our knowledge, is not applicable to any other production language model that we have tested."
- Why unresolved: The attack exploits a particular weakness in ChatGPT's alignment process, but it's unclear whether other proprietary models (e.g., GPT-4, Claude) have similar vulnerabilities or if their alignment techniques are more robust.
- What evidence would resolve it: Systematic testing of other proprietary models using the same divergence attack methodology, and development of model-specific attacks if needed.

### Open Question 2
- Question: What is the exact mechanism that causes ChatGPT to diverge and emit training data when prompted to repeat a single token many times?
- Basis in paper: [explicit] The paper observes that repeating a single token causes ChatGPT to diverge and emit training data, but doesn't fully explain why this specific prompting strategy works.
- Why unresolved: The authors speculate it might simulate the <|endoftext|> token that resets the model during pre-training, but this is not confirmed. Understanding the mechanism could help develop more effective attacks or defenses.
- What evidence would resolve it: Detailed analysis of the model's attention patterns and internal representations during divergence, and experiments with other tokens or prompts that might trigger similar behavior.

### Open Question 3
- Question: How much does training data deduplication actually reduce extractable memorization in practice, and what is the optimal deduplication strategy?
- Basis in paper: [explicit] The paper notes that Pythia models were trained with deduplication techniques, but "the total quantity of extractable memorization only decreases slightly" and suggests "coarse-grained deduplication was insufficient."
- Why unresolved: While deduplication is commonly used to reduce memorization, its practical effectiveness is unclear. The paper suggests it may even increase data emission rates, indicating complex trade-offs.
- What evidence would resolve it: Comparative studies of different deduplication strategies (e.g., fine-grained vs. coarse, frequency-based vs. content-based) across multiple model families, measuring both memorization rates and data emission patterns.

## Limitations

- Inability to construct a complete ground truth of the training data for closed models like ChatGPT, creating uncertainty about true memorization rates
- Divergence attack mechanism relies on undocumented assumptions about ChatGPT's internal handling of repeated tokens and document boundaries
- Scale of experiments (terabytes of data, millions of generations) raises questions about reproducibility without access to similar computational resources

## Confidence

**High confidence**: The claim that larger models leak more training data is well-supported by experimental evidence across 17 diverse models, with clear quantitative relationships showing increasing memorization with model size and training epochs. The effectiveness of the divergence attack on ChatGPT is also well-demonstrated with specific extraction rates (150x improvement over baseline).

**Medium confidence**: The assertion that alignment does not eliminate memorization is supported by comparisons between aligned and unaligned models, but the mechanism by which alignment fails to prevent data extraction is not fully explained. The extrapolation methods used to estimate total memorization from limited samples are methodologically sound but rely on assumptions about the statistical properties of the data that may not hold in all cases.

**Low confidence**: The specific mechanism by which repeated tokens cause ChatGPT to diverge and emit training data is speculative. While the authors provide a plausible explanation involving document boundary tokens, this is not empirically verified and may not capture the true underlying cause of the attack's success.

## Next Checks

1. **Ground truth verification**: Manually annotate a random sample of 1000 generated sequences identified as memorized using web search, and compare against AUXDATASET matches to quantify false positive/negative rates and estimate true memorization rates.

2. **Mechanism validation**: Test whether the divergence attack works on smaller, open-source models that have been fine-tuned with alignment techniques similar to ChatGPT, to determine if the attack exploits a general vulnerability in aligned models or a specific implementation detail of ChatGPT.

3. **Scaling experiment**: Run the extraction attack on a series of models of increasing size (e.g., Pythia 160M, 410M, 1B, 2.8B, 6B, 12B) and verify that the relationship between model size and memorization follows the same pattern as reported, with quantitative measurements of extraction rates at each scale.