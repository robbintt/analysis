---
ver: rpa2
title: ChatGPT and post-test probability
arxiv_id: '2311.12188'
source_url: https://arxiv.org/abs/2311.12188
tags:
- test
- cough
- covid
- probability
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates ChatGPT\u2019s ability to perform formal,\
  \ probabilistic medical diagnostic reasoning. We find that ChatGPT performs well\
  \ on pure probability reasoning but makes more errors when using medical terminology."
---

# ChatGPT and post-test probability

## Quick Facts
- arXiv ID: 2311.12188
- Source URL: https://arxiv.org/abs/2311.12188
- Reference count: 40
- Key outcome: ChatGPT performs well on pure probability reasoning but makes more errors when using medical terminology, suggesting the need for combining language models with formal probabilistic solvers for reliable medical diagnostic reasoning.

## Executive Summary
This work investigates ChatGPT's ability to perform formal, probabilistic medical diagnostic reasoning, specifically computing post-test probabilities while conditioning on both test results and patient symptoms. The study finds that ChatGPT performs well on pure probability reasoning when prompted with abstract variable names, but its performance degrades significantly when medical terminology is introduced. Common errors include ignoring symptom information, assuming test sensitivity is independent of patient covariates, and not properly conditioning pre-test probabilities on symptoms. The authors show that a carefully engineered prompt can reduce some of these errors, but ChatGPT still makes occasional probability mistakes, indicating that combining language models with formal probabilistic solvers may be necessary for reliable medical diagnostic reasoning.

## Method Summary
The study used ChatGPT (version 3.5) via browser interface to evaluate its performance on probabilistic medical diagnostic reasoning tasks. The researchers executed 20 repeated conversations per prompt type, using prompts ranging from pure probability terms (e.g., "posterior probability of A given B and C") to medical terminology (e.g., "post-test probability of Covid given a positive test and cough"). Sensitivity and specificity values were provided in the prompts. For each response, the symbolic expression for P(disease|test,covariate) was identified and classified according to four error types: ignoring symptom information, assuming test sensitivity is independent of patient covariates, not conditioning pre-test probability on symptoms, or miscellaneous probability errors. Error proportions and Wilson confidence intervals were computed to compare results across prompt types.

## Key Results
- ChatGPT performs well on pure probability reasoning with abstract variable names but makes more errors when medical terminology is introduced
- Common errors include ignoring symptom information and assuming test sensitivity is independent of patient covariates
- Prompt engineering can reduce some errors but does not eliminate probability mistakes entirely
- The findings suggest that combining language models with formal probabilistic solvers may be needed for reliable medical diagnostic reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical terminology shifts the latent reasoning space ChatGPT operates in, causing reliance on non-rigorous associations.
- Mechanism: When prompted with terms like "Covid," "sensitivity," or "post-test probability," ChatGPT appears to access a latent space associated with medical diagnostic literature, which emphasizes rule-of-thumb usage over formal probabilistic reasoning. This can cause errors like ignoring symptom information (Error 1) or assuming test sensitivity is independent of covariates (Error 2).
- Core assumption: ChatGPT's internal representations are sensitive to the semantic domain invoked by variable names, not just the underlying mathematical structure.
- Evidence anchors:
  - [abstract] "We show how the introduction of medical variable names leads to an increase in the number of errors that ChatGPT makes."
  - [section 5.2] "The simple change in the names of the variables from A, B, C to 'Covid', 'test', and 'cough' has changed the behavior of ChatGPT, even though the problem it is solving is essentially identical to the one it solved correctly in Section 5.1."
- Break condition: If the prompt uses only abstract probability notation (A, B, C), ChatGPT performs well; introducing medical terms degrades performance.

### Mechanism 2
- Claim: ChatGPT's probabilistic reasoning errors align with known human misconceptions in medical diagnostics.
- Mechanism: ChatGPT inherits tendencies from its training data, which includes human-generated content that may contain statistical misconceptions. For example, the assumption that sensitivity is constant and independent of patient covariates (Error 2) mirrors a common error in medical literature noted by Moons et al. (1997) and Harrell et al. (2001).
- Core assumption: ChatGPT's outputs reflect patterns in its training data, including human errors.
- Evidence anchors:
  - [section 5.3] "It is interesting to note that ChatGPT often considers sensitivity to be unrelated to patient covariates, which is Error 2. This is an error that is commonly made by humans who are using the medical diagnosis framework, which is discussed in Moons et al. (1997); Harrell et al. (2001)."
  - [abstract] "We show how one can use prompt engineering to facilitate ChatGPT's partial avoidance of these errors."
- Break condition: If prompted with explicit instructions to avoid independence assumptions, performance improves but errors persist.

### Mechanism 3
- Claim: Prompt engineering can isolate the probabilistic manipulation step from the medical terminology interpretation step, reducing errors.
- Mechanism: By breaking the problem into explicit steps—identifying disease, test, covariate; writing the Bayes rule expression; expanding the denominator; mapping to sensitivity and pre-test probability—ChatGPT is guided to focus on the symbolic manipulation without conflating it with domain interpretation. This reduces Errors 1-3 significantly.
- Core assumption: ChatGPT can follow multi-step procedural instructions when they are explicitly laid out.
- Evidence anchors:
  - [section 5.4] "The engineered prompt also guards against erroneous independence assumptions. The procedure in Listing 4 is quite general; it could apply to a variety of disease, test, covariate scenarios; note, however, that ChatGPT still makes a probability error when using Listing 4, precluding the use of such a procedure for real diagnostic reasoning."
  - [section 6] "The engineered prompt in Listing 4 breaks the dependence between interpreting the medical terminology and the probabilistic manipulations themselves, and we find that this improves ChatGPT's responses, as shown in Section 5.4."
- Break condition: Even with prompt engineering, ChatGPT can still make probability errors, indicating the need for external symbolic solvers.

## Foundational Learning

- Concept: Bayes' Theorem and conditional probability manipulation
  - Why needed here: The core task is updating beliefs based on test results and symptoms, which requires correct application of Bayes' rule and the law of total probability.
  - Quick check question: Can you write P(A|B,C) in terms of P(B|A,C), P(A|C), and P(B|C)?

- Concept: Sensitivity, specificity, and their proper conditioning
  - Why needed here: Sensitivity must be conditioned on both disease and covariate (e.g., P(test+|disease,cough)), not just disease. Misdefining sensitivity leads to Error 2.
  - Quick check question: If a test's sensitivity depends on symptoms, how would you write it correctly in a Bayes' rule expression?

- Concept: Law of total probability with conditioning
  - Why needed here: The denominator P(test+|cough) must be expanded by conditioning on disease status and properly marginalizing, not just on disease alone.
  - Quick check question: How do you expand P(test+|cough) using the law of total probability while conditioning on covariates?

## Architecture Onboarding

- Component map: User query -> Prompt preprocessor -> ChatGPT model -> Error checker -> Medical context mapper -> Output
- Critical path:
  1. Receive user query with medical variables.
  2. Preprocess into structured steps (identify disease/test/covariate, write Bayes rule, expand denominator, map to sensitivity).
  3. Pass to ChatGPT with explicit instructions.
  4. Parse and validate output for conditioning and independence assumptions.
  5. If errors detected, prompt for correction or use symbolic solver.
- Design tradeoffs:
  - Full automation vs. human-in-the-loop: Full automation risks propagating errors; human-in-the-loop increases reliability but reduces scalability.
  - Prompt complexity vs. robustness: More detailed prompts reduce errors but may be brittle to slight query variations.
  - Real-time performance vs. accuracy: External symbolic solvers increase accuracy but add latency.
- Failure signatures:
  - Output conditions sensitivity on disease only, not covariate → Error 2.
  - Denominator uses P(test+) instead of P(test+|cough) → Error 1 or 4.
  - Prior written as P(disease) instead of P(disease|cough) → Error 3.
- First 3 experiments:
  1. Compare ChatGPT outputs for P(A|B,C) with abstract vs. medical variable names to confirm domain sensitivity.
  2. Test prompt engineering steps on varied disease/test/covariate scenarios to measure error reduction.
  3. Integrate a symbolic solver for Bayes' rule expressions and compare outputs with ChatGPT-only results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does combining language models with formal probabilistic solvers consistently improve performance on medical diagnostic reasoning tasks across different types of diagnostic problems?
- Basis in paper: [explicit] Authors suggest that combining language models like ChatGPT with formal probabilistic solvers might be necessary for reliable medical diagnostic reasoning.
- Why unresolved: The paper only demonstrates this potential approach conceptually and through one specific engineered prompt, without empirically comparing the combined approach to language models alone across multiple diagnostic scenarios.
- What evidence would resolve it: Empirical studies comparing language model performance with and without formal probabilistic solver integration across various diagnostic problems (different diseases, tests, and patient covariates).

### Open Question 2
- Question: What specific aspects of medical terminology cause language models to make probabilistic reasoning errors, and can this be systematically addressed through training or prompt engineering?
- Basis in paper: [explicit] The paper shows that introducing medical variable names (e.g., "Covid," "test," "cough") leads to increased errors compared to abstract probability terms, and that prompt engineering helps reduce some errors.
- Why unresolved: The paper identifies the problem but doesn't systematically investigate which medical terms are most problematic or develop comprehensive strategies to address the issue.
- What evidence would resolve it: Systematic analysis of error rates across different medical terminologies and evaluation of various prompt engineering techniques to mitigate these errors.

### Open Question 3
- Question: How do language models learn and perpetuate statistical errors commonly made by humans in medical diagnostic reasoning, and what are the implications for medical education and AI development?
- Basis in paper: [explicit] The paper notes that ChatGPT's tendency to assume sensitivity is independent of patient covariates mirrors a common human error in medical diagnosis, suggesting language models can learn and propagate human statistical misconceptions.
- Why unresolved: The paper observes this phenomenon but doesn't explore its broader implications or investigate methods to prevent the propagation of such errors.
- What evidence would resolve it: Analysis of language model training data to identify sources of statistical misconceptions, and development of strategies to prevent or correct these learned errors in medical AI systems.

## Limitations
- ChatGPT's performance degrades when medical terminology is introduced, even for tasks it can handle with abstract probability notation
- The engineered prompt reduces but does not eliminate errors, indicating language models alone may be insufficient for reliable medical diagnostic reasoning
- The study only tested ChatGPT 3.5, so results may not generalize to other language models or versions

## Confidence
- High confidence: ChatGPT performs well on pure probability reasoning with abstract variable names (A, B, C). The error-free responses in Section 5.1 are consistent and reproducible.
- Medium confidence: ChatGPT makes systematic errors when medical terminology is introduced, particularly ignoring symptom information (Error 1) and assuming sensitivity is independent of covariates (Error 2). These patterns are observed across multiple trials but may vary with prompt phrasing.
- Low confidence: The complete elimination of errors through prompt engineering. While the engineered prompt significantly reduces errors, ChatGPT still makes occasional probability mistakes, indicating the approach is not fully reliable.

## Next Checks
1. Cross-model validation: Test the same prompts across different large language models (e.g., Claude, Gemini, Llama) to determine if the sensitivity to medical terminology is specific to ChatGPT or a broader LLM phenomenon.
2. Controlled variable experiments: Systematically vary the introduction of medical terms (e.g., disease names, test descriptions) while keeping the underlying probability structure constant to isolate which aspects of medical terminology trigger errors.
3. Hybrid system evaluation: Implement the proposed symbolic solver integration and measure error rates on a larger, more diverse set of medical diagnostic scenarios to assess whether the hybrid approach achieves the reliability needed for clinical applications.