---
ver: rpa2
title: Understanding the robustness difference between stochastic gradient descent
  and adaptive gradient methods
arxiv_id: '2308.06703'
source_url: https://arxiv.org/abs/2308.06703
tags:
- learning
- frequency
- sign
- images
- signgd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines why SGD-trained neural networks are more robust\
  \ to input perturbations than those trained with adaptive gradient methods (Adam,\
  \ RMSProp). It observes that natural datasets contain irrelevant frequencies\u2014\
  high-frequency, low-energy components\u2014whose removal barely affects standard\
  \ accuracy."
---

# Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods

## Quick Facts
- arXiv ID: 2308.06703
- Source URL: https://arxiv.org/abs/2308.06703
- Authors: 
- Reference count: 40
- Key outcome: SGD-trained neural networks are more robust to input perturbations than adaptive gradient methods due to implicit handling of irrelevant frequency components

## Executive Summary
This paper explains why neural networks trained with Stochastic Gradient Descent (SGD) exhibit better robustness to input perturbations than those trained with adaptive gradient methods like Adam and RMSProp. The key insight is that natural datasets contain irrelevant high-frequency components whose removal barely affects standard accuracy, but models trained with adaptive methods are highly sensitive to perturbations along these frequencies. Through synthetic dataset analysis and theoretical bounds, the authors demonstrate that SGD implicitly assigns near-zero weights to irrelevant frequencies, while adaptive methods retain non-zero weights for these components, leading to reduced robustness. This work establishes a fundamental connection between optimization dynamics and model robustness.

## Method Summary
The authors conduct empirical robustness comparisons across seven benchmark datasets (MNIST, FashionMNIST, CIFAR10, CIFAR100, SVHN, Caltech101, Imagenette) using modified PreActResNet18 architectures without batch normalization and two-layer CNNs for smaller datasets. They train models with SGD, Adam, and RMSProp using default PyTorch configurations with extensive learning rate sweeps. Robustness is evaluated under Gaussian noise, ℓ2-bounded, and ℓ∞-bounded adversarial perturbations. Theoretical analysis includes synthetic linear regression with over-parameterization to study learning dynamics, and Lipschitz constant estimation for neural networks to explain the empirical observations.

## Key Results
- SGD-trained models consistently show better robustness to input perturbations across all seven benchmark datasets compared to Adam and RMSProp
- In synthetic linear regression, GD converges to solutions with zero weights on irrelevant frequencies while signGD (a proxy for adaptive methods) retains non-zero weights
- Models trained by SGD have noticeably smaller Lipschitz constants than those trained by Adam and RMSProp, explaining better robustness
- The robustness of linear models to ℓ2-norm bounded changes is inversely proportional to the model parameters' weight norm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD-trained models are more robust because they assign near-zero weights to irrelevant frequency components, while adaptive methods retain non-zero weights for these components
- Mechanism: In over-parameterized settings, multiple solutions achieve zero training risk, but only those with zero weights on irrelevant frequencies are maximally robust. SGD implicitly finds such solutions by not actively adapting weights on low-energy high-frequency components, whereas adaptive methods update these weights based on gradient variance
- Core assumption: The training dataset contains irrelevant frequencies whose removal does not affect standard generalization performance
- Evidence anchors:
  - [abstract] "models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations."
  - [section] "linear models' robustness to ℓ2-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness."
  - [corpus] Weak - no direct mention of frequency irrelevance or weight norm relationships
- Break condition: If the dataset contains no irrelevant frequencies or if all frequency components contribute equally to the target, then the robustness difference would vanish

### Mechanism 2
- Claim: The Lipschitz constant of a neural network is inversely related to its robustness, and SGD-trained models have smaller Lipschitz constants than adaptive method-trained models
- Mechanism: For multi-layer networks, the Lipschitz constant is bounded by the product of layer-wise Lipschitz constants. Smaller weight norms lead to smaller operator norms, reducing the overall Lipschitz constant and thus the sensitivity to input perturbations
- Core assumption: The operator norm of each layer's weight matrix directly contributes to the network's Lipschitz constant
- Evidence anchors:
  - [abstract] "SGD-trained neural networks show smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods."
  - [section] "We show that models trained by SGD have a noticeably larger Lipschitz constant than those trained by Adam and RMSProp."
  - [corpus] Weak - no mention of Lipschitz constants or operator norms in neighboring papers
- Break condition: If batch normalization is used (which affects Lipschitzness) or if network architectures differ significantly, the Lipschitz-based explanation may not hold

### Mechanism 3
- Claim: SignGD, a memory-free version of Adam/RMSProp, behaves similarly to these adaptive methods in allocating weights to irrelevant frequencies, leading to larger weight norms and reduced robustness
- Mechanism: SignGD updates weights using the sign of gradients scaled by coordinate-wise learning rates. Unlike GD, it does not drive irrelevant frequency weights to zero but instead keeps them oscillating around small values, resulting in non-zero final weights that increase the model's sensitivity to perturbations in those directions
- Core assumption: The learning dynamics of signGD approximate those of Adam/RMSProp in the linear regression setting
- Evidence anchors:
  - [section] "We study the learning dynamics of GD and signGD... With a three-dimensional input space, the analysis shows that models optimized with GD exhibit a smaller weight norm compared to their signGD counterparts."
  - [corpus] Weak - no direct mention of signGD or its relationship to Adam/RMSProp dynamics
- Break condition: If the synthetic dataset does not accurately represent natural data structure or if the signGD approximation breaks down for deeper networks, the mechanism may not generalize

## Foundational Learning

- Concept: Discrete Cosine Transform (DCT) and frequency-domain signal representation
  - Why needed here: The paper's analysis relies on decomposing signals into frequency components to identify irrelevant frequencies and study their impact on model robustness
  - Quick check question: Given a 1D signal x = [1, 2, 3, 4], what is the DC component (0-frequency) after applying DCT?

- Concept: Lipschitz continuity and operator norms
  - Why needed here: The paper connects model robustness to Lipschitz constants, which measure the maximum rate of change of the model output with respect to input changes
  - Quick check question: For a linear function f(x) = Wx, what is the Lipschitz constant with respect to the ℓ2 norm?

- Concept: Implicit bias in optimization algorithms
  - Why needed here: The paper argues that different optimization algorithms converge to different solutions with the same training risk but different robustness properties, due to their implicit regularization effects
  - Quick check question: Why might two optimization algorithms with identical training loss minimization criteria produce models with different generalization performance?

## Architecture Onboarding

- Component map: Empirical robustness comparison -> Synthetic linear regression analysis -> Lipschitz constant estimation for neural networks
- Critical path: The core insight flows from observing robustness differences → identifying irrelevant frequencies → analyzing linear model dynamics → generalizing to neural networks via Lipschitz analysis. The linear analysis is the critical bridge between empirical observations and theoretical understanding
- Design tradeoffs: The paper uses signGD as a simplified proxy for adaptive methods, trading analytical tractability for exactness. This choice enables closed-form analysis but may not capture all nuances of Adam/RMSProp behavior. The synthetic dataset design prioritizes analytical clarity over real-world complexity
- Failure signatures: If robustness differences disappear when batch normalization is enabled, or if the linear analysis predictions fail to match neural network behavior, the theoretical framework may be incomplete. Inconsistent results across different datasets or architectures would also indicate fragility
- First 3 experiments:
  1. Replicate the band-limited perturbation analysis on a new dataset to verify that irrelevant frequencies consistently exist and affect adaptive method robustness
  2. Train a linear regression model with the synthetic dataset and measure the final weight norms for GD vs signGD to confirm the analytical predictions
  3. Compute exact Lipschitz constants (not just upper bounds) for a small neural network trained with SGD vs Adam to validate the operator norm relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed robustness difference between SGD and adaptive methods generalize to other optimizers (e.g., SGD with momentum, AMSGrad, or second-order methods) and architectural choices (e.g., batch normalization, skip connections)?
- Basis in paper: [inferred] The paper focuses on SGD, Adam, and RMSProp, but does not systematically vary optimizers or architectures
- Why unresolved: The current analysis is limited to a specific set of optimizers and architectures without batch normalization. The role of other optimization strategies and architectural components in handling irrelevant frequencies remains unexplored
- What evidence would resolve it: Empirical comparisons across a broader range of optimizers and architectures, measuring robustness to irrelevant frequency perturbations, would clarify whether the phenomenon is specific to the studied methods or a more general property of training dynamics

### Open Question 2
- Question: Can we theoretically characterize the frequency-domain bias induced by different optimizers in over-parameterized models, beyond the signGD approximation used in the linear analysis?
- Basis in paper: [explicit] The paper uses signGD as a proxy for adaptive methods in the linear setting but acknowledges limitations in extending this to deep networks
- Why unresolved: The linear analysis provides intuition but does not capture the full dynamics of adaptive methods in deep networks. A rigorous theoretical framework linking optimizer dynamics to frequency-domain biases in high-dimensional settings is lacking
- What evidence would resolve it: A theoretical analysis extending the linear model insights to deep networks, possibly using neural tangent kernels or other high-dimensional approximation techniques, would establish a formal connection between optimization dynamics and frequency-domain behavior

### Open Question 3
- Question: How does the presence of irrelevant frequencies in the training data affect the robustness of models under real-world corruptions (e.g., blur, compression artifacts, sensor noise) that have non-uniform frequency profiles?
- Basis in paper: [inferred] The paper studies Gaussian and adversarial perturbations but does not address realistic corruptions with structured frequency characteristics
- Why unresolved: Real-world data corruptions often have specific frequency signatures that differ from the uniform or adversarial perturbations studied. The impact of irrelevant frequencies on robustness to such structured corruptions is unknown
- What evidence would resolve it: Experiments evaluating model robustness to a diverse set of realistic corruptions (e.g., from the ImageNet-C or COCO-C datasets) and analyzing the correlation between frequency-domain sensitivity and performance under these corruptions would provide insight into practical implications

## Limitations
- Dataset Generalization: The synthetic linear regression analysis assumes a specific signal structure that may not fully capture natural data complexity
- Architecture Dependence: The analysis primarily uses PreActResNet18 and two-layer CNNs, potentially limiting generalizability
- Optimization Hyperparameters: The study sweeps learning rates but uses default configurations for other hyperparameters

## Confidence
- High Confidence: The empirical observation that SGD-trained models are more robust to input perturbations than adaptive method-trained models
- Medium Confidence: The theoretical mechanism linking irrelevant frequencies to robustness differences
- Low Confidence: The claim that signGD accurately approximates Adam/RMSProp behavior in the linear setting

## Next Checks
1. Apply the band-limited perturbation analysis to datasets with known high-frequency importance to test whether the "irrelevant frequency" assumption holds universally
2. Repeat the robustness comparison using architectures with batch normalization or layer normalization enabled to determine if these mechanisms alter the observed robustness patterns
3. Compute exact Lipschitz constants (not upper bounds) for small neural networks trained with SGD vs Adam to verify the operator norm relationship