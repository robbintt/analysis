---
ver: rpa2
title: Attention-Only Transformers and Implementing MLPs with Attention Heads
arxiv_id: '2309.08593'
source_url: https://arxiv.org/abs/2309.08593
tags:
- attention
- heads
- transformer
- matrix
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that attention-only transformers can replicate
  the functionality of MLP layers in standard transformers. The core method shows
  that a masked attention head with internal dimension 1 can implement an MLP neuron
  when using certain activation functions like SiLU or approximations of ReLU and
  GeLU.
---

# Attention-Only Transformers and Implementing MLPs with Attention Heads

## Quick Facts
- arXiv ID: 2309.08593
- Source URL: https://arxiv.org/abs/2309.08593
- Reference count: 6
- Key outcome: Attention-only transformers can replicate MLP functionality by replacing each MLP neuron with a masked attention head

## Executive Summary
This paper establishes a theoretical foundation showing that attention-only transformers can fully replicate the functionality of standard transformers that use both attention and MLP layers. The key insight is that MLP layers, which are sums of masked attention heads, can be decomposed into attention operations. This allows conversion of standard transformers into attention-only architectures by replacing each MLP neuron with an attention head, though this significantly increases the number of attention heads required. The work opens possibilities for applying interpretability techniques developed for attention-only transformers to standard transformer architectures.

## Method Summary
The paper proves theoretical equivalences between MLP components and attention head operations. It shows that an MLP neuron can be implemented by a masked attention head with internal dimension 1 when using generalized SiLU activation functions (including SiLU, ReLU approximations, and GeLU approximations). The method involves specific weight matrix constructions for the attention head parameters (WQK and WOV matrices) and masking patterns. The conversion from MLP-and-attention transformers to attention-only transformers is achieved by replacing each MLP neuron with an attention head, where each MLP layer becomes multiple attention heads. The paper also proves that attention heads can separately implement linear transformations and activation functions, and can encode arbitrary masking patterns with small errors through parameter scaling.

## Key Results
- An MLP neuron can be implemented by a masked attention head with internal dimension 1 using generalized SiLU activation functions
- Attention heads can separately implement linear transformations and activation functions
- Attention heads can encode arbitrary masking patterns in their weight matrices to within arbitrarily small error
- MLP layers can be converted to sums of masked attention heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An MLP neuron can be implemented by a masked attention head with internal dimension 1 using generalized SiLU activation functions
- Mechanism: The masked attention head uses a special bias token that attends only to itself and input tokens. The attention weights approximate the sigmoid part of SiLU, while the attention output computes the linear transformation, recreating the MLP neuron computation
- Core assumption: Activation functions must be generalized SiLU functions of the form a1*SiLU(a2x), including SiLU, ReLU approximations, and GeLU approximations
- Evidence anchors: Theorem 3 shows mathematical construction using specific parameter matrices WQK = a2[0 -V1; 0 0] and WOV = a1*a2*V1*V2 ⊕ [0] with specific masking pattern
- Break condition: Mechanism breaks if activation function is not in restricted class of generalized SiLU functions, or if attention heads cannot properly implement required masking patterns

### Mechanism 2
- Claim: Attention heads can separately implement linear transformations and activation functions
- Mechanism: Theorem 7 shows identity masking (Λ = IN) allows row-wise linear transformations by outputting X*WOV. Theorem 8 shows D attention heads can implement entry-wise activation functions by combining MLP-to-attention conversion with linear transformation capability
- Core assumption: Attention heads can be configured to perform these specific operations without interference
- Evidence anchors: Theorems 7 and 8 provide mathematical proofs for these separate implementations
- Break condition: Mechanism breaks if attention heads cannot be configured for pure linear transformations or if activation functions cannot be properly decomposed into attention operations

### Mechanism 3
- Claim: Attention heads can encode arbitrary masking patterns in weight matrices to within arbitrarily small error
- Mechanism: Theorem 9 shows that by augmenting residual stream with identity matrix and using large values in WQK matrix, attention heads can approximate any desired masking pattern while using standard masking pattern internally
- Core assumption: WQK matrix can be made sufficiently large to overcome limitations of internal masking pattern
- Evidence anchors: Theorem 9 provides mathematical construction and proof of convergence as parameter Ω approaches infinity
- Break condition: Mechanism breaks if required Ω values become computationally infeasible or if approximation error becomes too large for practical applications

## Foundational Learning

- Concept: Matrix operations and linear algebra (matrix multiplication, block matrices, element-wise operations)
  - Why needed here: Paper relies heavily on matrix operations to describe transformer components and their equivalences
  - Quick check question: Given matrices A ∈ Mn,k and B ∈ Mk,m, what is the dimension of the product AB?

- Concept: Activation functions and their properties (ReLU, SiLU, GeLU)
  - Why needed here: Paper proves equivalences for specific activation functions and requires understanding their mathematical properties
  - Quick check question: What is the derivative of the SiLU function with respect to its input?

- Concept: Softmax and attention mechanisms
  - Why needed here: Attention heads are core component being analyzed, understanding masked softmax is crucial
  - Quick check question: How does masked softmax function differ from standard softmax function?

## Architecture Onboarding

- Component map: Transformer architecture consists of alternating attention and MLP sublayers. Each MLP neuron can be replaced by an attention head with internal dimension 1, and each MLP layer can be replaced by multiple attention heads. Overall architecture becomes attention-only transformer with additional bias tokens
- Critical path: Key transformation occurs in Theorem 3, which shows how to implement an MLP neuron with an attention head. This is extended in Theorem 6 to convert entire transformer architectures
- Design tradeoffs: Converting MLPs to attention heads increases number of attention heads significantly (potentially by orders of magnitude) but enables application of interpretability techniques developed for attention-only transformers. Computational cost tradeoffs exist between matrix multiplication and vector operations
- Failure signatures: If activation function is not in restricted class of generalized SiLU functions, conversion fails. If masking patterns cannot be properly encoded, attention heads may not compute correct values. If Ω parameter in Theorem 9 becomes too large, training may become unstable
- First 3 experiments:
  1. Implement Theorem 3 by converting simple MLP layer with SiLU activation into attention heads and verify equivalence on small test matrices
  2. Test Theorem 6 by converting simple transformer with one attention and one MLP layer into attention-only transformer and comparing outputs
  3. Experiment with Theorem 9 by implementing masking pattern encoding and measuring error as Ω increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would an attention-only transformer converted from a standard transformer architecture be competitive with the original in terms of practical considerations like training or inference speed?
- Basis in paper: The paper discusses that converting MLP layers to attention heads increases the number of attention heads by several orders of magnitude and may slow down model inference or training
- Why unresolved: Paper presents theoretical results about possibility of such conversion but does not provide empirical evidence or benchmarks comparing performance of converted attention-only transformers with their original counterparts
- What evidence would resolve it: Experimental comparison of training and inference speed, as well as model performance, between original MLP-and-attention transformers and their converted attention-only counterparts

### Open Question 2
- Question: How does the scaling of Ω in the pseudo-masking technique affect the training dynamics and convergence of the transformer?
- Basis in paper: Paper mentions that large Ω terms added to WQK matrix in pseudo-masking technique would interact poorly with most forms of dropout regularization and with ℓ2 regularization on entries of WQK
- Why unresolved: Paper provides theoretical framework for pseudo-masking technique but does not explore its practical implications on training dynamics and convergence
- What evidence would resolve it: Empirical studies on training dynamics, convergence behavior, and generalization performance of transformers using pseudo-masking technique with different values of Ω

### Open Question 3
- Question: Can the interpretability techniques developed for attention-only transformers be directly applied to the MLP layers in standard transformers after converting them to attention heads?
- Basis in paper: Paper suggests that conversion of MLP layers to attention heads could allow application of interpretability techniques that work on attention-only transformers to standard transformer architectures
- Why unresolved: While paper provides theoretical basis for converting MLP layers to attention heads, it does not investigate whether interpretability techniques developed for attention-only transformers can be directly applied to these converted layers
- What evidence would resolve it: Application of interpretability techniques to converted attention heads in standard transformers and comparison of their effectiveness with those applied to original attention-only transformers

## Limitations

- Implementation complexity: Converting MLP layers to attention heads increases the number of attention heads by orders of magnitude, creating significant computational challenges
- Activation function restrictions: Mechanism relies on activation functions being in restricted class of generalized SiLU functions, excluding many commonly used activation functions
- Numerical stability: Theorem 9 requires Ω to approach infinity for perfect convergence, introducing approximation errors with finite values

## Confidence

- High Confidence: Theoretical proofs establishing that attention heads can implement MLP neurons and separate components (linear transformations and activation functions) are mathematically rigorous
- Medium Confidence: Claim that attention-only transformers can fully replace MLP-and-attention transformers in practice, given significant increase in attention heads required
- Low Confidence: Practical implications for transformer interpretability, given that increased number of attention heads may actually complicate rather than simplify interpretability analysis

## Next Checks

1. **Numerical Verification of Theorem 3**: Implement specific matrix constructions from Theorem 3 using actual numerical values and verify that attention head computation matches MLP neuron output for various inputs and activation functions (SiLU, ReLU approximations, GeLU approximations)

2. **Error Analysis for Theorem 9**: Systematically measure approximation error when implementing arbitrary masking patterns with finite Ω values. Determine relationship between Ω, error magnitude, and computational cost to establish practical bounds

3. **Performance Benchmarking**: Convert a small MLP-and-attention transformer (e.g., 1-2 layers) to an attention-only transformer using theoretical framework, then benchmark both models on simple task (like synthetic sequence modeling) to verify functional equivalence while measuring computational overhead