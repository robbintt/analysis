---
ver: rpa2
title: 'MoCo-Transfer: Investigating out-of-distribution contrastive learning for
  limited-data domains'
arxiv_id: '2311.09401'
source_url: https://arxiv.org/abs/2311.09401
tags:
- data
- moco
- chest
- learning
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether contrastive self-supervised pretraining
  on unlabeled out-of-distribution medical imaging data can improve performance in
  limited-data domains. The authors compare MoCo pretraining on spine X-rays, chest
  X-rays, and ImageNet, followed by linear or end-to-end finetuning on spine and chest
  X-ray datasets with varying amounts of labeled data.
---

# MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains

## Quick Facts
- arXiv ID: 2311.09401
- Source URL: https://arxiv.org/abs/2311.09401
- Reference count: 40
- Primary result: MoCo pretraining on larger out-of-distribution datasets (e.g., chest X-rays for spine classification) can match or outperform in-domain pretraining, especially with limited labeled data.

## Executive Summary
This paper investigates whether contrastive self-supervised pretraining on unlabeled out-of-distribution medical imaging data can improve performance in limited-data domains. The authors compare MoCo pretraining on spine X-rays, chest X-rays, and ImageNet, followed by linear or end-to-end finetuning on spine and chest X-ray datasets with varying amounts of labeled data. They find that MoCo pretraining on larger out-of-distribution datasets can match or outperform in-domain pretraining, especially with limited labeled data. Linear finetuning often outperforms end-to-end finetuning in low-data regimes. Additionally, they propose a preliminary method to quantify dataset similarity using SVCCA, finding that chest and spine X-rays are more similar to each other than to ImageNet, consistent with observed transfer benefits.

## Method Summary
The study uses DenseNet-121 architecture initialized with ImageNet weights, followed by MoCo pretraining on different datasets (VinDr-SpineXR, MIMIC-CXR, or ImageNet). MoCo pretraining uses InfoNCE loss with SGD optimizer (weight decay=1e-4, momentum=0.9, cosine annealing). After pretraining, the model is finetuned using either linear finetuning (freezing all but last layer) or end-to-end finetuning on varying percentages of labeled data (1%, 10%, 100% for spine; 0.05%, 0.1%, 1%, 10% for chest). Evaluation uses binary cross-entropy loss and weighted AUROC for MIMIC-CXR.

## Key Results
- MoCo pretraining on larger out-of-distribution datasets (e.g., chest X-rays for spine classification) can match or outperform in-domain pretraining, especially with limited labeled data.
- Linear finetuning often outperforms end-to-end finetuning in low-data regimes.
- Chest and spine X-rays are more similar to each other than to ImageNet according to SVCCA similarity scores, consistent with observed transfer benefits.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoCo pretraining on out-of-distribution data can transfer useful representations even when in-domain data is limited.
- Mechanism: Contrastive self-supervised learning creates invariant representations by pulling together augmented views of the same image and pushing apart different images. These representations capture domain-agnostic features that generalize across related medical imaging tasks.
- Core assumption: The out-of-distribution dataset shares enough structural similarity with the target domain for useful features to transfer.
- Evidence anchors:
  - [abstract] "we find that depending on quantity of labeled and unlabeled data, contrastive pretraining on larger out-of-distribution datasets can perform nearly as well or better than MoCo pretraining in-domain"
  - [section 4] "MoCo pretraining on a dataset, and finetunes and evaluates on the dataset of interest"
  - [corpus] Weak - the corpus papers don't directly address cross-domain transfer from larger out-of-distribution datasets, though they mention foundation models and contrastive learning
- Break condition: If the out-of-distribution dataset is too dissimilar (e.g., natural images vs. specialized medical modalities), the transferred representations may not capture relevant features and could even hurt performance.

### Mechanism 2
- Claim: Linear finetuning preserves more of the MoCo representation compared to end-to-end finetuning, leading to better performance with limited labeled data.
- Mechanism: When labeled data is scarce, freezing all but the last layer prevents overfitting and maintains the useful features learned during MoCo pretraining. The model can only adapt the final classification layer to the new task without corrupting the learned representations.
- Core assumption: The last layer is sufficient to adapt the frozen representations to the downstream task when data is limited.
- Evidence anchors:
  - [abstract] "linear finetuning often outperforms end-to-end finetuning in low-data regimes"
  - [section 4] "linear finetuning...where all but the last layer of weights are frozen"
  - [section 5] "in settings with limited labeled data...linear finetuning often outperforms end-to-end finetuning"
- Break condition: If the labeled data becomes sufficient or abundant, end-to-end finetuning may allow the model to better adapt all layers to the specific task, potentially outperforming linear finetuning.

### Mechanism 3
- Claim: Dataset similarity can be quantified using SVCCA to predict transfer learning benefits.
- Mechanism: SVCCA measures the alignment between neural activations from different datasets. Higher similarity scores indicate that datasets share common features in their representations, suggesting that pretraining on one could benefit the other.
- Core assumption: SVCCA similarity between datasets correlates with the practical benefit of transfer learning between them.
- Evidence anchors:
  - [abstract] "we propose a preliminary method to quantify dataset similarity using SVCCA, finding that chest and spine X-rays are more similar to each other than to ImageNet, consistent with observed transfer benefits"
  - [section 4] "VinDR-SpineXR, MIMIC-CXR, and ImageNet datasets are compared pairwise by performing SVCCA on the activations"
  - [section 5] "VinDR-CXR and MIMIC-CXR obtain higher a CCA similarity score than ImageNet with either of the X-ray datasets"
- Break condition: If SVCCA similarity doesn't correlate with actual transfer learning performance in other dataset pairs, the method may not be reliable for predicting transferability.

## Foundational Learning

- Concept: Contrastive self-supervised learning (MoCo)
  - Why needed here: Understanding how MoCo learns representations without labels is crucial for grasping why out-of-distribution pretraining can help limited-data domains
  - Quick check question: What is the InfoNCE loss trying to achieve in MoCo pretraining?

- Concept: Transfer learning and domain adaptation
  - Why needed here: The paper's core contribution relies on understanding when and how representations transfer between related but distinct domains
  - Quick check question: Why might pretraining on chest X-rays help with spine X-ray classification but ImageNet might not?

- Concept: Dataset similarity metrics and their interpretation
  - Why needed here: The SVCCA method is proposed to quantify relatedness between datasets, which is key to understanding which domains might benefit from transfer
  - Quick check question: What does a higher SVCCA similarity score between two datasets indicate about their representational space?

## Architecture Onboarding

- Component map: DenseNet-121 -> MoCo pretraining module -> Finetuning module (linear/end-to-end) -> Evaluation pipeline
- Critical path: 1. Initialize with ImageNet weights 2. MoCo pretrain on either in-domain or out-of-distribution data 3. Finetune using either linear or end-to-end approach 4. Evaluate on test set 5. Optionally compute SVCCA similarity between datasets
- Design tradeoffs:
  - Linear vs. end-to-end finetuning: Preservation of representations vs. flexibility to adapt
  - Amount of unlabeled data for MoCo: More data may improve representations but increases computational cost
  - Choice of out-of-distribution dataset: Larger datasets may provide better representations but could be less related
- Failure signatures:
  - Poor performance despite MoCo pretraining: Likely indicates insufficient similarity between pretraining and target domains
  - Overfitting with end-to-end finetuning on small labeled datasets: Suggests need for linear finetuning or more data
  - No correlation between SVCCA similarity and transfer performance: Indicates the similarity metric may not be reliable for this application
- First 3 experiments:
  1. Run MoCo pretraining on MIMIC-CXR with different amounts of unlabeled data, then linear finetune on VinDr-SpineXR with 1% labeled data
  2. Compare linear vs. end-to-end finetuning on MIMIC-CXR with 0.1% labeled data using MoCo pretraining on the same dataset
  3. Compute SVCCA similarity between VinDr-SpineXR and MIMIC-CXR across different DenseNet layers and verify correlation with transfer performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MoCo pretraining on out-of-distribution data compare to supervised pretraining on ImageNet for medical image classification tasks?
- Basis in paper: [inferred] The paper compares MoCo pretraining on chest X-rays and spine X-rays to ImageNet pretraining, finding that MoCo pretraining on larger out-of-distribution datasets can perform nearly as well or better than MoCo pretraining in-domain. However, it does not directly compare MoCo pretraining to supervised ImageNet pretraining.
- Why unresolved: The paper focuses on comparing different MoCo pretraining strategies and does not include a direct comparison with supervised pretraining on ImageNet.
- What evidence would resolve it: Experiments comparing the performance of MoCo pretraining on out-of-distribution data to supervised pretraining on ImageNet for various medical image classification tasks, using the same model architectures and evaluation metrics.

### Open Question 2
- Question: How does the similarity between datasets, as quantified by SVCCA, relate to the performance of transfer learning between those datasets?
- Basis in paper: [explicit] The paper proposes using SVCCA to quantify dataset similarity and finds that chest and spine X-rays are more similar to each other than to ImageNet, consistent with observed transfer benefits.
- Why unresolved: The paper provides a preliminary analysis of dataset similarity using SVCCA, but does not establish a clear relationship between the SVCCA similarity score and the performance of transfer learning between datasets.
- What evidence would resolve it: A comprehensive study examining the relationship between SVCCA similarity scores and transfer learning performance across a wide range of dataset pairs, using various model architectures and transfer learning strategies.

### Open Question 3
- Question: How does the choice of linear versus end-to-end finetuning affect the performance of models in low-data regimes for different medical imaging tasks?
- Basis in paper: [explicit] The paper finds that linear finetuning often outperforms end-to-end finetuning in low-data regimes for both spine and chest X-ray classification tasks.
- Why unresolved: The paper only investigates linear and end-to-end finetuning for two specific medical imaging tasks. The generalizability of these findings to other medical imaging tasks and domains remains unclear.
- What evidence would resolve it: Experiments comparing linear and end-to-end finetuning for a diverse set of medical imaging tasks and domains, using various model architectures and evaluation metrics, to determine the conditions under which each finetuning strategy is most effective.

## Limitations
- Experiments limited to X-ray imagery, limiting generalizability to other medical imaging modalities
- DenseNet-121 architecture may not capture the full potential of transfer learning compared to larger architectures
- SVCCA similarity metric's predictive power needs broader validation across more dataset pairs

## Confidence
- Out-of-distribution MoCo pretraining performance: High
- SVCCA similarity metric reliability: Medium

## Next Checks
1. Test SVCCA similarity predictions on additional medical imaging dataset pairs (e.g., CT scans, MRI) to verify the metric's broader applicability for transfer learning.
2. Evaluate the impact of different MoCo pretraining dataset sizes on transfer performance to establish optimal data scaling relationships.
3. Compare linear and end-to-end finetuning across a wider range of labeled data percentages (particularly in the 10-50% range) to better characterize the transition point where end-to-end finetuning becomes advantageous.