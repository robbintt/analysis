---
ver: rpa2
title: Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive
  Losses for Medical Visual Question Answering
arxiv_id: '2307.05314'
source_url: https://arxiv.org/abs/2307.05314
tags:
- medical
- image
- text
- visual
- masked
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUMC, a self-supervised pre-training approach
  for medical visual question answering that leverages unimodal and multimodal contrastive
  losses along with masked image and text modeling. The method learns feature representations
  from medical image caption datasets and transfers them to downstream VQA tasks.
---

# Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering

## Quick Facts
- arXiv ID: 2307.05314
- Source URL: https://arxiv.org/abs/2307.05314
- Reference count: 28
- Key outcome: State-of-the-art medical VQA performance with 2.2%, 14.7%, and 1.7% accuracy improvements on three datasets

## Executive Summary
This paper introduces MUMC, a self-supervised pre-training approach for medical visual question answering that leverages unimodal and multimodal contrastive losses along with masked image and text modeling. The method learns feature representations from medical image caption datasets and transfers them to downstream VQA tasks. By using both image-text matching and masked language modeling, along with contrastive losses that align unimodal and multimodal features, MUMC achieves state-of-the-art performance on three medical VQA datasets.

## Method Summary
MUMC employs a transformer-based architecture with 12-layer ViT image encoder, 6-layer BERT text encoder, and 6-layer multimodal encoder, plus a decoder for fine-tuning. The pre-training uses four objectives: Masked Language Modeling (MLM), Image-Text Matching (ITM), Unimodal Contrastive Loss (UCL), and Multimodal Contrastive Loss (MCL). Images are partitioned into 16×16 patches with 25% random masking as data augmentation. The pre-trained model is transferred to VQA tasks by reusing the encoders and adding an answering decoder, fine-tuned for 30 epochs on downstream datasets.

## Key Results
- Achieves state-of-the-art performance on VQA-RAD, PathVQA, and SLAKE datasets
- Shows significant accuracy improvements of 2.2%, 14.7%, and 1.7% respectively
- Validates masked images as effective data augmentation for learning robust medical representations

## Why This Works (Mechanism)

### Mechanism 1
MUMC improves medical VQA performance by learning aligned unimodal and multimodal representations via contrastive losses. The approach uses UCL to align different views of the same modality and MCL to align image and text features, improving feature discriminability and cross-modal alignment for better downstream accuracy. Core assumption: Contrastive learning on masked/unmasked views and image-text pairs captures semantic similarity and improves generalization. Break condition: If contrastive losses overfit to pretraining caption data or fail to generalize to VQA questions requiring reasoning beyond caption semantics.

### Mechanism 2
Masked image strategy serves as effective data augmentation that improves model robustness. Randomly masking 25% of image patches during pretraining forces the model to learn to infer missing visual context, improving robustness to partial occlusions or incomplete inputs during fine-tuning. Core assumption: Partial masking simulates realistic missing information in medical images and the model can recover semantics from unmasked patches. Break condition: If masking probability is too high (>50%) and the model cannot recover sufficient context, leading to degraded pretraining representations.

### Mechanism 3
Transfer learning from multimodal pretraining to VQA tasks is effective due to shared encoder architecture. The same image encoder, text encoder, and multimodal encoder used in pretraining are reused in fine-tuning with added answering decoder, preserving learned feature representations while adapting to the VQA task. Core assumption: The multimodal encoder trained on image-text pairs can generalize to question-image pairs where text is a question, not a caption. Break condition: If the shift from captions to questions introduces domain shift that the encoder cannot handle, leading to poor question-image alignment.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To learn discriminative unimodal and multimodal representations that generalize across medical image-text pairs
  - Quick check question: What is the difference between UCL and MCL in MUMC, and why are both needed?

- Concept: Masked language modeling (MLM)
  - Why needed here: To learn contextual language representations that condition on both text and visual inputs
  - Quick check question: How does MLM in MUMC differ from standard BERT MLM?

- Concept: Vision Transformer (ViT) architecture
  - Why needed here: To extract patch-level visual features that can be masked and aligned with text
  - Quick check question: Why does MUMC partition images into 16×16 patches before masking?

## Architecture Onboarding

- Component map:
  Image encoder (12-layer ViT) -> Text encoder (6-layer BERT) -> Multimodal encoder (6-layer BERT with cross-attention) -> Answering decoder (6-layer transformer)

- Critical path:
  Pretraining → Image-text pair encoding → Contrastive loss computation → Multimodal fusion → Fine-tuning on VQA

- Design tradeoffs:
  - ViT vs CNN: ViT enables patch masking but increases compute
  - Momentum vs online: Momentum models stabilize contrastive learning but add memory overhead
  - Masking rate: 25% balances augmentation and information retention

- Failure signatures:
  - Pretraining loss plateaus early: Possible overfitting or insufficient negative samples
  - VQA accuracy improves slowly: Possible domain shift or poor question-image alignment
  - Memory overflow: Large batch size or high-resolution images

- First 3 experiments:
  1. Ablation of UCL vs MCL to measure individual contribution to VQA accuracy
  2. Vary masking probability (0%, 25%, 50%, 75%) to find optimal augmentation rate
  3. Replace ViT with CNN backbone to test impact on representation quality and compute

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MUMC change when trained on larger medical image caption datasets, such as those containing millions of image-text pairs?
- Basis in paper: The paper mentions using ROCO, MedICaT, and ImageCLEF2022 datasets, but does not explore the impact of dataset size on performance
- Why unresolved: The paper does not provide experiments or analysis on the relationship between dataset size and model performance
- What evidence would resolve it: Experiments comparing MUMC's performance on varying sizes of medical image caption datasets would provide insights into the scalability and effectiveness of the approach

### Open Question 2
- Question: Can the MUMC approach be extended to handle more diverse types of medical images, such as X-rays, CT scans, and pathology slides, and how would the performance vary across these modalities?
- Basis in paper: The paper focuses on radiology images and does not explore the generalizability of the approach to other medical imaging modalities
- Why unresolved: The paper does not provide experiments or analysis on the performance of MUMC across different types of medical images
- What evidence would resolve it: Experiments comparing MUMC's performance on various medical imaging modalities would provide insights into the approach's versatility and potential limitations

### Open Question 3
- Question: How does the masking probability affect the model's ability to learn robust representations for rare or underrepresented medical conditions in the training data?
- Basis in paper: The paper mentions that a 25% masking probability yields the best results but does not explore its impact on learning representations for rare conditions
- Why unresolved: The paper does not provide analysis on how different masking probabilities affect the model's ability to generalize to rare or underrepresented medical conditions
- What evidence would resolve it: Experiments comparing MUMC's performance on rare medical conditions with varying masking probabilities would provide insights into the optimal masking strategy for handling data imbalance

## Limitations
- Contrastive losses (UCL and MCL) effectiveness not explicitly validated through ablation studies
- Transfer assumption from image captions to medical questions lacks direct empirical support
- Critical hyperparameters for contrastive learning not fully specified, affecting reproducibility

## Confidence

- Mechanism 1 (Contrastive losses improve alignment): Medium - supported by design rationale but lacks ablation evidence
- Mechanism 2 (Masked images improve robustness): Low - stated as validated but no empirical comparison provided
- Mechanism 3 (Transfer learning effectiveness): Medium - standard practice but domain shift not explicitly tested

## Next Checks
1. Perform controlled ablation experiments removing UCL and MCL individually to quantify their independent contributions to VQA accuracy improvements
2. Test masking probabilities at 0%, 15%, 25%, 50%, and 75% during pretraining to empirically determine optimal augmentation rate for medical images
3. Evaluate feature similarity between caption-pretrained and question-finetuned representations using cross-modal retrieval metrics to quantify domain shift effects