---
ver: rpa2
title: 'Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series
  Forecasting'
arxiv_id: '2307.00493'
source_url: https://arxiv.org/abs/2307.00493
tags:
- attention
- fwin
- window
- fourier
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to accelerate Informer for long sequence
  time-series forecasting by replacing its ProbSparse attention with a local-global
  window-based attention approach. The proposed Fourier-Mixed Window Attention (FWin)
  uses a window attention layer to capture local information, followed by a Fourier
  transform layer to achieve global information mixing.
---

# Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting

## Quick Facts
- **arXiv ID**: 2307.00493
- **Source URL**: https://arxiv.org/abs/2307.00493
- **Reference count**: 40
- **Primary result**: Replaces ProbSparse attention with window attention + Fourier mixing to achieve 40-50% speedup while improving accuracy on long sequence time-series forecasting

## Executive Summary
This paper presents a method to accelerate Informer for long sequence time-series forecasting by replacing its ProbSparse attention with a local-global window-based attention approach. The proposed Fourier-Mixed Window Attention (FWin) uses a window attention layer to capture local information, followed by a Fourier transform layer to achieve global information mixing. This method does not rely on query sparsity assumptions or empirical approximations used in Informer's ProbSparse attention. Experiments on univariate and multivariate datasets show that FWin improves prediction accuracy while accelerating inference speeds by 40-50% compared to Informer. A lightweight version, FWin-, without the Fourier mixing layer in the decoder, also demonstrates competitive performance with faster inference. Theoretical analysis and experiments in a nonlinear regression model show that FWin attention can match or exceed the performance of full softmax attention, providing further support for its effectiveness.

## Method Summary
The method replaces Informer's ProbSparse attention with a two-stage approach: window attention followed by Fourier mixing. Window attention restricts query-key interactions to local windows of size W, reducing complexity from O(L²) to O(LW). The Fourier transform then mixes tokens globally by projecting the windowed attention output into the frequency domain, spreading information across all tokens. This combination captures both local high-frequency fluctuations and global low-frequency trends. The architecture consists of an encoder with repeated Window Attention → Distillation → Fourier Mix layers, and a decoder with Masked Window Self-Attention → Fourier Mix → Window Cross-Attention → FC Layer. A lightweight variant FWin- removes the Fourier mixing layer from the decoder.

## Key Results
- Achieves 40-50% faster inference speed compared to Informer on benchmark datasets
- Improves prediction accuracy (MAE/MSE) on ETTh1, ETTh2, ETTm1, Weather, and ECL datasets
- FWin- variant provides competitive performance with even faster inference by removing decoder Fourier mixing
- Theoretical analysis shows FWin can match or exceed full softmax attention performance under certain conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fourier mixing compensates for the lack of global token interactions in window attention
- Mechanism: Window attention limits query-key interactions to local windows, capturing high-frequency local fluctuations. Fourier transform then mixes tokens globally by projecting the windowed attention output into frequency domain, spreading information across all tokens. This provides the low-frequency global trend information that window attention alone cannot capture.
- Core assumption: Fourier transform effectively approximates the global mixing function of full attention without quadratic complexity
- Evidence anchors: [abstract], [section 4.2.1], [corpus]

### Mechanism 2
- Claim: Window attention reduces computational complexity from O(L²) to O(LW)
- Mechanism: By dividing the sequence into non-overlapping windows of size W and computing attention only within each window, the number of query-key interactions drops from L² to L×W. This linear complexity makes long sequence processing feasible.
- Core assumption: The assumption that most useful attention information is local, making window attention a reasonable approximation
- Evidence anchors: [section 3.2], [section 4.3], [corpus]

### Mechanism 3
- Claim: FWin attention can match or exceed full softmax attention performance through learned compensation
- Mechanism: The paper shows empirically that FWin achieves better or comparable accuracy to full attention while being faster. The learned linear layer after Fourier mixing (in the nonlinear regression model) further adapts the frequency-domain representation to better approximate full attention's output.
- Core assumption: The combination of window attention, Fourier mixing, and learned compensation can effectively approximate full attention's information mixing capability
- Evidence anchors: [abstract], [section 6.2], [corpus]

## Foundational Learning

- Concept: Fourier Transform for signal mixing
  - Why needed here: Understanding how Fourier transform can mix tokens globally without learning parameters is crucial for grasping why FWin works
  - Quick check question: Why does applying Fourier transform along both model and time dimensions help spread information globally?

- Concept: Attention mechanism fundamentals
  - Why needed here: To understand the difference between full, window, and sparse attention mechanisms and their computational trade-offs
  - Quick check question: What is the computational complexity of full attention and why does it become prohibitive for long sequences?

- Concept: Local vs global information in sequences
  - Why needed here: To appreciate why combining local window attention with global Fourier mixing is beneficial for time series forecasting
  - Quick check question: In time series data, what types of patterns are typically local versus global?

## Architecture Onboarding

- Component map: Input → Embedding → Encoder Layers (Window Attention → Distillation → Fourier Mix) → Decoder Input → Decoder Layers (Masked Window Self-Attention → Fourier Mix → Window Cross-Attention → FC Layer) → Output
- Critical path: Input → Embedding → Encoder Layers → Decoder Input → Decoder Layers → Output
- Design tradeoffs: FWin trades some potential accuracy (compared to full attention) for significant speed gains. The Fourier Mix layer adds global context but may not capture all dependencies as well as full attention.
- Failure signatures: Poor performance on data with strong long-range dependencies, unexpected accuracy drops when window size is too small, memory issues if window size is too large
- First 3 experiments:
  1. Compare FWin with Informer on ETTh1 dataset with prediction length 24 to verify speed and accuracy claims
  2. Remove Fourier Mix layer from decoder (create FWin-) and compare performance to understand its contribution
  3. Vary window size (e.g., 12, 24, 48) and observe impact on both accuracy and inference speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Fourier-Mixed Window Attention perform on datasets with highly non-stationary time series compared to stationary ones?
- Basis in paper: [inferred] The paper discusses the model's effectiveness on various datasets, but does not specifically address performance on non-stationary time series.
- Why unresolved: The experiments conducted in the paper focus on benchmark datasets without explicitly analyzing the model's behavior on non-stationary data.
- What evidence would resolve it: Comparative experiments on both stationary and non-stationary datasets to evaluate the model's adaptability and performance.

### Open Question 2
- Question: What are the theoretical conditions under which the Fourier-Mixed Window Attention can outperform the full softmax attention in all scenarios?
- Basis in paper: [explicit] The paper provides a mathematical definition and proves that the Fourier-Mixed Window Attention is equivalent to full attention under the Block Diagonal Invertibility (BDI) condition.
- Why unresolved: While the paper establishes the conditions for equivalence, it does not explore scenarios where the Fourier-Mixed Window Attention consistently outperforms full softmax attention.
- What evidence would resolve it: Theoretical analysis and empirical studies identifying specific scenarios and conditions where the Fourier-Mixed Window Attention is superior.

### Open Question 3
- Question: How does the performance of the Fourier-Mixed Window Attention change with different window sizes, and what is the optimal window size for various types of time series data?
- Basis in paper: [inferred] The paper uses a default window size of 24 but does not explore the impact of varying window sizes on model performance.
- Why unresolved: The choice of window size is crucial for capturing local information, and its impact on the model's effectiveness is not thoroughly investigated.
- What evidence would resolve it: Experiments varying the window size across different datasets to determine the optimal size for capturing local and global information effectively.

## Limitations

- Limited ablation studies for critical design choices like window size, Fourier mixing frequency, and layer configurations
- Complexity analysis gaps - no comprehensive runtime benchmarks across varying sequence lengths and window sizes
- Theoretical assumptions about query sparsity may not hold for all real-world time series data

## Confidence

**High confidence**: The core architectural contribution of combining window attention with Fourier mixing is clearly specified and the general mechanism (local + global information fusion) is sound.

**Medium confidence**: The empirical performance claims (40-50% speedup, improved accuracy) are supported by experiments on four datasets, but limited ablation studies reduce confidence in generalizability.

**Low confidence**: The claim that FWin can match or exceed full softmax attention performance is based on limited experiments in a controlled setting without extensive comparisons across diverse datasets.

## Next Checks

1. **Window size sensitivity analysis**: Systematically vary the window size (e.g., 12, 24, 48, 96) and measure both accuracy and inference speed on ETTh1 dataset to validate the claimed 40-50% speedup holds across configurations.

2. **Long sequence scalability test**: Evaluate FWin and Informer on sequences of length 10,000+ with varying prediction horizons (24, 48, 96) to verify computational advantages scale to the "long sequence" regime.

3. **Fourier mixing ablation**: Create variants of FWin with different Fourier transform strategies (only time dimension, only model dimension, both dimensions with different frequencies) to validate whether the specific approach is critical to success.