---
ver: rpa2
title: An Empirical Evaluation of Prompting Strategies for Large Language Models in
  Zero-Shot Clinical Natural Language Processing
arxiv_id: '2309.08008'
source_url: https://arxiv.org/abs/2309.08008
tags:
- clinical
- prompt
- prompts
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates prompt engineering techniques
  for five clinical NLP tasks using three large language models (GPT-3.5, BARD, and
  LLAMA2) in zero-shot and few-shot settings. The authors test seven prompt types
  including novel heuristic and ensemble approaches, finding that task-specific prompt
  tailoring is critical for accuracy.
---

# An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing

## Quick Facts
- arXiv ID: 2309.08008
- Source URL: https://arxiv.org/abs/2309.08008
- Reference count: 0
- One-line primary result: Task-specific prompt tailoring is critical for accuracy in clinical NLP tasks, with heuristic and chain-of-thought prompts generally achieving highest performance

## Executive Summary
This study systematically evaluates prompt engineering techniques for five clinical NLP tasks using three large language models (GPT-3.5, BARD, and LLAMA2) in zero-shot and few-shot settings. The authors test seven prompt types including novel heuristic and ensemble approaches, finding that task-specific prompt tailoring is critical for accuracy. Heuristic and chain-of-thought prompts generally achieve highest performance, with GPT-3.5 showing superior adaptability across tasks. Few-shot prompting consistently improves accuracy by providing contextual examples, while ensemble approaches leverage complementary strengths of multiple prompt types. The research demonstrates that large language models can effectively perform clinical NLP tasks without fine-tuning through appropriate prompt engineering, offering a cost-effective and scalable solution for clinical information extraction where labeled data is scarce.

## Method Summary
The study systematically evaluates seven prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, ensemble, and few-shot) across three large language models (GPT-3.5, BARD, and LLAMA2) for five clinical NLP tasks. The evaluation uses accuracy as the sole metric and compares zero-shot and two-shot prompting configurations. The authors employ an iterative prompt design process, testing each prompt type on datasets including CASI (for clinical sense disambiguation, coreference resolution, medication status extraction, and medication attribute extraction) and EBM-NLP (for biomedical evidence extraction). The heuristic prompts use rule-based sentence generation, while ensemble approaches combine outputs from multiple prompt types through voting mechanisms.

## Key Results
- Task-specific prompt tailoring is critical for achieving high accuracy in clinical NLP tasks
- Heuristic and chain-of-thought prompts generally achieve the highest performance across tasks
- GPT-3.5 shows superior adaptability across different clinical NLP tasks compared to BARD and LLAMA2
- Few-shot prompting consistently improves accuracy by providing contextual examples for the models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific prompt tailoring is crucial for achieving high accuracy in clinical NLP tasks.
- Mechanism: Different clinical NLP tasks require different levels of information and constraints to guide the language model to produce the desired output. By designing prompts that are relevant and specific to each task, the model can better understand the context and extract the correct information.
- Core assumption: The effectiveness of prompt engineering depends on the alignment between the prompt structure and the specific requirements of each clinical NLP task.
- Evidence anchors:
  - [abstract]: "However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data."
  - [section]: "We found that different prompt types have different strengths and weaknesses for different task types, depending on the level of information and constraints they provide to the language model."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If the prompt structure does not align with the task requirements, the model's performance will degrade regardless of the language model's capabilities.

### Mechanism 2
- Claim: Heuristic prompts are generally very effective for guiding the language model to produce clear and unambiguous outputs.
- Mechanism: Heuristic prompts use a rule-based approach to generate sentences that contain definitions or examples of the target words or concepts. This helps the model understand and match the context, especially for tasks that involve disambiguation, extraction, or classification of entities or relations.
- Core assumption: Rule-based prompts can capture the salient features and constraints of clinical information extraction tasks.
- Evidence anchors:
  - [abstract]: "We also introduce two new types of prompting approaches: 1) Heuristic prompts and 2) Ensemble."
  - [section]: "We found that heuristic, prefix and chain of thought prompts are generally very effective for guiding the language model to produce clear and unambiguous outputs."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If the predefined rules are too rigid or do not cover all possible scenarios, the model may fail to handle edge cases or ambiguous inputs.

### Mechanism 3
- Claim: Chain of thought prompts are effective for guiding the language model to produce logical and coherent outputs.
- Mechanism: Chain of thought prompts use a multi-step approach to generate sentences that contain a series of questions and answers that resolve the task in the context. This helps the model perform reasoning, inference, or coreference resolution tasks.
- Core assumption: Multi-step reasoning approaches can improve the model's ability to handle complex logical relationships.
- Evidence anchors:
  - [abstract]: "We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts."
  - [section]: "For coreference resolution, the chain of thought prompt type performed best among all prompt types with two language models-GPT3.5 and LLAMA2."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If the task does not require multi-step reasoning, chain of thought prompts may introduce unnecessary complexity and reduce performance.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Understanding in-context learning is essential because the study focuses on zero-shot and few-shot prompting strategies without fine-tuning.
  - Quick check question: What is the difference between zero-shot and few-shot prompting in the context of in-context learning?

- Concept: Prompt engineering
  - Why needed here: Prompt engineering is the core methodology being evaluated, so understanding its principles and techniques is crucial.
  - Quick check question: How does the structure of a prompt influence the output of a large language model?

- Concept: Clinical NLP tasks
  - Why needed here: The study evaluates five different clinical NLP tasks, so understanding their characteristics and requirements is necessary.
  - Quick check question: What are the main differences between named entity recognition and coreference resolution in clinical text?

## Architecture Onboarding

- Component map:
  Prompt types (prefix, cloze, anticipatory, heuristic, chain-of-thought, ensemble, few-shot) -> Language models (GPT-3.5, BARD, LLAMA2) -> Clinical NLP tasks (clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, medication attribute extraction) -> Evaluation metrics (accuracy)

- Critical path:
  1. Define the clinical NLP task type
  2. Choose the appropriate prompt type based on task requirements
  3. Select the language model compatible with the chosen prompt type
  4. Evaluate performance using accuracy metrics
  5. Iterate if performance is unsatisfactory

- Design tradeoffs:
  - Zero-shot vs few-shot: Zero-shot is faster but may have lower accuracy; few-shot provides context but requires examples
  - Simple vs complex prompts: Simple prompts are easier to implement but may be less effective; complex prompts may improve accuracy but are harder to design
  - Single vs ensemble approaches: Single prompts are straightforward but may miss complementary strengths; ensemble approaches can improve accuracy but add complexity

- Failure signatures:
  - Low accuracy across all prompt types may indicate issues with the language model's understanding of clinical domain
  - Inconsistent performance across different prompt types may suggest the need for better prompt-task alignment
  - High variance in outputs may indicate randomness issues that need to be addressed through prompt specificity

- First 3 experiments:
  1. Test simple prefix prompts across all five tasks with GPT-3.5 to establish baseline performance
  2. Compare zero-shot and few-shot performance for clinical sense disambiguation using heuristic prompts
  3. Evaluate ensemble approaches by combining outputs from different prompt types for biomedical evidence extraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question:
What is the optimal number of examples (shots) to include in few-shot prompting for clinical NLP tasks?
- Basis in paper: [inferred]
The paper discusses contrasting zero-shot and few-shot prompting strategies, using two examples for each task, but doesn't systematically explore the optimal number of shots.
- Why unresolved:
The study uses a fixed number of examples (2-shot) without exploring whether more or fewer examples would improve performance. Different tasks might require different numbers of examples for optimal results.
- What evidence would resolve it:
A systematic study varying the number of examples (e.g., 1, 2, 3, 5, 10) across different clinical NLP tasks to identify the point of diminishing returns or optimal performance.

### Open Question 2
- Question:
How do prompt engineering approaches generalize across different medical specialties and languages?
- Basis in paper: [explicit]
The paper mentions that clinical NLP faces challenges including heterogeneity across institutions and the need for domain knowledge, but doesn't test across different medical specialties or languages.
- Why unresolved:
The study uses general clinical datasets but doesn't explore whether prompts need to be tailored for different medical specialties (e.g., cardiology vs oncology) or translated for different languages.
- What evidence would resolve it:
Testing the same prompt engineering approaches across clinical notes from different medical specialties and languages to measure performance variations and identify specialty-specific adaptations needed.

### Open Question 3
- Question:
What is the relationship between prompt complexity and model performance for different clinical NLP tasks?
- Basis in paper: [inferred]
The paper introduces heuristic and ensemble prompts as novel approaches but doesn't systematically analyze how prompt complexity affects performance across task types.
- Why unresolved:
While the study compares different prompt types, it doesn't analyze whether more complex prompts (like ensemble approaches) consistently outperform simpler ones, or if there's an optimal complexity level for different task categories.
- What evidence would resolve it:
A controlled experiment varying prompt complexity while keeping other factors constant, measuring performance across different clinical NLP task categories to establish complexity-performance relationships.

### Open Question 4
- Question:
How does model training data composition affect zero-shot performance on clinical NLP tasks?
- Basis in paper: [explicit]
The paper acknowledges that high accuracy could be due to models having seen the data during training rather than prompt effectiveness, but doesn't investigate this further.
- Why unresolved:
The study doesn't explore how the proportion of clinical data in pre-training affects zero-shot performance, or whether models trained with more clinical data perform better on these tasks.
- What evidence would resolve it:
Comparing zero-shot performance across language models with different proportions of clinical data in their training corpora on the same clinical NLP tasks.

### Open Question 5
- Question:
What are the error patterns and failure modes specific to prompt-based clinical NLP systems?
- Basis in paper: [inferred]
The paper discusses randomness in outputs and mentions limitations but doesn't provide a systematic analysis of specific error patterns or failure modes.
- Why unresolved:
While the study reports accuracy metrics, it doesn't categorize or analyze the types of errors made by different prompt types or identify systematic failure modes in clinical contexts.
- What evidence would resolve it:
A detailed error analysis categorizing mistakes by type (e.g., context misunderstanding, abbreviation confusion, temporal reasoning errors) and identifying patterns specific to different prompt types and clinical NLP tasks.

## Limitations
- The study uses accuracy as the sole performance metric, which may not capture the full spectrum of clinical NLP task quality
- Only three language models were evaluated, which may not represent the full landscape of available LLMs
- The study focuses on English-language clinical text, limiting generalizability to other languages or cross-lingual applications
- The datasets used may not be representative of the full diversity of clinical documentation styles found in real-world healthcare settings

## Confidence
- High confidence: Task-specific prompt tailoring is critical for accuracy
- Medium confidence: Heuristic and chain-of-thought prompts generally achieve highest performance
- Low confidence: Generalizability of few-shot improvements across all task types

## Next Checks
1. Test the best-performing prompt types (heuristic and chain-of-thought) on additional clinical NLP datasets beyond the CASI and EBM-NLP corpora to assess generalizability across different clinical documentation styles and domains.

2. Implement additional evaluation metrics beyond accuracy, including precision, recall, F1-score, and clinical validity assessment by medical experts, to provide a more comprehensive understanding of model performance for each task type.

3. Conduct a systematic ablation study to determine the minimal effective prompt structure for each task type, identifying which prompt components contribute most to performance improvements and whether simpler prompts can achieve comparable results to complex approaches.