---
ver: rpa2
title: Cross Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction
arxiv_id: '2312.06424'
source_url: https://arxiv.org/abs/2312.06424
tags:
- domain
- lifelong
- sequence
- attention
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Lifelong Cross Network (LCN) to improve cross-domain
  click-through rate prediction by modeling lifelong sequences from both target and
  source domains. LCN features a Lifelong Attention Pyramid (LAP) module with three
  cascaded attention levels that progressively extract user interests, and a Cross
  Representation Production (CRP) module that aligns cross-domain representations
  using contrastive loss.
---

# Cross Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2312.06424
- Source URL: https://arxiv.org/abs/2312.06424
- Reference count: 33
- Primary result: LCN achieves AUC 0.7294, GAUC 0.6423, logloss 0.2223, +2.93% CTR, +3.27% stay time improvement

## Executive Summary
This paper introduces Lifelong Cross Network (LCN), a novel approach for cross-domain click-through rate prediction that models lifelong sequences from both target and source domains. LCN features a Lifelong Attention Pyramid (LAP) module with three cascaded attention levels that progressively extract user interests, and a Cross Representation Production (CRP) module that aligns cross-domain representations using contrastive loss. The method demonstrates significant improvements over state-of-the-art models on industrial WeChat Channels data and benchmarks, with notable gains in both offline metrics (AUC, GAUC, logloss) and online A/B testing (CTR, stay time, inference latency).

## Method Summary
LCN addresses cross-domain lifelong sequential modeling for CTR prediction by processing extremely long user behavior sequences (up to 2000 items) across multiple domains. The model consists of a Lifelong Attention Pyramid (LAP) with three cascaded attention levels (complete-scope, median-scope, focused-scope) that progressively filter and refine user interests, and a Cross Representation Production (CRP) module that uses contrastive loss to align cross-domain representations. The final prediction is made through joint training of both modules with combined cross-entropy and CRP losses.

## Key Results
- Offline improvements: AUC increases from 0.7237 to 0.7294, GAUC from 0.6379 to 0.6423, logloss from 0.2330 to 0.2223
- Online A/B testing: +2.93% CTR improvement, +3.27% stay time improvement, 3ms latency increase
- State-of-the-art performance on industrial WeChat Channels dataset (18B source domain samples, 2B target domain samples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Lifelong Attention Pyramid (LAP) progressively narrows the attention scope from complete to focused, reducing computational complexity while maintaining performance.
- Mechanism: LAP uses three cascaded attention levels—complete-scope, median-scope, and focused-scope—where each level filters and refines the input based on the previous level's output, focusing on increasingly relevant items.
- Core assumption: Filtering at early levels removes noise and irrelevant items, allowing later levels to operate on a smaller, more relevant subset without loss of critical information.
- Evidence anchors:
  - [abstract] "The proposed LCN contains a LifeLong Attention Pyramid (LAP) module that comprises of three levels of cascaded attentions to effectively extract interest representations..."
  - [section] "The three levels of attentions are cascaded, which means that the input of each attention level is based on the output of its preceding level. The scope of the attention would become more concentrated with respect to the candidate item as the attention level grows."
- Break condition: If early filtering is too aggressive, it may remove items that become relevant in later stages due to non-linear interactions.

### Mechanism 2
- Claim: Cross Representation Production (CRP) uses contrastive loss to align cross-domain representations, enabling better knowledge transfer.
- Mechanism: CRP creates positive and negative item pairs across domains and applies contrastive loss to minimize distance between positive pairs and maximize distance between negative pairs, learning cross-domain invariant representations.
- Core assumption: Items that a user interacts with across different domains share underlying preference signals that can be captured through contrastive learning.
- Evidence anchors:
  - [abstract] "We also propose Cross Representation Production (CRP) module to enforce additional supervision on the learning and alignment of cross-domain representations..."
  - [section] "Inspired by contrastive learning... we propose to incorporate a contrastive loss in the CRP module to enforce rectified supervision to improve learning efficiency of cross-domain representations."
- Break condition: If contrastive pairs are not representative or balanced, the learned representations may not capture true cross-domain preferences.

### Mechanism 3
- Claim: Joint training of LAP and CRP allows the model to benefit from both progressive attention filtering and cross-domain representation alignment simultaneously.
- Mechanism: The final loss combines the main CTR prediction loss with CRP loss, allowing gradients from cross-domain supervision to flow back and improve the attention modules' representations.
- Core assumption: Cross-domain representation learning and sequential attention modeling are complementary tasks that reinforce each other when trained jointly.
- Evidence anchors:
  - [section] "The loss of the LCN is a cross entropy loss to minimize the CTR prediction error... As the CRP module is jointly trained with the main network, the final loss function of the LCN is a combination of LT, LCRP"
  - [section] "It is jointly trained and provide representations significantly better than the ones may be learned solely in the main network."
- Break condition: If CRP loss weight is too high, it may dominate training and degrade main task performance.

## Foundational Learning

- Concept: Attention mechanisms in deep learning
  - Why needed here: LCN uses multi-head attention in focused-scope level and simple weighted attention in earlier levels to extract relevant features from sequences.
  - Quick check question: What is the difference between additive attention and dot-product attention, and when might each be preferred?

- Concept: Contrastive learning
  - Why needed here: CRP module uses contrastive loss to align cross-domain representations by pulling positive pairs together and pushing negative pairs apart.
  - Quick check question: How does contrastive loss differ from traditional classification loss, and what are its key advantages for representation learning?

- Concept: Lifelong sequential modeling
  - Why needed here: LCN processes extremely long user behavior sequences (up to 2000 items) across multiple domains, requiring efficient attention mechanisms.
  - Quick check question: What are the computational challenges of applying standard attention to lifelong sequences, and how do sampling-based approaches address them?

## Architecture Onboarding

- Component map: Input → Feature Embeddings → LAP (Complete→Median→Focused) → CRP (Contrastive Loss) → Concatenation → MLP → Output
- Critical path: Feature embedding → LAP processing → CRP alignment → Final prediction
- Design tradeoffs: LAP balances complexity reduction vs. information retention; CRP adds training complexity but improves cross-domain generalization
- Failure signatures:
  - Overfitting on short sequences but poor performance on lifelong sequences
  - Good source domain performance but poor target domain transfer
  - High training accuracy but low online CTR improvement
- First 3 experiments:
  1. Validate LAP ablation (remove each attention level) to confirm progressive filtering importance
  2. Test CRP ablation (remove contrastive loss) to measure cross-domain alignment contribution
  3. Compare computational complexity vs. accuracy trade-off between LAP and full-sequence attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed LAP module perform compared to other attention mechanisms in lifelong sequential modeling when the sequence length exceeds 2000?
- Basis in paper: [explicit] The paper mentions that the industrial dataset has a lifelong sequence length of up to 2000, but does not provide performance comparisons beyond this length.
- Why unresolved: The paper does not explore the scalability of the LAP module to sequences longer than 2000.
- What evidence would resolve it: Performance comparisons of the LAP module on datasets with sequence lengths significantly longer than 2000.

### Open Question 2
- Question: What is the impact of different side information types on the performance of the LAP module?
- Basis in paper: [explicit] The paper mentions that side information is used in the MSA level of the LAP module, but does not explore the impact of different types of side information.
- Why unresolved: The paper does not provide a detailed analysis of how different side information types affect the LAP module's performance.
- What evidence would resolve it: Experimental results showing the performance of the LAP module with different types of side information.

### Open Question 3
- Question: How does the proposed CRP module perform in scenarios where the source and target domains have a large number of overlapping items?
- Basis in paper: [explicit] The paper assumes that the source and target domains do not have overlapping items, but does not explore the performance in scenarios with overlapping items.
- Why unresolved: The paper does not provide experimental results for scenarios with overlapping items in the source and target domains.
- What evidence would resolve it: Experimental results showing the performance of the CRP module in scenarios with overlapping items in the source and target domains.

## Limitations
- Implementation gaps: Exact hyperparameters for contrastive loss in CRP remain unspecified
- Missing statistical validation: Online A/B testing results lack confidence intervals and p-values
- Limited scalability analysis: No computational complexity comparison with baseline methods
- Side information integration: Mechanism only abstractly described without implementation details

## Confidence
- **High confidence**: The architectural design of LAP with cascaded attention levels and the overall CTR improvement metrics (AUC from 0.7237 to 0.7294, GAUC from 0.6379 to 0.6423) are well-supported by the methodology and experimental results.
- **Medium confidence**: The claim that CRP improves cross-domain representation alignment is plausible given the contrastive learning framework, but the exact mechanism and effectiveness depend on implementation details not fully specified in the paper.
- **Low confidence**: The online A/B testing results lack statistical validation and detailed implementation specifications, making it difficult to verify the claimed 2.93% CTR improvement without access to the production environment.

## Next Checks
1. **LAP Ablation Study**: Remove each attention level sequentially and measure performance degradation to validate the progressive filtering hypothesis and quantify the contribution of each level.
2. **CRP Effectiveness Test**: Train identical models with and without the CRP module and contrastive loss, measuring both cross-domain representation quality (via t-SNE visualization) and downstream CTR performance.
3. **Statistical Significance Analysis**: Re-analyze the online A/B testing results using proper statistical methods to determine confidence intervals and p-values for the reported CTR, stay time, and latency improvements.