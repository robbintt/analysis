---
ver: rpa2
title: Least Squares Regression Can Exhibit Under-Parameterized Double Descent
arxiv_id: '2305.14689'
source_url: https://arxiv.org/abs/2305.14689
tags:
- error
- generalization
- page
- peak
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that double descent can occur in the under-parameterized
  regime for a ridge regularized least squares denoising problem with data on a line.
  The key insight is that shifting noise from the output to the input variable fundamentally
  changes the behavior of the generalization error curve.
---

# Least Squares Regression Can Exhibit Under-Parameterized Double Descent

## Quick Facts
- arXiv ID: 2305.14689
- Source URL: https://arxiv.org/abs/2305.14689
- Reference count: 40
- Key outcome: Double descent can occur in the under-parameterized regime for ridge-regularized least squares denoising when noise is shifted from output to input.

## Executive Summary
This work demonstrates that double descent can occur in the under-parameterized regime for a ridge-regularized least squares denoising problem with data on a line. By shifting noise from the output to the input variable, the generalization error curve exhibits a peak at aspect ratios less than 1, contrary to prior results. The peak location depends on the ridge regularization strength μ and is approximately at c = 1/(μ²+1). This finding challenges the common belief that double descent only occurs at the boundary between under and over-parameterized regimes or in the over-parameterized regime. The study also reveals an implicit bias in the model that favors using ridge regularization over input data noise regularization for optimal generalization performance.

## Method Summary
The authors analyze a ridge-regularized least squares denoising problem where data points lie on a line embedded in high-dimensional space, with noise added to the input rather than the output. They derive theoretical formulas for the generalization error using random matrix theory and verify these predictions empirically. The key innovation is shifting noise from output to input while maintaining equivalent total noise variance, which fundamentally changes the behavior of the generalization error curve and enables double descent in the under-parameterized regime.

## Key Results
- Double descent peaks can occur in the under-parameterized regime (d < Ntrn) when noise is shifted from output to input
- The peak location is approximately at c = 1/(μ²+1), where μ is the ridge regularization parameter
- The model exhibits an implicit bias favoring ridge regularization over input noise regularization for optimal generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting noise from output to input enables double descent in under-parameterized regime
- Mechanism: Moving noise to input changes model sensitivity to data dimensionality, shifting the peak into under-parameterized regime
- Core assumption: Noise variance remains equivalent when shifted between input and output
- Evidence anchors:
  - [abstract]: "shifting noise from the output to the input variable fundamentally changes the behavior of the generalization error curve"
  - [section]: "their results, Figure 3 in [23], show that the peaks only occur if the model is over-parameterized"
  - [corpus]: "Weak. Only 1/8 related papers directly address input vs output noise positioning."
- Break condition: If noise variance scaling is incorrect or data dimensionality not properly scaled

### Mechanism 2
- Claim: Peak location depends on ridge regularization strength μ at approximately c = 1/(μ²+1)
- Mechanism: Ridge regularization interacts with input noise to create non-monotonic generalization error curve
- Core assumption: Relationship between μ and peak location follows derived formula from Theorem 3
- Evidence anchors:
  - [abstract]: "the peak of the sample-wise double descent curve corresponds to a peak in the curve for the norm of the estimator, and adjusting μ, the strength of the ridge regularization, shifts the location of the peak"
  - [section]: "Theorem 3 provides a method for estimating the location of the peak... Here, at c = (μ² + 1)⁻¹, the first term in the numerator is zero"
  - [corpus]: "Moderate. Several papers discuss regularization strength effects, but none derive this specific location formula."
- Break condition: If μ is extremely large or small, peak may disappear or shift outside under-parameterized regime

### Mechanism 3
- Claim: Ridge and input noise regularizers are not equivalent despite regularizing same quantity
- Mechanism: Model's implicit bias favors ridge regularization over input noise for optimal generalization
- Core assumption: Learning algorithm treats ridge and input noise regularization differently
- Evidence anchors:
  - [abstract]: "The model's implicit bias leads to the best generalization error occurring with high data quality (low noise regularization) and high ridge regularization"
  - [section]: "we study the trade-off between the two regularizers and notice that they are not equivalent"
  - [corpus]: "Weak. Only 1/8 related papers discuss regularizer equivalence, and findings are inconclusive."
- Break condition: If learning algorithm explicitly balances both regularizers or data structure eliminates asymmetry

## Foundational Learning

- Concept: Double descent phenomenon
  - Why needed here: Understanding baseline behavior where peaks occur at interpolation points is crucial for recognizing under-parameterized peak significance
  - Quick check question: In traditional linear regression with output noise, where does the double descent peak typically occur?

- Concept: Ridge regularization in linear models
  - Why needed here: Ridge regularization parameter μ directly controls peak location and is central to model behavior
  - Quick check question: How does increasing ridge regularization strength typically affect generalization error in standard models?

- Concept: Random matrix theory and Marchenko-Pastur distribution
  - Why needed here: Theoretical analysis relies on random matrix theory to derive asymptotic formulas for generalization error
  - Quick check question: What distribution describes the limiting eigenvalue distribution of sample covariance matrices in high dimensions?

## Architecture Onboarding

- Component map: Data generation -> Noise addition -> Ridge-regularized optimization -> Generalization error computation -> Peak location analysis
- Critical path: Data generation → Noise addition → Ridge-regularized optimization → Generalization error computation → Peak location analysis
- Design tradeoffs: Input vs output noise placement (enables under-parameterized peak but requires careful noise variance scaling), ridge regularization strength (controls peak location but affects convergence), data dimensionality (must be high enough for RMT approximations to hold)
- Failure signatures: No peak in generalization error curve (μ too large/small or d too small), peak at wrong location (incorrect noise variance scaling), poor match between theoretical and empirical results (insufficient trials or inappropriate parameter ranges)
- First 3 experiments:
  1. Verify peak location formula: Sweep μ values and confirm peak occurs near c = 1/(μ²+1)
  2. Test regularizer asymmetry: Compare optimal σtrn and μ values to confirm implicit bias
  3. Validate under-parameterized regime: Show peak persists for various d < Ntrn combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes double descent in the under-parameterized regime for the ridge regularized least squares denoising problem?
- Basis in paper: [explicit] The authors state that double descent occurs in the under-parameterized regime for this model, contrary to prior results.
- Why unresolved: The paper shows that double descent occurs, but does not provide a definitive explanation for the underlying cause. The authors suggest that the peak location depends on the technical properties of the spectrum and eigenvectors of the sample covariance, but this is not fully explored.
- What evidence would resolve it: Further theoretical analysis to identify the specific factors causing double descent in this regime, or experimental results demonstrating the effect of manipulating these factors on the double descent behavior.

### Open Question 2
- Question: How does the implicit bias of the model influence the choice between ridge regularization and input data noise regularization?
- Basis in paper: [explicit] The authors observe that the model's implicit bias leads to better generalization performance when using high-quality data (low noise regularization) and high ridge regularization.
- Why unresolved: While the paper demonstrates this implicit bias, it does not fully explain why the model favors one regularizer over the other or the mechanisms behind this preference.
- What evidence would resolve it: Additional theoretical analysis or experiments that shed light on the model's decision-making process regarding regularization, or empirical studies comparing the effects of different regularization strategies on generalization performance.

### Open Question 3
- Question: How does the shape of the risk curve depend on the dimension d in the data scaling regime?
- Basis in paper: [explicit] The authors note that the shape of the risk curve depends on the value of d, and that the results are accurate even for relatively small values of d.
- Why unresolved: The paper does not provide a detailed analysis of how the dimension d affects the risk curve's shape or the conditions under which the results hold.
- What evidence would resolve it: Further theoretical analysis or experiments that systematically explore the relationship between the dimension d and the risk curve's shape, or empirical studies that test the model's performance for various values of d.

## Limitations
- The peak location formula c = 1/(μ²+1) is derived as an asymptotic approximation that may deviate for finite dimensions
- Results assume Gaussian noise and specific data geometry (points on a line), limiting generalizability
- The implicit bias favoring ridge over input noise regularization is observed but not fully explained mechanistically

## Confidence
- Double descent in under-parameterized regime: **High** - Proven theoretically and verified empirically
- Peak location formula accuracy: **Medium** - Theoretical derivation sound but asymptotic approximations used
- Regularizer asymmetry claim: **Medium** - Observed in experiments but mechanistic understanding incomplete

## Next Checks
1. **Finite-sample validation**: Test the peak location formula c = 1/(μ²+1) across different (N, d) pairs to quantify deviations from asymptotic predictions
2. **Noise distribution sensitivity**: Repeat experiments with non-Gaussian noise (e.g., Laplacian, uniform) to assess robustness
3. **Data geometry generalization**: Extend to data lying on lower-dimensional manifolds (e.g., 2D surfaces) to test if under-parameterized double descent persists