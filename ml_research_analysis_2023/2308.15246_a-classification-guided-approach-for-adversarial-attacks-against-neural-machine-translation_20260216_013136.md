---
ver: rpa2
title: A Classification-Guided Approach for Adversarial Attacks against Neural Machine
  Translation
arxiv_id: '2308.15246'
source_url: https://arxiv.org/abs/2308.15246
tags:
- attack
- translation
- adversarial
- classifier
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACT, a novel adversarial attack framework
  against NMT systems guided by a classifier objective. Unlike previous attacks that
  focus solely on translation quality or inserting specific keywords, ACT aims to
  craft adversarial examples whose translations by the target NMT model belong to
  a different class than the original translations.
---

# A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation

## Quick Facts
- arXiv ID: 2308.15246
- Source URL: https://arxiv.org/abs/2308.15246
- Reference count: 16
- Primary result: ACT is a novel adversarial attack framework that crafts meaning-preserving adversarial examples whose translations by the target NMT model belong to a different class than the original translations.

## Executive Summary
This paper introduces ACT, a novel adversarial attack framework against NMT systems guided by a classifier objective. Unlike previous attacks that focus solely on translation quality or inserting specific keywords, ACT aims to craft adversarial examples whose translations by the target NMT model belong to a different class than the original translations. The authors propose enhancements to existing black-box word-replacement attacks by incorporating output translations of the target NMT model and the output logits of a classifier within the attack process. Extensive experiments demonstrate that ACT is considerably more successful in altering the class of the output translation and has more effect on the translation compared to existing untargeted attacks.

## Method Summary
ACT modifies existing word-replacement-based black-box attacks (TextFooler, BAE) by redefining the goal and score functions to incorporate both NMT output translation and classifier logits. The attack iteratively queries the target NMT model to obtain translations of candidate adversarial sentences, then feeds these translations to a classifier to evaluate class change. Success is determined by two thresholds: BLEU score below a threshold (indicating translation degradation) and logit margin above a threshold (indicating confident class change). The score function combines classifier logit difference with translation similarity to guide the search toward adversarial examples that both change class and degrade translation quality.

## Key Results
- ACT significantly outperforms baseline attacks in altering the class of NMT output translations
- ACT achieves higher attack success rates while maintaining semantic similarity to original sentences
- The dual-threshold goal function (BLEU + logit margin) effectively distinguishes NMT impact from classifier manipulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack succeeds by altering translation class via joint NMT classifier perturbation
- Mechanism: ACT modifies input tokens iteratively, using a classifier to evaluate whether the NMT output shifts to a different class. The adversarial example preserves source-language semantics but changes the NMT-generated translation enough to fool the classifier into a different label. Word-replacement-based search is guided by both translation similarity and classifier logits, balancing semantic preservation with class shift.
- Core assumption: The classifier reliably detects class changes in NMT output and its logits correlate with human-interpretable meaning shifts.
- Evidence anchors:
  - [abstract] "ACT aims to craft meaning-preserving adversarial examples whose translations... belong to a different class"
  - [section 3.2] "goal function... considers an adversarial attack to be successful if... BLEU score < threshold... AND difference between logits... > threshold"
- Break condition: If classifier predictions become unstable or collapse to uniform logits, the goal function can no longer distinguish meaningful class changes.

### Mechanism 2
- Claim: The attack's success is enhanced by modifying existing word-replacement-based search with NMT-aware scoring
- Mechanism: Standard word-replacement attacks like TextFooler or BAE are adapted by redefining the score function to include translation similarity. This ensures that token importance is not only based on classifier logit change but also on how much the NMT output diverges from the original. This dual scoring steers the search toward adversarial examples that both change class and degrade translation quality.
- Core assumption: NMT embeddings or BLEU-based similarity reliably measure meaningful change in translation; lower BLEU implies class shift.
- Evidence anchors:
  - [section 3.2] "score function... S(x') = w'_z + α sim(y, y')"
  - [section 4.2] "ACT TF... can generate adversarial examples whose translations are further away from the original translation than those generated by the baselines"
- Break condition: If BLEU or translation similarity metrics become saturated or non-informative for the NMT model, the scoring function loses discriminative power.

### Mechanism 3
- Claim: The attack's robustness is ensured by dual goal functions distinguishing NMT and classifier effects
- Mechanism: Two thresholds control attack success: one for translation similarity (BLEU), one for classifier logit margin. By requiring both conditions, ACT ensures that class change is not simply due to classifier misclassification but also due to meaningful NMT output change. This decouples classifier manipulation from NMT robustness assessment.
- Core assumption: NMT and classifier can be independently influenced; success requires both NMT translation to change and classifier to misclassify.
- Evidence anchors:
  - [section 3.2] "We need to distinguish between the impact of the attack on the NMT model T and on the classifier F"
  - [section 4.3] ablation study shows that adding translation similarity to goal function increases ASR and reduces translation similarity
- Break condition: If classifier logits become insensitive to NMT output changes (e.g., classifier overfits to source), the logit margin condition fails to reflect real class shift.

## Foundational Learning

- Concept: Adversarial attack methodology in NLP
  - Why needed here: ACT builds on word-replacement-based attacks; understanding how these work (token substitution, search strategies, constraints) is essential for modifying them to fit a classification-guided objective.
  - Quick check question: What is the difference between targeted and untargeted adversarial attacks in NLP?

- Concept: Neural Machine Translation (NMT) and evaluation metrics
  - Why needed here: ACT targets NMT models; understanding their architecture, output nature (full sentences), and metrics like BLEU/chrf is crucial for interpreting how attacks degrade translation and alter class.
  - Quick check question: Why is BLEU used instead of exact match when evaluating translation similarity?

- Concept: Classification models and logit interpretation
  - Why needed here: ACT uses a classifier to determine the class of NMT output; understanding how logits work and why they are preferred over predicted labels in attack goals is key to implementing and tuning ACT.
  - Quick check question: What advantage do raw logits provide over predicted labels in adversarial attack settings?

## Architecture Onboarding

- Component map:
  Input sentence -> NMT model (outputs translation) -> Classifier (outputs class + logits) -> Word-replacement transformation engine -> Search loop -> Goal functions (BLEU threshold + logit margin threshold)

- Critical path:
  1. Start with original sentence
  2. Generate candidate token substitutions under constraints
  3. For each candidate, query NMT and get translation
  4. Feed translation to classifier, get class and logits
  5. Compute score (logits + translation similarity)
  6. Select substitution that improves score while satisfying both goal functions
  7. Repeat until success or budget exhausted

- Design tradeoffs:
  - Higher α in score function → more focus on translation change, may hurt semantic preservation
  - Lower BLEU threshold thrT → stricter requirement for translation change, may reduce ASR
  - Higher logit margin thrF → more confident class change, but may be harder to achieve
  - Choice of classifier (off-the-shelf vs fine-tuned) affects reliability of class detection

- Failure signatures:
  - ASR remains low despite many iterations → candidate space too constrained or thresholds too strict
  - High perplexity, low BLEU, but class unchanged → search not finding effective perturbations
  - Classifier predictions match original class for all candidates → classifier invariant to perturbations

- First 3 experiments:
  1. Run ACT on Marian NMT (En-Fr) with SST-2, use BLEU < 0.4 and logit margin > 2, record ASR and BLEU degradation
  2. Vary α (e.g., 1, 3, 5) and observe effect on ASR vs BLEU tradeoff
  3. Replace classifier with a different model (e.g., off-the-shelf vs fine-tuned) and compare ASR and translation similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of similarity metric (BLEU vs BLEURT) affect the attack success rate and translation quality degradation?
- Basis in paper: [explicit] The paper compares ACTTF performance using BLEU score vs BLEURT-20 as the similarity metric in Table 9.
- Why unresolved: The comparison is limited to one NMT model and one dataset. Different NMT models and translation tasks might yield different results.
- What evidence would resolve it: A comprehensive comparison across multiple NMT models (Marian, mBART50, etc.), translation directions (En-Fr, En-De), and datasets (SST-2, MR, AG's News) using both BLEU and BLEURT metrics.

### Open Question 2
- Question: What is the impact of using an ensemble of classifiers on the attack's effectiveness and its ability to distinguish between affecting the NMT model vs the classifier?
- Basis in paper: [explicit] The paper discusses using two classifiers in the attack process and reports results in Table 6, showing increased success rate and translation impact.
- Why unresolved: The study only uses two classifiers. The optimal number and composition of classifiers for the ensemble, as well as the trade-off between increased success rate and potential over-reliance on the classifier, remain unexplored.
- What evidence would resolve it: Experiments varying the number of classifiers in the ensemble, using classifiers with different architectures (BERT, GPT-2, etc.) and accuracies, and analyzing the correlation between ensemble size and the ability to isolate NMT model impact.

### Open Question 3
- Question: How transferable are the adversarial examples across different NMT models and translation directions, and what factors influence this transferability?
- Basis in paper: [explicit] The paper conducts a transferability analysis in Table 12, showing moderate transferability from Marian (En-Fr) to mBART50 (En-Fr) and Marian (En-De).
- Why unresolved: The analysis is limited to one source model and two target models. The impact of model architecture differences, training data overlap, and translation direction on transferability is not fully explored.
- What evidence would resolve it: A broader transferability study involving multiple source and target NMT models with varying architectures (transformer-based, RNN-based, etc.), training data sizes, and translation directions (En-Fr, Fr-En, En-De, De-En). Analyzing the correlation between model similarity and adversarial example transferability.

## Limitations
- The attack's effectiveness depends heavily on classifier reliability for detecting translation class changes
- Translation similarity metrics (BLEU/chrf) may not always reflect meaningful semantic change
- Generalizability to different NMT architectures and language pairs is untested
- No human evaluation to validate semantic preservation and class change claims

## Confidence
- High Confidence: ACT outperforms baselines in altering translation class (supported by quantitative results and ablation studies)
- Medium Confidence: ACT reveals new vulnerabilities by focusing on translation class (plausible but not deeply analyzed)
- Low Confidence: Adversarial examples are both semantically preserved and meaningfully change class (weakly supported without human evaluation)

## Next Checks
1. Replace the classifier with a different architecture (e.g., RoBERTa) and re-run ACT to test classifier dependence
2. Conduct human evaluation to rate adversarial examples for semantic preservation and class shift
3. Apply ACT to a different NMT architecture and language pair to test generalizability