---
ver: rpa2
title: Composition-contrastive Learning for Sentence Embeddings
arxiv_id: '2307.07380'
source_url: https://arxiv.org/abs/2307.07380
tags:
- learning
- contrastive
- simcse
- sentence
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to contrastive learning
  for sentence embeddings by maximizing alignment between texts and compositions of
  their phrasal constituents. Instead of relying on minimal augmentations, the method
  decomposes sentences into phrases, encodes them separately, and aggregates their
  representations to form positive pairs.
---

# Composition-contrastive Learning for Sentence Embeddings

## Quick Facts
- **arXiv ID**: 2307.07380
- **Source URL**: https://arxiv.org/abs/2307.07380
- **Reference count**: 22
- **Primary result**: Introduces compositional augmentation for sentence embeddings that achieves state-of-the-art performance on semantic textual similarity tasks without additional parameters

## Executive Summary
This paper proposes a novel approach to contrastive learning for sentence embeddings by maximizing alignment between full sentences and compositions of their phrasal constituents. The method decomposes sentences into non-overlapping phrasal spans, encodes them separately, and aggregates their representations to form positive pairs in the contrastive objective. This compositional augmentation improves alignment and uniformity in the embedding space while stabilizing and accelerating training convergence. Experiments show the approach achieves results comparable to state-of-the-art methods on semantic textual similarity benchmarks without requiring additional network parameters or auxiliary training objectives.

## Method Summary
The approach builds on SimCSE's unsupervised contrastive learning framework but introduces compositional augmentation as a data augmentation strategy. Sentences are split into phrasal constituents using a discourse parser, encoded separately (either in parallel or via multiple forward passes), and their [CLS] embeddings are aggregated through sum, average, or concatenation. The aggregated phrase representation serves as a positive example alongside the full sentence encoding in the InfoNCE loss. The method optionally expands training batches with subsamples of phrasal constituents to create harder in-batch negatives. Additionally, computing contrastive loss on a subset of embedding dimensions (d0 < d) mitigates dimensional collapse. The approach is evaluated across BERT and RoBERTa architectures on multiple STS benchmarks.

## Key Results
- Achieves competitive performance on semantic textual similarity tasks comparable to state-of-the-art methods
- Stabilizes and accelerates training convergence compared to traditional augmentation strategies
- Improves alignment and uniformity in the embedding space without requiring additional network parameters
- Shows effectiveness across different PLMs (BERT and RoBERTa) and aggregation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing sentences into phrasal constituents and aligning their aggregated representations with the full sentence improves semantic representation quality.
- Mechanism: The model learns to represent meaning at both the sentence and phrase level by maximizing alignment between the full sentence embedding and the composition of its phrase embeddings. This encourages the encoder to capture compositional semantics.
- Core assumption: Semantic meaning can be effectively reconstructed from the composition of phrasal constituents.
- Evidence anchors:
  - [abstract]: "we propose maximizing alignment between texts and a composition of their phrasal constituents"
  - [section]: "We hypothesize that this is closely tied to short-cut learning... such solutions can yield non-generalizable features that poorly represent data from new domains"
  - [corpus]: Weak - the corpus analysis shows moderate FMR scores but no direct evidence for compositional alignment benefits
- Break condition: If phrasal constituents cannot adequately capture the semantic meaning of the full sentence, or if the aggregation method fails to properly combine phrase embeddings.

### Mechanism 2
- Claim: Expanding the training set with subsamples (phrases) creates harder in-batch negatives, improving contrastive learning.
- Mechanism: By including phrase-level subsamples in the training batch, the model encounters more semantically similar negative examples, forcing it to learn more discriminative features and reducing reliance on spurious features like sentence length.
- Core assumption: Harder negative examples lead to better feature discrimination in contrastive learning.
- Evidence anchors:
  - [section]: "Expanding the training set with subsamples... encourages a more uniform embedding distribution"
  - [section]: "expanding the training set with subsamples... is generally beneficial to contrastive learning"
  - [corpus]: Weak - corpus shows related papers but no direct evidence for subsample effectiveness
- Break condition: If the subsamples are too semantically similar to their parent sentences, they may not provide meaningful negative examples.

### Mechanism 3
- Claim: Computing contrastive loss on a subset of embedding vector axes mitigates dimensional collapse and improves representation quality.
- Mechanism: By restricting the loss computation to a subset of dimensions (d0 < d), the model is forced to distribute information across more dimensions, preventing dimensional collapse where embeddings differ only in a small proportion of feature axes.
- Core assumption: Dimensional collapse occurs in contrastive learning with minimal augmentation, and restricting loss computation can mitigate this.
- Evidence anchors:
  - [section]: "This was theoretically posited in Jing et al. (2022)... coupled with an over-parameterized network, results in low rank solutions"
  - [section]: "computing the contrastive loss on a subset of the embedding vector axes before backpropagating to the entire representation"
  - [corpus]: Weak - corpus shows related works but no direct evidence for dimensional collapse mitigation
- Break condition: If the chosen subset of dimensions is not representative of the full embedding space, or if the model cannot effectively distribute information across dimensions.

## Foundational Learning

- **Concept**: Contrastive learning and InfoNCE objective
  - Why needed here: The entire approach builds upon contrastive learning framework, using InfoNCE to maximize alignment between positive pairs (sentence and its phrasal composition) while repelling negatives
  - Quick check question: What is the difference between the standard InfoNCE objective and the modified version used in this work (with d0 < d)?

- **Concept**: Sentence embedding and semantic compositionality
  - Why needed here: The method relies on the idea that sentence meaning can be composed from phrasal meanings, and that learning this compositionality improves representation quality
  - Quick check question: How does the model ensure that the aggregated phrase embeddings capture the full semantic content of the original sentence?

- **Concept**: Dimensional collapse in representation learning
  - Why needed here: The work addresses dimensional collapse by computing loss on subset of dimensions, which is crucial for understanding why this approach improves representation quality
  - Quick check question: What evidence would indicate that dimensional collapse is occurring in a contrastive learning setup?

## Architecture Onboarding

- **Component map**: Text → Parser → Encoder × (1 or 2+ passes) → Aggregator → Projector → Loss
- **Critical path**: Text → Parser → Encoder × (1 or 2+ passes) → Aggregator → Projector → Loss
- **Design tradeoffs**:
  - Number of phrase partitions: More partitions increase computation but may capture finer-grained composition
  - Aggregation method: Different methods may work better for different PLMs
  - d0 parameter: Smaller values may prevent collapse but reduce information
  - Subsampling strategy: Affects training set size and negative hardness
- **Failure signatures**:
  - Performance worse than baseline SimCSE: Check aggregation method, d0 parameter, or subsampling strategy
  - Training instability: May indicate issues with harder negatives or improper d0 setting
  - Dimensional collapse: Check if loss is computed on full vector (d0 = d) and if embeddings show low rank structure
- **First 3 experiments**:
  1. Baseline comparison: Run SimCSE with and without compositional augmentation on STS-B development set
  2. Aggregation method ablation: Compare sum, average, and concatenation methods on BERTbase
  3. d0 parameter sweep: Test different subset sizes (e.g., 192, 256, 384, 768) to find optimal value for preventing dimensional collapse

## Open Questions the Paper Calls Out
- **Question**: How can compositional augmentations be effectively incorporated into supervised contrastive learning frameworks for sentence embeddings?
  - Basis in paper: [explicit] The paper focuses solely on unsupervised learning and acknowledges the limitation of not exploring supervised settings.
  - Why unresolved: The paper does not provide experimental results or theoretical analysis for supervised settings, leaving a gap in understanding how the compositional approach performs when labeled data is available.
  - What evidence would resolve it: Experiments comparing compositional augmentations in supervised vs. unsupervised settings, along with analysis of their impact on downstream task performance.

- **Question**: What is the theoretical justification for the effectiveness of compositional augmentations in improving alignment and uniformity in sentence embedding spaces?
  - Basis in paper: [inferred] The paper shows empirical improvements but does not provide a formal theoretical explanation for why decomposing sentences into phrasal constituents enhances contrastive learning.
  - Why unresolved: The paper relies on empirical observations without establishing a rigorous theoretical framework explaining the mechanism behind the gains.
  - What evidence would resolve it: Theoretical analysis linking compositional augmentations to improved alignment/uniformity metrics, possibly through information-theoretic or geometric arguments.

- **Question**: How does the proposed compositional approach interact with other training objectives or architectural modifications, such as momentum encoders or auxiliary losses?
  - Basis in paper: [explicit] The paper explicitly states that interoperability with other training objectives has not been explored.
  - Why unresolved: The paper isolates the compositional approach without examining potential synergies or conflicts with other state-of-the-art techniques.
  - What evidence would resolve it: Experiments combining compositional augmentations with momentum encoders, auxiliary losses, or other architectural innovations, followed by comparative analysis of performance trade-offs.

## Limitations
- The empirical validation of compositional alignment benefits is limited, with corpus analysis showing only moderate FMR scores without direct proof of superior semantic representation quality
- The effectiveness of subsampling for creating harder negatives lacks strong empirical support in the corpus signals
- The dimensional collapse mitigation claim is theoretically grounded but lacks direct experimental evidence for this specific implementation
- The method's effectiveness may be limited by the quality of the discourse parser used for phrasal decomposition

## Confidence
- Mechanism 1 (Compositional alignment): Medium - conceptually sound but lacks direct empirical validation
- Mechanism 2 (Harder negatives through sampling): Low - theoretical justification but weak empirical support
- Mechanism 3 (Dimensional collapse mitigation): Medium - grounded in prior theory but needs specific evidence for this implementation
- Overall performance claims: Medium - competitive results reported but methodology limitations not fully explored

## Next Checks
1. Conduct controlled experiments comparing STS performance across different aggregation methods (sum, average, concatenation) on BERTbase to identify optimal compositional strategy
2. Perform ablation studies systematically varying the d0 parameter (subset dimension size) to quantify the impact on dimensional collapse prevention and overall performance
3. Design experiments with controlled sentence lengths and semantic complexity to test whether the method truly learns compositional semantics or relies on spurious correlations like sentence length