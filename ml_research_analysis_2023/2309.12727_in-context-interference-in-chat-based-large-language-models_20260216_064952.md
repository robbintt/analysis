---
ver: rpa2
title: In-context Interference in Chat-based Large Language Models
arxiv_id: '2309.12727'
source_url: https://arxiv.org/abs/2309.12727
tags:
- learning
- information
- in-context
- went
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies in-context interference in chat-based large language
  models, where the models can suffer from performance degradation as new information
  is added to the context. The authors propose a benchmark based on the bAbI dataset
  to evaluate the accumulation and retention of information in LLMs.
---

# In-context Interference in Chat-based Large Language Models

## Quick Facts
- arXiv ID: 2309.12727
- Source URL: https://arxiv.org/abs/2309.12727
- Reference count: 6
- One-line primary result: Chat-based LLMs experience performance degradation as new information is added to the context, revealing limitations in in-context learning.

## Executive Summary
This paper investigates in-context interference in chat-based large language models, where model performance degrades as more information is added to the context. Using a simplified bAbI dataset benchmark, the authors demonstrate that as stories accumulate in the context, the model's accuracy in answering questions about entity locations decreases significantly. The study reveals that this interference cannot be effectively mitigated by simplifying stories, summarizing old information, or removing earlier stories, suggesting fundamental limitations in how LLMs process and retain information in the context.

## Method Summary
The authors create a benchmark based on the bAbI dataset, simplified by replacing default entity names with unique names and shortening stories to the last two sentences. They use the Vicuna 13B model with a maximum token limit of 2048 and temperature of 0.7. The method involves incrementally adding stories to the context and evaluating the model's accuracy in answering questions about entity locations. The study systematically increases the number of stories in the context and measures how this affects the model's ability to retain and retrieve information.

## Key Results
- Model accuracy drops from 100% with one story to around 75% when eight stories are in the same context
- Simplifying stories improves baseline accuracy but still shows performance degradation as more stories are added
- Summarizing or removing old stories does not effectively mitigate the interference problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding new stories to the context increases token count, which dilutes attention on earlier facts and degrades retrieval accuracy.
- Mechanism: The transformer's self-attention weights are normalized over the full context window; as more tokens are added, earlier story tokens receive proportionally less attention mass, reducing their effective retrieval.
- Core assumption: Attention scores remain fixed-length softmax over all tokens; no per-story positional embeddings or gating mechanisms.
- Evidence anchors:
  - "as we add new knowledge to the prompts of the model, we can see that the performance of the model decrease from a 100% with only one story to around 75% when we have 8 in the same context."
  - "we cannot significantly increase the amount of stories since we have a limitations on the amount of tokens the model can receive."
- Break condition: If the model implements sliding window attention or sparse attention that re-weights earlier tokens.

### Mechanism 2
- Claim: Model confuses entity identities when multiple stories use similar action verbs and locations, leading to semantic interference.
- Mechanism: Shared vocabulary ("moved to", "journeyed to", "went to") creates overlapping embeddings; the model conflates entities across stories, especially when names are unique but actions are semantically similar.
- Core assumption: Token embeddings for verbs and locations are reused across stories without per-story disambiguation.
- Evidence anchors:
  - "the model confusing or erasing previous facts when new information is delivered."
  - "we replaced all the default names of the entities in the dataset with unique names" to reduce confusion.
- Break condition: If per-story prompt templates or entity disambiguation layers are added.

### Mechanism 3
- Claim: Model's in-context learning is fundamentally limited by a fixed context window; exceeding it forces truncation, causing catastrophic forgetting of earliest facts.
- Mechanism: Context truncation removes oldest stories from the input sequence, so the model cannot retrieve them; accuracy drops once more than ~6 stories are retained in the buffer.
- Core assumption: Token limit is enforced by hard truncation; no circular buffer or adaptive compression.
- Evidence anchors:
  - "we decided to delete old stories and only keep in buffer 6 of them. By removing old stories, we are removing the option for the model to change its response... we are reducing the amount of tokens that cause interference."
  - "as we increase the number of tokens the model can receive."
- Break condition: If the model implements dynamic context expansion or selective retention.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Interference in this paper is analogous to forgetting, but without weight updates—understanding the analogy clarifies why context dilution causes performance loss.
  - Quick check question: What is the key difference between catastrophic forgetting in continual learning and interference in in-context learning?

- Concept: Self-attention normalization and scaling
  - Why needed here: Explains how attention weights are distributed across tokens and why adding more tokens dilutes attention on earlier ones.
  - Quick check question: How does the softmax over attention scores change when the number of tokens increases?

- Concept: Token limit and context window in transformers
  - Why needed here: Clarifies why there is a hard cap on how much information can be retained and why truncation is necessary.
  - Quick check question: What happens to the earliest tokens in the input sequence when the token limit is exceeded?

## Architecture Onboarding

- Component map: Input prompt builder -> Token encoder -> Multi-head self-attention -> Feed-forward network -> Output decoder
- Critical path: 1) Assemble prompt with all stories + questions 2) Encode tokens with positional embeddings 3) Compute self-attention over entire sequence 4) Generate answer tokens
- Design tradeoffs: Longer context → more information retained but higher compute cost and slower inference; Simpler stories → higher baseline accuracy but still subject to interference; Summarizing old stories → reduces token count but may lose discriminative detail
- Failure signatures: Accuracy plateaus then drops as stories exceed ~6 in buffer; Response time increases linearly with number of stories (token count); Inconsistent answers for same entity across stories
- First 3 experiments: 1) Measure attention score distribution for first vs. last story tokens as context grows; 2) Replace unique names with generic placeholders to test entity confusion; 3) Implement sliding window attention (keep only last N stories) and compare accuracy decay

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the model's performance degrade as the number of stories increases, and what is the relationship between the number of stories and the token limit?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that there are limitations on the number of tokens the model can receive, but it doesn't provide a detailed analysis of the relationship between the number of stories and the token limit.
- What evidence would resolve it: A study that systematically varies the number of stories and measures the model's performance and token usage.

Open Question 2
- Question: How effective are different summarization techniques in mitigating the interference problem, and what are the best practices for summarizing information in the context?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that summarizing or removing old stories does not mitigate the interference problem effectively, but it doesn't provide a detailed analysis of different summarization techniques.
- What evidence would resolve it: A study that compares the effectiveness of different summarization techniques in reducing interference and improving the model's performance.

Open Question 3
- Question: How does the interference problem differ between in-context learning and traditional fine-tuning, and what are the implications for model design and training?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that the interference problem is different from catastrophic forgetting, but it doesn't provide a detailed comparison between in-context learning and traditional fine-tuning.
- What evidence would resolve it: A study that compares the interference problem in in-context learning and traditional fine-tuning and analyzes the implications for model design and training.

Open Question 4
- Question: How does the interference problem affect the model's ability to reason and generalize to new tasks, and what are the implications for real-world applications?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that the interference problem can reduce the model's performance, but it doesn't provide a detailed analysis of how it affects the model's ability to reason and generalize to new tasks.
- What evidence would resolve it: A study that evaluates the model's performance on reasoning and generalization tasks and analyzes the impact of the interference problem on real-world applications.

## Limitations

- The study lacks direct empirical evidence measuring attention score distributions or embedding overlaps across stories, relying on correlational rather than causal analysis.
- The simplified bAbI dataset may not generalize to more complex real-world scenarios, limiting the external validity of the findings.
- The claim that summarizing or removing old stories does not mitigate interference is based on limited interventions without exploring alternative memory management strategies.

## Confidence

**High Confidence**: The empirical observation that accuracy degrades as more stories are added to the context is well-supported by the presented data (accuracy drops from 100% with one story to ~75% with eight stories). The methodology for constructing the benchmark and measuring performance is clear and reproducible.

**Medium Confidence**: The interpretation that this degradation represents "interference" between old and new knowledge is reasonable but not definitively proven. Alternative explanations, such as simple token limit truncation or attention dilution due to fixed softmax normalization, could account for the results. The claim that simplifying stories improves accuracy is supported but the improvement may be partly due to reduced token count rather than reduced semantic complexity.

**Low Confidence**: The assertion that summarizing or removing old stories does not mitigate interference is based on a single intervention strategy. Without testing alternative approaches (e.g., weighted attention, story compression, or adaptive context windows), this conclusion is premature. The mechanism of "confusing or erasing previous facts" is described qualitatively but lacks quantitative support from attention or embedding analyses.

## Next Checks

1. Measure and visualize the self-attention weights for tokens from the first story versus the last story as context grows, quantifying how attention mass shifts from early to late tokens and correlating this with accuracy degradation.

2. Compute cosine similarity between entity and action embeddings across different stories to determine whether semantically similar actions have overlapping representations that could cause confusion.

3. Implement and compare different strategies for handling context growth, such as weighted attention for older stories, adaptive context compression, or selective retention based on question relevance, measuring whether these approaches outperform simple truncation or summarization.