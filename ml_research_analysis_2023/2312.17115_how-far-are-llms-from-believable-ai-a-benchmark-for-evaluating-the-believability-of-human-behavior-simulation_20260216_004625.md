---
ver: rpa2
title: How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability
  of Human Behavior Simulation
arxiv_id: '2312.17115'
source_url: https://arxiv.org/abs/2312.17115
tags:
- agents
- profile
- human
- llms
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a benchmark for evaluating the believability
  of large language models (LLMs) simulating human behavior. The authors propose two
  key metrics: consistency, measuring how accurately LLMs depict character information,
  and robustness, assessing how well LLMs handle perturbations in the input profile.'
---

# How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation

## Quick Facts
- arXiv ID: 2312.17115
- Source URL: https://arxiv.org/abs/2312.17115
- Reference count: 17
- Key outcome: Introduces a benchmark (SimulateBench) with 65 character profiles and 8,400 questions to evaluate LLM believability, finding current models struggle with consistency and robustness when simulating human behavior.

## Executive Summary
This paper addresses the critical challenge of evaluating whether large language models can convincingly simulate human behavior, a capability essential for applications ranging from entertainment to customer service. The authors introduce SimulateBench, a comprehensive benchmark that measures two key aspects of believability: consistency (how accurately models depict character information) and robustness (how well models handle perturbations to input profiles). Through systematic evaluation of 10 widely used LLMs, the study reveals significant limitations in current models' ability to maintain character consistency and resist profile perturbations, highlighting fundamental challenges in creating truly believable AI agents.

## Method Summary
The benchmark employs a structured approach where LLMs are prompted with character profiles and instructed to simulate behavior through answering single-choice questions. Consistency is measured by accuracy in answering questions about the assigned character, while robustness is assessed by comparing consistency scores across perturbed versions of the profile. The evaluation uses 65 character profiles with demographic variations and 8,400 questions covering immutable characteristics, social roles, and relationships. Five prompting strategies are tested, including zero-shot, few-shot, chain-of-thought, and self-ask approaches, to examine their impact on believability across commercial and open-source models.

## Key Results
- Current LLMs exhibit poor consistency due to "Simulate Hallucination" - generating unsupported responses even for straightforward questions
- Models show significant vulnerability to profile perturbations, with robustness scores indicating sensitivity to demographic changes
- Demographic factors and information position in profiles substantially influence believability, with age and race showing particular impact
- API-based commercial models generally outperform open-source alternatives, though context size doesn't guarantee better performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The benchmark works because it evaluates consistency through structured, answerable questions rather than subjective human judgment.
- **Mechanism**: The benchmark uses a single-choice question format where gold answers are predetermined. This allows for reproducible, objective measurement of whether the LLM-generated responses align with the character profile.
- **Core assumption**: The character profiles are comprehensive and accurate, and the questions are well-designed to capture key aspects of the character.
- **Evidence anchors**:
  - [abstract] "SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs' simulated behaviors."
  - [section] "To measure the consistency, we assess whether the agent can correctly answer single-choice questions about the character that the agent is assigned to simulate in the consistency dataset."
- **Break condition**: If character profiles are incomplete or questions are poorly constructed, the benchmark cannot accurately measure consistency.

### Mechanism 2
- **Claim**: The benchmark captures LLM vulnerabilities through profile perturbations.
- **Mechanism**: By systematically modifying demographic factors in character profiles (age, race, surname, education), the benchmark can measure how sensitive LLMs are to input changes. This reveals robustness issues that wouldn't be apparent with static profiles.
- **Core assumption**: The perturbations are meaningful and the LLM responses change significantly when perturbed profiles are used.
- **Evidence anchors**:
  - [abstract] "robustness: the ability of LLMs' simulated behaviors to remain robust when faced with perturbations"
  - [section] "To measure the robustness, we perturb the profile of the assigned character and compare how the agent's consistency ability changed faced with the profile perturbation."
- **Break condition**: If LLM responses are largely invariant to profile changes, the perturbation approach won't reveal meaningful differences in robustness.

### Mechanism 3
- **Claim**: The benchmark identifies model-specific biases through comparative analysis.
- **Mechanism**: By evaluating multiple LLMs on the same benchmark, the framework can reveal systematic differences in how models handle consistency and robustness. This allows for identification of model-specific weaknesses.
- **Core assumption**: The evaluated LLMs have sufficient architectural and training differences to exhibit varied performance patterns.
- **Evidence anchors**:
  - [section] "We comprehensively assess 10 LLMs, including commercial models utilizing APIs and open-source models."
  - [section] "API-based models perform better than open-source models, even if they are equipped with long context size"
- **Break condition**: If all evaluated models perform similarly, the comparative approach won't reveal meaningful differences.

## Foundational Learning

- **Concept**: Single-choice question design
  - **Why needed here**: The benchmark relies on structured questions with predetermined correct answers to objectively measure consistency
  - **Quick check question**: What are the two categories of questions based on their gold answers, and what weights are assigned to each?

- **Concept**: Coefficient of variation (CV)
  - **Why needed here**: CV is used as the metric for robustness, measuring how much the consistency score varies when the profile is perturbed
  - **Quick check question**: How is the robustness performance (RCoV) calculated from the consistency scores of perturbed profiles?

- **Concept**: In-context learning with different prompting strategies
  - **Why needed here**: The benchmark tests five combinations of prompting strategies (Zero, Zero+CoT, Few, Few+CoT, Few+Self-Ask) to evaluate their impact on believability
  - **Quick check question**: Which prompting strategy consistently improves performance across all models for simulating human behavior?

## Architecture Onboarding

- **Component map**: Profile Descriptive Framework -> Character Dataset (56 profiles) -> Consistency Dataset (8,400 questions) -> Robustness Dataset (perturbed profiles) -> Evaluation pipeline

- **Critical path**: Load character profile → Apply instruction prompt template with profile → Generate response using LLM → Answer single-choice questions → Calculate consistency score (CA) → Repeat with perturbed profiles → Calculate robustness score (RCoV)

- **Design tradeoffs**: Single-choice questions vs. open-ended responses (reproducibility vs. realism) | Perturbation granularity (sensitivity vs. computational cost) | Number of models evaluated (comprehensive vs. focused insights)

- **Failure signatures**: High variance in consistency scores across similar characters | Low sensitivity to profile perturbations | Inconsistent performance across different prompting strategies

- **First 3 experiments**: Evaluate a single LLM on one character's consistency dataset | Compare consistency scores for a character and its age-variant | Test the impact of reversing profile information order on consistency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different prompt engineering strategies affect the believability of LLM-based agents?
- **Basis in paper**: Explicit - The paper mentions using various prompt combinations (Zero, Zero+CoT, Few, Few+CoT, Few+Self-Ask) and their impact on agent believability.
- **Why unresolved**: The paper found that no single prompt combination consistently improved believability across all models. The reasons behind this variability are not fully explored.
- **What evidence would resolve it**: Systematic experiments comparing different prompt engineering strategies across a wider range of LLMs and tasks, coupled with analysis of the underlying mechanisms.

### Open Question 2
- **Question**: To what extent do demographic factors in the character profile influence the consistency and robustness of LLM-based agents?
- **Basis in paper**: Explicit - The paper found that agents exhibit preferences for specific demographic factors, but the reasons behind this are not fully understood.
- **Why unresolved**: The paper identifies correlations between demographic factors and agent performance, but does not establish causality or explore the underlying mechanisms.
- **What evidence would resolve it**: Experiments that systematically vary demographic factors in character profiles and measure the resulting changes in agent behavior, along with analysis of the LLMs' training data and biases.

### Open Question 3
- **Question**: How can we improve the robustness of LLM-based agents to profile perturbations?
- **Basis in paper**: Explicit - The paper found that agents are vulnerable to profile perturbations, but does not explore methods for improving robustness.
- **Why unresolved**: The paper identifies the problem of robustness but does not propose solutions or investigate the underlying causes of this vulnerability.
- **What evidence would resolve it**: Experiments that test different techniques for improving robustness, such as adversarial training or data augmentation, along with analysis of the LLMs' internal representations and decision-making processes.

## Limitations
- Benchmark effectiveness depends on quality and comprehensiveness of character profiles, which may not capture full complexity of human behavior
- Perturbation analysis focuses primarily on demographic factors, potentially missing other important dimensions of profile variation
- Findings may not generalize across all domains of human behavior simulation beyond TV dramas and real people profiles

## Confidence

- **High confidence**: Benchmark methodology for measuring consistency through single-choice questions with predetermined gold answers is technically sound and reproducible
- **Medium confidence**: Robustness metric (RCoV) effectively captures LLM sensitivity to profile perturbations, though demographic perturbations may not represent all meaningful variations
- **Low confidence**: Generalizability of findings across different domains of human behavior simulation due to focus on specific character types

## Next Checks

1. **Profile completeness validation**: Test whether expanding character profiles with additional contextual information (hobbies, preferences, past experiences) significantly impacts consistency scores, helping determine if the current 3,277-token limit is sufficient.

2. **Cross-domain benchmarking**: Apply the benchmark framework to character profiles from different domains (historical figures, fictional characters from various genres, professional personas) to test generalizability of the consistency and robustness findings.

3. **Alternative perturbation schemes**: Implement and compare robustness measurements using non-demographic perturbations (e.g., relationship status changes, career shifts, temporal shifts) to determine if demographic factors are the most sensitive dimensions for LLM behavior simulation.