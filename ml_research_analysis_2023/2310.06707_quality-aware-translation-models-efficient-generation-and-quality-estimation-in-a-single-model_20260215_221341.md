---
ver: rpa2
title: 'Quality-Aware Translation Models: Efficient Generation and Quality Estimation
  in a Single Model'
arxiv_id: '2310.06707'
source_url: https://arxiv.org/abs/2310.06707
tags:
- quality
- decoding
- translation
- quality-aware
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces quality-aware translation models that integrate
  quality estimation into neural machine translation systems. The key innovation is
  training NMT models to simultaneously predict translations and assess their own
  quality, eliminating the need for separate quality estimation models during decoding.
---

# Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model

## Quick Facts
- arXiv ID: 2310.06707
- Source URL: https://arxiv.org/abs/2310.06707
- Reference count: 37
- Trains NMT models to predict both translations and quality scores, reducing MBR candidate list size by two orders of magnitude

## Executive Summary
This paper introduces quality-aware translation models that integrate quality estimation directly into neural machine translation systems. The key innovation is training NMT models to simultaneously predict translations and assess their own quality, eliminating the need for separate quality estimation models during decoding. The approach demonstrates significant improvements across multiple metrics and language pairs, with human evaluations confirming the gains. By making translation models self-aware of their output quality, the research enables both higher-quality translations and dramatically faster inference compared to traditional methods.

## Method Summary
The authors propose two approaches: Quality-Aware Prompting, where quality tokens are appended to source sentences to prompt high-quality translations, and Quality-Aware Prediction, where models learn to predict both translations and quality scores. During training, translation quality scores are computed using Bleurt-QE and discretized into bins mapped to single tokens. The model is then trained with these quality tokens appended to either source (prompting) or target (prediction) sentences. During inference, QA Prompting enables single-pass high-quality generation, while QA Prediction enables quality-aware reranking and MBR decoding. The approach significantly reduces the required candidate list size for MBR decoding while maintaining or improving translation quality.

## Key Results
- Quality-aware models reduce MBR candidate list size by two orders of magnitude while maintaining translation quality
- Quality-aware prompting achieves results comparable to external quality reranking but with single-pass efficiency
- Human evaluations (MQM) show significant improvements across multiple language pairs (English→German, English→Japanese)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns to distinguish between different quality levels of its own translations.
- Mechanism: During training, the model receives discretized quality scores for each source-target pair. It learns to associate these quality tokens with the input and output, enabling it to predict quality scores during inference.
- Core assumption: The model can effectively learn the mapping between input patterns and quality scores when trained with labeled data.
- Evidence anchors: [abstract]: "We propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output." [section 5.1]: "Figure 1a shows a histogram with the distribution of the ground truth QE scores... We observe that the model... performs well in assigning samples with high ground truth values to the highest quality score bin."

### Mechanism 2
- Claim: Quality-aware prompting guides the model to generate higher-quality translations by leveraging learned associations between quality tokens and output quality.
- Mechanism: By appending the highest quality token to the source sentence during decoding, the model is prompted to generate translations that align with the learned association between that token and high-quality outputs.
- Core assumption: The model has learned a strong association between the quality token and the features of high-quality translations during training.
- Evidence anchors: [abstract]: "Quality-Aware Prompting, where quality tokens are appended to source sentences to prompt high-quality translations." [section 3.3.2]: "At translation time we append the token corresponding to the highest quality level to the sentence to translate to prompt the system to generate a sentence of the highest quality."

### Mechanism 3
- Claim: Quality-aware prediction enables the model to function as its own quality estimation model, eliminating the need for external QE models during decoding.
- Mechanism: The model is trained to predict both a translation and a quality score. During inference, it can directly output quality scores for its translations, which can be used for reranking or MBR decoding.
- Core assumption: The model can learn to accurately predict quality scores for its own translations without access to references or external QE models.
- Evidence anchors: [abstract]: "Quality-Aware Prediction, where models learn to predict both translations and quality scores." [section 5.1]: "Figure 1b, where we show the distribution of ground truth scores... across the predicted bins. The correlation coefficients between the ground truth QE scores and our model's predicted scores are as follows: 0.73 for Pearson, 0.72 for Spearman's rank, and 0.58 for Kendall's Tau correlation."

## Foundational Learning

- Concept: Neural Machine Translation (NMT) basics
  - Why needed here: Understanding how NMT models work is fundamental to grasping how quality awareness can be integrated into them.
  - Quick check question: How does a transformer-based NMT model generate translations, and what is the role of the attention mechanism?

- Concept: Quality Estimation (QE) in MT
  - Why needed here: Quality-aware models rely on QE principles, so understanding QE metrics and methods is crucial.
  - Quick check question: What is the difference between reference-based and reference-free quality estimation metrics in MT?

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR decoding is a key application of quality-aware models, and understanding its mechanics is essential.
  - Quick check question: How does MBR decoding differ from MAP decoding, and why might it lead to better translation quality?

## Architecture Onboarding

- Component map: Transformer-based NMT model (encoder-decoder architecture) -> Quality score discretization module (equal mass binning) -> Quality-aware prompting module (appends quality tokens to source) -> Quality-aware prediction module (predicts quality scores for outputs) -> MBR decoding module (uses quality-aware model for candidate selection) -> Quality estimation model (Bleurt-QE for training data labeling)

- Critical path: 1. Train NMT model with quality scores appended to source (QA Prompting) or target (QA Prediction) 2. During inference, use QA Prompting for single-pass high-quality generation or QA Prediction for quality-aware reranking/MBR 3. For MBR decoding, use quality-aware model to generate and select high-quality candidates

- Design tradeoffs: Quality discretization granularity vs. model complexity, Training with full vs. filtered dataset (impact on quality awareness), Single-pass QA Prompting vs. multi-step QA Prediction for different use cases

- Failure signatures: Poor correlation between predicted and ground truth quality scores, Degradation in translation quality when using quality-aware approaches, No improvement in MBR decoding efficiency despite quality awareness

- First 3 experiments: 1. Train a quality-aware model (QA Prediction) and evaluate its ability to distinguish between quality levels (Section 5.1) 2. Compare QA Prompting and QA Prediction against baseline and external QE reranking (Section 5.2) 3. Evaluate the impact of quality-aware models on MBR decoding efficiency and quality (Section 5.3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality-aware prompting approach scale to larger language models with significantly more parameters?
- Basis in paper: [explicit] The authors mention evaluating a large language model (LLM) and showing results in the appendix, but do not provide detailed analysis of scalability.
- Why unresolved: The paper only shows results for a transformer-based model with 551M parameters and briefly mentions LLM results in the appendix without extensive discussion.
- What evidence would resolve it: Detailed experiments comparing quality-aware prompting across models of varying sizes (e.g., 1B, 10B, 100B+ parameters) with quantitative metrics would clarify scalability.

### Open Question 2
- Question: What is the optimal number of quality score bins for different language pairs and translation domains?
- Basis in paper: [explicit] The authors conduct a sensitivity analysis showing improvements when increasing bins from 2 to 5, but don't explore the optimal range for different scenarios.
- Why unresolved: The paper only tests bin numbers of 2, 3, 5, 10, and 20 on one language pair without examining how optimal bin count might vary across languages or domains.
- What evidence would resolve it: Systematic experiments varying bin counts across multiple language pairs and domains (e.g., news, legal, conversational) would identify optimal bin configurations.

### Open Question 3
- Question: How do quality-aware models perform when evaluated on human judgment rather than automated metrics?
- Basis in paper: [explicit] The authors conduct MQM human evaluations showing significant improvements, but only for selected experiments.
- Why unresolved: The paper relies primarily on automated metrics (Comet, MetricX) and only provides limited human evaluation data.
- What evidence would resolve it: Comprehensive human evaluation studies across multiple language pairs, domains, and translation scenarios would validate whether quality-aware improvements generalize to human preferences.

## Limitations
- Quality discretization approach may lose fine-grained information about translation quality through equal mass binning
- Approach relies heavily on the quality of external QE model (Bleurt-QE) used for training data labeling
- Limited scalability evidence beyond 551M parameter models, with only brief LLM results mentioned

## Confidence
- **High Confidence**: The core finding that quality-aware models can reduce MBR candidate list size by two orders of magnitude while maintaining quality is well-supported by experimental results
- **Medium Confidence**: The claim that QA Prompting achieves results comparable to external QE reranking is supported by metric improvements but would benefit from more extensive ablation studies
- **Medium Confidence**: The efficiency gains for standard MAP decoding (single-pass) are demonstrated but limited to specific model sizes and language pairs

## Next Checks
1. Test quality-aware models on additional language pairs beyond English→German and English→Japanese to validate the approach's general applicability across diverse linguistic structures

2. Conduct detailed analysis of quality score calibration across different quality levels to identify potential biases or systematic errors in the quality prediction component

3. Systematically vary the number of quality bins to determine the optimal balance between preserving quality information and maintaining model efficiency