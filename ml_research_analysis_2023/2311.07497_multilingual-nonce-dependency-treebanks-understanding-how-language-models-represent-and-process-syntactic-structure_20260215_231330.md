---
ver: rpa2
title: 'Multilingual Nonce Dependency Treebanks: Understanding how Language Models
  represent and process syntactic structure'
arxiv_id: '2311.07497'
source_url: https://arxiv.org/abs/2311.07497
tags:
- data
- spud
- nonce
- than
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPUD (Semantically Perturbed Universal Dependencies),
  a framework for creating nonce treebanks for multilingual Universal Dependencies
  (UD) corpora. SPUD data satisfies syntactic argument structure, provides syntactic
  annotations, and ensures grammaticality via language-specific rules.
---

# Multilingual Nonce Dependency Treebanks: Understanding how Language Models represent and process syntactic structure

## Quick Facts
- arXiv ID: 2311.07497
- Source URL: https://arxiv.org/abs/2311.07497
- Authors: 
- Reference count: 40
- Key outcome: This paper introduces SPUD (Semantically Perturbed Universal Dependencies), a framework for creating nonce treebanks for multilingual Universal Dependencies (UD) corpora. SPUD data satisfies syntactic argument structure, provides syntactic annotations, and ensures grammaticality via language-specific rules. The authors create nonce data in Arabic, English, French, German, and Russian, and demonstrate two use cases of SPUD treebanks. First, they investigate the effect of nonce data on word co-occurrence statistics, as measured by perplexity scores of autoregressive (ALM) and masked language models (MLM). They find that ALM scores are significantly more affected by nonce data than MLM scores. Second, they show how nonce data affects the performance of syntactic dependency probes. They replicate the findings of Müller-Eberstein et al. (2022) on nonce test data and show that the performance declines on both MLMs and ALMs wrt. original test data. However, a majority of the performance is kept, suggesting that the probe indeed learns syntax independently from semantics.

## Executive Summary
This paper introduces SPUD (Semantically Perturbed Universal Dependencies), a framework for creating nonce treebanks for multilingual Universal Dependencies (UD) corpora. SPUD data satisfies syntactic argument structure, provides syntactic annotations, and ensures grammaticality via language-specific rules. The authors create nonce data in Arabic, English, French, German, and Russian, and demonstrate two use cases of SPUD treebanks. First, they investigate the effect of nonce data on word co-occurrence statistics, as measured by perplexity scores of autoregressive (ALM) and masked language models (MLM). They find that ALM scores are significantly more affected by nonce data than MLM scores. Second, they show how nonce data affects the performance of syntactic dependency probes. They replicate the findings of Müller-Eberstein et al. (2022) on nonce test data and show that the performance declines on both MLMs and ALMs wrt. original test data. However, a majority of the performance is kept, suggesting that the probe indeed learns syntax independently from semantics.

## Method Summary
The paper introduces SPUD, a framework for creating nonce treebanks from UD corpora. The method involves replacing content words while preserving their syntactic context (POS tag, dependency relation to head, and dependency relations to dependents) and ensuring grammaticality via language-specific rules. The authors create SPUD data for Arabic, English, French, German, and Russian. They then evaluate how this nonce data affects language model representations and performance on syntactic dependency parsing tasks. Specifically, they compute perplexity scores for autoregressive and masked language models, and train a structural probe (DepProbe) to evaluate syntactic knowledge in the models' representations.

## Key Results
- ALM perplexity scores are significantly more affected by SPUD data than MLM scores, suggesting that bidirectional context in MLMs provides more reliable syntactic cues.
- DepProbe performance declines on SPUD data compared to original data, but a majority of the performance is retained, indicating that the probe learns syntax independently of semantics.
- The performance drop on SPUD data is consistent across both ALMs and MLMs, suggesting that the probe's resilience to semantic perturbations is a general property rather than specific to a particular model architecture.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPUD data preserves syntactic structure while randomizing semantic content, enabling controlled evaluation of syntax vs. semantics.
- Mechanism: The algorithm replaces content words based on their syntactic context (POS tag, dependency relation to head, and dependency relations to dependents), ensuring the new sentence maintains grammatical structure while altering meaning.
- Core assumption: The syntactic context is sufficient to determine appropriate replacements that maintain grammaticality across languages.
- Evidence anchors:
  - [abstract]: "SPUD data satisfies syntactic argument structure, provides syntactic annotations, and ensures grammaticality via language-specific rules."
  - [section]: "The syntactic context of a token t is defined as (i) the UPOS tag of t, (iii) the dependency relation of t to its head, and (iv) the dependency relations of t to its dependents."
  - [corpus]: Weak - corpus neighbors discuss general dependency parsing and syntactic representations but do not specifically address the SPUD framework's mechanism for preserving syntax while randomizing semantics.
- Break condition: If the syntactic context fails to capture necessary morphosyntactic constraints (e.g., verb valency, case marking), replacements may produce ungrammatical sentences.

### Mechanism 2
- Claim: Masked language models (MLMs) are less affected by SPUD data than autoregressive language models (ALMs) due to bidirectional context providing syntactic cues.
- Mechanism: Bidirectional context in MLMs allows the model to use surrounding words from both directions to predict masked tokens, making it easier to identify syntactic properties even when semantic content is randomized.
- Core assumption: Bidirectional context provides more reliable syntactic cues than unidirectional context for predicting masked tokens.
- Evidence anchors:
  - [abstract]: "We find that ALM scores are significantly more affected by nonce data than MLM scores."
  - [section]: "Bidirectional context determines the syntactic properties of a predicted token to a larger degree than unidirectional context."
  - [corpus]: Weak - corpus neighbors discuss general MLM and ALM performance but do not specifically address differential effects of semantic randomization on these architectures.
- Break condition: If the bidirectional context in MLMs is not sufficient to disambiguate syntactic properties in highly randomized semantic contexts, the expected performance difference may not hold.

### Mechanism 3
- Claim: DepProbe performance declines on SPUD data but maintains majority performance, suggesting syntax is learned independently of semantics.
- Mechanism: The structural probe extracts syntactic information from LM representations by projecting them into a lower-dimensional space where vector distances correspond to dependency tree distances, and this process is less sensitive to semantic perturbations than direct likelihood scoring.
- Core assumption: The syntactic subspace learned by the probe captures structural information that is relatively independent of lexical semantics.
- Evidence anchors:
  - [abstract]: "We show that the performance declines on both MLMs and ALMs wrt. original test data. However, a majority of the performance is kept, suggesting that the probe indeed learns syntax independently from semantics."
  - [section]: "B ∈ Rdh×b projects the LM representation in a syntactic subspace with b < dh, where word vector distances mimic the distance between words in the tree."
  - [corpus]: Weak - corpus neighbors discuss general probing methods but do not specifically address the resilience of syntactic probes to semantic randomization.
- Break condition: If the probe relies heavily on semantic cues that are disrupted in SPUD data, the performance drop may be more severe than observed, suggesting syntax is not learned independently of semantics.

## Foundational Learning

- Concept: Dependency parsing and Universal Dependencies (UD)
  - Why needed here: The SPUD framework is built on UD treebanks, and the evaluation involves dependency probing, requiring understanding of syntactic dependencies and UD annotation.
  - Quick check question: What is the difference between a head and a dependent in a dependency relation, and how are they represented in UD?

- Concept: Language model architectures (MLM vs. ALM)
  - Why needed here: The paper compares how MLMs and ALMs handle SPUD data differently, requiring understanding of their architectural differences and how they process context.
  - Quick check question: How does bidirectional context in MLMs differ from unidirectional context in ALMs, and what implications does this have for predicting masked vs. sequential tokens?

- Concept: Perplexity and pseudo-perplexity scoring
  - Why needed here: The paper uses these metrics to evaluate how well LMs predict SPUD data compared to original data, requiring understanding of how these scoring functions are computed and interpreted.
  - Quick check question: How is perplexity calculated for ALMs, and how does pseudo-perplexity differ for MLMs in terms of the context available for predicting each token?

## Architecture Onboarding

- Component map: SPUD framework -> Scoring functions (PPL, PPP, PPPl2r) -> DepProbe. The SPUD framework takes UD treebanks as input and outputs nonce treebanks. Scoring functions take LM representations and SPUD/original data as input and output likelihood scores. DepProbe takes LM representations and dependency-annotated data as input and outputs predicted dependency trees.
- Critical path: Create SPUD treebanks -> Evaluate LMs using scoring functions -> Probe syntactic knowledge using DepProbe. Each component must be functional for the overall system to work.
- Design tradeoffs: The SPUD framework trades semantic coherence for syntactic preservation, enabling controlled evaluation of syntax vs. semantics. Scoring functions tradeoff between capturing overall sentence likelihood (PPL) and focusing on individual token predictability (pseudo-PPL). DepProbe trades model complexity for interpretability, using lightweight linear transformations to extract syntactic information.
- Failure signatures: If SPUD data is ungrammatical, it may not effectively isolate syntax from semantics. If scoring functions are not sensitive to semantic perturbations, they may not effectively evaluate LM performance on SPUD data. If DepProbe relies heavily on semantic cues, it may not effectively probe for syntax-independent knowledge.
- First 3 experiments:
  1. Create SPUD treebanks for a new language using the provided tutorial and evaluate their grammaticality with native speakers.
  2. Compare PPL and pseudo-PPL scores of an MLM and ALM on SPUD vs. original data for a single language to observe differential effects.
  3. Train DepProbe on an MLM and ALM using original data, then evaluate on SPUD data to observe performance drop and compare across languages.

## Open Questions the Paper Calls Out

- **Question**: Does the performance drop on SPUD data for structural probing vary systematically with the amount of semantic information retained in the perturbations?
  - **Basis in paper**: [explicit] The paper shows that SPUD data is grammatically correct but semantically nonce, and investigates how this affects probing performance across different model architectures and tasks.
  - **Why unresolved**: The paper demonstrates a performance drop on SPUD data but does not systematically analyze whether the degree of semantic perturbation (e.g., through different replacement strategies) correlates with the magnitude of the drop.
  - **What evidence would resolve it**: Experiments comparing probing performance across SPUD variants with controlled semantic perturbation levels (e.g., more vs. less semantic similarity between original and replaced words).

- **Question**: Is the difference in SPUD data effects between ALMs and MLMs consistent across different tokenization schemes and training data compositions?
  - **Basis in paper**: [inferred] The paper finds that ALMs are more affected by SPUD data than MLMs, but notes that these models differ in many aspects beyond architecture, including tokenization and training data.
  - **Why unresolved**: The observed differences could be due to architectural differences, tokenization effects, or training data properties rather than a fundamental difference in how ALMs and MLMs process syntax vs. semantics.
  - **What evidence would resolve it**: Experiments with models that vary only one factor at a time (e.g., same architecture but different tokenizers, or same tokenizer but different training data distributions).

- **Question**: How does the performance on SPUD data for structural probing relate to downstream parsing performance on nonce data?
  - **Basis in paper**: [inferred] The paper uses SPUD data to probe syntactic knowledge but does not test whether this knowledge transfers to actual parsing tasks on nonce sentences.
  - **Why unresolved**: Probing experiments measure the ability to decode syntactic structure from representations, but this may not directly translate to the ability to actually parse nonce sentences correctly.
  - **What evidence would resolve it**: Experiments finetuning parsers on SPUD data and evaluating their performance on nonce test sets, compared to their performance on original test sets.

## Limitations
- The framework's effectiveness depends on the assumption that syntactic context alone is sufficient to maintain grammaticality across diverse languages, but this may not hold for languages with complex morphosyntactic dependencies or free word order.
- The paper does not address how the SPUD framework handles rare or out-of-vocabulary words, which could affect the generalizability of the results.
- The confidence intervals for the observed performance drops in DepProbe and language model scoring are not provided, making it difficult to assess the statistical significance of the findings.

## Confidence
- **High Confidence**: The methodology for creating SPUD treebanks and the general observation that ALMs are more affected by semantic perturbations than MLMs are well-supported by the experimental results.
- **Medium Confidence**: The claim that DepProbe learns syntax independently of semantics is supported but could be strengthened with additional control experiments, such as ablating specific types of semantic information.
- **Low Confidence**: The generalizability of the SPUD framework to languages not covered in the study (Arabic, English, French, German, Russian) remains untested.

## Next Checks
1. **Cross-linguistic Validation**: Apply the SPUD framework to treebanks from languages with typologically distinct features (e.g., agglutinative languages like Turkish or Japanese) and verify that grammaticality is preserved.
2. **Statistical Significance Analysis**: Compute confidence intervals for the perplexity score differences and DepProbe performance drops to confirm that the observed effects are statistically significant.
3. **Control Experiment for DepProbe**: Train a probe that explicitly incorporates semantic information and compare its performance on SPUD data to the original DepProbe to isolate the contribution of syntax-independent learning.