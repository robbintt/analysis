---
ver: rpa2
title: An Evaluation of GPT-4 on the ETHICS Dataset
arxiv_id: '2309.10492'
source_url: https://arxiv.org/abs/2309.10492
tags:
- ethics
- gpt-4
- answer
- reasonable
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated GPT-4 on the ETHICS dataset covering justice,
  deontology, virtue ethics, utilitarianism, and commonsense ethics. GPT-4 achieved
  86% accuracy on justice, 88% on virtue ethics, 76% on deontology, 88% on utilitarianism,
  and 86% on commonsense morality short stories.
---

# An Evaluation of GPT-4 on the ETHICS Dataset

## Quick Facts
- arXiv ID: 2309.10492
- Source URL: https://arxiv.org/abs/2309.10492
- Reference count: 2
- GPT-4 achieves high accuracy on ETHICS dataset covering justice, deontology, virtue ethics, utilitarianism, and commonsense ethics

## Executive Summary
This study evaluates GPT-4's performance on the ETHICS dataset, which tests moral reasoning across five ethical frameworks: justice, deontology, virtue ethics, utilitarianism, and commonsense morality. GPT-4 significantly outperforms previous models, achieving high accuracy across all categories without explicit fine-tuning on the dataset. The study demonstrates that large language models can learn to predict human moral judgments through pretraining alone, and that embedding-based dynamic example selection further improves performance. However, the authors note important limitations regarding the gap between predicting moral judgments and actual ethical behavior in practice.

## Method Summary
The researchers tested GPT-4 on the ETHICS dataset using various prompt engineering techniques, both with and without training examples. They employed OpenAI's "ada-002" text embeddings to find semantically similar examples from the training set, using cosine similarity to dynamically select relevant examples for prompts. Performance was measured using accuracy across the five ethical frameworks, comparing results against random baselines ranging from 6.25% to 50%.

## Key Results
- GPT-4 achieved 86% accuracy on justice, 88% on virtue ethics, 76% on deontology, 88% on utilitarianism, and 86% on commonsense morality short stories
- Performance significantly exceeds previous models and random baselines
- Dynamic example selection using embeddings improved accuracy by up to 16% compared to static selection
- GPT-4 performs well without training examples, suggesting it learned shared human values during pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's high performance on the ETHICS dataset stems from its ability to learn shared human values through large-scale pretraining, rather than from memorization of the specific dataset.
- Mechanism: The model's pretraining on diverse web data exposed it to common moral intuitions and reasoning patterns, allowing it to generalize to the ETHICS dataset's scenarios without explicit fine-tuning on it.
- Core assumption: The ETHICS dataset's scenarios represent common moral intuitions that would be present in web-scale pretraining data.
- Evidence anchors:
  - [abstract] GPT-4 significantly outperforms previous models on the ETHICS dataset without using training examples in prompts.
  - [section] The paper notes that GPT-4 was not trained on the ETHICS dataset specifically.
  - [corpus] Related work on LLM moral reasoning also finds strong performance without explicit fine-tuning.
- Break condition: If the ETHICS dataset contains highly unusual moral scenarios not present in pretraining data, GPT-4's performance would degrade.

### Mechanism 2
- Claim: The dynamic prompt refinement method using embeddings significantly improves GPT-4's performance by providing relevant context.
- Mechanism: By finding semantically similar examples from the training set using embeddings, the prompts give GPT-4 additional relevant context to reason from.
- Core assumption: The cosine similarity between embeddings captures semantic similarity relevant to moral reasoning.
- Evidence anchors:
  - [section] Table 1 shows dynamic example selection achieving 86% accuracy vs 70% with static selection.
  - [section] The paper uses OpenAI's text embedding "ada-002" with cosine similarity for example selection.
  - [corpus] Related work on SimPrompting also finds embedding-based example selection improves performance.
- Break condition: If the embedding space doesn't capture the relevant aspects of moral reasoning, the dynamic selection won't help.

### Mechanism 3
- Claim: GPT-4's performance gap between predicting moral judgments and practical ethical behavior stems from the brittleness of its reasoning.
- Mechanism: Small changes in wording or framing can significantly alter GPT-4's moral judgments, making it vulnerable to adversarial manipulation.
- Core assumption: The large performance differences between prompts (e.g., "doing the right thing" vs "whether someone is an asshole") indicate brittleness.
- Evidence anchors:
  - [section] GPT-4 achieves 95% accuracy on short stories with one prompt but only 78% with a slightly different prompt.
  - [section] The paper discusses how performance varies significantly based on small wording changes.
  - [corpus] Related work on LLM robustness also finds sensitivity to prompt wording.
- Break condition: If the performance differences are due to other factors (e.g., prompt clarity rather than brittleness), the mechanism wouldn't hold.

## Foundational Learning

- Concept: Embedding similarity measures
  - Why needed here: The dynamic prompt refinement method relies on finding semantically similar examples using embeddings and cosine similarity.
  - Quick check question: What is the mathematical formula for cosine similarity between two vectors?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: The paper mentions GPT-4 is a MoE model, which may explain its strong performance through ensemble effects.
  - Quick check question: How does a Mixture-of-Experts architecture differ from a standard transformer?

- Concept: Prompt engineering
  - Why needed here: The paper extensively discusses different prompt formulations and their impact on performance.
  - Quick check question: What are the key components of an effective prompt for moral reasoning tasks?

## Architecture Onboarding

- Component map:
  Input processing -> Embedding generation -> Similarity matching -> Prompt construction -> GPT-4 inference

- Critical path:
  1. Load ETHICS dataset example
  2. Generate embeddings for all training examples
  3. Compute cosine similarity to find most similar examples
  4. Construct prompt with selected examples
  5. Send prompt to GPT-4 API
  6. Parse and evaluate GPT-4's response

- Design tradeoffs:
  - Using dynamic vs static example selection: Dynamic selection is more effective but computationally expensive
  - Number of examples to include: More examples provide more context but increase prompt length
  - Prompt wording: Different formulations significantly impact performance but optimal wording is task-dependent

- Failure signatures:
  - Poor similarity matching leading to irrelevant examples in prompts
  - GPT-4 API errors or rate limiting
  - Incorrect parsing of GPT-4's responses
  - Performance degradation when examples are removed from prompts

- First 3 experiments:
  1. Replicate the justice dataset results with 3 vs 50 training examples using static selection
  2. Test the dynamic selection method on a new ethical reasoning dataset
  3. Systematically vary the number of examples in prompts to find the optimal balance

## Open Questions the Paper Calls Out

The paper itself doesn't explicitly call out open questions, but based on the discussion section, it identifies the substantial gap between predicting moral judgments and acting ethically in practice as a key limitation that requires further research.

## Limitations

- Performance was evaluated primarily on a single dataset (ETHICS) without external validation on alternative benchmarks
- The study tested only a subset of the ETHICS dataset due to API costs, potentially limiting generalizability
- The evaluation focused on predicting moral judgments rather than measuring actual ethical behavior in real-world contexts

## Confidence

- **High Confidence**: GPT-4 outperforms previous models on the ETHICS dataset across all tested scenarios
- **Medium Confidence**: Embedding-based dynamic example selection significantly improves performance
- **Medium Confidence**: GPT-4 can learn to predict human moral judgments through pretraining alone
- **Low Confidence**: The performance gap between judgment prediction and practical ethical behavior will persist in real-world applications

## Next Checks

1. Test GPT-4's performance on additional moral reasoning benchmarks (e.g., Moral Stories, Moral Scenarios) to validate generalizability beyond the ETHICS dataset
2. Conduct ablation studies to determine the minimum number of training examples needed for the dynamic selection method to remain effective
3. Design experiments that measure GPT-4's actual ethical behavior (through actions/decisions) rather than just moral judgment prediction