---
ver: rpa2
title: Optimistic Multi-Agent Policy Gradient
arxiv_id: '2311.01953'
source_url: https://arxiv.org/abs/2311.01953
tags:
- policy
- tasks
- methods
- optimism
- optimistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of relative overgeneralization
  (RO) in multi-agent reinforcement learning, where agents converge to suboptimal
  joint policies due to overfitting to suboptimal behavior of other agents. The authors
  propose a general framework to enable optimistic updates in multi-agent policy gradient
  (MAPG) methods to alleviate the RO problem.
---

# Optimistic Multi-Agent Policy Gradient

## Quick Facts
- arXiv ID: 2311.01953
- Source URL: https://arxiv.org/abs/2311.01953
- Reference count: 40
- Outperforms strong baselines on 13 out of 19 tested tasks in MARL benchmarks

## Executive Summary
This paper addresses the problem of relative overgeneralization (RO) in multi-agent reinforcement learning, where agents converge to suboptimal joint policies due to overfitting to suboptimal behavior of other agents. The authors propose a general framework to enable optimistic updates in multi-agent policy gradient (MAPG) methods by applying a Leaky ReLU function to reshape advantage values during policy updates. The method is instantiated as OptiMAPPO based on MAPPO and demonstrates clear improvements across matrix games, Multi-agent MuJoCo, and Overcooked benchmarks, outperforming strong baselines on 13 out of 19 tasks.

## Method Summary
The method applies a Leaky ReLU function to advantage values during policy updates in MAPG methods. By transforming negative advantages to zero (when η = 0), the approach prevents agents from being penalized for actions that appear suboptimal due to other agents' current suboptimality. This creates optimism toward potentially optimal actions that are currently masked by coordination failures. The framework preserves optimal policies as fixed points while enabling exploration of actions with temporarily negative advantages, and leverages on-policy advantage estimation to naturally avoid the overestimation problems that plague optimistic Q-learning approaches.

## Key Results
- OptiMAPPO outperforms strong baselines on 13 out of 19 tested tasks
- Shows clear improvements in complex domains like MA-MuJoCo
- Demonstrates effectiveness of optimism in mitigating the RO problem in MAPG methods
- Matches baseline performance on the remaining 6 tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optimistic updates through Leaky ReLU advantage shaping prevent premature convergence to suboptimal joint policies in multi-agent settings.
- **Mechanism**: The Leaky ReLU function (with negative slope η = 0) transforms negative advantage values to zero, removing penalties for actions that appear suboptimal due to other agents' current suboptimality. This creates optimism toward potentially optimal actions that are currently masked by coordination failures.
- **Core assumption**: In cooperative tasks, negative advantages for individual actions often result from suboptimal behavior of other agents rather than inherent action suboptimality.
- **Break condition**: If negative advantages genuinely reflect action suboptimality (rather than coordination issues), optimism could delay learning optimal policies.

### Mechanism 2
- **Claim**: The advantage reshaping preserves optimality at fixed points while enabling exploration of actions with temporarily negative advantages.
- **Mechanism**: By transforming advantages with Leaky ReLU, the method creates a new improvement operator ILR_V that maintains π* as a fixed point. This allows exploration of actions that appear suboptimal in the short term while still converging to globally optimal policies.
- **Core assumption**: The optimal policy π* remains a fixed point under the transformed advantage operator when negative slope η = 0.
- **Break condition**: If the transformation alters the relative ordering of advantages in a way that changes the basin of attraction for optimal policies.

### Mechanism 3
- **Claim**: On-policy advantage estimation through MAPG methods naturally avoids the overestimation problem that plagues optimistic Q-learning approaches.
- **Mechanism**: Unlike Q-learning based optimism which can amplify overestimation with function approximation, the on-policy nature of MAPG advantage estimation prevents this issue while still benefiting from optimistic updates.
- **Core assumption**: On-policy estimation is inherently more stable than off-policy Q-learning estimation when optimism is introduced.
- **Break condition**: If the on-policy advantage estimation still suffers from bias or variance issues that could be amplified by optimism.

## Foundational Learning

- **Concept: Relative Overgeneralization (RO) in MARL**
  - Why needed here: Understanding RO is essential because the entire paper addresses this specific pathology where agents converge to suboptimal joint policies.
  - Quick check question: In a cooperative task, if agent A's optimal action yields low return because agent B is acting suboptimally, what phenomenon describes A's convergence to a suboptimal policy based on this experience?

- **Concept: Policy Gradient Methods and Advantage Estimation**
  - Why needed here: The method builds on MAPG foundations and modifies advantage estimation, so understanding standard policy gradient mechanics is crucial.
  - Quick check question: How does the generalized advantage estimator (GAE) combine TD errors to reduce variance while maintaining bias control in policy gradient methods?

- **Concept: Function Approximation and Overestimation in RL**
  - Why needed here: The paper contrasts its approach with optimistic Q-learning, which suffers from overestimation with function approximation.
  - Quick check question: Why does function approximation in Q-learning lead to overestimation, and how does this differ from on-policy value estimation?

## Architecture Onboarding

- **Component map**: Centralized critic (state value function V(s)) → Advantage calculation for each agent → Leaky ReLU transformation → Policy update for each agent
- **Critical path**: State → Value estimation → Advantage calculation → Leaky ReLU transformation → Policy update
- **Design tradeoffs**: Full optimism (η = 0) provides stronger mitigation of RO but may delay learning in stochastic environments; partial optimism (η > 0) offers a balance but requires hyperparameter tuning
- **Failure signatures**: If the method fails to outperform baselines, potential causes include insufficient exploration in deterministic environments, the RO problem not being severe enough in the tested tasks, or the Leaky ReLU transformation removing too much information
- **First 3 experiments**:
  1. Verify the basic RO phenomenon on climbing/penalty matrix games with standard MAPPO vs OptiMAPPO
  2. Test ablation study by varying η values on a simple task to find the optimal degree of optimism
  3. Compare performance against hysteretic DQN on discrete action tasks to confirm the overestimation advantage of the MAPG approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the degree of optimism (η) in OptiMAPPO affect its performance on complex continuous control tasks like MA-MuJoCo?
- Basis in paper: The paper mentions that OptiMAPPO uses a Leaky ReLU function with a negative slope η to reshape advantages, and in experiments, η is set to 0 for full optimism. An ablation study (Section 5.4) explores different η values.
- Why unresolved: The paper only tests a limited range of η values (0.2, 0.5, 0.8) on two tasks (HalfCheetah 6x1 and Random 3). The optimal η might vary depending on the task complexity and reward structure.
- What evidence would resolve it: Extensive experiments varying η across a wider range and on more diverse MA-MuJoCo tasks would reveal the relationship between η and performance.

### Open Question 2
- Question: Can the optimistic updating approach in OptiMAPPO be extended to other policy gradient methods beyond MAPPO?
- Basis in paper: The paper states that OptiMAPPO is instantiated based on MAPPO but the framework can be applied to other advantage actor-critic (A2C) based MARL methods.
- Why unresolved: The paper only evaluates OptiMAPPO on MAPPO and does not provide theoretical analysis or empirical results for other A2C methods.
- What evidence would resolve it: Implementing and testing OptiMAPPO with other A2C methods like MADDPG or IPPO would demonstrate the generalizability of the approach.

### Open Question 3
- Question: How does OptiMAPPO handle tasks with misleading stochastic rewards where full optimism might lead to suboptimal solutions?
- Basis in paper: The paper mentions that full optimism might risk converging to suboptimal solutions in tasks with misleading stochastic rewards and suggests that OptiMAPPO can adapt η to balance optimism and neutrality.
- Why unresolved: The paper does not provide empirical results on tasks with misleading stochastic rewards or a detailed strategy for adapting η.
- What evidence would resolve it: Experiments on tasks with misleading stochastic rewards and a method for dynamically adjusting η based on the reward structure would address this limitation.

## Limitations

- Theoretical analysis of fixed-point optimality is limited to tabular cases and may not extend to function approximation settings
- Limited analysis of how the Leaky ReLU transformation affects learning dynamics beyond preventing premature convergence
- The optimal choice of η remains heuristic without principled guidance

## Confidence

- High confidence: The empirical effectiveness of OptiMAPPO in overcoming RO compared to baselines (demonstrated across 19 tasks)
- Medium confidence: The theoretical guarantee of preserving optimal policies as fixed points (proven for tabular case but not extended to function approximation)
- Low confidence: The generality of optimism as a solution across all MARL coordination challenges (limited to specific RO problem)

## Next Checks

1. Conduct ablation studies varying the negative slope η across a broader range of tasks to characterize the tradeoff between optimism and stability
2. Test OptiMAPPO in stochastic environments where negative advantages may genuinely reflect action suboptimality to identify break conditions
3. Extend theoretical analysis to function approximation settings and characterize conditions under which fixed-point optimality is preserved