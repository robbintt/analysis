---
ver: rpa2
title: Resolving References in Visually-Grounded Dialogue via Text Generation
arxiv_id: '2309.13430'
source_url: https://arxiv.org/abs/2309.13430
tags:
- referent
- dialogue
- image
- language
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generative approach to resolving references
  in visually-grounded dialogue using a causal large language model (LLM) fine-tuned
  to generate referent descriptions. The model generates descriptions conditioned
  on the linguistic context of references, which are then used by a pretrained vision-language
  model (VLM) for zero-shot referent identification.
---

# Resolving References in Visually-Grounded Dialogue via Text Generation

## Quick Facts
- arXiv ID: 2309.13430
- Source URL: https://arxiv.org/abs/2309.13430
- Reference count: 21
- Primary result: Generative approach to reference resolution in visually-grounded dialogue achieves text-image retrieval accuracy exceeding baselines, with larger context windows showing potential for higher performance

## Executive Summary
This paper presents a novel approach to resolving references in visually-grounded dialogue by fine-tuning a causal large language model (LLM) to generate referent descriptions conditioned on linguistic context. The generated descriptions are then used with a pretrained vision-language model (VLM) for zero-shot text-image retrieval. The method is evaluated on a manually annotated dataset of dialogues from a collaborative image ranking task, demonstrating that the generative approach outperforms several baselines including random chance, mention-only retrieval, substitution, and coreference resolution. The authors find that using larger context windows for generating descriptions shows promise for improving performance.

## Method Summary
The proposed method involves fine-tuning a causal LLM (GPT-2 or GPT-3) on dialogue data to generate referent descriptions given marked mentions and contextual information. The fine-tuning process uses special tokens to mark mentions within their linguistic context, requiring the model to implicitly learn coreference resolution without explicit supervision. After fine-tuning, the LLM generates descriptions for mentions in test dialogues. These descriptions are then encoded by a pretrained VLM and compared to image encodings via matrix-vector multiplication, with the highest-scoring image selected as the referent. The approach is evaluated using five-fold cross-validation with varying context window sizes (3, 7, 13 utterances, or full dialogue) and multiple VLM models.

## Key Results
- The generative approach achieves text-image retrieval accuracy that exceeds random chance, mention-only retrieval, substitution, and coreference resolution baselines
- Ground truth labels based on the full dialogue achieve up to 83% accuracy with BLIP-2
- Using larger context windows for generating descriptions shows potential for higher performance, with 75% accuracy for context window 13 using BLIP-2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The causal LLM can implicitly learn coreference resolution without explicit supervision by conditioning on the linguistic context of marked mentions.
- Mechanism: During fine-tuning, the model observes mentions marked with special tokens embedded in their linguistic context. By predicting referent descriptions that summarize the coreferential information in the context, the model must implicitly resolve which earlier mentions refer to the same entity as the current mention.
- Core assumption: The linguistic context provided contains sufficient information for the model to disambiguate coreferences without needing explicit coreference labels.
- Evidence anchors:
  - [abstract]: "For the purpose of this study, we assume mention detection to be solved. As it stands, using this framework in production requires a separate model to propose candidate mentions at the span level."
  - [section 3.1]: "In order to produce accurate referent descriptions, the CRDG must implicitly learn to perform coreference resolution as we do not provide explicit supervision for this subtask."
- Break condition: If the linguistic context is insufficient to disambiguate coreferences (e.g., when multiple entities match the description), or if the model cannot learn the implicit patterns from the training data.

### Mechanism 2
- Claim: Using generated referent descriptions with a pretrained VLM enables zero-shot text-image retrieval that outperforms baselines.
- Mechanism: The fine-tuned LLM generates descriptive text for mentions based on dialogue context. This text is then encoded by the VLM's text encoder and compared to image encodings via matrix-vector multiplication. The image with the highest similarity score is selected as the referent.
- Core assumption: The generated descriptions capture discriminative features of the referents that the VLM can match to the correct images.
- Evidence anchors:
  - [abstract]: "We then use a pretrained VLM to identify referents based on the generated descriptions, zero-shot."
  - [section 4.3.1]: "We estimate the image retrieval performance based on accuracy [0, 1], mean reciprocal rank (MRR) [0, 1], and normalized discounted cumulative gain (NDCG) [0, 1]."
- Break condition: If the generated descriptions are too generic or ambiguous, leading to poor matching performance, or if the VLM's text-image matching capability is insufficient for the task.

### Mechanism 3
- Claim: Increasing the dialogue history context window improves text-image retrieval performance.
- Mechanism: Larger context windows provide more information about the referent, allowing the LLM to generate more complete and discriminative descriptions. These better descriptions lead to improved matching by the VLM.
- Core assumption: The additional context in larger windows contains relevant information about the referent that is not present in smaller windows.
- Evidence anchors:
  - [abstract]: "Furthermore, we find that using referent descriptions based on larger context windows has the potential to yield higher returns."
  - [section 5.1]: "Performance on the ground truth reference descriptions for context windows 13 and the full dialogue shows this trend persists, with BLIP-2 achieving approximately 75% and 83% accuracy, respectively."
- Break condition: If the additional context introduces noise or irrelevant information that confuses the LLM, or if the model cannot effectively utilize the larger context.

## Foundational Learning

- Concept: Coreference resolution
  - Why needed here: The system must identify which mentions in the dialogue refer to the same referent in order to generate accurate descriptions.
  - Quick check question: Given the sentence "John said he would come", what does "he" refer to?

- Concept: Text-image retrieval
  - Why needed here: The generated descriptions must be matched to images to identify the referent, requiring understanding of how text and images are represented and compared in vector space.
  - Quick check question: How does a vision-language model compute similarity between a text description and an image?

- Concept: Causal language modeling
  - Why needed here: The LLM is fine-tuned to predict the next token in a sequence, which is the approach used to generate referent descriptions conditioned on context.
  - Quick check question: What is the difference between causal and non-causal language models in terms of token prediction?

## Architecture Onboarding

- Component map:
  - Data -> Annotation pipeline -> LLM fine-tuning -> VLM encoding -> Text-image retrieval -> Evaluation

- Critical path:
  1. Load annotated dialogues
  2. Prepare context windows and mark mentions
  3. Fine-tune LLM on referent description generation
  4. Generate descriptions for test mentions
  5. Use VLM to retrieve images based on descriptions
  6. Evaluate retrieval performance

- Design tradeoffs:
  - LLM size vs. computational cost: GPT-3 performs better but is more expensive than GPT-2
  - Context window size vs. performance: Larger windows improve performance but increase computational cost
  - Fine-tuning vs. zero-shot: Fine-tuning improves performance but requires labeled data

- Failure signatures:
  - Low retrieval accuracy: Generated descriptions may be too generic or ambiguous
  - High variance across folds: Model may be sensitive to training data composition
  - Poor human evaluation scores: Generated descriptions may not match human expectations

- First 3 experiments:
  1. Fine-tune GPT-2 on a small subset of data with context window size 3, evaluate retrieval accuracy
  2. Fine-tune GPT-3 on the same data with context window size 7, compare performance to GPT-2
  3. Test different VLM models (CLIP, ALIGN, BLIP, BLIP-2) with GPT-3-generated descriptions, identify best performer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper bound performance for reference resolution in visually-grounded dialogue when using multimodal approaches that incorporate both linguistic and visual context?
- Basis in paper: [explicit] The authors mention that their unimodal approach based solely on linguistic context far exceeds chance level accuracy, and that the ground truth labels based on larger context windows achieve greater text-image retrieval performance. They also note that their holistic human evaluation results suggest that a multimodal approach should ultimately prove even more effective.
- Why unresolved: The current study only uses linguistic context for generating referent descriptions and does not incorporate visual context information. The human evaluation results suggest that combining both modalities could significantly improve performance, but this has not been tested experimentally.
- What evidence would resolve it: Experiments comparing the current approach with a multimodal approach that uses both linguistic and visual context information during referent description generation and text-image retrieval.

### Open Question 2
- Question: How does the performance of the proposed approach scale with the size of the fine-tuning dataset?
- Basis in paper: [explicit] The authors note that despite the relatively small size of the A Game Of Sorts dataset, they were able to fine-tune GPT-3 to perform the task with greater accuracy than the baselines. They suggest that any LLM used for the task would likely benefit from a larger fine-tuning dataset.
- Why unresolved: The current study only uses the A Game Of Sorts dataset, which contains a limited number of dialogues. The authors speculate that larger datasets would improve performance, but this has not been tested experimentally.
- What evidence would resolve it: Experiments fine-tuning the models on datasets of varying sizes to determine how performance scales with the amount of training data.

### Open Question 3
- Question: How generalizable is the proposed approach to other visually-grounded dialogue tasks and datasets?
- Basis in paper: [explicit] The authors suggest that benchmarking performance on other visually-grounded dialogue tasks would provide insights into the generalizability of the method.
- Why unresolved: The current study only evaluates the approach on the A Game Of Sorts dataset. The authors acknowledge that testing on other datasets would be valuable but have not done so.
- What evidence would resolve it: Experiments applying the proposed approach to other visually-grounded dialogue datasets and tasks to assess its generalizability and performance across different domains and task configurations.

## Limitations

- The evaluation relies on a manually annotated dataset of only 15 dialogues, which limits statistical power and generalizability.
- The mention detection assumption is significant - the paper states that mention detection is treated as a solved problem, but no empirical validation of mention detection performance is provided.
- The study focuses on single-image referents, excluding multi-image referents that may comprise a substantial portion of real dialogue.

## Confidence

**High confidence** in the core methodology: The approach of using a fine-tuned LLM to generate referent descriptions and a VLM for zero-shot retrieval is technically sound and well-supported by the experimental results.

**Medium confidence** in the implicit coreference learning claim: While the paper asserts that the LLM learns coreference resolution implicitly through context conditioning, the evidence is largely theoretical rather than empirical.

**Low confidence** in the scalability and robustness claims: The paper suggests that larger context windows and more powerful LLMs could yield higher returns, but these claims are based on limited empirical evidence.

## Next Checks

1. **Ablation study on context window size**: Systematically evaluate retrieval performance across multiple context window sizes (1, 3, 5, 7, 9, 13 utterances) to identify the optimal window size and quantify the marginal benefit of additional context.

2. **Error analysis of generated descriptions**: Conduct a qualitative analysis of failed retrievals by examining the generated descriptions that led to incorrect predictions. Categorize errors into types (coreference failures, insufficient context, VLM limitations) to identify whether the bottleneck is in the LLM generation or VLM matching stage.

3. **Mention detection performance evaluation**: Implement a simple mention detection system (e.g., noun phrase extraction with coreference linking) and evaluate its impact on downstream retrieval performance. This would test the assumption that mention detection is a solved problem and quantify the error propagation from this component.