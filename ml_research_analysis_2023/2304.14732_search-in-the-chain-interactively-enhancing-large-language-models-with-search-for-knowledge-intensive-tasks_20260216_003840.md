---
ver: rpa2
title: 'Search-in-the-Chain: Interactively Enhancing Large Language Models with Search
  for Knowledge-intensive Tasks'
arxiv_id: '2304.14732'
source_url: https://arxiv.org/abs/2304.14732
tags:
- reasoning
- answer
- knowledge
- searchain
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Search-in-the-Chain (SearChain), a framework
  that combines large language models (LLMs) with information retrieval (IR) to improve
  the accuracy, credibility, and traceability of generated content for knowledge-intensive
  tasks. SearChain enables LLM to construct a reasoning chain consisting of IR-oriented
  queries, with IR interacting at each step to verify, complete, and trace information.
---

# Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks

## Quick Facts
- arXiv ID: 2304.14732
- Source URL: https://arxiv.org/abs/2304.14732
- Reference count: 31
- Key outcome: Achieves up to 56.91% accuracy on HotpotQA by combining LLM with IR for knowledge-intensive tasks

## Executive Summary
Search-in-the-Chain (SearChain) is a framework that enhances large language models with information retrieval to improve accuracy, credibility, and traceability for knowledge-intensive tasks. The system constructs a reasoning chain of IR-oriented queries, allowing iterative interaction between the LLM and retrieval system at each step. Experiments on four multi-hop QA datasets show SearChain outperforms state-of-the-art baselines, achieving up to 56.91% accuracy on HotpotQA while maintaining effective knowledge decoupling.

## Method Summary
SearChain combines LLM with information retrieval for multi-hop question answering by first generating a chain-of-query (CoQ) containing sub-questions and answers, then iteratively interacting with a retrieval system that verifies, completes, and traces information. The framework uses GPT-3.5-turbo for CoQ generation, ColBERTv2 for retrieval, and a BERT-based reader for answer verification with confidence thresholds. The process runs for up to 5 interaction rounds, producing final answers with supporting references and reasoning chains.

## Key Results
- Achieves up to 56.91% accuracy on HotpotQA dataset
- Outperforms state-of-the-art baselines on four multi-hop QA datasets
- Demonstrates effective knowledge decoupling, avoiding misleading LLM with incorrect retrieved information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improves accuracy by letting LLM construct globally optimal chain-of-query before IR interaction
- Mechanism: LLM generates complete CoQ with IR-oriented query-answer pairs in one shot, providing structured plan for IR verification
- Core assumption: LLM can plan globally optimal sub-questions without intermediate IR feedback
- Evidence anchors: Abstract states IR interacts with each node of CoQ, section 3.1 describes LLM planning search chain
- Break condition: Fails if LLM cannot generate globally coherent CoQ or IR cannot effectively verify nodes

### Mechanism 2
- Claim: Improves credibility by verifying LLM answers with retrieved documents and correcting only when IR is confident
- Mechanism: IR extracts answers from documents and compares with LLM's answers, correcting only when confidence exceeds threshold
- Core assumption: IR can reliably extract correct answers and confidence thresholds filter misleading corrections
- Evidence anchors: Abstract describes IR verification and confidence-based correction, section 3.2 details correction logic
- Break condition: Breaks if Reader frequently extracts wrong answers or confidence threshold is poorly calibrated

### Mechanism 3
- Claim: Improves traceability by marking references to supporting documents for each reasoning step
- Mechanism: Stores retrieved document for each CoQ node as supporting evidence, includes reasoning chain and references in final output
- Core assumption: Users value traceability and can understand reasoning chain when references are marked at sub-fragment level
- Evidence anchors: Abstract mentions generating reasoning process with references, section 3.2 describes output format
- Break condition: Breaks if reasoning chain is too complex or document references aren't accurately aligned

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Understanding CoT helps grasp why SearChain's CoQ is different—CoT focuses on intermediate reasoning steps while CoQ plans sub-questions for IR interaction
  - Quick check question: What is the main difference between CoT and CoQ in terms of their focus during reasoning?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the foundation for combining IR with LLM, and understanding its strengths and limitations helps see why SearChain's approach is novel
  - Quick check question: How does traditional RAG differ from SearChain in terms of when and how IR is used during reasoning?

- Concept: Multi-hop question answering
  - Why needed here: The task SearChain is designed for requires understanding how complex questions can be decomposed into sub-questions that each require knowledge retrieval
  - Quick check question: Why is multi-hop QA a suitable testbed for evaluating the effectiveness of IR-LLM interaction frameworks?

## Architecture Onboarding

- Component map:
  LLM (Commander) -> IR System -> Reader Model -> Prompt Templates -> Output Formatter

- Critical path:
  1. LLM receives complex question
  2. LLM generates CoQ
  3. IR processes each node: retrieves document, verifies/corrects answers, completes unknown queries
  4. LLM reconstructs CoQ based on IR feedback
  5. Repeat steps 3-4 for up to rmax rounds
  6. Output final answer with reasoning chain and references

- Design tradeoffs:
  - CoQ generation vs. step-by-step reasoning: CoQ may be harder for LLM but provides better IR interface
  - Confidence threshold: Higher threshold reduces misleading corrections but may miss valid ones
  - Number of interaction rounds: More rounds allow better correction but increase latency and cost

- Failure signatures:
  - LLM generates incoherent CoQ (e.g., circular reasoning, irrelevant sub-questions)
  - IR retrieves irrelevant or low-quality documents leading to wrong corrections
  - Reader confidence is miscalibrated (overconfident in wrong answers or underconfident in correct ones)
  - Final output lacks proper references or reasoning chain is too complex to follow

- First 3 experiments:
  1. Compare accuracy of SearChain vs. direct RAG on a multi-hop QA dataset with varying complexity
  2. Test the effect of different confidence thresholds on the rate of correct vs. incorrect corrections
  3. Measure user comprehension of the reasoning chain and references in the final output compared to plain answers

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Claims about global reasoning planning rely heavily on single-shot prompt engineering rather than learned reasoning strategies
- Confidence-based correction effectiveness depends on Reader model calibration, which isn't thoroughly evaluated
- 56.91% accuracy on HotpotQA needs context regarding comparison methods and dataset splits

## Confidence
- **High confidence**: The basic framework architecture (CoQ generation → IR interaction → correction) is clearly described and logically sound
- **Medium confidence**: The knowledge decoupling mechanism's effectiveness is supported by ablation studies but could benefit from more rigorous error analysis
- **Low confidence**: The claim that globally planned reasoning chains outperform iterative step-by-step reasoning lacks direct comparative evidence

## Next Checks
1. **Error analysis validation**: Categorize failed cases to determine whether errors stem from LLM reasoning limitations, retrieval quality issues, or confidence threshold miscalibration
2. **Ablation on planning horizon**: Compare SearChain's one-shot CoQ generation against iterative sub-question generation to isolate the benefit of global planning
3. **Confidence threshold sensitivity**: Systematically vary the confidence threshold to identify optimal values and test robustness across different question types and dataset domains