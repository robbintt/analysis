---
ver: rpa2
title: Generalizing Differentially Private Decentralized Deep Learning with Multi-Agent
  Consensus
arxiv_id: '2306.13892'
source_url: https://arxiv.org/abs/2306.13892
tags:
- learning
- privacy
- private
- algorithms
- differentially
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose three new differentially private decentralized
  learning algorithms (DP-DSGD, DP-DSGT, DP-DiNNO) to protect agents' local datasets
  during cooperative training. These algorithms generalize DP-SGD to decentralized
  settings and build on subgradient- and ADMM-based distributed optimization methods.
---

# Generalizing Differentially Private Decentralized Deep Learning with Multi-Agent Consensus

## Quick Facts
- arXiv ID: 2306.13892
- Source URL: https://arxiv.org/abs/2306.13892
- Reference count: 40
- Key outcome: Proposed DP-DSGD, DP-DSGT, DP-DiNNO algorithms achieve within 3% of DP-SGD on MNIST and within 6% on CIFAR-100 under (10, 10^-5)-differential privacy while protecting individual data samples.

## Executive Summary
This paper introduces three new differentially private decentralized learning algorithms that extend DP-SGD to decentralized settings using subgradient and ADMM-based optimization. The algorithms ensure each agent's local dataset is protected from inference attacks while achieving consensus across a communication graph. Theoretical convergence guarantees are proven for strongly convex objectives, and experiments demonstrate practical utility on MNIST and CIFAR-100 classification tasks. The algorithms show robustness to communication graph structure beyond a threshold connectivity value.

## Method Summary
The authors propose three differentially private decentralized learning algorithms (DP-DSGD, DP-DSGT, DP-DiNNO) that build on existing decentralized optimization methods. They inject calibrated Gaussian noise into clipped gradients during each update step, using the sampled Gaussian mechanism to track cumulative privacy loss. Agents iteratively average model parameters or gradient estimates over a communication graph to reach consensus. The algorithms are evaluated on MNIST and CIFAR-100 classification tasks with varying privacy budgets and graph connectivities.

## Key Results
- DP-DSGT achieves accuracies within 3% of DP-SGD on MNIST under (1, 10^-5)-differential privacy
- DP-DSGT achieves accuracies within 6% of DP-SGD on CIFAR-100 under (10, 10^-5)-differential privacy
- Performance remains stable for communication graph connectivity above normalized Fiedler value of ~0.4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differential privacy is achieved by injecting calibrated Gaussian noise into the gradient during each decentralized update step.
- Mechanism: The noise scale is proportional to the gradient clipping norm and inversely proportional to the batch sampling probability, ensuring that the algorithm satisfies (ε, δ)-differential privacy for each agent's local dataset.
- Core assumption: Per-sample gradient clipping bounds the sensitivity of the update to any single data point, and the moments accountant (via sampled Gaussian mechanism) tracks cumulative privacy loss over training iterations.
- Evidence anchors:
  - [abstract] "We utilize the sampled Gaussian mechanism [33] to select the suitable standard deviation σ for adding Gaussian noise to gradients, ensuring cumulative (ϵ, δ)-differential privacy for each agent."
  - [section 5] "We utilize the sampled Gaussian mechanism [33] to select the suitable standard deviation σ for adding Gaussian noise to gradients, ensuring cumulative (ϵ, δ)-differential privacy for each agent."
  - [corpus] Related works in DPMAC and Dyn-D^2P also use Gaussian noise addition, confirming this is a standard approach in the field.

### Mechanism 2
- Claim: Consensus across agents is achieved through iterative averaging of model parameters or gradient estimates over a communication graph.
- Mechanism: Each agent updates its local model by combining its own noisy gradient step with weighted averages of its neighbors' parameters (or gradient estimates), causing all agents' models to converge to a common solution.
- Core assumption: The mixing matrix W is doubly stochastic and the graph is connected, ensuring that the consensus process converges to a unique solution.
- Evidence anchors:
  - [section 4.2] "Agents i, j ∈ V can exchange information if they are one-hop neighbors in the graph i.e. (i, j) ∈ E. Let Ni = {i} ∪ {j ∈ V | (i, j) ∈ E} denote the local neighborhood of agent i..."
  - [section 5.1] "agent i takes the final step using its aggregated local estimate of the parameters Pj∈Ni wijθj (note that this includes θi) and shares the noisy parameters with neighbors."
  - [corpus] DPMAC and Privacy Preserving Semi-Decentralized Mean Estimation also use iterative averaging over communication graphs to reach consensus.

### Mechanism 3
- Claim: The convergence rate of the differentially private algorithm matches that of the base decentralized algorithm up to an error term proportional to the noise standard deviation.
- Mechanism: The noise terms are scaled by a diminishing learning rate and the linear convergence rate of the base algorithm, bounding the cumulative noise effect on convergence.
- Core assumption: The objective function is strongly convex and the function M(θ) of interest for convergence is Lipschitz continuous with respect to θ.
- Evidence anchors:
  - [section 5.4] "Theorem 5.1 (Convergence Theorem). Let the objective function l be strongly convex and L-smooth, θ ∈ RN d be a vector concatenating local variables {θi}N i=1, M (θ) be a function of θ that is of interest for convergence, and σ be the standard deviation of Gaussian noise for differentially private gradient perturbations..."
  - [section 5.4] "The key insight to our proof is that the noise terms are scaled by a diminishing learning rate ηk and a linear convergence rate, such that the cumulative noise is bounded on the order of the standard deviation O(σ)."
  - [corpus] Dyn-D^2P also proves convergence for differentially private decentralized learning, supporting this mechanism.

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: To protect each agent's local dataset from being inferred by other agents during decentralized training.
  - Quick check question: What is the difference between ε and δ in the (ε, δ)-differential privacy definition?

- Concept: Decentralized Optimization
  - Why needed here: To allow agents to collaboratively learn a model without relying on a central server, improving scalability and privacy.
  - Quick check question: How does the choice of communication graph affect the convergence of decentralized optimization algorithms?

- Concept: Gradient Clipping
  - Why needed here: To bound the sensitivity of the gradient to any single data point, enabling the addition of calibrated noise for differential privacy.
  - Quick check question: Why is it important to clip the per-sample gradients rather than the batch gradient?

## Architecture Onboarding

- Component map:
  Agents -> Communication Graph -> Differentially Private Algorithms -> Privacy Accountant

- Critical path:
  1. Initialize local models and datasets on each agent.
  2. For each training iteration:
     a. Compute per-sample gradients and clip them.
     b. Aggregate clipped gradients and add calibrated Gaussian noise.
     c. Update local model using the noisy gradient and communicate with neighbors.
     d. Update privacy accountant.
  3. After training, evaluate the final model accuracy and privacy guarantees.

- Design tradeoffs:
  - Privacy vs. Accuracy: Stronger privacy guarantees (smaller ε) require more noise, leading to lower model accuracy.
  - Communication Cost vs. Convergence: More frequent communication between agents can improve convergence but increases communication overhead.
  - Model Complexity vs. Privacy Budget: Larger models may require more gradient updates, consuming more of the privacy budget.

- Failure signatures:
  - If the privacy budget is exceeded, the algorithm will no longer satisfy differential privacy.
  - If the communication graph is disconnected, consensus may not be achieved and the algorithm may diverge.
  - If the learning rate is too high or too low, the algorithm may not converge or may converge slowly.

- First 3 experiments:
  1. Run DP-DSGD on MNIST with a small ε (e.g., 1) and a fully connected communication graph. Verify that the algorithm achieves reasonable accuracy while satisfying differential privacy.
  2. Vary the connectivity of the communication graph and measure the impact on the convergence rate and final accuracy of DP-DSGD. Identify the threshold connectivity below which performance degrades.
  3. Compare the performance of DP-DSGD, DP-DSGT, and DP-DiNNO on MNIST with different data distributions (e.g., IID vs. non-IID). Analyze the impact of data heterogeneity on each algorithm's convergence and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed differentially private decentralized algorithms perform on more complex tasks beyond classification, such as cooperative representation learning or reinforcement learning?
- Basis in paper: [inferred] The authors mention that it would be interesting to apply their algorithms to problems beyond classification, such as cooperative representation learning, as done in [54] within a non-private setting.
- Why unresolved: The paper focuses on classification tasks and does not explore the performance of the algorithms on more complex tasks. This leaves an open question about the generalizability and effectiveness of the algorithms in other domains.
- What evidence would resolve it: Experimental results demonstrating the performance of the algorithms on tasks such as cooperative representation learning or reinforcement learning, compared to centralized baselines and non-private decentralized approaches.

### Open Question 2
- Question: How does the fairness loss of the differentially private decentralized algorithms vary across different sensitive attributes or subgroups, and how can it be mitigated?
- Basis in paper: [explicit] The authors acknowledge that differential privacy has been shown to have a disparate impact on model accuracy and mention that it would be important to understand and mitigate their algorithms' fairness loss when applied to sensitive use cases.
- Why unresolved: The paper does not investigate the fairness implications of the proposed algorithms, leaving an open question about their potential biases and how they can be addressed.
- What evidence would resolve it: Empirical studies measuring the fairness loss of the algorithms across different sensitive attributes or subgroups, along with proposed mitigation strategies to reduce bias and ensure equitable performance.

### Open Question 3
- Question: How do the differentially private decentralized algorithms perform under non-cooperative and adversarial threat models, where agents may not be honest-but-curious?
- Basis in paper: [explicit] The authors mention that future work could consider non-cooperative and adversarial threat models, as their current work assumes agents are honest-but-curious.
- Why unresolved: The paper assumes honest-but-curious agents and does not explore the robustness of the algorithms under more realistic threat models where agents may have malicious intentions.
- What evidence would resolve it: Experimental results evaluating the performance and privacy guarantees of the algorithms under various adversarial scenarios, such as Byzantine attacks or data poisoning, compared to centralized and non-private decentralized approaches.

### Open Question 4
- Question: How do the proposed differentially private decentralized algorithms scale to larger network sizes and more complex models?
- Basis in paper: [inferred] The authors mention that computing per-sample gradients for multiple agent models can be memory intensive, limiting the model architectures and batch sizes they can reasonably exploit. This suggests that scaling to larger networks and more complex models may be challenging.
- Why unresolved: The paper only evaluates the algorithms on small-scale experiments with 10 agents and relatively simple models, leaving an open question about their scalability and performance on larger and more complex setups.
- What evidence would resolve it: Experimental results demonstrating the performance and efficiency of the algorithms on larger network sizes (e.g., hundreds or thousands of agents) and more complex models (e.g., deep neural networks with many layers), compared to centralized and non-private decentralized approaches.

## Limitations

- The privacy analysis assumes per-sample gradient clipping bounds sensitivity, but practical implementations may have edge cases where this fails.
- The connectivity threshold effect is demonstrated empirically but lacks rigorous characterization of the exact threshold value.
- The experiments focus on relatively small-scale problems (10 agents) that may not capture performance at larger scales.

## Confidence

- Differential privacy mechanisms: High
- Consensus mechanism: High
- Convergence guarantees: Medium
- Scalability to larger networks: Low

## Next Checks

1. Test the algorithms on larger-scale problems with 50+ agents to verify the scalability of the privacy-utility tradeoff and the robustness of the connectivity threshold phenomenon.

2. Implement adversarial membership inference attacks on the trained models to empirically validate the privacy claims beyond the formal (ε, δ) bounds.

3. Systematically vary the learning rate schedule and observe its interaction with the noise injection mechanism to identify optimal hyperparameter configurations for different privacy budgets.