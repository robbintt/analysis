---
ver: rpa2
title: Learning Uniform Clusters on Hypersphere for Deep Graph-level Clustering
arxiv_id: '2311.13953'
source_url: https://arxiv.org/abs/2311.13953
tags:
- clustering
- graph
- representation
- pseudo
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deep graph-level clustering,
  which involves grouping multiple graphs into clusters. The authors propose a novel
  method called Uniform Deep Graph Clustering (UDGC) to overcome the limitations of
  existing approaches.
---

# Learning Uniform Clusters on Hypersphere for Deep Graph-level Clustering

## Quick Facts
- arXiv ID: 2311.13953
- Source URL: https://arxiv.org/abs/2311.13953
- Reference count: 40
- Primary result: UDGC outperforms state-of-the-art deep graph clustering models on 8 datasets with significant improvements in accuracy and normalized mutual information.

## Executive Summary
This paper addresses deep graph-level clustering by proposing Uniform Deep Graph Clustering (UDGC). The method tackles cluster collapse through uniform pseudo label generation using Augmentation-Consensus Optimal Transport (ACOT) and cluster scattering on a hypersphere via contrastive learning. Center Alignment Optimal Transport (CAOT) further enhances consensus between representation and parameter spaces. Extensive experiments on eight datasets demonstrate superior performance compared to existing methods.

## Method Summary
UDGC transforms graphs into embeddings using a GNN encoder, generates uniform pseudo labels via ACOT by exploiting perturbed graph representations, and scatters clusters on a hypersphere using contrastive instance-instance and instance-agent losses. CAOT aligns representation-space centers with model parameter-space agents to enhance consensus. The framework is optimized through an expectation-maximization process, with clustering assignments determined via k-means on the learned representations.

## Key Results
- UDGC achieves state-of-the-art performance on eight graph datasets.
- Significant improvements in clustering accuracy (ACC) and normalized mutual information (NMI) compared to existing methods.
- Demonstrates robustness across datasets with varying graph sizes and cluster numbers.

## Why This Works (Mechanism)

### Mechanism 1
ACOT generates uniformly distributed pseudo labels by exploiting consensus between original and augmented graph representations. The method uses a perturbed encoder to create augmentations and formulates bi-directional KL-divergence between transport matrices, encouraging uniform partitioning across clusters. This relies on the assumption that graph data preserves semantics well during encoder perturbations.

### Mechanism 2
Contrastive learning with cluster-level uniformity scatters clusters on the unit hypersphere to mitigate cluster collapse. Instance-instance and instance-agent contrastive losses enforce inter-cluster separability by pulling similar samples together and pushing dissimilar ones apart. This geometric approach leverages hyperspherical embeddings to reduce degeneracy.

### Mechanism 3
CAOT aligns representation-space centers with model parameter-space agents to enhance consensus and model fitting. The method discovers centers in the representation space via optimal transport and matches them with trainable agents. This alignment is based on the assumption that representation space better exploits semantic information than parameter space.

## Foundational Learning

- Concept: Graph neural networks (GNNs) for graph-level representation learning
  - Why needed here: UDGC uses a GNN encoder to transform graphs into embeddings for clustering.
  - Quick check question: What is the role of GIN (Graph Isomorphism Network) in the encoder part of UDGC?

- Concept: Optimal transport (OT) and Sinkhorn algorithm
  - Why needed here: ACOT and CAOT rely on OT to compute transport matrices and align distributions.
  - Quick check question: How does the Sinkhorn-Knopp algorithm solve the OT problem in ACOT?

- Concept: Contrastive learning with instance and cluster-level objectives
  - Why needed here: Contrastive losses enforce uniformity and separability on the hypersphere to avoid cluster collapse.
  - Quick check question: What is the difference between instance-instance and instance-agent contrastive learning in UDGC?

## Architecture Onboarding

- Component map: Graphs G -> GNN Encoder Φ(G; θ) -> Graph embeddings X -> Perturbed encoder Φ(G; θ′) -> Augmented embeddings X′ -> Projection heads FS, FZ -> Hyperspherical features S, S′, Z, Z′ -> ACOT module -> Pseudo labels q -> Contrastive module -> Instance-instance loss Linstance, instance-agent loss Lagent -> CAOT module -> Center-agent loss Lcenter -> K-means clustering -> Clustering assignment y

- Critical path: Graph → GNN Encoder → Hyperspherical Features → ACOT (pseudo labels) → Contrastive + CAOT → Updated embeddings → K-means clustering

- Design tradeoffs:
  - Augmentation via parameter perturbation vs. data augmentation: simpler but may introduce noise
  - Hyperspherical normalization: enforces uniform distribution but may limit expressiveness
  - OT-based alignment: robust but O(n²) complexity

- Failure signatures:
  - Cluster collapse: All points converge to one cluster
  - Poor pseudo labels: High entropy or uneven cluster sizes
  - Training instability: Oscillating losses in ACOT or CAOT

- First 3 experiments:
  1. Verify semantic preservation under encoder perturbation on a small synthetic graph dataset.
  2. Test ACOT pseudo label uniformity on balanced vs. imbalanced datasets.
  3. Compare contrastive losses with and without CAOT on cluster separation metrics.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of UDGC vary with different perturbation strengths (σ) in the graph encoder perturbation strategy? The paper mentions σ is selected from {0.1, 1, 2} but doesn't show performance changes across different values.

### Open Question 2
How does UDGC's performance compare to other deep graph-level clustering methods when applied to extremely large-scale graphs? The paper focuses on medium-sized datasets and doesn't address scalability to graphs with millions or billions of nodes.

### Open Question 3
How does the choice of graph neural network architecture (e.g., GIN, GCN, GAT) affect UDGC's performance? The paper uses GIN but doesn't explore the impact of different GNN architectures on performance.

## Limitations
- Semantic preservation assumption under encoder perturbations lacks external validation.
- Hyperspherical embeddings' effectiveness in preventing cluster collapse is inferred rather than directly demonstrated.
- CAOT's novel alignment mechanism lacks ablation studies to isolate its contribution.

## Confidence
- Medium confidence in overall framework effectiveness due to strong empirical results on eight datasets.
- Low confidence in theoretical guarantees of ACOT and CAOT without deeper mathematical analysis.
- Medium confidence in implementation details given specificity but missing implementation nuances.

## Next Checks
1. Conduct ablation studies to isolate contributions of ACOT, contrastive learning, and CAOT to overall performance.
2. Test framework on datasets with varying levels of noise and structural complexity to assess robustness.
3. Implement a simpler baseline using only ACOT without contrastive learning to evaluate necessity of hyperspherical uniformity component.