---
ver: rpa2
title: Controllable Prosody Generation With Partial Inputs
arxiv_id: '2303.09446'
source_url: https://arxiv.org/abs/2303.09446
tags:
- control
- pafs
- prosody
- sparse
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of controlling prosody in text-to-speech
  synthesis through human-in-the-loop (HitL) methods. Existing generative models lack
  an efficient interface for precise user modifications, making manual control difficult.
---

# Controllable Prosody Generation With Partial Inputs

## Quick Facts
- arXiv ID: 2303.09446
- Source URL: https://arxiv.org/abs/2303.09446
- Reference count: 22
- Key outcome: Novel framework for sparse control of prosody in TTS using human-in-the-loop methods, showing significant improvements in listener preference with as few as 4 input values

## Executive Summary
This paper introduces a novel approach to controlling prosody in text-to-speech synthesis through sparse human-in-the-loop inputs. The authors propose the Multiple-Instance Conditional Variational Autoencoder (MICV AE), which allows users to drive prosody generation with minimal input values. By encoding sparse prosodic features independently and aggregating them via self-attention, the model achieves efficient, robust, and faithful control of speech output. The approach addresses the challenge of manual prosody control in existing generative models by providing an intuitive interface for precise modifications.

## Method Summary
The proposed framework uses sparse prosodic inputs to control TTS output through a novel Multiple-Instance Conditional Variational Autoencoder (MICV AE). The model encodes each user-provided prosodic acoustic feature (PAF) independently using positional and feature-type encodings, then aggregates them via self-attention to produce a sentence-level latent vector. This design enables the generation of realistic outputs regardless of which and how many PAFs the user provides. The model is trained with speaker and style embeddings, and the decoder generates complete waveforms from the latent representation. Evaluation focuses on efficiency (improvement per additional driving value), robustness (performance across different sparsity patterns), and faithfulness (subjective listener preference).

## Key Results
- MICV AE significantly outperforms baseline Masked CV AE in listener preference tests (4:1 ratio)
- As few as ~4 input values are sufficient for effective prosody control
- The model demonstrates robustness across different sparsity patterns and maintains faithfulness to user intention

## Why This Works (Mechanism)

### Mechanism 1
The Multiple-Instance Conditional Variational Autoencoder (MICV AE) enables efficient sparse control of prosody by encoding each user-provided prosodic acoustic feature (PAF) independently and aggregating them via self-attention. The encoder embeds each driving PAF with positional and feature-type encodings, then uses a weighted average (self-attention) to produce a single latent vector z'. This design ensures that the model can generate realistic outputs regardless of which and how many PAFs the user provides.

### Mechanism 2
The self-attention-based encoder is robust to different sparsity patterns because the softmax operation ensures invariance to the number of inputs provided. By computing attention weights as a softmax of key-query products, the encoder produces a fixed-dimensional latent vector regardless of how many or which PAFs are driven, allowing for flexible user control.

### Mechanism 3
MICV AE is faithful to user intention because it maps driving values to an approximate posterior that maximizes the likelihood of the inputs, rather than blindly copying the values. The probabilistic encoder finds the point on the data manifold closest to the driving values, allowing for realistic interpolation and avoiding unrealistic or inconsistent outputs.

## Foundational Learning

- **Variational Autoencoder (VAE)**: Why needed - MICV AE is built on the Conditional VAE framework, so understanding how VAEs learn latent representations and generate data is essential. Quick check - What is the role of the encoder and decoder in a VAE, and how does the latent space enable conditional generation?

- **Self-Attention Mechanisms**: Why needed - The encoder uses self-attention to aggregate sparse inputs, so understanding how self-attention works and why it is effective for variable-length inputs is important. Quick check - How does self-attention differ from recurrent or convolutional approaches when processing sequences of varying length?

- **Prosodic Acoustic Features (PAFs)**: Why needed - The control space for prosody is defined by PAFs (F0, energy, duration), so understanding what these features are and how they relate to perceived prosody is crucial. Quick check - What are the perceptual correlates of intonation, loudness, and timing in speech, and how are they approximated by F0, energy, and duration?

## Architecture Onboarding

- **Component map**: User inputs sparse PAFs → Multiple-Instance Encoder → Latent vector → PAF Decoder → Generated audio
- **Critical path**: User inputs sparse PAFs → Multiple-Instance Encoder → Latent vector → PAF Decoder → Generated audio
- **Design tradeoffs**:
  - Using self-attention vs. recurrent or convolutional encoders for robustness to sparsity
  - Mapping to latent space vs. direct output for faithful but realistic prosody
  - Training with simulated control vs. real human-in-the-loop data for scalability
- **Failure signatures**:
  - Model generates unrealistic or inconsistent prosody when given sparse inputs
  - Model is not robust to different sparsity patterns (e.g., always performs best when all values are provided)
  - Model is not faithful to user intention (e.g., changes are not perceptible or are inconsistent with driving values)
- **First 3 experiments**:
  1. Train MICV AE with varying levels of sparsity (0%, 25%, 50%, 100%) and evaluate robustness on test data.
  2. Compare MICV AE to Masked CV AE baseline on simulated control tasks with different sparsity patterns.
  3. Conduct A/B/R listening tests to evaluate faithfulness of MICV AE with 4 driving PAFs vs. no control.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed sparse control framework perform in real-world human-in-the-loop scenarios compared to simulated control? The authors acknowledge that the ultimate use-case for their system is human-driven control, leaving UX experiments with human users for future work. Conducting user studies with actual humans interacting with the system in real-world scenarios would resolve this question.

### Open Question 2
Can the sparse control framework be effectively extended to other types of highly-structured data beyond speech, such as images or text? The authors discuss the potential for controlling other types of highly-structured data but do not provide empirical evidence for its effectiveness in other domains. Applying the framework to other data types and evaluating its performance would resolve this question.

### Open Question 3
How does the proposed Multiple-Instance CV AE model compare to other existing methods for controlling prosody in speech synthesis? The authors compare their model to a baseline Masked CV AE and a crude control method but do not compare it to other state-of-the-art methods for prosody control. Conducting a thorough comparison to other existing methods would resolve this question.

## Limitations
- Reliance on simulated human-in-the-loop control during training may limit generalization to real user preferences and behaviors
- Evaluation is limited to Spanish speech data with a fixed number of speakers, leaving robustness to other languages untested
- Subjective listening tests may not fully capture the diversity of user intentions in real-world applications

## Confidence

- **High Confidence**: The core mechanism of using self-attention to aggregate sparse PAFs and generate realistic prosody is well-supported by results and analysis. The model consistently outperforms the baseline Masked CV AE across different sparsity patterns.
- **Medium Confidence**: The claim that the model is faithful to user intention is supported by subjective listening tests, but evaluation is limited to 4 driving PAFs and specific tasks. The perceived change may not always align with user's intended modification.
- **Low Confidence**: The scalability of the approach to other languages and speaker populations is not demonstrated. The reliance on simulated control raises questions about handling real-world user preferences.

## Next Checks

1. **Generalization to Real Human-in-the-Loop Control**: Conduct user studies with real participants controlling the model in a human-in-the-loop setup. Compare the generated prosody to ground truth and evaluate user satisfaction and control efficiency.

2. **Robustness to Different Languages and Speakers**: Train and evaluate the model on diverse speech datasets covering multiple languages and speaker demographics. Assess performance degradation or improvement compared to Spanish-only evaluation.

3. **Ablation Studies on Encoder Architecture**: Systematically vary the encoder architecture (self-attention vs. recurrent vs. convolutional) and evaluate impact on model performance, robustness, and faithfulness. Identify critical components for achieving sparse control of prosody.