---
ver: rpa2
title: Human-AI Interactions and Societal Pitfalls
arxiv_id: '2309.10448'
source_url: https://arxiv.org/abs/2309.10448
tags:
- users
- when
- output
- bias
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how users' interactions with generative AI can
  lead to societal issues like homogenization and bias. It proposes a Bayesian model
  where users with heterogeneous preferences choose how much information to share
  with the AI, balancing output fidelity and communication cost.
---

# Human-AI Interactions and Societal Pitfalls

## Quick Facts
- arXiv ID: 2309.10448
- Source URL: https://arxiv.org/abs/2309.10448
- Reference count: 40
- Key outcome: Users' interactions with generative AI can lead to homogenization and bias propagation, but efficient interaction can preserve diversity

## Executive Summary
This paper develops a Bayesian model of human-AI interactions to study how user heterogeneity and interaction frictions affect output diversity and bias. The model shows that users with common preferences accept default AI outputs while those with unique preferences share information to improve fidelity. This leads to homogenization where outputs are less diverse than underlying preferences. The research reveals that AI bias can become societal bias when interaction frictions prevent users from correcting it, and that training AI on AI-generated content can trigger a "death spiral" of increasing homogenization.

## Method Summary
The paper proposes a Bayesian framework where users with preferences drawn from a normal distribution decide how much information to share with AI based on a trade-off between fidelity loss and communication cost. The AI uses Bayesian inference to update its beliefs and generate outputs. Users optimize their information sharing strategy to minimize expected utility loss. The model is computationally intractable in closed form, so the authors use discretization and simulations to analyze homogenization and bias effects under various conditions.

## Key Results
- Users with unique preferences share more information with AI to reduce fidelity errors, while common-preference users accept default outputs
- Outputs become less diverse than preferences, with homogenization degree depending on interaction costs
- AI bias can propagate to become societal bias when user interactions are inefficient
- Training AI on AI-generated content can trigger a homogenization death spiral

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Users with more unique preferences face higher utility loss and choose to interact more with AI
- Mechanism: The expected fidelity error increases with uniqueness when no information is shared, but decreases when sufficient information is provided at the cost of communication effort. Users optimize by balancing fidelity loss and communication cost.
- Core assumption: The fidelity error is inversely related to the amount of information shared and users are rational utility maximizers
- Evidence anchors:
  - [abstract]: "Users with more common preferences simply accept the default output, avoiding any communication costs at the expense of a small fidelity mismatch. In contrast, users with more unique preferences share information with the AI to reduce fidelity errors, albeit with higher communication costs."
  - [section]: "Users' choices depend on their uniqueness, the distance of their preference θ to the population mean µθ, d(θ) ≜ |θ − µθ|. We note that the derivation of σ⋆q(θ), presented in Section EC.1, is non-trivial as Equation (3) is neither concave nor convex."
- Break condition: If the communication cost is zero or users are indifferent to fidelity, the mechanism fails as all users would share maximum information

### Mechanism 2
- Claim: AI bias can propagate to become societal bias if user interactions are not efficient
- Mechanism: When AI has a directional bias, users with preferences in the opposite direction need more communication to correct the bias. If interaction is costly or slow, many users accept the biased output, leading to a societal bias in the population.
- Core assumption: The AI prior is not representative of the population and users cannot easily correct bias due to interaction frictions
- Evidence anchors:
  - [abstract]: "And any AI bias may propagate to become societal bias."
  - [section]: "Theorem 2... shows an encouraging result: human-AI interactions can partially prevent AI bias from becoming societal bias. For example, a left-wing journalist in Example 1 may increase their interactions with the AI to correct the output if the AI is biased to the right."
- Break condition: If human-AI interactions are sufficiently efficient (low communication cost), users can correct bias, preventing societal propagation

### Mechanism 3
- Claim: Training AI on AI-generated content leads to homogenization "death spiral"
- Mechanism: Outputs become less diverse than user preferences due to rational information-sharing choices. If these outputs are used to retrain AI, the model's prior becomes increasingly biased toward common preferences, reinforcing homogenization in future generations.
- Core assumption: The AI's prior distribution updates to match the distribution of its outputs over time
- Evidence anchors:
  - [abstract]: "Outputs may become more homogenized, especially when the AI is trained on AI-generated content, potentially triggering a homogenization death spiral."
  - [section]: "This model is not tractable... However, we use simulations... to illustrate its consequences for homogenization... the distribution of the outputs θ⋆ = θA (in orange) becomes more and more homogeneous."
- Break condition: If the training data includes diverse human-generated content, the death spiral is mitigated

## Foundational Learning

- Concept: Bayesian updating and normal distributions
  - Why needed here: The model relies on users sharing noisy signals and the AI updating its belief about user preferences using Bayesian inference with normal distributions
  - Quick check question: If a user with preference θ shares a signal q = θ + εq where εq ~ N(0, σ²q), what is the AI's posterior distribution for θ given q?

- Concept: Rational inattention and communication cost
  - Why needed here: Users decide how much information to share based on the trade-off between fidelity improvement and the cost of communication, modeled as reduction in entropy
  - Quick check question: How does the communication cost change as the amount of information shared (1/σ²q) increases?

- Concept: Variance and bias decomposition
  - Why needed here: The fidelity error is decomposed into variance (uncertainty) and bias (systematic deviation), which helps understand how information sharing affects output quality
  - Quick check question: What are the two components of the fidelity error e(θ, σq) and how do they change with σq?

## Architecture Onboarding

- Component map:
  User preferences θ ~ N(µθ, σ²θ) -> User optimization (chooses σq) -> AI Bayesian update -> Output θA -> Population distribution θ⋆

- Critical path:
  1. User chooses information sharing level σq to minimize l(θ, σq) = e(θ, σq) + γI(σq)
  2. AI updates belief using Bayesian inference and outputs θA = E[θ|q]
  3. Population outputs form new distribution θ⋆
  4. If AI is retrained on θ⋆, prior πA updates to match output distribution

- Design tradeoffs:
  - High γ (costly interaction) → less information sharing → more homogenization and bias propagation
  - Low γ → more information sharing → better fidelity but higher communication effort
  - Training on AI-generated content → homogenization spiral; training on diverse human data → diversity preservation

- Failure signatures:
  - Output distribution variance much lower than preference distribution variance
  - Majority of users accepting default AI output without interaction
  - AI outputs systematically biased in one direction across population

- First 3 experiments:
  1. Vary γ and measure output diversity vs. population preference diversity
  2. Introduce directional bias in AI prior and measure societal bias with different γ values
  3. Simulate self-training loop and observe homogenization over iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal interaction costs and user uniqueness levels that balance productivity gains and preference preservation?
- Basis in paper: [explicit] The paper discusses the trade-off between communication cost and output fidelity, and how this affects user decisions
- Why unresolved: The paper provides a theoretical framework but does not determine the specific optimal values for interaction costs and uniqueness levels
- What evidence would resolve it: Empirical studies measuring the actual communication costs and user preferences in real-world AI interactions, and how these relate to productivity and preference satisfaction

### Open Question 2
- Question: How do different AI bias types (directional vs. censoring) impact long-term societal preferences when users interact with the AI?
- Basis in paper: [explicit] The paper explores how AI bias can become societal bias and the effects of directional and censoring bias on user utility and population diversity
- Why unresolved: The paper models these effects but does not provide empirical evidence on how they manifest in real-world AI systems and user interactions
- What evidence would resolve it: Longitudinal studies tracking changes in societal preferences as users interact with biased AI systems over time

### Open Question 3
- Question: What are the long-term effects of AI-generated content being used to train future AI models on the diversity of user preferences and outputs?
- Basis in paper: [explicit] The paper discusses the "death spiral" of homogenization, where AI-generated content is used to train the next generation of AI, leading to more homogenized outputs
- Why unresolved: The paper models this process but does not provide empirical evidence on its actual occurrence and impact
- What evidence would resolve it: Studies analyzing the diversity of AI outputs over multiple generations of AI training, using real-world AI systems and user data

## Limitations
- The model assumes normally distributed preferences and priors, which may not capture real-world preference distributions
- The communication cost function is based on information entropy reduction, which may not reflect actual human-AI communication costs
- The death spiral mechanism assumes AI training exclusively on AI-generated content, which is not typical in current AI development

## Confidence

- **High confidence**: The core mathematical framework for modeling user choice under rational inattention - the derivations and results are well-established in information economics literature
- **Medium confidence**: The homogenization effects and bias propagation mechanisms - while theoretically sound, empirical validation with real human-AI interaction data is needed
- **Low confidence**: The death spiral scenario - this requires specific conditions that are unlikely in practice, making it more of a theoretical extreme case

## Next Checks

1. **Empirical validation of the communication cost function**: Conduct user studies to measure actual communication costs when interacting with AI systems, comparing them to the information-theoretic cost model proposed

2. **Test model sensitivity to distributional assumptions**: Run simulations with non-normal preference distributions (e.g., bimodal, heavy-tailed) to assess how robust the homogenization and bias effects are to distributional assumptions

3. **Validate death spiral mechanism with mixed training data**: Simulate scenarios where AI is trained on both human-generated and AI-generated content with varying proportions to identify the threshold conditions under which homogenization becomes problematic