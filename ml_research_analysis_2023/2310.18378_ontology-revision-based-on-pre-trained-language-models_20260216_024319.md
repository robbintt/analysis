---
ver: rpa2
title: Ontology Revision based on Pre-trained Language Models
arxiv_id: '2310.18378'
source_url: https://arxiv.org/abs/2310.18378
tags:
- ontology
- axioms
- mips
- revision
- unsatisfiable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to ontology revision by leveraging
  pre-trained language models. The key idea is to define scoring functions that rank
  axioms based on their semantic similarity to other axioms, using embeddings from
  a pre-trained model like BERT.
---

# Ontology Revision based on Pre-trained Language Models

## Quick Facts
- arXiv ID: 2310.18378
- Source URL: https://arxiv.org/abs/2310.18378
- Reference count: 40
- Primary result: Proposed algorithms achieve up to 96% time savings compared to baseline methods while resolving ontology incoherence

## Executive Summary
This paper introduces a novel approach to ontology revision that leverages pre-trained language models, specifically BERT, to encode axiom semantics for more effective incoherence resolution. The key innovation lies in defining scoring functions that rank axioms based on their semantic similarity rather than just frequency, allowing for more nuanced revision decisions. Two algorithms are presented: one that computes diagnoses based on all minimal incoherence-preserving subsets (MIPS), and another that processes unsatisfiable concepts in groups for improved efficiency. Experiments on 19 ontology pairs demonstrate significant performance improvements, with the adapted algorithm achieving up to 96% time savings while maintaining effective revision quality.

## Method Summary
The approach translates ontology axioms into natural language sentences using NaturalOWL2, then computes BERT embeddings for semantic representation. Four scoring functions are defined based on cosine and Euclidean similarity measures to rank axioms for potential removal. The primary algorithm computes all MIPS to find optimal revision solutions, while an adapted version processes unsatisfiable concepts in batches to improve computational efficiency. The method is evaluated on 19 ontology pairs from OAEI 2021 and km1500 datasets, comparing performance in terms of computation time and number of removed axioms against baseline methods.

## Key Results
- The adapted algorithm saves up to 96% time compared to the all-MIPS approach while maintaining revision effectiveness
- Semantic similarity-based scoring functions outperform frequency-based methods in identifying critical axioms for removal
- The approach successfully resolves incoherence in ontologies with varying expressivity levels (ALC and SHOIN)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained language models encode axiom semantics that traditional frequency-based methods miss
- Mechanism: BERT embeddings capture contextual semantic relationships between axioms, allowing scoring functions to differentiate axioms based on similarity rather than just occurrence counts
- Core assumption: Semantic similarity between axioms correlates with their logical interdependence and importance in maintaining ontology coherence
- Evidence anchors:
  - [abstract] "pre-trained models can be utilized to encode axiom semantics, and have been widely applied in many natural language processing tasks and ontology-related ones in recent years"
  - [section] "The three axioms also constitute a minimal axiom set to explain why PhDStudent is unsatisfiable. To resolve the unsatisfiability, one of the axioms needs to be removed. If ranking the axioms according to their frequencies, they have the same score since each axiom appears once in the minimal axiom set. When taking the semantics of axioms into account, the axioms can be ranked differently."
- Break condition: If the pre-trained model fails to capture the relevant semantic relationships between axioms, or if semantic similarity doesn't correlate with logical importance

### Mechanism 2
- Claim: Computing local MIPS groups instead of all MIPS dramatically improves efficiency without significantly compromising solution quality
- Mechanism: By processing unsatisfiable concepts in batches (groups), the algorithm reduces the computational complexity of finding MIPS from exponential to manageable levels while still finding effective revisions
- Core assumption: Local solutions for grouped unsatisfiable concepts can be combined to form effective global solutions
- Evidence anchors:
  - [abstract] "an adapted revision algorithm is designed to deal with unsatisfiable concepts group by group"
  - [section] "it may be infeasible to obtain them within limited resources. To deal with this problem, our previous work in [11] proposed an adapted revision algorithm to resolve unsatisfiable concepts one by one."
- Break condition: If unsatisfiable concepts interact in complex ways that require global optimization rather than local grouping

### Mechanism 3
- Claim: Multiple scoring functions capture different aspects of axiom importance for revision decisions
- Mechanism: Different scoring functions (based on problematic axioms, all MIPS, rebuttal ontology, reliable ontology) provide complementary perspectives on which axioms to remove, allowing users to choose based on their specific needs
- Core assumption: Different semantic contexts (local MIPS vs. global ontology) provide valuable different information for revision decisions
- Evidence anchors:
  - [abstract] "four scoring functions are defined for the task of ontology revision based on a pre-trained model by considering various information from an ontology"
  - [section] "Based on such a scoring function, we propose an ontology revision algorithm to deal with unsatisfiable concepts at once. If it is hard to resolve all unsatisfiable concepts in a rebuttal ontology together, an adapted revision algorithm is designed to deal with them group by group."
- Break condition: If one scoring function consistently outperforms others across all test cases, suggesting redundancy

## Foundational Learning

- Concept: Description Logic ontologies and OWL semantics
  - Why needed here: The entire approach relies on understanding DL ontology structure, axioms, and the distinction between consistency and coherence
  - Quick check question: What's the difference between an inconsistent ontology and an incoherent ontology?

- Concept: Pre-trained language models and semantic embeddings
  - Why needed here: The core innovation uses BERT to encode axiom semantics as vectors for similarity computation
  - Quick check question: How does BERT represent sentences as vectors, and what does cosine similarity measure in this context?

- Concept: Minimal unsatisfiability-preserving subsets (MUPS) and MIPS
  - Why needed here: These are the fundamental structures used to identify which axioms to remove for restoring coherence
  - Quick check question: Why is computing all MIPS often computationally prohibitive?

## Architecture Onboarding

- Component map: Input ontology pair (reliable + rebuttal) → Vector computation (BERT) → Scoring function selection → MIPS computation (full or local) → Subset extraction → Diagnosis computation → Output revised ontology
- Critical path: The most time-consuming steps are vector computation and MIPS computation; the scoring function and subset extraction are relatively fast
- Design tradeoffs: Full MIPS gives more complete solutions but is slower; local MIPS is faster but may remove more axioms; different scoring functions capture different semantic aspects
- Failure signatures: Algorithm fails to complete within time/memory limits; produces inconsistent results across runs; removes too many axioms compared to baseline methods
- First 3 experiments:
  1. Test vector computation time on a small ontology pair to verify BERT integration
  2. Compare MIPS computation time between full and local approaches on a medium-sized pair
  3. Evaluate scoring function performance differences on a simple ontology with known optimal revision

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ontology revision algorithms change when using more advanced pre-trained models beyond BERT, such as GPT or domain-specific models?
- Basis in paper: [inferred] The paper discusses using BERT for semantic encoding of axioms but does not explore other pre-trained models
- Why unresolved: The paper only uses BERT and does not compare with other pre-trained models
- What evidence would resolve it: Experimental results comparing revision performance using different pre-trained models on the same ontology pairs

### Open Question 2
- Question: What is the optimal step length for the adapted revision algorithm when dealing with ontologies of varying expressivity and size?
- Basis in paper: [inferred] The paper discusses varying step lengths but does not provide guidelines for choosing the optimal length based on ontology characteristics
- Why unresolved: The paper only tests step lengths up to 10 and does not analyze the relationship between step length and ontology properties
- What evidence would resolve it: A comprehensive study analyzing the impact of step length on revision performance across ontologies with different expressivity, size, and number of unsatisfiable concepts

### Open Question 3
- Question: How does the proposed ontology revision approach perform on ontologies containing nominals, datatypes, or other advanced OWL features beyond ALC and SHOIN?
- Basis in paper: [explicit] The paper focuses on ontologies with ALC and SHOIN expressivity, but does not test more expressive OWL features
- Why unresolved: The experiments are limited to ontologies with ALC and SHOIN expressivity
- What evidence would resolve it: Experimental results applying the revision algorithms to ontologies containing nominals, datatypes, or other advanced OWL features

## Limitations
- Performance comparison with existing ontology revision methods is not extensively validated across diverse ontology domains
- The impact of different BERT model configurations on semantic encoding quality is not explored
- Scalability to very large ontologies (>10K axioms) remains untested

## Confidence
- **High Confidence**: The core mechanism of using semantic similarity for axiom ranking is well-supported by the presented evidence
- **Medium Confidence**: The efficiency gains from local MIPS computation are demonstrated but could benefit from more extensive testing
- **Low Confidence**: The claim that this is the first work to use pre-trained models for ontology revision lacks thorough comparison with alternative semantic approaches

## Next Checks
1. Test the approach on ontologies from different domains (e.g., biomedical vs. financial) to verify domain generalizability
2. Compare results with other semantic similarity methods (e.g., WordNet-based approaches) to establish the unique contribution of BERT embeddings
3. Conduct ablation studies removing the pre-trained model component to quantify its specific contribution to revision performance