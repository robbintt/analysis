---
ver: rpa2
title: 'ReMasker: Imputing Tabular Data with Masked Autoencoding'
arxiv_id: '2309.13793'
source_url: https://arxiv.org/abs/2309.13793
tags:
- remasker
- latexit
- values
- missing
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReMasker, a method for imputing missing values
  in tabular data using masked autoencoding. Unlike previous approaches, ReMasker
  employs a "re-masking" strategy, where it randomly masks additional values besides
  the naturally missing ones, trains an autoencoder to reconstruct these re-masked
  values, and then applies the trained model to predict the missing values.
---

# ReMasker: Imputing Tabular Data with Masked Autoencoding

## Quick Facts
- arXiv ID: 2309.13793
- Source URL: https://arxiv.org/abs/2309.13793
- Authors: 
- Reference count: 40
- Key outcome: ReMasker achieves state-of-the-art performance on 12 benchmark datasets using a Transformer-based masked autoencoder with re-masking strategy

## Executive Summary
ReMasker introduces a novel approach to tabular data imputation using masked autoencoding with a re-masking strategy. Unlike traditional methods that only handle naturally missing values, ReMasker randomly masks additional values during training, forcing the model to learn missingness-invariant representations. Built on a Transformer architecture, the method effectively captures complex inter-feature correlations and demonstrates superior performance across various missingness mechanisms (MCAR, MAR, MNAR) compared to 13 state-of-the-art baselines.

## Method Summary
ReMasker is a Transformer-based masked autoencoder that imputes missing values through a re-masking strategy. During training, it randomly masks additional values beyond naturally missing ones, then reconstructs both re-masked and naturally missing values using a reconstruction loss. The asymmetric architecture features a deep encoder (8 blocks) and shallow decoder (4 blocks) to learn rich representations while avoiding overfitting. After training, the model applies learned representations to predict missing values in new data. The method is evaluated across 12 UCI benchmark datasets under various missingness scenarios using RMSE, Wasserstein distance, and AUROC metrics.

## Key Results
- ReMasker outperforms 13 state-of-the-art imputation methods on 12 benchmark datasets
- Achieves superior performance across MCAR, MAR, and MNAR missingness mechanisms
- Transformer architecture with re-masking learns missingness-invariant representations that generalize well
- Performance peaks with 8 encoder blocks, 4 decoder blocks, and embedding width of 64

## Why This Works (Mechanism)

### Mechanism 1
ReMasker learns missingness-invariant representations by reconstructing both re-masked and naturally missing values, forcing the model to generalize across different missingness patterns. By re-masking random values in addition to naturally missing ones, the autoencoder is trained to reconstruct both sets. This dual reconstruction task encourages the encoder to produce embeddings that are insensitive to missingness patterns. The core assumption is that re-masked values are sampled from the same distribution as naturally missing values, ensuring the model learns to handle missingness in general rather than memorizing specific patterns.

### Mechanism 2
Transformer-based architecture effectively captures inter-feature correlations in tabular data, enabling accurate reconstruction of masked values. The self-attention mechanism in Transformer blocks allows the model to attend to all other features when reconstructing a masked value, capturing complex dependencies that traditional methods miss. The core assumption is that tabular data contains sufficient inter-feature correlations that can be effectively modeled by self-attention, even with limited data.

### Mechanism 3
Asymmetric encoder-decoder depth (deep encoder, shallow decoder) provides sufficient capacity for learning representations while avoiding overfitting during reconstruction. The deep encoder learns rich representations from observed values, while the shallow decoder efficiently reconstructs masked values from these representations without memorizing training data. The core assumption is that learning high-quality representations is more important than complex decoding for imputation accuracy.

## Foundational Learning

- **Concept: Masked Autoencoding**
  - Why needed here: Provides the foundation for ReMasker's approach to handling missing values by training on artificially masked data
  - Quick check question: What is the key difference between ReMasker's masking strategy and traditional MAE?

- **Concept: Self-Attention Mechanisms**
  - Why needed here: Enables the model to capture complex inter-feature dependencies crucial for accurate imputation
  - Quick check question: How does self-attention in Transformers differ from convolutional approaches for tabular data?

- **Concept: Missingness Mechanisms (MCAR, MAR, MNAR)**
  - Why needed here: Understanding different missingness patterns is essential for evaluating ReMasker's robustness
  - Quick check question: Why might ReMasker perform differently under MCAR versus MNAR scenarios?

## Architecture Onboarding

- **Component map:** Input layer → Embedding layer (with positional encoding) → Encoder (Transformer blocks) → Mask tokens → Decoder (Transformer blocks) → Output layer
- **Critical path:** Input → Encoder → Latent representation → Decoder → Reconstructed values
  - The quality of latent representations directly determines imputation accuracy
- **Design tradeoffs:**
  - Masking ratio: Higher ratios encourage missingness-invariant learning but reduce training signal
  - Model depth: Deeper models capture more complex patterns but risk overfitting
  - Loss function: Including unmasked values improves training but adds complexity
- **Failure signatures:**
  - High reconstruction loss on validation set indicates overfitting
  - Performance degrades significantly with different missingness mechanisms
  - Results inconsistent across datasets of similar size
- **First 3 experiments:**
  1. Vary masking ratio (0.1 to 0.7) on a single dataset to find optimal balance
  2. Compare Transformer vs Linear vs Convolutional backbones with fixed architecture
  3. Test with only re-masked values in loss vs including unmasked values to validate design choice

## Open Questions the Paper Calls Out

### Open Question 1
How does ReMasker's performance scale with extremely high missingness ratios (e.g., 0.8-0.9) on large datasets?
- Basis in paper: The paper only tests up to 0.7 missingness ratio
- Why unresolved: Performance at extreme missingness unexplored
- What evidence would resolve it: Experimental results showing ReMasker's performance on datasets with 0.8-0.9 missingness ratios

### Open Question 2
What is the computational complexity of ReMasker compared to other imputation methods, especially for large-scale datasets?
- Basis in paper: Paper focuses on imputation quality but doesn't provide detailed computational complexity analysis
- Why unresolved: No runtime comparisons or complexity analysis for different methods
- What evidence would resolve it: Runtime comparisons and computational complexity analysis of ReMasker vs other methods on datasets of varying sizes

### Open Question 3
How does ReMasker perform on datasets with a mix of numerical and categorical features, especially with complex missingness patterns?
- Basis in paper: Paper mentions categorical features in problem formalization but doesn't extensively test mixed-type datasets
- Why unresolved: Evaluation focuses on numerical features with limited exploration of categorical data handling
- What evidence would resolve it: Performance evaluation of ReMasker on diverse datasets containing both numerical and categorical features with various missingness patterns

### Open Question 4
Can ReMasker's architecture be adapted for online/streaming data imputation scenarios?
- Basis in paper: Paper presents batch learning approach without addressing streaming data scenarios
- Why unresolved: Doesn't explore incremental learning or online adaptation capabilities
- What evidence would resolve it: Experimental results showing ReMasker's performance in online/streaming data scenarios and comparison with specialized streaming imputation methods

## Limitations

- The paper doesn't fully specify implementation details for the re-masking mechanism, creating uncertainty about exact procedures
- Evaluation focuses on controlled missingness patterns and may not reflect real-world scenarios with mixed or unknown missingness mechanisms
- Claims about performance relative to state-of-the-art methods lack detailed hyperparameter optimization comparisons across baselines

## Confidence

**High Confidence**: The core mechanism of using re-masking to learn missingness-invariant representations is well-supported by both theoretical formulation and empirical evidence. The mathematical framework for optimizing missingness-invariant representations is clearly articulated and consistently applied.

**Medium Confidence**: The Transformer architecture's effectiveness for tabular imputation is supported by results, but the specific architectural choices (depth ratios, embedding width) appear somewhat arbitrary. While the self-attention mechanism is theoretically sound, its superiority over other architectures isn't conclusively demonstrated.

**Low Confidence**: The claim that ReMasker "performs on par with or outperforms" state-of-the-art methods requires careful scrutiny. The comparison uses multiple metrics across diverse datasets, but the paper doesn't address potential hyperparameter tuning differences or whether all baselines were optimized to their full potential.

## Next Checks

1. **Ablation on Re-masking Strategy**: Systematically test whether the exclusion of naturally missing values from re-masking is crucial, or if including them improves or degrades performance. This directly tests the core assumption about learning missingness-invariant representations.

2. **Cross-dataset Architecture Transfer**: Train ReMasker on one dataset and evaluate on structurally similar but distinct datasets to assess whether the learned missingness-invariant representations truly generalize or are dataset-specific.

3. **Missingness Pattern Robustness**: Create mixed missingness scenarios (combining MCAR, MAR, and MNAR within the same dataset) and evaluate ReMasker's performance degradation compared to baselines, testing the claim of robust missingness handling.