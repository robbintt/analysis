---
ver: rpa2
title: 'FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based
  Feature Extraction'
arxiv_id: '2308.07527'
source_url: https://arxiv.org/abs/2308.07527
tags:
- features
- feature
- data
- featgenn
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FeatGeNN, a novel convolutional approach for
  automated feature engineering in tabular data. It leverages correlation-based pooling
  to extract and create new features, outperforming traditional max-pooling methods.
---

# FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction

## Quick Facts
- arXiv ID: 2308.07527
- Source URL: https://arxiv.org/abs/2308.07527
- Reference count: 30
- Primary result: Achieves 5.89% average improvement over baseline models on 6 benchmark datasets

## Executive Summary
FeatGeNN introduces a novel convolutional approach for automated feature engineering in tabular data. The key innovation is correlation-based pooling, which groups highly correlated features to preserve relationships that traditional max-pooling would lose. The model combines CNN local feature extraction with MLP global feature generation, achieving competitive results compared to state-of-the-art methods. Using a genetic algorithm for optimization, FeatGeNN demonstrates potential as a powerful tool for creating informative features in machine learning tasks involving tabular data.

## Method Summary
FeatGeNN employs a CNN architecture with two main blocks: Local Feature Extraction and Global Feature Generation. The correlation-pooling operation computes Pearson correlation coefficients between features, grouping highly correlated ones together to preserve linear relationships. Before feature generation, MRMR feature selection reduces dimensionality and removes redundant features. The model uses a genetic algorithm to optimize the CNN architecture over multiple generations. FeatGeNN is evaluated on six UCI classification datasets using 5-fold cross-validation with Random Forest as the base model.

## Key Results
- Achieves 5.89% average improvement over baseline models on six benchmark datasets
- Outperforms traditional max-pooling methods by preserving relationships between correlated features
- Demonstrates competitive performance compared to state-of-the-art AutoFE methods
- Successfully applies correlation-based pooling to improve feature quality in tabular data

## Why This Works (Mechanism)

### Mechanism 1
Correlation-based pooling outperforms max-pooling for tabular data because it preserves relationships between correlated features instead of selecting only the maximum value within a region. The correlation-pooling operation computes Pearson correlation coefficients between features, grouping highly correlated ones together. This preserves linear relationships between closely related features that would be lost with max-pooling's winner-take-all approach. The core assumption is that linear relationships between features in tabular data are meaningful for downstream model performance and should be preserved during feature extraction.

### Mechanism 2
Combining CNN local feature extraction with MLP global feature generation addresses the limitation of each approach alone for tabular data. CNN captures local feature interactions through convolutional filters, while MLP merges these local features to create global, higher-order features that capture complex interactions across the entire feature space. The core assumption is that effective feature engineering for tabular data requires both local pattern detection and global feature synthesis.

### Mechanism 3
Feature selection before feature generation reduces noise and computational burden while improving model performance. MRMR feature selection removes redundant and irrelevant features before CNN processing, reducing the feature space and preventing the model from learning spurious correlations. The core assumption is that the quality of input features significantly impacts the effectiveness of subsequent feature generation steps.

## Foundational Learning

- Pearson correlation coefficient
  - Why needed here: Understanding how correlation-pooling measures linear relationships between features is essential for implementing and debugging this component.
  - Quick check question: What does a Pearson correlation coefficient of 0.8 between two features indicate about their relationship?

- Convolutional neural networks
  - Why needed here: The local feature extraction relies on CNN operations, so understanding convolution and pooling mechanisms is fundamental.
  - Quick check question: How does a convolutional filter differ from a fully connected layer in terms of parameter sharing and spatial invariance?

- Genetic algorithms
  - Why needed here: The evolution process uses genetic algorithms to optimize the CNN architecture and parameters over multiple generations.
  - Quick check question: What are the three main genetic operators used in genetic algorithms, and what does each accomplish?

## Architecture Onboarding

- Component map:
  - Input: Raw tabular data matrix
  - MRMR Feature Selection: Reduces dimensionality and removes redundant features
  - Local Feature Extraction (CNN): Convolution + Correlation-based pooling
  - Global Feature Generation (MLP): Combines local features into higher-order features
  - Genetic Algorithm Evolution: Optimizes CNN architecture over generations
  - Output: Enhanced dataset with original + generated features

- Critical path: Input → MRMR Selection → CNN Local Extraction → MLP Global Generation → Evaluation → Genetic Algorithm Optimization loop

- Design tradeoffs:
  - Correlation-pooling vs max-pooling: Correlation-pooling preserves relationships but requires computing pairwise correlations, increasing computation time
  - CNN depth vs performance: Deeper networks may capture more complex patterns but risk overfitting on small tabular datasets
  - Genetic algorithm iterations vs training time: More generations improve optimization but increase training time significantly

- Failure signatures:
  - Correlation-pooling fails: Generated features show no improvement over baseline, correlation computation becomes bottleneck
  - CNN fails: Local features are not informative, training loss plateaus early
  - Genetic algorithm fails: Population converges to suboptimal solutions, or no improvement across generations

- First 3 experiments:
  1. Compare correlation-pooling vs max-pooling on a simple dataset (e.g., Ionosphere) to verify the core mechanism
  2. Test different MRMR feature selection thresholds to find optimal dimensionality reduction
  3. Evaluate the impact of genetic algorithm population size and mutation rate on final model performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FeatGeNN vary with different threshold values for the correlation pooling operation? The paper mentions a threshold of 70% of data for correlation calculation and suggests future research on exploring information theory methods for pooling operations. This remains unresolved because the paper only tested one threshold value and did not explore the impact of different thresholds on model performance.

### Open Question 2
Can FeatGeNN be effectively applied to regression problems, or is it limited to classification tasks? The paper only evaluates FeatGeNN on classification problems and uses f1-score as the evaluation metric, which is not suitable for regression tasks. This question remains unresolved because the paper does not provide any results or discussion on the application of FeatGeNN to regression problems.

### Open Question 3
How does the computational efficiency of FeatGeNN compare to other AutoFE methods when dealing with large-scale datasets? The paper mentions that some AutoFE methods can be computationally intensive and lead to overfitting, but does not provide a detailed comparison of computational efficiency. This question remains unresolved because the paper does not include any analysis or comparison of the computational resources required by FeatGeNN relative to other methods.

## Limitations

- Limited dataset diversity - only six UCI datasets used for evaluation
- No ablation study isolating the impact of correlation-pooling vs other architectural components
- Computational complexity of correlation-pooling not thoroughly analyzed
- Lack of comparison with recent deep learning approaches for tabular data

## Confidence

- High Confidence: The core mechanism of correlation-based pooling preserving relationships between correlated features is well-supported by the architectural description and comparison with max-pooling limitations.
- Medium Confidence: The claim that FeatGeNN achieves competitive results with 5.89% average improvement is supported by the experimental results, though statistical significance testing is not explicitly provided.
- Medium Confidence: The architectural design combining CNN local extraction with MLP global generation is logically sound, but the effectiveness depends heavily on proper hyperparameter tuning and the specific datasets used.

## Next Checks

1. Conduct statistical significance testing (paired t-tests) on the 5.89% improvement across all six datasets to verify the claimed performance gains
2. Perform an ablation study to isolate the contribution of correlation-pooling versus other components (MRMR selection, CNN architecture, MLP generation)
3. Test FeatGeNN on additional tabular datasets beyond the UCI repository to assess generalizability across different data distributions and problem domains