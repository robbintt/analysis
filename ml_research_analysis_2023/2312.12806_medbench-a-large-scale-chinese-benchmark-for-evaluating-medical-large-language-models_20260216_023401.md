---
ver: rpa2
title: 'MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language
  Models'
arxiv_id: '2312.12806'
source_url: https://arxiv.org/abs/2312.12806
tags:
- medical
- llms
- chinese
- questions
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MedBench is a large-scale Chinese medical benchmark for evaluating
  large language models (LLMs). It includes 40,041 questions from authentic medical
  exams and real-world clinical cases, covering four key components: Chinese Medical
  Licensing Exam, Resident Standardization Training Exam, Doctor In-Charge Qualification
  Exam, and real-world clinic cases.'
---

# MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models

## Quick Facts
- arXiv ID: 2312.12806
- Source URL: https://arxiv.org/abs/2312.12806
- Reference count: 4
- Primary result: MedBench contains 40,041 Chinese medical questions and shows Chinese medical LLMs significantly underperform, while general-domain LLMs show surprising medical knowledge

## Executive Summary
MedBench is a comprehensive Chinese medical benchmark designed to evaluate large language models on their clinical knowledge and diagnostic precision. The benchmark comprises 40,041 questions drawn from authentic Chinese medical exams and real-world clinical cases, covering the educational progression of Chinese doctors. Experiments reveal that Chinese medical LLMs underperform significantly on this benchmark, while general-domain LLMs demonstrate considerable medical knowledge, suggesting potential knowledge transfer. The benchmark employs Item Response Theory for difficulty stratification and includes human evaluation for real-world clinical cases.

## Method Summary
The MedBench benchmark collects 40,041 questions from Chinese medical licensing exams, resident training exams, doctor qualification exams, and real-world clinical cases. Evaluation uses five-shot prompting with Chain-of-Thought techniques, and results are analyzed using IRT-based difficulty stratification. The benchmark measures accuracy for multiple-choice questions and uses BLEU/ROUGE metrics for case-based questions, supplemented by human evaluation for real-world scenarios. The methodology includes both automated and human assessment to comprehensively evaluate LLM performance across different medical domains and difficulty levels.

## Key Results
- Chinese medical LLMs underperform significantly on MedBench, highlighting gaps in clinical knowledge and diagnostic precision
- General-domain LLMs show surprising medical knowledge, suggesting potential knowledge transfer from pretraining data
- Chain-of-Thought prompting improves LLM reasoning performance on medical questions
- IRT-based difficulty stratification reveals varying LLM performance across different question difficulty levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chinese medical LLMs underperform on MedBench because they lack exposure to comprehensive Chinese medical knowledge during pretraining.
- Mechanism: The benchmark mirrors real educational progression and clinical practice experiences of Chinese doctors, requiring both Western and Traditional Chinese Medicine knowledge. LLMs pretrained primarily on Western medical data or general Chinese corpora without specific medical fine-tuning struggle to answer these questions accurately.
- Core assumption: The training data distribution of Chinese medical LLMs is not representative of the breadth and depth of knowledge required for Chinese medical practice.
- Evidence anchors:
  - "Chinese medical LLMs underperform on this benchmark, highlighting the need for significant advances in clinical knowledge and diagnostic precision."
  - "ChatGPT exhibits subpar performance on questions pertaining to Traditional Chinese Medicine (TCM) and Chinese Western Medicine (CWM), with accuracy metrics oscillating between 40-45%."
- Break condition: If Chinese medical LLMs are retrained with a more comprehensive dataset including both Western and Traditional Chinese Medicine knowledge, they should perform significantly better on MedBench.

### Mechanism 2
- Claim: Chain-of-Thought prompting improves LLM reasoning on medical questions by encouraging step-by-step problem-solving.
- Mechanism: Chain-of-Thought prompting guides the LLM to articulate its reasoning process, helping identify relevant knowledge and apply logical steps to arrive at correct answers rather than jumping to conclusions based on surface-level cues.
- Core assumption: LLMs can benefit from explicit reasoning guidance, even if they possess necessary knowledge, because it helps structure their thought process.
- Evidence anchors:
  - "Baichuan-13B shows a strong inclination towards option F under the vanilla prompt, although this option is not part of the valid choice set (A-E). In contrast, when using Chain-of-Thought prompting, Baichuan-13B primarily gravitates towards valid choices."
  - "Table 3 details the accuracy associated with both the vanilla prompt and the Chain-of-Thought one. The data suggests a significant improvement in the accuracy of Baichuan-13B when using Chain-of-Thought prompting."
- Break condition: If the LLM lacks necessary medical knowledge, Chain-of-Thought prompting may not be sufficient to improve performance as it cannot compensate for knowledge gaps.

### Mechanism 3
- Claim: IRT-based difficulty stratification enables more efficient and nuanced evaluation of LLMs by matching them to questions of appropriate difficulty.
- Mechanism: By categorizing questions based on difficulty levels using IRT, the benchmark can present a smaller set of questions tailored to each LLM's proficiency level, avoiding inefficiencies of evaluating all LLMs on the entire dataset and allowing better differentiation between models.
- Core assumption: LLMs exhibit varying levels of proficiency, and presenting them with questions that are too easy or too difficult does not provide meaningful insights into their capabilities.
- Evidence anchors:
  - "Part (a) of Figure 8 displays the accuracy trends of these LLMs across these levels, highlighting a decrease in accuracy with increasing difficulty."
  - "Figure 8(b) shows the difference in accuracy between BLOOMZ-7.1B and Qwen-7B, as well as between Qwen-max and ChatGLM-turbo, for different levels of question difficulty. It is noticeable that the differences between the LLMs are significant when the question difficulty is appropriate."
- Break condition: If the IRT model does not accurately capture question difficulty or if the LLM's proficiency is not well-aligned with difficulty levels, the stratification may not lead to efficient or meaningful evaluation.

## Foundational Learning

- Concept: Item Response Theory (IRT)
  - Why needed here: IRT provides a framework for understanding the relationship between an examinee's ability and the probability of correctly answering a question, crucial for developing a robust and nuanced evaluation benchmark.
  - Quick check question: What are the three key parameters in the three-parameter logistic model (IRT-3PL) used in MedBench?

- Concept: Medical domain knowledge
  - Why needed here: The benchmark evaluates LLMs' mastery of medical knowledge, including both Western and Traditional Chinese Medicine, requiring deep understanding of subject matter.
  - Quick check question: What are the four key components of MedBench that cover the educational progression and clinical practice experiences of Chinese doctors?

- Concept: Prompt engineering
  - Why needed here: The effectiveness of LLMs on the benchmark can be influenced by prompts used, such as Chain-of-Thought prompting, which guides the LLM's reasoning process.
  - Quick check question: How does Chain-of-Thought prompting differ from a vanilla prompt, and what is its impact on LLM performance?

## Architecture Onboarding

- Component map: Question dataset -> IRT-based difficulty stratification -> LLM evaluation pipeline -> Human evaluation component
- Critical path: Select appropriate question subset based on LLM proficiency level -> Run LLM on questions -> Analyze results for medical knowledge and reasoning capabilities
- Design tradeoffs: Balances comprehensiveness (wide medical topic coverage) with practicality (difficulty stratification for efficient evaluation) and automated evaluation with human evaluation for real-world cases
- Failure signatures: Consistent poor performance across all difficulty levels indicates lack of medical knowledge; good performance on easy questions but poor on hard questions indicates gaps in advanced reasoning skills
- First 3 experiments:
  1. Evaluate Chinese medical LLM performance on MedBench without prompt engineering to establish baseline
  2. Apply Chain-of-Thought prompting to same LLM and compare performance to baseline to assess prompt engineering impact
  3. Use IRT-based difficulty stratification to select questions tailored to LLM proficiency level and evaluate performance on this subset to assess stratification effectiveness

## Open Questions the Paper Calls Out

- How does the performance of Chinese medical LLMs on MedBench compare to their performance on other Chinese medical benchmarks like CMExam?
  - Basis in paper: [explicit] The paper states that MedBench "surpasses prior benchmarks" and that Chinese medical LLMs "underperform on this benchmark."
  - Why unresolved: The paper does not provide direct performance comparisons between MedBench and other Chinese medical benchmarks.
  - What evidence would resolve it: Direct performance comparisons of Chinese medical LLMs on MedBench and other Chinese medical benchmarks.

- What specific factors contribute to ChatGPT's superior performance on MedBench compared to Chinese medical LLMs?
  - Basis in paper: [explicit] The paper notes that ChatGPT "consistently surpasses other models" and that this may be due to "limited exposure to Chinese data during its pretraining phase."
  - Why unresolved: The paper does not provide detailed analysis of specific factors contributing to ChatGPT's superior performance.
  - What evidence would resolve it: Detailed analysis of specific factors (training data, model architecture, fine-tuning process) contributing to ChatGPT's superior performance on MedBench.

- How can the Chain-of-Thought prompting method be further optimized to improve the performance of Chinese medical LLMs on MedBench?
  - Basis in paper: [explicit] The paper shows that Chain-of-Thought prompting improves performance of some LLMs on MedBench, but notes that "the improvement in accuracy when using Chain-of-Thought prompting is negligible for ChatGLM-6B."
  - Why unresolved: The paper does not explore further optimizations of the Chain-of-Thought prompting method.
  - What evidence would resolve it: Experimental results showing effectiveness of optimized Chain-of-Thought prompting methods on Chinese medical LLMs' performance on MedBench.

## Limitations

- The benchmark's performance gaps attributed to training data limitations lack direct evidence of model pretraining data composition
- Human evaluation component for real-world clinical cases may introduce subjectivity affecting result interpretation
- The specific mechanisms behind general-domain LLMs' surprising medical knowledge transfer are not fully explored

## Confidence

- High confidence: The benchmark's construction methodology, including IRT-based difficulty stratification and comprehensive coverage of Chinese medical education and practice
- Medium confidence: The specific performance gaps attributed to training data limitations, as direct evidence of model pretraining data is not provided
- Medium confidence: The effectiveness of Chain-of-Thought prompting improvements, which are demonstrated but may vary with different prompt formulations

## Next Checks

1. Conduct ablation studies comparing Chinese medical LLM performance before and after retraining with augmented datasets containing both Western and Traditional Chinese Medicine knowledge to directly test the training data hypothesis
2. Perform controlled experiments with different prompt engineering techniques (beyond Chain-of-Thought) to isolate the impact of prompting on medical reasoning performance
3. Implement cross-cultural validation by testing Western medical LLMs on MedBench and Chinese medical LLMs on Western medical benchmarks to quantify the knowledge transfer effects observed in current results