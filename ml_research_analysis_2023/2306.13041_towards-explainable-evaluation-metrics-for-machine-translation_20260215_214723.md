---
ver: rpa2
title: Towards Explainable Evaluation Metrics for Machine Translation
arxiv_id: '2306.13041'
source_url: https://arxiv.org/abs/2306.13041
tags:
- metrics
- metric
- translation
- explanations
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys approaches to explainable machine translation
  (MT) metrics. It defines key properties and goals of explainable MT metrics and
  provides a comprehensive synthesis of recent techniques, relating them to the established
  goals.
---

# Towards Explainable Evaluation Metrics for Machine Translation

## Quick Facts
- arXiv ID: 2306.13041
- Source URL: https://arxiv.org/abs/2306.13041
- Reference count: 40
- Key outcome: This paper surveys approaches to explainable machine translation (MT) metrics, defining key properties and goals while providing a comprehensive synthesis of recent techniques and identifying underexplored research directions.

## Executive Summary
This paper addresses the challenge of making machine translation evaluation metrics more transparent and understandable. While modern MT metrics based on large language models achieve strong performance, their black-box nature limits their adoption compared to simpler, more interpretable metrics like BLEU. The authors systematically survey explainability approaches for MT metrics, identifying key properties and goals that should guide future research. They argue that different audiences (developers, expert users, non-expert users) require different types of explanations and that explainability can serve multiple purposes beyond just understanding metric decisions.

## Method Summary
The paper conducts a comprehensive literature review of explainable MT evaluation approaches, creating a taxonomy based on explanation types (decision-understanding vs. model-understanding), model access (model-agnostic vs. model-specific), and target audiences. The authors identify four key goals for explainable MT metrics: diagnosing and improving metrics, making metrics more expressive and accessible, supporting semi-automatic labeling, and checking for social biases. Through their survey, they map existing approaches to these goals and identify underexplored research directions, particularly around faithfulness evaluation and interactive explanations using large language models.

## Key Results
- Current high-quality MT metrics remain underutilized partly due to their black-box nature
- Different audiences require tailored explanations for MT metrics
- Explainability techniques can support multiple goals beyond transparency
- LLMs present new opportunities for interactive and natural language explanations
- Faithfulness evaluation remains an underexplored area requiring new methodologies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explainability techniques can increase adoption of high-quality MT metrics by making their decision processes transparent.
- Mechanism: The paper argues that current high-quality MT metrics are "black box" and lack transparency, leading to continued reliance on simpler, lower-quality metrics like BLEU. Explainability techniques can provide insights into why a metric assigns a particular score, making the metric's behavior more understandable and trustworthy to users.
- Core assumption: Users and developers of MT systems prioritize transparency and understanding of the evaluation process over raw metric performance.
- Evidence anchors: Empirical investigations indicate that the majority of MT papers relies on surface-level evaluation metrics such as BLEU and METEOR, which were created two decades ago.

### Mechanism 2
- Claim: Different audiences require different types of explanations for MT metrics.
- Mechanism: The paper identifies various audiences for MT metrics, including metric developers, metric users (both experts and non-experts), and affected users. Each audience has different needs and goals for explainability, and the explanations provided should be tailored to meet these specific requirements.
- Core assumption: A one-size-fits-all approach to explainability is not effective for MT metrics, and explanations should be customized based on the audience's background and objectives.
- Evidence anchors: For users of the AI systems, explanations help them make more informed decisions, better understand and hence gain trust of the AI systems, and even learn from the AI systems to accomplish tasks more successfully.

### Mechanism 3
- Claim: Explainability techniques can be used for various goals beyond just understanding the metric's decision process.
- Mechanism: The paper identifies four main goals for explainable MT metrics: diagnosing and improving metrics, making metrics more expressive and accessible, supporting semi-automatic labeling, and checking for social biases. Explainability techniques can be applied to achieve these goals by providing insights into the metric's behavior and output.
- Core assumption: The identified goals are relevant and important for the development and adoption of MT metrics, and explainability techniques can effectively contribute to achieving these goals.
- Evidence anchors: In this section, we discuss goals that require specific focus for explainable MT evaluation. We orient ourselves on common goals proposed by Lipton (2016); Barredo Arrieta et al. (2020); Jacovi and Goldberg (2021) and identify four specific ones that match the literature we identify.

## Foundational Learning

- Concept: Understanding the different dimensions of MT metrics (input type, granularity, quality aspect, learning objective).
  - Why needed here: To classify and categorize the various MT metrics and explainability techniques discussed in the paper.
  - Quick check question: What are the four main dimensions used to classify MT metrics in the paper?

- Concept: Distinguishing between decision-understanding and model-understanding explanations.
  - Why needed here: To categorize the explainability techniques based on their focus (specific outputs vs. general properties) and understand their applicability.
  - Quick check question: What is the difference between decision-understanding and model-understanding explanations in the context of MT metrics?

- Concept: Familiarity with the concepts of faithfulness and plausibility in explainability.
  - Why needed here: To evaluate the quality and effectiveness of the explainability techniques and understand their limitations.
  - Quick check question: What is the difference between faithfulness and plausibility in the context of explainability, and why are both important?

## Architecture Onboarding

- Component map: MT metrics (black box models) -> Explainability techniques (external explainers or intrinsic properties) -> Target audiences (metric developers, metric users, affected users) -> Goals (diagnose and improve metrics, make metrics more expressive and accessible, support semi-automatic labeling, check for social biases)

- Critical path:
  1. Identify the target audience and their specific needs for explainability.
  2. Select appropriate explainability techniques based on the target audience and goals.
  3. Implement and integrate the explainability techniques with the MT metrics.
  4. Evaluate the effectiveness of the explanations in terms of faithfulness, plausibility, and goal achievement.
  5. Iterate and refine the explainability techniques based on feedback and evaluation results.

- Design tradeoffs: Balancing faithfulness and plausibility in explanations; Choosing between model-agnostic and model-specific explainability techniques; Prioritizing between different goals (e.g., metric improvement vs. accessibility).

- Failure signatures: Explanations that are not faithful to the metric's decision process; Explanations that are not plausible or understandable to the target audience; Explanations that do not effectively contribute to the identified goals; Explainability techniques that are too computationally expensive or complex to implement.

- First 3 experiments:
  1. Implement a feature importance technique (e.g., gradient-based or attention-based) for a black box MT metric and evaluate its faithfulness and plausibility.
  2. Design and conduct a human evaluation study to assess the effectiveness of different types of explanations for a specific target audience.
  3. Integrate an explainability technique into an existing MT metric and measure its impact on metric performance and user trust.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific evaluation methods should be developed to test the faithfulness of explanations in machine translation metrics?
- Basis in paper: The paper notes that current works on decision-understanding of MT metrics mostly dismiss faithfulness and encourages future work to address this.
- Why unresolved: Faithfulness is multifaceted and there are pitfalls in faithfulness evaluation methods like AOPC scores.
- What evidence would resolve it: Development and validation of new evaluation methods specifically designed to test faithfulness in MT metric explanations, along with case studies demonstrating their effectiveness.

### Open Question 2
- Question: How can the correspondence between highlighted error words in source and target languages be improved in feature importance explanations?
- Basis in paper: The paper mentions that current feature importance approaches do not provide correspondence between highlighted error words in source and target language.
- Why unresolved: This is a technical challenge that requires novel approaches to link features across languages.
- What evidence would resolve it: Development and evaluation of new techniques that can effectively link and highlight corresponding error words in source and target languages, with user studies demonstrating improved understanding.

### Open Question 3
- Question: Can interactive explanations using large language models improve the accessibility and usefulness of machine translation metrics for non-expert users?
- Basis in paper: The paper suggests that interactive explanations due to the context size and instruction fine-tuning of recent LLMs might achieve successful communication of the explained subject to an explainee.
- Why unresolved: This is a novel application of LLMs that requires further research and validation.
- What evidence would resolve it: Development and evaluation of interactive explanation systems using LLMs, with user studies demonstrating improved accessibility and usefulness for non-expert users compared to traditional explanations.

## Limitations
- The analysis relies heavily on existing literature without providing original empirical validation of explainability techniques.
- The taxonomy may not capture all relevant approaches due to the literature selection process being under-specified.
- The claim that explainability will increase adoption of better metrics lacks direct empirical support.
- The four identified goals may not be exhaustive or universally applicable across all MT evaluation contexts.

## Confidence

- High confidence: The conceptual framework for classifying MT metrics and explainability approaches is well-founded and logically consistent
- Medium confidence: The claim that explainability can increase adoption of better metrics is plausible but lacks direct empirical support
- Medium confidence: The identification of different audience needs is reasonable but may oversimplify the complexity of user requirements
- Low confidence: Predictions about LLM-based explainability (e.g., ChatGPT, GPT-4) are speculative given the rapidly evolving nature of these technologies

## Next Checks

1. Conduct a user study comparing metric adoption rates when explainability features are present versus absent, measuring whether transparency actually drives preference for higher-quality metrics
2. Perform a systematic literature search following the paper's criteria to verify whether the taxonomy captures all relevant approaches and identify any significant gaps
3. Implement and evaluate one explainability technique across multiple MT metrics to measure trade-offs between faithfulness, plausibility, and computational overhead in practice