---
ver: rpa2
title: Lightweight Diffusion Models with Distillation-Based Block Neural Architecture
  Search
arxiv_id: '2311.04950'
source_url: https://arxiv.org/abs/2311.04950
tags:
- loss
- search
- diffusion
- teacher
- subnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of diffusion models
  by proposing a method to automatically reduce their structural redundancy. The core
  method, called DiffNAS, employs a block-wise neural architecture search (NAS) framework
  to find the smallest architecture that achieves performance comparable to or better
  than a larger pretrained teacher model.
---

# Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search

## Quick Facts
- arXiv ID: 2311.04950
- Source URL: https://arxiv.org/abs/2311.04950
- Authors: 
- Reference count: 40
- Key outcome: DiffNAS achieves 30% MACs reduction on pixel-wise models and 50% MACs and parameter reduction on latent diffusion models while maintaining or improving performance.

## Executive Summary
This paper addresses the high computational cost of diffusion models by proposing DiffNAS, a method to automatically reduce structural redundancy through block-wise neural architecture search (NAS). The approach consists of three stages: block-wise supernet training with distillation supervision, block-wise local search for the smallest subnet with better performance than a pretrained teacher model, and subnet retraining using a dynamic joint loss. Experiments demonstrate significant computational reduction while maintaining or improving image generation quality on various datasets.

## Method Summary
DiffNAS employs a block-wise NAS framework to find the smallest architecture achieving performance comparable to or better than a larger pretrained teacher model. The method operates in three stages: (1) block-wise supernet training with distillation supervision using L2 loss between teacher and student feature maps, (2) block-wise local search selecting the smallest subnet with better evaluation loss than the teacher-based subnet, and (3) subnet retraining with a dynamic joint loss combining distillation loss and noise prediction loss with gradually adjusted weights. This approach leverages the natural block-wise structure of UNet architectures in diffusion models to reduce the search space significantly.

## Key Results
- Achieved approximately 30% MACs reduction on pixel-wise diffusion models while maintaining or improving FID scores
- Reduced both MACs and parameters by about 50% on latent diffusion models
- Demonstrated effectiveness across multiple datasets including CIFAR-10, CelebA, LSUN-church, and CelebA-HQ

## Why This Works (Mechanism)

### Mechanism 1: Block-wise Local Search
Block-wise local search avoids unfairness in subnet evaluation by comparing each block independently to a baseline rather than summing relative losses globally. By block-wisely selecting the smallest subnet with better evaluation loss compared to the subnet having the same architecture as the teacher, each block is evaluated fairly without being dominated by blocks with much larger losses.

### Mechanism 2: Dynamic Joint Loss
Dynamic joint loss maintains consistency between supernet training and subnet retraining by combining distillation loss and noise prediction loss, gradually increasing the weight of noise prediction loss and decreasing the weight of distillation loss. This maintains the consistency between subnet evaluation and retraining while providing informative objectives for each block and shortening the paths of gradient propagation.

### Mechanism 3: Block-wise NAS Paradigm
The block-wise NAS paradigm reduces the search space significantly by dividing the whole UNet into blocks and performing NAS independently for each block. By regarding each resolution level as a block and performing NAS independently, the search space is reduced from the product of the number of optional operations in each layer across all blocks to the sum of the number of optional operations in each layer within each block.

## Foundational Learning

- Concept: Diffusion models
  - Why needed here: Understanding the basics of diffusion models is crucial for grasping the motivation behind the proposed method and the challenges it aims to address.
  - Quick check question: What is the key idea behind diffusion models, and how do they differ from other generative models like GANs or VAEs?

- Concept: Neural Architecture Search (NAS)
  - Why needed here: The proposed method leverages NAS techniques to automatically search for the smallest architecture that achieves on-par or better performance than a larger pretrained teacher model.
  - Quick check question: What are the main components of a typical NAS framework, and how does the block-wise NAS paradigm differ from other NAS approaches?

- Concept: Knowledge Distillation
  - Why needed here: The proposed method uses knowledge distillation to guide the training of the supernet and the retraining of the searched subnet, ensuring consistency between the two stages.
  - Quick check question: What is the main idea behind knowledge distillation, and how does it differ from other model compression techniques like pruning or quantization?

## Architecture Onboarding

- Component map:
  - Supernet -> Block-wise search -> Dynamic joint loss -> Searched subnet

- Critical path:
  1. Train the supernet block-wisely with distillation supervision
  2. Block-wisely search for the smallest subnet with better performance than the teacher
  3. Retrain the searched subnet with the dynamic joint loss

- Design tradeoffs:
  - Block-wise vs. global search: Block-wise search reduces the search space but may miss globally optimal architectures
  - Dynamic joint loss vs. single loss: Dynamic joint loss maintains consistency but adds complexity to the retraining process

- Failure signatures:
  - Poor performance of the searched subnet compared to the teacher
  - Instability or divergence during supernet training or subnet retraining
  - High computational cost or memory usage during the search or retraining stages

- First 3 experiments:
  1. Train a supernet with a simple block-wise architecture and evaluate its performance on a small dataset
  2. Implement the block-wise search strategy and compare its results to a global search strategy on a larger dataset
  3. Integrate the dynamic joint loss into the retraining process and evaluate its impact on the final performance of the searched subnet

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of DiffNAS vary with different levels of redundancy in the initial diffusion model architecture? The paper does not explore varying levels of initial redundancy, providing experiments only with specific model architectures.

### Open Question 2
Can the DiffNAS framework be extended to search and optimize for other model parameters beyond kernel sizes, such as the number of layers or channels? While the paper states that DiffNAS can be suitable for larger search spaces including layers and channel numbers, it does not explore these extensions in the experiments.

### Open Question 3
What is the impact of the dynamic joint loss function on the training stability and convergence speed of the subnetworks? The paper introduces this mechanism but focuses on performance outcomes rather than detailed training dynamics.

## Limitations

- The block-wise local search strategy's effectiveness depends on the assumption that evaluation losses vary significantly across blocks, which lacks empirical validation across different model architectures
- The dynamic joint loss mechanism requires careful tuning of the β parameter schedule, but the paper lacks systematic analysis of hyperparameter sensitivity
- The block-wise NAS paradigm assumes independent optimization within each block will not compromise global architecture optimality, but comparative experiments with global search strategies are limited

## Confidence

- High confidence: The basic effectiveness of knowledge distillation in guiding supernet training and subnet evaluation
- Medium confidence: The superiority of block-wise local search over global search strategies
- Low confidence: The generalizability of the dynamic joint loss mechanism to other model architectures and tasks

## Next Checks

1. Conduct ablation studies varying the β schedule in the dynamic joint loss to quantify its impact on performance and identify optimal schedules for different model sizes
2. Compare block-wise local search against global search strategies on the same supernet architectures using identical evaluation metrics to isolate the contribution of search strategy
3. Test the proposed method on additional generative model architectures beyond UNet-based diffusion models to assess generalizability of the block-wise NAS approach