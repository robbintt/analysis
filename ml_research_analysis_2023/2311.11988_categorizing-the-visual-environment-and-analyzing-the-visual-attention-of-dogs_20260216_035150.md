---
ver: rpa2
title: Categorizing the Visual Environment and Analyzing the Visual Attention of Dogs
arxiv_id: '2311.11988'
source_url: https://arxiv.org/abs/2311.11988
tags:
- dogs
- visual
- objects
- fixation
- maskrcnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of understanding how dogs visually
  perceive and attend to objects in their environment. The authors collect a dataset
  of over 11,000 gazes from 11 dogs walking around a college campus and urban area
  while wearing head-mounted eye trackers.
---

# Categorizing the Visual Environment and Analyzing the Visual Attention of Dogs

## Quick Facts
- arXiv ID: 2311.11988
- Source URL: https://arxiv.org/abs/2311.11988
- Reference count: 37
- Dogs selectively attend to people, cars, plants, construction, and pavement rather than uniformly observing their environment

## Executive Summary
This paper presents a novel approach to understanding canine visual attention by collecting gaze data from dogs wearing head-mounted eye trackers during walks around a college campus and urban area. The authors develop an end-to-end pipeline that combines fine-tuned Mask R-CNN segmentation with eye-tracking data to automatically predict which objects dogs are fixating on. Statistical analysis reveals that dogs actively direct their attention to specific object classes rather than passively observing their entire field of view. The method achieves strong quantitative performance despite limited training data through transfer learning from MS COCO, demonstrating the potential for studying visual attention in other species using similar approaches.

## Method Summary
The method involves collecting eye-tracking data from dogs during walks, then fine-tuning a pre-trained Mask R-CNN model on annotated images to segment objects in the dog's field of view. The eye-tracking data provides fixation points with a radius of error, which are combined with the segmentation masks to estimate probability distributions over potential fixated objects. The pipeline uses a gradual freezing approach during fine-tuning, first training the RPN and FPN layers for 15 epochs, then the mask generation and classification heads for 5 epochs. Performance is evaluated using class-based accuracy, mask-based precision, and statistical comparison of attention patterns across object classes.

## Key Results
- Fine-tuned Mask R-CNN achieves median class-based accuracy of 74% and mask-based precision of 56%
- Dogs show selective attention to people, cars, plants, construction, and pavement over other object classes
- Transfer learning enables strong performance with limited training data (< 20% of total dataset)
- Statistical analysis confirms significant differences in attention across object classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pre-trained Mask R-CNN with transfer learning enables strong performance on the novel dog vision dataset despite limited training data (< 20% of total data).
- Mechanism: Pre-trained models like Mask R-CNN learn generic low and mid-level visual features on large datasets (e.g. MS COCO). These features can be transferred and fine-tuned to new domains with limited data, mitigating overfitting and reducing time to convergence.
- Core assumption: The visual features learned on MS COCO are sufficiently generic and transferable to the dog vision domain.
- Evidence anchors:
  - [abstract] "Transfer learning is used for our segmentation task in a novel image domain by fine-tuning the MaskRCNN on 610 images in our training set... Transfer learning enables SOTA performance with limited fine-tuning."
  - [section] "Previous work has demonstrated that CNNs trained on large datasets (e.g. ImageNet) learn useful generic low and mid level representations that can be transferred to new domains with limited data."
  - [corpus] Weak evidence, no direct mentions of transfer learning or Mask R-CNN performance on novel domains.
- Break condition: If the visual features in the dog domain are too different from those in the MS COCO dataset, the pre-trained Mask R-CNN may not generalize well, leading to poor segmentation performance even after fine-tuning.

### Mechanism 2
- Claim: Integrating head-mounted eye-tracking data with Mask R-CNN segmentation enables automatic prediction of which objects dogs are fixating on.
- Mechanism: The eye-tracking camera provides a radius of error around the estimated fixation point, defining a "fixation region". The Mask R-CNN segments objects in the scene. The intersection of predicted object masks with the fixation region is used to estimate a probability distribution over potential fixated objects.
- Core assumption: The true fixation point lies within the circular "fixation region" defined by the eye-tracking camera's radius of error.
- Evidence anchors:
  - [abstract] "The calibrated eye-tracking data is integrated with MaskRCNN predictions to provide a label predicting the class object the dog was attending to at each fixation."
  - [section] "Using the unique spatial accuracy from the eye-mounted camera for each dog, we extended a 'radius of error' around the estimated fixation pixel... To estimate the probability that a particular fixation attended to object class A, we considered the subset of predicted masks overlapping the fixation region..."
  - [corpus] Weak evidence, no direct mentions of integrating eye-tracking with object segmentation.
- Break condition: If the spatial accuracy of the eye-tracking camera is too low, the fixation region may be too large, leading to unreliable probability distributions over fixated objects.

### Mechanism 3
- Claim: Dogs do not uniformly attend to objects in their view, but selectively direct their attention to certain object classes like people, cars, plants, construction, and pavement.
- Mechanism: The fine-tuned Mask R-CNN segments objects in the dog's field of view. The eye-tracking data identifies the fixation point. The intersection of object masks with the fixation region is used to compute the probability distribution over fixated objects. Statistical analysis reveals the relative frequency of fixations on different object classes.
- Core assumption: The probability distribution over fixated objects, estimated from the intersection of object masks and the fixation region, accurately reflects the true distribution of objects dogs are attending to.
- Evidence anchors:
  - [abstract] "Statistical analysis of the visual attention data reveals that dogs do not uniformly attend to objects in their view, but rather selectively direct their attention to certain object classes such as people, cars, plants, construction, and pavement."
  - [section] "We find a significant effect of object class, suggesting that dogs do not passively observe their field of view but rather actively directing their attention to certain objects... Post-hoc pairwise comparisons revealed that dogs looked significantly more to people than other non-social classes like sculptures, and the sky."
  - [corpus] Weak evidence, no direct mentions of dogs' selective attention to certain object classes.
- Break condition: If the Mask R-CNN segmentation is inaccurate, the estimated probability distributions over fixated objects may not reflect the true distribution, leading to incorrect conclusions about dogs' selective attention.

## Foundational Learning

- Concept: Transfer learning
  - Why needed here: To enable strong performance of the Mask R-CNN on the novel dog vision dataset despite limited training data.
  - Quick check question: What is the main benefit of using transfer learning when fine-tuning a pre-trained model on a new dataset with limited data?

- Concept: Instance segmentation
  - Why needed here: To segment individual objects in the dog's field of view, enabling identification of which objects the dog is fixating on.
  - Quick check question: What is the difference between object detection and instance segmentation?

- Concept: Chi-square goodness of fit test
  - Why needed here: To statistically compare the probability distributions over fixated objects estimated from the Mask R-CNN segmentation with the ground-truth distributions.
  - Quick check question: What is the null hypothesis tested by the chi-square goodness of fit test?

## Architecture Onboarding

- Component map: Head-mounted eye-tracking camera → Mask R-CNN segmentation → Intersection of object masks with fixation region → Probability distribution over fixated objects → Statistical analysis of visual attention patterns
- Critical path: Eye-tracking camera calibration → Mask R-CNN fine-tuning → Fixation region computation → Object mask intersection → Probability distribution estimation
- Design tradeoffs: Using a pre-trained Mask R-CNN enables strong performance with limited data but may not capture domain-specific features as well as a model trained from scratch on the dog vision dataset. The fixation region defined by the eye-tracking camera's radius of error is a simplifying assumption that may introduce noise.
- Failure signatures: Low Mask R-CNN segmentation accuracy, large eye-tracking camera radius of error, inaccurate fixation region computation, unreliable probability distribution estimation
- First 3 experiments:
  1. Evaluate Mask R-CNN segmentation accuracy on the test set using metrics like IoU, precision, and recall.
  2. Compute the chi-square distance between the probability distributions over fixated objects estimated from the Mask R-CNN segmentation and the ground-truth distributions on the test set.
  3. Analyze the statistical significance of the differences in visual attention patterns across object classes and individual dogs using ANOVA and logistic regression.

## Open Questions the Paper Calls Out

- Question: Does the performance of the Mask-RCNN transfer to different breeds of dogs or dogs with varying facial features?
  - Basis in paper: [inferred] The study was conducted on 11 dogs, but the authors do not mention if the dogs had similar facial features or if the model's performance would generalize to different breeds.
  - Why unresolved: The study did not explicitly test the model on different breeds of dogs or dogs with varying facial features.
  - What evidence would resolve it: Testing the model on a diverse set of dogs with different breeds and facial features to see if the performance remains consistent.

- Question: How does the model perform in different lighting conditions, especially in low-light environments?
  - Basis in paper: [inferred] The authors mention that the eye-tracking system's accuracy was affected by variable outdoor lighting, but they do not discuss the model's performance in different lighting conditions.
  - Why unresolved: The study did not explicitly test the model in different lighting conditions, especially in low-light environments.
  - What evidence would resolve it: Testing the model in various lighting conditions, including low-light environments, to see if the performance remains consistent.

- Question: Can the model be adapted to predict other aspects of a dog's visual behavior, such as their attention to specific body parts or facial expressions of humans?
  - Basis in paper: [explicit] The authors mention that the method can be extended to other species that can acclimatize to the head-mounted eye-tracker, but they do not discuss its potential for predicting other aspects of a dog's visual behavior.
  - Why unresolved: The study did not explore the model's potential for predicting other aspects of a dog's visual behavior, such as their attention to specific body parts or facial expressions of humans.
  - What evidence would resolve it: Adapting the model to predict other aspects of a dog's visual behavior and testing its performance in these areas.

## Limitations
- Limited sample size (11 dogs) may not capture full behavioral variation
- Head-mounted equipment may alter natural behavior or visual field
- Outdoor testing environments introduce uncontrolled variables
- Transfer learning performance assumes MS COCO features are sufficiently transferable to canine vision

## Confidence
- High Confidence: Technical implementation of Mask R-CNN fine-tuning and basic segmentation metrics
- Medium Confidence: Selective attention findings, as they rely on accuracy of eye-tracking calibration and object segmentation
- Low Confidence: Claim that dogs "selectively direct their attention" to specific object classes, as this interpretation depends heavily on pipeline accuracy

## Next Checks
1. Conduct controlled indoor experiments with consistent lighting and controlled object placement to validate attention patterns without environmental confounders
2. Perform cross-validation across different dog breeds and sizes to assess generalizability of attention patterns
3. Implement ablation studies removing the eye-tracking component to evaluate the impact of fixation estimation on attention conclusions