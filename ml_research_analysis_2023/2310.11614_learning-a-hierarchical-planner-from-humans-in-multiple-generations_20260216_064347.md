---
ver: rpa2
title: Learning a Hierarchical Planner from Humans in Multiple Generations
arxiv_id: '2310.11614'
source_url: https://arxiv.org/abs/2310.11614
tags:
- library
- learning
- goal
- search
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Natural Programming is a hierarchical planner that learns from
  human interactions by maintaining a library of goal decompositions. Unlike traditional
  programmatic approaches that are brittle to context changes, NP uses linguistic
  hints to guide its search for solutions and adapts by recursively leveraging decompositions
  from its library.
---

# Learning a Hierarchical Planner from Humans in Multiple Generations

## Quick Facts
- arXiv ID: 2310.11614
- Source URL: https://arxiv.org/abs/2310.11614
- Authors: 
- Reference count: 40
- Key outcome: Natural Programming learns from human interactions to build a hierarchical planner that outperforms traditional programmatic approaches, achieving 22% more items crafted and requiring less effort across generations.

## Executive Summary
Natural Programming (NP) is a hierarchical planner that learns from human interactions by maintaining a library of goal decompositions. Unlike traditional programmatic approaches that are brittle to context changes, NP uses linguistic hints to guide its search for solutions and adapts by recursively leveraging decompositions from its library. In a large-scale human experiment (n=360) and simulation studies, NP significantly outperformed programmatic baselines, building 22% more items on average and improving more rapidly across generations. The system required less individual and collective effort to reach the same performance levels, demonstrating its effectiveness at learning complex task structures from multiple users in changing contexts.

## Method Summary
NP operates in the Generational Learning under Changing Contexts (GLC) problem, where an agent learns from a sequence of users in different contexts with varying dynamics. The system maintains a library of hierarchical decompositions and uses a propose function that leverages linguistic hints to guide search. When a user provides a goal and hint, NP recursively searches its library for appropriate decompositions, executes the plan, and learns new decompositions from successful executions. The method was evaluated in the CraftLite environment with 29 items, comparing NP against a deterministic synthesis baseline across multiple generations with varying context dynamics.

## Key Results
- NP built 22% more items on average compared to programmatic baselines across all generations
- NP showed faster improvement rates across generations, with 0.45 items improvement per generation vs 0.15 for baselines
- NP required less individual and collective effort to achieve the same performance levels as traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NP achieves faster learning across generations by maintaining a library of hierarchical decompositions that can be adapted to new contexts.
- Mechanism: The system recursively leverages decompositions from its library, allowing it to adapt known solutions to the current context at runtime rather than requiring complete re-planning.
- Core assumption: The linguistic hints provided by users are sufficiently informative to guide the search for appropriate decompositions.
- Evidence anchors:
  - [abstract] "Unlike traditional programmatic approaches that are brittle to context changes, NP uses linguistic hints to guide its search for solutions and adapts by recursively leveraging decompositions from its library."
  - [section] "Unlike a programming system, the naive user does not need to explicitly specify the decomposition of a goal into sub-goals, but rather only linguistic hints on how a goal might decompose into sub-goals"
  - [corpus] Weak evidence - corpus focuses on retrieval augmentation and code generation, not hierarchical planning adaptation.
- Break condition: If linguistic hints become too ambiguous or users consistently provide misleading information, the propose function's effectiveness would degrade, leading to inefficient searches.

### Mechanism 2
- Claim: NP reduces individual effort by enabling users to achieve more tasks with fewer interactions.
- Mechanism: By maintaining a growing library of successful decompositions, NP allows users to leverage previous solutions rather than manually constructing plans for each new goal.
- Core assumption: The library grows at a rate that provides meaningful reuse opportunities across generations.
- Evidence anchors:
  - [abstract] "The system required less individual and collective effort to reach the same performance levels"
  - [section] "We argue instead that NP reduces the amount of effort required to obtain the same results."
  - [corpus] Weak evidence - corpus focuses on code generation and task-oriented dialogue, not effort reduction in planning systems.
- Break condition: If the library becomes too large or heterogeneous, the propose function may struggle to efficiently identify relevant decompositions, increasing search time and user effort.

### Mechanism 3
- Claim: NP's recursive execution allows it to find novel compositions of programs that traditional synthesis approaches cannot.
- Mechanism: The system treats search problems as composable elements that can be recursively decomposed, allowing it to explore solution spaces that grow with library size rather than remaining fixed.
- Core assumption: The recursive decomposition process can terminate in a finite number of steps with a valid solution.
- Evidence anchors:
  - [abstract] "Unlike traditional library-building program synthesis approaches, where search is performed on top of a library of deterministic programs, NP searches for a solution recursively over the library of hierarchical search problems"
  - [section] "The most salient aspect of the NP execution is the recursive call to NP itself. This allows NP to recursively try different decompositions until a satisfying sequence is found."
  - [corpus] Weak evidence - corpus neighbors focus on code generation and retrieval, not recursive search problem decomposition.
- Break condition: If the recursion depth becomes too large or the propose function cannot find appropriate decompositions, the system may fail to find solutions within the time budget.

## Foundational Learning

- Concept: Hierarchical planning and problem decomposition
  - Why needed here: NP's core mechanism relies on recursively breaking down goals into sub-goals, which requires understanding how to structure and solve hierarchical planning problems.
  - Quick check question: Can you explain the difference between flat planning and hierarchical planning in terms of search space complexity?

- Concept: Program synthesis and library learning
  - Why needed here: NP combines elements of both program synthesis (searching for solutions) and library learning (accumulating reusable components), so understanding both paradigms is crucial.
  - Quick check question: How does NP's approach to library learning differ from traditional program synthesis approaches?

- Concept: Natural language understanding and semantic search
  - Why needed here: The propose function uses linguistic hints to guide search, requiring understanding of how to map natural language descriptions to program structures.
  - Quick check question: What challenges arise when using semantic similarity between natural language hints and program descriptions?

## Architecture Onboarding

- Component map: User provides goal + hint -> propose function samples decompositions -> recursive execution attempts to solve -> if successful, decomposition added to library -> repeat
- Critical path: User provides goal + hint → propose function samples decompositions → recursive execution attempts to solve → if successful, decomposition added to library → repeat
- Design tradeoffs: NP trades off deterministic execution (like traditional programming) for adaptive search (like planning), which enables context adaptation but requires more complex search mechanisms.
- Failure signatures: System gets stuck in infinite recursion, propose function returns irrelevant decompositions, library grows too large to search efficiently, or linguistic hints fail to guide search effectively.
- First 3 experiments:
  1. Test propose function with simple linguistic hints and known decompositions to verify basic search capability
  2. Verify recursive execution can find solutions when library contains appropriate decompositions
  3. Test library learning by running NP on a simple problem and verifying new decompositions are added correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would NP perform in domains with more complex goal spaces and richer hierarchical structures beyond the fixed set of 29 items in CraftLite?
- Basis in paper: [explicit] The paper mentions this as a direct future work direction: "The most direct future work will be scaling to a richer domain consisting of a rich goal space not limited to the fixed set of goal items in CraftLite."
- Why unresolved: The current evaluation is limited to the CraftLite environment with a constrained goal space and relatively simple hierarchical structures. The paper acknowledges this limitation and identifies it as an open research direction.
- What evidence would resolve it: Implementing NP in a domain with variable-length goal specifications, nested hierarchical structures, and more complex dependencies between goals. Measuring performance metrics like success rate, adaptation speed, and user effort in this richer domain.

### Open Question 2
- Question: How does the choice of LLM backend (e.g., proposesim vs proposeprompt) affect NP's performance across different types of domains and linguistic hints?
- Basis in paper: [explicit] The paper compares two propose implementations (proposesim and proposeprompt) in the simulation study and finds different performance characteristics, but doesn't fully explore the impact across varied domains.
- Why unresolved: The comparison was limited to CraftLite with a specific type of linguistic hints. The paper notes that "the optimal form of propose is ultimately domain-specific" but doesn't provide a comprehensive analysis of how different LLM backends perform across diverse scenarios.
- What evidence would resolve it: Systematic evaluation of NP with different propose implementations across multiple domains with varying linguistic hint characteristics, measuring success rates, adaptation speed, and computational efficiency.

### Open Question 3
- Question: How would NP handle domains requiring control-flow statements and variables in the programmatic representation, as mentioned as future work?
- Basis in paper: [explicit] The paper states: "To scale NP to this richer domain, the planning language must be richer, with variables and control-flow."
- Why unresolved: The current NP implementation and evaluation are based on a simple imperative language without variables or control flow. The paper identifies this as a future direction but doesn't explore how the current architecture would need to adapt.
- What evidence would resolve it: Implementation of NP with support for variables and control flow, followed by evaluation in domains requiring conditional logic and variable manipulation, measuring how this affects learning efficiency and adaptation capabilities.

## Limitations
- The propose function's effectiveness heavily depends on the quality and informativeness of linguistic hints, but the paper doesn't provide detailed analysis of hint quality requirements or failure modes when hints are ambiguous.
- The simulation study uses synthetic users rather than real human participants, which may not capture the full complexity of human behavior and hint generation.
- The recursive execution mechanism could potentially encounter infinite loops or excessive recursion depth, though the paper mentions a time budget but doesn't specify termination conditions in detail.

## Confidence
- High confidence: NP's basic architecture and recursive execution mechanism (supported by clear algorithmic description and pseudocode)
- Medium confidence: NP's performance improvements across generations (supported by simulation results but dependent on synthetic user models)
- Low confidence: Exact implementation details of the propose function and semantic search components (described conceptually but not in full technical detail)

## Next Checks
1. Implement a stress test of the propose function with varying quality of linguistic hints to determine the minimum information content required for effective search guidance.
2. Create a formal verification of the recursive execution mechanism to ensure termination conditions and detect potential infinite recursion scenarios.
3. Run ablation studies removing different components (library, propose function, recursive execution) to quantify the contribution of each to overall performance improvements.