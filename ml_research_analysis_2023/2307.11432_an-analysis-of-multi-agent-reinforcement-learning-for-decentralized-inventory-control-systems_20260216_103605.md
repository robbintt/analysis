---
ver: rpa2
title: An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory
  Control Systems
arxiv_id: '2307.11432'
source_url: https://arxiv.org/abs/2307.11432
tags:
- performance
- each
- policy
- agent
- marl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of multi-agent reinforcement learning
  (MARL) for decentralized inventory control in supply chain networks. The problem
  addressed is finding the optimal re-order policy for nodes in a supply chain, which
  is a well-known planning problem in operations research.
---

# An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems

## Quick Facts
- arXiv ID: 2307.11432
- Source URL: https://arxiv.org/abs/2307.11432
- Reference count: 40
- Primary result: Multi-agent PPO with centralized critic achieves near-centralized solution performance and outperforms distributed model-based methods in decentralized inventory control

## Executive Summary
This paper investigates the application of multi-agent reinforcement learning (MARL) to decentralized inventory control in supply chain networks. The authors propose using three variations of proximal policy optimization (PPO) - independent PPO, IPPO with shared parameters, and multi-agent PPO with centralized critic - to find optimal reorder policies for nodes in supply chains. The study evaluates these methods across different network structures and uncertainty levels, demonstrating that MAPPO achieves performance close to centralized solutions while maintaining the benefits of decentralized execution.

## Method Summary
The authors implement a decentralized MARL approach where each supply chain node is controlled by an independent agent using PPO algorithms. Three algorithm variants are tested: independent PPO (IPPO), IPPO with shared network parameters, and multi-agent PPO (MAPPO) with a centralized critic during training. The supply chain environment is simulated with configurable demand patterns and lead time uncertainty. Agents learn reorder policies through interaction with the environment, with MAPPO using a centralized critic to provide better value estimates during training while maintaining decentralized execution at test time.

## Key Results
- MAPPO with centralized critic achieves performance very close to centralized data-driven solutions
- MARL methods outperform distributed model-based optimization methods in most scenarios
- IPPO with shared network parameters performs poorly for larger or more complex networks
- MARL methods show greater robustness to demand and lead time uncertainty
- MARL approaches are orders of magnitude faster during online execution compared to traditional model-based optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Centralized training with decentralized execution enables coordination without requiring real-time information sharing.
- Mechanism: During training, MAPPO uses a centralized critic that observes all agents' states and actions, providing better value estimates and enabling stable learning of coordinated policies. At execution time, each agent uses only its local observations to select actions independently.
- Core assumption: The simulation environment during training can accurately represent real-world dynamics and uncertainty.
- Evidence anchors:
  - [abstract] "The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system."
  - [section 3.1] "While centralized training is required to provide the critic with the actions of other agents, each agent can still execute actions in a decentralized manner as the actor network takes only the agent's own local observations as input."
- Break condition: If the simulation environment poorly represents real-world dynamics, the trained policies may fail when deployed due to distributional shift.

### Mechanism 2
- Claim: Independent PPO with shared network parameters performs poorly for larger or more complex networks because it cannot capture node-specific dynamics.
- Mechanism: When using shared network parameters, the same policy must work for all nodes regardless of their position in the supply chain or number of downstream nodes. This limits the policy's ability to learn optimal behavior for nodes with different roles.
- Core assumption: Nodes in different positions of the supply chain have distinct dynamics and optimal policies.
- Evidence anchors:
  - [section 4.2] "Using a shared network with IPPO leads to much worse performance. This could be due to the fact that different nodes experience slightly different dynamics in the divergent SCN as opposed to nodes in a serial SCN."
- Break condition: If all nodes in the supply chain have identical or very similar dynamics, shared network parameters might perform adequately.

### Mechanism 3
- Claim: Data-driven methods are more robust to lead time uncertainty than model-based LP methods.
- Mechanism: RL agents learn policies that implicitly account for stochastic lead times through experience, while LP methods explicitly model deterministic lead times as constraints. When lead times become stochastic, the LP solution quality degrades significantly.
- Core assumption: The RL training process exposes agents to sufficient variability in lead times to learn robust policies.
- Evidence anchors:
  - [section 4.3] "It can be seen that the performance of the MARL is less affected by the stochastic delivery lead times relative to the distributed LP method. They outperform the distributed LP method significantly at levels of pÏ„ as the model-based method experiences a severe drop in performance."
- Break condition: If lead time uncertainty is extreme or has patterns not well-represented in training data, RL methods may also struggle.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: Each agent in the supply chain only observes local state information, not the full system state, making this a POMDP.
  - Quick check question: Why can't agents in this system observe the complete state of all nodes?

- Concept: Policy Gradient Methods
  - Why needed here: The paper uses Proximal Policy Optimization (PPO), which is a policy gradient method for learning optimal policies.
  - Quick check question: How does PPO differ from value-based methods like Q-learning?

- Concept: Actor-Critic Architecture
  - Why needed here: Both the single-agent and MARL approaches use actor-critic networks where the actor selects actions and the critic estimates value functions.
  - Quick check question: What role does the critic network play in stabilizing policy learning?

## Architecture Onboarding

- Component map:
  - Environment: Supply chain network simulator with nodes, inventory, lead times, and demand
  - Agents: Independent PPO agents with actor networks (and critic networks for MAPPO)
  - Training system: Ray Tune for hyperparameter optimization, PPO algorithm with GAE advantage estimation
  - Evaluation: Test episodes with fixed length, performance metrics (reward, inventory, backlog)

- Critical path:
  1. Initialize environment and agents
  2. Collect trajectories through environment interaction
  3. Compute advantages using GAE
  4. Update actor and critic networks
  5. Repeat until convergence
  6. Evaluate trained policies

- Design tradeoffs:
  - Centralized vs decentralized training: Centralized training (MAPPO) provides better coordination but requires more information during training
  - Shared vs independent networks: Shared networks reduce parameter count but may limit policy expressiveness for different node types
  - RNN vs MLP: RNNs could capture temporal dependencies but add complexity and training time

- Failure signatures:
  - High variance in rewards across training runs suggests instability
  - Poor performance on divergent networks indicates inability to handle complex coordination
  - Degradation under lead time uncertainty suggests insufficient training variability

- First 3 experiments:
  1. Train IPPO on a simple serial supply chain and verify it learns a reasonable policy
  2. Compare IPPO with MAPPO on the same serial chain to observe coordination benefits
  3. Test both algorithms on a divergent supply chain to identify performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of MARL methods change when using differentiable communication channels between agents in the inventory control system?
- Basis in paper: [explicit] The paper discusses the potential for using differentiable communication channels to improve agents' handling of system disruptions, noting that this could allow agents to learn to communicate useful information and reduce the problem caused by partial observability.
- Why unresolved: The paper only mentions this as a potential improvement area but does not implement or test it.
- What evidence would resolve it: Empirical results comparing MARL performance with and without differentiable communication channels in various supply chain network configurations.

### Open Question 2
- Question: What is the impact of using Graph Neural Networks (GNNs) on the coordination and performance of multi-agent reinforcement learning in supply chain networks?
- Basis in paper: [explicit] The paper suggests that GNNs could make use of the underlying supply chain graph structure to allow for more efficient and effective communication between agents, potentially improving performance.
- Why unresolved: The paper does not implement or test GNNs in the context of the inventory control problem.
- What evidence would resolve it: Comparative results of MARL performance with and without GNNs across different supply chain network sizes and uncertainty levels.

### Open Question 3
- Question: How does vertical federated reinforcement learning affect the scalability and privacy of multi-agent inventory control systems in real-world supply chains?
- Basis in paper: [explicit] The paper mentions vertical federated RL as a potential solution to overcome the problem of centralization of information during training, but does not explore it further.
- Why unresolved: The paper does not implement or test vertical federated RL in the inventory control context.
- What evidence would resolve it: Performance and privacy metrics comparing federated and non-federated MARL implementations in large-scale supply chain simulations.

## Limitations
- Centralized training requires complete system information during training, which may not be feasible in all real-world scenarios
- Evaluation focuses primarily on synthetic supply chain networks rather than real-world data
- Performance claims relative to centralized data-driven solutions lack detailed comparison metrics

## Confidence
- High Confidence: MAPPO's superior performance compared to IPPO with shared parameters; computational efficiency advantage of MARL over model-based methods
- Medium Confidence: Claims about robustness to lead time uncertainty; generalizability across different network structures
- Medium-Low Confidence: Performance claims relative to centralized data-driven solutions (limited comparison details provided)

## Next Checks
1. Test the trained policies on actual supply chain data from operational systems to verify performance holds under real-world conditions with noise and missing data
2. Evaluate the approach on larger supply chain networks (10+ nodes) to assess how performance scales with system complexity and whether training time becomes prohibitive
3. Systematically vary the training distribution of lead times and demand patterns to determine the sensitivity of learned policies to distributional shifts between training and deployment environments