---
ver: rpa2
title: Performance Prediction for Multi-hop Questions
arxiv_id: '2308.06431'
source_url: https://arxiv.org/abs/2308.06431
tags:
- question
- performance
- questions
- retrieval
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first pre-retrieval query performance
  prediction (QPP) method for open-domain multi-hop QA, a setting where retrieval
  difficulty depends on complex multi-step reasoning paths. The core idea is to estimate
  the difficulty of a question by modeling its retrieval path (bridge, comparison,
  or mixed) and computing the likelihood of reaching each supporting document using
  named entities and frozen phrases as retrieval cues.
---

# Performance Prediction for Multi-hop Questions

## Quick Facts
- **arXiv ID:** 2308.06431
- **Source URL:** https://arxiv.org/abs/2308.06431
- **Reference count:** 40
- **Primary result:** First pre-retrieval QPP method for multi-hop QA showing 4.87% higher pairwise accuracy for bridge questions on HotpotQA

## Executive Summary
This paper introduces the first pre-retrieval query performance prediction (QPP) method for open-domain multi-hop QA. The method estimates question difficulty by modeling retrieval paths (bridge, comparison, or mixed) and computing document reachability probabilities using named entities and frozen phrases as retrieval cues. Experiments on HotpotQA demonstrate significant improvements over single-hop QPP baselines, with strong correlations between predicted difficulty and actual retriever performance for both sparse and dense retrievers.

## Method Summary
The method extracts named entities and frozen phrases from questions, uses them as retrieval cues, and estimates the probability of reaching each supporting document. Difficulty is computed as the inverse of reachability probability, conditioned on retrieval path type. The approach operates pre-retrieval, avoiding computational cost while modeling multi-step reasoning paths through probability calculations based on corpus statistics.

## Key Results
- 4.87% higher pairwise accuracy for bridge questions compared to single-hop baselines
- 0.23-0.29 Pearson correlation between predicted difficulty and actual retriever performance
- Adaptive retrieval with difficulty-based document allocation improves F1-score when operating under time constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-retrieval QPP can estimate multi-hop QA difficulty by modeling retrieval path types and computing document reachability probabilities
- Mechanism: Extracts named entities and frozen phrases as retrieval cues, estimates probability of reaching supporting documents, computes difficulty as inverse probability
- Core assumption: Named entities and frozen phrases are sufficient retrieval cues for multi-step reasoning paths
- Evidence: Abstract states core idea; section 3.4.1 details n-gram extraction methodology

### Mechanism 2
- Claim: Correlation between predicted difficulty and actual retriever performance is significantly higher for multi-hop questions
- Mechanism: Conditioning difficulty estimates on retrieval path type aligns better with retriever behavior
- Core assumption: Correlation reflects QPP model effectiveness
- Evidence: Section 4.4.2 shows significant correlation with MDR and GoldEn retriever performance; section 4.4.1 demonstrates multHP outperforming baselines

### Mechanism 3
- Claim: Difficulty estimation enables adaptive retrieval improving overall QA performance
- Mechanism: Classifies questions into difficulty levels and allocates more documents to harder questions
- Core assumption: Allocating more resources to harder questions improves performance without degrading easy questions
- Evidence: Section 4.6.2 shows F1-score improvements with adaptive retrieval; section 4.5.1 demonstrates performance drops across difficulty classes

## Foundational Learning

- **Concept:** Retrieval path types in multi-hop QA (bridge, comparison, mixed)
  - Why needed: Difficulty estimation depends on modeling correct retrieval path type
  - Quick check: Can you explain the difference between bridge and comparison retrieval paths?

- **Concept:** Pre-retrieval vs post-retrieval QPP
  - Why needed: This work introduces a pre-retrieval method, unlike most existing QPP work
  - Quick check: What is the key difference between pre-retrieval and post-retrieval QPP methods?

- **Concept:** Named entity and frozen phrase extraction
  - Why needed: These are the primary retrieval cues used to estimate document reachability
  - Quick check: Why does the model extract unigrams from frozen phrases but n-grams from named entities?

## Architecture Onboarding

- **Component map:** Question → Named entity/frozen phrase extraction → Path detection → Difficulty estimation → Adaptive retrieval
- **Critical path:** Question → Named entity/frozen phrase extraction → Path detection → Difficulty estimation → Adaptive retrieval
- **Design tradeoffs:** Using only named entities and frozen phrases simplifies the model but may miss some retrieval cues; pre-retrieval approach avoids computational cost but relies on retrieval behavior assumptions
- **Failure signatures:** Low correlation indicates path detection or probability estimation issues; poor adaptive retrieval suggests misaligned difficulty classification thresholds
- **First 3 experiments:** 1) Evaluate pairwise difficulty accuracy on bridge questions using Max scheme; 2) Measure correlation between predicted difficulty and actual retriever performance for comparison questions; 3) Test adaptive retrieval performance across difficulty classes on dev set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval path types be accurately predicted for unseen multi-hop questions using only question text?
- Basis: Paper discusses training classifier achieving high F1 scores on development set
- Why unresolved: Classifier performance on truly unseen questions or different domains not evaluated
- What evidence would resolve it: Evaluation on held-out test set or different multi-hop QA datasets

### Open Question 2
- Question: How does the performance prediction model handle multi-hop questions requiring more than two retrieval steps?
- Basis: Paper focuses on 2-hop questions but mentions generalization to n-hop questions
- Why unresolved: Model's effectiveness for n-hop questions not empirically tested
- What evidence would resolve it: Experimental results on datasets containing n-hop questions

### Open Question 3
- Question: Can the performance prediction model be integrated with dense retrievers using semantic embeddings?
- Basis: Paper evaluates with both sparse and dense retrievers but notes potential misalignment with semantic approaches
- Why unresolved: Model's effectiveness with purely semantic dense retrievers not demonstrated
- What evidence would resolve it: Experiments showing correlation with dense retrievers like DPR or ANCE on multi-hop questions

## Limitations

- Method assumes named entities and frozen phrases are sufficient retrieval cues, which may not hold for complex linguistic structures
- Pre-retrieval approach cannot leverage actual retrieval results, limiting accuracy when retriever behavior deviates from modeled paths
- Performance validation primarily focused on bridge questions, with less comprehensive evaluation for comparison and mixed question types

## Confidence

- **Multi-hop QPP effectiveness:** High - Strong experimental support on bridge questions
- **Correlation with retriever performance:** Medium - Moderate correlation values, limited to specific retriever types  
- **Adaptive retrieval improvements:** Medium - Positive results but tested on single dataset
- **Named entity sufficiency:** Low - Assumption not thoroughly validated across question types

## Next Checks

1. Test the multHP method on comparison and mixed question types to verify if correlation improvements extend beyond bridge questions
2. Evaluate performance when named entities are sparse or missing to assess robustness to data quality variations
3. Compare against post-retrieval QPP methods on the same dataset to quantify trade-off between computational efficiency and accuracy