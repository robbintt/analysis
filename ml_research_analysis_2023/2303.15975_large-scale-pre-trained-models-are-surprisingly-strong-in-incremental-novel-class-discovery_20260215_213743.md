---
ver: rpa2
title: Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel
  Class Discovery
arxiv_id: '2303.15975'
source_url: https://arxiv.org/abs/2303.15975
tags:
- step
- baseline
- methods
- novel
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of incremental novel class discovery
  (iNCD) where the goal is to discover new visual categories from unlabeled data without
  accessing any labeled data. The key idea is to leverage large-scale pre-trained
  models, such as DINO, as feature extractors and train a linear classifier on top
  for discovering new classes.
---

# Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery

## Quick Facts
- arXiv ID: 2303.15975
- Source URL: https://arxiv.org/abs/2303.15975
- Reference count: 40
- Key outcome: Simple baselines using large-scale pre-trained models outperform sophisticated state-of-the-art methods in incremental novel class discovery

## Executive Summary
This paper addresses incremental novel class discovery (iNCD) where new visual categories must be discovered from unlabeled data without accessing labeled data or past data. The key insight is that large-scale pre-trained models like DINO serve as powerful frozen feature extractors, providing rich semantic embeddings that outperform supervised pre-training for novel class discovery. The authors propose two simple baselines - Baseline and Baseline++ - that significantly outperform sophisticated existing methods on five datasets, establishing a new standard for the challenging MSc-iNCD setting.

## Method Summary
The method uses DINO-pretrained ViT-B/16 as a frozen feature extractor with a learnable linear classifier for discovering new classes. Baseline trains with Sinkhorn-Knopp cross-view pseudo-labeling, while Baseline++ adds knowledge transfer via robust feature replay (KTRFR) by storing and replaying pseudo prototypes. Both use cosine normalization for task-agnostic inference. Training runs for 200 epochs per task using SGD with cosine annealing.

## Key Results
- Simple baselines using large-scale pre-trained models outperform sophisticated state-of-the-art methods across all five datasets
- Cosine normalization plays a substantial role in enhancing overall accuracy by addressing weight vector magnitude dominance
- KTRFR improves cross-task class discrimination and mitigates forgetting, especially in longer task sequences

## Why This Works (Mechanism)

### Mechanism 1
Large-scale self-supervised pre-trained models (LsPt) provide richer priors than supervised pre-training for novel class discovery. The DINO pre-trained ViT backbone acts as a frozen feature extractor, preserving strong semantic embeddings learned from massive unlabeled data. These embeddings are already well-separated in feature space, reducing the need for fine-tuning and preventing catastrophic forgetting.

### Mechanism 2
Cosine normalization (CosNorm) enables task-agnostic inference by balancing classifier weights across tasks. L2-normalizing both feature embeddings and classifier weight vectors ensures all output logits are on the same scale [-1, 1], preventing classifiers from dominating due to magnitude differences and allowing concatenation for unified inference.

### Mechanism 3
Knowledge Transfer with Robust Feature Replay (KTRFR) mitigates forgetting by simulating past data distributions. Pseudo per-class feature prototypes (mean and variance) are stored and replayed via sampling from Gaussian distributions during training, approximating joint training across tasks without accessing real past data.

## Foundational Learning

- **Self-supervised pre-training via contrastive learning (e.g., DINO)**: Provides strong starting point for feature extraction without requiring labeled base classes, aligning with unsupervised nature of MSc-iNCD. Quick check: Does the model maintain discriminative power for novel classes when initialized from self-supervised vs supervised backbone?
- **Incremental learning and catastrophic forgetting**: MSc-iNCD involves sequential learning of new classes without revisiting old data, so forgetting is central. Quick check: After training on 5 tasks, how does clustering accuracy on first task compare to joint-trained model?
- **Sinkhorn-Knopp cross-view pseudo-labeling**: Enables self-supervised discovery of novel classes by assigning soft labels based on feature similarity across augmented views. Quick check: Can the model discover correct number of clusters without any label information?

## Architecture Onboarding

- **Component map**: ViT-B/16 backbone (frozen, DINO-initialized) -> Task-specific linear classifier (CosNorm applied) -> Sinkhorn-Knopp clustering module (self-supervised) -> Prototype storage and replay buffer (KTRFR)
- **Critical path**: 1) Load DINO-ViT-B/16 and freeze, 2) Initialize task-specific classifier, 3) Train with Sinkhorn-Knopp objective, 4) Store prototypes after each task, 5) Concatenate classifiers for task-agnostic inference, 6) Replay prototypes in KTRFR mode for improved cross-task discrimination
- **Design tradeoffs**: Frozen backbone vs fine-tuning (preserves LsPt knowledge but limits adaptation), simple concatenation vs gating (simple but may cause interference), prototype replay vs real data replay (avoids data storage but relies on Gaussian approximation)
- **Failure signatures**: Rapid drop in accuracy on early tasks indicates forgetting, high variance in pseudo-labels suggests poor feature separation, degraded performance on novel classes suggests insufficient LsPt generalization
- **First 3 experiments**: 1) Train Baseline on CIFAR-10 two-step split; compare with K-means and Joint (frozen) baselines, 2) Enable KTRFR in Baseline++; measure forgetting reduction vs Baseline, 3) Test task-agnostic inference on held-out test set from all tasks; report per-task accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of large-scale pre-trained models compare to supervised pre-training when discovering novel classes in datasets with significantly different distributions from the pre-training data? The paper focuses on standard vision datasets but doesn't evaluate on datasets with significantly different distributions like medical imaging or satellite imagery.

### Open Question 2
How does the proposed method perform when the number of novel classes per task is unknown or varies significantly across tasks? The paper assumes known class numbers per task, but real-world applications may not have this information available.

### Open Question 3
How does the performance of the proposed method scale with the number of incremental steps in the task sequence? The paper evaluates on task sequences with up to 5 steps but doesn't explore performance on longer sequences.

## Limitations
- Reliance on frozen DINO backbone without exploring fine-tuning strategies or domain adaptation for specialized datasets
- Gaussian replay mechanism assumes feature distributions remain stable across tasks, which may not hold in domain-shift scenarios
- Evaluation focuses on classification accuracy without extensive ablation on the number of clusters or robustness to class imbalance

## Confidence

- **High confidence**: Superiority of large-scale pre-trained models over supervised pre-training for novel class discovery is well-supported by experimental results across five datasets
- **Medium confidence**: Effectiveness of cosine normalization for task-agnostic inference is demonstrated, though mechanism may break down with significant feature distribution shifts
- **Medium confidence**: KTRFR approach shows promise for mitigating forgetting, but Gaussian approximation of feature distributions may not capture complex class structures

## Next Checks

1. Test Baseline++ on a dataset with significant domain shift (e.g., natural images â†’ medical images) to assess generalization limits of frozen DINO backbone
2. Compare Gaussian replay approximation against real feature replay (when data access is permitted) to quantify approximation error
3. Evaluate impact of varying the number of pseudo-labeling Sinkhorn-Knopp iterations on clustering quality and computational efficiency