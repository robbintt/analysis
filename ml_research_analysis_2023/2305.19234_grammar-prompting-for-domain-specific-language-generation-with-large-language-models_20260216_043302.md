---
ver: rpa2
title: Grammar Prompting for Domain-Specific Language Generation with Large Language
  Models
arxiv_id: '2305.19234'
source_url: https://arxiv.org/abs/2305.19234
tags:
- grammar
- prompting
- language
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Grammar prompting improves few-shot DSL generation by providing
  LLMs with specialized BNF grammars for each demonstration example, enabling better
  handling of domain-specific constraints and syntax. The method predicts a specialized
  grammar for a new input and then generates output conditioned on that grammar.
---

# Grammar Prompting for Domain-Specific Language Generation with Large Language Models

## Quick Facts
- arXiv ID: 2305.19234
- Source URL: https://arxiv.org/abs/2305.19234
- Reference count: 40
- Key outcome: Grammar prompting improves few-shot DSL generation by providing LLMs with specialized BNF grammars for each demonstration example, enabling better handling of domain-specific constraints and syntax.

## Executive Summary
This paper introduces grammar prompting, a technique that augments few-shot examples with specialized Backus-Naur Form (BNF) grammars to improve domain-specific language generation with large language models. The approach predicts minimal specialized grammars for each demonstration example, then generates output conditioned on those grammars. Experiments across diverse structured language tasks including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation demonstrate consistent improvements over standard prompting baselines.

## Method Summary
Grammar prompting views DSL program generation as a grammar specialization process where given a natural language specification, a set of production rules is selected from the full DSL grammar, and then a program is deduced according to the selected rules. The method involves predicting a specialized grammar first, then generating output constrained by that grammar using Earley parsing. This two-stage approach enables the LLM to focus on relevant syntactic constraints rather than learning them from scratch through examples.

## Key Results
- Grammar prompting improves program accuracy by 5.1% on SMCalFlow, 2.6% on Overnight, and 2.2% on GeoQuery semantic parsing tasks
- For molecule generation, grammar prompting achieves 94.8% validity rate and 5.0% diversity compared to 90.1% and 4.1% for standard prompting
- In PDDL planning, grammar prompting increases success rate from 53.7% to 61.7% while reducing nodes created/expanded by 18.3%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs have implicit knowledge of BNF metalanguage from pretraining, enabling them to generate and use grammars for structured output tasks.
- Mechanism: The LLM learns to predict minimal specialized grammars that encode the necessary production rules for generating valid outputs in domain-specific languages (DSLs). This intermediate grammar representation helps the model focus on the relevant syntactic structure rather than learning it from scratch through examples.
- Core assumption: LLMs have been exposed to enough BNF-style grammars during pretraining to understand the metalanguage and generate valid grammars for unseen DSLs.
- Evidence anchors: [abstract] "we explore grammar prompting as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backusâ€“Naur Form (BNF), during in-context learning" [section 2.2] "DSLs often incorporate domain-specific abstractions and semantics that are difficult to characterize via just a few demonstrations"

### Mechanism 2
- Claim: Using specialized grammars as intermediate reasoning steps improves few-shot learning by focusing the model on relevant syntactic constraints.
- Mechanism: By first predicting a grammar and then generating output conditioned on that grammar, the LLM performs a two-stage reasoning process. This is similar to chain-of-thought prompting but uses formal grammar instead of natural language reasoning steps.
- Core assumption: The two-stage process of grammar prediction followed by constrained generation is more effective than direct generation from examples alone.
- Evidence anchors: [abstract] "Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example" [section 3.1] "Grammar prompting views DSL program generation as a grammar specialization process where given a natural language specification x, a set of production rules is selected from G, and then a program y is deduced according to the selected rules"

### Mechanism 3
- Claim: Constrained decoding based on predicted grammars ensures syntactic validity of generated outputs.
- Mechanism: After predicting a specialized grammar, the LLM uses Earley parsing to enforce that generated outputs conform to the grammar rules, preventing syntactically invalid programs.
- Core assumption: The predicted grammar accurately captures the necessary constraints for valid output generation, and the LLM can effectively use this grammar for constrained decoding.
- Evidence anchors: [section 3.2] "The use of a formal grammar as an intermediate variable makes it possible to enforce grammatical constraints during autoregressive LLM decoding" [section 4.1] "we also analyze the effect of constrained decoding on the number of LLM API calls in Table 7 of appendix A.1, where we observe that constrained decoding requires roughly three times more API calls than unconstrained decoding"

## Foundational Learning

- Concept: Backus-Naur Form (BNF) grammars and context-free languages
  - Why needed here: Understanding BNF is essential for comprehending how specialized grammars are created and used for DSL generation
  - Quick check question: What is the difference between a terminal and non-terminal symbol in a BNF grammar?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The paper builds on few-shot learning techniques, using demonstration examples to teach the LLM how to generate structured outputs
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- Concept: Earley parsing and grammar-based constrained generation
  - Why needed here: The paper uses Earley parsing to enforce that generated outputs conform to predicted grammars
  - Quick check question: What is the purpose of using Earley parsing in the constrained decoding process?

## Architecture Onboarding

- Component map: LLM model (Codex, GPT-3.5, or GPT-4) -> Specialized grammar predictor -> Constrained decoder (Earley-based) -> DSL grammar repository -> Evaluation metrics

- Critical path: 1. Receive input and demonstration examples 2. Predict specialized grammar from input 3. Generate output constrained by predicted grammar 4. Evaluate output against reference

- Design tradeoffs:
  - Using specialized grammars vs. full DSL grammars: Specialized grammars are more focused but may miss some valid constructions
  - Constrained vs. unconstrained decoding: Constrained decoding ensures validity but increases API calls and may reduce diversity
  - Grammar prediction vs. direct generation: Grammar prediction adds a step but may improve reasoning about structure

- Failure signatures:
  - Low program accuracy despite high grammar prediction accuracy: Indicates grammar prediction is not translating to good output generation
  - High API call count with marginal accuracy improvements: Suggests constrained decoding is too restrictive
  - Inconsistent performance across different DSLs: May indicate the approach is not robust to different grammar structures

- First 3 experiments:
  1. Test grammar prompting on a simple DSL (like the calendar example) with oracle grammars to verify the basic approach works
  2. Compare constrained vs. unconstrained decoding on a medium-complexity DSL to understand the tradeoff
  3. Test on a completely new DSL type (like SMILES) to evaluate generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can grammar prompting be effectively combined with chain-of-thought prompting by adding natural language comments to the BNF rules?
- Basis in paper: [explicit] The paper mentions experimenting with enhancing BNF rules by appending natural language comments and observed marginal improvements on SMCalFlow (+1.0%) and Overnight-Blocks (0.5%).
- Why unresolved: The gains were modest and the authors only conducted preliminary experiments. The potential benefits of this approach for interpretability and generation constraints were not fully explored.
- What evidence would resolve it: Systematic experiments comparing grammar prompting with and without annotated rules across multiple DSL tasks, measuring both performance and interpretability metrics.

### Open Question 2
- Question: Would fine-tuning moderately-sized LLMs using specialized grammars lead to better grammar-based models for DSL generation compared to prompting-based approaches?
- Basis in paper: [inferred] The paper discusses limitations of grammar prompting and suggests that fine-tuning LLMs using specialized grammars could be an interesting direction for future work.
- Why unresolved: The paper focuses on prompting-based approaches and does not explore fine-tuning strategies. The computational costs and performance trade-offs of fine-tuning vs prompting are unknown.
- What evidence would resolve it: Comparative experiments fine-tuning smaller models on specialized grammar data versus using prompting approaches, measuring both performance and computational efficiency.

### Open Question 3
- Question: Can grammar prompting be extended to more advanced string representations like SELFIES that guarantee molecular validity?
- Basis in paper: [explicit] The paper mentions that SMILES does not guarantee valid molecules and suggests using grammar prompting on more advanced representations like SELFIES as future work.
- Why unresolved: The experiments were conducted using SMILES representation, and the authors did not test more robust molecular string representations.
- What evidence would resolve it: Experiments applying grammar prompting to SELFIES-based molecule generation, measuring validity, diversity, and retrosynthesis scores compared to SMILES-based results.

## Limitations

- Grammar prompting shows limited improvements for DSLs frequently encountered during pretraining (e.g., regular expressions, SQL), suggesting the approach is most effective for less common DSLs
- Constrained generation based on specialized grammars leads to increased API calls (roughly 3x) and decreased performance for tasks beyond semantic parsing, particularly molecule generation
- The approach requires access to full DSL grammar in BNF format and may not generalize well to DSLs with ambiguous or overlapping grammar rules

## Confidence

- High confidence in the core finding that grammar prompting improves few-shot DSL generation performance compared to standard prompting baselines
- Medium confidence in the claim that specialized grammars serve as effective intermediate reasoning steps
- Low confidence in the scalability of the approach to more complex DSLs with ambiguous or overlapping grammar rules

## Next Checks

1. Conduct an ablation study on grammar complexity by systematically varying the complexity of specialized grammars (number of production rules, depth of nesting) to determine the point at which grammar prediction becomes unreliable or when constrained decoding becomes too restrictive

2. Test cross-domain grammar transfer by evaluating whether grammars learned from one DSL type can be effectively transferred or adapted to structurally similar DSLs, reducing the need for task-specific grammar prediction

3. Quantify the efficiency-accuracy tradeoff by measuring the relationship between grammar prediction accuracy, constrained decoding overhead, and final program accuracy to determine optimal parameter settings for different deployment scenarios