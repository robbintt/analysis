---
ver: rpa2
title: 'DSS: A Diverse Sample Selection Method to Preserve Knowledge in Class-Incremental
  Learning'
arxiv_id: '2312.09357'
source_url: https://arxiv.org/abs/2312.09357
tags:
- data
- task
- learning
- exemplars
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method, called DSS (Diverse Sample Selection),
  for selecting a diverse set of exemplars to preserve knowledge in class-incremental
  learning. The method is based on k-center clustering and is robust to outliers.
---

# DSS: A Diverse Sample Selection Method to Preserve Knowledge in Class-Incremental Learning

## Quick Facts
- arXiv ID: 2312.09357
- Source URL: https://arxiv.org/abs/2312.09357
- Authors: 
- Reference count: 26
- Key outcome: DSS achieves 51.93% average accuracy on CIFAR100 in disjoint setup and 54.8% in fuzzy10 setup

## Executive Summary
This paper proposes DSS (Diverse Sample Selection), a method for selecting diverse exemplars in class-incremental learning using k-center clustering with t-SNE dimensionality reduction. The approach iteratively selects the furthest point from current exemplars while requiring a minimum number of neighbors within a radius to exclude outliers. DSS demonstrates superior performance on CIFAR100 compared to state-of-the-art methods in both disjoint and fuzzy task boundary scenarios.

## Method Summary
DSS selects exemplars through a two-step process: first reducing high-dimensional data to 2D using t-SNE, then iteratively choosing the point furthest from current exemplars that has at least n neighbors within radius r. This k-center inspired approach aims to maintain diversity while filtering outliers. The selected exemplars are used in rehearsal-based training with cross-distillation loss to preserve knowledge across incremental tasks.

## Key Results
- Achieves 51.93% average accuracy in disjoint CIFAR100 setup
- Achieves 54.8% average accuracy in fuzzy10 CIFAR100 setup
- Outperforms BiC (49.62% and 53.02%) and GDumb baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DSS selects exemplars that are maximally distant from each other while being surrounded by sufficient local neighbors, thus preserving class boundaries.
- Mechanism: The algorithm first reduces the dataset to 2D using t-SNE to preserve local structure, then iteratively picks the point furthest from the current exemplar set, but only if it has at least n neighbors within radius r.
- Core assumption: The 2D t-SNE embedding preserves enough class structure to make distance meaningful for exemplar selection.
- Evidence anchors: Weak evidence; no direct comparison to other dimensionality reduction choices.

### Mechanism 2
- Claim: The neighbor count filter (n within radius r) effectively excludes outliers from the exemplar set.
- Mechanism: By requiring each selected exemplar to have at least n neighbors within radius r, the algorithm ensures that only points embedded in dense regions are chosen.
- Core assumption: Outliers are sparse and will not meet the n-neighbor criterion, while true class representatives will.
- Evidence anchors: Weak evidence; no explicit outlier rejection experiments shown.

### Mechanism 3
- Claim: DSS maintains diversity in the exemplar set, which helps the model retain knowledge across disjoint and fuzzy task boundaries.
- Mechanism: By always selecting the furthest point from the current set, DSS ensures that the exemplars span the space, reducing redundancy.
- Core assumption: Diversity in exemplars correlates with better preservation of the decision boundary for unseen data.
- Evidence anchors: Weak evidence; no ablation on diversity vs. accuracy tradeoff.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The core problem DSS addresses is forgetting previously learned classes when training on new ones.
  - Quick check question: What happens to model accuracy on old classes if you train only on new data without rehearsal?

- Concept: K-center clustering approximation
  - Why needed here: DSS uses a greedy farthest-point selection inspired by the 2-approximation algorithm for K-center clustering.
  - Quick check question: Why does picking the furthest point from the current set give a 2-approximation to optimal K-center?

- Concept: Knowledge distillation
  - Why needed here: The training objective combines cross-entropy on new data with distillation loss on old exemplars to preserve old knowledge.
  - Quick check question: How does the temperature parameter T in distillation smooth the logit distribution?

## Architecture Onboarding

- Component map: Input data stream -> Dimensionality reduction (t-SNE) -> Exemplar selection (DSS) -> Rehearsal memory -> Model training (cross-distillation) -> Evaluation
- Critical path: Exemplar selection -> Rehearsal memory management -> Model update loop
- Design tradeoffs:
  - 2D t-SNE is fast but may lose discriminative structure; higher dimensions could improve selection quality at computational cost.
  - Neighbor count r and n are hyperparameters; too strict and selection stalls, too loose and outliers creep in.
- Failure signatures:
  - Accuracy plateaus or drops sharply on old classes -> exemplar set not diverse enough or too many outliers.
  - Selection loop fails to add exemplars -> neighbor criteria too strict for current data distribution.
- First 3 experiments:
  1. Run DSS on CIFAR100 with default (n=5, r=0.5) and compare AA to random selection.
  2. Vary n (0, 3, 5, 8) and measure AA in disjoint setup to confirm neighbor filter benefit.
  3. Visualize t-SNE embedding of selected exemplars vs. full dataset to verify class separation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DSS change when using higher-dimensional representations (e.g., 10D) instead of 2D t-SNE for exemplar selection?
- Basis in paper: [explicit] The paper mentions that "an arbitrarily chosen 2 is used as this is computationally very efficient and without losing the important structure of data. However, X can also be reduced to slightly higher dimensions."
- Why unresolved: The paper only reports results using 2D t-SNE and does not explore the impact of higher-dimensional representations on DSS performance.
- What evidence would resolve it: Experiments comparing DSS performance using 2D, 5D, and 10D t-SNE reductions on CIFAR100, measuring average accuracy across disjoint and fuzzy setups.

### Open Question 2
- Question: How does DSS perform on other benchmark datasets like ImageNet or Omniglot compared to CIFAR100?
- Basis in paper: [inferred] The paper only evaluates DSS on CIFAR100, but does not test its generalization to other datasets with different characteristics.
- Why unresolved: The experiments are limited to a single dataset, so the robustness of DSS to dataset variations is unknown.
- What evidence would resolve it: Running DSS experiments on multiple datasets (e.g., ImageNet, Omniglot, Tiny ImageNet) and comparing average accuracy and performance trends across datasets.

### Open Question 3
- Question: What is the impact of varying the exemplar memory size (currently set to 1000) on DSS performance?
- Basis in paper: [explicit] The paper states that "the memory capacity of the exemplars was set to 1000" but does not explore the effect of changing this parameter.
- Why unresolved: The experiments use a fixed memory size, so the sensitivity of DSS to memory constraints is unknown.
- What evidence would resolve it: Experiments varying the exemplar memory size (e.g., 500, 1000, 2000) and measuring average accuracy and forgetting metrics across different memory capacities.

### Open Question 4
- Question: How does DSS compare to online continual learning methods that do not use exemplar replay?
- Basis in paper: [inferred] The paper compares DSS only to rehearsal-based methods like BiC and GDumb, but does not test against regularization-based or parameter isolation methods.
- Why unresolved: The comparison is limited to rehearsal-based methods, so DSS's relative effectiveness in broader CIL scenarios is unknown.
- What evidence would resolve it: Experiments comparing DSS to regularization-based methods (e.g., EWC, RTRA) and parameter isolation methods (e.g., Piggyback, PackNet) on CIFAR100, measuring average accuracy and forgetting metrics.

## Limitations

- Reliance on 2D t-SNE for exemplar selection may lose discriminative structure without comparison to alternative dimensionality reduction methods
- Neighbor count filter effectiveness lacks direct experimental validation or sensitivity analysis
- Limited comparison to recent rehearsal-based methods beyond BiC and GDumb baselines

## Confidence

- **High confidence**: The core algorithmic framework (k-center clustering with farthest-point selection) is well-established and correctly implemented.
- **Medium confidence**: The method's ability to improve average accuracy on CIFAR100 in both disjoint and fuzzy task boundary scenarios, as the experimental results are presented with clear metrics but limited ablation studies.
- **Low confidence**: The claim that the neighbor count filter effectively excludes outliers, as this mechanism lacks direct experimental validation or comparison to alternative outlier rejection strategies.

## Next Checks

1. **Ablation on dimensionality reduction**: Run DSS with PCA and UMAP (instead of t-SNE) on CIFAR100 and compare average accuracy to assess the impact of dimensionality reduction choice on exemplar quality.

2. **Sensitivity analysis on neighbor parameters**: Systematically vary n (0, 3, 5, 8) and r (0.3, 0.5, 0.7) in the DSS algorithm and measure their effect on average accuracy and outlier rejection rate.

3. **Outlier rejection experiment**: Generate a synthetic dataset with known outliers, apply DSS with and without the neighbor count filter, and quantify the proportion of outliers in the selected exemplar set.