---
ver: rpa2
title: 'Large Language Models for Failure Mode Classification: An Investigation'
arxiv_id: '2309.08181'
source_url: https://arxiv.org/abs/2309.08181
tags:
- failure
- mode
- prompt
- data
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of large language models (LLMs)
  for automated failure mode classification in maintenance work orders. The authors
  investigate prompt engineering techniques to enable an off-the-shelf GPT-3.5 model
  to predict failure modes from maintenance observations.
---

# Large Language Models for Failure Mode Classification: An Investigation

## Quick Facts
- arXiv ID: 2309.08181
- Source URL: https://arxiv.org/abs/2309.08181
- Authors: 
- Reference count: 6
- One-line primary result: Fine-tuning GPT-3.5 on annotated data achieves F1=0.80 for failure mode classification, significantly outperforming both non-fine-tuned LLM (F1=0.46) and traditional text classifier (F1=0.60)

## Executive Summary
This paper investigates the application of large language models (LLMs) for automated failure mode classification in maintenance work orders. The authors compare three approaches: a traditional Flair-based text classifier, an off-the-shelf GPT-3.5 model with prompt engineering, and a fine-tuned GPT-3.5 model. Their results demonstrate that while prompt engineering enables basic functionality, fine-tuning on annotated data yields substantially better performance (F1=0.81) compared to both the un fine-tuned LLM (F1=0.46) and the traditional classifier (F1=0.60). The study emphasizes the critical importance of high-quality annotated datasets for achieving domain-specific performance with LLMs.

## Method Summary
The authors conducted a comparative study using a dataset of 626 total examples (502 training, 62 validation, 62 test) consisting of maintenance work order observations paired with ISO 14224 failure mode codes. They implemented three classification approaches: a Flair-based text classifier from prior work, an off-the-shelf GPT-3.5 model using prompt engineering techniques, and a fine-tuned GPT-3.5 model trained on the annotated dataset. All models were evaluated using Micro-F1 and Macro-F1 metrics on the same test set to ensure fair comparison.

## Key Results
- Fine-tuned GPT-3.5 achieved F1=0.81, significantly outperforming non-fine-tuned GPT-3.5 (F1=0.46) and Flair baseline (F1=0.60)
- Prompt engineering alone provided limited improvement over random baseline for failure mode classification
- The fine-tuned LLM showed consistent performance across different temperature settings when using deterministic inference
- All three models were evaluated on the same test set with identical evaluation metrics for direct comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning significantly improves LLM performance on domain-specific classification tasks
- Mechanism: The LLM's general knowledge is supplemented with task-specific patterns through exposure to annotated examples during fine-tuning
- Core assumption: The annotated dataset contains sufficient representative examples to capture the domain's classification patterns
- Evidence anchors:
  - [abstract]: "fine-tuning the model on annotated data yields significantly better results (F1=0.80) compared to both the un fine-tuned LLM (F1=0.46)"
  - [section]: "Table 3 shows the results of each model on the test dataset. It is clear that fine-tuning has a significant impact on performance, as the Micro-F1 score jumps from 0.46 to 0.81 between the non fine-tuned and fine-tuned models respectively"
  - [corpus]: Weak corpus support - no directly relevant papers found
- Break condition: If the annotated dataset is too small or not representative of the task's variation, fine-tuning will not provide significant improvement

### Mechanism 2
- Claim: Prompt engineering enables zero-shot or few-shot performance from LLMs on specialized tasks
- Mechanism: Carefully crafted prompts guide the LLM's generation process to produce desired outputs without modifying model weights
- Core assumption: The LLM has sufficient latent knowledge about the task domain from its pre-training
- Evidence anchors:
  - [abstract]: "We detail our approach to prompt engineering to enable an LLM to predict the failure mode of a given observation"
  - [section]: "The default behaviour of the GPT-based models is to act as a chatbot... Structuring an input prompt to elicit a particular response from a large language model is known as prompt engineering"
  - [corpus]: Weak corpus support - no directly relevant papers found
- Break condition: If the task requires domain-specific knowledge not present in the pre-training corpus, prompt engineering alone will not achieve satisfactory performance

### Mechanism 3
- Claim: LLMs outperform traditional text classification models when fine-tuned on domain data
- Mechanism: LLMs leverage their larger capacity and pre-training to capture complex patterns that traditional models miss
- Core assumption: The task complexity exceeds what traditional models can capture effectively
- Evidence anchors:
  - [abstract]: "The fine-tuned model also outperforms the out-of-the box GPT-3.5 (F1=0.46). This investigation reinforces the need for high quality fine-tuning data sets for domain-specific tasks using LLMs"
  - [section]: "After fine-tuning on the annotated data, the LLM performs significantly better than Flair"
  - [corpus]: Weak corpus support - no directly relevant papers found
- Break condition: If the task is relatively simple and well-defined, traditional models may perform comparably without requiring extensive resources

## Foundational Learning

- Concept: Text classification fundamentals (tokenization, embedding, model architecture)
  - Why needed here: Understanding how both traditional and LLM-based classifiers process text is essential for interpreting results
  - Quick check question: What is the difference between how Flair and GPT-based models handle text input?

- Concept: Prompt engineering techniques and best practices
  - Why needed here: The paper demonstrates multiple prompt engineering approaches and their effectiveness
  - Quick check question: What are the three components of a GPT prompt structure?

- Concept: Evaluation metrics for classification (F1-score, micro vs macro averaging)
  - Why needed here: The paper uses specific metrics to compare model performance
  - Quick check question: How does micro-F1 differ from macro-F1 in handling class imbalance?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Prompt generation system -> LLM inference engine -> Result post-processing -> Evaluation framework
- Critical path: Data preprocessing → Prompt generation → LLM inference → Result post-processing → Evaluation
- Design tradeoffs:
  - Fine-tuning vs. prompt engineering: Fine-tuning requires annotated data but yields better results; prompt engineering is cheaper but less effective
  - Cloud vs. local deployment: Cloud offers better performance but raises data privacy concerns; local deployment requires significant computational resources
  - Temperature setting: Lower temperature reduces variability but may limit creativity in ambiguous cases
- Failure signatures:
  - Low F1 scores: Could indicate insufficient training data, poor prompt design, or domain mismatch
  - Inconsistent outputs: May result from high temperature settings or lack of prompt constraints
  - API failures: Could indicate rate limiting or server overload during inference
- First 3 experiments:
  1. Run inference with basic prompt engineering on test data to establish baseline
  2. Fine-tune GPT-3.5 on training data and evaluate on validation set
  3. Compare fine-tuned LLM performance against Flair baseline on test set

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of fine-tuned LLMs compare to traditional text classification models when using larger annotated datasets for Failure Mode Classification?
- Open Question 2: How effective are offline LLMs like LLaMA for Failure Mode Classification compared to OpenAI's GPT models?
- Open Question 3: Can prompt engineering alone achieve acceptable performance for Failure Mode Classification without fine-tuning?

## Limitations

- The study uses a relatively small annotated dataset (502 training examples) that may not capture full complexity of real-world failure modes
- Investigation is limited to a single cloud-based LLM (GPT-3.5) without comparing against other architectures or offline alternatives
- The paper lacks detailed prompt engineering templates, making exact reproduction challenging

## Confidence

- High confidence: Comparative performance between fine-tuned and non-fine-tuned LLMs (F1=0.81 vs 0.46)
- Medium confidence: Superiority of LLMs over traditional classifiers given small sample size
- Medium confidence: Prompt engineering mechanisms due to limited exploration of alternative strategies

## Next Checks

1. **Dataset Scaling Experiment**: Systematically vary the training set size (e.g., 50, 100, 250, 502 examples) to determine the minimum dataset size required for effective fine-tuning and identify the point of diminishing returns.

2. **Offline LLM Comparison**: Implement the same pipeline using an open-source LLM (e.g., LLaMA or GPT-Neo) fine-tuned locally to assess whether the performance advantages persist without relying on proprietary APIs.

3. **Prompt Engineering Ablation**: Conduct a systematic comparison of different prompt engineering approaches (few-shot examples, chain-of-thought prompting, structured output formats) to quantify their individual contributions to performance.