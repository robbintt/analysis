---
ver: rpa2
title: Better Handling Coreference Resolution in Aspect Level Sentiment Classification
  by Fine-Tuning Language Models
arxiv_id: '2307.05646'
source_url: https://arxiv.org/abs/2307.05646
tags:
- alsc
- performance
- auxiliary
- cases
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies that aspect-level sentiment classification
  models struggle with coreference resolution, leading to errors in ~15% of cases.
  The authors propose fine-tuning large language models on highly inferential auxiliary
  tasks (e.g., Commongen, QQP) to improve coreference resolution ability, resulting
  in a 16% increase in coreference resolution accuracy and a 5% improvement in aspect-level
  sentiment classification F1 score on coreference resolution cases.
---

# Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models

## Quick Facts
- arXiv ID: 2307.05646
- Source URL: https://arxiv.org/abs/2307.05646
- Reference count: 16
- Primary result: ~15% of ALSC errors are due to coreference resolution failures; fine-tuning on inferential tasks improves CR ability and ALSC performance by ~5% F1

## Executive Summary
This paper addresses a critical bottleneck in aspect-level sentiment classification (ALSC): coreference resolution. The authors identify that approximately 15% of ALSC errors stem from the model's inability to correctly resolve pronouns to their antecedents, leading to misclassification of sentiments for specific aspects. To address this, they propose a novel approach of fine-tuning large language models (T5-large) on highly inferential auxiliary tasks (Commongen, QQP) before fine-tuning on the ALSC task. This two-stage fine-tuning process results in a 16% increase in coreference resolution accuracy and a 5% improvement in ALSC F1 score on coreference resolution cases. The authors also release a new dataset (ALSC-CR) specifically focused on coreference resolution challenges in ALSC.

## Method Summary
The authors fine-tune a T5-large model on highly inferential auxiliary tasks (Commongen, CosmosQA, SQuAD, QQP) before fine-tuning on the ALSC-CR dataset, which contains reviews requiring coreference resolution. The method involves two stages: (1) auxiliary fine-tuning on tasks that require inferential reasoning and coreference resolution skills, and (2) fine-tuning on the ALSC-CR dataset to evaluate improvements in aspect-level sentiment classification performance. The model's coreference resolution ability is evaluated using the DPR dataset, while ALSC performance is measured using F1 scores on the ALSC-CR test set.

## Key Results
- ~15% of ALSC errors are attributed to coreference resolution failures
- Fine-tuning on highly inferential auxiliary tasks (Commongen, QQP) improves coreference resolution accuracy by 16%
- ALSC performance on coreference resolution cases improves by 5% F1 score
- No correlation found between auxiliary task dataset size and target task performance improvement
- Generative auxiliary tasks (Commongen) show more effectiveness than discriminative tasks (CosmosQA) for improving coreference resolution in generative models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auxiliary fine-tuning on highly inferential tasks improves coreference resolution ability, which directly translates to better ALSC performance.
- Mechanism: Training on tasks like Commongen and QQP enhances the model's ability to resolve pronouns to their antecedents, a critical skill for correctly associating sentiments with specific aspects in reviews.
- Core assumption: Coreference resolution ability is a bottleneck in ALSC, and improving this ability through auxiliary tasks will lead to performance gains.
- Evidence anchors: [abstract], [section 3.3]
- Break condition: If auxiliary tasks don't improve coreference resolution ability, or if CR isn't a bottleneck in ALSC.

### Mechanism 2
- Claim: Generative auxiliary tasks (like Commongen) are more effective than discriminative tasks (like CosmosQA) for improving coreference resolution in generative models.
- Mechanism: Generative tasks require the model to produce text that adheres to commonsense, which helps in learning relationships between pronouns and their antecedents.
- Core assumption: The nature of auxiliary task (generative vs. discriminative) impacts how well the model learns coreference resolution skills.
- Evidence anchors: [section 3.2]
- Break condition: If model architecture or task requirements change such that generative vs. discriminative nature no longer impacts CR learning.

### Mechanism 3
- Claim: The size of the auxiliary dataset does not significantly correlate with improvement in target task performance.
- Mechanism: The model learns necessary skills from the auxiliary task regardless of dataset size, as long as the task is sufficiently challenging and relevant.
- Core assumption: There's a threshold effect where beyond a certain point, increasing auxiliary dataset size doesn't lead to further improvements.
- Evidence anchors: [section 3.2]
- Break condition: If model requires minimum data to learn auxiliary task effectively, or if auxiliary task complexity scales with dataset size.

## Foundational Learning

- Concept: Coreference resolution
  - Why needed here: Coreference resolution determines which entity a pronoun refers to in a sentence. This is crucial for correctly associating sentiments with specific aspects in reviews, especially when pronouns are used instead of directly naming the aspect.
  - Quick check question: Given "The laptop's battery life is impressive, but it overheats easily," what does "it" refer to, and what sentiment is associated with it?

- Concept: Aspect-level sentiment classification
  - Why needed here: ALSC involves determining sentiment polarity (positive, negative, neutral) associated with specific aspects mentioned in text. This task requires understanding both the aspect and sentiment expression, which can be challenging when coreference resolution is needed.
  - Quick check question: In "The restaurant's ambiance is great, but the food is mediocre," what are the aspects, and what are their associated sentiment polarities?

- Concept: Auxiliary fine-tuning
  - Why needed here: Auxiliary fine-tuning involves first training the model on a related task before fine-tuning on the target task. This can help the model learn general skills that are beneficial for the target task, such as coreference resolution skills for ALSC.
  - Quick check question: Why might training a model on a question-answering task before fine-tuning it on sentiment analysis help improve its performance on sentiment analysis?

## Architecture Onboarding

- Component map:
  T5-large language model -> Auxiliary tasks (Commongen, CosmosQA, SQuAD, QQP) -> ALSC-CR dataset -> Coreference resolution evaluation (DPR dataset)

- Critical path:
  1. Pre-train T5 model on one of the auxiliary tasks
  2. Fine-tune pre-trained model on ALSC-CR dataset
  3. Evaluate model's performance on ALSC-CR test set and coreference resolution ability on DPR dataset

- Design tradeoffs:
  - Auxiliary task selection: Choosing tasks that improve coreference resolution without causing catastrophic forgetting of original LLM capabilities
  - Dataset size: Balancing between having enough data to learn auxiliary task effectively and avoiding overfitting or catastrophic forgetting
  - Model architecture: Using a generative model (T5) that can handle the generative nature of auxiliary tasks and target task

- Failure signatures:
  - No improvement in ALSC-CR performance after auxiliary fine-tuning
  - Degradation in coreference resolution ability as measured by DPR
  - Overfitting to auxiliary task, leading to poor performance on target task

- First 3 experiments:
  1. Fine-tune T5 model on Commongen dataset and evaluate performance on ALSC-CR test set and DPR dataset
  2. Fine-tune T5 model on QQP dataset and evaluate performance on ALSC-CR test set and DPR dataset
  3. Compare performance of models fine-tuned on Commongen and QQP to baseline model (without auxiliary fine-tuning) to assess impact on coreference resolution and ALSC performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the research:

## Limitations
- Small test set size (250 examples) raises concerns about statistical significance of reported improvements
- Evaluation focuses only on definite pronouns, potentially missing other coreference resolution challenges
- Lack of external validation beyond internal DPR dataset evaluation
- Limited comparison to alternative methods for improving coreference resolution in ALSC

## Confidence
- Core claim (auxiliary fine-tuning improves CR and ALSC): Medium
- Mechanism (Commongen/QQP specifically improve CR): Low
- Dataset size independence claim: Medium
- Generative vs discriminative task effectiveness: Low

## Next Checks
1. Re-run ALSC-CR experiments with 10 different random seeds and compute 95% confidence intervals for F1 score improvements to determine statistical significance given small test set size.

2. Test fine-tuned models on external aspect-level sentiment classification dataset not used in original ALSC-CR creation to validate generalization beyond specific dataset.

3. Extend evaluation to include other pronoun types (indefinite, relative) and noun phrase coreference to test whether improvements are specific to definite pronouns or represent more general coreference resolution capability enhancement.