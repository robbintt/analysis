---
ver: rpa2
title: Revisiting Recommendation Loss Functions through Contrastive Learning (Technical
  Report)
arxiv_id: '2312.08520'
source_url: https://arxiv.org/abs/2312.08520
tags:
- loss
- learning
- recommendation
- contrastive
- debiased
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically examines recommendation loss functions
  through the lens of contrastive learning, introducing two novel losses: MINE+ and
  Debiased CCL. MINE+ is an optimized generalization of InfoNCE with balance coefficients,
  while Debiased CCL leverages debiased InfoNCE to address bias in pointwise recommendation
  loss.'
---

# Revisiting Recommendation Loss Functions through Contrastive Learning (Technical Report)

## Quick Facts
- arXiv ID: 2312.08520
- Source URL: https://arxiv.org/abs/2312.08520
- Reference count: 40
- This paper systematically examines recommendation loss functions through the lens of contrastive learning, introducing two novel losses: MINE+ and Debiased CCL.

## Executive Summary
This technical report systematically examines recommendation loss functions through the lens of contrastive learning, introducing two novel losses: MINE+ and Debiased CCL. MINE+ is an optimized generalization of InfoNCE with balance coefficients, while Debiased CCL leverages debiased InfoNCE to address bias in pointwise recommendation loss. The paper also reveals that linear models like iALS and EASE are inherently debiased under reasonable assumptions. Empirical results demonstrate the effectiveness of MINE+ and Debiased CCL, with MINE+ achieving up to 16% improvement over InfoNCE.

## Method Summary
The paper introduces InfoNCE+, an optimized generalization of InfoNCE with balance coefficients λ and ϵ, and Debiased CCL, which leverages debiased InfoNCE to address bias in pointwise recommendation loss. The method uses a Matrix Factorization (MF) backbone with various loss functions including InfoNCE, CCL, debiased versions, and MINE+. Hyperparameters include negative sample count (800), batch size (512), learning rate (1e-4), and regularization weights. The proposed losses are evaluated on three commonly used datasets (Amazon-Books, Yelp2018, Gowalla) using Recall@20 and NDCG@20 metrics.

## Key Results
- MINE+ achieves up to 16% improvement over InfoNCE in recommendation performance.
- Debiased losses consistently outperform their biased counterparts, with improvements up to 4% for the CCL loss function.
- Linear models like iALS and EASE are inherently debiased under reasonable assumptions, contributing to their strong performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Introducing balance coefficients (λ, ϵ) into InfoNCE loss generalizes its form and allows exploration of a broader hypothesis space for recommendation models.
- **Mechanism**: The coefficients λ and ϵ control the trade-off between positive and negative terms in the contrastive loss, effectively adjusting the weight of noise during optimization and the contribution of positive-negative pairs in the denominator.
- **Core assumption**: The optimal coefficient values (particularly ϵ=0) align with the latest decoupled contrastive learning loss (DCL), which omits the positive term from the denominator.
- **Evidence anchors**:
  - [abstract]: "InfoNCE+, an optimized generalization of InfoNCE with balance coefficients, and highlight its performance advantages, particularly when aligned with our new decoupled contrastive loss, MINE+."
  - [section]: "Our empirical results (in Fig. 1 right) reveal that when ϵ = 0, the recommendation models achieve peak performance. Interestingly, this empirical discovery is consistent with the latest decoupled contrastive learning (Yeh et al. 2022), where they omit the positive term as well."
- **Break condition**: If the balance coefficients are fixed at default values (λ=1, ϵ=1), the model may not achieve optimal performance as it cannot explore the broader hypothesis space enabled by these coefficients.

### Mechanism 2
- **Claim**: Debiasing contrastive losses like InfoNCE and CCL addresses the bias introduced by negative sampling, leading to improved recommendation performance.
- **Mechanism**: Debiased losses use a debiased estimator that accounts for the selection bias in negative sampling by leveraging the distribution of observed positive items and unlabeled training data.
- **Core assumption**: The debiased estimator effectively corrects for the bias in negative sampling, leading to a more accurate representation of user-item relationships.
- **Evidence anchors**:
  - [abstract]: "We also leverage debiased InfoNCE to debias pointwise recommendation loss (CCL) as Debiased CCL."
  - [section]: "By leveraging the latest debiased contrastive loss (Chuang et al. 2020), we propose debiased InfoNCE and generalize the debiased contrastive loss (Chuang et al. 2020) to the pointwise loss CCL (Mao et al. 2021)) in recommendation models, referred as Debiased CCL."
- **Break condition**: If the debiasing estimator is not accurate or the assumptions about the data distribution are violated, the debiased loss may not effectively correct for the bias and could even introduce additional errors.

### Mechanism 3
- **Claim**: Linear models like iALS and EASE are inherently debiased under reasonable assumptions, contributing to their strong performance.
- **Mechanism**: The solvers of iALS and EASE can absorb their debiased counterparts under existing frameworks, indicating that these models naturally account for the bias in negative sampling.
- **Core assumption**: The debiased loss can be transformed into a form that is equivalent to the original loss used in iALS and EASE, allowing the solvers to handle it without modification.
- **Evidence anchors**:
  - [abstract]: "Interestingly, our analysis reveals that linear models like iALS and EASE are inherently debiased under reasonable assumptions."
  - [section]: "Surprisingly, we found these linear models are naturally debiased which contributes to the following observation: Observation 1. The solvers of both iALS and EASE models can absorb their debiased counterparts under existing frameworks with reasonable conditions."
- **Break condition**: If the assumptions about the data distribution or the form of the debiased loss are violated, the inherent debiasing property of iALS and EASE may not hold, and their performance could degrade.

## Foundational Learning

- **Concept**: Contrastive Learning
  - **Why needed here**: Contrastive learning provides a framework for understanding and designing recommendation loss functions by leveraging the principles of pulling similar data pairs closer and pushing dissimilar pairs apart.
  - **Quick check question**: What is the main goal of contrastive learning in the context of recommendation systems?

- **Concept**: Mutual Information
  - **Why needed here**: Mutual information is a key concept in understanding the relationship between user and item embeddings in recommendation systems, and it is closely related to the contrastive learning loss functions.
  - **Quick check question**: How does maximizing mutual information between user and item embeddings relate to the performance of recommendation models?

- **Concept**: Negative Sampling
  - **Why needed here**: Negative sampling is a crucial technique in recommendation systems for handling the large item space, but it can introduce bias that needs to be addressed through debiasing methods.
  - **Quick check question**: What are the potential sources of bias in negative sampling for recommendation systems?

## Architecture Onboarding

- **Component map**: User-Item Interaction Matrix -> User/Item Embeddings -> Loss Function (InfoNCE+, Debiased CCL, MINE+) -> Optimization Algorithm (Adam)
- **Critical path**: The critical path involves generating user and item embeddings, computing the loss function, and updating the embeddings through optimization.
- **Design tradeoffs**: The choice of loss function (e.g., InfoNCE+ vs. Debiased CCL) involves a tradeoff between model complexity and performance. The hyperparameters (e.g., λ, ϵ) need to be tuned for optimal results.
- **Failure signatures**: Poor performance could indicate issues with the loss function, hyperparameters, or data preprocessing. Degraded performance over time may suggest the need for model retraining or hyperparameter tuning.
- **First 3 experiments**:
  1. Implement and compare the performance of InfoNCE+ and Debiased CCL on a small dataset to validate the theoretical claims.
  2. Conduct an ablation study to assess the impact of the balance coefficients (λ, ϵ) on the performance of InfoNCE+.
  3. Evaluate the inherent debiasing property of iALS and EASE by comparing their performance with and without the debiased loss functions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hyperparameters λ and ϵ in InfoNCE+ affect recommendation performance?
- Basis in paper: [explicit] The paper states that InfoNCE+ introduces balance coefficients λ and ϵ, and mentions that optimal values are around 1.1 for λ and 0 for ϵ.
- Why unresolved: The paper only provides empirical observations for these values, without a theoretical explanation for why these specific values work best.
- What evidence would resolve it: A theoretical analysis explaining the optimal values of λ and ϵ, or extensive empirical studies varying these parameters across different datasets and model architectures.

### Open Question 2
- Question: How does the debiased MSE loss affect the solution of linear recommendation models like iALS and EASE?
- Basis in paper: [explicit] The paper claims that both iALS and EASE can absorb their debiased counterparts under reasonable conditions, but does not provide extensive empirical validation.
- Why unresolved: The paper provides a theoretical analysis but lacks empirical studies to confirm these claims.
- What evidence would resolve it: Empirical studies comparing the performance of iALS and EASE with their debiased versions on various datasets and model architectures.

### Open Question 3
- Question: How effective are the proposed loss functions when combined with more sophisticated neural architectures?
- Basis in paper: [explicit] The paper states that MINE+ and Debiased CCL are model-agnostic and can be applied to any representation-based model, but only tests them with a basic Matrix Factorization (MF) backbone.
- Why unresolved: The paper does not explore the effectiveness of these loss functions with more advanced neural architectures.
- What evidence would resolve it: Empirical studies applying MINE+ and Debiased CCL to more advanced neural architectures like LightGCN, NGCF, or other deep learning-based recommendation models.

## Limitations
- The theoretical foundations for why MINE+ and Debiased CCL work remain incompletely explained.
- The claim that linear models like iALS and EASE are inherently debiased relies on reasonable assumptions that may not hold in all real-world scenarios.
- The paper only tests the proposed losses on a basic Matrix Factorization (MF) backbone, without exploring their effectiveness with more sophisticated neural architectures.

## Confidence
- **High Confidence**: Empirical performance improvements of MINE+ and Debiased CCL over baseline methods, as demonstrated across multiple datasets and metrics.
- **Medium Confidence**: The theoretical justification for debiasing contrastive losses, as the relationship between negative sampling bias and the proposed corrections requires further validation.
- **Medium Confidence**: The claim about linear models being inherently debiased, as this depends on specific conditions and data distributions that may not generalize.

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component in MINE+ and Debiased CCL, particularly the balance coefficients and debiasing terms.
2. Test the proposed losses on additional datasets with different characteristics (e.g., implicit vs. explicit feedback, different sparsity levels) to assess generalizability.
3. Implement a theoretical analysis framework to formally prove the debiasing properties of iALS and EASE under varying conditions.