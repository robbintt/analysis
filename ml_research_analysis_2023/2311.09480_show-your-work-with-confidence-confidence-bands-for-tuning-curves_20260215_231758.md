---
ver: rpa2
title: 'Show Your Work with Confidence: Confidence Bands for Tuning Curves'
arxiv_id: '2311.09480'
source_url: https://arxiv.org/abs/2311.09480
tags:
- bands
- tuning
- confidence
- curve
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tuning curves measure model performance against hyperparameter
  search budget. Bootstrap confidence bands fail to capture uncertainty; the paper
  presents the first valid confidence bands for tuning curves.
---

# Show Your Work with Confidence: Confidence Bands for Tuning Curves

## Quick Facts
- arXiv ID: 2311.09480
- Source URL: https://arxiv.org/abs/2311.09480
- Reference count: 36
- Primary result: First valid, simultaneous, exact, distribution-free confidence bands for hyperparameter tuning curves

## Executive Summary
This paper addresses the challenge of constructing valid confidence bands for tuning curves, which measure model performance against hyperparameter search budget. The authors present a novel method that transforms simultaneous confidence bands for the cumulative distribution function (CDF) of single scores into confidence bands for the tuning curve. Unlike bootstrap methods, these bands achieve their target coverage exactly and are robust to the underlying score distribution. The method recommends median tuning curves over mean due to interpretability and computational advantages.

## Method Summary
The method constructs simultaneous confidence bands for the CDF of scores from hyperparameter search using Learned-Miller-DeStefano (LD) bands, then transforms these bounds algebraically into confidence bands for the tuning curve. For median tuning curves, the transformation is exact and distribution-free. The approach uses kernel density estimation (KDE) to model score distributions, with boundary correction applied. The authors demonstrate the method on DeBERTa and DeBERTaV3 models fine-tuned on MultiNLI using random hyperparameter search.

## Key Results
- Bootstrap confidence bands fail to achieve meaningful coverage for tuning curves
- LD confidence bands produce tighter bounds than DKW or KS alternatives
- Median tuning curves are more interpretable and computationally tractable than mean
- Highest density intervals produce tighter bands than equal-tailed intervals
- Band width depends on score distribution and sample size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence bands for tuning curves can be derived exactly from simultaneous CDF bands
- Mechanism: The method translates bounds on the CDF of single scores into bounds on the CDF of the best score after k trials, then extracts quantiles or expectations from these transformed bounds
- Core assumption: The score distribution is continuous and the CDF bounds are simultaneous (covering the entire curve at once)
- Evidence anchors:
  - [abstract] "Our key insight is to translate simultaneous confidence bands for the test scores' cumulative distribution function (CDF) into confidence bands for the tuning curve, via an algebraic relationship."
  - [section 3.2] "Our core strategy translates simultaneous confidence bands for the test scores' cumulative distribution function (CDF) into confidence bands for the tuning curve, via an algebraic relationship."
- Break condition: If the score distribution is discrete, the bands may become conservative or lose exactness

### Mechanism 2
- Claim: Learned-Miller-DeStefano (LD) bands provide tighter confidence bands than DKW or KS bands for tuning curves
- Mechanism: LD bands narrow at the extremes of the CDF, matching the shape of the distribution more closely than constant-width DKW or KS bands, resulting in tighter bounds on the tuning curve
- Core assumption: The score distribution is continuous and has nontrivial variation across its range
- Evidence anchors:
  - [section 3.3] "To fix this issue, Learned-Miller and DeStefano (2008) derived simultaneous confidence bands that are violated equally often at all points, and thus much narrower at the extremes."
  - [section 5.3] "Our recommended choice, the highest density LD bands, gives a tighter bound over a greater range than the alternatives."
- Break condition: If the score distribution is uniform or has very little variation, the advantage of LD bands diminishes

### Mechanism 3
- Claim: Median tuning curves are more interpretable and computationally tractable than mean tuning curves
- Mechanism: The median avoids issues with unbounded expectations and skewness, providing a clear "50% chance of doing better" interpretation
- Core assumption: The score distribution is continuous and the median is well-defined
- Evidence anchors:
  - [section 5.4] "Interpreting the median, in contrast, is simple and straightforward: half the time you do better, half the time you do worse."
  - [section 5.4] "it is impossible to construct a non-parametric confidence interval for the mean unless Tk is globally bounded."
- Break condition: If the score distribution has heavy tails or is highly skewed, the median may not capture the full picture of expected performance

## Foundational Learning

- Concept: Cumulative Distribution Function (CDF)
  - Why needed here: The method constructs confidence bands by bounding the CDF of scores and then transforming these bounds to obtain tuning curve bands
  - Quick check question: Given a set of independent scores, how would you construct their empirical CDF?

- Concept: Order Statistics
  - Why needed here: The LD bands use order statistics to create pointwise confidence intervals for the CDF at each observed value
  - Quick check question: If you have n ordered samples, what is the distribution of the i-th order statistic under a uniform CDF?

- Concept: Bootstrap Resampling
  - Why needed here: The paper contrasts its exact confidence bands with bootstrap methods, which fail to achieve meaningful coverage for tuning curves
  - Quick check question: Why might bootstrap confidence intervals fail to capture the true uncertainty in tuning curves, even with large sample sizes?

## Architecture Onboarding

- Component map: Scores from hyperparameter search -> CDF estimation and bounding (DKW, KS, or LD methods) -> Transform CDF bounds to tuning curve bounds -> Confidence bands for median or mean tuning curves

- Critical path:
  1. Collect scores from hyperparameter search
  2. Estimate the CDF of scores
  3. Construct simultaneous confidence bands for the CDF
  4. Transform CDF bands into tuning curve bands
  5. Extract median or mean tuning curve estimates from transformed bounds

- Design tradeoffs:
  - DKW vs. KS vs. LD bands: Simplicity vs. tightness of bounds
  - Equal-tailed vs. highest density intervals: Computational cost vs. bound tightness
  - Median vs. mean tuning curves: Interpretability vs. sensitivity to outliers

- Failure signatures:
  - Bands are too wide: May indicate need for more samples or that the score distribution is highly variable
  - Bands fail to cover the true tuning curve: Suggests issues with the continuous distribution assumption or implementation errors
  - Bootstrap bands show poor coverage: Confirms the need for exact methods over resampling approaches

- First 3 experiments:
  1. Compare DKW, KS, and LD confidence bands on a synthetic dataset with known CDF
  2. Evaluate the effect of sample size on the width of median tuning curve confidence bands
  3. Test the coverage of median tuning curve bands on a real hyperparameter search dataset (e.g., DeBERTa tuning on MultiNLI)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the LD bands remain exact for discrete score distributions, or are they conservative as the KS bands are?
- Basis in paper: [explicit] The paper states "If Y has a continuous distribution, then both the KS and LD bands have exact coverage" and notes the KS bands are conservative for discrete distributions
- Why unresolved: The paper only proves exactness for continuous distributions and suggests investigating whether LD bands are conservative for discrete distributions
- What evidence would resolve it: Empirical or theoretical analysis comparing LD band coverage on discrete vs continuous score distributions

### Open Question 2
- Question: What is the exact relationship between sample size and the search iteration at which the upper confidence band becomes trivial?
- Basis in paper: [explicit] The paper states "increasing the sample size seems to linearly increase this range" and shows the relationship explains 99.95% of variance
- Why unresolved: The paper only demonstrates an empirical linear relationship for one specific model and task
- What evidence would resolve it: Systematic experiments across different models, tasks, and sample sizes to determine if the linear relationship holds generally

### Open Question 3
- Question: Can a non-parametric confidence interval for the mean tuning curve be constructed without global bounds on the score distribution?
- Basis in paper: [explicit] The paper states "it is impossible to construct a non-parametric confidence interval for the mean unless Tk is globally bounded" and notes this causes slower convergence
- Why unresolved: The paper only shows this is impossible under current non-parametric methods, not that it's theoretically impossible
- What evidence would resolve it: Either a theoretical proof that non-parametric mean confidence intervals require global bounds, or a construction of such intervals without global bounds

## Limitations

- The method's exactness relies on the assumption of a continuous score distribution, potentially becoming conservative for discrete distributions
- Limited empirical validation to only two specific NLP tasks (DeBERTa and DeBERTaV3 on MultiNLI)
- Computational complexity of exact methods may become prohibitive for very large search spaces (>10,000 trials)

## Confidence

- **High Confidence**: The algebraic relationship between CDF bands and tuning curve bands, the superiority of LD bands over DKW/KS alternatives for the same coverage level, and the computational advantages of median over mean tuning curves
- **Medium Confidence**: The method's robustness across different score distributions and search spaces, given the limited empirical validation on just two model variants
- **Low Confidence**: The method's performance in extreme scenarios (very small sample sizes, highly multimodal score distributions, or non-standard search spaces)

## Next Checks

1. Test the method's coverage guarantees on synthetic score distributions with varying degrees of skewness and multimodality to identify conditions where the bands may become conservative

2. Apply the method to hyperparameter search results from non-NLP domains (e.g., computer vision or reinforcement learning) to assess cross-domain robustness

3. Compare the computational efficiency of the exact method against bootstrap approaches for very large search spaces (10,000+ trials) to identify scalability limits