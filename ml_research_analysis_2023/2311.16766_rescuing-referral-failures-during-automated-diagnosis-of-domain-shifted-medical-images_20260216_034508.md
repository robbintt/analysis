---
ver: rpa2
title: Rescuing referral failures during automated diagnosis of domain-shifted medical
  images
arxiv_id: '2311.16766'
source_url: https://arxiv.org/abs/2311.16766
tags:
- referral
- remedis
- data
- entropy
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automated diagnosis with domain-shifted
  medical images, where deep learning models must generalize well across diverse data
  domains. The authors show that even state-of-the-art domain generalization approaches
  fail during referral when tested on medical images acquired from a different demographic
  or using a different technology.
---

# Rescuing referral failures during automated diagnosis of domain-shifted medical images

## Quick Facts
- arXiv ID: 2311.16766
- Source URL: https://arxiv.org/abs/2311.16766
- Reference count: 40
- One-line primary result: Novel combinations of domain generalization and referral approaches achieve >10% performance improvements over baseline methods for automated diagnosis with domain-shifted medical images

## Executive Summary
This paper addresses the critical challenge of automated diagnosis with domain-shifted medical images, where deep learning models must generalize across diverse data domains. The authors demonstrate that even state-of-the-art domain generalization approaches fail during referral when tested on medical images from different demographics or acquired with different technologies. They propose and evaluate novel combinations of robust generalization and post hoc referral approaches that rescue these failures, achieving significant performance improvements (typically >10%) over baseline methods. The study focuses on two benchmark diagnostic medical imaging datasets exhibiting strong covariate shifts: diabetic retinopathy prediction with retinal fundus images and multilabel disease prediction with chest X-ray images.

## Method Summary
The study evaluates novel combinations of domain generalization approaches (REMEDIS, ViT+IW, ViT+OSP, ViT+SR) and referral strategies (MCD, OSP, SR) for automated diagnosis of domain-shifted medical images. The core methodology involves training Vision Transformer models on in-domain data with various domain adaptation techniques, then applying referral strategies to rank predictions by uncertainty for out-of-domain test samples. Key approaches include domain adversarial training (DAN) that learns domain-invariant features while maintaining classification accuracy, split referral that eliminates uncertainty imbalance across classes, and self-supervised pretraining with masked image modeling that captures local features relevant for medical anomalies. The methods are evaluated on two benchmark datasets exhibiting covariate shifts: diabetic retinopathy prediction (EyePACS vs APTOS) and chest X-ray multilabel disease prediction (CheXpert vs ChestX-ray14).

## Key Results
- Domain adversarial training (DAN) enables successful out-of-domain generalization by learning domain-invariant features while ignoring domain-distinctive features
- Split referral (SR) eliminates uncertainty imbalance across classes, rescuing referral failures even when models are miscalibrated
- Self-supervised pretraining with masked image modeling (SimMIM/MAE) captures local, high spatial frequency features more relevant for localized medical anomalies than global features
- REMEDIS/DAN and ViT+SR approaches achieve significant performance improvements, typically >10%, over baseline methods in terms of area under referral curve (AURC)
- The proposed methods outperform other approaches for both diabetic retinopathy and chest X-ray datasets

## Why This Works (Mechanism)

### Mechanism 1
Domain adversarial training (DAN) enables successful out-of-domain (OOD) generalization for referral by learning domain-invariant features that separate classes while ignoring domain-distinctive features. The DAN approach concurrently optimizes a label loss for in-domain (ID) samples and a domain loss that penalizes representations distinguishing ID from OOD data using a gradient reversal layer. This forces the model to learn features that are useful for classification but invariant to the domain shift. Core assumption: Class-distinctive features are independent of domain-distinctive features, allowing the model to rely on the latter alone for label predictions after domain-invariant feature learning.

### Mechanism 2
Split referral (SR) eliminates uncertainty imbalance across classes, rescuing referral failures even when models are miscalibrated. SR ranks OOD test samples separately for each predicted class based on their entropies, then makes referrals in order of least to most confident predictions proportionately for each predicted class. This maintains the proportion of predicted labels at every referral rate. Core assumption: As long as the calibration curve increases monotonically, SR yields a monotonically increasing accuracy-referral curve regardless of calibration error.

### Mechanism 3
Self-supervised pretraining with masked image modeling (SimMIM/MAE) captures local, high spatial frequency features that are more relevant for localized medical anomalies than global features captured by contrastive learning. SimMIM and MAE approaches mask random patches of images and train the model to predict the masked pixels, encouraging the model to learn local texture and spatial relationships. Core assumption: Medical images contain localized anomalies that are better captured by local feature learning than global feature learning.

## Foundational Learning

- **Domain generalization**: Why needed: The core challenge is that models trained on one medical imaging domain (e.g., retinal fundus images from one population) fail to generalize to out-of-domain data. Quick check: What is the difference between covariate shift and semantic shift in domain generalization?

- **Selective classification and referral**: Why needed: The study focuses on selective classification during automated diagnosis, where models must avoid making predictions when label confidence is low. Quick check: How does the area under referral curve (AURC) differ from traditional classification metrics like AUROC?

- **Uncertainty estimation in deep learning**: Why needed: Reliable referral requires accurate estimates of the model's predictive uncertainty. The study evaluates both deterministic and Bayesian approaches to uncertainty estimation. Quick check: What is the difference between aleatoric and epistemic uncertainty in Bayesian deep learning?

## Architecture Onboarding

- **Component map**: ViT initialization -> self-supervised pretraining (if applicable) -> domain adaptation or importance weighting (if applicable) -> referral strategy application (if applicable) -> fine-tuning on ID data -> evaluation on OOD data with referral analysis
- **Critical path**: For each experiment, the critical path is: ViT initialization → self-supervised pretraining (if applicable) → domain adaptation or importance weighting (if applicable) → referral strategy application (if applicable) → fine-tuning on ID data → evaluation on OOD data with referral analysis
- **Design tradeoffs**: Self-supervised pretraining trades compute for improved generalization; DAN trades model complexity for domain invariance; importance weighting trades potential ID performance for improved OOD generalization; referral strategies trade prediction coverage for reliability
- **Failure signatures**: Non-monotonic referral curves (AUROC or accuracy decreasing as referral rate increases), high AURC values, and poor performance at high referral rates (>70%) indicate referral failures
- **First 3 experiments**: 
  1. Implement ViT-Base (no self-supervised pretraining, no domain adaptation) and verify referral failures on the diabetic retinopathy dataset
  2. Implement REMEDIS/DAN and verify successful referral on the same dataset
  3. Implement ViT+SR and verify successful referral, comparing performance with REMEDIS/DAN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed methods perform on other types of domain shifts, such as semantic shifts or multi-domain shifts?
- Basis in paper: [explicit] The paper mentions that the proposed methods are tested on covariate shifts, but does not explicitly evaluate their performance on other types of domain shifts.
- Why unresolved: The paper only provides experimental results for covariate shifts, and does not explore the generalizability of the proposed methods to other types of domain shifts.
- What evidence would resolve it: Conducting experiments on datasets with different types of domain shifts, such as semantic shifts or multi-domain shifts, would provide evidence for the generalizability of the proposed methods.

### Open Question 2
- Question: How do the proposed methods compare to other state-of-the-art domain generalization approaches?
- Basis in paper: [inferred] The paper mentions that they evaluate the proposed methods against a baseline model, but does not explicitly compare them to other state-of-the-art domain generalization approaches.
- Why unresolved: The paper only provides a comparison with a baseline model, and does not explore how the proposed methods compare to other advanced techniques in the field.
- What evidence would resolve it: Conducting experiments comparing the proposed methods to other state-of-the-art domain generalization approaches, such as meta-learning or domain adaptation methods, would provide evidence for their effectiveness.

### Open Question 3
- Question: How do the proposed methods perform on datasets with different levels of label noise?
- Basis in paper: [explicit] The paper mentions that the label noise in the chest X-ray dataset may affect the performance of the proposed methods, but does not explicitly evaluate their robustness to different levels of label noise.
- Why unresolved: The paper only provides experimental results on a dataset with a certain level of label noise, and does not explore how the proposed methods perform on datasets with different levels of label noise.
- What evidence would resolve it: Conducting experiments on datasets with varying levels of label noise would provide evidence for the robustness of the proposed methods to label noise.

## Limitations
- The empirical claims rely heavily on controlled experimental settings with specific medical imaging datasets
- Limited corpus support for the specific mechanisms proposed (DAN, SR, self-supervised pretraining for medical imaging)
- The study focuses on binary and multilabel classification without exploring regression or other task types
- Does not extensively validate approaches across diverse medical imaging domains or with different disease types

## Confidence
- **High confidence**: The existence of referral failures in domain-shifted medical images and the basic framework for evaluating AURC metrics
- **Medium confidence**: The effectiveness of REMEDIS/DAN and ViT+SR approaches in rescuing referral failures, based on experimental results
- **Low confidence**: The specific mechanisms by which self-supervised pretraining with masked image modeling improves local feature learning for medical anomalies, due to limited corpus evidence

## Next Checks
1. Replicate the referral failure experiments with additional medical imaging datasets beyond diabetic retinopathy and chest X-ray to test generalizability across different imaging modalities and disease types
2. Conduct ablation studies to isolate the individual contributions of domain adversarial training, importance weighting, and split referral to the overall performance improvements
3. Test the robustness of the proposed approaches under varying levels of domain shift severity by creating synthetic domain shifts through controlled data augmentation and preprocessing variations