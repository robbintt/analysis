---
ver: rpa2
title: Implicit Chain of Thought Reasoning via Knowledge Distillation
arxiv_id: '2311.01460'
source_url: https://arxiv.org/abs/2311.01460
tags:
- reasoning
- teacher
- steps
- implicit
- intermediate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores an alternative to explicit chain-of-thought
  (CoT) reasoning in language models, proposing a method called implicit CoT reasoning.
  Instead of generating intermediate reasoning steps in natural language, the model
  performs reasoning internally using its hidden states.
---

# Implicit Chain of Thought Reasoning via Knowledge Distillation

## Quick Facts
- arXiv ID: 2311.01460
- Source URL: https://arxiv.org/abs/2311.01460
- Reference count: 30
- Primary result: Achieves 96% accuracy on 5-digit multiplication and 22% on GSM8K using implicit CoT reasoning

## Executive Summary
This paper proposes a method called implicit chain-of-thought (CoT) reasoning that performs reasoning internally through transformer hidden states rather than generating explicit intermediate steps. The approach uses knowledge distillation to transfer the teacher model's horizontal reasoning process (across tokens) into the student model's vertical reasoning process (across layers). Experiments show this enables solving tasks previously unsolvable without explicit CoT, with inference speed comparable to models without CoT.

## Method Summary
The method involves three key steps: (1) "Mind-Reading the Teacher" - training a student model to predict final answers using teacher hidden states from explicit CoT generation, (2) "Thought Emulation" - training an emulator to predict these hidden states directly from input, and (3) "Couple and Optimize" - end-to-end fine-tuning of the combined emulator-student system. This allows the model to develop its own reasoning pathways while maintaining high accuracy on tasks like multi-digit multiplication and grade school math problems.

## Key Results
- Achieves 96% accuracy on 5-digit by 5-digit multiplication (GPT-2 Medium)
- Solves tasks previously unsolvable without explicit CoT
- Inference speed comparable to no CoT while maintaining CoT-level accuracy
- 22% accuracy on GSM8K test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit reasoning via hidden state distillation can achieve high accuracy on tasks that explicit CoT cannot solve directly.
- Mechanism: Teacher's CoT reasoning generates intermediate hidden states across layers. These states are distilled into an emulator model that predicts them directly from input, bypassing explicit step generation. The student uses these predicted states to produce final answers, enabling "vertical" reasoning through layers instead of "horizontal" token generation.
- Core assumption: Teacher's internal hidden states contain sufficient information about intermediate reasoning steps.
- Evidence anchors:
  - [abstract] "Experiments on a multi-digit multiplication task and a grade school math problem dataset show that this approach enables solving tasks previously not solvable without explicit chain-of-thought"
  - [section] "standard training cannot yield the final answer without explicit reasoning (even GPT-4 struggles with five-digit by five-digit multiplication), our method... is able to provide direct answers for up to five-digit by five-digit multiplications"

### Mechanism 2
- Claim: Mixture model approach is crucial for handling multiple valid reasoning pathways in complex problems.
- Mechanism: Instead of predicting single hidden state per layer, emulator predicts mixture of components where each component corresponds to different reasoning path. Trained using mean squared loss and supervision from intermediate tokens to prevent mode collapse.
- Core assumption: Different reasoning pathways produce distinct hidden state distributions capturable by mixture model.
- Evidence anchors:
  - [section] "To account for multiple reasoning pathways, instead of predicting one ˆzl per layer, we predict a mixture of components P (ˆzl) = ∑ cl P (ˆzcl l |cl)P (cl)"
  - [section] "Empirically, we found that directly fitting this mixture is prone to mode collapsing (He et al., 2019), where only a few mixture components get used"

### Mechanism 3
- Claim: End-to-end optimization allows model to develop its own reasoning pathways, potentially diverging from teacher's approach.
- Mechanism: After emulator predicts teacher's hidden states and student uses them to generate answers, entire system is fine-tuned together. Allows student to adapt internal processing, potentially finding more efficient reasoning routes.
- Core assumption: Student can learn to use emulated hidden states in ways differing from teacher's explicit reasoning, leading to improved performance.
- Evidence anchors:
  - [section] "Finally, we combine the student and emulator... We then finetune this combined system end-to-end to improve the internal reasoning process, a step we name 'Couple and Optimize'"
  - [section] "Allowing the model to develop its own reasoning pathway is also important: if we fix the emulator and only optimize the student, accuracy drops to 13.0%"

## Foundational Learning

- Concept: Transformer hidden state dynamics
  - Why needed here: Understanding how information flows through transformer layers is crucial for grasping how implicit reasoning works via vertical processing of hidden states.
  - Quick check question: What information is typically captured in the hidden states of a transformer at different layers, and how does this relate to the model's reasoning process?

- Concept: Knowledge distillation
  - Why needed here: The thought emulation step is a form of knowledge distillation, where teacher's horizontal reasoning process is distilled into student and emulator's vertical reasoning process.
  - Quick check question: How does knowledge distillation typically work in context of language models, and what are key differences when distilling hidden states instead of output probabilities?

- Concept: Mixture models and mode collapse
  - Why needed here: Mixture model approach for handling multiple reasoning pathways requires understanding how mixture models work and how to prevent mode collapse during training.
  - Quick check question: What is mode collapse in context of mixture models, and what techniques can be used to prevent it during training?

## Architecture Onboarding

- Component map: Input → Teacher (CoT generation) → Hidden state extraction → Emulator (prediction) → Student (answer generation) → Output

- Critical path: Input → Teacher (CoT generation) → Hidden state extraction → Emulator (prediction) → Student (answer generation) → Output

- Design tradeoffs:
  - Number of layers: More layers allow for more complex reasoning but increase computational cost
  - Mixture model complexity: More components can capture more reasoning paths but increase training difficulty
  - Temperature in mixture prediction: Lower temperature makes prediction more deterministic but may reduce exploration

- Failure signatures:
  - Poor accuracy: Indicates issues with hidden state extraction, emulator prediction, or student usage of predicted states
  - Slow inference: Suggests emulator is too complex or not well-optimized
  - Overfitting: May occur during couple and optimize step, especially with limited data

- First 3 experiments:
  1. Test hidden state extraction: Extract diagonal elements from teacher's hidden states and verify they contain reasoning information
  2. Validate emulator training: Train emulator to predict teacher's hidden states and measure prediction accuracy
  3. Evaluate coupled system: Combine emulator and student, test end-to-end performance on simple task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many transformer layers are necessary for implicit chain-of-thought reasoning to be effective on increasingly complex tasks?
- Basis in paper: [inferred] Paper found GPT-2 Small (12 layers) performed well on 4x4 multiplication but significantly dropped to 10% on 5x5, whereas GPT-2 Medium (24 layers) achieved 96% accuracy on 5x5 multiplication.
- Why unresolved: Paper only tested two model sizes (12 and 24 layers), leaving unclear exact layer requirements for various task complexities.
- What evidence would resolve it: Systematic experiments testing models with varying layer counts across range of task complexities would reveal relationship between depth and reasoning capability.

### Open Question 2
- Question: Can implicit chain-of-thought reasoning be effectively combined with explicit chain-of-thought methods to achieve both interpretability and efficiency?
- Basis in paper: [inferred] Paper mentions implicit CoT lags behind explicit CoT in accuracy, suggesting potential complementarity between approaches.
- Why unresolved: Paper focuses solely on implicit reasoning without exploring hybrid approaches that might leverage strengths of both methods.
- What evidence would resolve it: Experiments comparing hybrid models that use explicit CoT for difficult steps while relying on implicit reasoning for simpler parts, measuring both accuracy and inference speed compared to pure approaches.

### Open Question 3
- Question: What are the optimal techniques for handling multiple reasoning pathways in implicit chain-of-thought reasoning?
- Basis in paper: [explicit] Paper discusses using mixture model to account for multiple reasoning pathways and notes that directly fitting this mixture is prone to mode collapsing.
- Why unresolved: While mixture approach showed improvement, paper only tested one variant and acknowledged issues with mode collapsing.
- What evidence would resolve it: Comparing different techniques for handling multiple pathways on tasks with known alternative solutions would identify optimal approaches.

## Limitations
- Performance generalization remains unclear - results specific to tested model scales and datasets
- Efficiency claims lack detailed timing comparisons and analysis of emulator computational overhead
- Limited exploration of failure conditions when teacher hidden states are insufficient for reasoning

## Confidence
- High confidence: Basic mechanism of distilling teacher hidden states through emulator to enable implicit reasoning is technically sound and well-supported
- Medium confidence: Claims about handling multiple reasoning pathways through mixture models are supported but extent of necessity versus benefit not fully established
- Low confidence: Claim about enabling solving tasks "previously not solvable without explicit chain-of-thought" difficult to verify without broader benchmarking

## Next Checks
1. **Ablation on mixture model necessity**: Train system with and without mixture modeling on GSM8K to quantify exact contribution of handling multiple reasoning pathways. Measure both accuracy and mode collapse metrics during training.

2. **Scaling experiment**: Apply implicit CoT method to larger model (e.g., GPT-2 Large or GPT-Neo) and test whether accuracy improvements scale proportionally. This would validate whether method's benefits are model-size dependent.

3. **Generalization to other reasoning tasks**: Test method on different type of reasoning task, such as logical deduction or commonsense reasoning benchmarks (e.g., StrategyQA or CommonsenseQA). This would assess whether hidden state distillation approach generalizes beyond arithmetic reasoning.