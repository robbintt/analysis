---
ver: rpa2
title: Adaptive Weighted Co-Learning for Cross-Domain Few-Shot Learning
arxiv_id: '2312.03928'
source_url: https://arxiv.org/abs/2312.03928
tags:
- learning
- instances
- each
- few-shot
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Adaptive Weighted Co-Learning (AWCoL), a method
  for cross-domain few-shot learning (CDFSL) that addresses the challenges of limited
  labeled instances and domain shift. The method fine-tunes two prototypical classification
  models pre-trained on a source domain dataset using an adaptive weighted co-learning
  procedure in the target domain.
---

# Adaptive Weighted Co-Learning for Cross-Domain Few-Shot Learning

## Quick Facts
- arXiv ID: 2312.03928
- Source URL: https://arxiv.org/abs/2312.03928
- Authors: 
- Reference count: 32
- Primary result: Proposes AWCoL method achieving state-of-the-art CDFSL performance across eight benchmark datasets

## Executive Summary
This paper introduces Adaptive Weighted Co-Learning (AWCoL), a method for cross-domain few-shot learning that addresses the challenges of limited labeled instances and domain shift. The approach fine-tunes two prototypical classification models pre-trained on a source domain using an adaptive weighted co-learning procedure in the target domain. By employing weighted moving average predictions, alternating fine-tuning, and negative pseudo-label regularization, AWCoL achieves state-of-the-art performance on eight benchmark datasets.

## Method Summary
AWCoL employs two prototypical classification models pre-trained on a source domain dataset (Mini-ImageNet). For each target task, the models are fine-tuned using an adaptive weighted co-learning procedure. The method uses weighted moving average (WMA) to generate robust probabilistic predictions for query instances, then jointly fine-tunes the two models based on pseudo-labels and instance weights produced from these predictions. A negative pseudo-labeling regularizer is applied to penalize false predictions. The fine-tuning process uses alternating updates between the two models for stability, with weighted cross-entropy loss and Adam optimization.

## Key Results
- AWCoL achieves state-of-the-art CDFSL performance across eight benchmark datasets
- Outperforms existing methods on most datasets in 5-way 5-shot, 5-way 20-shot, and 5-way 50-shot tasks
- Demonstrates effectiveness of adaptive weighted co-learning strategy for domain adaptation

## Why This Works (Mechanism)

### Mechanism 1
Weighted Moving Average (WMA) stabilizes pseudo-label generation across fine-tuning iterations. WMA maintains a running average of prediction probabilities using a decay factor α, updated each iteration. The annealing schedule reduces α over time to allow larger early updates and smaller later updates for convergence. Core assumption: Smooth transitions in predictions prevent oscillations that could destabilize co-learning.

### Mechanism 2
Alternating fine-tuning between two models prevents co-adaptation instability. Models are updated in alternating blocks of β iterations. During each block, one model is updated while the other's WMA predictions are held fixed, producing a more stable co-prediction signal. Core assumption: Simultaneous updates could lead to feedback loops that destabilize learning.

### Mechanism 3
Negative pseudo-label regularization improves robustness to false positives. For each query, a negative pseudo-label is randomly chosen from classes other than the predicted one, and the model is encouraged to maximize the loss for this label. Core assumption: Even if the positive pseudo-label is wrong, the randomly chosen negative pseudo-label has a chance of being a true negative, providing a learning signal.

## Foundational Learning

- **Concept: Prototypical networks for few-shot learning**
  - Why needed here: AWCoL builds on prototypical classification, using class prototypes computed from support instances to generate predictions
  - Quick check question: How is a prototype for a class computed in a prototypical network?

- **Concept: Cross-domain few-shot learning challenges**
  - Why needed here: The paper explicitly addresses domain shift between source and target, limited labeled instances, and adaptation difficulty
  - Quick check question: What are the two main challenges in CDFSL that make it harder than in-domain FSL?

- **Concept: Weighted moving average in optimization**
  - Why needed here: WMA is used to stabilize prediction probabilities across iterations, preventing oscillations
  - Quick check question: How does a decay factor α control the contribution of new versus old predictions in WMA?

## Architecture Onboarding

- **Component map**: Two prototypical models (M1, M2) -> WMA modules -> Co-prediction module -> Pseudo-label generator -> Adaptive weight calculator -> Alternating fine-tuning loop
- **Critical path**: 1) Pre-train M1 and M2 on source, 2) For each target task, initialize WMA vectors, 3) Loop: generate WMA predictions → co-prediction → pseudo-labels + weights → alternate fine-tune M1/M2 → repeat
- **Design tradeoffs**: Alternating vs. simultaneous updates (alternating is more stable but slower), WMA vs. direct predictions (WMA is more stable but introduces hyperparameters), negative pseudo-label regularization (adds robustness but may introduce noise)
- **Failure signatures**: Degraded performance on certain datasets suggests overfitting or poor adaptation to some domain shifts
- **First 3 experiments**: 1) Verify alternating update logic with β=1 vs simultaneous, 2) Test WMA impact by disabling WMA (α=1), 3) Evaluate negative pseudo-label effect by removing LN term

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AWCoL scale with the number of pre-trained models in the source domain? Would using more than two models lead to further improvements in CDFSL performance?
- Basis in paper: The paper describes AWCoL using two pre-trained models, but does not explore the impact of using more models
- Why unresolved: The paper only evaluates AWCoL with two models
- What evidence would resolve it: Experiments comparing AWCoL with varying numbers of pre-trained models on the same CDFSL benchmark datasets

### Open Question 2
- Question: How sensitive is AWCoL to the choice of distance function in the prototypical classification model? Would alternative distance functions improve performance?
- Basis in paper: The paper states that the squared L2 norm is used as the distance function, but does not explore other options
- Why unresolved: The paper does not investigate the impact of different distance functions on AWCoL's performance
- What evidence would resolve it: Experiments comparing AWCoL using different distance functions on the same CDFSL benchmark datasets

### Open Question 3
- Question: How does the performance of AWCoL change when applied to cross-domain few-shot learning tasks with larger domain shifts?
- Basis in paper: The paper evaluates AWCoL on eight benchmark datasets, but does not explicitly test the method's robustness to varying degrees of domain shift
- Why unresolved: The paper does not systematically vary the degree of domain shift between source and target domains
- What evidence would resolve it: Experiments comparing AWCoL on CDFSL tasks with varying degrees of domain shift

## Limitations

- Effectiveness depends heavily on several hyper-parameters that are tuned for specific datasets but may not generalize well to new domains
- Alternating update mechanism, while stabilizing, may limit the speed of convergence compared to simultaneous updates
- Method requires pre-training two independent models, doubling computational requirements

## Confidence

- **High Confidence**: The core methodology of using weighted moving average for prediction stabilization and alternating fine-tuning for co-learning is well-grounded and supported by experimental results
- **Medium Confidence**: The effectiveness of negative pseudo-label regularization and the specific values of hyper-parameters used for optimal performance, as these may vary across different domain shifts
- **Low Confidence**: The claim that the method "significantly outperforms state-of-the-art" across all datasets, as performance gains vary considerably between datasets

## Next Checks

1. **Ablation Study on Hyper-parameters**: Conduct a comprehensive ablation study varying αmin, α0, γ, β, and λ across different target domains to understand their impact on performance

2. **Simultaneous vs. Alternating Updates**: Compare the performance of AWCoL with simultaneous updates of both models to quantify the trade-off between stability and convergence speed

3. **Negative Pseudo-label Effectiveness Analysis**: Systematically analyze the quality of negative pseudo-labels by measuring the frequency of true negatives versus false negatives, and test alternative strategies for selecting negative pseudo-labels