---
ver: rpa2
title: Seq2seq for Automatic Paraphasia Detection in Aphasic Speech
arxiv_id: '2312.10518'
source_url: https://arxiv.org/abs/2312.10518
tags:
- paraphasia
- speech
- detection
- aphasia
- seq2seq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses automatic paraphasia detection in aphasic
  speech, a task traditionally done manually by clinicians. The authors propose a
  novel end-to-end sequence-to-sequence (seq2seq) model that jointly performs automatic
  speech recognition (ASR) and paraphasia detection.
---

# Seq2seq for Automatic Paraphasia Detection in Aphasic Speech

## Quick Facts
- arXiv ID: 2312.10518
- Source URL: https://arxiv.org/abs/2312.10518
- Reference count: 40
- Primary result: Multitask seq2seq model with pretrained WavLM achieves AWERs of 45.0, 30.4, and 48.4 for phonemic, neologistic, and combined paraphasias, with F1-scores of 0.643, 0.688, and 0.706.

## Executive Summary
This paper introduces a novel end-to-end sequence-to-sequence (seq2seq) model for joint automatic speech recognition (ASR) and paraphasia detection in aphasic speech. The authors evaluate various ASR architectures and demonstrate that using a pretrained WavLM model with a seq2seq architecture yields the best performance. They compare single-task and multitask learning objectives, finding that the multitask approach significantly outperforms the previous state-of-the-art. The study also investigates the impact of tokenizer size on model performance, providing valuable insights for future research in this domain.

## Method Summary
The proposed method involves training a seq2seq model on the AphasiaBank dataset for joint ASR and paraphasia detection. The model uses a pretrained WavLM encoder followed by a 6-layer decoder, with both CTC and attention-based losses. The authors evaluate several ASR architectures (Hybrid HMM-BLSTM, Wav2Vec2, HuBERT, WavLM, Whisper) and compare single-task learning (STL) and multitask learning (MTL) objectives for paraphasia detection. They also explore the impact of tokenizer size on model performance.

## Key Results
- Pretrained WavLM with seq2seq architecture yields best ASR performance
- Multitask learning outperforms single-task learning for paraphasia detection
- AWERs: 45.0 (phonemic), 30.4 (neologistic), 48.4 (combined)
- F1-scores: 0.643 (phonemic), 0.688 (neologistic), 0.706 (combined)
- Tokenizer size of 500 generally yields best performance

## Why This Works (Mechanism)

### Mechanism 1
Pretraining on large-scale speech corpora (WavLM, HuBERT, Wav2Vec2) significantly improves aphasic speech recognition compared to traditional HMM-DNN models. Pretrained SSL models capture rich acoustic and contextual representations from diverse speech data, which transfer well to disordered speech even with limited aphasic data.

### Mechanism 2
Multitask learning (MTL) of ASR and paraphasia detection in a single seq2seq model outperforms single-task learning (STL) for both word-level and utterance-level paraphasia detection. Joint optimization allows the model to learn shared representations that benefit both tasks, avoiding degradation in ASR performance that occurs in STL when switching objectives.

### Mechanism 3
Tokenizer size critically impacts paraphasia detection performance in seq2seq models; too small a vocabulary causes subword label overlap, too large causes label sparsity. Appropriate vocabulary granularity ensures paraphasic words are tokenized into a manageable number of subword units, preserving label clarity for classification.

## Foundational Learning

- **Connectionist Temporal Classification (CTC) loss**
  - Why needed: Enables alignment-free training of encoder-only ASR models by summing probabilities over all possible alignments
  - Quick check: How does CTC handle variable-length speech without explicit alignments?

- **Transformer-based seq2seq architecture**
  - Why needed: Captures long-range dependencies and context in speech, critical for disambiguating paraphasias
  - Quick check: Why is an encoder-decoder architecture preferred over encoder-only for joint ASR and paraphasia detection?

- **Subword tokenization (SentencePiece, unigram)**
  - Why needed: Breaks words into manageable units for seq2seq models, especially important for rare paraphasic words
  - Quick check: How does tokenizer size affect label assignment for paraphasias during training and inference?

## Architecture Onboarding

- **Component map**: WavLM/HuBERT/Wav2Vec2 pretrained encoder → seq2vec encoder (N layers) → decoder (6 layers) → CTC layer + attention-based decoder → paraphrase classifier
- **Critical path**: Pretrained encoder → seq2vec encoder → decoder → paraphasia label aggregation → evaluation metrics (AWER, TD, TTR)
- **Design tradeoffs**: Pretrained encoder choice vs ASR performance vs paraphasia detection balance; Tokenizer size vs subword granularity vs label sparsity; MTL vs STL objective vs ASR stability vs paraphasia detection granularity
- **Failure signatures**: High AWER but low TD/TTR → misalignment or poor ASR; Low F1 but high TD/TTR → false positives/negatives in paraphasia classification; Large gap between STL and MTL → ASR degradation when switching tasks
- **First 3 experiments**:
  1. Compare MTL vs STL objectives using the same pretrained encoder to isolate learning objective effects
  2. Sweep tokenizer sizes (100, 500, 1000, 2000) to find optimal subword granularity
  3. Ablate pretrained encoder (random init) vs fine-tuned WavLM to measure pretraining impact

## Open Questions the Paper Calls Out
1. How does the seq2seq model's paraphasia detection performance change when trained on larger datasets of aphasic speech?
2. Can the seq2seq model be adapted to detect paraphasias in real-time, and what are the computational constraints for such an application?
3. How does the seq2seq model perform on paraphasias that are not present in the training data, such as those specific to other language disorders or non-aphasic speech errors?

## Limitations
- Data representation limited by small AphasiaBank dataset size
- Technical scope restricted to English language paraphasia detection
- Model generalization unclear due to limited comparison between ASR architectures

## Confidence
- **High Confidence**: Multitask learning outperforms single-task learning for joint ASR and paraphasia detection
- **Medium Confidence**: Pretrained SSL models are critical for low-resource domains like aphasia
- **Low Confidence**: Optimal tokenizer size (500) recommendation based on limited experimental sweeps

## Next Checks
1. Cross-Dataset Validation: Evaluate the proposed model on additional aphasic speech datasets to assess generalizability beyond AphasiaBank
2. Component Ablation Study: Systematically ablate each model component to quantify their individual contributions to paraphasia detection performance
3. Clinical Relevance Assessment: Collaborate with speech-language pathologists to validate model outputs against clinical ground truth and assess practical utility in diagnostic workflows