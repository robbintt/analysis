---
ver: rpa2
title: 'GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?'
arxiv_id: '2310.13833'
source_url: https://arxiv.org/abs/2310.13833
tags:
- graph
- node
- graphs
- attributes
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GraphMaker, a novel diffusion model designed
  to generate large attributed graphs with complex node attributes and structures.
  The authors address the challenge of scalability and capturing intricate attribute-structure
  correlations by employing an asynchronous generation process and node-level conditioning.
---

# GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?

## Quick Facts
- arXiv ID: 2310.13833
- Source URL: https://arxiv.org/abs/2310.13833
- Authors: 
- Reference count: 15
- Primary result: Novel diffusion model for large attributed graphs using asynchronous generation and node-level conditioning

## Executive Summary
GraphMaker introduces a scalable diffusion model for generating large attributed graphs with complex node attributes and structures. The method addresses key challenges in scalability and capturing intricate attribute-structure correlations through asynchronous generation and node-level conditioning. The authors propose a novel evaluation pipeline using models trained on generated synthetic graphs and tested on original graphs to assess synthetic data quality. GraphMaker demonstrates superior performance in generating realistic and diverse large-attributed graphs compared to existing methods, with applications in downstream graph machine learning tasks.

## Method Summary
GraphMaker employs a diffusion probabilistic model extended to discrete state spaces for attributed graph generation. The model uses an asynchronous generation process where node attributes and graph structure are generated separately to better capture their correlations. Node labels are sampled from their empirical distribution and used as explicit conditioning information. The encoder uses a message-passing neural network for scalability, while edge prediction is performed in mini-batches to handle large graphs. The method includes two variants: GraphMaker-Sync (synchronous generation) and GraphMaker-Async (asynchronous generation).

## Key Results
- GraphMaker-Async consistently outperforms GraphMaker-Sync in capturing attribute-structure correlations
- The method achieves superior performance in generating realistic graph statistics (degree distribution, clustering coefficient, etc.)
- GraphMaker-generated graphs improve downstream task performance when used for training discriminative models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling attribute and structure generation (GraphMaker-Async) better captures attribute-structure correlations than synchronous generation (GraphMaker-Sync).
- **Mechanism:** The diffusion process is split into two sequential phases: first generate node attributes given node labels, then generate edges conditioned on both attributes and labels. This allows the model to focus on one correlation at a time rather than mixing them during each denoising step.
- **Core assumption:** The joint distribution of attributes and structure can be factorized into sequential conditional steps without losing information.
- **Evidence anchors:**
  - [section]: "We suspect that this problem stems from synchronous refinement of node attributes and graph structure, hence propose to denoise node attributes and graph structure asynchronously instead."
  - [section]: "GraphMaker-Async indeed addresses the problem mentioned in Section 3.2, consistently surpassing GraphMaker-Sync for clustering coefficient distribution and triangle count."
  - [corpus]: Weak. No direct comparisons in related work to asynchronous vs synchronous attribute-structure generation.
- **Break condition:** If the attribute-structure correlation is highly non-factorizable, the asynchronous approach could lose important joint patterns.

### Mechanism 2
- **Claim:** Using node labels as explicit conditioning improves the quality of generated graphs compared to treating labels as just another attribute.
- **Mechanism:** Node labels are sampled from their empirical distribution and passed as conditioning information to the denoising network. This guides the generation process to produce attribute and structure patterns consistent with each label class.
- **Core assumption:** The conditional distribution of attributes and edges given the label is significantly different from the marginal distribution, and this difference is important for realistic graph generation.
- **Evidence anchors:**
  - [section]: "We propose to generate node labels with their empirical distribution, and leverage node labels as conditions to learn a conditional diffusion model."
  - [section]: "For more than 80% cases, conditional generation yields a better performance."
  - [corpus]: Weak. Related works do not explicitly condition on node labels in diffusion models for attributed graph generation.
- **Break condition:** If labels are not predictive of attribute-structure patterns, conditioning on them could introduce noise rather than signal.

### Mechanism 3
- **Claim:** Mini-batch edge prediction enables scalable generation of large graphs by avoiding full O(N²) enumeration.
- **Mechanism:** During training and generation, only a random subset of node pairs are processed per iteration. This reduces memory and computation while still allowing the model to learn edge probabilities.
- **Core assumption:** Random sampling of node pairs is sufficient to approximate the full edge distribution without introducing significant bias.
- **Evidence anchors:**
  - [section]: "Due to limited GPU memory, it is intractable to perform edge prediction for all N² node pairs at once. During training, we randomly choose a subset of the node pairs for a gradient update."
  - [section]: "For graph generation, we perform edge prediction over minibatches of node pairs."
  - [corpus]: Weak. Related works do not discuss mini-batching strategies for large graph generation in diffusion models.
- **Break condition:** If the graph has strong local structure, random sampling might miss important patterns, leading to poor edge predictions.

## Foundational Learning

- **Concept:** Diffusion probabilistic models and score-based generative modeling
  - **Why needed here:** The paper builds on D3PM (Austin et al., 2021) and extends it to discrete state spaces for attributed graphs.
  - **Quick check question:** How does the forward diffusion process corrupt graph structure and attributes, and what is the role of the transition matrices?

- **Concept:** Message passing neural networks (MPNNs) and their scalability
  - **Why needed here:** The encoder uses an MPNN with O(|E|) complexity instead of O(N²) graph transformers for scalability.
  - **Quick check question:** What are the trade-offs between MPNNs and graph transformers for encoding large attributed graphs?

- **Concept:** Conditional generation and conditioning on node labels
  - **Why needed here:** Node labels are used as conditioning information to guide generation of attributes and structure.
  - **Quick check question:** How does conditioning on labels differ from treating them as regular node attributes in terms of model architecture and generation quality?

## Architecture Onboarding

- **Component map:** Forward diffusion process -> Reverse diffusion process (Sync/Async) -> MPNN encoder -> MLP/MLP classifier decoder -> Evaluation pipeline
- **Critical path:**
  1. Preprocess graph data (one-hot encode attributes, labels).
  2. Sample node labels from empirical distribution.
  3. Forward diffusion to corrupt attributes and edges.
  4. Train denoising network with mini-batch edge prediction.
  5. Generate new graphs by sampling labels and running reverse diffusion.
  6. Evaluate generated graphs using discriminative models.
- **Design tradeoffs:**
  - Sync vs Async generation: Sync is simpler but Async better captures attribute-structure correlations.
  - MPNN vs Graph Transformer: MPNN is more scalable but may miss long-range dependencies.
  - Mini-batch edge prediction: Reduces memory but may introduce sampling bias.
- **Failure signatures:**
  - Poor edge prediction: Could indicate insufficient mini-batch size or ineffective conditioning.
  - Unrealistic attribute distributions: Could suggest issues with attribute corruption schedule or encoder architecture.
  - Degenerate label distribution: Could mean problems with label sampling or conditioning.
- **First 3 experiments:**
  1. Train GraphMaker-Sync on Cora dataset and evaluate attribute and structure statistics.
  2. Train GraphMaker-Async on Cora and compare to GraphMaker-Sync on the same metrics.
  3. Evaluate both variants using the discriminative model protocol (train on generated, test on original).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can GraphMaker be extended to generate graphs with continuous-valued node attributes instead of categorical ones?
- **Basis in paper:** [explicit] The authors mention that GraphMaker is currently limited to generating graphs without edge attributes and propose extending it to handle continuous-valued node attributes as a future work.
- **Why unresolved:** The paper focuses on categorical node attributes, and the extension to continuous-valued attributes is not explored. This extension could significantly broaden the applicability of GraphMaker to real-world scenarios where continuous attributes are more common.
- **What evidence would resolve it:** Experimental results demonstrating the successful generation of large attributed graphs with continuous-valued node attributes, including comparisons with existing methods and evaluations on downstream tasks.

### Open Question 2
- **Question:** How can GraphMaker be modified to generate graphs with node labels that indicate anomalies for anomaly detection tasks?
- **Basis in paper:** [explicit] The authors suggest extending GraphMaker to generate graphs with node labels that indicate anomalies as a future direction. This could be valuable for anomaly detection tasks in various domains.
- **Why unresolved:** The paper does not explore the generation of graphs with anomaly labels, and the extension to handle such labels is not discussed. This extension could open up new applications for GraphMaker in anomaly detection and related fields.
- **What evidence would resolve it:** Experimental results showing the successful generation of large attributed graphs with node labels indicating anomalies, along with evaluations on anomaly detection tasks and comparisons with existing methods.

### Open Question 3
- **Question:** How can GraphMaker be scaled to generate even larger graphs with millions of nodes?
- **Basis in paper:** [explicit] The authors mention that GraphMaker can currently handle graphs with up to 13K nodes and suggest extending it to generate even larger graphs with 1M+ nodes as a future work.
- **Why unresolved:** The paper does not explore the scaling of GraphMaker to handle graphs with millions of nodes, and the challenges and solutions for such scaling are not discussed. This extension could significantly increase the applicability of GraphMaker to large-scale real-world networks.
- **What evidence would resolve it:** Experimental results demonstrating the successful generation of large attributed graphs with millions of nodes, including evaluations on scalability, performance, and comparisons with existing methods.

## Limitations
- Evaluation protocol may overstate practical utility by training on synthetic data and testing on real data
- Limited qualitative analysis of generated graphs beyond statistical metrics
- Scalability claims primarily demonstrated on graphs up to 13K nodes, not millions
- Computational cost comparison with other methods not thoroughly explored

## Confidence

- **High Confidence:** The core claim that asynchronous generation outperforms synchronous generation for capturing attribute-structure correlations is well-supported by experimental evidence across multiple datasets and metrics.
- **Medium Confidence:** The claim that GraphMaker produces more diverse and useful synthetic data compared to existing methods is supported but relies heavily on the discriminative-model evaluation protocol, which has known limitations.
- **Medium Confidence:** The scalability claims are reasonable given the mini-batch edge prediction approach, but lack comprehensive benchmarks against other scalable graph generation methods.

## Next Checks
1. **Qualitative Analysis:** Generate visual representations of graphs from GraphMaker, competing methods, and real data to qualitatively assess structural realism beyond statistical metrics.

2. **Cross-Domain Utility:** Test whether models trained on synthetic graphs generated by GraphMaker can transfer to real downstream tasks (e.g., training on synthetic Cora and testing on real Citeseer) to validate practical utility.

3. **Scalability Benchmark:** Evaluate GraphMaker on larger graph datasets (e.g., OGB-LSC datasets) and compare runtime and memory usage against other scalable graph generation methods to substantiate scalability claims.