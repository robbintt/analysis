---
ver: rpa2
title: "Building the Bridge of Schr\xF6dinger: A Continuous Entropic Optimal Transport\
  \ Benchmark"
arxiv_id: '2306.10161'
source_url: https://arxiv.org/abs/2306.10161
tags:
- benchmark
- solvers
- pairs
- optimal
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We propose a novel methodology for constructing pairs of probability\
  \ distributions for which the ground truth optimal transport (OT) solution is known\
  \ by construction. Our approach is generic and works for a wide range of OT formulations,\
  \ including entropic OT (EOT) and its equivalent Schr\xF6dinger Bridge (SB) formulation."
---

# Building the Bridge of Schrödinger: A Continuous Entropic Optimal Transport Benchmark

## Quick Facts
- arXiv ID: 2306.10161
- Source URL: https://arxiv.org/abs/2306.10161
- Reference count: 40
- We propose a novel methodology for constructing pairs of probability distributions for which the ground truth optimal transport (OT) solution is known by construction.

## Executive Summary
This paper introduces a novel methodology for constructing benchmark pairs of probability distributions with analytically known optimal transport solutions, specifically targeting entropic optimal transport (EOT) and its equivalent Schrödinger Bridge (SB) formulation. The key innovation uses log-sum-exp of quadratic functions to create benchmark pairs where the ground truth EOT/SB solution is known by construction. This allows for rigorous evaluation of neural EOT/SB solvers on both toy examples and high-dimensional problems like 64×64 celebrity face images.

## Method Summary
The methodology constructs benchmark pairs by defining a base distribution P0 and using log-sum-exp potentials of quadratic functions to generate P1, ensuring the EOT plan π* is analytically known. For toy problems, P0 is a Gaussian mixture, while for images, P0 is approximated using a normalizing flow trained on CelebA-HQ. The construction guarantees that conditional distributions π*(y|x) are Gaussian mixtures, enabling efficient sampling without MCMC. The benchmark is evaluated using metrics like cBW2-UVP (conditional Bures-Wasserstein) and FID, revealing that existing neural solvers struggle particularly in high dimensions and with small entropic regularization parameters.

## Key Results
- Neural EOT/SB solvers show poor accuracy in recovering true solutions, especially in high dimensions (64-128D) and with small entropic regularization (ε=0.1)
- Performance degrades significantly on image benchmarks (64×64 celebrity faces) compared to low-dimensional mixtures
- Existing solvers exhibit mode collapse, failing to capture all modes of the target distribution in high-dimensional spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Log-sum-exp of quadratic functions (LSE potentials) allow efficient sampling from conditional EOT plans without MCMC.
- Mechanism: The LSE potential is constructed such that when combined with quadratic cost, the conditional distribution π*(y|x) becomes a Gaussian mixture. Each component has analytically known mean and covariance, enabling direct sampling.
- Core assumption: The LSE potential must be a weighted sum of exponentials of negative quadratic forms, with weights non-negative and quadratic terms having specific matrix properties.
- Evidence anchors:
  - [section] "Proposition 3.3 (Entropic OT solution for LSE potentials)... dπ*(y|x) = ΣγnN(y|µn(x), Σn)"
  - [abstract] "We use log-sum-exp of quadratic functions to construct benchmark pairs with analytically known EOT/SB solutions"

### Mechanism 2
- Claim: The constructed benchmark pairs allow direct evaluation of neural EOT/SB solvers by comparing their learned plans against ground truth.
- Mechanism: By constructing P1 from P0 using known LSE potential, the optimal EOT plan π* is analytically known by construction. Solvers can then be evaluated by comparing their learned plan bπ to π* using metrics like cBW2-UVP.
- Core assumption: The solver's learned conditional plan bπ(·|x) approximates the true conditional plan π*(·|x) for the same P0 and P1.
- Evidence anchors:
  - [abstract] "Our approach is generic and works for a wide range of OT formulations, in particular, it covers the EOT which is equivalent to SB"
  - [section] "Theorem 3.2... Consider the joint distribution π*... Then if P1 = π*1 belongs to Pp(Y), the distribution π* is an EOT plan for P0, P1"

### Mechanism 3
- Claim: The benchmark construction generalizes to high-dimensional spaces like 64×64 celebrity faces.
- Mechanism: By using a normalizing flow to approximate P0 (celebrity faces) and constructing P1 via LSE potential, the methodology extends from low-dimensional toy examples to realistic high-dimensional distributions.
- Core assumption: The normalizing flow provides a tractable approximation to the true data distribution with known density.
- Evidence anchors:
  - [section] "As distribution P0, we use the approximation of the distribution of 64 × 64 RGB images... Namely, we train a normalizing flow with Glow architecture"
  - [section] "By the construction of distribution P1, obtaining the conditional EOT plan π*(y|x)... may be viewed as learning the noising model"

## Foundational Learning

- Concept: Entropic Optimal Transport (EOT)
  - Why needed here: The benchmark is specifically designed for EOT and its equivalent Schrödinger Bridge formulation, which are the main focus of the paper.
  - Quick check question: What is the key difference between classic OT and EOT, and how does entropy regularization affect the optimal plan?

- Concept: Schrödinger Bridge (SB) Problem
  - Why needed here: SB is equivalent to EOT and provides the theoretical foundation for constructing the benchmark pairs with known solutions.
  - Quick check question: How does the Schrödinger Bridge formulation relate to diffusion processes, and what is the role of the optimal drift function?

- Concept: Log-Sum-Exp (LSE) Potentials
  - Why needed here: LSE potentials are the key mathematical tool that enables constructing benchmark pairs with tractable solutions.
  - Quick check question: Why do LSE potentials of quadratic functions lead to Gaussian mixture conditional distributions in the EOT framework?

## Architecture Onboarding

- Component map: Distribution Generator -> Solver Interface -> Conditional Sampling -> Metric Computation -> Result Aggregation
- Critical path: P0,P1 generation → Solver execution → Conditional sampling → Metric computation → Result aggregation
- Design tradeoffs: Accuracy vs. computational efficiency in sampling (direct Gaussian mixture sampling vs. MCMC), simplicity vs. expressiveness in LSE potential construction
- Failure signatures: Solver divergence on small ε values, numerical instability in high dimensions, poor performance on image benchmarks despite good toy results
- First 3 experiments:
  1. Verify Gaussian mixture structure of π*(y|x) for simple 2D case with known parameters
  2. Test solver performance on mixtures benchmark with varying ε and dimensions
  3. Validate image benchmark construction by checking FID between P0 and learned π1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LSE quadratic functions as optimal Kantorovich potentials affect the representativeness of the benchmark for practical EOT/SB scenarios?
- Basis in paper: [explicit] "We employ LSE quadratic functions (15) as optimal Kantorovich potentials to construct benchmark pairs. It is unclear whether our benchmark sufficiently reflects the practical scenarios in which the EOT/SB solvers are used."
- Why unresolved: The paper acknowledges that the representativeness of the benchmark for practical scenarios is unknown, but does not provide empirical evidence or theoretical arguments to support its representativeness.
- What evidence would resolve it: Testing the benchmark with a wider range of practical EOT/SB scenarios and comparing the results with those obtained using other benchmarks or real-world data.

### Open Question 2
- Question: What are the limitations of using BW2_2-UVP as the primary metric for evaluating the accuracy of recovered EOT/SB solutions?
- Basis in paper: [explicit] "We employ BW2_2-UVP for the quantitative evaluation (M5) as it is popular in OT field [31, 12, 16, 21, 39] . However, it may not capture the full picture as it only compares the 1st and 2nd moments of distributions."
- Why unresolved: The paper identifies the limitation of BW2_2-UVP but does not explore alternative metrics or provide a comprehensive analysis of the trade-offs between different evaluation metrics.
- What evidence would resolve it: Developing and testing new evaluation metrics that capture more aspects of the EOT/SB solution, such as higher-order moments or the entire distribution, and comparing their performance with BW2_2-UVP.

### Open Question 3
- Question: How can the Langevin sampling in the latent space of the normalizing flow be further improved to reduce the time and computational resources required for generating test datasets?
- Basis in paper: [inferred] "To overcome this issue, we employ the Langevin sampling in the latent space of the normalizing flow... We empirically found this approach works much better, presumably due to the fact that (30) is just the score of the Normal distribution which is slightly adjusted with the information coming from π*(y|G(z))."
- Why unresolved: The paper suggests that Langevin sampling in the latent space is more efficient than in the data space, but does not provide a detailed analysis of the factors affecting its performance or potential improvements.
- What evidence would resolve it: Investigating the impact of different hyperparameters, such as the number of steps, step size, and initialization, on the efficiency and accuracy of the Langevin sampling in the latent space, and comparing the results with other sampling methods.

## Limitations

- Numerical instability for small entropic regularization parameters (ε=0.1) due to exponential terms in the log-sum-exp potential
- Poor mode coverage in learned plans, especially for high-dimensional cases (D=64,128), indicating solver mode collapse
- Evaluation metrics like BW2-UVP only compare first and second moments, potentially missing other aspects of solution quality

## Confidence

- High confidence: The theoretical foundation linking LSE potentials to Gaussian mixture conditional distributions (Proposition 3.3)
- Medium confidence: The scalability claims to high-dimensional image spaces, given limited empirical validation
- Low confidence: The generalizability to other cost functions beyond quadratic, as the current construction is specifically tailored to ||x-y||²/2

## Next Checks

1. **Numerical Stability Verification**: Test the LSE potential construction with increasingly large dimensions (D=256, 512) and extreme parameter values to identify breaking points and stability thresholds.

2. **Metric Sensitivity Analysis**: Compare solver rankings across different evaluation metrics (cBW2-UVP, FID, KL divergences) to identify which metrics best capture true solver performance differences.

3. **Cross-Architecture Comparison**: Implement the benchmark with a different class of EOT solvers (e.g., flow-based methods) to validate that results are not specific to the neural solver architectures tested.