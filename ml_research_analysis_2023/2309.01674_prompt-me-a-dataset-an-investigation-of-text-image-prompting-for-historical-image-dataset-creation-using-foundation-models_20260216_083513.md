---
ver: rpa2
title: 'Prompt me a Dataset: An investigation of text-image prompting for historical
  image dataset creation using foundation models'
arxiv_id: '2309.01674'
source_url: https://arxiv.org/abs/2309.01674
tags:
- historical
- dataset
- datasets
- data
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pipeline for extracting visual elements from
  historical documents using foundation models, specifically GroundingDINO and Meta's
  Segment-Anything-Model (SAM). The authors propose a sequential approach that relies
  on text-image prompts to retrieve visual data from historical documents, which can
  be used for downstream development tasks and dataset creation.
---

# Prompt me a Dataset: An investigation of text-image prompting for historical image dataset creation using foundation models

## Quick Facts
- arXiv ID: 2309.01674
- Source URL: https://arxiv.org/abs/2309.01674
- Authors: 
- Reference count: 30
- Key outcome: Sequential approach using GroundingDINO and SAM achieves AP scores of 0.42-0.82 on historical document datasets

## Executive Summary
This paper presents a pipeline for extracting visual elements from historical documents using foundation models, specifically GroundingDINO and Meta's Segment-Anything-Model (SAM). The authors propose a sequential approach that relies on text-image prompts to retrieve visual data from historical documents, which can be used for downstream development tasks and dataset creation. The pipeline is evaluated on three datasets of varying complexity: S-VED, Chapbook, and HORAE. The results show that the proposed approach achieves promising results, with Average Precision (AP) scores ranging from 0.42 to 0.82, depending on the dataset and prompt used. The authors also discuss the limitations of the current approach and propose future work, including fine-tuning GroundingDINO on humanities-specific datasets and building a user-friendly application for non-experts.

## Method Summary
The proposed pipeline combines GroundingDINO for text-image matching with SAM for segmentation, using engineered prompts to guide detection. Images are preprocessed to 1000x1000px with autocontrast, then processed through GroundingDINO with domain-specific prompts. Non-maximal suppression removes duplicate detections, and the cleaned bounding boxes are passed to SAM to generate segmentation masks. The approach is evaluated on three historical document datasets with varying complexity.

## Key Results
- AP scores range from 0.42 to 0.82 across three historical document datasets
- Domain-specific prompts improve detection performance compared to generic prompts
- Single inference takes approximately 40 times longer than YOLOv8, highlighting computational efficiency concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GroundingDINO + SAM pipeline enables zero-shot object detection on historical documents without domain-specific training data.
- Mechanism: GroundingDINO performs text-image matching to locate relevant image regions via bounding boxes, which SAM then refines into precise segmentation masks. This two-stage approach leverages large-scale pre-training to generalize across domains.
- Core assumption: Pre-trained models have sufficient visual-linguistic understanding to semantically match historical visual elements with natural language prompts.
- Evidence anchors:
  - [abstract]: "propose a sequential approach that relies on GroundDINO and Meta's Segment-Anything-Model (SAM) to retrieve a significant portion of visual data from historical documents"
  - [section 3]: "GroundingDINO takes an image-text input pair, and returns a bounding box that corresponds to the image region that in turns semantically corresponds to its textual counterpart. These bounding boxes are then passed on as data prompts to SAM"
  - [corpus]: Weak - related papers focus on modern documents or different tasks, limited direct evidence for historical document performance
- Break condition: Performance degrades when visual elements have low semantic similarity to training data or when prompt interpretation is ambiguous.

### Mechanism 2
- Claim: Prompt engineering significantly improves detection performance on heterogeneous historical datasets.
- Mechanism: Carefully crafted domain-aware prompts inject historical context into the text-image matching process, guiding GroundingDINO to focus on relevant visual elements rather than false positives.
- Core assumption: The linguistic-semantic space of pre-trained models overlaps sufficiently with historical visual element terminology to enable meaningful prompt-based guidance.
- Evidence anchors:
  - [section 4]: "To inject more domain-knowledge into our prompts, we provide dataset-tailored prompts... In the Chapbook dataset, we focus on identifying the complete visual element, which is often square or rectangular in shape, thus the prompt in this case is{image - square - rectangle - photo}"
  - [section 4]: "The suggested prompt of{figure} has led the GroundingDINO module to return bounding boxes of human figures within the visual elements, instead of the desired output of a figure in the literal sense"
  - [corpus]: Weak - related work focuses on automated prompt generation but not specifically for historical document analysis
- Break condition: When prompt vocabulary fails to capture the semantic space of historical visual elements or when multiple interpretations exist.

### Mechanism 3
- Claim: Sequential architecture balances detection accuracy with computational efficiency for dataset creation.
- Mechanism: Non-maximal suppression removes duplicate detections from multiple prompts, while SAM refines bounding boxes into accurate masks, reducing the need for computationally expensive end-to-end segmentation models.
- Core assumption: Bounding box predictions from GroundingDINO are sufficiently accurate to serve as reliable prompts for SAM's segmentation.
- Evidence anchors:
  - [section 3]: "With the multiple prompt classes, multiple bounding box detections are expected. We thus add a Non-Maximal Suppression module that operates on the selected prompt group classes to ensure that each object is detected once."
  - [section 3]: "The cleaned results, i.e., the bounding boxes, are then passed on as box-prompts to the SAM block to return clean segmentation masks of the desired regions."
  - [section 4.1]: "a single inference across this pipeline takes ca. 40 times longer (on CPU) than a single inference using models such as YOLOv8"
- Break condition: When bounding box predictions are too imprecise to generate useful segmentation prompts or when NMS removes correct detections.

## Foundational Learning

- Concept: Zero-shot learning and transfer learning
  - Why needed here: Understanding how pre-trained models generalize to unseen domains without fine-tuning is critical for evaluating the pipeline's effectiveness
  - Quick check question: What is the key difference between zero-shot and few-shot learning in the context of object detection?

- Concept: Text-image alignment and cross-modal representation
  - Why needed here: The GroundingDINO component relies on fusing text and image features to locate objects, requiring understanding of how these modalities interact
  - Quick check question: How does a text encoder typically interact with a visual encoder in multimodal models like GroundingDINO?

- Concept: Intersection over Union (IoU) and Average Precision metrics
  - Why needed here: These metrics are used to evaluate detection quality and compare different prompt strategies
  - Quick check question: If a predicted bounding box has IoU of 0.6 with the ground truth, is this considered a true positive at an IoU threshold of 0.5?

## Architecture Onboarding

- Component map: Image → GroundingDINO (text prompt) → NMS → SAM → Masks
- Critical path: Image → GroundingDINO (text prompt) → NMS → SAM → Masks
- Design tradeoffs:
  - Accuracy vs. speed: Using large foundation models provides better zero-shot performance but increases inference time significantly
  - Prompt specificity vs. generalization: More specific prompts improve performance on target datasets but may reduce cross-dataset applicability
  - Bounding box vs. direct segmentation: Two-stage approach is more efficient but depends on quality of initial box predictions
- Failure signatures:
  - High false positive rate: Likely caused by ambiguous prompt interpretation or insufficient domain knowledge in the model
  - Missing small elements: May indicate need for scale-specific prompts or multi-scale processing
  - Poor segmentation masks: Could result from inaccurate bounding boxes or SAM's limitations on historical document textures
- First 3 experiments:
  1. Run baseline evaluation with simple "{figure}" prompt on S-VED subset to establish performance floor
  2. Test domain-specific prompts ("{figure - diagram - geometry - sketch}") on same subset to measure prompt engineering impact
  3. Compare AP scores across all three datasets with both prompt types to identify dataset-specific challenges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GroundingDINO and SAM models be effectively fine-tuned on humanities-specific datasets to improve text-image prompting accuracy for historical document analysis?
- Basis in paper: [explicit] The paper mentions the intention to fine-tune GroundingDINO on humanities-specific datasets as part of future developments, indicating that this is an open area of research.
- Why unresolved: Fine-tuning models for specific domains requires careful consideration of the unique characteristics and variability of historical documents, as well as the creation of annotated datasets that capture the nuances of these materials.
- What evidence would resolve it: Comparative studies demonstrating the performance improvement of fine-tuned models versus pre-trained models on historical document datasets, along with detailed analyses of the challenges and successes encountered during the fine-tuning process.

### Open Question 2
- Question: What are the most effective strategies for prompt engineering in text-image prompting systems for historical document analysis, and how do they vary across different types of historical materials?
- Basis in paper: [explicit] The paper discusses the use of different linguistic prompts and their effects on the model's ability to extract visual elements from historical documents, highlighting the need for effective prompt engineering.
- Why unresolved: The effectiveness of prompts may vary depending on the type of historical material, the complexity of the visual elements, and the specific goals of the analysis, requiring further investigation into best practices for prompt engineering.
- What evidence would resolve it: Empirical studies comparing the performance of various prompt engineering strategies across different historical document types and tasks, along with qualitative assessments of the strengths and limitations of each approach.

### Open Question 3
- Question: How can the computational efficiency of the proposed pipeline be improved for large-scale historical document analysis without compromising the quality of the results?
- Basis in paper: [explicit] The paper acknowledges the high computational cost of the pipeline and suggests the need for more efficient solutions for production scenarios or domain-specific requirements.
- Why unresolved: Balancing computational efficiency with result quality is a common challenge in machine learning, and finding solutions that are both effective and efficient for historical document analysis requires further research and development.
- What evidence would resolve it: Comparative studies of different approaches to improving computational efficiency, such as model optimization, hardware acceleration, or algorithmic improvements, along with evaluations of their impact on the quality of the results.

## Limitations

- Reliance on three relatively small datasets that may not represent full diversity of historical documents
- Text-image prompt engineering approach shows dataset-specific effectiveness but lacks systematic framework for optimization
- Significant computational efficiency trade-off (40x slower than YOLOv8) raises scalability concerns for large-scale digitization projects

## Confidence

**High Confidence:** The sequential architecture combining GroundingDINO and SAM is technically sound, and the reported AP scores (0.42-0.82) are plausible given the foundation models' capabilities.

**Medium Confidence:** The specific prompt engineering strategies and their impact on performance are reasonable but not fully validated across diverse historical document types.

**Low Confidence:** The computational efficiency claims and scalability projections lack detailed benchmarking, making optimistic claims about replacing domain-specific models in production scenarios.

## Next Checks

1. Cross-dataset validation: Test the prompt engineering framework on at least two additional historical document collections with different visual element characteristics to assess generalizability.

2. Computational efficiency analysis: Conduct detailed benchmarking comparing the full pipeline inference time against optimized domain-specific models across varying image resolutions and batch sizes, including GPU acceleration effects.

3. Prompt optimization study: Systematically evaluate different prompt formulations using controlled experiments on a subset of each dataset, measuring AP score changes to develop a more principled approach to prompt engineering for historical documents.