---
ver: rpa2
title: 'Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through
  the Lens of News Headline Generation'
arxiv_id: '2310.10706'
source_url: https://arxiv.org/abs/2310.10706
tags:
- headlines
- headline
- news
- selection
- guidance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated different human-AI interaction methods for\
  \ news headline generation. Three types of assistance\u2014selection, guidance +\
  \ selection, and guidance + selection + post-editing\u2014were compared against\
  \ manual and AI-only methods."
---

# Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation

## Quick Facts
- arXiv ID: 2310.10706
- Source URL: https://arxiv.org/abs/2310.10706
- Reference count: 30
- Primary result: AI-assisted methods improve headline quality compared to manual writing, with guidance + selection being most efficient

## Executive Summary
This study evaluates human-AI interaction methods for news headline generation, comparing manual writing, AI-only, and three AI-assisted approaches: selection, guidance + selection, and guidance + selection + post-editing. The research found that while AI-only generated the highest quality headlines on average, all AI-assisted methods significantly improved headline quality compared to manual writing. Guidance + selection emerged as the most efficient approach, producing high-quality headlines quickly with minimal effort. Interestingly, post-editing did not significantly enhance headline quality or perceived control, suggesting that minor textual adjustments are insufficient to meaningfully alter headline quality or increase sense of ownership.

## Method Summary
The study recruited 40 participants with journalism or editing backgrounds to write 20 headlines each across four experimental conditions: manual writing, selection from AI-generated options, guidance + selection with keyword perspectives, and guidance + selection + post-editing. A GPT-3.5 model generated headlines using zero-shot prompting for perspective extraction and headline generation. Expert evaluators ranked 840 headlines using the TACT framework (tone, accuracy, clarity, and timeliness). The analysis employed Kruskal-Wallis H tests for overall comparisons and Mann-Whitney U tests for pairwise comparisons, examining headline quality, creation time, perceived difficulty, and perceived control.

## Key Results
- AI-assisted methods significantly outperformed manual writing in headline quality
- Guidance + selection was most efficient, producing high-quality headlines with minimal time and effort
- Post-editing did not significantly improve headline quality or perceived control compared to guidance + selection alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyword guidance before headline generation improves headline quality more than selection alone
- Mechanism: Providing structured keyword perspectives to the LLM narrows its focus and produces more targeted headlines, reducing irrelevant content and improving alignment with article key points
- Core assumption: The LLM's headline generation is sensitive to supplied keyword constraints and will produce more relevant output when guided
- Evidence anchors:
  - [abstract] "Guidance + selection resulted in the rapid creation of high-quality headlines compared to conditions involving further human intervention."
  - [section] "Participants comments revealed that the quality of keywords given in the Guidance interaction played a crucial role in the final headline quality."
  - [corpus] Weak—no direct citations in the corpus specifically supporting keyword guidance efficacy in headline tasks
- Break condition: If keyword extraction from the article is poor or irrelevant, the guidance may mislead the LLM and reduce headline quality

### Mechanism 2
- Claim: Post-editing does not significantly improve headline quality or perceived control
- Mechanism: Participants make only minor edits during post-editing (hedging, catering, clarifying), which are insufficient to meaningfully alter headline quality or increase sense of ownership
- Core assumption: Small textual changes do not alter the underlying model's creative contribution enough to shift perceived authorship or quality
- Evidence anchors:
  - [abstract] "Post-editing did not significantly enhance headline quality or perceived control."
  - [section] "Analysis of their post-editing process revealed that changes made by participants during post-editing were typically minor, falling into three categories: hedging, catering, clarifying."
  - [corpus] Weak—no corpus evidence directly quantifying the impact of minor edits on quality or control perception
- Break condition: If the LLM's output contains significant factual errors or hallucinations, post-editing may become necessary and could increase control perception

### Mechanism 3
- Claim: AI-assisted headline generation reduces headline creation time compared to manual methods
- Mechanism: Selection and guidance interactions eliminate the need for full headline drafting, allowing participants to choose or refine rather than create from scratch, thus reducing time
- Core assumption: Headline generation by the LLM is faster than manual writing and that participants can effectively select or guide without extensive effort
- Evidence anchors:
  - [abstract] "Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort)."
  - [section] "As expected, the Selection and Guidance + Selection conditions were markedly faster (H-value = 13.0, p-value = 0.005) than conditions that involved manual editing."
  - [corpus] Weak—no direct citations in the corpus on headline task time reduction
- Break condition: If the model's outputs are of poor quality or require heavy post-editing, the time savings may disappear

## Foundational Learning

- Concept: Kruskal-Wallis H test
  - Why needed here: To compare headline quality rankings across more than two headline generation conditions without assuming normal distribution
  - Quick check question: What test should you use to compare headline quality rankings across four different headline generation methods if the data is not normally distributed?

- Concept: Mann-Whitney U test
  - Why needed here: To perform pairwise comparisons of headline quality rankings between specific conditions after the Kruskal-Wallis test shows overall differences
  - Quick check question: After finding overall differences in headline quality, which test would you use to compare two specific headline generation methods?

- Concept: Zero-shot prompting
  - Why needed here: The study uses zero-shot prompts to extract perspectives and generate headlines without example headlines, ensuring broad coverage across news topics
  - Quick check question: Why might the study choose zero-shot prompting over few-shot prompting when generating headlines across diverse news topics?

## Architecture Onboarding

- Component map: News article display -> Keyword selection panel -> Headline selection panel -> Post-editing interface -> Difficulty rating slider
- Critical path: 1. Participant reads article → 2. AI extracts perspectives (if in guidance condition) → 3. AI generates headlines → 4. Participant selects headline → 5. Optional post-editing → 6. Submit headline → 7. Record time and difficulty
- Design tradeoffs:
  - Zero-shot prompting vs. few-shot: Broader topic coverage vs. potentially higher quality on common headline formats
  - Between-subjects design vs. within-subjects: Avoids fatigue and confusion vs. loses direct comparison across conditions
  - Single AI model vs. multiple: Consistent quality and interface vs. richer comparison of model capabilities
- Failure signatures:
  - Headline quality rankings not significantly different across conditions: Could indicate poor prompt quality or insufficient participant skill variation
  - No difference in perceived control despite post-editing: May indicate weak operationalization of control or limited edits made
  - Long headline creation times in selection condition: Could signal model output issues or poor selection interface
- First 3 experiments:
  1. Validate prompt quality by running GPT-3.5 on sample articles and inspecting generated keywords and headlines
  2. Test time tracking accuracy by simulating headline creation workflows and verifying timestamps
  3. Conduct a pilot with a small participant group to ensure interface clarity and data collection completeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models (LLMs) compare to human writers in terms of accuracy, efficiency, and creativity for news headline generation?
- Basis in paper: [explicit] The paper states that "LLMs alone generated the highest quality headlines on average" and that "all AI-assisted methods improved headline quality compared to manual writing." It also mentions that "guidance + selection was the most efficient, producing high-quality headlines quickly with minimal effort."
- Why unresolved: The paper only compares different human-AI interaction methods for headline generation, not the performance of LLMs versus human writers directly. Additionally, the study focuses on news headlines, which may not generalize to other types of writing tasks.
- What evidence would resolve it: A direct comparison study between LLMs and human writers for various writing tasks, measuring accuracy, efficiency, and creativity.

### Open Question 2
- Question: What are the potential ethical implications of using LLMs for news headline generation?
- Basis in paper: [inferred] The paper mentions that LLMs can "generate inappropriate content, such as toxic, discriminatory, or misleading information" and that "human intervention is necessary for editing and final decisions, particularly in high-stakes tasks."
- Why unresolved: The paper does not delve into the ethical considerations of using LLMs for news headline generation in detail. It only briefly mentions the potential for inappropriate content and the need for human oversight.
- What evidence would resolve it: A thorough analysis of the ethical implications of using LLMs for news headline generation, including potential biases, misinformation, and the impact on journalistic integrity.

### Open Question 3
- Question: How can human-AI interaction methods for news headline generation be further improved?
- Basis in paper: [explicit] The paper discusses various human-AI interaction methods, including selection, guidance + selection, and guidance + selection + post-editing. It also mentions that "guidance + selection was the most efficient, producing high-quality headlines quickly with minimal effort" and that "post-editing did not significantly enhance headline quality or perceived control."
- Why unresolved: The paper provides insights into the effectiveness of different interaction methods, but does not explore potential improvements or alternative approaches in detail.
- What evidence would resolve it: Research into novel human-AI interaction methods for news headline generation, focusing on enhancing efficiency, quality, and user experience. This could include exploring different types of guidance, interactive interfaces, or real-time feedback mechanisms.

## Limitations
- The finding that post-editing doesn't improve quality may be limited by the observation that participants made only minor edits
- The between-subjects design limits direct comparison between conditions and may introduce participant skill variation effects
- Claims about keyword guidance efficacy are supported primarily by qualitative participant feedback rather than robust quantitative evidence

## Confidence
- High confidence: Headline quality rankings showing AI-assisted methods outperforming manual writing
- Medium confidence: Efficiency claims about guidance+selection being fastest with minimal effort
- Low confidence: Claims about post-editing's ineffectiveness due to limited quantitative support

## Next Checks
1. Replicate the study with a within-subjects design where participants complete headlines across all conditions to enable direct comparison and control for individual skill variation
2. Implement a more structured post-editing protocol that requires substantive edits to test whether meaningful post-editing can improve headline quality and control perception
3. Conduct a follow-up study with diverse news article types (beyond Yahoo! News) to validate whether keyword guidance remains effective across different journalism domains and writing styles