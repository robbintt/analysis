---
ver: rpa2
title: A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained
  Masked Language Models
arxiv_id: '2310.12936'
source_url: https://arxiv.org/abs/2310.12936
tags:
- mlms
- social
- language
- factors
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates which factors affect social biases in pretrained
  Masked Language Models (MLMs) and their downstream task performance. Using 39 diverse
  MLMs and 30 factors across 5 categories (model size, training methods, corpora,
  tokenization, and language), it conducts predictive factor analysis via regression
  modeling.
---

# A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models

## Quick Facts
- arXiv ID: 2310.12936
- Source URL: https://arxiv.org/abs/2310.12936
- Authors: 
- Reference count: 40
- Primary result: Model size, tokenization method, and training objectives are the most influential factors affecting social biases and downstream task performance in pretrained Masked Language Models.

## Executive Summary
This study investigates which factors affect social biases in pretrained Masked Language Models (MLMs) and their downstream task performance. Using 39 diverse MLMs and 30 factors across 5 categories (model size, training methods, corpora, tokenization, and language), it conducts predictive factor analysis via regression modeling. Model size, tokenization method, and training objectives are identified as the most influential factors. Tokenization (particularly BPE) is key for social bias, while larger models show better downstream performance but mixed bias effects. Multilingual models exhibit lower social bias than monolingual ones.

## Method Summary
The study collects 39 pretrained MLMs with diverse characteristics and evaluates their social biases using AULA on StereoSet and Crowdsourced Stereotype Pairs benchmarks. Downstream task performance is measured on GLUE and TweetEval. Regression analysis using Gradient Boosting and other models is performed to determine the importance of 30 factors across model size, training methods, corpora, tokenization, and language. Feature importance is extracted via Gini importance scores to identify the most influential factors for bias and performance prediction.

## Key Results
- Model size, tokenization method, and training objectives are the top three most influential factors for both social bias and downstream task performance.
- Tokenization method (especially BPE) is key for social bias mitigation, while larger models show better downstream performance but mixed bias effects.
- Multilingual models exhibit lower social bias than monolingual ones, with domain-specific models showing varied bias patterns across different domains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient boosting regression can predict social bias and task performance from model factor features with reasonable accuracy.
- Mechanism: Gradient boosting builds an ensemble of weak decision trees, each correcting the errors of the previous one, which captures non-linear interactions among the 30 model factors.
- Core assumption: The 39 MLMs span a sufficiently diverse set of factor combinations so that the model learns generalizable patterns rather than overfitting to idiosyncrasies.
- Evidence anchors: [abstract] "Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives." [section 6] "We train a regression model... gradient boosting obtains the best performance in terms of both R2 and RMSE on both A-CP and TweetEval, while the decision tree obtains the best performance on A-SS."
- Break condition: If the factor space is too sparse or if the 39 models are not representative, the learned patterns may not generalize.

### Mechanism 2
- Claim: Tokenization method (especially BPE) is a primary driver of both social bias levels and downstream performance.
- Mechanism: BPE tokenization yields a more granular and consistent subword representation that reduces frequency bias in rare words, which in turn moderates stereotypical associations measured by AULA.
- Core assumption: Frequency bias in tokenization directly translates to bias in pseudo-likelihood scores used by AULA.
- Evidence anchors: [abstract] "Tokenization (particularly BPE) is key for social bias, while larger models show better downstream performance but mixed bias effects." [section 7] "Regarding the tokenization methods, we see that the models using BPE obtain better downstream task performance... models using SentencePiece contain the lowest level of social biases."
- Break condition: If frequency bias is not the dominant source of bias in AULA, or if other tokenization methods can be tuned to achieve similar granularity, the claimed primacy of BPE may not hold.

### Mechanism 3
- Claim: Model size, training objectives, and tokenization form the top three factor categories influencing bias and performance.
- Mechanism: Larger models have more capacity to encode subtle linguistic patterns; training objectives shape what patterns are emphasized; tokenization determines the granularity of token representations, all of which jointly influence bias and task metrics.
- Core assumption: These three categories capture the bulk of variance in bias/performance, and their interactions are more important than other factors like training corpus domain or language count.
- Evidence anchors: [abstract] "Model size, tokenization method, and training objectives are identified as the most influential factors." [section 6] "Table 5 shows the Gini importance scores... Parameter Size obtains the largest importance score for A-CP, while it is the second most important factor for TweetEval and GLUE. Tokenization is the most important factor for A-SS and the second most for A-CP."
- Break condition: If other factors have hidden interactions not captured by the Gini metric, the claimed primacy may be overstated.

## Foundational Learning

- Concept: Pseudo-Log-Likelihood (PLL) as a bias evaluation measure
  - Why needed here: PLL, weighted by attention, quantifies how much an MLM prefers stereotypical vs. anti-stereotypical sentences without requiring task-specific fine-tuning.
  - Quick check question: What does a PLL score closer to 50 indicate about an MLM's bias?
    - Answer: It indicates lower bias (closer to unbiased).

- Concept: Gradient boosting regression and feature importance
  - Why needed here: Gradient boosting is used to rank the predictive power of each factor on bias and task performance; understanding its mechanics is key to interpreting the results.
  - Quick check question: How is feature importance computed in gradient boosting (as used here)?
    - Answer: By normalized total reduction in the splitting criterion (Gini importance).

- Concept: Tokenization granularity and its effect on language model bias
  - Why needed here: Tokenization affects how often rare or biased words appear, influencing both frequency bias and downstream performance; BPE's subword splits can mitigate this.
  - Quick check question: Why might SentencePiece yield lower bias scores than BPE?
    - Answer: SentencePiece's unigram or BPE variants may produce more balanced token frequency distributions, reducing bias amplification.

## Architecture Onboarding

- Component map: Data ingestion -> Feature extraction (30 factors per MLM) -> Regression model training (gradient boosting) -> Prediction of AULA bias scores and GLUE/TweetEval performance -> Feature importance analysis.
- Critical path: 1. Gather pretrained MLM metadata (parameters, training corpus, tokenization, etc.). 2. Compute bias metrics (AULA on StereoSet and Crowdsourced Stereotype Pairs). 3. Evaluate downstream performance (GLUE and TweetEval). 4. Train gradient boosting to predict each metric from the 30 factors. 5. Extract and interpret Gini importance scores.
- Design tradeoffs:
  - Using intrinsic bias measures (AULA) vs. extrinsic measures: intrinsic is faster but may not reflect real-world task bias.
  - Binary vs. categorical encoding of factors: categorical encoding preserves granularity but increases model complexity.
  - Including multilingual models vs. monolingual only: increases diversity but introduces language-specific confounds.
- Failure signatures:
  - Low R2 for GLUE prediction suggests the chosen factors do not capture the drivers of GLUE performance.
  - Zero Gini importance for a factor indicates it does not vary meaningfully across models or has no predictive signal.
  - High RMSE relative to metric scale indicates poor fit or noisy target labels.
- First 3 experiments:
  1. Train gradient boosting on monolingual models only; compare R2 and RMSE to full 39-model model.
  2. Remove tokenization as a factor; measure change in predictive performance for AULA metrics.
  3. Add a synthetic factor (e.g., random noise) to confirm that Gini importance can detect non-informative features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the correlation between model size and social bias generalize across all types of biases, or are there specific biases that are more influenced by model size than others?
- Basis in paper: [inferred] The paper mentions that larger models demonstrate lesser social biases in terms of A-CP and A-SS scores, but it also notes that Tal et al. (2022) showed that even if gender bias scores are smaller for larger MLMs, they make more stereotypical errors with respect to gender.
- Why unresolved: The paper only briefly touches on gender bias and does not provide a comprehensive analysis of how model size affects different types of social biases.
- What evidence would resolve it: A detailed study analyzing the relationship between model size and various types of social biases, such as gender, race, and religion, would provide clarity on this issue.

### Open Question 2
- Question: How does the impact of tokenization methods on social bias and downstream task performance vary across different languages?
- Basis in paper: [explicit] The paper discusses the impact of tokenization methods on social bias and task performance, noting that models using BPE contain the lowest level of social bias and the best performance on both TweetEval and GLUE, while WordPiece is better for A-SS. However, the analysis is primarily focused on English and multilingual models.
- Why unresolved: The paper does not explore the impact of tokenization methods on social bias and task performance in languages other than English and multilingual models.
- What evidence would resolve it: A study comparing the impact of tokenization methods on social bias and task performance across a diverse set of languages would provide insights into this question.

### Open Question 3
- Question: To what extent do domain-specific models exhibit different levels of social bias compared to general domain models across various domains?
- Basis in paper: [explicit] The paper mentions that models in the social media domain contain the least bias according to A-CP and achieve the best performance on TweetEval, while the performance of models in the general domain is better than domain-specific ones on GLUE and obtains the lowest bias score for A-SS.
- Why unresolved: The analysis is limited to a few specific domains (social media, legal, and biomedical), and it is unclear how domain-specific models in other domains might exhibit different levels of social bias.
- What evidence would resolve it: A comprehensive study evaluating domain-specific models across a wide range of domains and comparing their levels of social bias to general domain models would provide a clearer understanding of this issue.

## Limitations

- The observational nature of the factor analysis and lack of external validation on held-out MLMs limits generalizability of the findings.
- The AULA bias evaluation, while efficient, may not fully capture real-world bias manifestations in downstream tasks due to its reliance on pseudo-log-likelihood and attention weighting.
- The study does not account for potential interactions between factors beyond what the regression model can infer, and lacks detailed implementation parameters for reproducibility.

## Confidence

- **High Confidence**: Model size, tokenization method, and training objectives are the top three influential factors for both bias and task performance.
- **Medium Confidence**: BPE tokenization is key for social bias mitigation.
- **Low Confidence**: The generalizability of the regression model's predictive accuracy to unseen MLMs.

## Next Checks

1. **External Validation**: Apply the trained gradient boosting regression model to a new set of pretrained MLMs (not included in the original 39) and evaluate whether the predicted bias and task performance metrics correlate with actual measurements.

2. **Factor Ablation Study**: Systematically remove each of the top three factors (model size, tokenization, training objectives) from the regression input and measure the change in predictive performance for both AULA bias scores and downstream task metrics.

3. **Tokenization Granularity Analysis**: Conduct a controlled experiment where the same MLM is retrained with different tokenization granularities (e.g., BPE vs. SentencePiece with varying merge/vocab sizes) and evaluate the resulting bias and task performance.