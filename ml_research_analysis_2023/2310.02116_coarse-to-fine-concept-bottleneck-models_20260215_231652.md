---
ver: rpa2
title: Coarse-to-Fine Concept Bottleneck Models
arxiv_id: '2310.02116'
source_url: https://arxiv.org/abs/2310.02116
tags:
- concept
- concepts
- image
- low-level
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel framework for hierarchical concept
  discovery in deep learning models, aiming to enhance interpretability in safety-critical
  applications. The proposed Concept Pyramid Models (CPMs) leverage recent advances
  in image-text models and employ a two-level concept discovery formulation.
---

# Coarse-to-Fine Concept Bottleneck Models

## Quick Facts
- arXiv ID: 2310.02116
- Source URL: https://arxiv.org/abs/2310.02116
- Reference count: 8
- Primary result: Introduces hierarchical Concept Pyramid Models (CPMs) that discover granular concept information from image regions, outperforming existing approaches in classification accuracy while significantly improving interpretation capacity.

## Executive Summary
This paper addresses the interpretability challenge in deep learning by proposing Concept Pyramid Models (CPMs), a novel hierarchical framework for concept discovery. The method leverages recent advances in image-text models to uncover both high-level and low-level concepts, linking them through a coarse-to-fine approach. By utilizing data-driven and sparsity-inducing Bayesian arguments, CPMs discover essential concepts for each example while maintaining computational efficiency through pre-computed embeddings. The framework introduces a new Jaccard index-based metric for assessing interpretation capacity, demonstrating substantial improvements over existing methods on benchmark datasets including CUB, SUN, and ImageNet-1k.

## Method Summary
The CPM framework introduces a two-level concept discovery formulation with high-level concepts discovered from whole images and low-level concepts discovered from image patches. Binary latent variables drawn from Bernoulli distributions determine concept relevance for each example, with Variational Bayesian arguments encouraging sparsity. The model pre-computes CLIP embeddings for images, patches, and concepts, then trains only linear layers during optimization. A predefined relationship matrix links high-level concepts to their low-level attributes, enabling information flow between the two discovery levels. The training objective balances classification accuracy with concept sparsity through cross-entropy loss and KL divergence regularization.

## Key Results
- CPMs outperform state-of-the-art approaches in classification accuracy across CUB, SUN, and ImageNet-1k datasets
- The Jaccard index-based interpretability metric shows significant improvements over baseline methods
- The hierarchical framework demonstrates superior performance in fine-grained classification tasks compared to traditional Concept Bottleneck Models
- Pre-computed embeddings and linear layer training result in significantly lower computational complexity than training full neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical concept discovery improves interpretability by capturing both high-level and low-level concept information.
- Mechanism: The CPM framework introduces two levels of concept discovery - high-level concepts discovered from the whole image and low-level concepts discovered from image patches. These levels are linked through binary indicators and concept relationships, allowing information flow between them.
- Core assumption: Low-level concepts can be effectively discovered from image patches and meaningfully linked to high-level concepts.
- Evidence anchors:
  - [abstract] "To this end, we propose a novel hierarchical concept discovery formulation leveraging: (i) recent advances in image-text models, and (ii) an innovative formulation for multi-level concept selection via data-driven and sparsity inducing Bayesian arguments."
  - [section] "In this work, we consider a novel hierarchical concept discovery formulation, introducing the notion of concept hierarchy, represented by two distinct yet dependent modeling levels: High (H) and Low (L)."
  - [corpus] Weak evidence - no direct citation of hierarchical approaches in corpus.
- Break condition: If low-level concepts discovered from patches are not meaningfully related to high-level concepts, or if the linking mechanism fails to capture these relationships effectively.

### Mechanism 2
- Claim: Data-driven concept discovery via Bayesian arguments enables sparsity and flexibility in concept selection.
- Mechanism: The framework introduces binary latent variables (ZH for high-level, ZL for low-level) drawn from Bernoulli distributions. These indicators determine which concepts are relevant for each example, with Variational Bayesian arguments encouraging sparsity.
- Core assumption: Binary indicators can effectively capture concept relevance on a per-example basis.
- Evidence anchors:
  - [abstract] "we propose a novel two-level concept discovery formulation leveraging: ... an innovative formulation for coarse-to-fine concept selection via data-driven and sparsity-inducing Bayesian arguments."
  - [section] "To discover the essential subset of high-level concepts to represent each example, we introduce appropriate auxiliary binary latent variables ZH ∈ {0, 1}N×H; these operate in an 'on'-'off' fashion, indicating, for each example, if a given concept needs to be considered to achieve the downstream task."
  - [corpus] Weak evidence - no direct citation of Bayesian approaches for concept discovery in corpus.
- Break condition: If the Bayesian framework fails to produce meaningful sparsity or if the concept discovery becomes too rigid, limiting the model's ability to capture relevant concepts.

### Mechanism 3
- Claim: Pre-computed embeddings and efficient training enable practical implementation of the CPM framework.
- Mechanism: The framework pre-computes CLIP embeddings for images, patches, and concepts, then trains only linear layers during optimization. This significantly reduces training complexity compared to training a full neural network.
- Core assumption: Pre-computed embeddings capture sufficient information for the concept discovery task.
- Evidence anchors:
  - [section] "To avoid having to calculate the embeddings of both images/patches and text at each iteration, we pre-compute them with the chosen backbone. Then, during training, we directly load them and compute the necessary quantities."
  - [section] "Concerning the complexity of the proposed CPM framework, by precomputing all the required embeddings for a considered task, the resulting complexity is significantly lower than training a backbone such as ResNet-18."
  - [corpus] No direct evidence - corpus focuses on CBM approaches but doesn't discuss computational efficiency.
- Break condition: If pre-computed embeddings become outdated or if the linear layers cannot capture the necessary relationships between concepts and images.

## Foundational Learning

- Concept: Variational Bayesian inference
  - Why needed here: Used to train the binary concept indicators (ZH, ZL) by maximizing the Evidence Lower Bound (ELBO) in the objective function.
  - Quick check question: How does the ELBO objective balance classification accuracy with concept sparsity?

- Concept: Gumbel-Softmax trick
  - Why needed here: Enables differentiable sampling from Bernoulli distributions for the binary concept indicators during training.
  - Quick check question: What role does the temperature parameter (τ) play in the Gumbel-Softmax approximation?

- Concept: Concept hierarchy and relationships
  - Why needed here: The framework relies on a predefined relationship matrix (B) between high-level concepts and their low-level attributes to link the two discovery levels.
  - Quick check question: How is the relationship matrix B constructed from the concept sets without ground truth information?

## Architecture Onboarding

- Component map: CLIP encoders (frozen) -> High-level concept discovery -> Low-level concept discovery -> Concept relationship matrix B -> Classification layers -> Output

- Critical path: Image → CLIP embedding → High-level discovery (ZH) → Low-level discovery (ZL) → Linked concepts → Classification

- Design tradeoffs:
  - Pre-computing embeddings vs. real-time computation
  - Number of patches (P) vs. granularity vs. computational cost
  - Sparsity level (via β) vs. classification accuracy
  - Complexity of concept hierarchy vs. interpretability

- Failure signatures:
  - Low classification accuracy on any dataset
  - Jaccard index scores significantly below baseline
  - Concept indicators ZH, ZL showing no meaningful sparsity
  - Disproportionate activation of certain concepts across examples

- First 3 experiments:
  1. Implement and test high-level concept discovery alone on CUB dataset, comparing accuracy and sparsity to baseline CBM approaches
  2. Add low-level patch-based discovery without linking, evaluating improvement in fine-grained classification
  3. Implement full CPM with linking mechanism, measuring classification accuracy and Jaccard index on both CUB and SUN datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Concept Pyramid Models (CPMs) compare to other state-of-the-art methods in terms of interpretability and classification accuracy across different datasets?
- Basis in paper: [explicit] The paper mentions that CPMs outperform other SOTA approaches classification-wise while substantially improving interpretation capacity.
- Why unresolved: The paper provides a new metric based on the Jaccard index to assess interpretation capacity, but does not provide a direct comparison with other methods in terms of interpretability.
- What evidence would resolve it: A direct comparison of CPMs with other SOTA methods in terms of interpretability and classification accuracy across different datasets would help resolve this question.

### Open Question 2
- Question: How does the proposed hierarchical framework handle the trade-off between granularity and interpretability in complex tasks such as fine-grained classification?
- Basis in paper: [explicit] The paper discusses the potential limitations of CBMs in tasks requiring greater granularity and introduces a novel multi-level paradigm to address this issue.
- Why unresolved: The paper does not provide a detailed analysis of how the hierarchical framework handles the trade-off between granularity and interpretability in complex tasks.
- What evidence would resolve it: A detailed analysis of the performance of the hierarchical framework in complex tasks, such as fine-grained classification, would help resolve this question.

### Open Question 3
- Question: How does the proposed framework adapt to different input representations, such as super-pixels, and what are the implications for interpretability and classification accuracy?
- Basis in paper: [explicit] The paper mentions that the framework can easily accommodate more than two levels of hierarchy using different input representations, such as super-pixels.
- Why unresolved: The paper does not provide an experimental evaluation of the framework's performance with different input representations.
- What evidence would resolve it: An experimental evaluation of the framework's performance with different input representations, such as super-pixels, would help resolve this question.

## Limitations

- The framework's effectiveness heavily depends on the quality of pre-trained CLIP embeddings and may not generalize well to domains where these representations are less effective.
- The interpretability metric assumes ground truth concept availability, limiting its applicability to real-world scenarios without concept annotations.
- The construction of the concept hierarchy matrix B without ground truth supervision may lead to suboptimal concept relationships and reduced interpretability.

## Confidence

- **High confidence**: The hierarchical architecture design and the use of pre-trained CLIP embeddings for concept discovery are well-supported by existing literature and the proposed framework's mathematical formulation.
- **Medium confidence**: The effectiveness of the Bayesian framework for sparse concept discovery and the interpretability improvements demonstrated through Jaccard index metrics, as these depend on specific implementation details and concept hierarchy construction.
- **Low confidence**: The generalization of computational efficiency claims across different domains and the assumption that CLIP embeddings will remain optimal for diverse concept discovery tasks.

## Next Checks

1. **Concept Hierarchy Validation**: Conduct ablation studies removing the hierarchical linking mechanism to quantify the specific contribution of the coarse-to-fine approach versus independent concept discovery at each level.

2. **Embedding Dependency Test**: Evaluate CPM performance using alternative backbone embeddings (e.g., ResNet features) to assess the dependency on CLIP representations and robustness to different feature extractors.

3. **Interpretability Metric Robustness**: Test the Jaccard index-based interpretability assessment on datasets without ground truth concept annotations, developing alternative metrics for real-world applications where concept labels are unavailable.