---
ver: rpa2
title: What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and
  Rationales for Disambiguating Defeasible Social and Moral Situations
arxiv_id: '2310.15431'
source_url: https://arxiv.org/abs/2310.15431
tags:
- moral
- action
- student
- data
- contextualizations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel task of generating defeasible moral
  contexts and rationales, where the goal is to provide grounded contextualizations
  that strengthen or weaken the moral acceptability of an action, accompanied by commonsense
  rationales that justify the reasoning. To elicit high-quality data for this task,
  an iterative self-distillation approach is proposed, which alternates between self-distillation
  from student models, targeted filtering with a critic model and NLI, and self-imitation
  learning.
---

# What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations

## Quick Facts
- arXiv ID: 2310.15431
- Source URL: https://arxiv.org/abs/2310.15431
- Authors: 
- Reference count: 40
- Key outcome: This work introduces a novel task of generating defeasible moral contexts and rationales, where the goal is to provide grounded contextualizations that strengthen or weaken the moral acceptability of an action, accompanied by commonsense rationales that justify the reasoning.

## Executive Summary
This paper introduces a novel task of generating defeasible moral contexts and rationales, where the goal is to provide grounded contextualizations that strengthen or weaken the moral acceptability of an action, accompanied by commonsense rationales that justify the reasoning. To elicit high-quality data for this task, an iterative self-distillation approach is proposed, which alternates between self-distillation from student models, targeted filtering with a critic model and NLI, and self-imitation learning. This process yields a student model that generates defeasible contexts with improved validity, diversity, and defeasibility. From this model, a high-quality dataset, δ-Rules-of-Thumb (δ-ROT), of 1.2M entries of contextualizations and rationales for 115K defeasible moral actions is distilled, rated highly by human annotators 85.9% to 99.8% of the time. Using δ-ROT, a final student model is obtained that outperforms all intermediate student models by a notable margin.

## Method Summary
The method involves an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between self-distillation from student models, targeted filtering with a critic model and NLI, and self-imitation learning. The process begins by generating seed knowledge from GPT-3 for a subset of actions, then training a critic model on human quality assessments to filter the raw seed data and obtain an initial medium-quality training corpus. A student model (Flan-T5) is fine-tuned on this corpus, and the resulting model is used to generate a new corpus of contextualizations and rationales for additional actions. This self-generated corpus is then filtered using the critic model and NLI, and the filtered data is used to further fine-tune the student model. This iterative process is repeated multiple times, with each iteration aiming to improve the validity, diversity, and defeasibility of the generated contexts and rationales. Finally, the highest quality model is used to generate the final dataset, δ-ROT, by producing contextualizations and rationales for a larger set of actions and applying additional filtering to ensure high confidence in the quality of the data.

## Key Results
- A novel task of generating defeasible moral contexts and rationales is introduced, with the goal of providing grounded contextualizations that strengthen or weaken the moral acceptability of an action, accompanied by commonsense rationales.
- An iterative self-distillation approach is proposed, which alternates between self-distillation from student models, targeted filtering with a critic model and NLI, and self-imitation learning, yielding a student model that generates defeasible contexts with improved validity, diversity, and defeasibility.
- From the highest quality model, a dataset, δ-Rules-of-Thumb (δ-ROT), of 1.2M entries of contextualizations and rationales for 115K defeasible moral actions is distilled, rated highly by human annotators 85.9% to 99.8% of the time.
- Using δ-ROT, a final student model is obtained that outperforms all intermediate student models by a notable margin on the task of generating defeasible moral contexts and rationales.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iterative self-distillation with targeted filtering improves model validity and diversity by iteratively refining the training corpus.
- **Mechanism**: The pipeline alternates between (1) generating diverse candidates using nucleus sampling, (2) filtering out low-quality and repetitive examples using a critic model and NLI, and (3) fine-tuning the student model on the filtered corpus. This process amplifies desired properties like validity and diversity over multiple iterations.
- **Core assumption**: The critic model and NLI filter are sufficiently accurate to identify low-quality and redundant examples without removing valuable diverse examples.
- **Evidence anchors**:
  - [abstract]: "We take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality)."
  - [section]: "Using the trained critic model, we filter the teacher model generations to remove errors from the distillation data and obtain an initial medium-quality training corpus, D0, of 85K examples."
  - [corpus]: Weak, no explicit quantitative evaluation of critic model accuracy or NLI filter effectiveness is provided in the corpus.
- **Break condition**: If the critic model or NLI filter become overly aggressive, they may remove too many valid examples, causing the model to lose diversity and coverage.

### Mechanism 2
- **Claim**: Training on diverse self-generated data improves the model's ability to produce varied and valid outputs compared to greedy decoding.
- **Mechanism**: By generating multiple candidates per action and moral variance using nucleus sampling, the model learns to produce a wider range of valid contextualizations. Ablation studies show that models trained only on top-1 candidates have significantly lower diversity and validity.
- **Core assumption**: The diversity of generated candidates during training directly translates to the diversity of outputs during inference.
- **Evidence anchors**:
  - [section]: "We find that omitting diverse candidates during the self-distillation process results in a drastic decrease in the diversity of the subsequent models. In particular, ablations using only the top 1 candidate in distillation (SelfDistill1-Top 1 Only and SelfDistill2-Top 1 Only) produce significantly less valid and unique generations compared to SelfDistill1 (5.05→2.54) and SelfDistill2 (5.21→2.16)."
  - [corpus]: Strong, quantitative results are provided showing the impact of candidate diversity on model performance.
- **Break condition**: If the sampling strategy during training is too constrained (e.g., low temperature), it may not generate sufficiently diverse candidates to improve the model's diversity.

### Mechanism 3
- **Claim**: Successive iterative training leads to a higher quality student model than a single iteration by amplifying learning signals over multiple rounds.
- **Mechanism**: The model is fine-tuned on the filtered corpus, then used to generate a new, higher-quality corpus, which is again filtered and used for further fine-tuning. This process refines the model's understanding and generation capabilities over multiple iterations.
- **Core assumption**: Each iteration of self-distillation provides a meaningful improvement in the quality and diversity of the generated data, which in turn improves the student model.
- **Evidence anchors**:
  - [section]: "Successive iterative training leads to a higher quality student model than a single iteration. We train an ablation model (SelfDistill2-No Self-distill) combining the actions in the training sets of both the first and second rounds of distillation, but with only one iteration of self-learning from Distillbase. SelfDistill2, which has been trained using the same actions over two rounds of successive training for the same number of total training steps, outperforms this ablation on almost all metrics, showing the effectiveness of amplifying learning signals via successive iterations of self-learning."
  - [corpus]: Strong, quantitative results are provided comparing the performance of models trained with and without successive iterations.
- **Break condition**: If the quality of the generated data plateaus or degrades after a certain number of iterations, further iterations may not provide additional benefits and could even harm performance.

## Foundational Learning

- **Concept**: Natural Language Inference (NLI)
  - Why needed here: NLI is used to filter out repetitive contextualizations by identifying pairs that are mutually entailed, ensuring diversity in the training data.
  - Quick check question: How does the NLI filter determine if two contextualizations are semantically repetitive?

- **Concept**: Knowledge Distillation
  - Why needed here: Knowledge distillation is the core technique used to transfer knowledge from a large teacher model (GPT-3) to a smaller student model, enabling the student to generate high-quality contextualizations and rationales.
  - Quick check question: What are the key steps in the symbolic knowledge distillation framework used in this work?

- **Concept**: Critic Model
  - Why needed here: The critic model is trained to approximate human judgment on the quality of generated contextualizations, allowing for automated filtering of low-quality examples.
  - Quick check question: How is the critic model trained, and what is its role in the iterative self-distillation process?

## Architecture Onboarding

- **Component map**: Teacher Model (GPT-3) -> Seed Data Generation -> Critic Model Filtering -> Initial Student Model -> Self-distillation Loop (Generation -> NLI Filtering -> Student Model Fine-tuning) -> Final Student Model

- **Critical path**: Teacher Model -> Seed Data Generation -> Critic Model Filtering -> Initial Student Model -> Self-distillation Loop (Generation -> NLI Filtering -> Student Model Fine-tuning) -> Final Student Model

- **Design tradeoffs**:
  - Using a large teacher model (GPT-3) vs. a smaller student model (Flan-T5) for efficiency and accessibility.
  - Balancing the aggressiveness of the critic model and NLI filter to maintain diversity while ensuring quality.
  - Deciding the number of iterations in the self-distillation loop to optimize performance without overfitting.

- **Failure signatures**:
  - If the critic model is too lenient, the student model may learn from noisy data, leading to low validity.
  - If the NLI filter is too strict, it may remove valid but semantically similar examples, reducing diversity.
  - If the self-distillation loop is run for too many iterations, the model may overfit to the training data and lose generalization ability.

- **First 3 experiments**:
  1. Evaluate the impact of the critic model threshold on the validity and diversity of the generated contextualizations.
  2. Compare the performance of models trained with and without the NLI filter to assess its effect on diversity.
  3. Test the effect of the number of iterations in the self-distillation loop on the final model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the student model compare to GPT-3 on the full test set versus a subset focused on culturally sensitive actions?
- Basis in paper: [explicit] The paper mentions a qualitative cultural bias analysis and provides an example with varying outputs for different countries, but does not provide a quantitative comparison of performance on culturally sensitive actions.
- Why unresolved: The paper does not present a systematic evaluation of the model's performance on culturally diverse or sensitive actions, which is crucial for understanding its real-world applicability and potential biases.
- What evidence would resolve it: A quantitative analysis of the model's performance on a diverse set of actions representing various cultural contexts, including a comparison with GPT-3 and other baselines.

### Open Question 2
- Question: How does the iterative self-distillation process affect the model's ability to generate diverse and novel contexts compared to a single iteration or no distillation?
- Basis in paper: [explicit] The paper discusses the effectiveness of iterative self-distillation in improving validity, diversity, and defeasibility, but does not provide a direct comparison of the diversity of generated contexts across different iterations.
- Why unresolved: While the paper shows overall improvements in diversity metrics, it does not specifically analyze the novelty and diversity of generated contexts at each iteration, which could provide insights into the strengths and limitations of the distillation process.
- What evidence would resolve it: A detailed analysis of the diversity and novelty of generated contexts at each iteration of the distillation process, including a comparison with the initial student model and GPT-3.

### Open Question 3
- Question: How does the critic model's performance generalize to new, unseen actions and contexts, and what are its limitations in identifying valid and diverse contexts?
- Basis in paper: [inferred] The paper relies on the critic model for filtering and selecting high-quality contexts, but does not extensively evaluate its performance on unseen data or discuss its potential limitations.
- Why unresolved: The critic model is a key component of the distillation process, and its ability to generalize and identify valid contexts is crucial for the overall performance of the student model. However, the paper does not provide a comprehensive evaluation of its capabilities and limitations.
- What evidence would resolve it: An evaluation of the critic model's performance on a held-out test set of unseen actions and contexts, including an analysis of its precision, recall, and potential biases in identifying valid and diverse contexts.

## Limitations

- Limited human evaluation: While the paper reports high human agreement scores (85.9% to 99.8%), the exact methodology and sample sizes for human evaluation are not fully specified, leaving some uncertainty about the robustness of these results.
- Dependence on GPT-3 quality: The quality of the initial seed knowledge generated by GPT-3 is crucial for the success of the iterative self-distillation process. If the seed data contains significant errors or biases, these may propagate through the subsequent iterations.
- Potential for overfitting: The iterative self-distillation process, while effective at improving model quality, may also lead to overfitting to the specific moral actions and contexts in the SOCIAL-CHEM-101 dataset. The generalizability of the approach to other domains or moral scenarios is not fully explored.

## Confidence

- High confidence: The core mechanism of iterative self-distillation with targeted filtering is well-supported by both theoretical arguments and empirical results, showing consistent improvements in validity, diversity, and defeasibility across iterations.
- Medium confidence: The effectiveness of the NLI filter in ensuring diversity is supported by quantitative results, but the exact impact on model performance is not fully isolated from other factors in the self-distillation process.
- Medium confidence: The quality of the final δ-ROT dataset is rated highly by human annotators, but the specific methodology and sample sizes for human evaluation are not fully specified, leaving some uncertainty about the robustness of these results.

## Next Checks

1. Conduct a more extensive human evaluation of the δ-ROT dataset, including diverse annotators and a larger sample size, to further validate the quality and generalizability of the generated contextualizations and rationales.
2. Test the iterative self-distillation approach on a different moral scenario dataset or domain to assess its generalizability and identify potential limitations or biases in the approach.
3. Perform an ablation study to isolate the impact of the NLI filter on model diversity and performance, by comparing models trained with and without the NLI filter while controlling for other factors in the self-distillation process.