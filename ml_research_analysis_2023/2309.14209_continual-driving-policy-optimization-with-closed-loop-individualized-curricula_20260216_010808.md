---
ver: rpa2
title: Continual Driving Policy Optimization with Closed-Loop Individualized Curricula
arxiv_id: '2309.14209'
source_url: https://arxiv.org/abs/2309.14209
tags:
- scenarios
- training
- scenario
- learning
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of safely training autonomous
  driving (AD) models by leveraging a large, diverse library of pre-collected traffic
  scenarios. The authors propose a Continual Learning framework called Closed-Loop
  Individualized Curricula (CLIC) that iteratively improves AD policies by: (1) evaluating
  the current policy on a subset of scenarios, (2) training a "difficulty predictor"
  to estimate collision risk per scenario, and (3) sampling training scenarios weighted
  by predicted difficulty to create an individualized curriculum.'
---

# Continual Driving Policy Optimization with Closed-Loop Individualized Curricula

## Quick Facts
- arXiv ID: 2309.14209
- Source URL: https://arxiv.org/abs/2309.14209
- Reference count: 40
- One-line primary result: CLIC achieves 6.5% higher success rate, 20% lower collision frequency, and 15% better handling of previously failing scenarios in autonomous driving

## Executive Summary
This paper tackles the challenge of safely training autonomous driving models by leveraging a large library of pre-collected traffic scenarios. The authors propose CLIC, a Continual Learning framework that iteratively improves driving policies by predicting scenario difficulty and creating individualized curricula. The method shows substantial improvements in handling risky scenarios while maintaining proficiency on simpler cases, addressing the distribution inconsistency problem when using scenarios collected by different driving models.

## Method Summary
CLIC frames AV evaluation as a collision prediction task where a difficulty predictor estimates failure probabilities in scenarios at each iteration. The framework iteratively evaluates the current policy, trains a discriminator to predict collision risk, and samples training scenarios weighted by predicted difficulty to create individualized curricula. The method prevents catastrophic forgetting by maintaining a small proportion of easier scenarios while progressively introducing harder ones, enabling transfer learning from diverse scenario libraries.

## Key Results
- CLIC achieves 6.5% higher success rate compared to curriculum-based baselines
- 20% lower collision frequency per second (CPS) than existing methods
- 15% better ability to handle previously failing scenarios while maintaining proficiency on simpler cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIC improves autonomous driving safety by iteratively focusing training on scenarios where the current policy is most likely to fail.
- Mechanism: The framework evaluates the current policy on a subset of scenarios, predicts collision probabilities using a learned discriminator, and then resamples scenarios weighted by these predicted failure probabilities to create individualized curricula.
- Core assumption: The predicted collision probabilities from the discriminator accurately reflect the actual risk of failure in each scenario for the current policy.
- Evidence anchors:
  - [abstract] "CLIC frames AV Evaluation as a collision prediction task, where it estimates the chance of AV failures in these scenarios at each iteration."
  - [section] "We treat the scenarios used for evaluation deval and their collision labels lgt as the ground truth, and then employ a supervised learning approach to train a difficulty predictor model pψ(d|ϕ)..."
- Break condition: If the predictor becomes overfit to the evaluation set or the evaluation subset is not representative, the curriculum may focus on the wrong scenarios, leading to ineffective or even harmful training.

### Mechanism 2
- Claim: CLIC prevents catastrophic forgetting by maintaining a small proportion of easier scenarios in the training set while progressively introducing harder ones.
- Mechanism: By weighting scenario sampling based on predicted collision probabilities, the method ensures that easier scenarios (low predicted failure probability) are still sampled, albeit less frequently, preserving the model's ability to handle them.
- Core assumption: The model's performance on easier scenarios degrades significantly if they are not included in training, necessitating their continued presence.
- Evidence anchors:
  - [abstract] "Experimental results clearly indicate that CLIC surpasses other curriculum-based training strategies, showing substantial improvement in managing risky scenarios, while still maintaining proficiency in handling simpler cases."
  - [section] "it is important to focus training on more challenging scenarios while including a small number of less difficult scenarios to prevent forgetting."
- Break condition: If the weighting scheme becomes too aggressive in selecting only the hardest scenarios, the model may lose proficiency on simpler cases, increasing the overall failure rate.

### Mechanism 3
- Claim: CLIC enables transfer learning from a large, diverse scenario library by adapting the training distribution to the current policy's capabilities.
- Mechanism: The predictor is retrained at each iteration based on the current policy's performance, allowing the curriculum to adapt dynamically as the policy improves, thus handling the distribution inconsistency problem from scenarios collected by different driving models.
- Core assumption: The scenario library contains sufficient diversity and the policy's performance on a small evaluation set is indicative of its performance across the entire library.
- Evidence anchors:
  - [abstract] "it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement."
  - [section] "Pre-collected scenarios can vary significantly across different driving patterns... A V models trained directly on such scenarios may not always yield improvement due to the significant distribution inconsistency [21]."
- Break condition: If the scenario library is too biased or the evaluation set is too small, the predictor may not capture the true difficulty distribution, leading to ineffective curriculum adaptation.

## Foundational Learning

- Concept: Reinforcement Learning with Soft Actor-Critic (SAC)
  - Why needed here: The paper uses SAC to train the autonomous driving policy because it incorporates an entropy term that encourages exploration and robustness, which is crucial for handling diverse and challenging traffic scenarios.
  - Quick check question: What is the role of the temperature hyperparameter α in SAC, and how does it affect the policy's behavior?

- Concept: Curriculum Learning
  - Why needed here: Curriculum learning is used to progressively introduce more challenging scenarios as the policy improves, preventing the model from being overwhelmed early on and ensuring efficient learning.
  - Quick check question: How does the weighting of scenarios based on predicted difficulty differ from traditional curriculum learning approaches that use discrete difficulty levels?

- Concept: Supervised Learning for Difficulty Prediction
  - Why needed here: A supervised learning model (a three-layer feedforward neural network) is trained to predict collision probabilities, which are then used as weights for scenario sampling. This allows for a data-driven and individualized curriculum.
  - Quick check question: Why is binary cross-entropy loss used for training the difficulty predictor, and what does it optimize for?

## Architecture Onboarding

- Component map:
  Scenario Library -> AV Evaluation -> Difficulty Predictor -> Scenario Selection -> AV Training -> Repeat

- Critical path:
  1. Evaluate current policy on a random subset of scenarios (deval)
  2. Train difficulty predictor on deval and its collision labels
  3. Use predictor to assign weights to all scenarios in the library
  4. Sample training scenarios (dtrain) based on these weights
  5. Train policy on dtrain using SAC
  6. Repeat from step 1 for a fixed number of iterations

- Design tradeoffs:
  - Evaluation vs. Training Time: Evaluating on a larger subset of scenarios provides a better estimate of policy performance but increases the time per iteration
  - Predictor Complexity vs. Generalization: A more complex predictor might better capture scenario difficulty but risks overfitting to the evaluation set
  - Sampling Weighting vs. Diversity: Aggressive weighting towards hard scenarios might improve performance on them but could reduce diversity in the training set

- Failure signatures:
  - If SR (success rate) does not improve over iterations, it might indicate the predictor is not accurately capturing difficulty or the curriculum is not effectively targeting weaknesses
  - If CPS (collisions per second) or CPM (collisions per meter) does not decrease, it suggests the policy is not learning to avoid collisions even in the focused scenarios
  - If TNR (true negative rate) decreases significantly while FNR (false negative rate) improves, it indicates the model is forgetting how to handle easier scenarios

- First 3 experiments:
  1. Run a single iteration of CLIC and verify that the difficulty predictor can accurately predict collision probabilities on the evaluation set
  2. Compare the scenario distributions (e.g., histogram of predicted labels) before and after training to confirm that the curriculum is shifting towards harder scenarios
  3. Test the policy before and after training on a fixed set of scenarios to measure improvement in SR, CPS, and CPM

## Open Questions the Paper Calls Out

- Question: Can CLIC's individualized curricula approach generalize to other domains beyond autonomous driving, such as robotics or healthcare?
  - Basis in paper: [explicit] The paper states "We are interested in exploring scenario libraries with more complex road topologies and other types of traffic participants" as future work
  - Why unresolved: The current work only validates CLIC in autonomous driving scenarios. The generalizability to other domains with different dynamics and evaluation metrics remains untested
  - What evidence would resolve it: Empirical results showing CLIC's effectiveness when applied to robotics control tasks or personalized treatment planning in healthcare

- Question: What theoretical guarantees can be established for convergence and performance improvement when using CLIC's difficulty predictor-based curriculum selection?
  - Basis in paper: [inferred] The paper mentions preventing "catastrophic forgetting" and avoiding "local optima" but doesn't provide theoretical analysis of the convergence properties of the framework
  - Why unresolved: The paper focuses on empirical evaluation without theoretical analysis of the learning dynamics or convergence guarantees for the CLIC framework
  - What evidence would resolve it: Formal proofs showing that the difficulty predictor-based curriculum selection leads to monotonic improvement in expected return or convergence to optimal policies

- Question: How sensitive is CLIC's performance to the choice of hyperparameters such as the number of evaluation scenarios, predictor architecture, or training iteration frequency?
  - Basis in paper: [explicit] The paper mentions specific values (e.g., "α = 0.1 yields promising results") but doesn't provide sensitivity analysis
  - Why unresolved: The paper uses fixed hyperparameters without exploring the sensitivity of performance to these choices or providing guidance on hyperparameter selection
  - What evidence would resolve it: Ablation studies showing performance across different hyperparameter settings and identification of robust ranges for key parameters

## Limitations

- The evaluation is conducted in simulation using a limited set of pre-collected scenarios, without addressing real-world generalization
- The paper does not provide a thorough ablation study of individual components like the difficulty predictor's architecture or weighting scheme
- Performance sensitivity to hyperparameters such as evaluation scenario count, predictor architecture, and training frequency is not explored

## Confidence

- Medium for overall performance claims due to simulation-based evaluation and lack of real-world testing
- Low for generalizability claims as the method is only validated in autonomous driving scenarios
- Medium for architectural claims as the paper provides implementation details but lacks component-level ablation studies

## Next Checks

1. Evaluate the trained policies on a held-out set of scenarios not seen during training to assess their ability to handle novel situations
2. Test the policies under varying environmental conditions (e.g., weather, lighting) and with different types of disturbances (e.g., sensor noise, unexpected pedestrian behavior)
3. Assess the performance of CLIC when applied to a much larger and more diverse scenario library, potentially with scenarios collected from different sources or using different driving models