---
ver: rpa2
title: The Program Testing Ability of Large Language Models for Code
arxiv_id: '2310.05727'
source_url: https://arxiv.org/abs/2310.05727
tags:
- code
- test
- cases
- program
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the program testing ability of large language
  models (LLMs) for code, an underexplored aspect compared to their program synthesis
  capabilities. The authors analyze how well different LLMs can generate test cases
  for given code implementations.
---

# The Program Testing Ability of Large Language Models for Code

## Quick Facts
- arXiv ID: 2310.05727
- Source URL: https://arxiv.org/abs/2310.05727
- Reference count: 4
- LLMs show higher test-case correctness for self-generated code vs. placeholder code

## Executive Summary
This paper investigates the program testing ability of large language models (LLMs) for code, an underexplored area compared to program synthesis. The authors evaluate how well LLMs generate test cases for code implementations across four settings: self-generated, all-generated, oracle, and placeholder code. Using metrics like pass rate and coverage rate, they test 11 LLMs on HumanEval+ and MBPP datasets. Results show that self-generated code prompts lead to better test-case correctness, and that test-case quality correlates with the model's program synthesis ability. By leveraging these insights, the authors improve program synthesis performance, achieving significant gains over baseline models.

## Method Summary
The authors evaluate 11 LLMs on their ability to generate test cases for code implementations. They use datasets HumanEval+ (164 problems) and MBPP (427 problems), with four code settings: self-generated, all-generated, oracle, and placeholder. For each problem and code implementation, LLMs generate three test cases using assert statements. The authors compute pass rate (fraction of test cases passing oracle), deduplicated pass rate, and coverage rate (branch coverage). They analyze results across models and settings to identify trends in test-case correctness and coverage.

## Key Results
- Self-generated code prompts yield higher test-case correctness than placeholder prompts
- Test-case correctness correlates positively with program synthesis ability
- Test-case quality decreases with generation order within a single prompt
- Improved program synthesis by leveraging weighted test cases based on generation order

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-generated code prompts lead to higher test-case correctness than placeholder prompts.
- Mechanism: Generating code first before test cases mimics a chain-of-thought prompting pattern, giving the model more context and reducing distribution shift between training and prompt data.
- Core assumption: The distribution of self-generated code is closer to the model's training distribution than placeholder code.
- Evidence anchors:
  - "Second, the correctness of the generated test cases is positively correlated with the LLM's ability of generating code... which means an LLM showing the state-of-the-art program synthesis performance is possibly also the state-of-the-art LLM for program testing."
  - "This observation is in fact unsurprising, considering that generating code first and test case afterwards resembles the chain-of-thought prompting (Wei et al., 2022)..."
- Break condition: If the model's self-generated code is highly incorrect, the test-case quality will degrade.

### Mechanism 2
- Claim: Test-case correctness correlates with overall program synthesis ability.
- Mechanism: Models that can generate more correct programs have better understanding of code semantics, which translates into generating more accurate test cases.
- Core assumption: The same underlying model capabilities (code understanding, semantic reasoning) are used for both synthesis and test generation.
- Evidence anchors:
  - "Second, the correctness of the generated test cases is positively correlated with the LLM's ability of generating code... which means an LLM showing the state-of-the-art program synthesis performance is possibly also the state-of-the-art LLM for program testing."
  - "As shown in Tables 2 and 3, GPT-3.5-turbo, which synthesizes programs/code with the highest correctness, provides test cases with the highest pass rate (71.03%) on HumanEval+."
- Break condition: If the model overfits to synthesis patterns that don't generalize to test generation.

### Mechanism 3
- Claim: Test-case quality decreases with generation order within a single prompt.
- Mechanism: The model's confidence and accuracy diminish as it continues generating multiple outputs in sequence, likely due to attention drift or loss of context.
- Core assumption: The model prioritizes higher-confidence outputs first, with later outputs being less reliable.
- Evidence anchors:
  - "Sixth, by conducting an additional experiment, we further compare the quality of test cases collected from different positions in the generation results... the first generated test case often shows the best correctness and the latterly generated ones are more incorrect."
  - "This may be due to the fact that the model tends to first generate content with a high level of confidence (which is also more likely to be correct)."
- Break condition: If the model is prompted with a single test case per run, this mechanism doesn't apply.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Explains why self-generated code leads to better test cases (code first, then test cases).
  - Quick check question: What is the main benefit of chain-of-thought prompting in the context of this paper?

- Concept: Coverage rate in software testing
  - Why needed here: Measures how much of the code is executed by generated test cases, indicating test diversity.
  - Quick check question: How is coverage rate calculated in this paper?

- Concept: Distribution shift
  - Why needed here: Explains why placeholder prompts may yield lower-quality test cases (mismatch between prompt and training data).
  - Quick check question: What causes distribution shift between placeholder code and the model's training data?

## Architecture Onboarding

- Component map:
  - LLM code models (InCoder, CodeGen2, CodeT5+, SantaCoder, etc.) -> Test case generation pipeline -> Evaluation metrics (pass rate, coverage rate) -> Program synthesis improvement module

- Critical path:
  1. Generate code (self-generated or oracle)
  2. Generate test cases with weighted prompts
  3. Evaluate test cases (pass rate, coverage)
  4. Use test cases to improve synthesis (dual execution agreement)

- Design tradeoffs:
  - Self-generated vs. placeholder: Self-generated gives better test cases but requires an extra synthesis step.
  - Number of test cases: More test cases increase coverage but may reduce correctness.
  - Weighting by order: Improves synthesis but adds complexity.

- Failure signatures:
  - Low pass rate: Model may not understand code semantics well.
  - Low coverage: Test cases are too simple or repetitive.
  - Inconsistent results: Model may be sensitive to prompt formatting.

- First 3 experiments:
  1. Generate code and test cases using self-generated prompts vs. placeholder.
  2. Compare test case quality when given correct vs. incorrect code in prompts.
  3. Test the effect of weighting test cases by generation order on synthesis performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correctness of test cases generated by large language models correlate with the complexity of the tested code?
- Basis in paper: [inferred] The paper shows that SantaCoder, which tends to generate longer and more complex test cases, has lower correctness on some problems compared to CodeT5+. This suggests a potential relationship between test case complexity and correctness.
- Why unresolved: The paper only provides a few examples and does not conduct a systematic analysis of the relationship between test case complexity and correctness across a wide range of problems.
- What evidence would resolve it: A comprehensive study analyzing the complexity of test cases generated by different models and their corresponding correctness rates across a large set of problems would provide insights into this relationship.

### Open Question 2
- Question: How does the distribution shift between self-generated code and training code affect the testing ability of large language models?
- Basis in paper: [explicit] The paper mentions that less distribution shift between self-generated code in prompt and the training code may contribute to the superior testing ability of some models when using self-generated code.
- Why unresolved: The paper does not provide a detailed analysis of the distribution shift and its impact on testing ability. Further investigation is needed to understand the extent of this effect and how it varies across different models and datasets.
- What evidence would resolve it: Analyzing the distribution of code features in self-generated code and training data, and correlating this with the testing performance of models, would shed light on the role of distribution shift.

### Open Question 3
- Question: What are the optimal strategies for leveraging generated test cases to improve program synthesis performance?
- Basis in paper: [explicit] The paper proposes two modifications to the CodeT model (using self-generated code and rank-based weighted test cases) that lead to improved program synthesis performance. However, it acknowledges that there may be other effective strategies.
- Why unresolved: The paper only explores two specific strategies and does not exhaustively search for the optimal approaches. Different models and datasets may require different strategies.
- What evidence would resolve it: Conducting a comprehensive study comparing various strategies for leveraging generated test cases, such as different weighting schemes, prompt engineering techniques, and model architectures, would identify the most effective approaches.

## Limitations

- Evaluation limited to specific models and datasets (HumanEval+ and MBPP), which may not generalize to other programming tasks or languages.
- Test case generation process relies on specific prompt formats and temperatures, and results may vary with different configurations.
- Study does not account for edge cases or more complex code scenarios, which could impact the generalizability of the findings.

## Confidence

- High Confidence: The observation that self-generated code prompts lead to higher test-case correctness than placeholder prompts is well-supported by the experimental results and aligns with established chain-of-thought prompting principles.
- Medium Confidence: The correlation between test-case correctness and program synthesis ability is plausible but may be influenced by model-specific factors not fully explored in the study.
- Medium Confidence: The claim that test-case quality decreases with generation order is supported by the data but could vary depending on the specific model and task complexity.

## Next Checks

1. Replicate the study with additional datasets and programming languages to validate whether the observed trends in test-case correctness and correlation with synthesis ability hold across diverse coding tasks and languages beyond HumanEval+ and MBPP.

2. Test the impact of prompt variations on test-case quality by experimenting with different prompt formats, temperatures, and code contexts to determine the robustness of the findings related to self-generated vs. placeholder prompts and generation order.

3. Analyze edge cases and complex code scenarios to evaluate the performance of LLMs on more challenging code implementations and assess whether the observed patterns in test-case correctness and coverage persist in less straightforward scenarios.