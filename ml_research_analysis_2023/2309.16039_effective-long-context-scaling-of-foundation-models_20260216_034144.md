---
ver: rpa2
title: Effective Long-Context Scaling of Foundation Models
arxiv_id: '2309.16039'
source_url: https://arxiv.org/abs/2309.16039
tags:
- long
- llama
- data
- context
- rope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a series of long-context large language models\
  \ (LLMs) with context windows up to 32,768 tokens, built by continual pretraining\
  \ from Llama 2. The approach involves modifying Llama 2\u2019s rotary positional\
  \ encoding to reduce decay for distant tokens and using longer training sequences\
  \ with upsampled long texts."
---

# Effective Long-Context Scaling of Foundation Models

## Quick Facts
- arXiv ID: 2309.16039
- Source URL: https://arxiv.org/abs/2309.16039
- Reference count: 40
- Primary result: Long-context LLMs up to 32,768 tokens built by continual pretraining from Llama 2

## Executive Summary
This paper presents a series of long-context large language models (LLMs) with context windows up to 32,768 tokens, built by continual pretraining from Llama 2. The approach involves modifying Llama 2's rotary positional encoding to reduce decay for distant tokens and using longer training sequences with upsampled long texts. The models are evaluated on language modeling, synthetic context probing tasks, and real-world benchmarks covering both short and long contexts. Results show consistent improvements over Llama 2 on most tasks, with significant gains on long-context tasks. Notably, the 70B variant surpasses GPT-3.5-turbo-16k's overall performance on long-context tasks after instruction tuning without human-annotated long data. The paper also provides in-depth analysis on the impact of positional encoding, data mix, and training curriculum, offering insights into building effective long-context models.

## Method Summary
The method involves continual pretraining from Llama 2 checkpoints with modified rotary positional encoding (RoPE) that reduces decay for distant tokens by adjusting the base frequency. The training uses longer sequences (32,768 tokens for 7B/13B models, 16,384 for 34B/70B) with upsampled long texts in the data mix. The models are trained for 400B tokens over 100,000 steps with cosine learning rate schedule. After pretraining, instruction tuning is performed using a large short-prompt dataset augmented with synthetic self-instruct long data. The approach aims to efficiently extend short-context models to long contexts while maintaining performance on both short and long tasks.

## Key Results
- Long-context models show consistent improvements over Llama 2 on most tasks, with significant gains on long-context tasks
- The 70B variant surpasses GPT-3.5-turbo-16k's overall performance on long-context tasks after instruction tuning
- Continual pretraining is 40% more efficient than training from scratch while maintaining similar performance
- Modified RoPE positional encoding with adjusted base frequency reduces decay for distant tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decreasing the base frequency of RoPE positional encoding reduces the decay of attention scores for distant tokens, enabling longer effective context.
- Mechanism: RoPE introduces a frequency-dependent rotation that causes the attention score between tokens to decay as their distance increases. By lowering the base frequency, the rotation angle per dimension decreases, flattening the decay curve and preserving relative token information over longer distances.
- Core assumption: The original RoPE positional encoding imposes a hard limitation on the effective context length due to its inherent frequency-based decay.
- Evidence anchors:
  - [abstract] "We adopt a minimal yet necessary modification on the RoPE positional encoding... decreasing the rotation angle (controlled by the hyperparameter 'base frequency b'), which reduces the decaying effect of RoPE for distant tokens."
  - [section 4.1] Empirical comparison shows that ROPE ABF outperforms other variants including PI on synthetic context probing tasks and validation perplexity.
- Break condition: If the base frequency is decreased too much, the model may lose its ability to distinguish between nearby tokens, causing confusion in short-range dependencies.

### Mechanism 2
- Claim: Continual pretraining from a short-context model with longer sequences is more efficient and similarly effective compared to pretraining from scratch with long sequences.
- Mechanism: By starting from a model already pretrained on short sequences, the model can quickly adapt to longer sequences without relearning basic language modeling capabilities. This reduces the total computational cost while still acquiring long-context abilities.
- Core assumption: The short-context pretraining has already established robust language modeling foundations that can be efficiently extended to longer contexts.
- Evidence anchors:
  - [abstract] "We empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences."
  - [section 4.4] Ablation comparing different training curricula shows that models starting from short sequences achieve similar performance with ~40% less FLOPs.
- Break condition: If the short-context model is too specialized or lacks certain long-range dependencies, continual pretraining may not fully recover those capabilities.

### Mechanism 3
- Claim: Adjusting the data mix to upsample long texts and include high-quality long data improves long-context performance without sacrificing short-context capabilities.
- Mechanism: By increasing the proportion of long documents in the pretraining corpus, the model is exposed to more examples of long-range dependencies, improving its ability to utilize extended context. The quality of the data ensures that these dependencies are meaningful.
- Core assumption: The presence of long texts in the pretraining data is necessary for the model to learn effective long-context processing, but the quality of the text is more important than just the length.
- Evidence anchors:
  - [section 4.2] Ablation experiments show that removing long text data results in most of the performance gain being lost, while upsampling existing long text data does not provide additional benefits.
  - [section 4.2] "We observe similar results on the FIRST-SENTENCE-RETRIEVAL task as shown by Figure 7 in the Appendix."
- Break condition: If the long texts are of low quality or do not contain meaningful long-range dependencies, simply increasing their proportion may not improve performance.

## Foundational Learning

- Concept: Positional encoding and its impact on attention mechanisms
  - Why needed here: Understanding how different positional encoding schemes affect the model's ability to attend to distant tokens is crucial for designing effective long-context models.
  - Quick check question: How does the base frequency parameter in RoPE influence the decay of attention scores between tokens at different distances?

- Concept: Continual pretraining vs. training from scratch
  - Why needed here: Knowing the trade-offs between these approaches helps in making informed decisions about resource allocation and model development strategies.
  - Quick check question: What are the key advantages of continual pretraining from a short-context model compared to training a long-context model from scratch?

- Concept: Data mix and its effect on model performance
  - Why needed here: Understanding how the composition of the pretraining corpus affects the model's capabilities is essential for optimizing the training process.
  - Quick check question: How does adjusting the ratio of long texts in the pretraining data influence the model's performance on long-context tasks?

## Architecture Onboarding

- Component map:
  Tokenizer (Llama 2 tokenizer with 32k vocabulary) -> Modified RoPE positional encoding -> Transformer layers (similar to Llama 2) -> Language modeling head

- Critical path:
  1. Load Llama 2 checkpoint
  2. Modify positional encoding to reduce decay
  3. Prepare pretraining data with longer sequences and adjusted data mix
  4. Configure training with increased sequence length and adjusted learning rate
  5. Train for 400B tokens with cosine learning rate schedule
  6. Evaluate on language modeling and downstream tasks
  7. Perform instruction tuning with self-instruct data

- Design tradeoffs:
  - Using continual pretraining saves computational resources but may limit the model's ability to learn certain long-range dependencies from scratch.
  - Modifying RoPE positional encoding is a lightweight solution but may not be as effective as more complex methods like sparse attention.
  - Upsampling long texts in the pretraining data improves long-context performance but may require careful curation to ensure data quality.

- Failure signatures:
  - If the model fails to improve on long-context tasks, it may indicate that the positional encoding modification is insufficient or the pretraining data lacks meaningful long-range dependencies.
  - If the model's performance on short-context tasks degrades, it may suggest that the continual pretraining process is causing catastrophic forgetting of previously learned capabilities.
  - If the model overfits to the pretraining data, it may result in poor generalization to downstream tasks.

- First 3 experiments:
  1. Test the modified RoPE positional encoding on a synthetic context probing task (e.g., FIRST-SENTENCE-RETRIEVAL) to verify its effectiveness in reducing decay.
  2. Perform an ablation study on the pretraining data mix by comparing models trained with and without long texts to assess the impact on long-context performance.
  3. Compare the performance of models trained with different training curricula (e.g., 4k→32k vs. 32k from scratch) to evaluate the efficiency of continual pretraining.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal base frequency for RoPE positional encoding when extending context windows beyond 32,768 tokens?
- Basis in paper: [inferred] The paper discusses modifying RoPE's base frequency to reduce decay for distant tokens, finding that increasing it from 10,000 to 500,000 improves performance. However, it doesn't explore the upper limits of this parameter.
- Why unresolved: The analysis only goes up to 32,768 tokens and uses a specific base frequency of 500,000. The relationship between base frequency and context length may not be linear, and there could be diminishing returns or negative effects at extremely high frequencies.
- What evidence would resolve it: Systematic experiments varying the base frequency across a wider range (e.g., 10,000 to 10,000,000) and testing on context lengths beyond 32,768 tokens would reveal the optimal scaling relationship.

### Open Question 2
- Question: How does the quality of pretraining data compare to its length distribution in determining long-context model performance?
- Basis in paper: [explicit] The authors find that adjusting the length distribution of pretraining data does not provide major benefits, and improvements mostly come from data quality rather than length.
- Why unresolved: While the paper shows that length distribution isn't the key factor, it doesn't quantify the relative importance of data quality versus length. It's unclear how much quality matters compared to having some long texts present.
- What evidence would resolve it: Controlled experiments systematically varying both data quality (e.g., using clean vs. noisy corpora) and length distribution (proportion of long vs. short texts) while holding other factors constant would reveal their relative contributions to final performance.

### Open Question 3
- Question: What is the most efficient training curriculum for long-context models - continual pretraining or training from scratch with long sequences?
- Basis in paper: [explicit] The authors find that continual pretraining from short-context models saves around 40% FLOPs while maintaining similar performance to training from scratch with long sequences.
- Why unresolved: The comparison is limited to a two-stage curriculum (short → long). It doesn't explore other possibilities like gradual increases in sequence length throughout training, or the impact of when to switch from short to long sequences.
- What evidence would resolve it: Experiments testing various training curricula including gradual sequence length increases, different switch points, and comparing against pure scratch training would identify the most compute-efficient approach.

## Limitations

- Training data composition details are lacking, making it difficult to assess the impact of data quality versus length distribution
- Specific positional encoding parameters are not provided, hindering exact reproduction of the modifications
- Evaluation focuses on a limited set of long-context benchmarks, with unclear performance on diverse long-context tasks
- Limited information about instruction tuning data and process, particularly regarding long-context instruction handling

## Confidence

**High Confidence Claims**:
- The effectiveness of modifying RoPE positional encoding to reduce decay for distant tokens, as evidenced by consistent improvements across multiple tasks.
- The efficiency of continual pretraining from short-context models compared to training from scratch, supported by ablation studies showing similar performance with reduced computational cost.

**Medium Confidence Claims**:
- The impact of upsampling long texts in the pretraining data on long-context performance, based on ablation experiments, but with limited details on data quality and composition.
- The overall superiority of the proposed models over Llama 2 on long-context tasks, with results showing significant gains, but with potential concerns about the comprehensiveness of the evaluation.

**Low Confidence Claims**:
- The claim that the 70B variant surpasses GPT-3.5-turbo-16k's overall performance on long-context tasks, as this comparison is based on a specific set of benchmarks and may not generalize to all long-context scenarios.

## Next Checks

1. Reproduce the Positional Encoding Modification: Implement the modified RoPE positional encoding with the specified base frequency adjustment and evaluate its impact on attention scores for distant tokens using synthetic context probing tasks. Compare the results with the original RoPE and other positional encoding variants.

2. Analyze Data Quality and Composition: Conduct a detailed analysis of the pretraining data, including the sources, quality metrics, and proportion of long versus short documents. Perform ablation studies to isolate the impact of data quality and quantity on long-context performance.

3. Evaluate on Diverse Long-Context Tasks: Extend the evaluation to a broader range of long-context tasks, such as multi-document question answering, long-form content generation, and retrieval-augmented generation. Assess the models' performance across these tasks to gain a more comprehensive understanding of their long-context capabilities.