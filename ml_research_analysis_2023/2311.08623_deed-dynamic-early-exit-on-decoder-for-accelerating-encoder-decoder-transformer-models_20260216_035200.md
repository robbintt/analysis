---
ver: rpa2
title: 'DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer
  Models'
arxiv_id: '2311.08623'
source_url: https://arxiv.org/abs/2311.08623
tags:
- decoder
- layer
- layers
- accuracy
- exit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DEED, a dynamic early exit approach for accelerating
  encoder-decoder transformer models. The core idea is to use confidence-based step-level
  dynamic early exit, where the decoder may decide to use fewer layers based on confidence
  at each decoding step.
---

# DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models

## Quick Facts
- **arXiv ID**: 2311.08623
- **Source URL**: https://arxiv.org/abs/2311.08623
- **Reference count**: 30
- **Key outcome**: DEED achieves 30%-60% latency reduction on vision-language tasks while maintaining or improving accuracy compared to baselines.

## Executive Summary
This paper proposes DEED, a dynamic early exit approach for accelerating encoder-decoder transformer models. The core idea is to use confidence-based step-level dynamic early exit, where the decoder may decide to use fewer layers based on confidence at each decoding step. To enable this, they build a multi-exit model with shared generation head and adaptation modules, trained with deep supervision. They also propose a just-in-time computation algorithm to resolve semantic misalignment. Experiments on LaTr++ and OFA models across various vision-language tasks show DEED can reduce inference latency by 30%-60% with comparable or higher accuracy compared to baselines.

## Method Summary
DEED implements dynamic early exit on decoder layers by computing confidence scores at each decoding step. If confidence exceeds threshold τ, the model exits early, avoiding computation of deeper layers. The architecture uses a shared generation head across all decoder layers with adaptation modules inserted between shallow layers and the head to align feature spaces. The model is trained with deep supervision emphasizing final layer loss. During inference, a just-in-time computation algorithm dynamically computes deeper-layer features for previous steps when needed, resolving semantic misalignment while maintaining efficiency.

## Key Results
- Reduces inference latency by 30%-60% across vision-language tasks
- Maintains or improves accuracy compared to baseline models
- Outperforms baseline approaches with clear margins on DocVQA, OCR-VQA, ST-VQA, TextVQA, VQAv2, and referring expression tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic early exit reduces decoder latency by selectively skipping unnecessary layers at each decoding step.
- Mechanism: The model computes a confidence score at each decoder layer during each decoding step. If the confidence exceeds a threshold τ, the model exits early and avoids computing deeper layers for that step. This is implemented via a multi-exit architecture with shared generation heads and adaptation modules, trained with deep supervision.
- Core assumption: The confidence score is a reliable proxy for the model's certainty about its prediction at each layer.
- Evidence anchors:
  - [abstract] "the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step"
  - [section 3.3] "At each decoding step, the model decides how many decoder layers to use based on its confidence about the output token"
  - [corpus] Weak evidence - corpus mentions related work like "DeeBERT: Dynamic early exiting for accelerating BERT inference" but no direct confidence-based mechanism evidence for decoder-specific dynamic early exit
- Break condition: Confidence scores become unreliable (e.g., overconfident wrong predictions) or the threshold τ is poorly chosen, leading to accuracy degradation.

### Mechanism 2
- Claim: Shared generation head with adaptation modules improves shallow layer accuracy without sacrificing final layer performance.
- Mechanism: Instead of having separate prediction heads for each decoder layer, DEED shares a single generation head across all layers. Adaptation modules are inserted between shallow decoder layers and the shared generation head to align feature spaces. The model is trained with deep supervision emphasizing the final layer loss.
- Core assumption: Sharing generation knowledge across layers through adaptation modules can strengthen shallow layer generative ability.
- Evidence anchors:
  - [section 3.2] "we share the generation head across different decoder layers and insert adaptation modules between the shallow decoder layers and the generation head"
  - [section 4.3] "By using both the shared generation head and the adaptation module, the model achieves consistently better accuracy than the baseline"
  - [corpus] Weak evidence - corpus mentions related work like "DeeBERT" and "RomeBERT" but no direct evidence of shared generation head with adaptation modules for decoder acceleration
- Break condition: Adaptation modules fail to properly align feature spaces, causing the shared generation head to perform poorly across all layers.

### Mechanism 3
- Claim: Just-in-time computation resolves semantic misalignment between different decoding steps using different numbers of layers.
- Mechanism: When exiting at different layers at different steps, DEED dynamically computes the deeper-layer features for previous steps just before they're needed, rather than copying shallow features or pre-computing everything. This ensures semantic alignment while maintaining efficiency.
- Core assumption: Computing deeper features on-demand is more efficient than pre-computing or copying features, and resolves semantic misalignment.
- Evidence anchors:
  - [section 3.1] "we do step-level dynamic early exit with just-in-time computation"
  - [section 3.3] "we design an algorithm to compute the past key-value features just-in-time"
  - [section 4.1] "Compared to other baseline approaches, DEED always outperforms them with clear margins" (implying the just-in-time computation is effective)
- Break condition: Just-in-time computation becomes too computationally expensive for very long sequences or fails to properly align features across steps.

## Foundational Learning

- Concept: Auto-regressive decoding and its computational cost
  - Why needed here: Understanding why decoders are slow and why early exit is beneficial requires knowing that each token is generated conditioned on previous tokens, requiring sequential computation
  - Quick check question: Why can't we generate all tokens in parallel in an auto-regressive decoder?

- Concept: Deep supervision and multi-exit model training
  - Why needed here: DEED's training strategy relies on training each decoder layer to produce plausible predictions through deep supervision
  - Quick check question: How does adding loss at each decoder layer during training help the model exit early without accuracy loss?

- Concept: Confidence-based decision making in neural networks
  - Why needed here: The early exit mechanism depends on confidence scores to decide when to exit, requiring understanding of how these scores are computed and interpreted
  - Quick check question: What are the pros and cons of using confidence scores versus learned policies for early exit decisions?

## Architecture Onboarding

- Component map:
  - Input image → patch encoding → visual tokens
  - OCR text → word embeddings
  - Encoder processes multi-modal input
  - Decoder processes tokens auto-regressively
  - At each layer and step: compute confidence → decide to exit or continue
  - Just-in-time computation of deeper features when needed
  - Final prediction from generation head

- Critical path:
  Input image → patch encoding → visual tokens → OCR text → word embeddings → Encoder → Decoder → confidence scoring → early exit decision → just-in-time computation → generation head → final prediction

- Design tradeoffs:
  - Shared vs. separate generation heads: Shared saves parameters but requires adaptation modules
  - Confidence threshold selection: Higher threshold → more accuracy but less speed-up
  - Just-in-time vs. pre-computation: Just-in-time saves memory but may add latency
  - Adaptation module complexity: More complex modules might better align features but add parameters

- Failure signatures:
  - Confidence scores are consistently low → model rarely exits early
  - Accuracy drops significantly with early exit → confidence threshold too aggressive or adaptation modules ineffective
  - Latency reduction is minimal → model rarely exits early or just-in-time computation overhead is high
  - Memory issues → just-in-time computation saves too many intermediate features

- First 3 experiments:
  1. Ablation study on confidence threshold τ to find optimal accuracy-latency tradeoff
  2. Compare shared vs. separate generation heads with and without adaptation modules
  3. Test just-in-time computation vs. copying features vs. pre-computation on a small dataset to verify semantic alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of varying the confidence threshold τ on the trade-off between accuracy and latency for DEED?
- Basis in paper: [explicit] The paper mentions that DEED can realize different trade-offs between accuracy and latency by tuning the confidence score threshold τ.
- Why unresolved: The paper provides some examples of how changing τ affects accuracy and latency, but does not provide a comprehensive analysis of the impact of different threshold values.
- What evidence would resolve it: A detailed study showing the accuracy and latency of DEED for a range of confidence threshold values would help understand the trade-off better.

### Open Question 2
- Question: How does the performance of DEED compare to other dynamic early exit approaches, such as Depth-Adaptive Transformer (DAT), on a wider range of vision-language tasks?
- Basis in paper: [explicit] The paper compares DEED to DAT on some vision-language tasks, but does not provide a comprehensive comparison.
- Why unresolved: The paper only provides a limited comparison with DAT, and does not explore how DEED performs compared to other dynamic early exit approaches on a wider range of tasks.
- What evidence would resolve it: A comprehensive comparison of DEED with other dynamic early exit approaches on a variety of vision-language tasks would provide a clearer picture of its relative performance.

### Open Question 3
- Question: How does the performance of DEED scale with the size of the model and the dataset?
- Basis in paper: [explicit] The paper evaluates DEED on two state-of-the-art encoder-decoder transformer models and various vision-language tasks, but does not explore how its performance scales with model size or dataset size.
- Why unresolved: The paper does not provide any insights into how the performance of DEED changes as the model or dataset size increases.
- What evidence would resolve it: A study exploring the performance of DEED on models and datasets of varying sizes would help understand its scalability.

## Limitations

- Confidence-based early exit relies on the assumption that confidence scores reliably indicate prediction certainty, which is not thoroughly validated
- The just-in-time computation algorithm lacks detailed implementation specifications for proper reproduction
- Evaluation focuses primarily on vision-language tasks, limiting generalizability to other encoder-decoder applications

## Confidence

**High Confidence**: The architectural design of shared generation heads with adaptation modules is clearly specified and theoretically sound. The deep supervision training approach is standard practice in multi-exit models.

**Medium Confidence**: The experimental results showing 30-60% latency reduction with comparable or higher accuracy appear robust across multiple datasets and models. However, the exact confidence threshold values and their sensitivity analysis are not fully detailed.

**Low Confidence**: The effectiveness of the just-in-time computation algorithm in resolving semantic misalignment is asserted but not empirically validated against alternative approaches. The generalization of results to non-vision-language tasks remains unproven.

## Next Checks

1. **Confidence Score Reliability**: Conduct an ablation study measuring the correlation between confidence scores and actual prediction accuracy at each decoder layer. Test whether confidence scores predict successful early exits or if they sometimes lead to premature exits with accuracy degradation.

2. **Just-in-Time vs. Alternatives**: Implement and compare the just-in-time computation algorithm against two alternatives: (a) pre-computing all features upfront, and (b) copying shallow features to deeper layers. Measure both semantic alignment quality and computational efficiency.

3. **Threshold Sensitivity Analysis**: Systematically vary the confidence threshold τ across a wide range (0.5 to 0.99) on a representative dataset and plot the accuracy-latency tradeoff curve. Identify the optimal threshold for each task and analyze whether a universal threshold is feasible.