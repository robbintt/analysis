---
ver: rpa2
title: Input margins can predict generalization too
arxiv_id: '2308.15466'
source_url: https://arxiv.org/abs/2308.15466
tags:
- margins
- margin
- constrained
- generalization
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Input margins, which measure the shortest distance from a sample
  to the decision boundary, are not generally predictive of generalization in deep
  neural networks. The paper shows that this limitation can be overcome by constraining
  the search space to high-utility directions identified via PCA.
---

# Input margins can predict generalization too

## Quick Facts
- arXiv ID: 2308.15466
- Source URL: https://arxiv.org/abs/2308.15466
- Reference count: 40
- Key outcome: Input margins, when constrained to high-utility directions identified via PCA, achieve a Kendall rank correlation of 0.66 on average across PGDL tasks, significantly outperforming standard input margins (0.24) and hidden margins (0.51).

## Executive Summary
This paper challenges the prevailing view that input margins are not predictive of generalization in deep neural networks. The authors show that standard input margins fail to predict generalization because they are measured in directions of low utility, often relying on spurious features. By constraining the margin calculation to high-utility directions identified via PCA, the proposed "constrained margins" achieve strong predictive power, with a Kendall rank correlation of 0.66 on average across PGDL benchmark tasks. This method outperforms both standard input margins and hidden margins, demonstrating that input margins can indeed predict generalization when measured appropriately.

## Method Summary
The method involves calculating margins in a PCA-constrained subspace of the input space. First, the training data is normalized and PCA is applied to extract principal components. The number of components is selected using the Kneedle algorithm applied to explained variance. For each model, the DeepFool algorithm is adapted to find the nearest decision boundary in this constrained subspace, using a first-order Taylor approximation for efficiency. The mean constrained margin over 5,000 training samples is then used as a complexity measure to predict generalization.

## Key Results
- Constrained margins achieve an average Kendall rank correlation of 0.66 with test accuracy across PGDL tasks, outperforming standard input margins (0.24) and hidden margins (0.51).
- The method demonstrates superior performance in terms of Conditional Mutual Information (CMI), outperforming several other complexity measures.
- Constrained margins are more robust than hidden margins when comparing models with varying topology, as no per-layer normalization is required.

## Why This Works (Mechanism)

### Mechanism 1
Standard input margins are not generally predictive of generalization in DNNs because they are measured in directions of low utility, often relying on spurious features. The proposed constrained margin method restricts the search for the decision boundary to directions of high utility, identified by PCA on the training data, thereby avoiding spurious features and improving correlation with generalization. Core assumption: Principal components of the training data capture directions of high utility for classification, and directions of low utility (low variance) are more likely to be spurious.

### Mechanism 2
Constrained margins outperform hidden margins because they do not require per-layer normalization and are more robust when comparing models with varying topology. By operating in the input space and using a consistent distance metric (Euclidean in the PCA subspace), constrained margins avoid the complexity of normalizing across different hidden layer representations. Core assumption: The input space margin, when constrained to high-utility directions, captures generalization-relevant information as well as or better than hidden layer margins.

### Mechanism 3
The DeepFool algorithm adapted to the PCA subspace efficiently approximates the constrained margin while maintaining predictive power. The iterative DeepFool approach in the lower-dimensional PCA subspace reduces computational cost compared to exact boundary search while still capturing the relationship between margin and generalization. Core assumption: The first-order Taylor approximation in the PCA subspace is sufficiently accurate to approximate the true margin, and the iterative refinement captures most of the predictive signal.

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to identify high-utility directions in the input space, which are then used to constrain the margin calculation.
  - Quick check question: How does PCA identify directions of maximum variance in the data, and why are these directions considered "high utility" for classification?

- Concept: Classification margins
  - Why needed here: The paper builds on the concept of classification margins (distance to decision boundary) and extends it by constraining the search space.
  - Quick check question: What is the difference between input margins, hidden margins, and output margins in the context of deep neural networks?

- Concept: DeepFool algorithm
  - Why needed here: DeepFool is adapted to work in the PCA-constrained subspace to efficiently approximate the constrained margin.
  - Quick check question: How does DeepFool iteratively find the closest point on the decision boundary, and what is the role of the first-order Taylor approximation in this process?

## Architecture Onboarding

- Component map: Input data preprocessing -> PCA decomposition -> DeepFool margin calculation -> Kendall correlation and CMI evaluation
- Critical path:
  1. Preprocess training data (normalize, compute PCA)
  2. For each model, compute constrained margins on 5000 training samples using DeepFool in PCA subspace
  3. Calculate mean constrained margin as complexity measure
  4. Rank models by mean constrained margin and compare to true generalization using Kendall's rank correlation
- Design tradeoffs:
  - PCA vs. other dimensionality reduction: PCA is computationally efficient and interpretable, but may not capture nonlinear manifolds
  - Number of principal components: More components capture more variance but increase computational cost and may include low-utility directions
  - DeepFool vs. exact boundary search: DeepFool is efficient but relies on first-order approximation, which may be inaccurate for highly nonlinear boundaries
- Failure signatures:
  - Low Kendall's rank correlation between constrained margin and test accuracy
  - High variance in constrained margin across different hyperparameter settings (e.g., network depth)
  - Computational cost significantly higher than expected (e.g., >2 minutes per model)
- First 3 experiments:
  1. Verify PCA captures meaningful variance: Plot explained variance ratio and check if elbow method selects reasonable number of components
  2. Validate DeepFool adaptation: Compare constrained margins using Taylor approximation vs. DeepFool for a small set of models