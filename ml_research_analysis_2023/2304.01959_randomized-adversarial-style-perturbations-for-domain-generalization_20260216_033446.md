---
ver: rpa2
title: Randomized Adversarial Style Perturbations for Domain Generalization
arxiv_id: '2304.01959'
source_url: https://arxiv.org/abs/2304.01959
tags:
- domain
- style
- domains
- rasp
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses domain generalization, which aims to improve
  model robustness on unseen target domains by augmenting training data with novel
  styles. The core method, Randomized Adversarial Style Perturbation (RASP), perturbs
  feature statistics corresponding to style in an adversarial direction toward randomly
  selected target classes, thereby generating challenging examples that improve model
  generalization.
---

# Randomized Adversarial Style Perturbations for Domain Generalization

## Quick Facts
- arXiv ID: 2304.01959
- Source URL: https://arxiv.org/abs/2304.01959
- Reference count: 32
- Primary result: RASP+NFM improves domain generalization accuracy by 1.1-5.4% on DomainNet and 2.5-5.7% on PACS compared to baselines

## Executive Summary
This paper introduces Randomized Adversarial Style Perturbation (RASP) to improve domain generalization by augmenting training data with challenging style variations. RASP perturbs feature statistics in an adversarial direction toward randomly selected target classes, creating diverse and plausible style augmentations. To prevent degradation of source domain knowledge, the authors propose Normalized Feature Mixup (NFM) that mixes normalized features from perturbed and unperturbed paths while preserving the augmented style. Experiments on DomainNet, Office-Home, and PACS datasets demonstrate consistent improvements over baseline and existing methods, particularly on challenging domains.

## Method Summary
RASP augments training data by perturbing feature statistics (mean and std) corresponding to style in an adversarial direction toward a randomly selected target class. This generates diverse style variations that expose the model to challenging examples not present in source domains. NFM addresses the risk of losing source domain knowledge by mixing normalized features from both perturbed and unperturbed paths, then applying the augmented style to the mixed content representation. The method is applied probabilistically to ResNet18/50 residual blocks, with training using SGD and evaluation following DomainBed criteria.

## Key Results
- RASP+NFM achieves 1.1-5.4% accuracy gains on DomainNet and 2.5-5.7% on PACS compared to DeepAll baseline
- Significant improvements on challenging domains (e.g., Clipart, Painting, Sketch in PACS)
- Consistent performance across ResNet18 and ResNet50 architectures
- RASP+NFM outperforms other domain generalization methods including RSC, MetaReg, DDAIG, and CrossGrad

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Style perturbation in feature space improves generalization by exposing the model to adversarial styles not present in source domains
- Mechanism: RASP perturbs feature statistics (mean and std) in an adversarial direction toward a randomly selected target class, forcing the model to learn representations robust to unexpected styles in unseen domains
- Core assumption: Feature statistics captured by instance normalization correspond to domain style, and manipulating these statistics adversarially generates plausible but challenging examples
- Break condition: If the perturbation step size is too large or the stopping threshold too low, the perturbed features may leave the plausible style manifold and degrade learning from source domains

### Mechanism 2
- Claim: Mixing normalized features from perturbed and unperturbed paths preserves source domain knowledge while gaining robustness
- Mechanism: NFM performs Mixup on instance-normalized features (content) from both paths, then denormalizes using the perturbed style
- Core assumption: Mixup on normalized (style-removed) features will blend content representations without undoing the style augmentation
- Break condition: If the mixup ratio is too extreme or applied to denormalized features, the augmented style can be washed out, eliminating the adversarial benefit

### Mechanism 3
- Claim: Randomizing target classes for adversarial attacks ensures style diversity across training iterations
- Mechanism: For each RASP augmentation, a random target class is selected (different from the ground truth), preventing the model from overfitting to a fixed adversarial style direction
- Core assumption: Diversity in adversarial directions is necessary to simulate the variety of domain shifts encountered in unseen domains
- Break condition: If target classes are not randomized, the model may overfit to a narrow set of perturbations, reducing generalization

## Foundational Learning

- Concept: Adversarial training with targeted attacks
  - Why needed here: To generate hard examples that challenge the model's reliance on style cues and improve robustness to domain shifts
  - Quick check question: What is the difference between untargeted and targeted adversarial attacks, and why is targeted chosen here?

- Concept: Instance normalization and feature statistics
  - Why needed here: Feature statistics (mean and std) are used to represent and manipulate style information, which is central to RASP's approach
  - Quick check question: How does instance normalization disentangle style from content in feature maps?

- Concept: Mixup regularization
  - Why needed here: To combine information from perturbed and unperturbed paths without losing the effect of style augmentation
  - Quick check question: Why is Mixup applied to normalized (content) features instead of raw features?

## Architecture Onboarding

- Component map: Input → Backbone (ResNet18/50) → Residual blocks → RASP module (optional) → NFM module (optional) → Classifier
- Critical path: 1) Forward pass through perturbation-free path (baseline), 2) With 0.5 probability, apply RASP before residual block → forward through RASP path, 3) With 0.5 probability, apply NFM after residual block (only if RASP applied), 4) Loss computed from both paths; gradients flow through shared weights
- Design tradeoffs: RASP alone risks overfitting to adversarial styles and forgetting source domain knowledge; NFM mitigates this but adds computational overhead and requires careful balance of mixup ratio
- Failure signatures: Degraded source domain accuracy → NFM not effective or RASP step size too large; Overfitting to adversarial styles → insufficient NFM application or high RASP aggressiveness; Poor target domain performance → insufficient style diversity or implausible perturbations
- First 3 experiments: 1) Baseline ERM without RASP/NFM → establish performance floor, 2) RASP only (no NFM) → measure trade-off between adversarial robustness and source domain accuracy, 3) RASP + NFM with varying mixup ratios → find optimal balance preserving source knowledge while improving target generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of stopping criterion (threshold τ) impact the diversity-plausibility trade-off of generated styles across different domain shift magnitudes?
- Basis in paper: [explicit] The paper states "the threshold is a hyper-parameter that balances the diversity and plausibility of the synthesized styles" and analyzes varying τ values
- Why unresolved: The paper only tests a few discrete τ values and does not explore continuous variations or analyze the threshold's interaction with domain shift severity
- What evidence would resolve it: A comprehensive ablation study varying τ continuously across datasets with different domain shift magnitudes, coupled with quantitative diversity metrics and qualitative style analysis

### Open Question 2
- Question: Can the RASP framework be extended to handle multi-class adversarial perturbations beyond random class selection?
- Basis in paper: [explicit] "RASP selects a target class randomly for augmentation" and compares against "RASP GT" which uses ground-truth-based direction
- Why unresolved: The paper only compares random class selection against ground-truth-based attacks, without exploring other strategies like multi-class or curriculum-based target selection
- What evidence would resolve it: Experiments comparing RASP with different target selection strategies (e.g., hardest-class selection, curriculum learning) across multiple datasets and domain generalization tasks

### Open Question 3
- Question: What is the theoretical relationship between RASP's adversarial perturbation magnitude and the resulting domain generalization performance?
- Basis in paper: [inferred] The paper varies attack iteration count and step size ϵ in experiments, observing their effects on performance, but does not provide theoretical analysis
- Why unresolved: While empirical results show performance changes with perturbation magnitude, the paper does not explain why certain magnitudes work better or provide theoretical bounds
- What evidence would resolve it: Theoretical analysis connecting perturbation magnitude to domain gap size, coupled with empirical validation showing optimal perturbation magnitudes for different domain shift scenarios

## Limitations
- The assumption that instance normalization feature statistics effectively disentangle style from content is not rigorously validated for the specific datasets and models used
- The choice of hyperparameters (τ=0.8, ϵ=2/255, 5 attack iterations) appears heuristic and may not generalize to other settings
- Evaluation focuses on standard benchmarks that may not capture the full range of domain shifts encountered in real-world applications

## Confidence

- High: The core mechanism of RASP (adversarial style perturbations) and its integration with NFM is clearly described and experimentally validated
- Medium: The claim that RASP improves generalization to unseen domains is supported by experimental results, but the underlying assumption about style representation through instance normalization is not fully validated
- Medium: The comparison with existing domain generalization methods is comprehensive, but the exact implementation details of these baselines are not provided, which may affect the fairness of the comparison

## Next Checks

1. Validate the assumption that instance normalization feature statistics effectively disentangle style from content by visualizing perturbed features and their impact on style transfer across domains
2. Perform ablation studies to quantify the contribution of each component (RASP, NFM, random target class selection) to the overall performance improvement, and explore the sensitivity of results to hyperparameter choices
3. Evaluate the method on additional datasets with more diverse and challenging domain shifts (e.g., WILDS, VisDA, or real-world medical imaging datasets) to assess its generalizability beyond standard benchmarks