---
ver: rpa2
title: A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling
  in Text-Image Models
arxiv_id: '2302.06235'
source_url: https://arxiv.org/abs/2302.06235
tags:
- photo
- prompt
- type
- prompts
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of prompt engineering for zero-shot
  image classification with text-image models like CLIP. It proposes a fully automatic
  method called Zero-shot Prompt Ensembling (ZPE) that scores and ensembles prompts
  without needing labeled validation data.
---

# A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models

## Quick Facts
- arXiv ID: 2302.06235
- Source URL: https://arxiv.org/abs/2302.06235
- Authors: 
- Reference count: 40
- Primary result: Achieves 68.61% ImageNet accuracy with CLIP ViT-B/16 using automatic prompt ensembling vs 68.27% for hand-crafted prompts

## Executive Summary
This paper introduces Zero-shot Prompt Ensembling (ZPE), a method for automatically scoring and combining prompts in text-image models like CLIP without requiring labeled validation data. The core innovation is a novel logit normalization scheme that corrects for frequency biases in both pre-training and test data, making prompt scores more reliable. By combining this normalization with softmax weighting and prompt selection, ZPE outperforms both naive equal averaging of prompts and hand-crafted prompt sets on multiple classification benchmarks while remaining fully automatic and interpretable.

## Method Summary
ZPE scores prompts by computing logits between image and text embeddings, then normalizing these logits using expected values from both pre-training data (LAION400m subsample) and the test set itself. The normalization corrects for frequency biases in common words that would otherwise inflate scores regardless of image content. After normalization, softmax weighting is applied to create a probability distribution over prompts, followed by optional prompt selection using outlier detection. The method is optimization-free, requiring only access to a large pool of candidate prompts and the model's pre-training data distribution.

## Key Results
- Achieves 68.61% accuracy on ImageNet with CLIP ViT-B/16 vs 68.27% for hand-crafted prompts
- Outperforms both naive equal averaging and hand-crafted prompt sets on ImageNet variants and 11 fine-grained classification benchmarks
- Demonstrates effectiveness of logit normalization in correcting frequency biases
- Shows that softmax weighting effectively handles long-tail prompt quality distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompt scoring can correct for frequency bias in pre-training data by normalizing logits with respect to expected values from a large reference corpus.
- Mechanism: The method subtracts the expected maximum logit under a reference distribution from the observed maximum logit for each prompt, reducing the impact of common words that inflate scores regardless of image content.
- Core assumption: The reference distribution (LAION400m subsample) is sufficiently representative of the pre-training distribution to capture frequency bias effects.
- Evidence anchors:
  - [abstract]: "we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring method that corrects for the biases."
  - [section]: "Given a pair of a test image and a prompt, we compare the maximum logit for the pairlogits(test img, prompt) with the expected maximum logit for a random image with the same prompt logits(random img, prompt)."
  - [corpus]: Weak - the related papers focus on entropy-based weighting or few-shot ensembling, not frequency bias correction through normalization.
- Break condition: If the reference corpus distribution differs significantly from the actual pre-training distribution, the normalization will mis-correct and potentially harm performance.

### Mechanism 2
- Claim: Zero-shot prompt scoring can correct for spurious concept frequency bias in test data by normalizing logits with respect to expected values from the test set itself.
- Mechanism: The method subtracts the average logits computed over test images from the observed logits, reducing the impact of common but irrelevant concepts shared across test images.
- Core assumption: Common spurious concepts in test images can be effectively captured by averaging logits over the test set.
- Evidence anchors:
  - [abstract]: "we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in both training and test data"
  - [section]: "We solve spurious concept frequency bias subtracting the expected logits for the images in the test data itselfEtest = Eimg~Dtest [zimgÂ· ztxt]"
  - [corpus]: Missing - no direct evidence in corpus about test-set-based normalization for bias correction.
- Break condition: If the test set contains insufficient diversity or the spurious concepts are not common enough to be captured by averaging, the normalization will be ineffective.

### Mechanism 3
- Claim: Softmax weighting of prompt scores can effectively handle the long-tail distribution of prompt quality, preventing irrelevant prompts from collectively degrading performance.
- Mechanism: Applying softmax to the prompt scores creates a probability distribution over prompts, ensuring that a small number of high-quality prompts dominate while preventing the accumulation of negative impact from many low-quality prompts.
- Core assumption: The distribution of prompt scores exhibits a long-tail pattern where most prompts are of low quality.
- Evidence anchors:
  - [abstract]: "we observe a long tail behaviour, where a small number of prompts have large scores, but most prompts are 'bad' and have small scores"
  - [section]: "When scoring a large number of prompts, we observe a long tail behaviour, where a small number of prompts have large scores, but most prompts are 'bad' and have small scores"
  - [corpus]: Weak - related work mentions ensembling but doesn't specifically address long-tail prompt quality distributions.
- Break condition: If the prompt score distribution is not long-tailed (e.g., many prompts are of similar quality), softmax weighting may unnecessarily compress the score differences.

## Foundational Learning

- Concept: Contrastive learning and text-image embedding spaces
  - Why needed here: Understanding how CLIP and similar models create joint text-image representations is essential for grasping why prompt ensembling works and how logit normalization corrects biases.
  - Quick check question: What is the relationship between the inner product of normalized embeddings and the distance between them?

- Concept: Logit normalization and bias correction
  - Why needed here: The core innovation relies on normalizing raw scores by subtracting expected values to correct for frequency biases.
  - Quick check question: Why does subtracting expected logits help reduce overconfidence due to frequent words?

- Concept: Prompt engineering in zero-shot learning
  - Why needed here: Understanding how different prompts affect model performance is crucial for appreciating why automatic prompt weighting is valuable.
  - Quick check question: How does prompt ensembling improve zero-shot classification performance compared to using a single prompt?

## Architecture Onboarding

- Component map:
  Text encoder (T) -> Image encoder (I) -> Logit computation -> Prompt scoring with normalization -> Softmax weighting -> Optional prompt selection

- Critical path:
  1. Compute embeddings for all images and prompts
  2. Calculate raw logits and maximum logits per prompt
  3. Normalize logits using Epretrain and Etest expectations
  4. Compute prompt scores from normalized logits
  5. Apply softmax weighting to scores
  6. Combine weighted logits for final prediction

- Design tradeoffs:
  - Using LAION400m subsample vs. actual CLIP pre-training data for Epretrain estimation
  - Image-only vs. image-and-class normalization for Epretrain
  - Weighted average vs. prompt selection approaches
  - Fixed threshold vs. adaptive outlier detection for prompt selection

- Failure signatures:
  - Degraded performance on datasets with different domain characteristics than the reference corpus
  - Poor performance when spurious concepts are not common across test images
  - Suboptimal results when prompt quality distribution is not long-tailed
  - Sensitivity to the choice of normalization parameters

- First 3 experiments:
  1. Verify that naive max-logit scoring produces overconfident results on prompts with frequent words
  2. Test Epretrain normalization alone vs. combined Epretrain+Etest normalization on a controlled dataset
  3. Compare softmax weighting vs. raw scores on a dataset with known long-tail prompt quality distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ZPE change when using a much larger pool of prompts, especially ones generated by advanced language models like GPT-4?
- Basis in paper: [explicit] The paper mentions using ChatGPT to generate 179 additional prompts, increasing the pool to 426. It also notes that the quality of the pool is important, with the 80-prompt pool performing well on ImageNet but less well on fine-grained datasets, while the 179 ChatGPT-generated prompts in the 426-prompt pool seem to bring the quality down.
- Why unresolved: The paper only tested a relatively small increase in the prompt pool size (from 247 to 426). It's unclear how ZPE would perform with a significantly larger pool, such as one generated by a more advanced language model like GPT-4.
- What evidence would resolve it: Experiments comparing ZPE's performance on various datasets using prompt pools of different sizes and qualities, including those generated by advanced language models.

### Open Question 2
- Question: Can ZPE be adapted to score and weight prompt combinations, rather than treating prompts independently?
- Basis in paper: [explicit] The paper states, "Scoring combinations of prompts could yield further gains," and "ZPE assumes access to a large and varied pool of high-quality prompt templates, which to the best of our knowledge has not been collected."
- Why unresolved: The current ZPE algorithm scores prompts independently, without considering potential synergies or conflicts between different prompts. Exploring how to score and weight combinations of prompts could potentially lead to improved performance.
- What evidence would resolve it: Development and testing of an extension to ZPE that can score and weight combinations of prompts, along with experiments comparing its performance to the original ZPE algorithm.

### Open Question 3
- Question: How does ZPE perform when applied to other types of models beyond CLIP and LiT, such as those trained on different modalities or tasks?
- Basis in paper: [explicit] The paper mentions investigating the sensitivity of ZPE to the architecture of the underlying text-image model, but only tests CLIP and LiT models.
- Why unresolved: While the paper shows that ZPE improves performance on CLIP and LiT models, it's unclear how well it would generalize to other types of models, such as those trained on different modalities (e.g., audio, text) or for different tasks (e.g., object detection, segmentation).
- What evidence would resolve it: Experiments applying ZPE to a diverse set of models, including those trained on different modalities and tasks, and comparing their performance to baselines.

## Limitations

- Modest absolute gains over hand-crafted prompts (68.61% vs 68.27% on ImageNet)
- Requires access to large reference corpus (LAION400m subsample) which may not be available in all deployment scenarios
- Effectiveness on datasets with significantly different domain characteristics than ImageNet remains untested
- Performance depends on quality of prompt pool, with automatically generated prompts potentially degrading results

## Confidence

High confidence: The core mechanism of logit normalization to correct for frequency biases in pre-training data, the observation of long-tail prompt quality distributions, and the general effectiveness of softmax weighting for prompt ensembling.

Medium confidence: The specific effectiveness of test-set-based normalization (Etest) for spurious concept bias correction, and the optimal threshold values for prompt selection.

Low confidence: Performance on datasets with significantly different characteristics than ImageNet and its variants, and the method's robustness to different pre-training distributions than the LAION400m reference corpus.

## Next Checks

1. Test the method on datasets with known spurious concept biases (like certain medical imaging datasets) to verify Etest normalization effectiveness
2. Evaluate performance degradation when using a reference corpus that differs significantly from the actual pre-training distribution
3. Compare against state-of-the-art prompt engineering methods that also address frequency bias to isolate the contribution of the specific normalization scheme