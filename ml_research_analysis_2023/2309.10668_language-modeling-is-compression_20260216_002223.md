---
ver: rpa2
title: Language Modeling Is Compression
arxiv_id: '2309.10668'
source_url: https://arxiv.org/abs/2309.10668
tags:
- compression
- data
- coding
- language
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that language models, when viewed as predictive
  models, can serve as powerful general-purpose compressors. By applying arithmetic
  coding to large transformer-based models like Chinchilla, it demonstrates that these
  models can achieve state-of-the-art compression rates not only on text but also
  on image and audio data, outperforming domain-specific compressors like PNG and
  FLAC.
---

# Language Modeling Is Compression

## Quick Facts
- arXiv ID: 2309.10668
- Source URL: https://arxiv.org/abs/2309.10668
- Reference count: 10
- Key outcome: Large language models can serve as general-purpose compressors, achieving state-of-the-art compression rates across text, image, and audio data.

## Executive Summary
This work demonstrates that language models, when used as predictive models with arithmetic coding, can serve as powerful general-purpose compressors. By applying arithmetic coding to large transformer-based models like Chinchilla, the authors show that these models can achieve state-of-the-art compression rates not only on text but also on image and audio data, outperforming domain-specific compressors. The paper also provides new insights into scaling laws, showing that beyond a certain point, increasing model size can degrade compression performance due to the cost of encoding model parameters.

## Method Summary
The method uses arithmetic coding with large language models (e.g., Chinchilla) as predictive models to compress data. Datasets (enwik9, ImageNet patches, LibriSpeech) are chunked into sequences and fed into the model, which outputs probability distributions over next symbols. Arithmetic coding converts these probabilities into compressed bitstreams. Both raw compression rates (compressed size / raw size) and adjusted rates (accounting for model parameter size) are measured and compared to domain-specific and general-purpose compressors.

## Key Results
- Chinchilla 70B compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, outperforming PNG (58.5%) and FLAC (30.3%).
- Tokenization improves prediction accuracy but does not necessarily improve compression rates.
- Beyond a certain model size, adjusted compression rates degrade due to parameter encoding overhead.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Arithmetic coding turns probabilistic predictions into optimal lossless compression.
- Mechanism: Given a model's probability distribution over symbols, arithmetic coding maps each symbol to a sub-interval whose size is proportional to that symbol's probability. As symbols are processed, the interval narrows, and the final binary representation is derived from this interval. The resulting code length is approximately `-log2 P(x)`, matching the theoretical minimum.
- Core assumption: The model provides accurate probability estimates for each symbol conditioned on context.
- Evidence anchors:
  - [abstract] "Arithmetic coding, in particular, is known to be optimal in terms of coding length, meaning that the overall compression performance depends on the capabilities of the probabilistic model."
  - [section] "Arithmetic coding transforms a prediction model into a compressor, and, conversely, a compressor can be transformed into a predictor by using the coding lengths to construct probability distributions."
- Break condition: If the model's probability estimates are poor or if the coding precision is insufficient, the theoretical optimality degrades.

### Mechanism 2
- Claim: Foundation models compress across modalities by leveraging in-context learning.
- Mechanism: Trained primarily on text, large language models can adapt their predictions to non-text data (images, audio) without retraining. Arithmetic coding uses these predictions to compress, achieving competitive rates by conditioning the model to the task at hand.
- Core assumption: The model's internal representations are sufficiently general to handle diverse data modalities when given in-context examples.
- Evidence anchors:
  - [abstract] "Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively."
  - [section] "Chinchilla models, while trained primarily on text, also appear to be general-purpose compressors, as they outperform all other compressors, even on image and audio data."
- Break condition: If the context length is insufficient to capture the relevant patterns, or if the model lacks exposure to analogous structures in pretraining data.

### Mechanism 3
- Claim: Scaling laws for compression differ from log-loss scaling due to parameter encoding cost.
- Mechanism: While log-loss decreases monotonically with model size, the adjusted compression rate includes the size of the model parameters. For small datasets, beyond a certain model size, the overhead of encoding parameters outweighs gains in predictive performance, causing the adjusted rate to increase.
- Core assumption: The dataset size is finite and the model's parameter count grows with model size.
- Evidence anchors:
  - [abstract] "it highlights that tokenization, while improving prediction performance, does not necessarily enhance compression rates."
  - [section] "Scaling beyond a certain point will deteriorate the compression performance since the model parameters need to be accounted for in the compressed output."
  - [corpus] "Weak: The corpus does not directly discuss scaling laws for compression, though related work on learned compression is present."
- Break condition: If the dataset is extremely large (e.g., orders of magnitude larger than model parameters), parameter encoding overhead becomes negligible.

## Foundational Learning

- Concept: Shannon's source coding theorem
  - Why needed here: It establishes the theoretical link between probabilistic models and lossless compression, underpinning the entire arithmetic coding approach.
  - Quick check question: Why does maximizing log-likelihood correspond to minimizing expected code length?
- Concept: Arithmetic coding mechanics
  - Why needed here: It is the core algorithm for turning model probabilities into compressed bitstreams, and understanding it is essential for implementation.
  - Quick check question: How does the interval narrowing process guarantee that the output code represents the original sequence?
- Concept: In-context learning in transformers
  - Why needed here: It explains how large language models adapt to compress data outside their training distribution without gradient updates.
  - Quick check question: What limits the length of data that can be compressed in one pass with a transformer?

## Architecture Onboarding

- Component map: Tokenizer (optional) -> Predictive model (e.g., Chinchilla 70B) -> Arithmetic coding engine -> Model parameter store
- Critical path:
  1. Tokenize input (if using tokenizer)
  2. Feed token sequence into model
  3. Extract probability distribution for next symbol
  4. Update arithmetic coding interval
  5. Repeat until all symbols processed
- Design tradeoffs:
  - Model size vs. compression rate: larger models compress better but increase overhead in adjusted rate
  - Context length: longer context improves prediction but increases latency and memory use
  - Tokenization: improves prediction accuracy but may not improve compression rate
- Failure signatures:
  - Degraded compression rates on long sequences: likely context length limitation
  - Numerical instability in arithmetic coding: insufficient precision or underflow
  - Model too large for dataset: adjusted compression rate increases beyond optimal model size
- First 3 experiments:
  1. Verify arithmetic coding works on synthetic data with known probabilities.
  2. Test compression of small text dataset with and without tokenizer.
  3. Measure compression rates on enwik9 with different model sizes to observe scaling law effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal model size for compression change when evaluating on larger datasets (e.g., petabytes vs. terabytes)?
- Basis in paper: [explicit] The paper discusses scaling laws and optimal model size for datasets up to 1GB, showing that larger datasets allow for larger models before the adjusted compression rate starts to increase.
- Why unresolved: The paper only evaluates datasets up to 1GB, while the theoretical discussion suggests optimal model size is tied to dataset size. The behavior at truly large scales (petabytes) remains unexplored.
- What evidence would resolve it: Empirical results showing compression rates and optimal model sizes across datasets of varying scales (e.g., 1GB, 1TB, 1PB) would clarify the scaling behavior.

### Open Question 2
- Question: Does tokenization improve compression rates for larger models trained on diverse datasets?
- Basis in paper: [explicit] The paper shows that tokenization does not necessarily improve compression rates but helps models pack more information into their context. However, the effect on larger models trained on diverse datasets is not explored.
- Why unresolved: The experiments focus on smaller models trained on a single dataset (enwik8), leaving the impact of tokenization on larger, more diverse models unclear.
- What evidence would resolve it: Comparative experiments with large models (e.g., Chinchilla 70B) trained on diverse datasets using different tokenization strategies would provide insights.

### Open Question 3
- Question: How does the compression performance of foundation models generalize to other data modalities beyond text, images, and audio?
- Basis in paper: [explicit] The paper demonstrates that foundation models achieve strong compression rates on text, images, and audio, but does not explore other modalities like video or structured data.
- Why unresolved: The experiments are limited to three data modalities, and the generalizability of foundation models to other types of data remains untested.
- What evidence would resolve it: Compression experiments on additional data modalities (e.g., video, time series, or structured data) would reveal the breadth of foundation models' compression capabilities.

## Limitations

- The exact scaling behavior of compression performance beyond tested models (Chinchilla 70B) is not explored.
- The role of tokenization is shown to be non-monotonic with respect to compression, but the underlying reasons are not fully explained.
- Experiments are limited to three modalities (text, image, audio) and specific datasets; generalization to other data types is not demonstrated.
- Implementation details of arithmetic coding (numerical stability, precision) are not fully specified.

## Confidence

- High confidence: The core claim that language models, when used as predictive models with arithmetic coding, can achieve state-of-the-art compression rates is well-supported by both theory and experimental results.
- Medium confidence: The claim about scaling laws—that increasing model size can eventually degrade adjusted compression performance due to parameter encoding costs—is logically sound and supported by the data, but the exact threshold is not fully characterized.
- Medium confidence: The assertion that tokenization does not necessarily improve compression rates is supported by experimental evidence, but the underlying reasons are not fully explored.

## Next Checks

1. **Scaling Law Validation**: Conduct experiments compressing a range of dataset sizes (small to large) with models of varying parameter counts to precisely map the scaling behavior and identify the optimal model size for different dataset regimes.
2. **Tokenization Impact Analysis**: Systematically test compression with different tokenization strategies (byte-level, word-level, subword) on the same datasets to determine which scenarios benefit from tokenization and which do not, and why.
3. **Cross-Modality Generalization**: Apply the same compression methodology to additional data modalities (e.g., video, scientific data) and datasets to evaluate the generality of the observed compression performance and scaling laws.