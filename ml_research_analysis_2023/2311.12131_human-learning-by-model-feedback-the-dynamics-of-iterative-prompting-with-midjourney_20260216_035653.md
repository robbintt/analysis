---
ver: rpa2
title: 'Human Learning by Model Feedback: The Dynamics of Iterative Prompting with
  Midjourney'
arxiv_id: '2311.12131'
source_url: https://arxiv.org/abs/2311.12131
tags:
- prompts
- prompt
- words
- image
- threads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the dynamics of user prompts during iterative
  interactions with Midjourney, a text-to-image generation model. The authors compile
  a dataset of 107,051 interaction threads from the Midjourney Discord server and
  examine how prompts change across iterations.
---

# Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney

## Quick Facts
- arXiv ID: 2311.12131
- Source URL: https://arxiv.org/abs/2311.12131
- Reference count: 40
- Primary result: Prompts converge toward model-preferred linguistic patterns during iterative refinement with Midjourney

## Executive Summary
This paper analyzes how user prompts evolve during iterative interactions with Midjourney, a text-to-image generation model. By examining 107,051 interaction threads from the Midjourney Discord server, the authors identify predictable convergence patterns in prompts across iterations, including increases in length, magic words ratio, and sentence complexity. The analysis reveals that this convergence stems from users either adding missing details or adapting their language to match the model's preferences, raising concerns about using such data for future model training due to potential bias toward specific model preferences rather than natural human expression.

## Method Summary
The authors compiled a dataset of 107,051 interaction threads from Midjourney Discord, extracted linguistic features from prompts (length, magic words ratio, perplexity, repeated words ratio, sentence rate, tree depth), and performed statistical analysis to identify convergence patterns. They used thread segmentation to isolate single-scene interactions, applied Mann-Whitney U tests to detect significant differences between upscaled and non-upscaled prompts, and trained classifiers to predict user satisfaction. The methodology involved scraping prompts, splitting them into threads using intersection-over-union methods, extracting features, and analyzing dynamics through statistical tests and classification experiments.

## Key Results
- Prompts converge toward specific linguistic traits along iterations, including increased length, magic words ratio, and syntactic complexity
- Classification models can distinguish between upscaled and non-upscaled prompts with 55.6% accuracy for images and 58.2% for text, both above random chance
- All analyzed features except concreteness showed statistically significant differences between upscaled and non-upscaled prompts
- Convergence patterns suggest both user-driven detail addition and model-driven language adaptation mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompts converge toward model-preferred linguistic patterns during iterative refinement.
- **Mechanism:** Human users observe that certain prompt styles (e.g., more magic words, lower perplexity, repeated words) yield better images, and adapt their language accordingly. This creates a feedback loop where model preferences shape human input over time.
- **Core assumption:** The model's internal preference for certain language styles can be inferred by the user through image quality feedback, and users are willing to adapt their natural expression to match it.
- **Evidence anchors:**
  - [abstract] "prompts predictably converge toward specific traits along these iterations... adaptation to the model's 'preferences'"
  - [section 6.2] "all the features excluding the concreteness found to be significant"
  - [corpus] Weak — the cited papers discuss prompt optimization and personalization, but none directly measure convergence toward model-preferred patterns.
- **Break condition:** If users prioritize expressing their true intent over image quality, or if the model's output becomes too ambiguous to signal preference, convergence halts.

### Mechanism 2
- **Claim:** Iterative refinement reveals omitted details, driving prompt expansion and complexity.
- **Mechanism:** When a generated image lacks expected details, users incrementally add them in subsequent prompts, increasing length, sentence rate, and syntactic depth.
- **Core assumption:** Users have a clear mental image of the target scene and can identify missing components from the generated output.
- **Evidence anchors:**
  - [abstract] "human users realizing they missed important details"
  - [section 7] "when users input a prompt... they may realize that their original prompt lacked some important details"
  - [corpus] Weak — neighbor papers focus on aesthetic preference or prompt refinement, not on identifying and filling omitted details.
- **Break condition:** If the model's output becomes saturated with details, or if users can't discern what is missing, further refinement stalls.

### Mechanism 3
- **Claim:** Statistical differences between upscaled and non-upscaled prompts enable classification models to predict user satisfaction.
- **Mechanism:** Certain linguistic features (length, magic words ratio, perplexity, etc.) statistically correlate with higher user satisfaction, allowing both image and text classifiers to distinguish between upscaled and non-upscaled cases above random chance.
- **Core assumption:** There exists a measurable signal in the prompt that reflects its effectiveness in producing a satisfying image.
- **Evidence anchors:**
  - [abstract] "prompts predictably converge toward specific traits"
  - [section 6.1] "We get an accuracy of 55.6%... 5.6 points above random"
  - [corpus] Moderate — the cited papers on aesthetic preference and prompt optimization provide related evidence, though not direct classifier results.
- **Break condition:** If user satisfaction depends on factors outside the prompt (e.g., mood, context), or if the model's mapping from prompt to image becomes too stochastic, the predictive signal weakens.

## Foundational Learning

- **Concept:** Text-to-image generation via iterative prompting.
  - **Why needed here:** Understanding how users interact with Midjourney requires grasping the feedback loop between prompt and output.
  - **Quick check question:** If a user sees a generated image missing a red hat, what might they do next? *(Add "red hat" to the prompt.)*

- **Concept:** Linguistic feature extraction (length, perplexity, magic words, etc.).
  - **Why needed here:** These features quantify how prompts change over iterations and reveal convergence patterns.
  - **Quick check question:** If a prompt has many magic words and low perplexity, what might that indicate? *(It is closer to the model's preferred style.)*

- **Concept:** Thread segmentation in interaction logs.
  - **Why needed here:** Grouping prompts into threads isolates single-scene interactions for accurate dynamics analysis.
  - **Quick check question:** If two consecutive prompts describe the same scene but with different details, should they be in the same thread? *(Yes, if the change is incremental rather than a new scene.)*

## Architecture Onboarding

- **Component map:** Data scraper -> thread splitter -> feature extractor -> classifier trainer -> dynamics plotter -> convergence analyzer
- **Critical path:** Scraper -> thread splitter -> feature extractor -> classifier trainer (for upscaled vs non-upscaled distinction)
- **Design tradeoffs:** Simple intersection-over-union for thread splitting vs more complex semantic similarity; using pre-trained models vs training from scratch for classification
- **Failure signatures:** Thread splitter misclassifies interleaved threads; classifier fails to generalize beyond training split; feature extractor mislabels magic words
- **First 3 experiments:**
  1. Verify that intersection-over-union thread splitting achieves high F1 compared to manual annotation.
  2. Confirm that linguistic features differ significantly between upscaled and non-upscaled prompts (Mann-Whitney U test).
  3. Plot feature trends over iterations to observe convergence patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the convergence of prompt features toward specific ranges represent an optimal communication strategy between humans and models, or is it a learned artifact of specific model architectures?
- Basis in paper: [explicit] The authors show that prompts converge toward specific trait ranges along iterations and suggest this might be due to either users adding omitted details or adapting to model preferences, but don't distinguish between these possibilities or determine which is more dominant.
- Why unresolved: The paper only provides initial evidence for both possibilities without quantifying their relative contributions or determining whether the convergence represents optimal communication or model-specific adaptation.
- What evidence would resolve it: Experiments comparing prompt convergence patterns across different model architectures, or controlled studies where users interact with models with systematically varied "preferences" to see how prompt features adapt.

### Open Question 2
- Question: How does the observed prompt adaptation process affect the quality and diversity of training data for future model iterations?
- Basis in paper: [explicit] The authors explicitly caution that user adaptation to model preferences raises concerns about reusing user data for further training, as prompts may be biased toward specific model preferences rather than human intentions.
- Why unresolved: While the paper identifies this concern, it doesn't empirically examine the effect of training with such adapted data or quantify how this bias impacts model performance and alignment.
- What evidence would resolve it: Comparative studies training models on different datasets (original vs. adapted prompts), measuring both performance metrics and alignment with human preferences.

### Open Question 3
- Question: Are the observed linguistic feature changes (length, magic words, perplexity, etc.) primarily driven by conscious user strategies or unconscious adaptation processes?
- Basis in paper: [inferred] The paper shows that users systematically adjust features like prompt length and magic word usage along iterations, but doesn't investigate whether users are consciously employing these strategies or adapting unconsciously to model feedback.
- Why unresolved: The analysis focuses on observable patterns rather than user intent or awareness, and the paper doesn't include any measures of user strategy awareness or deliberate optimization attempts.
- What evidence would resolve it: User studies combining interaction logs with surveys or interviews about user awareness and strategies, or experiments manipulating user knowledge about model preferences to observe changes in adaptation patterns.

## Limitations

- The causal relationship between prompt features and model preferences is demonstrated through correlation rather than experimental manipulation
- The classification accuracy (55.6% for images, 58.2% for text) is only marginally above chance, raising questions about signal robustness
- The paper doesn't distinguish between user-driven detail addition and model-driven language adaptation as primary mechanisms

## Confidence

- **Medium** for convergence pattern observations (supported by statistical tests across large dataset)
- **Low** for causal mechanisms explaining why convergence occurs (relies on interpretation rather than experimental manipulation)
- **Medium** for concerns about data reuse bias (logical extrapolation from observed patterns, but not empirically demonstrated)

## Next Checks

1. Conduct controlled experiments where users are randomly assigned to models with known linguistic preferences, then measure whether prompt convergence patterns match those preferences specifically rather than general refinement trends.

2. Perform ablation studies removing individual features (length, magic words, perplexity, etc.) to determine which contribute most to classifier performance and convergence patterns.

3. Test the data reuse concern by training a model on prompts collected from multiple versions of Midjourney and measuring whether convergence patterns shift based on the training corpus's model preferences.