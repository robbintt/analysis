---
ver: rpa2
title: 'Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical
  Studies and Practical Insights'
arxiv_id: '2305.15036'
source_url: https://arxiv.org/abs/2305.15036
tags:
- adapter
- recommendation
- transrec
- item
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores adapter-based transfer learning for recommender
  systems (RS) with different item modalities (text and images). It investigates whether
  adapters can achieve parameter-efficient transfer learning with good performance
  in RS, and compares adapter-based methods to standard fine-tuning approaches.
---

# Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights

## Quick Facts
- arXiv ID: 2305.15036
- Source URL: https://arxiv.org/abs/2305.15036
- Reference count: 40
- Primary result: Adapter tuning achieves comparable performance to full fine-tuning with 97% fewer parameters for text-based recommendation

## Executive Summary
This paper investigates adapter-based transfer learning for recommender systems across different item modalities. The authors benchmark four popular adapter techniques (Houlsby, Pfeiffer, Compacter, K-Adapter) on both text and image-based sequential recommendation tasks. They systematically evaluate factors affecting adapter performance including insertion position, adapter capacity, and tuning strategies. The study demonstrates that adapters can achieve significant parameter efficiency while maintaining competitive performance compared to full fine-tuning, particularly for text-based recommendation tasks.

## Method Summary
The authors adapt transformer-based recommendation models (TransRec) by inserting adapter modules into pre-trained item modality encoders (BERT for text, ViT/MAE for images). Adapters are bottleneck networks that project features to lower dimensions and back, allowing adaptation with minimal parameter updates. The study compares four adapter architectures and evaluates them on both cross-domain and cross-platform transfer learning scenarios. Experiments are conducted using SASRec and CPC frameworks on datasets including MIND, Adressa, H&M, Amazon, and Bili vertical channels.

## Key Results
- For text-based recommendation, adapters achieve comparable performance to full fine-tuning with over 97% parameter reduction
- The Houlsby adapter yields the best overall results, while Pfeiffer offers slightly worse performance with half the parameters
- For image-based recommendation, adapters perform well in cross-domain scenarios but lag in cross-platform settings
- Item encoder adaptation is more critical than user encoder adaptation for successful transfer learning

## Why This Works (Mechanism)

### Mechanism 1
Adapter tuning achieves parameter-efficient transfer learning in text-based TransRec with performance comparable to full fine-tuning. Adapters act as bottleneck networks inserted into transformer blocks, projecting intermediate features to a lower dimension and back, allowing adaptation with minimal parameter updates. Core assumption: The pre-trained item modality encoder (e.g., BERT) has learned general feature representations that can be effectively adapted via small task-specific adapters without modifying core weights. Evidence: For text-based RS, adapters achieve comparable performance to fine-tuning with over 97% parameter reduction. Break condition: If the source domain data differs too much from the target domain, or if the adapter capacity (hidden dimension) is too small to capture necessary adaptation.

### Mechanism 2
For visual TransRec, adapter tuning performs well in cross-domain but lags in cross-platform transfer learning. Adapters can adapt general visual features learned from pre-training to new item distributions within the same platform, but struggle when the visual style and content distribution differ significantly across platforms. Core assumption: Visual features are more sensitive to domain shifts than textual features, requiring larger adapter capacity or full fine-tuning for cross-platform adaptation. Evidence: For image-based RS, adapters perform well in cross-domain scenarios but lag behind in cross-platform settings. Break condition: If the visual domain gap is too large, adapter tuning will underperform compared to full fine-tuning regardless of adapter design.

### Mechanism 3
The item encoder is more critical than the user encoder for successful adapter-based adaptation in TransRec. Since item modality features are domain-specific and vary across datasets, the item encoder requires more adaptation to capture new item characteristics, while the user encoder can often reuse knowledge from pre-training. Core assumption: User behavior patterns are more transferable across domains than item content representations, especially in visual domains. Evidence: First, we can clearly see that AdaTitem outperforms AdaTuser by a large margin in all experimental settings. Break condition: If the user behavior patterns are domain-specific (e.g., different platform usage patterns), adapter-only user encoder might become necessary.

## Foundational Learning

- **Concept**: Transformer architecture and self-attention mechanism
  - Why needed here: TransRec uses transformer-based user and item encoders, and understanding how adapters interact with these blocks is crucial
  - Quick check question: How does the residual connection in the Houlsby adapter preserve original transformer behavior while adding adaptation capacity?

- **Concept**: Pre-training and fine-tuning paradigm
  - Why needed here: TransRec relies on pre-trained item modality encoders (BERT, ViT) and needs to understand how adapter tuning differs from standard fine-tuning
  - Quick check question: What is the key difference between updating all transformer parameters versus only updating adapter parameters during adaptation?

- **Concept**: Cross-domain vs cross-platform transfer learning
  - Why needed here: The paper distinguishes between these two scenarios, especially for visual recommendation, and understanding the differences is crucial for interpreting results
  - Quick check question: Why might adapter tuning work well for cross-domain but not cross-platform transfer in visual recommendation?

## Architecture Onboarding

- **Component map**: Item encoder (BERT/RoBERTa/ViT/MAE) → dimension transformation layer → user encoder (SASRec/CPC) → dot-product similarity for recommendation. Adapters are inserted after MHA and FFN layers in both encoders.

- **Critical path**: Pre-training → Adapter insertion in both user and item encoders → Adaptation to target domain via adapter tuning → Evaluation using HR@10 and NDCG@10 metrics.

- **Design tradeoffs**: Parameter efficiency (adapters) vs performance (full fine-tuning), adapter insertion position (after MHA vs FFN), adapter capacity (hidden dimension size), LayerNorm tuning inclusion.

- **Failure signatures**: Performance drops when adapter capacity is too small, when only item or only user encoder is adapted, when LayerNorm is not tuned (though paper says it doesn't matter much), or when cross-platform visual domain shift is too large.

- **First 3 experiments**:
  1. Compare adapter tuning vs full fine-tuning on text-based TransRec using MIND→Adressa transfer
  2. Test different adapter positions (after MHA only vs after both MHA and FFN) using SASRec+BERT
  3. Evaluate adapter tuning on visual cross-domain vs cross-platform transfer using H&M→Amazon and Bili_MC→Bili_F datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does adapter-based transfer learning perform in multimodal recommendation scenarios combining both text and image features? Basis: The authors explicitly mention this as a future direction: "Then, we are also interested in investigating the effects of AdaT for multimodal (i.e., both text and image) TransRec." Why unresolved: The paper only tested adapters separately for text and image modalities, not combined. What evidence would resolve it: Experimental results comparing adapter performance on recommendation tasks using both text and image features simultaneously, versus using single modalities.

### Open Question 2
Can parameter-efficient adapters significantly reduce training time and computational costs in practice for TransRec models? Basis: The authors state: "Given that most typical AdaT does not help to speed up the training process in practice (including for NLP and CV tasks), it is important to explore effective optimization techniques to reduce the computational cost and time for TransRec through end-to-end training of item modality encoders." Why unresolved: The paper focuses on parameter efficiency but doesn't provide empirical evidence about training speed or computational cost reduction. What evidence would resolve it: Comparative experiments measuring training time, GPU memory usage, and computational costs between full fine-tuning and adapter-based approaches.

### Open Question 3
What is the optimal number and size of trainable parameters for adapter-based TransRec models? Basis: The authors discuss the importance of trainable parameters (TP) and show performance varies with adapter size, but don't determine optimal values: "One key finding is that the adapter's trainable parameter size, insertion positions, and information flow directions are all key factors for the recommendation task." Why unresolved: The paper shows TP matters but doesn't establish specific guidelines for optimal parameter counts. What evidence would resolve it: Systematic experiments determining the sweet spot for adapter size that balances parameter efficiency with recommendation accuracy across different recommendation tasks and modalities.

## Limitations
- Cross-platform visual transfer results may not generalize to other visual domains beyond fashion and video
- The comparison between adapter tuning and full fine-tuning is limited to the specific TransRec architecture
- The study focuses on sequential recommendation, so results may not transfer to non-sequential or point-wise recommendation tasks

## Confidence

- **High confidence**: Adapter tuning achieves significant parameter reduction (97%) with comparable performance to full fine-tuning for text-based recommendation across multiple datasets
- **Medium confidence**: The Houlsby adapter consistently outperforms other adapter variants, though the performance gap varies by dataset and modality
- **Medium confidence**: Item encoder adaptation is more critical than user encoder adaptation, but this may depend on the specific pre-training task and dataset characteristics

## Next Checks

1. Test adapter tuning on a third item modality (e.g., audio or multimodal) to validate generalizability beyond text and images
2. Compare adapter performance against other parameter-efficient methods like LoRA and prefix tuning on the same TransRec architecture
3. Conduct ablation studies on the pre-training objectives used for the item encoders to determine how much adapter performance depends on specific pre-training tasks