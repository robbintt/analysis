---
ver: rpa2
title: On the improvement of model-predictive controllers
arxiv_id: '2308.15157'
source_url: https://arxiv.org/abs/2308.15157
tags:
- control
- data
- prediction
- system
- shown
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This article investigates the relationship between the prediction
  accuracy of a model-predictive controller (MPC) and the quality of the controller
  as a whole. The authors compare a deep neural network (DNN)-based prediction model
  (PM) to an optimal baseline PM for three control problems of varying complexity.
---

# On the improvement of model-predictive controllers

## Quick Facts
- arXiv ID: 2308.15157
- Source URL: https://arxiv.org/abs/2308.15157
- Reference count: 26
- One-line primary result: Prediction errors in DNN-based PMs are the sole source of control quality loss in ML-MPCs.

## Executive Summary
This paper investigates the relationship between prediction accuracy and overall control quality in model-predictive controllers (MPCs) using deep neural networks (DNNs) as prediction models. The authors compare DNN-based MPCs (ML-MPCs) to a perfect baseline MPC (C-MPC) that accesses the simulation directly, across three control problems of increasing complexity. Their key finding is that any performance gap between ML-MPC and C-MPC is entirely due to prediction errors in the DNN, not other components of the control system. This suggests that improving prediction accuracy will directly improve control quality, allowing researchers to focus on ML model improvements without redesigning the MPC architecture.

## Method Summary
The study uses three synthetic control problems (pendulum, cartpole, and three-tank system) with varying complexity. DNN-based prediction models are trained on historical data from these systems, with state correction implemented to reset predictions to true system states before each optimization step. A genetic algorithm serves as the non-linear optimizer in the MPC loop. The authors compare ML-MPC performance against a C-MPC baseline that has perfect prediction accuracy by accessing the simulation directly. Training data is prepared using lookback packages containing current and previous time steps.

## Key Results
- Prediction errors in the DNN-based PM are the sole source of control quality loss in ML-MPCs
- The performance gap between ML-MPC and C-MPC scales directly with prediction error magnitude
- State correction effectively prevents error accumulation across optimization steps

## Why This Works (Mechanism)

### Mechanism 1
Prediction error in the DNN-based PM is the sole source of control quality loss. The DNN predicts future states, which feed into a non-linear optimization algorithm that selects actions. If predictions are wrong, the optimization picks suboptimal actions, degrading control performance. Core assumption: State correction effectively aligns DNN predictions with true system states between control iterations. Evidence anchors: The baseline PM achieves perfect accuracy by accessing the simulation directly; state correction was shown to be effective, making prediction errors the result of inherent DNN errors. Break condition: If state correction fails or is imperfect, prediction errors accumulate across optimization steps and become a secondary source of control degradation.

### Mechanism 2
Improving DNN prediction accuracy directly improves control quality without changing MPC architecture. The DNN's output is fed directly into the MPC's optimization loop. A more accurate prediction yields a better optimization objective value, leading to better control actions. Core assumption: The optimization algorithm is robust to small prediction errors and the control loop compensates for remaining inaccuracies. Evidence anchors: The paper argues that improvement of the PM will always improve the controller as a whole; any proven better network prediction directly improves any ML-MPC. Break condition: If the optimization is too sensitive to prediction errors or the control loop cannot compensate, further accuracy gains may have diminishing returns.

### Mechanism 3
Separating ML prediction improvement from MPC architecture research accelerates progress. By isolating prediction accuracy as the bottleneck, researchers can focus on ML model improvements (e.g., better architectures, training) without redesigning the control loop. Core assumption: The control loop (MPC + optimization) is sufficiently general and not the limiting factor. Evidence anchors: The problem of finding good ML-MPCs can be split into sub-problems for future research; separating issues into categories of machine learning, control, and non-linear optimization. Break condition: If the MPC architecture itself becomes a bottleneck (e.g., cannot handle certain types of prediction errors), then separation alone won't yield improvements.

## Foundational Learning

- Concept: Model Predictive Control (MPC) fundamentals
  - Why needed here: MPC is the core control strategy being evaluated; understanding its prediction-optimization loop is essential.
  - Quick check question: What are the two main phases in each MPC control iteration?

- Concept: Deep Neural Networks (DNN) for system identification
  - Why needed here: The DNN acts as the internal model predicting future states; its training and accuracy are central to the study.
  - Quick check question: What type of data is used to train the DNN, and how is it formatted?

- Concept: State correction in predictive control
  - Why needed here: Ensures the DNN's predictions are anchored to the true system state, preventing error accumulation across optimization steps.
  - Quick check question: How does the paper reset the DNN's internal state before each optimization step?

## Architecture Onboarding

- Component map: Controlled System (CS) → Prediction Model (PM) → Optimizer → Controller loop
- Critical path: CS output → PM prediction → Optimization → Action → CS input
- Design tradeoffs:
  - DNN prediction horizon vs. computational cost
  - State correction frequency vs. prediction accuracy over long horizons
  - Optimization algorithm complexity vs. real-time feasibility
- Failure signatures:
  - Systematic drift in control performance over time (prediction error accumulation)
  - High variance in control quality across runs (inconsistent DNN predictions)
  - Long computation times per control step (overly complex optimization)
- First 3 experiments:
  1. Run the ML-MPC and C-MPC side-by-side on a simple pendulum; measure tracking error and control effort.
  2. Vary the DNN's prediction horizon and observe the impact on control performance.
  3. Introduce noise into the training data and retrain the DNN; compare control performance to the clean-data baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prediction accuracy of the DNN-based PM affect the overall control performance of the ML-MPC compared to the C-MPC?
- Basis in paper: [explicit] The paper investigates the relationship between prediction accuracy and control quality.
- Why unresolved: The paper only demonstrates that prediction errors lead to control discrepancies but does not quantify the exact impact of prediction accuracy on control performance.
- What evidence would resolve it: A quantitative analysis showing the correlation between prediction accuracy metrics (e.g., mean squared error) and control performance metrics (e.g., tracking error, control cost).

### Open Question 2
- Question: How does the complexity of the control problem affect the prediction accuracy required for effective ML-MPC control?
- Basis in paper: [inferred] The paper uses three control problems of varying complexity but does not explicitly analyze the relationship between problem complexity and prediction accuracy requirements.
- Why unresolved: The experiments show different levels of control performance for different problems, but the paper does not investigate how the complexity of the problem affects the necessary prediction accuracy.
- What evidence would resolve it: A systematic study varying problem complexity and analyzing the corresponding prediction accuracy requirements for maintaining control performance.

### Open Question 3
- Question: How does the prediction horizon length affect the control performance of ML-MPC with DNN-based PMs?
- Basis in paper: [inferred] The paper mentions prediction horizon but does not investigate its effect on control performance.
- Why unresolved: The paper does not explore how the length of future predictions impacts the overall control quality.
- What evidence would resolve it: Experiments varying the prediction horizon length and analyzing its effect on control performance metrics.

### Open Question 4
- Question: How does the state correction process impact the long-term prediction accuracy and control performance of ML-MPC?
- Basis in paper: [explicit] The paper introduces state correction but does not investigate its long-term effects.
- Why unresolved: While the paper shows that state correction removes immediate prediction errors, it does not analyze how this affects long-term prediction accuracy or control performance over extended periods.
- What evidence would resolve it: Long-term experiments comparing control performance with and without state correction over many control iterations.

## Limitations

- State correction effectiveness is asserted but not independently verified
- Results are based on synthetic control problems; real-world systems may introduce additional sources of degradation
- The DNN's prediction horizon and the optimization's tolerance for error are not explored systematically

## Confidence

- High: Prediction error directly impacts control quality (mechanism is well-established)
- Medium: State correction is sufficient to prevent error accumulation (unverified outside the study)
- Low: Separating ML and MPC research will always accelerate progress (overly optimistic generalization)

## Next Checks

1. Test state correction robustness by introducing noise or delays in the correction process
2. Evaluate MPC performance on a real-world system (e.g., a robotic manipulator) to check external validity
3. Systematically vary the DNN's prediction horizon and optimization algorithm to map the error-tolerance boundary