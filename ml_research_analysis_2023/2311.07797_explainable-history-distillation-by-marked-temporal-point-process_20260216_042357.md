---
ver: rpa2
title: Explainable History Distillation by Marked Temporal Point Process
arxiv_id: '2311.07797'
source_url: https://arxiv.org/abs/2311.07797
tags:
- events
- mtpp-ehd
- learning
- conference
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Explainable History Distillation
  (EHD) task, aimed at identifying the minimal subset of historical events that significantly
  influences future predictions in Marked Temporal Point Processes (MTPP). The proposed
  MTPP-based Explainable History Distiller (MTPP-EHD) leverages a Transformer-based
  encoder-decoder architecture combined with the Straight-Through Gumbel-Softmax trick
  to heuristically estimate the solution to the EHD problem.
---

# Explainable History Distillation by Marked Temporal Point Process

## Quick Facts
- **arXiv ID:** 2311.07797
- **Source URL:** https://arxiv.org/abs/2311.07797
- **Reference count:** 40
- **Key outcome:** MTPP-EHD achieves around 50% fewer events while maintaining prediction performance compared to baseline methods

## Executive Summary
This paper introduces Explainable History Distillation (EHD), a novel task aimed at identifying the minimal subset of historical events that significantly influences future predictions in Marked Temporal Point Processes (MTPP). The authors propose MTPP-EHD, which leverages a Transformer-based encoder-decoder architecture combined with the Straight-Through Gumbel-Softmax trick to heuristically estimate the solution to the EHD problem. The method is trained with two loss functions: one enforcing that distilled events should degrade future prediction performance, and another minimizing the number of distilled events. Experiments on Retweet and StackOverflow datasets demonstrate that MTPP-EHD significantly outperforms baseline methods in both efficiency and accuracy of selecting informative historical events.

## Method Summary
The MTPP-EHD approach reformulates the EHD task as a 0-1 integer program and uses a Transformer-based encoder-decoder architecture to estimate the solution. The model employs the Straight-Through Gumbel-Softmax trick for differentiable event selection, allowing gradient-based optimization. Two loss functions guide the training: Lc enforces that distilled events degrade prediction performance by maximizing perplexity differences, while Ln minimizes the number of distilled events. The method is evaluated on Retweet and StackOverflow datasets using sliding window approaches to generate (Hf, xo) pairs, with performance measured through Explainable History Events Distinguishment (EHED) and Explainable Minimal History Distillation (EMHD) tasks.

## Key Results
- MTPP-EHD achieves around 50% fewer events while maintaining prediction performance compared to baseline methods
- Outperforms Greedy Search and Random Distillation in both efficiency and accuracy of selecting informative historical events
- Demonstrates faster evaluation times compared to baselines, with lower computational complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MTPP-EHD can solve the EHD task by heuristically selecting the minimal subset of historical events that significantly influences future predictions.
- **Mechanism:** MTPP-EHD rewrites EHD into a 0-1 integer program and uses a Transformer-based encoder-decoder architecture combined with the Straight-Through Gumbel-Softmax trick to estimate the solution. The model is trained with two loss functions: one enforcing that the distilled events should degrade future prediction performance, and another minimizing the number of distilled events.
- **Core assumption:** The Straight-Through Gumbel-Softmax trick can effectively sample differentiable yet continuous masks that approximate the discrete 0-1 selection of events, allowing gradient-based optimization.
- **Evidence anchors:**
  - [abstract] "The proposed MTPP-based Explainable History Distiller (MTPP-EHD) leverages a Transformer-based encoder-decoder architecture combined with the Straight-Through Gumbel-Softmax trick to heuristically estimate the solution to the EHD problem."
  - [section] "MTPP-EHD utilizes an encoder-decoder Transformer[95] to estimate P (yi|xo, Hf), further optimizing the distribution according to two loss functions, Ln and Lc."
- **Break condition:** If the Gumbel-Softmax sampling fails to approximate the discrete 0-1 mask accurately, the model may not be able to effectively select the minimal subset of events.

### Mechanism 2
- **Claim:** The two loss functions, Ln and Lc, effectively guide MTPP-EHD to select the most informative events for explaining future predictions.
- **Mechanism:** Loss Lc enforces the constraint that the distilled events should degrade future prediction performance by maximizing the difference in perplexity between the original and distilled histories. Loss Ln minimizes the number of distilled events, encouraging the model to select the most informative subset.
- **Core assumption:** Perplexity is a suitable metric for measuring the impact of distilled events on future prediction performance, and minimizing the number of events while maintaining high perplexity difference leads to the most informative subset.
- **Evidence anchors:**
  - [abstract] "MTPP-EHD is trained with two loss functions: one enforcing the constraint that the distilled events should degrade future prediction performance, and another minimizing the number of distilled events."
  - [section] "Loss Lc for enforcing the constraints... Loss Ln for optimizing the size of Hd... Now, we can express the training loss of MTPP-EHD. The training loss L, as shown in Equation (14), is the sum of Ln and Lc."
- **Break condition:** If perplexity is not a reliable indicator of the importance of events for future predictions, the loss functions may not effectively guide the model to select the most informative subset.

### Mechanism 3
- **Claim:** MTPP-EHD outperforms baseline methods in terms of efficiency and accuracy of selecting informative historical events.
- **Mechanism:** MTPP-EHD's ability to heuristically estimate the solution to the EHD problem using the Transformer-based architecture and Gumbel-Softmax trick allows it to efficiently and accurately select the minimal subset of events that significantly influences future predictions.
- **Core assumption:** The Transformer-based architecture and Gumbel-Softmax trick provide a more effective approach to solving the EHD problem compared to baseline methods like Greedy Search and Random Distillation.
- **Evidence anchors:**
  - [abstract] "Experiments on Retweet and StackOverflow datasets demonstrate that MTPP-EHD significantly outperforms two baseline methods (Greedy Search and Random Distillation) in both efficiency and accuracy of selecting informative historical events, achieving around 50% fewer events while maintaining prediction performance."
  - [section] "Evaluation time ratios prove our analysis of computational complexity. GS achieves last in both evaluation tasks because of its quadruple computational complexity. RD is consistently faster than GS on both tasks. Both baselines perform faster on EHED than EMHD. However, they are very slow compared with MTPP-EHD."
- **Break condition:** If the baseline methods are more effective at solving the EHD problem or if the computational complexity of MTPP-EHD is significantly higher, it may not outperform the baselines in terms of efficiency and accuracy.

## Foundational Learning

- **Concept:** Marked Temporal Point Processes (MTPP)
  - **Why needed here:** MTPP provides the mathematical framework for modeling event sequences in continuous time with associated marks, which is the core problem domain of the EHD task.
  - **Quick check question:** How does the conditional intensity function in MTPP determine the probability of an event occurring at a given time?

- **Concept:** Counterfactual Analysis
  - **Why needed here:** Counterfactual analysis is the underlying principle for explaining the influence of historical events on future predictions, which is the main goal of the EHD task.
  - **Quick check question:** What is the key difference between factual and counterfactual worlds in the context of counterfactual analysis?

- **Concept:** 0-1 Integer Programming
  - **Why needed here:** The EHD task is reformulated as a 0-1 integer programming problem, where the goal is to find the minimal subset of events that significantly influences future predictions.
  - **Quick check question:** Why is the 0-1 integer programming problem considered NP-hard, and what are the implications for solving the EHD task?

## Architecture Onboarding

- **Component map:** Encoder-decoder Transformer -> Gumbel-Softmax trick -> Loss functions (Lc, Ln) -> Event selection
- **Critical path:** 1) Process observed history and future events using Transformer-based encoder-decoder architecture. 2) Sample differentiable masks using Straight-Through Gumbel-Softmax trick. 3) Calculate loss functions based on sampled masks and prediction performance. 4) Update model parameters using calculated losses.
- **Design tradeoffs:** Using Gumbel-Softmax trick allows for gradient-based optimization but introduces approximation errors in discrete event selection. Choice of loss functions balances accurate prediction performance and minimal event selection. Transformer-based architecture provides powerful representation learning but may increase computational complexity.
- **Failure signatures:** If model fails to select minimal subset of events, it may indicate issues with Gumbel-Softmax sampling or loss function optimization. If selected events don't significantly influence future predictions, it may suggest problems with Transformer-based architecture or underlying MTPP model.
- **First 3 experiments:**
  1. Evaluate model's performance on synthetic dataset with known ground truth event importance.
  2. Compare model's event selection with baseline methods on real-world dataset.
  3. Analyze impact of varying temperature parameter in Gumbel-Softmax trick on model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MTPP-EHD approach handle event sequences with varying mark distributions across different datasets, and what are the implications for generalizability?
- Basis in paper: [inferred] The paper discusses the effectiveness of MTPP-EHD on two datasets (Retweet and StackOverflow) with different mark distributions, but does not explicitly address generalizability to other datasets with varying mark distributions.
- Why unresolved: The paper focuses on evaluating MTPP-EHD on specific datasets and does not explore its performance on a wider range of datasets with varying mark distributions.
- What evidence would resolve it: Experimental results demonstrating the performance of MTPP-EHD on a diverse set of datasets with varying mark distributions, including both synthetic and real-world datasets.

### Open Question 2
- Question: What are the limitations of using the Straight-Through Gumbel-Softmax trick (ST-GS) for differentiable sampling in the context of EHD, and how might alternative methods improve the performance?
- Basis in paper: [explicit] The paper mentions the use of ST-GS for differentiable sampling and discusses its limitations, such as the inability to directly convert the sampled tensor to an integer tensor without losing gradients.
- Why unresolved: The paper does not explore alternative methods for differentiable sampling or discuss potential improvements to the ST-GS approach.
- What evidence would resolve it: Comparative analysis of alternative differentiable sampling methods, such as the Concrete distribution or the Gumbel-Softmax with temperature annealing, and their impact on the performance of MTPP-EHD.

### Open Question 3
- Question: How does the proposed MTPP-EHD approach handle long event sequences, and what are the computational implications for scalability?
- Basis in paper: [inferred] The paper mentions the computational complexity of MTPP-EHD and baselines, but does not explicitly address the handling of long event sequences or the scalability implications.
- Why unresolved: The paper focuses on evaluating MTPP-EHD on datasets with moderate-length event sequences and does not explore its performance on longer sequences or discuss scalability concerns.
- What evidence would resolve it: Experimental results demonstrating the performance of MTPP-EHD on long event sequences, including computational time and memory requirements, and a discussion of potential scalability limitations and solutions.

## Limitations
- The approach relies heavily on the Straight-Through Gumbel-Softmax trick, which introduces approximation errors that may not guarantee finding the true minimal subset of events
- Evaluation focuses on two specific datasets with limited event types, which may not generalize to more complex temporal point process scenarios
- Assumes that perplexity differences are a reliable measure of event importance, but this metric may not capture all aspects of prediction performance degradation

## Confidence
- **High Confidence:** The technical feasibility of using Transformer-based architectures with Gumbel-Softmax for differentiable event selection is well-established in the literature
- **Medium Confidence:** The claim that MTPP-EHD significantly outperforms baselines is supported by experimental results but may be sensitive to hyperparameter choices and implementation details
- **Medium Confidence:** The assertion that around 50% fewer events can be distilled while maintaining prediction performance needs validation across more diverse datasets and temporal point process types

## Next Checks
1. Conduct ablation studies varying the temperature parameter in the Gumbel-Softmax trick to quantify its impact on event selection quality and approximation accuracy
2. Evaluate MTPP-EHD on additional datasets with more event types and different temporal characteristics to assess generalization capabilities
3. Compare MTPP-EHD's explanations against human-annotated event importance rankings to validate the semantic meaningfulness of the selected events