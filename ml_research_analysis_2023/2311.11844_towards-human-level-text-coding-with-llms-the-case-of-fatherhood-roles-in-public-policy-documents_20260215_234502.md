---
ver: rpa2
title: 'Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles in
  Public Policy Documents'
arxiv_id: '2311.11844'
source_url: https://arxiv.org/abs/2311.11844
tags:
- text
- human
- language
- coding
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (GPT-3 and GPT-4) for
  text coding in political science research, using Swedish policy documents on fatherhood
  roles. It finds that providing detailed label definitions and coding examples (few-shot
  learning) yields the best performance.
---

# Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles in Public Policy Documents

## Quick Facts
- arXiv ID: 2311.11844
- Source URL: https://arxiv.org/abs/2311.11844
- Reference count: 13
- Key outcome: GPT-4 outperforms human coders in automatic text coding of Swedish policy documents on fatherhood roles, with few-shot learning and joint task coding yielding optimal results.

## Executive Summary
This paper evaluates large language models (GPT-3 and GPT-4) for automatic text coding in political science research, using Swedish policy documents on fatherhood roles. The study finds that providing detailed label definitions and coding examples (few-shot learning) yields the best LLM performance. GPT-3 achieves comparable results to human coders, while GPT-4 consistently outperforms them. Jointly coding multiple tasks is faster and cheaper without sacrificing quality. LLMs offer significant speed and cost advantages over human coders and can match or exceed their performance when properly prompted.

## Method Summary
The study used 1,911 Swedish sentences from policy documents (1993-2021) containing terms for "father" or "fathers", manually coded by human annotators. Researchers constructed prompts for GPT-3/GPT-4 with instructions, label definitions, and 15 labeled examples. They tested zero-shot and few-shot learning, single vs. joint task coding, and different example orders on a validation set. The best-performing prompt was then used to code the full corpus, with results compared to human annotations using Cohen's kappa, raw agreement, and F1 score across three interdependent coding tasks.

## Key Results
- GPT-4 consistently outperforms human coders in agreement and F1 score
- Detailed label definitions and coding examples (few-shot learning) yield optimal LLM performance
- Jointly coding multiple interdependent tasks maintains quality while reducing costs
- GPT-3 achieves comparable results to human coders when provided with detailed instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detailed label definitions and coding examples (few-shot learning) yield the best LLM performance.
- Mechanism: LLMs benefit from explicit instruction and examples that bridge the gap between abstract constructs and concrete text instances.
- Core assumption: The model has been trained on diverse enough data to understand the task given sufficient context.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the task requires reasoning beyond the model's training data distribution, few-shot examples may not generalize.

### Mechanism 2
- Claim: Jointly coding multiple interdependent tasks is faster, cheaper, and maintains quality.
- Mechanism: Shared information in hierarchical coding schemes allows efficient simultaneous task completion without quality loss.
- Core assumption: Tasks are truly interdependent and share contextual cues.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If tasks are too complex or independent, joint coding may degrade quality.

### Mechanism 3
- Claim: GPT-4 consistently outperforms GPT-3 and human coders in this domain.
- Mechanism: Larger model size and more diverse training data enable better generalization and alignment with human annotations.
- Core assumption: GPT-4's pretraining data included sufficient Swedish political discourse.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the domain is highly specialized or out-of-distribution, GPT-4 may not outperform smaller models.

## Foundational Learning

- Concept: Few-shot learning
  - Why needed here: LLMs can learn new tasks from a small number of examples without retraining.
  - Quick check question: Can you explain the difference between few-shot and zero-shot learning?

- Concept: Interdependent task coding
  - Why needed here: Multiple coding tasks share contextual information, enabling joint coding without quality loss.
  - Quick check question: How would you determine if two coding tasks are interdependent?

- Concept: Prompt engineering
  - Why needed here: The way instructions and examples are presented to the LLM significantly affects performance.
  - Quick check question: What are the key components of an effective prompt for text coding?

## Architecture Onboarding

- Component map: Swedish policy documents -> LLM API (OpenAI) -> Prompt template -> Validation set -> Label definitions -> Example sentences -> Evaluation metrics
- Critical path: Construct prompt → Evaluate on validation set → Deploy on full corpus → Analyze results
- Design tradeoffs: Detailed vs concise label definitions (accuracy vs token budget), joint vs separate task coding (speed vs potential complexity), GPT-3 vs GPT-4 (cost vs performance)
- Failure signatures: Poor agreement with human coders, inconsistent labels, hallucination of facts, alignment issues
- First 3 experiments:
  1. Compare zero-shot vs few-shot learning performance on a small validation set.
  2. Test different levels of detail in label definitions.
  3. Evaluate joint vs separate coding of interdependent tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-4's performance compare to human coders when analyzing non-English text versus English text?
- Basis in paper: [explicit] The paper notes that GPT-3 performs better with English instructions and that performance might vary according to the languages used.
- Why unresolved: The paper only evaluated GPT-4 on Swedish text and did not conduct a direct comparison between English and non-English text performance.
- What evidence would resolve it: Testing GPT-4 on both English and non-English text datasets with the same coding tasks and comparing the performance metrics.

### Open Question 2
- Question: What is the long-term cost-effectiveness of using LLMs for text coding as the technology continues to develop and improve?
- Basis in paper: [explicit] The paper mentions that LLMs are expected to become faster, cheaper, and more powerful over time, making them an even more economic choice.
- Why unresolved: The paper does not provide specific projections or data on how costs and performance will change in the future.
- What evidence would resolve it: Longitudinal studies tracking the costs and performance of LLMs over several years as the technology advances.

### Open Question 3
- Question: How does the performance of LLMs vary when coding text across different languages and cultural contexts?
- Basis in paper: [inferred] The paper discusses the use of LLMs for coding Swedish text and notes that performance might vary according to the languages used, suggesting potential differences across languages and cultural contexts.
- Why unresolved: The paper only evaluated LLMs on Swedish text and did not explore performance across different languages or cultural contexts.
- What evidence would resolve it: Testing LLMs on text datasets from multiple languages and cultural contexts with the same coding tasks and comparing the performance metrics.

## Limitations

- Findings are based on a specific domain (Swedish policy documents on fatherhood roles) and may not generalize to other languages, cultures, or policy domains.
- Exact content of the 15 example sentences used in few-shot learning is not provided, making exact replication challenging.
- While the paper demonstrates strong performance in this specific case, broader applicability across diverse political science research contexts remains to be validated.

## Confidence

- High Confidence: The finding that detailed label definitions and coding examples significantly improve LLM performance is well-supported by experimental evidence and aligns with established few-shot learning principles.
- Medium Confidence: The claim that GPT-4 consistently outperforms both GPT-3 and human coders is supported by the experimental results but may be context-dependent and influenced by the specific nature of the fatherhood policy domain.
- Medium Confidence: The assertion that joint coding of interdependent tasks maintains quality while reducing costs is supported by the experimental design but would benefit from additional validation across different task combinations and domains.

## Next Checks

1. **Cross-domain validation:** Apply the same few-shot prompting strategy and joint coding approach to a different policy domain (e.g., climate policy or healthcare) to assess generalizability of the findings.
2. **Language generalization test:** Translate the Swedish policy documents to another language (e.g., English or German) and evaluate whether the LLM performance patterns hold across languages.
3. **Human-LLM hybrid evaluation:** Conduct a study where human coders use LLM outputs as a starting point for annotation, measuring whether this approach improves speed, accuracy, or both compared to either method alone.