---
ver: rpa2
title: Leveraging Language Models to Detect Greenwashing
arxiv_id: '2311.01469'
source_url: https://arxiv.org/abs/2311.01469
tags:
- greenwashing
- climatebert
- reports
- risk
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to detect greenwashing in corporate
  sustainability reports using language models. The authors generate ground-truth
  labels for greenwashing risk based on climate-specific attributes and fine-tune
  ClimateBERT on these labels.
---

# Leveraging Language Models to Detect Greenwashing

## Quick Facts
- **arXiv ID**: 2311.01469
- **Source URL**: https://arxiv.org/abs/2311.01469
- **Reference count**: 13
- **Key outcome**: ClimateBERT achieves 86.34% accuracy and 0.67 F1 score on sustainability report greenwashing detection

## Executive Summary
This paper presents a method to detect greenwashing in corporate sustainability reports using language models. The authors develop a mathematical formulation to quantify greenwashing risk based on four linguistic attributes (sentiment, climate commitment, specificity, and hedging) and generate ground truth labels from these attributes. They fine-tune ClimateBERT, a climate-specific language model, on this labeled data to classify paragraphs as containing greenwashing risk or not. The model demonstrates promising results on a test set of sustainability reports from various industries, though the generalizability and ground truth generation method require further validation.

## Method Summary
The authors generate ground truth labels for greenwashing risk using a linear combination of four linguistic metrics (sentiment, climate commitment, specificity, hedging) calculated from text data. These labels are used to fine-tune ClimateBERT, a transformer model pre-trained on climate-related text. The model is evaluated on a test set of sustainability reports, with predictions aggregated from paragraph-level to report-level using majority voting. The approach leverages the language understanding capabilities of transformer models while incorporating domain-specific knowledge about greenwashing characteristics.

## Key Results
- Best model achieved 86.34% accuracy and 0.67 F1 score on sustainability report test set
- Full fine-tuning of all ClimateBERT layers outperformed frozen layer approaches
- Model generalizes to out-of-domain sustainability reports from oil/gas and tech sectors
- No strong correlation found between model predictions and reported carbon emissions or revenue data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The model can identify greenwashing risk by detecting specific linguistic cues in corporate reports.
- **Mechanism**: The system uses a linear combination of four linguistic metrics (sentiment, climate commitment, specificity, hedging) to generate a risk score, which is then thresholded to produce binary labels. This score is used to fine-tune ClimateBERT.
- **Core assumption**: The four linguistic attributes chosen are sufficient to capture the nuances of greenwashing language in sustainability reports.
- **Evidence anchors**:
  - [abstract] "developing a mathematical formulation to quantify greenwashing risk"
  - [section] "We present a calculation for greenwashing risk as a linear combination of the attributes outlined above."
  - [corpus] Weak evidence: No direct citation that these four attributes are comprehensive or optimal.
- **Break condition**: If any of the four metrics fail to capture greenwashing, the model's accuracy will degrade significantly. For example, if a company uses complex legal language not captured by the hedging list, the model may miss it.

### Mechanism 2
- **Claim**: Fine-tuning all layers of ClimateBERT improves detection accuracy compared to freezing layers.
- **Mechanism**: By fine-tuning all layers, the model adapts both the pre-trained representations and the classification head to the specific task of greenwashing detection, capturing task-specific patterns.
- **Core assumption**: The greenwashing detection task is sufficiently distinct from general climate text understanding to benefit from full fine-tuning.
- **Evidence anchors**:
  - [section] "We found that not freezing the RoBERTa layers resulted in higher mean validation accuracy and F1 scores"
  - [abstract] "fine-tuned ClimateBERT model for this problem"
  - [corpus] No comparative study with other architectures provided.
- **Break condition**: If the task were too similar to general climate text, full fine-tuning might overfit or not provide significant gains.

### Mechanism 3
- **Claim**: The model generalizes to out-of-domain sustainability reports from different industries.
- **Mechanism**: The model, trained on a mix of climate-related news and corporate reports, can adapt to sustainability reports from oil/gas and tech sectors through majority voting across paragraph-level predictions.
- **Core assumption**: The linguistic patterns of greenwashing are consistent across industries, allowing transfer learning.
- **Evidence anchors**:
  - [section] "Our OOD test set (from the Open for Good Data) come from APA Corporation, Devon Energy, and Diamondback Energy in the oil and gas sector and Autodesk, NVIDIA, and ServiceNow in the tech sector."
  - [abstract] "On a test set comprising of sustainability reports, our best model achieved an average accuracy score of 86.34% and F1 score of 0.67"
  - [corpus] Weak evidence: Only 6 companies tested; no cross-industry ablation study provided.
- **Break condition**: If greenwashing language varies significantly by industry, the model's performance will drop on reports from unseen sectors.

## Foundational Learning

- **Concept**: Linear regression for label generation
  - Why needed here: To create initial ground truth labels from linguistic attributes when expert-annotated data is scarce.
  - Quick check question: What happens to the ground truth quality if the linear regression fit is poor (high residual)?

- **Concept**: Fine-tuning vs. feature extraction in transformer models
  - Why needed here: To decide whether to adapt all layers or just the classification head for optimal performance.
  - Quick check question: When would freezing layers be preferable to full fine-tuning?

- **Concept**: Out-of-distribution (OOD) evaluation
  - Why needed here: To test if the model generalizes beyond the training data distribution to real-world reports.
  - Quick check question: Why is majority voting used at the report level instead of averaging scores?

## Architecture Onboarding

- **Component map**: ClimateBERT pre-training corpus -> ground truth generation -> fine-tuning dataset -> ClimateBERT with classification head -> paragraph-level predictions -> report-level majority voting

- **Critical path**:
  1. Generate ground truth labels using linear combination of linguistic metrics
  2. Fine-tune ClimateBERT on labeled data (all layers, not frozen)
  3. Apply model to OOD test set (sustainability reports)
  4. Aggregate paragraph predictions via majority voting
  5. Evaluate accuracy and F1 score

- **Design tradeoffs**:
  - Full fine-tuning increases accuracy but requires more compute and risks overfitting
  - Paragraph-level chunking loses report context but enables model application
  - Linear combination is interpretable but may oversimplify greenwashing complexity

- **Failure signatures**:
  - Low F1 score indicates poor precision-recall balance, possibly due to label noise
  - High accuracy but low F1 on OOD data suggests overfitting to training distribution
  - Inconsistent report-level predictions indicate chunking strategy issues

- **First 3 experiments**:
  1. Compare frozen vs. non-frozen fine-tuning on validation set to confirm full fine-tuning benefits
  2. Test different thresholds for binary classification to optimize F1 score
  3. Evaluate model on a single industry (e.g., tech only) before full OOD test to check for domain shift

## Open Questions the Paper Calls Out

The paper explicitly notes that the intermediate attributes used for ground truth generation (sentiment, climate specificity, commitment, and hedging) may fail to capture certain nuances, such as when a company admits to bad performance but isn't necessarily greenwashing. This limitation suggests that the current linear combination approach may not fully capture the complexity of greenwashing detection, and the authors call for more sophisticated attribute definitions or additional metrics to improve ground truth generation accuracy.

## Limitations

- Ground truth generation relies on a linear combination of four attributes without external validation of this formula's accuracy
- Test set evaluation covers only six companies across two industries, providing limited evidence for generalizability claims
- Paragraph-level chunking strategy may lose important contextual information that exists at the report level

## Confidence

- **High Confidence**: The technical implementation of ClimateBERT fine-tuning and the basic methodology of using linguistic attributes for label generation are well-documented and reproducible.
- **Medium Confidence**: The claim that full fine-tuning outperforms frozen layers is supported by validation results, but lacks comparative analysis with alternative architectures.
- **Low Confidence**: The generalizability to out-of-domain sustainability reports and the sufficiency of the four-attribute linear combination for capturing greenwashing risk are weakly supported by limited test data.

## Next Checks

1. Conduct expert annotation of a subset of reports to compare against the linear combination-generated labels and assess the accuracy of the ground truth generation method.

2. Evaluate the model on sustainability reports from at least 20 additional companies across 5+ industries to properly assess generalizability and identify potential domain-specific failure modes.

3. Test alternative methods for ground truth generation, such as using a different combination of linguistic attributes or incorporating domain expert knowledge, to determine if the current linear combination is optimal.