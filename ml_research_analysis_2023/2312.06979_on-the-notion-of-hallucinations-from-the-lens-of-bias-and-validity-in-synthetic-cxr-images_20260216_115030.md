---
ver: rpa2
title: On the notion of Hallucinations from the lens of Bias and Validity in Synthetic
  CXR Images
arxiv_id: '2312.06979'
source_url: https://arxiv.org/abs/2312.06979
tags:
- images
- medical
- bias
- diffusion
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of a fine-tuned Stable Diffusion
  model (RoentGen) for generating synthetic Chest X-Ray (CXR) images. The primary
  goal was to assess bias, validity, and hallucinations in these generated images
  compared to the original MIMIC-CXR dataset.
---

# On the notion of Hallucinations from the lens of Bias and Validity in Synthetic CXR Images

## Quick Facts
- arXiv ID: 2312.06979
- Source URL: https://arxiv.org/abs/2312.06979
- Authors: 
- Reference count: 26
- Key outcome: Fine-tuned Stable Diffusion model (RoentGen) for synthetic CXR generation shows significant bias disparities, especially for Female Hispanic subgroup, and latent hallucinations with 42% of images incorrectly indicating COVID.

## Executive Summary
This study evaluates bias, validity, and hallucinations in synthetic Chest X-Ray (CXR) images generated by RoentGen, a fine-tuned Stable Diffusion model. The analysis compares generated images to the original MIMIC-CXR dataset across demographic subgroups and disease classes. The results reveal pronounced bias disparities when demographic attributes are included in prompts, lower classification performance for certain subgroups, and significant variability in synthetic image quality. Latent hallucinations are detected using an out-of-distribution validation approach, highlighting potential risks in using synthetic medical images for training or augmentation.

## Method Summary
The study fine-tunes a Stable Diffusion model on MIMIC-CXR text-image pairs to generate synthetic CXR images. Generated images are evaluated for bias using subgroup classification performance metrics (TPR and SR), validity through disease classifier accuracy and uncertainty, and hallucinations via an independent COVID classifier trained on out-of-training classes. The analysis includes comparisons with and without demographic attributes (gender, race) in text prompts to assess fairness impacts.

## Key Results
- Significant TPR and SR disparities across subgroups, with Female Hispanic subgroup showing pronounced bias (44% TPR vs 64% for White females).
- Including race and gender in prompts exacerbated fairness issues, resulting in 26% deviation from baseline SR for White females.
- Approximately 42% of synthetic images exhibited latent hallucinations, incorrectly indicating COVID when evaluated by an independent classifier.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pretrained diffusion model on domain-specific medical text-image pairs improves the model's ability to generate clinically relevant synthetic CXR images.
- Mechanism: The model learns the conditional distribution P(image | text) by optimizing a denoising objective on paired CXR reports and images, which allows it to synthesize images that match the clinical descriptions.
- Core assumption: The pretrained latent diffusion model has already learned general visual features that can be adapted to medical imagery with sufficient fine-tuning data.
- Evidence anchors:
  - [abstract] "researchers at Stanford used the technique to generate radiology images (RoentGen)...found that fine-tuning the model on a fixed training set increases classifier performance by 5%"
  - [section] "diffusion models follow a two-stage process: (1) a prior generating CLIP image embedding with a text caption; (2) a decoder (U-net) producing an image conditioned upon the image embedding"
- Break condition: If the fine-tuning dataset lacks diversity or contains biased reporting, the model may amplify those biases in the generated images, as shown by the pronounced Hispanic female subgroup disparity.

### Mechanism 2
- Claim: Adding protected attributes (gender, race) into text prompts increases model sensitivity to those attributes, which can worsen fairness disparities in generated outputs.
- Mechanism: When race/gender terms are included in prompts, the model's text encoder weights those tokens more heavily, causing the decoder to overfit demographic features into the synthetic image content.
- Core assumption: The text encoder's attention mechanism treats demographic descriptors as strong visual cues during image generation.
- Evidence anchors:
  - [abstract] "incorporating race and gender into input prompts exacerbated fairness issues in the generated images"
  - [section] "The introduction of racial and gender references in prompts further exacerbates bias, resulting in a substantial 26% deviation from the SR of White females"
- Break condition: If prompts are carefully balanced or anonymized, the model may generate more equitable synthetic samples; however, the current study shows that even with fine-tuning, demographic bias persists.

### Mechanism 3
- Claim: Latent hallucinations arise when the diffusion decoder introduces features not present in the conditioning text, detectable by evaluating outputs with classifiers trained on independent (out-of-training) classes.
- Mechanism: The model's latent space learns spurious correlations between visual patterns and textual tokens; during sampling, these correlations manifest as features unrelated to the prompt, such as COVID indicators in non-COVID cases.
- Core assumption: The latent space of the diffusion model contains compressed representations that can encode features absent from the conditioning text but correlated with training data patterns.
- Evidence anchors:
  - [abstract] "we observed latent hallucinations, with approximately 42% of the images incorrectly indicating COVID"
  - [section] "when passing the generated images through the classifier containing the (out-of-training) independent class, the images exposed potential latent hallucinations"
- Break condition: If the validation classifier is replaced with an out-of-distribution robustness test, similar hallucinatory artifacts may be detected across other disease classes, indicating a systemic issue.

## Foundational Learning

- Concept: Bias in AI classification
  - Why needed here: To interpret subgroup performance disparities and understand fairness metrics like TPR and SR.
  - Quick check question: What is the difference between statistical bias and algorithmic bias in medical imaging models?

- Concept: Diffusion probabilistic models
  - Why needed here: To grasp how denoising and latent space sampling produce synthetic images from text prompts.
  - Quick check question: How does the U-Net architecture in diffusion models differ from a standard convolutional decoder?

- Concept: Out-of-distribution detection
  - Why needed here: To design experiments that reveal hallucinations by using classifiers trained on independent classes.
  - Quick check question: Why would a classifier trained only on COVID X-rays detect hallucinated COVID features in non-COVID synthetic images?

## Architecture Onboarding

- Component map: Text encoder (CLIP-based) → Latent diffusion prior → U-Net decoder → Image output; auxiliary classifiers for bias and hallucination detection.
- Critical path: Text → CLIP embedding → Prior network → Gaussian noise schedule → U-Net denoising → Generated CXR.
- Design tradeoffs: Fine-tuning on small medical datasets improves domain fit but risks overfitting and bias amplification; using race/gender prompts increases controllability but can worsen fairness.
- Failure signatures: Systematic TPR/SR disparities across demographic subgroups; high uncertainty in disease classification; spurious features detected by out-of-distribution classifiers.
- First 3 experiments:
  1. Generate synthetic CXR images with and without demographic prompts; compare classifier performance across subgroups.
  2. Run disease classification on original vs. synthetic images; compute confidence variance and false-negative rates.
  3. Pass both sets of images through a COVID classifier; measure proportion of hallucinated COVID detections.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause the observed bias against the Hispanic female subgroup in synthetic CXR images?
- Basis in paper: [explicit] The paper shows pronounced bias against the Hispanic female subgroup, with TPR dropping to 44% compared to 64% for White females and 56% for Asian females.
- Why unresolved: The study identifies the bias but doesn't investigate the underlying mechanisms or data representation issues that cause this disparity.
- What evidence would resolve it: Analysis of training data distribution, feature importance scores, and attention mechanisms could reveal why this subgroup is systematically misclassified.

### Open Question 2
- Question: How can latent hallucinations in synthetic medical images be reliably detected and quantified?
- Basis in paper: [explicit] The study found 42% of synthetic images incorrectly indicated COVID, suggesting hallucinations, but notes this detection method may not be suitable for all circumstances.
- Why unresolved: The paper uses an out-of-training class validation method, but acknowledges this may not generalize to other hallucination types or medical conditions.
- What evidence would resolve it: Development and validation of domain-specific hallucination detection metrics that can reliably identify hallucinated features across different medical imaging contexts.

### Open Question 3
- Question: What are the clinical implications of synthetic image quality variability across different disease classes?
- Basis in paper: [explicit] The study found synthetic images had higher uncertainty and were more prone to false negatives compared to original images, with significant variability across disease classes.
- Why unresolved: While the paper quantifies accuracy differences, it doesn't investigate how these quality variations impact clinical decision-making or patient outcomes.
- What evidence would resolve it: Clinical validation studies comparing diagnostic accuracy and treatment decisions made using synthetic versus original images across different disease categories.

## Limitations
- Limited to MIMIC-CXR dataset, which may not represent broader population diversity.
- Hallucination detection method using out-of-distribution classifiers may produce false positives.
- Study does not address potential confounds such as prompt quality, image resolution differences, or classifier calibration.

## Confidence
- High confidence in bias amplification findings when including demographic attributes in prompts.
- Medium confidence in hallucination detection mechanism due to indirect validation approach.
- Medium confidence in validity assessment showing synthetic image quality variability across disease classes.

## Next Checks
1. Test hallucination detection by using multiple independent classifiers (not just COVID) to verify if spurious features appear consistently across different disease categories.
2. Conduct a controlled ablation study varying only the inclusion of demographic terms in prompts while holding all other generation parameters constant, to isolate the effect on bias.
3. Compare synthetic image quality metrics (e.g., structural similarity, radiologist scoring) against the original dataset to quantify the validity gap more precisely.