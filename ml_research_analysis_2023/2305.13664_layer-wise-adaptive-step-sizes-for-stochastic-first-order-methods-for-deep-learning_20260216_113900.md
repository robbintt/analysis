---
ver: rpa2
title: Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep
  Learning
arxiv_id: '2305.13664'
source_url: https://arxiv.org/abs/2305.13664
tags:
- learning
- loss
- adamw-lw
- methods
- sgd-m-lw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a layer-wise adaptive step-size (LW-AS) method
  for stochastic first-order optimization in deep learning, eliminating the need for
  manual learning rate tuning. The method exploits layer-wise curvature information
  from the Hessian to compute adaptive step sizes for each layer, while maintaining
  comparable memory requirements and only slightly increased per-iteration time complexity
  compared to first-order methods.
---

# Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning

## Quick Facts
- arXiv ID: 2305.13664
- Source URL: https://arxiv.org/abs/2305.13664
- Reference count: 40
- This paper proposes a layer-wise adaptive step-size method for stochastic first-order optimization in deep learning, eliminating the need for manual learning rate tuning.

## Executive Summary
This paper introduces a layer-wise adaptive step-size (LW-AS) method for stochastic first-order optimization in deep learning. The method exploits layer-wise curvature information from the Hessian to compute adaptive step sizes for each layer, while maintaining comparable memory requirements and only slightly increased per-iteration time complexity compared to first-order methods. Theoretical analysis proves linear convergence for an idealized version using full-batch gradients. Extensive experiments on CNN, GCN, and autoencoder models show that SGD with momentum and AdamW combined with LW-AS outperform fine-tuned learning rate versions and achieve competitive results with state-of-the-art first and second-order methods, both in terms of accuracy and training time. The method demonstrates robustness across different architectures and datasets, providing a practical tool for fast and effective optimization without learning rate tuning.

## Method Summary
The paper proposes a layer-wise adaptive step-size (LW-AS) method that computes per-layer learning rates using curvature information from diagonal Hessian blocks. For each layer, the method approximates the restricted loss function as self-concordant, then computes a closed-form optimal step-size using the layer's gradient and curvature. This is implemented efficiently using vector-Hessian products computed via automatic differentiation, with only a small overhead per iteration. The method is integrated with SGD with momentum and AdamW, using exponential moving averages to smooth stochastic fluctuations. Experiments validate the approach on CNNs, GCNs, and autoencoders across multiple datasets, showing competitive performance without manual learning rate tuning.

## Key Results
- SGD-m-LW and AdamW-LW outperform fine-tuned versions of SGD-m and AdamW on multiple architectures and datasets
- LW-AS methods achieve competitive performance with state-of-the-art first and second-order methods like KFAC and Shampoo
- The method eliminates manual learning rate tuning while maintaining similar memory requirements and only marginally increased per-iteration time complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-wise adaptive step-sizes exploit local curvature information from diagonal Hessian blocks to set layer-specific learning rates, enabling faster convergence than fixed or global adaptive rates.
- Mechanism: Each layer's Hessian diagonal block is approximated using a vector-Hessian product (via automatic differentiation), then combined with the layer's descent direction to compute a layer-specific step-size. This scaling adjusts the update magnitude per layer based on its local loss curvature.
- Core assumption: Diagonal blocks of the Hessian are positive definite (or at least positive semi-definite) for each layer when L2 regularization or weight decay is applied, making them suitable for closed-form step-size computation.
- Evidence anchors:
  - [abstract] "The proposed approach exploits the layer-wise stochastic curvature information contained in the diagonal blocks of the Hessian in deep neural networks (DNNs) to compute adaptive step-sizes (i.e., LRs) for each layer."
  - [section 3] "Using results in [47], if one assumes that the loss function is general self-concordant, then the restricted loss F (λ) becomes standard self-concordant."
- Break condition: If the diagonal Hessian blocks become negative definite or zero, the closed-form step-size formula breaks down and may produce invalid or divergent updates.

### Mechanism 2
- Claim: The layer-wise step-size method achieves memory and per-iteration time costs comparable to first-order methods while incorporating second-order curvature information.
- Mechanism: By computing only diagonal Hessian blocks (not full Hessian) and reusing gradient computations, the overhead per iteration is roughly equivalent to one extra gradient evaluation. The per-layer update structure avoids storing large matrices.
- Core assumption: The cost of computing H (λ) d(λ) via vector-Hessian products is comparable to a gradient computation and the memory footprint remains layer-local.
- Evidence anchors:
  - [abstract] "The method has memory requirements that are comparable to those of first-order methods, while its per-iteration time complexity is only increased by an amount that is roughly equivalent to an additional gradient computation."
  - [section 5] "Fortunately, for functions that can be computed using a computational graph ... there are automatic methods available for computing Hessian-vector products exactly [40], which take about as much computation as gradient evaluations."
- Break condition: If the vector-Hessian product computation becomes significantly more expensive (e.g., due to very large layer sizes or limited GPU memory bandwidth), the claimed efficiency advantage erodes.

### Mechanism 3
- Claim: Adaptive layer-wise learning rates stabilize training across diverse architectures and datasets without requiring manual learning rate tuning.
- Mechanism: The step-size formula uses both the gradient norm and the local Hessian norm to adapt the learning rate, making the update direction and magnitude responsive to both gradient magnitude and curvature. Exponential moving averages further smooth stochastic fluctuations.
- Core assumption: The combination of curvature-aware step-size computation and moving averages produces a stable, well-scaled update that generalizes across different problem types.
- Evidence anchors:
  - [abstract] "SGD with momentum and AdamW combined with the proposed per-layer step-sizes are able to choose effective LR schedules and outperform fine-tuned LR versions of these methods."
  - [section 6] "SGD-m-LW and AdamW-LW have a similar (and sometimes better) optimization and generalization performance compared to fine-tuned standard versions of SGD-m and AdamW, and do not require any tuning procedure for the learning rate."
- Break condition: If the curvature estimation is noisy or the moving average window is poorly chosen, the method may oscillate or converge slowly.

## Foundational Learning

- Concept: Self-concordant functions and their role in step-size selection.
  - Why needed here: The closed-form step-size formula derives from properties of self-concordant functions, ensuring a guaranteed decrease in the loss function.
  - Quick check question: What property of self-concordant functions guarantees a decrease in the loss when using the computed step-size?

- Concept: Vector-Hessian product computation via automatic differentiation.
  - Why needed here: Layer-wise step-sizes require computing δ(λ) = ||d(λ)||H^(λ)d(λ)||, which is obtained efficiently using vector-Hessian products.
  - Quick check question: How does automatic differentiation enable efficient computation of Hessian-vector products without forming the full Hessian?

- Concept: Diagonal Hessian block structure in feed-forward networks.
  - Why needed here: Understanding that each layer's parameter Hessian decomposes into a Kronecker product of activation outer products and pre-activation Hessians explains why per-layer curvature is computable.
  - Quick check question: In a feed-forward network, how is the layer-wise Hessian block related to the pre-activation Hessian and the activation vector?

## Architecture Onboarding

- Component map: Forward pass -> Gradient computation -> Direction computation -> Layer-wise curvature estimation -> Step-size computation -> Moving average update -> Weight update
- Critical path: Forward pass → gradient computation → direction computation → layer-wise curvature estimation → step-size computation → moving average update → weight update.
- Design tradeoffs:
  - Frequency of step-size recomputation (T): Higher T reduces overhead but may use stale curvature; lower T increases accuracy but costs more.
  - Warm-up period: Skipping step-size computation initially stabilizes early training but delays benefits.
  - Weight decay integration: Adding γW^(λ) to the direction ensures positive definiteness but changes the update behavior.
- Failure signatures:
  - Step-sizes becoming NaN or Inf: Usually due to numerical instability in δ(λ) computation.
  - Training diverging: May indicate overly large step-sizes from inaccurate curvature estimates.
  - No improvement over baseline: Could be due to too infrequent updates or inappropriate damping.
- First 3 experiments:
  1. Run SGD-m-LW on a small CNN (e.g., VGG11) on CIFAR-10 with T=5 and warm-up=5 epochs; compare loss curves to vanilla SGD-m.
  2. Compare AdamW-LW vs AdamW on a GCN on Cora with T=1; monitor layer-wise step-size evolution.
  3. Test sensitivity to T by running SGD-m-LW on CIFAR-10 + VGG16 with T ∈ {5,10,15,20,25}; plot final accuracy vs process time.

## Open Questions the Paper Calls Out

- **Question**: How does the layer-wise adaptive step-size method perform on recurrent neural networks (RNNs) and transformer architectures compared to CNNs, GCNs, and autoencoders?
  - Basis in paper: [inferred] The paper mentions exploring extensions to RNNs and transformers as future work, indicating these architectures were not tested.
  - Why unresolved: The paper only presents experimental results on CNNs, GCNs, and autoencoders, leaving performance on RNNs and transformers unknown.
  - What evidence would resolve it: Experimental results comparing LW-AS methods against state-of-the-art optimizers on RNN and transformer architectures across various tasks and datasets.

- **Question**: What is the theoretical convergence behavior of the LW-AS method when applied to non-convex DNN loss functions where the layer-wise restricted functions are not guaranteed to be self-concordant?
  - Basis in paper: [explicit] The paper proves linear convergence for an idealized version using full-batch gradients on self-concordant functions, but notes extending this to non-convex DNNs is future work.
  - Why unresolved: The theoretical analysis in the paper relies on self-concordance assumptions that do not hold for general non-convex DNN loss functions.
  - What evidence would resolve it: A convergence proof or rigorous analysis of LW-AS method for non-convex DNN optimization, possibly using alternative techniques from recent SGD convergence theory.

- **Question**: How sensitive is the LW-AS method to the choice of the damping parameter ϵ in AdamW-LW and the update frequency T for different model architectures and datasets?
  - Basis in paper: [explicit] The paper mentions sensitivity to T and initial warm-up learning rate, but does not systematically explore the damping parameter sensitivity.
  - Why unresolved: The paper provides limited sensitivity analysis, only exploring T on one problem and initial learning rate on two GCN problems, without investigating the damping parameter.
  - What evidence would resolve it: A comprehensive sensitivity analysis showing performance across a range of damping parameter values and update frequencies for multiple architectures and datasets.

## Limitations

- The theoretical convergence proof only covers an idealized full-batch version using self-concordant loss assumptions that may not hold for all neural network architectures
- Experimental evaluation is limited to specific architectures (CNNs, GCNs, autoencoders) and datasets, leaving performance on transformers or large-scale vision models unknown
- Claims about eliminating learning rate tuning entirely are tempered by the need to tune hyperparameters like update frequency T and warm-up periods

## Confidence

- **High confidence**: Claims about the basic mechanism of layer-wise step-size computation and its integration with SGD-m and AdamW; memory and time complexity comparisons with first-order methods (supported by theoretical analysis and implementation details)
- **Medium confidence**: Claims about generalization across diverse architectures and datasets (supported by experimental results but limited to specific model families); claims about competitive performance with state-of-the-art second-order methods (based on comparison with KFAC and Shampoo on selected tasks)
- **Low confidence**: Theoretical convergence guarantees for the practical stochastic method (the proof only covers an idealized full-batch version); claims about eliminating learning rate tuning entirely (the method still has hyperparameters like update frequency T and warm-up periods)

## Next Checks

1. **Sensitivity analysis**: Systematically vary the update frequency parameter T and warm-up period across multiple architectures to quantify their impact on convergence speed and final performance.

2. **Memory/time profiling**: Conduct detailed profiling of SGD-m-LW vs standard SGD-m on models with varying layer sizes to empirically verify the claimed complexity advantages, particularly for very wide layers.

3. **Broader architecture testing**: Evaluate the method on transformer-based architectures (e.g., ViT or BERT-small) to test generalization beyond CNNs, GCNs, and autoencoders.