---
ver: rpa2
title: 'TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models'
arxiv_id: '2305.11171'
source_url: https://arxiv.org/abs/2305.11171
tags:
- data
- trueteacher
- factual
- summaries
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrueTeacher is a method for generating synthetic training data
  for factual consistency evaluation in summarization. It annotates diverse model-generated
  summaries using a large language model (LLM), such as FLAN-PaLM, to label them for
  consistency with their source documents.
---

# TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models

## Quick Facts
- **arXiv ID:** 2305.11171
- **Source URL:** https://arxiv.org/abs/2305.11171
- **Reference count:** 33
- **Primary result:** TrueTeacher generates synthetic training data for factual consistency evaluation using LLM annotations of model-generated summaries, outperforming state-of-the-art models and the LLM teacher on the TRUE benchmark.

## Executive Summary
TrueTeacher introduces a novel method for generating synthetic training data to evaluate factual consistency in summarization. Instead of relying on human-written summaries or perturbed gold summaries, it uses a large language model (LLM) to annotate diverse model-generated summaries for consistency with their source documents. This approach is multilingual by nature and does not require language-specific components. Experiments demonstrate that a student model trained on TrueTeacher's synthetic data substantially outperforms both state-of-the-art models of similar capacity and the LLM teacher itself, achieving 87.8% ROC-AUC compared to the 84.9% of the FLAN-PaLM 540B teacher on the TRUE benchmark.

## Method Summary
TrueTeacher generates synthetic training data for factual consistency evaluation by first training diverse summarization models (T5 variants of different sizes) on a corpus like CNN/DailyMail. These models generate summaries of source documents, creating a diverse set of model-generated summaries. A large language model (FLAN-PaLM 540B) then annotates these summaries for factual consistency with their sources using zero-shot prompting. A student model (T5-11B) is trained on this synthetic data and evaluated on the TRUE benchmark. The method also generalizes to multilingual scenarios, showing improvements on 35 out of 45 languages in the mFACE dataset.

## Key Results
- Student model trained on TrueTeacher data achieves 87.8% ROC-AUC on TRUE benchmark, outperforming both state-of-the-art T5-11B (84.9%) and the 540B FLAN-PaLM teacher (84.9%).
- Systematic study shows TrueTeacher's superiority and robustness to domain shift compared to existing synthetic data generation methods.
- On mFACE dataset, TrueTeacher improves results on 35 out of 45 languages, demonstrating strong multilingual generalization.
- Large-scale synthetic dataset of 1.4 million examples generated and released for factual consistency evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model-generated summaries provide more realistic training data for factual consistency evaluation than perturbed human-written summaries.
- **Mechanism:** Summaries from diverse summarization models capture the style and error patterns of real model outputs, which differ from human-written summaries. LLMs can label these summaries for factual consistency, providing realistic training examples.
- **Core assumption:** A strong LLM can accurately label factual consistency in model-generated summaries.
- **Evidence anchors:**
  - [abstract] "Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature."
  - [section] "Our main motivation is to use factual inconsistencies from real model-generated summaries instead of perturbed gold-summaries."
- **Break condition:** If the LLM teacher cannot accurately label factual consistency in model-generated summaries, the quality of the synthetic data will degrade.

### Mechanism 2
- **Claim:** Knowledge distillation from a large LLM to a smaller model improves factual consistency evaluation performance.
- **Mechanism:** The LLM's reasoning abilities for factual consistency evaluation are distilled into a smaller, more practical model through training on the synthetic data generated by the LLM.
- **Core assumption:** The smaller model can effectively learn and generalize the factual consistency evaluation capability from the LLM's annotations.
- **Evidence anchors:**
  - [abstract] "Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher."
  - [section] "Our model also outperforms the×50 times larger FLAN-PaLM that we used as the teacher (84.9→ 87.8)."
- **Break condition:** If the student model cannot effectively learn from the LLM's annotations, the performance gains will not materialize.

### Mechanism 3
- **Claim:** TrueTeacher generalizes well to new domains and languages due to the use of model-generated summaries and multilingual LLMs.
- **Mechanism:** The diversity of model-generated summaries and the multilingual nature of the LLM teacher allow the method to generalize to different domains and languages without requiring language-specific components.
- **Core assumption:** The diversity in model-generated summaries and the multilingual capabilities of the LLM are sufficient for generalization.
- **Evidence anchors:**
  - [abstract] "Using the mFACE dataset, we also show that our method generalizes to multilingual scenarios. Results on the mFACE dataset... show improvements on 35 out of 45 languages."
  - [section] "We notice that previous work vary in their generated data size, the corpus used for data synthesis, the model used for training, and the evaluation sets... Conversely, TrueTeacher successfully generalizes to documents from new domains, which demonstrates its robustness."
- **Break condition:** If the diversity in model-generated summaries is insufficient or the LLM's multilingual capabilities are limited, the method may not generalize well.

## Foundational Learning

- **Concept:** Natural Language Inference (NLI) and its application to factual consistency evaluation in summarization.
  - **Why needed here:** Factual consistency evaluation is often cast as an NLI task, so understanding NLI is crucial for understanding how models evaluate factual consistency in summaries.
  - **Quick check question:** How does NLI help in evaluating factual consistency in summaries?

- **Concept:** Knowledge distillation and its application in machine learning.
  - **Why needed here:** TrueTeacher uses knowledge distillation to transfer the factual consistency evaluation capability from a large LLM to a smaller, more practical model.
  - **Quick check question:** What is knowledge distillation and how does it help in making large models more practical?

- **Concept:** Multilingual pre-training and its impact on model generalization.
  - **Why needed here:** TrueTeacher leverages multilingual pre-training to generalize to multiple languages without requiring language-specific components.
  - **Quick check question:** How does multilingual pre-training enable models to generalize to multiple languages?

## Architecture Onboarding

- **Component map:** Diverse T5 summarization models -> FLAN-PaLM 540B LLM teacher -> T5-11B student model -> TRUE benchmark and mFACE dataset
- **Critical path:** Generate diverse model-generated summaries → Label summaries using LLM → Train student model on synthetic data → Evaluate student model on factual consistency benchmark
- **Design tradeoffs:**
  - Using diverse summarization models increases the variability of model-generated summaries but also increases computational cost.
  - Using a large LLM as the teacher provides high-quality labels but increases computational cost during data generation.
  - Generating multilingual synthetic data increases the applicability of the method but also increases computational cost and data size.
- **Failure signatures:**
  - Poor performance on factual consistency evaluation if the LLM teacher cannot accurately label model-generated summaries.
  - Overfitting to the domain used for data synthesis if the diversity in model-generated summaries is insufficient.
  - Limited applicability to languages not well-represented in the LLM's training data.
- **First 3 experiments:**
  1. Evaluate FLAN-PaLM 540B on factual consistency evaluation to validate its capability as a teacher.
  2. Train a T5-11B model on the synthetic data generated using TrueTeacher and evaluate its performance on the TRUE benchmark.
  3. Compare TrueTeacher to existing synthetic data generation methods in a systematic study to demonstrate its superiority and robustness to domain shift.

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the performance of TrueTeacher-generated synthetic data compare when used to train models of varying sizes and architectures (e.g., BERT, RoBERTa, DeBERTa)?
- **Open Question 2:** What is the impact of using few-shot or chain-of-thought prompting when generating labels with the LLM teacher in TrueTeacher?
- **Open Question 3:** How does TrueTeacher perform on summarization datasets other than CNN/DailyMail and XSum, particularly those with different characteristics?
- **Open Question 4:** What is the optimal size of the synthetic dataset generated by TrueTeacher for achieving the best performance in factual consistency evaluation?

## Limitations
- **High computational cost:** TrueTeacher requires access to a 540B parameter LLM (FLAN-PaLM) for generating synthetic data, creating significant computational barriers for practical deployment.
- **Dependence on LLM quality:** The method's success depends heavily on the LLM's ability to accurately label factual consistency in model-generated summaries, which may vary across domains and languages.
- **Limited low-resource language evaluation:** While TrueTeacher shows improvements on 35/45 languages in mFACE, the performance on low-resource languages with limited representation in the LLM's training data remains unclear.

## Confidence
- **High confidence:** Knowledge distillation mechanism (student outperforming teacher) - strongly supported by experimental results showing 84.9→87.8 ROC-AUC improvement.
- **Medium confidence:** Generalization to multilingual scenarios - supported by improvements on 35/45 languages but lacks detailed error analysis per language.
- **Medium confidence:** Superiority over perturbed gold-summaries - conceptually sound but experimental comparison with recent methods could be more comprehensive.

## Next Checks
1. **Compute cost analysis:** Quantify the exact computational resources required to generate the 1.4M synthetic examples using FLAN-PaLM 540B, including GPU hours and associated costs, to assess practical feasibility.
2. **Error analysis on low-resource languages:** Conduct detailed analysis of performance degradation patterns on languages with minimal representation in the LLM's training data to identify specific failure modes.
3. **Ablation study on summary diversity:** Systematically vary the diversity of summarization models used to generate training data and measure the impact on out-of-domain generalization to quantify the importance of model diversity.