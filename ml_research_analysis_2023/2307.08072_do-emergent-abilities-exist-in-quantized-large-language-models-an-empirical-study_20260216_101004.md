---
ver: rpa2
title: 'Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical
  Study'
arxiv_id: '2307.08072'
source_url: https://arxiv.org/abs/2307.08072
tags:
- quantization
- letters
- performance
- name
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of quantization on emergent
  abilities of large language models (LLMs), including in-context learning, chain-of-thought
  reasoning, and instruction-following. Empirical experiments on LLaMA models show
  that 4-bit quantization preserves these abilities with minimal performance degradation,
  while 2-bit quantization causes severe drops.
---

# Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study

## Quick Facts
- arXiv ID: 2307.08072
- Source URL: https://arxiv.org/abs/2307.08072
- Reference count: 40
- Primary result: 4-bit quantization preserves emergent abilities with minimal performance degradation, while 2-bit quantization causes severe performance drops

## Executive Summary
This study investigates how quantization affects emergent abilities in large language models, focusing on in-context learning, chain-of-thought reasoning, and instruction-following capabilities. Through extensive experiments on LLaMA models across different bit precisions (2-bit, 4-bit, 8-bit, 16-bit), the research reveals that 4-bit quantization successfully preserves these emergent abilities with minimal performance degradation, while 2-bit quantization leads to severe performance drops approaching random levels. The study also identifies that feed-forward network components are particularly sensitive to quantization in low-bit models and demonstrates that fine-tuning can partially compensate for performance losses in quantized models.

## Method Summary
The study employs post-training quantization using the GPTQ method to quantize LLaMA models to 2-bit, 4-bit, 8-bit, and 16-bit precisions. Emergent abilities are evaluated across multiple datasets including MMLU, BBH, GSM8K, AutoEval, and WikiText. The research conducts component-level sensitivity analysis by preserving specific model components at higher precision during quantization. Fine-tuning experiments using both LoRA and full-parameter approaches are performed to assess performance recovery potential across different quantization levels.

## Key Results
- 4-bit quantization preserves emergent abilities with minimal performance degradation across all tested LLaMA model sizes
- 2-bit quantization causes severe performance degradation, with results approaching random levels for most tasks
- FFN components show higher sensitivity to quantization than attention components in low-bit models
- Fine-tuning after quantization can partially restore performance, particularly effective for 4-bit models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 4-bit quantization preserves emergent abilities while 2-bit causes severe performance degradation
- Mechanism: Quantization reduces precision of weights and activations; 4-bit retains enough fidelity to maintain model behaviors, while 2-bit loses critical information
- Core assumption: Emergent abilities depend on sufficient precision in model parameters to preserve learned representations
- Evidence anchors:
  - [abstract] "Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation"
  - [section 3.2] "However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels"
  - [corpus] Weak - no direct citation found for this mechanism in related papers
- Break condition: If emergent abilities are more sensitive to weight precision than activation precision, or if other components (like activations) are quantized more aggressively

### Mechanism 2
- Claim: FFN components are more sensitive to quantization than attention components in low-bit models
- Mechanism: FFN has larger weight matrices and more parameters; quantization errors accumulate more significantly in these layers
- Core assumption: Different model components have different sensitivity to quantization based on their size and role
- Evidence anchors:
  - [section 4.1.2] "We conducted test experiments to evaluate the quantization sensitivity of different model components, specifically attention and FFN components"
  - [section 4.1.2] "Results in Figure 2 demonstrate the FFN component exhibits substantial significance for 2-bit models"
  - [corpus] Weak - related papers discuss component-level quantization but don't specifically analyze FFN vs attention sensitivity
- Break condition: If other components (like attention) are found to be equally or more sensitive, or if model architecture changes alter sensitivity patterns

### Mechanism 3
- Claim: Fine-tuning after quantization can compensate for performance degradation in low-bit models
- Mechanism: Fine-tuning adapts quantized weights to the loss landscape, recovering some lost performance
- Core assumption: The quantized model retains enough capacity to be improved through adaptation
- Evidence anchors:
  - [section 4.2] "Experimental results demonstrate that fine-tuning can alleviate the performance degradation from low-bit quantization"
  - [section 4.2.2] "Post-quantization fine-tuning yields substantial performance improvement"
  - [corpus] Weak - related papers discuss quantization-aware training but not post-quantization fine-tuning specifically
- Break condition: If fine-tuning fails to recover performance or if quantization-induced errors are too severe to overcome through adaptation

## Foundational Learning

- Concept: Post-training quantization (PTQ)
  - Why needed here: Understanding how quantization methods like GPTQ work is essential to interpreting the results and designing experiments
  - Quick check question: What is the main difference between PTQ and quantization-aware training (QAT)?

- Concept: Emergent abilities in LLMs
  - Why needed here: The paper focuses on how quantization affects abilities like in-context learning, chain-of-thought reasoning, and instruction-following
  - Quick check question: What are the three key emergent abilities examined in this study?

- Concept: Transformer architecture components
  - Why needed here: The study analyzes quantization sensitivity at the component level (attention vs FFN), requiring understanding of these structures
  - Quick check question: What are the main subcomponents of the feed-forward network in a Transformer layer?

## Architecture Onboarding

- Component map:
  - Model weights: Quantized to 2/4/8/16-bit precision
  - Activations: Generally kept at higher precision due to outlier dimensions
  - Attention layers: Query, key, value, output projection matrices
  - Feed-forward networks: Gate, up, down projection matrices

- Critical path:
  1. Quantize model weights using GPTQ
  2. Evaluate emergent abilities on test datasets
  3. Analyze quantization sensitivity at component/substructure level
  4. Apply fine-tuning to compensate for performance loss

- Design tradeoffs:
  - Memory vs performance: Lower bit precision reduces memory but may degrade performance
  - Component preservation: Keeping certain components at higher precision can improve performance but increases memory usage
  - Fine-tuning cost: Full-parameter fine-tuning is more effective but computationally expensive compared to parameter-efficient methods

- Failure signatures:
  - Performance degradation approaching random levels (e.g., 0.25 for 4-choice classification)
  - Specific components showing high quantization error
  - Inability to recover performance through fine-tuning

- First 3 experiments:
  1. Quantize LLaMA models at different bit precisions (2/4/8/16-bit) and evaluate emergent abilities
  2. Analyze quantization sensitivity by preserving components at higher precision and measuring performance impact
  3. Apply fine-tuning (both pre- and post-quantization) to assess performance compensation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does 2-bit quantization completely eliminate emergent abilities in LLMs, or are there specific tasks or model scales where some abilities can still be preserved?
- Basis in paper: [explicit] The paper shows that 2-bit quantization leads to severe performance degradation across all tested emergent abilities, with results approaching random levels for most tasks and model sizes
- Why unresolved: While the study provides extensive empirical evidence of performance degradation, it does not definitively prove that no emergent abilities can exist in 2-bit models under any circumstances. There might be unexplored tasks, model architectures, or fine-tuning strategies that could preserve some emergent abilities
- What evidence would resolve it: Systematic testing of 2-bit models on a wider range of tasks and model scales, including more complex reasoning tasks or specialized domains, could reveal if any emergent abilities can be preserved. Additionally, exploring alternative quantization methods or fine-tuning techniques specifically designed for 2-bit models could provide insights into potential preservation of certain abilities

### Open Question 2
- Question: How do different components of the Transformer architecture contribute to the preservation or loss of emergent abilities during quantization, and can this knowledge be used to develop more effective quantization strategies?
- Basis in paper: [explicit] The paper analyzes the quantization sensitivity of different model components, particularly attention layers and feed-forward networks (FFN). It finds that FFN plays a crucial role in retaining model performance for low-bit quantization and that outlier dimensions in feature activations significantly impact quantization performance
- Why unresolved: While the study identifies important components and substructures, it does not provide a comprehensive understanding of how each component contributes to specific emergent abilities or how to optimize quantization strategies based on this knowledge
- What evidence would resolve it: Detailed ablation studies examining the impact of quantizing individual components on specific emergent abilities, coupled with the development and testing of component-specific quantization strategies, could provide a more complete understanding of the relationship between model structure and emergent abilities in quantized models

### Open Question 3
- Question: What is the long-term impact of quantization on the generalization and robustness of LLMs, and how does this affect their performance on out-of-distribution data or adversarial examples?
- Basis in paper: [inferred] The paper focuses on the immediate impact of quantization on emergent abilities but does not explore the long-term effects on model behavior or robustness. The use of fine-tuning techniques to compensate for quantization-induced performance degradation suggests potential concerns about model stability and generalization
- Why unresolved: The study does not provide insights into how quantization might affect the model's ability to handle novel or challenging inputs over time. This is particularly important for real-world applications where models may encounter data distributions different from their training or fine-tuning data
- What evidence would resolve it: Extensive testing of quantized models on out-of-distribution datasets and adversarial examples, along with long-term performance monitoring, could reveal the impact of quantization on model generalization and robustness. Additionally, investigating the relationship between quantization-induced performance degradation and model stability could provide insights into potential risks and mitigation strategies

## Limitations
- The study focuses only on LLaMA models without testing other architectures or model families
- Fine-tuning experiments use a limited set of datasets without exploring the full hyperparameter space
- The quantization sensitivity analysis only examines two main components without deeper substructure analysis
- The study does not investigate dynamic quantization methods or mixed-precision approaches

## Confidence
- High confidence: 4-bit quantization preserves emergent abilities with minimal performance degradation
- Medium confidence: component-level sensitivity analysis and its implications
- Low confidence: generalizability of fine-tuning effectiveness across all quantization levels

## Next Checks
1. **Cross-architecture validation**: Test the same quantization and fine-tuning protocols on different LLM architectures (e.g., OPT, BLOOM) to verify if the observed patterns hold beyond LLaMA models, particularly examining whether component sensitivity varies by architecture

2. **Ablation study on fine-tuning hyperparameters**: Systematically vary learning rates, batch sizes, and fine-tuning durations across all bit precisions to identify optimal fine-tuning configurations, with particular focus on understanding why 2-bit models show limited recovery despite fine-tuning

3. **Mixed-precision analysis**: Implement and evaluate a mixed-precision approach where different model components use different bit precisions (building on the sensitivity findings), measuring whether targeted preservation of sensitive components improves overall performance while maintaining memory benefits