---
ver: rpa2
title: Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation
arxiv_id: '2312.05508'
source_url: https://arxiv.org/abs/2312.05508
tags:
- adversarial
- soft
- robust
- fairness
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust fairness in adversarial
  training, where models exhibit strong robustness for some classes (easy classes)
  but weak robustness for others (hard classes). The authors propose a novel method
  called Anti-Bias Soft Label Distillation (ABSLD) to mitigate this issue.
---

# Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation

## Quick Facts
- arXiv ID: 2312.05508
- Source URL: https://arxiv.org/abs/2312.05508
- Reference count: 40
- Improves robust fairness in adversarial training by up to 3.00% compared to best baseline

## Executive Summary
This paper addresses the problem of robust fairness in adversarial training, where models exhibit strong robustness for some classes (easy classes) but weak robustness for others (hard classes). The authors propose Anti-Bias Soft Label Distillation (ABSLD), a novel method that adjusts the smoothness degree of soft labels for different classes during knowledge distillation by assigning different temperatures to different classes. This approach aims to reduce the error risk gap between classes, thereby improving robust fairness. The method combines adversarial training with knowledge distillation, using class-specific temperature adjustments to guide the student model's learning process.

## Method Summary
ABSLD is a knowledge distillation-based method that improves robust fairness in adversarial training by adjusting the smoothness of soft labels for different classes. It uses class-specific temperatures to re-temper the teacher's soft labels, with harder classes receiving sharper soft labels (lower temperature) and easier classes receiving smoother soft labels (higher temperature). The method simultaneously adjusts temperatures for both clean and adversarial examples using separate temperature sets (τt for clean and ˜τt for adversarial examples), ensuring robust fairness for both types of data. The student model is trained using a loss function that combines KL divergence between the student's predictions and the re-tempered teacher's soft labels for both clean and adversarial examples.

## Key Results
- ABSLD improves overall performance of robustness and fairness by up to 3.00% compared to best baseline methods
- Achieves better normalized standard deviation of robustness and fairness across different classes
- Demonstrates effectiveness on both CIFAR-10 and CIFAR-100 datasets with ResNet-18 and MobileNet-v2 architectures
- Outperforms state-of-the-art methods including SAT, TRADES, RSLAD, AdaAD, FRL, BAT, CFA, and Fair-ARD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adjusting the smoothness degree of soft labels for different classes (hard vs easy) reduces the error risk gap between classes, thereby improving robust fairness.
- Mechanism: The smoothness of soft labels, controlled by temperature in knowledge distillation, determines how much the model focuses on distinguishing classes. Sharper soft labels (lower temperature) for hard classes force the model to learn these classes more precisely, while smoother soft labels (higher temperature) for easy classes allow the model to be less precise on those classes. This differential treatment reduces the disparity in error risk between classes.
- Core assumption: The smoothness degree of soft labels directly influences the model's class-wise error risk during training.
- Evidence anchors:
  - [abstract]: "ABSLD adaptively reduces the student's error risk gap between different classes to achieve fairness by adjusting the class-wise smoothness degree of samples' soft labels during the training process..."
  - [section]: "Interestingly, we find that the smoothness degree of soft labels for different classes (i.e., hard class and easy class) can affect the robust fairness of DNN models from both empirical observation and theoretical analysis."
- Break condition: If the smoothness adjustment does not correlate with error risk reduction, or if class difficulty cannot be reliably determined during training.

### Mechanism 2
- Claim: Anti-Bias Soft Label Distillation (ABSLD) uses class-specific temperatures to re-temper the teacher's soft labels, guiding the student to learn harder classes more intensely.
- Mechanism: ABSLD assigns different temperatures to the teacher's soft labels for different classes based on the student's current error risk. Classes with higher error risk (hard classes) receive lower temperatures (sharper soft labels), while classes with lower error risk (easy classes) receive higher temperatures (smoother soft labels). This encourages the student to focus more on learning the harder classes.
- Core assumption: The teacher's soft labels, when re-tempered with class-specific temperatures, can effectively guide the student's learning process to achieve better robust fairness.
- Evidence anchors:
  - [abstract]: "Specifically, ABSLD adaptively reduces the student's error risk gap between different classes to achieve fairness by adjusting the class-wise smoothness degree of samples' soft labels during the training process..."
  - [section]: "ABSLD can adaptively adjust the smoothness degree of soft labels by re-temperating the teacher's soft labels for different classes, and each class has its own teacher's temperatures based on the student's error risk."
- Break condition: If the temperature adjustment mechanism fails to correlate with error risk reduction, or if the teacher's soft labels are not reliable for guiding the student.

### Mechanism 3
- Claim: By simultaneously adjusting temperatures for both clean and adversarial examples, ABSLD achieves robust fairness for both types of data.
- Mechanism: ABSLD uses two sets of teacher temperatures: one for clean examples (τt) and one for adversarial examples (˜τt). This dual adjustment ensures that the model achieves fairness not only in clean accuracy but also in adversarial robustness, addressing the fact that clean and adversarial examples of the same class may have different error risks.
- Core assumption: Clean and adversarial examples require separate temperature adjustments to achieve robust fairness for both types of data.
- Evidence anchors:
  - [section]: "Since clean examples and adversarial examples of the same classes may have different error risks during the training process, it is unreasonable to use the same set of class temperatures to adjust both clean examples and adversarial examples."
  - [section]: "Here we simultaneously adjust the student's clean error risk R(fs(xk)) and the student's robust error risk R(fs(˜xk)), in other words, we apply two different sets of teacher temperatures: τt k and ˜τt k, for the adjustment of the teacher's soft labels for clean examples and adversarial examples, respectively."
- Break condition: If separate temperature adjustments for clean and adversarial examples do not lead to improved robust fairness for both types of data.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: ABSLD is built within the framework of KD, using the teacher's soft labels to guide the student's learning. Understanding KD is essential to grasp how ABSLD adjusts the smoothness of soft labels.
  - Quick check question: How does the temperature parameter in KD affect the smoothness of the teacher's soft labels, and what is its impact on the student's learning?

- Concept: Adversarial Training (AT)
  - Why needed here: ABSLD is applied to improve robust fairness within adversarial training. Understanding AT is crucial to comprehend the context and the problem ABSLD addresses.
  - Quick check question: What is the primary goal of adversarial training, and how does it differ from standard training in terms of the optimization objective?

- Concept: Robust Fairness
  - Why needed here: ABSLD aims to improve robust fairness, which is the focus of the paper. Understanding what robust fairness means and why it is important is essential to appreciate the significance of ABSLD.
  - Quick check question: What is robust fairness, and why is it a concern in adversarial training? How does it differ from overall robustness?

## Architecture Onboarding

- Component map:
  Student model (fs) -> Anti-Bias Soft Label Generator -> Teacher model (ft) -> Temperature Adjustment Mechanism -> Loss Function (Labsld)

- Critical path:
  1. Initialize student model and teacher model.
  2. For each training epoch, compute the student's error risk for each class on both clean and adversarial examples.
  3. Adjust the teacher's temperatures for each class based on the student's error risk.
  4. Generate re-tempered soft labels using the adjusted temperatures.
  5. Update the student model using the loss function that compares the student's predictions to the re-tempered soft labels.

- Design tradeoffs:
  - Using class-specific temperatures allows for more precise control over the smoothness of soft labels, but it increases the complexity of the training process.
  - Separating temperature adjustments for clean and adversarial examples ensures robust fairness for both types of data, but it doubles the number of temperature parameters to manage.
  - The effectiveness of ABSLD depends on the reliability of the teacher's soft labels and the ability to accurately estimate the student's error risk for each class.

- Failure signatures:
  - If the temperature adjustment mechanism fails to correlate with error risk reduction, the robust fairness may not improve.
  - If the teacher's soft labels are not reliable, the student may learn incorrect patterns, leading to poor robust fairness.
  - If the class difficulty cannot be reliably determined during training, the temperature adjustments may be ineffective.

- First 3 experiments:
  1. Implement a basic version of ABSLD using a single temperature for all classes and compare its performance to standard adversarial training on CIFAR-10.
  2. Extend the implementation to use class-specific temperatures and evaluate its impact on robust fairness compared to the basic version.
  3. Implement the dual temperature adjustment for clean and adversarial examples and assess its effect on robust fairness for both types of data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ABSLD vary across different datasets and model architectures beyond CIFAR-10 and CIFAR-100?
- Basis in paper: [explicit] The authors state that "extensive experiments on different datasets and models demonstrate that our ABSLD can outperform state-of-the-art methods," but only present results for CIFAR-10 and CIFAR-100 with ResNet-18 and MobileNet-v2.
- Why unresolved: The paper lacks experiments on a broader range of datasets (e.g., ImageNet, SVHN) and model architectures (e.g., VGG, DenseNet) to fully validate the generalizability of ABSLD.
- What evidence would resolve it: Conducting experiments on diverse datasets and model architectures, comparing ABSLD's performance against state-of-the-art methods, and analyzing the consistency of improvements across different settings.

### Open Question 2
- Question: What is the impact of ABSLD on the computational efficiency and training time compared to baseline methods?
- Basis in paper: [inferred] The paper focuses on the effectiveness of ABSLD in improving robust fairness but does not discuss the computational overhead or training time compared to baseline methods.
- Why unresolved: Understanding the trade-off between improved robust fairness and computational efficiency is crucial for practical applications, especially for large-scale models and datasets.
- What evidence would resolve it: Providing a detailed analysis of the computational requirements, training time, and memory usage of ABSLD compared to baseline methods across different model sizes and datasets.

### Open Question 3
- Question: How does ABSLD perform against more diverse and adaptive adversarial attacks beyond those mentioned in the paper?
- Basis in paper: [explicit] The authors evaluate ABSLD against white-box attacks (FGSM, PGD, CW∞) and AutoAttack (AA), which includes four attacks. However, the robustness against other attack methods or adaptive attacks is not explored.
- Why unresolved: The robustness of ABSLD against a wider range of attack methods, including more recent and adaptive attacks, is not fully assessed, which limits the understanding of its true effectiveness.
- What evidence would resolve it: Conducting extensive experiments using a diverse set of attack methods, including adaptive attacks and black-box attacks, to evaluate the robustness of ABSLD and compare its performance against state-of-the-art methods.

## Limitations
- Effectiveness of ABSLD may be sensitive to the choice of teacher model and temperature adjustment hyperparameters, which are not extensively explored
- The method's computational efficiency and training time compared to baseline methods are not discussed, limiting understanding of practical applicability
- Robustness against a wider range of attack methods, including adaptive and black-box attacks, is not fully assessed

## Confidence
Medium confidence due to several key limitations:
- Claims about improving robust fairness through temperature-adjusted soft label distillation are supported by empirical results but lack direct mechanistic validation
- Corpus search revealed no directly comparable methods for validating the specific temperature adjustment mechanism
- Major uncertainties include how class difficulty is determined during training and the specific parameters of the temperature update rule

## Next Checks
1. Implement a controlled experiment comparing ABSLD with random temperature assignments to verify that the class-specific adjustment mechanism is essential for the observed improvements
2. Test ABSLD on additional datasets (e.g., Tiny ImageNet) to evaluate generalization beyond CIFAR
3. Conduct ablation studies isolating the impact of separate temperature adjustments for clean vs adversarial examples on overall robust fairness