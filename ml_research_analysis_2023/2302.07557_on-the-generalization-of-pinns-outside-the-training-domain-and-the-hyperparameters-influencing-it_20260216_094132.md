---
ver: rpa2
title: On the Generalization of PINNs outside the training domain and the Hyperparameters
  influencing it
arxiv_id: '2302.07557'
source_url: https://arxiv.org/abs/2302.07557
tags:
- training
- pinn
- domain
- generalization
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization capabilities of Physics-Informed
  Neural Networks (PINNs) outside their training domain and evaluates how various
  hyperparameters affect this generalization. The authors propose a novel error metric,
  the Generalization Level (Gl), to quantify how far outside the training domain a
  PINN can maintain accurate predictions.
---

# On the Generalization of PINNs outside the training domain and the Hyperparameters influencing it

## Quick Facts
- arXiv ID: 2302.07557
- Source URL: https://arxiv.org/abs/2302.07557
- Reference count: 35
- Primary result: Novel Generalization Level (Gl) metric quantifies PINN generalization beyond training domain; shallow architectures and smaller training domains improve generalization

## Executive Summary
This paper investigates how Physics-Informed Neural Networks (PINNs) generalize outside their training domain and identifies key hyperparameters affecting this generalization. The authors propose a novel Generalization Level (Gl) metric to quantify how far predictions remain accurate beyond the training area. Through extensive experiments on a 1D Poisson equation, they demonstrate that shallow and thin architectures, along with smaller training domains, significantly improve both generalization performance and training efficiency.

The study provides critical insights for PINN practitioners, suggesting that domain decomposition approaches combined with PINNs could be particularly effective. The results are statistically validated using the Kruskal-Wallis H test, demonstrating the significance of hyperparameter effects on generalization. These findings challenge conventional wisdom about neural network depth and width, showing that simpler architectures often outperform more complex ones for PDE solving tasks.

## Method Summary
The authors investigate PINN generalization using a 1D Poisson equation with source term f(x) = Σ(2k sin(2kx)) for k=1 to 5. They systematically vary network architecture (layers: 1, 4, 10, 20; neurons: 10, 20, 50, 100, 200, 400, 600), collocation points (18, 25, 36, 50, 100, 200 via Latin hypercube sampling), and training domain sizes across subdomains of [-π, π]. The training procedure uses 5000 Adam iterations followed by 5000 L-BFGS iterations. The custom Generalization Level metric measures prediction accuracy outside the training domain, and results are statistically validated using Kruskal-Wallis H test and Mann-Whitney U tests.

## Key Results
- Shallow and thin architectures (fewer layers and neurons) generalize better than deeper/wider ones
- Training on smaller domains significantly improves both generalization and training efficiency
- Collocation point density has minimal impact on generalization once a sufficient number is used
- Domain decomposition approaches combined with PINNs show promise for solving PDEs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PINNs trained on smaller domains can generalize better to adjacent regions than those trained on larger domains.
- Mechanism: Reducing the training domain size limits the complexity of the function the network must learn, leading to smoother predictions and reduced variability outside the training area.
- Core assumption: The target function's complexity is such that it can be accurately approximated on a smaller subdomain without losing critical information.
- Evidence anchors:
  - [abstract]: "training on smaller domains can significantly improve both generalization and training efficiency"
  - [section]: "the generalization level decays when the size of the training area increases, except for the smallest domain considered"
  - [corpus]: Weak - no direct corpus evidence for this specific claim.
- Break condition: If the PDE solution is highly oscillatory or has significant features within the reduced domain, the approximation will fail and generalization will degrade.

### Mechanism 2
- Claim: Shallower and thinner PINN architectures generalize better than deeper or wider ones.
- Mechanism: Smaller networks are less prone to overfitting and produce smoother, more stable predictions outside the training domain, while deeper networks tend to flatten predictions to constants.
- Core assumption: The target PDE solution does not require deep or wide architectures for accurate approximation.
- Evidence anchors:
  - [abstract]: "shallow and thin architectures perform better for generalization"
  - [section]: "the depth of the PINN appears to smoothen out the prediction of the network to a constant value outside the boundary of the training domain"
  - [corpus]: Weak - no direct corpus evidence for this specific claim.
- Break condition: If the target function is highly complex, shallow networks may underfit and fail to capture essential features, leading to poor generalization.

### Mechanism 3
- Claim: Increasing the number of collocation points does not significantly improve generalization once a sufficient number is used.
- Mechanism: Once the network can accurately approximate the target function within the training domain, additional collocation points provide diminishing returns for generalization outside the domain.
- Core assumption: The initial set of collocation points is sufficient to capture the essential features of the target function.
- Evidence anchors:
  - [abstract]: "collocation points, and domain size on generalization performance"
  - [section]: "the generalization level is not largely affected by the number of collocation points used, as long as enough points are provided"
  - [corpus]: Weak - no direct corpus evidence for this specific claim.
- Break condition: If the initial collocation points are poorly distributed or insufficient, increasing their number will be necessary to achieve good generalization.

## Foundational Learning

- Concept: Generalization in PINNs
  - Why needed here: Understanding how PINNs generalize outside their training domain is the core focus of the paper.
  - Quick check question: What is the difference between generalization in traditional ML and generalization in PINNs?

- Concept: Physics-informed neural networks (PINNs)
  - Why needed here: PINNs are the specific architecture being studied for their generalization capabilities.
  - Quick check question: How do PINNs incorporate physical laws into their training process?

- Concept: Hyperparameter tuning in neural networks
  - Why needed here: The paper investigates how various hyperparameters affect PINN generalization.
  - Quick check question: What are the most common hyperparameters that affect neural network performance?

## Architecture Onboarding

- Component map: Collocation points -> Physics-Informed Neural Network -> PDE solution approximation
- Critical path:
  1. Sample collocation points in the training domain
  2. Initialize PINN with chosen architecture and hyperparameters
  3. Train PINN to minimize the loss function
  4. Evaluate generalization outside the training domain using the proposed GL metric
- Design tradeoffs:
  - Network complexity vs. generalization: Shallower and thinner networks may generalize better but risk underfitting
  - Collocation points vs. training time: More points can improve accuracy but increase training time
  - Domain size vs. generalization: Smaller domains may improve generalization but risk missing important features
- Failure signatures:
  - Poor generalization outside the training domain
  - High variance in predictions
  - Slow convergence during training
  - Overfitting to the training data
- First 3 experiments:
  1. Train a PINN on a 1D Poisson equation with varying network widths and evaluate generalization
  2. Train PINNs on different domain sizes and compare generalization performance
  3. Vary the number of collocation points and assess their impact on generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the generalization level metric (GL) behave for higher-dimensional PDEs, and what are the computational challenges in extending this metric beyond 1D?
- Basis in paper: [explicit] The paper discusses the extension of GL to higher dimensions but notes it as computationally challenging and conceptually non-trivial.
- Why unresolved: The authors acknowledge the difficulty in extending the metric to higher dimensions due to computational complexity and the need for alternative formulations like distance-based metrics.
- What evidence would resolve it: Experimental results showing GL values for 2D or 3D PDEs, along with computational benchmarks comparing different metric formulations.

### Open Question 2
- Question: Does the optimal PINN architecture (shallow and thin) for generalization also perform best for training efficiency and accuracy in more complex PDEs beyond the 1D Poisson equation?
- Basis in paper: [explicit] The paper finds shallow and thin architectures perform better for generalization in a 1D Poisson equation but notes this may not generalize to more complex PDEs.
- Why unresolved: The study is limited to a single 1D test case, and the authors caution that results may not apply to more complex equations or higher dimensions.
- What evidence would resolve it: Comparative studies of PINN architectures on various PDEs (e.g., Navier-Stokes, advection-diffusion) measuring generalization, training time, and accuracy.

### Open Question 3
- Question: What is the relationship between domain size and the number of local minima in the loss landscape for PINNs, and how does this affect generalization?
- Basis in paper: [inferred] The authors observe that smaller domains lead to faster training and better generalization, suggesting fewer local minima, but do not explicitly analyze the loss landscape.
- Why unresolved: The paper does not provide a direct analysis of the loss landscape or its connection to domain size and generalization.
- What evidence would resolve it: Visualization or quantification of the loss landscape for different domain sizes, showing the number and depth of local minima and their correlation with generalization performance.

## Limitations
- Single 1D Poisson equation test case limits generalizability to other PDE types and higher dimensions
- Custom Generalization Level metric lacks validation against established ML generalization measures
- Computational cost of repeated training runs across hyperparameter combinations not fully addressed

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Shallow architectures perform better for generalization | High |
| Domain decomposition approaches show promise | Medium |
| Collocation point density has minimal effect on generalization | Low |

## Next Checks
1. Test the proposed generalization principles on additional PDEs (e.g., Navier-Stokes, heat equation) to verify if shallow architectures consistently outperform deep ones across different physics.

2. Extend experiments to 2D and 3D domains to assess whether the observed benefits of smaller training domains and simpler architectures persist in higher dimensions.

3. Compare the proposed Generalization Level metric against established ML generalization measures (e.g., cross-validation error) to establish its validity and practical utility.