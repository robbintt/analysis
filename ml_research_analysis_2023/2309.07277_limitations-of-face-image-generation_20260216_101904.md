---
ver: rpa2
title: Limitations of Face Image Generation
arxiv_id: '2309.07277'
source_url: https://arxiv.org/abs/2309.07277
tags:
- face
- faces
- image
- images
- realism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the efficacy and limitations of text-to-image
  diffusion models in generating face images. The authors develop a framework to audit
  the characteristics of generated faces conditioned on social attributes like demographics.
---

# Limitations of Face Image Generation

## Quick Facts
- arXiv ID: 2309.07277
- Source URL: https://arxiv.org/abs/2309.07277
- Reference count: 24
- Primary result: Analyzes limitations of text-to-image diffusion models in generating face images, including demographic disparities and distributional shifts

## Executive Summary
This paper investigates the efficacy and limitations of text-to-image diffusion models in generating face images, focusing on faithfulness to text prompts, demographic disparities, and distributional shifts. The authors develop a framework to audit face characteristics conditioned on social attributes like demographics, applying it to state-of-the-art diffusion models. Their findings reveal significant limitations in synthetic face generation, including degraded face verification performance, poor correlation between quantitative metrics and human assessment, and demographic biases stemming from training data selection. The study concludes that further research is needed to improve the quality and fairness of synthetic face generation.

## Method Summary
The authors generate face images using Stable Diffusion v2.1 and Realistic Vision Model 2 with semantic editing via SEGA. They create datasets with celebrity and non-celebrity identities across 8 demographic groups using prompts containing demographic attributes. The generated faces are evaluated through face verification accuracy comparisons with LFW, quantitative metrics (CLIP-I, DINO-I, CLIP-directional), and user studies via Qualtrics surveys. The study applies transformations for five semantic concepts and analyzes demographic disparities across all evaluation methods.

## Key Results
- Face verification accuracy degrades on synthetic faces compared to natural faces across all demographic groups
- CLIP-directional metrics poorly correlate with human assessment of face quality and transformation correctness
- Demographic disparities in synthetic face generation arise from biased training data distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic disparities in synthetic face generation arise from biased training data.
- Mechanism: Generative models learn to mimic the demographic distribution of their training data, which is typically biased toward certain groups due to internet sampling.
- Core assumption: Training data reflects real-world demographic imbalances, and diffusion models approximate this distribution.
- Evidence anchors:
  - [abstract] "We also develop an analytical model that provides insights into how training data selection contributes to the performance of generative models."
  - [section 5] "We assume training dataset S is drawn i.i.d. from biased data distribution DS... Distributions DS and D are only equivalent if α = β."
  - [corpus] Weak correlation; no explicit evidence in neighbors.
- Break condition: If training data is perfectly balanced or the model is explicitly trained to mitigate demographic bias, disparities may not emerge.

### Mechanism 2
- Claim: Face verification accuracy degrades on synthetic data due to distributional shift from natural faces.
- Mechanism: Face recognition models trained on natural data perform worse on synthetic faces because synthetic faces are sampled from a different distribution, even if they appear realistic.
- Core assumption: Face recognition models are sensitive to subtle distributional differences and cannot generalize perfectly to synthetic faces.
- Evidence anchors:
  - [section 4.1] "Across all demographic groups and datasets, with one exception, we observe that generated faces perform worse than natural faces (LFW)."
  - [section 5] "This proposition suggests that even with a large number of samples in S, it can be impossible to learn D exactly."
  - [corpus] No direct evidence; neighbors focus on generation quality, not verification accuracy.
- Break condition: If synthetic data is generated to closely match the natural distribution (e.g., via domain adaptation), verification accuracy may improve.

### Mechanism 3
- Claim: CLIP-directional metrics poorly correlate with human assessment of face quality.
- Mechanism: CLIP-directional metrics measure semantic changes but do not align with human perception of face quality, identity retention, or transformation correctness.
- Core assumption: CLIP embeddings capture visual semantics relevant to text prompts but not the nuanced human perception of face quality.
- Evidence anchors:
  - [section 4.3] "We also similarly reject null hypothesis 5 on the CLIP Directional metric... Hence, in the context of face recognition, image quality metrics are not a suitable proxy for humans."
  - [section 4.3] "Consistent with our findings, the CLIP-I metric is known to have low potential in distinguishing images with similar textual descriptions."
  - [corpus] Weak correlation; neighbors focus on generation quality, not metric evaluation.
- Break condition: If CLIP or similar models are explicitly trained to align with human perceptual judgments, metrics may become more reliable.

## Foundational Learning

- Concept: Gaussian Mixture Models (GMMs)
  - Why needed here: GMMs are used as a tractable proxy to model demographic groups and analyze how biased training data leads to biased generative models.
  - Quick check question: How does the Hellinger distance between two Gaussian distributions depend on their means and covariances?
- Concept: Total Variation Distance
  - Why needed here: Total variation distance is used to measure the discrepancy between the learned distribution and the true distribution in the analytical model.
  - Quick check question: Why is total variation distance a suitable measure for distributional discrepancy in generative models?
- Concept: CLIP Embeddings
  - Why needed here: CLIP embeddings are used as a metric to assess semantic similarity between images and text prompts, and to evaluate transformation correctness.
  - Quick check question: How do CLIP embeddings encode visual-semantic relationships, and what are their limitations?

## Architecture Onboarding

- Component map:
  - Text-to-Image Diffusion Models (e.g., Stable Diffusion v2.1, Realism) -> Semantic Guidance Image Editing (SEGA) -> Face Verification Networks (e.g., Facenet) -> Metric Embedding Networks (e.g., CLIP, DINO) -> User Study Platform (e.g., Qualtrics)
- Critical path:
  1. Generate diverse face images using diffusion models and SEGA
  2. Evaluate face verification accuracy on synthetic and natural datasets
  3. Conduct user study to assess image quality and transformation correctness
  4. Analyze quantitative metrics (CLIP-I, DINO-I, CLIP-directional) against user study results
- Design tradeoffs:
  - Using SEGA improves diversity but may introduce artifacts and reduce image quality
  - Manual filtering of non-celebrity identities is necessary but time-consuming and subjective
  - User studies provide direct human feedback but are expensive and not scalable
- Failure signatures:
  - Poor face verification accuracy indicates distributional shift or bias in generated data
  - Low correlation between metrics and user study results suggests metrics do not capture human perception
  - Artifacts or identity mismatch in transformed images indicate SEGA hyperparameters need tuning
- First 3 experiments:
  1. Generate 100 synthetic faces for each demographic group and compare face verification accuracy to LFW
  2. Apply SEGA transformations to 50 source images and evaluate transformation correctness using CLIP-directional and user study
  3. Vary the proportion of demographic groups in training data and analyze its impact on generated image quality and face verification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can face image quality metrics be developed that better align with human assessment?
- Basis in paper: [explicit] The authors observe that existing quantitative metrics correlate poorly with human assessment of face image quality.
- Why unresolved: Developing metrics that capture the nuances of human perception is challenging and requires further research.
- What evidence would resolve it: New quantitative metrics that demonstrate high correlation with human ratings in user studies.

### Open Question 2
- Question: How does the Own Race Effect (ORE) manifest in the perception of generated faces?
- Basis in paper: [explicit] The authors note that their user study results seem to disagree with ORE, which is the documented tendency of individuals to better recognize faces from within their racial group.
- Why unresolved: Understanding how ORE applies to generated faces requires further study of human perception mechanisms.
- What evidence would resolve it: Rigorous user studies examining perceived identity under semantic transformation and across different respondent demographics.

### Open Question 3
- Question: How does the size of the training dataset impact the performance of generative models?
- Basis in paper: [explicit] The authors conjecture that faces generated by diffusion models trained on larger datasets should close the gap in verification accuracy between LFW and synthetic datasets.
- Why unresolved: The relationship between dataset size and model performance is complex and requires empirical validation.
- What evidence would resolve it: Comparative analysis of generative models trained on datasets of varying sizes, evaluating face verification accuracy and image quality.

## Limitations

- Limited generalizability to other generative models and newer versions
- Theoretical analysis of training data bias lacks empirical validation on actual datasets
- No exploration of alternative metrics that better align with human perception

## Confidence

**High Confidence Claims**:
- Demographic disparities exist in synthetic face generation across all tested models and metrics
- Face verification accuracy degrades on synthetic faces compared to natural faces
- CLIP-directional metrics do not correlate well with human assessment of face quality

**Medium Confidence Claims**:
- Training data selection contributes to generative model performance (theoretical support, limited empirical validation)
- SEGA transformations introduce artifacts that affect image quality (supported by user studies but not quantitatively characterized)

**Low Confidence Claims**:
- The exact relationship between training data bias magnitude and generation bias (theoretical model only)
- Specific thresholds for when distributional shift becomes problematic for face verification (based on limited dataset comparisons)

## Next Checks

1. **Empirical Training Data Analysis**: Analyze the actual demographic distribution of popular face image datasets (LAION-Aesthetics, etc.) to quantify the training data bias that contributes to generation disparities.

2. **Cross-Model Validation**: Test the same experimental framework on newer diffusion models (e.g., SDXL, SD3) and alternative architectures (e.g., GAN-based generators) to assess generalizability of the limitations.

3. **Metric Development**: Design and validate new metrics that better align with human perception of face quality and transformation correctness, potentially incorporating perceptual loss functions or human-in-the-loop optimization.