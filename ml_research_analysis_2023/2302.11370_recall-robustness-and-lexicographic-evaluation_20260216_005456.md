---
ver: rpa2
title: Recall, Robustness, and Lexicographic Evaluation
arxiv_id: '2302.11370'
source_url: https://arxiv.org/abs/2302.11370
tags:
- recall
- relevant
- items
- metrics
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a principled definition of recall in ranked
  retrieval evaluation, grounding it in sensitivity to the position of the lowest-ranked
  relevant item. The authors define recall orientation formally and introduce total
  search efficiency (TSE) as a metric that precisely captures this orientation.
---

# Recall, Robustness, and Lexicographic Evaluation

## Quick Facts
- arXiv ID: 2302.11370
- Source URL: https://arxiv.org/abs/2302.11370
- Reference count: 40
- Primary result: Introduces TSE and lexirecall metrics for principled recall evaluation with improved discriminative power and robustness

## Executive Summary
This paper presents a principled definition of recall in ranked retrieval evaluation, grounding it in sensitivity to the position of the lowest-ranked relevant item. The authors define recall orientation formally and introduce total search efficiency (TSE) as a metric that precisely captures this orientation. To address TSE's low discriminative power, they develop lexirecall, a preference-based lexicographic evaluation method. Through extensive empirical analysis across 17 TREC tracks, they demonstrate that lexirecall is correlated with existing recall metrics while exhibiting substantially higher discriminative power and stability in the presence of missing labels compared to traditional metrics like R@1000 and R-precision.

## Method Summary
The paper introduces two key metrics: Total Search Efficiency (TSE) and lexirecall. TSE measures the sensitivity of a ranking to a user interested in finding every relevant item by using an exposure function at the position of the last relevant item. Lexirecall improves upon TSE by using lexicographic comparison across all relevant items to reduce ties while preserving robustness properties. The empirical validation uses 17 TREC tracks across 3 recommendation tasks and 17 information retrieval tasks, comparing these metrics against baselines including R@1000, R-precision, Average Precision, NDCG, and Reciprocal Rank.

## Key Results
- TSE precisely captures recall orientation by measuring sensitivity to the position of the lowest-ranked relevant item
- Lexirecall achieves substantially higher discriminative power than TSE while maintaining robustness to missing labels
- Lexirecall shows improved correlation with existing recall metrics and better stability in the presence of incomplete relevance judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TSE precisely captures recall orientation by measuring sensitivity to the position of the lowest-ranked relevant item
- Mechanism: The metric TSE(π, R) = e(p_m) uses the exposure function e at the position of the last relevant item, making it directly sensitive to recall valence
- Core assumption: All topheavy recall-level metrics share the property that their worst-case performance equals TSE
- Evidence anchors:
  - [abstract] "We formally define 'recall-orientation' as the sensitivity of a metric to a user interested in finding every relevant item"
  - [section] "Although simple, this definition of recall connects to both early work in position-based evaluation as well as recent work in technology-assisted review"
  - [corpus] Corpus shows papers discussing sensitivity of reciprocal rank and precision metrics, supporting the general framework
- Break condition: If exposure function e is not monotonically decreasing, the theoretical connection to worst-case performance breaks

### Mechanism 2
- Claim: Lexirecall improves discriminative power by using lexicographic comparison across all relevant items
- Mechanism: Lexirecall(π, π') uses leximin ordering to compare rankings based on the rank positions of all relevant items, not just the last one
- Core assumption: Leximin ordering reduces ties compared to TSE while preserving robustness properties
- Evidence anchors:
  - [abstract] "To address TSE's low discriminative power, they develop lexirecall, a preference-based lexicographic evaluation method"
  - [section] "Using lexirecall, we can define a ordering over unique rankings, which addresses the ties observed in TSE"
  - [corpus] Corpus contains papers on preference-based evaluation and lexicographic methods, validating the approach
- Break condition: If the number of relevant items m is very large relative to n, lexicographic comparison becomes computationally prohibitive

### Mechanism 3
- Claim: Lexirecall is robust to missing labels while maintaining high discriminative power
- Mechanism: By using preference-based evaluation over the full set of relevant items rather than metric thresholds, lexirecall degrades more gracefully when labels are missing
- Core assumption: Preference-based evaluation is inherently more robust to label degradation than metric-based evaluation
- Evidence anchors:
  - [abstract] "lexirecall... exhibits substantially higher discriminative power and stability in the presence of missing labels"
  - [section] "Effective evaluation methods are robust to missing relevance labels... lexirecall degrades comparably to metrics like AP and NDCG while existing recall metrics are more sensitive"
  - [corpus] Corpus includes papers on label degradation and robustness, supporting the general principle
- Break condition: If too many labels are missing such that fewer than two relevant items remain per query, preference-based evaluation cannot function

## Foundational Learning

- Concept: Monotonic exposure functions in topheavy recall-level metrics
  - Why needed here: This property ensures that the worst-off user is always associated with the lowest-ranked relevant item, enabling the TSE connection
  - Quick check question: Why does monotonically decreasing exposure guarantee that the last relevant item determines worst-case performance?

- Concept: Leximin ordering in social choice theory
  - Why needed here: Leximin provides the theoretical foundation for lexicographic comparison that reduces ties while preserving fairness properties
  - Quick check question: How does leximin differ from simple min-based comparison, and why does this difference matter for evaluation sensitivity?

- Concept: Preference-based versus metric-based evaluation
  - Why needed here: Understanding this distinction explains why lexirecall can achieve higher discriminative power and robustness to missing labels
  - Quick check question: What is the computational complexity difference between preference-based and metric-based evaluation, and how does this affect practical deployment?

## Architecture Onboarding

- Component map: Relevance projection module -> permutation imputation system -> exposure function calculator -> TSE metric engine / lexirecall comparator -> statistical sensitivity analyzer
- Critical path: For a given ranking and relevance set, the system must (1) project relevance to positions, (2) impute missing positions, (3) compute TSE or lexirecall, and (4) compare with baseline metrics
- Design tradeoffs: TSE offers theoretical elegance and robustness but suffers from low discriminative power; lexirecall addresses this at the cost of increased computational complexity from preference-based comparison
- Failure signatures: High tie rates indicate problems with discriminative power; poor agreement with existing metrics suggests implementation errors; sensitivity to label degradation indicates robustness issues
- First 3 experiments:
  1. Verify that TSE correctly identifies worst-case performance by testing on known permutations with single relevant items at different positions
  2. Compare tie rates between TSE and lexirecall on synthetic datasets with varying m and n to confirm improved discriminative power
  3. Test label degradation robustness by systematically removing labels from benchmark datasets and measuring changes in metric values and preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively gather comprehensive relevance judgments for recall-oriented evaluation in dynamic or extremely large corpora?
- Basis in paper: [explicit] The paper discusses that recall-oriented evaluation depends critically on having comprehensive relevance labels, but gathering these labels is time-consuming and challenging, especially in dynamic production environments or recommender systems where judgments are often personalized and incomplete
- Why unresolved: The paper acknowledges this as an open problem but doesn't provide solutions, noting that initiatives like TREC's pooling strategy don't address situations where relevance comes from behavioral feedback or where information needs are transient
- What evidence would resolve it: Development and empirical validation of new techniques for expanding labeled sets specifically for recall evaluation that work in dynamic environments, with quantitative comparisons showing improved comprehensiveness and reliability compared to existing methods

### Open Question 2
- Question: How predictable is worst-case performance based on performance at the top of rankings, and under what conditions does this relationship break down?
- Basis in paper: [inferred] The paper mentions that while there may be systematic behavior where top-ranking performance predicts worst-case performance for many systems or domains, this shouldn't be presumed, especially when worst-case performance is systematic among smaller-sized groups unlikely to appear at the top
- Why unresolved: The paper cautions against assuming predictability but doesn't empirically investigate when and why this relationship holds or fails, leaving open questions about the conditions under which top performance correlates with worst-case performance
- What evidence would resolve it: Empirical studies across multiple domains and system types showing when top-ranking performance is predictive of worst-case performance, including analysis of when and why this relationship fails, particularly for minority groups or less common information needs

### Open Question 3
- Question: What optimization strategies are most effective for directly optimizing lexicographic criteria like lexirecall in learning-to-rank systems?
- Basis in paper: [explicit] The paper mentions that optimizing for lexicographic criteria could be an alternative method for designing recall-oriented algorithms, suggesting approaches like optimizing metric lexirecall or using stochastic ranking techniques, but doesn't empirically evaluate these strategies
- Why unresolved: While the paper discusses theoretical approaches to optimization, it doesn't provide empirical evidence on which optimization strategies work best in practice for lexicographic criteria, leaving open questions about computational efficiency and effectiveness
- What evidence would resolve it: Empirical comparison of different optimization approaches (e.g., gradient-based methods for metric lexirecall vs. stochastic ranking techniques) across multiple recall-oriented tasks, showing which methods achieve better lexicographic performance while maintaining reasonable computational costs

## Limitations

- The theoretical framework assumes all topheavy recall-level metrics share the same worst-case performance as TSE, which may not account for all possible metric formulations
- The lexicographic approach introduces computational complexity that could become prohibitive for very large numbers of relevant items
- Empirical validation is limited to TREC tracks, and performance in other retrieval scenarios with different types of relevance judgments remains unexplored

## Confidence

**High Confidence**: The formal definition of recall orientation and its connection to TSE is well-grounded in the theoretical framework presented. The empirical results demonstrating improved discriminative power and robustness of lexirecall are clearly supported by the data.

**Medium Confidence**: The claim that all topheavy recall-level metrics share the same worst-case performance as TSE is theoretically sound but may not account for all possible metric formulations. The robustness claims to missing labels, while demonstrated, could benefit from more diverse degradation scenarios.

**Low Confidence**: The scalability of lexirecall for very large numbers of relevant items (m approaching n) is not thoroughly explored, and the computational complexity implications for real-time evaluation systems are not fully addressed.

## Next Checks

1. **Scalability Test**: Evaluate lexirecall's performance and computational requirements on synthetic datasets with varying ratios of relevant items (m) to total items (n), particularly in the regime where m is close to n

2. **Alternative Exposure Functions**: Test whether non-monotonically decreasing exposure functions break the TSE connection to worst-case performance, and identify which commonly used metrics might violate this assumption

3. **Cross-Domain Validation**: Apply the proposed metrics to non-TREC retrieval scenarios (e.g., web search, enterprise search) to validate their generalizability beyond the benchmark datasets used in the paper