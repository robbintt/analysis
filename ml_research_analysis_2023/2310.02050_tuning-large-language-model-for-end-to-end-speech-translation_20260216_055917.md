---
ver: rpa2
title: Tuning Large language model for End-to-end Speech Translation
arxiv_id: '2310.02050'
source_url: https://arxiv.org/abs/2310.02050
tags:
- speech
- translation
- training
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving end-to-end speech
  translation (E2E-ST) performance by leveraging large language models (LLMs). The
  authors propose LST, a multimodal model that combines a speech frontend, an adapter,
  and an LLM backend.
---

# Tuning Large language model for End-to-end Speech Translation

## Quick Facts
- arXiv ID: 2310.02050
- Source URL: https://arxiv.org/abs/2310.02050
- Authors: 
- Reference count: 18
- Primary result: LST-13B achieves SOTA BLEU scores of 30.39/41.55/35.33 on En-De/En-Fr/En-Es language pairs

## Executive Summary
This paper presents LST (Language model for Speech Translation), a multimodal model that leverages large language models for end-to-end speech translation. The approach uses a two-stage training strategy where a lightweight adapter first aligns speech representations with text embedding space, then both the adapter and LLM are fine-tuned for the translation task. Experiments on the MuST-C benchmark demonstrate that LST-13B achieves state-of-the-art performance across three language pairs, significantly surpassing previous models while using a simpler architecture than knowledge distillation approaches.

## Method Summary
LST combines a speech frontend (Wav2vec 2.0), an adapter, and an LLM backend (LLaMA2). The model is trained in two stages: modality adjustment, where the adapter aligns speech representations with text embedding space without updating LLM parameters, and downstream task fine-tuning, where both components are optimized for E2E-ST. The adapter consists of a length adapter (1D convolution) and a modality adapter (linear layer) that transforms speech features into the LLM's embedding space using soft prompts. The approach is evaluated on MuST-C with CTC-finetuned Wav2vec 2.0 large and LLaMA2 7B/13B models.

## Key Results
- LST-13B achieves SOTA BLEU scores of 30.39/41.55/35.33 on En-De/En-Fr/En-Es language pairs
- Scaling LLM backend from 7B to 13B improves performance from 34.05 to 35.22 BLEU
- Two-stage training strategy outperforms single-stage approaches and knowledge distillation methods
- CTC-finetuned Wav2vec 2.0 frontend provides superior speech representations compared to non-finetuned versions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage training is necessary for optimal E2E-ST performance
- Mechanism: The first stage performs modality alignment between speech and text embeddings without updating LLM parameters, preserving LLM's language understanding capabilities. The second stage fine-tunes both adapter and LLM for the specific translation task.
- Core assumption: LLM's general language understanding benefits from being preserved during initial modality alignment
- Evidence anchors:
  - [abstract] "Experimental results on the MuST-C speech translation benchmark demonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on En-De/En-Fr/En-Es language pairs, significantly surpassing previous models"
  - [section] "Results from Table 3 demonstrate that the final performance of strategies (a) and (b) is inferior to that of strategy (d). When considering the first stage alone, it can be viewed as performing prompt tuning on LLM"
  - [corpus] Weak - neighboring papers discuss similar two-stage approaches but lack direct comparative evidence
- Break condition: If adapter becomes capable of full modality transformation in first stage, or if LLM fine-tuning is applied earlier without performance degradation

### Mechanism 2
- Claim: CTC-finetuned Wav2vec 2.0 provides superior speech representations for E2E-ST
- Mechanism: Linguistic supervision during Wav2vec 2.0 pretraining aligns speech representations closer to text embedding space, reducing the burden on the lightweight adapter
- Core assumption: Speech representations closer to text space require less complex transformation for effective translation
- Evidence anchors:
  - [section] "Incorporating the CTC fine-tuned Wav2vec 2.0 large model as the frontend significantly enhances the model's performance, indicating linguistic supervision contained in the speech frontend plays a key role"
  - [section] "The CTC fine-tuned large model is obtained by finetuning the Wav2vec 2.0 large model using Librispeech data on ASR task"
  - [corpus] Weak - corpus neighbors mention Wav2vec 2.0 but don't directly compare CTC-finetuned vs non-finetuned versions
- Break condition: If adapter architecture becomes more sophisticated, or if other speech frontend approaches provide better alignment

### Mechanism 3
- Claim: Scaling LLM backend size improves E2E-ST performance
- Mechanism: Larger LLMs possess stronger foundational language capabilities that generalize better to the translation task, particularly for long speech sequences
- Core assumption: Larger LLMs maintain better reasoning capabilities across longer contexts
- Evidence anchors:
  - [section] "By simply scaling the LLM backend from 7B to 13B, the model performance can be significantly improved (34.05->35.22)"
  - [section] "The larger the LLM model, the higher the final performance achieved on the E2E-ST task. LLaMA2 13B outperforms the 7B version substantially"
  - [corpus] Weak - corpus neighbors discuss scaling but don't specifically address E2E-ST performance improvements
- Break condition: If diminishing returns occur beyond certain model sizes, or if architectural changes negate size benefits

## Foundational Learning

- Concept: Modality alignment in multimodal learning
  - Why needed here: Understanding how to bridge speech and text representations is fundamental to LST's approach
  - Quick check question: What architectural component performs the modality transformation between speech and text embeddings?

- Concept: Knowledge distillation in speech translation
  - Why needed here: Previous works used knowledge distillation; understanding this helps contextualize LST's simpler approach
  - Quick check question: How does LST's approach differ from knowledge distillation methods in terms of training complexity?

- Concept: Transfer learning from pretrained models
  - Why needed here: LST builds on pretrained Wav2vec 2.0 and LLaMA2, so understanding their capabilities and limitations is crucial
  - Quick check question: Why does the paper choose CTC-finetuned Wav2vec 2.0 over the original version?

## Architecture Onboarding

- Component map:
  - Speech → Wav2vec 2.0 → Length adapter → Modality adapter → Soft prompt → LLaMA2 → Translation

- Critical path: Speech features extracted by Wav2vec 2.0 are transformed by the length adapter (1D convolution) to match text sequence length, then the modality adapter (linear layer) aligns them with text embedding space, soft prompts are added, and the LLaMA2 LLM generates the translation.

- Design tradeoffs:
  - Simplicity vs. performance: LST uses a simple adapter rather than more complex architectures
  - Training efficiency vs. capability: Two-stage training preserves LLM capabilities but requires more training time
  - Model size vs. performance: Larger LLMs improve performance but increase computational requirements

- Failure signatures:
  - Poor modality alignment: Translation quality degrades if adapter cannot effectively bridge speech and text spaces
  - Overfitting in stage 2: LLM parameters may overfit to training data if learning rate is too high
  - Length mismatch: Translation quality drops if length adapter cannot adequately reduce speech sequence length

- First 3 experiments:
  1. Implement single-stage training (both stages combined) to verify the necessity of the two-stage approach
  2. Compare CTC-finetuned vs non-finetuned Wav2vec 2.0 to quantify the benefit of linguistic supervision
  3. Test different adapter architectures (e.g., adding non-linear layers) to explore the impact of more sophisticated transformations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The two-stage training strategy requires more computational resources than single-stage approaches, though the paper claims it's more efficient than knowledge distillation methods without quantifying this advantage.
- The approach's generalizability to language pairs beyond English-to-German/French/Spanish and other speech domains remains untested.
- The specific contribution of each adapter component (length vs. modality) to overall performance is not isolated through ablation studies.

## Confidence
**High Confidence:**
- The necessity of two-stage training for preserving LLM capabilities during modality alignment
- The performance improvement from scaling LLM backend from 7B to 13B
- The benefit of using CTC-finetuned Wav2vec 2.0 over non-finetuned versions

**Medium Confidence:**
- The claimed simplicity advantage over knowledge distillation approaches
- The generalizability of findings to language pairs beyond En→De/Fr/Es
- The sufficiency of the lightweight adapter architecture for modality transformation

**Low Confidence:**
- The specific contribution of each adapter component (length vs. modality) to overall performance
- The scalability of the approach to very long speech sequences beyond MuST-C's typical duration
- The impact of soft prompt design on final translation quality

## Next Checks
1. **Ablation Study on Adapter Complexity:** Implement and compare LST with alternative adapter architectures (e.g., adding non-linear layers, attention mechanisms) to isolate the contribution of architectural simplicity to performance gains.

2. **Computational Cost Analysis:** Measure and report training time, GPU memory usage, and inference latency for LST's two-stage approach versus single-stage alternatives to quantify the claimed efficiency benefits.

3. **Cross-Domain Generalization Test:** Evaluate LST on non-MuST-C datasets (e.g., FLEURS, VoxPopuli) with different language pairs and speech characteristics to assess robustness beyond the primary benchmark.