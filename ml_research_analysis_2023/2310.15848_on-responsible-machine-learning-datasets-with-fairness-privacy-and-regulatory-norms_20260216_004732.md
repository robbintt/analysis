---
ver: rpa2
title: On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory
  Norms
arxiv_id: '2310.15848'
source_url: https://arxiv.org/abs/2310.15848
tags:
- datasets
- dataset
- data
- privacy
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework to evaluate datasets for responsible
  machine learning through the lenses of fairness, privacy, and regulatory compliance.
  It proposes quantitative metrics to assess dataset fairness (based on diversity,
  inclusivity, and annotation reliability), privacy preservation (by identifying sensitive
  attributes), and regulatory compliance (institutional approval, consent, and expungement).
---

# On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms

## Quick Facts
- arXiv ID: 2310.15848
- Source URL: https://arxiv.org/abs/2310.15848
- Authors: 
- Reference count: 40
- Key outcome: The paper introduces a framework to evaluate datasets for responsible machine learning through the lenses of fairness, privacy, and regulatory compliance.

## Executive Summary
This paper proposes a quantitative framework to assess the responsibility of machine learning datasets across three dimensions: fairness, privacy, and regulatory compliance. The framework evaluates fairness through diversity, inclusivity, and annotation reliability metrics; assesses privacy by identifying sensitive attribute annotations; and measures regulatory compliance through institutional approval, subject consent, and expungement capabilities. When applied to 60 face and medical datasets, the evaluation reveals significant shortcomings in responsible dataset practices, with most datasets failing to meet responsible standards across all three dimensions. The authors provide recommendations for improving dataset responsibility and suggest modifications to existing dataset documentation practices to include questions on fairness, privacy, and regulatory compliance.

## Method Summary
The framework evaluates datasets through three quantitative metrics: fairness (combining inclusivity, diversity, and label reliability scores), privacy preservation (identifying sensitive attribute annotations), and regulatory compliance (checking for institutional approval, subject consent, and expungement facilities). The method was applied to 60 face and medical datasets, including 52 face-based biometric datasets and 8 chest X-ray medical datasets. The fairness score is calculated by multiplying inclusivity and diversity scores for each demographic attribute, then adding a label reliability component. Privacy preservation is assessed by identifying six types of potentially sensitive annotations. Regulatory compliance is evaluated based on three factors: institutional approval, subject consent, and data expungement capabilities.

## Key Results
- Most evaluated datasets failed to meet responsible standards across fairness, privacy, and regulatory compliance dimensions
- Face datasets showed particularly poor performance in fairness and privacy metrics compared to medical datasets
- Only a small fraction of datasets demonstrated adequate regulatory compliance with ethical safeguards
- The framework successfully identified specific areas where datasets require improvement in responsible data practices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset fairness can be quantified by combining inclusivity, diversity, and label reliability scores.
- Mechanism: The paper proposes a formula where fairness is the sum of products of inclusivity and diversity for each demographic attribute, plus a label reliability score. Inclusivity measures the presence of subgroups, diversity measures their distribution, and label reliability accounts for the source of annotations (self-reported, classifier-generated, or apparent).
- Core assumption: A balanced distribution of demographic subgroups leads to fairer model performance.
- Evidence anchors:
  - [abstract]: "To the best of our knowledge, this is the first framework that quantitatively evaluates the trustability of the data used for training ML models."
  - [section]: "Fairness across each of the demographics is measured between 0 to 1... By multiplying the inclusivity and diversity scores, we are providing information about the presence as well as distribution of samples corresponding to a given demographic."
  - [corpus]: Weak evidence. Related work focuses on fairness-aware graph learning and empirical analysis of privacy-fairness-accuracy trade-offs, but lacks direct support for the specific inclusivity-diversity-label formula.
- Break condition: If the assumption that balanced datasets lead to fairer performance is violated, or if label reliability does not correlate with fairness.

### Mechanism 2
- Claim: Privacy leakage can be quantified by identifying sensitive annotations in datasets.
- Mechanism: The paper identifies six types of annotations (name, sensitive attributes, accessories, critical objects, location, medical condition) that potentially leak private information. A privacy preservation score is calculated as the maximum possible minus the sum of present sensitive annotations.
- Core assumption: The presence of these specific annotations directly correlates with privacy leakage risk.
- Evidence anchors:
  - [abstract]: "We identify vulnerable label annotations that can lead to the leakage of private information and devise a mathematical formulation."
  - [section]: "For a dataset, we manually check for the presence of the annotations from the aforementioned list to estimate its privacy leakage, and a point is awarded for each attribute annotation."
  - [corpus]: Weak evidence. Related work on privacy-preserving AI and privacy-fairness-accuracy trade-offs exists, but lacks direct support for the specific six-annotation model.
- Break condition: If new types of privacy leaks are discovered that are not captured by the six identified annotations.

### Mechanism 3
- Claim: Regulatory compliance can be quantified by checking for institutional approval, subject consent, and data expungement facilities.
- Mechanism: The paper assigns a compliance score based on the presence of three factors: institutional approval (e.g., IRB), subject consent, and the ability to remove data upon request. Each factor contributes equally to the total score.
- Core assumption: These three factors are sufficient and necessary to determine regulatory compliance for ML datasets.
- Evidence anchors:
  - [abstract]: "Finally, we assess datasets for their degree of compliance with contemporary regulatory norms."
  - [section]: "The regulatory compliance score, R, in the dataset is quantified based on three factors--institutional approval (yes/no: the numerical value of 1/0), the subject's consent to the data collection (yes/no: the numerical value of 1/0), and the facility for expungement/correction of the subject's data from the dataset (yes/no: the numerical value of 1/0)."
  - [corpus]: Weak evidence. Related work on policy-driven AI and legal considerations for LLMs exists, but lacks direct support for the specific three-factor compliance model.
- Break condition: If new regulatory requirements emerge that are not captured by the three identified factors.

## Foundational Learning

- Concept: Demographic subgroup analysis
  - Why needed here: To quantify inclusivity and diversity in datasets, which are key components of the fairness score.
  - Quick check question: How would you calculate the inclusivity score for a dataset that contains images of only two ethnicities out of six predefined subgroups?

- Concept: Privacy risk assessment
  - Why needed here: To identify and quantify potential privacy leaks in datasets through sensitive annotations.
  - Quick check question: What are the six types of annotations identified as potential privacy leaks in the paper?

- Concept: Regulatory compliance frameworks
  - Why needed here: To understand and implement the three factors (institutional approval, subject consent, expungement) that determine regulatory compliance.
  - Quick check question: Why is the ability to remove data upon request an important factor in regulatory compliance?

## Architecture Onboarding

- Component map: Dataset → Fairness quantification → Privacy assessment → Regulatory compliance → Responsibility score
- Critical path: Dataset metadata → Demographic analysis → Annotation review → Compliance verification → Responsibility score calculation
- Design tradeoffs: Balancing the granularity of demographic subgroups with the practicality of data collection, and the comprehensiveness of privacy annotations with the usability of the dataset.
- Failure signatures: Low fairness scores indicating underrepresentation, high privacy leakage scores indicating sensitive information, and low regulatory compliance scores indicating lack of ethical safeguards.
- First 3 experiments:
  1. Apply the fairness quantification module to a publicly available face dataset and analyze the results.
  2. Use the privacy assessment module to identify potential privacy leaks in a medical imaging dataset.
  3. Evaluate the regulatory compliance of a biometric dataset using the compliance module and suggest improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we resolve the fairness-privacy paradox in dataset development, where sensitive attribute annotations aid fairness evaluation but potentially leak private information?
- Basis in paper: [explicit] The authors explicitly identify this as an open problem, noting that "The fairness-privacy paradox remains an open problem for datasets containing sensitive attribute information such as biometrics and medical imaging."
- Why unresolved: Current fairness algorithms require sensitive attribute labels for evaluation, while privacy preservation requires minimizing such information. The authors note this creates conflicting requirements for dataset development.
- What evidence would resolve it: Development of methods that can evaluate fairness without requiring sensitive attribute labels, or techniques that can preserve privacy while still enabling meaningful fairness assessment.

### Open Question 2
- Question: What quantitative methods can be developed to assess fairness in object-based datasets, similar to the demographic-based approach used for face datasets?
- Basis in paper: [inferred] The authors note that "While fairness and privacy issues persist across different data domains such as objects and scenes, current regulatory norms are designed in accordance with the impact on human individuals. We leave analysis on object-based datasets for future work."
- Why unresolved: The current framework focuses on demographic attributes (age, gender, ethnicity, skin tone) which are specific to human subjects. Object datasets may require different fairness metrics based on geographical representation or context.
- What evidence would resolve it: Development and validation of fairness metrics for object-based datasets that capture relevant biases and inequities in these domains.

### Open Question 3
- Question: How can we develop more effective mechanisms for expunging data from already-trained models while preserving model utility?
- Basis in paper: [explicit] The authors note that "Similarly, removing instances of data from already trained models requires unlearning techniques, which while being actively explored, are far from being perfect."
- Why unresolved: Current unlearning techniques are imperfect and may not completely remove the influence of specific data points while maintaining model performance.
- What evidence would resolve it: Demonstration of unlearning techniques that can reliably remove data subject information while maintaining acceptable model performance.

## Limitations

- The framework relies heavily on the availability and quality of demographic annotations, which many datasets lack
- The six-category privacy assessment may not capture all potential privacy leakage vectors
- The three-factor regulatory compliance model may not address all regulatory requirements across different jurisdictions

## Confidence

**Confidence: Low** for the completeness of the fairness quantification framework. While the paper introduces a novel approach combining inclusivity, diversity, and label reliability, the framework's effectiveness depends heavily on the availability and quality of demographic annotations. Many datasets lack comprehensive demographic information, which could lead to incomplete or misleading fairness assessments.

**Confidence: Medium** for the privacy assessment methodology. The six-category annotation framework provides a structured approach to identifying privacy risks, but it may not capture all potential privacy leakage vectors. The manual inspection process is subjective and could miss nuanced privacy concerns.

**Confidence: Medium** for the regulatory compliance evaluation. The three-factor model (institutional approval, subject consent, and expungement) covers essential ethical considerations but may not address all regulatory requirements across different jurisdictions and domains.

## Next Checks

1. **Validation of Fairness Metrics**: Apply the fairness quantification framework to additional datasets with varying demographic distributions to test its robustness and identify edge cases where the model may fail.

2. **Privacy Assessment Expansion**: Evaluate the privacy framework's effectiveness by testing it against datasets known to contain sophisticated privacy leakage vectors, such as face recognition datasets with identity attributes.

3. **Regulatory Compliance Mapping**: Map the three-factor compliance model against specific regulatory frameworks (e.g., GDPR, HIPAA) to identify gaps and ensure comprehensive coverage of legal requirements.