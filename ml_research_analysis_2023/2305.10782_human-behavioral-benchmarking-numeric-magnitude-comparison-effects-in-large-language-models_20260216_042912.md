---
ver: rpa2
title: 'Human Behavioral Benchmarking: Numeric Magnitude Comparison Effects in Large
  Language Models'
arxiv_id: '2305.10782'
source_url: https://arxiv.org/abs/2305.10782
tags:
- number
- effect
- llms
- distance
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether Large Language Models (LLMs) capture
  human-like number magnitude representations using behavioral benchmarks inspired
  by cognitive science. The authors evaluate six popular LLMs (BERT, RoBERTa, XLNet,
  GPT-2, T5, BART) across 12-24 layers using three input formats: lowercase number
  words, mixed-case number words, and digits.'
---

# Human Behavioral Benchmarking: Numeric Magnitude Comparison Effects in Large Language Models

## Quick Facts
- arXiv ID: 2305.10782
- Source URL: https://arxiv.org/abs/2305.10782
- Reference count: 40
- Key outcome: Large language models exhibit human-like number magnitude representations, showing distance, size, and ratio effects across multiple architectures and input formats, with particularly strong performance for digits.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) capture human-like number magnitude representations using behavioral benchmarks inspired by cognitive science. The authors evaluate six popular LLMs (BERT, RoBERTa, XLNet, GPT-2, T5, BART) across 12-24 layers using three input formats: lowercase number words, mixed-case number words, and digits. They test for three effects observed in humans: distance effect, size effect, and ratio effect. Using a linking hypothesis that maps vector similarity to reaction time, they find that LLMs show strong distance and ratio effects across all architectures and input formats, with particularly strong performance for digits. The size effect is only observed for digit inputs. Multidimensional scaling reveals varying correspondence between LLM representations and the logarithmically compressed mental number line of humans.

## Method Summary
The study uses six pre-trained LLMs (BERT, RoBERTa, XLNet, GPT-2, T5, BART) and extracts embeddings for number words ("one"-"nine") and digits ("1"-"9") in three formats. For each model and layer, pairwise cosine similarities are computed between all number representations. Three regression analyses test for distance effect (linear fit of similarity vs. numerical distance), size effect (linear fit of similarity vs. smaller number), and ratio effect (exponential fit of similarity vs. numerical ratio). Multidimensional scaling reduces embeddings to one dimension to compare with log-compressed mental number line, and residual analysis identifies deviations from expected positions.

## Key Results
- LLMs consistently show strong distance and ratio effects across all architectures and input formats, with R² values typically above 0.7
- Size effect appears only for digit inputs, not for word-based formats, suggesting format-dependent processing
- GPT-2 shows highest correlation (0.92) between its digit-based representations and human log-compressed mental number line
- Anomalous residuals for numbers 2, 5, and 9 across multiple models suggest systematic deviations from human-like representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn human-like number magnitude representations through statistical patterns in text alone
- Mechanism: The distributional statistics of number words and digits in natural language contain sufficient information to induce a mental number line (MNL) representation, even without specialized neural circuitry
- Core assumption: The frequency and co-occurrence patterns of numbers in text encode magnitude relationships in a way that can be extracted by general language models
- Evidence anchors: [abstract] "This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number representations of LLMs and their cognitive plausibility."

### Mechanism 2
- Claim: The linking hypothesis connecting vector similarity to reaction time is valid for evaluating LLM number representations
- Mechanism: Cosine similarity between number embeddings in LLM layers can serve as a proxy for comparison difficulty, with more similar representations indicating longer "reaction times" for magnitude comparisons
- Core assumption: The difficulty humans experience in comparing similar numbers is reflected in the representational similarity of those numbers in LLM embeddings
- Evidence anchors: [section] "Here, we choose a standard metric in distributional semantics: the cosine of the angle between the vectors"

### Mechanism 3
- Claim: Different input formats reveal distinct aspects of LLM number representation capabilities
- Mechanism: The tokenization and contextual processing of different number formats exposes whether LLMs have learned implicit numeration and whether certain formats are processed more like human magnitude representations
- Core assumption: The way LLMs process different surface forms of the same numerical concept reveals underlying representational differences that map to human cognitive processing
- Evidence anchors: [section] "Do the models show implicit numeration ('four' = '4'), i.e., do they exhibit these effects equally for all number symbol types or more for some types (e.g., digits) than others (e.g., number words)?"

## Foundational Learning

- Concept: Mental Number Line (MNL) theory
  - Why needed here: Provides the theoretical framework for understanding human number magnitude representations and the three effects being tested
  - Quick check question: What are the three primary effects (distance, size, ratio) that indicate human use of MNL representations, and how does each relate to the logarithmic compression of the number line?

- Concept: Linking hypotheses in cognitive modeling
  - Why needed here: Essential for translating between LLM internal representations (vector similarities) and human behavioral measures (reaction times)
  - Quick check question: Why is cosine similarity chosen as the linking measure, and what would be the implications if a different similarity metric produced contradictory results?

- Concept: Multidimensional scaling (MDS) for latent representation analysis
  - Why needed here: Allows visualization and quantification of how LLM number representations align with human MNL structure
  - Quick check question: How does anchoring the MDS solution to "1" on the left help evaluate log-compressed number line representations, and what anomalies in the GPT-2 results suggest about LLM number understanding?

## Architecture Onboarding

- Component map: Input → Tokenization → Layer extraction → Cosine similarity → Effect fitting → MDS visualization → Residual analysis

- Critical path: Input number formats are tokenized and passed to each LLM architecture, embeddings are extracted from all layers, cosine similarities are computed between all number pairs, appropriate regression models are fitted to test each effect, MDS reduces representations to one dimension, and residuals are analyzed to identify anomalies.

- Design tradeoffs: Using cosine similarity vs. other metrics (simpler but may miss nuances); analyzing all layers vs. selected layers (comprehensive but computationally expensive); focusing on numbers 1-9 vs. broader range (cognitively validated but limited scope)

- Failure signatures: Absence of distance effect across all architectures; size effect only appearing for digits but not words; inconsistent MDS correlations suggesting non-human-like representations; anomalies in specific numbers (2, 5, 9) across multiple models

- First 3 experiments:
  1. Verify distance effect by plotting normalized cosine similarity vs. absolute distance for each architecture and input format, checking for negative linear relationships
  2. Test size effect by plotting normalized cosine similarity vs. minimum number for same-distance comparisons, looking for positive linear relationships (digits only)
  3. Examine ratio effect by plotting normalized cosine similarity vs. ratio of numbers, fitting exponential decay and checking for negative exponential relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific neural mechanisms or training strategies could explain why LLMs show distance and ratio effects but not size effects for number words and mixed-case inputs?
- Basis in paper: [explicit] The paper explicitly states that "The size effect is also observed among models for the digit number format, but not for the other number formats" and notes this as a significant finding requiring explanation.
- Why unresolved: The paper identifies this as a limitation but does not provide a theoretical explanation for why the size effect appears only with digit inputs. The difference between number formats remains unexplained despite the fact that all three effects are observed in humans regardless of input format.
- What evidence would resolve it: Experimental evidence showing whether pre-training with mixed-format numerical data improves size effect representation for word-based inputs, or analysis of attention patterns across different input formats that could reveal why digits are processed differently.

### Open Question 2
- Question: How do LLMs process decimal numbers, negative numbers, and numbers greater than 9 with respect to magnitude representation effects?
- Basis in paper: [explicit] The paper explicitly states "We only study the three magnitude effects for the number word and digit denotations of the numbers 1 to 9" and "The effects for the number 0, numbers greater than 10, decimal numbers, negative numbers, etc. are beyond the scope of this study."
- Why unresolved: The study's scope was deliberately limited to single-digit numbers, leaving the generalization of findings to other number classes completely unexplored. This is particularly important since the logarithmic compression of the mental number line may behave differently for larger numbers or non-integer values.
- What evidence would resolve it: Testing the same three magnitude effects (distance, size, ratio) and MDS analysis on multi-digit numbers, decimals, and negative numbers to determine if LLMs show similar human-like representations across these numerical domains.

### Open Question 3
- Question: What explains the anomalous residuals observed for numbers 2, 5, and 9 in the MDS analysis, and do these patterns reflect genuine differences in how LLMs represent these numbers versus humans?
- Basis in paper: [explicit] The paper identifies "high deviation from expected outputs for the numbers 2, 5, 9" and notes these "anomalies are a target for future research," suggesting they may relate to "patterns observed in previous linguistics studies" about privileged status of 2 and special significance of 9.
- Why unresolved: While the paper notes potential linguistic explanations (Piraha and Mundurucu languages privileging 2, 9 as "bargain price numeral"), it does not empirically test whether these anomalies reflect genuine linguistic/cultural patterns in the training data or are artifacts of model architecture or training.
- What evidence would resolve it: Corpus analysis of numerical references in training data to determine if 2 and 9 appear in statistically significant privileged contexts, or ablation studies removing specific training data patterns to see if anomalies persist.

## Limitations

- Limited scope to numbers 1-9 prevents generalization to broader numerical understanding and logarithmic compression behavior for larger numbers
- Linking hypothesis using cosine similarity may not fully capture the complexity of human numerical processing or reaction time relationships
- Anomalous residuals for numbers 2, 5, and 9 suggest systematic differences from human cognition that require further investigation

## Confidence

**Confidence: High** for the core finding that LLMs learn numerical magnitude representations from text alone. The consistent observation of distance and ratio effects across six diverse architectures and three input formats, combined with the failure conditions identified in the mechanism analysis, provides strong evidence that general language models can extract numerical magnitude information from distributional statistics in natural language.

**Confidence: Medium** for the methodological approach. The use of multiple architectures, input formats, and analytical techniques (linear/exponential regression, MDS) provides robust evidence for systematic numerical representations in LLMs. However, the choice of cosine similarity as the linking measure, while standard in distributional semantics, may not be optimal for all numerical comparisons, and the absence of comparison with non-linguistic numerical processing models limits the interpretation of results.

**Confidence: Low** for claims about human-like numerical cognition in LLMs. While the paper demonstrates that LLMs exhibit distance and ratio effects similar to humans, the behavioral benchmarks used cannot definitively prove that LLMs possess genuinely human-like mental number line representations. The linking hypothesis connecting vector similarity to reaction time remains a theoretical construct that may not fully capture the complexity of human numerical processing.

## Next Checks

1. **Cross-linguistic validation**: Test the same models with number words in languages with different numerical naming systems (e.g., Chinese, French) to determine whether the observed effects are language-dependent or reflect universal numerical processing capabilities.

2. **Numerical range extension**: Expand testing beyond numbers 1-9 to include larger numbers (10-99) and fractions to assess whether LLMs maintain human-like representations across broader numerical domains and whether logarithmic compression persists.

3. **Comparative cognitive modeling**: Implement alternative linking hypotheses (Euclidean distance, dot product) and compare their predictive power for human reaction times in numerical comparison tasks to validate the chosen approach and identify potential limitations.