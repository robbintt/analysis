---
ver: rpa2
title: LLM Guided Inductive Inference for Solving Compositional Problems
arxiv_id: '2309.11688'
source_url: https://arxiv.org/abs/2309.11688
tags:
- rebel
- tool
- answer
- arxiv
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents REBEL, a recursive reasoning algorithm that
  leverages large language models (LLMs) to solve compositional problems requiring
  knowledge from external sources. The core idea is to recursively decompose questions
  into subquestions, utilize external tools (specified by natural language descriptions)
  to gather facts, and then synthesize these facts to infer the final answer.
---

# LLM Guided Inductive Inference for Solving Compositional Problems

## Quick Facts
- arXiv ID: 2309.11688
- Source URL: https://arxiv.org/abs/2309.11688
- Authors: 
- Reference count: 7
- Key outcome: REBEL achieves 27.6% average accuracy improvement over ReAct on Compositional Celebrities and 78% accuracy on FEVER

## Executive Summary
This paper presents REBEL, a recursive reasoning algorithm that leverages large language models (LLMs) to solve compositional problems requiring knowledge from external sources. The core idea is to recursively decompose questions into subquestions, utilize external tools (specified by natural language descriptions) to gather facts, and then synthesize these facts to infer the final answer. The REBEL algorithm employs dynamic planning and forward-chaining strategies to navigate complex reasoning tasks. The authors evaluate REBEL on three datasets and show that it outperforms state-of-the-art methods like ReAct on multi-hop fact retrieval tasks and compositional question answering.

## Method Summary
REBEL is a recursive reasoning algorithm that solves compositional problems by breaking down questions into subquestions, gathering facts from external tools, and synthesizing these facts to infer final answers. The algorithm employs dynamic planning and forward-chaining strategies, using a tool picker to select appropriate external tools for each subquestion, generating input for these tools using the LLM, and utilizing the tool outputs to answer subquestions. REBEL recursively decomposes questions until no further decomposition is possible, accumulates facts from each subquestion answer, and uses this memory to infer the final answer.

## Key Results
- REBEL achieves an average accuracy improvement of 27.6% over ReAct across five categories on Compositional Celebrities
- On FEVER, REBEL's accuracy is 78% compared to ReAct's 72%
- REBEL's performance on HotPotQA is slightly lower than ReAct (50% vs. 63%), attributed to generation of overly complex recursive trees that lose context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive decomposition with memory accumulation enables solving compositional problems that require combining multiple facts.
- Mechanism: The REBEL algorithm recursively breaks down a question into subquestions until no further decomposition is possible, accumulates facts from each subquestion answer, and uses this memory to infer the final answer.
- Core assumption: The LLM can effectively split questions into meaningful subquestions and use accumulated facts to reason toward the final answer.
- Evidence anchors:
  - [abstract] "REBEL allows LLMs to reason via recursive problem decomposition and utilization of external tools."
  - [section 3.1] "The split subroutine divides Questionn into Subquestions n with the size of Subquestions n being the number of subquestions that the LLM generates."
  - [corpus] Weak - no direct evidence of recursive decomposition success in related papers.
- Break condition: The recursion becomes too deep and loses context of the original question, leading to incorrect answers.

### Mechanism 2
- Claim: Dynamic planning and forward-chaining strategies enable navigation of complex reasoning tasks.
- Mechanism: REBEL uses a tool picker to select the appropriate external tool for each subquestion, generates input for the tool using the LLM, and uses the tool output to answer the subquestion.
- Core assumption: The LLM can effectively choose the right tool and generate appropriate input for each subquestion.
- Evidence anchors:
  - [abstract] "REBEL handles open-world, deep reasoning tasks by employing automated reasoning techniques like dynamic planning and forward-chaining strategies."
  - [section 3.3] "Here we evoke the LLM to decide what member of Tool List... would be best to decide the answer to a question."
  - [corpus] Weak - no direct evidence of dynamic planning and forward-chaining in related papers.
- Break condition: The LLM fails to choose the right tool or generate appropriate input, leading to incorrect answers.

### Mechanism 3
- Claim: REBEL improves upon state-of-the-art methods on multi-hop fact retrieval tasks and compositional question answering.
- Mechanism: REBEL's recursive approach with tool usage allows it to gather facts from disparate external sources and combine them to answer complex questions.
- Core assumption: REBEL's recursive approach with tool usage is more effective than sequential module invocation for multi-hop fact retrieval and compositional question answering.
- Evidence anchors:
  - [abstract] "The results show that REBEL outperforms state-of-the-art methods like ReAct on multi-hop fact retrieval tasks and compositional question answering."
  - [section 4.2.1] "We found that the REBEL system largely outperformed the ReAct system at all of the 5 categories that were experimented on for Compositional Celebrities."
  - [corpus] Weak - no direct evidence of REBEL's performance on related papers.
- Break condition: REBEL's recursive approach leads to increased latency and cost, making it less efficient than alternative methods.

## Foundational Learning

- Concept: Recursive problem decomposition
  - Why needed here: REBEL relies on recursively breaking down questions into subquestions to solve complex compositional problems.
  - Quick check question: How does REBEL determine when to stop recursively decomposing a question?
- Concept: External tool usage
  - Why needed here: REBEL uses external tools to gather facts from disparate sources, which are then combined to answer complex questions.
  - Quick check question: How does REBEL choose which external tool to use for each subquestion?
- Concept: Memory accumulation
  - Why needed here: REBEL accumulates facts from each subquestion answer in memory, which is then used to infer the final answer.
  - Quick check question: How does REBEL ensure that the accumulated facts are relevant to the final answer?

## Architecture Onboarding

- Component map: Question splitter -> Memory checker -> Tool picker -> Tool input generator -> Tool user -> Answer generator
- Critical path: Question splitter -> Memory checker -> Tool picker -> Tool input generator -> Tool user -> Answer generator
- Design tradeoffs: REBEL's recursive approach allows it to solve complex compositional problems but leads to increased latency and cost.
- Failure signatures: Incorrect answers due to failed tool usage, incorrect tool choice, or loss of context in deep recursion.
- First 3 experiments:
  1. Test REBEL on a simple compositional problem with a known answer to verify that it can correctly decompose the question and use external tools.
  2. Test REBEL on a more complex compositional problem with multiple subquestions to verify that it can handle deeper recursion.
  3. Compare REBEL's performance to a baseline method on a dataset of compositional problems to verify that it outperforms the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of REBEL compare to state-of-the-art methods when applied to tasks that require more than two-hop reasoning?
- Basis in paper: [inferred] The paper mentions that REBEL tends to generate a massive recursive tree of subquestions on questions that are significantly more than 2-hops, which can lead to loss of context and reduced accuracy.
- Why unresolved: The paper only tests REBEL on HotPotQA, which has questions that require synthesis of information from diverse sources but does not explicitly test on tasks requiring more than two-hop reasoning.
- What evidence would resolve it: Conducting experiments on datasets with tasks requiring more than two-hop reasoning and comparing the performance of REBEL to state-of-the-art methods.

### Open Question 2
- Question: How does the performance of REBEL vary with the depth of the recursive tree of subquestions generated?
- Basis in paper: [explicit] The paper mentions that the recursive nature of REBEL leads to increased latency and cost, and that it tends to over-complicate simple problems.
- Why unresolved: The paper does not provide any quantitative analysis of how the performance of REBEL varies with the depth of the recursive tree of subquestions generated.
- What evidence would resolve it: Conducting experiments to measure the performance of REBEL at different depths of the recursive tree of subquestions generated and analyzing the trade-off between performance and computational cost.

### Open Question 3
- Question: How does the performance of REBEL vary with the complexity of the tools used?
- Basis in paper: [explicit] The paper mentions that REBEL allows for an arbitrary tools to be added to it, and that the k-shot examples that are provided to the LLM for generating input given T ooln are designed around the 3 base tools.
- Why unresolved: The paper does not provide any analysis of how the performance of REBEL varies with the complexity of the tools used.
- What evidence would resolve it: Conducting experiments to measure the performance of REBEL with different sets of tools of varying complexity and analyzing the impact of tool complexity on performance.

## Limitations

- REBEL's recursive approach leads to increased latency and computational cost compared to alternative methods
- REBEL underperforms ReAct on HotPotQA, potentially due to loss of context in overly complex recursive trees
- The method's performance may degrade with tasks requiring more than two-hop reasoning due to context loss in deeper recursion

## Confidence

*High Confidence:*
- REBEL employs recursive decomposition to solve compositional problems
- REBEL uses external tools specified by natural language descriptions
- REBEL outperforms ReAct on Compositional Celebrities and FEVER datasets

*Medium Confidence:*
- REBEL's recursive approach with tool usage is more effective than sequential module invocation for multi-hop fact retrieval
- The decreased performance on HotPotQA is due to overly complex recursive trees

## Next Checks

1. **Efficiency Benchmark**: Measure and compare the latency and computational cost of REBEL versus ReAct on identical compositional tasks, quantifying the trade-off between accuracy gains and resource usage.

2. **Context Preservation Test**: Design a controlled experiment varying recursion depth on HotPotQA to empirically verify whether context loss at deeper levels explains the performance gap, and test if limiting recursion depth improves results.

3. **Tool Selection Robustness**: Evaluate REBEL's tool picker module by introducing controlled variations in tool descriptions and input generation prompts to assess how sensitive the system is to prompt engineering versus inherent reasoning capability.