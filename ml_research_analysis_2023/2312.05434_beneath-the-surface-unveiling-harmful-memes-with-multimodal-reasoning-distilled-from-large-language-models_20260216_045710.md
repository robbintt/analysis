---
ver: rpa2
title: 'Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled
  from Large Language Models'
arxiv_id: '2312.05434'
source_url: https://arxiv.org/abs/2312.05434
tags:
- meme
- harmful
- reasoning
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of detecting harmful memes, which
  is challenging due to the implicit meaning not explicitly conveyed through text
  and image. Existing methods oversimplify the problem as end-to-end classification,
  ignoring in-depth cognition of the meme content.
---

# Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models

## Quick Facts
- arXiv ID: 2312.05434
- Source URL: https://arxiv.org/abs/2312.05434
- Reference count: 24
- Key outcome: Novel generative framework learns multimodal reasoning from LLMs to improve harmful meme detection, outperforming state-of-the-art methods on three datasets.

## Executive Summary
This paper addresses harmful meme detection by proposing a two-stage generative framework that learns multimodal reasoning distilled from Large Language Models. Unlike end-to-end classification methods that oversimplify the task, the approach captures implicit semantic relationships between text and image through LLM-generated rationales. The method achieves superior performance on three meme datasets by first distilling reasoning knowledge and then using it for harmfulness inference.

## Method Summary
The approach consists of two training stages: first, a smaller language model is fine-tuned to generate rationales from multimodal inputs using abductive reasoning with LLMs; second, the generative framework conditions harmfulness prediction on the reasoning ability learned in stage one. The model uses CLIP-ViT for vision features, T5 encoder layers with cross-attention fusion, and a T5 decoder for output. Training involves generating rationales with ChatGPT and using them to train the downstream model through staged fine-tuning.

## Key Results
- Achieves superior performance compared to state-of-the-art methods on three meme datasets (Harm-C, Harm-P, FHM)
- Demonstrates effectiveness of multimodal reasoning distilled from LLMs for implicit meaning capture
- Shows two-stage training prevents mutual interference between reasoning and classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal reasoning distilled from LLMs improves harmful meme detection by capturing implicit semantic relationships that end-to-end classifiers miss.
- Mechanism: LLMs generate natural language rationales that reveal how image-text interplay leads to harmfulness, and these rationales train a smaller LM to perform similar reasoning before final classification.
- Core assumption: Abductive reasoning with LLMs produces accurate, non-hallucinated rationales that reflect true implicit meaning.
- Evidence anchors:
  - [abstract]: "we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning"
  - [section]: "we propose to utilize abductive reasoning with multimodal inputs to train smaller downstream models. LLMs can produce natural language rationales unveiling the implicit meaning beneath the surface of the memes to justify the reason why the meme is harmful or not"
  - [corpus]: Weak. Nearby papers focus on explainable detection or contrastive learning but don't detail rationale generation from LLMs.
- Break condition: If LLM rationales are hallucinated or too generic, the distilled knowledge becomes misleading.

### Mechanism 2
- Claim: Cross-attention fusion between vision and text in a smaller LM enables better multimodal reasoning than simple concatenation or late fusion.
- Mechanism: Vision features are attended to by text tokens through cross-attention layers, allowing semantic alignment and context-aware integration rather than treating modalities as independent inputs.
- Core assumption: Vision Transformers provide patch-level features that align semantically with language embeddings when fused via attention.
- Evidence anchors:
  - [section]: "we exploit a cross-attention mechanism (Luo et al., 2022) for multimodal fusion of the textual and visual information" and "we exploit a simple one-head cross-attention mechanism in each layer of the T5 encoder"
  - [section]: "the text features are utilized as the query, while the image features act as the key and value"
  - [corpus]: Weak. No explicit mention of cross-attention fusion in related work.
- Break condition: If vision features are too abstract or misaligned with text, cross-attention cannot create meaningful multimodal representations.

### Mechanism 3
- Claim: Two-stage training (reasoning distillation then harmfulness inference) prevents mutual interference and stabilizes multimodal reasoning learning.
- Mechanism: First stage learns to generate rationales from multimodal input; second stage conditions harmfulness prediction on the reasoning ability learned in stage one.
- Core assumption: Separating intermediate reasoning from final prediction allows the model to focus on each task without conflicting gradients.
- Evidence anchors:
  - [section]: "Reasoning Distillation is the predecessor fine-tuning phase of Harmfulness Inference" and "the prior reasoning knowledge absorbed in Reasoning Distillation could be well induced for Harmfulness Inference"
  - [section]: "This is because this setting causes mutual interference between intermediate reasoning and final prediction, which affects the convergence effect of harmfulness inference and damages the model's performance and stability"
  - [corpus]: Weak. Related papers don't describe staged training for meme reasoning.
- Break condition: If the two stages are too decoupled, knowledge transfer may not occur effectively.

## Foundational Learning

- Concept: Abductive reasoning
  - Why needed here: Memes often encode implicit meaning not directly stated; abductive reasoning infers the most plausible explanation for how image and text together convey harm.
  - Quick check question: Can you explain abductive reasoning in one sentence and give a meme example?

- Concept: Cross-attention fusion
  - Why needed here: Enables the language model to attend to relevant visual patches dynamically, integrating multimodal context rather than treating modalities separately.
  - Quick check question: What is the difference between cross-attention fusion and simple concatenation?

- Concept: Knowledge distillation from LLMs
  - Why needed here: LLMs have rich reasoning and background knowledge; distilling this into a smaller LM makes the model lightweight while retaining reasoning ability.
  - Quick check question: Why not just use the LLM directly for inference?

## Architecture Onboarding

- Component map: CLIP-ViT-B/32 (frozen) → Vision Extractor → patch-level image features; T5 encoder layers → text token embeddings; Cross-Attention Fusion → multimodal representation; T5 decoder → generates rationales or predicts harmfulness
- Critical path: Image/Text → Vision Extractor + Text Encoder → Cross-Attention Fusion → LM Decoder → Output
- Design tradeoffs:
  - Using CLIP captions as LLM input avoids vision modality complexity but risks information loss.
  - Two-stage training adds complexity but stabilizes reasoning learning.
  - Cross-attention is more expressive than late fusion but computationally heavier.
- Failure signatures:
  - Hallucinated rationales → poor harmfulness predictions
  - Cross-attention misalignment → multimodal fusion fails
  - Stage 1 underfitting → stage 2 lacks reasoning ability
- First 3 experiments:
  1. Run ablation w/o Reasoning Distillation to confirm degradation.
  2. Compare cross-attention fusion vs simple concatenation on validation set.
  3. Test single-stage training vs two-stage to quantify interference effects.

## Open Questions the Paper Calls Out

- Question: How effective is the proposed approach at handling memes that require rich background knowledge for interpretation?
  - Basis in paper: [inferred] The paper mentions that the model struggles with images that require rich background knowledge, such as recognizing the Ku Klux Klan members in one example.
  - Why unresolved: The paper provides a limited number of examples where the model fails due to lack of background knowledge, but does not conduct a systematic analysis of this limitation.
  - What evidence would resolve it: A comprehensive evaluation of the model's performance on memes that require various levels of background knowledge, including both common and specialized knowledge domains.

- Question: How does the performance of the model change when using different sizes of fine-tuned language models as the backbone?
  - Basis in paper: [explicit] The paper discusses the performance of different versions of the fine-tuned language model backbone, including Small, Base, and Large.
  - Why unresolved: While the paper provides results for different sizes, it does not explore the full range of possible model sizes or provide a detailed analysis of the trade-offs between model size and performance.
  - What evidence would resolve it: A systematic study of the model's performance across a wider range of fine-tuned language model sizes, including very small and very large models, with a focus on the relationship between model size and performance.

- Question: How can the model be improved to better handle the hallucination issue in the generated rationales?
  - Basis in paper: [inferred] The paper mentions that the model sometimes generates hallucinated rationales, which can mislead the harmfulness inference.
  - Why unresolved: The paper does not provide a detailed analysis of the hallucination issue or propose specific solutions to address it.
  - What evidence would resolve it: An investigation into the causes of hallucination in the generated rationales and the development of techniques to mitigate this issue, such as filtering mechanisms or improved training procedures.

## Limitations

- The approach relies on LLM-generated rationales that may contain hallucinations, potentially misleading the downstream model
- Cross-attention fusion details are underspecified, making it difficult to assess implementation-dependent performance variations
- Two-stage training necessity is asserted but not empirically validated through ablation studies

## Confidence

- High confidence: The core innovation of using LLM-generated rationales for multimodal reasoning distillation is technically sound and addresses a genuine limitation of end-to-end classification approaches.
- Medium confidence: The cross-attention fusion mechanism will improve performance, but exact gains depend on implementation details not fully specified.
- Low confidence: The necessity of two-stage training is asserted but not empirically validated through ablation studies.

## Next Checks

1. **Hallucination Audit**: Conduct human evaluation of LLM-generated rationales to measure hallucination rates and assess their correlation with downstream prediction errors.

2. **Single-Stage Training Experiment**: Implement and evaluate a single-stage training variant to empirically test whether the two-stage approach genuinely prevents interference or if the performance gain is primarily from the reasoning distillation itself.

3. **Cross-Attention Sensitivity Analysis**: Systematically vary cross-attention parameters (number of heads, fusion depth) to determine which aspects are critical for performance and whether simpler fusion approaches could achieve similar results.