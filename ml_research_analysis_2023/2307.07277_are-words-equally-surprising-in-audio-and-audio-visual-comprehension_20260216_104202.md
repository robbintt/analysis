---
ver: rpa2
title: Are words equally surprising in audio and audio-visual comprehension?
arxiv_id: '2307.07277'
source_url: https://arxiv.org/abs/2307.07277
tags:
- surprisal
- language
- n400
- information
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared cognitive effort during audio-only and audio-visual
  comprehension of the same spoken material using N400 ERP measures and surprisal
  estimates from different language models. It found that N400 responses were only
  weakly correlated across modalities, indicating that cognitive effort differed significantly
  between them.
---

# Are words equally surprising in audio and audio-visual comprehension?

## Quick Facts
- arXiv ID: 2307.07277
- Source URL: https://arxiv.org/abs/2307.07277
- Authors: 
- Reference count: 6
- Primary result: N400 responses were only weakly correlated across modalities, with transformer models (GPT-2) better predicting N400 in audio-only and 2-gram models better predicting N400 in audio-visual comprehension.

## Executive Summary
This study investigated whether words are equally surprising in audio-only versus audio-visual spoken language comprehension by measuring N400 event-related potentials and surprisal estimates from different language models. Using 103 BNC passages with natural prosody and facial expressions, the researchers recorded EEG from separate groups of participants in audio-only (n=27) and audio-visual (n=31) conditions. The results revealed that cognitive effort, as indexed by N400 amplitude, differed significantly between modalities, with weak correlations (r=0.25) between them. Crucially, the predictive power of language models for N400 varied by modality, with transformer models performing better in audio-only settings and 2-gram models excelling in audio-visual contexts, suggesting that multimodal integration shifts processing strategies toward local lexical context.

## Method Summary
The study used 103 naturalistic passages from the British National Corpus, recorded with natural prosody and facial expressions. EEG data were collected from 27 participants in audio-only and 31 in audio-visual conditions. Surprisal estimates were computed using n-gram (2-6) and Transformer models (GPT-2, BERT) on the transcribed passages. Linear mixed-effects regression models compared baseline models with surprisal+ROI and surprisal×ROI interactions to evaluate model performance across modalities.

## Key Results
- N400 responses were only weakly correlated across modalities (r = 0.25)
- GPT-2 surprisal better predicted N400 in audio-only comprehension
- 2-gram surprisal was more effective at predicting N400 in audio-visual settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal integration alters surprisal-driven cognitive effort by prioritizing local lexical context.
- Mechanism: In multimodal conditions, visual cues (prosody, gestures, mouth movements) provide additional semantic support, allowing listeners to rely more on immediate lexical co-occurrence patterns (2-grams) rather than broader context windows. This shifts the optimal language model from large-context transformers (GPT-2) to small-context n-gram models.
- Core assumption: Visual and auditory cues together offer complementary information that reduces the need for global context in surprisal estimation.
- Evidence anchors:
  - [abstract]: "Transformer-based models, which have access to a larger lexical context, provide a better fit in the audio-only setting, 2-gram language models are more effective in the multimodal setting."
  - [section]: "we observe that local lexical information is more prominent in the multimodal setting."
- Break condition: If visual cues are degraded or absent, multimodal benefit disappears and large-context models regain superiority.

### Mechanism 2
- Claim: ERP signatures differ between unimodal and multimodal comprehension due to distinct neural resource allocation strategies.
- Mechanism: Audio-only comprehension relies heavily on lexical predictability and global sentence context, reflected in N400 patterns best explained by GPT-2 surprisal. Audio-visual comprehension recruits additional visual-semantic integration processes, reducing reliance on global context and increasing sensitivity to immediate lexical associations, thus favoring 2-gram models.
- Core assumption: N400 reflects differential cognitive load that varies with the richness of available contextual cues.
- Evidence anchors:
  - [abstract]: "cognitive effort differs significantly between multimodal and unimodal settings."
  - [section]: "we observe that local lexical information is more prominent in the multimodal setting."
- Break condition: If the visual channel is uninformative or conflicting, N400 patterns may revert to audio-only profiles.

### Mechanism 3
- Claim: Surprisal estimates from language models are valid predictors of N400 only when the model's context window matches the cognitive processing strategy employed in the experimental modality.
- Mechanism: In audio-only settings, comprehension involves maintaining and integrating broad context, aligning with GPT-2's large context window. In audio-visual settings, immediate lexical integration dominates, aligning with 2-gram's limited context. Mismatches between model architecture and processing strategy reduce predictive validity.
- Core assumption: Cognitive processing strategies are modality-dependent and can be approximated by specific LM architectures.
- Evidence anchors:
  - [abstract]: "surprisal measures (which quantify the predictability of words in their lexical context) are generated on the basis of different types of language models... predict N400 responses for each word."
  - [section]: "our findings suggest that while Transformer-based models... provide a better fit in the audio-only setting, 2-gram language models are more effective in the multimodal setting."
- Break condition: If cognitive strategies shift unexpectedly (e.g., due to task demands), model fit will degrade.

## Foundational Learning

- Concept: N400 ERP component
  - Why needed here: N400 is the primary neural measure of semantic processing difficulty; understanding its properties is essential for interpreting surprisal model fits.
  - Quick check question: What is the typical latency range of the N400, and what does its amplitude reflect?

- Concept: Surprisal theory and language models
  - Why needed here: Surprisal theory links word predictability to processing effort; language models estimate word probabilities used to compute surprisal. Matching model architecture to cognitive strategy is central to the study.
  - Quick check question: How does surprisal differ when computed with a 2-gram vs a Transformer-based model?

- Concept: Multimodal integration in language comprehension
  - Why needed here: The study's core claim is that visual cues alter lexical processing strategies; understanding multimodal integration is key to interpreting results.
  - Quick check question: Which types of visual cues were shown to modulate N400 amplitude in prior work?

## Architecture Onboarding

- Component map: Stimulus preparation -> EEG recording -> Preprocessing -> Surprisal estimation -> Statistical modeling -> Interpretation
- Critical path: Stimulus preparation → EEG recording → Preprocessing → Surprisal estimation → Statistical modeling → Interpretation
- Design tradeoffs: Using all electrodes increases sensitivity but reduces specificity; choosing between additive and multiplicative surprisal × ROI models affects interpretability; selecting different n-gram orders balances granularity and overfitting risk.
- Failure signatures: Poor model fit (high AIC) may indicate mismatch between cognitive strategy and model architecture; weak N400 correlations may suggest modality effects or stimulus issues.
- First 3 experiments:
  1. Replicate the main analysis but test additional n-gram orders (7-gram, 8-gram) to confirm the local context advantage in multimodal settings.
  2. Conduct a control study with degraded visual cues (e.g., blurred video) to test whether multimodal benefit depends on cue informativeness.
  3. Compare GPT-2 and BERT surprisal fits in both modalities to isolate the effect of cloze-style training objectives vs autoregressive modeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Transformer-based language models compare to n-gram models in predicting N400 responses across different modalities when using naturalistic stimuli beyond the British National Corpus?
- Basis in paper: [explicit] The paper found that GPT-2 (a Transformer model) was better at predicting N400 in audio-only comprehension, while 2-gram models were better in audio-visual settings, but this was tested on a specific corpus.
- Why unresolved: The study used a limited corpus