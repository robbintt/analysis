---
ver: rpa2
title: 'Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive
  Review'
arxiv_id: '2308.04306'
source_url: https://arxiv.org/abs/2308.04306
tags:
- metaphor
- knowledge
- task
- recognition
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive review of knowledge injection
  approaches for metaphor detection using deep learning. It systematically summarizes
  the types of knowledge used (syntactic, semantic, and emotional) and how they are
  injected into models through methods like fine-tuning, additional inputs, output
  modulation, and multi-task learning.
---

# Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review

## Quick Facts
- **arXiv ID:** 2308.04306
- **Source URL:** https://arxiv.org/abs/2308.04306
- **Reference count:** 19
- **Key outcome:** This paper provides a comprehensive review of knowledge injection approaches for metaphor detection using deep learning, showing that knowledge injection significantly improves metaphor detection performance.

## Executive Summary
This paper systematically reviews knowledge injection approaches for metaphor detection in natural language processing. It categorizes knowledge types into syntactic, semantic, and emotional, and injection methods into fine-tuning, additional inputs, output modulation, and multi-task learning. The review demonstrates that knowledge injection significantly improves metaphor detection performance, with state-of-the-art results on benchmark datasets like VUA, MOH-X, and TroFi. The paper also identifies current challenges and future research directions, including refining metaphor definition criteria and improving injection methods.

## Method Summary
The paper provides a comprehensive review of knowledge injection approaches for metaphor detection using deep learning. It systematically summarizes the types of knowledge used (syntactic, semantic, and emotional) and how they are injected into models through methods like fine-tuning, additional inputs, output modulation, and multi-task learning. The review covers key datasets (VUA, MOH-X, TroFi) and evaluation metrics (precision, recall, F1, accuracy). The methodology involves categorizing existing approaches, analyzing their effectiveness, and identifying trends and challenges in the field.

## Key Results
- Knowledge injection significantly improves metaphor detection performance, with some models achieving state-of-the-art results on benchmarks
- Different types of knowledge (syntactic, semantic, emotional) can be effectively injected through various methods (fine-tuning, additional inputs, output modulation, multi-task learning)
- Current challenges include refining metaphor definition criteria, improving injection methods, and exploring emotional and multilingual knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge injection improves metaphor detection by providing richer semantic and syntactic context to the model
- Mechanism: The model is provided with external knowledge sources such as VerbNet, FrameNet, WordNet, and emotional knowledge, which are integrated through fine-tuning, additional inputs, output modulation, and multi-task learning
- Core assumption: The additional knowledge sources contain relevant information that the base model lacks, and this information can be effectively integrated to improve performance
- Evidence anchors:
  - [abstract] "knowledge injection significantly improves metaphor detection performance, with some models achieving state-of-the-art results on benchmarks"
  - [section 3.2] "This type of approach aims to enhance the model's understanding of the context by inputting knowledge into the model along with the text to be detected"
- Break condition: If the external knowledge sources do not contain relevant information for the specific metaphor detection task, or if the integration methods are not effective

### Mechanism 2
- Claim: Multi-task learning with related tasks (e.g., word sense disambiguation, sentiment analysis) can improve metaphor detection by leveraging shared representations
- Mechanism: The model is trained on multiple tasks simultaneously, allowing it to learn shared representations that are beneficial for all tasks, including metaphor detection
- Core assumption: The related tasks share some underlying representations with metaphor detection, and learning these representations can improve performance on the metaphor detection task
- Evidence anchors:
  - [section 3.4] "Introducing other associated tasks can effectively promote knowledge fusion between tasks, thus helping to improve metaphor recognition performance"
  - [section 3.4] "For metaphor detection and lexical disambiguation input s = (w0, ..., wn), s' = (w'0, ..., w'n), (Le et al., 2020) have used different models to extract features with respectively: H_wsd = f_wsd_b(s), H_md = f_md_b(s')"
- Break condition: If the related tasks do not share underlying representations with metaphor detection, or if the multi-task learning approach is not effective

### Mechanism 3
- Claim: Adjusting model outputs based on predefined knowledge information can improve metaphor detection by guiding the model's attention to specific semantic content or syntactic structures
- Mechanism: The model's outputs are adjusted based on the introduced knowledge, such as by assigning different weights to model outputs based on the introduced knowledge
- Core assumption: The predefined knowledge information is relevant to the metaphor detection task, and adjusting the model outputs based on this knowledge can improve performance
- Evidence anchors:
  - [section 3.3] "Pre-defined knowledge information can not only be used as input to the model, but also direct its attention to specific semantic content or syntactic structures when adjusting the model output"
  - [section 3.3] "For the text output feature: H = (h1, h2, ..., hn), its final output is: h'_i = 1/n * sum(h_i, i in C_n)"
- Break condition: If the predefined knowledge information is not relevant to the metaphor detection task, or if adjusting the model outputs based on this knowledge does not improve performance

## Foundational Learning

- **Concept:** Understanding of metaphor and its detection in natural language processing
  - **Why needed here:** The paper is focused on metaphor detection using knowledge injection, so a solid understanding of metaphor and its detection is crucial
  - **Quick check question:** What are the key characteristics of metaphor that make it challenging to detect automatically?

- **Concept:** Knowledge injection techniques in deep learning
  - **Why needed here:** The paper explores various knowledge injection techniques for metaphor detection, so understanding these techniques is essential
  - **Quick check question:** What are the main methods for injecting knowledge into deep learning models, and how do they differ?

- **Concept:** Multi-task learning in deep learning
  - **Why needed here:** The paper discusses the use of multi-task learning for metaphor detection, so understanding this concept is important
  - **Quick check question:** How does multi-task learning work in deep learning, and what are its potential benefits and challenges?

## Architecture Onboarding

- **Component map:** Base deep learning model (e.g., BERT) -> External knowledge sources (e.g., VerbNet, FrameNet, WordNet) -> Knowledge injection methods (e.g., fine-tuning, additional inputs, output modulation, multi-task learning) -> Metaphor detection output
- **Critical path:** Processing of input text through base model and integration of external knowledge, followed by classification as metaphorical or non-metaphorical
- **Design tradeoffs:** Choice of base model, selection of external knowledge sources, and integration methods used can impact performance, complexity, and interpretability
- **Failure signatures:** Poor performance on metaphor detection, overfitting to training data, or difficulty integrating external knowledge sources
- **First 3 experiments:**
  1. Train base model (e.g., BERT) on metaphor detection task without knowledge injection to establish baseline performance
  2. Integrate one external knowledge source (e.g., VerbNet) using one knowledge injection method (e.g., fine-tuning) and evaluate performance improvement
  3. Compare performance of different knowledge injection methods (e.g., fine-tuning vs. additional inputs) for same external knowledge source to determine most effective approach

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop a more refined and dynamic definition of metaphors that accounts for lexical lag and contextual variations across different time periods?
- **Basis in paper:** Explicit - The paper discusses the issue of lexical lag in metaphor definition criteria, noting that current methods treat literal meanings as more basic and ignore the time-sensitive nature of metaphors
- **Why unresolved:** Current metaphor detection systems rely on outdated labeling methods and lack a clear basis for defining literal meanings when introducing knowledge. This results in inaccurate judgments and limits the model's ability to capture evolving metaphorical expressions
- **What evidence would resolve it:** Developing and validating a dynamic metaphor definition framework that incorporates temporal context and adapts to changing linguistic patterns would resolve this question. Testing this framework on datasets spanning different time periods would provide empirical evidence of its effectiveness

### Open Question 2
- **Question:** What are the most effective knowledge injection strategies for fully utilizing the rich contextual information in knowledge resources and improving metaphor detection performance?
- **Basis in paper:** Explicit - The paper identifies the need for improving knowledge injection methods, noting that direct input or output modulation may not fully utilize the contextual information in knowledge
- **Why unresolved:** While some progress has been made in developing alternative knowledge injection strategies, there is still a lack of a comprehensive understanding of the most effective approaches
- **What evidence would resolve it:** Systematic evaluation and comparison of different knowledge injection strategies on diverse metaphor detection datasets would provide evidence of their relative effectiveness

### Open Question 3
- **Question:** How can we effectively incorporate emotional knowledge into metaphor detection models to capture the interplay between metaphor and emotion?
- **Basis in paper:** Explicit - The paper discusses the close association between textual emotions and metaphors, citing studies that have shown metaphorical texts contain more and stronger emotions
- **Why unresolved:** The application of emotional knowledge in metaphor detection is still in its early stages, and there is a lack of understanding of how to effectively capture the complex relationship between metaphor and emotion
- **What evidence would resolve it:** Developing and evaluating models that effectively incorporate emotional knowledge, considering contextual factors and addressing affective ambiguity, would provide evidence of their impact on metaphor detection performance

## Limitations

- The review methodology may not capture the full landscape of knowledge injection approaches, as corpus analysis revealed only 25 related papers with limited citation impact
- The paper doesn't provide direct experimental validation of why specific knowledge types or injection methods work better than others
- Limited discussion of computational costs and insufficient analysis of failure cases or limitations of current approaches

## Confidence

- **High confidence** in the general finding that knowledge injection improves metaphor detection performance, based on multiple state-of-the-art results reported across benchmark datasets
- **Medium confidence** in capturing the full landscape of knowledge injection approaches, as the corpus analysis revealed only 25 related papers with limited citation impact
- **Medium confidence** in the proposed mechanisms linking knowledge injection to improved performance, due to the indirect nature of evidence

## Next Checks

1. **Experimental validation of knowledge type effectiveness:** Design controlled experiments comparing the impact of syntactic vs semantic vs emotional knowledge injection on metaphor detection performance across multiple datasets, isolating the contribution of each knowledge type

2. **Implementation replication study:** Select 2-3 representative knowledge injection approaches from the review and implement them from scratch to verify claimed performance improvements and identify practical challenges not discussed in the original papers

3. **Cross-linguistic generalization test:** Evaluate whether knowledge injection approaches that work well on English metaphor detection generalize to other languages with different metaphor structures and available knowledge resources