---
ver: rpa2
title: Emergent Representations of Program Semantics in Language Models Trained on
  Programs
arxiv_id: '2305.11169'
source_url: https://arxiv.org/abs/2305.11169
tags:
- program
- semantics
- semantic
- programs
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents evidence that language models (LMs) trained
  on programs can learn to represent the formal semantics of programs, despite being
  trained only on next-token prediction. The authors train a Transformer model on
  a synthetic corpus of programs in a domain-specific language for navigating 2D grid
  world environments, where each program is preceded by a (partial) specification
  in the form of input-output grid world states.
---

# Emergent Representations of Program Semantics in Language Models Trained on Programs

## Quick Facts
- arXiv ID: 2305.11169
- Source URL: https://arxiv.org/abs/2305.11169
- Reference count: 12
- Key outcome: Language models trained on programs can learn to represent formal semantics despite being trained only on next-token prediction

## Executive Summary
This paper presents evidence that language models trained on programs can learn to represent formal semantics, despite being trained only on next-token prediction. The authors train a Transformer model on a synthetic corpus of programs in a domain-specific language for navigating 2D grid world environments, where each program is preceded by a (partial) specification in the form of input-output grid world states. They then probe the trained model's hidden states to extract representations of unobserved intermediate grid world states, finding that semantic information emerges in the model's representations over the course of training and correlates strongly with the model's ability to generate correct programs.

## Method Summary
The authors train a 350M parameter Transformer model on a synthetic corpus of one million Karel programs, each with input-output specifications. They use greedy decoding to generate programs and train linear probes to extract semantic state information from the model's hidden states. To distinguish between semantics represented in the model versus learned by the probe, they develop an interventional experiment where they swap the semantics of operations while preserving syntax. They also compare program lengths and perplexity between generated and reference programs to assess distributional differences.

## Key Results
- A linear probe can extract increasingly accurate representations of unobserved intermediate grid world states from the LM's hidden states over the course of training
- The variability in the LM's ability to synthesize correct programs is almost completely explained by the semantic content of the LM's hidden layers (R² = 0.968)
- When intervening on the semantics of the language while preserving the lexicon and syntax, the probe's ability to extract semantics degrades significantly
- The LM learns to generate correct programs that are, on average, shorter than those in the training set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LM learns to represent semantic states through the emergent correlation between hidden states and program state trajectories during next-token prediction
- Mechanism: During training, the LM's hidden states develop linear encodings of the program's semantic state (e.g., robot facing direction) as a byproduct of modeling the next token distribution. This occurs because the correct next token depends on the current semantic state, creating pressure for the model to encode this information in its representations
- Core assumption: The semantic state is necessary for accurate next-token prediction in the Karel domain
- Evidence anchors: [abstract] "we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training"; [section 3] "the variability in the LM's ability to synthesize correct programs is almost completely explained by the semantic content of the LM's hidden layers" (R² = 0.968)

### Mechanism 2
- Claim: The LM represents future semantic states, not just current ones, enabling predictive generation
- Mechanism: The model's representations encode not only the current program state but also predict future states corresponding to tokens yet to be generated. This creates a forward-looking representation that guides the generation process
- Core assumption: The generation process benefits from anticipating future program states, similar to how human speech production involves preverbal messages
- Evidence anchors: [section 3] "we train a linear probe to predict future semantic states from model states" and find "strong correlation between the semantic content of future states and the generative accuracy"; [section 3] "the probe's ability to extract future semantic states from the model states cannot be explained by simply extrapolating from a representation of the current semantic state"

### Mechanism 3
- Claim: The interventional experiment distinguishes whether semantics are represented in model states or learned by the probe
- Mechanism: By swapping the semantics of operations while preserving syntax, the experiment tests whether the probe can still extract "semantics" from the same model states. If the probe fails, it indicates the model states contained original semantics rather than just syntactic information
- Core assumption: If model states only contain syntactic transcripts, the probe should be able to reinterpret them according to alternative semantics with similar accuracy
- Evidence anchors: [abstract] "intervene on the semantics of the language while preserving the lexicon and syntax" and "the probe's ability to extract semantics degrades significantly"; [section 4] "the semantic content for the alternative semantics is significantly degraded when compared to the original semantics"

## Foundational Learning

- Concept: Linear probe classification
  - Why needed here: The study uses linear probes to test whether semantic information is linearly separable in the LM's hidden states, which is a common method for evaluating representation quality
  - Quick check question: If a linear probe achieves high accuracy on a task, what does this imply about the relationship between the input representations and the target labels?

- Concept: Abstract interpretation in program semantics
  - Why needed here: The paper uses abstract interpretation (e.g., tracking robot facing direction) as a precise way to capture program semantics that can be probed from the model
  - Quick check question: What makes abstract interpretation "precise" in the context of program semantics, and how does this precision enable the probing experiments?

- Concept: Trace semantics
  - Why needed here: The paper defines program meaning through execution traces, which provides the formal framework for what semantic information should be extractable from the model
  - Quick check question: How does trace semantics differ from other semantic models like denotational semantics, and why is it particularly suitable for this experimental setup?

## Architecture Onboarding

- Component map: Karel DSL with 5 operations -> Transformer LM (350M CodeGen variant) -> Linear probe classifier -> Interventional experiment framework
- Critical path: Program specification -> LM generation -> model state capture -> linear probe training -> semantic accuracy measurement. The interventional experiment adds an alternative semantics generation step before probing
- Design tradeoffs: Using a synthetic DSL enables precise semantic control but limits generalizability; linear probes are simple but may miss nonlinear semantic representations; greedy decoding ensures determinism but may not reflect best-case LM performance
- Failure signatures: Low semantic content at all training steps suggests the LM isn't encoding semantic information; high alternative semantic content suggests the probe learned interpretation rather than the model encoding semantics; high perplexity on training programs suggests the LM is generating semantically different programs
- First 3 experiments:
  1. Train the LM on Karel programs and measure semantic content over training steps to observe emergence
  2. Test the interventional experiment by swapping operation semantics and measuring probe degradation
  3. Compare program lengths and perplexity between generated and reference programs to assess distributional differences

## Open Questions the Paper Calls Out
- Can language models trained purely on next-token prediction acquire and represent formal semantics beyond the specific domain of programming languages?
- What is the relationship between the emergence of semantic representations in LMs and their generalization capabilities?
- How do different architectures and training objectives affect the emergence of semantic representations in LMs?

## Limitations
- The findings are based on a synthetic domain-specific language for grid world navigation, which may not generalize to real-world programming languages with richer semantics
- The interventional experiment cannot definitively rule out the probe learning semantic interpretation from the training data distribution
- The study does not directly investigate the impact of semantic representations on the LM's ability to generalize to unseen tasks or domains

## Confidence
- High Confidence: The observation that semantic content in model states correlates strongly with generative accuracy (R² = 0.968)
- Medium Confidence: The claim that the model learns to represent future semantic states
- Medium Confidence: The interventional experiment demonstrating that semantic content degrades under alternative semantics

## Next Checks
1. Train the same experimental setup on a slightly more complex DSL with additional operations or state dimensions to determine whether the emergence of semantic representations scales with language complexity
2. Compare linear probes with nonlinear probes (e.g., small MLPs) to determine whether semantic information is truly linearly separable in the model states or requires nonlinear extraction
3. Create a more challenging alternative semantics where the syntactic-syntactic mappings are more ambiguous or where the original and alternative semantics share more structural similarities to better test whether the probe can learn to reinterpret the same model states