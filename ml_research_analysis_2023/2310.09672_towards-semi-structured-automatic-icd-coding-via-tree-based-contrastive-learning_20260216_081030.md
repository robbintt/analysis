---
ver: rpa2
title: Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning
arxiv_id: '2310.09672'
source_url: https://arxiv.org/abs/2310.09672
tags:
- clinical
- notes
- section
- coding
- sections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a semi-structured automatic ICD coding method
  using tree-based contrastive learning. The authors address challenges in automatic
  ICD coding, including limited data availability, ignoring structural information,
  and high variability of clinical notes.
---

# Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning

## Quick Facts
- arXiv ID: 2310.09672
- Source URL: https://arxiv.org/abs/2310.09672
- Reference count: 40
- Authors: not specified
- Key outcome: Introduces semi-structured automatic ICD coding using tree-based contrastive learning with section segmentation and masked training

## Executive Summary
This paper addresses the challenge of automatic ICD coding by leveraging the semi-structured nature of clinical notes. The authors propose an automatic algorithm to segment clinical notes into sections and introduce a contrastive pre-training approach based on these sections. Additionally, they design a masked section training strategy to help ICD coding models locate relevant sections for code prediction. The proposed methods are evaluated on real-world EHR datasets, demonstrating significant improvements in coding performance across multiple baseline models.

## Method Summary
The paper proposes a semi-structured approach to automatic ICD coding that addresses limited data availability, ignores structural information, and handles high variability in clinical notes. The method consists of three main components: an automatic section segmentation algorithm (DF-IAPF) that extracts section titles from clinical notes, a contrastive pre-training approach that learns representations from sections using soft multi-label similarity based on tree edit distance, and a masked section training strategy that serves as data augmentation. These components work together to enhance existing ICD coding models by providing better section-level representations and reducing reliance on any single section for code prediction.

## Key Results
- The proposed methods improve performance across multiple baseline ICD coding models (MultiResCNN, HyperCore, JointLAAT, EffectiveCAN, PLM-ICD, Hierarchical, MSMN)
- Contrastive pre-training on sections with tree-based similarity consistently enhances model performance
- Masked section training serves as effective data augmentation, with optimal masking ratio varying by model
- Extensive experiments on MIMIC-III subsets (MIMIC-50, MIMIC-rare-50, MIMIC-full) demonstrate robustness across different code distributions

## Why This Works (Mechanism)

### Mechanism 1
Automatic section segmentation reduces variability in clinical notes by providing order-agnostic structures that focus the model on relevant content for ICD codes. The DF-IAPF algorithm identifies section titles based on their unique presence across the corpus and concentrated occurrence within individual documents, breaking sequential dependency and allowing models to learn section-specific representations.

### Mechanism 2
Contrastive pre-training on sections using soft multi-label similarity based on tree edit distance improves the model's ability to recognize related sections across different clinical notes. The method constructs positive pairs from sections within the same note and across different notes that share similar ICD code labels, with soft similarity accounting for hierarchical relationships between ICD codes.

### Mechanism 3
Masked section training serves as effective data augmentation by forcing the model to learn ICD codes from incomplete section information, reducing reliance on any single section. By randomly masking sections during training and shuffling their order, the model learns to extract relevant information from various sections rather than overfitting to specific section positions or dependencies.

## Foundational Learning

- Concept: Multi-label text classification
  - Why needed here: ICD coding requires predicting multiple codes from a single clinical note, where each label can be independently present or absent
  - Quick check question: What distinguishes multi-label classification from multi-class classification in the context of ICD coding?

- Concept: Hierarchical classification and tree edit distance
  - Why needed here: ICD codes have a hierarchical structure where parent-child relationships exist between codes, and tree edit distance allows measuring similarity between sets of codes while respecting this hierarchy
  - Quick check question: How does tree edit distance differ from simple Jaccard similarity when comparing sets of ICD codes?

- Concept: Contrastive learning and data augmentation
  - Why needed here: Limited training data and high variability in clinical notes require techniques that can learn meaningful representations from limited samples
  - Quick check question: Why might traditional data augmentation techniques be less effective for clinical text compared to contrastive learning approaches?

## Architecture Onboarding

- Component map: Clinical note → DF-IAPF segmentation → Encoder → Attention fusion → Prediction
- Critical path: Clinical note → DF-IAPF segmentation → Encoder → Attention fusion → Prediction
- Design tradeoffs: Trades computational complexity of tree edit distance calculations for improved semantic similarity measurement, sacrifices some context by segmenting notes into sections rather than treating them as continuous text
- Failure signatures: Poor section extraction leads to meaningless contrastive pairs; excessive masking prevents accurate code prediction; incorrect tree edit distance implementation fails to capture code relationships
- First 3 experiments:
  1. Validate DF-IAPF extraction accuracy by comparing extracted section titles against manually annotated sections from a small sample of clinical notes
  2. Test contrastive pre-training effectiveness by comparing performance with and without contrastive loss on a validation set, ensuring the soft similarity metric is properly implemented
  3. Evaluate masked section training by gradually increasing the masking threshold γ and observing the point where performance degradation begins, to find the optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
How would the proposed contrastive pre-training and masked section training strategies perform on datasets with different distributions of ICD codes (e.g., more evenly distributed codes)? The paper focuses on MIMIC-III data with a long-tail distribution, and performance on datasets with different code distributions remains unknown.

### Open Question 2
Can the proposed section-based segmentation algorithm be further improved to handle more diverse writing styles and clinical note formats? The current algorithm relies on DF-IAPF, which may not capture all variations in writing styles and formats across different medical systems.

### Open Question 3
How do the proposed training strategies affect the interpretability and explainability of ICD coding models? The paper focuses on performance improvements but does not explicitly address how these strategies impact model transparency and trustworthiness.

## Limitations

- The DF-IAPF section segmentation algorithm's effectiveness across diverse clinical note formats remains uncertain, with limited discussion of handling variations across different hospitals or medical specialties
- Tree edit distance-based similarity metric may become computationally expensive for the full ICD-10 code set (over 14,000 codes), and scalability concerns are not addressed
- The proposed methods are only validated on MIMIC-III data from a single US hospital system, limiting generalizability to clinical notes from different medical systems or countries

## Confidence

- **High Confidence**: The core hypothesis that semi-structured clinical notes can be effectively leveraged for ICD coding through contrastive learning on sections
- **Medium Confidence**: The effectiveness of masked section training as a data augmentation strategy, though optimal masking ratio varies by model
- **Low Confidence**: The claim that the proposed methods will generalize to clinical notes from different medical systems or countries

## Next Checks

1. **Cross-hospital validation**: Apply the DF-IAPF algorithm and complete training pipeline to clinical notes from at least two additional hospital systems with different note formatting conventions to assess generalizability

2. **Computational efficiency analysis**: Measure the runtime and memory requirements of the tree edit distance calculations during contrastive pre-training, particularly when scaling to the full ICD-10 code set versus the MIMIC-III subsets used in experiments

3. **Ablation study on masking ratio**: Conduct a more granular analysis of the masking threshold γ across the full range [0, 1] for each baseline model to identify optimal values and determine whether a universal ratio exists or if model-specific tuning is required