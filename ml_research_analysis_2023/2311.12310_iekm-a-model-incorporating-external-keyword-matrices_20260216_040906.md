---
ver: rpa2
title: 'IEKM: A Model Incorporating External Keyword Matrices'
arxiv_id: '2311.12310'
source_url: https://arxiv.org/abs/2311.12310
tags:
- similarity
- sentence
- iekm
- external
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles two challenges in industrial semantic text\
  \ similarity (STS) tasks: adapting to different customer domains and distinguishing\
  \ hard negative samples\u2014sentence pairs that are textually similar but semantically\
  \ different. The proposed IEKM model addresses these by incorporating external keyword\
  \ matrices into the self-attention layers of a Transformer using gate control units."
---

# IEKM: A Model Incorporating External Keyword Matrices

## Quick Facts
- **arXiv ID**: 2311.12310
- **Source URL**: https://arxiv.org/abs/2311.12310
- **Reference count**: 17
- **Primary result**: IEKM improves semantic text similarity performance by incorporating external keyword matrices into Transformer self-attention via gate control units.

## Executive Summary
IEKM addresses two challenges in industrial semantic text similarity tasks: adapting to different customer domains and distinguishing hard negative samples (textually similar but semantically different sentence pairs). The model incorporates external keyword similarity matrices into the self-attention layers of a Transformer using learnable gate control units. These matrices, constructed from external tools like Cilin and Hownet or domain-specific dictionaries, enable dynamic adjustment of attention based on context. Experiments demonstrate IEKM's effectiveness across multiple datasets, with domain-specific keyword dictionaries increasing F1 scores from 56.61 to 73.53, solving both challenges without retraining or deploying multiple models.

## Method Summary
IEKM modifies the self-attention mechanism in Transformers by incorporating external keyword similarity matrices. The model constructs positive (M) and negative (Mr) correlation matrices between words in sentence pairs using external tools or domain-specific dictionaries. These matrices are fused into the self-attention layer via learnable gate control units that scale the contribution of M and Mr based on input context. This allows the model to emphasize semantically relevant words in a given domain without retraining. Domain adaptation is achieved through flexible keyword dictionary updates, where new keyword pairs can be inserted to correct the model's output for new domains or hard negative samples.

## Key Results
- IEKM improves semantic text similarity performance across multiple datasets
- Domain-specific keyword dictionaries increase F1 scores from 56.61 to 73.53
- The model solves both domain adaptation and hard negative distinction challenges without retraining
- Performance improvements achieved through flexible dictionary updates rather than model retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IEKM's attention modification incorporates external keyword similarity matrices to adjust self-attention dynamically based on domain context.
- Mechanism: The model computes similarity (M) and dissimilarity (Mr) matrices between words using external tools like Cilin and Hownet. These matrices are fused into the self-attention layer via a learnable gate control unit (g), which scales the contribution of M and Mr. This enables emphasis on semantically relevant words in a given domain without retraining.
- Core assumption: Domain-specific keyword similarity strongly influences sentence semantic similarity in customer service contexts.
- Evidence anchors:
  - [abstract]: "The model uses external tools or dictionaries to construct external matrices and fuses them to the self-attention layers of the Transformer structure through gating units, thus enabling flexible corrections to the model results."
  - [section 3.1]: "We construct a similarity matrix M and a dissimilarity matrix Mr based on the two input sentences... The score of each cell between sentence pairs is calculated by the word similarity tools (Cilin and Hownet)."
  - [corpus]: Weak/no direct evidence that external keyword matrices alone are the sole driver of performance; the corpus focuses on unrelated topics like UAV computing and XR headsets.
- Break condition: If keyword similarity does not correlate with domain-specific sentence meaning, the matrices will not provide useful corrections.

### Mechanism 2
- Claim: The gate control unit learns to modulate the influence of external matrices based on the input context.
- Mechanism: For each attention head, a learnable gate g = Linear(G) = GW_Tl + b scales M and Mr before adding them to the raw attention scores. This enables the model to automatically decide how much to trust external similarity information versus learned attention patterns.
- Core assumption: The context of the sentence pair determines whether external keyword information is more or less relevant.
- Evidence anchors:
  - [section 3.2]: "The learnable gate control unit g is used to control the exact amount of information obtained from the matrices M and Mr, respectively. In this way when faced with an actual sentence pair, the model can automatically adjust whether to pay more attention to similar words or irrelevant words."
  - [abstract]: "the model adjusts the gate control units' scores according to the actual inputs and thus decides how much information to obtain from the external matrices to correct the self-attention scores."
  - [corpus]: No direct evidence; corpus neighbors do not discuss gating mechanisms in attention.
- Break condition: If the gate fails to learn meaningful scaling, the external matrices may add noise instead of signal.

### Mechanism 3
- Claim: Flexible dictionary updates enable domain adaptation without retraining.
- Mechanism: After deployment, domain-specific keyword dictionaries are built and used to construct M and Mr on the fly. This allows IEKM to correct its output for new domains or hard negative samples simply by inserting new keyword pairs into the dictionary.
- Core assumption: Keywords capture domain-specific semantic nuances that are hard for generic models to learn.
- Evidence anchors:
  - [abstract]: "When facing a new domain customer or a new hard negative sample, we only need to 'INSERT' the relevant keyword information into the relevant database to meet the demand, no need to retrain the model and redeploy it."
  - [section 4.4]: "During the test, input data [ s1, s2] is replaced with [s1, s2, keyword 1, keyword 2, score]... the method enables the model to address the above challenges without the need to retrain the model by re-labeling the data, and without the need to deploy multiple models."
  - [corpus]: No evidence in corpus; topics unrelated to keyword dictionary updates.
- Break condition: If domain differences cannot be captured by keyword pairs, the dictionary approach will fail.

## Foundational Learning

- **Concept**: Self-attention mechanism in Transformers
  - Why needed here: IEKM modifies the self-attention layer to incorporate external matrices; understanding how queries, keys, and values interact is essential.
  - Quick check question: In a standard Transformer self-attention, how is the attention score between a query q and key k computed before softmax?

- **Concept**: Matrix operations (Hadamard product, addition)
  - Why needed here: M and Mr are combined with attention scores using element-wise multiplication and addition; correct tensor shapes and broadcasting rules are critical.
  - Quick check question: Given two matrices A and B of the same shape, what is the result of A ⊙ B in PyTorch notation?

- **Concept**: Learnable gating mechanisms
  - Why needed here: The gate control unit g learns to scale external matrices; understanding linear layers and parameter updates is necessary for debugging.
  - Quick check question: If g = Linear(G), what are the dimensions of the weight matrix W_l given that G has shape (batch_size, seq_len, hidden_size)?

## Architecture Onboarding

- **Component map**: Input layer → Tokenizer (BERT-based) → Embedding layer → Multi-head self-attention (modified with M, Mr, g) → Feed-forward network → [CLS] token output → Classifier/Regression head → External module: Keyword dictionary lookup → Matrix construction (M, Mr) → Fusion into attention
- **Critical path**: Tokenization → Embedding → Modified self-attention (where external matrices are fused) → Feed-forward → Output
- **Design tradeoffs**:
  - Pros: No retraining needed for domain shifts; flexible keyword updates; leverages existing similarity tools
  - Cons: Performance depends on quality of external similarity tools; added computational cost for matrix operations; gate may overfit to training domains
- **Failure signatures**:
  - No improvement on hard negative samples → gate not learning or dictionaries missing key terms
  - Performance drops on generic data → external matrices introducing noise
  - Training instability → incorrect matrix shapes or scaling issues
- **First 3 experiments**:
  1. Ablation: Run IEKM without M and Mr to confirm baseline performance drop
  2. Sensitivity: Vary the scaling factor on M and Mr manually to see impact on hard negative samples
  3. Dictionary test: Build a small domain-specific dictionary and measure change in F1 on LOGS test set

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several key questions emerge:

- How does the IEKM model's performance scale with the size and quality of the external keyword dictionaries used for domain adaptation?
- Can the IEKM approach be effectively extended to other Transformer-based models beyond BERT, such as GPT or RoBERTa?
- How does the IEKM model handle polysemous words (words with multiple meanings) when using external keyword dictionaries for domain adaptation?

## Limitations

- The paper lacks explicit details on how keyword dictionaries are constructed during inference and how the gate control units are implemented, making faithful reproduction challenging.
- The model's performance is heavily dependent on the quality of external similarity tools (Cilin and Hownet), with no validation provided for their accuracy in customer service contexts.
- While claiming domain adaptation without retraining, the mechanism relies on manual keyword dictionary updates, introducing human effort and potential bias that limits true automation and scalability.

## Confidence

- **High Confidence**: The core architecture of incorporating external keyword matrices into self-attention via gating mechanisms is well-specified and logically sound.
- **Medium Confidence**: The claim that IEKM solves both domain adaptation and hard negative distinction without retraining is supported by the flexible correction study, but the exact process of dictionary construction and integration is underspecified.
- **Low Confidence**: The assumption that keyword similarity directly correlates with domain-specific semantic meaning is untested and may not hold across all customer service domains.

## Next Checks

1. **Ablation Study**: Remove the external matrices (M and Mr) from IEKM and measure performance degradation on hard negative samples to confirm their contribution.

2. **Dictionary Sensitivity**: Manually construct a small domain-specific keyword dictionary and measure the change in F1 score on the LOGS test set to validate the domain adaptation claim.

3. **Tool Validation**: Test the quality of Cilin and Hownet word similarity scores on a held-out sample of customer service sentence pairs to assess whether the external matrices provide meaningful corrections.