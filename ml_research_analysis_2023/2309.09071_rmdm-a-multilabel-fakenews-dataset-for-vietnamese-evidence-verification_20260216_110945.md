---
ver: rpa2
title: 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification'
arxiv_id: '2309.09071'
source_url: https://arxiv.org/abs/2309.09071
tags:
- news
- information
- fake
- real
- electronic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RMDM, a Vietnamese multilabel dataset designed
  to evaluate large language models in verifying electronic evidence related to legal
  contexts, with a focus on distinguishing between real information and various types
  of fake news. The dataset comprises 1,556 samples across four labels: real, mis
  (misinformation), dis (disinformation), and mal (mal-information).'
---

# RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification

## Quick Facts
- arXiv ID: 2309.09071
- Source URL: https://arxiv.org/abs/2309.09071
- Reference count: 25
- Primary result: First Vietnamese multilabel fake news dataset for legal electronic evidence verification with four labels (real, mis, dis, mal)

## Executive Summary
This paper introduces RMDM, a Vietnamese multilabel dataset designed to evaluate large language models in verifying electronic evidence related to legal contexts, with a focus on distinguishing between real information and various types of fake news. The dataset comprises 1,556 samples across four labels: real, mis (misinformation), dis (disinformation), and mal (mal-information). Each label contains 389 samples. Preliminary tests using GPT-based and BERT-based models show significant performance variations across labels, indicating the dataset's effectiveness in challenging language models' ability to verify electronic information. The findings suggest that verifying electronic evidence in legal contexts remains difficult for current language models, highlighting the need for further research and development in AI models for legal applications.

## Method Summary
The RMDM dataset is constructed from real news sources and systematically modified to create fake news samples across three categories: misinformation (slight alterations), disinformation (substantial changes or negation), and mal-information (true but harmful content). The dataset contains 1,556 Vietnamese text samples, each labeled as real, mis, dis, or mal, with 389 samples per label. The dataset is designed to assess language model performance in distinguishing between these categories for electronic evidence verification in legal contexts. Preliminary evaluations use GPT-based models (ChatGPT, GPT-4) and BERT-based models (Multilingual-BERT) for classification tasks.

## Key Results
- RMDM dataset contains 1,556 samples across four labels: real, mis, dis, and mal (389 samples each)
- Language models show significant performance variations across labels, struggling particularly with fake news detection
- Models frequently misclassify fake news as real news, indicating difficulty in electronic evidence verification
- Dataset successfully captures complexity of fake news categories and challenges language models' verification abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The four-label structure provides granular view of fake news categories that improves model understanding of different information types
- Mechanism: By explicitly separating misinformation (unintentional), disinformation (intentional harm), and mal-information (true but harmful), the dataset forces models to learn distinct patterns for each category rather than collapsing them into binary real/fake classification
- Core assumption: Different fake news categories have distinguishable linguistic and contextual patterns that can be learned by language models
- Evidence anchors:
  - [abstract] "By including these diverse labels, RMDM captures the complexities of differing fake news categories and offers insights into the abilities of different language models to handle various types of information"
  - [section] "A key limitation of existing Vietnamese fake news datasets, such as VFND, VNTC, and VLSP Fake News, is that they predominantly focus on binary classification, which may oversimplify the complexity of fake news types."
- Break condition: If the linguistic patterns between categories are too similar or overlapping, models cannot learn meaningful distinctions between the labels

### Mechanism 2
- Claim: Creating fake news by modifying real news samples ensures linguistic consistency while introducing controlled variations in information authenticity
- Mechanism: The dataset construction process takes real news from reputable sources and systematically transforms it into different fake news types by altering dates, locations, or adding judgmental content, preserving the original writing style while changing the truthfulness
- Core assumption: Language models can learn to detect subtle modifications in text that indicate changes in information authenticity when those modifications are systematic and consistent
- Evidence anchors:
  - [section] "For samples with the mis label, the text field was populated by slightly altering some information (but not the entire content) in the samples with the real label"
  - [section] "For samples with the dis label, the text field was created by changing almost all the information or turning it into a negation of the samples with the real label"
- Break condition: If the modifications are too subtle for models to detect, or if models rely on superficial features rather than understanding content authenticity

### Mechanism 3
- Claim: The dataset's focus on Vietnamese legal contexts creates a specialized domain where language models must integrate domain knowledge with general language understanding
- Mechanism: By framing fake news verification within legal electronic evidence contexts, the dataset requires models to understand both legal terminology and the specific requirements for evidence verification, creating a more challenging task than general fake news detection
- Core assumption: Legal contexts provide additional constraints and vocabulary that can help models better distinguish between authentic and fake information
- Evidence anchors:
  - [abstract] "designed to assess the performance of large language models (LLMs), in verifying electronic information related to legal contexts"
  - [section] "The dataset, referred to as RMDM, consists of 1556 samples, with each sample containing two fields: text and label"
- Break condition: If legal domain knowledge doesn't provide useful features for fake news detection, or if the legal terminology overwhelms the fake news detection task

## Foundational Learning

- Concept: Vietnamese language processing fundamentals
  - Why needed here: The dataset is specifically for Vietnamese, requiring understanding of Vietnamese linguistic features, tokenization, and script handling
  - Quick check question: Can you explain the difference between Vietnamese diacritical marks and their impact on word meaning?

- Concept: Fake news taxonomy and classification
  - Why needed here: Understanding the distinctions between misinformation, disinformation, and mal-information is crucial for working with this dataset's four-label structure
  - Quick check question: What are the three main categories of fake news according to Claire Wardle's classification?

- Concept: Legal document structure and terminology
  - Why needed here: The dataset is designed for legal electronic evidence verification, requiring familiarity with legal concepts and terminology
  - Quick check question: What is the difference between electronic evidence and traditional documentary evidence in Vietnamese legal proceedings?

## Architecture Onboarding

- Component map: Dataset loader -> Text preprocessing -> Model architecture (BERT/GPT-based) -> Classification layer (4-way) -> Evaluation metrics
- Critical path: Data loading and preprocessing -> Model fine-tuning -> Evaluation and analysis of confusion matrices
- Design tradeoffs: Four-label classification increases task complexity but provides more granular insights vs. simpler binary classification
- Failure signatures: Models consistently predicting "real" label, confusion between fake news categories, poor performance on specific label types
- First 3 experiments:
  1. Train baseline BERT model on the dataset and analyze confusion matrix patterns
  2. Compare performance of GPT-based models (ChatGPT, GPT-4) against BERT baseline
  3. Test model performance on subsets of data (e.g., only real vs. fake, or individual fake news categories)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of language models be improved for accurately identifying different types of fake news in legal contexts?
- Basis in paper: [explicit] The paper states that current language models struggle to differentiate between mis, dis, and mal news types, often classifying most information as real news, highlighting the need for further research and development in AI models for legal applications.
- Why unresolved: The paper identifies the limitations of current language models but does not provide specific solutions for improving their performance in identifying different types of fake news.
- What evidence would resolve it: Evidence would include research demonstrating the effectiveness of specific techniques, such as incorporating external knowledge bases or logical verifiers, in improving language models' ability to accurately identify different types of fake news in legal contexts.

### Open Question 2
- Question: What are the key factors contributing to the difficulty of language models in verifying electronic evidence in legal contexts?
- Basis in paper: [inferred] The paper suggests that language models struggle to understand the complexities of fake news categories and lack a proper understanding of fake news, which contributes to their difficulty in verifying electronic evidence.
- Why unresolved: The paper does not provide a detailed analysis of the specific factors contributing to the difficulty of language models in verifying electronic evidence in legal contexts.
- What evidence would resolve it: Evidence would include research identifying the key factors that contribute to the difficulty of language models in verifying electronic evidence, such as the complexity of fake news categories, the lack of context, or the presence of misleading information.

### Open Question 3
- Question: How can the RMDM dataset be expanded to further improve the evaluation of language models in verifying electronic evidence in legal contexts?
- Basis in paper: [explicit] The paper presents the RMDM dataset as a multilabel dataset designed to assess the performance of language models in verifying electronic evidence in legal contexts, capturing the complexities of fake news categories.
- Why unresolved: The paper does not discuss potential ways to expand the RMDM dataset to further improve the evaluation of language models in verifying electronic evidence.
- What evidence would resolve it: Evidence would include research proposing and evaluating methods for expanding the RMDM dataset, such as incorporating additional labels, increasing the number of samples, or including more diverse types of electronic evidence.

## Limitations
- Dataset size (1,556 samples) may be insufficient for robust generalization across all Vietnamese legal contexts
- Dataset construction relies on modifications of real news samples rather than naturally occurring fake news examples
- No evaluation of cross-domain generalization or performance on real-world legal cases

## Confidence
- Foundational contribution: High confidence - first Vietnamese multilabel fake news dataset for legal contexts
- Claimed mechanisms: Medium confidence - the multilabel structure and legal domain focus are promising but effectiveness varies across models
- Performance claims: Medium confidence - based on limited preliminary tests with GPT and BERT models

## Next Checks
1. Test model performance on out-of-domain Vietnamese legal documents not used in dataset construction to assess generalization
2. Conduct ablation studies to determine which label distinctions (mis/dis/mal) are most informative for model performance
3. Evaluate human performance on the dataset to establish a baseline for what constitutes "challenging" verification tasks