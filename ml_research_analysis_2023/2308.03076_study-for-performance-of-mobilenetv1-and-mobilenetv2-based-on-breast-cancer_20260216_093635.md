---
ver: rpa2
title: Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer
arxiv_id: '2308.03076'
source_url: https://arxiv.org/abs/2308.03076
tags:
- cancer
- mobilenetv1
- breast
- mobilenetv2
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares MobileNetV1 and MobileNetV2 models for detecting
  breast cancer from histopathological images downloaded from Kaggle. The dataset
  contains 277,524 images, with 198,738 benign and 78,786 malignant tumor patches.
---

# Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer

## Quick Facts
- arXiv ID: 2308.03076
- Source URL: https://arxiv.org/abs/2308.03076
- Authors: 
- Reference count: 0
- MobileNetV1 achieved higher validation accuracy (0.857) than MobileNetV2 (0.743) on breast cancer histopathology image detection

## Executive Summary
This study compares MobileNetV1 and MobileNetV2 for detecting breast cancer from histopathological images downloaded from Kaggle. The dataset contains 277,524 images with 198,738 benign and 78,786 malignant tumor patches. Both models were trained on the same dataset with resizing, normalization, and additional pooling and fully connected layers. MobileNetV1 achieved a training accuracy of 0.849 and validation accuracy of 0.857, while MobileNetV2 achieved a training accuracy of 0.819 and validation accuracy of 0.743. The results show that MobileNetV1 outperformed MobileNetV2 in this task, likely due to MobileNetV2's increased structural complexity and potential overfitting issues with the dataset size.

## Method Summary
The study compares MobileNetV1 and MobileNetV2 architectures for breast cancer detection using histopathological images from Kaggle. Both models were trained on the same dataset (277,524 images, 198,738 benign and 78,786 malignant) with resizing, normalization, and additional pooling and fully connected layers. Training used the Adam optimizer, cross-entropy loss, a learning rate of 0.0001, and 5 epochs. The main comparison metric was validation accuracy.

## Key Results
- MobileNetV1 achieved training accuracy of 0.849 and validation accuracy of 0.857
- MobileNetV2 achieved training accuracy of 0.819 and validation accuracy of 0.743
- MobileNetV1 outperformed MobileNetV2, likely due to MobileNetV2's increased structural complexity leading to overfitting on the dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MobileNetV1 outperformed MobileNetV2 due to MobileNetV2's increased structural complexity and potential overfitting issues with the dataset size.
- Mechanism: The higher number of parameters in MobileNetV2, including the introduction of inverted residuals and linear bottlenecks, led to overfitting when trained on a dataset of limited size (277,524 images).
- Core assumption: The dataset size was insufficient to adequately train the more complex MobileNetV2 architecture without overfitting.
- Evidence anchors:
  - [abstract]: "MobileNetV2 achieved a training accuracy of 0.819 and validation accuracy of 0.743. The results show that MobileNetV1 outperformed MobileNetV2 in this task, likely due to MobileNetV2's increased structural complexity and potential overfitting issues with the dataset size."
  - [section]: "The structural parameters of MobileNetV2 are increased compared to MobileNetV1 and may not be appropriate for the processing of this dataset since it is prone to overfit."
  - [corpus]: Weak evidence - corpus papers focus on other aspects of breast cancer detection and do not specifically address MobileNetV1 vs. MobileNetV2 performance differences.
- Break condition: If the dataset size were significantly increased, MobileNetV2 might outperform MobileNetV1 due to its more sophisticated architecture.

### Mechanism 2
- Claim: MobileNetV1's simpler architecture was better suited for the dataset's characteristics, leading to more stable and higher validation accuracy.
- Mechanism: The depthwise separable convolutions in MobileNetV1 provided sufficient feature extraction capability for the histopathological images without introducing excessive complexity that could lead to overfitting.
- Core assumption: The dataset's characteristics (size, image complexity) were better matched to MobileNetV1's simpler architecture.
- Evidence anchors:
  - [abstract]: "MobileNetV1 achieved a training accuracy of 0.849 and validation accuracy of 0.857, while MobileNetV2 achieved a training accuracy of 0.819 and validation accuracy of 0.743."
  - [section]: "MobileNetV1 is mainly used by deep separable convolution in the Inception model to reduce the computational effort of the first few layers."
  - [corpus]: Weak evidence - corpus papers do not directly compare MobileNetV1 and MobileNetV2 performance on this specific dataset.
- Break condition: If the dataset were significantly more complex or larger, MobileNetV2's additional complexity might become beneficial.

### Mechanism 3
- Claim: The instability in MobileNetV2's validation accuracy curve indicates overfitting and suggests that the model was not generalizing well to the validation set.
- Mechanism: The validation accuracy of MobileNetV2 showed significant fluctuations (72.6% → 59.6% → 74.3%) across epochs, indicating that the model was not learning stable patterns that generalize well to unseen data.
- Core assumption: The validation accuracy curve is a reliable indicator of overfitting and model generalization.
- Evidence anchors:
  - [abstract]: "It can be observed that MobileNetV1 has better validation accuracy and overfit during MobileNetV2 training."
  - [section]: "observing the curve of detection accuracy, it can be found that the detection accuracy obtained from the first three training rounds is nearly the same value, all about 72.6%, while it suddenly drops to about 59.6% in the fourth round, and then reaches 74.3% in the fifth round."
  - [corpus]: Weak evidence - corpus papers do not provide specific validation accuracy curves for comparison.
- Break condition: If regularization techniques (e.g., dropout, weight decay) were applied to MobileNetV2, the validation accuracy curve might become more stable.

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: Understanding the basic principles of CNNs is crucial for comprehending how MobileNetV1 and MobileNetV2 process histopathological images.
  - Quick check question: What are the main components of a CNN, and how do they contribute to image classification tasks?

- Concept: Depthwise Separable Convolutions
  - Why needed here: MobileNetV1 and MobileNetV2 both utilize depthwise separable convolutions, which are key to their efficiency and performance characteristics.
  - Quick check question: How do depthwise separable convolutions differ from standard convolutions, and what are the computational advantages?

- Concept: Model Overfitting
  - Why needed here: Understanding overfitting is essential for interpreting why MobileNetV2 performed worse than MobileNetV1 in this study.
  - Quick check question: What are the signs of overfitting in a neural network, and what techniques can be used to mitigate it?

## Architecture Onboarding

- Component map:
  Data preprocessing -> MobileNetV1/V2 initialization -> Additional layers -> Training -> Evaluation

- Critical path:
  1. Load and preprocess the dataset
  2. Initialize MobileNetV1 and MobileNetV2 models
  3. Add pooling and fully connected layers to each model
  4. Train both models on the dataset
  5. Evaluate and compare training and validation accuracy

- Design tradeoffs:
  - MobileNetV1: Simpler architecture, less prone to overfitting, potentially lower maximum accuracy
  - MobileNetV2: More complex architecture with inverted residuals and linear bottlenecks, higher risk of overfitting but potentially higher maximum accuracy with larger datasets

- Failure signatures:
  - High training accuracy but low validation accuracy (overfitting)
  - Unstable validation accuracy across epochs
  - Significantly lower validation accuracy compared to training accuracy

- First 3 experiments:
  1. Compare MobileNetV1 and MobileNetV2 on a smaller subset of the dataset to observe overfitting behavior more clearly.
  2. Apply regularization techniques (e.g., dropout, weight decay) to MobileNetV2 and re-evaluate its performance.
  3. Increase the dataset size (if possible) and re-train both models to determine if MobileNetV2's performance improves with more data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would MobileNetV2 outperform MobileNetV1 if trained on a larger dataset?
- Basis in paper: [explicit] The paper suggests MobileNetV2's residual blocks may be beneficial for larger datasets, and its poor performance was attributed to overfitting due to dataset size limitations.
- Why unresolved: The current study only tested on one dataset size, so the impact of dataset size on MobileNetV2 performance remains unknown.
- What evidence would resolve it: Comparative experiments training both models on progressively larger datasets while monitoring accuracy and overfitting metrics.

### Open Question 2
- Question: How would MobileNetV3 and ResNet perform compared to MobileNetV1 and V2 on this breast cancer histopathological image dataset?
- Basis in paper: [explicit] The authors mention plans to add MobileNetV3 and ResNet for future comparative analysis.
- Why unresolved: These models were not tested in the current study, leaving their relative performance unknown.
- What evidence would resolve it: Direct comparison experiments training all four models on the same dataset with identical preprocessing and hyperparameters.

### Open Question 3
- Question: How would different data augmentation techniques affect the performance of MobileNetV1 and V2 on this dataset?
- Basis in paper: [inferred] The authors used basic resizing and normalization but didn't explore data augmentation, despite the potential overfitting issues observed with MobileNetV2.
- Why unresolved: No augmentation techniques were tested in the current study.
- What evidence would resolve it: Experiments applying various augmentation techniques (rotation, flipping, scaling, etc.) to both models and comparing performance metrics.

## Limitations
- The study only tested one dataset and training duration (5 epochs), which may not capture the full potential of either architecture
- No detailed hyperparameter tuning or ablation studies were conducted to isolate the effects of model complexity versus other factors
- The validation accuracy instability observed in MobileNetV2 suggests potential overfitting, but the exact causes are not thoroughly investigated

## Confidence

**High**: MobileNetV1 achieved higher validation accuracy than MobileNetV2 (0.857 vs 0.743)

**Medium**: MobileNetV2's overfitting is attributed to its increased structural complexity

**Medium**: The dataset size (277,524 images) was insufficient for MobileNetV2's complexity

## Next Checks

1. Conduct experiments with increased training epochs and regularization techniques (dropout, weight decay) on MobileNetV2 to assess if overfitting can be mitigated

2. Perform experiments with varying dataset sizes (subsets and augmented versions) to determine the minimum data requirements for optimal MobileNetV2 performance

3. Implement cross-validation and test on multiple breast cancer histopathology datasets to verify if the performance trends are consistent across different data distributions