---
ver: rpa2
title: Exploring the psychology of LLMs' Moral and Legal Reasoning
arxiv_id: '2308.01264'
source_url: https://arxiv.org/abs/2308.01264
tags:
- gpt-4
- human
- moral
- responses
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies experimental psychology methods to investigate
  moral and legal reasoning in large language models (LLMs), specifically testing
  GPT-4, Gemini Pro, Claude 2.1, and Llama 2. By replicating eight established psychological
  studies, the authors assess whether LLM responses align with human judgments.
---

# Exploring the psychology of LLMs' Moral and Legal Reasoning

## Quick Facts
- arXiv ID: 2308.01264
- Source URL: https://arxiv.org/abs/2308.01264
- Reference count: 0
- Primary result: GPT-4's moral and legal reasoning correlates with human responses but exhibits systematic exaggerations and reduced variance, cautioning against using LLMs as human research substitutes.

## Executive Summary
This paper investigates moral and legal reasoning in large language models (LLMs) by applying experimental psychology methods. The authors tested GPT-4, Gemini Pro, Claude 2.1, and Llama 2 by replicating eight established psychological studies. Results show that while correlations between LLM and human responses are often high, systematic differences remain. LLMs tend to exaggerate certain effects present in humans and show reduced variance, supporting the "correct answer" effect. These findings caution against using current LLMs as substitutes for human participants in psychological research and highlight the need for further investigation into machine psychology.

## Method Summary
The study replicated eight psychological experiments using GPT-4 via the OpenAI API. The authors generated responses using a system message to format outputs correctly and set temperature to 1.2 for diversity. They compared GPT-4 responses to human data from the original studies using statistical analyses including t-tests, ANOVAs, logistic regressions, and correlations. The experiments covered topics such as intentionality, causation, deception, moral foundations, consent, hindsight bias, and rule violations.

## Key Results
- GPT-4's responses in moral and legal reasoning experiments are highly correlated with human responses, but with systematic exaggerations of human effects
- LLMs exhibit the "correct answer" effect, producing uniform responses to nuanced questions and showing reduced variance compared to human samples
- GPT-4's judgments are sensitive to moral valence in ways that parallel human participants, but the magnitude and consistency differ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's responses in moral and legal reasoning experiments are highly correlated with human responses, but with systematic exaggerations of human effects.
- Mechanism: GPT-4's training on large text corpora captures statistical regularities in human reasoning, including moral judgments. However, its autoregressive architecture tends toward modal, "safe" answers, leading to the "correct answer" effect‚Äîwhere ambiguous or divisive human judgments are collapsed to a single, often more extreme response.
- Core assumption: The training data contains sufficient representation of human moral reasoning to allow GPT-4 to mimic these patterns statistically.
- Evidence anchors:
  - [abstract] "Results show that while correlations between LLM and human responses are often high, systematic differences remain. LLMs tend to exaggerate certain effects present in humans and show reduced variance, supporting the 'correct answer' effect."
  - [section] "Park et al [60] documented what they called the 'correct answer' effect, '[text-davinci-003]'s tendency to sometimes answer nuanced questions ... in a highly uniform and sometimes completely uniform way'."
- Break condition: If GPT-4 is trained on filtered or curated data that suppresses controversial moral reasoning, the correlation with human responses would degrade and the exaggeration effect would disappear.

### Mechanism 2
- Claim: GPT-4's judgments are sensitive to moral valence in ways that parallel human participants, but the magnitude and consistency differ.
- Mechanism: GPT-4's pattern recognition captures human tendencies to moralize concepts like intentionality and causation. However, the model's lack of true understanding leads to over-application‚Äîassigning intentionality or causal strength more uniformly in line with moral valence than humans do.
- Core assumption: Moral valence effects are statistically encoded in the training data and are recoverable by the model.
- Evidence anchors:
  - [section] "Just as humans, GPT-4's judgments about the protagonist's moral responsibility ... and intentionality (ùùå2(1) = 50, p < .001, V = 1) differed significantly between conditions."
  - [section] "GPT-4 differed from humans insofar as it was not as sensitive to differences in the causal structure that dictates how several actions combine to bring about their effects."
- Break condition: If moral reasoning in training data is highly context-dependent or contradictory, the model may fail to replicate human sensitivity to valence.

### Mechanism 3
- Claim: GPT-4's "virtual population" behavior is systematically less diverse than real human samples, undermining its validity as a research proxy.
- Mechanism: GPT-4's temperature-controlled sampling is insufficient to capture the full variance of human responses, especially for nuanced moral questions. The model collapses variation toward modal responses, especially for questions with no clear consensus.
- Core assumption: The "correct answer" effect reflects an inherent limitation of the model's sampling strategy rather than a true reflection of human diversity.
- Evidence anchors:
  - [abstract] "LLMs tend to exaggerate certain effects present in humans and show reduced variance, supporting the 'correct answer' effect."
  - [section] "Instead of returning a range of different answers to tricky scenarios, as one would reasonably expect from multi-voice groups, all of the answers generated by GPT-4 to some of the questions we posed had the exact same content."
- Break condition: If the model is fine-tuned or prompted to explicitly vary responses across queries, the variance collapse may be reduced.

## Foundational Learning

- Concept: Statistical learning from large text corpora
  - Why needed here: GPT-4's behavior is driven by learned statistical associations rather than explicit reasoning; understanding this is key to interpreting its moral judgments.
  - Quick check question: Does GPT-4 generate moral judgments by applying learned statistical patterns or by reasoning about ethics?
- Concept: The "correct answer" effect
  - Why needed here: This phenomenon explains why GPT-4's responses sometimes match human modal responses too closely, masking real human diversity.
  - Quick check question: In what way does GPT-4's "correct answer" effect differ from typical human response variance in psychological studies?
- Concept: Experimental jurisprudence
  - Why needed here: The paper applies methods from experimental jurisprudence to probe AI's legal reasoning; knowing this framework is essential for interpreting the studies.
  - Quick check question: How does experimental jurisprudence differ from traditional philosophical approaches to legal concepts?

## Architecture Onboarding

- Component map: System prompt generation -> GPT-4 API call with temperature=1.2 -> Response parsing -> Statistical analysis -> Comparison with human data
- Critical path: Prompt generation ‚Üí API call with temperature=1.2 ‚Üí response parsing ‚Üí statistical aggregation ‚Üí comparison to human data
- Design tradeoffs: Higher temperature increases response diversity but also noise; lower temperature risks over-consolidation to modal answers
- Failure signatures: All responses identical (correct answer effect); systematic bias toward extreme moral judgments; failure to replicate known human effects (e.g., abnormal deflation)
- First 3 experiments:
  1. Replicate Knobe's side-effect effect on intentionality judgments with moral valence manipulation
  2. Test abnormal selection in causation judgments under conjunctive vs. disjunctive causal structures
  3. Measure moral foundations using the Moral Foundations Questionnaire and compare with human samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise computational mechanisms behind the "correct answer effect" in LLMs, and how can they be systematically identified and measured?
- Basis in paper: [explicit] The paper discusses the "correct answer" effect observed in GPT-4, where the model tends to provide uniform answers to nuanced questions, but the underlying mechanisms remain unclear.
- Why unresolved: The paper notes that while the effect is observed, the exact computational processes causing it are not fully understood, and further research is needed to uncover these mechanisms.
- What evidence would resolve it: Detailed analysis of model internals, such as attention patterns or internal representations, combined with controlled experiments varying model parameters and training data, could elucidate the mechanisms.

### Open Question 2
- Question: How can we ethically and legitimately use LLMs to simulate human behavior in multi-voice groups, such as jury trials or elections, without compromising democratic principles?
- Basis in paper: [explicit] The paper raises concerns about using GPT-4 to replace human participants in scenarios requiring legitimacy, like elections or jury trials, due to the lack of genuine commitment and representation.
- Why unresolved: The paper highlights the ethical and political objections to using LLMs as proxies for human groups but does not provide a clear framework for addressing these issues.
- What evidence would resolve it: Development of ethical guidelines and empirical studies on the legitimacy and representation of LLM-generated responses in multi-voice group simulations could provide clarity.

### Open Question 3
- Question: Should alignment efforts focus on reducing the "correct answer" effect in LLMs, or could this effect represent a more rational or moral decision-making process than human responses?
- Basis in paper: [explicit] The paper discusses the "correct answer" effect and raises the question of whether misalignment with human values could be an improvement, especially in cases where LLMs might be less biased than humans.
- Why unresolved: The paper presents arguments for both sides but does not provide a definitive answer on whether to prioritize alignment with human responses or to embrace potential improvements from LLM behavior.
- What evidence would resolve it: Comparative studies on the outcomes of LLM-generated decisions versus human decisions in morally and legally relevant scenarios could inform whether the "correct answer" effect represents a beneficial divergence.

## Limitations
- Reliance on a single model version (March 2023) and limited temperature variation constrains generalizability
- The analysis focuses on correlation and statistical significance without exploring the underlying mechanisms driving GPT-4's responses
- Only one model and configuration tested, leaving open questions about generalizability to other LLMs, model versions, or prompting strategies

## Confidence

- **High Confidence**: The observation that GPT-4 responses correlate with human data (supported by direct statistical comparison across multiple studies)
- **Medium Confidence**: The "correct answer" effect and variance reduction claims (based on limited temperature settings and single model version)
- **Low Confidence**: Generalizability to other LLMs, model versions, or prompting strategies (only one model and configuration tested)

## Next Checks

1. **Temperature Sweep Validation**: Replicate key experiments across a range of temperature settings (0.0, 0.7, 1.2, 1.5) to quantify how response diversity changes and whether the "correct answer" effect persists at different sampling temperatures.

2. **Model Update Comparison**: Compare current GPT-4 responses with those from the March 2023 version to assess whether model updates alter the pattern of moral reasoning and correlation with human data.

3. **Cross-Model Generalization**: Test the same experimental battery with Claude 2.1, Gemini Pro, and Llama 2 to determine whether the observed patterns are specific to GPT-4 or represent broader LLM characteristics.