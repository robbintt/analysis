---
ver: rpa2
title: 'CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized
  Category Discovery'
arxiv_id: '2304.06928'
source_url: https://arxiv.org/abs/2304.06928
tags:
- data
- labelled
- unlabelled
- number
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CiPR is a generalized category discovery framework that addresses
  the problem of clustering a partially labeled dataset, where unlabeled data may
  contain instances from both known and novel categories. The method uses semi-supervised
  hierarchical clustering (SNC) to generate reliable pseudo labels for cross-instance
  positive relations in contrastive learning, which are neglected in existing methods.
---

# CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery

## Quick Facts
- arXiv ID: 2304.06928
- Source URL: https://arxiv.org/abs/2304.06928
- Reference count: 40
- Key outcome: CiPR achieves state-of-the-art performance on public generic image recognition and challenging fine-grained datasets for Generalized Category Discovery.

## Executive Summary
CiPR is a novel framework for Generalized Category Discovery (GCD) that addresses the challenge of clustering partially labeled datasets where unlabeled data may contain instances from both known and novel categories. The framework introduces a semi-supervised hierarchical clustering (SNC) algorithm to generate reliable pseudo labels for cross-instance positive relations in contrastive learning, which are neglected in existing methods. CiPR also presents an efficient method to estimate the unknown class number using SNC with a joint reference score considering clustering indexes of both labeled and unlabeled data.

## Method Summary
CiPR leverages semi-supervised hierarchical clustering (SNC) to generate reliable pseudo labels for cross-instance positive relations in contrastive learning. The SNC algorithm iteratively constructs a hierarchy of partitions while satisfying constraints imposed by labeled instances, using selective neighbor rules to form clusters that serve as pseudo labels. The framework then performs joint contrastive learning with two supervised contrastive losses - one using true labels from labeled data and another using pseudo labels from SNC for all data. Additionally, CiPR introduces a one-by-one merging strategy to estimate the unknown class number efficiently without requiring multiple clustering runs.

## Key Results
- CiPR achieves state-of-the-art performance on public generic image recognition and challenging fine-grained datasets for GCD
- The framework outperforms existing methods on CIFAR-10, CIFAR-100, ImageNet-100, CUB-200, Stanford Cars, and Herbarium19 datasets
- CiPR demonstrates robust performance across datasets with varying numbers of classes and class distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SNC generates reliable pseudo labels for cross-instance positive relations that improve contrastive learning
- Core assumption: Selective neighbor constraints preserve semantic consistency while preventing over-connection
- Evidence: "SNC iteratively constructs a hierarchy of partitions while satisfying constraints imposed by labeled instances"
- Break condition: If selective neighbor rules create chains that are too short or too long relative to class distributions

### Mechanism 2
- Claim: Joint contrastive learning with true and pseudo positive relations provides unbiased representation learning
- Core assumption: SNC pseudo labels have sufficient quality to provide meaningful contrastive pairs
- Evidence: "CiPR is a generalized category discovery framework that addresses the problem of clustering a partially labeled dataset"
- Break condition: If SNC produces too many incorrect pseudo labels, contrastive signal becomes noisy

### Mechanism 3
- Claim: One-by-one merging strategy enables efficient class number estimation
- Core assumption: Hierarchical structure from SNC provides meaningful cluster granularity levels
- Evidence: "We further present a method to estimate the unknown class number using SNC with a joint reference score"
- Break condition: If hierarchy levels are too sparse or dense, merging process overshoots or undershoots correct class count

## Foundational Learning

- Concept: Semi-supervised learning
  - Why needed here: GCD requires leveraging both labelled and unlabelled data to discover novel categories while respecting known categories
  - Quick check question: What distinguishes semi-supervised learning from unsupervised learning in the context of GCD?

- Concept: Contrastive learning
  - Why needed here: Framework uses contrastive learning to learn discriminative representations by pulling together positive pairs and pushing apart negative pairs
  - Quick check question: How does supervised contrastive learning differ from self-supervised contrastive learning?

- Concept: Hierarchical clustering
  - Why needed here: SNC produces hierarchy of partitions that enables both pseudo label generation and class number estimation through merging
  - Quick check question: Why is hierarchical clustering more suitable than flat clustering for the GCD problem?

## Architecture Onboarding

- Component map: Input -> Backbone (ViT-B-16 with DINO) -> Projection head -> SNC module -> Contrastive loss -> Output
- Critical path:
  1. Initialize ViT with DINO weights
  2. Generate pseudo labels using SNC on current features
  3. Compute supervised contrastive losses with true and pseudo labels
  4. Update model parameters
  5. Repeat until convergence
  6. Estimate class number using joint reference score
  7. Assign final labels using one-by-one merging

- Design tradeoffs:
  - SNC chain length λ: Longer chains preserve more connections but risk false positives; shorter chains are safer but may miss semantic relationships
  - Level selection for pseudo label generation: Lower levels provide more overclustering (safer) but fewer relations; higher levels provide more relations but less purity
  - Temperature parameters τs and τa: Higher temperatures soften contrastive signal; lower temperatures make it more peaked

- Failure signatures:
  - Poor performance on unseen classes: Indicates pseudo labels are not capturing novel category structure
  - High variance in results: Suggests SNC is sensitive to initialization or hyper-parameters
  - Slow convergence: May indicate contrastive losses are not well-balanced or pseudo labels are too noisy

- First 3 experiments:
  1. Train baseline GCD method [34] on CIFAR-10 with standard split to establish performance floor
  2. Implement SNC with fixed chain length and evaluate pseudo label purity at different hierarchy levels
  3. Add joint contrastive learning with SNC pseudo labels and measure improvement on unseen classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of chain length λ affect the performance of the selective neighbor clustering (SNC) algorithm, and is there an optimal value for λ that works across all datasets?
- Basis in paper: [explicit] The paper discusses different formulations of chain length λ and their impact on performance
- Why unresolved: While the paper presents some formulations of λ and their performance, it does not provide a definitive answer on the optimal value of λ or how it affects the performance of SNC across different datasets
- What evidence would resolve it: Experiments comparing the performance of SNC with different values of λ on various datasets would help determine the optimal value of λ and its impact on performance

### Open Question 2
- Question: How does the proposed CiPR framework perform on datasets with a long-tailed distribution of classes, and what modifications, if any, are needed to handle such datasets?
- Basis in paper: [inferred] The paper mentions that the Herbarium19 dataset contains many classes and has a long-tailed distribution, but it does not explicitly discuss how CiPR handles such datasets
- Why unresolved: The paper does not provide any insights into how CiPR performs on datasets with a long-tailed distribution or what modifications, if any, are needed to handle such datasets
- What evidence would resolve it: Experiments evaluating the performance of CiPR on datasets with a long-tailed distribution and comparing it with other methods would help understand its effectiveness and potential modifications needed

### Open Question 3
- Question: How does the proposed CiPR framework handle cases where the unlabelled data contains instances from both seen and unseen classes, and what strategies can be employed to improve its performance in such scenarios?
- Basis in paper: [inferred] The paper discusses the problem of generalized category discovery (GCD) and proposes a framework to address it, but it does not explicitly discuss how CiPR handles cases where the unlabelled data contains instances from both seen and unseen classes
- Why unresolved: The paper does not provide any insights into how CiPR handles cases where the unlabelled data contains instances from both seen and unseen classes or what strategies can be employed to improve its performance in such scenarios
- What evidence would resolve it: Experiments evaluating the performance of CiPR on datasets with unlabelled data containing instances from both seen and unseen classes and comparing it with other methods would help understand its effectiveness and potential strategies for improvement

## Limitations

- SNC algorithm's effectiveness heavily depends on the quality of feature representations, which may degrade if the backbone model fails to capture semantic similarity
- One-by-one merging strategy assumes the hierarchical structure provides meaningful granularity levels, which may not hold for all datasets
- Selective neighbor constraints in SNC require careful tuning of chain length λ, which could be dataset-dependent

## Confidence

- High confidence in the overall framework design and experimental validation
- Medium confidence in the theoretical guarantees of SNC algorithm convergence
- Medium confidence in the generalizability of the class number estimation method

## Next Checks

1. Ablation study on chain length λ to quantify its impact on clustering performance across different datasets
2. Comparison of class number estimation accuracy with alternative methods like eigengap analysis
3. Analysis of pseudo label quality at different hierarchy levels during training to validate the selective neighbor approach