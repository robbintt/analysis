---
ver: rpa2
title: Stethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation
  on Respiratory Sound Classification
arxiv_id: '2312.09603'
source_url: https://arxiv.org/abs/2312.09603
tags:
- domain
- sound
- learning
- stethoscope
- sg-scl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of domain dependency in lung
  sound classification caused by the use of different electronic stethoscopes. They
  propose a stethoscope-guided supervised contrastive learning approach that adapts
  to cross-domain variations while maintaining class-specific consistency.
---

# Stethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation on Respiratory Sound Classification

## Quick Facts
- arXiv ID: 2312.09603
- Source URL: https://arxiv.org/abs/2312.09603
- Reference count: 0
- Key outcome: 61.71% ICBHI Score, 2.16% improvement over baseline

## Executive Summary
This paper addresses the challenge of domain dependency in lung sound classification caused by different electronic stethoscopes. The authors propose a stethoscope-guided supervised contrastive learning approach that adapts to cross-domain variations while maintaining class-specific consistency. Using a multi-viewed batch strategy with supervised contrastive loss and gradient reversal, the method effectively reduces domain dependency and improves classification accuracy for abnormal respiratory sounds. Experiments on the ICBHI dataset demonstrate the method's effectiveness, achieving an ICBHI Score of 61.71%, a 2.16% improvement over the baseline.

## Method Summary
The proposed method combines supervised contrastive learning with domain adversarial training to address cross-domain adaptation in respiratory sound classification. The approach uses a multi-viewed batch strategy where augmented samples from the same source are used as anchors and targets. Supervised contrastive loss pulls together samples of the same class while pushing apart samples from different classes. Gradient reversal is applied to reduce domain-specific variations, and stop-gradient operations on target representations prevent overfitting to specific stethoscope characteristics. The method is evaluated using the ICBHI dataset with 4 stethoscope types and achieves improved classification performance through domain-invariant feature learning.

## Key Results
- Achieved ICBHI Score of 61.71%, representing a 2.16% improvement over baseline
- Demonstrated effectiveness of stethoscope-guided supervised contrastive learning for cross-domain adaptation
- Successfully reduced domain dependency while maintaining class-specific consistency in respiratory sound classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The stethoscope-guided supervised contrastive learning (SG-SCL) method improves cross-domain adaptation by encouraging the model to learn domain-invariant features while maintaining class-specific consistency.
- Mechanism: SG-SCL uses a multi-viewed batch strategy where augmented samples from the same source are used as anchors and targets. The supervised contrastive loss pulls together samples of the same class while pushing apart samples from different classes, but crucially, it also applies gradient reversal to reduce domain-specific variations. The stop-gradient operation on target representations ensures that the model focuses on domain adaptation without overfitting to specific stethoscope characteristics.
- Core assumption: The stop-gradient operation on target representations is necessary for effective domain adaptation in the latent space.
- Evidence anchors: [abstract] The method uses supervised contrastive loss and gradient reversal to mitigate domain shifts; [section] The augmented samples from the multi-viewed batch are first fed into the encoder and then computed with Eq. 2. This encourages the model to reduce the dependencies between distinct classes while maintaining equivalence in the same class. Note that the gradient reversal in Fig. 2 depicted that the gradients are multiplied by a negative constant during the back-propagation process.
- Break condition: If the stop-gradient operation is removed, the model's performance may degrade due to overfitting to specific stethoscope characteristics.

### Mechanism 2
- Claim: Domain adversarial training (DAT) helps the model learn features that are indistinguishable across different stethoscope domains.
- Mechanism: DAT introduces a domain classifier that attempts to distinguish between different stethoscope types. The feature extractor is trained to minimize classification error for respiratory sound classes while maximizing the domain classifier's error, effectively forcing the model to learn domain-invariant features. This is achieved through a gradient reversal layer that reverses the gradient during back-propagation.
- Core assumption: The domain classifier can effectively distinguish between different stethoscope domains, and the gradient reversal layer can force the feature extractor to learn domain-invariant features.
- Evidence anchors: [abstract] The method uses supervised contrastive loss and gradient reversal to mitigate domain shifts; [section] We formulate the proposed DAT as: LDAT = LCE + λLDA where λ is a domain regularization parameter drawn from [11]. In other words, the goal of DAT is to minimize the classification error for the respiratory sound class and to ensure that the learned features cannot distinguish between the stethoscope domains.
- Break condition: If the domain classifier is not effective in distinguishing between different stethoscope domains, the gradient reversal layer may not force the feature extractor to learn domain-invariant features effectively.

### Mechanism 3
- Claim: The multi-viewed batch strategy improves the effectiveness of contrastive learning by providing multiple perspectives of the same data.
- Mechanism: The multi-viewed batch strategy involves creating multiple augmented versions of the same data sample. These different views serve as positive pairs in the contrastive learning framework, encouraging the model to learn robust and invariant features. This strategy is particularly effective in addressing the scarcity of data in medical applications.
- Core assumption: Multiple augmented views of the same data provide meaningful positive pairs for contrastive learning.
- Evidence anchors: [abstract] The method uses a multi-viewed batch strategy, applying supervised contrastive loss and gradient reversal to mitigate domain shifts; [section] In other words, the augmented samples from the multi-viewed batch are first fed into the encoder and then computed with Eq. 2. This encourages the model to reduce the dependencies between distinct classes while maintaining equivalence in the same class.
- Break condition: If the augmentations do not provide meaningful variations or if the batch size is too small, the effectiveness of the multi-viewed batch strategy may be limited.

## Foundational Learning

- Concept: Domain adaptation
  - Why needed here: The paper addresses the challenge of domain dependency in lung sound classification caused by the use of different electronic stethoscopes. Domain adaptation techniques are necessary to transfer knowledge from a source domain (one stethoscope type) to a distinct target domain (another stethoscope type).
  - Quick check question: What is the primary goal of domain adaptation in the context of this paper?

- Concept: Supervised contrastive learning
  - Why needed here: The paper proposes a stethoscope-guided supervised contrastive learning approach to adapt to cross-domain variations while maintaining class-specific consistency. Supervised contrastive learning is used to pull together samples of the same class and push apart samples from different classes, with the added benefit of gradient reversal for domain adaptation.
  - Quick check question: How does supervised contrastive learning differ from traditional contrastive learning in this context?

- Concept: Gradient reversal
  - Why needed here: The paper uses gradient reversal in both the domain adversarial training and the supervised contrastive learning approaches. Gradient reversal is a technique that reverses the gradient during back-propagation, which is crucial for forcing the model to learn domain-invariant features.
  - Quick check question: What is the role of gradient reversal in the proposed methods?

## Architecture Onboarding

- Component map: Encoder -> Projector -> (Label Classifier OR Domain Classifier) -> Loss Computation -> Backpropagation with Gradient Reversal
- Critical path: Encoder → Projector → (Label Classifier OR Domain Classifier) → Loss Computation → Backpropagation with Gradient Reversal
- Design tradeoffs:
  - Using a larger projector may improve the quality of the representations but increases computational cost
  - Increasing the batch size may improve the effectiveness of contrastive learning but requires more memory
  - The choice of temperature parameter (τ) in the contrastive loss affects the sharpness of the cosine similarity
- Failure signatures:
  - If the model overfits to specific stethoscope characteristics, it may perform well on the training set but poorly on unseen stethoscope types
  - If the domain classifier is not effective, the gradient reversal may not force the feature extractor to learn domain-invariant features
- First 3 experiments:
  1. Train the model with only the label classifier and cross-entropy loss to establish a baseline
  2. Add the domain classifier and train with domain adversarial training to see if it improves performance
  3. Implement the stethoscope-guided supervised contrastive learning and compare its performance to the baseline and DAT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed stethoscope-guided supervised contrastive learning method perform on other medical audio datasets with different domain shifts, such as heart sound classification or cough detection?
- Basis in paper: [inferred] The paper demonstrates effectiveness on the ICBHI dataset for respiratory sound classification but does not explore other medical audio domains.
- Why unresolved: The method's generalizability to other medical audio tasks with domain shifts remains untested, limiting understanding of its broader applicability.
- What evidence would resolve it: Experiments applying SG-SCL to heart sound or cough detection datasets, comparing performance against baselines and existing domain adaptation methods.

### Open Question 2
- Question: What is the impact of varying the temperature parameter τ in the supervised contrastive loss on the model's ability to reduce domain dependency and maintain class consistency?
- Basis in paper: [explicit] The paper mentions using τ = 0.06 but does not explore the sensitivity of the method to this hyperparameter.
- Why unresolved: The choice of τ may significantly influence the trade-off between domain adaptation and class discrimination, but its optimal value and effects are not investigated.
- What evidence would resolve it: A hyperparameter study varying τ across a range of values, analyzing its impact on domain dependency reduction, class consistency, and overall classification performance.

### Open Question 3
- Question: How does the proposed method handle more extreme domain shifts, such as those between stethoscopes with vastly different frequency responses or recording environments?
- Basis in paper: [inferred] The paper focuses on cross-domain adaptation between different stethoscope types but does not test the method's robustness to more extreme domain shifts.
- Why unresolved: The effectiveness of SG-SCL in handling severe domain discrepancies is unknown, which is crucial for real-world applications where recording conditions vary widely.
- What evidence would resolve it: Experiments involving data from stethoscopes with significantly different characteristics or recorded in diverse environments, evaluating the method's performance degradation and adaptation capabilities.

## Limitations

- Limited discussion of hyperparameter sensitivity, particularly for the temperature parameter τ and domain adaptation weight λ
- No ablation studies provided to isolate the contribution of individual components (gradient reversal, multi-view strategy, domain classifier)
- Performance improvement (61.71% ICBHI Score) is modest (2.16% over baseline) given the methodological complexity

## Confidence

- **High**: The core methodology (AST architecture, SpecAugment preprocessing, weighted cross-entropy loss) is clearly specified and reproducible
- **Medium**: The general approach of combining supervised contrastive learning with domain adversarial training is sound, but implementation details have gaps
- **Low**: Claims about the specific contribution of the "stethoscope-guided" component are not fully supported by ablation studies or comparative analysis

## Next Checks

1. Implement ablation studies to quantify the individual contributions of supervised contrastive learning, gradient reversal, and multi-viewed batch strategy to overall performance
2. Test the model on additional respiratory sound datasets (e.g., ICBHI Challenge 2017 test set or other publicly available datasets) to validate cross-domain generalization
3. Conduct hyperparameter sensitivity analysis for the temperature parameter τ in the contrastive loss and the domain adaptation weight λ schedule