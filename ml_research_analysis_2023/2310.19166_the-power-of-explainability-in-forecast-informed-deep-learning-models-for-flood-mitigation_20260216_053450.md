---
ver: rpa2
title: The Power of Explainability in Forecast-Informed Deep Learning Models for Flood
  Mitigation
arxiv_id: '2310.19166'
source_url: https://arxiv.org/abs/2310.19166
tags:
- flood
- water
- evaluator
- schedules
- mitigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes FIDLAR, a Forecast Informed Deep Learning Architecture,
  for flood mitigation in watersheds with hydraulic structures. The core method idea
  involves training two deep learning components: a Flood Manager to predict control
  schedules for gates and pumps, and a Flood Evaluator to validate these schedules
  by predicting resulting water levels.'
---

# The Power of Explainability in Forecast-Informed Deep Learning Models for Flood Mitigation

## Quick Facts
- arXiv ID: 2310.19166
- Source URL: https://arxiv.org/abs/2310.19166
- Reference count: 35
- Key outcome: FIDLAR outperforms state-of-the-art methods with several orders of magnitude speedup and provably better pre-release schedules.

## Executive Summary
This paper introduces FIDLAR, a Forecast-Informed Deep Learning Architecture for flood mitigation in watersheds with hydraulic structures. The system trains two deep learning components: a Flood Manager that predicts optimal gate and pump schedules, and a Flood Evaluator that validates these schedules by predicting resulting water levels. The training process uses backpropagation from the Evaluator to improve the Manager, optimizing a loss function that balances flood mitigation and water wastage. Experiments using South Florida Water Management District data show FIDLAR achieves superior performance with significant speedup over existing methods.

## Method Summary
FIDLAR consists of two deep learning components: the Flood Manager generates control schedules for gates and pumps, while the Flood Evaluator predicts resulting water levels to validate these schedules. The system uses backpropagation from the Evaluator to improve the Manager during training. The FloodGTN model, a Graph Transformer Network combining GNNs, transformers, LSTMs, and CNNs, serves as the best-performing architecture for flood prediction and mitigation. Training optimizes a loss function balancing flood mitigation effectiveness against water wastage, with the Evaluator providing feedback signals to the Manager.

## Key Results
- FloodGTN achieves MAE of 0.040 ft and RMSE of 0.056 ft for flood prediction
- FIDLAR reduces flood time steps and area compared to rule-based and GA-based methods
- The approach provides several orders of magnitude speedup over current state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backpropagation from the Flood Evaluator improves the Flood Manager's gate and pump schedules.
- Mechanism: The Flood Manager generates control schedules, the Flood Evaluator predicts resulting water levels, and the loss function gradients are propagated back to the Manager to adjust its parameters.
- Core assumption: The Evaluator is pre-trained and fixed during Manager training, providing stable feedback signals.
- Evidence anchors:
  - [abstract] "FIDLAR uses backpropagation from the Evaluator to improve the Manager by evaluating the generated schedules"
  - [section] "The backpropagation algorithm is used to backpropagate the feedback on the quality of the generated schedules to 'nudge' the Flood Manager to produce more effective schedules"
  - [corpus] Strong evidence: FIDLAR is mentioned as the Forecast-Informed Deep Learning Architecture that uses backpropagation from Evaluator to Manager.
- Break condition: If the Evaluator is not sufficiently accurate or overfits, its feedback could mislead the Manager, preventing convergence to good schedules.

### Mechanism 2
- Claim: The FloodGTN model achieves superior flood prediction by combining GNNs, transformers, LSTMs, and CNNs.
- Mechanism: GNN-LSTM learns spatio-temporal water level dynamics, CNN-Transformer extracts feature representations, and attention identifies interactions between variables.
- Core assumption: Different network architectures can capture distinct aspects of flood dynamics, and their combination improves overall prediction accuracy.
- Evidence anchors:
  - [section] "The best-performing DL model for Flood prediction and mitigation is described here and is referred to as FloodGTN (Graph Transformer Network). More specifically, FloodGTN combines graph neural networks (GNNs), attention-based transformer networks, long short-term memory networks (LSTMs), and convolutional neural networks (CNNs) for various objectives."
  - [corpus] Strong evidence: Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates is mentioned as related work, suggesting this combination approach is recognized in the field.
- Break condition: If the attention mechanism incorrectly weights variables, or if the component networks are not well-aligned in their representations, the combined model could underperform compared to simpler baselines.

### Mechanism 3
- Claim: LIME explainability reveals that tidal information and gate schedules are the dominant factors in flood predictions.
- Mechanism: LIME computes the contribution of each input variable to the model output, producing heatmaps that highlight which variables influence predictions most strongly.
- Core assumption: Local surrogate models can approximate the behavior of complex deep learning models in a way that is interpretable and actionable.
- Evidence anchors:
  - [section] "The contribution of each input variable (at each time point) on the model output was computed using LIME... First, what jumps out immediately is that the predicted water levels throughout the river system under normal conditions (mild to no rain) are overwhelmingly impacted by the tidal conditions measured at S4"
  - [corpus] Weak evidence: No direct mention of LIME in related papers, but Explainable parallel rcnn with novel feature representation for time series forecasting is mentioned, suggesting explainability methods are relevant to this domain.
- Break condition: If LIME explanations are inconsistent across similar inputs or fail to capture non-linear interactions, they may mislead users about the true drivers of model predictions.

## Foundational Learning

- Concept: Backpropagation and gradient descent optimization
  - Why needed here: The Flood Manager is trained by propagating loss gradients from the Evaluator back through the model to update its parameters and improve schedule generation.
  - Quick check question: In the context of FIDLAR, what component's parameters are updated using gradients from the loss function, and through which mechanism does this occur?

- Concept: Graph neural networks and attention mechanisms
  - Why needed here: The FloodGTN model uses GNNs to capture spatial relationships between river stations and attention mechanisms to identify which variables most influence water levels.
  - Quick check question: What two complementary deep learning components does FloodGTN use to handle spatial dependencies and variable importance in flood prediction?

- Concept: Explainability methods (LIME)
  - Why needed here: LIME provides interpretable explanations of which input variables contribute most to model predictions, helping validate that the model learns physically meaningful patterns.
  - Quick check question: What explainability technique is used to compute the contribution of each input variable to the model output in the form of heatmaps?

## Architecture Onboarding

- Component map: Flood Manager -> Flood Evaluator -> Loss Function -> Backpropagation to Flood Manager
- Critical path: During training, the Manager generates schedules → Evaluator predicts water levels → loss is computed → gradients are backpropagated to Manager → Manager parameters are updated. During inference, only the Manager is used for real-time flood mitigation.
- Design tradeoffs: Using a pre-trained Evaluator provides stable training signals but adds complexity; combining multiple network architectures in FloodGTN increases accuracy but also model complexity and training time.
- Failure signatures: If the Evaluator is inaccurate, the Manager may learn poor schedules; if attention weights are incorrect, the model may miss important variables; if LIME explanations are inconsistent, users may lose trust in the system.
- First 3 experiments:
  1. Train the Flood Evaluator alone on historical data to ensure it can accurately predict water levels given schedules.
  2. Train the Flood Manager with a simple Evaluator (e.g., linear model) to verify the backpropagation mechanism works before using the full FloodGTN.
  3. Compare FloodGTN against individual component models (GNN, Transformer, LSTM, CNN) to verify the combined architecture provides measurable improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the model explainability tools (LIME, attention scores) perform when applied to flood mitigation scenarios in other geographical locations or with different hydraulic structures?
- Basis in paper: [explicit] The paper states "The main contribution of this paper is the effective use of tools for model explainability, allowing us to understand the contribution of the various environmental factors towards its decisions."
- Why unresolved: The experiments were conducted only on the South Florida watershed, so generalizability to other locations remains unknown.
- What evidence would resolve it: Applying the same explainability techniques to FIDLAR models trained on data from different river systems, coastal areas, or with different types of hydraulic structures would show whether the insights about tidal influence and gate scheduling patterns hold universally.

### Open Question 2
- Question: Can the explainability methods distinguish between correlation and causation when identifying which variables influence flood levels most strongly?
- Basis in paper: [inferred] The paper shows attention scores and LIME contributions for input variables, but doesn't address whether these relationships are causal or merely correlational.
- Why unresolved: Model explainability tools can reveal statistical associations but cannot by themselves establish causal mechanisms without additional analysis.
- What evidence would resolve it: Conducting causal inference analysis (e.g., using instrumental variables or causal graphs) alongside the explainability methods would determine whether the identified influential variables are truly driving flood behavior.

### Open Question 3
- Question: How sensitive is the model's explainability to different threshold values for flooding and water wastage in the loss function?
- Basis in paper: [explicit] The loss function uses thresholds X_flood and X_waste, and the explainability analysis shows how input variables contribute to predictions.
- Why unresolved: The paper uses specific threshold values but doesn't explore how changing these thresholds affects the model's decision-making patterns revealed through explainability tools.
- What evidence would resolve it: Running sensitivity analysis by varying the threshold values and observing how the attention scores and LIME contributions change would show whether the explainability insights are robust to different operational constraints.

## Limitations
- The study relies on a specific watershed dataset that may not generalize to different hydrological conditions.
- The computational complexity of training and deploying multiple deep learning models presents practical challenges.
- The assumption that the pre-trained Evaluator provides reliable feedback during Manager training is critical but untested.

## Confidence
- Backpropagation from Evaluator to Manager: High confidence based on established optimization principles.
- FloodGTN architecture performance: Medium confidence as it combines well-established components but needs broader validation.
- LIME-based explainability findings: Medium confidence as the method is established but may have limitations with complex spatio-temporal models.

## Next Checks
1. Test FIDLAR on flood datasets from different geographic regions and watershed characteristics to assess generalizability.
2. Perform ablation studies to quantify the contribution of each component in FloodGTN (GNN, Transformer, LSTM, CNN) to overall performance.
3. Evaluate model performance under extreme weather conditions and climate change scenarios to assess robustness and reliability.