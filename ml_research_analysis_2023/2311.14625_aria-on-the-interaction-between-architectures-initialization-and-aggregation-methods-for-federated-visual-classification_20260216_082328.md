---
ver: rpa2
title: 'ARIA: On the Interaction Between Architectures, Initialization and Aggregation
  Methods for Federated Visual Classification'
arxiv_id: '2311.14625'
source_url: https://arxiv.org/abs/2311.14625
tags:
- training
- learning
- pre-training
- arxiv
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing federated learning
  (FL) pipelines for medical image classification by jointly studying the impact of
  architectures, initialization methods, and aggregation strategies. The authors evaluate
  9 architectures with 3 initialization methods (random, ImageNet pre-training, self-supervised
  learning on task-relevant data) and 3 aggregation strategies (FedAvg, FedOpt, SCAFFOLD)
  across 3 medical imaging datasets.
---

# ARIA: On the Interaction Between Architectures, Initialization and Aggregation Methods for Federated Visual Classification

## Quick Facts
- arXiv ID: 2311.14625
- Source URL: https://arxiv.org/abs/2311.14625
- Reference count: 0
- Key outcome: This paper addresses the challenge of optimizing federated learning (FL) pipelines for medical image classification by jointly studying the impact of architectures, initialization methods, and aggregation strategies.

## Executive Summary
This study systematically evaluates 9 architectures with 3 initialization methods (random, ImageNet pre-training, self-supervised learning on task-relevant data) and 3 aggregation strategies (FedAvg, FedOpt, SCAFFOLD) across 3 medical imaging datasets. The authors find that ARIA elements must be chosen together for optimal performance, with ResNet-18 and DenseNet-121 showing strong results. Normalization-free architectures with scaled weight standardization are promising alternatives to batch normalization. SSL pre-training significantly improves transformer performance and is particularly effective when target domains differ from ImageNet. FedOpt slightly improves IID settings while SCAFFOLD consistently helps in heterogeneous scenarios.

## Method Summary
The paper evaluates 9 architectures (ResNet variants, DenseNet, SWIN Transformer, ViT, EfficientNetV2, ConvNext) × 3 initializations (random, ImageNet pre-training, SSL pre-training on task-relevant datasets) × 3 aggregation methods (FedAvg, FedOpt, SCAFFOLD) across 3 medical imaging datasets. The datasets include Fed-ISIC (23,247 RGB skin lesion images, 8 classes, 6 clients) and OrganAMNIST (58,850 grayscale CT images, 11 organ labels, 4 clients with Dirichlet partitioning). Training uses NVFlare for federated learning with 20-50 rounds depending on dataset, using Adam optimizer with learning rate 5e-4 for Fed-ISIC.

## Key Results
- ARIA elements (Architecture, Initialization, Aggregation) must be chosen together rather than independently for optimal FL performance
- ResNet-18 and DenseNet-121 architectures show consistently strong results across datasets and configurations
- SSL pre-training significantly improves transformer performance and outperforms ImageNet pre-training when target domains differ from natural images
- Normalization-free architectures with scaled weight standardization provide competitive performance to batch normalization, especially in heterogeneous settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on task-relevant data through SSL significantly improves FL performance, especially for transformers and when target domains differ from ImageNet.
- Mechanism: SSL pre-training enables models to learn useful representations specific to the medical imaging domain before federated training begins. This reduces the need for large local datasets and helps models converge faster with better generalization.
- Core assumption: SSL pre-training on data similar to the target task provides more domain-specific features than general ImageNet pre-training.
- Evidence anchors:
  - [abstract] "SSL pre-training significantly improves transformer performance and is particularly effective when target domains differ from ImageNet."
  - [section] "SSL pre-training greatly increases transformer performance specifically, and even outperforms IN weights in Fed-ISIC, despite the lack of longer training and tuning of the SSL pipeline."
  - [corpus] Weak - no corpus evidence directly addressing SSL effectiveness in FL.

### Mechanism 2
- Claim: Batch Normalization degrades FL performance in heterogeneous settings due to clients calculating statistics that are not representative of each other's datasets.
- Mechanism: BN requires batch statistics that vary across clients in heterogeneous FL settings, causing inconsistency in model updates. Normalization-free architectures with scaled weight standardization (SWS) provide more stable training.
- Core assumption: Local batch statistics in BN lead to client drift and inconsistent updates across heterogeneous clients.
- Evidence anchors:
  - [abstract] "Normalization-free architectures with scaled weight standardization are promising alternatives to batch normalization."
  - [section] "It has been widely discussed, most recently in [13], that BN impedes FL performance under heterogeneous settings due to the local clients calculating statistics that are not representative of each other's datasets."
  - [corpus] Weak - no corpus evidence directly addressing BN degradation in FL.

### Mechanism 3
- Claim: ARIA elements (Architecture, Initialization, Aggregation) must be chosen together rather than independently for optimal FL performance.
- Mechanism: The effectiveness of each ARIA element depends on the choices made for the others. For example, certain architectures work better with specific initialization methods, and aggregation strategies perform differently based on architecture choices.
- Core assumption: There are interaction effects between architecture choices, initialization methods, and aggregation strategies that cannot be optimized independently.
- Evidence anchors:
  - [abstract] "ARIA elements must be chosen together for optimal performance, with ResNet-18 and DenseNet-121 showing strong results."
  - [section] "Our results after training more than 300 ARIAs indicate to researchers and practitioners designing FL pipelines for medical imaging data that all elements of an ARIA have to be evaluated together."
  - [corpus] Weak - no corpus evidence directly addressing the need to jointly optimize all three elements.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: Understanding FL is fundamental to grasping why the paper's joint optimization of ARIA elements matters and how it differs from centralized training.
  - Quick check question: What is the key difference between federated learning and traditional centralized training?

- Concept: Self-Supervised Learning (SSL)
  - Why needed here: SSL is used as an alternative to ImageNet pre-training, and understanding its mechanisms helps explain why it performs well for medical imaging tasks.
  - Quick check question: How does self-supervised learning differ from supervised learning in terms of label requirements?

- Concept: Normalization Methods (BN vs LN vs SWS)
  - Why needed here: Different normalization methods have different effects on FL performance, particularly in heterogeneous settings.
  - Quick check question: What is the main advantage of Layer Normalization over Batch Normalization in federated learning scenarios?

## Architecture Onboarding

- Component map:
  Model Architectures (9 options) -> Initialization Methods (3 options) -> Aggregation Strategies (3 options) -> Datasets (Fed-ISIC, OrganAMNIST) -> Federated Training Pipeline

- Critical path:
  1. Select architecture based on memory constraints and task requirements
  2. Choose initialization method based on data similarity to ImageNet and availability of task-relevant pre-training data
  3. Select aggregation strategy based on data heterogeneity level
  4. Train and evaluate on target federated dataset

- Design tradeoffs:
  - Model size vs. memory footprint (ResNet-18 vs. larger models)
  - Pre-training benefits vs. computational cost (ImageNet vs. SSL vs. random)
  - Aggregation method complexity vs. performance gain (FedAvg vs. SCAFFOLD)
  - Normalization method stability vs. regularization effects (BN vs. LN vs. SWS)

- Failure signatures:
  - Poor performance on heterogeneous datasets with BN-normalized models
  - Transformers underperforming when randomly initialized
  - Suboptimal results when choosing ARIA elements independently
  - High memory usage with larger architectures on resource-constrained clients

- First 3 experiments:
  1. Test ResNet-18 with random initialization and FedAvg on OrganAMNIST to establish baseline performance
  2. Compare ResNet-18 with ImageNet pre-training vs. SSL pre-training on Fed-ISIC to evaluate pre-training effectiveness
  3. Test DenseNet-121 with normalization-free architecture and FedAvg on heterogeneous OrganAMNIST to validate BN alternatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of normalization-free architectures with scaled weight standardization compare to batch normalization and layer normalization in federated learning across different data heterogeneity levels and model scales?
- Basis in paper: [explicit] The paper notes that normalization-free architectures using scaled weight standardization perform competitively with batch normalization and layer normalization, particularly in heterogeneous settings, but calls for further investigation into how generalization is affected when substituting batch normalization with these alternatives.
- Why unresolved: The study provides preliminary insights but does not conduct a comprehensive comparative analysis across varying levels of data heterogeneity or different model scales to fully understand the generalization capabilities of these normalization methods.
- What evidence would resolve it: Systematic experiments comparing normalization-free, batch normalization, and layer normalization across a range of data heterogeneity levels and model architectures, measuring not only accuracy but also robustness and generalization performance.

### Open Question 2
- Question: To what extent does self-supervised learning pre-training on task-relevant data improve federated learning performance compared to ImageNet pre-training, especially for transformer models in medical imaging tasks?
- Basis in paper: [explicit] The paper finds that self-supervised learning pre-training on task-relevant data significantly improves transformer performance and can outperform ImageNet pre-training in certain federated learning scenarios, particularly when target domains differ from natural images.
- Why unresolved: While the study demonstrates the benefits of self-supervised learning, it does not explore the full potential of this approach, such as the impact of different self-supervised learning algorithms, longer pre-training times, or more extensive tuning.
- What evidence would resolve it: Extensive experiments varying self-supervised learning algorithms, pre-training durations, and tuning strategies to quantify the performance gains over ImageNet pre-training across diverse federated learning tasks.

### Open Question 3
- Question: How do different aggregation strategies, such as FedOpt and SCAFFOLD, perform in federated learning under varying levels of data heterogeneity and client participation, and what are the trade-offs in terms of communication efficiency and model accuracy?
- Basis in paper: [explicit] The paper observes that FedOpt slightly improves performance in IID settings while SCAFFOLD consistently helps in heterogeneous scenarios, but also notes that the best ARIA uses FedAvg, suggesting that the benefits of switching aggregation methods are less significant than those of changing the architecture.
- Why unresolved: The study provides insights into the performance of aggregation strategies but does not fully explore the trade-offs between communication efficiency and model accuracy, nor does it investigate the impact of varying client participation levels.
- What evidence would resolve it: Comprehensive experiments evaluating aggregation strategies under different data heterogeneity levels and client participation rates, measuring both model accuracy and communication efficiency to determine optimal strategies for various federated learning scenarios.

## Limitations

- The study's conclusions are based on evaluation across only 3 medical imaging datasets with specific characteristics
- The computational expense of running all 81 ARIA combinations limits the exploration of more extensive hyperparameter spaces or additional architectures
- SSL pre-training effectiveness may not generalize to other medical imaging modalities or tasks

## Confidence

- **High confidence**: The observation that ARIA elements must be chosen together for optimal performance, supported by systematic evaluation across multiple datasets and configurations
- **Medium confidence**: The superiority of SSL pre-training for transformers and in non-ImageNet-like domains, as results are promising but limited to specific datasets and pre-training procedures
- **Medium confidence**: The recommendation to avoid BN in favor of normalization-free alternatives, as this is based on theoretical understanding and initial experimental results but needs broader validation

## Next Checks

1. **Cross-domain validation**: Test the recommended ARIA combinations (ResNet-18/DenseNet-121 with SSL pre-training and FedOpt/SCAFFOLD) on non-medical imaging federated datasets to assess generalizability beyond the medical domain

2. **Ablation of pre-training scale**: Systematically vary the amount of SSL pre-training data and compute resources to determine the minimum effective pre-training requirements for different ARIA configurations

3. **Long-term stability assessment**: Evaluate model performance across extended federated training periods (100+ rounds) to identify potential degradation patterns in the recommended ARIA combinations that may not be apparent in shorter training runs