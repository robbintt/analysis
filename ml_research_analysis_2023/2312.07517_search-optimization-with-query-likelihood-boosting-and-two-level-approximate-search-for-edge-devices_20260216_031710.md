---
ver: rpa2
title: Search Optimization with Query Likelihood Boosting and Two-Level Approximate
  Search for Edge Devices
arxiv_id: '2312.07517'
source_url: https://arxiv.org/abs/2312.07517
tags:
- search
- tree
- query
- latency
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an approximate nearest neighbor (ANN) search
  algorithm for resource-constrained edge devices to address the challenge of entity
  resolution (ER) in voice assistants. The algorithm uses a Query Likelihood Boosted
  Tree (QLBT) to optimize average search latency for frequently used small datasets
  and a two-level approximate search to enable efficient retrieval with large datasets
  on edge devices.
---

# Search Optimization with Query Likelihood Boosting and Two-Level Approximate Search for Edge Devices

## Quick Facts
- arXiv ID: 2312.07517
- Source URL: https://arxiv.org/abs/2312.07517
- Reference count: 25
- Key outcome: Achieves deployable accuracy and latency on a 10 million dataset for edge devices with P90 latency < 80ms and recall@10 > 80%

## Executive Summary
This paper addresses the challenge of entity resolution for voice assistants on resource-constrained edge devices by proposing an approximate nearest neighbor (ANN) search algorithm that optimizes for frequently used small datasets and efficiently scales to large datasets. The algorithm combines a Query Likelihood Boosted Tree (QLBT) that reduces average search latency by 16% compared to standard projection trees, with a two-level approximate search approach that partitions large datasets into manageable subsets for efficient retrieval. The method is validated on simulated and real data, demonstrating deployable performance with the two-level search achieving optimal results when using product quantization for top-level search and brute force for bottom-level search with approximately 100 entities per subset.

## Method Summary
The paper proposes a two-pronged approach to ANN search optimization for edge devices. First, it introduces the Query Likelihood Boosted Tree (QLBT), a modified spatial partitioning tree that splits nodes based on equal probability of visiting branches rather than equal entity count, reducing average search latency by exploiting skewed query likelihood distributions. Second, it implements a two-level approximate search framework where large datasets are pre-partitioned into smaller subsets using K-means clustering, with product quantization (PQ) as the top-level search algorithm and brute force search as the bottom-level, achieving optimal performance when the average number of entities per subset is around 100. The algorithm is designed to meet strict edge device constraints with P90 latency under 80ms while maintaining recall@10 above 80%.

## Key Results
- QLBT reduces real-world query latency by 16% compared to standard projection trees
- Two-level search achieves optimal performance with PQ as top-level and brute force as bottom-level search
- Achieves deployable accuracy and latency on a 10 million dataset for edge devices with P90 latency < 80ms and recall@10 > 80%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QLBT reduces average search latency by exploiting skewed query likelihood distributions.
- Mechanism: By splitting nodes based on equal query likelihood probability rather than equal entity count, QLBT places frequently queried entities closer to the root, reducing average search depth.
- Core assumption: Query likelihood distribution is highly skewed (fat-head vs. long-tail) in real-world VA ER systems.
- Evidence anchors:
  - [abstract] "QLBT reduces real-world query latency by 16% compared to a standard projection tree."
  - [section] "The traditional ANN implementations...are designed to have approximately the same latency for each entity without factoring in query likelihood."
  - [corpus] Weak evidence - corpus neighbors discuss ANN search but not query likelihood boosting specifically.
- Break condition: If query likelihood distribution becomes uniform or nearly uniform, the advantage of QLBT diminishes.

### Mechanism 2
- Claim: Two-level approximate search enables efficient retrieval on large-scale datasets within edge device constraints.
- Mechanism: Pre-partitioning the dataset into smaller subsets allows using brute force search on manageable-sized subsets, which outperforms approximate methods when subset size is ~100 entities.
- Core assumption: Brute force search has acceptable latency when dataset is partitioned into small enough subsets.
- Evidence anchors:
  - [abstract] "The two-level search achieves optimal performance with product quantization (PQ) as the top-level search algorithm and brute search as the bottom-level, with an average of around 100 entities per subset."
  - [section] "When the average entities number within each subset is around 100, the two-level search achieves the optimal."
  - [corpus] Weak evidence - corpus neighbors discuss ANN search methods but not two-level partitioning specifically.
- Break condition: If partitioning features don't create meaningful subsets or subset size cannot be reduced to ~100 entities.

### Mechanism 3
- Claim: QLBT with early stopping and pre-grouping leaves provides robustness against tail traffic latency.
- Mechanism: After reaching depth ℓ=3, QLBT stops likelihood balancing and groups 8 entities per leaf to ensure balanced worst-case latency.
- Core assumption: Without these safeguards, unbalanced trees could have poor latency for rare queries.
- Evidence anchors:
  - [section] "Though a different construction, we use the same searching procedure described in [19] for QLBT. We emphasize that the boosted search tree must be updated for changes in users' query likelihood distribution."
  - [section] "To hedge the risk that unbalanced trees may have large latency on tail traffic...we apply additional regulations on the QLBT building procedure for robustness."
  - [corpus] No direct evidence found in corpus neighbors about tree depth limitations or pre-grouping.
- Break condition: If real-world query patterns show significant changes that invalidate the depth ℓ=3 assumption.

## Foundational Learning

- Concept: Approximate Nearest Neighbor (ANN) search
  - Why needed here: Forms the foundation of the entity resolution system being optimized for edge devices.
  - Quick check question: What is the trade-off between exact nearest neighbor search and ANN search?

- Concept: Tree-based search structures (kd-tree, R-tree, projection trees)
  - Why needed here: QLBT is built upon spatial partitioning tree concepts, modifying them for query likelihood.
  - Quick check question: How does a balanced tree differ from an unbalanced tree in terms of search latency distribution?

- Concept: Product Quantization (PQ) and its variants
  - Why needed here: PQ is used as the top-level search algorithm in the two-level approach.
  - Quick check question: How does PQ compress high-dimensional vectors and what are the implications for search accuracy?

## Architecture Onboarding

- Component map: Data ingestion layer (vector embeddings, query likelihood data) -> QLBT construction module (likelihood-aware splitting) -> Two-level search orchestrator (partitioning + algorithm selection) -> Evaluation/benchmarking module (P90 latency, recall@10 metrics)

- Critical path:
  1. Entity embeddings and query likelihood data collected
  2. QLBT constructed with likelihood-aware splitting criteria
  3. For large datasets: K-means clustering creates subsets
  4. PQ indexes centroids (top level)
  5. Brute force search within relevant subset (bottom level)
  6. Results returned with latency under 80ms P90

- Design tradeoffs:
  - QLBT vs balanced tree: better average latency vs. consistent worst-case latency
  - Brute force vs. ANN at bottom level: optimal when subset size ~100 vs. scalability
  - Pre-grouping leaves: reduced latency vs. potential recall impact
  - Depth limit ℓ=3: tail latency protection vs. potential missed likelihood optimization

- Failure signatures:
  - High P90 latency: likely subset size too large or wrong bottom-level algorithm
  - Low recall@10: too aggressive likelihood boosting or insufficient tree depth
  - High memory usage: too many PQ centroids or improper subset partitioning
  - Poor average latency improvement: query likelihood distribution not sufficiently skewed

- First 3 experiments:
  1. Run QLBT vs standard projection tree on a small dataset (10K entities) with simulated skewed query likelihood - measure average and P90 latency.
  2. Test two-level search with varying numbers of subsets on SIFT dataset (1M entities) - find optimal subset size around 100 entities.
  3. Implement the full algorithm on simulated edge device (AWS t3.xlarge) with 10M dataset - verify meets 80ms P90 latency constraint with recall@10 > 80%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Query Likelihood Boosted Tree (QLBT) performance scale with varying degrees of query likelihood skew beyond the tested range?
- Basis in paper: [explicit] The paper evaluates QLBT on simulated datasets with varying skew levels, showing increased latency reduction with higher skew, but does not test the upper limits of skew.
- Why unresolved: The paper does not explore the performance of QLBT under extreme skew conditions, which could reveal its limitations or the need for additional optimizations.
- What evidence would resolve it: Experimental results showing QLBT's performance on datasets with extreme query likelihood skew, potentially revealing saturation points or diminishing returns.

### Open Question 2
- Question: What is the impact of using different clustering algorithms (e.g., DBSCAN, Gaussian Mixture Models) instead of K-means for the two-level approximate search partitioning?
- Basis in paper: [inferred] The paper uses K-means clustering for pre-partitioning in the two-level search but does not explore alternative clustering methods that might better capture the data's structure.
- Why unresolved: The choice of K-means is not justified or compared against other clustering algorithms, leaving open the question of whether a different method could yield better performance.
- What evidence would resolve it: Comparative analysis of two-level search performance using various clustering algorithms, highlighting any improvements in accuracy or efficiency.

### Open Question 3
- Question: How does the two-level approximate search algorithm perform with non-vector data features, such as categorical or textual metadata?
- Basis in paper: [explicit] The paper mentions using features like geolocation and vectorized embeddings for partitioning but does not address the use of other data types.
- Why unresolved: The paper focuses on vector-based features for partitioning, leaving uncertainty about the algorithm's effectiveness with other data types that may be relevant in real-world applications.
- What evidence would resolve it: Experimental results demonstrating the two-level search's performance with diverse data types, such as categorical or textual metadata, to assess its generalizability.

### Open Question 4
- Question: What are the trade-offs in terms of memory usage and search speed when increasing the depth of the QLBT beyond the tested early-stop level?
- Basis in paper: [explicit] The paper sets an early-stop level for QLBT to balance between query likelihood boosting and data separation but does not explore the effects of deeper trees.
- Why unresolved: The paper does not investigate how deeper QLBT structures might affect memory usage and search speed, which could be crucial for understanding its scalability.
- What evidence would resolve it: Detailed analysis of QLBT's memory consumption and search latency as a function of tree depth, identifying optimal configurations for different use cases.

## Limitations

- Effectiveness of QLBT depends heavily on skewed query likelihood distributions, which may not generalize to other domains
- Two-level search requires careful tuning of subset sizes, with optimal performance only achieved when average entities per subset is around 100
- Algorithm performance on datasets with different characteristics (e.g., uniform query distribution or non-vector-based features) remains untested

## Confidence

- High Confidence: The 16% latency reduction claim for QLBT compared to standard projection trees is well-supported by the methodology and experimental design.
- Medium Confidence: The two-level search achieving optimal performance with ~100 entities per subset is based on empirical testing but may vary with different dataset characteristics.
- Medium Confidence: The P90 latency < 80ms and recall@10 > 80% claims for the 10M dataset, as these metrics depend heavily on the specific implementation and edge device constraints.

## Next Checks

1. Test QLBT performance on datasets with varying query likelihood distributions to verify the algorithm's robustness when the distribution deviates from the assumed skewed pattern.
2. Conduct ablation studies on the two-level search approach to determine the impact of subset size variations and alternative bottom-level search algorithms beyond brute force.
3. Implement the full algorithm on actual edge devices (rather than simulated AWS instances) to validate the claimed memory footprint and latency constraints under real hardware limitations.