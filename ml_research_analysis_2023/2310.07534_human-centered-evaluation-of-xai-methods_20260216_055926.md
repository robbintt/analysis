---
ver: rpa2
title: Human-Centered Evaluation of XAI Methods
arxiv_id: '2310.07534'
source_url: https://arxiv.org/abs/2310.07534
tags:
- image
- methods
- explanation
- clickme
- pixels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study conducted a human-centered evaluation of three local
  explanation methods (Occlusion, Layer-wise Relevance Propagation (LRP), and Prototypical
  Part Network (ProtoPNet)) by comparing them to human-generated relevance maps in
  a web-based experiment. The goal was to determine how well these methods could convey
  the decision-making process of a neural network to users, using an image classification
  task.
---

# Human-Centered Evaluation of XAI Methods

## Quick Facts
- arXiv ID: 2310.07534
- Source URL: https://arxiv.org/abs/2310.07534
- Reference count: 40
- Key outcome: Three local XAI methods (Occlusion, LRP, ProtoPNet) provided nearly equivalent image classification understanding to human explanations, though highlighting different regions.

## Executive Summary
This study evaluates the interpretability of three local explanation methods (Occlusion, LRP, and ProtoPNet) by comparing them to human-generated relevance maps in a web-based image classification experiment. The research measures how efficiently these methods convey neural network decisions to users by progressively revealing pixels ranked by relevance and tracking classification performance. Results show that while XAI methods and human explanations achieve similar classification efficiency, they often highlight different image regions, suggesting no universally optimal explanation method exists. The findings emphasize that explanation method selection should depend on specific context and user needs rather than assuming one method universally outperforms others.

## Method Summary
The study conducted a user-centric evaluation of three XAI methods using a web-based experiment with 102 images from 17 classes. Participants viewed progressively revealed images where pixels were ordered by relevance according to each method, with 4-second intervals between revelations over 1-minute trials. The experiment measured classification accuracy, average tries, and pixels revealed, comparing results against human-generated ClickMe relevance maps. The evaluation used cumulative recognition rates and Spearman rank-order correlation to assess both efficiency and alignment with human reasoning patterns.

## Key Results
- XAI methods achieved classification efficiency nearly equivalent to human explanations, with LRP and Occlusion reaching 90% accuracy with ~6,500 pixels revealed
- ProtoPNet required more pixels than other methods to achieve similar classification performance
- Correlation between XAI methods and human reasoning was weak, indicating different regions/features are highlighted despite similar classification outcomes
- No universally optimal explanation method was identified; effectiveness depends on context and user needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The human-centered evaluation framework enables comparison of XAI methods by measuring their efficiency in conveying model decisions to users.
- Mechanism: The study uses a game-like interface where users are progressively exposed to increasingly relevant pixels ranked by XAI methods, measuring the minimum number of pixels needed for correct image classification.
- Core assumption: Users can recognize image classes based solely on visible pixel features, and the efficiency of this recognition reflects the effectiveness of the explanation method.
- Evidence anchors:
  - [abstract]: "our research embarked on a user-centric study... measure the interpretability of three leading explanation methods"
  - [section]: "Our experiment aims to assess interpretability by evaluating participants' ability to identify the primary object within an image"
  - [corpus]: Weak evidence - corpus lacks direct mention of similar experimental frameworks for XAI evaluation
- Break Condition: If users cannot recognize images based on partial pixel information, or if the pixel ranking does not reflect true model decision relevance, the evaluation framework would fail to accurately measure XAI effectiveness.

### Mechanism 2
- Claim: Different XAI methods reveal similar information efficiency for image classification tasks despite highlighting different regions.
- Mechanism: The study compares the number of pixels needed by each method (LRP, Occlusion, ProtoPNet) to achieve similar recognition rates as human-generated explanations (ClickMe).
- Core assumption: Image classification relies on a core set of visually important features that multiple explanation methods can identify, even if they emphasize different aspects.
- Evidence anchors:
  - [abstract]: "while the regions spotlighted by these methods can vary widely, they all offer humans a nearly equivalent depth of understanding"
  - [section]: "Results showed that the explanation methods provided nearly equivalent understanding to human explanations, enabling users to classify images with similar efficiency"
  - [corpus]: Moderate evidence - several related works discuss XAI evaluation but lack comparable experimental comparisons
- Break Condition: If image classification fundamentally requires different feature sets that vary by method, or if human visual perception differs significantly from model feature importance, this equivalence would break down.

### Mechanism 3
- Claim: The weak correlation between XAI methods and human reasoning indicates that explanation methods often highlight different features than humans would.
- Mechanism: Spearman's ranked-order correlation coefficient is used to measure the monotonicity between relevance maps from XAI methods and human-generated ClickMe maps.
- Core assumption: Low correlation implies that XAI methods and human reasoning use different visual features for classification, despite achieving similar classification performance.
- Evidence anchors:
  - [abstract]: "the correlation between the explanation methods and human reasoning was found to be weak, indicating that these methods often highlight different regions or features than humans would"
  - [section]: "Spearman's ranked-order correlation coefficient to measure the correlation between the relevance maps generated by the explanation methods and ClickMe maps"
  - [corpus]: Weak evidence - corpus lacks studies specifically measuring correlation between XAI methods and human reasoning
- Break Condition: If the correlation measure is inappropriate for sparse, non-normal relevance maps, or if other factors (like image context) affect correlation, the interpretation of weak correlation could be invalid.

## Foundational Learning

- Concept: Image classification fundamentals
  - Why needed here: Understanding how convolutional neural networks process and classify images is essential for interpreting what XAI methods reveal about model decisions
  - Quick check question: How do convolutional filters in CNNs detect edges, textures, and patterns that contribute to image classification?

- Concept: Explanation methods in XAI
  - Why needed here: Different XAI techniques (occlusion, LRP, ProtoPNet) have distinct mechanisms for identifying relevant features, affecting their interpretability and user understanding
  - Quick check question: What is the fundamental difference between gradient-based methods like LRP and perturbation-based methods like occlusion in identifying relevant pixels?

- Concept: Human visual perception and attention
  - Why needed here: The study compares XAI explanations to human-generated relevance maps, requiring understanding of how humans visually process and prioritize image features
  - Quick check question: How does human visual attention differ from algorithmic attention mechanisms in identifying important image regions?

## Architecture Onboarding

- Component map:
  Image dataset and preprocessing -> CNN model (ResNet-50) for classification -> XAI explanation methods (LRP, Occlusion, ProtoPNet) -> Human baseline (ClickMe relevance maps) -> Web-based experimental interface -> Data collection and analysis pipeline

- Critical path:
  1. Load image and generate explanations from all methods
  2. Create progressive revelation sequences for each method
  3. Present images to participants through web interface
  4. Collect user responses and timing data
  5. Analyze recognition rates and correlation metrics
  6. Compare efficiency across methods

- Design tradeoffs:
  - Limited pixel revelation (10-15%) balances efficiency with potential loss of complete explanation
  - Four-second intervals between revelations optimize for human reaction time without causing boredom
  - Simplified ProtoPNet explanation (localization only) enables direct comparison but loses concept-level insights

- Failure signatures:
  - Consistently low recognition rates across all methods suggest fundamental issues with image presentation or participant understanding
  - Large variance in pixel counts needed for recognition indicates inconsistent feature importance across images
  - Weak or negative correlation between methods and human maps suggests misalignment between algorithmic and human visual reasoning

- First 3 experiments:
  1. Run a small pilot with 10 participants using only LRP and ClickMe to validate the experimental interface and timing
  2. Test all three XAI methods on a subset of 20 images to verify the revelation sequence generation and correlation calculation
  3. Perform a complete run with all 102 images and 5-10 participants to check data collection pipeline and initial analysis results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the exclusion of negative relevance impact the interpretability and overall performance of XAI methods in various real-world scenarios?
- Basis in paper: [explicit] The paper mentions that one of the main limitations of the experiment is excluding negative relevance, which could significantly influence a model's output.
- Why unresolved: The paper suggests that seeing what speaks against a particular decision alongside what speaks for it may enhance interpretability in various scenarios, but it does not explore this aspect experimentally.
- What evidence would resolve it: Conducting experiments that include both positive and negative relevance in the XAI methods and comparing the results with the current approach would provide evidence on the impact of negative relevance.

### Open Question 2
- Question: Can the artifact issue in LRP's grid-like appearance, caused by skip connections during backpropagation, be resolved or minimized, and how would this affect the method's interpretability?
- Basis in paper: [explicit] The paper notes that when using the LRP on the ResNet-50 model, a grid-like artifact appeared due to the skip connections during backpropagation.
- Why unresolved: The paper acknowledges the issue but does not provide a solution or explore its implications on interpretability.
- What evidence would resolve it: Developing and testing methods to address or minimize the grid-like artifact in LRP's explanations and evaluating their impact on interpretability would provide evidence on this matter.

### Open Question 3
- Question: How would expanding the explanation of ProtoPNet to include its full concept explanation, rather than just localization, compare to similar concept-based methods in terms of interpretability and user understanding?
- Basis in paper: [explicit] The paper mentions that the experiment limited the multilevel explanation of ProtoPNet to only localization to make it comparable to the selected local explanation methods.
- Why unresolved: The paper suggests that future studies should compare ProtoPNet's full concept explanation to similar concept-based methods, but this comparison is not conducted.
- What evidence would resolve it: Conducting a study that compares ProtoPNet's full concept explanation with other concept-based methods and evaluating user understanding and interpretability would provide evidence on this question.

## Limitations
- The study's controlled experimental setup with a specific image classification task may limit generalizability to other domains or more complex models
- Using only 102 images from 17 classes may not capture the full diversity of real-world scenarios and edge cases
- The assumption that participants can recognize images based on partial pixel information may not hold for complex or ambiguous images

## Confidence

- **High Confidence**: The experimental framework for comparing XAI methods is sound, and the results showing similar efficiency between methods and human explanations are reliable within the study's scope.
- **Medium Confidence**: The interpretation of weak correlation between XAI methods and human reasoning is reasonable but may be influenced by the specific dataset and task.
- **Low Confidence**: The generalizability of findings to other domains or more complex models is uncertain due to the study's controlled nature and limited scope.

## Next Checks
1. Replicate the experiment with a larger and more diverse dataset to assess the robustness of the findings across different image classes and complexities.

2. Conduct user studies in different domains (e.g., medical imaging, autonomous driving) to evaluate whether the observed patterns of explanation efficiency and correlation hold in varied contexts.

3. Test alternative correlation measures and explanation methods to determine if the weak correlation is specific to the Spearman coefficient and the chosen XAI techniques or a more general phenomenon.