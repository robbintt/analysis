---
ver: rpa2
title: Graph Neural Network contextual embedding for Deep Learning on Tabular Data
arxiv_id: '2303.06455'
source_url: https://arxiv.org/abs/2303.06455
tags:
- tabular
- embedding
- features
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Interaction Network Contextual Embedding
  (INCE), a novel deep learning model that uses Graph Neural Networks (GNNs) and specifically
  Interaction Networks to improve contextual embedding for tabular data. INCE addresses
  the challenge of applying deep learning to tabular data, which is typically dominated
  by tree-based ensemble models.
---

# Graph Neural Network contextual embedding for Deep Learning on Tabular Data

## Quick Facts
- arXiv ID: 2303.06455
- Source URL: https://arxiv.org/abs/2303.06455
- Authors: 
- Reference count: 40
- Primary result: INCE achieves competitive results with boosted-tree solutions while using fewer trainable parameters

## Executive Summary
This paper introduces Interaction Network Contextual Embedding (INCE), a novel deep learning model that uses Graph Neural Networks (GNNs) and specifically Interaction Networks to improve contextual embedding for tabular data. INCE addresses the challenge of applying deep learning to tabular data, which is typically dominated by tree-based ensemble models. The method projects tabular features into a common latent space, builds a fully-connected graph, and uses a stack of Interaction Networks to model feature interactions and enhance their representation.

## Method Summary
INCE uses an encoder-decoder architecture with columnar embedding (MLP for continuous features, embedding lookup for categorical), Interaction Network-based contextual embedding, and a decoder MLP. The model projects tabular features into a common latent space, creates a fully-connected graph with feature nodes and a CLS virtual node, and applies stacked Interaction Networks to model feature interactions. It's trained for 200 epochs with Adam optimizer (lr=0.001, batch=256).

## Key Results
- INCE outperforms all other deep learning baselines on five public datasets
- Achieves competitive results compared to boosted-tree solutions
- Uses fewer trainable parameters than Transformer-based approaches while achieving similar or better performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interaction Networks capture feature interactions better than traditional deep learning methods because they explicitly model pairwise feature relationships in a fully-connected graph structure.
- Mechanism: The IN creates directed edges between all feature pairs and updates edge representations using the node features they connect. This allows the model to learn interaction strengths between features that would be missed by standard MLPs.
- Core assumption: Feature interactions are sufficiently captured by pairwise relationships in a fully-connected graph.
- Evidence anchors:
  - [abstract] "a stack of INs models the relationship among all the nodes - original features and CLS virtual node - and enhances their representation."
  - [section III] "In the first step, the representation of each edge... is updated using the information of the adjacent nodes... In the second step, all the messages coming from the incoming edges are aggregated and used to update the node representation."
  - [corpus] Weak evidence - no direct corpus support for IN's pairwise modeling effectiveness, but related works (e.g., TabTransformer) use similar interaction-based approaches.
- Break condition: If feature interactions are non-pairwise or higher-order, the fully-connected graph with pairwise edges may be insufficient.

### Mechanism 2
- Claim: Adding a CLS virtual node allows the model to learn a global representation of the tabular row that aggregates information from all features.
- Mechanism: The CLS node is connected to every feature node and receives messages from all of them through the IN layers. Its final representation after message passing serves as the contextual embedding for the entire row.
- Core assumption: A single virtual node can effectively aggregate the information from all features to represent the whole row.
- Evidence anchors:
  - [abstract] "a virtual CLS node is introduced to characterize the global graph state."
  - [section III] "As in the BERT [4], a virtual CLS node connected to each existing node is added to the graph... The final CLS vector embedding produced by the stack of IN is used as global representation of the graph."
  - [corpus] Weak evidence - CLS token is well-established in Transformers but not in GNNs; this is an adaptation.
- Break condition: If the dataset has highly heterogeneous features that need separate global representations, a single CLS node may be insufficient.

### Mechanism 3
- Claim: INs provide permutation invariance for tabular data, which is important because the order of features in a tabular row has no semantic meaning.
- Mechanism: The message-passing scheme in INs uses permutation-invariant aggregation (e.g., summation) when combining messages from neighboring nodes. This ensures that reordering features doesn't change the model's output.
- Core assumption: Permutation invariance is achieved through the aggregation functions used in message passing.
- Evidence anchors:
  - [abstract] "Permutation invariance is a crucial feature distinguishing GNN from the rest of neural networks. The order of nodes in a graph has no relevance, that is how we order the nodes in a graph does not impact the results produced by GNNs."
  - [section II] "The aggregation must be permutation-invariant."
  - [corpus] No direct evidence - this is a general property of GNNs not specific to this paper.
- Break condition: If the aggregation function is not truly permutation-invariant (e.g., uses max instead of sum), the model may be sensitive to feature order.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and Message Passing
  - Why needed here: The paper uses GNNs, specifically Interaction Networks, as the core mechanism for contextual embedding. Understanding how GNNs process graph-structured data is essential to grasp the model architecture.
  - Quick check question: What are the three main steps in the Message-Passing scheme for GNNs?

- Concept: Interaction Networks (INs)
  - Why needed here: INCE specifically uses Interaction Networks, a type of GNN designed for modeling pairwise interactions. Knowing how INs differ from other GNNs helps understand why they're chosen for tabular data.
  - Quick check question: How do Interaction Networks update edge representations differently from standard GNNs?

- Concept: Contextual Embedding
  - Why needed here: The paper's main contribution is improving feature representations by considering feature interactions (contextual embedding). Understanding what contextual embedding means in NLP (e.g., BERT) helps draw parallels to the tabular case.
  - Quick check question: How does contextual embedding in tabular data differ from that in NLP, given that tabular feature order is arbitrary?

## Architecture Onboarding

- Component map: Columnar Embedding -> Graph Construction -> Interaction Network Stack -> Decoder MLP
- Critical path: Tabular row → Columnar Embedding → Homogeneous latent space → Graph → Stack of IN layers → Updated node/edge representations → CLS node → Decoder MLP → Final prediction
- Design tradeoffs:
  - Using a fully-connected graph vs. sparse graph: Fully-connected ensures all feature interactions are modeled but increases computational complexity quadratically with the number of features.
  - Number of IN layers: More layers allow information to propagate further but may lead to over-smoothing or unnecessary complexity for small graphs.
  - IN vs. Transformer: INs have fewer parameters and are permutation-invariant by design, but Transformers may be more flexible with their attention mechanisms.
- Failure signatures:
  - Poor performance on datasets with many features: The fully-connected graph and pairwise edge modeling may become computationally prohibitive.
  - Sensitivity to feature order: If permutation invariance is not properly implemented, reordering features could change predictions.
  - Overfitting on small datasets: The model has many parameters (especially with large latent space or many IN layers) that may overfit limited data.
- First 3 experiments:
  1. Train INCE on a small tabular dataset (e.g., HELOC) with default hyperparameters to verify basic functionality.
  2. Compare INCE's performance with and without the CLS virtual node to confirm its importance for global representation.
  3. Test permutation invariance by randomly shuffling feature order and checking if predictions remain consistent.

## Open Questions the Paper Calls Out

- Question: How does the performance of INCE scale with the number of features in tabular datasets, particularly for datasets with hundreds or thousands of features?
  - Basis in paper: [inferred] The paper discusses that INCE's computational time grows quadratically with the number of features and mentions scalability issues with the number of features as a future research direction.
  - Why unresolved: The paper only tested INCE on datasets with up to 28 features and did not explore its behavior on larger, more complex tabular datasets.
  - What evidence would resolve it: Empirical results comparing INCE's performance on datasets with varying numbers of features, particularly those with hundreds or thousands of features, would clarify its scalability.

- Question: How does INCE's interpretability compare to that of tree-based models like XGBoost or CatBoost in terms of feature importance and decision-making processes?
  - Basis in paper: [explicit] The paper discusses INCE's interpretability through feature-feature interactions and compares it to KernelShap, but does not directly compare it to tree-based models.
  - Why unresolved: The paper focuses on INCE's internal interpretability mechanisms but does not benchmark these against established interpretability methods used for tree-based models.
  - What evidence would resolve it: A comparative study of INCE's interpretability features against those of tree-based models, using standard interpretability metrics and real-world use cases, would provide a clearer picture.

- Question: Can INCE be effectively integrated with other deep learning architectures, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), to handle hybrid datasets containing both tabular and non-tabular data?
  - Basis in paper: [explicit] The paper mentions that standard tree-based approaches have limitations when tabular data is only part of the model input, which also includes data such as images, texts, or audio, suggesting potential for integration.
  - Why unresolved: The paper does not explore any hybrid models or integrations with other deep learning architectures, focusing solely on tabular data.
  - What evidence would resolve it: Experimental results demonstrating the effectiveness of INCE when integrated with other deep learning architectures on hybrid datasets would show its potential in multi-modal learning scenarios.

## Limitations
- Limited empirical validation beyond five datasets with modest feature counts; scalability to high-dimensional tabular data remains unproven
- No ablation studies isolating the contribution of Interaction Networks vs. the CLS virtual node vs. the fully-connected graph structure
- Computational complexity implications of fully-connected graph construction not thoroughly analyzed

## Confidence
- High: INCE's ability to outperform other deep learning baselines on tested datasets
- Medium: Claims about permutation invariance and pairwise interaction modeling being superior to Transformers
- Low: Generalization to datasets with hundreds of features or very large cardinalities

## Next Checks
1. Conduct ablation studies systematically removing the CLS node, varying the graph connectivity pattern (from fully-connected to sparse), and testing different message-passing schemes to isolate each component's contribution
2. Evaluate INCE on datasets with >100 features to assess scalability and computational feasibility of the fully-connected graph approach
3. Test permutation invariance rigorously by measuring prediction stability across multiple random feature orderings on the same input data