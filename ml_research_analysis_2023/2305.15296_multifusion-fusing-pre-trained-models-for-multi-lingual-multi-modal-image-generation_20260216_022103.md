---
ver: rpa2
title: 'MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image
  Generation'
arxiv_id: '2305.15296'
source_url: https://arxiv.org/abs/2305.15296
tags:
- image
- multifusion
- diffusion
- images
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MULTIFUSION introduces a modular, pre-trained encoder architecture
  that enables text-to-image diffusion models to interpret multilingual and arbitrarily
  interleaved multimodal inputs. By integrating an autoregressive language model with
  visual adapters and semantic bias tuning, the approach preserves prior generative
  capabilities while adding support for text-image prompts across five languages.
---

# MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation

## Quick Facts
- **arXiv ID**: 2305.15296
- **Source URL**: https://arxiv.org/abs/2305.15296
- **Reference count**: 40
- **Key outcome**: Introduces a modular encoder architecture that enables multilingual and multimodal prompting for text-to-image diffusion models without retraining the generative backbone.

## Executive Summary
MULTIFUSION enables text-to-image diffusion models to process multilingual and interleaved multimodal inputs by integrating a pre-trained autoregressive language model with visual adapters and semantic bias tuning. The approach preserves prior generative capabilities while adding support for text-image prompts across five languages. Experiments show multimodal prompts improve image fidelity and compositional robustness compared to text-only baselines, with CLIPScore and FID metrics matching or exceeding the original model. Human evaluation on the MCC-250 benchmark demonstrates significantly better object-attribute composition for interleaved visual-text prompts. The work highlights how modular pre-training and alignment can transfer language and multimodal capabilities to downstream generative models without extensive retraining.

## Method Summary
MULTIFUSION replaces the CLIP text encoder in a pre-trained diffusion model with a pre-trained autoregressive LM augmented with multimodal adapters. An image prefix and adapters are added to handle image tokens, trained on multimodal datasets (LAION-5B, CC3M, CoCo Captions, VQA). Semantic bias weights are tuned on multilingual NLI data for alignment, then the diffusion model's cross-attention layers are aligned to the new encoder while keeping the U-Net frozen. Attention manipulation is used to balance text and image token influence during inference.

## Key Results
- Multimodal prompts improve compositional robustness, almost doubling correct object-attribute pairs compared to text-only baselines
- Multilingual alignment transfers from encoder to diffusion model without requiring multilingual training data for the generative model
- CLIPScore and FID metrics match or exceed original Stable Diffusion performance
- Human evaluation on MCC-250 shows significantly better object-attribute composition for interleaved visual-text prompts

## Why This Works (Mechanism)

### Mechanism 1
Multilingual capabilities transfer from encoder to diffusion model without multilingual training data because strong multilingual embedding alignment in the encoder induces similar alignment in downstream image generation. Core assumption: Encoder embeddings preserve multilingual semantic relationships even when diffusion model is trained only on English.

### Mechanism 2
Multimodal prompts improve compositional robustness compared to text-only prompts because visual references provide unambiguous object representations, reducing attribute leakage and missing objects in composition. Core assumption: Images convey object identity more precisely than text descriptions alone.

### Mechanism 3
Attention manipulation balances influence between text and image tokens for stable multimodal prompting by up-weighting text tokens when image tokens outnumber them, preventing image dominance in generation. Core assumption: Token count discrepancy between modalities affects relative influence in attention layers.

## Foundational Learning

- **Diffusion models iteratively denoise Gaussian noise to generate images**: Understanding the core generation process is essential for modifying conditioning mechanisms. Quick check: What role does the noise prediction play in diffusion model sampling?
- **Cross-attention layers enable text-to-image conditioning in diffusion models**: The cross-attention layers are the primary interface between encoder embeddings and the diffusion model. Quick check: Which layers are modified when integrating a new encoder with a frozen diffusion model?
- **Contrastive learning for semantic alignment creates better embedding spaces for retrieval tasks**: The semantic bias tuning creates multilingual alignment that transfers to downstream tasks. Quick check: What type of training pairs are used to achieve multilingual semantic alignment?

## Architecture Onboarding

- **Component map**: Pre-trained autoregressive transformer (frozen) → Multimodal adapters (tuned) → Semantic bias tuner (tuned) → Stable Diffusion U-Net (frozen except cross-attention) → Attention manipulation layer
- **Critical path**: Encoder → Cross-attention layers → Diffusion denoising steps
- **Design tradeoffs**: Parameter efficiency vs. full fine-tuning capability; Multilingual alignment quality vs. training data diversity; Modality balance vs. user control
- **Failure signatures**: Poor multilingual alignment → degraded multilingual generation; Insufficient multimodal training → broken multimodal prompts; Incorrect attention manipulation weights → modality dominance issues
- **First 3 experiments**:
  1. Test multilingual embedding alignment with translated prompts
  2. Validate multimodal prompt effectiveness with simple object composition
  3. Measure attention manipulation impact on modality balance in generated outputs

## Open Questions the Paper Calls Out

### Open Question 1
Does the attention manipulation technique generalize to other types of multimodal inputs beyond interleaved text-image sequences, such as audio or video? Basis: Authors demonstrate attention manipulation for text-image inputs but do not explore other modalities. What evidence would resolve it: Experiments testing attention manipulation with audio, video, or other modalities.

### Open Question 2
How does the performance of MultiFusion change when using different pre-trained language models (e.g., larger or multilingual-specific LMs) as the base encoder? Basis: Authors use a 13B transformer-based LM but do not compare it to other models. What evidence would resolve it: Comparative studies using different LMs to assess performance differences.

### Open Question 3
Can the multilingual alignment achieved in MultiFusion be extended to languages not included in the initial training data? Basis: Authors demonstrate multilingual alignment for five languages but do not test on others. What evidence would resolve it: Testing the model on a broader set of languages.

### Open Question 4
How does the model's performance scale with the size and diversity of the training dataset, particularly for multimodal and multilingual tasks? Basis: Authors use a large dataset but do not explore the impact of dataset size or diversity on performance. What evidence would resolve it: Experiments varying dataset size and diversity.

### Open Question 5
What are the long-term societal impacts of deploying MultiFusion in real-world applications, particularly regarding bias and fairness? Basis: Authors acknowledge potential societal impacts but do not conduct a detailed analysis. What evidence would resolve it: Longitudinal studies and bias audits in real-world deployments.

## Limitations

- Evaluation relies heavily on automated metrics (FID-30k, CLIPScore) and a single human benchmark (MCC-250), leaving gaps in assessing real-world usability across diverse multilingual contexts
- Method assumes semantic alignment in encoder fully transfers to diffusion model without further fine-tuning, but transfer degree is not exhaustively tested
- Multimodal prompt benefits are demonstrated primarily with simple object-attribute compositions; approach's robustness to complex, nuanced, or culturally specific imagery remains unverified

## Confidence

- **High Confidence**: Encoder-based multilingual alignment enables generation in multiple languages without retraining diffusion model; attention manipulation effectively balances modality influence
- **Medium Confidence**: Multimodal prompts consistently improve compositional robustness (object-attribute accuracy) compared to text-only baselines; visual references reduce missing objects and attribute leakage
- **Low Confidence**: Method's effectiveness on culturally diverse, complex scene generation; generalizability of reported improvements across diverse linguistic families; long-term stability of multilingual and multimodal performance

## Next Checks

1. **Linguistic Diversity Test**: Generate images from prompts in linguistically diverse, low-resource languages and evaluate semantic fidelity using human raters from native speaker communities
2. **Complex Scene Composition**: Design prompts with nested attributes, abstract concepts, and compound scenes to test compositional robustness beyond simple object-attribute pairs
3. **Modality Interference Analysis**: Systematically vary the ratio of text to image tokens in prompts and measure generation quality to quantify the robustness and limits of the attention manipulation strategy