---
ver: rpa2
title: 'ML-Bench: Evaluating Large Language Models and Agents for Machine Learning
  Tasks on Repository-Level Code'
arxiv_id: '2311.09835'
source_url: https://arxiv.org/abs/2311.09835
tags:
- code
- readme
- arxiv
- github
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ML-Bench addresses the gap between LLM code generation benchmarks
  and real-world usage by focusing on library-based machine learning tasks. The benchmark
  uses GitHub repositories and README files to provide instructions and code examples.
---

# ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code

## Quick Facts
- arXiv ID: 2311.09835
- Source URL: https://arxiv.org/abs/2311.09835
- Reference count: 9
- Primary result: GPT-4o achieves Pass@5 rate over 50% on ML-Bench; ML-Agent improves to 76.47% success rate

## Executive Summary
ML-Bench introduces a new benchmark for evaluating large language models on real-world machine learning tasks using GitHub repositories and README documentation. The benchmark addresses the gap between synthetic code generation tasks and practical usage by requiring models to navigate codebase documentation and generate executable code for library-based ML tasks. A novel agent-based approach, ML-Agent, is proposed that uses iterative retrieval and feedback to improve performance over single-shot generation methods. Results show GPT-4o achieves the highest baseline performance with Pass@5 rates exceeding 50%, while ML-Agent further improves success rates to 76.47%, demonstrating the value of structured navigation and iterative refinement.

## Method Summary
ML-Bench evaluates LLMs using real GitHub repositories where models must read README files, navigate codebases, and generate executable code for machine learning tasks. The benchmark provides 9,641 samples across 18 repositories covering various ML domains. Models are evaluated using Pass@K (probability of correct execution) and Parameter Hit Precision metrics. The ML-Agent approach implements a four-step iterative process: keyword extraction and repository retrieval, repository ranking, README reading and planning, and code generation with validation. CodeLlama is fine-tuned on the dataset to enhance performance in these specific use cases.

## Key Results
- GPT-4o achieves Pass@5 rate over 50% on ML-Bench, outperforming GPT-3.5, Claude 2, and CodeLlama baselines
- ML-Agent, built on GPT-4o, achieves 76.47% success rate, demonstrating iterative feedback improves complex task performance
- Hallucination errors remain the largest failure category, followed by incorrect code generation and parameter mismatches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The task formulation demands long-context comprehension of README documentation plus cross-file code navigation, creating a gap that single-shot generation cannot fill.
- Mechanism: ML-Bench tasks require the model to synthesize natural language instructions with detailed README examples, then generate code that invokes existing library functions with precise parameters. This introduces multi-step reasoning that benefits from iterative retrieval and feedback.
- Core assumption: The README contains sufficient, unambiguous examples and parameter guidance to allow correct code synthesis without external knowledge.
- Evidence anchors:
  - [abstract] "The benchmark uses GitHub repositories and README files to provide instructions and code examples."
  - [section] "In our benchmark, we focus on a scenario where, given a GitHub repository F, the language model has access to all files f ∈ F within the repository."
  - [corpus] Weak - no corpus papers directly discuss README-driven long-context synthesis.
- Break condition: If READMEs are fragmented, ambiguous, or parameter values conflict, the agent will fail to produce correct code.

### Mechanism 2
- Claim: ML-Agent's iterative retrieval and evaluation loop outperforms single-shot generation by aligning repository selection, README parsing, and code synthesis with human-like task navigation.
- Mechanism: The agent retrieves top repositories, ranks them, reads relevant READMEs, then iteratively decides whether to generate code, read more files, or switch repositories. This process mirrors human trial-and-error and avoids relying on memorization.
- Core assumption: The agent's keyword extraction and BM25 retrieval can accurately surface the most relevant repositories and documentation segments.
- Evidence anchors:
  - [abstract] "A new agent-based approach, ML-Agent, is proposed to better navigate codebases and generate executable code."
  - [section] "ML-A GENT follows a four-step process upon receiving human instructions: Step1: Extracting keywords from the instructions and conducting searches on GitHub to identify the five most relevant repositories."
  - [corpus] Weak - related work focuses on general agent benchmarks, not README-specific navigation.
- Break condition: If the retrieval step consistently fails to return relevant repositories, the agent's iterative loop offers no advantage.

### Mechanism 3
- Claim: The "hallucination" problem in closed-source LLMs is mitigated in ML-Agent by grounding decisions in actual README content and local file structure rather than model-generated knowledge.
- Mechanism: By forcing the agent to base each decision on explicit README text and repository structure, it reduces the temptation to invent code snippets or parameters that are not supported by the documentation.
- Core assumption: READMEs are complete and accurate enough that code synthesis can be fully grounded in them.
- Evidence anchors:
  - [abstract] "Hallucination and bash script generation issues persist."
  - [section] "Hallucination errors constitute the largest portion in both models, followed by instances where models generate incorrect code."
  - [corpus] Weak - no direct corpus evidence on hallucination mitigation via grounding.
- Break condition: If READMEs contain gaps or implicit conventions, the agent will still hallucinate to fill missing information.

## Foundational Learning

- Concept: Long-context processing and cross-file navigation
  - Why needed here: Tasks require reading multi-file repositories and understanding dependencies between README sections and code.
  - Quick check question: How would you retrieve a README snippet that describes a parameter used in a Python script located in a subdirectory?

- Concept: Iterative retrieval and planning
  - Why needed here: The agent must decide dynamically whether to generate code, read more documentation, or switch repositories.
  - Quick check question: What criteria should the agent use to rank candidate repositories before selecting one?

- Concept: Parameter alignment and syntax validation
  - Why needed here: Generated code must match user-specified parameters and be syntactically valid for execution.
  - Quick check question: How would you validate that a generated Bash script uses the correct argument syntax for a given library?

## Architecture Onboarding

- Component map: Repository Retriever -> Repository Ranker -> README Reader -> Decision Engine -> Code Generator -> Execution Validator
- Critical path: Retrieval → Ranking → README reading → Decision → Code generation → Validation
- Design tradeoffs:
  - Retrieval breadth vs. precision: wider search risks irrelevant results; narrow search risks missing the right repo.
  - Parameter extraction vs. hallucination: aggressive extraction risks fabricating parameters; conservative extraction risks missing user intent.
- Failure signatures:
  - Retrieval failures: agent loops on irrelevant repositories.
  - Parameter mismatch: generated code passes syntax but fails PHP metric.
  - Hallucination spikes: agent invents code outside README scope.
- First 3 experiments:
  1. Validate retrieval accuracy on a small set of labeled tasks.
  2. Measure PHP improvement when grounding code generation in README examples.
  3. Stress-test the decision loop with ambiguous or incomplete READMEs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ML-Bench compare to other existing code generation benchmarks?
- Basis in paper: [explicit] The paper mentions that ML-Bench addresses the gap between LLM code generation benchmarks and real-world usage, but does not provide a direct comparison to other benchmarks.
- Why unresolved: The paper focuses on introducing ML-Bench and its evaluation results, without providing a comprehensive comparison to other benchmarks.
- What evidence would resolve it: A comparative study of ML-Bench with other popular code generation benchmarks, such as HumanEval or MBPP, would provide insights into its relative performance and effectiveness.

### Open Question 2
- Question: How does the fine-tuning process of CodeLlama on ML-Bench affect its performance in other code generation tasks?
- Basis in paper: [explicit] The paper mentions fine-tuning CodeLlama on ML-Bench to enhance its performance in the specific use cases, but does not discuss its impact on other tasks.
- Why unresolved: The paper does not provide information on the generalizability of the fine-tuned CodeLlama model to other code generation tasks.
- What evidence would resolve it: Evaluating the fine-tuned CodeLlama model on other code generation benchmarks or tasks would determine its transferability and effectiveness beyond ML-Bench.

### Open Question 3
- Question: How does the performance of ML-Agent vary across different types of machine learning tasks (e.g., text, image, GNN)?
- Basis in paper: [explicit] The paper mentions that ML-Bench covers various machine learning domains, including text, image, GNN, and others, but does not provide a detailed analysis of ML-Agent's performance across these domains.
- Why unresolved: The paper focuses on the overall performance of ML-Agent without breaking down the results by task type.
- What evidence would resolve it: A comprehensive analysis of ML-Agent's performance on different task types within ML-Bench would reveal its strengths and weaknesses across various domains of machine learning.

## Limitations

- The benchmark relies heavily on the completeness and accuracy of README documentation, which may contain gaps or ambiguities that impact model performance
- The evaluation framework focuses specifically on library-based ML tasks, limiting generalizability to other programming domains
- Hallucination issues persist even with the ML-Agent approach, indicating that grounding alone cannot fully prevent unsupported code generation

## Confidence

**High confidence**: The core finding that repository-level benchmarks with iterative feedback improve performance over single-shot generation is well-supported by comparative results showing ML-Agent's 76.47% success rate versus GPT-4o's 50%+ and other baselines.

**Medium confidence**: The claim about hallucination reduction through grounding is supported but needs further validation, as the paper notes hallucination remains the largest error category.

**Low confidence**: The assertion that README documentation is "sufficient" for correct code synthesis is not empirically validated, as the paper assumes README completeness without systematic analysis of documentation quality.

## Next Checks

1. **Documentation Quality Audit**: Systematically evaluate the completeness and accuracy of README files across the benchmark repositories to quantify how documentation gaps correlate with model errors, particularly hallucinations.

2. **Cross-Domain Generalization Test**: Apply the ML-Agent framework to non-ML domains (e.g., web development or data engineering) to assess whether the iterative retrieval approach generalizes beyond the current library-based ML focus.

3. **Ablation Study on Retrieval Components**: Conduct controlled experiments isolating the impact of BM25 retrieval precision versus the iterative feedback loop on overall performance, to determine which mechanism drives the largest gains.