---
ver: rpa2
title: 'Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit
  Tests'
arxiv_id: '2310.17867'
source_url: https://arxiv.org/abs/2310.17867
tags:
- test
- learning
- positive
- tests
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reproducibility issues in Multiple Instance
  Learning (MIL) models, where many popular deep learning approaches fail to respect
  fundamental MIL assumptions. The authors propose "algorithmic unit tests" - synthetic
  datasets designed to test whether MIL models respect the presence-based assumption
  (a positive bag must contain at least one positive instance) and threshold-based
  assumptions (specific numbers of concept classes must be present).
---

# Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests

## Quick Facts
- arXiv ID: 2310.17867
- Source URL: https://arxiv.org/abs/2310.17867
- Reference count: 40
- Primary result: Deep MIL models violate fundamental MIL assumptions by learning anti-correlated patterns and frequency shortcuts

## Executive Summary
This paper reveals a critical reproducibility issue in Multiple Instance Learning where many popular deep learning approaches fail to respect fundamental MIL assumptions. Through "algorithmic unit tests" - synthetic datasets designed to test presence-based and threshold-based assumptions - the authors demonstrate that all deep MIL models except CausalMIL learn invalid relationships like "absence of instances indicates positive labels." The work provides practical guidance for testing MIL models and highlights the need for careful validation of algorithmic assumptions in MIL research.

## Method Summary
The authors create three synthetic algorithmic unit tests to evaluate whether MIL models respect fundamental assumptions. The tests use controlled distributions where training data contains "poison" features that create easy-to-learn but invalid signals, then test with distributions where these signals are inverted. Five deep MIL models (MI-Net, MIL-Pooling, TransMIL, GCN-MIL, Hopfield MIL) plus two SVM-based methods are evaluated on these tests. Models fail if they achieve high training AUC (>0.5) but low testing AUC (<0.5), indicating they learned invalid patterns rather than true MIL relationships.

## Key Results
- All deep MIL models except CausalMIL fail at least one algorithmic unit test
- Models learn to associate negative labels with absence of specific "poison" features present only in negative bags during training
- Threshold MIL models learn frequency-based shortcuts instead of proper counting mechanisms
- CausalMIL, designed specifically to respect MIL assumptions, passes all tests
- The tests reveal that many recent MIL improvements may be overfitting to training data by learning non-MIL patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep MIL models can learn anti-correlated patterns that violate fundamental MIL assumptions
- Mechanism: Models learn to associate negative labels with the absence of specific "poison" features that only appear in negative bags during training, then fail when this pattern is reversed at test time
- Core assumption: MIL models must respect the presence-based assumption where positive labels require at least one positive instance
- Evidence anchors:
  - [abstract] "They are able to learn anti-correlated instances, i.e., defaulting to 'positive' labels until seeing a negative counter-example, which should not be possible for a correct MIL model."
  - [section 3.1] "This creates an easier-to-learn signal, where ~h(N(−10,Id·0.1)) := ∅ and the remaining spaces ~h(N(0,Id·1)) = ~h(N(0,Id·3)) = ~h(N(1,Id·1)) := c1"
  - [corpus] Weak evidence - related papers focus on implementation details rather than assumption violations
- Break condition: When training and testing distributions differ in ways that expose learned anti-correlations

### Mechanism 2
- Claim: Threshold MIL models can learn frequency-based shortcuts instead of proper counting mechanisms
- Mechanism: Models detect the number of instances rather than recognizing distinct concept classes, then fail when instance frequencies change between training and testing
- Core assumption: Threshold MIL requires counting specific concept classes, not just detecting total instance counts
- Evidence anchors:
  - [section 3.2.2] "it is possible for a model that is not well aligned with the MIL model to learn a degenerate solution ~h that maps ~h(N(2,Id·0.1)) := c1 and ~h(N(−2,Id·0.1)) := c1"
  - [section 4.2] "the mi-SVM and MI-SVM learn to over-focus on the magnitude of coordinate features to indicate a positive direction, which inverts at test time"
  - [corpus] Weak evidence - no related work directly addresses frequency shortcut learning
- Break condition: When training frequency patterns don't match testing patterns

### Mechanism 3
- Claim: Algorithmic unit tests can expose model assumption violations through synthetic data distributions
- Mechanism: Create training distributions with easy-to-learn but invalid signals, then test with distributions where these signals are inverted but true MIL signals remain
- Core assumption: Valid MIL models should be invariant to certain distribution shifts that don't affect the underlying MIL relationship
- Evidence anchors:
  - [abstract] "We identify and demonstrate this problem via a proposed 'algorithmic unit test', where we create synthetic datasets that can be solved by a MIL respecting model"
  - [section 3] "if an algorithm passes the generalized Weidmann MIL tests (specified below), but fails the basic Dietterich test, it means the model fails all possible MIL models"
  - [corpus] Moderate evidence - torchmil library mentioned but no specific focus on unit testing methodology
- Break condition: When model performance drops significantly on test distribution despite training success

## Foundational Learning

- Concept: Multiple Instance Learning (MIL) problem formulation
  - Why needed here: Understanding that MIL bags have labels based on instance presence/absence is fundamental to recognizing assumption violations
  - Quick check question: In MIL, what determines if a bag gets a positive label?

- Concept: Causal assumptions in MIL
  - Why needed here: The paper emphasizes that MIL has implicit causal assumptions about presence-based relationships that must be preserved
  - Quick check question: Why can't you swap positive and negative labels in MIL without changing the problem semantics?

- Concept: Synthetic data testing methodology
  - Why needed here: The unit tests use carefully constructed synthetic datasets to probe model behavior under controlled conditions
  - Quick check question: What is the purpose of creating training distributions with "poison" features?

## Architecture Onboarding

- Component map: Synthetic dataset generator → MIL model implementation → Evaluation metrics (AUC, accuracy) → Failure analysis pipeline
- Critical path: Data generation → Model training → Distribution shift testing → Performance comparison → Assumption violation identification
- Design tradeoffs: Easy-to-learn poison features vs. true MIL signals; synthetic simplicity vs. realistic complexity
- Failure signatures: Training AUC > 0.5 with testing AUC < 0.5 indicates learned anti-correlated patterns
- First 3 experiments:
  1. Run the standard presence test (Algorithm 1) on a new MIL model and observe if training AUC > 0.5 but testing AUC < 0.5
  2. Test threshold learning with Algorithm 2 to check if the model learns to count distinct concepts vs. frequency
  3. Evaluate distribution shift robustness by modifying test data while preserving true MIL relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What additional algorithmic unit tests could be developed to detect violations of more complex MIL assumptions beyond the standard and threshold MIL formulations tested in this paper?
- Basis in paper: [explicit] The paper mentions "more complex MIL problems" and states "Our work has demonstrated that many articles have not properly vetted the more basic MIL setting, and so we suspect other more complex MIL problems are equally at risk."
- Why unresolved: The authors only provide three specific unit tests for the most basic MIL formulations. They acknowledge that other MIL problems exist but do not explore what specific tests would detect violations of those assumptions.
- What evidence would resolve it: Development and validation of additional synthetic datasets that test for violations of generalized MIL assumptions like "Maximum" and "Noiseless-OR" MIL formulations mentioned in the Foulds and Frank survey.

### Open Question 2
- Question: How can algorithmic unit tests be designed to distinguish between valid MIL models that happen to pass the tests by coincidence versus those that genuinely respect the underlying MIL assumptions?
- Basis in paper: [explicit] The authors state "Algorithmic unit tests are not certificates of correctness" and note that the SIL model (designed to violate MIL assumptions) passes their standard test, while MissSVM (designed to respect MIL) only marginally passes.
- Why unresolved: The paper demonstrates that passing unit tests doesn't guarantee correctness but doesn't provide methodology for distinguishing between true and false positives in test results.
- What evidence would resolve it: Development of a framework for analyzing model behavior on unit tests that can identify when passing is due to coincidental pattern matching versus genuine adherence to MIL constraints.

### Open Question 3
- Question: What architectural modifications to popular deep MIL models (MI-Net, MIL-Pooling, TransMIL, etc.) would enable them to pass the algorithmic unit tests while maintaining their current accuracy advantages on real-world MIL problems?
- Basis in paper: [explicit] The paper shows all deep MIL models except CausalMIL fail at least one test, suggesting their improvements may be learning invalid patterns rather than true MIL relationships.
- Why unresolved: The authors identify the problem but don't propose solutions for modifying existing architectures to respect MIL assumptions while preserving performance.
- What evidence would resolve it: Experimental results showing modified versions of deep MIL architectures that pass all three unit tests while achieving comparable or improved performance on standard MIL benchmarks like MUSK1, MUSK2, and medical imaging datasets.

## Limitations

- The synthetic nature of unit tests may not capture all real-world distribution shifts that could expose model violations
- The paper focuses on basic MIL formulations and doesn't address more complex MIL variants like maximum or noiseless-OR MIL
- Passing unit tests doesn't guarantee correctness, as demonstrated by SIL passing the standard test despite being designed to violate MIL assumptions

## Confidence

- High confidence: The mechanism by which models learn anti-correlated patterns (Mechanism 1) is well-demonstrated with clear mathematical formulation and experimental validation
- Medium confidence: The threshold-based violation (Mechanism 2) shows consistent results across models but relies on specific synthetic distributions that may not reflect all real-world scenarios
- Medium confidence: The algorithmic unit test methodology (Mechanism 3) provides a valuable framework, but the generality of synthetic tests to complex real datasets requires further validation

## Next Checks

1. Apply the algorithmic unit tests to a benchmark MIL dataset (e.g., MUSK1/MUSK2) and analyze whether models that pass synthetic tests also perform robustly on real data
2. Test whether retraining deep MIL models with explicit regularization against the identified failure modes (anti-correlation, frequency shortcuts) improves real-world performance
3. Evaluate whether the proposed CausalMIL model maintains its assumption-respecting behavior across diverse data distributions and varying noise levels