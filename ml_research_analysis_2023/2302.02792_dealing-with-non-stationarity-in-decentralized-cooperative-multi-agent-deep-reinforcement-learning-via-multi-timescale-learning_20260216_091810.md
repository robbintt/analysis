---
ver: rpa2
title: Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep
  Reinforcement Learning via Multi-Timescale Learning
arxiv_id: '2302.02792'
source_url: https://arxiv.org/abs/2302.02792
tags:
- learning
- agents
- multi-timescale
- mtql
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-timescale learning for decentralized cooperative MARL addresses
  non-stationarity by having agents learn at different rates. In multi-timescale learning,
  agents update their policies concurrently but at different learning rates, speeding
  up sequential learning while minimizing non-stationarity caused by concurrent updates.
---

# Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning

## Quick Facts
- arXiv ID: 2302.02792
- Source URL: https://arxiv.org/abs/2302.02792
- Reference count: 40
- Multi-timescale learning outperforms state-of-the-art decentralized learning methods on challenging multi-agent cooperative tasks, with performance gains of up to 33.76% on MMM2 and 36.55% on Foraging-15x15-4p-3f compared to independent learning.

## Executive Summary
This paper addresses the non-stationarity problem in decentralized cooperative multi-agent reinforcement learning (MARL) by introducing a multi-timescale learning approach. The method allows agents to update their policies concurrently but at different learning rates, with one agent learning at a faster rate while others learn more slowly. This approach speeds up learning compared to sequential methods while minimizing the non-stationarity caused by concurrent updates, leading to improved performance across 12 benchmark environments including StarCraft Multi-Agent Challenge, Multi-Agent Particle Environment, and Level-Based Foraging tasks.

## Method Summary
The multi-timescale learning framework assigns different learning rates to agents, with one designated as the "fast" learner updating at rate ηfast while others update at a slower rate ηslow << ηfast. The fast learner role rotates periodically based on a switching period s. This creates a quasi-stationary environment for slow learners since the fast learner's policy appears approximately fixed during their update steps. The approach generalizes independent learning (ηslow = ηfast), sequential learning (ηslow = 0), and two-timescale stochastic approximation (s = ∞). Two implementations are presented: Multi-timescale PPO (MTPPO) and Multi-timescale Q-learning (MTQL), both compared against IPPO and IQL baselines.

## Key Results
- Multi-timescale learning outperforms state-of-the-art decentralized learning methods on challenging multi-agent cooperative tasks
- Performance gains of up to 33.76% on MMM2 and 36.55% on Foraging-15x15-4p-3f compared to independent learning
- The method generalizes both independent and sequential learning as special cases, providing a unified framework for different levels of non-stationarity handling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-timescale learning reduces non-stationarity by ensuring that the faster-learning agent's policy changes dominate the slower agent's experience, making the environment appear more stationary to the slower agent.
- Mechanism: When agent i updates its policy at a faster rate ηfast and other agents update at ηslow << ηfast, the slower agents perceive agent i's policy as approximately fixed during their own updates. This creates a quasi-stationary environment for the slower agents, reducing the non-stationarity they experience.
- Core assumption: The learning rate difference between fast and slow agents is large enough that slower agents can effectively treat the faster agent's policy as constant during their update steps.
- Evidence anchors:
  - [abstract] "when one agent updates its policy, other agents are allowed to update their policies as well, but at a slower rate. This speeds up sequential learning, while also minimizing non-stationarity caused by other agents updating concurrently."
  - [section 3] "Our key insight is that we can speed up overall learning while still minimizing the perceived non-stationarity by allowing the non-active agents to update their policy parameters as well, but at a slower timescale."

### Mechanism 2
- Claim: Multi-timescale learning accelerates learning compared to sequential learning by allowing all agents to learn simultaneously rather than in strict alternation.
- Mechanism: In sequential learning, only one agent updates while others remain frozen, wasting computational resources. Multi-timescale learning keeps all agents updating but at different rates, utilizing parallel computation while maintaining the convergence benefits of sequential updates.
- Core assumption: Parallel computation resources are available and the overhead of managing different learning rates is less than the speedup gained from parallel updates.
- Evidence anchors:
  - [abstract] "This speeds up sequential learning, while also minimizing non-stationarity caused by other agents updating concurrently."
  - [section 2] "Although sequential learning can completely side-step the challenge of non-stationarity introduced by other agents learning concurrently, it slows down the learning process because only one agent is learning at any time."

### Mechanism 3
- Claim: Multi-timescale learning generalizes both independent and sequential learning as special cases, providing a unified framework for different levels of non-stationarity handling.
- Mechanism: By varying the learning rate ratio (ηslow/ηfast) and switching period s, multi-timescale learning can interpolate between independent learning (ηslow = ηfast), sequential learning (ηslow = 0), and two-timescale stochastic approximation (s = ∞). This flexibility allows adapting to different task requirements.
- Core assumption: The environment and task structure allow for meaningful parameterization of learning rate differences and switching strategies.
- Evidence anchors:
  - [section 3] "If we set ηslow = ηfast, then multi-timescale learning reduces to independent learning. Similarly, if we set s = ∞, multi-timescale learning reduces to a standard two-timescale stochastic approximation... Furthermore, if we set ηslow = 0 multi-timescale learning reduces to sequential learning."
  - [section 4.2] Performance gains across diverse tasks suggest the framework's adaptability.

## Foundational Learning

- Concept: Stationary vs non-stationary environments in reinforcement learning
  - Why needed here: Understanding the core problem that multi-timescale learning addresses - the environment becomes non-stationary when other agents are learning simultaneously.
  - Quick check question: What makes a multi-agent environment non-stationary from a single agent's perspective, and how does this differ from single-agent RL?

- Concept: Best Response dynamics and iterative learning schemes
  - Why needed here: The paper builds on concepts from game theory (IIBR and SIBR) and shows how multi-timescale learning relates to these established methods.
  - Quick check question: How do IIBR and SIBR differ in their approach to handling concurrent agent updates, and what are the convergence implications of each?

- Concept: Two-timescale stochastic approximation
  - Why needed here: Multi-timescale learning draws inspiration from this established technique, adapting it for the multi-agent setting where timescales are assigned to different agents rather than different parameters of a single agent.
  - Quick check question: In traditional two-timescale stochastic approximation, what is the typical relationship between the two learning rates, and why does this help with convergence?

## Architecture Onboarding

- Component map:
  - Agent actors with policy networks
  - Agent critics with value networks (for PPO version)
  - Learning rate scheduler with two timescales (fast and slow)
  - Switching period controller that rotates the fast-learning agent
  - Environment interaction module
  - Experience replay buffer (for Q-learning version)

- Critical path: Agent selection → Fast agent update → Slow agent updates → Environment step → Experience collection → Policy/value update

- Design tradeoffs:
  - Learning rate ratio vs. convergence speed: Higher ratio reduces non-stationarity but may cause instability
  - Switching period vs. coordination: Frequent switching improves fairness but may increase variance
  - Number of timescales vs. complexity: More than two timescales could better capture agent dependencies but increases implementation complexity

- Failure signatures:
  - If ηslow ≈ ηfast: Performance similar to independent learning with no non-stationarity benefits
  - If switching period too short: High variance in learning due to frequent policy changes
  - If learning rate ratio too large: Faster agent may destabilize slower agent's learning
  - If ηslow = 0 and switching period poorly chosen: Performance similar to sequential learning with slow convergence

- First 3 experiments:
  1. Implement basic independent learning baseline with single learning rate for all agents
  2. Add multi-timescale learning with fixed learning rate ratio (e.g., 10:1) and moderate switching period (e.g., 100 steps)
  3. Perform ablation study by varying the learning rate ratio and switching period to find optimal configuration for a simple environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does multi-timescale learning consistently outperform independent learning across all cooperative MARL tasks and environments?
- Basis in paper: [explicit] The paper shows performance gains of multi-timescale learning over independent learning in most tasks, but acknowledges that independent learning can sometimes perform comparably or even better in certain environments.
- Why unresolved: The paper only evaluated a limited set of tasks and environments. The performance of multi-timescale learning may vary depending on the specific characteristics of the task, such as the level of non-stationarity, the number of agents, and the complexity of the environment.
- What evidence would resolve it: A comprehensive evaluation of multi-timescale learning across a wider range of cooperative MARL tasks and environments, including those with varying levels of non-stationarity, different numbers of agents, and different types of coordination challenges.

### Open Question 2
- Question: What is the optimal way to determine the learning rates and switching periods for multi-timescale learning?
- Basis in paper: [explicit] The paper discusses the impact of different learning rates and switching periods on the performance of multi-timescale learning, but does not provide a definitive answer on how to choose these hyperparameters.
- Why unresolved: The optimal choice of learning rates and switching periods likely depends on the specific task and environment, and may require a more systematic approach to hyperparameter tuning or adaptive methods to adjust these parameters during training.
- What evidence would resolve it: A study comparing different methods for determining the learning rates and switching periods, such as grid search, random search, Bayesian optimization, or adaptive methods, and evaluating their impact on the performance of multi-timescale learning across a range of tasks.

### Open Question 3
- Question: How does multi-timescale learning perform in non-cooperative or mixed cooperative-competitive MARL settings?
- Basis in paper: [inferred] The paper focuses on cooperative MARL, but the concept of multi-timescale learning could potentially be extended to non-cooperative or mixed settings.
- Why unresolved: The paper does not explore the application of multi-timescale learning to non-cooperative or mixed settings, and it is unclear how the approach would need to be modified to handle competitive or mixed objectives.
- What evidence would resolve it: An extension of the multi-timescale learning framework to non-cooperative or mixed cooperative-competitive MARL settings, along with an evaluation of its performance in these environments.

## Limitations

- The theoretical guarantees are limited to two-agent cases, and the paper acknowledges that the multi-timescale approach requires additional hyperparameter tuning (learning rate ratio and switching period) which could make it less practical for some applications.
- Performance gains are highly dependent on task-specific characteristics - while the method excels in scenarios with clear coordination requirements, it may offer diminishing returns in tasks where agents have minimal interaction.
- The method introduces additional complexity through the switching mechanism and multiple learning rates, which may increase implementation difficulty and computational overhead compared to simpler baselines.

## Confidence

- **High**: The core mechanism of reducing non-stationarity through differential learning rates is well-established in the broader RL literature
- **Medium**: The empirical performance improvements are demonstrated but rely on extensive hyperparameter tuning
- **Medium**: The theoretical connections to two-timescale stochastic approximation are sound but don't fully extend to the multi-agent setting

## Next Checks

1. **Ablation study on learning rate ratios**: Systematically vary the fast-to-slow learning rate ratio (e.g., 2:1, 5:1, 10:1, 20:1) across all benchmark environments to identify the optimal range and determine if the 10:1 ratio used in the paper is universally optimal.

2. **Switching period sensitivity analysis**: Test the impact of different switching periods (e.g., 10, 50, 100, 500, 1000 steps) on learning stability and final performance to find the sweet spot between exploration and exploitation.

3. **Scalability evaluation**: Test multi-timescale learning with increasing numbers of agents (4, 8, 16, 32) in the same environments to assess whether the method scales effectively or if non-stationarity becomes too severe as agent count increases.