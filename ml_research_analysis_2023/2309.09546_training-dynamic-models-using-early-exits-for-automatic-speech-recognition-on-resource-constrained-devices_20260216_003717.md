---
ver: rpa2
title: Training dynamic models using early exits for automatic speech recognition
  on resource-constrained devices
arxiv_id: '2309.09546'
source_url: https://arxiv.org/abs/2309.09546
tags:
- layer
- early-exit
- exit
- training
- conformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates early-exit architectures for automatic
  speech recognition (ASR) to enable dynamic computational load adjustment on resource-constrained
  devices. Three models were compared: Conformer-CTC, Conformer-AED, and Wav2Vec2-CTC,
  trained both from scratch and via fine-tuning pre-trained models.'
---

# Training dynamic models using early exits for automatic speech recognition on resource-constrained devices

## Quick Facts
- arXiv ID: 2309.09546
- Source URL: https://arxiv.org/abs/2309.09546
- Reference count: 0
- This paper investigates early-exit architectures for ASR to enable dynamic computational load adjustment on resource-constrained devices.

## Executive Summary
This paper explores early-exit architectures for automatic speech recognition (ASR) to enable dynamic computational load adjustment on resource-constrained devices. The authors compare three model architectures - Conformer-CTC, Conformer-AED, and Wav2Vec2-CTC - trained both from scratch and via fine-tuning pre-trained models. Results show that training from scratch with joint early-exit losses outperforms single-exit models and fine-tuned pre-trained models, especially at lower layers. The Conformer-AED model achieved the best performance with WERs of 2.3% on test-clean and 6.0% on test-other.

## Method Summary
The study trains three ASR model architectures with early-exit branches at intermediate encoder layers: Conformer-CTC, Conformer-AED, and Wav2Vec2-CTC. Models are trained from scratch using joint early-exit losses (CTC, cross-entropy, or both) at all exits. The authors compare these against single-exit baselines and fine-tuned pre-trained Wav2Vec2-CTC. Exit selection is implemented using both entropy-based thresholding and N-best posterior confidence (K=300). Performance is evaluated on LibriSpeech test-clean/test-other sets, measuring WER at each exit layer and computational efficiency trade-offs.

## Key Results
- Early-exit models trained from scratch outperform both single-exit models and fine-tuned pre-trained models, especially at lower layers
- Conformer-AED architecture achieves the best overall performance (WER 2.3% test-clean, 6.0% test-other)
- Exit selection based on N-best posterior probabilities provides slightly better computational efficiency than entropy-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training from scratch with joint early-exit losses outperforms fine-tuning pre-trained models for ASR.
- Mechanism: Joint optimization of all exit losses during training encourages each intermediate exit to learn meaningful representations, whereas fine-tuning a pre-trained model without early-exit objectives leads to poor performance at lower exits.
- Core assumption: Early-exit architectures benefit more from being trained end-to-end with all exit losses rather than being adapted from models optimized only for a final exit.
- Evidence anchors:
  - [abstract] "Experiments conducted on public datasets reveal that early-exit models trained from scratch not only preserve performance when using fewer encoder layers but also exhibit enhanced task accuracy compared to single-exit or pre-trained models."
  - [section] "Interestingly, early-exit training is found to be more effective when training the model from scratch, as opposed to fine-tuning an existing model."
  - [corpus] Weak evidence - only one neighbor paper mentions early-exit training from scratch, but no direct comparison to fine-tuning.

### Mechanism 2
- Claim: Exit selection based on N-best posterior probabilities provides better computational efficiency than entropy-based methods.
- Mechanism: Using the softmax over N-best hypothesis scores (confidence metric) to select exits captures sentence-level certainty better than averaging frame-wise entropies, allowing earlier exits for easier inputs.
- Core assumption: The distribution of N-best hypothesis scores correlates more strongly with overall transcription quality than frame-wise entropy measures.
- Evidence anchors:
  - [abstract] "Exit selection based on N-best posterior probabilities was slightly more effective than entropy-based methods."
  - [section] "We also investigate a metric based on an estimate of the sentence confidence... Preliminary experiments... suggested the value K = 300."
  - [corpus] No direct evidence - corpus papers do not compare exit selection metrics.

### Mechanism 3
- Claim: Early-exit architectures enable dynamic computational load adjustment for ASR on resource-constrained devices.
- Mechanism: Intermediate exit branches allow the model to return results after processing fewer encoder layers, saving computation for easier inputs while maintaining accuracy for harder inputs.
- Core assumption: ASR tasks exhibit varying difficulty levels where simpler inputs can be processed with fewer layers without significant accuracy loss.
- Evidence anchors:
  - [abstract] "This allows for the development of dynamic models that adjust their computational cost to the available resources and recognition performance."
  - [section] "An example is shown in Figure 1... where a layer-specific classifier/decoder... is appended to some intermediate encoder layers."
  - [corpus] Weak evidence - neighbor papers mention dynamic inference but don't specifically address resource-constrained ASR scenarios.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: CTC loss is used for the Conformer-CTC and Wav2Vec2-CTC models to handle unsegmented sequence data without requiring alignment between input frames and output tokens.
  - Quick check question: What is the key advantage of CTC loss for ASR compared to sequence-to-sequence cross-entropy?

- Concept: Self-supervised speech representation learning
  - Why needed here: Wav2Vec2-CTC is initialized from a pre-trained model, demonstrating the role of self-supervised pre-training in ASR and its limitations for early-exit architectures.
  - Quick check question: How does self-supervised pre-training typically differ from supervised training in terms of objectives and data requirements?

- Concept: Byte Pair Encoding (BPE) tokenization
  - Why needed here: BPE is used for tokenization in the Conformer models, reducing the vocabulary size and handling rare words, which is crucial for building efficient ASR systems.
  - Quick check question: What problem does BPE tokenization solve in sequence modeling tasks like ASR?

## Architecture Onboarding

- Component map:
  - Input audio → Feature extraction (80 MFCCs or raw waveform → conv layers) → 12-layer Conformer/Transformer encoder → Early-exit branches (linear + softmax or 4-layer decoder) → Exit selection (entropy or N-best posterior) → Output

- Critical path: Input → Feature extraction → Encoder layers → Exit selection → Output

- Design tradeoffs:
  - Single-exit vs. multi-exit: Single-exit is simpler but less flexible; multi-exit enables dynamic computation but adds complexity
  - Pre-training vs. training from scratch: Pre-training provides good initial representations but may not optimize for early exits; training from scratch with joint losses is more effective for early-exit architectures
  - Exit selection metric: Entropy is simpler but N-best posterior may be more accurate

- Failure signatures:
  - High WER at lower exits: Model wasn't trained effectively with early-exit losses or pre-trained model isn't suitable for early-exit
  - No computational savings: Exit selection threshold is too conservative or metric doesn't correlate with input difficulty
  - Gradient instability: Joint loss optimization causes interference between exit branches

- First 3 experiments:
  1. Train a single-exit Conformer-CTC model and measure baseline WER on LibriSpeech
  2. Add early-exit branches to the same architecture and train with joint CTC losses from scratch
  3. Implement both entropy and N-best posterior exit selection and compare computational efficiency on a subset of the test set

## Open Questions the Paper Calls Out

- The authors mention that future work will investigate weighting schemes for the compound loss function in equation 2, as the current study uses equal weighting for all exits.

## Limitations

- The superiority of scratch training over fine-tuning is based on ASR-specific experiments and may not generalize to other sequence modeling tasks.
- Computational efficiency gains depend heavily on the exit selection strategy, and the N-best posterior method requires additional computation.
- The study focuses on English ASR datasets and may not translate directly to other languages or domains.

## Confidence

- High confidence: Early-exit architectures can reduce computational load while maintaining accuracy when trained with joint losses from scratch
- Medium confidence: N-best posterior confidence is a more effective exit selection metric than entropy for ASR tasks
- Medium confidence: Conformer-AED architecture provides the best trade-off between accuracy and computational efficiency

## Next Checks

1. Test whether the superiority of scratch training over fine-tuning extends to other sequence-to-sequence tasks like machine translation or speech translation
2. Compare the N-best posterior exit selection method against alternative confidence metrics like uncertainty sampling or reinforcement learning-based approaches
3. Evaluate the robustness of early-exit models to varying input lengths and acoustic conditions beyond the LibriSpeech and TED-LIUM datasets used in the study