---
ver: rpa2
title: Diagrammatization and Abduction to Improve AI Interpretability With Domain-Aligned
  Explanations for Medical Diagnosis
arxiv_id: '2302.01241'
source_url: https://arxiv.org/abs/2302.01241
tags:
- murmur
- explanations
- shape
- diagnosis
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diagrammatization, an approach to provide
  diagrammatic and abductive explanations for AI models. The key idea is to have the
  AI model perform abductive reasoning to generate hypotheses and evaluate them, then
  provide explanations in the form of domain-relevant diagrams.
---

# Diagrammatization and Abduction to Improve AI Interpretability With Domain-Aligned Explanations for Medical Diagnosis

## Quick Facts
- arXiv ID: 2302.01241
- Source URL: https://arxiv.org/abs/2302.01241
- Reference count: 40
- Key outcome: Introduces Diagrammatization approach providing diagrammatic and abductive explanations for AI models, demonstrated with DiagramNet for cardiac diagnosis from heart sounds.

## Executive Summary
This paper introduces Diagrammatization, an approach to provide diagrammatic and abductive explanations for AI models by embedding domain hypotheses directly into the prediction process. The key idea is to have the AI model perform abductive reasoning to generate hypotheses and evaluate them, then provide explanations in the form of domain-relevant diagrams. The authors demonstrate this with DiagramNet, a deep neural network for predicting cardiac diagnoses from heart sounds and explaining with murmur diagrams. In modeling studies, DiagramNet outperforms baseline models in both prediction accuracy and explanation faithfulness. A qualitative user study with medical students shows that clinicians prefer the diagrammatic explanations over technical saliency maps.

## Method Summary
The DiagramNet architecture processes 2-channel input tensors (displacement and amplitude) through a multi-stage pipeline: murmur segmentation using U-Net, initial diagnosis prediction using CNN, shape parameter estimation through heuristics and optimization, and hypothesis-driven final prediction. The model is trained using L-BFGS optimization and outputs both diagnosis predictions and fitted murmur diagrams. The approach formalizes murmur shapes as parametric piecewise linear functions optimized during prediction, constraining outputs to clinically meaningful forms while providing domain-aligned explanations.

## Key Results
- DiagramNet outperformed baseline models in diagnosis prediction accuracy and explanation faithfulness
- Clinicians preferred diagrammatic explanations over technical saliency maps in user study
- Model demonstrated low shape parameter and fit estimation errors while maintaining high prediction accuracy
- Successfully discriminated between similar murmurs (MVP vs MS) using abductive reasoning

## Why This Works (Mechanism)

### Mechanism 1
Embedding domain hypotheses directly into the AI prediction process improves both prediction accuracy and explanation faithfulness by constraining outputs to clinically meaningful forms, reducing spurious fits. Core assumption is that predefined murmur shape functions are sufficient to capture all relevant diagnostic variations.

### Mechanism 2
Diagrammatic explanations aligned with clinical conventions reduce user interpretability burden compared to generic saliency maps by leveraging clinicians' prior training in murmur diagram conventions. Core assumption is that users have prior training in murmur diagram conventions.

### Mechanism 3
Performing abductive reasoning within the AI model closes the interpretability gap by generating and evaluating hypotheses autonomously, mirroring clinician reasoning. Core assumption is that the abductive process accurately reflects the clinical reasoning process.

## Foundational Learning

- **Abductive reasoning (inference to the best explanation)**: Why needed - To generate plausible diagnostic hypotheses from observed heart sounds rather than just predicting labels. Quick check - What distinguishes abduction from deduction and induction in the context of heart sound diagnosis?

- **Diagrammatic reasoning**: Why needed - To represent hypotheses in a form that is both constrained by domain rules and familiar to clinicians. Quick check - How does a murmur diagram encode both categorical (diagnosis type) and continuous (amplitude over time) information?

- **Encoder-decoder architectures for segmentation**: Why needed - To locate the murmur region in the audio signal before fitting shape hypotheses. Quick check - Why might a U-Net be preferred over a simple CNN for murmur segmentation?

## Architecture Onboarding

- **Component map**: Input (2-channel tensor) -> Murmur segmentation (U-Net) -> Initial diagnosis (CNN) -> Shape parameter estimation -> Fit measurement (MSE) -> Final diagnosis (ensemble)
- **Critical path**: Segmentation → Initial prediction → Shape fitting → Hypothesis evaluation → Final prediction
- **Design tradeoffs**: Explicit shape fitting vs. implicit feature learning (better interpretability at potential cost of flexibility); multi-task learning vs. single-task (balances performance with auxiliary outputs)
- **Failure signatures**: High MSE on shape fitting despite correct diagnosis (model overfits to amplitude but misaligns with shape); confusion between similar diagnoses (segmentation or shape fitting insufficiently discriminates)
- **First 3 experiments**: 1) Ablation: Remove murmur segmentation, predict directly from raw amplitude; measure drop in accuracy and increase in MSE. 2) Visualization: Plot predicted vs. ground-truth murmur shapes for each diagnosis; inspect systematic errors. 3) User study: Test clinician trust and interpretability with and without diagrammatic explanations; measure agreement and explanation quality.

## Open Questions the Paper Calls Out
- How generalizable is the diagrammatization approach to other domains beyond medical diagnosis? The authors discuss potential applications but only demonstrate for heart auscultation diagnosis.
- How does the interpretability and trustworthiness of diagrammatic explanations compare to other forms of XAI explanations, such as feature attribution or case-based reasoning? Comparison is limited to saliency maps.
- How can diagrammatization be extended to provide more comprehensive explanations that include low-level, concept-based, and rationalization explanations? Paper discusses potential but doesn't provide concrete implementation.

## Limitations
- Murmur shape functions are constrained to a small parametric family, potentially failing to capture rare or atypical murmurs
- Qualitative user study involved only 12 medical students, raising concerns about representativeness and potential bias
- Abductive reasoning process is partially heuristic-driven, introducing potential bias in hypothesis generation

## Confidence
- High confidence: Architectural design of DiagramNet and multi-stage abductive reasoning process is well-specified and technically sound
- Medium confidence: Improvement in prediction accuracy over baseline models is demonstrated, but clinical significance requires further validation
- Medium confidence: Preference for diagrammatic explanations over saliency maps is supported by user study, but limited sample size restricts generalizability

## Next Checks
1. **External Validation**: Test DiagramNet on an independent dataset of heart sound recordings to assess robustness and generalizability to different patient populations and acquisition conditions.

2. **Clinical Relevance Assessment**: Conduct a study with experienced cardiologists to evaluate the clinical utility and diagnostic value of the diagrammatic explanations in real-world scenarios.

3. **Shape Function Expansion**: Investigate the impact of expanding the murmur shape function family to include more complex and atypical shapes, measuring the trade-off between interpretability and expressiveness.