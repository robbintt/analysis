---
ver: rpa2
title: 'HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in
  Large Language Models'
arxiv_id: '2310.16755'
source_url: https://arxiv.org/abs/2310.16755
tags:
- llms
- story
- agent
- reasoning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HI-TOM, a new benchmark designed to evaluate
  higher-order Theory of Mind (ToM) reasoning in Large Language Models (LLMs). While
  previous work focused on first and second-order ToM, HI-TOM extends to fourth-order
  ToM, requiring multi-pass recursive reasoning about others' beliefs.
---

# HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2310.16755
- Source URL: https://arxiv.org/abs/2310.16755
- Reference count: 40
- Key outcome: Introduces HI-TOM benchmark testing zeroth to fourth-order Theory of Mind reasoning; finds significant performance decline on higher-order tasks across multiple LLMs

## Executive Summary
This paper introduces HI-TOM, a new benchmark designed to evaluate higher-order Theory of Mind (ToM) reasoning in Large Language Models (LLMs). While previous work focused on first and second-order ToM, HI-TOM extends to fourth-order ToM, requiring multi-pass recursive reasoning about others' beliefs. The benchmark consists of stories with agent interactions and multiple-choice questions testing zeroth to fourth-order ToM. Experiments with various LLMs, including GPT-4, GPT-3.5, Claude, and Guanaco, show a significant decline in performance on higher-order ToM tasks. The study also analyzes different failure cases, such as insufficient reasoning depth, commonsense errors, and hallucinations. The results highlight the limitations of current LLMs in higher-order ToM reasoning and suggest directions for future research in enhancing LLMs' ToM abilities.

## Method Summary
The study introduces HI-TOM, a benchmark for evaluating higher-order Theory of Mind reasoning in LLMs. The benchmark consists of stories with agent interactions and multiple-choice questions testing zeroth to fourth-order ToM. Researchers evaluated GPT-4, GPT-3.5, Claude, and Guanaco using zero-shot inference with both vanilla and chain-of-thought prompting. The evaluation measured accuracy and joint accuracy across different ToM orders and story types, including those with deceptive agent communications.

## Key Results
- LLMs show high accuracy on zeroth and first-order ToM tasks but performance drops significantly on third and fourth-order tasks
- GPT-4 achieves highest overall accuracy (85.3%) while other models show lower performance (70.4%-75.8%)
- Performance decreases further when stories include deceptive agent communications
- Chain-of-thought prompting yields insignificant performance gains and sometimes decreases accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The HI-TOM benchmark extends Theory of Mind evaluation beyond first and second-order by introducing third and fourth-order tasks requiring recursive reasoning about others' beliefs.
- Mechanism: By designing stories with agent communications and multi-pass reasoning chains, the benchmark forces models to track nested belief states (e.g., "Where does A3 think A2 thinks A1 thinks O is?").
- Core assumption: Recursive belief attribution can be effectively tested through structured story-question pairs with controlled agent interactions.
- Evidence anchors:
  - [abstract] "While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs."
  - [section 3.1] "HI-TOM stories consist of four fundamental elements... Each story comprises one to three chapters... In each chapter, we design multiple actions and optional communication protocols among agents."
  - [corpus] Weak: no corpus evidence available for higher-order ToM benchmarks specifically.
- Break condition: If agent interactions are too sparse or belief chains are ambiguous, models may default to shallow reasoning patterns.

### Mechanism 2
- Claim: HI-TOM introduces deceptive agent communications to increase realism and complexity of ToM evaluation.
- Mechanism: Stories include both public and private communications where agents may lie, requiring models to reason about trust and belief revision rather than just tracking object locations.
- Core assumption: Deceptive communications create scenarios where correct answers cannot be derived from final object positions alone.
- Evidence anchors:
  - [section 3.1] "For agent communications, we set the shared information to be deceptive in order to emulate the dynamics of the complicated social life."
  - [section 5] "LLMs' performance decreases as there are more deception communications involved."
  - [corpus] Weak: no corpus evidence for deceptive communication effects on LLM ToM performance.
- Break condition: If models rely on surface patterns rather than genuine belief reasoning, deceptive communications may simply increase error rates without revealing reasoning limitations.

### Mechanism 3
- Claim: HI-TOM's story structure and question design create position bias in LLM responses, with better performance on answers appearing at story beginning or end.
- Mechanism: Longer stories with multiple containers and chapters create cognitive load, causing models to overweight information at temporal extremes.
- Core assumption: LLMs exhibit attention or positional biases similar to those observed in other language tasks.
- Evidence anchors:
  - [section 6.1] "LLMs handle answers that appear at the beginning and end better... We observe similar patterns of the Claude model focusing on the beginning and end of stories."
  - [section 3.3] "HI-TOM features a larger pool of potential answers and a balanced distribution of correct answers throughout the story."
  - [corpus] Weak: no corpus evidence for position bias in ToM reasoning specifically.
- Break condition: If stories are shortened or containers are reordered, this positional advantage may diminish.

## Foundational Learning

- Concept: Theory of Mind and recursive belief attribution
  - Why needed here: Understanding different ToM orders (zeroth to fourth) is essential for interpreting HI-TOM results and designing effective evaluation stories.
  - Quick check question: What distinguishes a third-order ToM question from a second-order one in terms of belief nesting?

- Concept: Chain-of-thought prompting and its limitations
  - Why needed here: The paper tests CoTP prompting to see if step-by-step reasoning improves ToM performance, but finds limited gains.
  - Quick check question: Why might CoTP prompting decrease performance on deceptive communication stories?

- Concept: Position bias in language models
  - Why needed here: Understanding why models perform better on beginning/end answers helps interpret results and design better prompts.
  - Quick check question: How might positional encoding in transformer architectures contribute to this bias?

## Architecture Onboarding

- Component map:
  Story generation engine (Python scripts) → HI-TOM dataset → LLM API calls → Accuracy evaluation → Error analysis pipeline
  Key components: Story templates, question generators, answer trackers, prompting templates, evaluation metrics

- Critical path:
  1. Generate HI-TOM stories with controlled agent interactions
  2. Format stories into prompts for different LLM models
  3. Execute prompts and collect responses
  4. Parse responses and calculate accuracy metrics
  5. Analyze error patterns across ToM orders

- Design tradeoffs:
  - Story complexity vs. generation reliability: More complex stories provide better evaluation but harder to generate consistently
  - Manual vs. automatic quality checking: Manual review ensures quality but limits scalability
  - Single vs. multiple correct answers: Single correct answers simplify evaluation but may miss nuanced reasoning

- Failure signatures:
  - Consistently correct on zeroth-order but poor on higher orders suggests shallow reasoning
  - Performance drop with deceptive communications indicates reliance on surface patterns
  - Positional bias (better on beginning/end answers) suggests attention limitations

- First 3 experiments:
  1. Generate and test a small HI-TOM subset with varying deception levels to establish baseline performance
  2. Compare vanilla prompting vs. CoTP across all ToM orders to identify performance patterns
  3. Analyze error types by manually reviewing LLM responses to understand reasoning failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reasoning depth limitation in LLMs be overcome through architectural changes or training paradigms?
- Basis in paper: [inferred] The paper identifies "insufficient reasoning depth" as a major error type in LLM responses, noting that models tend to oversimplify questions and skip required multi-step reasoning steps. This suggests LLMs struggle with recursive reasoning needed for higher-order ToM.
- Why unresolved: While the paper observes this limitation, it doesn't explore potential solutions like architectural modifications or alternative training approaches that could improve reasoning depth.
- What evidence would resolve it: Experiments comparing different LLM architectures or training methods on HI-TOM would show whether specific approaches improve performance on higher-order ToM tasks.

### Open Question 2
- Question: Does pre-training data contamination explain LLMs' performance on commonsense reasoning tasks?
- Basis in paper: [explicit] The paper notes that LLMs show high performance on standard commonsense benchmarks but make frequent errors on ToM tasks, suggesting possible data contamination in publicly available commonsense benchmarks.
- Why unresolved: The paper raises this possibility but doesn't investigate whether LLMs' commonsense knowledge comes from pre-training versus genuine reasoning ability.
- What evidence would resolve it: Analysis of whether LLMs perform better on novel commonsense scenarios versus established benchmarks would indicate the role of data contamination.

### Open Question 3
- Question: How do different prompting strategies affect LLM performance on higher-order ToM tasks?
- Basis in paper: [explicit] The paper finds that chain-of-thought prompting yields insignificant performance gains and sometimes decreases accuracy, suggesting current prompting strategies may not effectively support higher-order reasoning.
- Why unresolved: While the paper tests two prompting styles, it doesn't explore alternative prompting approaches that might better facilitate complex ToM reasoning.
- What evidence would resolve it: Systematic testing of various prompting strategies on HI-TOM would identify approaches that improve higher-order ToM performance.

## Limitations

- The benchmark relies entirely on synthetic stories, which may not capture the full complexity and ambiguity of real human belief attribution scenarios
- Evaluation focuses on multiple-choice questions, potentially missing nuanced reasoning capabilities
- The study does not explore alternative prompting strategies beyond vanilla and chain-of-thought prompting

## Confidence

High confidence: Claims about the existence of position bias in LLM responses, where models perform better on answers appearing at story beginning/end. This is directly observable from the presented data and follows established patterns in language model behavior.

Medium confidence: Claims about the general decline in performance for higher-order ToM tasks. While the experimental results are clear, the synthetic nature of the benchmark and the specific prompting strategies used may influence the observed performance patterns.

Low confidence: Claims about the specific mechanisms behind LLM failures in higher-order ToM reasoning. The error analysis identifies patterns but cannot definitively determine whether failures stem from insufficient reasoning depth, commonsense errors, or genuine limitations in recursive belief attribution.

## Next Checks

1. Real-world validation study: Test HI-TOM questions on human subjects to establish baseline performance and validate the benchmark's difficulty calibration. Compare human and LLM performance patterns to identify whether observed LLM limitations reflect genuine reasoning gaps or benchmark-specific artifacts.

2. Cross-prompting strategy comparison: Systematically evaluate different prompting strategies beyond vanilla and chain-of-thought, including few-shot examples, structured reasoning templates, and multi-stage prompting. This would help isolate whether the observed performance patterns are due to prompting limitations rather than fundamental LLM capabilities.

3. Transferability assessment: Test whether LLMs that perform well on HI-TOM also demonstrate better performance on other Theory of Mind tasks and real-world scenarios requiring recursive belief attribution. This would help determine if HI-TOM success correlates with genuine ToM capabilities rather than benchmark-specific memorization or pattern matching.