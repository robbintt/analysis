---
ver: rpa2
title: Disentangling Linear Mode-Connectivity
arxiv_id: '2312.09832'
source_url: https://arxiv.org/abs/2312.09832
tags:
- layer
- linear
- connectivity
- barrier
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates linear mode connectivity (LMC) in neural
  networks by systematically isolating the effects of architecture, training strategy,
  and dataset complexity. Starting from the simple logistic regression setting, the
  authors gradually introduce non-linearity and complexity, identifying a minimal
  1-hidden-layer ReLU MLP trained with SGD as a robust LMC model.
---

# Disentangling Linear Mode-Connectivity

## Quick Facts
- arXiv ID: 2312.09832
- Source URL: https://arxiv.org/abs/2312.09832
- Reference count: 34
- One-line primary result: Identifies minimal 1-hidden-layer ReLU MLP trained with SGD as robust LMC model, while showing weight-sharing, ADAM, and dataset complexity break LMC.

## Executive Summary
This work systematically investigates linear mode connectivity (LMC) in neural networks by isolating the effects of architecture, training strategy, and dataset complexity. Starting from logistic regression and progressively adding complexity, the authors identify a minimal 1-hidden-layer ReLU MLP trained with SGD as a robust LMC model. They then demonstrate that weight-sharing architectures (like CNNs) break LMC more than locality alone, ADAM amplifies barriers compared to SGD, and task complexity increases the normalized performance-aware barrier. These insights provide a foundation for future theoretical work on LMC by establishing a minimal but non-trivial setting for study.

## Method Summary
The authors establish a systematic methodology for analyzing LMC by training two models from the same initialization using different SGD noise patterns (data ordering and augmentation). They measure connectivity using error barrier and normalized performance-aware barrier metrics along linear interpolation paths between model parameters. The study progresses from simple logistic regression through increasingly complex architectures, testing how architectural modifications (sparsity, weight-sharing), optimization choices (SGD vs ADAM, learning rate, warm-up), and dataset complexity impact LMC. The minimal model is a 1-hidden-layer ReLU MLP trained on MNIST, which serves as the baseline for systematic architectural and optimization experiments.

## Key Results
- A 1-hidden-layer ReLU MLP trained with SGD maintains robust LMC on MNIST, serving as a minimal non-trivial baseline
- Weight-sharing architectures break LMC more severely than locality alone, with attention-based models showing similar connectivity decay
- ADAM amplifies LMC barriers compared to SGD due to per-parameter adaptive learning rates
- Dataset complexity increases normalized performance-aware barriers, with TinyImageNet showing higher barriers than simpler datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADAM breaks linear mode connectivity (LMC) more severely than SGD due to its adaptive learning rate per parameter.
- Mechanism: ADAM's per-parameter learning rates create asymmetric updates across parameters. When interpolating between two models trained with ADAM, these asymmetric updates accumulate along the path, creating performance barriers that don't exist with uniform learning rates in SGD.
- Core assumption: The interpolation path maintains the same relative parameter importance scaling that was learned during training.
- Evidence anchors:
  - [abstract] "ADAM amplifies barriers compared to SGD"
  - [section] "ADAM on the other hand quickly breaks connectivity even for shallower networks"
  - [corpus] Weak - no direct mention of ADAM in related papers
- Break condition: When the adaptive learning rates cause parameter updates to diverge significantly between the two models along the interpolation path.

### Mechanism 2
- Claim: Weight-sharing architectures (like CNNs) break LMC more than locality alone because shared parameters must maintain consistent roles across all spatial positions during interpolation.
- Mechanism: In weight-sharing architectures, a single parameter appears in multiple locations in the network. During interpolation, this parameter must simultaneously satisfy different spatial constraints from each model, creating tension that manifests as barriers. Local architectures without weight-sharing avoid this by having independent parameters for each position.
- Core assumption: The spatial dependencies created by weight-sharing cannot be reconciled through simple linear interpolation.
- Evidence anchors:
  - [abstract] "weight-sharing breaks LMC more than locality"
  - [section] "attention-based model experience a decay in connectivity" - attention also involves weight-sharing
  - [corpus] Weak - related papers don't discuss weight-sharing's role in LMC
- Break condition: When the interpolation path forces shared parameters to simultaneously satisfy incompatible spatial relationships from both models.

### Mechanism 3
- Claim: Dataset complexity increases normalized performance-aware barriers because more complex tasks have higher baseline performance variance, amplifying the relative impact of interpolation errors.
- Mechanism: As datasets become more complex (MNIST → CIFAR-10 → CIFAR-100 → TinyImageNet), the model's ability to generalize becomes more sensitive to parameter perturbations. The normalized barrier metric captures this by scaling the absolute barrier by the average accuracy of the two models, revealing that complex tasks have more fragile LMC.
- Core assumption: Complex tasks require more precise parameter configurations, making them more sensitive to interpolation errors.
- Evidence anchors:
  - [section] "task complexity increases the normalized performance-aware barrier"
  - [section] Figure 3 shows increasing barriers across datasets of increasing complexity
  - [corpus] Weak - related papers don't discuss dataset complexity's effect on LMC
- Break condition: When dataset complexity creates a regime where small parameter perturbations cause disproportionate performance degradation.

## Foundational Learning

- Concept: Convex optimization and loss landscape geometry
  - Why needed here: Understanding why logistic regression (convex) maintains LMC while neural networks (non-convex) may not requires grasping how convexity ensures connected solution spaces
  - Quick check question: If a loss function is strictly convex, what can we say about the connectivity of its minima?

- Concept: Parameter space interpolation and barrier metrics
  - Why needed here: The paper's core methodology involves linear interpolation between model parameters and measuring performance barriers along this path
  - Quick check question: What does a high error barrier value indicate about the relationship between two models along their interpolation path?

- Concept: Architectural equivalences (MLP vs CNN via weight tying)
  - Why needed here: The paper establishes correspondences between different architectures to isolate architectural effects on LMC
  - Quick check question: How can a convolutional layer be represented as a sparse, weight-tied MLP layer?

## Architecture Onboarding

- Component map: 1-hidden-layer ReLU MLP (baseline) -> Depth (more layers) -> Non-linearity strength (Leaky ReLU slopes) -> Layer types (locally-connected CNN, CNN, attention layer) -> Dataset complexity (MNIST → CIFAR-10 → CIFAR-100 → TinyImageNet)
- Critical path: To reproduce the key findings, first establish LMC in the 1-hidden-layer ReLU MLP on MNIST, then systematically introduce each architectural change (depth, non-linearity, weight-sharing) and measure barrier changes, followed by testing different optimization algorithms and datasets.
- Design tradeoffs: The 1-hidden-layer ReLU MLP balances simplicity (easy to analyze theoretically) with non-triviality (exhibits real LMC behavior). Adding depth increases expressivity but breaks LMC; weight-sharing increases parameter efficiency but breaks LMC; ADAM increases optimization efficiency but breaks LMC.
- Failure signatures: High error barriers (>0.02) indicate broken LMC. Different failure modes: (1) optimization-related - ADAM consistently creates barriers, (2) architectural - weight-sharing creates barriers, (3) dataset-related - complex datasets create barriers.
- First 3 experiments:
  1. Train two independent 1-hidden-layer ReLU MLPs on MNIST with SGD from the same initialization, measure the error barrier along their interpolation path
  2. Repeat experiment 1 with ADAM optimizer instead of SGD, observe barrier increase
  3. Modify the hidden layer to be a locally-connected CNN (same parameter count), measure barrier change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does weight-sharing play a more fundamental role in breaking LMC than other architectural features like locality or depth?
- Basis in paper: [explicit] The authors systematically compared locally-connected CNNs (locality only), CNNs (locality + weight-sharing), and attention-based models, finding that weight-sharing architectures consistently exhibited reduced LMC compared to their non-weight-sharing counterparts.
- Why unresolved: While the paper demonstrates a correlation between weight-sharing and reduced LMC, it does not establish a causal mechanism or determine whether weight-sharing is the primary factor or if other architectural differences (e.g., parameter efficiency, inductive bias) are contributing.
- What evidence would resolve it: Controlled experiments isolating weight-sharing while holding other architectural properties constant, combined with theoretical analysis of how weight-sharing affects the loss landscape geometry.

### Open Question 2
- Question: How does the early phase of training (e.g., warm-up period) determine whether networks will exhibit LMC, and can this be manipulated to ensure connectivity?
- Basis in paper: [explicit] The authors found that adding a warm-up period to ADAM training significantly reduced the performance barrier, suggesting that LMC is determined early in training. They also noted that ADAM tends to amplify barriers compared to SGD.
- Why unresolved: The paper observes the effect of warm-up but does not explain the underlying mechanism by which early training dynamics influence long-term connectivity, nor does it provide a method to predict or control LMC based on early training behavior.
- What evidence would resolve it: Detailed analysis of the optimization trajectory during the warm-up phase, including the evolution of the loss landscape and the relationship between early and late-stage minima, could clarify how early training shapes connectivity.

### Open Question 3
- Question: Does the normalized performance-aware barrier provide a more accurate measure of LMC across datasets of varying complexity, and how does it relate to the absolute error barrier?
- Basis in paper: [explicit] The authors introduce the normalized performance-aware barrier to account for performance gaps across different datasets, observing that task complexity increases the normalized barrier. However, they do not fully explore the relationship between normalized and absolute barriers.
- Why unresolved: While the normalized barrier accounts for performance differences, it is unclear how it relates to the absolute error barrier in terms of interpretability and whether it captures all aspects of connectivity that are relevant for generalization or robustness.
- What evidence would resolve it: Comparative studies of both barrier metrics across diverse datasets and architectures, along with analysis of their correlation with downstream tasks like generalization or adversarial robustness, would clarify their respective strengths and limitations.

## Limitations
- The mechanistic explanations for why weight-sharing and ADAM break LMC rely on assumptions that aren't directly validated through quantitative experiments
- The dataset complexity findings are limited to vision datasets and may not generalize to other domains like text or tabular data
- The normalized barrier metric's relationship to the absolute error barrier and its interpretability across different tasks remains unclear

## Confidence
- **High confidence**: SGD vs ADAM comparison showing ADAM breaks LMC more severely - this is directly observed across multiple experiments
- **Medium confidence**: Weight-sharing breaks LMC more than locality - supported by experiments but the underlying mechanism isn't rigorously proven
- **Medium confidence**: Dataset complexity increases normalized barriers - consistent experimental pattern but limited to vision datasets

## Next Checks
1. **Parameter importance validation**: Measure the correlation between parameter importance (e.g., gradient magnitude) and interpolation barrier contributions to test the core assumption about asymmetric updates in ADAM.

2. **Weight-sharing mechanism quantification**: Design an experiment where weight-sharing parameters are gradually untied during interpolation and measure how this affects barrier magnitude, directly testing the spatial constraint hypothesis.

3. **Cross-domain dataset complexity**: Test the normalized barrier metric on non-vision tasks (text classification, tabular data) to determine if the complexity-barrier relationship generalizes beyond image datasets.