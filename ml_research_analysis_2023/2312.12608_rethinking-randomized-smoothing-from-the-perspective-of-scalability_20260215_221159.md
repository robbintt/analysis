---
ver: rpa2
title: Rethinking Randomized Smoothing from the Perspective of Scalability
arxiv_id: '2312.12608'
source_url: https://arxiv.org/abs/2312.12608
tags:
- smoothing
- randomized
- robustness
- certified
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey reviews randomized smoothing (RS) techniques for certified
  adversarial robustness in machine learning, analyzing both theoretical foundations
  and practical scalability challenges. The paper identifies three major challenges:
  the curse of dimensionality affecting lp norms (p 2), the robustness-accuracy tradeoff,
  and high inference costs due to Monte Carlo sampling.'
---

# Rethinking Randomized Smoothing from the Perspective of Scalability

## Quick Facts
- arXiv ID: 2312.12608
- Source URL: https://arxiv.org/abs/2312.12608
- Reference count: 40
- Key outcome: This survey reviews randomized smoothing techniques for certified adversarial robustness, identifying three major challenges: curse of dimensionality, robustness-accuracy tradeoff, and high inference costs due to Monte Carlo sampling.

## Executive Summary
This comprehensive survey examines randomized smoothing (RS) techniques for certified adversarial robustness in machine learning. The paper systematically analyzes both theoretical foundations and practical scalability challenges across multiple dimensions. While RS provides strong theoretical guarantees for l2 robustness through Gaussian noise convolution, the survey critically highlights that most optimizations focus on improving certified radii rather than addressing fundamental scalability issues that limit practical deployment in high-dimensional settings.

## Method Summary
The survey synthesizes and analyzes numerous randomized smoothing variants including input-dependent methods (DDRS, ANCER, RDDRS), ensemble-based approaches, and applications across domains like graph neural networks, reinforcement learning, and natural language processing. The methodology involves reviewing theoretical proofs, empirical effectiveness studies, and practical implementations to identify scalability bottlenecks. Key focus areas include the curse of dimensionality affecting lp norms (p > 2), the tradeoff between robustness and accuracy, and the computational overhead of Monte Carlo sampling required for certification.

## Key Results
- RS provides certified l2 robustness through Neyman-Pearson lemma with Monte Carlo sampling, but certified radii decrease as O(d^(1/2 - 1/p)) for p > 2 dimensions
- Input-dependent smoothing techniques can improve certified radii by adapting noise parameters to local data geometry but add significant computational overhead
- The robustness-accuracy tradeoff remains a fundamental challenge, with most RS variants prioritizing certified radii over inference cost reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomized smoothing provides certified l2 robustness bounds through probabilistic guarantees derived from Gaussian noise convolution.
- Mechanism: The base classifier f is smoothed by convolving with Gaussian noise N(0, σ²I), creating a new classifier g that predicts the most likely class c that f would predict for a noised version of input x+ε. The certified radius r = σ(Φ⁻¹(pA) - Φ⁻¹(pB)) is derived using Neyman-Pearson lemma, where pA and pB are the probabilities of the top two classes.
- Core assumption: The Neyman-Pearson lemma applies to the binary hypothesis testing problem where we compare the likelihood of class cA versus class cB under Gaussian noise perturbations.
- Evidence anchors:
  - [abstract]: "Randomized smoothing has emerged as a promising technique among notable advancements. This study reviews the theoretical foundations, empirical effectiveness, and applications of randomized smoothing in verifying machine learning classifiers."
  - [section]: "Cohen et al. [1] was the first paper to prove tight robustness guarantees for a randomized smoothing classifier against adversarial attacks constrained under the l2 norm."
  - [corpus]: Weak evidence - the corpus contains papers about randomized smoothing but lacks direct theoretical proofs of the Neyman-Pearson application.
- Break condition: If the underlying noise distribution deviates significantly from Gaussian assumptions, or if the classifier's decision boundary has high curvature that violates the smoothness assumptions.

### Mechanism 2
- Claim: The curse of dimensionality causes certified radii to decrease as O(d^(1/2 - 1/p)) for lp norms with p > 2.
- Mechanism: As input dimensionality d increases, the volume of the lp ball grows exponentially, requiring exponentially more noise to maintain the same relative robustness. For p > 2, the certified radius scales inversely with dimension.
- Core assumption: The noise distribution's tail behavior dominates the robustness guarantees, and higher dimensions require proportionally more noise to maintain coverage.
- Evidence anchors:
  - [section]: "Kumar, Aounon, et al. [11] show, in the specific case of a generalized Gaussian distribution, tighter bounds than those attained by Blum, Avrim, et al. [7]. Their work states that the certified radius for an lp norm decreases as O(d^(1/2 - 1/p)) with data dimension d for p > 2."
  - [abstract]: No direct evidence about dimensionality effects.
  - [corpus]: Moderate evidence - contains related papers discussing dimensionality effects but lacks comprehensive theoretical proofs.
- Break condition: If dimensionality reduction techniques are applied, or if the data actually lies on a lower-dimensional manifold as suggested by manifold hypothesis.

### Mechanism 3
- Claim: Input-dependent smoothing techniques can improve certified radii by adapting noise parameters to local data geometry.
- Mechanism: Instead of using a fixed σ for all inputs, data-dependent randomized smoothing (DDRS) optimizes σx per input using gradient ascent to maximize the certified radius R(x, μ). This exploits local decision boundary geometry.
- Core assumption: The certified radius is concave in σ for most data points, allowing gradient-based optimization to find better parameters.
- Evidence anchors:
  - [section]: "Alfarra et al. [15] propose the use of additional information, as is suggested in the curse of dimensionality section, which can be extracted from the data itself, in a scheme known as Data Dependent Randomized Smoothing (DDRS)."
  - [abstract]: No direct evidence about input-dependent techniques.
  - [corpus]: Strong evidence - contains multiple papers (DDRS, ANCER, RDDRS) implementing input-dependent smoothing with empirical improvements.
- Break condition: If the optimization landscape is non-concave or if the computational overhead outweighs the certified radius improvements.

## Foundational Learning

- Concept: Neyman-Pearson lemma and its application to randomized smoothing
  - Why needed here: Forms the theoretical foundation for deriving certified robustness bounds from noisy classifier outputs
  - Quick check question: Can you explain why the Neyman-Pearson lemma provides a lower bound on the certified radius rather than an exact value?

- Concept: Monte Carlo sampling and statistical confidence intervals
  - Why needed here: Randomized smoothing relies on Monte Carlo sampling to estimate class probabilities, and Clopper-Pearson intervals provide probabilistic guarantees
  - Quick check question: What is the relationship between the number of Monte Carlo samples and the width of the confidence interval for class probability estimates?

- Concept: Lipschitz continuity and its role in adversarial robustness
  - Why needed here: The Lipschitz constant of smoothed classifiers bounds how much outputs can change for small input perturbations, directly affecting certified radii
  - Quick check question: How does the Lipschitz constant of the base classifier affect the certified radius of the smoothed classifier?

## Architecture Onboarding

- Component map: Input → Noise generator → Base classifier forward passes → Probability aggregation → Statistical estimator → Certified radius calculator
- Critical path: Input → Noise generation → Base classifier forward passes → Probability aggregation → Statistical estimation → Certification calculation
- Design tradeoffs:
  - Fixed vs. data-dependent σ: Fixed is simpler but may underperform; data-dependent requires optimization overhead
  - Number of Monte Carlo samples: More samples give tighter bounds but increase inference cost linearly
  - Noise distribution choice: Gaussian is theoretically justified for l2 but other distributions may be better for different threat models
- Failure signatures:
  - Low certified radii across all inputs: May indicate overly conservative σ or poor base classifier
  - High variance in certification results: Suggests insufficient Monte Carlo samples or unstable base classifier
  - Degraded natural accuracy: Could indicate σ is too large relative to the decision boundary geometry
- First 3 experiments:
  1. Implement basic randomized smoothing with fixed σ on a simple CNN classifier for CIFAR-10, measure certified radii vs. number of Monte Carlo samples
  2. Compare fixed σ vs. data-dependent DDRS approach on the same dataset, measure both certified radii and inference time overhead
  3. Test dimensionality effects by training on CIFAR-10 vs. higher-resolution variants, measure how certified radii scale with input dimension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective approaches to reduce the inference cost of input-specific randomized smoothing techniques like DDRS, Insta-RS, and ANCER?
- Basis in paper: [explicit] The paper explicitly states "Noting the dearth of inference cost-reducing methods like [59], particularly for input-specific techniques which add a significant overhead to RS, which itself is computationally expensive, we also encourage works to work towards reducing these costs, since they prohibit scalable applications of RS."
- Why unresolved: Current input-specific methods add significant computational overhead to already expensive RS, making them impractical for real-world deployment despite improved robustness.
- What evidence would resolve it: Empirical studies demonstrating reduced inference time for input-specific RS methods while maintaining or improving certified robustness, possibly through approximation techniques, efficient optimization algorithms, or hardware acceleration.

### Open Question 2
- Question: How can randomized smoothing techniques be made more robust against PRNG-based attacks that target the randomness source?
- Basis in paper: [explicit] The paper mentions that "Adversarial label-flipping attacks manipulate the labeling of data to deceive machine learning models" and "A major vulnerability of RS against adversarial attacks is where the attacker can backdoor even the randomness in a model."
- Why unresolved: Current RS techniques assume truly random noise sources, but PRNG attacks can exploit weaknesses in pseudo-random number generators to compromise certification guarantees.
- What evidence would resolve it: Development and evaluation of RS variants with verifiable randomness sources, or methods that are provably robust to PRNG manipulation while maintaining certification guarantees.

### Open Question 3
- Question: What is the theoretical relationship between the curse of dimensionality and the effectiveness of input-specific information (gradient, Hessian) in randomized smoothing?
- Basis in paper: [inferred] The paper discusses both the curse of dimensionality limiting lp norm certifications as dimensionality increases, and the use of local information (gradients, Hessians) to improve certified radii, but doesn't analyze their interaction.
- Why unresolved: While both issues are well-documented separately, it's unclear whether using local information actually helps mitigate the curse of dimensionality or if it introduces additional dimensionality-dependent costs that compound the problem.
- What evidence would resolve it: Theoretical analysis showing whether higher-order information provides dimension-independent improvements in certified radii, or empirical studies demonstrating the scaling behavior of local-information-based RS methods across dimensions.

## Limitations

- Theoretical claims about randomized smoothing rely heavily on Gaussian noise assumptions that may not hold for complex, non-linear classifiers in practice
- Scalability analysis focuses primarily on computational costs rather than fundamental mathematical limitations imposed by high-dimensional geometry
- Treatment of advanced variants like Riemannian optimization lacks quantitative comparisons of relative benefits

## Confidence

**High Confidence**: The basic mechanism of randomized smoothing using Gaussian noise and Monte Carlo sampling is well-established and experimentally validated across multiple domains. The computational cost scaling (O(n) for n samples) and the existence of the robustness-accuracy tradeoff are directly observable.

**Medium Confidence**: Claims about the curse of dimensionality and its O(d^(1/2 - 1/p)) scaling for p > 2 norms are supported by theoretical analysis but lack comprehensive empirical validation across diverse datasets and architectures. The effectiveness of input-dependent smoothing techniques shows promise but depends heavily on optimization stability.

**Low Confidence**: The survey's treatment of advanced variants like Riemannian optimization and adaptive smoothing mechanisms is more descriptive than analytical, with limited quantitative comparisons of their relative benefits.

## Next Checks

1. **Dimensionality Scaling Experiment**: Systematically measure certified radii across input dimensions (e.g., CIFAR-10 at different resolutions) to empirically validate the O(d^(1/2 - 1/p)) scaling relationship and identify at what dimensionality the theoretical bounds become practically meaningless.

2. **Adaptive Attack Validation**: Implement state-of-the-art adaptive attacks specifically designed to exploit the gap between certified bounds and actual robustness, measuring the true security margin provided by randomized smoothing across different threat models.

3. **Scalability Benchmark**: Compare the wall-clock inference time of various RS variants (fixed σ, DDRS, ensemble methods) on high-resolution ImageNet-scale datasets to quantify the practical deployment constraints and identify optimization opportunities beyond sampling efficiency.