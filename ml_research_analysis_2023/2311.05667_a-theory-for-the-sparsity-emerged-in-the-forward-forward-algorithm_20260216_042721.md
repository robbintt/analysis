---
ver: rpa2
title: A theory for the sparsity emerged in the Forward Forward algorithm
arxiv_id: '2311.05667'
source_url: https://arxiv.org/abs/2311.05667
tags:
- data
- theorem
- batch
- relu
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents theoretical analysis of sparsity emergence
  in the Forward-Forward (FF) algorithm. Two theorems predict when activation sparsity
  increases: Theorem 1 addresses sparsity change when decreasing batch goodness, and
  Theorem 2 handles the complete FF algorithm with both positive and negative data.'
---

# A theory for the sparsity emerged in the Forward Forward algorithm

## Quick Facts
- arXiv ID: 2311.05667
- Source URL: https://arxiv.org/abs/2311.05667
- Reference count: 1
- Key outcome: Theoretical analysis predicts sparsity emergence in Forward-Forward algorithm through two theorems with >99.98% satisfaction rate at initialization and >50% during training

## Executive Summary
This paper presents theoretical analysis explaining why sparsity emerges during Forward-Forward (FF) algorithm training. The authors propose two theorems that predict when activation sparsity increases for individual data points. Theorem 1 addresses sparsity changes when decreasing batch goodness, while Theorem 2 handles the complete FF algorithm with both positive and negative data. The theoretical framework shows that under assumptions of infinite width, proper initialization, and ReLU activation, sparsity naturally emerges as a consequence of the optimization dynamics. Experiments on MNIST validate the theoretical predictions, demonstrating that the vast majority of data points satisfy the theorem conditions and that sparsity increases during training.

## Method Summary
The method centers on analyzing sparsity changes in single-layer ReLU networks during Forward-Forward training. Two theorems are derived: Theorem 1 predicts sparsity increase when goodness function (sum of squared activations minus threshold) decreases, while Theorem 2 extends this to the complete FF algorithm with positive and negative data. The analysis assumes infinite width, proper weight initialization, and ReLU activation. Sparsity is formally defined using L1 and L2 norms. The goodness function creates gradients that, under specific conditions captured by the theorems, push activations toward sparser configurations. Experiments verify theorem satisfaction rates on MNIST across initialization and multiple training epochs.

## Key Results
- Theorem 1 verified: Over 99.98% of MNIST data points satisfy the inequality predicting sparsity increase at initialization
- Theorem 2 validated: More than 50% of data points satisfy the inequality during FF training for both positive and negative samples
- Sparsity increase confirmed: Actual sparsity ratios during training align with theoretical predictions
- Good agreement: Theoretical framework accurately predicts emergent sparsity patterns in FF algorithm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decreasing goodness increases sparsity for data points satisfying Theorem 1 inequality
- Mechanism: Gradient descent reducing goodness (L2 norm of activations) pushes small positive ReLU activations to zero, increasing zero fraction
- Core assumption: Pre-activation weighted sums never equal zero, ReLU activation used, proper initialization and learning rate
- Evidence anchors: [abstract] predicts sparsity changes; [section] states Theorem 1 inequality; [corpus] weak evidence from related FF discussions
- Break condition: Pre-activation equals zero (ReLU gradient undefined), incorrect initialization or learning rate

### Mechanism 2
- Claim: Theorem 2's inequality ensures sparsity increase during complete FF training with opposing gradients
- Mechanism: Loss function L = G(H-) - G(H+) creates opposing gradient effects that push activations toward sparser configurations when theorem conditions are met
- Core assumption: Both positive and negative data contribute, proper initialization, ReLU preserves sign stability
- Evidence anchors: [abstract] applies complete FF algorithm; [section] states Theorem 2 inequality; [corpus] weak evidence from FF training dynamics
- Break condition: Insufficient batch diversity, overly similar positive/negative data

### Mechanism 3
- Claim: ReLU activation combined with goodness function naturally creates sparsity emergence
- Mechanism: ReLU sets negative pre-activations to zero; goodness function gradients selectively suppress below-threshold activations while preserving larger ones
- Core assumption: ReLU activation used, goodness function implemented as specified
- Evidence anchors: [section] defines ReLU and goodness G(H); [section] notes non-negative elements with ReLU; [corpus] no direct evidence
- Break condition: Different activation function used, goodness function significantly modified

## Foundational Learning

- Concept: Gradient descent optimization and loss function effects
  - Why needed here: Entire framework depends on understanding how gradient updates affect goodness function and subsequently activation sparsity
  - Quick check question: If we have loss function L and perform gradient descent with learning rate Î·, what is the weight update rule?

- Concept: Properties of ReLU activation function
  - Why needed here: Proof relies on ReLU behavior (setting negatives to zero, specific gradient structure) and interactions with weight updates
  - Quick check question: What is ReLU derivative at x=0, and how does this affect backpropagation through ReLU layers?

- Concept: Vector norms and their relationships (L1, L2, sparsity measures)
  - Why needed here: Theorems expressed in terms of L1 and L2 norms, sparsity formally defined using these norms
  - Quick check question: Given vector v, how do you compute its L1 norm, L2 norm, and sparsity measure S(v)?

## Architecture Onboarding

- Component map: Input layer (784-dim MNIST images) -> Single fully-connected layer (n neurons, W matrix) -> ReLU activation -> Goodness function computation

- Critical path:
  1. Forward pass: Compute pre-activation Wx, apply ReLU to get h
  2. Compute goodness for positive and negative batches
  3. Calculate gradients of goodness with respect to W
  4. Update weights using gradient descent
  5. Monitor sparsity changes and check theorem conditions

- Design tradeoffs:
  - Width n vs. sparsity: Larger n provides more sparsity opportunities but increases computation
  - Batch size: Larger batches give stable gradients but may reduce theorem satisfaction ratio
  - Learning rate: Must follow O(1/n) schedule for theoretical guarantees
  - Negative data construction: Generation method affects co-occurrence of sparsity increases

- Failure signatures:
  - Sparsity doesn't increase despite satisfying conditions: Check for zero pre-activation weighted sums
  - Training diverges: Verify O(1/n) learning rate schedule and proper initialization
  - Theorem conditions rarely satisfied: Check batch size and data distribution

- First 3 experiments:
  1. Verify Theorem 1 at initialization: Train single linear layer on MNIST, check ratio satisfying theorem inequality
  2. Track sparsity during training: Monitor actual sparsity changes and theorem satisfaction ratios over epochs
  3. Test Theorem 2 with different negatives: Compare sparsity emergence using random wrong labels vs. other negative sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can theorems be extended to deeper neural networks with multiple layers?
- Basis in paper: [explicit] Authors suggest this direction for extension to RNNs and spiking neurons
- Why unresolved: Current theorems only address single-layer networks; multi-layer interactions aren't captured
- What evidence would resolve it: Deriving analogous theorems for multi-layer networks or experimental validation in deeper FF networks

### Open Question 2
- Question: What is the intuitive geometric interpretation of Theorem 1 and 2 conditions?
- Basis in paper: [explicit] Authors want to interpret intuitive meaning and explain why satisfied by most MNIST/CIFAR10 data
- Why unresolved: Theorems presented mathematically with cosine similarities and norms lacking clear geometric intuition
- What evidence would resolve it: Geometric explanation relating conditions to data alignment in activation space, potentially with visualizations

### Open Question 3
- Question: How does negative sample generation strategy affect sparsity emergence?
- Basis in paper: [explicit] Authors suggest investigating principles of negative data design based on Theorem 2
- Why unresolved: Current experiments use specific strategy (random wrong labels), but theorems suggest outcome depends on positive/negative relationship
- What evidence would resolve it: Systematic comparison of different negative sampling strategies and their impact on sparsity patterns

## Limitations
- Theoretical analysis relies heavily on infinite width assumption which may not hold in practical finite-width networks
- Current theorems only address single-layer networks, limiting applicability to deeper architectures
- Mechanism by which Theorem 2's conditions guarantee sparsity increase requires further empirical validation

## Confidence
**High Confidence**: Empirical verification of Theorem 1 (99.98% satisfaction) and observed sparsity increase during FF training are well-supported.

**Medium Confidence**: Mathematical framework connecting goodness gradients to sparsity changes is sound, but practical applicability of infinite-width assumption needs investigation.

**Low Confidence**: Claim that Theorem 2 conditions are sufficient for sparsity increase during complete FF training requires additional experimental validation.

## Next Checks
1. **Finite-width validation**: Test Theorem 1 and 2 predictions on networks with varying widths (n=64, 256, 512) to assess finite width impact.

2. **Alternative architectures**: Evaluate sparsity emergence mechanism extension to multi-layer FF networks and different activation functions beyond ReLU.

3. **Negative data sampling robustness**: Systematically compare Theorem 2 predictions and sparsity outcomes across different negative data generation strategies (random labels, adversarial examples, etc.).