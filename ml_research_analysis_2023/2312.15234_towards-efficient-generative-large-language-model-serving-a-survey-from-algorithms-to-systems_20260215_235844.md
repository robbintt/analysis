---
ver: rpa2
title: 'Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms
  to Systems'
arxiv_id: '2312.15234'
source_url: https://arxiv.org/abs/2312.15234
tags:
- arxiv
- inference
- preprint
- language
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of efficient serving
  methodologies for generative large language models (LLMs), addressing the significant
  computational and memory challenges these models present. It systematically examines
  a wide spectrum of solutions, including algorithmic innovations like non-autoregressive
  and speculative decoding, architecture design optimizations such as attention simplification
  and conditional computing, model compression techniques including knowledge distillation
  and pruning, and system-level optimizations like low-bit quantization, parallel
  computation, and request scheduling.
---

# Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems

## Quick Facts
- arXiv ID: 2312.15234
- Source URL: https://arxiv.org/abs/2312.15234
- Reference count: 40
- Primary result: Comprehensive survey of LLM serving optimizations covering algorithms, architectures, model compression, and system-level improvements

## Executive Summary
This survey systematically examines the challenges and solutions for efficient serving of generative large language models (LLMs), which face significant computational and memory constraints during inference. The paper categorizes optimization techniques into algorithmic innovations (non-autoregressive decoding, speculative decoding), architecture optimizations (attention simplification, conditional computing), model compression methods (knowledge distillation, pruning), and system-level optimizations (quantization, parallel computation, request scheduling). By surveying representative open-source LLM serving systems and their optimization approaches, the work provides a comprehensive roadmap for researchers and practitioners to understand current capabilities and identify future research directions in highly efficient LLM deployment.

## Method Summary
This paper conducts a comprehensive literature survey of efficient LLM serving techniques, systematically categorizing optimizations across algorithmic, architectural, model compression, and system-level dimensions. Rather than presenting new empirical results, the survey synthesizes findings from 40+ research papers to provide an organized taxonomy of approaches for reducing latency, improving throughput, and minimizing memory footprint in LLM inference systems. The methodology involves identifying key challenges in LLM serving, surveying representative open-source systems, and organizing optimization techniques into a coherent framework that highlights both individual approaches and their interconnections.

## Key Results
- LLM serving requires balancing low latency and high throughput while managing substantial memory requirements
- Speculative decoding reduces latency by using draft models to generate multiple tokens in parallel, verified by the full LLM
- Paged attention improves memory utilization by organizing KV cache into non-contiguous pages, reducing fragmentation
- Model parallelism distributes large model computations across multiple GPUs, enabling inference of models exceeding single-device memory capacity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speculative decoding can significantly reduce latency without modifying model outputs.
- Mechanism: A smaller draft model generates multiple candidate tokens in parallel, which are then verified by the full LLM. If verification fails, the original LLM generates the correct token(s).
- Core assumption: The draft model's predictions are sufficiently accurate to minimize fallback frequency, and verification can be performed efficiently in parallel.
- Evidence anchors:
  - [abstract] "Speculative decoding has been proposed to make decoding predictions of multiple steps first in an efficient manner (e.g., using a smaller draft model with fewer model parameters) and verify these predictions simultaneously with the LLM."
  - [section] "The main advantage of speculative decoding is that it increases the parallelism without any changes to the outputs."
- Break condition: If the draft model accuracy drops below a threshold, fallback frequency increases dramatically, negating latency benefits.

### Mechanism 2
- Claim: Paged attention reduces memory fragmentation and improves throughput by organizing KV cache into non-contiguous pages.
- Mechanism: Instead of allocating a single large contiguous memory block for KV cache, the system partitions it into pages that can be allocated and freed independently, reducing memory waste.
- Core assumption: Memory fragmentation is a significant bottleneck in LLM serving, particularly with variable sequence lengths.
- Evidence anchors:
  - [section] "vLLM [150] proposes paged attention that partitions the KV cache into non-contiguous memory blocks and significantly improves the batch size as well as throughput."
- Break condition: If page management overhead exceeds memory savings, or if the underlying memory allocator performs poorly with fragmented allocations.

### Mechanism 3
- Claim: Model parallelism across multiple GPUs can reduce inference latency by distributing computation of large models.
- Mechanism: Different model layers or tensor dimensions are partitioned across multiple devices, allowing parallel execution of model components.
- Core assumption: The communication overhead between devices is lower than the computational savings from parallel execution.
- Evidence anchors:
  - [section] "PaLM inference [203] extends TP on large-scale Transformer inference by involving 2D tensor parallelism [252] and claims lower theoretical communication complexity for large clusters (more than 256 devices)."
- Break condition: When network bandwidth becomes a bottleneck or when model size is too small to benefit from distributed computation.

## Foundational Learning

- Concept: Auto-regressive decoding mechanism
  - Why needed here: Understanding the default generation process is crucial for appreciating why optimizations like non-autoregressive decoding or speculative decoding are beneficial.
  - Quick check question: In auto-regressive decoding, how is each new token generated given the sequence generated so far?

- Concept: Transformer architecture fundamentals
  - Why needed here: The survey focuses on optimizing Transformer-based LLMs, so understanding self-attention, multi-head attention, and feed-forward networks is essential.
  - Quick check question: What is the computational complexity of self-attention with respect to sequence length?

- Concept: GPU architecture and parallelism
  - Why needed here: Most optimization techniques leverage GPU parallelism, so understanding streaming multiprocessors, shared memory, and tensor cores is important.
  - Quick check question: How do tensor cores in modern GPUs accelerate matrix multiplication operations?

## Architecture Onboarding

- Component map: Request scheduler -> Memory manager -> Execution engine -> Communication layer -> Optimization modules
- Critical path: Request → Scheduling → Memory allocation → Model execution → Response generation
- Design tradeoffs:
  - Memory vs. throughput: Paged attention improves memory utilization but adds management overhead
  - Accuracy vs. speed: Low-bit quantization reduces memory but may impact model quality
  - Latency vs. throughput: Speculative decoding reduces latency but may increase per-request computation
- Failure signatures:
  - Memory allocation failures: Request rejections or degraded throughput
  - Kernel launch failures: System crashes or degraded performance
  - Communication bottlenecks: Increased latency in distributed settings
- First 3 experiments:
  1. Measure baseline latency and throughput on a single GPU with default FasterTransformer implementation
  2. Implement and test paged attention with variable sequence lengths
  3. Evaluate speculative decoding with different draft model sizes and accuracy thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can non-autoregressive decoding be improved to match the output quality of autoregressive methods while maintaining speed benefits?
- Basis in paper: [explicit] The survey notes that non-autoregressive methods still face challenges in achieving output quality comparable to autoregressive decoding despite improvements in speed.
- Why unresolved: The conditional dependencies between output tokens are complex and difficult to model accurately without sequential processing.
- What evidence would resolve it: A non-autoregressive decoding method that achieves comparable quality metrics (e.g., BLEU, perplexity) to autoregressive decoding while demonstrating significant speed improvements on standard benchmarks.

### Open Question 2
- Question: What is the optimal trade-off between latency and throughput for LLM serving systems, and how can systems dynamically adapt to different workload requirements?
- Basis in paper: [explicit] The survey identifies low latency and high throughput as dual optimization targets that are often conflicting objectives, requiring a balanced strategy.
- Why unresolved: The optimal balance depends on specific use cases and workloads, and current systems typically optimize for one target at the expense of the other.
- What evidence would resolve it: A serving system that can dynamically adjust its optimization strategy based on real-time workload analysis while maintaining predictable performance bounds for both latency and throughput.

### Open Question 3
- Question: How can LLM serving systems effectively manage memory for variable-length sequences while minimizing overhead and maintaining performance?
- Basis in paper: [explicit] The survey highlights memory management challenges, particularly for KV cache, due to variable input/output lengths and parallel decoding scenarios.
- Why unresolved: Current memory management techniques either introduce significant overhead or fail to handle all edge cases effectively, especially when combined with other optimizations.
- What evidence would resolve it: A memory management scheme that significantly reduces memory usage for variable-length sequences without introducing measurable latency overhead or limiting batch sizes, validated across diverse workload patterns.

## Limitations

- The survey does not provide empirical benchmarking data to validate the claimed performance improvements of various optimization techniques
- The rapidly evolving nature of LLM serving technology means some techniques may become obsolete shortly after publication
- The survey lacks discussion of failure modes and edge cases for the various optimization techniques

## Confidence

**High Confidence**: The fundamental challenges of LLM serving (computational intensity, memory requirements, latency constraints) and the general categorization of optimization approaches are well-established and consistently reported across the literature.

**Medium Confidence**: The described mechanisms for individual optimization techniques (speculative decoding, paged attention, model parallelism) are accurately represented based on cited papers, but their relative effectiveness and practical implementation details vary significantly across different systems and workloads.

**Low Confidence**: Claims about the specific performance improvements or optimal configurations for various techniques, as these would require empirical validation across diverse scenarios not provided in this survey.

## Next Checks

1. **Empirical benchmarking**: Implement and compare at least three different LLM serving systems (e.g., vLLM, FasterTransformer, TensorRT-LLM) on the same hardware with identical models to measure actual latency, throughput, and memory usage differences.

2. **Speculative decoding accuracy threshold analysis**: Systematically vary the draft model size and accuracy thresholds to determine the break-even point where fallback frequency negates latency benefits across different model architectures and sequence lengths.

3. **Memory fragmentation impact study**: Conduct controlled experiments varying sequence lengths and batch sizes to quantify the actual memory utilization improvements from paged attention versus the overhead of page management across different GPU memory configurations.