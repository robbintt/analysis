---
ver: rpa2
title: An LLM Compiler for Parallel Function Calling
arxiv_id: '2312.04511'
source_url: https://arxiv.org/abs/2312.04511
tags:
- function
- llmcompiler
- tasks
- parallel
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents LLMCompiler, a framework that optimizes parallel
  function calling for Large Language Models (LLMs). Inspired by classical compilers,
  LLMCompiler enables efficient orchestration of multiple function calls by introducing
  three key components: an LLM Planner that generates execution strategies and dependencies,
  a Task Fetching Unit that dispatches and updates function calling tasks, and an
  Executor that executes these tasks in parallel.'
---

# An LLM Compiler for Parallel Function Calling

## Quick Facts
- arXiv ID: 2312.04511
- Source URL: https://arxiv.org/abs/2312.04511
- Reference count: 40
- One-line primary result: LLMCompiler achieves up to 3.7x latency speedup, 6.7x cost savings, and 9% accuracy improvement over ReAct in parallel function calling

## Executive Summary
LLMCompiler is a framework that optimizes parallel function calling for Large Language Models by automatically identifying inter-task dependencies and executing independent tasks concurrently. Inspired by classical compilers, it introduces three key components: an LLM Planner that generates execution strategies and dependencies, a Task Fetching Unit that dispatches and updates function calling tasks, and an Executor that executes these tasks in parallel. The framework can be used with both open-source models like LLaMA-2 and closed-source models like GPT, demonstrating significant improvements in latency, cost, and accuracy compared to sequential approaches.

## Method Summary
LLMCompiler works by first using an LLM Planner to analyze user input and generate a Directed Acyclic Graph (DAG) representing tasks and their dependencies. The Task Fetching Unit then dispatches tasks that have all dependencies resolved to the Executor, which runs them in parallel using the provided tools. The system supports streaming output to reduce latency by overlapping planning and execution phases. Users provide tool definitions and optional in-context examples, and LLMCompiler automatically computes an optimized orchestration for the function calls. The framework is evaluated on benchmarks including embarrassingly parallel tasks, tasks with complex dependencies, and tasks requiring dynamic replanning.

## Key Results
- Up to 3.7x latency speedup compared to ReAct framework
- Up to 6.7x cost savings in function calling operations
- Up to 9% accuracy improvement on tasks like Movie Recommendation
- 1.35x latency gain over OpenAI's parallel function calling feature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMCompiler achieves parallel execution by automatically identifying inter-task dependencies and executing independent tasks concurrently
- Mechanism: The LLM Planner component analyzes user input and generates a Directed Acyclic Graph (DAG) representing tasks and their dependencies. The Task Fetching Unit then dispatches tasks that have all dependencies resolved, allowing the Executor to run them in parallel
- Core assumption: The LLM can accurately identify task dependencies from natural language input through sophisticated reasoning
- Evidence anchors: [abstract] "LLMCompiler enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel" [section] "The LLM Planner is responsible for generating a sequence of tasks to be executed along with any dependency among them"
- Break condition: If the LLM Planner incorrectly identifies dependencies, tasks may execute in wrong order or fail to execute in parallel when possible

### Mechanism 2
- Claim: Separating reasoning from observation processing improves accuracy by preventing interference from intermediate results
- Mechanism: LLMCompiler separates the reasoning process (Planner) from the execution and observation phases, avoiding the concatenation of intermediate results into the prompt that can confuse the LLM
- Core assumption: Concatenating intermediate observations into the prompt creates noise that degrades reasoning performance
- Evidence anchors: [abstract] "LLMCompiler demonstrates on-par and better accuracy than ReAct† by up to 7% for Movie Recommendation, where we observe particularly frequent early stopping even with the ReAct-specific prompts" [section] "concatenating intermediate function results might affect the execution flow of the LLM, potentially reducing accuracy [60]"
- Break condition: If intermediate observations contain critical context needed for subsequent reasoning, separation might reduce accuracy

### Mechanism 3
- Claim: Streaming the Planner's output reduces latency by overlapping planning and execution phases
- Mechanism: Instead of waiting for the complete DAG, the Planner streams tasks to the Task Fetching Unit as they are generated, allowing execution to begin before planning completes
- Core assumption: Tasks can be planned and executed in an interleaved fashion without requiring complete upfront planning
- Evidence anchors: [section] "the Planner may incur a non-trivial overhead for user queries that involve a lot of tasks to be executed. This can be mitigated by enabling the Planner to asynchronously stream the dependency graph" [section] "This method allows each task to be immediately processed by the Task Fetching Unit and forwarded to the Executor as soon as all of its dependencies are resolved"
- Break condition: If task dependencies are complex and cannot be determined incrementally, streaming may produce invalid execution plans

## Foundational Learning

- Directed Acyclic Graph (DAG) fundamentals
  - Why needed here: Understanding DAGs is crucial for comprehending how LLMCompiler represents task dependencies and execution order
  - Quick check question: What property of a DAG makes it suitable for representing task execution order without cycles?

- Function calling in LLMs
  - Why needed here: The paper builds on existing function calling mechanisms, so understanding how LLMs invoke and use function outputs is essential
  - Quick check question: How does the ReAct framework handle function calling differently from LLMCompiler?

- Compiler optimization principles
  - Why needed here: The paper draws inspiration from classical compilers, particularly instruction scheduling and parallelization techniques
  - Quick check question: What classical compiler optimization technique is most analogous to LLMCompiler's parallel task execution?

## Architecture Onboarding

- Component map: User Input -> LLM Planner -> Task Fetching Unit -> Executor -> Final Answer
- Critical path: User input → LLM Planner → Task Fetching Unit → Executor → Final answer
  The bottleneck is typically the Planner when not using streaming, as it must generate the complete dependency graph before execution begins
- Design tradeoffs:
  - Parallel execution vs. sequential reasoning: LLMCompiler prioritizes parallelism but may sacrifice some dynamic reasoning capabilities
  - Planning overhead vs. execution efficiency: More sophisticated planning can enable better parallelization but increases upfront cost
  - Streaming vs. batch planning: Streaming reduces latency but may complicate dependency management
- Failure signatures:
  - Incorrect task ordering: Results from flawed dependency detection by the Planner
  - Deadlocks: Occur when tasks have circular dependencies that weren't properly detected
  - Resource contention: Parallel execution may exhaust available computational resources
- First 3 experiments:
  1. Run a simple embarrassingly parallel task (like HotpotQA comparison) to verify basic functionality
  2. Test with a task that has simple dependencies to verify dependency resolution works correctly
  3. Measure latency improvement with streaming enabled vs disabled on a medium-complexity task

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond those already discussed in the limitations and confidence sections.

## Limitations
- Evaluation relies heavily on synthetic benchmarks with controlled conditions
- Accuracy improvements are context-dependent and may not generalize to all reasoning patterns
- Cost savings analysis assumes specific pricing models that may not hold across all deployment scenarios

## Confidence
- High Confidence: The latency improvement claims are well-supported by controlled experiments comparing against ReAct and OpenAI's parallel function calling. The 3.7x speedup is consistently demonstrated across multiple benchmarks with clear methodology.
- Medium Confidence: The accuracy improvements are context-dependent. While the paper shows up to 7% accuracy gains on Movie Recommendation, this appears to stem from reducing the "early stopping" problem in ReAct rather than fundamental improvements in reasoning capability.
- Low Confidence: The cost savings analysis (up to 6.7x) assumes specific pricing models and may not hold for self-hosted models or different API pricing structures.

## Next Checks
1. **Robustness Testing**: Evaluate LLMCompiler on out-of-distribution queries where task dependencies are ambiguous or contradictory. Measure how often the Planner generates invalid dependency graphs that cause execution failures.
2. **Cost Analysis Under Different Pricing Models**: Replicate the cost analysis using different LLM API pricing structures (including self-hosted deployments) to verify the claimed cost savings hold across scenarios.
3. **Streaming Overhead Measurement**: Instrument the streaming implementation to measure the overhead of managing partial DAGs versus batch planning, particularly for tasks with complex interdependencies.