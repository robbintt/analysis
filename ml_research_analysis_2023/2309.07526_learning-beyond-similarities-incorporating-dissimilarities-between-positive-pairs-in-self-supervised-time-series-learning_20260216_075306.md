---
ver: rpa2
title: 'Learning Beyond Similarities: Incorporating Dissimilarities between Positive
  Pairs in Self-Supervised Time Series Learning'
arxiv_id: '2309.07526'
source_url: https://arxiv.org/abs/2309.07526
tags:
- data
- debs
- dynamic
- learning
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEBS introduces the first SSL method that incorporates dissimilarities
  between positive pairs to capture both static and dynamic features in time series
  data. Applied to ECG signals for AFib detection, it achieves +10% improvement in
  accuracy over existing SSL methods.
---

# Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning

## Quick Facts
- arXiv ID: 2309.07526
- Source URL: https://arxiv.org/abs/2309.07526
- Reference count: 0
- Primary result: +10% improvement in AFib detection accuracy using a dual-path SSL method that incorporates dissimilarities between positive pairs

## Executive Summary
DEBS introduces the first self-supervised learning method that explicitly incorporates dissimilarities between positive pairs to capture both static and dynamic features in time series data. The method achieves a notable +10% improvement in AFib detection accuracy on ECG signals compared to existing SSL techniques. By employing two distinct projection paths—one for similarity and another for dissimilarity—the model can encode both subject-specific characteristics and temporal dynamics simultaneously.

## Method Summary
DEBS uses a Vision Transformer (ViT) encoder with two parallel projection paths: a similarity path optimized with standard BYOL loss and a dissimilarity path incorporating both direct dissimilarity loss and gradual loss. The gradual loss ensures temporal coherence by constraining intermediate representations to lie between temporally adjacent states through linear interpolation. The model is trained on SHHS dataset for 25,000 iterations, with dissimilarities incorporated after 15,000 iterations using a coefficient α=0.1.

## Key Results
- Achieves +10% improvement in AFib detection accuracy over existing SSL methods
- Outperforms state-of-the-art SSL techniques including PCLR, Mixing-Up, and TF-C
- Demonstrates effectiveness across multiple ECG databases (MIT-ARR, MIT-AFIB, CINC2017)
- Shows superior performance in capturing temporal dynamics compared to similarity-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating dissimilarity objectives alongside similarity allows the model to encode dynamic temporal features rather than just static subject-specific characteristics.
- Mechanism: The model uses two parallel projection paths: one optimized for similarity (capturing static features) and one for dissimilarity (capturing dynamic transitions). The dissimilarity path includes both a direct dissimilarity loss and a gradual loss that ensures intermediate representations lie between temporally adjacent states.
- Core assumption: Static features remain constant across the time window while dynamic features change, and both can be encoded separately but simultaneously.
- Evidence anchors:
  - [abstract] "by integrating dissimilarities among positive pairs... leads to a notable enhancement of +10% in the detection accuracy"
  - [section] "the second path to capture the dynamic features"
- Break condition: If the time window is too large and contains multiple state transitions, the gradual loss assumption breaks down as intermediate representations cannot be meaningfully interpolated between non-adjacent states.

### Mechanism 2
- Claim: Using non-contrastive learning (no negative pairs) enables the incorporation of dissimilarity among positive pairs without mode collapse.
- Mechanism: The model follows a BYOL-like framework with teacher-student architecture and exponential moving average updates, but introduces two projectors per network. The absence of negative pairs removes the need to push dissimilar representations apart, allowing the model to focus dissimilarity objectives on temporal dynamics within positive pairs.
- Core assumption: Mode collapse can be avoided through EMA-based teacher updates without requiring negative pairs for contrastive repulsion.
- Evidence anchors:
  - [section] "Non-Contrastive Method: The omission of the negative pairs enable the proposed SSL methods to surpass the conventional emphasis on similarity"
  - [section] "As Bootstrap Your Own Latent (BYOL) [8] framework, DEBS incorporates both a teacher network and a student network"
- Break condition: If EMA updates become too slow relative to training dynamics, the teacher network may not provide stable targets, causing instability in the dissimilarity learning process.

### Mechanism 3
- Claim: The gradual loss ensures temporal coherence by constraining intermediate representations to lie between temporally adjacent states.
- Mechanism: The gradual loss computes a pondered average representation (PAR) between two temporally separated states and penalizes the intermediate representation for deviating from this path, ensuring smooth temporal evolution in the latent space.
- Core assumption: Temporal transitions are smooth and can be approximated by linear interpolation in the latent space.
- Evidence anchors:
  - [section] "we introduce the 'Gradual Loss (Lgra)'... for the representation of Xt to lie between them"
  - [section] "PAR(zt t−i, zt t+j) = zt t−i · j + zt t+j · i / (i + j)"
- Break condition: If the signal contains abrupt transitions or non-linear dynamics, the linear interpolation assumption fails and the gradual loss may force representations away from optimal encoding.

## Foundational Learning

- Concept: Self-Supervised Learning without labels
  - Why needed here: ECG signals lack consistent, easily obtainable labels for training, making SSL essential for leveraging large unlabeled datasets
  - Quick check question: What distinguishes self-supervised learning from supervised learning in terms of training signal availability?

- Concept: Temporal data augmentation through natural views
  - Why needed here: Instead of artificial augmentation, using naturally occurring time-separated segments from the same subject provides organic positive pairs
  - Quick check question: Why might natural temporal views be preferable to artificial augmentation for physiological signals?

- Concept: Vision Transformer adaptation for 1D signals
  - Why needed here: The model uses a ViT architecture adapted for time series by treating signal patches as "tokens," requiring understanding of both ViT mechanics and time series preprocessing
  - Quick check question: How does the patch size of 20 samples affect the model's ability to capture temporal dependencies?

## Architecture Onboarding

- Component map: Encoder (ViT-based) → Projector 1 (similarity path) → Predictor 1 → Similarity loss → Projector 2 (dissimilarity path) → Predictor 2 → Dissimilarity loss + Gradual loss → EMA-updated teacher network
- Critical path: Input → Encoder → Both projectors simultaneously → Separate predictors → Combined loss optimization
- Design tradeoffs: Using two projectors increases parameter count but enables separate encoding of static/dynamic features; non-contrastive approach avoids negative pairs but requires careful EMA tuning
- Failure signatures: Performance plateaus early (similarity path dominates), high variance in training loss (EMA instability), degradation when incorporating dissimilarities (window size too large)
- First 3 experiments:
  1. Train with only similarity path to establish baseline performance and verify the encoder works
  2. Add dissimilarity path with α=0.1 and small window size to test basic functionality without overwhelming the similarity objective
  3. Vary the dissimilarity coefficient α systematically to find optimal balance between static and dynamic feature encoding

## Open Questions the Paper Calls Out

- Question: What is the optimal window size for incorporating dissimilarities in DEBS to balance capturing dynamic changes and maintaining static features?
- Basis in paper: [inferred] The paper discusses the importance of the window size in implementing DEBS, indicating it must be large enough to accommodate signal changes but narrow enough to contain only a single one.
- Why unresolved: The paper suggests that the spacing window size is crucial but does not provide a specific method or empirical evidence for determining the optimal window size.
- What evidence would resolve it: Empirical studies or theoretical analysis that provide guidelines or optimal values for the window size in different types of time series data.

- Question: How does the inclusion of dissimilarities in DEBS affect the model's ability to generalize across different types of time series data beyond ECG signals?
- Basis in paper: [explicit] The paper demonstrates DEBS's effectiveness on ECG signals for AFib detection but suggests the strategy could be applied to other temporal data.
- Why unresolved: The paper does not provide evidence or experiments on the application of DEBS to other types of time series data.
- What evidence would resolve it: Experimental results showing DEBS's performance on various types of time series data, such as financial data, speech signals, or other physiological signals.

- Question: What are the implications of the choice of the dissimilarity coefficient (α) on the model's performance and learning dynamics?
- Basis in paper: [explicit] The paper introduces the dissimilarity coefficient (α) as a regularization factor but does not explore its impact on the model's performance.
- Why unresolved: The paper does not provide a detailed analysis of how different values of α affect the learning process or the final model performance.
- What evidence would resolve it: Sensitivity analysis or ablation studies showing how varying α influences the model's accuracy, convergence speed, and the quality of learned representations.

## Limitations

- The theoretical foundation connecting dissimilarity objectives to dynamic feature learning remains underdeveloped
- Optimal window size for temporal sampling is not specified and may vary significantly across different types of time series data
- The method's generalization to non-ECG time series data has not been empirically validated

## Confidence

- **High**: The empirical results showing DEBS outperforming existing SSL methods on multiple ECG databases
- **Medium**: The architectural design with dual paths for similarity and dissimilarity learning
- **Low**: The theoretical justification for how dissimilarity among positive pairs specifically captures dynamic features rather than introducing noise

## Next Checks

1. **Ablation study**: Train DEBS with only the dissimilarity path (no similarity path) to determine if dynamic features can be learned independently and to isolate the contribution of each path.

2. **Window size sensitivity**: Systematically vary the temporal window size for positive pair selection to identify the range where gradual loss assumptions remain valid and to find optimal window parameters.

3. **Domain transfer evaluation**: Test DEBS on non-ECG time series datasets (e.g., motion sensors, audio) to verify whether the dual-path architecture generalizes beyond cardiac signals and if the dynamic feature encoding is domain-agnostic.