---
ver: rpa2
title: 'Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning Classification'
arxiv_id: '2310.16293'
source_url: https://arxiv.org/abs/2310.16293
tags:
- label
- workers
- each
- labels
- worker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Crowd-Certain, a novel label aggregation
  technique for crowdsourced and ensemble learning classification tasks. The method
  uses the consistency of annotators versus a trained classifier to determine a reliability
  score for each annotator.
---

# Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning Classification

## Quick Facts
- arXiv ID: 2310.16293
- Source URL: https://arxiv.org/abs/2310.16293
- Reference count: 40
- Key outcome: Novel label aggregation technique using classifier ensemble variance and penalization against majority vote, outperforming 10 existing methods across 10 datasets

## Executive Summary
This paper introduces Crowd-Certain, a novel label aggregation technique for crowdsourced and ensemble learning classification tasks. The method uses classifier ensemble variance instead of data augmentation to measure worker uncertainty, enabling uncertainty quantification on non-image feature vectors. By penalizing workers who disagree with majority vote and leveraging predicted probabilities for classifier reuse, Crowd-Certain achieves superior performance compared to ten existing aggregation methods across multiple datasets and evaluation metrics.

## Method Summary
Crowd-Certain trains G different classifiers per worker on their labeled data, then measures disagreement across classifier outputs to quantify uncertainty. This uncertainty informs consistency scores for each worker-instance pair. The method penalizes workers who disagree with majority vote labels and uses weighted soft majority voting for final label aggregation. By employing predicted probabilities rather than raw worker labels, Crowd-Certain enables classifier reuse on future data samples without requiring additional crowdsourcing.

## Key Results
- Outperforms 10 existing techniques (Tao, Sheng, KOS, MACE, MajorityVote, MMSR, Wawa, Zero-Based Skill, GLAD, and Dawid Skene) across 10 datasets
- Achieves higher average accuracy, F1 scores, and AUC rates in nearly all scenarios
- Delivers higher Brier Score and lower Expected Calibration Error (ECE) across majority of datasets, indicating better calibrated results

## Why This Works (Mechanism)

### Mechanism 1: Classifier Ensemble Variance for Uncertainty
Using classifier ensemble variance instead of repeated spatial transformations enables uncertainty measurement on non-image feature vectors. The method trains G different classifiers per worker and measures disagreement across classifier outputs for each instance. Core assumption: Different random seeds or model architectures capture sufficient variation in predictions to reveal worker uncertainty patterns. Break condition: If classifier diversity is insufficient, variance becomes uninformative and uncertainty estimates collapse.

### Mechanism 2: Penalized Consistency Scoring
Penalizing workers for disagreeing with majority vote labels filters out consistently wrong but consistent workers. After computing consistency scores from uncertainty, the method sets weights to zero when workers disagree with majority vote. Core assumption: Majority vote provides a reasonable proxy for ground truth when no true labels are available. Break condition: If majority vote itself is highly unreliable, penalization may remove actually correct workers.

### Mechanism 3: Classifier-Based Label Aggregation
Using predicted probabilities instead of raw worker labels enables classifier reuse on future data without re-running crowdsourcing. The method trains classifiers per worker, then uses these classifiers to predict probabilities on new instances. Core assumption: Trained classifiers capture worker labeling patterns sufficiently well to generalize to new instances. Break condition: If worker labeling patterns change over time or differ significantly between training and test distributions, classifier predictions become unreliable.

## Foundational Learning

- **Uncertainty quantification through ensemble variance**: Why needed here? Replaces expensive data augmentation with classifier ensemble variance to measure worker uncertainty on feature vectors rather than images. Quick check: Why can't we simply use the entropy of worker label distributions to measure uncertainty?
- **Penalized consistency scoring**: Why needed here? Simple consistency scoring would give high scores to consistently wrong workers; penalization against majority vote removes this bias. Quick check: What happens to workers who are always wrong but in the same way as other workers?
- **Classifier-based label aggregation**: Why needed here? Enables future prediction without crowdsourcing by using trained models to generate pseudo-labels for new instances. Quick check: How does this differ from standard ensemble learning?

## Architecture Onboarding

- **Component map**: Data preprocessing → Worker label generation → Classifier training per worker → Uncertainty calculation → Consistency scoring → Weight computation → Label aggregation → Confidence scoring
- **Critical path**: Classifier training → Uncertainty measurement → Weight calculation → Label aggregation
- **Design tradeoffs**: Using G classifiers per worker increases computational cost but enables uncertainty estimation without data augmentation; penalization improves accuracy but may remove correct minority opinions
- **Failure signatures**: If all classifiers converge to same predictions, uncertainty collapses; if majority vote is unreliable, penalization removes correct workers; if worker patterns drift, classifier reuse fails
- **First 3 experiments**:
  1. Compare uncertainty measurement using ensemble variance vs. entropy on synthetic data with known worker reliability
  2. Test penalization effectiveness by creating datasets with consistently wrong but consistent workers
  3. Evaluate classifier reuse by training on one dataset distribution and testing on a different distribution

## Open Questions the Paper Calls Out

### Open Question 1
How does Crowd-Certain's performance scale with the number of annotators beyond the tested range (3-7)? The authors tested with 3-7 annotators but did not explore higher numbers, which is common in many real-world applications. Additional experiments testing with 8+ annotators would resolve this.

### Open Question 2
How does Crowd-Certain perform on highly imbalanced datasets compared to balanced ones? The paper mentions F1 score as a metric useful for imbalanced datasets but does not specifically analyze performance on imbalanced data. Experiments using datasets with varying degrees of class imbalance would resolve this.

### Open Question 3
How sensitive is Crowd-Certain to the choice of classifier used in the ensemble? The authors mention using random forests but note that other classifiers could be used, without providing detailed analysis of sensitivity to classifier choice. Experiments comparing performance using different classifier types would resolve this.

## Limitations
- Reliance on classifier diversity may not hold across all datasets or model architectures
- Penalization assumes majority vote provides reasonable ground truth, which may fail with small worker pools
- Classifier reuse assumes worker patterns remain stable over time, potentially limiting real-world applicability

## Confidence

### Confidence Assessment
- **High confidence**: Empirical performance claims due to extensive testing across ten datasets with multiple baselines
- **Medium confidence**: Theoretical mechanisms, as corpus lacks direct supporting literature for the three core innovations
- **Low confidence**: Scalability claims, as computational costs of training multiple classifiers per worker are not thoroughly analyzed

## Next Checks

1. Test ensemble variance sensitivity by varying the number of classifiers (G) and model architectures to determine minimum diversity requirements for reliable uncertainty estimation.

2. Evaluate penalization robustness by creating synthetic datasets with varying degrees of majority vote reliability and measuring impact on worker weight estimation.

3. Assess classifier reuse effectiveness by training on one dataset distribution and evaluating performance on temporally or distributionally shifted test data.