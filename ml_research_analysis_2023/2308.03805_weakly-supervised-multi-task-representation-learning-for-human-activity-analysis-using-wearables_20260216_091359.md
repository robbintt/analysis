---
ver: rpa2
title: Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis
  Using Wearables
arxiv_id: '2308.03805'
source_url: https://arxiv.org/abs/2308.03805
tags:
- data
- activity
- person
- multi-task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a weakly supervised multi-task learning framework
  for human activity recognition and person identification using wearable sensors.
  The approach uses a multi-output siamese network with temporal convolutions to learn
  shared representations for multiple tasks, trained using only similarity information
  between data pairs.
---

# Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables

## Quick Facts
- arXiv ID: 2308.03805
- Source URL: https://arxiv.org/abs/2308.03805
- Reference count: 40
- Key outcome: Achieves 98.93% F1-score for activity recognition and 98.81% for person identification on PAMAP2 dataset using only similarity pairs

## Executive Summary
This paper presents a weakly supervised multi-task learning framework for human activity recognition and person identification using wearable sensors. The approach uses a multi-output siamese network with temporal convolutions to learn shared representations for multiple tasks, trained using only similarity information between data pairs. The method learns to map sensor data into task-specific representation spaces where similar data points are close together. Experiments on four public datasets show the model achieves competitive performance compared to fully supervised single-task methods while reducing labeling requirements.

## Method Summary
The proposed framework uses a multi-output siamese network architecture where pairs of sensor data sequences are processed through shared temporal convolutional networks (TCNs) to extract temporal features. These features are then passed through separate fully connected layers to learn two distinct representation spaces - one for activity recognition and another for person identification. The model is trained using contrastive loss functions that optimize the distance between representations based on their semantic similarity, requiring only binary similarity information (same/different activity, same/different person) rather than explicit labels. The architecture is designed to scale to additional tasks and benefits from shared representation learning across related tasks.

## Key Results
- Achieves 98.93% F1-score for activity recognition and 98.81% for person identification on PAMAP2 dataset
- Outperforms fully supervised single-task methods on activity clustering tasks
- Demonstrates effective learning with only partial similarity information available
- Successfully scales to three-task learning (activity, person, and gender identification)

## Why This Works (Mechanism)

### Mechanism 1
- Multi-task siamese networks learn separate representation spaces for activity and person identity using only similarity pairs, without explicit labels
- Shared TCN layers extract temporal features, then split into separate FC layers for task-specific representations
- Contrastive loss positions similar pairs close together and dissimilar pairs apart in each space
- Core assumption: Temporal patterns in sensor data are sufficient to distinguish both activities and persons when trained with appropriate similarity pairs

### Mechanism 2
- Weak supervision (similarity pairs only) achieves competitive performance compared to fully supervised methods
- Binary similarity relationships contain enough information to learn separable representations through contrastive loss optimization
- Model reduces labeling burden while still learning discriminative representations
- Core assumption: Binary similarity relationships contain enough information to learn separable representations for both tasks simultaneously

### Mechanism 3
- Multi-task learning improves performance on individual tasks through shared representation learning
- Shared TCN layers learn general temporal features that benefit both activity recognition and person identification
- Task-specific FC layers specialize these features for each task
- Cross-task regularization prevents overfitting and improves generalization

## Foundational Learning

- **Concept: Temporal Convolutional Networks (TCNs)**
  - Why needed here: TCNs efficiently capture temporal patterns in sequential sensor data at multiple time scales through dilated convolutions and residual connections
  - Quick check question: How does the receptive field size in TCNs grow with depth, and why is this important for activity recognition?

- **Concept: Siamese Network Architecture**
  - Why needed here: Siamese networks enable learning similarity metrics by comparing pairs of inputs, allowing training with only similarity/dissimilarity information rather than explicit labels
  - Quick check question: What role does the contrastive loss function play in positioning representation vectors in the learned spaces?

- **Concept: Contrastive Loss for Representation Learning**
  - Why needed here: Contrastive loss directly optimizes the distance between representations based on semantic similarity, enabling weak supervision
  - Quick check question: How does the margin hyperparameter in contrastive loss affect the separation between positive and negative pairs?

## Architecture Onboarding

- **Component map**: Input pair ‚Üí Shared TCN blocks ‚Üí Split FC layers (activity & person) ‚Üí Dual contrastive losses ‚Üí Backpropagation
- **Critical path**: Input ‚Üí TCN blocks ‚Üí Split FC layers ‚Üí Contrastive loss ‚Üí Backpropagation
- **Design tradeoffs**:
  - Shared vs. separate TCNs: Shared weights reduce parameters but may limit task-specific feature learning
  - Number of TCN blocks: More blocks increase temporal context but risk overfitting
  - Task weighting: Balancing ùõº and ùõΩ affects convergence and final performance
- **Failure signatures**:
  - Poor activity clustering: TCN blocks not capturing relevant temporal patterns
  - Poor person identification: Person-specific features not separable across activities
  - Training instability: Learning rate too high or margin hyperparameter poorly chosen
- **First 3 experiments**:
  1. Train on PAMAP2 with only activity similarity pairs, evaluate activity clustering F1-score
  2. Train on PAMAP2 with only person similarity pairs, evaluate person identification accuracy
  3. Train on PAMAP2 with both similarity types, compare to single-task baselines and analyze learned representation spaces via t-SNE visualization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed weakly supervised multi-task learning framework scale to datasets with more than three tasks, and what is the impact on performance?
- **Basis in paper**: The paper mentions that the model is "easily extendable to larger numbers of tasks" and evaluates it with two and three tasks, but does not explore the scalability limits or performance degradation with additional tasks
- **Why unresolved**: The paper only tests the framework with up to three tasks, leaving the performance and scalability for larger numbers of tasks unexplored
- **What evidence would resolve it**: Experiments testing the model with four or more tasks, measuring performance metrics like F1-score and clustering accuracy, and analyzing the computational overhead and potential performance degradation

### Open Question 2
- **Question**: What is the optimal number of TCN blocks and feature maps for different datasets and tasks, and how does this affect the model's performance and computational efficiency?
- **Basis in paper**: The paper uses a fixed architecture with 3 TCN blocks and 128 feature maps across all experiments, but does not explore the impact of varying these hyperparameters on performance or efficiency
- **Why unresolved**: The paper does not provide an ablation study or sensitivity analysis on the number of TCN blocks and feature maps, leaving their optimal configuration for different scenarios unknown
- **What evidence would resolve it**: A systematic study varying the number of TCN blocks and feature maps, reporting performance metrics and computational costs, to identify optimal configurations for different datasets and tasks

### Open Question 3
- **Question**: How does the model's performance change when trained on real-world data with varying levels of noise, missing values, and sensor drift compared to the benchmark datasets used in the paper?
- **Basis in paper**: The paper evaluates the model on four public datasets with controlled conditions, but does not address its robustness to real-world data imperfections
- **Why unresolved**: The experiments use clean, well-labeled datasets, and the paper does not discuss or test the model's performance on noisy or imperfect real-world data
- **What evidence would resolve it**: Experiments applying the model to real-world datasets with known noise levels, missing data patterns, and sensor drift, measuring performance degradation and comparing it to baseline methods

## Limitations

- Limited exploration of hyperparameter sensitivity, particularly for contrastive loss margin and task weighting coefficients
- Performance degradation observed on datasets with high person-to-activity ratios (SBHAR, WISDM)
- No analysis of model robustness to real-world data imperfections such as noise, missing values, or sensor drift

## Confidence

- **High confidence**: The core siamese architecture and multi-task learning framework are well-specified and validated across multiple datasets
- **Medium confidence**: Weak supervision effectiveness is demonstrated but could benefit from more extensive analysis of similarity information requirements
- **Low confidence**: The generalizability to datasets with extreme class imbalance ratios remains uncertain

## Next Checks

1. Perform controlled ablation studies varying the proportion of available similarity pairs to quantify the minimum supervision required for effective learning
2. Test the framework on additional datasets with different activity-to-person ratios to establish performance bounds and identify failure modes
3. Analyze the learned representations using visualization techniques (t-SNE, UMAP) to understand how temporal patterns differentiate both activities and persons across the learned spaces