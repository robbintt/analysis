---
ver: rpa2
title: 'Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling'
arxiv_id: '2311.00430'
source_url: https://arxiv.org/abs/2311.00430
tags:
- whisper
- speech
- training
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Distil-Whisper, a distilled version of the
  Whisper model that achieves significant model compression and inference speed improvements
  while maintaining robustness to various acoustic conditions. The authors leverage
  pseudo-labelling on a large-scale open-source dataset and apply a simple word error
  rate (WER) heuristic to select high-quality pseudo-labels for training.
---

# Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling

## Quick Facts
- **arXiv ID**: 2311.00430
- **Source URL**: https://arxiv.org/abs/2311.00430
- **Reference count**: 40
- **One-line primary result**: Distil-Whisper achieves 5.8× speedup and 51% fewer parameters while maintaining robustness to various acoustic conditions.

## Executive Summary
This paper presents Distil-Whisper, a compressed version of the Whisper speech recognition model that achieves significant inference speed improvements while maintaining robustness to various acoustic conditions. The authors use knowledge distillation with pseudo-labelling on a large-scale open-source dataset (21,170 hours) and apply a WER-based filtering heuristic to select high-quality training data. The resulting model performs within 1% WER of the original Whisper while being 5.8 times faster and using 51% fewer parameters. Distil-Whisper demonstrates particular robustness to noisy environments and reduced hallucination errors on long-form audio.

## Method Summary
The authors employ a layer-based compression approach using knowledge distillation with a two-component loss function (KL divergence and pseudo-label loss). They generate pseudo-labels on 21,170 hours of open-source speech data using the Whisper large-v2 model, then filter these labels using a WER threshold of 10% to ensure high quality. The student model copies the full encoder from the teacher and freezes it during training, while the decoder is initialized with a subset of teacher decoder layers. For long-form audio, they implement a chunked transcription algorithm with overlapping segments. The training uses a weighted combination of KL divergence and pseudo-label loss objectives.

## Key Results
- Distil-Whisper achieves 5.8× inference speedup and 51% parameter reduction compared to Whisper
- Maintains within 1% WER on out-of-distribution test data while demonstrating robustness to noisy conditions
- Reduces hallucination errors on long-form audio through chunked transcription approach
- Can be paired with Whisper for speculative decoding, achieving 2× speed-up while maintaining identical outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-labelling with a large open-source dataset preserves robustness to acoustic conditions while enabling model compression.
- Mechanism: The teacher model generates pseudo-labels on a diverse, large-scale open-source dataset covering 10 distinct domains. These pseudo-labels serve as training targets for the smaller student model, transferring the teacher's robustness to various acoustic conditions without requiring manual annotation.
- Core assumption: The pseudo-labels generated by the teacher model accurately reflect the acoustic conditions present in the open-source dataset.
- Evidence anchors:
  - [abstract] "Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training."
  - [section 4.1] "We propose a simple word error rate (WER) based heuristic for filtering the pseudo-labelled data and demonstrate that it is an effective method for ensuring good downstream performance of the distilled model."
  - [corpus] Weak - the paper mentions using a WER threshold but does not explicitly validate the accuracy of the pseudo-labels themselves.
- Break condition: If the pseudo-labels contain significant errors or hallucinations, the student model will inherit these inaccuracies, leading to degraded performance and loss of robustness.

### Mechanism 2
- Claim: Distil-Whisper maintains robustness to noisy acoustic conditions by sharing the same encoder as the original Whisper model.
- Mechanism: The encoder of the student model is copied directly from the teacher and frozen during training. Since the encoder is responsible for extracting features from the audio input, this ensures that the student model inherits the teacher's ability to handle noisy environments.
- Core assumption: The encoder's ability to handle noisy audio is primarily determined by its architecture and weights, not the decoder.
- Evidence anchors:
  - [section 8.3] "Since we copy the full encoder and freeze it during training, the student and teacher models share the same encoder. Thus, they show similar robustness to noise, particularly under more natural distribution shifts like pub noise."
  - [section 7.2] "We initialise the student models by copying the entire encoder from the teacher and freeze it during training."
  - [corpus] Weak - the paper demonstrates robustness on the LibriSpeech dataset with added noise, but does not provide direct evidence of the encoder's role in noise robustness.
- Break condition: If the decoder significantly impacts the model's ability to handle noise, then simply copying the encoder may not be sufficient to maintain robustness.

### Mechanism 3
- Claim: Distil-Whisper reduces hallucination errors on long-form audio by using a chunked transcription algorithm.
- Mechanism: The chunked algorithm processes long audio files by dividing them into smaller segments with overlapping regions. This allows the model to transcribe each segment independently, reducing the likelihood of accumulating errors or generating hallucinations over long sequences.
- Core assumption: The chunked algorithm effectively prevents the model from generating hallucinations by limiting the context window.
- Evidence anchors:
  - [section 5] "We use an alternative strategy, first proposed by Patry (2022), in which the long audio file is chunked into smaller segments with a small overlap between adjacent segments."
  - [section 8.4] "Table 8 reports the number of repeated 5-gram word duplicates (5-Dup.) and insertion error rate (IER) metrics averaged over the four long-form test sets. In addition to the Whisper and Distil-Whisper models, we report the results for the official Wav2Vec 2.0 large model fine-tuned on 960 hours of LibriSpeech data."
  - [corpus] Weak - the paper demonstrates a reduction in hallucinations with Distil-Whisper, but does not provide direct evidence that the chunked algorithm is the primary cause.
- Break condition: If the model still hallucinates within individual chunks, then the chunked algorithm will not prevent hallucinations in the final transcription.

## Foundational Learning

- **Knowledge Distillation (KD)**
  - Why needed here: KD is the core technique used to compress the Whisper model while preserving its performance.
  - Quick check question: What are the two main components of the KD objective used in this paper, and what do they represent?

- **Pseudo-labelling**
  - Why needed here: Pseudo-labelling is used to generate training targets for the student model without requiring manual annotation.
  - Quick check question: How does the WER threshold heuristic improve the quality of the pseudo-labels used for training?

- **Sequence-to-Sequence (Seq2Seq) models**
  - Why needed here: Whisper is a Seq2Seq model, and understanding its architecture is crucial for understanding the distillation process.
  - Quick check question: What are the two main components of a Seq2Seq model, and what are their respective roles in speech recognition?

## Architecture Onboarding

- **Component map**: Audio input → Encoder (copied from Whisper, frozen) → Decoder (subset of Whisper layers) → Transcription output
- **Critical path**: Audio input → Encoder → Decoder → Transcription output
- **Design tradeoffs**:
  - Model size vs. performance: Distil-Whisper is smaller and faster than Whisper but may have slightly lower accuracy.
  - Encoder sharing vs. independent training: Sharing the encoder ensures robustness to noise but limits the potential for further compression.
  - Chunked vs. sequential transcription: Chunked transcription is faster but may introduce errors at chunk boundaries.
- **Failure signatures**:
  - High WER: Indicates that the student model is not effectively learning from the teacher's pseudo-labels.
  - Hallucinations: Suggests that the model is generating text that is not present in the audio input, potentially due to errors in the pseudo-labels or the chunked transcription algorithm.
  - Slow inference: May indicate that the model is not efficiently utilizing the hardware or that the chunked algorithm is not optimized.
- **First 3 experiments**:
  1. Train Distil-Whisper on a subset of the open-source dataset with varying WER thresholds to determine the optimal filtering strategy.
  2. Evaluate the robustness of Distil-Whisper to different types of noise (e.g., white noise, pub noise) and compare it to the original Whisper model.
  3. Experiment with different chunk lengths for the chunked transcription algorithm to find the optimal balance between speed and accuracy for long-form audio.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implied by the limitations and scope of the work, including the scalability of performance with larger datasets, the potential for early exit strategies, and comparisons to alternative distillation methods.

## Limitations
- The WER-based filtering heuristic lacks validation of absolute pseudo-label quality
- The encoder-sharing approach assumes the decoder plays a minimal role in noise handling
- The chunked transcription algorithm parameters (overlap size, longest common sequence method) are underspecified
- The paper does not compare Distil-Whisper to alternative distillation methods or architectures

## Confidence

**High Confidence**: Claims about model compression ratios (5.8× speedup, 51% fewer parameters) and relative WER performance (within 1% on out-of-distribution test data). These are directly measurable and well-supported by the results tables.

**Medium Confidence**: Claims about noise robustness and hallucination reduction. While results show Distil-Whisper performs similarly to Whisper on noisy LibriSpeech data and has fewer hallucinations on long-form audio, the mechanisms (encoder sharing, chunked algorithm) are not independently validated.

**Low Confidence**: Claims about the effectiveness of the WER threshold filtering heuristic. The paper demonstrates that filtering works but doesn't establish what happens with different threshold values or validate the quality of the filtered pseudo-labels against ground truth.

## Next Checks

1. **Pseudo-label quality validation**: Take a random sample of pseudo-labels generated by Whisper on the open-source datasets and manually verify their accuracy against the ground truth. Measure the actual error rate of pseudo-labels that pass the WER threshold versus those that don't.

2. **Encoder contribution isolation**: Train two versions of Distil-Whisper - one with shared encoder (as proposed) and one with an independently trained encoder. Test both models on noisy audio conditions to determine whether encoder sharing is truly responsible for the noise robustness.

3. **Chunked algorithm parameter sensitivity**: Systematically vary the chunk overlap size (e.g., 0s, 0.5s, 1s, 2s) and measure the impact on hallucination metrics (5-Dup., IER) and overall WER. This would establish whether the chunked approach is robust to parameter choices or if specific values are critical.