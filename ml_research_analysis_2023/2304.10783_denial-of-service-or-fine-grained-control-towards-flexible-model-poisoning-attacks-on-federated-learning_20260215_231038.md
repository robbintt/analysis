---
ver: rpa2
title: 'Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning
  Attacks on Federated Learning'
arxiv_id: '2304.10783'
source_url: https://arxiv.org/abs/2304.10783
tags:
- attack
- poisoning
- malicious
- clients
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Flexible Model Poisoning Attack (FMPA), a
  novel approach to poisoning attacks in federated learning (FL) that overcomes limitations
  of existing methods. Unlike previous attacks that optimize perturbation magnitude
  along prescribed directions, FMPA uses historical global model information to predict
  a benign reference model and fine-tunes it to create poisoned models with low accuracy
  and small perturbations.
---

# Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning

## Quick Facts
- **arXiv ID**: 2304.10783
- **Source URL**: https://arxiv.org/abs/2304.10783
- **Reference count**: 6
- **Primary result**: FMPA achieves up to 3.5× better global accuracy reduction than six state-of-the-art attacks while maintaining small perturbations

## Executive Summary
This paper introduces Flexible Model Poisoning Attack (FMPA), a novel approach to poisoning attacks in federated learning that overcomes limitations of existing methods. Unlike previous attacks that optimize perturbation magnitude along prescribed directions, FMPA uses historical global model information to predict a benign reference model and fine-tunes it to create poisoned models with low accuracy and small perturbations. The method achieves versatile attack goals beyond denial-of-service, including precise control over accuracy degradation. Experiments show FMPA significantly outperforms six state-of-the-art attacks, reducing global accuracy by up to 3.5× more than competing methods.

## Method Summary
FMPA operates by first predicting the next round's benign global model using exponential smoothing of historical global models. This predicted reference model is then fine-tuned to minimize accuracy while keeping parameter changes small. The attack can target either denial-of-service (by reducing accuracy below a threshold) or fine-grained control (by setting a specific accuracy target). Updates are projected to stay within a small radius to evade Byzantine-robust defenses. The method requires only historical global model information and local datasets from malicious clients, making it practical and difficult to defend against.

## Key Results
- FMPA reduces global model accuracy by up to 3.5× more than six state-of-the-art attacks
- Achieves effective attacks with significantly smaller perturbations than competing methods
- Provides precise control over accuracy degradation through adjustable thresholds
- Works effectively against multiple Byzantine-robust aggregation defenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Historical global model information can predict the next round's benign global model with sufficient accuracy to serve as a reference for crafting poisoned updates
- Mechanism: FMPA uses exponential smoothing on historical global models to construct a predictive reference model that approximates what the global model would be without attacks
- Core assumption: The global model evolution follows a smooth trajectory that can be captured by exponential smoothing
- Evidence anchors:
  - [abstract]: "FMPA exploits the global historical information to construct an estimator that predicts the next round of the global model as a benign reference"
  - [section]: "We use exponential smoothing...to build a PRM. Assume that gt is global model in the t-th round...we calculate the estimatorˆgt+1 as the reference model"
- Break condition: If the global model trajectory becomes highly irregular or if the learning rate changes dramatically between rounds

### Mechanism 2
- Claim: Fine-tuning a benign reference model directly achieves better attack performance than optimizing perturbation magnitude along prescribed directions
- Mechanism: Instead of crafting poisoned updates by maximizing distance in a prescribed direction, FMPA directly optimizes the reference model to minimize accuracy while keeping perturbations small
- Core assumption: Direct optimization of model parameters is more effective than indirect perturbation-based approaches
- Evidence anchors:
  - [abstract]: "Unlike recent model poisoning attacks that optimize the amplitude of malicious perturbations along certain prescribed directions to cause DoS, we propose a Flexible Model Poisoning Attack (FMPA)"
  - [section]: "In contrast to optimizing for the best perturbation magnitude along a prescribed malicious direction...FMPA fine-tunes the reference model to directly obtain a malicious model with low accuracy and small perturbations"
- Break condition: If defenses become highly sensitive to any model parameter changes regardless of perturbation size

### Mechanism 3
- Claim: Controlling the accuracy threshold allows precise manipulation of global model performance degradation
- Mechanism: By setting a target accuracy threshold (τ) and fine-tuning until this threshold is met, the adversary can control exactly how much the global model's performance degrades
- Core assumption: The relationship between local model accuracy and global model accuracy is predictable enough to enable precise control
- Evidence anchors:
  - [abstract]: "Besides the goal of causing DoS, FMPA can be naturally extended to launch a fine-grained controllable attack, making it possible to precisely reduce the global accuracy"
  - [section]: "To perform our fine-grained control attack...the adversary will set τ to his desired accuracy level...and call Algorithm 1 to produce a malicious model"
- Break condition: If the aggregation algorithm introduces non-linear effects that make local-to-global accuracy mapping unpredictable

## Foundational Learning

- Concept: Federated Learning basics and threat models
  - Why needed here: Understanding how FL works and what assumptions adversaries typically make is crucial for understanding FMPA's novelty
  - Quick check question: What are the three main steps in each round of federated learning?

- Concept: Model poisoning vs data poisoning attacks
  - Why needed here: FMPA is a model poisoning attack, and understanding the difference from data poisoning is essential for grasping its unique characteristics
  - Quick check question: How does model poisoning differ from data poisoning in terms of what the adversary manipulates?

- Concept: Exponential smoothing and time series forecasting
  - Why needed here: FMPA uses exponential smoothing to predict the next global model, so understanding this technique is necessary to grasp how the reference model is constructed
  - Quick check question: What is the main advantage of using exponential smoothing for predicting the next global model in FL?

## Architecture Onboarding

- Component map: Global model storage -> Historical model tracking -> Exponential smoothing engine -> Reference model construction -> Fine-tuning module -> Attack validation -> Update projection
- Critical path: Global model reception -> Reference model prediction -> Model fine-tuning -> Accuracy validation -> Update projection -> Model submission
- Design tradeoffs: Accuracy vs stealth (smaller perturbations are stealthier but may be less effective), computational cost of fine-tuning vs attack effectiveness, prediction horizon length vs accuracy
- Failure signatures: High variance in attack success rate across different defenses, inability to meet accuracy thresholds consistently, updates being rejected by defenses despite small perturbations
- First 3 experiments:
  1. Test exponential smoothing prediction accuracy on historical global models from a running FL system
  2. Verify that fine-tuning reference models can achieve target accuracy thresholds on validation data
  3. Test attack effectiveness against a single known defense mechanism to establish baseline performance

## Open Questions the Paper Calls Out
- **Open Question 1**: How does FMPA's performance change when adversaries have access to only a small subset of the global model history (e.g., limited to the last 3-5 rounds)?
- **Open Question 2**: Can FMPA's fine-grained control mechanism be extended to achieve targeted attacks on specific model features rather than just overall accuracy reduction?
- **Open Question 3**: What is the theoretical lower bound on perturbation size that FMPA can achieve while maintaining attack effectiveness across different model architectures?

## Limitations
- The attack assumes relatively stable global model trajectories, which may not hold in practice with heterogeneous data distributions
- Computational overhead of fine-tuning reference models for each attack round could be prohibitive in resource-constrained FL environments
- Effectiveness against real-world Byzantine-robust defenses with certified robustness guarantees is not fully validated

## Confidence
- **High confidence**: The core mechanism of using historical global models for reference prediction and the mathematical formulation of the fine-tuning objective
- **Medium confidence**: The attack's practical effectiveness against production-grade defenses and its scalability to large models and datasets
- **Low confidence**: The attack's performance in highly dynamic FL environments with non-stationary data distributions and frequent learning rate adjustments

## Next Checks
1. **Defense Robustness Test**: Evaluate FMPA against adaptive Byzantine-robust defenses that incorporate model update validation with certified robustness guarantees, measuring both attack success rate and computational overhead.

2. **Dynamic Environment Validation**: Test the attack's effectiveness when benign client data distributions shift between rounds or when learning rates are adjusted dynamically, measuring reference model prediction accuracy degradation.

3. **Scalability Assessment**: Implement FMPA on larger models (ResNet-50 or larger) and datasets (ImageNet subset) to evaluate computational feasibility and attack effectiveness at realistic scale, measuring fine-tuning time and memory requirements.