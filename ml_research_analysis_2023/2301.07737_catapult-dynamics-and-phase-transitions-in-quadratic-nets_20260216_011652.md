---
ver: rpa2
title: Catapult Dynamics and Phase Transitions in Quadratic Nets
arxiv_id: '2301.07737'
source_url: https://arxiv.org/abs/2301.07737
tags:
- quadratic
- will
- phase
- catapult
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the catapult phase of neural networks trained
  with super-critical learning rates, where the training loss first grows exponentially
  before decreasing. The authors prove the existence of this phase in quadratic models
  and two-layer, homogeneous neural networks by showing the weight norm decreases
  whenever the loss becomes large.
---

# Catapult Dynamics and Phase Transitions in Quadratic Nets

## Quick Facts
- arXiv ID: 2301.07737
- Source URL: https://arxiv.org/abs/2301.07737
- Authors: 
- Reference count: 40
- Key outcome: This work studies the catapult phase of neural networks trained with super-critical learning rates, where the training loss first grows exponentially before decreasing. The authors prove the existence of this phase in quadratic models and two-layer, homogeneous neural networks by showing the weight norm decreases whenever the loss becomes large. They also empirically study ReLU networks at large learning rates, observing that the activation map becomes increasingly sparse as the learning rate increases. Despite increases in weight norm, models in the catapult phase generalize well. The authors conjecture this is due to the sparse activation map.

## Executive Summary
This paper investigates the dynamics of neural networks in the catapult phase, where training loss initially grows exponentially before decreasing when using super-critical learning rates. The authors provide theoretical proofs for the existence of this phase in quadratic models and two-layer, homogeneous neural networks by demonstrating that weight norm decreases whenever the loss becomes large. Empirically, they show that ReLU networks in this phase develop increasingly sparse activation maps, which may explain their surprising generalization capabilities despite large weight norms.

## Method Summary
The study uses full-batch gradient descent to train quadratic models and two-layer ReLU networks on synthetic data and two-class image datasets (MNIST, FMNIST, CIFAR-10). The authors analyze the weight norm dynamics and activation sparsity across varying super-critical learning rates. Theoretical analysis focuses on proving conditions for the existence of the catapult phase, while empirical work examines the relationship between learning rate, activation sparsity, and generalization performance.

## Key Results
- Proved existence of catapult phase in quadratic models and homogeneous MLPs by showing weight norm decreases when loss becomes large
- Demonstrated that ReLU networks exhibit increasingly sparse activation maps at super-critical learning rates
- Observed that models in catapult phase generalize well despite having large weight norms
- Established theoretical bounds on learning rates that ensure weight norm remains finite

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The weight norm decreases monotonically when the learning rate is below a critical threshold.
- Mechanism: The NTK is bounded by the weight norm, and the update equation for the weight norm has a sign that depends on the ratio of learning rate to NTK magnitude.
- Core assumption: The NTK scales linearly with the weight norm for the models studied (quadratic models and homogeneous MLPs).
- Evidence anchors:
  - [abstract] "we show that for a certain range of learning rates the weight norm decreases whenever the loss becomes large."
  - [section] "if η < 4/(Cθ²₀) then the weight norm decreases monotonically for all time."
  - [corpus] Weak: No direct neighbor evidence for monotonic weight norm decrease in the catapult phase.
- Break condition: The NTK no longer scales linearly with the weight norm, or the NTK becomes unbounded.

### Mechanism 2
- Claim: The loss can diverge if the weight norm grows without bound.
- Mechanism: The loss is a quadratic function of the weights, so if the weights grow large, the loss also grows large.
- Core assumption: The loss is a convex function of the weights.
- Evidence anchors:
  - [abstract] "we show that for a certain range of learning rates the weight norm decreases whenever the loss becomes large."
  - [section] "if the loss becomes large, the weight norm decreases."
  - [corpus] Weak: No direct neighbor evidence for loss divergence in the catapult phase.
- Break condition: The loss becomes non-convex, or the weights become bounded.

### Mechanism 3
- Claim: The activation map becomes sparse in the catapult phase.
- Mechanism: The ReLU activation function sets negative values to zero, so if the weights become large and negative, the activation map becomes sparse.
- Core assumption: The weights can become large and negative in the catapult phase.
- Evidence anchors:
  - [abstract] "we also empirically study learning rates beyond this theoretically derived range and show that the activation map of ReLU nets trained with super-critical learning rates becomes increasingly sparse as we increase the learning rate."
  - [section] "we conjecture that these models still generalize well because the activation map of ReLU nets becomes sparse in the catapult phase."
  - [corpus] Weak: No direct neighbor evidence for sparse activation maps in the catapult phase.
- Break condition: The weights become bounded, or the activation function changes.

## Foundational Learning

- Concept: Gradient descent dynamics in the catapult phase.
  - Why needed here: Understanding how the weight norm and loss evolve in the catapult phase is crucial for proving the existence of the phase and characterizing its properties.
  - Quick check question: What is the update equation for the weight norm in the catapult phase, and how does it depend on the learning rate and NTK?

- Concept: Neural tangent kernel (NTK) and its properties.
  - Why needed here: The NTK plays a key role in characterizing the learning dynamics of neural networks, and its properties determine whether the catapult phase exists.
  - Quick check question: How does the NTK scale with the weight norm in the models studied, and what is the critical value of the learning rate for the catapult phase?

- Concept: Sparsity and its effects on generalization.
  - Why needed here: The sparsity of the activation map in the catapult phase may explain why models trained in this phase generalize well, despite having large weight norms.
  - Quick check question: How does the sparsity of the activation map change as the learning rate increases in the catapult phase, and what is the relationship between sparsity and generalization?

## Architecture Onboarding

- Component map: Input -> Quadratic/Homogeneous MLP with ReLU -> MSE Loss
- Critical path: Weight norm evolution -> Loss dynamics -> Catapult phase existence -> Generalization performance
- Design tradeoffs: The tradeoff is between the learning rate and the magnitude of the weight norm. A larger learning rate can lead to faster convergence but may also cause the weight norm to grow large and the loss to diverge.
- Failure signatures: The model fails to converge if the weight norm grows without bound or the loss diverges. The model may also fail to generalize well if the activation map is not sparse.
- First 3 experiments:
  1. Train a quadratic model with bias on the toy dataset (x, y) = (1, 0) and vary the learning rate to observe the catapult phase.
  2. Train a homogeneous MLP with ReLU activation on the toy dataset and vary the learning rate to observe the catapult phase and sparse activation maps.
  3. Train a ReLU MLP on a two-class version of MNIST and vary the learning rate to observe the catapult phase and sparse activation maps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise conditions under which the catapult phase exists for ReLU networks with multiple datapoints?
- Basis in paper: [explicit] The authors mention this is an open question and note that their analysis for ReLU nets is restricted to 1d input data, while their empirical study uses multi-dimensional data.
- Why unresolved: The authors note that their analytic bounds for ReLU nets are restricted to 1d input data, and deriving bounds for higher dimensions remains an open question.
- What evidence would resolve it: Proving convergence bounds for ReLU nets with multi-dimensional input data would resolve this question.

### Open Question 2
- Question: Why do models trained in the catapult phase generalize well despite the increase in weight norm?
- Basis in paper: [explicit] The authors conjecture that models generalize well in the catapult phase because the activation map becomes increasingly sparse as the learning rate increases.
- Why unresolved: While the authors provide empirical evidence for increased sparsity in the activation map, a theoretical explanation for why this leads to better generalization is still needed.
- What evidence would resolve it: A theoretical proof linking the sparsity of the activation map to improved generalization in the catapult phase would resolve this question.

### Open Question 3
- Question: How can the bounds for the existence of the catapult phase be weakened to capture a larger portion of the catapult regime?
- Basis in paper: [inferred] The authors note that their bounds can be too strong because they are derived by imposing that the weight norm decreases monotonically, which is stronger than only requiring that the weight norm is finite.
- Why unresolved: The authors do not provide a method for deriving weaker bounds that still guarantee convergence.
- What evidence would resolve it: Deriving weaker bounds that still guarantee convergence but capture a larger portion of the catapult regime would resolve this question.

## Limitations
- Analysis is limited to specific model classes (quadratic and homogeneous networks)
- Empirical validation is restricted to two-class image datasets
- Lack of quantitative analysis of sparsity and its relationship to generalization

## Confidence
- High: Theoretical proof of catapult phase existence in quadratic models and homogeneous MLPs
- Medium: Claim that weight norm decreases monotonically in catapult phase, supported by theoretical analysis with specific assumptions
- Low: Empirical observation of sparse activation maps and conjecture about generalization, limited quantitative analysis

## Next Checks
1. Test catapult dynamics in architectures beyond quadratic and homogeneous networks
2. Quantify the relationship between activation sparsity and generalization across different learning rates
3. Evaluate catapult phase behavior on multi-class classification tasks