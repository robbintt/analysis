---
ver: rpa2
title: Contextual Pre-planning on Reward Machine Abstractions for Enhanced Transfer
  in Deep Reinforcement Learning
arxiv_id: '2307.05209'
source_url: https://arxiv.org/abs/2307.05209
tags:
- context
- c-prep
- transfer
- learning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Contextual Pre-Planning (C-PREP) improves transfer learning in
  deep reinforcement learning by representing tasks using reward machines (RMs) with
  shared symbolic transitions. C-PREP provides agents with optimal abstract transitions
  from RMs, rewarding progress toward them.
---

# Contextual Pre-planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.05209
- Source URL: https://arxiv.org/abs/2307.05209
- Authors: 
- Reference count: 40
- Primary result: C-PREP improves transfer learning in deep RL with 22.84%-42.31% better time-to-threshold and 11.86%-36.5% better jumpstart performance

## Executive Summary
C-PREP (Contextual Pre-Planning) enhances transfer learning in deep reinforcement learning by using reward machines (RMs) with shared symbolic transitions across tasks. The method provides agents with optimal abstract transitions from RMs and rewards them for achieving these transitions. C-PREP demonstrates significant improvements in sample efficiency and few-shot transfer compared to baseline methods, particularly in complex tasks requiring multiple subtasks. The approach also shows better generalization and requires fewer source contexts for training.

## Method Summary
C-PREP operates by generating task-specific reward machines with a shared symbol set across contexts. For each context, it computes optimal policies over the RM using value iteration, then provides the agent with the next desired symbolic transition from the current RM state. This symbolic transition is concatenated to the state representation and fed to the policy network. The method also applies potential-based reward shaping using the optimal RM state values to guide learning. C-PREP is designed as a modular context representation function that can be integrated into any transfer learning algorithm following the CMDP flow.

## Key Results
- C-PREP shows 22.84%-42.31% improvement in time-to-threshold compared to baselines
- Jumpstart performance improves by 11.86%-36.5% with C-PREP
- Better generalization achieved with fewer source contexts needed for training
- Performance gains are most pronounced in complex tasks requiring multiple subtasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C-PREP provides symbolic representations of optimal RM transitions that are shared across tasks, enabling reuse of prior knowledge.
- Mechanism: For each context, C-PREP generates an RM with a shared symbol set P, plans optimal paths via value iteration over abstract states, and provides the agent with the next desired abstract transition label from the current RM state. This symbolic transition label is concatenated to the state representation and fed to the policy network.
- Core assumption: Abstract transitions in RMs capture high-level task structure that transfers between contexts; the symbol set P is sufficiently expressive to represent shared subtasks while distinguishing context-specific ones.
- Evidence anchors:
  - [abstract]: "Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks..."
  - [section 3]: "Using Value Iteration (VI), we find an optimal policy in the RM... At each timestep, sample an optimal transition l ~ π*(·|u) given the current RM abstract state u and return it."
  - [corpus]: No direct evidence; assumption stated.
- Break condition: If the shared symbol set P is too coarse (cannot distinguish contexts) or too fine (blows up computation), transfer benefits diminish.

### Mechanism 2
- Claim: C-PREP reshapes rewards using potential-based shaping based on optimal RM state values, guiding agents toward optimal paths.
- Mechanism: The value iteration V*(u) computed over the RM is used as a potential function ϕ. Reward reshaping δ′r(u,l) = δr(u,l) + γϕ(δu(u,l)) − ϕ(u) highlights transitions that progress toward high-value abstract states, effectively providing dense reward signals.
- Core assumption: Potential-based shaping preserves optimal policies and accelerates learning by reducing exploration variance; the optimal RM value function is a good potential.
- Evidence anchors:
  - [section 3]: "We employ potential-based reward-shaping as defined in Section 2... use V* as the potential function ϕ to generate the reward signal..."
  - [section 2]: "potential-based reward shaping... guarantee that optimal policies... are optimal using the RM reshaped rewards."
  - [corpus]: Weak; corpus neighbors mention interpretability but not shaping specifics.
- Break condition: If the RM abstraction is poor (misrepresents task structure), shaping may mislead rather than help.

### Mechanism 3
- Claim: C-PREP's context representation function is modular and can be integrated into any transfer learning algorithm that follows the CMDP flow.
- Mechanism: The context representation function generates an RM, computes V* via VI, and at each timestep samples an optimal transition label. This is concatenated to the state and fed to the policy. Algorithm 1 shows DQN integration; the same flow applies to other RL algorithms.
- Core assumption: The policy network can process concatenated state + symbolic transition input; the symbolic transition is meaningful to the network for learning transfer.
- Evidence anchors:
  - [section 3]: "The C-PREP context representation function can be integrated into any algorithm following the transfer learning flow depicted in Fig. 1a."
  - [section 3]: "Algorithm 1 (Appendix G) demonstrates an implementation of a DQN [Mnih et al., 2015] for transfer learning settings using C-PREP as the context representation function and RM reward shaping."
  - [corpus]: No direct evidence; assumption stated.
- Break condition: If the policy network architecture cannot handle symbolic inputs or the symbolic representation is noisy, transfer gains may be lost.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their formulation (S, A, T, R, γ).
  - Why needed here: C-PREP operates over RMs which are abstractions of MDPs; understanding the base MDP formulation is essential to grasp RM structure and reward shaping.
  - Quick check question: In an MDP, what does the function T(s,a,s') represent, and how is it used in policy evaluation?

- Concept: Reward Machines (RMs) and their role in abstracting MDP structure.
  - Why needed here: C-PREP relies on generating task-specific RMs with shared symbols to encode high-level task stages and transitions; without this concept, the transfer mechanism is opaque.
  - Quick check question: How does an RM's transition function δu differ from an MDP's transition function T?

- Concept: Value Iteration (VI) and its use for planning in MDPs and RMs.
  - Why needed here: C-PREP uses VI to compute optimal policies over the RM abstraction, which in turn provides the next desired symbolic transition; understanding VI is key to grasping the planning step.
  - Quick check question: What is the update rule for VI in an MDP, and how is it adapted for use in an RM?

## Architecture Onboarding

- Component map: Context → RM generation → VI → optimal transition label → concatenate to state → policy network → action → environment step → update RM state → repeat
- Critical path: Context → RM generator → Value iteration → Optimal transition label → Policy network (with state + label) → Action → Environment → Update RM state
- Design tradeoffs:
  - Symbol set granularity: finer symbols → better context differentiation but higher VI cost; coarser symbols → faster but may merge distinct tasks
  - RM generation method: domain knowledge vs learned vs discrete optimization; trade-off between accuracy and flexibility
  - Reward shaping strength: stronger shaping can accelerate learning but risks misleading the agent if abstraction is poor
- Failure signatures:
  - No transfer gain: likely symbol set too coarse or VI not capturing optimal paths
  - Degraded performance vs baseline: reward shaping misleading due to poor RM abstraction
  - Slow training: VI computation too heavy; consider limiting RM size or using approximate VI
- First 3 experiments:
  1. Verify RM generation: feed a known context, check that the generated RM matches expected structure and symbol set
  2. Test VI planning: run VI on a small RM, confirm V* values align with expected optimal path rewards
  3. Validate symbolic input integration: pass (state, sampled symbolic transition) through policy network, ensure network processes it without error and outputs valid actions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific conditions must be met for context representations to guarantee improved transfer performance?
- Basis in paper: [inferred] The authors mention they will "examine alternative symbolic representations beyond RMs for enhancing learning and transfer" in future work, implying current conditions are unclear.
- Why unresolved: The paper focuses on empirical evaluation of C-PREP rather than theoretical analysis of when context representations help.
- What evidence would resolve it: A formal proof or sufficient conditions demonstrating when context representations guarantee transfer improvement.

### Open Question 2
- Question: How does C-PREP's performance scale with increasing complexity of reward machines?
- Basis in paper: [explicit] "The ability of C-PREP to support transfer depends on the resolution of the generated RMs" - authors note this dependency but don't provide systematic scaling analysis.
- Why unresolved: Experiments use fixed RM resolutions without exploring how performance changes with RM complexity.
- What evidence would resolve it: Systematic experiments varying RM complexity and measuring transfer performance across different scales.

### Open Question 3
- Question: Why does RM information cause the observed "spike" in performance before overfitting occurs in PCG experiments?
- Basis in paper: [explicit] "We still do not have a clear explanation as to why specifically RM information causes this spike" - authors acknowledge this phenomenon but don't explain it.
- Why unresolved: The authors observe this effect but haven't investigated the underlying mechanism.
- What evidence would resolve it: Analysis of learned representations or training dynamics showing what knowledge is acquired before overfitting that causes the performance spike.

## Limitations
- RM abstraction quality and shared symbol set design are critical but implementation details are not fully specified
- Performance is sensitive to context representation choice, with PO showing weaker results than EL/CM
- Empirical evaluation limited to relatively simple grid-world domains, leaving scalability to complex environments uncertain

## Confidence
- Mechanism 1 (Symbolic transfer via shared transitions): Medium - Core concept is sound but implementation specifics are unclear
- Mechanism 2 (Potential-based reward shaping): High - Well-established technique with clear theoretical guarantees
- Mechanism 3 (Modular integration): Medium - Assumed but not extensively validated across different RL algorithms

## Next Checks
1. Test C-PREP with varying symbol set granularity on a simple task to identify the optimal balance between expressiveness and computational efficiency
2. Implement RM generation using only domain knowledge (no learned components) to establish a baseline for comparison
3. Evaluate transfer performance when RM abstraction is deliberately degraded to quantify sensitivity to representation quality