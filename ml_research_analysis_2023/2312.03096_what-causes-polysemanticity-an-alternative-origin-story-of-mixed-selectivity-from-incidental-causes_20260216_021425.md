---
ver: rpa2
title: What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity
  from Incidental Causes
arxiv_id: '2312.03096'
source_url: https://arxiv.org/abs/2312.03096
tags:
- polysemanticity
- features
- feature
- will
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new, non-mutually-exclusive cause of polysemanticity
  in neural networks, called "incidental polysemanticity." This phenomenon arises
  when random initialization assigns multiple features to the same neuron, and training
  dynamics then strengthen this overlap, even when there are enough neurons to avoid
  it. The authors develop a theoretical model using a sparse nonlinear autoencoder
  with tied weights, l1 regularization, and ReLU activation.
---

# What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes

## Quick Facts
- arXiv ID: 2312.03096
- Source URL: https://arxiv.org/abs/2312.03096
- Reference count: 8
- Primary result: Random initialization can cause unrelated features to share neurons, and training dynamics strengthen this overlap even when sufficient neurons exist.

## Executive Summary
This paper identifies a new cause of polysemanticity in neural networks called "incidental polysemanticity." The phenomenon occurs when random initialization assigns multiple features to the same neuron, and training dynamics then strengthen this overlap through winner-take-all effects. Using a theoretical model of a sparse nonlinear autoencoder with l1 regularization and ReLU activation, the authors show that benign collisions (features with opposite signs) don't cause polysemanticity while malign collisions (same sign) do. The number of polysemantic neurons scales as n^2/m, where n is the number of features and m is the number of neurons. The paper suggests current interpretability methods may fail to address this form of polysemanticity and calls for further research into its implications.

## Method Summary
The paper develops a theoretical model using a shallow nonlinear autoencoder with tied weights, l1 regularization, and ReLU activation. The model encodes n-dimensional one-hot vectors through a hidden layer of m neurons. Training dynamics are analyzed through gradient descent on a loss function combining squared error with l1 regularization on activations. The authors theoretically derive that incidental polysemanticity arises from winner-take-all dynamics where regularization forces push for sparsity faster than interference forces can break up feature collisions.

## Key Results
- Incidental polysemanticity arises from random initialization assigning multiple features to the same neuron
- Winner-take-all dynamics strengthen this overlap through l1 regularization-induced sparsity
- The number of polysemantic neurons scales as n^2/m
- Benign collisions (opposite sign features) don't cause polysemanticity, while malign collisions (same sign) do

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polysemanticity arises incidentally when random initialization assigns multiple features to the same neuron, and winner-take-all dynamics then strengthen this overlap.
- Mechanism: Winner-take-all dynamics (induced by l1 regularization) cause sparse activations. If two features initially share the same neuron and have the same sign on their largest weight, the interference force will push both weights down, causing one to dominate and represent both features.
- Core assumption: The regularization force pushes for sparsity faster than the interference force can break up feature collisions.
- Evidence anchors: [abstract] "random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap." [section 2.3] "the training dynamics are... dWi/dt = 4(1 - ||Wi||^2)Wi - λ sign(Wi)" showing the feature benefit and regularization forces.
- Break condition: If m ≥ n^2, the number of collisions becomes negligible, reducing incidental polysemanticity.

### Mechanism 2
- Claim: The sparsity force uniformly stretches gaps between nonzero weights, causing a "rich get richer, poor get poorer" dynamic.
- Mechanism: The feature benefit force is proportional to the weight size while regularization is constant, creating a threshold effect where weights above threshold grow and those below shrink, leading to sparsity.
- Core assumption: The relative variance of nonzero weights remains approximately constant during training.
- Evidence anchors: [section 2.4.1] "crucially, the upwards push is relative to how large Wik is, while the downwards push is absolute" explaining the threshold dynamic. [section 2.4.2] Theoretical analysis showing ||Wi||1 decreases as 1/λt with training time.
- Break condition: If λ becomes too large, the regularization force dominates and kills all weights before sparsity can develop.

### Mechanism 3
- Claim: Benign collisions (features with opposite signs on the same neuron) don't cause polysemanticity, while malign collisions (same sign) do.
- Mechanism: When two features collide on a neuron, if their weights have opposite signs, the ReLU activation clips negative values to zero. If they have the same sign, both weights are pushed down by interference, potentially causing one to dominate both features.
- Core assumption: The probability of a malign collision is 1/2 when two features collide on the same neuron.
- Evidence anchors: [section 2.5.2] "if Wik and Wjk have opposite signs... nothing actually happens... if Wik and Wjk have the same sign... both weights will be under pressure to shrink" [section 2.5.3] Experimental results showing Θ(n^2/m) polysemantic neurons matching theoretical predictions.
- Break condition: If the ReLU activation is removed or modified, this mechanism may not apply.

## Foundational Learning

- Concept: Winner-take-all dynamics
  - Why needed here: Understanding how l1 regularization creates competition between neurons for feature representation.
  - Quick check question: What force in the training dynamics pushes for sparsity and causes one neuron to dominate multiple features?

- Concept: Sparse coding theory
  - Why needed here: Understanding why having fewer neurons than features leads to polysemanticity and how this relates to incidental polysemanticity.
  - Quick check question: How does the number of neurons relative to features affect the probability of polysemanticity?

- Concept: Interference force in neural networks
  - Why needed here: Understanding how different features compete when they share neurons and how this leads to malign vs benign collisions.
  - Quick check question: What determines whether a feature collision results in polysemanticity or not?

## Architecture Onboarding

- Component map:
  - Input layer: n-dimensional one-hot vectors
  - Hidden layer: m neurons with tied encoder-decoder weights
  - Output layer: ReLU activation with no biases
  - Loss function: Squared error with l1 regularization on activations

- Critical path:
  1. Random initialization of weights
  2. Feature encoding through hidden layer
  3. Interference force computation between encodings
  4. Regularization force pushing for sparsity
  5. Gradient descent updates based on combined forces

- Design tradeoffs:
  - Using tied weights simplifies the model but may not capture all real-world dynamics
  - l1 regularization induces sparsity but may not be representative of all neural network architectures
  - ReLU activation creates benign collisions but other activations may behave differently

- Failure signatures:
  - If m << n, polysemanticity becomes necessary rather than incidental
  - If λ is too small, sparsity never develops and no winner-take-all dynamic occurs
  - If λ is too large, all weights are killed before meaningful feature representation

- First 3 experiments:
  1. Vary m from n to n^2 while keeping n fixed to observe how polysemanticity scales
  2. Remove l1 regularization to see if sparsity and incidental polysemanticity still occur
  3. Change the activation function from ReLU to sigmoid to test if benign collisions still exist

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relative prevalence of incidental versus necessary polysemanticity in real-world neural networks, and how does this vary across different architectures and datasets?
- Basis in paper: [inferred] The paper discusses two types of polysemanticity (incidental and necessary) but does not empirically compare their prevalence in practical settings.
- Why unresolved: The paper's theoretical and experimental analysis focuses on a simplified toy model, leaving open the question of how these findings generalize to complex, real-world networks.
- What evidence would resolve it: Empirical studies measuring polysemanticity in various neural network architectures trained on diverse datasets, distinguishing between incidental and necessary cases.

### Open Question 2
- Question: Can we develop methods to reliably distinguish incidental polysemanticity from necessary polysemanticity based solely on the final, trained state of a model?
- Basis in paper: [explicit] The authors suggest this as an interesting future direction, noting that some tools effective against one type of polysemanticity might not work against the other.
- Why unresolved: The paper does not provide a concrete methodology for this distinction, leaving it as an open research question.
- What evidence would resolve it: Development and validation of diagnostic tools or metrics that can classify polysemantic neurons as incidental or necessary based on their final state and local network context.

### Open Question 3
- Question: How do different types of winner-take-all dynamics (induced by factors other than l1 regularization, such as nonlinear activation functions or layer normalization) affect the development of incidental polysemanticity?
- Basis in paper: [explicit] The authors acknowledge their use of l1 regularization as a simplification and suggest exploring other mechanisms that could induce similar dynamics.
- Why unresolved: The paper's theoretical analysis is specific to l1 regularization, and the authors do not investigate alternative mechanisms.
- What evidence would resolve it: Theoretical analysis and experimental comparison of incidental polysemanticity development under various winner-take-all inducing mechanisms.

## Limitations

- The theoretical model relies on several simplifying assumptions that may not hold in real neural networks
- The paper doesn't address how incidental polysemanticity might interact with other known causes of polysemanticity
- Implications for mechanistic interpretability methods are speculative and not directly tested

## Confidence

- High confidence: The basic mechanism of incidental polysemanticity arising from random initialization and winner-take-all dynamics is well-supported by both theoretical analysis and experimental evidence. The n^2/m scaling relationship is clearly demonstrated.
- Medium confidence: The benign vs. malign collision distinction is theoretically sound but may be oversimplified. Real neural networks may exhibit more complex behaviors when features share neurons.
- Low confidence: The implications for mechanistic interpretability methods are speculative and not directly tested. The paper suggests current methods may fail to address incidental polysemanticity, but this claim needs empirical validation.

## Next Checks

1. Test the n^2/m scaling relationship in deeper networks with multiple hidden layers to determine if incidental polysemanticity persists beyond shallow architectures.

2. Replace l1 regularization with l2 regularization to see if sparsity still emerges and whether incidental polysemanticity still occurs, which would validate the core mechanism beyond the specific regularization choice.

3. Implement ablation studies where benign collisions are artificially converted to malign collisions (or vice versa) to directly test whether the benign/malign distinction is causal for the observed polysemanticity patterns.