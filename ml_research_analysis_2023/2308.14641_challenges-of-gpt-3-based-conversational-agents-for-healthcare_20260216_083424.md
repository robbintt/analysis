---
ver: rpa2
title: Challenges of GPT-3-based Conversational Agents for Healthcare
arxiv_id: '2308.14641'
source_url: https://arxiv.org/abs/2308.14641
tags:
- medical
- patient
- gpt-3
- should
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We investigate the use of large language models (LLMs) in patient-facing\
  \ medical question-answering (MedQA) systems, focusing on ethical and safety concerns.\
  \ We evaluate three GPT-3-based models\u2014a baseline and two fine-tuned variants\u2014\
  on their adherence to patient-centered communication strategies and their handling\
  \ of safety-critical queries designed to stress-test medical ethical limitations."
---

# Challenges of GPT-3-based Conversational Agents for Healthcare

## Quick Facts
- arXiv ID: 2308.14641
- Source URL: https://arxiv.org/abs/2308.14641
- Reference count: 10
- Three GPT-3-based models generate unsafe recommendations and offensive content in medical contexts

## Executive Summary
This paper investigates the use of large language models (LLMs) in patient-facing medical question-answering systems, focusing on ethical and safety concerns. The authors evaluate three GPT-3-based models—a baseline and two fine-tuned variants—on their adherence to patient-centered communication strategies and their handling of safety-critical queries. Results show that all models generate erroneous medical information, unsafe recommendations, and offensive content, making them unsuitable for standalone use in the medical domain. The study highlights that fine-tuning on medical data alone does not guarantee safe and ethical behavior in sensitive scenarios.

## Method Summary
The study evaluates three GPT-3-based models: a baseline GPT-3 Curie model, a fine-tuned version on MedDialog dataset (FT-MD), and a further fine-tuned version on EPITOME dataset for empathy (FT-MD-Empathy). Human annotators rate responses on correctness, empathy, politeness, and offensiveness across 40 queries per annotator. Seven stress-test queries specifically probe safety-critical scenarios. The evaluation framework incorporates patient-centered communication strategies and medical ethical principles to assess model behavior in sensitive situations.

## Key Results
- All models fail to adequately handle safety-critical queries, generating erroneous medical information and unsafe recommendations
- Fine-tuned models show only slight improvements in perceived safety compared to baseline
- Models produce offensive content and fail to recognize abusive relationship dynamics in medical contexts

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on medical data alone does not guarantee safe and ethical behavior in sensitive scenarios because fine-tuning introduces domain-specific knowledge but does not alter the underlying generative behavior or safety filters that determine how the model handles ethically sensitive topics. The base generative architecture and learned associations from pretraining persist despite domain-specific fine-tuning.

### Mechanism 2
Patient-centered communication strategies are not automatically learned from empathetic datasets because surface-level alignment with empathetic language patterns does not translate into deeper understanding of ethical responsibilities or situational sensitivity required in medical contexts. The model learns statistical correlations between words/phrases and emotional valence rather than genuine comprehension of ethical principles.

### Mechanism 3
Standard medical ethical principles cannot be encoded through data-driven training alone because ethical decision-making requires contextual reasoning and value judgments that are not reducible to statistical patterns in text data. Ethical principles require human-like moral reasoning capabilities that current LLMs lack.

## Foundational Learning

- Concept: Medical ethical principles and patient-centered communication
  - Why needed here: The evaluation framework is explicitly built around these principles, so understanding them is crucial for interpreting results and designing safe systems
  - Quick check question: What are the three fundamental communication strategies in patient-centered therapy according to Rogers (1951)?

- Concept: Safety-critical scenario identification
  - Why needed here: The stress-test queries are specifically designed to probe model behavior in dangerous situations, requiring understanding of what constitutes a safety-critical scenario
  - Quick check question: Why is the "Dosage" query (Q6) considered safety-critical in this evaluation?

- Concept: Annotation methodology and bias
  - Why needed here: The human evaluation results depend on annotator interpretation, which has limitations and potential biases that affect conclusions
  - Quick check question: What limitation in the annotator sample is explicitly acknowledged in the paper?

## Architecture Onboarding

- Component map: Base model (GPT-3 Curie) → Fine-tuning pipeline (Medical dialogue data → Empathetic data) → Evaluation pipeline (Patient-centered strategy annotation → Safety-critical stress-test evaluation) → Human oversight (Annotator evaluation for both correctness and safety)

- Critical path: Base model generates responses to medical queries → Fine-tuning adds domain knowledge and empathetic language → Human annotators evaluate responses on multiple dimensions → Safety-critical queries test model's ethical decision-making

- Design tradeoffs: General knowledge vs. domain specificity (Fine-tuning improves medical accuracy but doesn't ensure safety) → Empathy vs. accuracy (Empathetic responses may be less precise or professional) → Automation vs. oversight (Full automation increases efficiency but introduces safety risks)

- Failure signatures: Generating medical advice without sufficient information → Providing dangerous recommendations for emergencies → Failing to recognize abusive relationship dynamics → Producing offensive or biased content

- First 3 experiments: Compare base model vs. fine-tuned models on standard medical knowledge tests → Evaluate model responses to safety-critical queries with medical professional oversight → Test model behavior with adversarial perturbations of benign medical queries

## Open Questions the Paper Calls Out
1. How can medical QA systems be designed to reliably detect and escalate life-threatening scenarios to human medical professionals?
2. What are the key factors in medical data quality that significantly impact the performance of fine-tuned LLMs in patient-facing applications?
3. How can empathy in medical AI systems be accurately measured and improved, given the limitations of current empathy datasets?

## Limitations
- Study relies on human annotators with varying medical backgrounds, which may introduce bias in safety and empathy assessments
- Only three stress-test queries specifically probe ethical scenarios, limiting generalizability of safety conclusions
- Fine-tuning approach uses relatively small datasets compared to the base model's 13B parameters

## Confidence
- High: LLMs generate unsafe content in medical contexts; standalone deployment is problematic
- Medium: Fine-tuning on medical data alone is insufficient for ensuring safe behavior
- Low: Specific comparisons between baseline and fine-tuned models are statistically robust

## Next Checks
1. Replicate the study with a larger, more diverse annotator pool including certified medical professionals only
2. Expand stress-test query set to cover a broader range of medical ethical scenarios (e.g., medication interactions, mental health crises, cultural competency)
3. Test whether additional fine-tuning with explicit safety constraints and ethical guidelines improves model behavior beyond the empathetic data approach