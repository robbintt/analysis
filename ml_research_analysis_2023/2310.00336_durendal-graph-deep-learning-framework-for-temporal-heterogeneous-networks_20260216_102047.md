---
ver: rpa2
title: 'DURENDAL: Graph deep learning framework for temporal heterogeneous networks'
arxiv_id: '2310.00336'
source_url: https://arxiv.org/abs/2310.00336
tags:
- temporal
- graph
- learning
- heterogeneous
- durendal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DURENDAL is a general framework for temporal heterogeneous networks
  (THNs) that repurposes existing heterogeneous graph neural networks to evolving
  graphs. It combines snapshot-based and multirelational message-passing principles,
  offering two update schemes (Update-Then-Aggregate and Aggregate-Then-Update) to
  update node embeddings over time.
---

# DURENDAL: Graph deep learning framework for temporal heterogeneous networks

## Quick Facts
- arXiv ID: 2310.00336
- Source URL: https://arxiv.org/abs/2310.00336
- Reference count: 40
- DURENDAL achieves superior performance on future link prediction tasks compared to state-of-the-art models, particularly when leveraging semantic heterogeneity and temporal dynamics.

## Executive Summary
DURENDAL is a general framework for temporal heterogeneous networks (THNs) that repurposes existing heterogeneous graph neural networks to evolving graphs. It combines snapshot-based and multirelational message-passing principles, offering two update schemes (Update-Then-Aggregate and Aggregate-Then-Update) to update node embeddings over time. Experiments on four THN datasets—including two newly introduced high-resolution benchmarks—show that DURENDAL achieves superior performance on future link prediction tasks compared to state-of-the-art models, particularly when leveraging semantic heterogeneity and temporal dynamics. The framework supports inductive learning, scalability, and incremental training with live-update evaluation.

## Method Summary
DURENDAL treats GNN layers as hierarchical node states updated over time via customizable embedding modules (MLP, GRU, or moving average). It supports two update schemes: Update-Then-Aggregate (UTA) updates node embeddings for each relation type separately before aggregation, while Aggregate-Then-Update (ATU) aggregates partial representations first, then updates using a single module. The framework enables incremental training with live-update evaluation, where models are trained and tested on each snapshot sequentially, reflecting real-world scenarios where models must adapt to new data over time.

## Key Results
- DURENDAL outperforms state-of-the-art models on future link prediction tasks across four THN datasets
- UTA scheme provides richer temporal heterogeneous representations while ATU reduces memory footprint and parameters
- Live-update evaluation better reflects evolving network dynamics compared to fixed-split evaluation

## Why This Works (Mechanism)

### Mechanism 1
DURENDAL repurposes heterogeneous GNNs to temporal graphs by treating GNN layers as hierarchical node states updated over time via customizable embedding modules. The framework maintains a sequence of node embedding matrices H^(l) for each layer l, where each snapshot's embeddings are recurrently updated using either a moving average, MLP, or GRU cell, enabling temporal evolution while preserving heterogeneity.

### Mechanism 2
Two update schemes—Update-Then-Aggregate (UTA) and Aggregate-Then-Update (ATU)—provide trade-offs between capturing relational temporal dynamics and reducing model complexity. UTA updates node embeddings for each relation type separately before aggregation, preserving partial temporal states; ATU aggregates partial representations first, then updates using a single module, reducing memory footprint and parameters.

### Mechanism 3
Live-update evaluation better reflects the evolving nature of temporal heterogeneous networks compared to fixed-split evaluation. Models are incrementally trained and tested on each snapshot sequentially, allowing performance to be measured as the network evolves, rather than training on historical data and testing only on the latest snapshot.

## Foundational Learning

- Concept: Snapshot-based temporal graph representation
  - Why needed here: DURENDAL models temporal evolution by dividing the graph into discrete snapshots, enabling the application of static GNN techniques to each time slice.
  - Quick check question: Can you explain how a temporal graph G = {Gt}T_{t=1} is constructed from a sequence of heterogeneous graph snapshots?

- Concept: Heterogeneous graph message passing
  - Why needed here: DURENDAL must handle multiple node and edge types, requiring specialized message passing that accounts for semantic differences between relations.
  - Quick check question: How does the message passing formula h^(l)_v = M_r∈R f^(l,r)_θ(...) differ from standard homogeneous GNN message passing?

- Concept: Live-update incremental training
  - Why needed here: The framework trains and evaluates on each snapshot sequentially, reflecting real-world scenarios where models must adapt to new data over time.
  - Quick check question: What is the difference between live-update evaluation and traditional train-test split evaluation in the context of temporal graphs?

## Architecture Onboarding

- Component map: Input layer -> GNN layers -> Update modules -> Aggregation -> Decoder -> Output
- Critical path: 1. Load snapshot Gt 2. Compute GNN layers using Ht-1 as input 3. Apply Update-Then-Aggregate or Aggregate-Then-Update scheme 4. Generate predictions for Gt+1 5. Evaluate and update model for next iteration
- Design tradeoffs: UTA vs ATU: UTA preserves more temporal dynamics but increases memory and parameters; ATU is more efficient but may lose heterogeneity. Update module choice: GRU captures complex temporal patterns but is heavier; MLP is lighter but may miss long-term dependencies. Snapshot granularity: Finer granularity captures more dynamics but increases computational load.
- Failure signatures: UTA underperforms ATU: May indicate relations evolve similarly, making separate updates unnecessary. Live-update evaluation shows degradation over time: Could signal catastrophic forgetting or insufficient model capacity. Memory overflow: Likely due to UTA scheme with many relation types and large snapshots.
- First 3 experiments: 1. Implement DURENDAL with UTA scheme on GDELT18, using HAN as base GNN; compare to static HAN baseline. 2. Switch to ATU scheme on ICEWS18; compare performance and memory usage to UTA version. 3. Test live-update evaluation on SteemitTH with different update modules (GRU vs ConcatMLP); analyze snapshot-by-snapshot performance trends.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different temporal granularities affect the performance of DURENDAL compared to baselines, and what is the optimal granularity for different types of THNs?
- Basis in paper: The paper mentions that DURENDAL was tested on datasets with different time granularities (weekly, monthly, 5-minute intervals) and notes that prediction performance can vary significantly depending on granularity, especially for TaobaoTH.
- Why unresolved: The paper does not provide a systematic study on how granularity impacts model performance across different THN types, nor does it identify a general optimal granularity rule.
- What evidence would resolve it: Empirical results comparing model performance across multiple granularities for a variety of THN datasets, including analysis of how temporal dynamics and relation types interact with granularity choices.

### Open Question 2
- Question: Can DURENDAL's framework be extended to incorporate continuous-time dynamic graphs rather than just snapshot-based representations?
- Basis in paper: The paper explicitly contrasts snapshot-based methods with continuous-time approaches like TGN and CAW, noting that continuous-time models did not perform well on snapshot-based data, but does not explore whether DURENDAL could be adapted for continuous-time settings.
- Why unresolved: The paper focuses on snapshot-based THNs and does not investigate whether the framework's design principles could be adapted to handle continuous-time data streams.
- What evidence would resolve it: Development and evaluation of a DURENDAL variant that integrates continuous-time modeling (e.g., using temporal point processes) while maintaining heterogeneous and inductive capabilities.

### Open Question 3
- Question: How does the choice of message-passing operator (e.g., GraphConv, GAT, SAGE) impact DURENDAL's performance on different THN tasks, and is there an optimal operator for specific relation types or network structures?
- Basis in paper: The paper mentions testing different message-passing operators but only reports using GraphConv for main results, and notes that DURENDAL's performance depends on the underlying static heterogeneous GNN efficiency.
- Why unresolved: The paper does not provide a detailed comparative analysis of how different operators affect performance across various THN tasks or relation types.
- What evidence would resolve it: Systematic ablation studies comparing multiple message-passing operators across diverse THN datasets and tasks, with analysis of which operators work best for specific relation types or network structures.

### Open Question 4
- Question: What are the fairness implications of DURENDAL when applied to recommendation systems, and how can the framework be modified to mitigate potential biases?
- Basis in paper: The paper discusses societal impacts, noting that incorrect predictions in recommender systems could lead to unfair or harmful recommendations and echo chamber formation.
- Why unresolved: The paper does not provide empirical analysis of fairness metrics or propose specific modifications to the framework to address bias in recommendations.
- What evidence would resolve it: Experimental evaluation of DURENDAL on recommendation tasks with fairness metrics (e.g., demographic parity, equal opportunity), along with proposed algorithmic modifications to improve fairness outcomes.

## Limitations
- Scalability to very large-scale temporal heterogeneous networks is not fully explored, particularly regarding memory consumption with the UTA scheme on graphs with many relation types
- Choice of update modules and their hyperparameters for each dataset are not explicitly specified, which could affect reproducibility
- Evaluation focuses on future link prediction, leaving open questions about performance on other downstream tasks such as node classification or anomaly detection

## Confidence
- High confidence in the general framework design and its ability to repurpose existing heterogeneous GNNs for temporal graphs
- Medium confidence in the empirical superiority of DURENDAL over baselines, given the controlled experimental conditions
- Medium confidence in the effectiveness of the two update schemes (UTA and ATU), as the paper discusses their strengths and weaknesses but does not provide exhaustive ablation studies
- Low confidence in the framework's scalability to very large graphs with many relation types without further optimization

## Next Checks
1. Implement a scalability test comparing memory usage and runtime between UTA and ATU schemes on graphs with increasing numbers of relation types and nodes
2. Conduct an ablation study varying the update module types (MLP, GRU, moving average) and their hyperparameters across all four datasets to assess their impact on performance
3. Extend the evaluation to include other downstream tasks such as node classification and anomaly detection to validate the framework's versatility beyond link prediction