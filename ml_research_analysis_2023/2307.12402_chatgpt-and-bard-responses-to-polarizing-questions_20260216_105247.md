---
ver: rpa2
title: ChatGPT and Bard Responses to Polarizing Questions
arxiv_id: '2307.12402'
source_url: https://arxiv.org/abs/2307.12402
tags:
- bard
- chatgpt
- responses
- have
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper created a dataset of ChatGPT and Bard responses to highly
  polarizing US topics to assess potential bias and guardrails around controversial
  issues. They found both models had left-leaning bias, with Bard more likely to provide
  comprehensive, human-like responses around polarizing topics.
---

# ChatGPT and Bard Responses to Polarizing Questions

## Quick Facts
- arXiv ID: 2307.12402
- Source URL: https://arxiv.org/abs/2307.12402
- Reference count: 13
- Left-leaning bias observed in both ChatGPT and Bard responses to polarizing US topics

## Executive Summary
This study compared ChatGPT and Bard responses to 2400 questions across 12 highly polarizing US topics to assess potential bias and guardrail differences. The analysis revealed that both models exhibited left-leaning bias in their responses, though Bard consistently provided more comprehensive and human-like answers around controversial issues. Bard appeared to have fewer guardrails around sensitive topics, making it more willing to engage with polarizing content in depth. The findings suggest Bard may be more susceptible to misuse by malicious actors but also indicates potential for fine-tuning to provide balanced, evidence-based information on controversial topics.

## Method Summary
The researchers collected 200 questions per topic from Quora across 12 polarizing US issues, then used ChatGPT Plus API and Bard web interface to generate responses to all questions. They analyzed responses for bias indicators, guardrail presence (generic refusals), and response quality metrics including length, depth, and human-likeness. The analysis involved text preprocessing, n-gram extraction, and comparative assessment between the two models' response patterns.

## Key Results
- Both ChatGPT and Bard showed left-leaning bias in responses to polarizing questions
- Bard provided more comprehensive and human-like responses than ChatGPT
- Bard had fewer guardrails around controversial topics and was more willing to engage deeply
- ChatGPT more frequently defaulted to generic disclaimers ("As an AI language model...")
- Some topics like veganism, immigration reform, and vaccination triggered more generic responses from ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bard provides more comprehensive and human-like responses to polarizing questions than ChatGPT.
- Mechanism: Bard has fewer built-in guardrails and defaults to conversational responses rather than generic disclaimers, allowing it to generate more detailed answers.
- Core assumption: Fewer guardrails = more willingness to engage with controversial content.
- Evidence anchors:
  - [abstract] "Bard more likely to provide comprehensive, human-like responses" and "Bard seemed to have fewer guardrails around controversial topics"
  - [section] "Unlike ChatGPT, Bard seemed to have more confidence in its responses, and did not default to generic text"
  - [corpus] Weak/no direct evidence on guardrail mechanisms; only comparative behavioral observations
- Break condition: If Bard is fine-tuned with stricter guardrails, the response style reverts to more generic outputs.

### Mechanism 2
- Claim: ChatGPT defaults to generic, balanced prefaces ("As an AI language model...") more often than Bard.
- Mechanism: ChatGPT's response generation pipeline includes a safety disclaimer step that triggers on polarizing topics, reducing engagement depth.
- Core assumption: Safety disclaimer insertion reduces perceived engagement quality.
- Evidence anchors:
  - [abstract] "ChatGPT would begin with As an AI language model, I do not have personal beliefs or feelings"
  - [section] "ChatGPT may be more likely to default to generic text around controversial questions"
  - [corpus] No direct evidence on the internal triggering logic for disclaimers
- Break condition: If disclaimer insertion is disabled or bypassed, ChatGPT responses become more detailed.

### Mechanism 3
- Claim: Left-leaning bias is observable in both models but manifests differently.
- Mechanism: Training data and fine-tuning corpora contain more left-leaning framing, which models reproduce unless explicitly counterbalanced.
- Core assumption: Bias in training data propagates to generation without explicit debiasing.
- Evidence anchors:
  - [abstract] "Broadly, our results indicated a left-leaning bias for both ChatGPT and Bard"
  - [section] "Many answers were left-leaning, and in some cases seemed somewhat radical"
  - [corpus] No direct evidence of training data composition; inference based on output patterns
- Break condition: If training data is balanced or debiased, output bias is reduced or eliminated.

## Foundational Learning

- Concept: Large Language Model (LLM) hallucination
  - Why needed here: The paper discusses that LLMs can produce convincing but incorrect text, which is central to understanding misinformation risk.
  - Quick check question: What is the term for when an LLM generates text that sounds plausible but is factually incorrect?

- Concept: Guardrail mechanisms in AI chatbots
  - Why needed here: The paper contrasts how ChatGPT and Bard handle controversial topics via guardrails, which determines response style and completeness.
  - Quick check question: What safety feature in LLMs limits engagement with polarizing content?

- Concept: N-gram frequency analysis
  - Why needed here: The authors use n-gram analysis to identify generic vs. topic-specific responses in the datasets.
  - Quick check question: How can counting word sequences reveal whether a model defaults to generic answers?

## Architecture Onboarding

- Component map: Quora question scraper -> topic filter -> 200 Q&A per topic -> ChatGPT Plus API / Bard web interface -> response capture -> text preprocessing -> n-gram extraction -> bias assessment
- Critical path: Question selection -> API call -> response storage -> statistical analysis
- Design tradeoffs:
  - Using web interface for Bard limits automation vs. API access
  - Manual annotation introduces subjectivity; automated scoring could reduce bias
  - 200 Q&A per topic balances coverage vs. collection cost
- Failure signatures:
  - Incomplete responses (e.g., "I'm a language model...") indicate guardrail triggers
  - Identical n-grams across topics suggest generic response fallback
  - Inconsistent response depth across topics signals non-uniform guardrail application
- First 3 experiments:
  1. Vary question phrasing to test guardrail sensitivity thresholds
  2. Compare responses across multiple LLM providers to benchmark bias levels
  3. Introduce controlled bias prompts to measure output shift magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do Bard's fewer guardrails around controversial topics lead to more persuasive or influential responses compared to ChatGPT?
- Basis in paper: Explicit - The paper states Bard "seemed to have fewer guardrails around controversial topics, and appeared more willing to provide comprehensive, and somewhat human-like responses" and "Bard may thus be more likely abused by malicious actors."
- Why unresolved: The paper did not directly compare the persuasiveness or influence of Bard vs ChatGPT responses. It only noted that Bard provided more comprehensive responses around polarizing topics.
- What evidence would resolve it: An experimental study measuring the persuasiveness or influence of Bard and ChatGPT responses on participants, particularly around controversial topics where Bard provides more detailed answers.

### Open Question 2
- Question: What is the underlying reason for ChatGPT providing more generic responses (e.g., "As an AI language model...") for certain topics like veganism, immigration reform, and vaccination?
- Basis in paper: Explicit - The paper notes ChatGPT "seemed to give generic answers... more commonly for veganism, immigration reform, and vaccination" and states "It was not clear why ChatGPT provided more descriptive responses for some topics compared to others."
- Why unresolved: The paper did not identify a clear pattern or reason for this behavior, despite these topics being highly controversial.
- What evidence would resolve it: Analysis of ChatGPT's training data and fine-tuning process to identify any biases or patterns that might lead to more generic responses for certain topics.

### Open Question 3
- Question: How can LLMs like ChatGPT and Bard be fine-tuned to provide more balanced, evidence-based information while still addressing controversial topics comprehensively?
- Basis in paper: Explicit - The paper suggests "with sufficient fine-tuning, Bard may be able to provide evidence-based information and ameliorate controversy by providing more balanced and moderate responses."
- Why unresolved: The paper does not provide specific methods or strategies for achieving this fine-tuning.
- What evidence would resolve it: Research demonstrating effective fine-tuning techniques that improve the balance and evidence-based nature of LLM responses on controversial topics without sacrificing comprehensiveness.

## Limitations

- Observational study without controlled experimental manipulation of model parameters
- Analysis lacks transparency in how bias was operationally defined and measured
- Web interface used for Bard collection introduces potential variability in responses
- Limited sample size of 200 questions per topic may not capture full response variability

## Confidence

- **High Confidence**: Observable behavioral differences in response patterns between ChatGPT and Bard (e.g., frequency of generic disclaimers, response completeness)
- **Medium Confidence**: Generalization of left-leaning bias across both models based on qualitative assessment
- **Low Confidence**: Attribution of specific mechanisms (like guardrail strength) to underlying model architecture or training differences

## Next Checks

1. Replicate the study using standardized bias scoring rubrics and multiple independent annotators to establish inter-rater reliability for bias classification

2. Conduct A/B testing with identical questions across different LLM providers to benchmark whether observed differences are specific to ChatGPT/Bard or representative of broader LLM behavior

3. Perform controlled experiments varying question phrasing and framing to systematically map the boundaries of each model's guardrail triggers and response variability