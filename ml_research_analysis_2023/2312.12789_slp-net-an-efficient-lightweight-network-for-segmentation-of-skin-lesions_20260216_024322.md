---
ver: rpa2
title: SLP-Net:An efficient lightweight network for segmentation of skin lesions
arxiv_id: '2312.12789'
source_url: https://arxiv.org/abs/2312.12789
tags:
- segmentation
- convolution
- skin
- lesion
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SLP-Net, a lightweight deep learning model
  for skin lesion segmentation. It leverages a novel spiking neural P systems (SNP)
  convolution mechanism, which differs from traditional CNNs by activating inputs
  before weighted summation.
---

# SLP-Net:An efficient lightweight network for skin lesions

## Quick Facts
- arXiv ID: 2312.12789
- Source URL: https://arxiv.org/abs/2312.12789
- Authors: 
- Reference count: 40
- One-line primary result: SLP-Net achieves state-of-the-art performance for skin lesion segmentation with only 0.2M parameters and 190 FPS processing speed.

## Executive Summary
This paper introduces SLP-Net, a highly efficient deep learning model for skin lesion segmentation that achieves state-of-the-art performance while using only 0.2M parameters. The model leverages a novel spiking neural P systems (SNP) convolution mechanism that activates inputs before weighted summation, enabling significant computational efficiency. SLP-Net incorporates three key components: a lightweight pyramid (SLP) for multi-scale feature extraction, feature self-adaptation skip connections (SFA) for boundary information recovery, and SNP-type downsampling (SDS) for efficiency. Experimental results on ISIC2018 and PH2 datasets demonstrate superior accuracy, sensitivity, and Dice coefficient compared to existing methods, while being approximately 4 times faster than U-Net.

## Method Summary
SLP-Net is a lightweight deep learning architecture designed for skin lesion segmentation that uses a novel SNP-based convolution mechanism. The model processes images through an initial block, followed by three stages each containing SDS (downsampling), SLP (multi-scale feature extraction with asymmetric dilated convolutions), and SFA (adaptive skip connections) modules. Instead of a traditional decoder, SLP-Net uses SFA modules to recover boundary information while maintaining computational efficiency. The entire network contains only 0.2M parameters compared to ~31M for U-Net. Training uses Adam optimizer with learning rate 1e-3, weight decay 1e-4, batch size 20, and data augmentation through rotation and flipping across 50 epochs.

## Key Results
- Achieves highest Acc (93.87%) and DSC (88.21%) among compared methods on ISIC2018 dataset
- Processes images at 190 FPS, approximately 4 times faster than U-Net
- Demonstrates strong generalization with stable performance on PH2 dataset for both small and large lesion areas
- Maintains high accuracy with only 0.2M parameters versus 31M for U-Net

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MSConvSNP mechanism improves computational efficiency by activating inputs before weighted summation.
- Mechanism: In traditional CNNs, activation occurs after weighted summation (Y = f(WX)), while in MSConvSNP, activation is applied to inputs before summation (Y = Wf(X)). This allows the model to share activation computations across channels and reduce parameter count.
- Core assumption: Activation before weighted summation maintains model expressiveness while reducing computational overhead.
- Evidence anchors:
  - [abstract]: "MSConvSNP...differs from traditional CNNs by activating inputs before weighted summation"
  - [section]: "MSConvSNP provides more scaled weight values for the input signal X, and it can better fit more complex data"
  - [corpus]: Weak evidence - no direct corpus support for MSConvSNP mechanism found
- Break condition: If activation-before-weighting reduces model capacity below acceptable threshold for skin lesion segmentation.

### Mechanism 2
- Claim: The lightweight multi-scale feature extractor (SLP) captures complex skin lesion boundaries effectively.
- Mechanism: SLP uses depthwise separable convolutions with asymmetric kernels and dilated rates (0, 4, 8, 16) to extract multi-scale features without heavy computational cost.
- Core assumption: Skin lesion boundaries require multi-scale analysis and asymmetric convolutions can capture irregular boundaries effectively.
- Evidence anchors:
  - [section]: "SLP is a multi-scale feature extractor, which is mainly composed of multiple sets of asymmetric convolutions with different dilated rates"
  - [abstract]: "The model is designed to be highly efficient, with only 0.2M parameters, while maintaining high segmentation accuracy"
  - [corpus]: Weak evidence - no direct corpus support for SLP module found
- Break condition: If multi-scale extraction fails to improve segmentation accuracy on irregular lesion boundaries.

### Mechanism 3
- Claim: The SNP-type feature self-adaptation skip connection (SFA) effectively recovers boundary information without full decoder.
- Mechanism: SFA uses residual-like concatenation with 3x3 and 1x1 convolutions to adaptively encode low-level information, compensating for information loss from downsampling.
- Core assumption: Adaptive feature recovery can replace full decoder structure while maintaining boundary detail.
- Evidence anchors:
  - [section]: "Rather than a decoder, a feature adaptation module is designed to replace it and implement multi-scale information decoding"
  - [abstract]: "incorporates three key components: SNP-type lightweight pyramid (SLP) for multi-scale feature extraction, SNP-type feature self-adaptation skip connection (SFA) for boundary information recovery"
  - [corpus]: Weak evidence - no direct corpus support for SFA mechanism found
- Break condition: If SFA cannot recover sufficient boundary information compared to traditional decoder structures.

## Foundational Learning

- Concept: Spiking Neural P Systems (SNP) and membrane computing
  - Why needed here: The paper's core innovation relies on SNP-based convolution mechanism
  - Quick check question: What is the key difference between ConvSNP and traditional CNN convolution operations?

- Concept: Multi-scale feature extraction and dilated convolutions
  - Why needed here: SLP module uses dilated convolutions with different rates to capture lesion boundaries at multiple scales
  - Quick check question: How do dilated convolutions increase receptive field without increasing parameters?

- Concept: Lightweight network design tradeoffs
  - Why needed here: The paper achieves 0.2M parameters compared to U-Net's ~31M, requiring understanding of parameter efficiency techniques
  - Quick check question: What are the benefits and limitations of depthwise separable convolutions?

## Architecture Onboarding

- Component map: Initblock → SDS1 → SLP1 → SFA1 → SDS2 → SLP2 → SFA2 → SDS3 → SLP3 → SFA3 → US → Output
- Critical path: Feature extraction (SLP modules) → Adaptive feature recovery (SFA modules) → Multi-scale integration → Output
- Design tradeoffs: Speed vs accuracy (190 FPS vs slightly lower sensitivity than Deeplabv3+), parameter efficiency vs model capacity
- Failure signatures: Under-segmentation of small lesions, over-segmentation of background, poor boundary detection on irregular lesions
- First 3 experiments:
  1. Compare MSConvSNP vs traditional convolution on a simple segmentation task
  2. Test SLP module with different dilation rates on multi-scale feature extraction
  3. Validate SFA module's boundary recovery by comparing with and without SFA on PH2 dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SNP-type convolution mechanism in SLP-Net compare to other lightweight architectures like MobileNet or ShuffleNet in terms of accuracy and efficiency on skin lesion segmentation tasks?
- Basis in paper: [explicit] The paper mentions that SLP-Net has only 0.2M parameters, which is much smaller than other models like U-Net. It also claims superior accuracy and speed compared to state-of-the-art methods.
- Why unresolved: While the paper provides comparisons with specific models, it doesn't directly compare the SNP mechanism to other lightweight architectures that use different techniques (e.g., depthwise separable convolutions).
- What evidence would resolve it: A direct comparison of SLP-Net with other lightweight architectures like MobileNet, ShuffleNet, or EfficientNet on the same datasets, using the same evaluation metrics.

### Open Question 2
- Question: Can the SLP-Net architecture be effectively adapted for other medical image segmentation tasks beyond skin lesion segmentation, such as tumor segmentation in CT or MRI scans?
- Basis in paper: [explicit] The paper mentions that the SLP, SFA, and SDS components "can be ported to other networks or construct new networks with promising generality."
- Why unresolved: While the paper claims generality, it only validates the model on skin lesion datasets. The effectiveness of these components on other medical imaging modalities remains to be seen.
- What evidence would resolve it: Testing SLP-Net or its components on other medical image segmentation tasks, such as brain tumor segmentation in MRI or liver tumor segmentation in CT scans, and comparing the results with state-of-the-art methods.

### Open Question 3
- Question: How does the performance of SLP-Net change when trained on smaller datasets, which are common in medical imaging?
- Basis in paper: [explicit] The paper mentions that deep learning algorithms tend to perform poorly when the number of samples is too small, which is a common problem in medical image analysis.
- Why unresolved: While the paper validates the model on two datasets, it doesn't explore how the model performs when trained on smaller subsets of these datasets or on other small medical image datasets.
- What evidence would resolve it: Training SLP-Net on progressively smaller subsets of the ISIC2018 and PH2 datasets, as well as on other small medical image datasets, and evaluating its performance compared to other methods.

## Limitations

- Computational efficiency gains from MSConvSNP mechanism lack direct ablation study comparison with traditional convolutions
- Model tested only on two skin lesion datasets, limiting generalizability assessment across diverse imaging conditions
- Slight drop in sensitivity compared to heavier models suggests potential accuracy limitations in edge cases

## Confidence

- **High Confidence**: Parameter efficiency (0.2M vs 31M for U-Net) and basic segmentation performance metrics
- **Medium Confidence**: Multi-scale feature extraction effectiveness and boundary recovery claims
- **Low Confidence**: Computational efficiency gains from MSConvSNP mechanism and overall model robustness

## Next Checks

1. Implement a version of SLP-Net using traditional convolutions instead of MSConvSNP to quantify the actual computational and accuracy differences.

2. Test SLP-Net on additional skin lesion datasets (e.g., MED-NODE, Derm7pt) to assess generalization across different imaging protocols and lesion types.

3. Conduct detailed analysis on challenging cases including very small lesions (<1000 pixels), irregular boundaries, and lesions with similar color to surrounding skin to identify potential failure modes.