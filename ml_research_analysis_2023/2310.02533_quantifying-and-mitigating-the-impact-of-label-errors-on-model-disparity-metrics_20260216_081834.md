---
ver: rpa2
title: Quantifying and mitigating the impact of label errors on model disparity metrics
arxiv_id: '2310.02533'
source_url: https://arxiv.org/abs/2310.02533
tags:
- training
- label
- error
- disparity
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of label errors on model disparity
  metrics, focusing on group calibration and other fairness-related measures. The
  authors conduct empirical sensitivity tests by flipping labels in both training
  and test data, finding that label errors disproportionately affect minority groups.
---

# Quantifying and mitigating the impact of label errors on model disparity metrics

## Quick Facts
- arXiv ID: 2310.02533
- Source URL: https://arxiv.org/abs/2310.02533
- Reference count: 39
- Key outcome: Label errors disproportionately affect minority groups' calibration metrics; influence functions can identify and correct problematic training labels, improving group calibration by 10-40% over baselines.

## Executive Summary
This paper investigates how label errors in training and test data affect model fairness metrics, particularly group calibration. Through synthetic label flipping experiments, the authors demonstrate that minority groups are more sensitive to label noise. They propose a novel influence function-based method to identify training samples whose labels most affect disparity metrics, and an automatic relabel-and-finetune scheme that provably improves group calibration error.

## Method Summary
The authors conduct empirical sensitivity tests by flipping labels in both training and test data to quantify the impact on disparity metrics. They develop a modified influence function approach to estimate how individual training labels affect group disparity metrics, allowing identification of the most influential mislabeled samples. The method is complemented by an automatic relabel-and-finetune scheme that improves group calibration through targeted relabeling of high-influence training points.

## Key Results
- Label errors in test data increase group calibration error more for minority groups than majority groups
- Influence functions can identify training points whose labels most affect group disparity metrics
- Automatic relabeling of top-influence training points provably improves group calibration error
- The approach improves identification of mislabeled training samples by 10-40% compared to alternatives

## Why This Works (Mechanism)

### Mechanism 1
Label errors in test data disproportionately distort group calibration for minority groups because these groups have fewer samples, so random label flips have a larger proportional impact. The effect is amplified when flipping test labels, as minority groups show sharper increases in model miscalibration compared to majority groups.

### Mechanism 2
Influence functions can identify training points whose labels most affect group disparity metrics by modifying the traditional influence function from loss to group disparity metric. This allows ranking training samples by their effect on group calibration, where upweighting or perturbing these labels changes disparity more than random samples.

### Mechanism 3
Automatic relabeling of top-influence training points provably improves group calibration by reducing expected calibration error on the test set. The approach relies on Theorem 1 showing that relabeling the subset of training samples with highest positive influence on test calibration error reduces ECE.

## Foundational Learning

- Concept: Expected Calibration Error (ECE) and Brier Score
  - Why needed here: Group calibration is the primary disparity metric; understanding ECE/Brier Score is essential to interpret results
  - Quick check question: What does an ECE of 0 indicate about a model's probability estimates?

- Concept: Influence Functions (Koh & Liang 2017)
  - Why needed here: The method builds directly on influence function theory to estimate label impact on disparity metrics
  - Quick check question: How does upweighting a training point by epsilon affect the model parameters?

- Concept: Label Noise and Synthetic Noise Experiments
  - Why needed here: The sensitivity tests rely on flipping labels to simulate noise; understanding synthetic noise helps interpret results
  - Quick check question: What is the difference between uniform random label flipping and systematic label corruption?

## Architecture Onboarding

- Component map: Data loading -> Model training (logistic regression/GBT/ResNet) -> Influence computation -> Relabeling pipeline -> Evaluation
- Critical path:
  1. Load data and split into train/validation/test with group annotations
  2. Train baseline model (logistic regression)
  3. Compute influence scores for training points on test calibration
  4. Identify top-influence mislabeled samples
  5. Relabel and fine-tune model
  6. Re-evaluate group calibration
- Design tradeoffs:
  - Logistic regression chosen for tractable Hessian; limits scalability to large deep nets
  - Uniform label flipping simplifies experiments but may not reflect real-world noise patterns
  - Relabeling assumes binary classification; extension to multi-class requires adaptation
- Failure signatures:
  - Poor Hessian conditioning → unreliable influence scores
  - High label noise rate → random relabeling may dominate true corrections
  - Group imbalance → minority group influence may be over or underestimated
- First 3 experiments:
  1. Train logistic regression on Adult dataset, compute baseline group calibration
  2. Flip 10% of test labels uniformly, measure change in ECE for majority vs. minority groups
  3. Apply influence-based relabeling to 20% of training samples, fine-tune, and compare group calibration

## Open Questions the Paper Calls Out

### Open Question 1
How does the impact of label errors on model disparity metrics differ across various model architectures (e.g., deep neural networks vs. logistic regression)? The paper notes that overparameterized models seem more resilient to training label error but doesn't provide detailed comparative analysis.

### Open Question 2
What are the long-term effects of label errors on model performance and fairness metrics when models are continuously updated with new data? The study focuses on static datasets and doesn't address the dynamic nature of real-world data updates.

### Open Question 3
How effective are current noise-aware algorithms in mitigating the impact of label errors on disparity metrics across different data modalities? The paper mentions disparate impact on group calibration but doesn't provide comprehensive evaluation across modalities.

### Open Question 4
What is the role of domain expertise in identifying and correcting label errors to improve model fairness metrics? The paper focuses on algorithmic approaches but doesn't consider the human element in this process.

### Open Question 5
How do label errors in training data affect the generalization ability of models across different demographic groups? The paper discusses impact on disparity metrics but doesn't explore broader implications for model generalization.

## Limitations
- Findings rely on synthetic label noise experiments that may not capture real-world error patterns
- Influence function approach requires strong assumptions about model convexity and Hessian conditioning
- Relabeling scheme assumes perfect identification of mislabeled samples, but influence scores can be noisy
- Evaluation focuses primarily on group calibration error, potentially overlooking other fairness metrics

## Confidence
- High confidence: Empirical observation that label errors disproportionately affect minority groups
- Medium confidence: The influence function method for identifying problematic training labels
- Low confidence: The automatic relabel-and-finetune scheme's generalizability beyond tested scenarios

## Next Checks
1. Test the influence function approach on non-convex models (e.g., neural networks) to assess robustness when theoretical assumptions break
2. Validate the relabeling scheme on datasets with known ground-truth label noise patterns rather than synthetic flips
3. Extend experiments to include additional fairness metrics (e.g., equalized odds, demographic parity) to understand if label error sensitivity generalizes beyond calibration error