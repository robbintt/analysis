---
ver: rpa2
title: 'LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities
  in Large Language Models'
arxiv_id: '2310.03903'
source_url: https://arxiv.org/abs/2310.03903
tags:
- agents
- agent
- coordination
- action
- llm-co
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-Coordination, a framework that enables
  large language models to play multi-agent coordination games. It presents a novel
  benchmark evaluating LLM agents on tasks requiring Theory of Mind, joint planning,
  and coordination in games like Collab Escape, Collab Capture, and Overcooked.
---

# LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models

## Quick Facts
- arXiv ID: 2310.03903
- Source URL: https://arxiv.org/abs/2310.03903
- Reference count: 17
- Primary result: LLM agents can achieve human-level coordination performance through reasoning-based action selection

## Executive Summary
This paper introduces LLM-Co, a framework enabling large language models to play multi-agent coordination games like Collab Escape, Collab Capture, and Overcooked. The authors evaluate LLM agents on tasks requiring Theory of Mind, joint planning, and sustained coordination, finding that GPT-4-based LLM agents can outperform reinforcement learning baselines while providing explainability through their reasoning processes. The framework demonstrates that LLMs can achieve strong coordination performance but require explicit prompts to provide proactive assistance to partners.

## Method Summary
The LLM-Co framework converts grid-based game states into natural language descriptions including partner inventory and relative positions, then uses an LLM (primarily GPT-4) to reason about the state and generate actions. The system includes a state representation module, feasible action generator, action manager, and optional helper directive system. The LLM-Co agents are evaluated across three games with different coordination challenges and compared against reinforcement learning baselines trained with Proximal Policy Optimization.

## Key Results
- GPT-4 LLM-Co agents achieve near-human performance on Theory of Mind reasoning tasks
- LLM-Co agents match or exceed Self-Play RL baselines on sustained coordination in Overcooked
- Explicit helper directives enable LLM-Co agents to provide proactive assistance to partners
- LLM-Co agents demonstrate robustness when paired with different types of partner agents

## Why This Works (Mechanism)

### Mechanism 1: Theory of Mind Reasoning
LLM-Co agents infer partner's next actions and adjust their own through structured state descriptions that include partner inventory and relative positions. This enables the LLM to reason about partner's likely next moves and generate appropriate actions. The core assumption is that LLMs can effectively translate structured state information into natural language and perform reasoning about other agents' mental states when given explicit partner information.

### Mechanism 2: Reasoning-Based Coordination
Instead of learning policies through reinforcement learning, LLM-Co agents generate actions through reasoning about current state and partner state at each timestep. This allows dynamic adjustment without retraining, assuming that reasoning-based action selection can match or exceed learned policy performance in coordination tasks when given sufficient context.

### Mechanism 3: Explicit Assistance Prompting
The LLM-Co framework includes directive-based prompting that can be modified to include helper intentions. When given explicit instructions to help partners, the LLM will reason about opportunities to assist and generate appropriate actions, based on the assumption that LLMs can adjust their behavior based on prompt modifications.

## Foundational Learning

- **Theory of Mind in AI systems**: Understanding partner's intentions and beliefs is critical for coordination tasks. The LLM-Co framework relies on LLMs inferring what partners will do next.
  - Quick check: Can you explain how providing partner inventory information in the state description enables Theory of Mind reasoning?

- **State representation for embodied agents**: The LLM-Co framework converts grid-based game states into natural language descriptions. Understanding this conversion is key to modifying or extending the framework.
  - Quick check: Why does the framework provide relative distances instead of absolute positions for locations of interest?

- **Prompt engineering for task-specific behavior**: The helper directive mechanism shows how prompt modifications can change agent behavior. Understanding prompt engineering is essential for customizing LLM-Co agents.
  - Quick check: How does adding "I want to prefer helping the other player with their cooking and delivery if the situation arises" change the LLM's behavior?

## Architecture Onboarding

- **Component map**: Game environment → State representation module → LLM (GPT-4) → Feasible action generator → Action manager → Game environment

- **Critical path**: Game state → State representation module → LLM (with partner info and feasible actions) → LLM output (analysis + action) → Action manager → Game environment

- **Design tradeoffs**: Reasoning vs. learning (explainability vs. potential optimality), prompt complexity (performance vs. token costs), action granularity (burden vs. expressiveness)

- **Failure signatures**: Stuck agents (action manager cannot execute LLM actions), poor coordination (LLM fails to infer partner intentions), suboptimal efficiency (LLM doesn't recognize assistance opportunities without directives)

- **First 3 experiments**: 1) Test LLM-Co with different LLMs on ToM reasoning test set, 2) Test with/without helper directives on gated delivery layout, 3) Compare against RL baselines on new Overcooked layout

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLM-Co agents scale with increased task complexity or longer time horizons in coordination tasks? The paper evaluates agents over 400 timesteps but doesn't explore significantly longer horizons or more complex tasks.

### Open Question 2
Can LLM-Co agents adapt their strategies in real-time to unexpected changes in the environment or partner behavior? While agents show some adaptability, the extent of real-time strategy modification in response to unforeseen circumstances is not fully explored.

### Open Question 3
What is the impact of varying the quality of the partner directive on the performance of LLM-Co agents in explicit assistance tasks? The paper uses a single directive type, leaving exploration of how directive quality influences agent behavior incomplete.

## Limitations

- Performance depends heavily on state representation quality and prompt engineering
- Framework's generalization to domains beyond tested game environments is unclear
- Explicit assistance mechanism requires specific helper directives, limiting proactive coordination

## Confidence

- **High confidence**: LLM-Co agents can achieve strong coordination performance in structured environments with proper state representation and prompting
- **Medium confidence**: The reasoning-based approach can match RL baselines, though this may be environment-specific
- **Medium confidence**: Theory of Mind reasoning is demonstrated but shows room for improvement

## Next Checks

1. Test LLM-Co agents on novel Overcooked layouts without retraining to assess generalization of coordination strategies
2. Systematically vary the level of detail in state descriptions to identify the minimum information needed for effective coordination
3. Compare LLM-Co performance when using different prompting strategies (temperature, few-shot examples) to optimize the explicit assistance mechanism