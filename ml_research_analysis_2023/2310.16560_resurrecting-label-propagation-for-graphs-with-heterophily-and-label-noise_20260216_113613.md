---
ver: rpa2
title: Resurrecting Label Propagation for Graphs with Heterophily and Label Noise
arxiv_id: '2310.16560'
source_url: https://arxiv.org/abs/2310.16560
tags:
- label
- noise
- graph
- labels
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles graph label noise under arbitrary heterophily.
  It observes that higher graph homophily mitigates label noise, and label propagation
  performs well in high homophily settings.
---

# Resurrecting Label Propagation for Graphs with Heterophily and Label Noise

## Quick Facts
- arXiv ID: 2310.16560
- Source URL: https://arxiv.org/abs/2310.16560
- Reference count: 40
- Primary result: LP4GLN outperforms 7 state-of-the-art baselines on 10 datasets under various noise types and ratios

## Executive Summary
This paper addresses node classification on graphs with noisy labels under arbitrary heterophily. It proposes LP4GLN, an iterative algorithm that reconstructs graphs to increase homophily, applies label propagation to correct noisy labels, and selects high-confidence labels for the next round. The method reduces time complexity to linear and provides theoretical guarantees on denoising effectiveness. Experiments show LP4GLN achieves superior performance across 10 datasets with various noise types and ratios.

## Method Summary
LP4GLN is an iterative algorithm that alternates between graph reconstruction, label propagation, and high-confidence label selection. It uses a GNN to learn node representations from the current clean label set, then constructs a similarity graph $Z^{(L)*}$ via closed-form solutions. Label propagation is applied to this graph to compute confidence scores for all nodes, from which the highest-confidence nodes are selected and added to the clean set. The process repeats until convergence or a maximum number of iterations. A final GNN is trained on the augmented clean set for test predictions.

## Key Results
- LP4GLN achieves state-of-the-art accuracy on 10 benchmark datasets under various noise types and ratios
- The method demonstrates robust performance across both homophilous and heterophilous graphs
- Theoretical analysis proves LP4GLN's denoising effectiveness and linear time complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph homophily reduces label noise impact through neighbor label consistency.
- Mechanism: Higher homophily means connected nodes are more likely to share true labels, so LP smoothing propagates correct labels more effectively than noisy ones.
- Core assumption: The similarity graph built via $Z^{(L)*}$ preserves true label relationships better than the original graph.
- Evidence anchors:
  - [abstract] "higher graph homophily mitigates label noise, and label propagation performs well in high homophily settings."
  - [section 3.3.1] Empirical results show accuracy improves with increased edge homophily across all methods.
  - [corpus] No direct evidence in neighbors; weak correlation.
- Break condition: If the reconstructed graph introduces heterophilous edges, the smoothing effect degrades and noise propagation increases.

### Mechanism 2
- Claim: Multi-round label selection iteratively purifies the training set.
- Mechanism: After each LP iteration, high-confidence labels are added to the clean set, shrinking the noisy set and improving downstream reconstruction quality.
- Core assumption: Confidence scores from $F^*$ correlate with label correctness.
- Evidence anchors:
  - [abstract] "select high-confidence labels to retain for the next iteration."
  - [section 4.3] Describes iterative selection of $\varepsilon^*|N|$ nodes with highest confidence.
  - [corpus] No direct neighbor evidence; relies on paper's internal validation.
- Break condition: If confidence estimation is poor (e.g., early rounds have too many false positives), the clean set contamination limits effectiveness.

### Mechanism 3
- Claim: Efficient closed-form LP via Taylor expansion and Woodbury formula reduces computation from cubic to linear time.
- Mechanism: Replaces explicit matrix inverse with efficient multiplications and sparse operations, keeping complexity $O(n^2)$ for $Z^{(L)*}$ and $O(n)$ for $F^*$.
- Core assumption: $d \ll n$ (feature dimension small relative to nodes) and graph sparsity.
- Evidence anchors:
  - [section 4.2] Derives $F^* = F^{(0)} + \alpha_1(...)$ using Woodbury and Taylor expansion.
  - [section 4.2] States final complexity is linear in node count.
  - [corpus] No neighbor evidence; derivation is theoretical.
- Break condition: If $d$ becomes large or graph dense, the complexity advantage diminishes.

## Foundational Learning

- Concept: Graph homophily vs heterophily.
  - Why needed here: The method assumes homophily can be engineered to combat noise; understanding its role is critical.
  - Quick check question: In a graph where 90% of edges connect nodes with different labels, what is the homophily ratio and why is LP risky here?

- Concept: Label propagation as a smoothing operator.
  - Why needed here: LP is the core correction mechanism; knowing its convergence and dependence on graph structure is key.
  - Quick check question: If you set $\alpha=1$ in LP, what does the update become and why might that oversmooth?

- Concept: Confidence-based sample selection.
  - Why needed here: The iterative improvement depends on selecting truly correct labels; poor selection breaks the loop.
  - Quick check question: How does the choice of selection ratio $\varepsilon$ affect the trade-off between speed and accuracy?

## Architecture Onboarding

- Component map: Graph reconstruction module -> Label propagation engine -> Selection controller -> GNN trainer

- Critical path:
  1. Build $Z^{(L)*}$ from current clean set.
  2. Compute $F^*$ via efficient propagation.
  3. Select high-confidence nodes and move to clean set.
  4. After final iteration, train GNN on clean set for test predictions.

- Design tradeoffs:
  - Using dense $Z^{(L)*}$ improves LP accuracy but increases memory.
  - Larger $\varepsilon$ speeds convergence but risks adding noisy labels.
  - More GNN layers in reconstruction improve homophily recovery but add cost.

- Failure signatures:
  - Accuracy plateaus early → likely contamination of clean set.
  - High variance across runs → unstable confidence estimation.
  - Memory errors → $Z^{(L)*}$ too dense for available RAM.

- First 3 experiments:
  1. Run LP4GLN on Cora with 20% flip noise, vary $\varepsilon$ (0.2, 0.5, 0.8) and record accuracy/convergence rounds.
  2. Compare dense vs sparse $Z^{(L)*}$ on a medium-sized heterophilous graph; measure runtime and accuracy.
  3. Disable confidence selection (take all labels above 0.5) and observe impact on final accuracy.

## Open Questions the Paper Calls Out

- Open Question 1
  - Question: How does the performance of LP4GLN degrade when no clean label set is available, and can the algorithm be modified to function effectively in such scenarios?
  - Basis in paper: [explicit] The paper discusses the performance of LP4GLN when no clean label set is available, noting that performance drops significantly with high flip noise.
  - Why unresolved: The paper does not explore modifications or alternative strategies to handle cases where no clean labels are available.
  - What evidence would resolve it: Experimental results comparing LP4GLN's performance with and without clean labels, and potential modifications to the algorithm that could improve performance in the absence of clean labels.

- Open Question 2
  - Question: How does LP4GLN perform on graphs with noise in node features or graph topology, in addition to label noise?
  - Basis in paper: [inferred] The paper focuses on label noise and graph homophily, but real-world graphs often have noise in node features and graph topology as well.
  - Why unresolved: The paper does not address the impact of feature or topology noise on LP4GLN's performance.
  - What evidence would resolve it: Experimental results showing LP4GLN's performance on graphs with varying levels of feature and topology noise, and comparisons with other methods that handle such noise.

- Open Question 3
  - Question: What is the theoretical guarantee of LP4GLN's performance on graphs with high heterophily, and how does it compare to other methods designed for heterophilous graphs?
  - Basis in paper: [explicit] The paper provides a theoretical analysis of LP4GLN's denoising effect, but it does not specifically address its performance on graphs with high heterophily.
  - Why unresolved: The theoretical analysis focuses on label propagation and denoising, but does not extend to the impact of graph heterophily on performance.
  - What evidence would resolve it: Theoretical proofs and experimental results comparing LP4GLN's performance on heterophilous graphs with other methods specifically designed for such graphs.

## Limitations
- The method relies on having at least some clean labels to initialize the reconstruction process, limiting its applicability in extremely noisy regimes
- The assumption that confidence scores reliably indicate label correctness is not thoroughly validated, particularly in early iterations
- The complexity advantage diminishes if the feature dimension becomes large relative to the number of nodes

## Confidence

- High confidence: Linear complexity claims and the theoretical foundation for graph reconstruction via closed-form solutions
- Medium confidence: Empirical performance improvements over baselines across 10 datasets
- Low confidence: The robustness of confidence-based selection in extremely noisy regimes (60% noise)

## Next Checks
1. Test LP4GLN's sensitivity to the selection ratio ε by running a grid search (0.2 to 0.8) on a medium-sized heterophilous dataset and measuring accuracy/convergence trade-offs
2. Validate the correlation between confidence scores and actual label correctness by examining precision-recall curves of the selection process at each iteration
3. Compare the dense vs sparse graph reconstruction approaches on memory-constrained hardware to confirm the claimed O(n) complexity holds in practice