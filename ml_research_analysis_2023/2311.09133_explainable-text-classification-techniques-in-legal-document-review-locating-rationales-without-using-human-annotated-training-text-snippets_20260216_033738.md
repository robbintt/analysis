---
ver: rpa2
title: 'Explainable Text Classification Techniques in Legal Document Review: Locating
  Rationales without Using Human Annotated Training Text Snippets'
arxiv_id: '2311.09133'
source_url: https://arxiv.org/abs/2311.09133
tags:
- document
- text
- snippet
- snippets
- responsive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes two methods for automatically identifying training\
  \ text snippets in legal document review without human annotation, addressing the\
  \ challenge of the \"black box\" nature of text classification models. The methods\u2014\
  Snippet Model and Iterative Snippet Model\u2014use document-level models to score\
  \ and select high-probability text snippets as training data, iteratively refining\
  \ snippet size."
---

# Explainable Text Classification Techniques in Legal Document Review: Locating Rationales without Using Human Annotated Training Text Snippets

## Quick Facts
- arXiv ID: 2311.09133
- Source URL: https://arxiv.org/abs/2311.09133
- Reference count: 0
- Key outcome: Two methods for automatically identifying training text snippets in legal document review without human annotation, improving rationale detection precision

## Executive Summary
This paper addresses the challenge of explainable text classification in legal document review by proposing methods to automatically identify responsive text snippets (rationales) without human-annotated training data. The authors develop two approaches - Snippet Model and Iterative Snippet Model - that use document-level models to score and select high-probability text snippets as training data. Experiments on three real legal datasets demonstrate significant improvements over traditional document-level training, with up to 50% more responsive documents identified at high score thresholds and average score reductions of 0.7 versus 0.47 for baseline methods.

## Method Summary
The paper proposes two methods for automatically identifying training text snippets in legal document review without human annotation. The Snippet Model method applies a document-level model to score all overlapping snippets in training documents, selecting high-scoring snippets from responsive documents and random snippets from non-responsive documents to train a snippet-level detection model. The Iterative Snippet Model method refines this approach by starting with large snippets (1000 words) and iteratively halving the snippet size while retraining models until reaching 50 tokens. Both methods use logistic regression with bag-of-words (3-grams) and Information Gain feature selection, evaluating performance by measuring score reduction when identified rationales are removed from documents.

## Key Results
- Both snippet methods significantly outperformed document-level training in identifying responsive text snippets
- Achieved average score reductions up to 0.7 vs 0.47 for traditional methods
- Detected 50% more responsive documents at high score thresholds (≥0.9)
- Snippet Model slightly outperformed Iterative Snippet Model in overall effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level models trained on full documents are less accurate at identifying short responsive text snippets than models trained on targeted snippet-level data
- Mechanism: By applying a document-level model to score all overlapping snippets in training documents, high-scoring snippets from responsive documents and randomly sampled snippets from non-responsive documents can be used to train a snippet-level detection model
- Core assumption: The document-level model can effectively identify the most responsive portions of a document when applied to snippets
- Evidence anchors: [abstract] "Our prior research identified that predictive models created using annotated training text snippets improved the precision of a model when compared to a model created using all of a set of documents' text as training."

### Mechanism 2
- Claim: Iteratively refining snippet size from large to small tokens improves detection of responsive rationales by progressively focusing the model
- Mechanism: Start with a document-level model to identify large responsive snippets (e.g., 1000 words), train a snippet model, then iteratively halve the snippet size and retrain until reaching a minimum size (50 tokens)
- Core assumption: The information about responsiveness in a document is hierarchically distributed
- Evidence anchors: [section III] "It starts with the document-level model and applies it to identify large responsive training snippets... During each iteration, a new snippet-level model is trained using a new iteration of training snippets."

### Mechanism 3
- Claim: Using score reduction after rationale removal as an evaluation metric effectively measures rationale detection quality when labeled snippets are unavailable
- Mechanism: Remove identified rationales from documents and re-score with the document-level model. The reduction in document score indicates how much the identified rationales contributed to the original classification
- Core assumption: The document-level model's probability score reflects the contribution of all text to classification
- Evidence anchors: [section IV.B] "We evaluated these models by measuring the reduction in the document's score when their identified rationales... were removed"

## Foundational Learning

- Concept: Logistic Regression for text classification
  - Why needed here: The paper uses logistic regression as the classification algorithm for both document-level and snippet-level models
  - Quick check question: How does logistic regression handle multi-class classification versus binary classification in text classification tasks?

- Concept: Bag of words with n-grams
  - Why needed here: The paper uses bag of words with 3-grams for document representation
  - Quick check question: What's the trade-off between using unigrams, bigrams, and trigrams in terms of feature space size and semantic information captured?

- Concept: Information Gain for feature selection
  - Why needed here: The paper uses Information Gain to select 2000 tokens as features
  - Quick check question: How does Information Gain differ from other feature selection methods like chi-square or mutual information in text classification?

## Architecture Onboarding

- Component map: Document-level model -> Snippet scoring -> Snippet selection -> Snippet-level model training -> Rationale detection
- Critical path: Document-level model → Snippet scoring → Snippet selection → Snippet-level model training → Rationale detection
- Design tradeoffs: The Snippet Model method trades training time for better performance by scoring all snippets upfront, while the Iterative Snippet Model trades multiple training iterations for potentially better precision at smaller snippet sizes
- Failure signatures: Poor performance indicates either the document-level model is inaccurate (leading to bad snippet selection), the snippet selection thresholds are too strict/loose, or the feature representation isn't capturing the relevant patterns
- First 3 experiments:
  1. Implement the Snippet Model method and verify it identifies more responsive documents than the Document-level model at high score thresholds
  2. Test different minimum snippet sizes (e.g., 30, 50, 70 tokens) to find the optimal balance between precision and recall
  3. Compare the Snippet Model with the Iterative Snippet Model to quantify the performance tradeoff between methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the iterative refinement process in the Iterative Snippet Model Method propagate errors through the sequence of models, and can this be mitigated?
- Basis in paper: [explicit] The paper mentions that the classification errors propagate through the sequence of models generated by reducing the snippet size by half in each iteration
- Why unresolved: The paper acknowledges the issue of error propagation but does not provide a solution or method to mitigate it

### Open Question 2
- Question: What are the most effective strategies for selecting responsive and nonresponsive training snippets in the Snippet Model Method?
- Basis in paper: [explicit] The paper states that selecting responsive training snippets from responsive documents is not a trivial task and mentions plans to develop a more sophisticated selection algorithm
- Why unresolved: The current method for selecting responsive training snippets is described but not optimized

### Open Question 3
- Question: How does the performance of the proposed methods vary with different document classification accuracies?
- Basis in paper: [explicit] The paper notes that the performance of the snippet models is significantly impacted by the accuracy of the document-level model used for initial training
- Why unresolved: The paper does not explore how varying the accuracy of the document-level model affects the overall performance of the snippet models

## Limitations

- The evaluation methodology using score reduction as a proxy for rationale quality depends heavily on document-level model performance and lacks ground truth validation
- Key implementation details for snippet selection (the `SelectResp` function) are underspecified, making exact reproduction difficult
- The iterative refinement approach faces error propagation issues that could degrade performance at smaller snippet sizes

## Confidence

**High Confidence:** The claim that document-level models are less accurate for identifying short responsive text snippets than models trained on targeted snippet-level data is well-supported by the experimental results.

**Medium Confidence:** The iterative refinement approach showing improved performance at smaller snippet sizes has experimental support but faces implementation uncertainties.

**Low Confidence:** The evaluation methodology using score reduction as a proxy for rationale quality is problematic due to its dependence on document-level model performance and lack of ground truth validation.

## Next Checks

**Validation Check 1:** Implement the Document-Level Model first and verify its performance on document classification before proceeding to snippet methods. Check precision, recall, and F1 scores specifically for the document classification task.

**Validation Check 2:** Test the Snippet Model Method with different minimum snippet sizes (30, 50, 70 tokens) to empirically determine the optimal balance between precision and recall.

**Validation Check 3:** Compare the Snippet Model with the Iterative Snippet Model using a consistent evaluation framework that includes both score reduction and a small sample of human-annotated snippets if available.