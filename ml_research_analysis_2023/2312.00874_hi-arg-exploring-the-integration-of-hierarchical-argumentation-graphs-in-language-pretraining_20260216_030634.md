---
ver: rpa2
title: 'Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in
  Language Pretraining'
arxiv_id: '2312.00874'
source_url: https://arxiv.org/abs/2312.00874
tags:
- graph
- hi-arg
- pre-training
- greasearg
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Hi-ArG, a new graph structure to organize
  arguments by recording semantic and logical relations at different levels. Two methods
  to exploit Hi-ArG are proposed: a text-graph multi-modal model called GreaseArG
  and a pre-training framework augmented with graph information.'
---

# Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining

## Quick Facts
- **arXiv ID**: 2312.00874
- **Source URL**: https://arxiv.org/abs/2312.00874
- **Reference count**: 29
- **Primary result**: Hi-ArG outperforms same-scale language models on argumentation tasks after further pre-training and fine-tuning, and incorporating graph information during pre-training improves vanilla language models.

## Executive Summary
This paper introduces Hi-ArG, a hierarchical graph structure that organizes arguments by recording semantic and logical relations at different levels. The structure consists of an intra-argument graph (using AMR) to capture fine-grained semantic relations within arguments, and an inter-argument graph to record logical relations between arguments. Two methods to exploit Hi-ArG are proposed: GreaseArG, a text-graph multi-modal model that integrates GNN layers with transformer layers, and a pre-training framework augmented with graph information. Experiments on two argumentation tasks (key point matching and stance classification) demonstrate that GreaseArG outperforms same-scale language models, and incorporating graph information during pre-training improves vanilla language models.

## Method Summary
The Hi-ArG construction pipeline extracts arguments from the args.me corpus, parses them into AMR graphs to create the intra-argument graph, and extracts topic and stance information to build the inter-argument graph. The GreaseArG model uses a RoBERTa backbone with GNN layers as add-ons and cross-modal attention layers to integrate text and graph representations. The pre-training framework uses masking tasks (MLM, MNM, MEM), graph structure tasks (GCL, TOP, DIR), and relative-augmented samples with an RSD task. The model is fine-tuned on downstream tasks (KPM and CESC) using task-specific heads.

## Key Results
- GreaseArG outperforms same-scale language models on argumentation tasks after further pre-training and fine-tuning
- Incorporating graph information during pre-training improves the performance of vanilla language models
- The hierarchical structure of Hi-ArG enables models to capture both fine-grained semantic relations within arguments and logical relations between arguments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hi-ArG's hierarchical structure allows models to capture both fine-grained semantic relations within arguments and logical relations between arguments.
- **Mechanism**: The intra-argument (intra-arg) graph stores semantic relations using AMR, preserving detailed concept and predicate connections. The inter-argument (inter-arg) graph stores logical relations like support/attack between arguments. This dual-level representation provides richer context than flat structures.
- **Core assumption**: AMR graphs are expressive enough to represent argument semantics without significant information loss, and logical relations between arguments can be effectively modeled as graph edges.
- **Evidence anchors**:
  - [abstract] "Hi-ArG can retain more semantics within arguments at the lower (intra-argument) level and record relations between arguments at the upper (inter-argument) level."
  - [section] "Concepts in the graph include gun, person, and kill-01... Relations in the graph include :ARG0 and :ARG1..."
  - [corpus] Weak. Only 8 papers in corpus; no direct evidence on AMR expressiveness for arguments.
- **Break condition**: If AMR parsing fails to capture critical semantic nuances or if logical relations between arguments are too complex to model as simple edges.

### Mechanism 2
- **Claim**: Multi-modal training on text and graph representations improves model understanding of argumentation tasks.
- **Mechanism**: GreaseArG processes both textual and graph inputs simultaneously using GNN layers integrated with transformer layers. Cross-modal attention layers enable information exchange between modalities. Pre-training tasks like MLM, MNM, MEM, GCL, TOP, and DIR train the model to understand both text and graph structures.
- **Core assumption**: The graph representation contains complementary information to text that improves task performance when properly integrated.
- **Evidence anchors**:
  - [abstract] "Experiments on two argumentation tasks have shown that after further pre-training and fine-tuning, GreaseArG supersedes same-scale language models on these tasks..."
  - [section] "GreaseArG uses graph neural network (GNN) layers as add-ons to facilitate this issue."
  - [corpus] Weak. Only 8 papers in corpus; no direct evidence on multi-modal training effectiveness.
- **Break condition**: If the integration between text and graph representations creates confusion rather than complementarity, or if the pre-training tasks don't align with downstream task requirements.

### Mechanism 3
- **Claim**: Relative-augmented pre-training samples improve model performance by exposing it to related arguments during training.
- **Mechanism**: During pre-training, samples are augmented with "relatives" - topic-related sentences from Hi-ArG that are either supporting, attacking, or non-relevant to the main document. The Relative Stance Detection (RSD) task trains the model to classify these relationships.
- **Core assumption**: Exposure to related arguments during pre-training helps the model learn argumentation patterns that transfer to downstream tasks.
- **Evidence anchors**:
  - [abstract] "incorporating graph information during further pre-training can also improve the performance of vanilla language models."
  - [section] "We can classify each relative as supporting, attacking, or non-relevant concerning the document it relates to."
  - [corpus] Weak. Only 8 papers in corpus; no direct evidence on relative-augmented training.
- **Break condition**: If the sampled relatives are not representative of the argumentation patterns in downstream tasks, or if the RSD task doesn't align with actual task requirements.

## Foundational Learning

- **Concept**: Abstract Meaning Representation (AMR)
  - Why needed here: AMR provides the semantic backbone for the intra-arg graph, representing argument structure as a directed graph with labeled nodes and edges.
  - Quick check question: What are the two main types of elements in an AMR graph, and what do they represent?

- **Concept**: Graph Neural Networks (GNNs)
  - Why needed here: GNNs process the graph structure in Hi-ArG, enabling message passing between nodes to capture relational information.
  - Quick check question: How do GNNs handle directed edges in Hi-ArG, given that message passing typically requires bidirectional information flow?

- **Concept**: Contrastive Learning
  - Why needed here: Used in some KPM approaches to learn representations by contrasting similar and dissimilar argument-key point pairs.
  - Quick check question: In the context of KPM, what is the key difference between using contrastive learning versus pair classification approaches?

## Architecture Onboarding

- **Component map**: Hi-ArG construction pipeline (extracting → parsing → merging) -> GreaseArG model (LM backbone + GNN layers + cross-modal attention) -> Pre-training framework (masking tasks + graph structure tasks + relative-augmented samples) -> Downstream task adapters (KPM and CESC specific heads)
- **Critical path**: Hi-ArG construction → GreaseArG pre-training → Fine-tuning on downstream task → Evaluation
- **Design tradeoffs**:
  - Using AMR vs. other semantic representations (AMR is expressive but parsing can be noisy)
  - Integrating text and graph vs. separate processing (integration captures complementarity but adds complexity)
  - Relative-augmented samples vs. plain samples (augmentation provides richer context but increases computational cost)
- **Failure signatures**:
  - Poor performance on both KPM and CESC: likely issues with Hi-ArG construction or pre-training task alignment
  - Good performance on KPM but poor on CESC: likely issues with stance classification in GreaseArG
  - Good performance on CESC but poor on KPM: likely issues with argument-key point matching in GreaseArG
- **First 3 experiments**:
  1. Test AMR parsing quality on a small sample of arguments to ensure semantic information is preserved
  2. Verify that Hi-ArG construction correctly identifies and merges isomorphic nodes
  3. Test GreaseArG's ability to process simple text-graph pairs before full pre-training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Hi-ArG-based models compare to large language models (LLMs) like GPT-4 on argumentation tasks when given the same computational resources?
- Basis in paper: [explicit] The authors mention that primitive experiments with ChatGPT showed poor performance compared to their models, but they did not conduct a full comparison due to resource limitations.
- Why unresolved: The authors acknowledge that a full comparison with LLMs is needed but was not feasible due to resource constraints.
- What evidence would resolve it: Conducting experiments to fine-tune LLMs on argumentation tasks using Hi-ArG and comparing their performance to Hi-ArG-based models under the same computational budget.

### Open Question 2
- Question: What is the impact of incorporating inter-argument graphs (representing logical relations between arguments) during further pre-training on the performance of argumentation tasks?
- Basis in paper: [inferred] The authors mention that they only use intra-argument graphs during further pre-training to avoid bias towards downstream tasks, but do not explore the potential benefits of inter-argument graphs.
- Why unresolved: The authors deliberately exclude inter-argument graphs during further pre-training, leaving their potential impact unexplored.
- What evidence would resolve it: Conducting experiments to further pre-train models with both intra- and inter-argument graphs and comparing their performance on argumentation tasks to models trained only with intra-argument graphs.

### Open Question 3
- Question: How does the quality of the Hi-ArG graph affect the performance of Hi-ArG-based models on argumentation tasks?
- Basis in paper: [inferred] The authors acknowledge that the quality of Hi-ArG can be improved using more powerful models and tools, implying that the current quality may impact performance.
- Why unresolved: The authors do not investigate the relationship between Hi-ArG quality and model performance.
- What evidence would resolve it: Conducting experiments to generate Hi-ArG graphs using different levels of model quality (e.g., different AMR parsing models) and comparing the performance of models trained on these graphs on argumentation tasks.

## Limitations
- The paper's claims are limited by the extremely small evaluation corpus (8 papers) and lack of ablation studies to isolate the contributions of individual components
- The AMR parsing stage introduces significant noise that is not adequately addressed, and the effectiveness of the relative-augmented pre-training samples remains unproven
- The paper lacks comparison against strong baseline models with similar parameter counts

## Confidence
- **High confidence**: The hierarchical graph structure concept and its potential to capture multi-level argumentation relations
- **Medium confidence**: The GreaseArG model architecture and its ability to integrate text and graph representations
- **Low confidence**: The effectiveness of relative-augmented pre-training samples and the overall performance gains

## Next Checks
1. Conduct ablation studies removing the inter-arg graph, intra-arg graph, and relative-augmented samples separately to quantify their individual contributions
2. Evaluate the AMR parsing quality on a larger sample of arguments to measure semantic information preservation
3. Test GreaseArG on additional argumentation datasets beyond the two used in the paper to assess generalizability