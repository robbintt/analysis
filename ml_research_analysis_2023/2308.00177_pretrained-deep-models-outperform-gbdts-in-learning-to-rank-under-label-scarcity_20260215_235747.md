---
ver: rpa2
title: Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity
arxiv_id: '2308.00177'
source_url: https://arxiv.org/abs/2308.00177
tags:
- data
- simclr
- pretraining
- pretrained
- finetuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that pretrained deep models can significantly
  outperform gradient boosted decision trees (GBDTs) in Learning-to-Rank (LTR) tasks
  when labeled data is scarce and unlabeled data is abundant. The key insight is that
  unsupervised pretraining methods like SimCLR and SimSiam, originally designed for
  images, can be adapted to tabular LTR problems to leverage unlabeled data effectively.
---

# Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity

## Quick Facts
- arXiv ID: 2308.00177
- Source URL: https://arxiv.org/abs/2308.00177
- Authors: [not specified]
- Reference count: 17
- Key result: Pretrained deep models achieve up to 38% improvement in NDCG ranking metrics over non-pretrained models and GBDTs when labeled data is scarce

## Executive Summary
This paper demonstrates that unsupervised pretraining methods like SimCLR and SimSiam can be effectively adapted to tabular Learning-to-Rank problems under label scarcity. By leveraging abundant unlabeled data, pretrained models significantly outperform traditional gradient boosted decision trees (GBDTs) on ranking tasks. The authors introduce SimCLR-Rank, a computationally efficient modification that contrasts items within query groups rather than across the entire batch, achieving 20x runtime savings while maintaining strong performance. Extensive experiments on public LTR datasets show that pretrained models not only achieve superior ranking performance but also exhibit better robustness on outlier queries compared to non-pretrained approaches.

## Method Summary
The method adapts contrastive learning pretraining to tabular LTR by applying data augmentations (zeroing features, query-group mixing, Gaussian noise) to create positive pairs within queries. SimCLR and SimSiam are pretrained on unlabeled data using these augmentations, with SimCLR-Rank optimizing computational efficiency by limiting contrastive comparisons to within-query items. The pretrained models are then finetuned using LambdaRank loss on the scarce labeled data, with identity initialization of finetuning layers to reduce variance. The approach is evaluated across three public LTR datasets (MSLR-30K, Yahoo, Istella) with 0.1% labeled data, measuring performance using NDCG@5 and robust-NDCG@5 metrics.

## Key Results
- Pretrained models achieve up to 38% improvement in NDCG ranking metrics over non-pretrained models and GBDTs
- SimCLR-Rank reduces memory and time complexity from O((N L)²) to O(N L²) with 20x runtime savings
- Pretrained models show better robustness on outlier queries compared to non-pretrained approaches
- Identity initialization of finetuning layers reduces variance and improves performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on unlabeled data improves ranking performance when labeled data is scarce
- Mechanism: The model learns useful representations from unlabeled data through contrastive learning, which transfers to downstream ranking tasks
- Core assumption: The unlabeled data contains sufficient signal about the underlying ranking structure
- Evidence anchors:
  - [abstract] "unsupervised pretraining methods like SimCLR and SimSiam, originally designed for images, can be adapted to tabular LTR problems to leverage unlabeled data effectively"
  - [section] "pretrained models achieve up to 38% improvement in NDCG ranking metrics over non-pretrained models and GBDTs"
  - [corpus] Weak - corpus contains related work but no direct evidence about this specific mechanism
- Break condition: If unlabeled data is too noisy or doesn't capture ranking-relevant patterns, pretraining won't transfer effectively

### Mechanism 2
- Claim: SimCLR-Rank modification reduces computational cost while maintaining performance
- Mechanism: By only contrasting items within the same query group instead of across the entire batch, the method reduces memory and time complexity
- Core assumption: Contrasting within query groups provides sufficient learning signal for LTR tasks
- Evidence anchors:
  - [section] "SimCLR-Rank reduces memory and time complexity from O((N L)²) to O(N L²)"
  - [section] "SimCLR-Rank brought us substantial savings in runtime (20x) and memory"
  - [corpus] Weak - corpus mentions related methods but doesn't discuss this specific computational optimization
- Break condition: If inter-query group relationships are crucial for learning good representations, limiting to intra-query contrast might limit performance

### Mechanism 3
- Claim: Pretrained models show better robustness on outlier queries
- Mechanism: Unsupervised pretraining helps models learn more generalizable representations that perform better on data points outside the typical distribution
- Core assumption: The contrastive learning process creates representations that are more robust to distributional shifts
- Evidence anchors:
  - [abstract] "pretrained models exhibit better robustness on outlier queries compared to non-pretrained approaches"
  - [section] "We find that unsupervised pretraining increases robustness against real outlier data"
  - [corpus] Weak - corpus doesn't discuss robustness to outliers specifically
- Break condition: If outliers are fundamentally different in structure rather than just being rare, pretraining might not help

## Foundational Learning

- Concept: Contrastive learning and positive/negative pairs
  - Why needed here: The paper adapts SimCLR and SimSiam methods which rely on contrasting augmented views of data
  - Quick check question: What's the difference between positive and negative pairs in contrastive learning?

- Concept: NDCG (Normalized Discounted Cumulative Gain) metric
  - Why needed here: Primary evaluation metric for ranking performance
  - Quick check question: How does NDCG@5 differ from regular NDCG and why is truncation used?

- Concept: Learning-to-Rank problem formulation
  - Why needed here: The entire paper addresses LTR under label scarcity
  - Quick check question: What distinguishes LTR from standard classification or regression tasks?

## Architecture Onboarding

- Component map:
  - Base encoder (h): Extracts initial features from input
  - Projection head (g): Maps features to contrastive space
  - Predictor (pred): Additional MLP used in SimSiam only
  - Augmentation module: Applies transformations to create positive pairs
  - Finetuning layers: Additional layers added for downstream ranking task

- Critical path:
  1. Pretrain on unlabeled data using SimCLR-Rank loss
  2. Freeze pretrained model and add finetuning layers
  3. Finetune on labeled data using ranking loss (LambdaRank)
  4. Evaluate on test set using NDCG

- Design tradeoffs:
  - Larger models and batch sizes improve performance but increase memory/time cost
  - More augmentation provides better generalization but can introduce noise
  - Identity initialization of finetuning layers reduces variance but may slow convergence

- Failure signatures:
  - High variance in test NDCG across runs suggests training instability
  - Zero vectors in output during pretraining indicate learning issues
  - Loss climbing during pretraining suggests optimization problems

- First 3 experiments:
  1. Compare SimCLR-Rank vs standard SimCLR on a small dataset to verify computational benefits
  2. Test different augmentation strategies (zeros, qg, Gaussian) to find best performing option
  3. Evaluate linear probing vs end-to-end finetuning to determine optimal transfer approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do unsupervised pretraining methods perform on other tabular LTR datasets beyond MSLR-30K, Yahoo, and Istella?
- Basis in paper: [explicit] The paper evaluates pretraining methods on three specific public datasets and notes that pretraining success is demonstrated "across all datasets"
- Why unresolved: The evaluation is limited to three well-known datasets. There are many other tabular LTR datasets (like LETOR, AOL, etc.) that could reveal whether the performance gains generalize to different domains and data distributions
- What evidence would resolve it: Testing SimCLR-Rank and other pretraining methods on a diverse set of 5-10 additional tabular LTR datasets with varying characteristics (different domains, feature distributions, query-to-document ratios)

### Open Question 2
- Question: What is the theoretical explanation for why linear probing fails in ranking but works in other domains like images?
- Basis in paper: [explicit] The paper states "Unlike in the image domain (where SimSiam and SimCLR were proposed), linear probing... cannot be used for finetuning and gives poor results"
- Why unresolved: The paper observes this phenomenon empirically but doesn't explain the underlying reason why ranking problems differ fundamentally from image classification in terms of representation utility
- What evidence would resolve it: A theoretical analysis comparing the representation characteristics needed for ranking versus classification, or ablation studies showing what properties of ranking representations make them incompatible with linear separation

### Open Question 3
- Question: How does the performance of pretrained deep models compare to state-of-the-art GBDTs on outlier queries when using larger labeled datasets (e.g., 1% or 10% instead of 0.1%)?
- Basis in paper: [explicit] The paper specifically studies "label scarcity" with 0.1% labeled data and finds pretrained models outperform GBDTs both overall and on outliers
- Why unresolved: The paper's focus on extreme label scarcity leaves open the question of whether the pretraining advantage persists or diminishes as labeled data becomes more abundant
- What evidence would resolve it: Comparative experiments testing the same models on varying fractions of labeled data (0.1%, 1%, 10%, 100%) to identify the crossover point where GBDTs regain superiority

### Open Question 4
- Question: Can the SimCLR-Rank loss be further optimized or adapted to work better with extremely long query groups where computational complexity becomes prohibitive?
- Basis in paper: [explicit] The paper introduces SimCLR-Rank specifically to reduce computational cost from O((NL)²) to O(NL²) but doesn't explore whether this approximation introduces ranking-specific biases
- Why unresolved: While SimCLR-Rank improves computational efficiency, the paper doesn't investigate whether the loss function could be modified to better handle edge cases or whether alternative ranking-specific contrastive losses might perform better
- What evidence would resolve it: Experiments testing variants of SimCLR-Rank (e.g., hierarchical contrastive losses, attention-based contrastive mechanisms) on datasets with very long query groups to identify optimal loss formulations for different query group sizes

## Limitations
- The study focuses specifically on tabular LTR problems with feature-level augmentations, limiting applicability to other domains
- Computational benefits of SimCLR-Rank, while demonstrated, rely on specific query-group structures that may not generalize
- The robustness findings for outlier queries, though promising, lack detailed analysis of which outlier types benefit most

## Confidence
- **High**: Core empirical findings about 38% NDCG improvement across multiple datasets
- **Medium**: Claims about computational efficiency gains, as these are dataset-dependent
- **Low**: Broader claims about pretraining benefits across all tabular problems, as the study focuses narrowly on LTR

## Next Checks
1. Test SimCLR-Rank performance on non-LTR tabular tasks to assess domain generality of computational improvements
2. Analyze outlier query characteristics to determine which types benefit most from pretraining
3. Compare pretraining benefits across different levels of label scarcity (0.01% to 10%) to identify optimal regimes