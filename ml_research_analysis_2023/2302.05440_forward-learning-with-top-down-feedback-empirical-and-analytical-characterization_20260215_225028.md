---
ver: rpa2
title: 'Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization'
arxiv_id: '2302.05440'
source_url: https://arxiv.org/abs/2302.05440
tags:
- learning
- pepita
- feedback
- forward
- herr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the dynamics and connections between "forward-only"
  learning algorithms, which avoid backward passes, and biologically-inspired learning
  mechanisms. The authors focus on two such algorithms, Forward-Forward (FF) and PEPITA,
  demonstrating that PEPITA is a special case of FF when negative samples are generated
  via top-down feedback.
---

# Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization

## Quick Facts
- arXiv ID: 2302.05440
- Source URL: https://arxiv.org/abs/2302.05440
- Authors: 
- Reference count: 40
- Key outcome: This paper explores the dynamics and connections between "forward-only" learning algorithms, which avoid backward passes, and biologically-inspired learning mechanisms.

## Executive Summary
This paper investigates the theoretical and practical aspects of forward-only learning algorithms, specifically Forward-Forward (FF) and PEPITA, which avoid backpropagation. The authors demonstrate that PEPITA is equivalent to a Forward-Forward framework with top-down feedback connections, providing a bridge between these biologically-inspired algorithms and more traditional learning methods. Through analytical derivations and empirical experiments, they show that PEPITA can train deeper networks than previously tested and can be enhanced with weight mirroring techniques to achieve performance close to backpropagation.

## Method Summary
The paper compares Forward-Forward and PEPITA algorithms for training neural networks without backpropagation. PEPITA uses top-down feedback connections and is shown to be equivalent to FF when negative samples are generated via top-down feedback. The authors propose a time-local formulation of PEPITA and extend weight mirroring techniques to improve alignment and accuracy. Experiments train fully connected networks with 1-5 hidden layers on MNIST, CIFAR-10, and CIFAR-100 datasets, comparing PEPITA's performance to Backpropagation (BP), Feedback Alignment (FA), and Direct Random Target Projection (DRTP) in terms of test accuracy and convergence rate.

## Key Results
- PEPITA can train deeper networks (up to 5 hidden layers) than previously tested forward-only algorithms
- PEPITA with weight mirroring achieves accuracy close to backpropagation on CIFAR-10 and CIFAR-100 datasets
- PEPITA is well-approximated by an "adaptive-feedback-alignment" algorithm, providing analytical insights into its learning dynamics

## Why This Works (Mechanism)

### Mechanism 1
PEPITA can be formulated as a sum of Hebbian and anti-Hebbian terms, making it local both in space and in time. By separating the brackets in the PEPITA update rule and mixing presynaptic terms from both forward passes, the update can be written as two Hebbian terms that can be applied independently during each pass. The core assumption is that replacing the presynaptic term in the modulated pass with the same term from the clean pass is valid and has negligible impact on accuracy.

### Mechanism 2
PEPITA effectively implements "feedback-alignment" with an adaptive feedback matrix that depends on the upstream weights. Through Taylor expansion, the perturbation in the modulated pass is shown to be small, allowing the update rule to be approximated as FA-like with an adaptive feedback matrix. The core assumption is that the perturbation applied in the modulation pass is small compared to the input, allowing for a Taylor expansion.

### Mechanism 3
Weight mirroring can be extended to PEPITA by factorizing the feedback matrix into layer-wise components and applying WM layer-wise. By factorizing F into the product of as many matrices as the number of layers, each layer can have its own feedback matrix that can be aligned with the corresponding forward weights using WM. The core assumption is that the factorization of F into layer-wise components is valid and can be used to apply WM in a layer-wise manner.

## Foundational Learning

- Concept: Taylor expansion and its application in approximating functions
  - Why needed here: To justify the approximation of PEPITA as an adaptive-feedback-alignment algorithm by assuming the perturbation is small enough for Taylor expansion
  - Quick check question: If the perturbation ∥Fe∥ is not small compared to ∥x∥, would the Taylor expansion still be a valid approximation?

- Concept: Hebbian learning and its formulation
  - Why needed here: To understand the Hebbian and anti-Hebbian terms in the modified PEPITA update rule and their implications for locality in time and space
  - Quick check question: If the presynaptic and postsynaptic terms are not multiplied together, can the update still be considered Hebbian?

- Concept: Weight mirroring algorithm and its extension to non-FA algorithms
  - Why needed here: To grasp how WM can be applied to PEPITA by factorizing the feedback matrix and aligning each layer's feedback weights with the corresponding forward weights
  - Quick check question: If the factorization of F is not unique, how does the choice of factorization affect the effectiveness of WM?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden layers (with forward weights Wℓ and feedback matrix F) -> Output layer

- Critical path:
  1. Clean forward pass: Compute activations hℓ = σℓ(Wℓhℓ-1) for each layer
  2. Compute error e = hL - y at the output layer
  3. Modulated forward pass: Compute activations herr_ℓ = σℓ(Wℓherr_ℓ-1) for each layer, with input modulated by the error
  4. Apply PEPITA weight updates: For hidden layers, ∆Wℓ = (hℓ - herr_ℓ)(herr_ℓ-1)⊤; for the output layer, ∆WL = e(herr_L-1)⊤
  5. Optionally, apply weight mirroring updates to align feedback weights with forward weights

- Design tradeoffs:
  - Time locality vs. accuracy: The time-local formulation of PEPITA (PEPITA-time-local) may reduce accuracy compared to the original formulation
  - Depth vs. performance: PEPITA can train deeper networks, but performance may degrade with increasing depth
  - Weight mirroring vs. convergence: WM improves alignment and accuracy but may slow down convergence initially

- Failure signatures:
  - If the network fails to learn or converges very slowly, check if the perturbation ∥Fe∥ is too large for the Taylor expansion to be valid
  - If the accuracy is significantly lower than expected, verify if the factorization of F for WM is stable and if the layer-wise updates are not interfering with each other
  - If the network's performance degrades with depth, consider using activation normalization or other techniques to stabilize learning in deeper networks

- First 3 experiments:
  1. Train a 1-hidden-layer network on MNIST using PEPITA with and without weight mirroring to compare accuracy and convergence
  2. Extend the network to 3 hidden layers and observe the impact on performance, using activation normalization if necessary
  3. Apply the time-local formulation of PEPITA (PEPITA-time-local) and compare its accuracy and convergence to the original formulation

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal initialization strategy for the feedback matrix F in PEPITA to maximize alignment and learning speed?
- Basis in paper: The paper discusses different initialization strategies for F (uniform vs normal distributions) and their impact on alignment and accuracy
- Why unresolved: The paper shows that different initializations lead to varying degrees of alignment and accuracy, but does not identify an optimal strategy
- What evidence would resolve it: Systematic comparison of different initialization strategies on various datasets and network architectures, measuring both alignment and final accuracy

### Open Question 2
How does the choice of activation function (e.g., ReLU vs erf) affect the equivalence between PEPITA and Forward-Forward algorithms?
- Basis in paper: The paper notes that the equivalence between PEPITA and FF updates depends on the activation function, with ReLU being a special case where the equivalence is exact
- Why unresolved: The paper only discusses ReLU and erf activations, leaving open the question of how other activation functions impact the relationship between the algorithms
- What evidence would resolve it: Analytical and empirical comparison of PEPITA and FF updates across a range of activation functions, quantifying the degree of equivalence

### Open Question 3
Can the weight mirroring technique be further optimized for PEPITA, perhaps by incorporating layer-specific learning rates or adaptive mirroring schedules?
- Basis in paper: The paper proposes a generalized weight mirroring approach for PEPITA and shows its benefits, but uses a uniform approach across layers
- Why unresolved: The paper does not explore variations in the weight mirroring technique that might lead to improved performance
- What evidence would resolve it: Experiments comparing different weight mirroring strategies (e.g., layer-specific rates, adaptive schedules) on multiple datasets and network depths

## Limitations
- The reliance on Taylor expansion approximations to characterize PEPITA as an adaptive-feedback-alignment algorithm may not hold for all network architectures or input distributions
- The extension of weight mirroring to PEPITA introduces additional complexity through the factorization of the feedback matrix, and the stability of this factorization across different network depths remains unverified
- The paper does not provide a theoretical explanation for the performance degradation in deeper networks or explore the limits of PEPITA's applicability

## Confidence
- **High**: The equivalence between PEPITA and Forward-Forward with top-down feedback is well-established mathematically
- **Medium**: The adaptive-feedback-alignment characterization via Taylor expansion is plausible but depends on perturbation size assumptions
- **Medium**: Empirical results showing PEPITA's ability to train deeper networks are convincing, though comparisons to backpropagation could be more extensive

## Next Checks
1. Systematically test the Taylor expansion approximation across different perturbation magnitudes and network architectures to establish its validity bounds
2. Evaluate the stability and uniqueness of the feedback matrix factorization under different initialization schemes and network depths
3. Conduct ablation studies on the time-local formulation to quantify the accuracy trade-off compared to the original PEPITA algorithm