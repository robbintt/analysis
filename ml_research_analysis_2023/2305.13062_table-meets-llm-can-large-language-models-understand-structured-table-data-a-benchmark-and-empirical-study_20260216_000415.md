---
ver: rpa2
title: 'Table Meets LLM: Can Large Language Models Understand Structured Table Data?
  A Benchmark and Empirical Study'
arxiv_id: '2305.13062'
source_url: https://arxiv.org/abs/2305.13062
tags:
- table
- llms
- tasks
- structural
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SUC, a benchmark to evaluate structural understanding
  capabilities of LLMs on tabular data through seven tasks. It finds that performance
  varies with input choices like table format, content order, role prompting, and
  partition marks.
---

# Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study

## Quick Facts
- arXiv ID: 2305.13062
- Source URL: https://arxiv.org/abs/2305.13062
- Reference count: 14
- Key outcome: Self-augmented prompting with HTML markup improves accuracy by 2-5.68% on downstream tabular tasks

## Executive Summary
This paper introduces SUC, a benchmark designed to evaluate large language models' (LLMs) structural understanding of tabular data across seven tasks. Through systematic experimentation, the authors identify that input design choices—particularly HTML markup format, role prompting, and self-augmented two-phase prompting—significantly impact LLM performance. The study demonstrates that HTML outperforms natural language separators by 6.76% accuracy, and self-augmented prompting yields 2-5.68% improvements on downstream tasks like TabFact, HybridQA, and ToTTo. The code and data are open-sourced to facilitate future research in table-aware LLM systems.

## Method Summary
The authors evaluate GPT-3 models on the SUC benchmark, testing seven structural understanding tasks (partition, cell lookup, reverse lookup, column/row retrieval, size detection, merged cell detection) across various input designs. They compare markup languages (HTML/XML/JSON) against natural language with separators, test role prompting and partition marks, and implement self-augmented prompting that first generates intermediate structural knowledge before answering downstream questions. The approach uses in-context learning without fine-tuning, measuring accuracy and BLEU scores across multiple tabular datasets.

## Key Results
- HTML markup format outperforms natural language separators by 6.76% accuracy on structural tasks
- Self-augmented prompting improves downstream task accuracy by 2-5.68% across TabFact, HybridQA, SQA, Feverous, and ToTTo
- Including structural features like table size and merged cell position in prompts boosts parsing task performance
- Role prompting and format explanations contribute to the highest overall accuracy of 65.43% on seven benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HTML markup input format outperforms natural language with separators by 6.76% accuracy on structural tasks.
- Mechanism: HTML format provides explicit structural tokens (e.g., `<td>`, `<tr>`, `<th>`) that LLMs trained on web data can recognize and parse more effectively than ad-hoc separators like `|`.
- Core assumption: The training corpus for LLMs includes substantial HTML/web table data, giving them learned priors for this format.
- Evidence anchors:
  - [abstract]: "We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting and partition marks."
  - [section]: "The results show that the system's overall accuracy gets highest when using the HTML markup language with format explanations and role prompts, and without order change, achieving a 65.43% overall accuracy on seven tasks."
- Break condition: If the model is trained without substantial HTML exposure, or if the HTML is malformed, the advantage disappears.

### Mechanism 2
- Claim: Self-augmented prompting (two-phase generation) improves downstream task accuracy by 2-5.68% across multiple datasets.
- Mechanism: First prompt forces the LLM to identify critical values and ranges from the table; second prompt includes this intermediate knowledge, giving the model a richer in-context learning environment for the final answer.
- Core assumption: LLMs can extract meaningful structural features from tables and use them effectively in subsequent reasoning.
- Evidence anchors:
  - [abstract]: "These structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, e.g., TabFact(↑2.31%), HybridQA(↑2.13%), SQA(↑2.72%), Feverous(↑0.84%), and ToTTo(↑5.68%)."
  - [section]: "We find one notable insights that the models perform better using self-augmented prompting than 1-shot, as seen in the 'SA' rows where the models are given self-generated information: format explanation, key range and values identification, and structural information description."
- Break condition: If the model lacks the ability to reliably identify critical values/ranges, or if the intermediate knowledge is incorrect, downstream performance may degrade.

### Mechanism 3
- Claim: Including structural features like table size and merged cell position in prompts boosts performance, especially on tasks requiring parsing.
- Mechanism: Adding explicit structural metadata constrains the model's search space and provides a "structural awareness" signal during in-context learning.
- Core assumption: LLMs struggle to extract these features autonomously, so manual injection of this information helps.
- Evidence anchors:
  - [section]: "We discover that the LLM have a lower performance over the table size detection task... It motivates us to append such structural features to the input, e.g., table size and merged cell position, to provide a more structure-aware in-context learning environment for downstream tasks."
  - [section]: "The drop of table size gives an overall 0.4% decrease, as seen by the small difference between the '1-shot' row and the '1-shot w/o table size' row."
- Break condition: If the structural metadata is incorrect or irrelevant to the task, it may mislead the model.

## Foundational Learning

- Concept: In-context learning (zero/few-shot prompting)
  - Why needed here: The benchmark evaluates LLMs without fine-tuning, relying entirely on prompt design to elicit correct behavior.
  - Quick check question: If you remove the few-shot example, does accuracy drop more than 30% on structural tasks?

- Concept: Chain of thought and intermediate reasoning
  - Why needed here: Self-augmented prompting builds on CoT principles but applies them to structural feature extraction rather than arithmetic reasoning.
  - Quick check question: Can you manually construct a two-step prompt where the first step identifies a table property and the second uses it to answer a downstream question?

- Concept: Structural parsing and tokenization
  - Why needed here: Different input formats (CSV, JSON, HTML) require different parsing strategies; the model's performance depends on how well it can tokenize and interpret the structure.
  - Quick check question: Given a table in HTML vs CSV format, can you predict which will yield higher accuracy on a cell lookup task?

## Architecture Onboarding

- Component map: Input Formatter -> Prompt Constructor -> Self-Augmentation Module -> LLM -> Evaluator
- Critical path:
  1. Load table and task specification
  2. Apply chosen input format and augmentations
  3. (Optional) Run self-augmented prompting to generate intermediate structural knowledge
  4. Send prompt to LLM
  5. Parse and evaluate output
- Design tradeoffs:
  - HTML format offers higher accuracy but may require escaping special characters
  - Self-augmented prompting increases inference time (two LLM calls) but improves accuracy
  - Adding structural metadata improves parsing tasks but may clutter prompts for pure reasoning tasks
- Failure signatures:
  - Low accuracy on size detection → likely missing or incorrect table size metadata
  - Poor cell lookup → check if input format uses ambiguous separators
  - Self-augmented prompting degrades performance → intermediate generation may be incorrect or irrelevant
- First 3 experiments:
  1. Run all seven tasks with HTML format vs NL+Sep format, measure accuracy delta
  2. Compare 1-shot vs zero-shot accuracy on all tasks to quantify in-context learning dependence
  3. Test self-augmented prompting on one downstream task (e.g., TabFact) with and without intermediate structural knowledge generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of LLMs in understanding complex merged cell structures in tables?
- Basis in paper: [inferred] The paper mentions that the performance of LLMs varies with different input choices and that merged cells pose a challenge for LLMs in detecting their positions.
- Why unresolved: The paper does not provide detailed analysis on the specific challenges or limitations of LLMs in handling merged cells.
- What evidence would resolve it: A comprehensive study comparing LLM performance on tables with various merged cell structures and configurations.

### Open Question 2
- Question: How does the order of supplementary information (like questions or context) relative to tables impact LLM performance on downstream tasks?
- Basis in paper: [explicit] The paper mentions that putting external information ahead of tables could help LLM to generalize better and gain more context of the structural information of tables.
- Why unresolved: The paper does not provide a detailed analysis of how the order of supplementary information impacts LLM performance on downstream tasks.
- What evidence would resolve it: A systematic study varying the order of supplementary information and its impact on LLM performance across different tasks.

### Open Question 3
- Question: What is the impact of different table serialization methods on LLM performance for various tabular tasks?
- Basis in paper: [explicit] The paper discusses various table serialization methods and their impact on LLM performance, but does not provide a comprehensive comparison of all methods across different tasks.
- Why unresolved: The paper does not provide a detailed analysis of how different serialization methods impact LLM performance for various tabular tasks.
- What evidence would resolve it: A comprehensive study comparing LLM performance using different serialization methods across various tabular tasks.

## Limitations
- The HTML format advantage relies on assumptions about web table exposure in pre-training that aren't directly verified
- Self-augmented prompting introduces computational overhead and depends on the LLM's ability to generate reliable intermediate knowledge
- The benchmark design is solid but the SUC dataset details remain partially unspecified, potentially limiting reproducibility

## Confidence
- HTML format advantage: Medium
- Self-augmented prompting improvements: Medium
- Structural metadata benefits: Low
- Generalizability across model families: Low

## Next Checks
1. Test HTML vs NL+Sep format advantage across multiple model families (GPT-4, Claude, LLaMA) to verify if the performance gap generalizes beyond GPT-3.
2. Conduct ablation studies on self-augmented prompting by systematically removing each intermediate knowledge type to isolate which structural features drive the improvements.
3. Evaluate the benchmark tasks with a human baseline to establish whether the accuracy levels reflect genuine structural understanding or pattern matching.