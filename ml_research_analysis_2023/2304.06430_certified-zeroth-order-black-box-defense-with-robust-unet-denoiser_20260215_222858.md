---
ver: rpa2
title: Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser
arxiv_id: '2304.06430'
source_url: https://arxiv.org/abs/2304.06430
tags:
- defense
- black-box
- proposed
- adversarial
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a certified zeroth-order black-box defense
  mechanism against adversarial perturbations using a robust UNet denoiser (RDUNet)
  and autoencoder architecture. The authors address the challenge of defending black-box
  models without access to their parameters or architecture, which is crucial for
  privacy and scalability.
---

# Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser

## Quick Facts
- arXiv ID: 2304.06430
- Source URL: https://arxiv.org/abs/2304.06430
- Reference count: 40
- Key outcome: Proposes RDUNet-based certified defense achieving 35% improvement on CIFAR-10 over state-of-the-art

## Executive Summary
This paper introduces a certified zeroth-order black-box defense mechanism using a robust UNet denoiser (RDUNet) and autoencoder architecture. The approach addresses the challenge of defending black-box models without access to their parameters or architecture, which is crucial for privacy and scalability. The method, ZO-RUDS, prepends RDUNet to the black-box model and optimizes it using zeroth-order techniques, achieving significant improvements in certified accuracy on high-dimensional datasets.

## Method Summary
The method prepends RDUNet to a black-box model and optimizes it using zeroth-order optimization techniques. For high-dimensional datasets, an autoencoder is added after RDUNet (ZO-AE-RUDS) to enable coordinate-wise gradient estimation in a lower-dimensional feature space. The training objective combines cross-entropy, cosine similarity, and MMD losses. The approach is evaluated on CIFAR-10, CIFAR-100, STL-10, Tiny Imagenet, and MNIST datasets, showing significant improvements over state-of-the-art methods.

## Key Results
- ZO-RUDS achieves 35% improvement in certified accuracy on CIFAR-10 compared to state-of-the-art methods
- ZO-AE-RUDS achieves 9% improvement on CIFAR-10 and 23.51% on STL-10
- The approach demonstrates robust performance in image reconstruction tasks on MNIST dataset
- RDUNet architecture enables effective defense on high-dimensional datasets while maintaining low reconstruction error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RDUNet architecture reduces model variance during zeroth-order optimization, enabling effective black-box defense on high-dimensional datasets.
- Mechanism: The UNet design with lateral skip connections preserves fine-scale spatial information through downsampling/upsampling paths, counteracting the noise introduced by zeroth-order gradient estimates.
- Core assumption: Preserving spatial detail is critical for denoising in high-dimensional spaces where autoencoder bottlenecks fail.
- Evidence anchors:
  - [abstract] "We propose a robust UNet denoiser (RDUNet) that ensures the robustness of black-box models trained on high-dimensional datasets."
  - [section] "Our proposed RDUNet type of architecture enables the model to learn fine-scale information while maintaining a low reconstruction error in comparison to previous custom-trained denoisers [62, 81]."
  - [corpus] Weak corpus support; no direct citations to UNet-based black-box defenses.
- Break condition: If lateral connections are removed or the encoder/decoder resolutions are mismatched, the denoising performance degrades on high-dimensional inputs.

### Mechanism 2
- Claim: Maximum Mean Discrepancy (MMD) loss aligns the feature distributions of clean and denoised images, improving robustness certification.
- Mechanism: MMD pulls the domain distributions of clean and denoised images closer in feature space, ensuring the smoothed classifier behaves consistently across both domains.
- Core assumption: Adversarial perturbations cause domain shift; reducing this shift improves certified radii.
- Evidence anchors:
  - [abstract] "In order to further increase the certification of our proposed approach, we utilize maximum mean discrepancy (MMD) to bring the distributions of original input images closer to obtained denoised output."
  - [section] "This distribution pulling of the original samples and denoised output is inspired by the task of domain adaptation [54, 80]."
  - [corpus] No direct MMD-based adversarial defense references in corpus.
- Break condition: If the feature extractor is not discriminative or the MMD kernel is poorly chosen, the distributional alignment fails.

### Mechanism 3
- Claim: Combining RDUNet with autoencoder (AE) enables CGE zeroth-order optimization by reducing dimensionality while preserving critical features.
- Mechanism: RDUNet denoises in full resolution; AE compresses features into a low-dimensional latent space suitable for coordinate-wise gradient estimation, avoiding the curse of dimensionality.
- Core assumption: High-dimensional zeroth-order optimization requires dimensionality reduction; AE bottleneck must be large enough to retain denoised information.
- Evidence anchors:
  - [abstract] "We further propose ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the black-box model."
  - [section] "It ensures that ZO optimization can be carried out in a feature embedding space with low dimensions."
  - [corpus] No direct references to AE + UNet combos for black-box ZO optimization.
- Break condition: If AE bottleneck is too narrow, critical denoised features are lost, collapsing performance back to baseline.

## Foundational Learning

- Concept: Zeroth-order (gradient-free) optimization
  - Why needed here: Black-box models only provide input-output queries; gradients are inaccessible.
  - Quick check question: What is the difference between randomized gradient estimate (RGE) and coordinate-wise gradient estimate (CGE) in zeroth-order optimization?

- Concept: Randomized smoothing for certified robustness
  - Why needed here: Provides formal l2-radius guarantees by convolving predictions with Gaussian noise.
  - Quick check question: How does the certified radius Rc depend on the gap between top and runner-up class probabilities?

- Concept: UNet architecture and skip connections
  - Why needed here: Preserves spatial resolution during denoising, critical for high-dimensional inputs.
  - Quick check question: What role do the lateral connections play in preventing information loss during downsampling?

## Architecture Onboarding

- Component map:
  Input → RDUNet denoiser → (optional) Autoencoder encoder → Black-box predictor → Autoencoder decoder → Output

- Critical path:
  1. Add Gaussian noise to input
  2. Denoise with RDUNet
  3. (Optional) Compress with AE encoder
  4. Query black-box predictor
  5. Backpropagate loss via zeroth-order gradient estimates
  6. Update RDUNet parameters

- Design tradeoffs:
  - RGE vs CGE: RGE works directly on high-dim inputs but suffers from high variance; CGE uses AE to reduce dimension but risks bottleneck loss.
  - RDUNet depth: Deeper nets capture more detail but increase optimization difficulty in ZO setting.
  - MMD weight: Too high slows training; too low loses distributional alignment benefit.

- Failure signatures:
  - High variance in gradient estimates → noisy updates, unstable training
  - Degraded SSIM/RMSE on reconstruction tasks → RDUNet underfitting
  - Low certified accuracy despite low training loss → MMD or cosine loss ineffective

- First 3 experiments:
  1. Replace RDUNet with DnCNN; measure variance and certified accuracy on CIFAR-10.
  2. Train without MMD loss; compare domain shift metrics and RCA.
  3. Vary AE bottleneck dimension; plot CGE performance vs dimensionality on STL-10.

## Open Questions the Paper Calls Out

- Question: How does the proposed RDUNet denoiser architecture generalize to other high-dimensional data types beyond images, such as audio or video?
- Question: What is the impact of varying the noise distribution (e.g., using Laplace instead of Gaussian) on the certified robustness achieved by ZO-RUDS and ZO-AE-RUDS?
- Question: How does the performance of ZO-RUDS and ZO-AE-RUDS scale with increasingly larger black-box models (e.g., deeper networks or models with more parameters)?

## Limitations

- The paper lacks detailed architectural specifications for RDUNet (channel dimensions, kernel sizes)
- Implementation details for MMD loss calculation are not specified
- Claims of superiority over state-of-the-art methods are weakened by inconsistent threat model comparisons

## Confidence

- **High confidence:** The theoretical framework combining zeroth-order optimization with randomized smoothing is sound and well-established
- **Medium confidence:** RDUNet architecture improves denoising performance, but specific architectural details needed for exact reproduction are missing
- **Low confidence:** Claims of superiority over state-of-the-art methods are weakened by inconsistent threat model comparisons

## Next Checks

1. Implement and compare RDUNet versus standard Unet on CIFAR-10 reconstruction tasks to isolate architectural benefits
2. Re-run experiments using l_inf threat model (matching prior work) to enable fair comparison
3. Measure feature distribution alignment (MMD metrics) between clean and denoised images across different noise levels