---
ver: rpa2
title: PaLM 2 Technical Report
arxiv_id: '2305.10403'
source_url: https://arxiv.org/abs/2305.10403
tags:
- palm
- language
- contributor
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PaLM 2 is a Transformer-based language model trained with a mixture
  of objectives on a diverse multilingual dataset, achieving state-of-the-art performance
  across a wide range of tasks. It significantly outperforms its predecessor PaLM
  in multilingual language understanding, reasoning, and translation, while being
  more compute-efficient and enabling faster, more natural interactions.
---

# PaLM 2 Technical Report

## Quick Facts
- arXiv ID: 2305.10403
- Source URL: https://arxiv.org/abs/2305.10403
- Reference count: 40
- Key outcome: PaLM 2 achieves state-of-the-art performance across tasks with improved multilingual understanding, reasoning, and translation while being more compute-efficient than its predecessor.

## Executive Summary
PaLM 2 is a Transformer-based language model that demonstrates significant improvements over its predecessor through compute-optimal scaling, multilingual dataset enhancement, and inference-time control mechanisms. The model achieves superior performance on downstream tasks while maintaining efficiency, with particular strengths in multilingual understanding, reasoning capabilities, and toxicity control. The technical report provides comprehensive evaluation across different model sizes and tasks, showing stable performance on responsible AI metrics.

## Method Summary
PaLM 2 employs a mixture of training objectives on a diverse multilingual dataset that includes significantly more non-English data than previous models. The architecture follows Transformer principles with improvements in dataset composition, including parallel text for hundreds of languages. Models are trained using compute-optimal scaling laws that balance parameter count with training tokens. The approach incorporates inference-time control tokens for toxicity mitigation without performance degradation, enabling conditional generation based on toxicity level markers.

## Key Results
- Demonstrates compute-optimal scaling with equal growth in parameters and training tokens
- Shows significant improvements in multilingual language understanding and translation
- Enables inference-time toxicity control without overhead or capability impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compute-optimal scaling with equal parameter and data growth improves performance.
- Mechanism: Scaling laws show optimal performance when model size (N) and training tokens (D) grow proportionally with compute budget.
- Core assumption: The relationship between compute, model size, and data follows predictable power laws.
- Evidence anchors:
  - [abstract] "Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes"
  - [section] "We arrive at a similar conclusion as Hoffmann et al. (2022), i.e., D and N should grow in equal proportions as the FLOPs budget increases"
  - [corpus] Weak - corpus shows related work but no direct scaling law measurements

### Mechanism 2
- Claim: Multilingual dataset mixture with parallel text improves cross-lingual understanding.
- Mechanism: Training on parallel multilingual data ingrains translation capabilities and cultural context understanding.
- Core assumption: Exposure to parallel text pairs creates implicit translation knowledge that transfers to downstream tasks.
- Evidence anchors:
  - [abstract] "PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"
  - [section] "In addition to non-English monolingual data, PaLM 2 is also trained on parallel data covering hundreds of languages in the form of source and target text pairs"
  - [corpus] Moderate - related work shows parallel data helps translation but not necessarily general reasoning

### Mechanism 3
- Claim: Inference-time control tokens enable toxicity mitigation without performance degradation.
- Mechanism: Special tokens marking toxicity levels in training data allow conditional generation at inference time.
- Core assumption: The model learns to condition generation on these tokens and can use them to control output properties.
- Evidence anchors:
  - [abstract] "PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities"
  - [section] "We employed several data cleaning and quality filtering methods, including de-duplication, removal of sensitive-PII and filtering"
  - [corpus] Weak - corpus shows related work on controllable generation but not specifically this token-based approach

## Foundational Learning

- Concept: Scaling laws for neural networks
  - Why needed here: Understanding how model size and training data should scale with compute budget is fundamental to optimizing performance
  - Quick check question: If you have 10x more compute, should you scale model size, training data, or both? By how much?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: PaLM 2 builds on Transformer architecture, so understanding self-attention and layer stacking is crucial
  - Quick check question: How does the attention mechanism allow the model to handle long-range dependencies in text?

- Concept: Multilingual representation learning
  - Why needed here: PaLM 2's multilingual capabilities depend on learning shared representations across languages
  - Quick check question: What challenges arise when training a single model on hundreds of languages with varying amounts of data?

## Architecture Onboarding

- Component map: Input text -> Embedding -> Stacked Transformer layers -> Control token conditioning (if applicable) -> Output projection -> Text generation
- Critical path: Input text → Embedding → Stacked Transformer layers → Control token conditioning (if applicable) → Output projection → Text generation
- Design tradeoffs: Smaller models with more data vs larger models with less data; monolingual vs multilingual focus; standard vs mixed objectives
- Failure signatures: Poor multilingual performance indicates insufficient parallel data; high toxicity in outputs suggests control token learning issues; degraded reasoning shows scaling law violations
- First 3 experiments:
  1. Test translation quality on held-out parallel text pairs
  2. Evaluate toxicity control with different control token settings
  3. Measure reasoning performance on BIG-Bench Hard tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PaLM 2's performance on reasoning tasks like BIG-Bench Hard scale with model size beyond the Large variant?
- Basis in paper: [explicit] The paper evaluates PaLM 2-L on BIG-Bench Hard and shows significant improvements over PaLM, but does not report results for the S and M variants on this benchmark.
- Why unresolved: The paper focuses on PaLM 2-L for most reasoning evaluations, leaving the scaling behavior of smaller variants on these tasks unclear.
- What evidence would resolve it: Evaluating the PaLM 2-S and PaLM 2-M models on BIG-Bench Hard and comparing their performance to PaLM 2-L would reveal how reasoning capabilities scale with model size.

### Open Question 2
- Question: What is the impact of PaLM 2's multilingual training data on its performance on English-only tasks compared to a model trained on a primarily English dataset?
- Basis in paper: [explicit] The paper states that PaLM 2 has a higher percentage of non-English data than previous models and shows improved English performance, but does not directly compare to a model trained on a similar amount of English data.
- Why unresolved: The paper does not isolate the effect of multilingual data on English performance, making it difficult to determine if the improvements are due to multilingual training or other factors.
- What evidence would resolve it: Training a model on a dataset with a similar amount of English data as PaLM 2 but with less multilingual data and comparing its English performance to PaLM 2 would isolate the effect of multilingual training.

### Open Question 3
- Question: How effective are inference-time control tokens at mitigating specific types of potential harms beyond toxicity, such as bias or misinformation?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of control tokens for reducing toxic language, but does not evaluate their impact on other potential harms.
- Why unresolved: The paper focuses on toxicity as a specific harm, leaving the broader applicability of control tokens to other potential harms unclear.
- What evidence would resolve it: Evaluating control tokens on benchmarks that measure bias, misinformation, or other potential harms would determine their effectiveness in mitigating a wider range of risks.

## Limitations

- The report lacks detailed analysis of how different language families and data quality levels affect multilingual performance.
- Limited empirical validation of the inference-time control mechanism beyond toxicity metrics.
- Insufficient mechanistic analysis of why certain architectural or training approaches work, limiting generalizability.

## Confidence

- Compute-optimal scaling: High confidence (well-aligned with scaling law literature)
- Multilingual training approach: Medium confidence (parallel data benefits established but detailed analysis lacking)
- Inference-time control mechanism: Medium-Low confidence (limited validation beyond toxicity)

## Next Checks

1. Replicate the scaling law experiments by training smaller models with controlled compute budgets to verify the D ∝ N relationship and identify optimal scaling points.
2. Conduct targeted experiments on low-resource language pairs to quantify the contribution of parallel data versus monolingual data in the multilingual training mixture.
3. Perform ablation studies on the control token mechanism by training models with and without toxicity control tokens to measure the actual impact on both safety metrics and general capability preservation.