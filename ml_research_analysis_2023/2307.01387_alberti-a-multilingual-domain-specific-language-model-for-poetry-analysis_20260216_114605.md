---
ver: rpa2
title: ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis
arxiv_id: '2307.01387'
source_url: https://arxiv.org/abs/2307.01387
tags:
- poetry
- language
- spanish
- alberti
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Alberti, the first multilingual pre-trained
  large language model for poetry analysis. Through domain-specific pre-training (DSP),
  Alberti was further trained on multilingual BERT using a corpus of over 12 million
  verses from 12 languages.
---

# ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis

## Quick Facts
- arXiv ID: 2307.01387
- Source URL: https://arxiv.org/abs/2307.01387
- Reference count: 40
- Key outcome: Alberti is the first multilingual pre-trained large language model for poetry analysis, outperforming mBERT on Spanish stanza classification and multilingual scansion tasks

## Executive Summary
This paper introduces Alberti, the first multilingual pre-trained large language model specifically designed for poetry analysis. The model was developed through domain-specific pre-training (DSP) of multilingual BERT using a corpus of over 12 million verses from 12 languages. Alberti was evaluated on two structural poetry tasks - Spanish stanza type classification and metrical pattern prediction for Spanish, English, and German - and demonstrated state-of-the-art performance, particularly for German scansion when compared to rule-based systems.

## Method Summary
Alberti was created by further training multilingual BERT on the PULPO corpus (12 million verses from 12 languages) using a masked language modeling (MLM) objective for 40 epochs. The model was then fine-tuned for downstream poetry analysis tasks including Spanish stanza classification and multilingual scansion. For stanza classification, the task was reformulated as a multi-class classification problem using structural features like metrical length, rhyme type, and rhyme scheme. For scansion, Alberti predicted metrical patterns through binary classification of syllable stress assignments.

## Key Results
- Alberti outperformed multilingual BERT on Spanish stanza classification and metrical pattern prediction tasks
- The model achieved state-of-the-art results for German scansion compared to rule-based systems
- Alberti demonstrated effective cross-lingual transfer capabilities across Spanish, English, and German poetry analysis tasks
- MLM accuracy on the PULPO validation set reached 57.77% after domain-specific pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alberti outperforms mBERT on poetry tasks because domain-specific pre-training (DSP) adapts multilingual BERT to poetic language patterns
- Mechanism: Further training mBERT on a large corpus of 12 million verses from 12 languages (PULPO) aligns the model's learned representations with poetic structures like rhythm, rhyme, and meter, improving performance on poetry-specific downstream tasks
- Core assumption: Poetic language has unique structural features that differ from general language corpora like Wikipedia, and these features can be captured by DSP
- Evidence anchors:
  - [abstract] "Through domain-specific pre-training (DSP), we further trained multilingual BERT on a corpus of over 12 million verses from 12 languages."
  - [section 3] "After training, we evaluate the model on 10% of the corpus held out as a validation set, achieving a final global MLM accuracy of 57.77%."
  - [corpus] PULPO includes over 72 million words from 12 languages and diverse historical periods, suggesting rich poetic variety for DSP

### Mechanism 2
- Claim: Alberti's multilingual architecture enables effective cross-lingual poetry analysis, unlike monolingual systems
- Mechanism: By leveraging a shared vocabulary and architecture across 12 languages, Alberti can transfer knowledge between languages for tasks like scansion and stanza classification, benefiting from similarities in poetic structures
- Core assumption: Poetic structures like meter and rhyme have cross-linguistic similarities that a multilingual model can exploit
- Evidence anchors:
  - [abstract] "Alberti was further trained on multilingual BERT using a corpus of over 12 million verses from 12 languages."
  - [section 3.3] Evaluation on scansion tasks in Spanish, English, and German shows Alberti outperforms both mBERT and monolingual baselines
  - [corpus] PULPO covers 12 languages, enabling multilingual DSP

### Mechanism 3
- Claim: Reformulating poetry analysis tasks as classification problems enables effective use of transformer-based models like Alberti
- Mechanism: Stanza type classification and metrical pattern prediction are cast as multi-class classification tasks, allowing Alberti to leverage its learned representations for accurate predictions
- Core assumption: Poetry analysis tasks can be effectively formulated as classification problems without losing essential structural information
- Evidence anchors:
  - [section 3.2] "we approached stanza type identification as a classification task."
  - [section 3.3] "The prediction of metre was approached as a multi-class binary classification task."
  - [section 4] Alberti achieves state-of-the-art results for German scansion compared to rule-based systems, validating the classification approach

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the self-supervised objective used during DSP to train Alberti on the poetry corpus, enabling it to learn poetic language patterns
  - Quick check question: What is the primary difference between MLM and standard language modeling objectives?

- Concept: Stanza structure and identification
  - Why needed here: Understanding stanza types (e.g., tercet, quatrain, sestina) and their identification criteria (metrical length, rhyme type, rhyme scheme) is crucial for the stanza classification task
  - Quick check question: How do metrical length, rhyme type, and rhyme scheme contribute to stanza identification?

- Concept: Metrical scansion and stress patterns
  - Why needed here: Scansion involves assigning stress to syllables to determine a poem's meter, which is the core task for Alberti's multilingual scansion evaluation
  - Quick check question: What are the key devices (e.g., synalepha, syneresis, dieresis) that affect metrical scansion in poetry?

## Architecture Onboarding

- Component map: Multilingual BERT -> Domain-specific pre-training (MLM on PULPO) -> Fine-tuning for downstream tasks (stanza classification, scansion) -> Classification heads for specific tasks
- Critical path: Pre-training (DSP on PULPO) -> Fine-tuning for downstream tasks -> Evaluation on held-out test sets
- Design tradeoffs: Alberti trades general language understanding (mBERT's strength) for poetry-specific knowledge. The multilingual architecture enables cross-lingual transfer but may introduce interference between languages
- Failure signatures: Poor performance on poetry tasks despite strong MLM accuracy could indicate overfitting to the pre-training corpus or insufficient task-specific fine-tuning. Cross-lingual failures may point to language-specific poetic structures not captured by the multilingual model
- First 3 experiments:
  1. Evaluate Alberti's MLM accuracy and perplexity on the PULPO validation set to assess domain adaptation
  2. Fine-tune Alberti for Spanish stanza classification and compare performance to mBERT and monolingual baselines
  3. Fine-tune Alberti for multilingual scansion (Spanish, English, German) and compare performance to mBERT, monolingual models, and rule-based systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Alberti's performance on multilingual poetry analysis compare to other specialized domain-specific models beyond those mentioned in the paper (such as models for other literary domains or languages)?
- Basis in paper: [explicit] The paper compares Alberti to mBERT, RoBERTa, and XLM RoBERTa, but does not explore other domain-specific models for poetry or other literary domains
- Why unresolved: The study focuses on comparing Alberti to general multilingual models and rule-based systems, but does not include comparisons with other specialized models that might be specifically trained on poetry or other literary domains
- What evidence would resolve it: Comparative experiments evaluating Alberti against a broader range of domain-specific models, including those trained on other literary domains or languages, would provide insights into its relative performance and generalizability

### Open Question 2
- Question: What are the specific limitations of Alberti when dealing with poetic styles or forms that deviate significantly from the training data in PULPO?
- Basis in paper: [inferred] The paper does not discuss the model's performance on poetic styles or forms that are not well-represented in the training corpus, which consists of traditional and historical poetry
- Why unresolved: The study does not address how Alberti handles modern, experimental, or culturally specific poetic forms that may not be present in the PULPO corpus
- What evidence would resolve it: Testing Alberti on a diverse set of poetic styles and forms, including modern and experimental poetry, would reveal its limitations and potential areas for improvement

### Open Question 3
- Question: How does the performance of Alberti vary across different languages, and what factors contribute to these variations?
- Basis in paper: [explicit] The paper reports Alberti's performance across 12 languages but does not deeply analyze the factors contributing to performance differences between languages
- Why unresolved: While the paper provides performance metrics for each language, it does not explore the linguistic or cultural factors that might influence the model's effectiveness in different languages
- What evidence would resolve it: A detailed analysis of Alberti's performance across languages, considering factors such as linguistic complexity, cultural context, and availability of training data, would help identify the reasons for performance variations

## Limitations

- The paper lacks ablation studies to determine which components (domain-specific pre-training, multilingual architecture, or additional training data) drive performance improvements
- The evaluation scope is limited to structural poetry analysis tasks, without exploring semantic analysis, emotion detection, or other poetry analysis applications
- German scansion comparisons use rule-based systems developed decades ago, which may not represent current state-of-the-art performance

## Confidence

**High Confidence**: The core methodology of domain-specific pre-training is sound and well-established in the literature. The reported MLM accuracy of 57.77% on the PULPO validation set and the consistent outperformance of Alberti over mBERT on all evaluated tasks are reliable findings given the clear experimental setup and reproducible evaluation metrics.

**Medium Confidence**: The claim that Alberti represents "the first multilingual pre-trained large language model for poetry analysis" is credible but difficult to verify definitively given the niche nature of this domain. The comparison to rule-based systems for German scansion is methodologically sound, though the age of those systems limits the conclusiveness of the comparison.

**Low Confidence**: The extent to which Alberti's improvements generalize to other poetry analysis tasks or to all 12 languages in the pre-training corpus is uncertain. The paper does not provide evidence for cross-lingual transfer between languages other than Spanish, English, and German, nor does it explore tasks beyond structural analysis.

## Next Checks

1. **Ablation Study**: Conduct controlled experiments to isolate the contribution of domain-specific pre-training versus multilingual architecture by comparing: (a) mBERT fine-tuned directly on poetry tasks, (b) monolingual poetry-specific models for each language, and (c) Alberti without multilingual training. This would clarify which mechanism drives performance improvements.

2. **Cross-Lingual Transfer Validation**: Evaluate Alberti's performance on poetry analysis tasks for the remaining 9 languages in the PULPO corpus (Czech, Arabic, Finnish, Italian, Russian, Hungarian, Chinese, Portuguese, French) to verify that the multilingual architecture provides consistent benefits across all pre-training languages.

3. **Task Generalization Test**: Assess Alberti on additional poetry analysis tasks beyond structural features, such as semantic similarity between poems, emotion classification in poetry, or authorship attribution. This would determine whether the model's learned representations generalize beyond the specific tasks evaluated in the current study.