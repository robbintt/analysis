---
ver: rpa2
title: Generalized Planning in PDDL Domains with Pretrained Large Language Models
arxiv_id: '2305.11014'
source_url: https://arxiv.org/abs/2305.11014
tags:
- plan
- tasks
- planning
- location
- generalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores whether large language models (LLMs) can act\
  \ as generalized planners\u2014generating programs that efficiently solve new tasks\
  \ in a PDDL domain after being trained on a few examples. The approach uses GPT-4\
  \ to synthesize Python programs given PDDL domain definitions and training tasks,\
  \ with optional Chain-of-Thought summarization and automated debugging to improve\
  \ performance."
---

# Generalized Planning in PDDL Domains with Pretrained Large Language Models

## Quick Facts
- arXiv ID: 2305.11014
- Source URL: https://arxiv.org/abs/2305.11014
- Reference count: 40
- Primary result: GPT-4 successfully acts as a generalized planner, generating efficient Python programs from PDDL domains that solve new tasks with minimal training data and automated debugging.

## Executive Summary
This paper investigates whether large language models can function as generalized planners, synthesizing programs that efficiently solve new tasks in a PDDL domain after training on few examples. The approach uses GPT-4 to generate Python programs from PDDL domain definitions and training tasks, with optional Chain-of-Thought summarization and automated debugging to improve performance. Evaluated on seven PDDL domains, the method successfully solved most evaluation tasks, especially when automated debugging was used. GPT-4 significantly outperformed GPT-3.5 and demonstrated high data efficiency, often requiring only two training tasks. The synthesized programs were also much faster than standard planners.

## Method Summary
The approach prompts GPT-4 with PDDL domain definitions and training tasks, asking it to synthesize Python programs that generate plans for new tasks. The method employs Chain-of-Thought summarization where GPT-4 first summarizes the domain and proposes a strategy in natural language before implementing it in Python. Automated debugging validates the generated code on training tasks, providing feedback on Python exceptions, timeouts, and plan semantics/syntax errors, and re-prompting GPT-4 up to four times for corrections. The final program is evaluated on held-out tasks and compared against baselines including domain-independent planners and random baselines.

## Key Results
- GPT-4 successfully generated generalized planning programs that solved most evaluation tasks across seven PDDL domains.
- Automated debugging significantly improved performance, with programs often solving tasks within seconds compared to traditional planners.
- GPT-4 demonstrated high data efficiency, typically requiring only two training tasks to achieve strong generalization.
- GPT-4 outperformed GPT-3.5 significantly across all domains and settings.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can function as a generalized planner by generating Python programs that efficiently produce plans for new tasks in a PDDL domain after training on a few examples.
- Mechanism: The LLM synthesizes Python code from PDDL domain definitions and training tasks, then validates and refines the code through automated debugging cycles using feedback from Python exceptions, timeouts, and plan semantics/syntax checks.
- Core assumption: The LLM's pretraining on code and language allows it to recognize patterns in PDDL domains and translate them into executable generalized strategies without needing search.
- Evidence anchors:
  - [abstract] "use GPT-4 to synthesize Python programs... automated debugging to improve performance"
  - [section] "we prompt GPT-4 with the domain and a small number of training tasks, all encoded in the Planning Domain Definition Language (PDDL) [14]. We then ask GPT-4 to write a Python program that consumes a (parsed) task description and outputs a plan"
  - [corpus] Weak - no direct mention of PDDL or generalized planning in corpus signals
- Break condition: If the LLM cannot recognize the domain structure from PDDL or produces syntactically/semantically invalid code that cannot be corrected through debugging.

### Mechanism 2
- Claim: Chain-of-Thought summarization helps GPT-4 decompose the generalized planning problem into domain summarization, strategy proposal, and strategy implementation stages.
- Mechanism: By prompting the LLM to first summarize the domain in natural language and propose a strategy before implementation, it creates a structured thinking process that guides code generation.
- Core assumption: Decomposing complex reasoning tasks into intermediate steps improves performance, as shown in other LLM applications.
- Evidence anchors:
  - [abstract] "we consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program"
  - [section] "inspired by Chain-of-Thought (CoT) [15, 16], we prompt GPT-4 to write a natural language summary of the PDDL domain. We then ask it to describe a solution strategy before finally implementing the strategy in Python"
  - [corpus] Weak - no direct evidence of CoT helping generalized planning
- Break condition: If the CoT strategy does not translate well to code, or if the additional steps introduce confusion rather than clarity.

### Mechanism 3
- Claim: Automated debugging is essential for correcting GPT-4's initial code implementations through iterative feedback.
- Mechanism: The system executes the generated Python code on training tasks, captures errors (Python exceptions, timeouts, invalid plan syntax/semantics), and re-prompts GPT-4 with specific error feedback to fix the code, repeating up to four times.
- Core assumption: LLMs can effectively use targeted feedback to correct their own code, similar to other automated program repair approaches.
- Evidence anchors:
  - [abstract] "automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback"
  - [section] "After the LLM has proposed an implementation of get_plan, we use the training tasks to validate the implementation... we re-prompt with one of four types of feedback"
  - [corpus] Weak - no mention of automated debugging in related work
- Break condition: If GPT-4 cannot correct fundamental logical errors in its code, or if the error patterns are too diverse for the debugging approach to handle effectively.

## Foundational Learning

- PDDL (Planning Domain Definition Language)
  - Why needed here: The entire approach is built around using PDDL domain definitions and tasks as input to the LLM, so understanding PDDL syntax and semantics is essential for designing prompts and interpreting results.
  - Quick check question: What are the main components of a PDDL domain definition (types, predicates, operators) and how do they relate to the objects, initial state, and goal in a PDDL task?

- Generalized Planning Concepts
  - Why needed here: The work aims to generate programs that solve many tasks in a domain, not just individual plans, so understanding the difference between standard planning and generalized planning is crucial.
  - Quick check question: How does generalized planning differ from standard planning in terms of input (training tasks vs. single task) and output (program vs. plan)?

- Large Language Model Capabilities and Limitations
  - Why needed here: The approach relies on GPT-4's ability to generate code and use feedback for self-correction, so understanding what LLMs can and cannot do is important for setting realistic expectations and designing effective prompts.
  - Quick check question: What are the key limitations of current LLMs that might affect their performance as generalized planners (e.g., context window size, tendency to use search, difficulty with complex logical reasoning)?

## Architecture Onboarding

- Component map:
  PDDL domain definitions and training tasks -> Prompt templates (CoT summarization, strategy proposal, implementation) -> GPT-4 API interface -> Validation pipeline (VAL tool for plan semantics, custom checks for syntax/exceptions) -> Error handling and re-prompting logic for automated debugging

- Critical path:
  1. Parse PDDL domain and tasks
  2. Generate CoT summary and strategy proposal
  3. Generate Python implementation
  4. Validate implementation on training tasks
  5. If errors, provide feedback and re-prompt GPT-4
  6. Repeat steps 4-5 up to 4 times
  7. Return final program for evaluation on held-out tasks

- Design tradeoffs:
  - Using two training tasks always vs. varying number based on domain complexity
  - Automated debugging with 4 attempts vs. restarting from scratch on persistent failures
  - GPT-4 vs. GPT-3.5 (significantly different performance)
  - With vs. without CoT summarization (mixed impact across domains)

- Failure signatures:
  - GPT-4 fails to recognize domain structure (e.g., Miconic's multiple buildings)
  - Generated code has fundamental logical errors that cannot be fixed through debugging
  - Overfitting to training tasks (rare, but possible)
  - Timeout errors due to inefficient algorithms (though generally efficient in practice)

- First 3 experiments:
  1. Run GPT-4 on a simple domain (e.g., Delivery) with default settings to verify basic functionality
  2. Compare GPT-4 with and without CoT summarization on the same domain to assess impact
  3. Test automated debugging by introducing deliberate errors in the code and verifying they are corrected through the feedback loop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to domain-independent planners like Fast Downward when scaling to very large problem sizes (beyond 250 objects)?
- Basis in paper: [explicit] The paper compares GPT-4's synthesized programs to Fast Downward in terms of runtime efficiency for problems up to 250 objects.
- Why unresolved: The paper only evaluates up to 250 objects, so performance on significantly larger problems remains unknown.
- What evidence would resolve it: Experiments showing runtime and success rate comparisons for problem sizes well beyond 250 objects.

### Open Question 2
- Question: What is the impact of increasing the number of training tasks beyond two on GPT-4's generalization performance?
- Basis in paper: [explicit] The paper finds that GPT-4 is highly data-efficient, often needing only two training tasks, but does not systematically explore performance with more training tasks.
- Why unresolved: The paper only uses two training tasks in most experiments, so the benefits of more training data are not quantified.
- What evidence would resolve it: Experiments varying the number of training tasks (e.g., 1, 2, 5, 10) and measuring corresponding generalization performance.

### Open Question 3
- Question: How does the quality of GPT-4's synthesized programs degrade when the PDDL domain definitions are significantly more complex or less structured?
- Basis in paper: [inferred] The paper tests on seven PDDL domains, some with more complex structures (e.g., Miconic with multiple buildings and elevators), and finds varying success rates.
- Why unresolved: The paper does not systematically vary domain complexity or structure to quantify the relationship with program quality.
- What evidence would resolve it: Experiments creating PDDL domains with systematically varied complexity (e.g., number of types, predicates, operators, nested structures) and measuring program synthesis quality.

## Limitations
- The approach relies heavily on GPT-4's ability to recognize domain structure and generate correct code, which may not generalize to more complex or less structured domains.
- Automated debugging can correct many errors but may struggle with fundamental logical flaws in the initial strategy that require significant redesign.
- The exact prompt engineering details are not fully specified, which may limit reproducibility and make it difficult to assess the approach's robustness to prompt variations.

## Confidence

- **High confidence**: GPT-4 can effectively generate generalized planning programs that outperform traditional planners in runtime efficiency, given appropriate prompting and debugging support.
- **Medium confidence**: The automated debugging mechanism reliably corrects most code errors, but may struggle with fundamental logical flaws in the initial strategy.
- **Medium confidence**: Chain-of-Thought summarization improves performance in some domains (Logistics, Miconic, Termes) but not others, suggesting domain-dependent effectiveness.

## Next Checks

1. Test the approach on additional PDDL domains not included in the original evaluation to assess generalization beyond the seven domains.
2. Compare the effectiveness of automated debugging with alternative error correction strategies, such as complete restarts with modified prompts.
3. Evaluate the impact of prompt engineering variations (e.g., different CoT structures, strategy descriptions) on success rates across domains.