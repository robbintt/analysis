---
ver: rpa2
title: 'AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting'
arxiv_id: '2312.13038'
source_url: https://arxiv.org/abs/2312.13038
tags:
- time
- forecasting
- data
- series
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoXPCR addresses the problem of automated model selection for
  time series forecasting, where traditional methods focus solely on predictive performance
  while neglecting interpretability and resource consumption. The method proposes
  a multi-objective meta-learning framework that estimates model performance across
  prediction error, complexity, and resource demand (PCR) criteria.
---

# AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2312.13038
- **Source URL**: https://arxiv.org/abs/2312.13038
- **Reference count**: 40
- **Key outcome**: AutoXPCR achieves 90% of optimal predictive performance while requiring only 20% of computational cost in automated time series forecasting model selection

## Executive Summary
AutoXPCR addresses the challenge of automated model selection for time series forecasting by proposing a multi-objective meta-learning framework that estimates model performance across prediction error, complexity, and resource demand (PCR) criteria. Unlike traditional methods that focus solely on predictive performance, AutoXPCR balances interpretability and resource consumption through interpretable meta-learners that predict PCR properties from time series characteristics. The framework provides explainability through property-specific estimates, feature importance analysis, and interactive control over PCR priorities, successfully recommending optimal models in over 95% of cases within the top-5 recommendations.

## Method Summary
AutoXPCR uses a meta-learning framework that estimates PCR properties (Prediction Error, Complexity, Resource consumption) without exhaustive model training. The system extracts meta-features from time series datasets, trains interpretable regression models to predict PCR values, and uses a weighted compound score to recommend optimal models. The approach employs relative index scaling to normalize property values across different environments and provides top-k recommendations for efficiency. The framework balances multiple objectives through weighted sums, allowing users to control the trade-offs between prediction accuracy, model complexity, and computational resources.

## Key Results
- Achieves 90% of optimal predictive performance while requiring only 20% of the computational cost
- Successfully recommends the optimal model in over 95% of cases within the top-5 recommendations
- Outperforms competing approaches including exhaustive search, AutoForecast, and AutoKeras across compound score, MASE, and power consumption metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AutoXPCR uses meta-learning to estimate PCR properties without exhaustive search
- **Mechanism**: The system trains regression models that map time series characteristics and model features to estimated PCR values, avoiding the need to train every candidate model on every dataset
- **Core assumption**: The relationship between meta-features and PCR properties is learnable and generalizable across datasets
- **Evidence anchors**:
  - [abstract] "The core approach uses interpretable meta-learners to estimate PCR properties from time series characteristics and model features"
  - [section 3.3] "Training the regressors requires to collect a property database D of meta-features and real function values"
- **Break condition**: If the meta-feature to PCR property mapping is non-stationary or dataset-dependent, the meta-learners will fail to generalize

### Mechanism 2
- **Claim**: The relative index scale makes PCR properties comparable across different environments
- **Mechanism**: Each property value is normalized by dividing by the best empirical result on that property, creating bounded scores between 0 and 1
- **Core assumption**: Having a common reference point (best empirical result) provides meaningful relative comparisons
- **Evidence anchors**:
  - [section 3.2] "We address this issue by assessing values of fi on a relative index scale, based on the real measurements Î¼i obtained from forecasting Y with m"
  - [section 3.2] "The now calculable PCR function values fi and compound score F are bounded by the interval (0, 1]"
- **Break condition**: If the reference model selection is unstable or if property distributions vary widely, the normalization may not provide meaningful comparisons

### Mechanism 3
- **Claim**: Multi-objective optimization with weighted sum approach enables controllable trade-offs
- **Mechanism**: The compound score F(Y,m) is a weighted sum of PCR properties, where weights can be adjusted to prioritize different objectives
- **Core assumption**: The weighted sum formulation adequately captures the Pareto-optimal trade-offs between prediction error, complexity, and resource consumption
- **Evidence anchors**:
  - [section 3.1] "The compound score F(Y,m) is a weighted sum of functions fi which describe properties that m exhibits"
  - [section 3.1] "The weights wi control each property's impact on the compound score"
- **Break condition**: If the objectives are highly conflicting with non-convex Pareto fronts, the weighted sum may miss optimal solutions

## Foundational Learning

- **Concept**: Meta-learning fundamentals
  - **Why needed here**: AutoXPCR relies on meta-learners that predict model performance from dataset characteristics
  - **Quick check question**: What is the difference between traditional learning and meta-learning in the context of model selection?

- **Concept**: Multi-objective optimization
  - **Why needed here**: The framework needs to balance prediction error, model complexity, and resource consumption simultaneously
  - **Quick check question**: Why might a weighted sum approach be preferred over Pareto optimization in this context?

- **Concept**: Time series forecasting evaluation metrics
  - **Why needed here**: Understanding MASE, RMSE, and MAPE is crucial for interpreting the PCR properties
  - **Quick check question**: How does MASE differ from RMSE in terms of interpretability for time series forecasting?

## Architecture Onboarding

- **Component map**: Meta-feature extraction module -> Property database with empirical results -> Interpretable meta-learners -> Weighted compound score calculator -> Recommendation engine -> Explainability interface

- **Critical path**: Dataset -> Meta-feature extraction -> Property prediction -> Weighted scoring -> Top-k recommendation

- **Design tradeoffs**:
  - Using interpretable meta-learners vs. black-box models: better explainability but potentially lower accuracy
  - Top-k recommendation vs. exhaustive search: efficiency vs. optimality
  - Relative index normalization vs. absolute values: comparability vs. interpretability

- **Failure signatures**:
  - High prediction error on held-out datasets
  - Top-k recommendations consistently missing optimal models
  - Meta-learners showing low feature importance for relevant characteristics

- **First 3 experiments**:
  1. Validate meta-learner accuracy on held-out datasets by comparing predicted vs. actual PCR values
  2. Test the sensitivity of recommendations to weight adjustments in the compound score
  3. Benchmark AutoXPCR against exhaustive search on a small subset of datasets to measure efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does AutoXPCR's performance scale with increasing model pool size M?
- **Basis in paper**: [inferred] The paper tests 11 DNNs but doesn't systematically explore larger model pools or establish scalability limits
- **Why unresolved**: The experimental validation only covers a fixed set of 11 models, without exploring how performance changes with more extensive model selection spaces
- **What evidence would resolve it**: Systematic experiments testing AutoXPCR with progressively larger model pools (e.g., 50, 100, 500 models) to identify performance degradation points and computational scaling behavior

### Open Question 2
- **Question**: How does AutoXPCR perform on time series with different temporal resolutions and patterns not represented in the Monash dataset?
- **Basis in paper**: [explicit] The paper acknowledges using only Monash dataset variants but doesn't test on diverse real-world datasets with varying temporal characteristics
- **Why unresolved**: The evaluation is limited to 19 original Monash datasets and their variants, which may not capture the full diversity of real-world time series applications
- **What evidence would resolve it**: Testing AutoXPCR on diverse real-world datasets from different domains (e.g., finance, healthcare, IoT sensor data) with varying temporal resolutions, patterns, and data characteristics

### Open Question 3
- **Question**: What is the impact of different meta-feature selection strategies on AutoXPCR's performance?
- **Basis in paper**: [inferred] The paper uses specific meta-features but doesn't systematically compare different feature selection approaches or explore feature engineering strategies
- **Why unresolved**: The methodology section describes the used meta-features but doesn't investigate whether alternative feature sets or selection methods could improve performance
- **What evidence would resolve it**: Comparative experiments testing AutoXPCR with different meta-feature selection strategies (e.g., automated feature selection, domain-specific features, deep meta-features) and analyzing their impact on recommendation quality

## Limitations
- Framework reliability depends heavily on the quality and representativeness of the meta-feature to PCR property mapping
- Relative index scaling introduces sensitivity to reference model selection and may obscure absolute performance differences
- Weighted sum approach may miss optimal solutions when PCR criteria exhibit strong non-linear trade-offs

## Confidence

**High Confidence**: The meta-learning framework architecture and its ability to estimate PCR properties without exhaustive search. The interpretability through relative index scaling and feature importance analysis is well-founded theoretically.

**Medium Confidence**: The effectiveness of the weighted sum approach for multi-objective optimization, as this depends heavily on the specific weight configurations and the nature of trade-offs between PCR criteria in practice.

**Low Confidence**: The generalizability of results to datasets significantly different from the Monash forecasting archive, particularly regarding the stability of meta-feature to PCR property relationships across diverse time series domains.

## Next Checks

1. **Robustness Testing**: Validate AutoXPCR's performance on time series datasets from different domains (financial, environmental, medical) to assess generalization beyond the Monash archive, measuring both prediction accuracy and computational efficiency.

2. **Pareto Frontier Analysis**: Compare the weighted sum approach against true Pareto optimization methods on selected datasets to quantify potential missed optimal solutions, particularly in cases where PCR criteria exhibit strong non-linear trade-offs.

3. **Sensitivity Analysis**: Systematically vary the weights in the compound score formula across multiple orders of magnitude to map the stability of top-k recommendations and identify regions where small weight changes cause large shifts in model selection.