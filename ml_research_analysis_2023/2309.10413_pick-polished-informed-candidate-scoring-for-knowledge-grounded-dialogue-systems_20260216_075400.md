---
ver: rpa2
title: 'PICK: Polished & Informed Candidate Scoring for Knowledge-Grounded Dialogue
  Systems'
arxiv_id: '2309.10413'
source_url: https://arxiv.org/abs/2309.10413
tags:
- responses
- dialogue
- knowledge
- response
- pick
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating dialogue responses
  that are both faithful to external knowledge and relevant to the conversation context
  in knowledge-grounded dialogue systems. The authors propose PICK, a generation re-scoring
  framework that leverages off-the-shelf automatic metrics to evaluate and re-rank
  response candidates, without requiring additional labeled data or model tuning.
---

# PICK: Polished & Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems

## Quick Facts
- arXiv ID: 2309.10413
- Source URL: https://arxiv.org/abs/2309.10413
- Authors: 
- Reference count: 14
- Primary result: PICK improves dialogue response faithfulness and relevance through re-ranking without requiring additional labeled data

## Executive Summary
This paper introduces PICK, a generation re-scoring framework for knowledge-grounded dialogue systems that addresses the challenge of producing responses faithful to external knowledge while remaining relevant to conversation context. The key innovation is leveraging off-the-shelf automatic metrics (KF1 for faithfulness, FED turn-level basic for relevance) to evaluate and re-rank multiple generated response candidates, selecting the optimal response without requiring additional labeled data or model tuning. PICK demonstrates substantial improvements across automatic metrics (BLEU-4, ROUGE-L, F1, KF1) and human evaluation on the Wizard of Wikipedia dataset, while being model-agnostic and effective across different decoding strategies and model architectures.

## Method Summary
PICK generates multiple hypotheses using a fine-tuned language model, then re-ranks them based on faithfulness to knowledge snippets (measured by KF1) and relevance to dialogue history (measured by FED turn-level basic metrics). The framework is model-agnostic, requiring no modifications to the underlying language model architecture or training process. It operates as a post-processing step that selects the highest-scoring candidate from the top-r hypotheses, effectively addressing the limitation of vanilla decoding that only considers the highest joint probability response. The approach leverages automatic metrics that correlate with human judgment, eliminating the need for additional labeled data or model tuning.

## Key Results
- PICK significantly improves response faithfulness and relevance across beam search, top-k, and top-p sampling decoding strategies
- Substantial gains in automatic metrics: BLEU-4, ROUGE-L, F1, and KF1 scores compared to vanilla decoding
- Human evaluation confirms PICK-generated responses are more faithful and relevant than baseline responses
- Framework works effectively with both oracle and retrieved knowledge snippets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Re-ranking the top-r hypotheses based on faithfulness and relevance scores selects more optimal responses than vanilla decoding.
- Mechanism: The model generates multiple hypotheses (top-r), and PICK uses automatic metrics (KF1 for faithfulness, FED for relevance) to evaluate and select the best response, bypassing the limitation of vanilla decoding which only considers the highest joint probability.
- Core assumption: Alternative generated responses within a single decoding process exist that are more faithful and relevant than the top-1 response prioritized by vanilla decoding.
- Evidence anchors:
  - [abstract] "Upon analyzing multiple language model generations, we observe the presence of alternative generated responses within a single decoding process. These alternative responses are more faithful and exhibit a comparable or higher level of relevance to prior conversational turns compared to the optimal responses prioritized by the decoding processes."
  - [section] "Driven by these observations, we propose a straightforward yet effective human-aligned re-ranking framework to direct model responses closer to KGD qualities."
- Break condition: If the automatic metrics used for scoring do not correlate well with human judgment, the re-ranking may not select the truly optimal responses.

### Mechanism 2
- Claim: Using off-the-shelf automatic metrics (KF1 and FED) allows PICK to evaluate response quality without requiring additional labeled data or model tuning.
- Mechanism: PICK leverages KF1 to measure the faithfulness of responses to the knowledge snippet and FED to assess relevance to the dialogue history. These metrics are pre-trained and do not require further training or labeled data.
- Core assumption: KF1 and FED correlate well with human judgment on faithfulness and relevance, respectively.
- Evidence anchors:
  - [abstract] "Furthermore, it circumvents the need for supplementary labeled data by exploiting off-the-shelves metrics that correlate well with human judgment."
  - [section] "We leverage off-the-shelf automatic metrics to evaluate the quality of response candidates. These metrics allow us to assess the faithfulness and the relevance of the responses without the need for additional labeled data."
- Break condition: If the correlation between the automatic metrics and human judgment is weak or inconsistent, the effectiveness of PICK will be compromised.

### Mechanism 3
- Claim: PICK is model-agnostic and can be applied to various language models with different architectures and sizes.
- Mechanism: PICK operates as a post-processing step that re-ranks the hypotheses generated by any language model. It does not require any modifications to the model architecture or training process.
- Core assumption: The generated hypotheses from different models can be evaluated using the same automatic metrics (KF1 and FED).
- Evidence anchors:
  - [abstract] "The proposed framework is also model-agnostic; thus, it can be applied to various LMs with different architectures and sizes."
  - [section] "Our proposed method allows more randomness in the decoding process, which may cause high meaningless repetition in some r hypotheses. To filter this, we remove the hypotheses that contain repetitive words."
- Break condition: If the generated hypotheses from a particular model are significantly different in structure or format, the automatic metrics may not be applicable or effective.

## Foundational Learning

- Concept: Knowledge-Grounded Dialogue (KGD) systems
  - Why needed here: PICK is designed specifically for KGD systems, which generate responses grounded in external knowledge.
  - Quick check question: What are the key challenges in KGD systems that PICK aims to address?

- Concept: Automatic metrics for evaluating response quality
  - Why needed here: PICK relies on automatic metrics (KF1 and FED) to assess the faithfulness and relevance of generated responses without requiring additional labeled data.
  - Quick check question: How do KF1 and FED differ in their evaluation criteria for response quality?

- Concept: Decoding strategies (beam search, top-k, top-p sampling)
  - Why needed here: PICK is evaluated across different decoding strategies to demonstrate its effectiveness in improving response quality regardless of the decoding method used.
  - Quick check question: How do beam search, top-k, and top-p sampling differ in their approach to generating hypotheses?

## Architecture Onboarding

- Component map: Input (dialogue history, knowledge snippet, topic) -> Fine-tuned language model (generates top-r hypotheses) -> PICK framework (re-ranks using KF1 + FED) -> Output (best response)
- Critical path: 1. Generate hypotheses using fine-tuned language model, 2. Evaluate each hypothesis using KF1 and FED metrics, 3. Select hypothesis with highest combined score
- Design tradeoffs: Computational overhead increases with multiple hypotheses generation and metric evaluation; metric selection may not perfectly align with human judgment
- Failure signatures: Low correlation between automatic metrics and human judgment; insufficient diversity in generated hypotheses; inability to handle complex knowledge snippets
- First 3 experiments: 1. Compare PICK against vanilla decoding using automatic metrics on Wizard of Wikipedia, 2. Conduct human evaluation on faithfulness and relevance, 3. Evaluate across different decoding strategies and model architectures

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense. However, several important areas for future research are implied:
- How PICK performs on dialogue datasets beyond Wizard of Wikipedia
- The impact of different scoring metric combinations and their comparison to human judgment
- PICK's effectiveness with longer conversational histories and more complex knowledge bases

## Limitations
- Effectiveness depends heavily on quality and reliability of automatic metrics used for re-ranking
- Requires generating multiple hypotheses, increasing computational overhead compared to standard decoding
- Filtering mechanism for repetitive responses is described but not extensively validated across diverse scenarios

## Confidence

**High Confidence**: The core mechanism of re-ranking generated hypotheses using automatic metrics is well-established and experimental results on Wizard of Wikipedia are robust. The framework's model-agnostic nature is clearly demonstrated across different architectures and decoding strategies.

**Medium Confidence**: Claims of significant improvements in faithfulness and relevance are supported by automatic metrics and human evaluation on a single dataset. Performance may vary across different knowledge-grounded dialogue domains. The framework works with both oracle and retrieved knowledge, but more extensive testing across varied retrieval quality scenarios would strengthen this claim.

**Low Confidence**: Long-term stability and generalization across diverse dialogue domains and knowledge sources has not been established. The paper doesn't address potential adversarial cases where automatic metrics might be gamed or where knowledge snippets are particularly complex or contradictory.

## Next Checks

1. **Cross-Domain Generalization**: Test PICK on multiple knowledge-grounded dialogue datasets (e.g., CMU Document Grounded Conversations, Molweni) to assess whether improvements in faithfulness and relevance transfer across different knowledge domains and conversation styles.

2. **Metric Correlation Analysis**: Conduct systematic ablation studies to quantify individual and combined contributions of KF1 and FED turn-level basic metrics to PICK's performance. Test whether PICK maintains effectiveness when using alternative metric combinations or when one metric is removed.

3. **Computational Efficiency Evaluation**: Measure and report computational overhead introduced by generating multiple hypotheses and computing metric scores. Analyze whether there's an optimal value of r that balances response quality improvement against computational cost, and test whether early-stopping mechanisms could reduce computational burden without significant quality loss.