---
ver: rpa2
title: Understanding the limitations of self-supervised learning for tabular anomaly
  detection
arxiv_id: '2309.08374'
source_url: https://arxiv.org/abs/2309.08374
tags:
- data
- anomaly
- detection
- features
- anomalies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores why self-supervised learning does not improve
  tabular anomaly detection, unlike its success in computer vision and NLP. Through
  extensive experiments on 26 benchmark datasets and various pretext tasks, the authors
  show that self-supervised representations do not outperform raw data representations
  for tabular anomaly detection.
---

# Understanding the limitations of self-supervised learning for tabular anomaly detection

## Quick Facts
- arXiv ID: 2309.08374
- Source URL: https://arxiv.org/abs/2309.08374
- Reference count: 40
- Self-supervised representations underperform raw tabular data for anomaly detection

## Executive Summary
This paper investigates why self-supervised learning (SSL) fails to improve tabular anomaly detection despite its success in computer vision and NLP. Through extensive experiments on 26 benchmark datasets using various neural network architectures and pretext tasks, the authors demonstrate that SSL representations consistently underperform raw data representations. They identify that neural networks introduce irrelevant features during representation learning, which reduces anomaly detection effectiveness. However, they show that using a subspace of the neural network's representation can recover performance, and that SSL can be beneficial for specific anomaly types like localized anomalies and those with different dependency structures.

## Method Summary
The authors conduct experiments on 26 tabular datasets from the ODDS benchmark, training neural networks (ResNet and FT-Transformer) on various pretext tasks including rotation, shuffle, mask classification, autoencoder, and contrastive learning. They standardize features per dimension and split data into 50% normal training, 20% validation, and test sets containing anomalies. After training, they extract penultimate layer features and apply five shallow anomaly detectors (k-NN, iForest, LOF, OCSVM, residual norms) to both raw and self-supervised embeddings. Performance is evaluated using AUROC scores, with models selected based on lowest validation loss on normal data.

## Key Results
- Self-supervised representations consistently underperform raw data representations across all tested anomaly detectors
- Neural networks introduce irrelevant features that reduce anomaly detection effectiveness
- Using a subspace of neural network representations (residual directions) can recover performance
- SSL outperforms raw representations specifically for localized anomalies and anomalies with different dependency structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks introduce irrelevant features that obscure anomaly detection performance
- Mechanism: Deep neural networks trained on pretext tasks learn a high-dimensional embedding space that stretches the data but does not add meaningful discriminative information for anomalies
- Core assumption: The original tabular data already provides a sufficient representation for anomaly detection, and neural networks do not improve this representation
- Evidence anchors:
  - [abstract] "We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors."
  - [section 4.3] "Compared to the original two-dimensional results, detection performance drops for almost all detectors after extracting representations from the ResNets. As two dimensions are sufficient to capture the characteristics of the datasets, projecting the data to a 128-dimensional space only results in a stretched and narrow representation without extra information."
  - [corpus] Weak evidence - no corpus neighbor directly supports this mechanism, but one neighbor paper mentions "Neural networks (NNs) are well-known for introducing irrelevant features"

### Mechanism 2
- Claim: Using a subspace of the neural network representation can recover performance
- Mechanism: By projecting the high-dimensional neural network embeddings onto their residual subspace (smallest eigenvalues), we remove the irrelevant directions and retain the meaningful information for anomaly detection
- Core assumption: The relevant information for anomaly detection is concentrated in the residual directions of the neural network embedding
- Evidence anchors:
  - [abstract] "However, we demonstrate that using a subspace of the neural network's representation can recover performance."
  - [section 4.4] "We now investigate the residual space of the embeddings...Throwing away the top 10% of principal components helps, although throwing away the top 90% also performs similarly. This observation aligns with previous findings that show residual directions capture information important for out-of-distribution detection."
  - [corpus] Weak evidence - no direct corpus support, but related work mentions "residual norms might be better at filtering out noisy directions"

### Mechanism 3
- Claim: Self-supervised learning can outperform raw representations for specific anomaly types
- Mechanism: Certain pretext tasks like contrastive learning are better at discerning local anomalies and dependency structures, while classification tasks like rotation and mask prediction are better at identifying anomalies with different dependency structures
- Core assumption: Different anomaly types have different characteristics that can be captured by specific self-supervised tasks
- Evidence anchors:
  - [section 4.5] "The contrastive objectives outperform the baseline in the local...and cluster anomaly...scenarios. This result suggests contrastive tasks are better at discerning differences at a local neighbourhood level."
  - [section 4.5] "For the dependency anomalies, rotation and mask classification surpass the baseline...Using a rotation or mask classification pretext task could help promote the intrinsic property that tabular data are non-invariant, which may help identify this type of anomaly."
  - [corpus] Weak evidence - no direct corpus support for this mechanism

## Foundational Learning

- Concept: Self-supervised learning pretext tasks
  - Why needed here: The paper uses multiple pretext tasks (rotation, shuffle, mask classification, autoencoder, contrastive learning) to learn representations for tabular data
  - Quick check question: What is the difference between contrastive learning and classification-based pretext tasks in the context of tabular anomaly detection?

- Concept: Anomaly detection metrics (AUROC)
  - Why needed here: The paper evaluates all anomaly detectors using AUROC score, which is a threshold-independent metric
  - Quick check question: Why is AUROC a better metric than accuracy for anomaly detection tasks?

- Concept: Residual subspace analysis
  - Why needed here: The paper projects neural network embeddings to their residual subspace to remove irrelevant features and recover performance
  - Quick check question: How does projecting to the residual subspace differ from using the principal components in PCA?

## Architecture Onboarding

- Component map: Tabular datasets → Neural network training with pretext task → Feature extraction → Shallow anomaly detector training → AUROC evaluation
- Critical path: Data → Neural network training with pretext task → Feature extraction → Anomaly detector training → Evaluation with AUROC
- Design tradeoffs: Using neural networks vs. raw features (performance vs. complexity), different pretext tasks vs. no self-supervision (potential benefits for specific anomaly types vs. general underperformance), standardized vs. non-standardized data (minimal impact on results)
- Failure signatures: Self-supervised representations underperforming raw data, k-NN performance dropping significantly with neural network features, OCSVM consistently poor performance across all tasks
- First 3 experiments:
  1. Train ResNet on rotation pretext task with ARPL loss, extract features, run k-NN anomaly detection, compare to k-NN on raw data
  2. Train FT-Transformer on contrastive shuffle task with InfoNCE loss, extract features, run iForest anomaly detection, compare to iForest on raw data
  3. Take neural network embeddings from any task, project to residual subspace (90% smallest eigenvalues), run residual norm anomaly detection, compare to original performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does self-supervised learning improve tabular anomaly detection performance when using deep neural network architectures designed specifically for tabular data?
- Basis in paper: [explicit] The paper notes that specialized methods like GOAD and ICL underperform k-NN on raw data, but uses generic architectures like ResNets and FT-Transformers rather than architectures designed specifically for tabular data.
- Why unresolved: The paper uses generic deep learning architectures that may not be optimal for tabular data, leaving open the possibility that purpose-built architectures could yield better results.
- What evidence would resolve it: Experiments comparing self-supervised tabular architectures (like FT-Transformer) against k-NN on raw data with the same pretext tasks.

### Open Question 2
- Question: Can regularization during self-supervised training prevent neural networks from introducing irrelevant features that degrade anomaly detection performance?
- Basis in paper: [inferred] The authors suggest that neural networks introduce irrelevant features that reduce anomaly detector effectiveness, and mention that future work could examine regularization strategies.
- Why unresolved: The paper identifies the problem of irrelevant features but does not test whether regularization techniques could mitigate this issue during self-supervised training.
- What evidence would resolve it: Experiments showing improved anomaly detection performance when applying various regularization techniques (dropout, weight decay, etc.) during self-supervised training.

### Open Question 3
- Question: Does self-supervised learning become more effective for tabular anomaly detection with larger datasets?
- Basis in paper: [explicit] The authors note that neural networks benefit from large amounts of data, and the ODDS datasets used are relatively small (ranging from 129 to 567,469 samples).
- Why unresolved: The experiments were conducted on small tabular datasets, leaving open the question of whether self-supervised learning would be more effective with larger tabular datasets.
- What evidence would resolve it: Experiments on larger tabular datasets (e.g., hundreds of thousands to millions of samples) comparing self-supervised representations to raw data representations.

### Open Question 4
- Question: Are there specific types of tabular anomalies (e.g., temporal, spatial, or structural) for which self-supervised learning could be particularly beneficial?
- Basis in paper: [explicit] The authors show that self-supervised learning outperforms raw representations for purely localized anomalies and those with different dependency structures in synthetic experiments.
- Why unresolved: While the paper identifies two specific anomaly types where self-supervision helps, it doesn't comprehensively explore whether other types of tabular anomalies might benefit more from self-supervised representations.
- What evidence would resolve it: Systematic experiments creating and testing various synthetic anomaly types (temporal anomalies, anomalies with specific structural patterns, etc.) to identify which benefit most from self-supervised representations.

## Limitations
- Findings are based on small tabular datasets (129 to 567,469 samples), leaving open whether results would generalize to larger datasets
- Experiments use generic neural network architectures (ResNet, FT-Transformer) rather than architectures specifically designed for tabular data
- Limited exploration of regularization techniques during self-supervised training that could potentially improve performance

## Confidence
- Mechanism 1 (neural networks introduce irrelevant features): Medium - well-supported by empirical results but the theoretical explanation could be deeper
- Mechanism 2 (residual subspace recovery): High - strong experimental evidence across multiple settings
- Mechanism 3 (task-specific performance): Medium - results are promising but based on limited anomaly type categorizations

## Next Checks
1. **Cross-architecture validation**: Test whether the findings hold for other neural network architectures (MLP, CNN variations) to ensure the mechanism is not architecture-specific.

2. **Dataset diversity analysis**: Systematically categorize benchmark datasets by feature types, dimensionality, and anomaly characteristics to identify which properties influence the effectiveness of self-supervised representations.

3. **Extended pretext task evaluation**: Include additional self-supervised tasks (e.g., contrastive learning with momentum encoders, masked autoencoders) and evaluate their performance across different anomaly types to better understand the task-advantage relationship.