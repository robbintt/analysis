---
ver: rpa2
title: Toward Building General Foundation Models for Language, Vision, and Vision-Language
  Understanding Tasks
arxiv_id: '2301.05065'
source_url: https://arxiv.org/abs/2301.05065
tags:
- language
- vision
- tasks
- encoder
- vision-language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes X-FM, a general foundation model that can perform
  well on language, vision, and vision-language understanding tasks. The key idea
  is to train separate encoders for language, vision, and fusion, using two new techniques:
  stopping gradients from vision-language training for the language encoder, and using
  vision-language training to guide the vision encoder.'
---

# Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks

## Quick Facts
- arXiv ID: 2301.05065
- Source URL: https://arxiv.org/abs/2301.05065
- Authors: Multiple authors from DAMO Academy, Alibaba Group
- Reference count: 40
- Key outcome: X-FM achieves state-of-the-art performance on 22 tasks across language, vision, and vision-language domains, outperforming existing general foundation models

## Executive Summary
This paper introduces X-FM, a general foundation model designed to perform well on language, vision, and vision-language understanding tasks. The key innovation lies in a novel training approach that leverages the asymmetric relationship between language and vision modalities. By separating the learning of three encoders (language, vision, and fusion) and introducing two new techniques—stopping gradients from vision-language training for the language encoder and using vision-language training to guide the vision encoder—X-FM achieves significant performance improvements over existing general foundation models while maintaining comparable or better performance than models specifically designed for individual task types.

## Method Summary
X-FM is a general foundation model with three distinct encoders: a language encoder (initialized from RoBERTa), a vision encoder (initialized from BEiTv2), and a fusion encoder with cross-attention layers. The model is trained using multiple objectives including Masked Language Modeling (MLM), Masked Image Modeling (MIM), Image-Text Contrastive learning (ITC), Image-Text Matching (ITM), Image-Text MLM (IMLM), Bounding Box Prediction (BBP), and MIM on image-text pairs. The key innovations are stopping gradients from the fusion encoder to the language encoder during vision-language training, and using vision-language training signals to guide the vision encoder's MIM targets. The model is pre-trained on a diverse set of datasets totaling over 400M image-text pairs and fine-tuned on 22 downstream tasks.

## Key Results
- X-FM achieves state-of-the-art performance on 22 diverse tasks spanning language, vision, and vision-language domains
- On the GLUE benchmark, X-FM outperforms existing general foundation models and achieves comparable results to BERT-base
- X-FM achieves competitive performance on ImageNet classification (both linear evaluation and fine-tuning) compared to specialized vision models
- The model demonstrates strong performance on vision-language tasks including VQA and NLVR2, surpassing existing multi-modal models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stopping gradients from vision-language training when learning the language encoder prevents the language encoder from being negatively affected by multimodal training.
- Mechanism: By halting the backward flow of gradients from the fusion encoder to the language encoder during ITM, IMLM, and BBP training, the language encoder is isolated and trained only with text data by MLM and with image-text pair data by ITC. This allows the language encoder to focus on learning text representations without interference from the vision-language alignment learning in the fusion encoder.
- Core assumption: Language and vision are fundamentally different modalities, and training them together without isolation can lead to suboptimal solutions.
- Evidence anchors: [abstract]: "One is to stop gradients from the vision-language training when learning the language encoder." [section]: "The gradient flow is stopped from the fusion encoder to the language encoder in training, while the activation flow from the language encoder to the fusion encoder is as usual." [corpus]: Weak evidence. No direct citations in related papers, but the concept of gradient isolation is common in multi-task learning literature.

### Mechanism 2
- Claim: Using vision-language training signals to guide the vision encoder's masked image modeling (MIM) improves its learning efficiency and effectiveness.
- Mechanism: The vision encoder is trained with MIM on both image data and image-text pair data. The targets for MIM are generated by the vision encoder itself when processing unmasked image-text pairs, leveraging the semantic representations learned during vision-language training. This creates a mutually beneficial relationship where the vision encoder's learning is enhanced by the vision-language training, and the vision-language training benefits from the improved vision encoder.
- Core assumption: Representations learned from vision-language training are more semantic and informative than those from image-only training, making them better targets for MIM.
- Evidence anchors: [abstract]: "The other is to leverage the vision-language training to guide the learning of the vision encoder." [section]: "The vision encoder can create the target representations because it is also trained in the vision-language training." [corpus]: Weak evidence. While MIM is a known technique, the specific use of vision-language training to generate targets is not widely cited in related papers.

### Mechanism 3
- Claim: The asymmetric relationship between language and vision allows for a modular training approach where the language encoder is primarily trained on text, the vision encoder on both image and image-text data, and the fusion encoder on image-text data.
- Mechanism: By recognizing that language is more abstract than vision, the model separates the learning of the three encoders. The language encoder is trained mainly from text data and isolated from the fusion encoder training. The vision encoder is simultaneously trained from image data and image-text pair data, guided by the vision-language training. The fusion encoder is trained from image-text pair data, focusing on learning alignments between language and vision features.
- Core assumption: The different nature of language and vision modalities necessitates separate training strategies to achieve optimal performance.
- Evidence anchors: [abstract]: "The essential thinking of our learning method is that language is more abstract than vision, and there is an asymmetric relationship between language and vision." [section]: "Therefore, we separate the learning of the three encoders." [corpus]: Moderate evidence. The concept of asymmetric relationships between modalities is discussed in related papers, but the specific modular training approach is not widely cited.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is a fundamental pre-training objective for language encoders, allowing them to learn contextual representations by predicting masked tokens in text.
  - Quick check question: What is the purpose of masking tokens in MLM, and how does it help the language encoder learn better representations?

- Concept: Masked Image Modeling (MIM)
  - Why needed here: MIM is a pre-training objective for vision encoders, enabling them to learn visual representations by predicting masked image patches. In X-FM, MIM is guided by vision-language training signals, improving its effectiveness.
  - Quick check question: How does the use of vision-language training signals as targets in MIM differ from traditional MIM approaches, and what are the potential benefits?

- Concept: Cross-modal Contrastive Learning (ITC)
  - Why needed here: ITC is a pre-training objective that aligns image and text representations by contrasting positive image-text pairs with negative ones. It helps the language and vision encoders learn aligned representations.
  - Quick check question: What is the role of contrastive learning in aligning image and text representations, and how does it contribute to the overall performance of the model?

## Architecture Onboarding

- Component map: Language Encoder (RoBERTa) -> Vision Encoder (BEiTv2) -> Fusion Encoder (Cross-attention Transformer)
- Critical path: The critical path for training X-FM involves: 1) Initializing the language encoder with RoBERTa and the vision encoder with BEiTv2, 2) Training the language encoder with MLM and ITC, 3) Training the vision encoder with MIM and ITC, using vision-language training signals as targets, 4) Training the fusion encoder with ITM, IMLM, BBP, and MIM, while stopping gradients to the language encoder.
- Design tradeoffs: Modularity vs. Joint Learning (modular approach allows specialized training but may miss joint learning opportunities), Gradient Isolation vs. Information Flow (stopping gradients prevents interference but may limit multimodal context learning), Vision-Language Guidance vs. Image-Only Training (using vision-language signals improves efficiency but may lead to overfitting).
- Failure signatures: Language Encoder (poor performance on language tasks), Vision Encoder (poor performance on vision tasks), Fusion Encoder (poor performance on vision-language tasks).
- First 3 experiments: 1) Train the language encoder with MLM and ITC, evaluate on language tasks (GLUE), 2) Train the vision encoder with MIM and ITC, evaluate on vision tasks (ImageNet, CIFAR), 3) Train the fusion encoder with ITM, IMLM, BBP, and MIM, evaluate on vision-language tasks (VQA, NLVR2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the X-FM model maintain its strong performance on all three task types (language, vision, and vision-language) when scaled to super-large sizes (over 1B parameters) and trained on super-large datasets (over 400M image-text pairs)?
- Basis in paper: [inferred] The paper mentions that they did not try super-large models due to considerations of fair comparisons and computational resources, but acknowledges that scalability is an important factor for foundation models.
- Why unresolved: The authors did not conduct experiments with super-large models or datasets, leaving the question of how well X-FM would perform in these settings unanswered.
- What evidence would resolve it: Experiments comparing the performance of X-FM when scaled to super-large sizes and trained on super-large datasets with other state-of-the-art super-large models on all three task types.

### Open Question 2
- Question: How does the proposed gradient stopping technique affect the overall training efficiency and convergence of the X-FM model compared to other general foundation models?
- Basis in paper: [explicit] The paper introduces a new technique to stop gradients from vision-language training when learning the language encoder, but does not provide a detailed analysis of its impact on training efficiency and convergence.
- Why unresolved: The paper focuses on the effectiveness of the gradient stopping technique in improving performance but does not provide a comprehensive comparison of training efficiency and convergence with other models.
- What evidence would resolve it: A detailed analysis of training time, convergence speed, and resource usage for X-FM compared to other general foundation models, both with and without the gradient stopping technique.

### Open Question 3
- Question: How does the performance of X-FM on vision-language tasks change when using different image resolutions during fine-tuning, and what is the optimal resolution for each task?
- Basis in paper: [inferred] The paper mentions that the image resolution differs between pre-training and fine-tuning, and position parameters are adapted using linear interpolation, but does not provide a detailed analysis of the impact of different resolutions on performance.
- Why unresolved: The paper does not explore the effect of varying image resolutions during fine-tuning on the performance of X-FM on vision-language tasks, leaving the question of optimal resolution unanswered.
- What evidence would resolve it: Experiments comparing the performance of X-FM on vision-language tasks using different image resolutions during fine-tuning, along with an analysis of the optimal resolution for each task.

## Limitations

- The paper lacks ablation studies to demonstrate the individual contribution of each proposed technique (gradient stopping and vision-language guided MIM) to the overall performance improvements.
- The theoretical justification for the asymmetric relationship between language and vision modalities is asserted rather than empirically validated, which could break if future research shows joint training yields better results.
- The training recipe requires substantial computational resources (200K steps with large batch sizes), making reproducibility difficult for researchers without access to similar infrastructure.

## Confidence

- **High Confidence**: The core architectural design (three separate encoders with cross-attention fusion) and the general framework for training on multiple modalities is well-established and the experimental results show clear performance improvements over baselines.
- **Medium Confidence**: The specific implementation details of the "stop gradient" technique and the vision-language guided MIM targets are described but not fully detailed in the paper.
- **Low Confidence**: The theoretical justification for the asymmetric relationship between language and vision modalities is asserted rather than demonstrated, and the claim that stopping gradients prevents "negative effects" lacks empirical evidence.

## Next Checks

1. **Ablation Study Implementation**: Conduct controlled experiments that isolate each of the two key innovations (gradient stopping and vision-language guided MIM) by implementing variants of X-FM with only one technique active, then compare performance differences to understand the marginal contribution of each component.

2. **Gradient Flow Analysis**: Implement gradient visualization tools to empirically verify that gradients are properly stopped from the fusion encoder to the language encoder during vision-language training, and measure whether this isolation actually prevents interference as claimed or simply limits information flow.

3. **Alternative Target Generation**: Replace the vision-language guided MIM targets with alternative approaches (such as image-only MIM targets or randomly initialized targets) while keeping all other components constant, to quantify whether the vision-language guidance provides statistically significant improvements in vision encoder performance.