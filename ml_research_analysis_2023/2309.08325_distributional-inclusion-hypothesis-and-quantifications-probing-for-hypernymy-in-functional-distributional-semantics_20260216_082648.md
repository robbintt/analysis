---
ver: rpa2
title: 'Distributional Inclusion Hypothesis and Quantifications: Probing for Hypernymy
  in Functional Distributional Semantics'
arxiv_id: '2309.08325'
source_url: https://arxiv.org/abs/2309.08325
tags:
- hypernymy
- corpus
- distributional
- each
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate whether Functional Distributional Semantics
  (FDS) models can learn hypernymy from text corpora. FDS models represent word meanings
  as truth-conditional functions and are trained using a variational autoencoder objective.
---

# Distributional Inclusion Hypothesis and Quantifications: Probing for Hypernymy in Functional Distributional Semantics

## Quick Facts
- arXiv ID: 2309.08325
- Source URL: https://arxiv.org/abs/2309.08325
- Reference count: 20
- Authors investigate whether Functional Distributional Semantics (FDS) models can learn hypernymy from text corpora

## Executive Summary
This paper investigates whether Functional Distributional Semantics (FDS) models can learn hypernymy from text corpora. FDS models represent word meanings as truth-conditional functions and are trained using a variational autoencoder objective. The authors demonstrate that FDS models learn hypernymy when trained on corpora that strictly follow the Distributional Inclusion Hypothesis (DIH), which states that all contexts of a hyponym should also appear with its hypernym. They further introduce a new training objective to handle universal quantifications, enabling FDS to learn hypernymy under the reverse of DIH. Experiments on synthetic and real data show that the proposed approach improves hypernymy detection performance compared to standard FDS training.

## Method Summary
The authors use Functional Distributional Semantics (FDS) models, which represent word meanings as truth-conditional functions. These models are trained using a variational autoencoder objective to infer approximate posterior distributions of pixies (entity representations) and optimize semantic functions. The authors create synthetic corpora from taxonomic hierarchies that strictly follow DIH or its reverse (rDIH), and evaluate the models' ability to learn hypernymy. They also introduce a new training objective that handles universal quantifications, allowing FDS to learn hypernymy under rDIH. The models are evaluated on hypernymy detection using the area under the receiver operating characteristic curves (AUC) metric.

## Key Results
- FDS models successfully learn hypernymy when trained on corpora that strictly follow the Distributional Inclusion Hypothesis (DIH)
- The proposed training objective enables FDS to learn hypernymy under the reverse of DIH (rDIH) by handling universal quantifications
- Experiments on real data (Wikiwoods corpus) show improved hypernymy detection performance compared to standard FDS training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FDS models learn hypernymy from corpora that strictly follow the Distributional Inclusion Hypothesis (DIH).
- **Mechanism**: During training, the approximate posterior distributions of pixies are inferred from observed predicate-argument structures. The semantic functions are optimized to maximize truth values for observed predicates and minimize them for negative samples. When a corpus follows DIH, the contexts of hyponyms are repeated for their hypernyms during inference, causing hypernymic semantic functions to learn higher probability values over the same pixie distributions.
- **Core assumption**: The variational autoencoding training objective preserves distributional patterns where hyponym contexts appear with hypernyms, enabling the semantic functions to encode hypernymic relationships.
- **Evidence anchors**:
  - [abstract]: "We demonstrate that FDS models learn hypernymy when trained on a restricted class of corpus that strictly follows the Distributional Inclusion Hypothesis."
  - [section 4.2]: "The local predicateâ€“argument information of nouns, i.e. contexts, is thus repeated for their hypernyms for inference during training. Consequently, the semantic functions of hypernyms are trained to return values higher than those of their hyponyms over the pixie distributions inferred from the same contexts, aligning with (9)."
  - [corpus]: Synthetic corpora were created to strictly follow DIH, showing that FDS models successfully learned hypernymy in these controlled settings.
- **Break condition**: If the corpus does not strictly follow DIH, the distributional patterns necessary for hypernymy learning would be absent, preventing the semantic functions from encoding hypernymic relationships.

### Mechanism 2
- **Claim**: The proposed training objective enables FDS models to learn hypernymy under the reverse of DIH (rDIH) by handling universal quantifications.
- **Mechanism**: The new objective incorporates universal quantification by optimizing semantic functions with respect to regions in pixie space rather than single points. This allows the model to learn that the extension of a universally quantified predicate is a subset of its argument's extension, which is necessary for learning hypernymy under rDIH.
- **Core assumption**: Universal quantifications in natural language can be approximated by requiring semantic functions to be true over regions of pixie space rather than individual points.
- **Evidence anchors**:
  - [abstract]: "We further introduce a training objective that both enables hypernymy learning under the reverse of the DIH and improves hypernymy detection from real corpora."
  - [section 4.3]: "To this end, we propose a method to enable FDS to be trained on simple sentences with universal quantification. Concretely, we want to optimize semantic functions with respect to not a point but a region in the pixie space."
  - [corpus]: Experiments on synthetic corpora following rDIH showed that FDS models with the new objective successfully learned hypernymy, while standard FDS models failed.
- **Break condition**: If the corpus contains complex quantificational structures beyond simple universal quantification, the proposed objective may not be sufficient to capture the necessary distributional patterns.

### Mechanism 3
- **Claim**: FDS models can generalize distributional information to identify hypernymic relationships even when direct hypernymy information is missing from the training corpus.
- **Mechanism**: When hypernymy information is removed from a corpus, FDS models can still infer hypernymic relationships based on shared contexts between words. If two words share contexts, the model can generalize that they likely share common hypernyms.
- **Core assumption**: Words that share contexts in a corpus are likely to have common hypernyms, and this distributional generalization can be captured by the semantic functions.
- **Evidence anchors**:
  - [section 5.4]: "Intuitively, when the information about mammal being a hypernym of dog is missing... knowing that both dogs and foxes share the same contexts in a corpus... is indicative that they share common hypernyms, e.g., mammal."
  - [corpus]: Experiments on synthetic corpora with removed hypernymy information showed that FDS models could still identify hypernymic relationships based on shared contexts.
- **Break condition**: If the corpus lacks sufficient contextual information or if the shared contexts are not representative of the true hypernymic relationships, the model may fail to generalize correctly.

## Foundational Learning

- **Concept**: Variational autoencoding and the role of the approximate posterior distribution
  - Why needed here: Understanding how pixie distributions are inferred and used to optimize semantic functions is crucial for grasping how FDS models learn hypernymic relationships.
  - Quick check question: How does the variational autoencoding objective in equation (5) contribute to learning hypernymic relationships in FDS models?

- **Concept**: Truth-conditional semantics and its application to distributional models
  - Why needed here: FDS represents word meanings as truth-conditional functions, which provides a natural framework for modeling hypernymy based on the subsumpion of extensions.
  - Quick check question: How does the fuzzy