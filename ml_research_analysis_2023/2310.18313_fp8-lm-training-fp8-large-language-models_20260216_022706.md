---
ver: rpa2
title: 'FP8-LM: Training FP8 Large Language Models'
arxiv_id: '2310.18313'
source_url: https://arxiv.org/abs/2310.18313
tags:
- training
- scaling
- data
- precision
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an FP8 mixed-precision training framework for
  large language models (LLMs). The key insight is that most variables in LLM training
  can employ low-precision FP8 data formats without compromising accuracy.
---

# FP8-LM: Training FP8 Large Language Models

## Quick Facts
- arXiv ID: 2310.18313
- Source URL: https://arxiv.org/abs/2310.18313
- Reference count: 28
- Key outcome: Achieves 42% memory reduction and 64% speedup using FP8 mixed-precision training for LLMs

## Executive Summary
This paper introduces an FP8 mixed-precision training framework for large language models that leverages low-precision FP8 data formats to significantly reduce memory usage and increase training speed. The framework employs three levels of FP8 utilization, incorporating 8-bit gradients, optimizer states, and distributed learning techniques. Through precision decoupling and automatic scaling mechanisms, the approach maintains model accuracy while achieving substantial computational efficiency gains on H100 GPU platforms.

## Method Summary
The paper proposes an FP8 mixed-precision training framework that gradually incorporates 8-bit gradients, optimizer states, and distributed learning techniques. The framework uses precision decoupling to separate critical variables from less critical ones based on sensitivity to quantization errors, and implements automatic scaling to prevent underflow/overflow in FP8 gradient aggregation. The methodology is applied to GPT-style models (7B, 13B, 175B parameters) using pre-training data from multiple sources and evaluated on fine-tuning tasks.

## Key Results
- 42% reduction in real memory usage during GPT-175B training on H100 GPUs
- 64% faster training compared to BF16 framework (Megatron-LM)
- Maintains model accuracy while using FP8 for most variables including gradients and optimizer states
- Framework is generic and applicable to LLM instruction tuning and RLHF tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Precision decoupling allows FP8 optimizer states while preserving accuracy
- Mechanism: Separates critical variables (master weights, second-order moments) from less critical ones (first-order moments, gradients) based on sensitivity to quantization errors
- Core assumption: First-order moments tolerate more quantization error than second-order moments due to gradient direction being more important than magnitude
- Evidence anchors: [abstract] "most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy" [section] "We find a guiding principle: the gradient statistics can use lower precision, while the master weights necessitate high precision"

### Mechanism 2
- Claim: Automatic scaling prevents underflow/overflow in FP8 gradient aggregation
- Mechanism: Dynamically adjusts scaling factor μ based on gradient statistics during training
- Core assumption: Real-time monitoring of gradient statistics allows effective prevention of numerical instability without manual tuning
- Evidence anchors: [section] "We propose an automatic scaling technique to resolve both the underflow and overflow issues in the pre-scaling and post-scaling approaches" [abstract] "The latter one is to preserve gradient values within the representation range of FP8 data formats through the dynamic adjustment of tensor scaling factors"

### Mechanism 3
- Claim: FP8 tensor scaling expands effective dynamic range for low-precision training
- Mechanism: Multiplies higher precision values with scaling factors before casting to FP8
- Core assumption: Scaling factors can be efficiently computed and applied without significant overhead
- Evidence anchors: [abstract] "To tackle them, we propose two techniques: precision decoupling and automatic scaling for preventing the loss of critical information" [section] "tensor scaling techniques are proposed... multiplying higher precision values with a scaling factor prior to their casting to FP8"

## Foundational Learning

- Concept: Floating-point representation and precision
  - Why needed here: Understanding FP8 format limitations (reduced dynamic range and precision) is crucial for designing effective low-precision training strategies
  - Quick check question: What are the two main challenges of using FP8 for model training compared to higher precision formats?

- Concept: Mixed-precision training techniques
  - Why needed here: The paper builds on established mixed-precision methods but extends them to FP8, requiring understanding of both foundations and limitations
  - Quick check question: How does the standard mixed-precision approach differ from the proposed FP8 framework?

- Concept: Distributed training paradigms (data, tensor, pipeline, sequence parallelism)
  - Why needed here: FP8 support is integrated across different parallel training strategies, each requiring specific adaptations
  - Quick check question: Which parallelism strategies require modifications for FP8 support and why?

## Architecture Onboarding

- Component map: Automatic scaling -> Precision decoupling -> FP8 ZeRO distribution -> Gradient all-reduce communication
- Critical path:
  - Forward pass: FP8 computation → gradient calculation → automatic scaling → FP8 all-reduce
  - Backward pass: FP8 gradient computation → scaling → aggregation → optimizer update with precision-decoupled variables
  - Communication: Single shared scalar for gradient scaling across GPUs
- Design tradeoffs:
  - Memory vs. accuracy: Using FP8 saves memory but requires careful precision management
  - Communication efficiency vs. implementation complexity: Single shared scalar reduces communication but requires coordinated scaling
  - Hardware support vs. portability: Leverages H100 FP8 capabilities but may not work on older GPUs
- Failure signatures:
  - NaN or inf values in training loss indicating overflow/underflow issues
  - Divergence in training curves suggesting precision issues
  - Memory allocation failures during ZeRO distribution
  - Communication bottlenecks due to inefficient scaling factor synchronization
- First 3 experiments:
  1. Verify automatic scaling prevents underflow/overflow by comparing pre-scaling, post-scaling, and auto-scaling on small model
  2. Test precision decoupling by training with different combinations of precision for optimizer variables
  3. Validate ZeRO distribution method by comparing memory usage and load balancing against standard ZeRO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different FP8 sub-formats (E4M3 vs E5M2) on model accuracy and training stability across various model scales and tasks?
- Basis in paper: [inferred] The paper mentions that FP8 consists of two sub-formats (E4M3 and E5M2) with different trade-offs between range and precision, but does not explicitly compare their performance
- Why unresolved: The paper does not provide a direct comparison of the two sub-formats in terms of their impact on model accuracy and training stability
- What evidence would resolve it: A systematic comparison of model performance (accuracy, training stability, convergence speed) when using E4M3 vs E5M2 across different model scales (e.g., 7B, 13B, 175B parameters) and tasks (pre-training, SFT, RLHF)

### Open Question 2
- Question: How does the proposed FP8 training framework perform on multi-modal models compared to unimodal LLMs?
- Basis in paper: [inferred] The paper mentions that future work includes applying the FP8 scheme to train multi-modal large models, but does not provide any experimental results
- Why unresolved: The paper focuses solely on text-based LLMs and does not explore the applicability of FP8 training to multi-modal models
- What evidence would resolve it: Experimental results comparing the performance and efficiency of FP8 training on multi-modal models (e.g., CLIP, GPT-4V) versus their unimodal counterparts

### Open Question 3
- Question: What is the long-term impact of using FP8 on model generalization and performance on out-of-distribution data?
- Basis in paper: [inferred] The paper demonstrates that FP8-trained models achieve comparable performance to BF16 models on standard benchmarks, but does not explore their behavior on out-of-distribution data
- Why unresolved: The experiments focus on in-distribution performance and do not investigate how FP8 training affects model robustness or generalization to unseen data
- What evidence would resolve it: Comprehensive evaluation of FP8-trained models on out-of-distribution datasets, robustness benchmarks, and long-term performance monitoring

## Limitations

- Results are based on H100 GPUs; performance on other GPU architectures (A100, older GPUs) remains unverified
- Automatic scaling mechanism lacks detailed algorithmic specifications that would enable straightforward replication
- Precision decoupling approach assumes certain gradient statistics that may not hold for all model architectures or training regimes

## Confidence

- High Confidence: The 42% memory reduction claim is well-supported by the methodology and measurement approach
- Medium Confidence: The 64% speedup claim depends heavily on specific hardware configurations (H100 GPUs) and may not generalize
- Low Confidence: The claim that the framework can be "seamlessly applied to other tasks" like instruction tuning and RLHF is largely speculative with limited experimental validation

## Next Checks

1. Cross-platform validation: Test the FP8 framework on A100 GPUs and older GPU architectures to verify if the claimed performance benefits (42% memory reduction, 64% speedup) are consistent across different hardware platforms

2. Algorithm specification: Request detailed algorithmic specifications for the automatic scaling mechanism, including pseudocode and parameter sensitivity analysis, to enable independent implementation and verification

3. Task generalization study: Conduct controlled experiments applying the FP8 framework to instruction tuning and RLHF tasks on multiple model sizes, measuring both performance retention and computational efficiency compared to BF16 baselines