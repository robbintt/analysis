---
ver: rpa2
title: Do Transformers Parse while Predicting the Masked Word?
arxiv_id: '2303.08117'
source_url: https://arxiv.org/abs/2303.08117
tags:
- uni00000011
- uni0000001b
- attention
- uni00000014
- uni0000001a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether transformers implicitly perform
  parsing while predicting masked words, addressing two key questions: (1) Can realistic-sized
  transformers actually execute parsing algorithms, and (2) Do pre-trained masked
  language models (MLMs) encode syntactic structure, and if so, why? The authors construct
  attention models that can execute the Inside-Outside algorithm for probabilistic
  context-free grammars (PCFGs), showing that even moderate-sized models can perform
  approximate parsing.'
---

# Do Transformers Parse while Predicting the Masked Word?

## Quick Facts
- arXiv ID: 2303.08117
- Source URL: https://arxiv.org/abs/2303.08117
- Reference count: 40
- Pre-trained models encode syntactic information useful for parsing, suggesting transformers implicitly perform parsing during MLM

## Executive Summary
This paper investigates whether transformers implicitly perform parsing while predicting masked words, addressing two key questions: (1) Can realistic-sized transformers actually execute parsing algorithms, and (2) Do pre-trained masked language models (MLMs) encode syntactic structure, and if so, why? The authors construct attention models that can execute the Inside-Outside algorithm for probabilistic context-free grammars (PCFGs), showing that even moderate-sized models can perform approximate parsing. Through experiments on synthetic PCFG data and natural language, they demonstrate that pre-trained models capture syntactic information useful for parsing, with probing methods recovering reasonable parse tree structures and marginal span probabilities.

## Method Summary
The authors construct attention models that explicitly compute inside probabilities α(A,i,j) and outside probabilities β(A,i,j) for non-terminals A and spans (i,j), corresponding to the Inside-Outside algorithm's dynamic programming formulation. They show this algorithm is optimal for MLM loss on PCFG-generated data. Experiments involve training RoBERTa-base variants on synthetic PCFG-generated text, then probing the models to recover constituency parse trees and marginal probabilities. The probing uses both linear and 2-layer neural network predictors trained to predict parse tree structures and normalized marginal probabilities computed by the Inside-Outside algorithm.

## Key Results
- A construction achieving >70% F1 score on the Penn Treebank with 50 layers, 15 attention heads, and 1275-dimensional embeddings
- Pre-trained models showing high correlation between predicted and actual marginal probabilities computed by the Inside-Outside algorithm
- Probing experiments revealing that syntactic information is captured even in models trained only on PCFG-generated data, without semantic cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can approximate the Inside-Outside algorithm for PCFG parsing
- Mechanism: The paper constructs attention models that explicitly compute inside probabilities α(A,i,j) and outside probabilities β(A,i,j) for non-terminals A and spans (i,j). These probabilities directly correspond to the Inside-Outside algorithm's dynamic programming formulation.
- Core assumption: The PCFG's probability distributions can be encoded in attention matrices and relative positional embeddings
- Evidence anchors:
  - [abstract] "We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm"
  - [section 3.1] "we give a construction (Theorem 3.1) that relies on hard attention... that can simulate the Inside-Outside algorithm on all sentences of length at most L"
  - [corpus] Weak evidence - corpus neighbors don't provide direct support for the Inside-Outside algorithm connection
- Break condition: If the PCFG's structure is too complex or sparse relative to the transformer's capacity, the attention mechanisms cannot accurately encode the necessary probability computations

### Mechanism 2
- Claim: MLM training incentivizes transformers to learn parsing-like computations
- Mechanism: The Inside-Outside algorithm is optimal for MLM loss on PCFG-generated data. Transformers trained on this data implicitly learn to approximate this algorithm to minimize their loss.
- Core assumption: The MLM loss surface contains local minima corresponding to approximate Inside-Outside computation
- Evidence anchors:
  - [abstract] "We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generated data"
  - [section 3.2] "If the language is generated from a PCFG, then the Inside-Outside algorithm reaches the optimal masked language modeling loss"
  - [corpus] Weak evidence - corpus neighbors focus on different parsing approaches without directly addressing MLM-optimality
- Break condition: If semantic information is too dominant in the training data, transformers may learn to rely on semantic cues rather than syntactic structure

### Mechanism 3
- Claim: Important non-terminals can be identified and focused on to reduce model complexity
- Mechanism: The frequency distribution of non-terminals in parse trees shows that a small subset of non-terminals appear most frequently. By focusing computation on these frequent non-terminals, the model can maintain parsing performance while reducing size.
- Core assumption: The frequency distribution of non-terminals in parse trees follows a power-law-like distribution
- Evidence anchors:
  - [section 3.3] "We observe that an extremely small subset of non-terminals have high frequency, which allows us to restrict our computation"
  - [Figure 1] "We compute the number of times a specific non-terminal appears in a span of a parse tree"
  - [corpus] Weak evidence - corpus neighbors don't provide data on non-terminal frequency distributions
- Break condition: If the frequency distribution is more uniform or if rare non-terminals are crucial for parsing performance, the approximation would fail

## Foundational Learning

- Concept: Probabilistic Context-Free Grammars (PCFGs)
  - Why needed here: The paper's theoretical framework and experiments are built around PCFGs as a generative model for language
  - Quick check question: Can you explain how a PCFG generates sentences through a sequence of derivation steps?

- Concept: Inside-Outside algorithm
  - Why needed here: This is the core parsing algorithm that the paper shows transformers can approximate, and it's optimal for MLM on PCFG data
  - Quick check question: What are the recursive formulations for inside probabilities α(A,i,j) and outside probabilities β(A,i,j)?

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper constructs specific attention patterns to compute parsing probabilities, and analyzes how pre-trained models capture parsing information
  - Quick check question: How do relative positional embeddings modify the attention computation compared to standard attention?

## Architecture Onboarding

- Component map: Attention heads with specific query/key/value matrices -> Relative positional embeddings -> Layer structure alternating between inside and outside probability computation
- Critical path: For a sentence of length L, the model needs 2L layers: L-1 layers to compute inside probabilities, 1 layer for initialization, and L-1 layers to compute outside probabilities
- Design tradeoffs: More attention heads allow computing probabilities for more non-terminals simultaneously, but increase model size. The paper shows approximations that reduce heads by focusing on frequent non-terminals
- Failure signatures: Poor parsing performance (low F1 scores), inability to recover marginal probabilities, or high MLM loss on PCFG-generated data would indicate failure
- First 3 experiments:
  1. Implement the basic construction for a simple PCFG with 5 non-terminals and test parsing on short sentences
  2. Add relative positional embeddings and compare performance to the hard attention version
  3. Apply the frequency-based approximation to identify important non-terminals and reduce model size while monitoring performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformers learn more efficient approximate parsing algorithms than the Inside-Outside algorithm while maintaining comparable performance?
- Basis in paper: [explicit] The paper shows transformers can approximate Inside-Outside algorithm with 70% F1 score, but suggests future work could explore "design of experiments to interpret the content of the contextualized embeddings and thus 'reverse-engineer' the algorithms used by the learned model"
- Why unresolved: Current constructions focus on executing Inside-Outside algorithm, but transformers may be using alternative mechanisms that achieve similar results with fewer resources
- What evidence would resolve it: Finding transformers that achieve similar parsing performance using fundamentally different computational approaches, or proving theoretical lower bounds on parsing efficiency for transformers

### Open Question 2
- Question: How does the depth of transformer layers relate to the complexity of syntactic structures that can be parsed?
- Basis in paper: [explicit] The paper notes "although our number is acceptable since GPT-3 has 96 layers...More understanding of how language models can process such information in such a small number of layers is needed"
- Why unresolved: While the paper shows shallow layers capture significant syntactic information, the relationship between layer depth and parsing complexity remains unclear
- What evidence would resolve it: Systematic experiments varying transformer depth and measuring parsing performance on increasingly complex syntactic structures, or theoretical analysis of computational capacity vs. depth

### Open Question 3
- Question: Do transformers trained on natural language learn to approximate parsing algorithms differently than those trained on PCFG-generated data?
- Basis in paper: [explicit] The paper observes that "BERT-like models trained on natural language indeed contain semantic cues in their embeddings that help to parse" compared to PCFG-trained models, but doesn't explore the algorithmic differences
- Why unresolved: The paper shows both PCFG and natural language models capture syntactic information, but doesn't investigate whether the underlying computational mechanisms differ
- What evidence would resolve it: Comparing the attention patterns and intermediate representations between models trained on natural vs. synthetic data to identify algorithmic differences, or testing whether models trained on natural language can generalize to PCFG-like syntactic structures better than vice versa

## Limitations
- The theoretical constructions rely on hard attention mechanisms that may not fully capture the behavior of soft attention in pre-trained models
- The PCFG framework is a significant simplification of natural language syntax, limiting generalizability
- The probing experiments use relatively simple linear and 2-layer neural network probes that may not capture full complexity of syntactic encoding

## Confidence

**High confidence**: The theoretical results showing that transformers can approximate the Inside-Outside algorithm for PCFGs. The construction is explicit and the mathematical proofs appear sound.

**Medium confidence**: The claim that MLM training incentivizes transformers to learn parsing-like computations. While the theoretical connection to PCFGs is established, the extension to natural language is more speculative.

**Low confidence**: The interpretation that pre-trained models are "implicitly parsing" during MLM. The probing results show syntactic information is present, but don't definitively prove that parsing is the primary mechanism for MLM prediction.

## Next Checks
1. **Empirical validation of the soft-to-hard attention approximation**: Run controlled experiments comparing the performance of transformers using soft attention versus those constrained to use hard attention patterns similar to the theoretical construction. Measure parsing accuracy and MLM loss across different attention sparsity levels.

2. **Cross-linguistic generalization**: Apply the probing methodology to pre-trained models in languages with different syntactic structures (e.g., morphologically rich languages, languages with free word order). This would test whether the syntactic encoding is a general phenomenon or specific to English-like syntax.

3. **Intervention experiments**: Design targeted interventions in transformer attention patterns (e.g., masking specific attention heads or modifying attention weights) and measure the impact on both parsing performance and MLM accuracy. This could provide causal evidence for the role of parsing-like computations in MLM.