---
ver: rpa2
title: Approximating Score-based Explanation Techniques Using Conformal Regression
arxiv_id: '2308.11975'
source_url: https://arxiv.org/abs/2308.11975
tags:
- explanation
- conformal
- regression
- non-conformity
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of score-based
  explanation methods like SHAP, which limits their use in time-critical contexts.
  The authors propose approximating these explanations using regression models (XGBoost
  or MLP) to reduce inference time while providing validity guarantees via the conformal
  prediction framework.
---

# Approximating Score-based Explanation Techniques Using Conformal Regression

## Quick Facts
- arXiv ID: 2308.11975
- Source URL: https://arxiv.org/abs/2308.11975
- Reference count: 8
- Primary result: Regression models (XGBoost or MLP) can approximate SHAP explanations with reduced inference time while providing validity guarantees via conformal prediction.

## Executive Summary
This paper addresses the high computational cost of SHAP-based explanation methods by proposing a regression-based approximation approach. The authors train regression models to predict SHAP feature importance scores from input features and model predictions, then apply inductive conformal prediction to provide validity guarantees. The approach significantly reduces explanation inference time while maintaining statistical validity, with XGBoost per-feature regressors outperforming MLP in accuracy. Custom non-conformity measures based on class distributions and prediction confidence further improve efficiency.

## Method Summary
The proposed method trains regression models (XGBoost or MLP) on augmented data (features + black-box prediction) to predict SHAP feature importance scores, then applies inductive conformal prediction to provide validity intervals. Three novel non-conformity measures (min distance, avg distance, prediction confidence) estimate sample difficulty efficiently. The approach is evaluated on 30 datasets, showing significant reduction in execution time compared to TreeSHAP while maintaining coverage guarantees.

## Key Results
- XGBoost per-feature regressors produce tighter approximation intervals than single MLP multi-target regressors
- Custom non-conformity measures provide tighter intervals than standard KNN-based approaches while being computationally cheaper
- The method achieves significant reduction in execution time while maintaining 95% coverage guarantees
- Prediction confidence-based non-conformity measure performs best in terms of interval tightness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regression models can approximate SHAP feature importance scores with significantly lower inference time while maintaining validity guarantees.
- Mechanism: The approach trains a regression model (XGBoost or MLP) on augmented data (features + prediction) to predict SHAP scores, then uses inductive conformal prediction to provide validity intervals for each feature importance.
- Core assumption: SHAP scores for each feature are independent enough that separate regression problems can be formulated and solved efficiently.
- Evidence anchors:
  - [abstract] "validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework"
  - [section] "The proposed approach will be thoroughly evaluated to demonstrate its effectiveness in approximating the explanation method while reducing the computational cost."

### Mechanism 2
- Claim: Custom non-conformity measures based on class distributions or prediction confidence provide tighter intervals than standard distance-based measures.
- Mechanism: Three proposed measures (min distance, avg distance, prediction confidence) estimate sample difficulty without expensive k-nearest neighbors computation, leading to smaller interval sizes at the same validity level.
- Core assumption: Difficulty of approximating a feature importance score correlates with class separation or prediction confidence, which can be estimated cheaply.
- Evidence anchors:
  - [section] "we propose three possible non-conformity measures for the conformal regression"
  - [section] "the proposed difficulty estimates significantly outperform the ones using KNN with respect to the execution time"

### Mechanism 3
- Claim: XGBoost per-feature regressors outperform single MLP multi-target regressors in approximation accuracy.
- Mechanism: Training separate regressors per feature allows each to specialize, while MLP must learn all feature relationships jointly, potentially limiting accuracy.
- Core assumption: Feature importance scores are sufficiently independent that separate modeling per feature is beneficial.
- Evidence anchors:
  - [section] "The results also show that using an XGBoost regressor per feature can produce a more accurate approximation than using one multi-target MLP regressor for all features"
  - [section] "XGBoost is affected by the number of features since we train a regressor per feature, while MLP is affected by the number of instances"

## Foundational Learning

- Concept: Inductive conformal prediction
  - Why needed here: Provides distribution-free validity guarantees for the approximated feature importance scores without expensive recalibration.
  - Quick check question: What is the role of the calibration set in inductive conformal prediction, and how does it differ from transductive conformal prediction?

- Concept: SHAP (Shapley Additive Explanations)
  - Why needed here: The target explanation method being approximated; understanding its computational cost motivates the approximation approach.
  - Quick check question: Why does computing exact Shapley values require exponential time in the number of features?

- Concept: Multi-target vs single-target regression
  - Why needed here: The choice between XGBoost per-feature regressors (single-target) versus MLP for all features (multi-target) impacts both accuracy and efficiency.
  - Quick check question: What are the tradeoffs between modeling feature importance scores jointly versus separately in terms of accuracy and computational cost?

## Architecture Onboarding

- Component map: Data preprocessing -> Black-box model (XGBoost) -> SHAP explanations -> Regression approximators -> Conformal calibration -> Inference with validity intervals
- Critical path: Training black-box model → Generating SHAP explanations → Training regression approximators → Conformal calibration → Inference with validity intervals
- Design tradeoffs:
  - Separate XGBoost regressors per feature provide better accuracy but scale linearly with feature count
  - MLP multi-target model is more compact but may sacrifice accuracy due to joint modeling constraints
  - Custom non-conformity measures reduce computation vs KNN but rely on distributional assumptions
- Failure signatures:
  - Poor approximation accuracy: Large intervals relative to true SHAP scores, indicating regression model inadequacy
  - Invalid intervals: Coverage below 1-ε, suggesting calibration set issues or non-conformity measure problems
  - Excessive computation: Unexpectedly high inference time, possibly due to feature count or dataset size
- First 3 experiments:
  1. Train XGBoost black-box on small dataset, generate TreeSHAP explanations, train single XGBoost regressor for one feature, evaluate approximation error
  2. Implement one custom non-conformity measure (e.g., prediction confidence), compare interval sizes vs baseline (no difficulty estimate)
  3. Run full pipeline on medium dataset, compare execution time of XGBoost regressors vs MLP multi-target model, verify statistical significance via Friedman test

## Open Questions the Paper Calls Out
The paper explicitly mentions extending the approach to provide validity guarantees for entire explanation vectors rather than per-feature intervals using conformal multi-target regression.

## Limitations
- The assumption of feature independence for per-feature regression may break down for highly correlated features
- Custom non-conformity measures rely on distributional assumptions that may not hold for all datasets
- The computational benefits diminish for very small datasets where SHAP computation is already fast

## Confidence
- **High confidence**: The core mechanism of using regression models to approximate SHAP scores and reduce computation time is well-supported by the empirical results across 30 datasets.
- **Medium confidence**: The claim that XGBoost per-feature regressors outperform MLP is supported but may be dataset-dependent.
- **Medium confidence**: The effectiveness of custom non-conformity measures in providing tighter intervals is demonstrated, but the underlying distributional assumptions require further validation.

## Next Checks
1. Test the approach on datasets with high feature correlation to evaluate breakdown conditions for the per-feature regression assumption.
2. Validate the custom non-conformity measures on datasets with non-Gaussian class distributions to assess their robustness.
3. Compare the approximation quality when using different black-box models (e.g., neural networks) as the underlying predictor to evaluate generalizability beyond tree-based models.