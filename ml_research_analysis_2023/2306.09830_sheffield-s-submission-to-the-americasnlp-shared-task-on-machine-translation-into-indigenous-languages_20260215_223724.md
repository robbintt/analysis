---
ver: rpa2
title: Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation
  into Indigenous Languages
arxiv_id: '2306.09830'
source_url: https://arxiv.org/abs/2306.09830
tags:
- languages
- data
- training
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We participate in the 2023 AmericasNLP shared task by fine-tuning
  various versions of NLLB-200 on training data collected from different sources.
  We experiment with multilingual training and ensembling to improve the performance
  of the models.
---

# Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation into Indigenous Languages

## Quick Facts
- arXiv ID: 2306.09830
- Source URL: https://arxiv.org/abs/2306.09830
- Authors: 
- Reference count: 10
- Key outcome: Achieved highest average chrF across all languages of any submission, ranking first for four of the eleven languages

## Executive Summary
Sheffield's team participated in the 2023 AmericasNLP shared task by fine-tuning NLLB-200 models on collected training data for machine translation into 11 indigenous languages. They experimented with multilingual training across all language pairs and various ensemble strategies. Their approach achieved the highest average chrF score across all languages and ranked first for four specific languages (Asháninka, Guarani, Hñähñu, and Quechua). The work demonstrates the effectiveness of multilingual fine-tuning and ensembling for low-resource machine translation tasks.

## Method Summary
The team fine-tuned various versions of NLLB-200 (600M, 1.3B, and 3.3B parameters) on training data collected from multiple sources including the AmericasNLP 2023 organizers, Helsinki University, REPUcs, NLLB, backtranslations, and Bibles. They trained models in a multilingual fashion across all 11 language pairs (Spanish to Aymara, Bribri, Asháninka, Chatino, Guarani, Wixarika, Nahuatl, Hñähñu, Quechua, Shipibo-Konibo, and Rarámuri). The approach involved experimenting with parameter freezing, ensembling different model variants, and evaluating performance using chrF scores. They compared their results against a baseline model and submitted multiple model configurations to the shared task.

## Key Results
- Achieved highest average chrF across all 11 languages compared to all other submissions
- Ranked first for four languages: Asháninka, Guarani, Hñähñu, and Quechua
- Multilingual training improved performance on Quechua compared to bilingual training
- Ensembling provided improvements for five languages but not all

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NLLB-200's multilingual training provides cross-linguistic transfer benefits for indigenous languages.
- Mechanism: The model's exposure to 202 languages during pretraining allows it to learn generalizable linguistic patterns that transfer to similar polysynthetic/agglutinative structures in the target languages.
- Core assumption: Linguistic features shared across the target languages (and other languages in NLLB-200) are sufficiently similar to enable positive transfer.
- Evidence anchors:
  - [abstract] "NLLB-200 is trained on 202 languages across 1220 language pairs, including three of the languages present in the AmericasNLP shared task"
  - [section 3] "we train the models in a multilingual fashion across all 11 language pairs"
- Break condition: If the target languages lack sufficient structural similarity to NLLB-200's training languages, transfer benefits diminish.

### Mechanism 2
- Claim: Fine-tuning NLLB-200 on task-specific data significantly improves performance over baseline models.
- Mechanism: The model's pretrained weights serve as a strong initialization that can be adapted to the specific characteristics of the indigenous languages through task-specific fine-tuning.
- Core assumption: NLLB-200's pretraining provides a better starting point than random initialization for these low-resource languages.
- Evidence anchors:
  - [section 4.1] "we observe that for all languages, at least one of our models outperforms the baseline"
  - [section 5] "we see that this model performs much worse than the equivalent NLLB-200 model"
- Break condition: If task-specific data is too dissimilar from pretraining data, fine-tuning may not provide significant benefits.

### Mechanism 3
- Claim: Ensembling multiple model variants improves overall performance by capturing different aspects of the translation task.
- Mechanism: Different model checkpoints capture different translation patterns, and combining their outputs reduces individual model biases.
- Core assumption: The errors made by different models are sufficiently diverse that ensembling can reduce overall error.
- Evidence anchors:
  - [section 3] "we experiment with various ensembles of models in attempt to improve performance further"
  - [section 4.1] "we only find improvements over Submission 2 through ensembling for five of the languages"
- Break condition: If models make correlated errors, ensembling may not provide meaningful improvements.

## Foundational Learning

- Concept: Low-resource machine translation
  - Why needed here: The task involves translating into 11 indigenous languages with limited parallel data
  - Quick check question: What are the primary challenges when training MT models with limited parallel data?

- Concept: Multilingual fine-tuning
  - Why needed here: Models are trained across all 11 language pairs simultaneously
  - Quick check question: How does multilingual fine-tuning differ from training separate models for each language pair?

- Concept: Backtranslation
  - Why needed here: Used to generate additional training data from monolingual sources
  - Quick check question: What are the key steps in creating backtranslated data for MT training?

## Architecture Onboarding

- Component map: NLLB-200 base model → multilingual fine-tuning → backtranslation augmentation → ensemble voting
- Critical path: Data collection → preprocessing → fine-tuning → evaluation → ensemble selection
- Design tradeoffs: Computational cost vs. performance (NLLB-3.3B vs. 1.3B), data diversity vs. noise (including backtranslations)
- Failure signatures: Degradation in chrF scores during fine-tuning, unstable validation curves, negative transfer effects
- First 3 experiments:
  1. Compare chrF scores of NLLB-200 inference vs. fine-tuned models on dev set
  2. Evaluate impact of backtranslations by training with/without this data
  3. Test multilingual vs. bilingual training approaches on a single language pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or structural similarities across the indigenous languages in the AmericasNLP task enable successful multilingual training, and how can these be systematically identified to improve low-resource MT?
- Basis in paper: [inferred] The authors note that multilingual training improved performance on Quechua and suggest it may be related to shared linguistic properties (e.g., polysynthetic or agglutinative) despite different language families.
- Why unresolved: The paper only speculates about shared linguistic properties but does not analyze which specific features contribute to transfer learning or provide a systematic method to identify them.
- What evidence would resolve it: A detailed linguistic analysis identifying common morphological, syntactic, or semantic features across the task languages, combined with controlled experiments isolating the impact of these features on MT performance.

### Open Question 2
- Question: Why does the inclusion of Bible data sometimes degrade performance in low-resource MT systems, and under what conditions does it provide benefits?
- Basis in paper: [explicit] The authors observe that training on Bible data led to a drop in average performance for most languages, with only minor benefits for Guarani and Hñähñu.
- Why unresolved: The paper notes the effect but does not investigate the underlying causes or identify conditions where Bible data is beneficial.
- What evidence would resolve it: Controlled experiments varying domain, data size, and linguistic distance between Bible text and target domains, along with analysis of vocabulary overlap and translation quality across different text types.

### Open Question 3
- Question: How can zero-shot translation capabilities be enhanced in low-resource settings without requiring additional parallel data?
- Basis in paper: [explicit] The authors find their model retains decent performance for zero-shot translations (max 25% drop in chrF) despite training all parameters, but do not explore methods to improve this.
- Why unresolved: The paper reports zero-shot performance but does not investigate techniques like adapter layers, meta-learning, or cross-lingual embeddings that could enhance it.
- What evidence would resolve it: Comparative experiments testing different architectural modifications or training strategies designed to improve zero-shot translation quality, measured against the baseline zero-shot performance.

### Open Question 4
- Question: What is the optimal strategy for selecting and combining different NLLB model sizes and checkpoints in ensemble methods for low-resource MT?
- Basis in paper: [inferred] The authors experiment with ensembles but only find improvements for five languages, suggesting that model selection and combination strategies significantly impact results.
- Why unresolved: The paper does not analyze which factors (model size, checkpoint timing, language similarity) contribute to successful ensembling or provide a principled selection method.
- What evidence would resolve it: Systematic experiments varying ensemble composition (different model sizes, checkpoints, and languages), combined with analysis of when and why certain combinations outperform others.

## Limitations
- Data preprocessing details are incomplete, making exact reproduction challenging
- Limited discussion of potential negative transfer effects in multilingual training
- Ensemble effectiveness is mixed, improving performance for only 5 of 11 languages

## Confidence
- High confidence in the overall methodology and experimental results
- Medium confidence in the generalizability of multilingual training benefits across all 11 languages
- Medium confidence in the specific contribution of each data source (especially backtranslations)
- Low confidence in the optimal hyperparameter settings due to limited ablation studies

## Next Checks
1. Replicate the preprocessing pipeline on a single language pair to verify the exact handling of punctuation, tokenization, and character normalization
2. Conduct controlled experiments comparing NLLB-200 fine-tuning against random initialization on languages with different resource levels
3. Perform error analysis on ensemble outputs to quantify whether ensembling reduces diversity of errors or simply averages correlated mistakes