---
ver: rpa2
title: Exploring the Limits of Historical Information for Temporal Knowledge Graph
  Extrapolation
arxiv_id: '2308.15002'
source_url: https://arxiv.org/abs/2308.15002
tags:
- historical
- events
- entities
- cenet
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines the limits of historical information for temporal
  knowledge graph extrapolation and proposes a new event forecasting model called
  Contrastive Event Network (CENET). CENET learns both the historical and non-historical
  dependency to distinguish the most potential entities that best match the given
  query, and employs contrastive learning to train representations of queries to probe
  whether the current moment is more dependent on historical or non-historical events.
---

# Exploring the Limits of Historical Information for Temporal Knowledge Graph Extrapolation

## Quick Facts
- arXiv ID: 2308.15002
- Source URL: https://arxiv.org/abs/2308.15002
- Authors: 
- Reference count: 40
- Key outcome: CENET achieves at least 8.3% relative improvement of Hits@1 over previous state-of-the-art baselines on event-based datasets

## Executive Summary
This paper addresses the challenge of temporal knowledge graph (TKG) extrapolation, particularly for predicting new events in event-based TKGs. The authors propose Contrastive Event Network (CENET), a novel model that learns both historical and non-historical dependencies simultaneously. By employing contrastive learning and a mask-based inference strategy, CENET significantly outperforms existing methods on benchmark datasets, demonstrating the importance of considering both historical recurrence patterns and non-historical underlying factors in event forecasting.

## Method Summary
CENET is a temporal knowledge graph extrapolation model that learns both historical and non-historical dependencies to improve event forecasting. The model generates frequency vectors for historical entities and transforms them into representations where positive values indicate historical events and negative values indicate non-historical events. Two context vectors capture dependencies from each category. CENET employs a two-stage contrastive learning process to separate query representations into historical and non-historical classes, which are then used by a binary classifier to create a boolean mask for the final prediction. The model combines probability distributions with entity relevance masks to produce the final output.

## Key Results
- CENET significantly outperforms all existing methods in most metrics
- Achieves at least 8.3% relative improvement of Hits@1 over previous state-of-the-art baselines on event-based datasets
- Demonstrates the importance of learning both historical and non-historical dependencies for TKG extrapolation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CENET improves prediction for new events by learning both historical and non-historical dependency simultaneously.
- Mechanism: The model generates frequency vectors (Fs,p,t) for historical entities and transforms them into Zs,p,t, where positive values indicate historical events and negative values indicate non-historical events. Two context vectors (Hs,p,his and Hs,p,nhis) are computed to capture dependencies from each category.
- Core assumption: Both historical recurrence patterns and non-historical underlying factors contribute to future event prediction, and distinguishing between them is critical for performance.
- Evidence anchors:
  - [abstract]: "CENET learns both the historical and non-historical dependency to distinguish the most potential entities that best match the given query"
  - [section 3.2]: "CENET takes not only historical but also non-historical entities into consideration"
  - [corpus]: Weak evidence - the corpus shows related works on TKG reasoning but doesn't directly confirm the dual-dependency mechanism
- Break condition: If the non-historical dependency learning is disabled, performance drops significantly on event-based datasets with high rates of new events (58.86% for ICEWS18).

### Mechanism 2
- Claim: Historical contrastive learning improves entity identification by training query representations to distinguish between historical and non-historical entity classes.
- Mechanism: Two-stage contrastive learning where stage 1 uses supervised contrastive loss to separate query representations based on whether the missing object is historical, and stage 2 trains a binary classifier using these representations.
- Core assumption: Queries can be effectively separated into two semantic classes (historical vs. non-historical object entities), and this separation improves prediction accuracy.
- Evidence anchors:
  - [abstract]: "by launching contrastive learning, it trains representations of queries to probe whether the current moment is more dependent on historical or non-historical events"
  - [section 3.3]: "CENET naturally employs supervised contrastive learning to train representations of the two classes of queries"
  - [section 4.3.2]: "Figure 6 illustrates the 2-d sphereized PCA projection of query embeddings... there exists an explicit border between the two types of queries for CENET with Lsup loss"
- Break condition: If the supervised contrastive loss (Lsup) is removed, query representations become less separable, particularly hurting performance on public KGs.

### Mechanism 3
- Claim: Mask-based inference strategy improves final predictions by combining probability distributions with entity relevance masks.
- Mechanism: A boolean mask vector Bs,p,t is generated from the binary classifier output, where positive positions indicate entities to prioritize. The final probability is computed as P(o|s,p,Fs,p,t) = Ps,p,t(o) × Bs,p,t(o) for hard mask or with softmax for soft mask.
- Core assumption: The binary classifier can reliably identify which entity set (historical or non-historical) is more relevant for each query, and this information should directly influence the final prediction distribution.
- Evidence anchors:
  - [abstract]: "These representations further help train a binary classifier, whose output is a boolean mask, indicating the related entities in the search space"
  - [section 3.4]: "CENET will choose the object with the highest probability as the final prediction ˆo"
  - [section 4.3.3]: "We also study different variants of mask strategy in CENET... untrained model with randomly generated mask vector is counterproductive to the prediction"
- Break condition: If the binary classifier accuracy drops below ~70%, the hard mask strategy may incorrectly eliminate correct answers, requiring fallback to soft mask.

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs) extend static KGs with time dimension, where facts are quadruples (s, p, o, t)
  - Why needed here: CENET operates specifically on TKGs and needs to understand how temporal information is represented and used for event forecasting
  - Quick check question: What distinguishes a TKG from a static KG in terms of data structure?

- Concept: Contrastive learning as a self-supervised technique for representation learning
  - Why needed here: CENET uses supervised contrastive learning to separate query representations into historical vs. non-historical classes
  - Quick check question: How does supervised contrastive loss differ from standard cross-entropy loss in terms of what it optimizes?

- Concept: Copy mechanism in sequence modeling
  - Why needed here: CENET uses a copy mechanism-based scoring strategy to incorporate frequency information without gradient interference
  - Quick check question: What problem does the copy mechanism solve in the context of incorporating frequency information?

## Architecture Onboarding

- Component map: Query (s, p, ?, t) and frequency vectors (Fs,p,t, Zs,p,t) -> Historical dependency module -> Non-historical dependency module -> Contrastive learning module -> Mask-based inference -> Predicted entity o

- Critical path: Query → Frequency vectors → Dual dependency computation → Combined distribution → Binary classifier mask → Final prediction

- Design tradeoffs:
  - Time vs. accuracy: Considering all historical events (no truncation) increases computational cost but improves performance on datasets with long-range dependencies
  - Hard vs. soft mask: Hard mask provides cleaner separation but is sensitive to classifier accuracy; soft mask is more robust but less decisive
  - α parameter: Balancing Lce and Lsup losses affects performance differently on event-based vs. public KGs

- Failure signatures:
  - Poor performance on new events suggests inadequate non-historical dependency learning
  - Degradation on public KGs suggests insufficient contrastive representation separation
  - Classifier accuracy below 70% indicates need to switch from hard to soft mask
  - Memory issues suggest frequency vector storage could be optimized

- First 3 experiments:
  1. Compare CENET variants: CENET-his (historical only) vs. CENET-nhis (non-historical only) vs. full CENET on ICEWS18 to validate dual dependency importance
  2. Test mask strategies: Compare hard mask vs. soft mask vs. random mask on YAGO to determine optimal mask type based on classifier accuracy
  3. Ablation study: Remove Lsup loss and measure impact on query representation separation using PCA visualization to verify contrastive learning contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CENET compare to other models when predicting new events on event-based TKGs?
- Basis in paper: [explicit] The paper states that CENET significantly outperforms all existing methods in most metrics, achieving at least 8.3% relative improvement of Hits@1 over previous state-of-the-art baselines on event-based datasets.
- Why unresolved: The paper does not provide a detailed comparison of CENET's performance with other models specifically for predicting new events on event-based TKGs.
- What evidence would resolve it: A detailed comparison of CENET's performance with other models for predicting new events on event-based TKGs, including metrics such as MRR and Hits@1/3/10.

### Open Question 2
- Question: How does the accuracy of the binary classifier in CENET affect its performance on different types of TKGs?
- Basis in paper: [explicit] The paper mentions that the accuracy of the binary classifier is higher in YAGO (about 90%) compared to ICEWS18 (about 70%), and suggests that the higher accuracy of the binary classification leads to better performance with the hard-mask strategy.
- Why unresolved: The paper does not provide a detailed analysis of how the accuracy of the binary classifier affects CENET's performance on different types of TKGs.
- What evidence would resolve it: A detailed analysis of how the accuracy of the binary classifier affects CENET's performance on different types of TKGs, including metrics such as MRR and Hits@1/3/10.

### Open Question 3
- Question: How does the performance of CENET vary with different values of the hyperparameters α and λ?
- Basis in paper: [explicit] The paper mentions that the values of α and λ are set to 0.2 and 2, respectively, and suggests that the performance of CENET may vary with different values of these hyperparameters.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of CENET varies with different values of the hyperparameters α and λ.
- What evidence would resolve it: A detailed analysis of how the performance of CENET varies with different values of the hyperparameters α and λ, including metrics such as MRR and Hits@1/3/10.

## Limitations

- The exact mechanism by which negative frequency subtraction captures "non-historical" information could be more explicitly explained
- The binary classifier's mask strategy effectiveness appears sensitive to classifier accuracy thresholds that aren't clearly defined
- The paper demonstrates superior performance on benchmark datasets but doesn't thoroughly explore failure cases or robustness to noise

## Confidence

- **High Confidence**: CENET's dual dependency learning significantly outperforms single-dependency approaches on event-based TKGs; contrastive learning effectively separates query representations into historical/non-historical classes; mask-based inference improves predictions when classifier accuracy is sufficient.
- **Medium Confidence**: The negative frequency subtraction mechanism robustly captures non-historical dependencies; the two-stage contrastive learning process generalizes across different TKG datasets; hard mask strategy is optimal for all scenarios.
- **Low Confidence**: The model's performance translates directly to real-world event forecasting applications; the computational overhead of considering all historical events is justified in all use cases; the frequency vector approach scales efficiently to very large TKGs.

## Next Checks

1. Conduct ablation studies removing either historical or non-historical dependency modules separately to quantify their individual contributions across different dataset types, particularly focusing on datasets with varying proportions of new events.

2. Test the model's robustness by introducing controlled noise into historical frequency vectors and measuring degradation in performance, to understand the sensitivity of the dual-dependency mechanism to data quality issues.

3. Evaluate classifier accuracy thresholds across datasets to determine optimal switching points between hard and soft mask strategies, and analyze how this affects overall model performance in scenarios where classifier accuracy varies.