---
ver: rpa2
title: Towards Understanding the Effect of Pretraining Label Granularity
arxiv_id: '2303.16887'
source_url: https://arxiv.org/abs/2303.16887
tags:
- pretraining
- training
- label
- class
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the granularity of pretraining labels affects
  the generalization of deep neural networks in image classification tasks. It focuses
  on the "fine-to-coarse" transfer learning setting, where the pretraining label space
  is more fine-grained than that of the target problem.
---

# Towards Understanding the Effect of Pretraining Label Granularity

## Quick Facts
- arXiv ID: 2303.16887
- Source URL: https://arxiv.org/abs/2303.16887
- Reference count: 40
- Key outcome: Fine-grained pretraining enables learning of rarer features and improves generalization when pretraining dataset is large and label functions align well.

## Executive Summary
This paper investigates how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. Focusing on "fine-to-coarse" transfer learning, the authors demonstrate that pretraining on fine-grained labels enables learning of rarer, subclass-specific features that improve downstream performance. Their comprehensive experiments on iNaturalist 2021 and ImageNet show that optimal transfer requires both a meaningful label hierarchy and good alignment between pretraining and target label functions. Theoretical analysis proves that coarse-label training biases models toward learning only common features, while fine-grained labels force discovery of rarer discriminative features.

## Method Summary
The authors conduct experiments using ResNet and Vision Transformer architectures, pretraining on datasets with varying label granularities then fine-tuning on target tasks. They generate label hierarchies through WordNet extraction and kMeans clustering on CLIP embeddings. Training uses cross-entropy loss with standard image classification pipelines. The theoretical component analyzes a two-layer convolutional ReLU network to prove that coarse-label training leads to learning only common features, while fine-grained labels enable learning of rarer features. Experiments systematically vary label granularity levels and compare transfer performance against baselines.

## Key Results
- Pretraining on fine-grained labels enables learning of rarer, subclass-specific features that improve downstream generalization
- There is a U-shaped relationship between label granularity and performance - too coarse misses fine features, too fine provides insufficient samples
- Optimal transfer requires both a meaningful label hierarchy and good alignment between pretraining and target label functions
- On ImageNet, pretraining on leaf labels of ImageNet21k produces better transfer results than coarser granularity levels

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained pretraining forces networks to learn rare features by making each subclass its own label. When training with coarse labels, networks are biased toward learning only common features that appear frequently. Fine-grained labels force discovery of subclass-specific features because each subclass now has equal importance. Core assumption: pretraining and target label functions must align well. Evidence: theoretical proof in Theorem 5.1, experimental results on ImageNet21k. Break condition: meaningless hierarchy or poor label function alignment.

### Mechanism 2
U-shaped relationship exists between pretraining label granularity and downstream performance. Coarse pretraining leads to shortcut learning on common features. Very fine pretraining causes severe class imbalance due to too few samples per subclass. Intermediate granularity balances feature richness with sample adequacy. Core assumption: pretraining dataset must be sufficiently large. Evidence: Figure 3 showing error rising with too fine granularity, iNaturalist experiments. Break condition: dataset too small for chosen granularity.

### Mechanism 3
Neural networks in feature-learning regime can adapt to learn new discriminative features during pretraining, unlike NTK where features are fixed. ReLU network architecture with sufficient width allows hidden neurons to move and specialize during SGD. Core assumption: network must be wide enough with small variance initialization. Evidence: analysis showing ReLU nonlinearity enables better generalization than linear models, experimental results across architectures. Break condition: network too small or large variance initialization collapses into NTK regime.

## Foundational Learning

- **Feature hierarchy in natural images (common vs. fine-grained features)**: Understanding how labels at different granularities capture different feature sets is central to why pretraining label choice matters. Quick check: In cats vs. dogs dataset, what features would be "common" vs. "fine-grained," and how would labeling at species vs. breed level affect what the model learns?

- **Shortcut learning and simplicity bias in neural networks**: Explains why models trained on coarse labels may ignore rare but useful features, opting for easier-to-learn common patterns. Quick check: Why might a model trained to distinguish cats vs. dogs ignore breed-specific fur patterns if those patterns are less frequent in the training set?

- **Stochastic gradient descent dynamics in overparameterized networks**: The paper's theoretical results depend on tracking how individual neurons evolve during training to capture feature learning. Quick check: How does movement of individual hidden neurons during SGD enable discovery of new features, as opposed to just adjusting linear classifier weights?

## Architecture Onboarding

- **Component map**: Input images -> CNN/ViT backbone -> Dense layer with softmax over labels -> Cross-entropy loss
- **Critical path**: Pretraining on fine-grained labels → Finetuning on coarse labels → Evaluation on target task
- **Design tradeoffs**: Granularity vs. sample size per class (finer → richer features but fewer samples); architecture choice (CNNs learn spatial hierarchies, ViTs capture global context); initialization strategy (small variance enables feature learning, large variance may push into NTK regime)
- **Failure signatures**: Performance flat/worse than baseline (granularity too fine or too coarse); severe overfitting during finetuning (too fine-grained with insufficient samples); no improvement over baseline (poor label function alignment or meaningless hierarchy)
- **First 3 experiments**: 1) Train ResNet/ViT directly on target task for baseline accuracy; 2) Pretrain on target task labels (same granularity) then finetune; 3) Pretrain on fine-grained labels (ImageNet21k leaf nodes) then finetune on ImageNet1k

## Open Questions the Paper Calls Out

- How do training dataset size and label granularity interact to determine success of fine-to-coarse transfer learning? The paper shows large datasets are necessary but doesn't provide precise quantitative relationship. Experiments systematically varying dataset size and granularity on same target task would resolve this.

- Does effectiveness of fine-grained pretraining depend on specific structure of label hierarchy or primarily on number of classes? The paper shows ImageNet21k leaf labels are more effective than coarser levels but doesn't compare different hierarchies with same number of classes. Experiments comparing different hierarchies with same class count would resolve this.

- How does alignment between pretraining and target label functions affect effectiveness of fine-grained pretraining? The paper shows manual hierarchies of iNaturalist 2021 work better than random/cluster hierarchies but doesn't quantify required alignment level. Experiments systematically varying alignment degree would resolve this.

## Limitations

- Theoretical claims rely heavily on simplified two-layer convolutional ReLU network model, limiting direct applicability to deep architectures
- Experimental validation focuses primarily on image classification tasks with hierarchical labels, limiting generalizability to other domains
- Does not provide precise quantitative relationship between dataset size, label granularity, and transfer performance

## Confidence

- **High confidence**: Empirical observation that intermediate label granularity often yields optimal transfer performance is well-supported across multiple datasets and architectures
- **Medium confidence**: Theoretical proof that fine-grained pretraining enables learning of rarer features is sound within simplified model but extension to practical deep networks requires further validation
- **Medium confidence**: Claim that label function alignment is crucial for effective transfer is supported by experiments but precise mechanisms and quantitative measures remain underdeveloped

## Next Checks

1. Replicate theoretical analysis using deeper network architectures to verify if feature learning dynamics persist beyond two-layer models
2. Design experiments with artificially constructed label hierarchies to systematically vary label function alignment and measure its impact on transfer performance
3. Test fine-to-coarse transfer framework on non-hierarchical label spaces (multi-label classification or regression tasks) to assess generalizability beyond image classification