---
ver: rpa2
title: Layer-wise Representation Fusion for Compositional Generalization
arxiv_id: '2307.10799'
source_url: https://arxiv.org/abs/2307.10799
tags:
- encoder
- decoder
- layers
- transformer
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles compositional generalization (CG) in neural
  sequence models, a critical gap in systematic language generalization. The authors
  identify that entangled syntactic and semantic representations in the uppermost
  layers of Transformers hinder CG.
---

# Layer-wise Representation Fusion for Compositional Generalization

## Quick Facts
- arXiv ID: 2307.10799
- Source URL: https://arxiv.org/abs/2307.10799
- Authors: 
- Reference count: 40
- Key outcome: Introduces FUSION, a layer-wise representation fusion framework that achieves significant gains on compositional generalization tasks, reducing CTER by 8.4% (instance-level) and 12.6% (aggregate-level) on CoGnition and improving CFQ exact-match accuracy by 21% relative to RoBERTa.

## Executive Summary
This paper addresses the critical challenge of compositional generalization (CG) in neural sequence models, where models fail to systematically generalize to novel combinations of familiar components. The authors identify that shallow residual connections in Transformers cause information forgetting between layers, leading to entangled syntactic and semantic representations in the uppermost layers. Their solution, FUSION, introduces fuse-attention modules at each encoder and decoder layer to learn appropriate fusion of previous layers' information, enabling effective composition of syntax and semantics. Extensive experiments on CFQ (semantic parsing) and CoGnition (machine translation) demonstrate significant performance improvements over strong baselines.

## Method Summary
FUSION extends the standard Transformer architecture by adding fuse-attention modules at each encoder and decoder layer. These modules take previous layers' outputs as keys/values and the current layer's self/cross-attention output as query, applying multi-head attention to create a weighted combination that preserves and reintegrates syntactic information from lower layers into deeper semantic representations. The fused output is then fed into the feed-forward sub-layer. The method is implemented in Fairseq and trained using standard cross-entropy loss with Adam optimization. The fuse-attention modules are designed to be compatible with any Seq2Seq model and are evaluated on both semantic parsing (CFQ) and machine translation (CoGnition) tasks.

## Key Results
- On CoGnition, FUSION reduces instance-level CTER by 8.4% and aggregate-level CTER by 12.6% compared to strong baselines.
- On CFQ, achieves 52.4% exact-match accuracy, representing a 21% relative improvement over RoBERTa.
- The improvements are shown to be cumulative when applying fuse-attention to both encoder and decoder layers, with encoder-side fusion having particularly significant impact.
- FUSION is demonstrated to be general, working effectively with pre-trained models and across different task types.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shallow residual connections in Transformers fail to fuse information from previous layers effectively, leading to information forgetting and representation entanglement (RE) problems.
- Mechanism: Standard Transformer layers only pass information forward through simple residual connections. This shallow merging means that deeper layers lose syntactic information from earlier layers, causing semantic and syntactic representations to become entangled in the uppermost layers.
- Core assumption: Different Transformer layers encode different aspects of language (bottom layers more syntactic, top layers more semantic), and proper fusion of these representations is necessary for compositional generalization.
- Evidence anchors:
  - [abstract]: "We find that the ``shallow'' residual connections within each layer fail to fuse previous layers' information effectively, leading to information forgetting between layers and further the RE problems."
  - [section 2.2.1]: "The Transformer contains syntactic and semantic information in different layers, so what leads to representations of the encoder and decoder uppermost layer are entangled and also forget most of syntactic information."
  - [corpus]: Weak. No direct corpus evidence for the shallow residual connection claim, though related work on deeper Transformers exists.
- Break condition: If residual connections were deep enough to preserve information from all previous layers, or if layer-wise information fusion was unnecessary for compositional generalization.

### Mechanism 2
- Claim: Fusing previous layers' information back into the encoding and decoding process through a fuse-attention module at each layer allows appropriate composition of syntactic and semantic representations.
- Mechanism: The fuse-attention module applies multi-head attention over all previous layer outputs at each encoder/decoder layer, creating a weighted combination that preserves and reintegrates syntactic information from lower layers into the deeper semantic representations.
- Core assumption: The appropriate composition of syntactic and semantic information requires active fusion mechanisms, not just passive residual connections.
- Evidence anchors:
  - [abstract]: "we propose LRF, a novel \textbf{L}ayer-wise \textbf{R}epresentation \textbf{F}usion framework for CG, which learns to fuse previous layers' information back into the encoding and decoding process effectively through introducing a \emph{fuse-attention module} at each encoder and decoder layer."
  - [section 2.2.1]: "The fuse-attention module fuses different aspects of language information from previous layers appropriately via the multi-head attention mechanism."
  - [corpus]: Weak. The related work "Learning to Compose Representations of Different Encoder Layers" appears directly related but corpus data doesn't provide specific evidence.
- Break condition: If the fuse-attention module learned inappropriate weightings that degraded performance, or if simple residual connections were sufficient for compositional generalization.

### Mechanism 3
- Claim: Addressing both encoder and decoder entanglement problems simultaneously through layer-wise fusion provides cumulative gains over solving either problem alone.
- Mechanism: The fuse-attention modules are placed at both encoder and decoder layers, allowing the model to properly compose representations in both the source encoding and target decoding processes, with the encoder improvements having particular impact on translation quality.
- Core assumption: Both encoder and decoder representations become entangled, and solving both problems jointly provides better compositional generalization than addressing either in isolation.
- Evidence anchors:
  - [section 4.1]: "We observe certain improvements (-7.2% and -2.6% CTERInst, -9.6% and -4.1% CTERAggr) when separately applying the fuse-attention modules at the encoder or decoder side... Furthermore, the combination of them brings further improvement (-8.4% CTERInst, -12.6% CTERAggr)."
  - [section 4.1]: "In addition, it is clear that the improvement from introducing the fuse-attention module at the encoder is more significant than that at the decoder, indicating that the encoder entanglement problem is more severe."
  - [corpus]: Weak. No corpus evidence for this specific cumulative gain claim.
- Break condition: If improvements were only additive rather than cumulative, or if one side (encoder vs decoder) dominated the gains completely.

## Foundational Learning

- Concept: Residual connections and information flow in deep neural networks
  - Why needed here: Understanding why shallow residual connections cause information loss is fundamental to grasping why the fuse-attention module is necessary
  - Quick check question: In a standard Transformer layer, how does information from layer 2 reach layer 4, and what information might be lost in this process?

- Concept: Multi-head attention mechanism
  - Why needed here: The fuse-attention module uses multi-head attention to combine information from previous layers, so understanding this mechanism is crucial
  - Quick check question: How does multi-head attention differ from simple weighted averaging when combining information from multiple sources?

- Concept: Compositional generalization and representation entanglement
  - Why needed here: The paper's core problem is that entangled representations prevent compositional generalization, so understanding both concepts is essential
  - Quick check question: Why would entangled syntactic and semantic representations in the uppermost layers specifically hinder compositional generalization?

## Architecture Onboarding

- Component map:
  Base Transformer encoder/decoder layers (standard self-attention, cross-attention, feed-forward) -> Fuse-attention module inserted at each encoder/decoder layer -> The fuse-attention module takes previous layers' outputs as keys/values and the current layer's self/cross-attention output as query -> Outputs from fuse-attention module feed into the feed-forward sub-layer

- Critical path:
  1. Input embedding → Layer 1 → Fuse-attention (over embeddings) → Feed-forward → Layer 2 output
  2. Layer 2 output + all previous outputs → Fuse-attention (over layers 1 and 2) → Feed-forward → Layer 3 output
  3. Continue this pattern through all layers
  4. Final layer outputs feed into standard Transformer cross-attention and output generation

- Design tradeoffs:
  - Memory vs performance: Fuse-attention requires storing all previous layer outputs, increasing memory usage
  - Parameter count: Additional parameters in fuse-attention modules (~12M extra in experiments)
  - Computational cost: Additional multi-head attention computation at each layer
  - Training stability: More complex gradient flow through additional attention mechanisms

- Failure signatures:
  - Performance degradation when fuse-attention modules are added (indicating inappropriate information fusion)
  - Memory overflow during training (indicating insufficient memory for storing previous layer outputs)
  - Training instability or slow convergence (indicating complex optimization landscape)

- First 3 experiments:
  1. Baseline: Train standard Transformer on CoGnition dataset, measure CTER metrics
  2. Single-side fusion: Add fuse-attention only to encoder layers, measure improvement vs baseline
  3. Full fusion: Add fuse-attention to both encoder and decoder, measure improvement vs single-side and baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fuse-attention module's attention weights evolve during training, and what factors influence their final distribution across layers?
- Basis in paper: [explicit] The paper mentions that different encoder and decoder layers learn different attention weights to previous layers' information, implying an evolution during training.
- Why unresolved: The paper only visualizes attention probabilities at the end of training, not during the learning process. The factors influencing these weights (e.g., initialization, data distribution, loss landscape) are not explored.
- What evidence would resolve it: Analyzing attention weight distributions across training epochs, correlating them with training dynamics (loss curves, gradient norms), and conducting ablation studies on initialization schemes or data augmentation methods.

### Open Question 2
- Question: Is the effectiveness of layer-wise fusion dependent on the specific task complexity or dataset characteristics (e.g., vocabulary size, sentence length distribution)?
- Basis in paper: [inferred] The paper shows FUSION works on both CFQ and CoGnition, but the relative improvements vary significantly (e.g., 21% relative improvement on CFQ vs. state-of-the-art on CoGnition). This suggests task-dependent effectiveness.
- Why unresolved: The paper does not analyze whether FUSION's performance gains correlate with dataset properties or task difficulty. No systematic study across diverse tasks or dataset modifications is provided.
- What evidence would resolve it: Benchmarking FUSION across a wider range of NLP tasks with varying complexities, conducting controlled experiments by modifying dataset properties (e.g., increasing sentence length, reducing vocabulary overlap), and analyzing performance trends.

### Open Question 3
- Question: Can the fuse-attention mechanism be extended to other sequence modeling architectures beyond Transformers (e.g., RNNs, CNNs) or adapted for non-sequential data?
- Basis in paper: [explicit] The authors state their method is "mostly applicable to any Seq2Seq models" but only demonstrate it on Transformer-based models.
- Why unresolved: The paper does not provide theoretical justification or empirical evidence for FUSION's applicability to other architectures. The mechanism's compatibility with non-sequential data (e.g., graphs, images) is unexplored.
- What evidence would resolve it: Implementing and evaluating FUSION on alternative architectures (e.g., LSTM, CNN-based Seq2Seq models), adapting the mechanism for graph neural networks or convolutional architectures, and analyzing performance relative to architecture-specific solutions.

## Limitations

- The core claim about shallow residual connections causing information forgetting relies on untested assumptions about information flow in Transformers, with no direct ablation studies comparing alternative architectures.
- The fuse-attention mechanism's effectiveness depends on learned attention patterns that are not fully characterized or visualized throughout training, making it difficult to understand when and why the method might fail.
- Claims about the relative severity of encoder vs decoder entanglement problems are based on limited ablation studies without deeper analysis of the underlying reasons for asymmetric improvements.

## Confidence

- High Confidence: Performance improvements on CFQ and CoGnition datasets are well-documented with proper metrics (CTER, exact-match accuracy). The implementation details provided are sufficient for reproduction.
- Medium Confidence: The mechanism by which fuse-attention improves compositional generalization is plausible but relies on untested assumptions about layer-wise representation properties and residual connection limitations.
- Low Confidence: Claims about the relative severity of encoder vs decoder entanglement problems are based on limited ablation studies without deeper analysis of why one side shows more improvement than the other.

## Next Checks

1. **Attention Pattern Analysis**: Visualize and analyze the attention weight distributions learned by fuse-attention modules across layers to verify they are capturing meaningful syntactic-semantic composition rather than defaulting to uniform weights.

2. **Residual Connection Ablation**: Implement and test an alternative architecture with deeper residual connections (e.g., skip connections from layer 1 to layer 3, layer 2 to layer 4, etc.) to isolate whether the performance gains are specifically due to fuse-attention or just deeper information flow.

3. **Memory-Efficient Variants**: Test a memory-optimized version of FUSION that uses gradient checkpointing or selective layer fusion to understand the performance-memory tradeoff and determine if the full fuse-attention across all layers is necessary for the observed gains.