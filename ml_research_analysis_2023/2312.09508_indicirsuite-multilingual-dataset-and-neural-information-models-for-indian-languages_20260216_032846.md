---
ver: rpa2
title: 'IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian
  Languages'
arxiv_id: '2312.09508'
source_url: https://arxiv.org/abs/2312.09508
tags:
- languages
- dataset
- neural
- indian
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndicIRSuite, a comprehensive set of neural
  information retrieval (IR) resources for 11 widely spoken Indian languages. The
  suite includes INDIC-MARCO, a large-scale multilingual dataset created by translating
  the MSMARCO dataset into Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam,
  Marathi, Oriya, Punjabi, Tamil, and Telugu using machine translation.
---

# IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages

## Quick Facts
- **arXiv ID**: 2312.09508
- **Source URL**: https://arxiv.org/abs/2312.09508
- **Reference count**: 23
- **Key outcome**: First large-scale neural IR resources for 11 Indian languages, achieving significant improvements over baselines with 47.47% MRR@10 improvement on INDIC-MARCO Dev-Set

## Executive Summary
This paper introduces IndicIRSuite, a comprehensive set of neural information retrieval resources for 11 widely spoken Indian languages. The suite includes INDIC-MARCO, a large-scale multilingual dataset created by translating the MSMARCO dataset into Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu using machine translation. Additionally, it provides Indic-ColBERT, a collection of 11 distinct monolingual neural IR models trained on INDIC-MARCO. The Indic-ColBERT models demonstrate significant performance improvements over baseline models, achieving 47.47% improvement in MRR@10 score on INDIC-MARCO Dev-Set (excluding Oriya), 12.26% improvement in NDCG@10 score on MIRACL Bengali and Hindi Language baselines, and 20% improvement in MRR@100 score on Mr.Tydi Bengali Language baseline.

## Method Summary
The paper creates INDIC-MARCO by machine translating 39 million English MSMARCO triplets into 11 Indian languages using an int-8 quantized NLLB-1.3B-Distilled Model with beam width of 4 and batch size of 64. Indic-ColBERT models are trained separately for each language on INDIC-MARCO using mBERT encoders for 50k iterations with batch size 128, employing pairwise softmax cross-entropy loss. The approach builds on ColBERTv2 architecture with language-specific training to capture semantic representations and retrieval patterns for each Indian language.

## Key Results
- Indic-ColBERT achieves 47.47% improvement in MRR@10 score averaged over INDIC-MARCO baselines for 11 Indian languages (excluding Oriya)
- 12.26% improvement in NDCG@10 score on MIRACL Bengali and Hindi Language baselines
- 20% improvement in MRR@100 score on Mr.Tydi Bengali Language baseline
- First large-scale effort to build neural IR resources for a large number of Indian languages

## Why This Works (Mechanism)

### Mechanism 1
Machine translation of MSMARCO dataset into 11 Indian languages provides sufficient scale for neural IR model training. The translation process converts 39 million English training triplets into equivalent multilingual data, maintaining query-passage relevance relationships while expanding language coverage. Core assumption: Machine-translated data preserves enough semantic and lexical relationships for effective neural IR model training. Evidence anchors: The translation process employs int-8 quantized NLLB-1.3B-Distilled Model with beam width of 4 and batch size of 64, indicating careful parameter selection for translation quality. The size of datasets holds greater importance than ensuring domain matching in the training of neural IR models [21]. Found 25 related papers with average neighbor FMR=0.403, suggesting moderate corpus relevance for this work. Break condition: If translation quality degrades significantly or if domain-specific terminology is mistranslated, model performance would deteriorate.

### Mechanism 2
Monolingual Indic-ColBERT models outperform multilingual baselines by leveraging language-specific training. Training separate ColBERTv2 models on language-specific INDIC-MARCO data allows each model to learn language-specific semantic representations and retrieval patterns. Core assumption: Language-specific training provides better retrieval performance than multilingual models for monolingual retrieval tasks. Evidence anchors: Indic-ColBERT achieves 47.47% improvement in the MRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian languages except Oriya. We train 11 distinct Indic-ColBERT (iCol) models separately for 50k iterations with a batch size of 128 on the first 6.4 million training triplets. No direct evidence in corpus neighbors, but related work on monolingual IR exists in the field. Break condition: If languages are too similar or if data quality is insufficient, the monolingual approach may not provide significant advantages.

### Mechanism 3
IndicIRSuite provides a foundation for low-resource Indian language IR research by combining large-scale dataset creation with model training. The suite creates both the training data and the models, establishing benchmarks and providing resources that other researchers can build upon. Core assumption: Having both dataset and models together accelerates research adoption and comparison. Evidence anchors: IndicIRSuite is the first attempt at building large-scale Neural Information Retrieval resources for a large number of Indian languages. We introduce neural IR resources to address this scarcity and facilitate Monolingual neural IR across 11 Indian languages. Related work on multilingual IR benchmarks exists, but none specifically for this scale across 11 Indian languages. Break condition: If the community doesn't adopt the resources or if better approaches emerge quickly, the impact would be limited.

## Foundational Learning

- **Concept**: Machine translation parameters and their impact on IR data quality
  - **Why needed here**: Understanding beam width, batch size, and quantization choices is crucial for evaluating translation quality and dataset reliability
  - **Quick check question**: What translation parameters were used and why might they affect downstream IR model performance?

- **Concept**: Neural IR model architecture (ColBERTv2) and its adaptation for multilingual contexts
  - **Why needed here**: The paper builds on ColBERTv2 architecture with specific modifications for Indian languages
  - **Quick check question**: How does Indic-ColBERT differ from standard ColBERTv2 in terms of query/document encoding?

- **Concept**: Evaluation metrics for IR systems (MRR@10, NDCG@10, Recall@100)
  - **Why needed here**: The paper reports performance using multiple metrics that measure different aspects of retrieval quality
  - **Quick check question**: What do MRR@10 and NDCG@10 measure, and why are both reported?

## Architecture Onboarding

- **Component map**: MSMARCO → Machine Translation → INDIC-MARCO dataset → Indic-ColBERT training → Evaluation
- **Critical path**: Machine translation quality → Dataset quality → Model training → Evaluation performance
- **Design tradeoffs**: Monolingual models vs multilingual models (performance vs scalability), machine translation vs human translation (speed vs quality)
- **Failure signatures**: Poor translation quality showing up as degraded model performance, language-specific failures (like Oriya), baseline performance not improving over zero-shot approaches
- **First 3 experiments**:
  1. Validate translation quality by sampling and human evaluation of translated queries/passages
  2. Test baseline BM25 performance on translated data to establish lower bounds
  3. Evaluate model performance on a held-out validation set during training to monitor convergence

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: What is the impact of training neural IR models on domain-specific datasets in Indian languages compared to using the general-domain INDIC-MARCO dataset?
**Basis in paper**: The paper mentions that models trained on domain-specific datasets cannot generalize well to other domains, but it doesn't directly compare performance on domain-specific Indian language datasets versus INDIC-MARCO.
**Why unresolved**: The paper focuses on creating a large-scale general-domain dataset and does not explore the trade-offs between domain-specific and general-domain training for Indian languages.
**What evidence would resolve it**: Experiments comparing the performance of neural IR models trained on domain-specific Indian language datasets versus INDIC-MARCO on various domain-specific test sets.

### Open Question 2
**Question**: How does the performance of Indic-ColBERT models vary across different Indian language families (Indo-Aryan vs. Dravidian)?
**Basis in paper**: The paper introduces neural IR resources for 11 Indian languages from two major families but does not analyze performance differences between these language families.
**Why unresolved**: While the paper provides aggregate results across all languages, it doesn't explore potential variations in model performance based on language family characteristics.
**What evidence would resolve it**: Detailed performance analysis of Indic-ColBERT models across Indo-Aryan and Dravidian language families, including comparisons of model effectiveness and potential reasons for differences.

### Open Question 3
**Question**: What is the optimal approach for handling low-resource Indian languages like Oriya and Assamese that are not well-supported by multilingual models like mBERT?
**Basis in paper**: The paper mentions that Oriya shows no improvement because mBERT is not pre-trained on it, and Assamese shows significant improvement due to its similarity to Bengali.
**Why unresolved**: The paper demonstrates the problem but doesn't provide a comprehensive solution or strategy for handling low-resource languages in neural IR.
**What evidence would resolve it**: Comparative studies of different approaches (e.g., cross-lingual transfer, synthetic data generation, language-specific pretraining) for improving neural IR performance on low-resource Indian languages.

## Limitations
- Machine translation approach may introduce quality degradation for low-resource languages like Oriya and Assamese that lack mBERT pre-training
- Evaluation relies heavily on automated metrics without extensive human validation of retrieval relevance
- Performance degradation explicitly noted for Oriya and potential issues with Assamese due to language support limitations

## Confidence
- **High confidence**: In the dataset creation methodology and its potential to accelerate research in Indian language IR
- **Medium confidence**: In the reported performance improvements, as results depend on machine-translated data quality
- **Low confidence**: In generalization across all 11 languages, particularly for Oriya and Assamese where performance degradation is explicitly noted

## Next Checks
1. Conduct human evaluation of translation quality across all 11 languages to quantify semantic preservation and identify systematic translation errors
2. Perform ablation studies comparing machine-translated vs human-translated subsets on model performance to measure translation quality impact
3. Test Indic-ColBERT models on out-of-domain Indian language queries to assess robustness beyond the INDIC-MARCO dataset distribution