---
ver: rpa2
title: 'Providing personalized Explanations: a Conversational Approach'
arxiv_id: '2307.11452'
source_url: https://arxiv.org/abs/2307.11452
tags:
- agent
- explanation
- justi
- explanations
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a conversational approach for providing personalized
  explanations to AI explainees with different backgrounds. The core idea is to model
  the explainer's beliefs about the explainee's background and iteratively construct
  explanations based on the explainee's feedback.
---

# Providing personalized Explanations: a Conversational Approach

## Quick Facts
- arXiv ID: 2307.11452
- Source URL: https://arxiv.org/abs/2307.11452
- Reference count: 16
- Primary result: Conversation terminates due to explainee's justification of initial claim when explainer is aware of an explanation the explainee understands

## Executive Summary
This paper presents a formal conversational approach for providing personalized explanations to AI explainees with different backgrounds. The system models the explainer's beliefs about the explainee's background and iteratively constructs explanations based on the explainee's feedback. By updating beliefs about what the explainee can understand and justify, the explainer selects the most preferred explanation accordingly, ensuring the conversation terminates successfully when an appropriate explanation exists.

## Method Summary
The approach uses a multi-agent modular model where agents have different justification capabilities and knowledge states. The explainer generates available explanations, selects the most preferred one based on minimizing unjustified components and explanation length, and sends it to the explainee. The explainee provides binary feedback indicating which hypotheses and deductions they can justify. The explainer updates their beliefs about the explainee's capabilities based on this feedback and repeats the process until the explainee can justify the initial claim.

## Key Results
- Conversation terminates due to explainee's justification of initial claim when there exists an explanation the explainee understands and explainer is aware of
- Explainer's belief updates based on binary feedback enable more targeted subsequent explanations
- Preference ordering prioritizes explanations with fewer unjustified components and shorter deduction chains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The explainer iteratively updates their beliefs about the explainee's understanding and justification capabilities based on conversational feedback.
- Mechanism: After each explanation, the explainer receives binary feedback indicating which hypotheses and deductions the explainee can justify. This feedback allows the explainer to refine their mental model of the explainee's knowledge state, enabling more targeted subsequent explanations.
- Core assumption: The explainee's feedback is truthful and accurate regarding their justification capabilities.
- Evidence anchors:
  - [abstract] "The explainer updates their beliefs about what the explainee can understand and justifies, and selects the most preferred explanation accordingly."
  - [section] "Once an agent hears an explanation, he can update his justification with derived terms that he constructs, which means that the agent learns from the explanation and has more justified beliefs."
- Break condition: If the feedback is untruthful or inaccurate, the explainer's belief updates become incorrect, leading to increasingly inappropriate explanations.

### Mechanism 2
- Claim: The conversation terminates when the explainee successfully justifies the initial claim through understanding the complete explanation chain.
- Mechanism: The explainer constructs explanations recursively, addressing only what the explainee cannot justify. Each feedback round eliminates unjustified components until the explainee can justify the entire explanation tree, including the original claim.
- Core assumption: There exists at least one explanation for the initial claim that the explainee can understand, and the explainer is aware of this explanation.
- Evidence anchors:
  - [abstract] "We prove that the conversation terminates due to the explainee's justification of the initial claim as long as there exists an explanation for the initial claim that the explainee understands and the explainer is aware of."
  - [section] "Proposition 5.3. Given a multi-agent modular model M and a world w, if there exists an explanation E such that claim(E) = F, agent 2 understands E and agent 1 is aware of E in world w, then there exists a conversation history... such that after η agent 2 can justify F."
- Break condition: If no such explanation exists, the conversation cannot terminate successfully.

### Mechanism 3
- Claim: The explainer prioritizes explanations that minimize the number of unjustified hypotheses and deductions while keeping explanations concise.
- Mechanism: The explainer uses a preference ordering that first compares the number of unjustified components (hypotheses and deductions) and then the number of deduction steps. This ensures explanations are both understandable and efficient.
- Core assumption: The explainer can accurately assess which components the explainee might not justify based on their current beliefs.
- Evidence anchors:
  - [section] "Definition 4.2 (Preferences over Available Explanations)... Given a multi-agent modular model M and a world w, agent 1 provides explanations to agent 2, for any two explanations E, E', E ≾M,w E' iff... /divides.alt0 N M,w( E)/divides.alt0 > /divides.alt0 N M,w( E ′)/divides.alt0 ; or /divides.alt0 N M,w( E)/divides.alt0 = /divides.alt0 N M,w( E ′)/divides.alt0 and D( E) ≥D( E ′)."
- Break condition: If the preference ordering is mis-specified or the explainer's belief about the explainee is incorrect, suboptimal explanations may be selected.

## Foundational Learning

- Concept: Justification Logic
  - Why needed here: The system relies on formal representations of justifications to model what agents can prove and understand. This logic provides the foundation for representing explanations as trees of justifications.
  - Quick check question: What is the difference between a justification term and a justification variable in this framework?

- Concept: Modal Logic with Multiple Agents
  - Why needed here: The system models multiple agents (explainer and explainee) with different knowledge states and justification capabilities. Modal operators represent beliefs and justifications.
  - Quick check question: How does the ◻i operator differ from the /llbrackett/rrbracketi operator in this system?

- Concept: Conversation History and State Updates
  - Why needed here: The system maintains conversation histories to track the explanation process and update agent beliefs based on feedback. This enables the iterative refinement of explanations.
  - Quick check question: What information is captured in the conversation history, and how is it used to select the next explanation?

## Architecture Onboarding

- Component map: Multi-agent modular model -> Explanation generator -> Feedback processor -> Preference engine -> Conversation manager
- Critical path:
  1. Initial question received from explainee
  2. Explainer generates available explanations
  3. Preference engine selects most suitable explanation
  4. Explanation sent to explainee
  5. Feedback received and processed
  6. Explainer beliefs updated
  7. Repeat from step 2 until termination condition met
- Design tradeoffs:
  - Precision vs. simplicity: More detailed justification logic provides better modeling but increases complexity
  - Feedback granularity: Binary feedback is simple but may miss nuanced understanding issues
  - Computation vs. explanation quality: More sophisticated preference algorithms improve explanations but require more processing
- Failure signatures:
  - Non-terminating conversations: Indicates either no suitable explanation exists or feedback is incorrect
  - Repeated explanations: Suggests belief updates are not working correctly
  - Complex explanations chosen: May indicate preference engine misconfiguration
- First 3 experiments:
  1. Test with simple explanation chains where explainee can justify all components
  2. Test with explainee unable to justify certain hypotheses to verify belief updates
  3. Test with multiple available explanations to verify preference engine selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the explainer agent's preferences over explanations be formalized using mathematical principles to ensure optimal selection of explanations?
- Basis in paper: [explicit] The paper discusses the explainer's preferences over available explanations, stating that the explainer should select the explanation that is most likely to be understood by the explainee. However, it does not provide a formal mathematical framework for this selection process.
- Why unresolved: The paper does not provide a formal mathematical framework for the explainer's preferences over explanations. It only mentions that the explainer should select the explanation that is most likely to be understood by the explainee.
- What evidence would resolve it: A formal mathematical framework for the explainer's preferences over explanations, including a clear definition of the preference relation and a proof of its properties.

### Open Question 2
- Question: How can the explainee's acceptance of explanations be incorporated into the model to ensure that the explanations are not only understood but also accepted by the explainee?
- Basis in paper: [explicit] The paper mentions that the explanation selected by the explainer should be not only understood but also accepted by the explainee. However, it does not provide a formal model for the explainee's acceptance of explanations.
- Why unresolved: The paper does not provide a formal model for the explainee's acceptance of explanations. It only mentions that the explanation should be accepted by the explainee.
- What evidence would resolve it: A formal model for the explainee's acceptance of explanations, including a clear definition of the acceptance relation and a proof of its properties.

### Open Question 3
- Question: How can the model be extended to handle cases where the explainee's feedback is not truthful, and what are the implications for the explainer's belief updates?
- Basis in paper: [explicit] The paper assumes that the explainee's feedback is truthful, but it does not provide a formal model for handling cases where the feedback is not truthful.
- Why unresolved: The paper does not provide a formal model for handling cases where the explainee's feedback is not truthful. It only assumes that the feedback is truthful.
- What evidence would resolve it: A formal model for handling cases where the explainee's feedback is not truthful, including a clear definition of the feedback relation and a proof of its properties.

## Limitations

- The binary feedback mechanism may not capture nuanced understanding gaps between explainer and explainee
- The system assumes truthful feedback from explainees, which may not hold in real-world scenarios
- The scalability of this approach to complex AI explanations remains unclear due to potential computational overhead

## Confidence

- High confidence: The core termination theorem and the mechanism of belief updates based on feedback are well-defined and supported by formal proofs in the paper
- Medium confidence: The preference ordering mechanism for selecting explanations appears sound but depends heavily on accurate modeling of the explainee's justification capabilities
- Low confidence: The practical implementation details and computational efficiency of the approach are not thoroughly discussed, making it difficult to assess real-world applicability

## Next Checks

1. **Scalability test**: Implement the system with progressively larger explanation trees to measure computational overhead and identify performance bottlenecks
2. **Feedback accuracy validation**: Conduct user studies comparing the binary feedback mechanism against more granular feedback methods to assess information loss
3. **Robustness test**: Evaluate system performance when explainees provide inconsistent or untruthful feedback to determine resilience to real-world communication challenges