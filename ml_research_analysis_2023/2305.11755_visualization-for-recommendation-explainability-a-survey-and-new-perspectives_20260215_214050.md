---
ver: rpa2
title: 'Visualization for Recommendation Explainability: A Survey and New Perspectives'
arxiv_id: '2305.11755'
source_url: https://arxiv.org/abs/2305.11755
tags:
- explanation
- recommendation
- recommender
- visual
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically reviews the use of visual explanations
  in recommender systems, focusing on four dimensions: explanation goals, scope, style,
  and format. The review identifies that while many tools provide justifications and
  transparency, goals like scrutability, persuasiveness, and efficiency are underexplored.'
---

# Visualization for Recommendation Explainability: A Survey and New Perspectives

## Quick Facts
- arXiv ID: 2305.11755
- Source URL: https://arxiv.org/abs/2305.11755
- Authors: 
- Reference count: 40
- Key outcome: This survey systematically reviews the use of visual explanations in recommender systems, focusing on four dimensions: explanation goals, scope, style, and format. The review identifies that while many tools provide justifications and transparency, goals like scrutability, persuasiveness, and efficiency are underexplored. Most visual explanations focus on justifying outputs rather than explaining processes or inputs. Node-link diagrams, bar charts, and tag clouds are the most common visualization idioms, with design choices strongly linked to data type and task. The study highlights the need for theory-informed, user-centric design and suggests future work to explore the effectiveness of visual explanations for different goals, styles, and user characteristics.

## Executive Summary
This survey provides a comprehensive analysis of visual explanations in recommender systems, examining 33 tools through a four-dimensional framework covering explanation goals, scope, style, and format. The authors find that transparency and justification are the most common explanation goals, while more interactive goals like scrutability remain underexplored. Visualizations predominantly focus on output justification rather than process or input explanation, with node-link diagrams, bar charts, and tag clouds being the most prevalent visualization idioms. The study emphasizes the need for more theory-informed, user-centric design approaches and highlights the importance of considering user characteristics when designing visual explanations.

## Method Summary
The authors conducted a systematic literature review following Kitchenham and Charters guidelines, starting with 440 papers and narrowing to 33 visual explanation tools through a three-stage filtering process. They searched four major databases using specific queries combining visualization, explanation, and recommender system terms. The selected tools were classified across four dimensions: explanation goals (transparency, justification, scrutability, trust, effectiveness, persuasiveness, efficiency, satisfaction), scope (input, process, output), style (content-based, collaborative-based, social, hybrid), and format (charts, graphs, tag clouds). The review also analyzed user studies evaluating the effects of visual explanations on different explanation goals.

## Key Results
- Transparency and justification are the most common explanation goals, while scrutability, persuasiveness, and efficiency remain underexplored
- Visual explanations predominantly focus on justifying outputs rather than explaining processes or inputs
- Node-link diagrams, bar charts, and tag clouds are the most common visualization idioms, with design choices strongly linked to data type and task
- The study identifies a need for more theory-informed, user-centric design approaches and empirical evaluation of visual explanation effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Munzner's what-why-how visualization framework is the core theoretical backbone for classifying and evaluating explanatory visualizations in recommender systems.
- Mechanism: The framework maps visualization design to data types (what), user tasks (why), and encoding/interaction idioms (how), creating a structured way to analyze tools across four dimensions (explanation goal, scope, style, format).
- Core assumption: The underlying visualization design in the surveyed tools aligns with Munzner's structured framework and can be mapped to the framework's vocabulary (data abstraction, task abstraction, idioms).
- Evidence anchors:
  - [abstract] The paper explicitly states it uses Munzner's framework as "theoretical background to systematically discuss the visual explanation format approaches used in the reviewed literature."
  - [section 4] The authors map surveyed tools' data, tasks, and idioms into the what-why-how framework (e.g., tables/networks/sets for data; {identify, paths}, {compare, similarity} for tasks; node-link diagrams, bar charts, tag clouds for idioms).
- Break condition: If surveyed tools' visualizations cannot be mapped cleanly to the framework's vocabulary, or if the framework does not capture key design choices, the classification will break down.

### Mechanism 2
- Claim: The four classification dimensions (explanation goal, scope, style, format) allow for a comprehensive taxonomy of visually explainable recommender systems.
- Mechanism: Each dimension addresses a distinct aspect of explainability: goals define what the system aims to achieve, scope defines what part of the system is explained, style defines the underlying recommendation algorithm, and format defines how explanations are displayed.
- Core assumption: These four dimensions are sufficient and mutually exclusive to capture the space of visually explainable recommender systems without overlap or missing categories.
- Evidence anchors:
  - [abstract] The authors state they classify based on "four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format."
  - [section 3] Each dimension is clearly defined and distinct (e.g., explanation goal covers transparency/trust/persuasiveness, scope covers input/process/output, style covers content-based/collaborative/social/hybrid, format covers visual/textual).
- Break condition: If tools cannot be categorized within these dimensions, or if significant dimensions are missing (e.g., user characteristics, evaluation methods), the taxonomy will be incomplete.

### Mechanism 3
- Claim: User studies evaluating the effects of visual explanations on different explanation goals reveal both complementary and contradictory relationships between goals.
- Mechanism: By analyzing user studies reported in the surveyed tools, the paper identifies how different visual explanations affect different explanation goals, providing empirical evidence for the effectiveness of visual explanations.
- Core assumption: The user studies reported in the surveyed tools are methodologically sound and provide reliable evidence about the effects of visual explanations on explanation goals.
- Evidence anchors:
  - [abstract] The authors mention "user studies evaluating the effects of visual explanations on different goals, with contradicting results."
  - [section 6.1] The paper reports that some studies found positive effects on transparency/trust/effectiveness, while others found negative effects on persuasiveness/effectiveness, and that different visual explanations result in different levels of perception for different goals.
- Break condition: If user studies are methodologically flawed, have small sample sizes, or are not representative of real-world usage, the evidence will be weak and the conclusions about goal relationships will be unreliable.

## Foundational Learning

- Concept: Explanation goals in recommender systems
  - Why needed here: Understanding the different goals (transparency, trust, effectiveness, etc.) is crucial for designing visual explanations that meet user needs and for evaluating their effectiveness.
  - Quick check question: What are the key differences between transparency and justification as explanation goals, and when might each be more appropriate?

- Concept: Munzner's what-why-how visualization framework
  - Why needed here: This framework provides the theoretical foundation for analyzing and classifying visual explanations in recommender systems based on data types, user tasks, and visualization idioms.
  - Quick check question: How would you classify a tool that uses a node-link diagram to show connections between a user profile and recommended items using the what-why-how framework?

- Concept: Interactive visualization techniques
  - Why needed here: Understanding different interaction types (select, filter, navigate, etc.) is essential for designing visual explanations that allow users to explore and understand the recommendation process.
  - Quick check question: What are the three main categories of interaction types in Munzner's framework, and how do they differ in terms of user control over the visualization?

## Architecture Onboarding

- Component map:
  - Data layer: User profiles, item attributes, social networks, etc.
  - Recommendation engine: Content-based, collaborative, hybrid, etc.
  - Explanation generator: Maps recommendations to visual explanations
  - Visualization renderer: Renders explanations using Munzner's framework
  - Interaction handler: Processes user interactions with visualizations

- Critical path: User request → Recommendation generation → Explanation generation → Visualization rendering → User interaction → Feedback loop

- Design tradeoffs:
  - Simplicity vs. completeness: Balancing the amount of information in visual explanations to avoid overwhelming users while providing enough detail for understanding
  - Static vs. interactive: Choosing between static visualizations and interactive ones based on user needs and system complexity
  - Generic vs. personalized: Designing visualizations that work for all users vs. adapting to individual user characteristics

- Failure signatures:
  - Users cannot understand the visualizations (too complex, unclear mappings)
  - Visualizations do not match user needs (wrong goals, wrong scope)
  - System performance degrades due to visualization rendering
  - User interactions do not provide expected feedback or control

- First 3 experiments:
  1. Implement a simple node-link diagram showing connections between a user profile and recommended items, using Munzner's framework for data and task mapping
  2. Add basic interaction (select, filter) to the node-link diagram and evaluate user understanding and satisfaction
  3. Compare the effectiveness of the node-link diagram vs. a bar chart for explaining item similarity based on user preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the right amount of information and interaction in a visual explanation which may foster a better understanding, satisfaction, acceptance, and trust in visually explainable recommender systems?
- Basis in paper: [explicit] The paper discusses the challenge of balancing information and interaction in visual explanations, noting that different users demand different levels of information and interaction. It emphasizes the need for empirical studies to investigate the right amount of information and interaction that fosters better understanding, satisfaction, acceptance, and trust.
- Why unresolved: The paper highlights the lack of systematic evaluations that explore the effects of different levels of information and interaction on users' perception of visually explainable recommender systems. It calls for studies to address this gap.
- What evidence would resolve it: Empirical studies that systematically evaluate the effects of varying levels of information and interaction in visual explanations on user outcomes such as understanding, satisfaction, acceptance, and trust.

### Open Question 2
- Question: How can visualization be used to open, scrutinize, and explain user models in order to support an effective and efficient personalization and correctability of the recommender system?
- Basis in paper: [explicit] The paper identifies the need for more work that leverages interactive visualizations to help users scrutinize their models and see the effect on the recommendation output. It suggests that transparent, scrutable, and explainable user models would help users effectively and efficiently personalize and correct the recommendation.
- Why unresolved: The paper notes that while some tools allow users to scrutinize and modify their models, none of them provide an explanation of how a user model was generated. It calls for systematic analysis of the effects of opening, scrutinizing, and explaining user models using visualization.
- What evidence would resolve it: Research that develops and evaluates interactive visualizations for opening, scrutinizing, and explaining user models, along with studies that assess the impact on personalization and correctability of the recommender system.

### Open Question 3
- Question: What are the effects of personal characteristics (e.g., visualization literacy, visual working memory, domain expertise, prior knowledge) on the perception of and interaction with visually explainable recommender systems?
- Basis in paper: [explicit] The paper acknowledges that the right amount of information and interaction in a visual explanation should be adapted for different groups of end-users based on their personal characteristics. It highlights the lack of research on the influencing criteria and their effects on users' perception of visually explainable recommender systems.
- Why unresolved: The paper points out that while some studies have addressed the influence of personal characteristics, they are not sufficient to understand the full impact. It calls for more profound research on the influencing criteria and their effects.
- What evidence would resolve it: Studies that investigate the effects of various personal characteristics on the perception and interaction with visually explainable recommender systems, providing insights into how to design effective visual explanations that take personal characteristics into account.

## Limitations

- The review's classification framework relies heavily on Munzner's visualization framework, which may not fully capture all aspects of explainability in recommender systems
- The analysis of user studies is limited by the quality and availability of reported empirical evidence across the surveyed tools
- The review focuses on English-language literature, potentially missing relevant work from other linguistic communities

## Confidence

- Classification framework application: High
- Data extraction methodology: High
- Analysis of explanation goal relationships: Medium (due to contradictory findings)
- Coverage of visual explanation formats: High
- Assessment of user study quality: Low (limited by reporting quality in source papers)

## Next Checks

1. Map additional tools from recent conferences (RecSys, IUI, CHI) against the four-dimensional framework to test classification boundaries
2. Conduct a small user study comparing different visual explanation styles for the same recommendation task to validate the goal-scope relationships identified in the survey
3. Perform a citation network analysis to identify the most influential papers and tools in visually explainable recommender systems that may have been missed by the systematic review methodology