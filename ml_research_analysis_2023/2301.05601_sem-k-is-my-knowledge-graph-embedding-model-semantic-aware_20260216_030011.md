---
ver: rpa2
title: 'Sem@$K$: Is my knowledge graph embedding model semantic-aware?'
arxiv_id: '2301.05601'
source_url: https://arxiv.org/abs/2301.05601
tags:
- semantic
- entities
- kgem
- knowledge
- kgems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sem@K, a semantic-oriented metric to evaluate
  knowledge graph embedding models (KGEMs) for link prediction. Unlike traditional
  rank-based metrics, Sem@K measures the capability of models to predict valid entities
  with respect to domain and range constraints.
---

# Sem@$K$: Is my knowledge graph embedding model semantic-aware?

## Quick Facts
- arXiv ID: 2301.05601
- Source URL: https://arxiv.org/abs/2301.05601
- Reference count: 34
- One-line primary result: Introduces Sem@K, a semantic-oriented metric to evaluate KGEMs for link prediction based on domain and range constraints.

## Executive Summary
This paper introduces Sem@K, a novel semantic-oriented metric for evaluating knowledge graph embedding models (KGEMs) in link prediction tasks. Unlike traditional rank-based metrics, Sem@K measures a model's ability to predict semantically valid entities according to domain and range constraints. The authors propose three variants of Sem@K to handle different KG characteristics: schema-defined graphs, schemaless graphs, and graphs with class hierarchies. Through extensive experiments on mainstream KGs, they demonstrate that Sem@K provides a new perspective on KGEM quality, revealing that some models are inherently better at semantic prediction than others, though this doesn't necessarily correlate with rank-based metric performance.

## Method Summary
The authors propose Sem@K, a semantic-oriented metric that evaluates KGEMs based on their ability to predict entities that are valid according to domain and range constraints. For each ground-truth triple, the metric checks whether the top-K predicted entities belong to the expected domain (for head predictions) or range (for tail predictions) of the relation. The metric has three variants: Sem@K[base] for schema-defined KGs, Sem@K[ext] for schemaless KGs where constraints are inferred from graph structure, and Sem@K[wup] that incorporates class hierarchies using Wu-Palmer similarity. The authors train various KGEMs (TransE, TransH, DistMult, ComplEx, SimplE, ConvE, ConvKB, R-GCN, CompGCN) on multiple KGs and evaluate them using both Sem@K and traditional rank-based metrics.

## Key Results
- Sem@K reveals that some KGEMs are inherently better at semantic prediction than others, independent of their rank-based metric performance
- Training longer typically improves rank-based metrics but degrades semantic awareness, suggesting a fundamental trade-off
- KGEMs generally struggle to recover semantic information from certain KGs, with varying difficulty across different datasets
- The proposed metric can evaluate semantic awareness even for schemaless KGs by inferring constraints from graph structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sem@K captures semantic awareness by measuring the proportion of semantically valid predictions in the top-K scored entities, based on domain and range constraints.
- **Mechanism**: For each ground-truth triple, the metric evaluates whether the predicted head (or tail) belongs to the expected domain (or range) of the relation. This is computed using either explicit schema definitions or inferred constraints from the graph structure.
- **Core assumption**: The domain and range constraints accurately reflect the semantic expectations of the relations in the knowledge graph.
- **Evidence anchors**:
  - [abstract] "Sem@K measures the capability of models to predict valid entities w.r.t. domain and range constraints."
  - [section] "compatibility(q,q′) = {1, if types(q′h)∩ domain(qr)̸=∅∧ types(q′t)∩ range(qr)̸=∅; 0, otherwise}"
  - [corpus] Weak evidence - no related papers directly discuss the Sem@K mechanism.
- **Break condition**: If the domain and range constraints are incomplete, incorrect, or missing, the metric may misclassify valid predictions as invalid, or vice versa.

### Mechanism 2
- **Claim**: Sem@K[ext] extends semantic evaluation to schemaless KGs by inferring domain and range constraints from the graph structure itself.
- **Mechanism**: For each relation, the domain is defined as the set of entities observed as heads, and the range as the set of entities observed as tails. This allows evaluation even without explicit schema definitions.
- **Core assumption**: The observed usage of relations in the graph reflects the intended semantic constraints of those relations.
- **Evidence anchors**:
  - [section] "domain(r) ={e :∃(e,r,t )∈T} (resp. range(r) ={e :∃(h,r,e )∈T} )"
  - [abstract] "we extend a previously defined semantic-oriented metric and tailor it to support a broader range of KGs."
  - [corpus] Weak evidence - no related papers discuss schemaless KG evaluation with inferred constraints.
- **Break condition**: If the graph is sparse or biased, the inferred constraints may not represent the true semantic expectations, leading to inaccurate evaluations.

### Mechanism 3
- **Claim**: Sem@K[wup] incorporates class hierarchy to provide a finer-grained semantic evaluation by considering the semantic similarity between entity types and expected domains/ranges.
- **Mechanism**: The compatibility function uses the Wu-Palmer similarity score to measure the semantic relatedness between the classes of predicted entities and the expected domain/range classes. This allows partial credit for semantically close predictions.
- **Core assumption**: The class hierarchy accurately represents the semantic relationships between entity types, and the Wu-Palmer similarity score effectively captures this semantic relatedness.
- **Evidence anchors**:
  - [section] "compatibility(q,q′) = min( maxc∈type(q′h)c′∈domain(qr)σ(c,c′), maxc∈type(q′t)c′∈range(qr)σ(c,c′))"
  - [abstract] "we integrate class hierarchy into Sem@K by means of a similarity measure between concepts."
  - [corpus] Weak evidence - no related papers discuss hierarchical semantic evaluation in KGEMs.
- **Break condition**: If the class hierarchy is incomplete, inconsistent, or the similarity measure does not accurately reflect semantic relationships, the metric may give misleading evaluations.

## Foundational Learning

- **Concept**: Knowledge Graph Embeddings (KGEs) and Link Prediction (LP)
  - Why needed here: The paper evaluates the semantic awareness of KGE models for the LP task, so understanding these concepts is crucial for interpreting the results and the proposed metric.
  - Quick check question: What is the difference between a knowledge graph and a knowledge graph embedding?

- **Concept**: Domain and Range Constraints in KGs
  - Why needed here: Sem@K relies on these constraints to evaluate the semantic validity of predictions. Understanding how they are defined and used is essential for grasping the metric's mechanism.
  - Quick check question: How are domain and range constraints typically represented in a knowledge graph schema?

- **Concept**: Class Hierarchies and Semantic Similarity
  - Why needed here: Sem@K[wup] incorporates class hierarchies and semantic similarity measures to provide a more nuanced evaluation. Familiarity with these concepts is necessary to understand this variant of the metric.
  - Quick check question: What is the Wu-Palmer similarity score, and how is it used to measure semantic similarity between classes?

## Architecture Onboarding

- **Component map**: KG -> KGE Models -> Sem@K Metric -> Domain/Range Constraints (and Class Hierarchy for Sem@K[wup])

- **Critical path**:
  1. Load the KG and KGE models.
  2. For each test triple, perform head and tail predictions using the KGE models.
  3. Compute the Sem@K score for each model by evaluating the semantic validity of the top-K predictions.
  4. Compare the Sem@K scores across models and variants to assess semantic awareness.

- **Design tradeoffs**:
  - Sem@K[base] vs. Sem@K[ext]: The base variant requires explicit schema definitions, while the ext variant can work with schemaless KGs but may be less accurate if the inferred constraints are not representative.
  - Sem@K[wup] vs. Sem@K[base/ext]: The wup variant provides a more nuanced evaluation by considering class hierarchies and semantic similarity, but it requires a well-defined hierarchy and an appropriate similarity measure.

- **Failure signatures**:
  - Low Sem@K scores across all models may indicate that the KG is inherently difficult for semantic evaluation (e.g., due to missing or incorrect constraints).
  - High Sem@K scores but low rank-based metrics may suggest that the models are good at semantic prediction but struggle with the specific test triples.
  - Inconsistent Sem@K scores across variants may indicate issues with the underlying schema or hierarchy.

- **First 3 experiments**:
  1. Evaluate a simple KGE model (e.g., TransE) on a schema-defined KG using Sem@K[base] to verify basic functionality.
  2. Evaluate the same model on a schemaless KG using Sem@K[ext] to test the inference of constraints from the graph structure.
  3. Evaluate a model with access to class hierarchies (e.g., R-GCN) on a KG with a well-defined hierarchy using Sem@K[wup] to assess the impact of semantic similarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific KG characteristics (e.g., number of relations, average instances per class) most strongly influence KGEMs' ability to recover semantic information?
- Basis in paper: [inferred] The paper notes that some KGs are more challenging for KGEMs to recover semantics from and suggests this deserves further study, but doesn't identify specific characteristics.
- Why unresolved: The authors acknowledge this as an important question but don't conduct the extensive study needed to determine which KG characteristics have the most impact on semantic awareness.
- What evidence would resolve it: A controlled study benchmarking KGEMs across a broad set of KGs with varying characteristics, systematically measuring the correlation between specific KG properties and KGEM semantic awareness performance.

### Open Question 2
- Question: Can we develop a unified evaluation framework that combines rank-based metrics, semantic-oriented metrics like Sem@K, computational efficiency, and environmental impact into a single comprehensive assessment?
- Basis in paper: [explicit] The paper's conclusion explicitly calls for experiments with additional evaluation components and metrics that combine different aspects of KGEM quality.
- Why unresolved: The paper only introduces semantic-oriented metrics as an additional dimension, but doesn't attempt to create a unified framework that balances multiple evaluation criteria.
- What evidence would resolve it: A proposed framework that mathematically combines different evaluation metrics (rank-based, semantic, efficiency, environmental) with appropriate weighting, validated through extensive experiments across multiple KGEMs and datasets.

### Open Question 3
- Question: How can we develop training strategies that optimize for both rank-based performance and semantic awareness, rather than finding a trade-off between them?
- Basis in paper: [inferred] The paper observes that training longer typically improves rank-based metrics but degrades semantic awareness, suggesting this trade-off could potentially be addressed through better training strategies.
- Why unresolved: The paper identifies the trade-off problem but doesn't explore training approaches that might optimize for both objectives simultaneously.
- What evidence would resolve it: Experiments with alternative training objectives (e.g., multi-task learning, regularization techniques) that show improved or comparable performance on both rank-based and semantic metrics compared to standard training.

## Limitations
- The metric's effectiveness depends on the availability and quality of domain and range constraints, which may be incomplete or incorrect in many KGs
- The paper doesn't provide a thorough analysis of the trade-offs between semantic awareness and rank-based metrics
- The evaluation is limited to link prediction tasks, with unclear applicability to other KG-related tasks

## Confidence
Medium. The paper presents a novel metric with clear methodology and experimental validation, but relies on the availability of accurate domain and range constraints, and doesn't fully explore the relationship between semantic and rank-based metrics.

## Next Checks
1. Evaluate the impact of incomplete or incorrect domain and range constraints on the Sem@K metric by artificially introducing errors in the constraints and measuring the effect on the metric scores.
2. Conduct a correlation analysis between Sem@K scores and rank-based metrics to understand the relationship between semantic awareness and other aspects of KGEM performance.
3. Investigate the applicability of Sem@K to other KG-related tasks, such as entity classification or relation extraction, to assess its broader utility in evaluating KGEMs.