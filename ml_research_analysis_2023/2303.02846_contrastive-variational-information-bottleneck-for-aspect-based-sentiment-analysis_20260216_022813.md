---
ver: rpa2
title: Contrastive variational information bottleneck for aspect-based sentiment analysis
arxiv_id: '2303.02846'
source_url: https://arxiv.org/abs/2303.02846
tags:
- sentiment
- network
- cvib
- https
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a Contrastive Variational Information Bottleneck
  (CVIB) framework to reduce spurious correlations in aspect-based sentiment analysis.
  The key idea is to learn an original network and a self-pruned network simultaneously
  via contrastive learning.
---

# Contrastive variational information bottleneck for aspect-based sentiment analysis

## Quick Facts
- arXiv ID: 2303.02846
- Source URL: https://arxiv.org/abs/2303.02846
- Reference count: 34
- Key outcome: CVIB outperforms strong baselines in ABSA across five datasets, showing improved performance, robustness, and generalization

## Executive Summary
This paper introduces Contrastive Variational Information Bottleneck (CVIB), a framework that simultaneously learns an original network and a self-pruned network to reduce spurious correlations in aspect-based sentiment analysis. The self-pruned network uses Variational Information Bottleneck (VIB) to compress representations while retaining sentiment-relevant information, and a self-pruning contrastive loss aligns the two networks. Extensive experiments on five benchmark datasets demonstrate CVIB's effectiveness in improving overall prediction performance, robustness to spurious correlations, and generalization to long-tail samples and cross-domain scenarios.

## Method Summary
CVIB employs a dual-network architecture where an original BERT-based network processes input text alongside a self-pruned network with VIB-based masking layers. The R-GAT component captures aspect-oriented syntactic dependencies. Both networks are jointly trained using three losses: cross-entropy for sentiment prediction, VIB loss for representation compression, and a self-pruning contrastive loss that pulls together representations from the original and self-pruned networks while pushing apart different instances. The framework iteratively optimizes both networks until convergence, with hyperparameters including βl=1.0 for VIB, τ=0.05 for contrastive temperature, and γ=0.25 for balancing the contrastive loss.

## Key Results
- CVIB achieves state-of-the-art performance on five ABSA benchmark datasets (REST14, LAP14, REST15, REST16, MAMS) in terms of both Accuracy and macro-averaged F1
- The framework shows improved robustness on ARTS test sets, demonstrating resilience to spurious correlations
- CVIB generalizes well to long-tail samples and cross-domain scenarios, outperforming strong baselines in transfer learning settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The self-pruned network learned via VIB reduces spurious correlations by compressing representations layer-by-layer while preserving sentiment-relevant information.
- **Mechanism**: VIB minimizes mutual information between adjacent hidden layers (I(˜Hl-1, ˜Hl)) to discard irrelevant patterns, and maximizes mutual information between compressed representations and labels (I(˜Hl, y)) to retain task-relevant information.
- **Core assumption**: Spurious correlations manifest as irrelevant mutual information between input features and hidden states that can be compressed without affecting sentiment prediction.
- **Evidence anchors**:
  - [abstract] "The self-pruned network is trained using Variational Information Bottleneck to compress the original network's hidden states while retaining task-relevant information for sentiment prediction."
  - [section] "The goal of VIB is to learn a compressed representation ˜Hl while retaining sufficient information in Hl required for prediction."
  - [corpus] Weak - the related papers don't specifically address VIB for spurious correlation reduction, but one mentions "Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis" which suggests VIB is being explored for ABSA.
- **Break condition**: If spurious correlations are distributed across multiple layers in a way that compression in one layer reintroduces them in another, or if the VIB compression threshold is too aggressive and removes sentiment-relevant features.

### Mechanism 2
- **Claim**: The self-pruning contrastive loss improves class separability by pulling together representations from original and self-pruned networks while pushing apart different instances.
- **Mechanism**: For each anchor, the original and self-pruned representations form a positive pair (pulled together), while representations of different instances form negative pairs (pushed apart) within mini-batches.
- **Core assumption**: The self-pruned network produces semantically similar representations for the same instance while the original network captures complementary information, and this complementarity can be exploited via contrastive learning.
- **Evidence anchors**:
  - [abstract] "A self-pruning contrastive loss is then used to pull together semantically similar pairs and push away dissimilar pairs."
  - [section] "We design a self-pruning contrastive loss to optimize the two networks and improve the separability of all the classes, which narrows the distance between the representations of each anchor produced by the self-pruned and original networks while pushing apart the distance between the representations of different instances within a batch."
  - [corpus] Weak - the related papers don't specifically address contrastive learning for ABSA, though one mentions "Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis."
- **Break condition**: If the mini-batch size is too small to provide meaningful negative pairs, or if the temperature parameter τ is poorly tuned, making the contrastive objective either too strong or too weak.

### Mechanism 3
- **Claim**: The joint training objective balances cross-entropy loss, VIB-based pruning, and contrastive learning to optimize both networks simultaneously.
- **Mechanism**: The original network is trained with L1 = LCE(θ1) + γLSCL(θ1,θ2), while the self-pruned network is trained with L(p)2 = LVIB(θ2) + γLSCL(θ1,θ2), allowing both to benefit from the contrastive signal while pursuing their individual objectives.
- **Core assumption**: The two networks can be optimized simultaneously without one dominating the other, and the shared contrastive loss creates beneficial alignment between them.
- **Evidence anchors**:
  - [abstract] "The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning."
  - [section] "We jointly train the original network Mθ1 and the self-pruned networkM(p)θ2 in an iterative manner until convergence."
  - [corpus] Weak - no related papers directly address joint training of dual networks with contrastive learning for ABSA.
- **Break condition**: If the weighting parameter γ is not properly tuned, one loss component may dominate and prevent the other network from learning effectively.

## Foundational Learning

- **Concept**: Variational Information Bottleneck (VIB) principle
  - Why needed here: VIB provides the theoretical foundation for selectively compressing hidden representations to remove spurious correlations while preserving task-relevant information
  - Quick check question: How does VIB balance compression (minimizing I(˜Hl-1, ˜Hl)) with retention (maximizing I(˜Hl, y))?

- **Concept**: Contrastive learning framework
  - Why needed here: Contrastive learning helps the model learn better representations by pulling together positive pairs and pushing apart negative pairs, improving class separability
  - Quick check question: In the self-pruning contrastive loss, why are the representations from the original and self-pruned networks treated as a positive pair?

- **Concept**: Spurious correlation identification and mitigation
  - Why needed here: Understanding what constitutes spurious correlations in ABSA (e.g., co-occurrence of "never had" with POSITIVE sentiment) is essential for designing effective mitigation strategies
  - Quick check question: How can we empirically determine whether a model is relying on spurious correlations rather than genuine sentiment indicators?

## Architecture Onboarding

- **Component map**: Input → BERT encoding → R-GAT syntactic awareness → Classification (original and self-pruned paths) → Joint training with three losses
- **Critical path**: Input → BERT encoding → R-GAT syntactic awareness → Classification (original and self-pruned paths) → Joint training with three losses
- **Design tradeoffs**: 
  - VIB compression level vs. information retention: Too much compression removes spurious correlations but may also remove sentiment-relevant features
  - Contrastive learning temperature τ: Affects how aggressively positive pairs are pulled together vs. negative pairs pushed apart
  - Mini-batch size: Larger batches provide more negative pairs but increase memory requirements
- **Failure signatures**:
  - Performance degradation on minority classes (NEUTRAL) suggests VIB pruning is too aggressive
  - Minimal improvement over baseline indicates contrastive loss is not effectively aligning the two networks
  - Overfitting on training data despite VIB suggests spurious correlations are not being properly identified
- **First 3 experiments**:
  1. Ablation study: Train with original network only (no VIB, no contrastive loss) to establish baseline performance
  2. Train with VIB-only self-pruned network (no contrastive loss) to isolate the effect of VIB-based pruning
  3. Train with contrastive loss only (no VIB) using two identical networks to verify contrastive learning alone can improve performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CVIB compare to ensemble learning approaches that explicitly model and down-weight bias-only models?
- Basis in paper: [explicit] The paper mentions ensemble learning approaches (e.g., Clark et al. 2020; Karimi Mahabadi et al. 2020) as a method to reduce spurious correlations by building bias-only models and re-weighting strategies, but does not compare CVIB to these methods.
- Why unresolved: The paper focuses on comparing CVIB to attention-based, graph-based, and BERT-based methods, but does not evaluate its performance against ensemble learning approaches that explicitly model and down-weight bias-only models.
- What evidence would resolve it: Conducting experiments comparing CVIB to ensemble learning approaches on the same benchmark datasets would provide evidence of CVIB's relative performance.

### Open Question 2
- Question: Can CVIB be extended to other natural language processing tasks beyond aspect-based sentiment analysis, such as natural language inference or question answering?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of CVIB in reducing spurious correlations and improving robustness and generalization for aspect-based sentiment analysis. However, it does not explore the applicability of CVIB to other NLP tasks.
- Why unresolved: The paper focuses on aspect-based sentiment analysis and does not provide evidence of CVIB's performance on other NLP tasks.
- What evidence would resolve it: Applying CVIB to other NLP tasks, such as natural language inference or question answering, and evaluating its performance on benchmark datasets would provide evidence of its generalizability.

### Open Question 3
- Question: How does the performance of CVIB vary with different pre-trained language models, such as RoBERTa or GPT?
- Basis in paper: [inferred] The paper uses BERT as the base text encoder for CVIB but does not explore the impact of using other pre-trained language models, such as RoBERTa or GPT.
- Why unresolved: The paper does not provide evidence of CVIB's performance with different pre-trained language models.
- What evidence would resolve it: Conducting experiments with CVIB using different pre-trained language models, such as RoBERTa or GPT, and evaluating their performance on benchmark datasets would provide evidence of the impact of the choice of pre-trained language model.

## Limitations

- The paper's claims about spurious correlation mitigation rely heavily on empirical demonstrations rather than theoretical guarantees
- The VIB mechanism's effectiveness depends critically on the compression threshold βl and temperature parameter τ, but systematic sensitivity analysis across different hyperparameter settings is not provided
- The R-GAT component introduces additional complexity that may interact with VIB and contrastive learning in ways not fully explored

## Confidence

- **High confidence**: The core experimental results showing CVIB outperforming baselines on standard ABSA benchmarks (REST14, LAP14, REST15, REST16, MAMS) with both Accuracy and F1 metrics
- **Medium confidence**: The robustness claims based on ARTS dataset perturbations, as these results depend on the specific perturbation methodology and may not generalize to all spurious correlation patterns
- **Low confidence**: The generalization claims for cross-domain transfer, as only two domain pairs (REST14→REST15 and REST16→REST14) are evaluated, which may not be representative of broader domain shifts

## Next Checks

1. Conduct hyperparameter sensitivity analysis by varying βl (VIB compression weight) and τ (contrastive temperature) across multiple orders of magnitude to determine the stability of reported improvements and identify optimal settings

2. Perform per-class analysis for long-tail samples, specifically examining whether the improvements are uniform across all sentiment categories or concentrated in specific classes like NEUTRAL, which could indicate VIB is selectively pruning sentiment-relevant features

3. Implement a controlled experiment comparing CVIB against a baseline that uses only VIB compression (no contrastive learning) and another using only contrastive learning (no VIB), to definitively isolate which mechanism drives the performance gains