---
ver: rpa2
title: 'L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language
  Models'
arxiv_id: '2311.06493'
source_url: https://arxiv.org/abs/2311.06493
tags:
- tasks
- ensemble
- performance
- language
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Lifelong Learning (L3) ensemble approach using
  Foundational Language Models (FLMs) to address catastrophic forgetting when adapting
  to sequential NLP tasks. The method leverages ensemble configurations of pre-trained
  models and external knowledge augmentation to improve performance and mitigate forgetting.
---

# L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models

## Quick Facts
- arXiv ID: 2311.06493
- Source URL: https://arxiv.org/abs/2311.06493
- Authors: 
- Reference count: 12
- Key outcome: Ensemble of smaller FLMs with knowledge infusion achieves 4-36% higher accuracy than fine-tuned FLMs and outperforms T5 by up to 15.4% on STS benchmark

## Executive Summary
This paper proposes a Lifelong Learning (L3) ensemble approach using Foundational Language Models (FLMs) to address catastrophic forgetting when adapting to sequential NLP tasks. The method leverages ensemble configurations of pre-trained models and external knowledge augmentation to improve performance and mitigate forgetting. Experimental results on GLUE and SuperGLUE benchmarks demonstrate that the L3 ensemble method achieves 4% to 36% higher accuracy compared to fine-tuned FLMs.

## Method Summary
The paper proposes a Lifelong Learning (L3) ensemble approach that combines multiple pre-trained FLMs (BERT, RoBERTa, DistilBERT, ELECTRA) of varying sizes (up to 500M parameters) through ensemble aggregation methods. The approach includes knowledge infusion using Wikipedia Knowledge Graph and LLM embeddings to enhance vector representations. The method is evaluated on GLUE and SuperGLUE benchmarks to measure performance improvements and knowledge transfer across sequential tasks.

## Key Results
- L3 ensemble achieves 4% to 36% higher accuracy compared to fine-tuned FLMs on sequential tasks
- Outperforms state-of-the-art T5 model by up to 15.4% accuracy on the STS benchmark task
- Demonstrates superior performance in mitigating catastrophic forgetting compared to individual model approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble of smaller FLMs with knowledge infusion mitigates catastrophic forgetting better than fine-tuning single large models.
- Mechanism: The ensemble aggregates diverse representations from multiple pre-trained models, while knowledge infusion injects structured external information, creating a more robust knowledge base that resists degradation when adapting to new tasks.
- Core assumption: The knowledge base constructed from multiple FLMs and external sources is sufficiently diverse and stable to prevent interference between tasks during sequential learning.
- Evidence anchors:
  - [abstract] "We propose an approach that focuses on extracting meaningful representations from unseen data, constructing a structured knowledge base, and improving task performance incrementally."
  - [section] "Our work highlighted the need for a L3 ensemble approach to mitigate this issue, demonstrating the superior performance of ensembles over individual models."
- Break condition: If the knowledge infusion process introduces conflicting information or if the ensemble members have too much task overlap, the diversity benefit may be lost and catastrophic forgetting may reoccur.

### Mechanism 2
- Claim: Knowledge-infused (KI) ensemble outperforms simple averaging ensembles by leveraging structured external knowledge.
- Mechanism: The KI ensemble method incorporates vectorized information from Wikipedia Knowledge Graph into the ensemble, providing task-relevant context that enhances the ensemble's decision-making beyond what the base models can offer individually.
- Core assumption: The Wikipedia Knowledge Graph contains relevant, accurate information that can be effectively vectorized and integrated with the ensemble's predictions to improve task performance.
- Evidence anchors:
  - [abstract] "In the KI Ensemble, as showcased in Figure 2, we capitalize on the vectorized information from Wikipedia Knowledge Graph."
  - [section] "We find that the ensemble performs better than the individual models, and crucially, all of the individual models are at most 500M in size."
- Break condition: If the knowledge graph information is noisy, outdated, or not well-aligned with the task domain, the knowledge infusion could degrade rather than improve performance.

### Mechanism 3
- Claim: LLM ensemble method using frozen embeddings from larger language models improves vector representations without requiring full fine-tuning.
- Mechanism: By incorporating embeddings from "Langchain text-embedding-ada-002 LLM" into the ensemble, the method enhances the vectorized representations of the base FLMs, providing richer semantic features that improve downstream task performance.
- Core assumption: The frozen embeddings from the LLM contain semantically rich features that can complement the representations from the smaller FLMs in the ensemble.
- Evidence anchors:
  - [abstract] "In the LLM Ensemble, we harness the power of frozen embeddings from LLMs to enhance vector representations through meticulous modulation."
  - [section] "We find that the ensemble performs better than the individual models, and crucially, all of the individual models are at most 500M in size."
- Break condition: If the frozen embeddings are not well-aligned with the specific NLP tasks being performed, or if the base FLMs already capture sufficient semantic information, the added embeddings may provide minimal benefit.

## Foundational Learning

- Concept: Catastrophic Forgetting in sequential learning
  - Why needed here: The paper's central problem is that fine-tuning FLMs on new tasks causes them to forget previously learned tasks. Understanding this phenomenon is essential to grasp why the L3 ensemble approach is necessary.
  - Quick check question: What happens to a model's performance on task A when it is fine-tuned on task B and then tested on task A again?

- Concept: Ensemble learning methods
  - Why needed here: The paper proposes using ensembles of multiple FLMs rather than single models, so understanding how ensembles work and their benefits is fundamental to the approach.
  - Quick check question: How does combining predictions from multiple models typically improve performance compared to using a single model?

- Concept: Knowledge infusion and knowledge graphs
  - Why needed here: The KI ensemble specifically incorporates external knowledge from Wikipedia Knowledge Graph, so understanding how knowledge graphs can be used to augment model knowledge is crucial.
  - Quick check question: What is the difference between a model learning knowledge from training data versus having knowledge injected through a knowledge graph?

## Architecture Onboarding

- Component map: Text input → Pre-processing → Multiple FLMs (BERT, RoBERTa, DistilBERT, ELECTRA) → Individual task predictions → Ensemble aggregation (naïve, weighted, LLM, or KI) → Final output. Knowledge infusion components (Wikipedia KG vectors and/or LLM embeddings) are integrated during aggregation.

- Critical path: Text input → Pre-processing → Multiple FLMs → Individual task predictions → Ensemble aggregation (with knowledge infusion if applicable) → Final output. The most critical components are the FLMs themselves and the ensemble aggregation method.

- Design tradeoffs: Smaller FLMs (≤500M) are used for resource efficiency on edge devices, trading off some performance potential for reduced computational requirements. The ensemble approach compensates for this by combining multiple smaller models rather than using a single large model. Knowledge infusion adds complexity but can improve performance without requiring additional model training.

- Failure signatures: If catastrophic forgetting is still occurring despite the ensemble approach, this suggests the knowledge base is not sufficiently robust or the ensemble members are too similar. If ensemble performance is worse than individual models, this indicates poor aggregation method or conflicting knowledge infusion. If the system is too slow for edge devices, this suggests the ensemble size or knowledge infusion process is too computationally intensive.

- First 3 experiments:
  1. Compare individual FLM performance on sequential GLUE tasks to establish baseline catastrophic forgetting rates.
  2. Test different ensemble configurations (naive, weighted, LLM, KI) on the STS benchmark to measure performance improvements and identify the most effective aggregation method.
  3. Evaluate knowledge transfer metrics by measuring performance retention on previously seen tasks after training on new tasks across different ensemble methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of L3 ensemble methods scale when applied to more diverse NLP tasks beyond the GLUE and SuperGLUE benchmarks, particularly in specialized domains?
- Basis in paper: [inferred] The paper mentions the need for further evaluation across various knowledge-intensive NLP tasks and suggests extending the KI Ensemble using a reinforcement learning-based approach, but does not provide results for diverse or specialized domains.
- Why unresolved: The experiments are limited to standard benchmarks, and the paper does not explore performance in specialized or domain-specific NLP tasks.
- What evidence would resolve it: Empirical results comparing L3 ensemble performance on specialized NLP tasks (e.g., biomedical, legal, or technical domains) against state-of-the-art models and baseline ensemble methods.

### Open Question 2
- Question: What is the optimal ensemble configuration for balancing computational efficiency and accuracy in resource-constrained environments, and how does it vary with task complexity?
- Basis in paper: [explicit] The paper highlights that ensemble methods improve performance even with models of modest size, but does not provide a systematic analysis of how ensemble size and composition affect efficiency-accuracy trade-offs across different task complexities.
- Why unresolved: While the paper demonstrates that ensembles outperform individual models, it does not investigate the optimal number of models or their specific configurations for achieving the best balance between resource constraints and task performance.
- What evidence would resolve it: A comprehensive study varying ensemble sizes, model types, and task complexities to identify configurations that maximize accuracy while minimizing computational overhead.

### Open Question 3
- Question: How does the L3 ensemble approach perform in dynamic environments where task distributions shift unpredictably, and how robust is it to such changes?
- Basis in paper: [inferred] The paper focuses on sequential task adaptation and mitigating catastrophic forgetting but does not address scenarios with unpredictable or non-stationary task distributions.
- Why unresolved: The experimental setup assumes a controlled sequence of tasks, leaving uncertainty about the method's robustness in environments with shifting or unpredictable task distributions.
- What evidence would resolve it: Experiments evaluating L3 ensemble performance in dynamic environments with non-stationary task distributions, comparing it to adaptive lifelong learning methods.

## Limitations

- Knowledge infusion mechanism lacks detailed implementation specifications, making it difficult to reproduce KI ensemble results
- Limited ablation studies showing which components contribute most to the reported 4-36% accuracy improvements
- Does not adequately address performance tradeoffs between ensemble of smaller models versus single larger models

## Confidence

- High confidence in the general premise that ensemble methods can help mitigate catastrophic forgetting
- Medium confidence in the specific knowledge infusion mechanism and its contribution to performance gains
- Low confidence in the reproducibility of the KI ensemble implementation due to insufficient methodological detail

## Next Checks

1. Conduct ablation studies removing knowledge infusion components to quantify their specific contribution to the reported 4-36% accuracy improvements
2. Test the ensemble approach on additional benchmark datasets beyond GLUE/SuperGLUE to validate generalizability
3. Compare the resource efficiency claims by benchmarking the ensemble's performance against single larger models (e.g., 1B+ parameter models) under identical computational constraints