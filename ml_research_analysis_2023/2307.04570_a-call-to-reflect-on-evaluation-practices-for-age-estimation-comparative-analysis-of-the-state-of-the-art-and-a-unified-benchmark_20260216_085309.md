---
ver: rpa2
title: 'A Call to Reflect on Evaluation Practices for Age Estimation: Comparative
  Analysis of the State-of-the-Art and a Unified Benchmark'
arxiv_id: '2307.04570'
source_url: https://arxiv.org/abs/2307.04570
tags:
- facial
- estimation
- methods
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unreliable age estimation results
  due to inconsistent benchmarking and data splitting practices. It identifies key
  issues with current evaluation protocols and proposes a unified benchmark for fair
  comparison of state-of-the-art methods.
---

# A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark

## Quick Facts
- arXiv ID: 2307.04570
- Source URL: https://arxiv.org/abs/2307.04570
- Reference count: 36
- Primary result: Pre-training with cross-entropy and proper preprocessing outperforms specialized age estimation methods in high-data regimes.

## Executive Summary
This paper critically examines evaluation practices in facial age estimation, revealing that common random data splitting methods introduce significant bias through intra-subject leakage. Through extensive comparative analysis across seven datasets and multiple deep learning approaches, the authors demonstrate that factors like facial coverage, image resolution, and pre-training data have substantially more impact on performance than the choice of age estimation method itself. They propose a unified benchmark with subject-exclusive splits and make their code and data publicly available to enable fair comparisons across methods.

## Method Summary
The authors conduct a comprehensive evaluation of facial age estimation methods using seven public datasets (AgeDB, AFAD, CACD2000, CLAP2016, FG-NET, MORPH, UTKFace) plus IMDB-WIKI for pre-training. They implement subject-exclusive data splitting to prevent intra-subject leakage, test multiple architectures (ResNet-50, EfficientNet-B4, ViT-B-16, VGG-16) and age estimation methods (cross-entropy, DLDL, DLDL-v2, SORD, Mean-Variance, Unimodal), and systematically evaluate the impact of facial alignment, coverage, and resolution. All experiments follow a consistent protocol: pre-training on IMDB-WIKI with cross-entropy, then fine-tuning on target datasets with various age estimation methods.

## Key Results
- Random data splitting inflates performance by 10-15% through intra-subject leakage
- Complete facial coverage and 256x256 resolution consistently improve performance
- Pre-training on IMDB-WIKI with cross-entropy yields near-optimal results, making specialized ordinal methods unnecessary in high-data regimes
- Choice of age estimation method has negligible impact compared to preprocessing choices and architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random data splitting inflates performance by introducing intra-subject leakage.
- Mechanism: When images of the same person appear in both train and test sets, the model learns person-specific rather than generalizable age cues, artificially boosting accuracy.
- Core assumption: Age estimation datasets contain multiple images per subject, some captured at the same age.
- Evidence anchors:
  - [abstract] "This overlap introduces a potential bias, resulting in overly optimistic evaluation outcomes."
  - [section 2.2] "There is a possibility for the same individual to be present in both the training and testing sets."
  - [corpus] Weak. Corpus papers focus on new architectures or data augmentation but do not address splitting bias explicitly.
- Break condition: If datasets contain only one image per subject or if age annotations are perfectly aligned across multiple images of the same subject.

### Mechanism 2
- Claim: Pre-training on large-scale noisy datasets (e.g., IMDB-WIKI) followed by fine-tuning with cross-entropy yields near-optimal performance.
- Mechanism: Cross-entropy loss with a Bayes-optimal predictor is asymptotically optimal for learning the true posterior distribution in high-data regimes; pre-training provides rich feature representations that transfer well.
- Core assumption: The high-data regime assumption holds for the target datasets.
- Evidence anchors:
  - [abstract] "we contend that with the pre-training, we are approaching this asymptotic regime."
  - [section 4.2] "The IMDB-WIKI pre-training method consistently outperforms other approaches in terms of performance."
  - [corpus] Weak. Neighboring papers propose new backbones or multimodal pretraining but do not challenge the cross-entropy + pre-training paradigm for age estimation.
- Break condition: In low-data regimes where the asymptotic optimality of cross-entropy does not hold.

### Mechanism 3
- Claim: Data preprocessing choices (facial coverage, resolution, alignment) have a larger impact on performance than the choice of age estimation method.
- Mechanism: The input signal quality directly affects the model's ability to learn discriminative features; poor preprocessing masks age-related cues, while good preprocessing amplifies them.
- Core assumption: Age-related information is spatially distributed across the face and benefits from full facial coverage and high resolution.
- Evidence anchors:
  - [abstract] "these factors frequently exert a more pronounced impact than the choice of the age estimation method itself."
  - [section 4.4] "Generally, complete facial coverage... yields the best results across the majority of datasets."
  - [corpus] Weak. Most corpus papers propose new architectures but do not perform ablation studies on preprocessing choices.
- Break condition: If age cues are concentrated in a small facial region or if the dataset is very small and overfitting dominates.

## Foundational Learning

- Concept: Subject-exclusive splitting in cross-dataset evaluation.
  - Why needed here: To prevent the model from memorizing subject identities rather than learning generalizable age patterns.
  - Quick check question: What is the difference between random splitting and subject-exclusive splitting in terms of potential data leakage?

- Concept: Asymptotic optimality of cross-entropy loss with Bayes predictor.
  - Why needed here: To understand why specialized ordinal regression methods may not outperform vanilla cross-entropy in high-data regimes.
  - Quick check question: Under what conditions does the cross-entropy loss become asymptotically optimal for age estimation?

- Concept: Influence of input resolution on learned feature quality.
  - Why needed here: To justify the empirical finding that higher resolution consistently improves performance.
  - Quick check question: How does increasing input resolution affect the model's ability to capture fine-grained age-related facial features?

## Architecture Onboarding

- Component map: Face detection & alignment (RetinaFace) -> Cropping & resizing (256x256) -> Normalization (ImageNet stats) -> Backbone (ResNet-50/EfficientNet-B4/ViT-B-16/VGG-16) -> Age estimation head (softmax + Bayes-optimal predictor) -> Loss functions (cross-entropy, DLDL, DLDL-v2, SORD, Mean-Variance, Unimodal) -> Optimizer (Adam)

- Critical path:
  1. Preprocess images with RetinaFace detection and alignment.
  2. Resize to 256x256 with complete facial coverage.
  3. Load ImageNet-pretrained ResNet-50.
  4. Replace final layer with age estimation head.
  5. Pre-train on IMDB-WIKI (cross-entropy).
  6. Fine-tune on target dataset with chosen loss.
  7. Evaluate with MAE on held-out subject-exclusive splits.

- Design tradeoffs:
  - Higher resolution vs. memory/compute: 256x256 works well; going higher may yield diminishing returns.
  - Complete facial coverage vs. partial coverage: Complete coverage generally better, but partial may help reduce irrelevant feature noise.
  - Cross-entropy vs. ordinal losses: In high-data regimes, cross-entropy suffices; ordinal losses may help in low-data settings.

- Failure signatures:
  - High train accuracy but low test accuracy: Possible subject leakage or overfitting.
  - Consistent underperformance across all methods: Likely preprocessing or data quality issue.
  - Unstable training with certain losses (e.g., Unimodal): Check initialization and learning rate.

- First 3 experiments:
  1. Run ResNet-50 with cross-entropy on MORPH using subject-exclusive splits; verify MAE is in the ~2.8 range.
  2. Swap ResNet-50 for EfficientNet-B4; check if MAE improves by ~0.05-0.1.
  3. Test different facial coverages (eyes-mouth only vs. full head) on AFAD; observe impact on MAE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures beyond ResNet-50, EfficientNet-B4, ViT-B-16, and VGG-16 compare in age estimation performance, and what architectural features most influence age estimation accuracy?
- Basis in paper: [explicit] The authors compared four backbone architectures (ResNet-50, EfficientNet-B4, ViT-B-16, VGG-16) and found that "the selection of the model has a more pronounced impact on the performance than the choice of the age estimation method itself."
- Why unresolved: The paper only tested a limited set of popular architectures. There are many other potential architectures (e.g., MobileNet, ShuffleNet, various transformer architectures) that were not evaluated.
- What evidence would resolve it: Systematic testing of a broader range of architectures, including newer ones, across the same datasets and evaluation protocols.

### Open Question 2
- Question: What is the optimal facial coverage for age estimation across different datasets and how does it vary with dataset characteristics?
- Basis in paper: [explicit] The authors tested different levels of facial coverage and found that "complete facial coverage, which includes the entire head in the model input, yields the best results across the majority of datasets" but noted exceptions for datasets like AFAD where partial coverage performed better.
- Why unresolved: The paper provides limited analysis of why certain datasets benefit from different coverage levels and doesn't establish clear criteria for choosing optimal coverage based on dataset characteristics.
- What evidence would resolve it: Detailed analysis of dataset characteristics (image quality, age range, demographic diversity) and their relationship to optimal facial coverage, along with systematic testing across more diverse datasets.

### Open Question 3
- Question: How does the performance of age estimation methods scale with dataset size, and at what point does the choice of method become irrelevant compared to other factors?
- Basis in paper: [explicit] The authors state that "for age estimation tasks outside of the low-data regime, designing specialized methods is unnecessary" and that with IMDB-WIKI pre-training, they "are approaching this asymptotic regime" where cross-entropy becomes optimal.
- Why unresolved: The paper doesn't provide quantitative analysis of the relationship between dataset size and method performance, nor does it define the threshold where the low-data regime ends.
- What evidence would resolve it: Systematic experiments varying dataset sizes and quantifying the performance gap between specialized methods and cross-entropy across different data regimes.

## Limitations
- Results depend on the high-data regime assumption, which may not hold for all datasets or age ranges
- Focus on single-image estimation limits generalizability to multi-image or video-based approaches
- Subject-exclusive splitting addresses one form of leakage but doesn't fully explore other potential biases

## Confidence
- High Confidence: The impact of preprocessing choices (resolution, facial coverage) on performance is well-established through systematic ablation studies.
- Medium Confidence: The claim that method choice has negligible impact is supported by extensive experiments, but may not generalize to specialized domains or very low-data regimes.
- Medium Confidence: The pre-training + cross-entropy approach is shown to be highly effective, though its asymptotic optimality in age estimation requires further theoretical validation.

## Next Checks
1. Conduct a detailed analysis of subject overlap in standard random splits across all seven datasets to quantify the extent of intra-subject leakage and its impact on reported performance.

2. Replicate the experiments on datasets with fewer than 1000 images to validate whether cross-entropy remains optimal or if ordinal regression methods show improved performance.

3. Perform a statistical analysis of age distributions across datasets to identify potential distribution shifts that could explain performance differences in cross-dataset evaluation scenarios.