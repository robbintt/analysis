---
ver: rpa2
title: 'FlexRound: Learnable Rounding based on Element-wise Division for Post-Training
  Quantization'
arxiv_id: '2306.00317'
source_url: https://arxiv.org/abs/2306.00317
tags:
- flexround
- quantization
- adaround
- weights
- element-wise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FlexRound, a novel post-training quantization
  (PTQ) method for deep neural networks. Unlike existing PTQ techniques that rely
  on element-wise addition for weight rounding, FlexRound employs element-wise division
  to enable joint learning of a common quantization grid size and individual scale
  factors for each pre-trained weight.
---

# FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization

## Quick Facts
- arXiv ID: 2306.00317
- Source URL: https://arxiv.org/abs/2306.00317
- Authors: Not specified
- Reference count: 40
- Key outcome: Introduces FlexRound, a novel post-training quantization method using element-wise division that outperforms existing techniques on diverse models and tasks.

## Executive Summary
FlexRound presents a new approach to post-training quantization that addresses the limitations of existing rounding methods by leveraging element-wise division instead of addition. The method enables joint learning of quantization grid size and individual scale factors for each pre-trained weight, allowing more flexible quantization based on weight magnitudes. Extensive experiments demonstrate FlexRound's effectiveness across vision and language models, showing minimal performance degradation even at low bit-widths.

## Method Summary
FlexRound employs element-wise division to implement a learnable rounding mechanism that jointly optimizes a common quantization grid size and individual scale factors for each weight. The method minimizes reconstruction error between full-precision and quantized layer outputs through iterative optimization using a small calibration dataset. It supports both per-tensor and per-channel quantization formats and can be applied to various model architectures including ResNet, MobileNet, BERT, and GPT-style transformers.

## Key Results
- Achieves minimal performance degradation for 4-bit quantization on ImageNet classification tasks
- Successfully quantizes large language models (LLaMA, GPT-Neo) with negligible accuracy loss
- Outperforms traditional rounding-to-nearest methods across multiple vision and language benchmarks
- Demonstrates flexibility in handling activation outliers through block-wise reconstruction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FlexRound leverages element-wise division to enable joint learning of quantization grid size and scale factors.
- Mechanism: By using division instead of addition, the reciprocal rule of derivatives allows the method to directly influence quantization scales based on pre-trained weight magnitudes.
- Core assumption: Larger-magnitude weights are more important and should be quantized with greater flexibility.
- Evidence anchors:
  - [abstract] "FlexRound employs element-wise division to enable joint learning of a common quantization grid size and individual scale factors for each pre-trained weight."
  - [section 3.2] "By implementing such a new rounding policy through element-wise division, we can make s1, S2, s3, and s4 all learnable."
  - [corpus] Weak corpus alignment: neighbor papers focus on QUBO formulations and layer-wise optimizations, not element-wise division.

### Mechanism 2
- Claim: FlexRound can quantize pre-trained weights flexibly depending on their magnitudes.
- Mechanism: The magnitude of a pre-trained weight influences the update magnitude of its corresponding scale, allowing important weights to be quantized to grids further from the nearest ones.
- Core assumption: The magnitude of a weight is a reliable proxy for its importance within the network.
- Evidence anchors:
  - [section 3.2] "FlexRound is inherently able to exploit pre-trained weights when updating an individual scale for each pre-trained weight."
  - [section 3.2] "We corroborate that a relatively wider range of discrete values needs to be explored when quantizing pre-trained weights of large magnitude."
  - [corpus] Neighbor papers do not explicitly address weight magnitude as a quantization policy factor.

### Mechanism 3
- Claim: FlexRound improves quantization quality by allowing more grid shifts than rounding-to-nearest.
- Mechanism: The element-wise division formulation enables larger deviations from the nearest quantization grid, especially for weights of large magnitude.
- Core assumption: Greater grid flexibility improves model performance after quantization.
- Evidence anchors:
  - [section 3.2] "FlexRound can quantize pre-trained weights flexibly depending on their magnitudes, thereby leading to better performance."
  - [section 4.1] "Figure 3 shows the amount of weight updates via FlexRound... the change of W(i,j,k,l) attained by minimizing L is more aggressive when the absolute value of W(i,j,k,l) is larger than one."
  - [corpus] Neighbor papers do not compare grid shift flexibility explicitly.

## Foundational Learning

- Concept: Gradient descent and backpropagation through element-wise operations.
  - Why needed here: FlexRound relies on the reciprocal rule of derivatives induced by element-wise division, which requires understanding how gradients flow through division.
  - Quick check question: What is the derivative of 1/x with respect to x, and how does this affect gradient flow in FlexRound?

- Concept: Quantization and rounding schemes in neural networks.
  - Why needed here: FlexRound is a new rounding mechanism for post-training quantization, so understanding standard rounding and quantization is foundational.
  - Quick check question: How does rounding-to-nearest differ from the element-wise division approach in FlexRound?

- Concept: Per-tensor vs. per-channel quantization formats.
  - Why needed here: FlexRound supports both per-tensor uniform and per-channel quantization, and understanding the difference is key to applying it correctly.
  - Quick check question: What is the main trade-off between per-tensor and per-channel quantization in terms of accuracy and memory?

## Architecture Onboarding

- Component map: Pre-trained weights (W) -> FlexRound rounding function (cW = s1 * ⌊W / (s1 ⊙ S)⌋) -> Quantized weights (cW) -> Reconstruction error (L) -> Scale updates

- Critical path:
  1. Initialize s1, S2, s3, s4 (s1 as scalar, S2 as matrix/tensor, s3 and s4 as channel/bias tensors)
  2. Forward pass: compute cW using FlexRound formula
  3. Compute reconstruction error: L = ||W X - cW fX||²F
  4. Backward pass: update s1, S2, s3, s4 using gradient of L
  5. Repeat until convergence or max iterations

- Design tradeoffs:
  - Per-tensor vs. per-channel: per-tensor is simpler and faster but may be less accurate for channels with different statistics
  - Number of learnable tensors: more tensors (s3, s4) improve accuracy but increase parameter count and memory
  - Bit-width: higher bit-width allows more grid shifts and better flexibility, but less compression

- Failure signatures:
  - Divergence during training: check learning rate, gradient clipping, or initialization
  - No improvement in accuracy: verify that s1 is learnable and not fixed, check gradient flow through division
  - Memory overflow: reduce number of learnable tensors or use per-tensor instead of per-channel

- First 3 experiments:
  1. Implement FlexRound on a small CNN (e.g., CIFAR-10) with per-tensor quantization, compare accuracy vs. rounding-to-nearest
  2. Vary the bit-width (2-bit, 3-bit, 4-bit) and measure accuracy degradation and grid shift flexibility
  3. Replace per-tensor with per-channel quantization and measure impact on accuracy and memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FlexRound perform when combining it with preprocessing techniques like cross-layer equalization (CLE) and absorbing high biases (AHB)?
- Basis in paper: [inferred] The paper mentions that CLE and AHB can improve quantization performance for vision models, but notes that the effectiveness of these techniques depends on the pre-trained model used.
- Why unresolved: The paper only briefly mentions the potential interaction between FlexRound and CLE/AHB, and does not provide a comprehensive analysis of their combined performance.
- What evidence would resolve it: Systematic experiments comparing FlexRound's performance with and without CLE/AHB preprocessing on various pre-trained models would provide insight into their interaction and potential benefits.

### Open Question 2
- Question: Can FlexRound be extended to other quantization schemes beyond per-tensor and per-channel uniform quantization?
- Basis in paper: [inferred] The paper focuses on uniform quantization schemes, but does not explore the potential of FlexRound for non-uniform or mixed-precision quantization.
- Why unresolved: The paper does not investigate the applicability of FlexRound to other quantization schemes, leaving the question of its versatility open.
- What evidence would resolve it: Applying FlexRound to different quantization schemes and evaluating its performance would demonstrate its potential for broader applications.

### Open Question 3
- Question: How does the choice of learning rate for the quantization grid size (s1) affect FlexRound's performance?
- Basis in paper: [explicit] The paper mentions that the learning rate for s1 is set to a fixed value for all experiments, without exploring its impact on performance.
- Why unresolved: The paper does not investigate the sensitivity of FlexRound's performance to the choice of learning rate for s1, leaving this aspect unexplored.
- What evidence would resolve it: Conducting experiments with varying learning rates for s1 and analyzing their impact on FlexRound's performance would provide insights into its sensitivity to this hyperparameter.

## Limitations
- The method's performance on extremely low-bit quantization (below 4-bit) is not thoroughly explored
- The relationship between weight magnitude and importance may vary across architectures and tasks
- Potential numerical stability issues when scale factors approach zero are not explicitly discussed or quantified

## Confidence

**High confidence** in the experimental methodology and results for 4-bit quantization on vision models, given the systematic comparison with baselines and multiple dataset evaluations.

**Medium confidence** in the claimed advantages over rounding-to-nearest, as the paper demonstrates improved flexibility but does not provide quantitative measures of how much more aggressive the grid shifts are compared to standard methods.

**Medium confidence** in the scalability claims for large language models, as the evaluation focuses on specific architectures (OPT, GPT-Neo, LLaMA) but does not explore the full range of possible LLM sizes or training configurations.

## Next Checks

1. **Numerical stability verification**: Implement gradient clipping and monitor scale factor magnitudes during optimization to ensure element-wise division does not cause numerical instability, particularly for weights with small magnitudes.

2. **Importance correlation study**: Conduct ablation experiments where weight magnitudes are artificially shuffled to test whether the magnitude-importance relationship is causal rather than coincidental in FlexRound's success.

3. **Low-bit extension validation**: Apply FlexRound to 2-bit and 3-bit quantization scenarios for both vision and language models to determine the method's effectiveness at the extreme compression levels claimed by related works.