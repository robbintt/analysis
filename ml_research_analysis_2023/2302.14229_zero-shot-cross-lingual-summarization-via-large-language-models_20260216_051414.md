---
ver: rpa2
title: Zero-Shot Cross-Lingual Summarization via Large Language Models
arxiv_id: '2302.14229'
source_url: https://arxiv.org/abs/2302.14229
tags: []
core_contribution: This paper presents the first empirical evaluation of large language
  models (LLMs) on the task of zero-shot cross-lingual summarization (CLS). The authors
  design various prompts to guide ChatGPT to perform CLS from different paradigms
  (end-to-end and pipeline) and compare its performance with GPT-3.5 and fine-tuned
  mBART-50 on three widely-used CLS datasets.
---

# Zero-Shot Cross-Lingual Summarization via Large Language Models

## Quick Facts
- arXiv ID: 2302.14229
- Source URL: https://arxiv.org/abs/2302.14229
- Reference count: 15
- Primary result: GPT-4 achieves state-of-the-art zero-shot CLS performance, outperforming fine-tuned mBART-50

## Executive Summary
This paper presents the first empirical evaluation of large language models (LLMs) on zero-shot cross-lingual summarization (CLS). The authors design various prompts to guide ChatGPT to perform CLS from different paradigms (end-to-end and pipeline) and compare its performance with GPT-3.5 and fine-tuned mBART-50 on three widely-used CLS datasets. They find that ChatGPT and GPT-4 can balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results show that GPT-4 achieves state-of-the-art zero-shot CLS performance and performs competitively compared with fine-tuned mBART-50.

## Method Summary
The paper evaluates zero-shot cross-lingual summarization using ChatGPT, GPT-4, and GPT-3.5 across three paradigms: end-to-end, translate-then-summarize, and summarize-then-translate. The authors test these approaches on three CLS datasets (CrossSum, WikiLingua, and XSAMSum) with English as source language and Chinese/German as target languages. They use interactive prompts to help balance informativeness and conciseness, and evaluate performance using ROUGE-1/2/L and BERTScore metrics. The results are compared against a fine-tuned mBART-50 baseline.

## Key Results
- GPT-4 achieves state-of-the-art zero-shot CLS performance, outperforming fine-tuned mBART-50
- ChatGPT with interactive prompts significantly improves CLS performance by balancing informativeness and conciseness
- Pipeline approaches (translate-then-summarize and summarize-then-translate) outperform end-to-end approaches for zero-shot CLS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive prompts enable ChatGPT to balance informativeness and conciseness in zero-shot cross-lingual summarization.
- Mechanism: The interactive prompt ("Please make the Chinese summary shorter") leverages ChatGPT's ability to refine outputs through conversation, allowing it to adjust summary length while preserving key information.
- Core assumption: ChatGPT's RLHF tuning makes it verbose by default, but its interactive capabilities allow for iterative refinement.
- Evidence anchors:
  - [abstract]: "ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance."
  - [section]: "With the help of the interactive prompt, the interaction capability of ChatGPT could be further exploited. As shown in Table 3, more concise summaries can be generated after inputting the interactive prompt"
- Break condition: If the interactive prompt fails to reduce summary length or degrades content quality, the mechanism breaks. This could occur if the model doesn't understand the refinement request or if conciseness comes at the cost of critical information loss.

### Mechanism 2
- Claim: Pipeline approaches (translate-then-summarize and summarize-then-translate) outperform end-to-end approaches for zero-shot cross-lingual summarization.
- Mechanism: Pipeline approaches separate the translation and summarization tasks, allowing the model to focus on one subtask at a time, while also providing intermediate context that aids the final output.
- Core assumption: The model struggles to simultaneously perform translation and summarization in an end-to-end fashion due to the complexity of the combined task.
- Evidence anchors:
  - [section]: "Without the interactive prompt, we find that ChatGPT (e2e) typically performs worse than the pipelines, i.e., ChatGPT (Trans-Sum) and ChatGPT (Sum-Trans), perhaps due to the following reasons: (1) It is still challenging for a single model to directly perform CLS which requires both the translation and summarization ability, making the model struggle to simultaneously perform them."
- Break condition: If the intermediate products (translated text or source-language summary) are of poor quality, the pipeline approach could degrade performance. This might happen with complex source documents or when the translation/summarization subtasks are particularly challenging.

### Mechanism 3
- Claim: ChatGPT outperforms GPT-3.5 (text-davinci-003) on zero-shot cross-lingual summarization when using interactive prompts.
- Mechanism: ChatGPT's advanced training (including RLHF) and interaction capabilities enable it to generate more concise and higher-quality summaries compared to the non-interactive GPT-3.5 model.
- Core assumption: The improvements in ChatGPT over GPT-3.5 include better understanding of refinement requests and more efficient information compression.
- Evidence anchors:
  - [abstract]: "Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with the fine-tuned mBART-50."
  - [section]: "Compared with ChatGPT (e2e), text-davinci-003 generates more concise target-language summaries and achieves better performance. This phenomenon also exists in monolingual summarization (Qin et al., 2023). After using the interactive prompt, ChatGPT outperforms text-davinci-003 by a large margin"
- Break condition: If the task requires extensive domain knowledge or if the source language is significantly different from the model's training data, the performance gap between ChatGPT and GPT-3.5 might narrow or reverse.

## Foundational Learning

- Concept: Prompt engineering for large language models
  - Why needed here: The paper demonstrates that different prompt formulations (end-to-end, translate-then-summarize, summarize-then-translate) significantly impact performance, highlighting the importance of understanding how to effectively communicate tasks to LLMs.
  - Quick check question: How would you modify the prompts if you needed to generate summaries in a language that the model is less proficient in?

- Concept: Zero-shot vs. few-shot learning in LLMs
  - Why needed here: The paper evaluates zero-shot performance, which is crucial for understanding the baseline capabilities of LLMs without any task-specific fine-tuning. This knowledge is essential for determining when additional training or examples are necessary.
  - Quick check question: What additional information or examples would you provide to potentially improve performance beyond the zero-shot setting?

- Concept: Cross-lingual evaluation metrics and their limitations
  - Why needed here: The paper uses ROUGE and BERTScore for evaluation, but acknowledges that lower automatic scores don't necessarily indicate worse performance. Understanding these metrics and their limitations is crucial for proper interpretation of results.
  - Quick check question: What are the potential drawbacks of relying solely on ROUGE and BERTScore for evaluating cross-lingual summarization quality?

## Architecture Onboarding

- Component map: Input Document -> Prompt Generator -> LLM (ChatGPT/GPT-4/GPT-3.5) -> Output Summary -> Evaluation Metrics (ROUGE/BERTScore)

- Critical path:
  1. Document ingestion and preprocessing
  2. Prompt generation based on selected paradigm
  3. LLM inference
  4. Interactive refinement (if applicable)
  5. Evaluation against reference summaries

- Design tradeoffs:
  - End-to-end vs. pipeline approaches: Simplicity vs. potential performance gains
  - Interactive vs. non-interactive: Potential for improved results vs. increased interaction complexity and time
  - Zero-shot vs. few-shot/fine-tuned: Ease of deployment vs. potential performance improvements

- Failure signatures:
  - Consistently long summaries despite interactive prompts
  - Significant degradation in ROUGE or BERTScore when switching between prompt types
  - Poor performance on specific language pairs or document types

- First 3 experiments:
  1. Compare end-to-end performance across different source-target language pairs to identify language-specific challenges
  2. Test the effectiveness of the interactive prompt on a subset of documents with varying complexity
  3. Evaluate the impact of document length on pipeline vs. end-to-end approaches to determine if there's a threshold where one method becomes clearly superior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's zero-shot cross-lingual summarization performance compare to fine-tuned models when evaluated by human judges?
- Basis in paper: [explicit] The paper states that ChatGPT achieves lower automatic evaluation scores than fine-tuned mBART-50, but notes that lower automatic scores don't necessarily indicate worse performance, referencing Goyal et al. (2022) who found GPT-3 summaries had lower ROUGE scores but higher human evaluation scores than fine-tuned methods.
- Why unresolved: The paper explicitly states that the comparison between ChatGPT and fine-tuned mBART-50 "needs more carefully-designed human evaluation, which we reserve for the future."
- What evidence would resolve it: A rigorous human evaluation study comparing summaries generated by ChatGPT (with various prompts) against those from fine-tuned mBART-50 across multiple dimensions such as coherence, informativeness, and fluency.

### Open Question 2
- Question: What is ChatGPT's performance on cross-lingual summarization for low-resource languages?
- Basis in paper: [explicit] The paper acknowledges that their experiments only covered high-resource languages (English, Chinese, and German) and references Jiao et al. (2023) who found ChatGPT's machine translation ability is limited on low-resource languages, leading to a conjecture that the same limitation might exist for CLS.
- Why unresolved: The paper explicitly states that "the performance of ChatGPT on low-resource languages still needs to be explored."
- What evidence would resolve it: Systematic evaluation of ChatGPT's zero-shot CLS performance on language pairs where one or both languages are low-resource, comparing results with translation quality benchmarks for those languages.

### Open Question 3
- Question: Can better prompts be designed to improve ChatGPT's cross-lingual summarization performance beyond what was explored in this study?
- Basis in paper: [explicit] The paper states that they "only evaluate the lower threshold of ChatGPT's CLS performance" and suggests that "future work could explore better prompts to obtain better results."
- Why unresolved: The study used a systematic but limited set of prompts (6 variations) and explicitly acknowledges this as a limitation, indicating there's potential for improvement through more sophisticated prompting strategies.
- What evidence would resolve it: Comparative experiments testing various prompt engineering techniques (such as few-shot prompting, chain-of-thought prompting, or role-based prompting) against the baseline prompts used in this study, measuring improvements in automatic metrics and potentially human evaluation scores.

## Limitations
- Limited evaluation to high-resource language pairs (English-Chinese and English-German), with potential performance degradation on low-resource languages
- Reliance on automatic metrics (ROUGE, BERTScore) without comprehensive human evaluation to validate quality
- Only explores a limited set of prompting strategies, potentially missing more effective approaches

## Confidence
- High confidence: ChatGPT and GPT-4 outperform GPT-3.5 on zero-shot CLS tasks when using interactive prompts
- Medium confidence: Pipeline approaches (translate-then-summarize and summarize-then-translate) consistently outperform end-to-end approaches
- Medium confidence: GPT-4 achieves state-of-the-art zero-shot CLS performance comparable to fine-tuned mBART-50

## Next Checks
1. Test the interactive prompt mechanism on a wider variety of source documents with varying complexity and length to assess its robustness
2. Evaluate the zero-shot performance on additional language pairs to determine if the observed patterns hold across different linguistic distances
3. Compare the computational efficiency and cost-effectiveness of the different approaches (end-to-end vs. pipeline) for practical deployment scenarios