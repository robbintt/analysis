---
ver: rpa2
title: 'YAYI 2: Multilingual Open-Source Large Language Models'
arxiv_id: '2312.14862'
source_url: https://arxiv.org/abs/2312.14862
tags:
- data
- training
- language
- yayi
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report introduces YAYI 2, a series of multilingual
  open-source large language models with 30 billion parameters, designed to address
  the limitations of existing models in Chinese contexts. The models are pre-trained
  from scratch on a multilingual corpus of 2.65 trillion tokens, filtered through
  a rigorous preprocessing pipeline, and fine-tuned using supervised learning and
  reinforcement learning from human feedback.
---

# YAYI 2: Multilingual Open-Source Large Language Models

## Quick Facts
- arXiv ID: 2312.14862
- Source URL: https://arxiv.org/abs/2312.14862
- Authors: Multiple authors
- Reference count: 8
- Primary result: 30B parameter multilingual LLM achieving state-of-the-art performance on multiple benchmarks

## Executive Summary
YAYI 2 is a 30 billion parameter multilingual large language model developed from scratch to address limitations of existing models in Chinese contexts. The model is pre-trained on a 2.65 trillion token multilingual corpus (41.5% Chinese) and fine-tuned using supervised learning and reinforcement learning from human feedback. YAYI 2 demonstrates superior performance across multiple benchmarks including MMLU, CMMLU, GSM8K, HumanEval, and MBPP, outperforming other similar-sized open-source models and even surpassing larger models like Qwen-72B in some tasks. The model incorporates advanced techniques like FlashAttention 2 and multi-query attention to enhance efficiency, and includes integrated safety measures throughout the training process.

## Method Summary
YAYI 2 is developed through a comprehensive training pipeline starting with a rigorously filtered multilingual corpus of 2.65 trillion tokens. The model uses a Transformer-based decoder-only architecture with FlashAttention 2 and multi-query attention for efficiency. Training occurs on over 1000 A800 GPUs, followed by supervised fine-tuning with millions of instruction-output pairs and reinforcement learning from human feedback. The tokenizer expands vocabulary to 81,920 tokens for improved multilingual processing, and safety measures are integrated throughout pre-training and fine-tuning to mitigate harmful content.

## Key Results
- Outperforms other similar-sized open-source models on MMLU, AGIEval, and CMMLU benchmarks
- Surpasses Qwen-72B (larger model) on certain evaluation tasks
- Demonstrates superior tokenization efficiency with lower compression ratio than competing Chinese models
- Supports long instructions, multi-turn conversations, and domain-specific applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: YAYI 2's multilingual tokenizer achieves better compression ratio than other Chinese models.
- Mechanism: Expanding vocab size to 81920 and training on diverse multilingual corpus improves efficiency.
- Core assumption: Larger vocab size and diverse training data lead to better tokenization efficiency.
- Evidence anchors:
  - [abstract]: "The YAYI 2 tokenizer expands the vocabulary size to 80,000...boasting a lower compression ratio indicative of superior training and inference efficiency."
  - [section]: "Table 1 reveal that, in both bilingual (CH-EN) and multilingual scenarios, the YAYI 2 tokenizer outperforms other Chinese models such as Baichuan 1, ChatGLM, Chinese Alpaca 2, XVERSE, boasting a lower compression ratio indicative of superior training and inference efficiency."
- Break condition: If multilingual data quality is poor or vocab size too large causing overfitting.

### Mechanism 2
- Claim: YAYI 2 outperforms other similar-sized open-source models on multiple benchmarks.
- Mechanism: Rigorous data preprocessing, multi-query attention, and FlashAttention 2 improve model performance.
- Core assumption: Better data quality and efficient attention mechanisms lead to superior model performance.
- Evidence anchors:
  - [abstract]: "Extensive experiments on multiple benchmarks...consistently demonstrate that the proposed YAYI 2 outperforms other similar sized open-source models."
  - [section]: "Table 3 shows the detailed results of our proposed model in the comparative experiments on these benchmarks. Our model outperforms other models on MMLU, AGIEval and CMMLU benchmarks, even surpassing the Qwen-72B with a much larger parameter size."
- Break condition: If benchmark datasets are not representative of real-world performance.

### Mechanism 3
- Claim: YAYI 2's safety measures effectively mitigate harmful content.
- Mechanism: Dual-filtering mechanism with sensitive word screening and classification model for secondary filtering.
- Core assumption: Multiple layers of filtering can effectively remove harmful content from training data.
- Evidence anchors:
  - [abstract]: "Safety measures are integrated throughout the training process to mitigate harmful content."
  - [section]: "To filter various duplication patterns, we adopt a multi-level collaborative deduplication strategy...Toxicity Filtering: The Internet contains a substantial amount of harmful and false information...To alleviate this problem, we propose a dual-filtering mechanism..."
- Break condition: If new types of harmful content emerge that bypass existing filters.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding LLMs is crucial for comprehending YAYI 2's architecture and training process.
  - Quick check question: What are the key components of an LLM architecture?

- Concept: Multilingual processing
  - Why needed here: YAYI 2 is designed to handle multiple languages, requiring knowledge of multilingual NLP techniques.
  - Quick check question: How does multilingual processing differ from monolingual processing in LLMs?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is used to align YAYI 2 with human values, an important aspect of its training process.
  - Quick check question: What is the purpose of RLHF in LLM training?

## Architecture Onboarding

- Component map: Data preprocessing → Tokenization → Pre-training → Supervised Fine-tuning → RLHF → Evaluation
- Critical path: Data preprocessing → Tokenization → Pre-training → Supervised Fine-tuning → RLHF → Evaluation
- Design tradeoffs: Larger vocab size improves efficiency but may increase memory usage; multi-query attention reduces parameters but may affect performance.
- Failure signatures: Poor benchmark performance, high toxicity in outputs, slow inference speed.
- First 3 experiments:
  1. Compare compression ratios of different tokenizers on a multilingual corpus.
  2. Evaluate model performance on a subset of benchmarks with varying data preprocessing techniques.
  3. Test the effectiveness of different attention mechanisms (e.g., multi-query vs. standard attention) on a validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal vocabulary size for multilingual large language models, and how does it vary across different language families and data distributions?
- Basis in paper: [explicit] The paper mentions using a vocabulary size of 81,920 for the YAYI 2 tokenizer, chosen to balance minor language support with efficiency for Chinese and English. It also compares the compression ratio of YAYI 2 tokenizer with other models.
- Why unresolved: The paper does not provide a systematic study of how vocabulary size affects model performance across different languages and data distributions. It only presents a single vocabulary size choice and compares its compression ratio with other models.
- What evidence would resolve it: A comprehensive study comparing model performance (e.g., perplexity, downstream task accuracy) across a range of vocabulary sizes for different language families and data distributions would help determine the optimal vocabulary size for multilingual LLMs.

### Open Question 2
- Question: How does the extrapolation capability of rotary positional embeddings (RoPE) with techniques like YaRN affect the performance of large language models on long-context reasoning tasks?
- Basis in paper: [explicit] The paper mentions using YaRN for RoPE extrapolation to enhance the YAYI 2 models' ability to process lengthy texts and multi-turn conversations. It shows that YAYI 2-30B with YaRN has lower perplexity and is more stable than other configurations.
- Why unresolved: The paper only presents a single comparison of YaRN with other extrapolation methods. It does not provide a detailed analysis of how RoPE extrapolation affects the model's performance on various long-context reasoning tasks or how it compares to other positional encoding methods.
- What evidence would resolve it: A thorough evaluation of the YAYI 2 models with different RoPE extrapolation techniques on a variety of long-context reasoning benchmarks, comparing their performance to models using other positional encoding methods, would help determine the effectiveness of RoPE extrapolation for long-context reasoning.

### Open Question 3
- Question: What are the most effective techniques for ensuring the safety and reducing hallucinations in large language models, and how do they impact the model's overall performance and generalization capabilities?
- Basis in paper: [explicit] The paper describes various safety measures implemented during pre-training and fine-tuning, including data filtering, toxicity filtering, and safety auditing. It also mentions that the YAYI 2 models can still produce harmful content or fabricate "facts" despite these measures.
- Why unresolved: The paper does not provide a comprehensive evaluation of the effectiveness of the safety measures or their impact on the model's performance and generalization. It also does not explore alternative safety techniques or their potential trade-offs.
- What evidence would resolve it: A detailed study comparing the YAYI 2 models' safety performance (e.g., rate of harmful content generation, hallucination detection) with and without different safety techniques, as well as their impact on overall performance and generalization, would help determine the most effective safety strategies for LLMs.

## Limitations
- Training infrastructure requirements (over 1000 A800 GPUs) create significant reproducibility barriers
- Safety claims lack quantitative evaluation metrics and empirical validation
- Performance superiority claims require independent verification due to limited transparency in evaluation methodology

## Confidence

**High Confidence Claims:**
- The architectural design choices (FlashAttention 2, multi-query attention, RMSNorm) are technically sound and represent current best practices in LLM development
- The multilingual corpus composition (41.5% Chinese) and overall size (2.65 trillion tokens) are clearly specified
- The training methodology (pre-training → supervised fine-tuning → RLHF) follows established LLM development patterns

**Medium Confidence Claims:**
- Performance superiority over other models is reported but lacks full transparency in evaluation methodology
- Tokenizer efficiency improvements are demonstrated through compression ratio comparisons, but real-world impact on downstream tasks is not quantified
- Safety measures are described but not empirically validated against benchmark datasets

**Low Confidence Claims:**
- Claims about surpassing larger models (Qwen-72B) require independent verification due to potential evaluation discrepancies
- Long-context handling capabilities (8K sequence length) are mentioned but not thoroughly benchmarked
- Domain-specific application performance is asserted without comprehensive evaluation across multiple domains

## Next Checks
1. **Benchmark Replication**: Re-run the reported benchmark evaluations (MMLU, CMMLU, GSM8K, HumanEval, MBPP) using the same evaluation protocols and dataset splits to verify performance claims.

2. **Safety Evaluation**: Conduct systematic toxicity and bias assessments using established benchmarks like RealToxicityPrompts and BOLD to validate the effectiveness of the dual-filtering safety mechanisms.

3. **Tokenization Efficiency Analysis**: Perform controlled experiments comparing compression ratios and downstream task performance between YAYI 2's tokenizer and alternative multilingual tokenizers on representative datasets.