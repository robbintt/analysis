---
ver: rpa2
title: 'PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using Large
  Language Models'
arxiv_id: '2312.06093'
source_url: https://arxiv.org/abs/2312.06093
tags:
- topic
- topics
- memes
- meme
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PromptMTopic, a novel multimodal topic modeling
  approach for memes using large language models (LLMs). The method extracts text
  and image features from memes, then uses carefully constructed prompts to guide
  ChatGPT in generating and clustering topics.
---

# PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using Large Language Models

## Quick Facts
- arXiv ID: 2312.06093
- Source URL: https://arxiv.org/abs/2312.06093
- Reference count: 40
- Key outcome: Novel multimodal topic modeling approach for memes using LLMs, outperforming baselines in topic coherence and diversity

## Executive Summary
This paper introduces PromptMTopic, a novel approach for unsupervised multimodal topic modeling of memes using large language models (LLMs). The method combines image captions generated by BLIP-2 with superimposed text from memes, then uses carefully constructed prompts to guide ChatGPT in generating and clustering topics. Two topic merging approaches are proposed: Prompt-Based Matching (PBM) and Word Similarity Matching (WSM). Experiments on three real-world meme datasets demonstrate that PromptMTopic outperforms state-of-the-art baselines in both topic coherence (NPMI) and diversity, with the WSM variant achieving the best performance.

## Method Summary
PromptMTopic extracts visual and textual features from memes using BLIP-2 for image captioning and OCR for text extraction. The combined features are then processed through carefully constructed prompts to guide ChatGPT in generating and clustering topics. Two approaches for topic merging are employed: Prompt-Based Matching (PBM) and Word Similarity Matching (WSM). The method is evaluated on three meme datasets using NPMI for topic coherence and topic diversity metrics.

## Key Results
- PromptMTopic outperforms state-of-the-art baselines in topic coherence (NPMI) and diversity
- WSM variant achieves the best performance across most datasets
- PBM variant provides more interpretable topics while maintaining competitive performance
- Qualitative analysis confirms the model's ability to identify meaningful and culturally relevant topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs enhance topic modeling by leveraging their extensive knowledge base to better interpret informal and culturally nuanced language in memes.
- Mechanism: The model uses carefully constructed prompts to guide ChatGPT in identifying and grouping topics from both text and visual modalities of memes.
- Core assumption: LLMs have accumulated sufficient knowledge about internet culture, memes, and informal language to effectively interpret and categorize meme content.
- Evidence anchors: [abstract], [section 4.1], [corpus]
- Break condition: If the LLM's training data doesn't cover specific meme trends or cultural references, it may fail to accurately interpret and categorize those memes.

### Mechanism 2
- Claim: Combining visual and textual modalities through image captioning improves topic modeling accuracy.
- Mechanism: The model uses BLIP-2 to generate captions for meme images, which are then combined with the superimposed text.
- Core assumption: Image captions generated by BLIP-2 accurately represent the semantic content of meme images and can be effectively combined with text for topic modeling.
- Evidence anchors: [section 3.2], [section 4], [corpus]
- Break condition: If the image captioning model fails to capture the essential meaning of meme images, the combined textual representation will be incomplete or misleading.

### Mechanism 3
- Claim: The two-stage topic collapse process (PBM and WSM) effectively reduces topic redundancy while maintaining topic quality.
- Mechanism: The model first generates numerous topics using ChatGPT, then employs either prompt-based matching (PBM) or word similarity matching (WSM) to consolidate similar topics into a smaller set of unique, representative topics.
- Core assumption: ChatGPT can accurately identify topic relationships and effectively merge similar topics based on either semantic understanding (PBM) or word similarity (WSM).
- Evidence anchors: [section 4.2], [section 5.2], [corpus]
- Break condition: If the topic collapse process is too aggressive, it may oversimplify complex topics and lose important nuances in the data.

## Foundational Learning

- Concept: Multimodal data processing
  - Why needed here: The model must effectively combine and process information from both text and image modalities of memes.
  - Quick check question: How does the model convert image information into a format that can be combined with text for topic modeling?

- Concept: Large language model prompting techniques
  - Why needed here: The model relies on carefully constructed prompts to guide ChatGPT in generating and clustering topics.
  - Quick check question: What specific instructions and demonstration examples are included in the prompts to ensure ChatGPT understands the task?

- Concept: Topic coherence and diversity metrics
  - Why needed here: The model's performance is evaluated using NPMI for coherence and topic diversity scores.
  - Quick check question: How do NPMI scores and topic diversity scores complement each other in evaluating topic model quality?

## Architecture Onboarding

- Component map: Image preprocessing and captioning (BLIP-2) -> Text preprocessing (OCR and cleaning) -> Prompt construction and LLM interaction (ChatGPT) -> Topic generation and collapse (PBM and WSM) -> Topic representation and evaluation

- Critical path: Image captioning → Text preprocessing → Prompt construction → LLM topic generation → Topic collapse → Evaluation

- Design tradeoffs:
  - Using ChatGPT provides strong language understanding but increases cost and dependency on external APIs
  - The two-stage topic collapse process balances between interpretability (PBM) and performance (WSM)
  - Image captioning may introduce noise if captions don't accurately represent meme content

- Failure signatures:
  - Low NPMI scores indicate poor topic coherence
  - High percentage of "Miscellaneous" topics suggests ineffective topic collapse
  - High costs or API failures indicate issues with LLM dependency

- First 3 experiments:
  1. Test image captioning quality by manually comparing BLIP-2 captions to ground truth descriptions of meme images
  2. Evaluate prompt effectiveness by running ChatGPT with different prompt variations and measuring topic quality
  3. Compare PBM and WSM approaches on a small dataset to determine which method better balances interpretability and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different image captioning models (besides BLIP-2) affect the quality of extracted visual descriptions and subsequent topic modeling performance?
- Basis in paper: [inferred] The paper mentions using BLIP-2 for image captioning but does not explore alternatives.
- Why unresolved: The choice of image captioning model could significantly impact the quality of visual descriptions, which in turn affects topic modeling performance. Different models may capture different aspects of images.
- What evidence would resolve it: Comparative experiments using multiple image captioning models (e.g., CLIP, Flamingo) with the same PromptMTopic framework, measuring topic coherence and diversity metrics.

### Open Question 2
- Question: What is the optimal number of demonstration examples for in-context learning with ChatGPT to maximize topic modeling performance?
- Basis in paper: [explicit] The paper mentions using 8 conversation turns but does not systematically explore the impact of varying this number.
- Why unresolved: The number of demonstrations affects the model's ability to understand the task, but the optimal number may vary depending on the dataset complexity and prompt design.
- What evidence would resolve it: Systematic experiments varying the number of demonstration examples (e.g., 2, 4, 6, 8, 10) and measuring the resulting topic coherence and diversity scores.

### Open Question 3
- Question: How does the performance of PromptMTopic compare to supervised multimodal topic modeling approaches when labeled data is available?
- Basis in paper: [explicit] The paper focuses on unsupervised topic modeling and mentions supervised approaches only briefly in related work.
- Why unresolved: Supervised methods could potentially leverage labeled data to improve topic modeling accuracy, but their performance relative to PromptMTopic remains unknown.
- What evidence would resolve it: Comparative experiments between PromptMTopic and supervised approaches (e.g., multimodal LDA with labeled data) on meme datasets with available annotations, measuring both quantitative metrics and qualitative topic quality.

## Limitations
- Heavy reliance on external APIs (ChatGPT and BLIP-2) introduces dependencies and potential points of failure
- Manual prompt engineering for different datasets may limit scalability and reproducibility
- Performance on memes with minimal or no superimposed text remains uncertain

## Confidence
- High Confidence: The overall methodology of combining multimodal features with LLM-based topic modeling is sound
- Medium Confidence: The specific implementation details, particularly prompt engineering and topic collapse process, show promise but may require fine-tuning
- Low Confidence: The scalability to larger datasets and performance on memes with minimal text remain uncertain

## Next Checks
1. **Reproducibility Test**: Implement the complete pipeline using the provided methodology and evaluate topic quality on a held-out test set from one of the meme datasets. Compare the generated topics with human-annotated ground truth to assess accuracy.

2. **Robustness Evaluation**: Test the approach on a diverse set of meme datasets with varying characteristics (e.g., different languages, cultural contexts, and levels of superimposed text). Analyze the impact of dataset characteristics on topic generation quality and model performance.

3. **Cost-Benefit Analysis**: Conduct a thorough analysis of the computational and financial costs associated with using ChatGPT and BLIP-2 APIs for large-scale meme topic modeling. Explore potential optimizations or alternative approaches to reduce dependency on external services while maintaining performance.