---
ver: rpa2
title: Making Large Language Models Better Data Creators
arxiv_id: '2310.20111'
source_url: https://arxiv.org/abs/2310.20111
tags: []
core_contribution: This paper presents a unified data creation framework that leverages
  instruction-following LLMs to generate high-quality training data for various downstream
  NLP tasks, including those with semantically devoid or unenumerable label spaces.
  The core method involves iteratively conditioning an LLM on an instruction and a
  single formatting example to generate diverse, well-formatted data.
---

# Making Large Language Models Better Data Creators

## Quick Facts
- arXiv ID: 2310.20111
- Source URL: https://arxiv.org/abs/2310.20111
- Authors: 
- Reference count: 14
- Key outcome: LLM-generated training data outperforms human-labeled data by up to 17.5% accuracy on out-of-distribution tasks while costing under $5 USD per dataset.

## Executive Summary
This paper introduces a unified data creation framework that uses instruction-following LLMs to generate high-quality training data for various NLP tasks, including those with semantically devoid or unenumerable label spaces. The approach leverages a single formatting example in JSON format to iteratively generate diverse, well-formatted data through a self-reference sampling strategy. Experiments across multiple task types demonstrate that models trained on LLM-generated data consistently outperform those trained on human-labeled data on out-of-distribution evaluation while maintaining comparable performance on in-distribution tasks.

## Method Summary
The method involves iteratively conditioning an LLM on an instruction and a single formatting example to generate diverse, well-formatted data. A key innovation is the "self-reference" strategy, which samples from newly created data to serve as the formatting example for subsequent iterations, promoting diversity and mitigating domain drift. The approach works across tasks with semantically devoid or unenumerable label spaces by focusing on formatting examples rather than label semantics. The pipeline uses JSON-structured prompts to ensure syntactically valid outputs and includes validation steps to filter malformed or duplicate examples.

## Key Results
- Models trained on LLM-generated data outperform human-labeled data by up to 17.5% accuracy on out-of-distribution evaluation
- Cost-effective approach with API costs under $5 USD per dataset
- Maintains comparable performance on in-distribution tasks while showing significant gains on out-of-distribution tasks
- Effective across multiple task types including multiple-choice QA, open-book yes/no QA, and closed-book yes/no QA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-reference sampling with tree strategy preserves domain coherence while generating diverse examples.
- Mechanism: By using all generated outputs from one iteration as formatting examples for the next, the approach maintains topical coherence and avoids drift from the seed distribution.
- Core assumption: Maintaining semantic proximity between successive generations prevents degradation of output quality.
- Evidence anchors:
  - [abstract] "we propose a 'self-reference' strategy, which iteratively samples from the pool of newly created examples to seed the prompt for the next round of generation (Section 3.4)."
  - [section] "This approach can be viewed as a breadth-first tree traversal over generated examples, and is in contrast with the other three sampling approaches that use a depth-first exploration strategy."
- Break condition: If the model begins to produce repetitive or low-quality outputs after several iterations, indicating loss of diversity or domain coherence.

### Mechanism 2
- Claim: JSON-structured formatting examples enable controlled generation of syntactically valid data.
- Mechanism: Structured prompts in JSON format constrain the model to produce outputs that conform to a predefined schema, facilitating validation and downstream use.
- Core assumption: Auto-regressive models can reliably generate syntactically correct JSON when provided with a structured prompt template.
- Evidence anchors:
  - [abstract] "Our data creation pipeline leverages an instruction-following LLM as a generator in conjunction with a single formatting example in a JSON format to yield multiple examples that vary in content but are formatted uniformly (Section 3.1 âˆ’3.2)."
  - [section] "The only assumed input to our example-based data creation pipeline is a single formatting example (xf, yf) and its corresponding label space Y. This example is formatted as a JSON-structured prompt Wf as shown in Figure 4."
- Break condition: If the model consistently produces malformed JSON or fails to adhere to the schema despite structured prompts.

### Mechanism 3
- Claim: Instruction-following LLMs can generate diverse examples without requiring semantically meaningful labels or label descriptions.
- Mechanism: By focusing on formatting examples rather than label semantics, the approach works for tasks with semantically devoid or unenumerable label spaces.
- Core assumption: The model can understand formatting requirements from examples without needing semantic label descriptions.
- Evidence anchors:
  - [abstract] "our approach iteratively conditions the generator on an instruction and a unique formatting example in a JSON format to yield multiple examples that vary in content but are formatted uniformly."
  - [section] "In contrast to current methods that require dataset-specific components (e.g., label description, example selection), our pipeline serves as a unified solution that can be applied to a wide range of tasks, including those where the label set is either semantically devoid of meaning, or unenumerable."
- Break condition: If the model fails to generate appropriate examples for tasks with semantically devoid labels, producing outputs that don't match the task requirements.

## Foundational Learning

- Concept: Few-shot learning and prompt engineering
  - Why needed here: Understanding how LLMs can perform tasks with minimal examples is crucial for leveraging the single formatting example approach.
  - Quick check question: How does the choice of few-shot examples impact the performance of an LLM on a new task?

- Concept: Domain drift and its mitigation
  - Why needed here: Recognizing how iterative sampling can lead to domain drift and implementing strategies to mitigate it is essential for maintaining data quality.
  - Quick check question: What are the signs of domain drift in generated data, and how can sampling strategies address it?

- Concept: JSON schema validation
  - Why needed here: Ensuring that generated data conforms to the expected JSON schema is critical for the usability of the data in downstream tasks.
  - Quick check question: How can you programmatically verify that generated JSON data adheres to the required schema?

## Architecture Onboarding

- Component map:
  - Instruction-following LLM (e.g., GPT-3.5-turbo) -> Formatting example in JSON format -> Self-reference sampling strategies (random, contrastive, similar, tree) -> Data validation and filtering (duplicate removal, JSON validation) -> Downstream model training pipeline

- Critical path:
  1. Prepare initial formatting example and instruction
  2. Generate batch of examples using LLM
  3. Validate and filter generated data
  4. Select next formatting example based on sampling strategy
  5. Repeat steps 2-4 until desired dataset size is reached
  6. Train downstream model on generated data

- Design tradeoffs:
  - Single formatting example vs. multiple examples: Simplicity and generalizability vs. potential for richer initial guidance
  - Different self-reference sampling strategies: Controlled diversity vs. computational efficiency
  - JSON format vs. other structured formats: Ease of validation vs. flexibility in data representation

- Failure signatures:
  - Rapid degradation in output quality or coherence
  - High rate of malformed JSON outputs
  - Insufficient diversity in generated examples
  - Inability to generate appropriate examples for semantically devoid label spaces

- First 3 experiments:
  1. Evaluate the impact of different self-reference sampling strategies on data diversity and quality using a small-scale task (e.g., BoolQ without context).
  2. Test the robustness of the approach by applying it to tasks with semantically devoid label spaces (e.g., closed-book yes/no QA).
  3. Assess the cost-effectiveness of the approach by comparing the API costs of different sampling strategies while maintaining comparable data quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of LLM-generated data vary with different initial formatting examples?
- Basis in paper: [explicit] The paper mentions that the selection of the initial seed formatting sample is important but did not perform an exhaustive analysis to understand its impact on data creation quality due to API performance bottlenecks.
- Why unresolved: The paper acknowledges the potential impact of the initial seed example on the quality of generated data but did not have the opportunity to thoroughly investigate this aspect.
- What evidence would resolve it: Conducting experiments with various initial formatting examples and analyzing the resulting data's quality and diversity would provide insights into the impact of the seed selection on the overall data generation process.

### Open Question 2
- Question: Can the proposed data creation pipeline be effectively integrated with open-source instruction-following language models?
- Basis in paper: [explicit] The paper mentions that they experimented with open-source instruction-following models but found their abilities to generate well-formatted JSON and comprehend instructions were limited. They expect that integration with other open-source LLMs will be possible in the future.
- Why unresolved: The paper's testing was restricted to ChatGPT due to performance bottlenecks and time-out issues with LLM APIs in general. The effectiveness of integrating the pipeline with open-source models remains uncertain.
- What evidence would resolve it: Conducting experiments with various open-source instruction-following language models and evaluating their performance in generating well-formatted JSON and comprehending instructions would provide insights into the feasibility of integrating the pipeline with these models.

### Open Question 3
- Question: How does the performance of models trained on LLM-generated data compare to those trained on human-labeled data in real-world scenarios?
- Basis in paper: [explicit] The paper presents results comparing the performance of models trained on LLM-generated data and human-labeled data in both in-distribution and out-of-distribution settings. It highlights the potential of LLM-generated data for real-world systems.
- Why unresolved: While the paper provides evidence of the effectiveness of LLM-generated data in controlled settings, it does not directly address how these models perform in real-world scenarios with messy, variable, and evolving data.
- What evidence would resolve it: Deploying models trained on LLM-generated data in real-world applications and evaluating their performance on diverse and dynamic datasets would provide insights into their effectiveness in practical settings.

## Limitations

- Evaluation primarily focuses on NLP tasks with relatively straightforward formats, leaving open questions about performance on more complex multimodal or structured generation tasks
- The outperformance on out-of-distribution tasks is based on limited domain shifts and doesn't extensively explore more challenging distribution shifts
- The cost-effectiveness claim is based on API pricing at a specific time and may vary significantly with different model choices or market conditions

## Confidence

- **High confidence**: The general framework for instruction-following LLM data generation with JSON formatting examples is well-established in the literature and the implementation details are clearly specified.
- **Medium confidence**: The self-reference sampling strategy's effectiveness in preventing domain drift is supported by the experiments, but the evaluation could benefit from more rigorous statistical analysis and ablation studies isolating the impact of different sampling strategies.
- **Medium confidence**: The outperformance on out-of-distribution tasks is compelling but based on limited domain shifts, and the paper doesn't extensively explore more challenging distribution shifts.

## Next Checks

1. **Sampling strategy ablation**: Systematically compare the tree-based self-reference strategy against alternative approaches (random, contrastive, similar) using standardized diversity metrics and human evaluation of output quality across multiple task types.

2. **Stress test with semantically devoid labels**: Apply the approach to extreme cases of semantically devoid label spaces (e.g., arbitrary code assignments or completely abstract categories) to validate the claimed generality beyond the yes/no classification tasks tested.

3. **Domain drift analysis**: Implement quantitative measures of semantic drift (e.g., embedding distance metrics, topic modeling) to track how the distribution of generated data evolves across iterations, and test whether the approach maintains quality after 10+ generations.