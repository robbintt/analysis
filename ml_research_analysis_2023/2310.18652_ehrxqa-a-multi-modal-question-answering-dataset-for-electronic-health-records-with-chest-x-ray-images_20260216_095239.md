---
ver: rpa2
title: 'EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records
  with Chest X-ray Images'
arxiv_id: '2310.18652'
source_url: https://arxiv.org/abs/2310.18652
tags:
- question
- dataset
- image
- lung
- templates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EHRXQA is a new multi-modal question answering dataset for electronic
  health records that combines structured EHRs with chest X-ray images. It was developed
  by integrating two uni-modal resources: MIMIC-CXR-VQA, a medical visual question
  answering dataset, and EHRSQL (MIMIC-IV), a table-based EHR QA dataset.'
---

# EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images

## Quick Facts
- **arXiv ID**: 2310.18652
- **Source URL**: https://arxiv.org/abs/2310.18652
- **Reference count**: 40
- **Primary result**: First multi-modal EHR QA dataset combining structured EHR tables with chest X-ray images, containing 46,152 samples

## Executive Summary
EHRXQA is a novel multi-modal question answering dataset that integrates structured electronic health records with chest X-ray images. The dataset combines MIMIC-CXR-VQA (a medical visual question answering dataset) with EHRSQL (a table-based EHR QA dataset) through the TB_CXR linking table, creating a comprehensive resource for developing models that can reason across both structured data and medical images. The dataset contains three types of questions: Image-related, Table-related, and Image+Table-related, totaling 46,152 samples.

The paper proposes a NeuralSQL-based approach to handle the unique challenges of multi-modal questions within EHRs. This approach uses an LLM-based parser combined with an external VQA API to process both structured information and images. The dataset and methodology aim to advance real-world medical applications such as clinical decision-making and research by enabling more comprehensive question answering over electronic health records.

## Method Summary
The method involves creating a multi-modal QA dataset by integrating MIMIC-IV structured EHR data with MIMIC-CXR chest X-ray images through the TB_CXR linking table. The dataset construction uses question templates categorized into three types: Image-related, Table-related, and Image+Table-related, with time filters incorporated to capture temporal aspects of medical events. A NeuralSQL-based approach is proposed that extends SQL syntax to process unstructured image data by using a pretrained neural model to extract features from medical images, combined with an external VQA API for answering questions about the images. The approach uses ChatGPT as the LLM parser with BM25 retrieval for few-shot examples.

## Key Results
- EHRXQA dataset contains 46,152 samples covering Image-related, Table-related, and Image+Table-related questions
- NeuralSQL-based approach achieves logical form accuracy (AccLF) of 91.1% and prediction execution accuracy (AccEX|pred) of 73.4% for image-related questions
- The dataset is publicly available and can catalyze advances in real-world medical scenarios such as clinical decision-making and research

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The NeuralSQL-based approach with an external VQA API effectively handles the unique challenges of multi-modal questions within EHRs.
- **Mechanism:** NeuralSQL extends SQL syntax to process unstructured image data by using a pretrained neural model to extract features from medical images, turning them into a structured format suitable for SQL queries. This is combined with an external VQA API that can answer questions about the images.
- **Core assumption:** The VQA model can accurately extract relevant information from chest X-ray images to answer natural language questions.
- **Evidence anchors:**
  - [abstract]: "We propose a NeuralSQL-based strategy equipped with an external VQA API."
  - [section]: "Our approach integrates a large language model (LLM)-based parser with an external VQA application programming interface (API) module, effectively handling both structured information and images."
- **Break condition:** The VQA model's performance degrades significantly, or the NeuralSQL parser cannot accurately translate natural language questions into executable queries.

### Mechanism 2
- **Claim:** The integration of MIMIC-CXR and MIMIC-IV databases through the TB_CXR table enables cross-referencing of CXR images with structured EHR data.
- **Mechanism:** The TB_CXR table contains columns linking patient identifiers with CXR images, allowing retrieval of patient CXR images alongside other table data (e.g., diagnosis, procedure, and prescriptions) from MIMIC-IV using the subject_id or hadm_id.
- **Core assumption:** The TB_CXR table accurately links CXR images to the corresponding patient records in the MIMIC-IV database.
- **Evidence anchors:**
  - [section]: "This table comprises six columns: subject_id, hadm_id, study_id, image_id, studydatetime, and viewposition, connecting patient-related identifiers with CXR images of MIMIC-CXR."
  - [section]: "Through this table, patient CXR images can be retrieved alongside other table data (e.g., diagnosis, procedure, and prescriptions) from MIMIC-IV using the subject_id or hadm_id."
- **Break condition:** The TB_CXR table contains errors or inconsistencies in linking CXR images to patient records.

### Mechanism 3
- **Claim:** The use of question templates with time filters enhances the dataset's ability to capture temporal aspects of medical events.
- **Mechanism:** Question templates incorporate time filters categorized into three types: 1) [time_filter_global] restricts the time range of interest, such as 'last year' or 'in 2022'; 2)[time_filter_within], incorporating the keyword 'within', pinpoints events happening within specific temporal boundaries, such as 'within the same hospital visit' or 'within the same day'; 3) [time_filter_exact] refers to a precise temporal point, such as the 'last CXR study' or a specific date and time like '2105-12-26 15:00:00'.
- **Core assumption:** Time filters accurately capture the temporal context of medical events and enable meaningful comparisons across different time points.
- **Evidence anchors:**
  - [section]: "We further refined our question templates. We adopted the time filter concept from EHRSQL and applied it to all question templates."
  - [section]: "This enhancement allows our question templates to better meet the specific needs in clinical practice."
- **Break condition:** Time filters are too restrictive or too broad, leading to irrelevant or missing information in the answers.

## Foundational Learning

- **Concept:** Electronic Health Records (EHRs)
  - **Why needed here:** EHRXQA is a multi-modal question answering dataset for EHRs, combining structured EHRs with chest X-ray images.
  - **Quick check question:** What are the different types of data typically stored in EHRs?

- **Concept:** Visual Question Answering (VQA)
  - **Why needed here:** MIMIC-CXR-VQA is a medical VQA dataset created to augment the imaging modality in EHR QA.
  - **Quick check question:** How does VQA differ from traditional image classification or object detection tasks?

- **Concept:** Text-to-SQL and NeuralSQL
  - **Why needed here:** EHRXQA uses SQL and NeuralSQL annotations to represent the queries corresponding to the questions.
  - **Quick check question:** What is the difference between SQL and NeuralSQL, and when would you use each?

## Architecture Onboarding

- **Component map:** MIMIC-IV database -> MIMIC-CXR database -> Chest ImaGenome -> TB_CXR table -> EHRXQA dataset -> NeuralSQL approach -> ChatGPT parser -> M3AE VQA API

- **Critical path:** 1. Parse natural language question into NeuralSQL query using ChatGPT. 2. Execute NeuralSQL query, calling VQA API for image-related parts. 3. Retrieve and process relevant data from MIMIC-IV and MIMIC-CXR databases. 4. Generate answer based on query results and VQA API output.

- **Design tradeoffs:** Using an external VQA API adds complexity but allows leveraging pre-trained models. Incorporating time filters increases dataset size but enables temporal reasoning. Focusing on chest X-ray images limits generalizability to other medical imaging modalities.

- **Failure signatures:** Low accuracy in answering image-related questions suggests VQA API performance issues. High logical form accuracy but low execution accuracy indicates parsing or data retrieval problems. Inability to handle unanswerable questions suggests dataset construction limitations.

- **First 3 experiments:**
  1. Evaluate VQA API performance on MIMIC-CXR-VQA test set.
  2. Test NeuralSQL parser with a subset of EHRXQA questions, checking logical form accuracy.
  3. Run end-to-end QA on EHRXQA, measuring overall accuracy and identifying failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the upper bound performance for medical visual question answering tasks on the MIMIC-CXR-VQA dataset, given that even radiologists cannot achieve perfect perception accuracy in CXRs?
- **Basis in paper:** [explicit] The paper states that "unlike logical operations with clear answers, even radiologists cannot achieve perfect perception accuracy in CXRs [7, 8]. Thus, it is very likely that the upper bound QA performance of MIMIC-CXR-VQA is lower than 100%."
- **Why unresolved:** The paper proposes a reference model to estimate the highest achievable perception accuracy for single-image verify questions, but does not provide a definitive upper bound performance for the overall VQA task.
- **What evidence would resolve it:** Conducting a large-scale study with expert radiologists to establish a gold standard for VQA tasks on chest X-rays, and using this as a benchmark to evaluate the performance of VQA models on the MIMIC-CXR-VQA dataset.

### Open Question 2
- **Question:** How does the performance of the proposed NeuralSQL-based approach vary across different types of multi-modal questions in EHRXQA, particularly those requiring complex reasoning across multiple images and tables?
- **Basis in paper:** [inferred] The paper presents experimental results showing a significant gap between AccLF (logical form accuracy) and AccEX|pred (prediction execution accuracy) for Image-related and Image+Table-related questions, suggesting that visual perception is a bigger roadblock than logical reasoning.
- **Why unresolved:** The paper does not provide a detailed breakdown of performance across different types of multi-modal questions, making it difficult to assess the approach's strengths and weaknesses in handling complex reasoning tasks.
- **What evidence would resolve it:** Conducting a comprehensive analysis of the NeuralSQL-based approach's performance on various types of multi-modal questions in EHRXQA, including those requiring comparative analysis of multiple images, temporal reasoning, and cross-modal inference.

### Open Question 3
- **Question:** How does the proposed EHRXQA dataset compare to other multi-modal question answering datasets in terms of its ability to capture real-world clinical scenarios and support downstream applications?
- **Basis in paper:** [explicit] The paper claims that EHRXQA is the first multi-modal EHR QA dataset for table and image modality, and that it can catalyze advances in real-world medical scenarios such as clinical decision-making and research.
- **Why unresolved:** The paper does not provide a direct comparison of EHRXQA with other multi-modal QA datasets in terms of its clinical relevance and practical utility.
- **What evidence would resolve it:** Conducting a comparative study of EHRXQA with other multi-modal QA datasets, such as those focused on general knowledge or specific domains like healthcare, to evaluate their ability to capture real-world clinical scenarios and support downstream applications like clinical decision support or research.

## Limitations

- The approach relies heavily on the performance of both the LLM parser and external VQA API, with limited error analysis provided
- The assumption that chest X-ray images can be meaningfully integrated with EHR tables through subject_id and hadm_id links is not validated
- The dataset's focus on chest X-ray images limits generalizability to other medical imaging modalities
- The use of question templates may not fully capture the complexity and diversity of real-world clinical questions

## Confidence

- **High Confidence**: The dataset construction methodology (integrating MIMIC-IV, MIMIC-CXR, and Chest ImaGenome) and the general architecture of the NeuralSQL approach are well-specified and technically sound.
- **Medium Confidence**: The reported performance metrics, while showing improvement over baseline methods, lack detailed ablation studies to isolate the contribution of individual components.
- **Low Confidence**: The paper's claims about the dataset's utility for "clinical decision-making and research" are not substantiated with user studies or clinical validation.

## Next Checks

1. **Error Analysis**: Conduct a detailed error analysis to identify whether failures stem from parsing errors, VQA API limitations, or data retrieval issues. This would help prioritize improvements to the system.

2. **Link Validation**: Verify the accuracy and completeness of the TB_CXR table links between CXR images and patient records in MIMIC-IV. Assess whether missing or incorrect links affect downstream QA performance.

3. **Generalization Test**: Evaluate the NeuralSQL approach on a held-out set of questions that deviate from the template-based construction to assess real-world applicability and robustness to diverse question formulations.