---
ver: rpa2
title: Assessing the Interpretability of Programmatic Policies with Large Language
  Models
arxiv_id: '2311.06979'
source_url: https://arxiv.org/abs/2311.06979
tags:
- units
- unit
- program
- train
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LINT (LLM-based INTerpretability), a novel
  metric for assessing the interpretability of programmatic policies using large language
  models (LLMs). The method generates a natural language explanation of a program,
  reconstructs the program from this explanation, and measures behavioral similarity
  between the original and reconstructed programs.
---

# Assessing the Interpretability of Programmatic Policies with Large Language Models

## Quick Facts
- arXiv ID: 2311.06979
- Source URL: https://arxiv.org/abs/2311.06979
- Reference count: 0
- Primary result: LINT scores strongly correlate with program obfuscation levels, successfully ranking less interpretable programs lower

## Executive Summary
This paper introduces LINT (LLM-based INTerpretability), a novel metric for assessing the interpretability of programmatic policies using large language models. The method generates natural language explanations of programs, reconstructs programs from these explanations, and measures behavioral similarity between original and reconstructed programs. Empirical results show strong negative correlation between LINT scores and obfuscation levels across MicroRTS programmatic policies and C programming problems.

## Method Summary
LINT uses two LLMs in a pipeline: an Explainer LLM generates natural language descriptions of programs given the program and DSL specification, while a Reconstructor LLM attempts to recreate the original program from the explanation and DSL description. A verifier LLM checks explanations for programming jargon. The system measures behavioral similarity between original and reconstructed programs using action, outcome, and feature metrics, with k trials selecting the lowest B value to account for LLM stochasticity.

## Key Results
- LINT scores strongly and negatively correlate with obfuscation levels in MicroRTS programs
- Behavioral similarity metrics (action, outcome, feature) effectively quantify program reconstruction quality
- Using k=5 trials with lowest B value selection provides reliable scores without random program reconstruction
- The approach works across different domains including MicroRTS policies and classical C programming problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LINT scores strongly correlate with obfuscation because LLMs detect semantic differences between clear vs. obfuscated code explanations
- Core assumption: Obfuscation reduces semantic clarity detectable by LLMs via natural language generation
- Evidence: Strong negative correlation between LINT scores and obfuscation levels
- Break condition: If LLMs learn to bypass obfuscation detection through plausible-sounding but incorrect logic

### Mechanism 2
- Claim: Reconstruction forces explanations to be precise enough for accurate program recreation
- Core assumption: Reconstructor must follow explanation precisely, not guess programs
- Evidence: Reconstructed programs deviate from originals when explanations are vague due to obfuscation
- Break condition: If Reconstructor can generate valid programs without precise explanations

### Mechanism 3
- Claim: Multiple trials with lowest B value selection accounts for LLM stochasticity
- Core assumption: Program space is large enough to prevent random correct reconstruction
- Evidence: Using k=5 trials safely prevents random sampling from yielding correct programs
- Break condition: If DSL/program space is small enough for random correct reconstructions

## Foundational Learning

- Concept: Domain-specific language (DSL) and context-free grammar (CFG)
  - Why needed: Reconstructor must generate valid programs using only DSL description
  - Quick check: What distinguishes a DSL from general-purpose programming language for programmatic policies?

- Concept: Behavior similarity metrics (B)
  - Why needed: LINT measures interpretability by comparing behavior, not syntax
  - Quick check: Why use multiple behavior metrics rather than a single one?

- Concept: Program obfuscation techniques
  - Why needed: Obfuscation serves as proxy for interpretability assessment
  - Quick check: How do garbage snippets affect program interpretability?

## Architecture Onboarding

- Component map: Program → Explainer → Explanation → Verifier → (Pass) → Reconstructor → Reconstructed program → Behavior metric B → LINT score

- Critical path: 1) Program → Explainer → Explanation 2) Explanation → Verifier → (Pass/Fail) 3) (Pass) → Reconstructor → Reconstructed program 4) Original + Reconstructed → Behavior metric B 5) B values → LINT score

- Design tradeoffs: LLM-based system offers scalability but introduces stochasticity; behavior metrics avoid syntax issues but require careful selection; obfuscation proxy is practical but assumes strong correlation

- Failure signatures: LINT scores don't match human judgments; Reconstructor generates semantically wrong programs; Verifier incorrectly flags clear explanations; LLM explanations become overly verbose

- First 3 experiments: 1) Run LINT on non-obfuscated C programs, verify B ≈ 1.0 2) Run on level 1 obfuscated programs, verify B decreases 3) Run on level 2 obfuscated programs, verify stronger B decrease than level 1

## Open Questions the Paper Calls Out
- The authors acknowledge LINT assumes LLM knowledge, which may not reflect target audience knowledge mismatches
- The paper mentions using k=5 trials but doesn't provide variance analysis or cross-model validation
- The methodology assumes sufficient program complexity for meaningful explanations but doesn't define complexity boundaries

## Limitations
- Correlation with obfuscation may not generalize to all domains and programming paradigms
- Effectiveness of jargon verifier and whether behavioral metrics capture semantic differences remains unclear
- k=5 trials may not sufficiently account for LLM stochasticity in all scenarios

## Confidence
- High Confidence: Core LINT methodology is sound with established MicroRTS correlation
- Medium Confidence: Approach generalizes to other domains (C programming, Atari games)
- Low Confidence: Verifier reliably prevents jargon and behavioral metrics capture meaningful semantic differences

## Next Checks
1. Apply LINT to programmatic policies from structurally different domains (financial algorithms, medical rules) to verify obfuscation correlation
2. Compare LINT scores against human expert interpretability ratings to validate metric alignment
3. Test adversarial programs that are semantically obfuscated but behaviorally equivalent to originals, verifying LINT identifies them as less interpretable