---
ver: rpa2
title: 'Consensus-based Participatory Budgeting for Legitimacy: Decision Support via
  Multi-agent Reinforcement Learning'
arxiv_id: '2307.12915'
source_url: https://arxiv.org/abs/2307.12915
tags:
- consensus
- projects
- participatory
- budgeting
- voting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel consensus-based participatory budgeting
  process using multi-agent reinforcement learning (MARL-PB). The approach enables
  voters to interact and compromise via decision support, aiming for more legitimate
  outcomes.
---

# Consensus-based Participatory Budgeting for Legitimacy: Decision Support via Multi-agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.12915
- Source URL: https://arxiv.org/abs/2307.12915
- Reference count: 16
- Primary result: MARL-PB achieves consensus in participatory budgeting with efficiency and fairness comparable to existing methods while providing a scalable, automated deliberation framework

## Executive Summary
This paper introduces a novel consensus-based participatory budgeting process using multi-agent reinforcement learning (MARL-PB) that enables voters to interact and compromise through decision support. The approach uses Q-learning with rewards derived from historical project success and peer communication to guide voters toward consensus bundles. Extensive experiments with real-world data from Polish districts demonstrate that consensus is achievable, efficient, and robust, with required compromise levels comparable to established fair voting methods. The framework provides a systematic, scalable way to model large-scale automated deliberation in participatory budgeting.

## Method Summary
MARL-PB employs a multi-agent reinforcement learning framework where each voter agent selects project bundles based on Q-learning updates that incorporate both deterministic rewards (historical project attribute frequencies) and stochastic rewards from peer communication via gossip-based protocols. The system operates on a dynamic random bidirectional graph where agents exchange information and update their preferences iteratively. The action space consists of valid project bundles constrained by budget limitations, with exploration-exploitation balance maintained through ϵ-greedy strategies. Convergence is assessed through bundle similarity across iterations, with consensus reached when voter choices stabilize.

## Key Results
- Consensus is achievable in participatory budgeting settings with efficiency and fairness comparable to equal shares and Phragmen methods
- Convergence speed improves with higher in-degree communication networks and smaller bundle sizes
- The required compromise cost is similar to existing fair voting aggregation methods while maintaining good budget utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MARL-PB approach enables voters to reach consensus through iterative reward-based learning and decentralized communication
- Mechanism: Voters start with individual preferences and iteratively update their choices based on both deterministic rewards (historical project success) and stochastic rewards from peer communication. The Q-learning update rule incorporates rewards from both sources, allowing agents to explore and exploit different bundles until convergence
- Core assumption: Voters can effectively exchange information through a decentralized gossip-based communication protocol, and this information exchange meaningfully influences their reward structures
- Evidence anchors: [abstract] "Consensus is designed to be a result of decision support via an innovative multi-agent reinforcement learning approach." [section] "At every iteration, we update a dynamic random bidirectional graph using a decentralized process such as the gossip-based peer sampling [8] for peer-to-peer communication."
- Break condition: Consensus fails if the communication network becomes too sparse (low in-degree) or if the action space (number of valid bundles) is too large for agents to effectively explore within reasonable iterations

### Mechanism 2
- Claim: The deterministic reward model based on historical project attributes effectively captures collective preferences for reaching consensus
- Mechanism: Project rewards are calculated from the frequency of attribute occurrences in past listed and selected projects, normalized by budget constraints. This creates a preference landscape that guides agents toward bundles that reflect community priorities
- Core assumption: Project attributes that were popular in the past will continue to be relevant indicators of collective preference in future participatory budgeting cycles
- Evidence anchors: [section] "The number of occurrence of such project attributes, which are put for voting and selected in the past years of a region is used as reward utilities." [section] "The projects selected using equal shares and MARL-PB correspond to similar public amenities... This also signifies that consensus projects in MARL-PB prioritize fairness and better representation."
- Break condition: The reward model breaks down if community preferences shift dramatically or if historical data becomes irrelevant due to changing social needs

### Mechanism 3
- Claim: The ϵ-greedy exploration strategy combined with communication enables efficient convergence to consensus bundles
- Mechanism: Agents initially explore different bundles based on their individual preferences, then gradually exploit the most rewarding bundles as they receive information from neighbors. The stochastic nature of peer communication adds exploration diversity beyond pure ϵ-greedy
- Core assumption: A balance between exploration and exploitation can be maintained through the ϵ-greedy parameter and the communication network structure
- Evidence anchors: [section] "The selection of the bundle at each iteration is based on the cumulative sum of both rewards, which the agents maximize using an ϵ greedy exploration strategy." [section] "Figure 7 shows the convergence time as function of in-degree and bundles size. Smaller bundles with higher in-degrees improve the speed."
- Break condition: Convergence fails if ϵ is set too high (excessive exploration) or too low (premature exploitation), or if the communication network topology prevents effective information spread

## Foundational Learning

- Concept: Multi-armed bandit formulation with knapsack constraints
  - Why needed here: The participatory budgeting problem requires selecting multiple projects under budget constraints, which maps naturally to a combinatorial multi-armed bandit where actions are valid project bundles
  - Quick check question: How does the knapsack constraint transform a standard multi-armed bandit into a combinatorial problem?

- Concept: Q-learning with augmented rewards from communication
  - Why needed here: Standard Q-learning needs to be extended to incorporate rewards from peer communication, requiring an additional learning rate parameter and modified update rule
  - Quick check question: What is the role of the δ parameter in the augmented Q-learning update, and how does it differ from the standard α?

- Concept: Decentralized gossip-based communication protocols
  - Why needed here: Large-scale participatory budgeting requires scalable information exchange without centralized coordination, making gossip protocols ideal for maintaining random communication graphs
  - Quick check question: How does the in-degree of the communication graph affect both convergence speed and the stability of consensus formation?

## Architecture Onboarding

- Component map: Voter agents (Q-learning) -> Communication layer (gossip-based peer sampling) -> Reward calculator (historical project attributes) -> Bundle validator (budget constraints) -> Consensus checker
- Critical path: 1) Initialize projects and calculate deterministic rewards, 2) Form valid bundles, 3) Initialize voter preferences, 4) Iterate: communication → reward aggregation → action selection → convergence check, 5) Output consensus bundle
- Design tradeoffs: Using deterministic rewards from historical data provides stability but may not capture emerging preferences; decentralized communication scales well but introduces randomness that can slow convergence; the ϵ-greedy strategy balances exploration and exploitation but requires careful parameter tuning
- Failure signatures: Slow convergence (high iteration count), low budget utilization in consensus bundles, high compromise cost with large variance, or failure to reach consensus within maximum iterations
- First 3 experiments:
  1. Test convergence speed with varying in-degree (2-26) and bundle count (5-90) using the Rembertow dataset
  2. Compare compromise cost of MARL-PB consensus with equal shares, phragmen, and greedy aggregation methods
  3. Measure robustness by running multiple simulations with different random seeds for communication topology and analyzing stability of convergence time

## Open Questions the Paper Calls Out

- Question: How do alternative communication topologies (e.g., small-world or scale-free networks) impact the efficiency and quality of consensus in MARL-PB compared to the current random graph approach?
  - Basis in paper: [inferred] The paper uses a random bidirectional graph updated via gossip-based peer sampling for inter-agent communication, but suggests future work to explore different dynamic topologies that represent social networks more closely
  - Why unresolved: The current experimental evaluation only considers random graphs. The impact of different network structures on convergence speed, consensus quality, and fairness is not investigated
  - What evidence would resolve it: Experiments comparing MARL-PB performance (convergence time, compromise cost, budget utilization) across various network topologies (random, small-world, scale-free) using the same datasets and metrics

- Question: How does the MARL-PB framework perform with alternative preferential elicitation methods beyond approval voting, such as ranked-choice or cumulative voting?
  - Basis in paper: [explicit] The paper states that expanding MARL-PB with other preferential elicitation methods beyond approval voting is expected to further strengthen the accuracy and legitimacy of consensus-based participatory budgeting
  - Why unresolved: The current implementation and evaluation only use approval voting data from the pabulib dataset. The framework's adaptability to other voting formats and their impact on consensus outcomes is untested
  - What evidence would resolve it: Implementing MARL-PB with ranked-choice and cumulative voting datasets, then comparing consensus quality metrics (popularity, compromise, budget utilization) against approval voting results

- Question: What is the optimal design for the reward modeling scheme that maximizes both consensus quality and voter satisfaction in MARL-PB?
  - Basis in paper: [inferred] The paper uses a deterministic reward function based on project attribute occurrences over time, but acknowledges that a more advanced reward design will further expand applicability
  - Why unresolved: The current reward function is a basic linear combination of historical attribute frequencies and project costs. More sophisticated reward models incorporating temporal dynamics, spatial factors, or voter heterogeneity are not explored
  - What evidence would resolve it: Systematic experimentation with alternative reward functions (e.g., time-decayed frequencies, geographic weighting, demographic-specific preferences) and their impact on consensus metrics across multiple datasets

## Limitations

- The reward model assumes historical project preferences remain valid indicators of current collective preferences, which may not hold in dynamic social contexts
- The decentralized communication protocol introduces stochasticity that could affect reproducibility and stability of outcomes
- Experimental validation focuses primarily on Polish datasets, limiting generalizability to other cultural or institutional contexts

## Confidence

- Consensus Achievement Mechanism: Medium confidence - The Q-learning framework and communication protocol are theoretically sound, but empirical validation of the exact convergence conditions remains incomplete
- Reward Model Effectiveness: Low-Medium confidence - While historical data provides a reasonable starting point, the assumption that past preferences predict future consensus needs further validation across different regions and time periods
- Scalability and Efficiency: Medium confidence - The results show reasonable performance for the tested dataset sizes, but computational scaling beyond 90 projects remains unverified

## Next Checks

1. **Reward Model Robustness Test**: Run MARL-PB with artificially modified historical data where attribute distributions are systematically shifted, then measure how consensus outcomes change. This would validate whether the reward model adapts appropriately to changing preference patterns.

2. **Cross-Regional Generalization**: Apply the MARL-PB framework to participatory budgeting datasets from at least two different countries with distinct cultural contexts, comparing consensus achievement rates and compromise costs against the Polish results.

3. **Communication Protocol Sensitivity**: Conduct ablation studies by varying the gossip-based communication parameters (in-degree range, update frequency) and measuring their impact on convergence speed and consensus stability across multiple random seeds.