---
ver: rpa2
title: Grounding for Artificial Intelligence
arxiv_id: '2312.09532'
source_url: https://arxiv.org/abs/2312.09532
tags:
- grounding
- world
- internal
- grounded
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fine-grained framework for analyzing grounding
  in natural language processing. It argues that grounding is a central issue for
  artificial general intelligence and is necessary to move beyond the capabilities
  of current large language models.
---

# Grounding for Artificial Intelligence

## Quick Facts
- arXiv ID: 2312.09532
- Source URL: https://arxiv.org/abs/2312.09532
- Reference count: 8
- This paper proposes a fine-grained framework for analyzing grounding in natural language processing, arguing that grounding is central to artificial general intelligence and necessary to move beyond current large language models.

## Executive Summary
This paper presents a theoretical framework for grounding natural language in artificial intelligence systems. The framework proposes that intelligent beings maintain an internal world model containing both sensorimotor experiences and subjective feelings, which serves as the basis for understanding language. The authors argue that grounding is essential for achieving artificial general intelligence and addresses the limitations of current large language models by connecting abstract language to concrete experiences.

## Method Summary
The paper proposes a theoretical framework rather than an implemented system. It introduces an internal world model that captures an intelligent being's sensorimotor experiences and subjective feelings, then presents a set of grounding rules for connecting natural language to this internal model. The framework distinguishes between instance-level knowledge (personal experiences) and aggregate-level knowledge (generalized concepts) to enable efficient grounding at appropriate levels of abstraction.

## Key Results
- Grounding is identified as a central issue for artificial general intelligence
- The framework introduces an internal world model with instance-level and aggregate-level knowledge representations
- A systematic set of grounding rules is proposed to connect natural language elements to internal representations
- The paper highlights the limitations of current large language models in terms of grounding and understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding connects abstract language to internal sensorimotor representations that enable understanding
- Mechanism: Natural language words and sentences are grounded by linking them to internal representations of sensorimotor experiences (visual scenes, actions, feelings) accumulated through direct or indirect experiences
- Core assumption: Internal world models contain rich representations of both concrete experiences and abstract concepts that can be connected to language
- Evidence anchors:
  - [abstract] "Human cognition is grounded in our sensorimotor experiences in the external world and subjective feelings in our internal world"
  - [section 2.1] "The perception can be direct or indirect. Direct perceptions mean personal experiences and indirect perceptions mean being told"
- Break condition: If internal representations cannot capture the complexity of real-world experiences or if language uses concepts that have no corresponding sensorimotor basis

### Mechanism 2
- Claim: Different levels of grounding (instance-level vs aggregate-level) enable efficient processing of language
- Mechanism: The framework distinguishes between grounding at instance level (personal experiences) and aggregate level (generalized concepts), allowing the system to ground language at the appropriate level of abstraction without unnecessary detail
- Core assumption: The intelligent being maintains both specific experience representations and generalized concept representations in its internal world model
- Evidence anchors:
  - [section 2.2] "The instance-level knowledge in the Internal World Model...contains each individual personal experience" and "The aggregate-level knowledge...means the knowledge about a class or a group of objects"
  - [section 2.3 Rule 4] "To save energy, unless necessary grounding goes only to the highest level that is sufficient"
- Break condition: If the system cannot distinguish between when to use instance-level vs aggregate-level grounding, or if it consistently chooses inefficient levels

### Mechanism 3
- Claim: Grounding rules provide systematic guidance for connecting language to internal representations
- Mechanism: The paper proposes specific rules (e.g., personal experiences ground at instance level, general concepts at aggregate level) that create a systematic framework for how language elements map to internal world model representations
- Core assumption: A formal set of rules can capture the complex relationship between natural language and internal representations
- Evidence anchors:
  - [section 2.3] "We now propose a set of grounding rules" followed by five specific rules for grounding
  - [section 3] Detailed discussion of how different parts of speech (nouns, verbs, adjectives, adverbs) can be grounded
- Break condition: If the rules prove too rigid to handle the full complexity of natural language or if they fail to capture edge cases

## Foundational Learning

- Concept: Internal world model representation
  - Why needed here: The entire grounding framework depends on having rich internal representations that capture both sensorimotor experiences and abstract concepts
  - Quick check question: Can you explain how a visual scene (si) and a feeling (fi) would be represented and linked in the internal world model?

- Concept: Multi-modal grounding
  - Why needed here: The framework explicitly addresses how different types of sensory information (visual, auditory, etc.) and different types of language elements (concrete nouns, abstract nouns, verbs) all need to be grounded
  - Quick check question: How would you ground the abstract noun "bravery" differently from the concrete noun "dog" according to the framework?

- Concept: Incremental learning and knowledge accumulation
  - Why needed here: The framework assumes the intelligent being continuously builds and updates its internal world model through experiences, which is essential for grounding new language
  - Quick check question: According to the paper, what are the key challenges in ensuring the internal world model can be continuously updated without losing previous knowledge?

## Architecture Onboarding

- Component map: Sensorimotor input processors -> Language parser -> Internal world model -> Grounding rule engine -> Learning system -> Source verification module
- Critical path: Language input → Parsing → Grounding rule application → Internal representation mapping → Plausibility checking → Response generation
- Design tradeoffs:
  - Richness vs efficiency: More detailed internal representations enable better grounding but require more computational resources
  - Instance vs aggregate grounding: Balancing between specific personal experiences and generalized concepts
  - Static vs dynamic world model: Tradeoff between a fixed knowledge base and continuous learning capability
- Failure signatures:
  - Inability to ground novel concepts that lack sensorimotor basis
  - Over-reliance on aggregate-level grounding when instance-level would be more appropriate
  - Inconsistent application of grounding rules across different language contexts
- First 3 experiments:
  1. Test grounding of concrete nouns by presenting known and novel objects and measuring grounding success rates
  2. Test the multi-level grounding system by presenting scenarios that could be grounded at different levels of abstraction
  3. Test the learning system's ability to incorporate new experiences into the internal world model without catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a flexible and powerful representation of the external world and internal emotional world that can capture fine-grained details of the real and dynamic environment?
- Basis in paper: [explicit] The paper states that this is perhaps the most important question about representations and discusses the need for a representation that can simulate the dynamic real world environment for AI agents' performance tasks.
- Why unresolved: Current neural networks are uninterpretable and cannot simulate the dynamic real world activities, while human thinking involves virtually manipulating physical objects in the brain to simulate real world scenarios.
- What evidence would resolve it: A new representation framework that can capture fine-grained details of the real world and enable AI agents to simulate dynamic environments for tasks like reasoning and planning.

### Open Question 2
- Question: How can we enable incremental and continuous learning in AI agents to overcome forgetting of previous knowledge and leverage previously learned knowledge to help learn new tasks better?
- Basis in paper: [explicit] The paper discusses the need for continual learning as the world is dynamic and full of unknowns, and highlights the challenges of overcoming forgetting and leveraging previous knowledge.
- Why unresolved: Although research has been done on continual learning in neural networks, the key challenges of forgetting and leveraging previous knowledge still remain.
- What evidence would resolve it: A learning framework that can continuously update knowledge, overcome forgetting, and leverage previous knowledge to improve learning of new tasks in dynamic environments.

### Open Question 3
- Question: How can we characterize and ground the knowledge learned in LLMs, and what is the theoretical limit of LLMs?
- Basis in paper: [explicit] The paper raises questions about how to characterize the knowledge in LLMs, how to ground it, and what the theoretical limit of LLMs is.
- Why unresolved: LLMs learn by next word prediction and generation without the grounding described in the paper, and there is still no formal definition of awareness for LLMs.
- What evidence would resolve it: A framework for characterizing and grounding LLM knowledge, and an understanding of the theoretical limits of LLMs in terms of their capabilities and limitations.

## Limitations
- The paper presents a theoretical framework rather than an implemented system, making empirical validation impossible
- The internal world model's representational format remains unspecified, and the plausibility checking mechanism lacks concrete implementation details
- The framework assumes all concepts can be grounded in sensorimotor experiences, but this may not hold for purely abstract mathematical or logical concepts

## Confidence
- **High Confidence**: The claim that grounding is central to artificial general intelligence is well-supported by cognitive science literature
- **Medium Confidence**: The proposed grounding rules provide a reasonable starting framework, but their practical effectiveness remains unproven
- **Low Confidence**: The specific implementation details for the internal world model and the plausibility checking mechanism are too vague to assess their effectiveness or feasibility

## Next Checks
1. Implement a minimal internal world model with instance-level and aggregate-level representations, then test grounding success rates for a controlled vocabulary of concrete and abstract nouns
2. Design an empirical study comparing grounding-based language understanding against standard LLM approaches on tasks requiring real-world common sense reasoning
3. Develop a prototype plausibility checker using the grounding rules, then systematically test edge cases where grounding should fail to verify the framework's robustness