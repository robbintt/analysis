---
ver: rpa2
title: 'Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language
  Models'
arxiv_id: '2312.02219'
source_url: https://arxiv.org/abs/2312.02219
tags:
- visual
- language
- image
- object
- it-lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERLIM is a multi-modal evaluation benchmark for large vision-language
  models (LVLMs) designed to assess their effectiveness in fundamental computer vision
  tasks. The benchmark contains over 279K image-question pairs and focuses on detecting
  cross-modal "hallucinations" where language outputs are not grounded in the visual
  input.
---

# Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models

## Quick Facts
- arXiv ID: 2312.02219
- Source URL: https://arxiv.org/abs/2312.02219
- Reference count: 40
- Key outcome: MERLIM reveals that state-of-the-art LVLMs struggle with visual grounding, exhibiting object hallucinations and prompt sensitivity despite achieving seemingly correct answers through language priors.

## Executive Summary
MERLIM is a multi-modal evaluation benchmark designed to assess how well large vision-language models (LVLMs) perform fundamental computer vision tasks like object recognition, counting, and relationship understanding. The benchmark contains over 279K image-question pairs and introduces a novel methodology to detect "hidden hallucinations" - cases where models give correct answers but lack visual grounding. By systematically editing out specific objects and measuring whether predictions change, MERLIM exposes that current LVLMs often rely on language biases and global visual patterns rather than actual visual content.

## Method Summary
MERLIM evaluates LVLMs on three core tasks using the MS-COCO validation set with extended vocabulary from LVIS and relationships from Visual Genome. The benchmark employs a systematic approach where each object instance is removed through inpainting, creating edited images for comparison. Models are queried with five semantically equivalent prompts to assess prompt sensitivity. For relationship understanding, the benchmark compares LVLM performance against LLM-only baselines to identify language priors. Evaluation metrics include F1 score for object recognition, mean absolute error for counting, and accuracy for relationship understanding, with additional analysis of hallucination rates and prompt consistency.

## Key Results
- State-of-the-art LVLMs show significant object hallucinations across all tasks, often predicting objects not present in images
- Models exhibit high sensitivity to small variations in semantically equivalent prompts, indicating reliance on language cues
- Edited images reveal that LVLMs frequently make correct predictions without visual grounding, a phenomenon termed "hidden hallucinations"
- BLIP2 demonstrates the most consistent performance across prompts, while other models show significant variability

## Why This Works (Mechanism)

### Mechanism 1
The benchmark exposes "hidden hallucinations" by systematically editing out specific object instances and comparing model predictions on original vs. edited images. For each object instance, the system removes it using inpainting, then queries the model with identical prompts. If the model still predicts the removed object, this indicates a hallucination not grounded in the visual input. Core assumption: The model's predictions should change when visual evidence is removed; if they don't, the prediction was based on spurious correlations rather than actual visual content.

### Mechanism 2
The benchmark controls for language bias by using multiple semantically equivalent prompts and measuring performance variability. The same visual task is evaluated using five different prompts that ChatGPT considers semantically identical. Performance consistency across prompts indicates whether the model is relying on visual grounding or being influenced by prompt phrasing. Core assumption: A model with strong visual grounding should produce similar predictions regardless of how the question is phrased.

### Mechanism 3
The benchmark distinguishes between visual grounding and language priors by comparing LLM-only performance to IT-LVLM performance on the same questions. The same relationship questions are posed to a pure LLM without visual input. Performance differences indicate how much the IT-LVLM is relying on visual information versus language priors built into the LLM component. Core assumption: If an IT-LVLM performs similarly to its underlying LLM on visual relationship questions, it suggests the model is not effectively using visual information.

## Foundational Learning

- **Cross-modal hallucination detection**: Why needed here - The benchmark specifically targets cases where language outputs are not grounded in visual input. Quick check question: If an object is removed from an image through inpainting and the model still predicts it, is this a visual grounding failure or a general object recognition failure?

- **Semantic equivalence vs. prompt sensitivity**: Why needed here - The benchmark uses multiple prompts that are semantically equivalent but may trigger different model responses. Quick check question: If a model gives different answers to "List the objects in the image" vs. "Itemize the things seen in the image," does this indicate poor visual grounding or sensitivity to language cues?

- **Visual grounding assessment methodology**: Why needed here - The benchmark employs specific techniques (inpainting, object removal, comparison of original vs. edited images) to assess whether predictions are based on actual visual content. Quick check question: What criteria must be met to ensure that object removal through inpainting doesn't leave detectable traces?

## Architecture Onboarding

- **Component map**: Image preprocessing pipeline (inpainting/object removal) -> Prompt generation module (5 semantically equivalent questions) -> IT-LVLM inference engine (processes image-prompt pairs) -> Response parsing component (extracts structured predictions) -> Evaluation module (compares predictions to ground truth)
- **Critical path**: The most time-consuming operation is the inpainting process for object removal, which must be performed for each object instance in each image. This should be parallelized across multiple GPUs or machines.
- **Design tradeoffs**: Using inpainting to remove objects provides cleaner experimental control than simply masking, but it's computationally expensive and may introduce artifacts that affect model predictions. The benchmark trades computational efficiency for experimental rigor.
- **Failure signatures**: High hallucination rates across all models suggest either fundamental limitations in current IT-LVLM architectures or problems with the benchmark methodology itself. Consistent performance drops on edited images across all models indicate genuine visual grounding issues rather than model-specific failures.
- **First 3 experiments**:
  1. Run the object recognition task on a small subset (100 images) with all five prompts to verify prompt consistency patterns before scaling up
  2. Test the inpainting pipeline on a few object instances to ensure the removed objects are truly undetectable by both YOLOv7 and human inspection
  3. Compare LLM-only vs. IT-LVLM performance on the relationship task with a small validation set to confirm the methodology for detecting language priors

## Open Questions the Paper Calls Out

### Open Question 1
How can instruction tuning be optimized to improve visual grounding in large vision-language models while maintaining their zero-shot capabilities? Basis in paper: Explicit - The paper highlights that state-of-the-art IT-LVLMs struggle with visual grounding, often relying on language biases or global visual patterns instead of accurate object recognition. What evidence would resolve it: Experiments comparing IT-LVLMs with different instruction tuning strategies and their performance on benchmarks like MERLIM.

### Open Question 2
What are the key factors contributing to object hallucinations in instruction-tuned large vision-language models, and how can they be mitigated? Basis in paper: Explicit - The paper identifies object hallucinations as a significant issue, where IT-LVLMs predict objects not present in the image. What evidence would resolve it: Analysis of hallucination patterns across different IT-LVLMs and datasets, along with ablation studies to identify root causes.

### Open Question 3
How does the sensitivity of instruction-tuned large vision-language models to small variations in input queries affect their practical usability, and what strategies can reduce this sensitivity? Basis in paper: Explicit - The paper notes that IT-LVLMs are biased by small variations in input queries, even when queries have the same semantics. What evidence would resolve it: Studies on the consistency of IT-LVLMs across semantically similar queries and techniques to improve robustness.

## Limitations
- The benchmark's reliance on inpainting may introduce artifacts that affect model predictions, though YOLOv7 verification claims complete removal
- Prompt variability may conflate language bias with task ambiguity, as five semantically equivalent prompts might not capture all sources of model inconsistency
- The LLM-only baseline comparison may not fully isolate language priors from visual grounding effects due to differences in how LVLMs process multimodal inputs

## Confidence
- **High**: Object recognition and counting task results are reproducible given standard evaluation protocols
- **Medium**: Relationship understanding task confidence depends on the validity of the LLM-only baseline comparison
- **Low**: "Hidden hallucination" phenomenon claims require further validation due to potential confounding factors in the inpainting methodology

## Next Checks
1. Test whether models trained with different visual encoders (e.g., CLIP vs. BLIP) show different hallucination patterns when presented with the same edited images
2. Conduct ablation studies varying the number of prompt variations to determine the minimum required for reliable bias detection
3. Compare MERLIM results with alternative grounding assessment methods like Grad-CAM visualization to verify hallucination detection accuracy