---
ver: rpa2
title: 'FM-G-CAM: A Holistic Approach for Explainable AI in Computer Vision'
arxiv_id: '2312.05975'
source_url: https://arxiv.org/abs/2312.05975
tags:
- saliency
- fm-g-cam
- class
- grad-cam
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FM-G-CAM, a novel explainable AI approach
  for computer vision that addresses the limitation of existing saliency map methods
  which only focus on a single target class. FM-G-CAM generates fused multi-class
  gradient-weighted class activation maps by considering multiple top predicted classes
  instead of just the top-1 prediction.
---

# FM-G-CAM: A Holistic Approach for Explainable AI in Computer Vision

## Quick Facts
- arXiv ID: 2312.05975
- Source URL: https://arxiv.org/abs/2312.05975
- Reference count: 40
- Key outcome: Novel explainable AI approach that generates fused multi-class gradient-weighted class activation maps by considering multiple top predicted classes instead of just top-1 prediction

## Executive Summary
FM-G-CAM addresses a fundamental limitation in existing saliency map methods by generating fused multi-class gradient-weighted class activation maps that consider multiple top predicted classes rather than focusing solely on the highest probability class. The method mathematically combines gradient-weighted activations from the top-K classes (typically 3), applies L2 normalization, and fuses them into a single saliency map. Through practical applications including general image classification and medical AI (chest X-ray diagnosis), the authors demonstrate that FM-G-CAM provides more holistic explanations compared to traditional Grad-CAM, particularly in scenarios where multiple classes may be relevant to the prediction.

## Method Summary
FM-G-CAM is an explainable AI approach that extends gradient-weighted class activation mapping (Grad-CAM) to multiple classes. The method generates saliency maps by calculating gradient-weighted activation maps for each of the top-K predicted classes, applying element-wise max filtering across these maps, and then performing L2 normalization on the concatenated result. This produces a single fused saliency map that highlights areas relevant to multiple potential predictions simultaneously. The approach is implemented in an open-source Python library and is particularly valuable for domains like medical diagnosis where understanding model reasoning across multiple potential classes is crucial.

## Key Results
- FM-G-CAM generates fused multi-class gradient-weighted class activation maps by considering multiple top predicted classes instead of just top-1 prediction
- The method mathematically combines gradient-weighted activations from top-K classes, applies L2 normalization, and fuses them into a single saliency map
- Demonstrated improved holistic explanations compared to traditional Grad-CAM in medical AI applications, particularly chest X-ray diagnosis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FM-G-CAM improves saliency map comprehensiveness by fusing gradient-weighted activations from multiple top predicted classes
- Mechanism: The method calculates gradient-weighted activation maps for each of the top-K classes, filters to keep only the highest activation per pixel across classes, then L2-normalizes the fused result
- Core assumption: Multiple top predictions often contain complementary information about what the CNN is "looking at" when making its decision
- Evidence anchors: "FM-G-CAM generates fused multi-class gradient-weighted class activation maps by considering multiple top predicted classes instead of just the top-1 prediction"; "FM-G-CAM targets multiple classes instead of a single class which affect the final prediction results"
- Break condition: If the top-K classes are highly similar or overlapping in their activation patterns, the fused map may not provide additional information beyond single-class approaches

### Mechanism 2
- Claim: L2 normalization across classes improves visibility of lower-weighted activations while reducing noise
- Mechanism: After fusing the top-K class activation maps and filtering to keep maximum values, the concatenated matrix undergoes L2 normalization
- Core assumption: Raw gradient-weighted activations can have widely varying magnitudes across different classes, making some relevant areas visually insignificant
- Evidence anchors: "L2-Normalisation is performed on a concatenated K-dimensional matrix... This allows filtering to be done based on raw gradient-weighted activations, and independent of the normalisation process"; "L2-Normalisation across the class gradient maps makes FM-G-CAM sensitive and unbiased, especially in the presence of classes with relatively lower weighted activations"
- Break condition: Excessive normalization could over-amplify noise or create artificial emphasis on irrelevant areas

### Mechanism 3
- Claim: Considering multiple classes addresses the gap between top-1 and top-5 accuracy by capturing model uncertainty
- Mechanism: By generating saliency maps based on multiple top predictions rather than just the highest probability class, FM-G-CAM visually represents the model's "consideration set" during classification
- Core assumption: The areas of focus for closely-ranked classes are often related and collectively represent the model's reasoning process
- Evidence anchors: "The difference in the top-1 and top-5 accuracy rates of models trained on ImageNet argues that even the most accurate models are not always able to predict the correct class as the top prediction"; "The saliency map is thus highly dependent on the model output with the highest probability"
- Break condition: In binary classification or when top predictions are vastly different, multi-class fusion may add complexity without benefit

## Foundational Learning

- Concept: Gradient-weighted class activation mapping (Grad-CAM)
  - Why needed here: FM-G-CAM builds directly on Grad-CAM methodology by extending it to multiple classes
  - Quick check question: What are the two main components that Grad-CAM combines to generate saliency maps?

- Concept: L2 normalization and its effect on feature maps
  - Why needed here: The normalization step is critical for FM-G-CAM's ability to highlight lower-weighted activations
  - Quick check question: How does L2 normalization affect the relative scale of activations in a feature map?

- Concept: CNN prediction uncertainty and top-K accuracy
  - Why needed here: Understanding why multiple classes matter requires grasping the difference between top-1 and top-5 accuracy
  - Quick check question: Why might a model's top-5 accuracy be significantly higher than its top-1 accuracy?

## Architecture Onboarding

- Component map: Input image -> Pre-trained CNN model -> Forward pass for predictions -> Backward pass for gradients -> Generate K gradient-weighted activation maps -> Element-wise max filtering -> L2 normalization -> Apply ReLU -> Output fused saliency map

- Critical path:
  1. Forward pass to get predictions and identify top-K classes
  2. Backward pass to compute gradients for each of the K classes
  3. Generate K gradient-weighted activation maps
  4. Apply max-filtering across maps
  5. Concatenate and L2-normalize
  6. Apply ReLU and visualize

- Design tradeoffs:
  - K value selection: Higher K provides more comprehensive explanation but may introduce noise; the paper recommends K=3 or 4
  - Computational cost: Scales linearly with K since each class requires separate gradient computation
  - Visualization complexity: Single fused map is cleaner than multiple separate maps but may obscure class-specific details

- Failure signatures:
  - All-white or all-black saliency maps indicate gradient calculation issues
  - Maps that don't align with image content suggest incorrect class selection or gradient weighting problems
  - Excessive noise after normalization may indicate inappropriate K value or model instability

- First 3 experiments:
  1. Compare FM-G-CAM output with Grad-CAM for a single class to verify the method extends correctly
  2. Test different K values (1, 3, 5) on the same image to observe how comprehensiveness changes
  3. Apply to a medical imaging dataset (like chest X-rays) where top-1/top-5 differences are clinically meaningful

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of K (number of classes to consider) for different types of classification tasks?
- Basis in paper: The authors state "The top 3 or top 5 classes are recommended to be used with FM-G-CAM" and discuss factors affecting this choice including "the nature of the use case, average area of interest on the image for each predictive class, and average number of distinct predictive labels per image."
- Why unresolved: The paper provides recommendations but doesn't present empirical evidence or systematic evaluation of different K values across various task types or datasets
- What evidence would resolve it: Controlled experiments comparing FM-G-CAM performance with different K values (e.g., 2, 3, 5, 10) across multiple datasets and task types, measuring both explanation quality and computational efficiency

### Open Question 2
- Question: How does FM-G-CAM perform on non-image classification tasks like document analysis or climate prediction?
- Basis in paper: The discussion section mentions "Further analysis could also be performed, such as for the use cases of document analysis[49], climate predictions[50]" suggesting this is an open direction
- Why unresolved: The paper only demonstrates FM-G-CAM on image classification and medical X-ray tasks, not on other potential application domains
- What evidence would resolve it: Empirical studies applying FM-G-CAM to document layout classification, climate data prediction, or other non-image classification tasks, with comparative performance metrics

### Open Question 3
- Question: How does the noise sensitivity of FM-G-CAM compare to Grad-CAM when using different activation functions?
- Basis in paper: The implementation section shows "comparative results of the use of the different activation functions available" with GeLU and ELU attracting "more noise than ReLU"
- Why unresolved: While the authors observe differences in noise levels, they don't quantify the trade-off between noise sensitivity and sensitivity to lower activations, nor do they compare this to Grad-CAM's behavior
- What evidence would resolve it: Systematic quantitative comparison of noise levels and sensitivity metrics for FM-G-CAM using different activation functions versus Grad-CAM across multiple test cases

## Limitations

- The method's effectiveness appears to be highly dependent on the specific CNN architecture used, with only ResNet50V2 and DenseNet being tested
- Claims of improved holistic explanations are primarily based on qualitative visual comparisons rather than quantitative metrics
- The selection of K=3 as the optimal value lacks rigorous empirical validation across diverse datasets and tasks

## Confidence

- **High confidence**: The mathematical formulation of the FM-G-CAM algorithm and its implementation in the Python package
- **Medium confidence**: The claim that FM-G-CAM provides more comprehensive explanations than single-class approaches, based on visual inspection
- **Low confidence**: The assertion that this method is particularly valuable for medical AI without quantitative validation on clinical outcomes

## Next Checks

1. Conduct experiments comparing FM-G-CAM saliency maps against Grad-CAM using quantitative metrics such as pointing game accuracy or region perturbation analysis to objectively measure explanation quality improvements

2. Test FM-G-CAM across a broader range of CNN architectures (Vision Transformers, EfficientNet, MobileNet) to assess whether the method's effectiveness is architecture-dependent or generalizable

3. For the medical AI claims, perform a study where radiologists evaluate whether FM-G-CAM explanations improve diagnostic accuracy or confidence compared to standard methods, including statistical significance testing