---
ver: rpa2
title: ConViViT -- A Deep Neural Network Combining Convolutions and Factorized Self-Attention
  for Human Activity Recognition
arxiv_id: '2310.14416'
source_url: https://arxiv.org/abs/2310.14416
tags:
- attention
- spatial
- transformer
- architecture
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ConViViT, a hybrid architecture combining convolutional
  neural networks (CNNs) and transformers for human activity recognition in RGB videos.
  The method uses a CNN module to enhance spatial representation and a factorized
  self-attention transformer to extract spatiotemporal features.
---

# ConViViT -- A Deep Neural Network Combining Convolutions and Factorized Self-Attention for Human Activity Recognition

## Quick Facts
- arXiv ID: 2310.14416
- Source URL: https://arxiv.org/abs/2310.14416
- Reference count: 33
- Primary result: Achieves 90.05% accuracy on HMDB51, 99.6% on UCF101, and 95.09% on ETRI-Activity3D

## Executive Summary
This paper introduces ConViViT, a hybrid deep learning architecture that combines convolutional neural networks (CNNs) with factorized self-attention transformers for human activity recognition in RGB videos. The approach uses a CNN module to enhance spatial representation by generating a 128-channel video that separates the human subject from background, followed by a transformer module with factorized self-attention that processes spatiotemporal features. The model achieves state-of-the-art performance on three benchmark datasets, demonstrating the effectiveness of combining CNN preprocessing with transformer-based temporal modeling.

## Method Summary
ConViViT processes RGB video through a two-stage pipeline: first, a CNN module with 3D convolutional layers (DW 3D-CNN 3×3×3, 3D-CNN 1×1×1, 3D-CNN 5×5×5, 3D-CNN 1×1×1) transforms the video into a 128-channel representation that enhances spatial features and isolates the human subject. Second, the transformer module with factorized self-attention divides the video into 16×16 patches, applies spatial attention first followed by temporal attention, and uses this staged approach to reduce computational complexity while capturing long-range dependencies. The architecture is trained using cross-entropy loss on the HMDB51, UCF101, and ETRI-Activity3D datasets.

## Key Results
- Achieves 90.05% accuracy on HMDB51, significantly outperforming previous state-of-the-art methods
- Reaches 99.6% accuracy on UCF101, demonstrating strong performance on large-scale activity recognition
- Obtains 95.09% accuracy on ETRI-Activity3D, showing effectiveness on depth-based activity datasets
- Ablation study confirms both CNN preprocessing and factorized self-attention transformer contribute significantly to performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorized self-attention improves spatiotemporal representation by decomposing attention into spatial and temporal stages
- Mechanism: The transformer applies attention along spatial dimensions first, then uses the output as input for temporal attention. This staged approach reduces computational complexity from quadratic to linear while preserving long-range dependencies.
- Core assumption: Spatial and temporal relationships can be effectively separated without losing critical interdependencies
- Evidence anchors:
  - [abstract] "Factorized Self-attention first applies spatial attention, followed by temporal attention on the output of the spatial attention"
  - [section] "Factorized Self-attention first applies spatial attention, followed by temporal attention on the output of the spatial attention"
  - [corpus] No direct evidence in corpus; claim based on cited work [2]
- Break condition: If spatial-temporal interdependencies cannot be meaningfully separated, or if the ordering of spatial-then-temporal attention is suboptimal for certain activity patterns

### Mechanism 2
- Claim: 3D convolutional preprocessing enhances transformer input quality by providing compact spatial tokens
- Mechanism: The CNN module transforms raw RGB video into a 128-channel representation that isolates the human subject from background, reducing the spatial complexity that the transformer must process while preserving relevant features
- Core assumption: CNNs are superior to transformers for low-level spatial feature extraction
- Evidence anchors:
  - [abstract] "we suggest employing a CNN network to enhance the video representation by generating a 128-channel video that effectively separates the human performing the activity from the background"
  - [section] "The primary objective of this CNN module is to enhance the video representation by extracting spatial tokens"
  - [corpus] Weak evidence; no corpus papers directly support this specific claim about CNNs improving transformer performance
- Break condition: If the CNN preprocessing introduces artifacts or removes discriminative spatial information, or if the background separation is not necessary for certain activity recognition tasks

### Mechanism 3
- Claim: Patch embedding with 16×16 patches provides sufficient spatial resolution while maintaining computational efficiency
- Mechanism: The input video is divided into non-overlapping patches that serve as tokens for the transformer, with position information encoded to preserve spatiotemporal order
- Core assumption: 16×16 patches strike an optimal balance between spatial resolution and token count
- Evidence anchors:
  - [section] "The main purpose of Patch Embedding is to provide the order information to the transformer by slicing the input into 16 × 16 patches"
  - [section] "Our proposed patch embedding block is inspired by the design and implementation of Uniformer's Dynamic Position Embedding (DPE) architecture"
  - [corpus] No direct evidence in corpus; claim based on cited work [1]
- Break condition: If 16×16 patches are too coarse to capture relevant spatial details, or if the patch boundary artifacts interfere with activity recognition

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: Understanding how transformers compute relationships between tokens is critical for grasping the factorized attention approach
  - Quick check question: What is the computational complexity of standard self-attention, and how does factorized attention reduce this?

- Concept: 3D convolutions for spatiotemporal feature extraction
  - Why needed here: The CNN module uses 3D convolutions to extract spatial and short-term temporal features before the transformer processes longer-term dependencies
  - Quick check question: How do 3D convolutions differ from 2D convolutions in their ability to capture temporal information?

- Concept: Vision transformer architecture fundamentals
  - Why needed here: The transformer module is adapted from ViViT, so understanding the base architecture is essential
  - Quick check question: What are the key differences between ViT for images and ViViT for videos in terms of tokenization strategy?

## Architecture Onboarding

- Component map: Input RGB video -> CNN Module (DW 3D-CNN 3×3×3, 3D-CNN 1×1×1, 3D-CNN 5×5×5, 3D-CNN 1×1×1) -> Patch Embedding (16×16 patches) -> Transformer Module (Factorized Self-Attention) -> Classification Head

- Critical path: Input -> CNN Module -> Patch Embedding -> Factorized Self-Attention Transformer -> Classification Head

- Design tradeoffs:
  - Spatial resolution vs. computational efficiency (16×16 patches)
  - CNN preprocessing depth vs. information loss
  - Factorized attention ordering (spatial-then-temporal vs. temporal-then-spatial)
  - 128-channel representation capacity vs. model complexity

- Failure signatures:
  - Poor performance on activities requiring fine spatial detail (patches too coarse)
  - Degraded performance when background information is relevant (excessive foreground isolation)
  - Computational inefficiency despite factorized approach (imbalanced attention computation)
  - Overfitting on smaller datasets (excessive model capacity)

- First 3 experiments:
  1. Compare single CNN block vs. double CNN block configuration on HMDB51 to validate the ablation study findings
  2. Swap factorized self-attention with factorized dot-product attention while keeping all other components constant
  3. Test different patch sizes (8×8, 16×16, 32×32) to find optimal spatial resolution for the target datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ConViViT change with different input video resolutions?
- Basis in paper: [inferred] The paper mentions the use of Patch Embedding to slice input into 16 × 16 patches, but does not discuss the impact of different input resolutions on performance.
- Why unresolved: The paper does not provide experimental results or analysis on how varying input video resolutions affect the model's accuracy and efficiency.
- What evidence would resolve it: Conduct experiments with different input resolutions (e.g., 224×224, 448×448, 512×512) and compare the accuracy and computational requirements on benchmark datasets.

### Open Question 2
- Question: Can the factorized self-attention mechanism be further optimized for better temporal modeling?
- Basis in paper: [explicit] The paper compares factorized self-attention with factorized dot-product attention and shows that the former performs better, but does not explore further optimizations.
- Why unresolved: The paper demonstrates the superiority of factorized self-attention but does not investigate potential improvements in its temporal modeling capabilities.
- What evidence would resolve it: Experiment with alternative temporal attention mechanisms or modifications to the existing factorized self-attention to improve its ability to capture long-term dependencies in video sequences.

### Open Question 3
- Question: How does ConViViT perform on datasets with more diverse action categories or longer video sequences?
- Basis in paper: [inferred] The paper evaluates ConViViT on three benchmark datasets with limited action categories and does not explore its performance on more diverse or complex datasets.
- Why unresolved: The paper's evaluation is limited to specific datasets, and it does not provide insights into the model's scalability or generalization to more challenging scenarios.
- What evidence would resolve it: Test ConViViT on larger-scale datasets with a higher number of action categories or longer video sequences (e.g., Kinetics-400, Something-Something V2) and analyze its performance in terms of accuracy and computational efficiency.

## Limitations

- Implementation details missing: Critical hyperparameters, CNN architecture specifics, and data augmentation strategies are not provided
- Performance verification challenges: State-of-the-art claims cannot be independently verified without code or pretrained models
- Limited generalization evidence: Performance on diverse datasets with varying complexity is not demonstrated
- Assumption dependencies: Relies on assumptions about CNN superiority for spatial preprocessing that lack strong empirical support

## Confidence

- Hybrid architecture effectiveness: Medium confidence - The conceptual framework is sound, but lacks complete implementation details for full verification
- Factorized self-attention superiority: Medium confidence - The mechanism is theoretically justified, but comparative results need independent replication
- State-of-the-art performance claims: Low confidence - Results are impressive but cannot be fully verified without code/data release
- CNN enhancement of transformer input: Low confidence - The assumption that CNNs improve transformer performance for this task lacks strong empirical support in the corpus

## Next Checks

1. Reproduce HMDB51 results using the exact CNN-transformer architecture with publicly available RGB videos, testing whether 90.05% accuracy is achievable without access to the original implementation details
2. Compare attention mechanisms by implementing factorized self-attention alongside standard self-attention and factorized dot-product attention using identical CNN preprocessing and training protocols
3. Test patch size sensitivity by training identical models with 8×8, 16×16, and 32×32 patches on all three benchmark datasets to empirically determine optimal spatial resolution tradeoffs