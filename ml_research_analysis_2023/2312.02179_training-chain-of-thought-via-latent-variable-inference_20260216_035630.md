---
ver: rpa2
title: Training Chain-of-Thought via Latent-Variable Inference
arxiv_id: '2312.02179'
source_url: https://arxiv.org/abs/2312.02179
tags:
- answer
- rationales
- trice
- gradient
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for training large language models
  to generate step-by-step rationales (chains-of-thought) that lead to correct answers,
  without requiring hand-labeled rationales. The method, called TRICE, treats CoT
  generation as a latent-variable inference problem and optimizes a marginal likelihood
  objective using MCMC-EM.
---

# Training Chain-of-Thought via Latent-Variable Inference

## Quick Facts
- arXiv ID: 2312.02179
- Source URL: https://arxiv.org/abs/2312.02179
- Reference count: 40
- Primary result: TRICE outperforms baselines on GSM8K and BIG-Bench Hard by training LLMs to generate correct rationales without hand-labeled data

## Executive Summary
This paper introduces TRICE, a method for training large language models to generate step-by-step rationales (chains-of-thought) that lead to correct answers without requiring hand-labeled rationales. TRICE treats CoT generation as a latent-variable inference problem, using MCMC-EM to optimize marginal likelihood while maintaining a memory of rationales for each training example. The method employs control variates to reduce gradient variance and achieves state-of-the-art performance on GSM8K and BIG-Bench Hard datasets, even outperforming supervised tuning on human-generated rationales.

## Method Summary
TRICE combines Markov-chain Monte Carlo expectation-maximization with control-variate variance reduction to train LLMs on rationale generation. The method maintains a memory of rationales for each training example, updates them using an independence sampler that accepts correct rationales, and computes gradient estimates of the marginal likelihood. Control variates use incorrect rationales as negative examples to reduce variance, while systematic resampling reduces computational cost. The approach optimizes soft prompt parameters to generate rationales that lead to correct answers without requiring ground-truth rationales.

## Key Results
- Outperforms prompt-tuning with and without CoT, STaR, and rejection sampling on GSM8K and BIG-Bench Hard
- Achieves better performance than supervised tuning on human-generated rationales
- Learns to generate correct rationales for nearly all training examples (vs ~10% failure rate for STaR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCMC-EM optimizes marginal log-likelihood by averaging over latent rationales
- Mechanism: Maintains rationale memory and iteratively updates using independence sampler; accepts correct rationales to approximate posterior p(z|x,y)
- Core assumption: Independence sampler converges quickly relative to θ changes
- Break condition: Low accuracy causes near-zero acceptance probability and poor mixing

### Mechanism 2
- Claim: Control variate reduces gradient variance using incorrect rationales as negative examples
- Mechanism: Scales gradients by (1-βm) for correct rationales and negates/scaled by βm for incorrect ones
- Core assumption: Leave-one-out estimate βm provides unbiased, independent estimate
- Break condition: Poor βm estimation when model accuracy is moderate

### Mechanism 3
- Claim: Subsampling gradients reduces computational cost while maintaining unbiased estimates
- Mechanism: Samples L examples from M with probability proportional to weights (1-β for correct, β for incorrect)
- Core assumption: Marginal selection probability proportional to weight ensures unbiased estimation
- Break condition: Too small L or highly skewed weight distribution increases variance

## Foundational Learning

- **Concept: Markov Chain Monte Carlo (MCMC) sampling**
  - Why needed: Samples from intractable posterior p(z|x,y) over rationales
  - Quick check: What's the difference between independence sampler and Metropolis-Hastings?

- **Concept: Expectation-Maximization (EM) algorithm**
  - Why needed: Alternates between E-step (MCMC rationale updates) and M-step (gradient updates)
  - Quick check: How does MCMC-EM differ from standard EM with intractable expectations?

- **Concept: Control variates for variance reduction**
  - Why needed: Reduces high variance of basic gradient estimator using incorrect rationales
  - Quick check: What property must a control variate have to reduce variance without bias?

## Architecture Onboarding

- **Component map**: LLM generation → correctness check → memory update → gradient computation → parameter update
- **Critical path**: Question → LLM generation → correctness check → memory update → gradient computation → parameter update (correctness checker is most critical)
- **Design tradeoffs**: MCMC vs variational inference, control variate vs no control variate, subsampling vs full gradients
- **Failure signatures**: No updates if acceptance probability near zero, increased variance with poor βm estimation, memory stuck with incorrect rationales
- **First 3 experiments**: 1) Basic gradient estimator only, 2) Control variate with fixed β=0.5, 3) Subsampling with different L values

## Open Questions the Paper Calls Out

1. **Scalability with model size**: Effectiveness on different model sizes beyond evaluated PaLM 2-M remains unknown; requires systematic experiments across model sizes.

2. **Extension to other domains**: Potential application to tool-use problems mentioned but not demonstrated; requires concrete examples and experimental results in other domains.

3. **Comparison to other latent-variable methods**: Brief mention of variational inference and reweighted wake-sleep issues but no detailed performance comparison; requires comprehensive comparison on various tasks.

## Limitations

- Limited empirical validation scope to two datasets (GSM8K and BIG-Bench Hard)
- Computational complexity of maintaining rationale memory and running MCMC sampling
- Dependence on reliable "is-correct" function that may be expensive or imperfect for complex tasks

## Confidence

- **High confidence**: Theoretical framework combining MCMC-EM with control variate variance reduction is sound
- **Medium confidence**: Empirical results on tested datasets are convincing but may not generalize to other domains
- **Low confidence**: Control variate driving variance to zero "as model improves" may not hold in practice due to finite sampling

## Next Checks

1. Apply TRICE to diverse reasoning tasks beyond mathematics to assess cross-domain generalization
2. Conduct ablation study systematically disabling MCMC sampling, control variates, and subsampling
3. Track rationale diversity and quality in memory throughout extended training for long-term stability analysis