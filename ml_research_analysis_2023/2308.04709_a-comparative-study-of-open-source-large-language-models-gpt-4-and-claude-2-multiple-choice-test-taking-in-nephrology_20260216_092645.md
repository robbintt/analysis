---
ver: rpa2
title: 'A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude
  2: Multiple-Choice Test Taking in Nephrology'
arxiv_id: '2308.04709'
source_url: https://arxiv.org/abs/2308.04709
tags:
- questions
- llms
- score
- nephrology
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the performance of several open-source large
  language models (LLMs) and GPT-4/Claude 2 on nephSAP multiple-choice questions in
  nephrology. The open-source models (Koala 7B, Falcon 7B, Stable-Vicuna 13B, and
  Orca Mini 13B) achieved only 17.1-25.5% accuracy, while GPT-4 achieved 73.3% and
  Claude 2 achieved 54.4%.
---

# A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology

## Quick Facts
- arXiv ID: 2308.04709
- Source URL: https://arxiv.org/abs/2308.04709
- Reference count: 29
- Open-source LLMs (Koala 7B, Falcon 7B, Stable-Vicuna 13B, Orca Mini 13B) achieved only 17.1-25.5% accuracy on nephSAP questions vs GPT-4 at 73.3% and Claude 2 at 54.4%

## Executive Summary
This study evaluated multiple large language models on nephSAP multiple-choice questions in nephrology. Open-source models (Koala, Falcon, Stable-Vicuna, Orca Mini) significantly underperformed proprietary models GPT-4 and Claude 2, with accuracy rates of 17.1-25.5% compared to 73.3% and 54.4% respectively. The results demonstrate that current open-source LLMs struggle with zero-shot reasoning in medical domains, highlighting the need for domain-specific fine-tuning and better training data. While GPT-4 approached human-level performance in most nephrology topics, both open-source and proprietary models showed particular difficulty with quantitative reasoning in fluid and electrolyte questions.

## Method Summary
The study used 858 nephSAP multiple-choice questions from 2016-2023, excluding those with complex tables. Open-source models were evaluated using zero-shot inference with LLaMA tokenizer and LLaMAForCausalLM from HuggingFace, while GPT-4 and Claude 2 were accessed via API. Performance was measured using accuracy, BLEU score, word error rate (WER), and cosine similarity. All models received the same prompts without any examples or explanations.

## Key Results
- GPT-4 achieved 73.3% accuracy, outperforming Claude 2 (54.4%) and all open-source models (17.1-25.5%)
- GPT-4 scored ≥72% on 9 of 11 nephrology topics, while open-source models remained near random guessing (23.8%)
- Open-source models showed poor explanation quality with low BLEU and cosine similarity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 and Claude 2 outperform open-source LLMs on nephSAP questions due to superior reasoning and training on richer data
- Mechanism: Proprietary models trained on larger, more diverse, and higher-quality datasets (including third-party data) achieve better zero-shot reasoning
- Core assumption: Access to curated, non-publicly available medical data (textbooks, peer-reviewed articles) is critical for medical domain performance
- Evidence anchors:
  - [abstract] "GPT-4 achieved 73.3% and Claude 2 achieved 54.4%" vs. open-source models at 17.1–25.5%
  - [section] "GPT-4 is reported to have been trained on 1.7 trillion parameters... Claude 2 was trained on 860 million parameters"
  - [corpus] Weak – corpus contains no direct evidence about proprietary training data; only shows related work on LLMs in medicine

### Mechanism 2
- Claim: Open-source LLMs struggle with medical MCQs due to lack of domain-specific fine-tuning and conceptual reasoning
- Mechanism: Without targeted adaptation, models rely on surface-level pattern matching rather than deep medical understanding
- Core assumption: Domain-specific fine-tuning significantly improves performance on specialized medical tasks
- Evidence anchors:
  - [abstract] "open-source LLMs struggle with zero-shot reasoning in medical domains compared to proprietary models"
  - [section] "Models trained without specific optimization in specialized knowledge domains may yield suboptimal results on domain specific questions"
  - [corpus] Weak – related papers focus on evaluating LLMs in medicine but do not provide direct evidence of the impact of fine-tuning

### Mechanism 3
- Claim: Current open-source LLMs lack the ability to reason about complex, quantitative medical topics (e.g., fluid and electrolytes)
- Mechanism: Models generate text probabilistically without deep understanding of cause-effect and temporal-spatial relationships
- Core assumption: Quantitative reasoning in medicine requires explicit training on structured, numerical problem-solving data
- Evidence anchors:
  - [abstract] "GPT-4 achieved a score of ≥72% in 9 of 11 nephSAP Nephrology question topics" but electrolyte questions received only 55.2%
  - [section] "LLMs generate text based on probabilities... improving their inherent quantitative understanding remains an active area of research"
  - [corpus] Weak – corpus contains general evaluations of LLMs in medicine but no specific analysis of quantitative reasoning gaps

## Foundational Learning

- Concept: Zero-shot reasoning in medical domains
  - Why needed here: Key to understanding why proprietary models outperform open-source ones on nephSAP questions without domain-specific training
  - Quick check question: What is the primary limitation of zero-shot reasoning for open-source LLMs on specialized medical MCQs?

- Concept: Domain-specific fine-tuning
  - Why needed here: Essential for improving performance on specialized medical tasks beyond surface-level pattern matching
  - Quick check question: How does domain-specific fine-tuning improve LLM performance on medical MCQs compared to general training?

- Concept: Quantitative reasoning in medicine
  - Why needed here: Critical for understanding why models struggle with topics like fluid and electrolytes that require structured, numerical problem-solving
  - Quick check question: Why do current LLMs find quantitative medical reasoning more challenging than memorization-based tasks?

## Architecture Onboarding

- Component map: nephSAP question bank (858 questions) -> JSON parsing -> tokenization -> Model inference (Koala 7B, Falcon 7B, Stable-Vicuna 13B, Orca Mini 13B, GPT-4, Claude 2) -> Evaluation (accuracy, BLEU, WER, cosine similarity) -> Analysis

- Critical path: Data acquisition -> Preprocessing -> Model inference -> Evaluation -> Analysis

- Design tradeoffs:
  - Larger models (GPT-4) require more compute but yield higher accuracy
  - Open-source models are more accessible but underperform without fine-tuning
  - Complex tables omitted due to tokenization limits, reducing dataset completeness

- Failure signatures:
  - Empty or nonsensical outputs from open-source models
  - Low BLEU and cosine similarity scores indicating poor explanation quality
  - Accuracy near random guessing (23.8%) for open-source models

- First 3 experiments:
  1. Compare performance of open-source models with and without domain-specific fine-tuning on nephSAP questions
  2. Evaluate impact of increasing model size and parameter count on medical MCQ accuracy
  3. Test zero-shot reasoning on a subset of quantitative medical questions (e.g., fluid and electrolytes) to isolate reasoning gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific fine-tuning approaches could improve open-source LLM performance in medical domains?
- Basis in paper: [explicit] The study discusses the need for domain-specific fine-tuning and better training data, particularly for complex medical topics
- Why unresolved: The paper does not explore or test specific fine-tuning methodologies
- What evidence would resolve it: Comparative study of various fine-tuning approaches (e.g., parameter-efficient fine-tuning, domain-specific data augmentation) on open-source models, measuring performance gains on medical MCQs

### Open Question 2
- Question: How do open-source LLMs perform on medical questions requiring quantitative reasoning vs. factual recall?
- Basis in paper: [inferred] The paper notes that GPT-4 performed worse on electrolyte questions, which involve more quantitative reasoning
- Why unresolved: The study did not systematically analyze performance differences across question types requiring different reasoning skills
- What evidence would resolve it: Categorization and analysis of question types by reasoning complexity, comparing open-source and proprietary model performance across these categories

### Open Question 3
- Question: What is the impact of training data sources (public vs. proprietary) on LLM medical knowledge capabilities?
- Basis in paper: [explicit] The authors suggest that access to copyrighted medical material not in public domain may be a key differentiator
- Why unresolved: The study does not investigate the relationship between training data sources and model performance
- What evidence would resolve it: Analysis of training data composition across different models and correlation with performance on specialized medical knowledge tasks

## Limitations
- Study relies on zero-shot inference without domain-specific fine-tuning, which may artificially limit open-source model performance
- Complex tables were excluded from the dataset, potentially biasing results toward simpler question types
- No direct analysis of model training data composition or parameter counts beyond manufacturer claims

## Confidence
- High Confidence: GPT-4 and Claude 2 significantly outperform open-source models on nephSAP questions (73.3% vs 17.1-25.5%)
- Medium Confidence: Proprietary models' superior performance is primarily due to training data quality and size
- Medium Confidence: Domain-specific fine-tuning would substantially improve open-source model performance
- Low Confidence: Quantitative reasoning is the primary driver of performance gaps in medical topics

## Next Checks
1. Evaluate open-source models after domain-specific fine-tuning on nephrology training data to measure performance gains
2. Test model performance on medical questions with complex tables and numerical reasoning to assess handling of quantitative problems
3. Compare zero-shot vs few-shot performance across all models to determine minimum effective prompting strategies