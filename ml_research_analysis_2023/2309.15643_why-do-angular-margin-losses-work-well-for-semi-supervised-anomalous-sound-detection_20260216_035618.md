---
ver: rpa2
title: Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound
  Detection?
arxiv_id: '2309.15643'
source_url: https://arxiv.org/abs/2309.15643
tags:
- loss
- machine
- anomalous
- task
- compactness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates why angular margin losses work well for
  semi-supervised anomalous sound detection (ASD). The key finding is that minimizing
  an angular margin loss using an auxiliary classification task also minimizes intra-class
  compactness loss while maximizing inter-class compactness loss.
---

# Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?

## Quick Facts
- arXiv ID: 2309.15643
- Source URL: https://arxiv.org/abs/2309.15643
- Authors: [List of authors from the paper]
- Reference count: 40
- Key outcome: Angular margin losses with auxiliary classification tasks achieve 84.2% AUC on DCASE 2022 development set and 76.8% on evaluation set, significantly outperforming one-class loss approaches.

## Executive Summary
This paper investigates why angular margin losses work well for semi-supervised anomalous sound detection in noisy conditions. The key finding is that minimizing angular margin losses using auxiliary classification tasks simultaneously minimizes intra-class compactness loss while maximizing inter-class separation, preventing trivial solutions. This approach enables the model to learn embeddings that are robust to background noise by forcing the network to focus on machine-specific acoustic patterns rather than noise, which is common across classes. The method achieves state-of-the-art performance on DCASE 2022 and 2023 ASD datasets, demonstrating superior generalization to noisy conditions compared to generative or one-class models.

## Method Summary
The method trains a convolutional neural network with two sub-models processing log-mel magnitude spectrograms and magnitude spectra separately, then concatenates their embeddings. The sub-cluster AdaCos loss is applied with 16 sub-clusters per class, combined with an auxiliary classification task (machine types, sections, or attribute information). After training for 10 epochs with mixup augmentation, k-means clustering is applied to normal training samples to compute anomaly scores based on minimum cosine distance. The approach is evaluated on DCASE 2022 and 2023 ASD datasets using AUC and partial AUC metrics at low false positive rates.

## Key Results
- Sub-cluster AdaCos loss achieves 84.2% AUC on DCASE 2022 development set and 76.8% on evaluation set
- Angular margin losses significantly outperform one-class loss approaches for ASD in noisy conditions
- Using auxiliary classification tasks teaches models to ignore background noise and focus on machine-specific acoustic patterns
- Visualization of learned embeddings confirms angular margin losses help isolate target machine sounds while ignoring noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Angular margin losses minimize intra-class compactness loss while maximizing inter-class compactness loss, preventing trivial solutions.
- Mechanism: By normalizing embeddings and applying softmax over angular margins, the loss function implicitly enforces minimum angular separation between classes while clustering samples within each class, preventing constant function learning.
- Core assumption: The auxiliary task contains multiple meaningful classes with distinguishable acoustic characteristics.
- Evidence anchors: Theorem 6 shows minimizing angular margin loss using auxiliary tasks can be considered minimizing regularized one-class loss while being less affected by noise.
- Break condition: If auxiliary task has only one class or acoustically indistinguishable classes, model may still learn trivial solutions.

### Mechanism 2
- Claim: Using auxiliary classification task teaches model to ignore background noise and focus on machine-specific acoustic patterns.
- Mechanism: Model learns to differentiate between classes defined by auxiliary task. Since background noise is common across classes, it provides no discriminative information, forcing extraction of features specific to each machine type.
- Core assumption: Background noise is not class-specific (similar noise patterns across different machine types).
- Evidence anchors: Model learns to ignore noise which can be assumed similar for all considered classes, therefore isolating target machine sound.
- Break condition: If background noise contains class-specific information (each machine operates in unique acoustic environment), auxiliary task may not improve anomaly detection.

### Mechanism 3
- Claim: Angular margin losses provide better domain generalization by ensuring margin between classes.
- Mechanism: Angular margin explicitly enforces minimum angular separation between class centers in embedding space, helping maintain discriminative boundaries during domain shifts.
- Core assumption: Embedding space is normalized and margin is appropriately scaled (using AdaCos with adaptive scaling).
- Evidence anchors: Ensuring angular margin between classes slightly improves overall performance.
- Break condition: If margin is too large/small or embedding space not properly normalized, model may not generalize well to new domains.

## Foundational Learning

- Concept: **Compactness Loss**
  - Why needed here: Serves as baseline for measuring how well model clusters normal samples in embedding space.
  - Quick check question: What happens to compactness loss if model learns constant function mapping all inputs to same point?

- Concept: **Angular Margin Loss**
  - Why needed here: Combines benefits of compactness loss with explicit margin between classes, improving discrimination and preventing trivial solutions.
  - Quick check question: How does angular margin loss differ from standard cross-entropy loss in terms of geometry of embedding space?

- Concept: **Domain Generalization**
  - Why needed here: DCASE datasets involve domain shifts requiring model to detect anomalies across different acoustic environments and machine conditions.
  - Quick check question: What challenges arise when applying model trained on one domain to different domain with shifted acoustic characteristics?

## Architecture Onboarding

- Component map: Input (spectrograms and spectra) → Two convolutional sub-models → Concatenation → Sub-cluster AdaCos loss with 16 sub-clusters → Embedding → Anomaly Score (k-means clustering on normal samples)
- Critical path: Input → Backbone → Fusion → Loss → Embedding → Anomaly Score
- Design tradeoffs:
  - Using two input representations (spectrograms and spectra) improves performance but increases computational cost
  - Sub-clusters allow more complex class distributions but add hyperparameter tuning complexity
  - Mixup improves generalization but requires careful handling of label mixing
- Failure signatures:
  - If embeddings collapse to single point, model has learned trivial solution
  - If anomaly scores are random, model is not capturing meaningful acoustic patterns
  - If performance degrades significantly on target domains, model is overfitting to source domain characteristics
- First 3 experiments:
  1. Train with IC compactness loss only (no auxiliary task) to establish baseline performance
  2. Train with sub-cluster AdaCos loss and machine type classification to evaluate impact of simple auxiliary task
  3. Train with sub-cluster AdaCos loss and machine type + section + attribute classification to evaluate impact of complex auxiliary task

## Open Questions the Paper Calls Out

- Question: How do self-supervised learning tasks compare to supervised classification tasks as auxiliary tasks for ASD?
  - Basis in paper: [explicit] Planned for future work - investigating whether self-supervised auxiliary tasks improve resulting ASD performance
  - Why unresolved: Paper only evaluates supervised classification tasks as auxiliary tasks
  - What evidence would resolve it: Experiments comparing ASD performance when using self-supervised auxiliary tasks versus supervised classification tasks while controlling for other variables

- Question: What visualization methods beyond RISE and t-SNE could better highlight anomalous regions in input representations for ASD?
  - Basis in paper: [explicit] Planned for future work - sophisticated methods for visualizing anomalous regions of input representations should be developed
  - Why unresolved: Paper only uses RISE and t-SNE for visualization
  - What evidence would resolve it: Development and validation of new visualization techniques specifically designed for ASD that provide clearer localization of anomalous regions

- Question: How do angular margin losses perform on ASD datasets with class-specific noise distributions?
  - Basis in paper: [inferred] Paper assumes noise is not class-specific and notes auxiliary classification task will likely not improve results if class-specific noise is present
  - Why unresolved: Paper only evaluates angular margin losses on datasets where noise is assumed to be class-agnostic
  - What evidence would resolve it: Experiments comparing angular margin losses to other loss functions on ASD datasets specifically designed to have class-specific noise patterns

## Limitations

- Theoretical claims about angular margin losses preventing trivial solutions and improving noise robustness rely heavily on one paper and unproven extensions
- Mechanisms proposed for how auxiliary tasks teach noise-robust representations lack direct empirical validation through ablation studies
- Assumption that background noise is class-agnostic may not hold in all scenarios, particularly with distinct acoustic environments per machine

## Confidence

- High confidence: Experimental results showing sub-cluster AdaCos outperforms other loss functions on DCASE 2022/2023 datasets
- Medium confidence: Claim that angular margin losses minimize intra-class compactness while maximizing inter-class separation (lacks direct empirical validation)
- Low confidence: Proposed mechanisms for how auxiliary tasks teach noise-robust representations (no ablation studies on auxiliary task complexity)

## Next Checks

1. Ablation study on auxiliary task complexity: Train with sub-cluster AdaCos using only machine type classification, only section classification, only attribute classification, and combinations thereof to quantify contribution of each auxiliary task component.

2. Trivial solution verification: Intentionally remove angular margins from loss function and train to confirm whether model collapses to constant function, directly testing "preventing trivial solutions" claim.

3. Noise sensitivity analysis: Add controlled amounts of synthetic noise to test recordings and measure how different loss functions and auxiliary tasks affect performance degradation, validating noise-robustness claims.