---
ver: rpa2
title: 'OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for
  Reasoning Skills of Large Language Models'
arxiv_id: '2305.12001'
source_url: https://arxiv.org/abs/2305.12001
tags:
- reasoning
- skills
- prompting
- explanations
- netuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the role of explanations in finetuning and
  prompting for reasoning skills of large language models. The authors finetune OPT
  models with and without explanations on a reasoning corpus, and evaluate them on
  57 tasks from the SUPER-NATURALINSTRUCTIONS benchmark.
---

# OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models

## Quick Facts
- arXiv ID: 2305.12001
- Source URL: https://arxiv.org/abs/2305.12001
- Reference count: 8
- Key outcome: Explanations in few-shot examples have no significant impact on finetuned model performance but positively affect non-finetuned models, with numerical and analogical reasoning benefiting most

## Executive Summary
This paper investigates how explanations affect the reasoning capabilities of large language models through finetuning and prompting strategies. The authors finetune OPT models (1.3B, 6.7B, 13B parameters) with and without explanations on seven reasoning datasets, then evaluate performance across 57 tasks from the SUPER-NATURALINSTRUCTIONS benchmark covering 26 reasoning skills. The key finding is that while finetuning significantly improves reasoning skills like numerical and analogical reasoning, the presence or absence of explanations in few-shot examples during inference does not significantly impact finetuned models but does help non-finetuned models.

## Method Summary
The authors finetuned OPT models for 10 epochs using MetaSeq implementation on seven reasoning datasets (AQuA-RAT, CoQA, CoS-E, ECQA, ESNLI, GSM8K, ProofWriter, StrategyQA) with and without explanations. Evaluation used 57 tasks from SUPER-NATURALINSTRUCTIONS with four scoring functions (mean, unconditional-norm, suffix, sum) taking maximum accuracy. Models were tested under zero-shot, few-shot, and few-shot with explanations prompting conditions. The finetuning employed a uniform template with task definition, two random in-context examples, and an answer prefix.

## Key Results
- Finetuning on reasoning datasets leads to statistically significant improvements in seven reasoning skills
- Explanations in few-shot examples have no significant impact on finetuned model performance
- Non-finetuned models show substantial performance gains when explanations are included in few-shot examples
- Numerical and Analogical reasoning skills benefit most from explanations during finetuning and prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanations in few-shot examples have no significant impact on finetuned model performance
- Mechanism: When models are trained with explanations, they internalize reasoning patterns that make external explanations during inference redundant
- Core assumption: Internalized reasoning patterns from training outweigh prompt information during inference
- Evidence anchors: Abstract and section observations about lack of significant impact on finetuned models
- Break condition: If training data doesn't adequately represent needed reasoning patterns

### Mechanism 2
- Claim: Explanations improve performance of non-finetuned models
- Mechanism: Non-finetuned models rely on explicit reasoning steps in few-shot examples to guide inference
- Core assumption: Non-finetuned models lack internal reasoning patterns that finetuned models possess
- Evidence anchors: Abstract and section observations about substantial differences for vanilla OPT
- Break condition: If explanations are unclear or irrelevant to the task

### Mechanism 3
- Claim: Finetuning on reasoning datasets with explanations leads to significant improvements on specific reasoning skills
- Mechanism: Models learn to associate reasoning steps with correct answers during finetuning
- Core assumption: Reasoning skills from finetuning datasets transfer to evaluation tasks
- Evidence anchors: Abstract and section observations about improvements in seven reasoning skills
- Break condition: If finetuning datasets don't cover relevant reasoning skills or explanations are inaccurate

## Foundational Learning

- Concept: Finetuning
  - Why needed here: To compare performance of finetuned models with vanilla OPT model
  - Quick check question: What is the difference between finetuning and training a model from scratch?

- Concept: Prompting methods
  - Why needed here: To understand how explanations in prompts affect model performance
  - Quick check question: How does few-shot prompting differ from zero-shot prompting?

- Concept: Reasoning skills
  - Why needed here: To evaluate which specific reasoning capabilities are affected by explanations
  - Quick check question: What are some examples of reasoning skills that might be evaluated in a natural language processing task?

## Architecture Onboarding

- Component map: OPT models (1.3B, 6.7B, 13B) -> Finetuning process (with/without explanations) -> 57 evaluation tasks -> Three prompting methods (zero-shot, few-shot, few-shot with explanations)
- Critical path: Evaluation of models on 57 tasks using three prompting methods, running each combination and comparing results
- Design tradeoffs: Model size vs. computational resources - larger models perform better but require more resources
- Failure signatures: Poor performance could indicate insufficient finetuning, inadequate explanations, or misalignment between finetuning and evaluation tasks
- First 3 experiments:
  1. Evaluate vanilla OPT model on subset of tasks using zero-shot prompting to establish baseline
  2. Finetune OPT model without explanations on reasoning datasets subset, evaluate with few-shot prompting
  3. Finetune OPT model with explanations on same subset, evaluate with few-shot with explanations prompting, compare results to understand impact of explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance on reasoning tasks generalize to other LLMs with different architectures or pretraining objectives?
- Basis in paper: Inferred
- Why unresolved: Study only investigates OPT models; results may not generalize to other architectures
- What evidence would resolve it: Conduct similar experiments using different LLMs with various architectures and pretraining objectives

### Open Question 2
- Question: How does performance change when finetuned on larger closed datasets with explanations?
- Basis in paper: Inferred
- Why unresolved: Study uses limited open-source datasets; findings may not hold for larger closed datasets
- What evidence would resolve it: Finetune OPT models on larger closed datasets with explanations and evaluate performance

### Open Question 3
- Question: How does performance change when finetuned using different prompting conditions?
- Basis in paper: Inferred
- Why unresolved: Study only considers fewshot prompting conditions
- What evidence would resolve it: Finetune OPT models using different prompting conditions and evaluate performance

## Limitations
- Lack of direct evidence for proposed mechanisms explaining why explanations don't impact finetuned models
- Unspecified task selection criteria for the 57 evaluation tasks raises questions about representativeness
- Missing critical hyperparameter details for finetuning process (learning rate, batch size, etc.)

## Confidence

**High Confidence**: Finetuning significantly improves seven reasoning skills (numerical, analogical, reasoning on objects) - supported by clear performance improvements across model sizes

**Medium Confidence**: Explanations improve non-finetuned model performance - supported by observed differences but lacks corpus evidence and may have confounding factors

**Low Confidence**: Explanations in few-shot examples don't impact finetuned models because they've internalized reasoning patterns - no empirical evidence provided and corpus search found no supporting literature

## Next Checks

1. **Mechanism Validation Study**: Design controlled experiment comparing finetuned model performance on tasks with varying training data overlap, testing whether models rely on internalized patterns versus prompt information

2. **Ablation Analysis of Evaluation Tasks**: Conduct systematic analysis of 57 evaluation tasks for relationship to finetuning datasets using similarity metrics and performance correlations

3. **Hyperparameter Sensitivity Analysis**: Perform comprehensive ablation study varying learning rate, batch size, and epochs for both finetuning with and without explanations to determine robustness of observed effects