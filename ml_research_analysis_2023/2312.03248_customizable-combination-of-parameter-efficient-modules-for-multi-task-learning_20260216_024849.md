---
ver: rpa2
title: Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning
arxiv_id: '2312.03248'
source_url: https://arxiv.org/abs/2312.03248
tags:
- tasks
- arxiv
- learning
- skills
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a parameter-efficient multi-task learning framework
  that explicitly separates task-specific and task-common skills using low-rank adapters.
  The method outperforms fully-shared, task-specific, and skill-indistinguishable
  baselines on SuperGLUE and SuperNaturalInstructions benchmarks, demonstrating improved
  sample efficiency and mitigation of negative transfer.
---

# Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning

## Quick Facts
- arXiv ID: 2312.03248
- Source URL: https://arxiv.org/abs/2312.03248
- Reference count: 40
- Primary result: C-Poly achieves 0.6% accuracy improvement on SuperGLUE over fully-shared and task-specific baselines while using fewer parameters

## Executive Summary
This paper proposes C-Poly, a parameter-efficient multi-task learning framework that explicitly separates task-specific and task-common skills using low-rank adapters. The method builds upon the MMoE architecture by introducing customizable combinations of skill modules, allowing each task to benefit from both shared knowledge and exclusive specialized skills. C-Poly demonstrates improved sample efficiency and mitigation of negative transfer across NLP tasks on SuperGLUE and SuperNaturalInstructions benchmarks.

## Method Summary
C-Poly combines task-common and task-specific skills using LoRA adapters, with skill assignment controlled by Gumbel-sigmoid sampled routing matrices WA (for common skills) and WB (for task-specific skills). The framework allows customizable numbers of task-common skills (A) and task-specific skills (B) per task, enabling flexible knowledge sharing while preventing gradient conflicts. Training uses a frozen base model with only adapter parameters updated, achieving parameter efficiency while maintaining performance across diverse NLP tasks.

## Key Results
- C-Poly achieves 0.6% accuracy improvement on SuperGLUE over fully-shared and task-specific baselines
- Demonstrates improved sample efficiency and mitigation of negative transfer across tasks
- Shows stable improvements across different model scales (T5-Large to XXL, FLAN-T5-Large to XXL)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit separation of task-common and task-specific skills mitigates negative transfer by preventing gradient conflicts across tasks.
- Mechanism: The model assigns distinct parameter modules for shared knowledge (ΦA) and unique task attributes (Φt_B), with separate routing matrices WA and WB to control activation.
- Core assumption: Different tasks share transferable knowledge but also require discriminative abilities; separating these prevents interference during multi-task learning.
- Evidence anchors: [abstract] "Each task is associated with a customizable number of exclusive specialized skills and also benefits from skills shared with peer tasks."
- Break condition: If task similarity is high across all tasks, the benefit of separation may be minimal or could hinder transfer by overly isolating shared patterns.

### Mechanism 2
- Claim: Low-rank adapters (LoRA) enable parameter-efficient multi-task adaptation while maintaining performance.
- Mechanism: Each adapter is parameterized as Wdown * Wup instead of full matrix updates, reducing parameter count while preserving expressiveness.
- Core assumption: Task adaptation can be captured by low-rank updates without significant loss in model capacity.
- Evidence anchors: [section 2.3] "Through the adoption of LoRA, updating each linear layer in the model only requires 2 × r × d parameters in the calculation, as opposed to the original d × d parameters."
- Break condition: If task-specific adaptation requires high-rank changes (e.g., complex syntactic shifts), low-rank approximation may be insufficient.

### Mechanism 3
- Claim: The dual routing structure (WA for common skills, WB for task-specific) allows balanced knowledge sharing and task distinction.
- Mechanism: WA uses Gumbel-sigmoid sampling for soft partitioning of general skills; WB is initialized as a diagonal matrix but allows off-diagonal updates during training for cross-task skill borrowing.
- Core assumption: Task relationships can be learned through routing matrices rather than hard task labels.
- Evidence anchors: [section 2.1] "We utilize a task-common allocation matrix WA ∈ {0, 1}^T×A with a uniform initialization... This matrix is employed to achieve the soft partitioning of general skills."
- Break condition: If routing becomes too sparse or too dense, the model may either under-share or over-share skills, hurting performance.

## Foundational Learning

- Concept: Multi-gate Mixture-of-Experts (MMoE)
  - Why needed here: MMoE provides the routing framework that C-Poly builds upon; understanding gating mechanisms is essential for grasping skill selection.
  - Quick check question: How does MMoE differ from vanilla MoE in terms of task adaptation?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is the parameterization choice for all adapters in C-Poly; without understanding its math, the efficiency claim is opaque.
  - Quick check question: What is the parameter reduction factor when using rank r=8 in a d=1024 layer?

- Concept: Gumbel-Softmax trick for differentiable sampling
  - Why needed here: This is how the model learns discrete skill assignments in a continuous optimization setting.
  - Quick check question: Why can't we directly optimize binary WA matrices with gradient descent?

## Architecture Onboarding

- Component map: Input -> Base Model (frozen) -> Task-common LoRAs (gated by WA) -> Task-specific LoRAs (gated by WB) -> Sum -> Task prediction
- Critical path:
  1. Forward pass: Input → base model → adapter outputs gated by WA/WB → sum → task prediction
  2. Backward pass: Gradients flow only to LoRAs and routing matrices; base model frozen
  3. Optimization: AdamW with linear decay, warmup ratio 0.06
- Design tradeoffs:
  - More common skills (↑A) → better sharing but risk of interference
  - More task-specific skills (↑B) → better discrimination but risk of overfitting
  - Higher LoRA rank (↑r) → more expressive but less parameter efficient
- Failure signatures:
  - Negative transfer persists → routing matrices not learning discriminative patterns
  - Overfitting on few tasks → too many task-specific skills relative to data
  - Underfitting overall → too few total adapters or too low LoRA rank
- First 3 experiments:
  1. Single-task fine-tuning baseline: Train LoRA on one SuperGLUE task, measure accuracy.
  2. Multi-task Poly baseline: Train Poly on all SuperGLUE tasks, measure average accuracy.
  3. C-Poly ablation: Train C-Poly with B=0 (no task-specific skills) vs B=1, measure delta in accuracy and parameter count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio between task-common and task-specific skills in C-Poly for different types of NLP tasks?
- Basis in paper: [inferred] The paper mentions ablation experiments with different (A, B) combinations but notes that the ideal parameter ratio may vary depending on task nature and datasets.
- Why unresolved: The paper only provides preliminary ablation results and suggests that finding the optimal configuration requires further investigation across diverse task types and datasets.
- What evidence would resolve it: Systematic experiments testing various (A, B) ratios across different NLP task categories (e.g., classification, generation, reasoning) and dataset sizes, with statistical analysis of performance differences.

### Open Question 2
- Question: How does C-Poly's performance scale with extremely large language models (e.g., trillion-parameter models) compared to other PEFT methods?
- Basis in paper: [explicit] The paper mentions that C-Poly demonstrates stable improvements across different model scales (T5-Large to XXL, FLAN-T5-Large to XXL) but doesn't test at extreme scales.
- Why unresolved: The experiments were limited to models up to 2B parameters, and the scaling behavior at trillion-parameter scales (where MoE architectures are common) remains unknown.
- What evidence would resolve it: Comparative experiments applying C-Poly to trillion-parameter models, measuring performance gains and parameter efficiency relative to other PEFT methods at this scale.

### Open Question 3
- Question: What are the long-term generalization capabilities of C-Poly when applied to continuously evolving task distributions?
- Basis in paper: [inferred] The paper focuses on static multitask learning benchmarks but doesn't address scenarios where task distributions change over time or new tasks are added.
- Why unresolved: The paper's experimental design uses fixed task sets, and the framework's adaptability to dynamic task environments and catastrophic forgetting prevention is not evaluated.
- What evidence would resolve it: Longitudinal studies where C-Poly is trained on expanding task sets over time, measuring performance on both old and new tasks, and comparing with other continual learning approaches.

## Limitations
- The paper doesn't provide systematic ablation studies isolating the individual contribution of skill separation versus low-rank parameterization
- No quantitative analysis of how much cross-task skill borrowing actually occurs through the WB matrix
- Limited evaluation on extremely large language models (trillion-parameter scale) where MoE architectures are common

## Confidence
- High confidence: Parameter efficiency claims (LoRA parameterization is well-established)
- Medium confidence: Negative transfer mitigation through skill separation
- Medium-Low confidence: Dual routing structure benefits and cross-task skill borrowing effectiveness

## Next Checks
1. Run ablation study comparing C-Poly with only task-common skills (B=0) versus only task-specific skills (A=0) to quantify the marginal benefit of skill separation.
2. Extract and visualize the learned WA and WB matrices across tasks to verify that the model is actually discovering meaningful task relationships rather than memorizing arbitrary patterns.
3. Systematically vary the LoRA rank parameter (r) from 2 to 64 while holding other factors constant to establish the minimum rank needed for competitive performance on each benchmark.