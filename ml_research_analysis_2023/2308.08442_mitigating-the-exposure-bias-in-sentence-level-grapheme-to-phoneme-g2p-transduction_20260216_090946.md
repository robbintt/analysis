---
ver: rpa2
title: Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction
arxiv_id: '2308.08442'
source_url: https://arxiv.org/abs/2308.08442
tags:
- bias
- sequence
- exposure
- sampling
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses exposure bias in sentence-level Grapheme-to-Phoneme
  (G2P) conversion using the ByT5 model. Due to its character-level operation, ByT5
  suffers from performance degradation in longer sequences caused by exposure bias.
---

# Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction

## Quick Facts
- arXiv ID: 2308.08442
- Source URL: https://arxiv.org/abs/2308.08442
- Reference count: 0
- Primary result: Loss-based sampling improves PER from 24.10% to 21.99% on longer sequences

## Executive Summary
This paper addresses exposure bias in sentence-level Grapheme-to-Phoneme (G2P) conversion using the ByT5 model. Due to its character-level operation, ByT5 suffers from performance degradation in longer sequences caused by exposure bias. The authors propose a loss-based sampling method that identifies high-risk positions in the sequence based on cross-entropy loss and adaptively samples these positions during training, replacing predictions with ground truth. Evaluated on a curated English G2P benchmark, the proposed method improves Phoneme Error Rate (PER) from 24.10% to 21.99% (greedy search) and from 23.65% to 21.50% (beam search) on longer sequences, demonstrating effective mitigation of exposure bias.

## Method Summary
The proposed method involves a two-pass decoding strategy where cross-entropy loss is calculated for each position in the phoneme sequence during the first pass. Positions with higher loss values are sampled more frequently based on an adaptive ratio determined by the previous epoch's PER. These sampled positions have their predictions replaced with ground truth before being input to the decoder for the second pass. This approach forces the model to learn from its errors at high-risk positions while better simulating the auto-regressive generation process during inference. The method is evaluated using the TIMIT corpus with concatenated sentences of varying lengths, measuring PER, WER, and AccErr≤(l) metrics.

## Key Results
- PER improved from 24.10% to 21.99% (greedy search) and from 23.65% to 21.50% (beam search) on longer sequences
- Loss-based sampling outperformed uniform sampling and teacher forcing baselines
- The adaptive sampling ratio based on PER showed better performance than fixed sampling ratios
- Exposure bias quantified using AccErr≤(l) metric showed significant reduction with the proposed method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss-based sampling mitigates exposure bias by increasing training focus on positions with high cross-entropy loss values.
- Mechanism: During training, positions with higher cross-entropy loss between predicted phoneme probabilities and ground truth are sampled more frequently. These positions have their predictions replaced with ground truth before being input to the decoder, forcing the model to learn from its errors at these high-risk positions.
- Core assumption: Positions with higher cross-entropy loss during training are more likely to be incorrectly predicted during inference.
- Evidence anchors:
  - [abstract] "The positions with higher loss values are then sampled more frequently during training, in which their predictions are replaced into the ground truth phoneme sequence before inputting into the decoder."
  - [section] "The method involves several steps, starting with inputting the ground truth phoneme sequence and obtaining the predicted phoneme probability distribution for each time step... Next, we normalize the cross-entropy loss to obtain a categorical distribution. From the distribution, we randomly sample a specific number of time steps without replacement, creating a mask to determine whether to replace each ground truth input with its corresponding prediction."

### Mechanism 2
- Claim: Adaptive sampling ratio based on PER from previous epoch ensures appropriate sampling intensity.
- Mechanism: The number of positions to sample for replacement is determined by the Phoneme Error Rate from the previous training epoch, allowing the sampling intensity to adapt to the model's current performance level.
- Core assumption: PER from the previous epoch is a reliable indicator of where errors are likely to occur in the current training epoch.
- Evidence anchors:
  - [section] "During this sampling process, we use an adaptive sampling ratio specified by the Phoneme Error Rate (PER) of the previous epoch to determine the number of desired replacements adaptively."

### Mechanism 3
- Claim: Two-pass decoding strategy with loss-based sampling creates a more realistic training environment that better matches inference conditions.
- Mechanism: By using predictions sampled based on loss as decoder input for the second prediction, the training procedure more closely mimics the auto-regressive generation process during inference, reducing the discrepancy between training and testing conditions.
- Core assumption: Training with sampled predictions as input better prepares the model for the inference scenario where it must rely on its own previous predictions.
- Evidence anchors:
  - [section] "The replaced sequence obtained from the loss-based sampling is once again used as the decoder input. Because the second prediction is generated based on its prediction randomly sampled, the resulting output can reflect the auto-regressive behavior."

## Foundational Learning

- Concept: Exposure bias in auto-regressive generation models
  - Why needed here: Understanding exposure bias is essential to grasp why sentence-level G2P performance degrades with longer sequences and why mitigation techniques are necessary.
  - Quick check question: What is the fundamental difference between training (teacher forcing) and inference (auto-regressive generation) that creates exposure bias?

- Concept: Cross-entropy loss as an error indicator
  - Why needed here: The method relies on cross-entropy loss values to identify positions where errors are likely to occur, so understanding how cross-entropy relates to prediction quality is crucial.
  - Quick check question: How does cross-entropy loss between predicted probabilities and one-hot encoded ground truth indicate the likelihood of a prediction error?

- Concept: Scheduled sampling and its variants
  - Why needed here: Loss-based sampling builds upon scheduled sampling concepts, so understanding the evolution of sampling-based training methods provides context for the proposed approach.
  - Quick check question: How does uniform sampling differ from loss-based sampling in terms of which positions are selected for ground truth replacement?

## Architecture Onboarding

- Component map:
  Input encoder -> Loss calculation module -> Sampling module -> Replacement module -> Decoder

- Critical path:
  1. Forward pass with ground truth input
  2. Cross-entropy loss calculation for each position
  3. Loss normalization and position sampling
  4. Ground truth replacement at sampled positions
  5. Second forward pass with modified input
  6. Backpropagation using second pass loss

- Design tradeoffs:
  - Sampling ratio adaptation vs. fixed ratio: Adaptive sampling responds to model performance but adds complexity and potential instability
  - Loss-based vs. uniform sampling: Loss-based focuses on high-risk positions but requires additional computation
  - Single vs. multiple passes: Two-pass approach better simulates inference but doubles forward computation

- Failure signatures:
  - No performance improvement: Sampling ratio may be inappropriate or loss does not correlate with error-prone positions
  - Training instability: Adaptive sampling ratio may cause volatile learning rates or convergence issues
  - Increased computational cost without benefit: Additional forward pass may not justify the performance gains

- First 3 experiments:
  1. Compare uniform sampling with loss-based sampling using fixed sampling ratios to validate the importance of loss-based selection
  2. Test different sampling ratio schedules (fixed, PER-based adaptive, exponential decay) to find optimal adaptation strategy
  3. Evaluate the impact of different loss normalization methods (softmax, sigmoid, rank-based) on sampling effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does exposure bias in sentence-level G2P conversion vary across different languages and writing systems?
- Basis in paper: [explicit] The authors note that ByT5 shows promising results on multilingual G2P conversion and mention its token-free advantage for handling unknown vocabulary across languages.
- Why unresolved: The paper only evaluates exposure bias mitigation on English G2P tasks using TIMIT. Different languages have varying orthographic depths and phoneme-grapheme correspondences that may affect exposure bias differently.
- What evidence would resolve it: Empirical studies comparing exposure bias effects and the proposed loss-based sampling method across multiple languages with different writing systems (e.g., transparent vs. opaque orthographies, logographic vs. alphabetic).

### Open Question 2
- Question: What is the relationship between exposure bias accumulation and sequence length in G2P tasks, and does it follow a predictable pattern?
- Basis in paper: [explicit] The authors analyze exposure bias using the AccErr metric and note that it "grows quadratically with the phoneme sequence length" in worst-case scenarios, but actual results may vary.
- Why unresolved: The paper only tests on sequences up to 5 sentences long. The theoretical quadratic growth model needs empirical validation across wider sequence length ranges and different model architectures.
- What evidence would resolve it: Systematic experiments measuring exposure bias accumulation across varying sequence lengths (both shorter and longer than tested), and analysis of whether the relationship follows quadratic, linear, or other patterns across different model sizes and architectures.

### Open Question 3
- Question: How does the proposed loss-based sampling method compare to other exposure bias mitigation techniques specifically for G2P tasks?
- Basis in paper: [explicit] The authors compare their method to uniform sampling and teacher forcing baseline, but acknowledge that other techniques exist including reinforcement learning, generative adversarial networks, and better decoding methods.
- Why unresolved: The paper focuses on comparing variants of sampling-based methods but doesn't benchmark against other exposure bias mitigation approaches that have been proposed in the broader NLP literature.
- What evidence would resolve it: Direct comparison studies between loss-based sampling and alternative exposure bias mitigation techniques (RL-based, GAN-based, or non-MLE methods) specifically for G2P tasks, measuring both quantitative performance and qualitative differences in phoneme prediction quality.

## Limitations

- The assumption that cross-entropy loss during training reliably predicts error-prone positions during inference is not explicitly validated
- The two-pass decoding strategy doubles computational overhead without runtime efficiency analysis
- Adaptive sampling ratio based on PER may not account for non-linear relationships between performance and error distribution

## Confidence

- **High confidence**: The improvement in PER metrics from 24.10% to 21.99% (greedy search) and from 23.65% to 21.50% (beam search) on longer sequences is well-supported by the experimental results. The methodology for calculating these metrics is standard in the G2P literature.
- **Medium confidence**: The effectiveness of loss-based sampling versus uniform sampling is demonstrated, but the specific optimal sampling ratio values and the sensitivity of results to these parameters remain unclear. The grid search results are mentioned but not fully detailed.
- **Low confidence**: The theoretical justification for why cross-entropy loss specifically identifies exposure bias-related errors, rather than general model uncertainty, lacks rigorous validation. The relationship between loss distribution and error distribution during inference is assumed rather than empirically proven.

## Next Checks

1. **Error distribution analysis**: Conduct a detailed analysis comparing the distribution of high-loss positions during training with actual error positions during inference across multiple training epochs. This would validate whether the loss-based sampling strategy targets the correct positions for exposure bias mitigation.

2. **Sampling ratio sensitivity**: Perform an ablation study systematically varying the sampling ratio from 0% to 100% in finer increments (e.g., 10% steps) to identify the optimal range and test the robustness of the adaptive PER-based approach. Include analysis of how sampling ratio affects convergence speed and final performance.

3. **Computational overhead quantification**: Measure and compare the wall-clock training time per epoch for the baseline ByT5 model versus the proposed method with loss-based sampling. Include analysis of memory usage and whether the performance gains justify the additional computational cost, particularly for deployment scenarios with resource constraints.