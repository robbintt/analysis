---
ver: rpa2
title: 'Log-based Anomaly Detection of Enterprise Software: An Empirical Study'
arxiv_id: '2310.20492'
source_url: https://arxiv.org/abs/2310.20492
tags:
- dataset
- logs
- anomaly
- detection
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates state-of-the-art log-based anomaly detection\
  \ models on industrial software datasets, which are smaller and less structured\
  \ than typical open-source benchmarks. The research compares models like DeepLog,\
  \ LogAnomaly, LogBERT, and a baseline LSTM across two datasets\u2014an industrial\
  \ microservice and the widely-used HDFS dataset."
---

# Log-based Anomaly Detection of Enterprise Software: An Empirical Study

## Quick Facts
- arXiv ID: 2310.20492
- Source URL: https://arxiv.org/abs/2310.20492
- Reference count: 40
- Primary result: LogAnomaly outperforms state-of-the-art models on industrial microservice datasets due to its ability to handle unstructured logs through vectorization and template approximation.

## Executive Summary
This study evaluates four log-based anomaly detection models—DeepLog, LogAnomaly, LogBERT, and an LSTM baseline—on both industrial microservice data and the widely-used HDFS dataset. The research reveals significant performance differences based on log structure, with LogAnomaly excelling on the loosely-structured industrial dataset while LogBERT performs best on the structured HDFS data. The study also exposes critical methodological issues including data leakage from random train-test splits and overfitting from excessive training data sizes.

## Method Summary
The study compares four log-based anomaly detection models using 5-fold cross-validation on two datasets: an industrial microservice dataset from AWS CloudWatch (170,566 logs, 142 templates) and the HDFS dataset (11,175,629 logs, 53 templates). Models are trained on normal logs only and evaluated using F1-score, precision, and recall. The research examines both random and time-series data splitting methods and explores the effect of varying training data sizes on model performance.

## Key Results
- LogAnomaly achieved F1-scores of 0.73 on industrial data and 0.92 on HDFS, outperforming all other models on the industrial dataset
- LogBERT achieved the highest F1-score of 0.98 on the structured HDFS dataset but performed poorly on unstructured industrial logs
- Random train-test splits caused data leakage, artificially inflating model performance by using future logs to predict past logs
- Increasing training data size improved performance up to a point, after which overfitting reduced effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LogAnomaly outperforms others on industrial datasets due to its ability to handle unstructured logs through vectorization and template approximation
- Mechanism: LogAnomaly uses Template2Vec to create semantic vectors of log templates and approximates new unseen templates by matching their similarity to existing ones, reducing noise from minor logging statement variations
- Core assumption: Log templates with similar semantic content produce similar vector representations, enabling effective anomaly detection even when logs are loosely structured
- Evidence anchors:
  - [abstract]: "Key findings include LogAnomaly outperforming others on the industrial dataset due to its ability to handle unstructured logs"
  - [section]: "LogAnomaly uses vectorization along with template approximation, and an added quantitative anomaly detection model. This vectorization step likely reduces noise in the logs, since minor updates to logging statements result in similar semantic vectors."
- Break condition: If log templates are too diverse or lack semantic consistency, the vector similarity matching may fail to accurately approximate new templates

### Mechanism 2
- Claim: Data leak from random train-test splits leads to overestimated model effectiveness by using future logs to predict past logs
- Mechanism: Random sampling inadvertently includes future log sequences in the training set, allowing models to "cheat" by learning patterns that haven't occurred yet when making predictions on past data
- Core assumption: Log data has temporal dependencies where future events influence the interpretation of past sequences
- Evidence anchors:
  - [abstract]: "The study also highlights how random train-test splits can introduce data leaks, leading to overestimated model effectiveness"
  - [section]: "According to a recent study [33], the data leak during random sampling makes the effectiveness of models seem higher than they actually are. Effectively, when doing random sampling, future logs are used as part of the training set, resulting in past logs being predicted using future logs."
- Break condition: If log sequences are truly independent across time or if time-based splitting severely limits available training data

### Mechanism 3
- Claim: Limited training data size significantly impacts model performance, with overfitting occurring when training set exceeds optimal size
- Mechanism: Models learn to memorize training patterns rather than generalize when provided excessive data relative to the complexity of the industrial dataset, leading to decreased effectiveness on unseen test data
- Core assumption: Industrial datasets have lower intrinsic complexity than open-source datasets, requiring less training data for effective learning
- Evidence anchors:
  - [abstract]: "The study also highlights how random train-test splits can introduce data leaks, leading to overestimated model effectiveness, and how increasing training data size can improve performance but may cause overfitting."
  - [section]: "Exploring the effect of the training size shows that LogAnomaly and DeepLog do get better results with a bigger training set, but over-sized training sets result in the model over-fitting and a reduced effectiveness."
- Break condition: If the dataset contains complex patterns requiring larger training samples, or if regularization techniques are applied to prevent overfitting

## Foundational Learning

- Concept: Log preprocessing and template mining
  - Why needed here: Models require structured log sequences as input, but raw logs are unstructured free-form text
  - Quick check question: What are the three main steps in log preprocessing before feeding data to anomaly detection models?

- Concept: Time-series vs random data splitting
  - Why needed here: The paper demonstrates how split methodology affects model evaluation validity
  - Quick check question: How does a time-series split prevent data leakage compared to random sampling?

- Concept: Sequence-based anomaly detection
  - Why needed here: All evaluated models treat anomaly detection as a sequence prediction problem
  - Quick check question: What distinguishes sequential anomalies from quantitative anomalies in log analysis?

## Architecture Onboarding

- Component map: Raw logs -> Drain3 template mining -> Sequence generation by request ID -> Sliding window history creation -> Model training -> Anomaly prediction
- Critical path: Raw logs → Drain3 template mining → Sequence generation by request ID → Sliding window history creation → Model training → Anomaly prediction
- Design tradeoffs: Structured vs unstructured log handling (LogBERT vs LogAnomaly), parameter inclusion (DeepLog vs others), training data efficiency
- Failure signatures: Data leakage from improper splitting, overfitting from excessive training data, poor performance on unstructured logs
- First 3 experiments:
  1. Replicate the random split baseline experiment on both HDFS and industrial datasets
  2. Implement time-series split and compare performance degradation
  3. Vary training set size (20%, 40%, 60%, 80%) to identify overfitting thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of log-based anomaly detection models change when applied to industrial datasets with even more unstructured or dynamic logging formats compared to the current study?
- Basis in paper: [explicit] The study notes that the industrial dataset has a "loosely-structured" format and explores LogAnomaly's superior performance with such data
- Why unresolved: The study only evaluated one type of loosely-structured dataset; the impact of further unstructured or rapidly evolving logs remains untested
- What evidence would resolve it: Testing the same models on multiple industrial datasets with varying degrees of log structure and dynamics, and comparing results

### Open Question 2
- Question: What is the optimal balance between training data size and model complexity for log-based anomaly detection in industrial applications?
- Basis in paper: [explicit] RQ4 shows that increasing training size can improve performance but may cause overfitting
- Why unresolved: The study tested a limited range of training sizes and did not explore the interaction between training size, model complexity, and overfitting in detail
- What evidence would resolve it: Systematic experiments varying both training data size and model architecture complexity across multiple datasets

### Open Question 3
- Question: How effective are current log-based anomaly detection models when dealing with log data from microservices with high request rates and short log sequences?
- Basis in paper: [explicit] The study mentions the industrial microservice handles "approximately a hundred user requests per day" and LogBERT struggles with longer sequences
- Why unresolved: The study focused on a microservice with relatively low request rates and did not test models on high-throughput scenarios
- What evidence would resolve it: Evaluating the models on microservices with high request rates and analyzing performance across different sequence lengths

### Open Question 4
- Question: Would incorporating semantic understanding of log messages (e.g., using pre-trained language models) improve anomaly detection in industrial datasets with unstructured logs?
- Basis in paper: [inferred] The study notes that the industrial dataset has "more of a natural language free format" and suggests future work could explore vectorization using GPT-like models
- Why unresolved: The study used traditional log template mining and vectorization methods, not semantic language models
- What evidence would resolve it: Implementing and comparing models that use pre-trained language models (like GPT) for log vectorization and anomaly detection

## Limitations

- Small industrial dataset size (170,566 logs) limits generalizability compared to the much larger HDFS benchmark
- Limited exploration of training size effects - exact overfitting thresholds not precisely quantified
- Single industrial dataset used for methodology comparisons leaves uncertainty about generalizability to other production environments

## Confidence

- **High confidence**: LogAnomaly's superior performance on unstructured industrial data (supported by multiple metrics and consistent across experiments)
- **Medium confidence**: Data leakage from random splits causing overestimation (mechanism is sound but impact magnitude varies by dataset)
- **Medium confidence**: Training size effects on model performance (results show clear trends but overfitting points are dataset-specific)

## Next Checks

1. Test the time-series split methodology across 3-5 additional industrial datasets to verify the data leakage impact generalizes beyond the single case study
2. Conduct hyperparameter sensitivity analysis for each model, particularly examining LogAnomaly's Template2Vec parameters and LogBERT's sequence length settings
3. Implement cross-dataset evaluation by training models on HDFS and testing on industrial data (and vice versa) to assess model portability and dataset-specific optimizations