---
ver: rpa2
title: Variational latent discrete representation for time series modelling
arxiv_id: '2306.15282'
source_url: https://arxiv.org/abs/2306.15282
tags:
- latent
- discrete
- prior
- variational
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a variational model with discrete latent
  variables for time series, where the discrete state is modeled as a Markov chain,
  enabling fast end-to-end training. The method outperforms state-of-the-art VQ-VAE
  in both accuracy and computation time on building management and electricity transformer
  datasets.
---

# Variational latent discrete representation for time series modelling

## Quick Facts
- arXiv ID: 2306.15282
- Source URL: https://arxiv.org/abs/2306.15282
- Reference count: 35
- Key outcome: Discrete latent Markov model outperforms VQ-VAE on building management and electricity transformer datasets with RMSE of 0.20±0.10 vs 0.28±0.12

## Executive Summary
This paper introduces a variational model with discrete latent variables for time series forecasting, where latent states are modeled as a Markov chain. The approach enables fast end-to-end training without separate codebook training stages required by VQ-VAE. The model achieves superior performance on building management and electricity transformer datasets while offering better interpretability through discrete regime detection.

## Method Summary
The proposed method uses a variational autoencoder with discrete latent variables where the discrete state evolves as a Markov chain. The encoder maps observations to a discrete latent space using Gumbel-Softmax sampling for differentiable training. The prior model (RNN, GRU, or CNN) captures temporal dependencies in latent states, while the decoder generates observations from latent states. The model is trained end-to-end by maximizing the ELBO with β-annealing, starting from β≈0 and increasing to β=1 over 100 epochs.

## Key Results
- Achieves RMSE of 0.20±0.10 on Electricity Transformer Dataset, outperforming VQ-VAE baseline (0.28±0.12)
- Outperforms state-of-the-art VQ-VAE in both accuracy and computation time
- Discrete latent representation enables regime switching detection and high-level feature analysis
- Three prior architectures tested: RNN, GRU, and causal CNN with varying performance characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete latent states modeled as a Markov chain enable end-to-end training while preserving temporal dependencies.
- Mechanism: By modeling the latent state evolution as a Markov chain, the prior distribution becomes tractable and differentiable through reparameterization, avoiding the need for separate training stages required in VQ-VAE.
- Core assumption: The discrete latent states follow a Markov property, making the joint prior factorizable into transition probabilities.
- Evidence anchors:
  - [abstract] "we introduce a latent data model where the discrete state is a Markov chain, which allows fast end-to-end training"
  - [section] "Conditionally on the commands, we assume that the latent states are Markovian and that the conditional law of the observations depends on past latent states"
  - [corpus] Weak evidence - no direct neighbor papers discussing Markov chain modeling for discrete latents
- Break condition: If the Markov assumption is violated (e.g., long-range dependencies not captured by Markov order), the model's performance degrades.

### Mechanism 2
- Claim: Gumbel-Softmax reparameterization enables gradient flow through discrete sampling operations.
- Mechanism: The Gumbel-Softmax distribution approximates categorical sampling with a differentiable softmax, allowing backpropagation through discrete latent selection.
- Core assumption: The temperature parameter τ can be tuned to balance between categorical-like sampling and gradient-friendly approximation.
- Evidence anchors:
  - [section] "We use a reparametrization designed for categorical variables based on the Gumbel-Softmax distribution"
  - [section] "we sample (g1, . . . , gK) independently from the Gumbel distribution and define, for all 1 ≤ k ≤ K, πk,t ∝ exp((log qk ϕ,t + gk)/τt)"
  - [corpus] No direct evidence in neighbors about Gumbel-Softmax usage
- Break condition: If τ is not properly annealed from high to low values, the model may either produce poor samples (high τ) or fail to train (τ too low).

### Mechanism 3
- Claim: Joint optimization with β-annealing balances reconstruction quality and latent capacity.
- Mechanism: Starting with β near zero (focusing on reconstruction) and gradually increasing to β=1 (encouraging latent usage) stabilizes training and improves performance.
- Core assumption: The model benefits from gradually increasing the KL divergence term's weight during training.
- Evidence anchors:
  - [section] "we can perform end-to-end training by penalizing the prior and posterior terms by this factor β initialized close to zero, and then slowly increasing its value until reaching β = 1"
  - [section] "Estimating θ, ϕ and E jointly can induce instability at the beginning of the training, leading to diminished performances after convergence"
  - [corpus] No neighbor papers discussing β-annealing strategy
- Break condition: If β increases too rapidly or too slowly, the model may either underfit (β too low throughout) or produce poor reconstructions (β too high too early).

## Foundational Learning

- Concept: Variational Inference and Evidence Lower Bound (ELBO)
  - Why needed here: The model maximizes ELBO to jointly learn encoder, decoder, and prior parameters without requiring explicit posterior computation
  - Quick check question: What are the three components of ELBO and why is maximizing it equivalent to minimizing KL divergence between approximate and true posteriors?

- Concept: Discrete latent variable training challenges
  - Why needed here: Understanding why standard backpropagation fails with discrete variables and how techniques like straight-through estimators or Gumbel-Softmax solve this
  - Quick check question: Why does sampling from a discrete distribution block gradient propagation, and how does the Gumbel-Softmax trick circumvent this?

- Concept: Markov chain properties and autoregressive modeling
  - Why needed here: The prior model assumes Markovian latent states, requiring understanding of transition probabilities and their estimation
  - Quick check question: How does assuming a Markov property on latent states simplify the joint prior distribution, and what are the implications for modeling long-range dependencies?

## Architecture Onboarding

- Component map: Encoder (LSTM) -> Gumbel-Softmax sampling -> Prior model (RNN/GRU/CNN) -> Decoder (LSTM)
- Critical path: Encoder → Gumbel-Softmax sampling → Prior model → Decoder
  - The forward pass generates samples through the Gumbel-Softmax approximation
  - The backward pass computes gradients through the continuous relaxation

- Design tradeoffs:
  - Number of codebooks (K): More codebooks increase expressiveness but require more training data
  - Codebook dimension (D): Higher dimensions capture more information but increase computational cost
  - Prior architecture: RNNs are simpler but may struggle with long-term dependencies; CNNs with wider receptive fields can capture longer patterns
  - β-annealing schedule: Too fast leads to posterior collapse, too slow leads to poor latent utilization

- Failure signatures:
  - Posterior collapse: All latent states collapse to a single codebook (check codebook usage statistics)
  - Training instability: Large variance in ELBO or reconstruction loss
  - Poor generalization: Large gap between training and validation performance
  - Latent underutilization: Many codebooks rarely or never get selected

- First 3 experiments:
  1. Baseline comparison: Train with K=8 codebooks using RNN prior, measure RMSE on validation set, compare to VQ-VAE baseline
  2. Architecture ablation: Replace RNN prior with GRU and CNN variants, measure impact on performance and computation time
  3. Codebook analysis: Visualize codebook usage over time on sample sequences to verify regime-switching behavior

## Open Questions the Paper Calls Out
- How does the choice of prior model (RNN, GRU, CNN) impact the long-term dependencies and accuracy in the proposed discrete latent representation model for time series forecasting?
- What is the optimal number of codebooks (K) for the discrete latent representation in different time series forecasting tasks, and how does it affect the model's interpretability and performance?
- How can the proposed model be extended to handle multivariate time series with complex dependencies between multiple variables, and what are the challenges in scaling the model to higher dimensions?

## Limitations
- Markov assumption may not hold for time series with complex long-range dependencies
- Limited testing on diverse time series domains beyond building management and electricity transformer data
- Hyperparameter sensitivity not thoroughly explored (codebook size, temperature schedule, β-annealing)

## Confidence
- **High Confidence**: The fundamental mechanism of using Gumbel-Softmax for end-to-end training of discrete latent models is well-established in the literature. The computational efficiency claims relative to VQ-VAE are directly measurable.
- **Medium Confidence**: The improved accuracy metrics (RMSE reduction from 0.28 to 0.20) are promising but based on a single baseline comparison. The regime-switching interpretation of latent states is plausible but not rigorously validated.
- **Low Confidence**: The broader applicability of the Markov discrete latent framework to arbitrary time series problems remains unproven without testing on more diverse datasets and problem types.

## Next Checks
1. **Markov Property Validation**: Analyze latent state transition matrices on validation data to empirically verify the Markov property assumption. Compute transition entropy and compare against random baselines.

2. **Architecture Sensitivity Analysis**: Systematically vary K (codebook size), codebook dimension D, and prior architecture types across a grid search. Document performance stability and identify breaking points.

3. **Cross-Domain Generalization**: Test the model on at least two additional time series datasets from different domains (e.g., financial time series and physiological signals) to evaluate domain transfer capability.