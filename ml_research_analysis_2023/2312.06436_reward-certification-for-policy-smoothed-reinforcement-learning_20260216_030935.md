---
ver: rpa2
title: Reward Certification for Policy Smoothed Reinforcement Learning
arxiv_id: '2312.06436'
source_url: https://arxiv.org/abs/2312.06436
tags:
- reward
- bound
- policy
- cumulative
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of certifying the robustness
  of reinforcement learning policies against adversarial attacks. The authors propose
  a novel approach based on f-divergence to measure the distance between the original
  and perturbed distributions, and solve a convex optimization problem to find the
  optimal lower bound of the mean cumulative reward.
---

# Reward Certification for Policy Smoothed Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.06436
- Source URL: https://arxiv.org/abs/2312.06436
- Authors: 
- Reference count: 15
- One-line primary result: Novel f-divergence-based approach for certifying robustness of smoothed RL policies against adversarial attacks, with tighter bounds and better efficiency than state-of-the-art methods

## Executive Summary
This paper introduces a novel approach for certifying the robustness of reinforcement learning policies against adversarial attacks. The method leverages f-divergence to measure the distance between original and perturbed distributions, and solves a convex optimization problem to find the optimal lower bound of the mean cumulative reward. The approach can handle various lp-norm bounded perturbations, including l0-norm bounded perturbations in action space, which is a first in the field.

## Method Summary
The proposed method trains a smoothed policy by injecting Gaussian noise into observations, then uses Monte Carlo sampling to approximate the expected cumulative reward. The robustness certification problem is transformed into a convex optimization problem using the generalized Donsker-Varadhan variational formula and f-divergence constraints. The method computes a certified lower bound on the mean cumulative reward under lp-norm bounded perturbations, including l0-norm perturbations in action space using R\'enyi divergence.

## Key Results
- Improved certified lower bound of mean cumulative reward compared to state-of-the-art techniques
- Better efficiency than existing methods for computing robustness certificates
- First method to certify l0-norm bounded perturbations in action space
- Tighter bounds for l2-norm perturbations across multiple benchmark environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method directly computes a lower bound of expected cumulative reward via convex optimization over f-divergence constraints, avoiding thresholding decomposition.
- Mechanism: Leverages the generalized Donsker-Varadhan variational formula to transform the robustness certification problem into a convex optimization problem. By parameterizing the perturbation distribution and using f-divergence as a constraint, the method solves for the worst-case expected reward under bounded perturbations.
- Core assumption: The f-divergence constraint (e.g., Hockey-Stick, total variation, R\'enyi) accurately upper-bounds the lp-norm perturbation constraint, and the smoothed policy is robust to the noise introduced during training.
- Evidence anchors:
  - [abstract]: "Our approach leverages f-divergence to measure the distinction between the original distribution and the perturbed distribution, subsequently determining the certification bound by solving a convex optimisation problem."
  - [section]: "By leveraging this theorem, we demonstrate that determining the lower bound of expected utility can be achieved by solving a convex optimisation problem."
  - [corpus]: Weak evidence; corpus neighbors focus on different domains (e.g., image classification) and do not address RL-specific robustness certification under f-divergence constraints.
- Break condition: If the f-divergence fails to tightly bound the lp-norm constraint (e.g., for certain noise distributions or high-dimensional spaces), the certified bound becomes overly loose, losing practical value.

### Mechanism 2
- Claim: Monte Carlo sampling from a smoothed policy approximates the expected cumulative reward with provable confidence bounds, enabling tractable optimization.
- Mechanism: Samples multiple trajectories by injecting Gaussian noise into observations, computes cumulative rewards, and uses Hoeffding's inequality to bound the estimation error. This empirical mean replaces the intractable expectation in the optimization.
- Core assumption: The reward function and noise distribution are bounded such that Hoeffding's inequality applies (finite range), and the number of samples is sufficient for the desired confidence level.
- Evidence anchors:
  - [abstract]: "Our method not only improves the certified lower bound of mean cumulative reward but also demonstrates better efficiency than state-of-the-art techniques."
  - [section]: "We employ Monte Carlo Sampling as a means to approximate the cumulative reward... we employ the widely recognised Hoeffding’s bound (Shivaswamy and Jebara 2010) to approximate the lower bound of this expectation."
  - [corpus]: No direct evidence; neighbors discuss randomized smoothing for image models but not Monte Carlo-based RL certification.
- Break condition: If the reward distribution has heavy tails or unbounded support, Hoeffding's inequality no longer holds, invalidating the confidence bound and making the optimization result unreliable.

### Mechanism 3
- Claim: Extending certification to l0-norm perturbations in action space is achieved by modeling action selection as a probability distribution and applying R\'enyi divergence.
- Mechanism: Treats the action sequence as a distribution over discrete choices, where the smoothed policy selects the best action with probability k and worst actions with probability (1-k). R\'enyi divergence measures the distance between the original and worst-case action distributions under l0 constraints.
- Core assumption: The action space is discrete and finite, and the smoothed policy's action distribution can be parameterized and differentiated for optimization.
- Evidence anchors:
  - [abstract]: "Furthermore, we extend our methodology to certify perturbations on action spaces... This work is the first of its kind to consider the certification of the l0-norm bounded perturbation in the action space."
  - [section]: "Our approach can also be extended to certify perturbations bounded by l0-norm... we use the R\'enyi divergence with parameter β to measure the l0 perturbation."
  - [corpus]: No direct evidence; corpus neighbors focus on general robustness certification, not l0-norm action space perturbations in RL.
- Break condition: If the action space is very large or continuous, the discrete probability model breaks down, making R\'enyi divergence inapplicable and the optimization intractable.

## Foundational Learning

- Concept: f-divergence and its variants (Hockey-Stick, total variation, R\'enyi)
  - Why needed here: These divergences provide tractable convex constraints that bound the lp-norm perturbation, enabling the conversion of robustness certification into a solvable optimization problem.
  - Quick check question: Can you derive the Hockey-Stick divergence for l2-bounded perturbations from a Gaussian reference distribution?

- Concept: Generalized Donsker-Varadhan variational formula
  - Why needed here: It establishes the duality between the primal robustness certification problem and a convex dual optimization, which is the core theoretical foundation for the method.
  - Quick check question: How does the Legendre-Fenchel transform of the Hockey-Stick divergence simplify the optimization objective?

- Concept: Hoeffding's inequality for Monte Carlo estimation
  - Why needed here: It provides a rigorous confidence bound on the empirical mean of cumulative rewards, ensuring the optimization result is statistically sound.
  - Quick check question: What is the range of the cumulative reward in Cartpole, and how does it affect the width of the Hoeffding bound?

## Architecture Onboarding

- Component map: Smoothed policy training with Gaussian noise injection -> Monte Carlo sampling module to generate trajectories -> Convex optimization solver (e.g., CVXPY) -> Divergence-specific bound calculators (Hockey-Stick, TV, R\'enyi) -> Confidence bound estimator using Hoeffding
- Critical path: Train smoothed policy -> Sample trajectories -> Compute rewards -> Solve convex optimization with f-divergence constraint -> Output certified lower bound
- Design tradeoffs: Higher smoothing parameter σ increases robustness but may degrade nominal performance; more samples improve confidence but increase runtime; tighter divergence bounds yield better certificates but may be harder to compute
- Failure signatures: If the certified bound is negative or worse than baseline, suspect loose divergence bounds or insufficient sampling; if optimization fails to converge, check convexity of the objective or numerical stability
- First 3 experiments:
  1. Verify that the smoothed policy achieves higher nominal reward than the base policy on Cartpole with σ=0.1
  2. Run Monte Carlo sampling with M=1000 and compare empirical mean reward to the theoretical expectation under Hoeffding's bound
  3. Solve the convex optimization for l2-norm perturbations with λ=0.5 and confirm that the certified bound is tighter than the baseline CDF-based method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the certified lower bound of mean cumulative reward scale with increasing smoothing parameter σ in different environments?
- Basis in paper: [explicit] The authors mention that as σ grows, the variance of the cumulative output reward distribution increases, which could lead to inaccuracies when solving the optimization problem.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between σ and the certified lower bound in various environments.
- What evidence would resolve it: Experimental results showing the certified lower bound for different σ values across multiple environments would help establish the relationship between σ and the certified lower bound.

### Open Question 2
- Question: How does the proposed method perform under non-adaptive adversaries compared to adaptive adversaries?
- Basis in paper: [inferred] The authors mention that (Wu et al., 2021) focused on certifying per-step perturbations against non-adaptive adversaries, while their method is designed for adaptive adversaries. However, a direct comparison is not provided.
- Why unresolved: The paper does not provide a comparison of the proposed method's performance under non-adaptive and adaptive adversaries.
- What evidence would resolve it: Experimental results comparing the proposed method's performance under both non-adaptive and adaptive adversaries would help determine its effectiveness in different attack scenarios.

### Open Question 3
- Question: How does the proposed method's performance change when applied to more complex, high-dimensional environments?
- Basis in paper: [inferred] The authors demonstrate the method's effectiveness in three standard environments, but the paper does not discuss its performance in more complex, high-dimensional environments.
- Why unresolved: The paper does not provide experimental results or analysis for more complex, high-dimensional environments.
- What evidence would resolve it: Experimental results showing the proposed method's performance in more complex, high-dimensional environments would help determine its scalability and applicability to real-world problems.

## Limitations
- The tightness of f-divergence bounds may degrade in high-dimensional state spaces, leading to overly conservative certificates
- The l0-norm certification extension relies on discrete probability modeling that may not scale to continuous or very large action spaces
- Monte Carlo sampling assumes bounded rewards, limiting applicability to environments where this assumption does not hold

## Confidence
- **High**: Theoretical derivation using f-divergence and Donsker-Varadhan formula for l2-norm perturbations
- **Medium**: Empirical results showing improved certified bounds on benchmark environments
- **Low**: l0-norm certification extension and scalability to high-dimensional continuous control tasks

## Next Checks

1. **Tightness Analysis**: Systematically vary the noise level σ and perturbation bound ε across multiple environments to quantify how the certified bound degrades. Compare against non-smooth baseline policies to verify the claimed improvement.

2. **l0-norm Scalability Test**: Implement the action space certification on a grid-world environment with varying action space sizes (from 4 to 100 discrete actions) to empirically measure the degradation in certification quality and computational efficiency.

3. **Distribution Verification**: Generate empirical cumulative distribution functions of cumulative rewards under perturbed policies and compare against the theoretical certified bounds to assess whether the convex optimization produces valid, non-vacuous certificates.