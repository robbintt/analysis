---
ver: rpa2
title: Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and Context
  Extraction
arxiv_id: '2308.02951'
source_url: https://arxiv.org/abs/2308.02951
tags:
- full
- measeval
- task
- corpus
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We construct a multi-domain corpus for measurement, unit and context
  extraction based on existing scientific corpora and pre-trained language models
  to benchmark the cross-domain generalization capability of our system. We evaluate
  adaptive pre-training and multi-source fine-tuning and perform an entity-level error
  analysis.
---

# Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and Context Extraction

## Quick Facts
- arXiv ID: 2308.02951
- Source URL: https://arxiv.org/abs/2308.02951
- Reference count: 40
- Key outcome: Multi-source training leads to best overall results, while single-source training yields best results for respective individual domains

## Executive Summary
This work addresses the challenge of extracting measurements, units, and their contextual entities across multiple scientific domains. The authors construct a multi-domain corpus from existing scientific datasets and evaluate different training strategies including single-source, multi-source, and adaptive pre-training approaches. Using a two-step token classification pipeline with BERT and SciBERT models, they demonstrate that multi-source training provides the best overall cross-domain generalization, while single-source training excels in individual domains. The system shows strong performance on quantity and unit extraction but struggles with contextual entities, particularly those with long-range dependencies.

## Method Summary
The authors implement a cascading two-step pipeline using BERT and SciBERT base models for measurement extraction. Task 1 performs binary IO tagging to extract quantities, while Task 2 uses BIO tagging to simultaneously predict units (U), measured entities (ME), and measured properties (MP) conditioned on the identified quantities. They create a multi-domain corpus from MeasEval, Battery Materials Patents, and Material Science Procedural corpora, then apply task-adaptive pre-training (TAPT) on unlabeled data followed by fine-tuning on single or multiple source domains. The system is evaluated using overlap F1 and token-based strict F1 metrics across cross-domain and in-domain settings.

## Key Results
- Multi-source training achieves the best overall cross-domain generalization performance
- Single-source training yields superior results for individual domain performance
- Quantity and unit extraction significantly outperforms contextual entity extraction
- Adapter-based pre-training was generally worse than full pre-training for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source training improves overall generalization by exposing the model to diverse domain patterns, while single-source training achieves higher in-domain performance.
- Mechanism: The model learns domain-invariant features from multiple sources, but also risks overfitting to specific patterns if trained on only one source. Multi-source training balances these effects by combining the strengths of each domain.
- Core assumption: The domains in the corpus are sufficiently diverse to provide complementary information for generalization.
- Evidence anchors:
  - [abstract] "Our results suggest that multi-source training leads to the best overall results, while single-source training yields the best results for the respective individual domain."
  - [section 5.2] "Cross-domain vs. in-domain : We observe that cross-domain fine-tuning is beneficial for overall generalization, while in-domain fine-tuning results in better in-domain performance."
- Break condition: If the domains are too similar, multi-source training may not provide additional benefits and could even lead to overfitting.

### Mechanism 2
- Claim: Intermediate pre-training on a curated multi-domain corpus improves model performance by adapting it to the target domain's characteristics.
- Mechanism: The model learns to recognize and extract relevant patterns from the target domain by being exposed to a larger corpus of similar data during pre-training.
- Core assumption: The curated corpus represents the target domain well enough to induce meaningful adaptations in the model.
- Evidence anchors:
  - [abstract] "We then apply multi-source task-adaptive pre-training and fine-tuning to benchmark the cross-domain generalization capability of our model."
  - [section 4] "We create intermediately pre-trained variants for each of the two BERT-models using task-adaptive pre-training (TAPT) (Gururangan et al., 2020): we continue pre-training of the models on the unla beled (training) data of our measurement extraction corpus."
- Break condition: If the curated corpus is too small or not representative of the target domain, the pre-training may not lead to significant improvements.

### Mechanism 3
- Claim: The cascading two-step pipeline effectively extracts quantities and their contexts by first identifying quantities and then using them to condition the extraction of related entities.
- Mechanism: The first step isolates the quantities, and the second step uses special tokens to mark the identified quantities, allowing the model to focus on extracting the related units and contextual entities for each quantity.
- Core assumption: The identified quantities are accurate enough to provide meaningful context for the extraction of related entities.
- Evidence anchors:
  - [section 3] "We model the extraction as a two-step pipeline made up of two token-classification models, which we coin as Task 1 and Task 2. We first extract all Quantities (Task 1) and then simultaneously predict U, ME and MP (Task 2) based on each extracted Quantity."
  - [section 5.2] "Table 3 shows the resulting E2E performance for selected model configurations, which allows us to assess the error propagation of the the cascading task flow."
- Break condition: If the quantity extraction in Task 1 is inaccurate, it can lead to errors in the extraction of related entities in Task 2.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: The task involves identifying and classifying specific entities (quantities, units, measured entities, measured properties) within text.
  - Quick check question: What is the difference between the BIO and IO tagging schemes used in the tasks?

- Concept: Transfer Learning
  - Why needed here: The model is pre-trained on a large corpus and then fine-tuned on the specific task of measurement extraction, leveraging knowledge learned from the pre-training phase.
  - Quick check question: How does task-adaptive pre-training differ from regular pre-training in the context of this work?

- Concept: Domain Adaptation
  - Why needed here: The model needs to generalize across different domains (e.g., scientific articles, patents) to perform well on unseen data.
  - Quick check question: What is the purpose of the curated multi-domain corpus used for intermediate pre-training?

## Architecture Onboarding

- Component map:
  - Input text -> Task 1 (Quantity extraction) -> Task 2 (Unit and context extraction) -> Output

- Critical path:
  - Input text -> Task 1 (Quantity extraction) -> Task 2 (Unit and context extraction) -> Output

- Design tradeoffs:
  - Single-source vs. multi-source training: Single-source may yield better in-domain performance, while multi-source improves overall generalization.
  - Full vs. adapter-based intermediate pre-training: Full pre-training may lead to better performance but is more computationally expensive.

- Failure signatures:
  - Poor quantity extraction in Task 1 leading to errors in Task 2
  - Overfitting to specific domains when using single-source training
  - Insufficient adaptation when using a small or unrepresentative curated corpus for intermediate pre-training

- First 3 experiments:
  1. Train Task 1 and Task 2 models on the MeasEval dataset only and evaluate their performance.
  2. Train Task 1 and Task 2 models on the multi-source corpus (MeasEval + MSP + BM) and evaluate their performance.
  3. Apply intermediate pre-training on the curated corpus and compare the performance of the pre-trained models with the non-pre-trained models on the multi-source corpus.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does adapter-based intermediate pre-training ever outperform full intermediate pre-training for measurement extraction tasks?
- Basis in paper: [explicit] The paper states that adapter-based intermediate pre-training was "worse in most cases" and suggests the task complexity may be a factor.
- Why unresolved: The paper only tested adapter-based pre-training with one configuration (pfeiffer+inv with reduction factor 12) and found it generally worse. There's no systematic exploration of whether different adapter configurations might yield better results.
- What evidence would resolve it: Experiments comparing adapter-based pre-training with different adapter architectures, reduction factors, and training configurations against full pre-training on larger pre-training corpora.

### Open Question 2
- Question: What is the optimal balance between specialized training data and general domain generalization for measurement extraction?
- Basis in paper: [explicit] The paper found that multi-source training produced the best overall results, but single-source training yielded the best results for individual domains, with the specialized BM dataset performing best in cross-domain settings.
- Why unresolved: The paper only tested three domains with limited data sizes. The relationship between dataset specialization, size, and generalization capability remains unclear.
- What evidence would resolve it: Systematic experiments varying the number of domains, dataset sizes, and domain similarity while measuring cross-domain performance on held-out domains.

### Open Question 3
- Question: How can long-range dependencies in measured entity extraction be effectively modeled?
- Basis in paper: [explicit] Error analysis revealed that the model struggles with long-range dependencies for measured entities, particularly when entities are far from their associated quantities.
- Why unresolved: The paper used a simple pipeline architecture with one-sentence context windows and didn't explore more sophisticated approaches for capturing long-range relationships.
- What evidence would resolve it: Experiments comparing different architectures (e.g., span-based models, graph-based approaches, or models with larger context windows) on measured entity extraction with varying entity-to-quantity distances.

## Limitations

- Domain coverage limitations: The multi-domain corpus is constructed from three scientific domains and may not adequately represent other measurement extraction scenarios.
- Error propagation in cascading pipeline: The two-step pipeline design creates dependency between tasks where errors in quantity extraction directly impact unit and context extraction.
- Computational resource constraints: The study uses BERT and SciBERT base models but doesn't explore larger models that might better handle cross-domain generalization.

## Confidence

- High confidence: The claim that multi-source training improves overall generalization while single-source training yields better in-domain performance is well-supported by experimental results.
- Medium confidence: The effectiveness of intermediate pre-training on curated multi-domain corpora is demonstrated but could be stronger with ablation studies.
- Medium confidence: The observation that quantity and unit extraction performs better than contextual entity extraction is supported by error analysis.

## Next Checks

1. **Cross-domain robustness test:** Evaluate the system on at least two additional domains outside the current corpus (e.g., medical records, financial reports) to assess true generalization capability beyond the tested domains.

2. **Error propagation quantification:** Design an experiment to systematically measure how Task 1 errors propagate to Task 2 by injecting controlled noise into quantity predictions and measuring downstream impact on unit and context extraction accuracy.

3. **Alternative architecture comparison:** Implement and compare the two-step pipeline against a single-step multi-label extraction approach to determine if the cascading design is optimal or if a unified model could achieve comparable or better performance with simpler error dynamics.