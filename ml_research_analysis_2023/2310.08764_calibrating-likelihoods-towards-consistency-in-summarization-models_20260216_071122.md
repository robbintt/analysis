---
ver: rpa2
title: Calibrating Likelihoods towards Consistency in Summarization Models
arxiv_id: '2310.08764'
source_url: https://arxiv.org/abs/2310.08764
tags:
- association
- summaries
- linguistics
- summarization
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to calibrate sequence likelihood of
  summarization models to improve their consistency. The core idea is to train models
  to rank candidate summaries based on textual entailment scores, using a multi-task
  learning objective.
---

# Calibrating Likelihoods towards Consistency in Summarization Models

## Quick Facts
- arXiv ID: 2310.08764
- Source URL: https://arxiv.org/abs/2310.08764
- Reference count: 40
- Key outcome: Proposed method significantly improves factual consistency and overall quality of generated summaries across five diverse datasets using NLI-based calibration

## Executive Summary
This paper introduces Sequence Likelihood Calibration with NLI (SLiC-NLI), a method to improve the factual consistency of abstractive summarization models by calibrating their sequence likelihood to align with natural language inference (NLI) scores. The approach generates multiple candidate summaries from a fine-tuned model, scores them using an NLI model, and then continues training with a contrastive reranking objective that increases likelihood of higher-NLI candidates while decreasing likelihood of lower-NLI ones. Experiments on five diverse summarization datasets demonstrate significant improvements in both factual consistency (measured by NLI scores) and overall quality (measured by ROUGE scores), with human evaluations confirming the gains in factuality.

## Method Summary
The SLiC-NLI method involves a two-stage process: first, generating candidate summaries from a fine-tuned T5 model using beam search and annotating them with NLI scores from a T5-11B model fine-tuned on ANLI; second, continuing training the model with a multi-task learning objective that combines a calibration loss based on contrastive reranking of candidates according to their NLI scores and a length regularization term to prevent exploitation of length-NLI correlations. The calibration weight β and length regularization weight α are key hyperparameters that control the trade-off between consistency improvements and maintenance of other quality metrics.

## Key Results
- SLiC-NLI achieves substantial improvements in factual consistency across all five tested datasets (CNN/DailyMail, ForumSum, RedditTIFU-long, SAMSum, XSUM)
- The method improves both automatic NLI metrics and human-judged factuality while maintaining or improving ROUGE scores
- Ablation studies confirm the effectiveness of both the calibration loss and length regularization components
- The approach shows particular strength on datasets with higher consistency demands (RedditTIFU-long, SAMSum)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibrating sequence likelihood with NLI scores improves factual consistency by aligning model probabilities with entailment judgments.
- Mechanism: The model generates multiple candidate summaries, computes NLI scores for each, and uses contrastive reranking during calibration to increase likelihood of higher-NLI candidates while decreasing likelihood of lower-NLI ones.
- Core assumption: NLI scores are reliable proxies for factual consistency and correlate well with human judgments.
- Evidence anchors: [abstract] "We solve this problem by calibrating the likelihood of model generated sequences to better align with a consistency metric measured by natural language inference (NLI) models."
- Break condition: If NLI models are poorly calibrated or have systematic biases, calibration may amplify these issues rather than improve consistency.

### Mechanism 2
- Claim: Length regularization prevents the model from exploiting NLI score correlations with summary length to artificially inflate consistency metrics.
- Mechanism: An additional regularization term penalizes length deviations from target summaries, scaled by α, to discourage generation of excessively long summaries that happen to score higher on NLI due to length correlation.
- Core assumption: There is a positive correlation between summary length and NLI scores that can be exploited by the model to "cheat" the consistency metric.
- Evidence anchors: [section] "It appears that there is a slight positive correlation between the length of the generated summaries and NLI."
- Break condition: If the length correlation is actually causal (longer summaries genuinely more consistent) rather than spurious, regularization could harm performance.

### Mechanism 3
- Claim: Multi-task learning with both calibration loss and KL divergence regularization maintains model quality while improving consistency.
- Mechanism: The calibration loss (contrastive reranking based on NLI) is combined with a KL divergence regularization term that prevents the calibrated model from deviating too far from the fine-tuned model's distribution.
- Core assumption: The KL divergence regularization effectively prevents catastrophic forgetting while allowing meaningful updates toward consistency.
- Evidence anchors: [section] "For the regularization loss Lreg we follow Zhao et al. (2023b) and use the KL divergence loss minimizing the probability distribution distance between the calibrated model and the fine-tuned model."

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI provides the consistency metric used to calibrate sequence likelihoods. Understanding how NLI models work and their limitations is crucial for implementing this approach.
  - Quick check question: What are the three classes in standard NLI datasets, and how does the entailment probability relate to these classes?

- Concept: Contrastive learning
  - Why needed here: The calibration method uses contrastive reranking of candidate summaries based on NLI scores, requiring understanding of how contrastive objectives work in sequence generation.
  - Quick check question: How does the contrastive loss in this paper differ from standard contrastive learning approaches used in representation learning?

- Concept: Multi-task learning with regularization
  - Why needed here: The approach combines calibration loss with KL divergence regularization, requiring understanding of how multiple objectives interact and how to balance them.
  - Quick check question: What happens if the regularization weight is set too high versus too low in this multi-task setup?

## Architecture Onboarding

- Component map: Fine-tuned summarization model (T5-Large) -> Candidate generation (beam search, size 5, alpha 0.8) -> NLI model (T5-11B fine-tuned on ANLI) -> Entailment scoring -> Calibration module (contrastive reranking loss + KL divergence regularization) -> Output: Calibrated summarization model

- Critical path: 1. Generate candidates from fine-tuned model 2. Score candidates with NLI model 3. Apply calibration loss with contrastive reranking 4. Apply regularization to prevent deviation 5. Output calibrated model

- Design tradeoffs: Calibration weight β vs. quality metrics: Higher β improves NLI but may hurt ROUGE scores; Length regularization weight α: Balances consistency gains against summary quality and repetition; Candidate generation strategy: Beam size affects diversity and computational cost; NLI model choice: Larger models may be more accurate but computationally expensive

- Failure signatures: NLI scores don't improve despite calibration: Calibration weight too low or regularization too strong; ROUGE scores drop significantly: Over-calibration or inappropriate regularization; Generated summaries become repetitive: Length regularization too weak or beam search parameters problematic; Training instability: KL divergence weight too high or candidate generation producing degenerate outputs

- First 3 experiments: 1. Ablation study on calibration weight β without length regularization to understand basic calibration effects; 2. Length regularization sweep to find optimal α balancing consistency and summary quality; 3. Full calibration with both components enabled, comparing against baselines on all five datasets

## Open Questions the Paper Calls Out

1. Can our method work effectively with instruction-tuned models of size PALM-2 and GPT-4?
2. How does the proposed method affect the creativity and helpfulness of model outputs?
3. Can the proposed method be extended to tasks beyond summarization?
4. How does the proposed method handle hallucinations not covered by the NLI metric?
5. What is the optimal trade-off between consistency and other metrics like ROUGE scores and summary length?

## Limitations

- The approach relies heavily on NLI scores as proxies for factual consistency, but NLI models themselves have known limitations including sensitivity to lexical variations and potential biases
- The calibration weight β is a critical hyperparameter that wasn't fully explored across the dataset spectrum
- The method requires generating multiple candidate summaries and computing NLI scores for each, significantly increasing inference time
- While tested on diverse datasets, it's unclear how well the method scales to extremely long documents

## Confidence

**High Confidence**: The core experimental results showing ROUGE and NLI score improvements are well-documented with specific numerical values across all five datasets.

**Medium Confidence**: The claim that the method improves factual consistency is supported by both automatic metrics and human evaluations, though the human evaluation methodology lacks detail.

**Low Confidence**: The assertion that length regularization is necessary to prevent "cheating" behavior is based on observed correlations rather than causal evidence.

## Next Checks

1. **NLI score validation study**: Conduct a human evaluation study specifically designed to validate whether the NLI scores used for calibration actually correlate with human judgments of factual consistency across all five datasets.

2. **Calibration weight sensitivity analysis**: Perform a systematic grid search over calibration weights β for each dataset to determine optimal values and identify whether there are systematic patterns in how β should be set based on dataset characteristics.

3. **Computational overhead benchmarking**: Measure and report the actual runtime overhead of the calibration approach compared to standard fine-tuning, including both training and inference times.