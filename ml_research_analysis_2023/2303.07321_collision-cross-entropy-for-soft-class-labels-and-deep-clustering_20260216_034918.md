---
ver: rpa2
title: Collision Cross-entropy for Soft Class Labels and Deep Clustering
arxiv_id: '2303.07321'
source_url: https://arxiv.org/abs/2303.07321
tags:
- cross-entropy
- loss
- entropy
- collision
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes collision cross-entropy as a loss function
  for training with soft labels, particularly in self-supervised clustering. The key
  idea is that instead of minimizing the standard cross-entropy which encourages the
  model to exactly reproduce the uncertainty in soft labels, collision cross-entropy
  maximizes the probability that two random variables (model prediction and unknown
  true label) are equal.
---

# Collision Cross-entropy for Soft Class Labels and Deep Clustering

## Quick Facts
- **arXiv ID:** 2303.07321
- **Source URL:** https://arxiv.org/abs/2303.07321
- **Reference count:** 40
- **Primary result:** Collision cross-entropy loss outperforms standard cross-entropy in self-supervised clustering by being more robust to uncertain pseudo-labels

## Executive Summary
This paper introduces collision cross-entropy as a loss function for training with soft labels in self-supervised clustering scenarios. The key insight is that instead of forcing the model to exactly reproduce uncertain pseudo-label distributions (as standard cross-entropy does), collision cross-entropy maximizes the probability that model predictions and true labels agree. This leads to more robust training that naturally reduces the impact of highly uncertain pseudo-labels. The authors derive an efficient EM algorithm for optimizing pseudo-labels and demonstrate consistent improvements over state-of-the-art methods across multiple datasets.

## Method Summary
The method replaces standard cross-entropy with collision cross-entropy in self-supervised clustering, where pseudo-labels are estimated jointly with model parameters. The loss is formulated as the collision entropy between predictions and pseudo-labels, combined with a fairness constraint via KL divergence. An efficient EM algorithm is derived for pseudo-label optimization using Newton's method, which converges faster than projected gradient descent. The method is evaluated on MNIST, CIFAR-10/100, and STL-10 using VGG-style networks and pretrained ResNet-50 features.

## Key Results
- Collision cross-entropy consistently improves clustering accuracy over state-of-the-art self-supervised methods across multiple datasets
- The EM algorithm for pseudo-label optimization converges significantly faster than projected gradient descent, especially as the number of classes increases
- The method shows superior robustness to label uncertainty compared to standard cross-entropy, as demonstrated in controlled label corruption experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Collision cross-entropy reduces the impact of uncertain pseudo-labels on training by treating label uncertainty differently than Shannon cross-entropy.
- Mechanism: When pseudo-labels are uniform distributions (high uncertainty), the collision probability becomes constant (1/K), leading to zero gradients. This contrasts with Shannon cross-entropy which still trains the model to mimic uncertainty.
- Core assumption: Soft pseudo-labels accurately represent uncertainty in self-labeled clustering.
- Evidence anchors: [abstract] "Unlike Shannon's CE, collision CE is symmetric for y and network predictions, which is particularly relevant when both distributions are estimated in the context of self-labeled clustering."

### Mechanism 2
- Claim: The symmetric property of collision cross-entropy is beneficial when both pseudo-labels and predictions are estimated jointly.
- Mechanism: Since collision cross-entropy treats both arguments symmetrically, it avoids the asymmetry where one distribution is treated as a fixed target while the other is optimized. This is particularly important in self-labeling where both y and σ are iteratively refined.
- Core assumption: Both pseudo-labels and model predictions are estimated distributions that should be treated equivalently in the loss.
- Evidence anchors: [abstract] "Unlike the Shannon's cross-entropy, our formulation is symmetric w.r.t. predictions σ and pseudo-labels y."

### Mechanism 3
- Claim: The EM algorithm for optimizing pseudo-labels converges faster than projected gradient descent.
- Mechanism: The derived EM algorithm exploits the convexity of the loss with respect to pseudo-labels and uses Newton's method for efficient optimization, achieving faster convergence especially as the number of classes increases.
- Core assumption: The collision cross-entropy loss is convex with respect to pseudo-labels, making EM optimization effective.
- Evidence anchors: [section 4] "Table 1. Comparison of our EM algorithm to Projected Gradient Descent (PGD)."

## Foundational Learning

- **Concept:** Information theory basics (entropy, cross-entropy, KL divergence)
  - Why needed here: The paper builds on these concepts to formulate the collision cross-entropy as an alternative to Shannon cross-entropy.
  - Quick check question: What's the difference between cross-entropy and KL divergence?

- **Concept:** Rényi entropy and its generalization
  - Why needed here: Collision cross-entropy is derived from Rényi entropy concepts, specifically the collision entropy (second-order Rényi entropy).
  - Quick check question: How does Rényi entropy of order α=2 relate to the probability of two independent samples being equal?

- **Concept:** Self-supervised learning and pseudo-labeling
  - Why needed here: The paper focuses on self-labeled classification where pseudo-labels are estimated jointly with model parameters.
  - Quick check question: What distinguishes self-supervised learning from supervised learning in terms of label availability?

## Architecture Onboarding

- **Component map:** Unlabeled data -> Backbone (VGG-4/ResNet-50) -> Classifier (linear layer) -> Softmax -> Collision cross-entropy + fairness term -> EM optimization for pseudo-labels

- **Critical path:**
  1. Initialize network and pseudo-labels
  2. Forward pass: compute predictions σ
  3. E-step: update cluster assignments Sk
  4. M-step: update pseudo-labels y using Newton's method
  5. Compute collision cross-entropy loss
  6. Backpropagate and update network parameters
  7. Repeat until convergence

- **Design tradeoffs:**
  - Symmetry vs. asymmetry: Collision cross-entropy treats y and σ symmetrically, while Shannon cross-entropy treats y as a fixed target
  - Robustness vs. sensitivity: Collision cross-entropy is more robust to label uncertainty but may be less sensitive to informative examples with high uncertainty
  - Computational efficiency: EM algorithm is faster than PGD but requires more complex implementation

- **Failure signatures:**
  - Slow convergence or oscillation: May indicate poor initialization or inappropriate hyperparameters (λ for fairness constraint)
  - All pseudo-labels becoming uniform: Could suggest the fairness constraint is too strong relative to the cross-entropy term
  - Poor test accuracy despite good training loss: Might indicate overfitting or that the collision cross-entropy is too aggressive in reducing gradients for uncertain examples

- **First 3 experiments:**
  1. Reproduce the fully-supervised label corruption experiment (Figure 2) to verify robustness to label uncertainty
  2. Compare EM vs. PGD optimization on a small dataset (e.g., MNIST) to verify faster convergence
  3. Test collision cross-entropy on a standard self-supervised clustering benchmark (e.g., STL-10 with VGG-4) to verify performance improvements

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the theoretical axioms or properties that uniquely characterize collision cross-entropy as a generalized cross-entropy measure?
- **Open Question 2:** How does collision cross-entropy perform in semi-supervised learning scenarios with varying amounts of labeled data?
- **Open Question 3:** What is the relationship between collision cross-entropy and other robust loss functions for training with noisy or uncertain labels?
- **Open Question 4:** How does the choice of λ in the fairness term affect the convergence and final performance of the EM algorithm?
- **Open Question 5:** Can collision cross-entropy be extended to handle structured output spaces or hierarchical label distributions?

## Limitations

- The experiments focus on standard benchmark datasets and may not capture performance in highly imbalanced or domain-shifted scenarios
- The theoretical analysis of the EM algorithm's convergence properties is limited, relying primarily on empirical comparisons with projected gradient descent
- The choice of fairness weight λ (set to 100 in all experiments) is not systematically explored, leaving uncertainty about optimal parameter selection

## Confidence

- **High Confidence:** The mathematical formulation of collision cross-entropy and its relationship to Rényi entropy is well-established and correctly derived
- **Medium Confidence:** The empirical improvements over state-of-the-art methods are significant and reproducible based on the experimental setup described
- **Medium Confidence:** The claim about faster EM convergence is supported by experimental evidence but lacks comprehensive theoretical analysis across all problem scales

## Next Checks

1. Evaluate collision cross-entropy on datasets with extreme class imbalance (e.g., long-tailed CIFAR variants) to verify if the gradient reduction for uncertain examples becomes detrimental when class distributions are highly skewed
2. Test the method with transformer-based architectures and Vision Transformers (ViTs) to determine if the benefits extend beyond convolutional networks used in the paper
3. Conduct a formal convergence analysis of the EM algorithm for pseudo-label optimization, proving convergence rates and conditions under which it outperforms projected gradient descent