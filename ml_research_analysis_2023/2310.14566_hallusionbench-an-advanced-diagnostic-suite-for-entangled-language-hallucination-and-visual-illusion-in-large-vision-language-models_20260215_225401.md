---
ver: rpa2
title: 'HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination
  and Visual Illusion in Large Vision-Language Models'
arxiv_id: '2310.14566'
source_url: https://arxiv.org/abs/2310.14566
tags:
- gpt-4v
- visual
- image
- llav
- illusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HallusionBench is a benchmark designed to evaluate the performance
  of large vision-language models (LVLMs) in handling image-context reasoning tasks.
  It consists of 346 images and 1129 questions, created by human experts to test models'
  ability to interpret visual data accurately.
---

# HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2310.14566
- Source URL: https://arxiv.org/abs/2310.14566
- Reference count: 4
- Primary result: GPT-4V achieved highest accuracy of 31.42%, other models scored below 16% on image-context reasoning tasks

## Executive Summary
HallusionBench is a benchmark designed to evaluate large vision-language models (LVLMs) on their ability to accurately interpret visual data and balance visual and language reasoning. The benchmark consists of 346 images and 1129 questions, focusing on two main types of errors: language hallucination (relying on prior knowledge instead of image context) and visual illusion (misinterpreting visual information). Testing 15 different models revealed significant performance gaps, with GPT-4V achieving the highest accuracy of 31.42% while other models scored below 16%. This highlights the ongoing challenges LVLMs face in properly integrating visual and language processing capabilities.

## Method Summary
HallusionBench employs a control group design where the same question is asked on both original and edited images. The benchmark includes "hard negative examples" with subtle modifications that change the correct answer, forcing models to choose between relying on parametric memory or actual visual input. Images are categorized into Visual Dependent (requiring visual context) and Visual Supplement (can be answered without visual input) types. The evaluation measures question-pair accuracy by comparing model responses to ground truth answers for both original and edited image versions.

## Key Results
- GPT-4V achieved highest accuracy of 31.42% on HallusionBench, while other tested models scored below 16%
- LVLMs show systematic failures in balancing visual and language reasoning, often preferring parametric memory over image context
- Models demonstrated specific weaknesses in optical illusion interpretation, geometric reasoning, and temporal sequence understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HallusionBench exposes the trade-off between language priors and visual context processing in LVLMs by using "hard negative" edits that create contradictions with known knowledge.
- Mechanism: The benchmark pairs original images with subtly edited versions that change the correct answer, forcing models to choose between relying on their parametric memory or the actual visual input.
- Core assumption: LVLMs have a strong bias toward language-based reasoning over visual reasoning, which can be triggered by introducing known visual illusions or contradictory visual edits.
- Evidence anchors:
  - [abstract] "This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes."
  - [section] "We design the control groups for analysis by asking the same question on both the original image from the Internet as well as the edited image based on the original one."
- Break condition: If LVLMs develop stronger visual grounding mechanisms or better visual-language integration that reduces reliance on parametric memory for visual tasks.

### Mechanism 2
- Claim: HallusionBench reveals visual illusion failures by testing models on optical illusions, geometric reasoning, and temporal sequence understanding.
- Mechanism: The benchmark includes specific categories of visual challenges (illusions, geometry, videos/multiple images) that exploit known weaknesses in visual perception systems, measuring how often models fail to interpret visual information correctly.
- Core assumption: Visual modules in LVLMs are inherently weaker than language modules, leading to systematic failures on tasks requiring precise visual interpretation.
- Evidence anchors:
  - [abstract] "This benchmark presents significant challenges to advanced large visual-language models... by emphasizing nuanced understanding and interpretation of visual data."
  - [section] "We further explore GPT-4V's and LLaVA-1.5's abilities in Optical Character Recognition... Both models have bad performance in recognizing and measuring length."
- Break condition: If visual reasoning capabilities in LVLMs improve to match or exceed language reasoning capabilities, or if models develop better visual grounding techniques.

### Mechanism 3
- Claim: HallusionBench demonstrates that LVLMs suffer from parametric memory over-fitting, preferring to answer based on memorized knowledge rather than analyzing visual context.
- Mechanism: The benchmark includes "visual supplement" questions where correct answers can be derived from either parametric memory or visual input, revealing whether models default to memory even when visual context contradicts it.
- Core assumption: LVLMs prioritize language-based reasoning from parametric memory over visual analysis, even when visual evidence is available and contradicts the memory-based answer.
- Evidence anchors:
  - [abstract] "the strong language prior in these SOTA LVLMs can be a double-edged sword: they may ignore the image context and solely rely on the (even contradictory) language prior for reasoning."
  - [section] "Given the image context, GPT-4V and LLaVA-1.5 are unable to understand the chart correctly, indicating that their chart reasoning ability is still limited."
- Break condition: If LVLMs develop better mechanisms for weighing visual context against parametric memory, or if training methods improve visual-language integration.

## Foundational Learning

- Concept: Visual vs. Language Modality Integration
  - Why needed here: Understanding how LVLMs combine visual and language processing is crucial for interpreting HallusionBench results and designing improvements.
  - Quick check question: What are the key architectural differences between how LVLMs process visual input versus language input?

- Concept: Optical Illusions and Visual Perception
  - Why needed here: HallusionBench uses optical illusions to test visual perception capabilities, requiring understanding of how these illusions work and why they challenge AI systems.
  - Quick check question: How do classic optical illusions like the Chubb illusion work, and what aspects of visual processing do they exploit?

- Concept: Parametric Memory vs. Contextual Reasoning
  - Why needed here: The benchmark reveals trade-offs between relying on memorized knowledge versus analyzing current context, which is fundamental to understanding model failures.
  - Quick check question: What mechanisms do LVLMs use to balance parametric memory against contextual information from input data?

## Architecture Onboarding

- Component map: Image-Question Pair -> Visual Feature Extraction -> Text Response Generation -> Ground Truth Comparison -> Failure Type Categorization (Visual Illusion vs. Language Hallucination) -> Pattern Analysis
- Critical path: Load image → Extract visual features → Generate text response → Evaluate against ground truth → Categorize failure type (Visual Illusion vs. Language Hallucination) → Analyze patterns across model versions
- Design tradeoffs: Balancing between challenging models enough to reveal weaknesses versus creating tasks that are impossible even for humans; choosing between quantitative metrics and qualitative case studies for evaluation
- Failure signatures: Language Hallucination shows up as consistent answers regardless of visual context changes; Visual Illusion appears as confident but incorrect visual interpretations; Mixed failures occur when both mechanisms are triggered
- First 3 experiments:
  1. Run HallusionBench on a baseline model to establish failure rates and patterns
  2. Create targeted edits of specific image types to isolate whether failures are visual or language-based
  3. Test model variants with different visual-language integration approaches to compare performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve large vision-language models (LVLMs) to reduce language hallucination and visual illusion errors?
- Basis in paper: [explicit] The paper highlights that GPT-4V and LLaVA-1.5 suffer from language hallucination and visual illusion, leading to incorrect answers in image-context reasoning tasks.
- Why unresolved: Despite advancements in LVLMs, they still struggle to balance visual and language reasoning, often relying on parametric memory or misinterpreting visual data.
- What evidence would resolve it: Developing and testing new training methods or architectures that prioritize accurate image-context interpretation over prior knowledge, and evaluating their performance on benchmarks like HallusionBench.

### Open Question 2
- Question: What specific mechanisms can enhance LVLMs' ability to distinguish between positive and reversed sequences in videos or multiple images?
- Basis in paper: [explicit] The paper shows that GPT-4V fails to capture temporal relations in sequences, such as distinguishing between "disappear" and "appear" or "park" and "leave."
- Why unresolved: LVLMs lack true temporal reasoning ability, leading to errors in understanding the order and context of actions in sequences.
- What evidence would resolve it: Creating and testing models that incorporate explicit temporal reasoning modules or training on datasets that emphasize sequence understanding.

### Open Question 3
- Question: How can LVLMs be trained to better handle image manipulations (e.g., flipping, order reversing, character editing) without losing accuracy?
- Basis in paper: [explicit] The paper demonstrates that GPT-4V and LLaVA-1.5 are easily misled by simple image manipulations, such as flipping charts or editing text in images.
- Why unresolved: Current LVLMs are sensitive to visual perturbations, which can lead to incorrect interpretations of the image context.
- What evidence would resolve it: Developing robust training techniques that expose models to manipulated images and evaluating their resilience to such perturbations.

## Limitations
- Limited corpus evidence supporting specific mechanisms proposed, with most related work focusing on hallucination detection rather than the particular control-group approach used
- Benchmark's ground truth answers and specific editing methodologies are not fully detailed in the paper, limiting independent verification
- Evaluation shows only 31.42% accuracy even for best-performing model (GPT-4V), suggesting significant limitations in current LVLM capabilities

## Confidence
- **High Confidence**: The existence of a performance gap between GPT-4V and other models, and the general observation that LVLMs struggle with image-context reasoning tasks
- **Medium Confidence**: The claim that LVLMs show systematic failures in balancing visual vs. language reasoning, based on the benchmark design and results presented
- **Low Confidence**: The specific mechanisms explaining why LVLMs fail (parametric memory over-fitting, visual illusion weaknesses) due to limited corpus evidence and unclear implementation details

## Next Checks
1. Obtain and run HallusionBench with the complete ground truth answers on at least 5 different LVLM models to verify the reported accuracy gap between GPT-4V and other models
2. Conduct ablation studies by testing models with progressively more challenging image edits to determine whether failures are primarily due to language hallucination or visual illusion weaknesses
3. Compare HallusionBench results against other hallucination benchmarks to assess whether the observed failure patterns are unique to this benchmark or representative of general LVLM limitations