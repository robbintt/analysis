---
ver: rpa2
title: A Cross Attention Approach to Diagnostic Explainability using Clinical Practice
  Guidelines for Depression
arxiv_id: '2311.13852'
source_url: https://arxiv.org/abs/2311.13852
tags:
- psat
- phq-9
- attention
- depression
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of explainability in AI-powered analysis
  of unstructured clinical dialogue, specifically in the context of mental health.
  The authors propose PSAT, a knowledge-infused cross-attention model that incorporates
  clinical practice guidelines (CPGs) for diagnosing depression.
---

# A Cross Attention Approach to Diagnostic Explainability using Clinical Practice Guidelines for Depression

## Quick Facts
- arXiv ID: 2311.13852
- Source URL: https://arxiv.org/abs/2311.13852
- Reference count: 36
- Primary result: PSAT achieves MCC of 44.4% on CLEF e-Risk and 39.8% on PRIMATE datasets for depression detection, surpassing nine baseline models

## Executive Summary
This paper addresses the critical challenge of explainability in AI systems for mental health diagnosis by proposing PSAT (Phrase-driven Semantic Attention Transformer), a knowledge-infused cross-attention model that incorporates clinical practice guidelines (specifically PHQ-9) for diagnosing depression. The model generates clinician-understandable explanations by aligning user text with clinically relevant concepts from the PHQ-9 ontology. PSAT demonstrates superior performance over nine baseline models on three expert-curated datasets while providing interpretable attention visualizations that mental health professionals can understand.

## Method Summary
PSAT uses a cross-attention mechanism that incorporates PHQ-9 clinical practice guidelines to enhance both accuracy and explainability in depression detection. The approach involves extracting keyphrases from user posts using KeyBERT, KeyBART, and KeyBERT+POS methods, then mapping these phrases to a PHQ-9 ontology containing 12 diagnostic questions with associated concepts. The model employs 12 cross-attention blocks - one for each PHQ-9 question - that compute attention between input tokens and phrase embeddings derived from the ontology. This knowledge-infused attention is then used for classification while providing interpretable explanations through attention visualizations aligned with clinical concepts.

## Key Results
- PSAT achieves Matthews Correlation Coefficient (MCC) of 44.4% on CLEF e-Risk dataset and 39.8% on PRIMATE dataset
- Outperforms nine baseline models including BERT, RoBERTa, Longformer, and knowledge-infused variants
- Achieves 72.1% accuracy on R-CSSRS dataset for suicide risk assessment
- Provides application-relevant explainability through attention visualizations aligned with PHQ-9 questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention blocks enable alignment of user text with clinically relevant PHQ-9 phrases
- Core assumption: Phrases mapping to PHQ-9 concepts are relevant for prediction
- Evidence: PSAT's attention matches ground truth PHQ-9 question importance vs. uniform attention in Longformer
- Break condition: Phrase embedding matrix fails to capture clinical semantics

### Mechanism 2
- Claim: PHQ-9 ontology integration provides MHP-meaningful explanations
- Core assumption: MHPs trust explanations grounded in familiar clinical concepts
- Evidence: Attention visualizations correspond to diagnostic questions used by professionals
- Break condition: Phrase-to-concept mapping becomes non-intuitive or heatmaps too cluttered

### Mechanism 3
- Claim: Knowledge infusion improves both accuracy and interpretability
- Core assumption: Standard transformers miss clinically relevant signals with broad attention
- Evidence: Performance gains (higher MCC, AKC) demonstrate improved precision
- Break condition: Overfitting to PHQ-9 ontology degrades performance on new conditions

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed: PSAT modifies standard self-attention to cross-attention
  - Quick check: What is the difference between self-attention and cross-attention in a transformer block?

- Concept: Clinical practice guidelines (e.g., PHQ-9)
  - Why needed: PSAT uses PHQ-9 questions as knowledge source
  - Quick check: How many questions are in the PHQ-9 and what do they assess?

- Concept: Explainability metrics (MCC, AKC)
  - Why needed: PSAT's contribution is both accuracy and interpretability
  - Quick check: What does a high AKC score indicate about the model's attention?

## Architecture Onboarding

- Component map: Input → Keyphrase extraction (KeyBERT/KeyBART/KeyBERT+POS) → Phrase tagging → PHQ-9 ontology lookup → 12 cross-attention blocks → Concatenation → Feed-forward classifier
- Critical path: Phrase extraction and embedding → Cross-attention computation → Prediction
- Design tradeoffs: Fixed ontology ensures clinical relevance but limits flexibility; cross-attention is more interpretable but computationally heavier
- Failure signatures: Uniform attention across PHQ-9 questions (model not leveraging knowledge); extremely sparse attention (extraction/embedding failed)
- First 3 experiments:
  1. Ablation: Disable cross-attention blocks to confirm performance drop
  2. Phrase extraction comparison: Replace KeyBERT+POS with TF-IDF and measure AKC change
  3. Ontology size test: Remove 50% of PHQ-9 concepts and check impact on accuracy and explainability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PSAT's performance on other mental health conditions compare to depression?
- Basis: Paper mentions potential for anxiety detection using GAD-7 but provides no results
- Why unresolved: Only depression and suicide risk datasets tested
- What evidence would resolve it: Experiments on anxiety datasets with performance comparison

### Open Question 2
- Question: How does PHQ-9 ontology size affect PSAT's performance and explainability?
- Basis: Ontology creation discussed but size-performance relationship unexplored
- Why unresolved: No experiments with ontologies of different sizes
- What evidence would resolve it: Performance evaluation across varying ontology sizes

### Open Question 3
- Question: How does PSAT handle out-of-vocabulary words not in PHQ-9 ontology?
- Basis: Paper doesn't address OOV word handling
- Why unresolved: No discussion of out-of-vocabulary handling
- What evidence would resolve it: Analysis of PSAT's behavior on texts containing OOV words

## Limitations
- Modest performance improvements (44.4% MCC) with unclear statistical significance
- Explainability claims lack direct clinician validation or user studies
- Knowledge infusion approach may have limited generalizability beyond depression/PHQ-9 framework

## Confidence
- Cross-attention mechanism improves explainability: **Medium** - Supported by AKC but lacks clinician validation
- Performance gains are meaningful: **Low-Medium** - Modest improvements, statistical significance unclear
- Transferability to other conditions: **Low-Medium** - Single suicide dataset tested, no domain-specific comparisons

## Next Checks
1. Conduct clinician study to evaluate whether PSAT's explanations are actually useful for clinical decision-making
2. Perform paired statistical tests comparing PSAT to strongest baselines to verify performance differences are significant
3. Evaluate PSAT on non-depression mental health conditions with different diagnostic criteria to assess knowledge-infusion transferability