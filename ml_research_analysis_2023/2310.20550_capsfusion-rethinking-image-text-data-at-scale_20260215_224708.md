---
ver: rpa2
title: 'CapsFusion: Rethinking Image-Text Data at Scale'
arxiv_id: '2310.20550'
source_url: https://arxiv.org/abs/2310.20550
tags:
- captions
- arxiv
- synthetic
- fusion
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies scalability deficiency and world knowledge
  loss issues in models trained with synthetic captions, caused by oversimplified
  language structures in existing synthetic caption data. To address this, the authors
  propose CapsFusion, a framework that leverages large language models to consolidate
  and refine information from both web-based image-text pairs and synthetic captions.
---

# CapsFusion: Rethinking Image-Text Data at Scale

## Quick Facts
- **arXiv ID:** 2310.20550
- **Source URL:** https://arxiv.org/abs/2310.20550
- **Authors:** [Multiple authors]
- **Reference count:** 40
- **Primary result:** CapsFusion captions achieve 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps datasets respectively, while requiring 11-16x less computation than baselines.

## Executive Summary
This paper identifies critical scalability limitations and world knowledge loss in multimodal models trained with synthetic captions, caused by oversimplified language structures in existing synthetic caption data. The authors propose CapsFusion, a framework that uses ChatGPT to merge knowledge-rich raw captions with structured synthetic captions, creating high-quality fused captions that retain both factual accuracy and syntactic consistency. By finetuning an open-source LLaMA model on these fused captions, CapsFusion achieves superior performance while reducing computational requirements by 11-16 times compared to synthetic caption approaches. The resulting CapsFusion-120M dataset demonstrates state-of-the-art performance across multiple image captioning benchmarks.

## Method Summary
CapsFusion operates by first generating synthetic captions for images using a captioning model, then using ChatGPT to organically integrate information from both raw and synthetic captions through carefully crafted instructions. The framework leverages ChatGPT's ability to extract real-world knowledge from noisy raw captions while maintaining the syntactic structure of synthetic captions. To address scalability limitations of direct ChatGPT usage, the authors fine-tune LLaMA-2 on a subset of ChatGPT-generated fused captions, creating CapsFusion-LLaMA for large-scale caption generation. This finetuned model is then used to create the CapsFusion-120M dataset, which is employed to train multimodal models that demonstrate superior performance and scalability compared to models trained on synthetic captions alone.

## Key Results
- CapsFusion captions achieve 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps datasets respectively
- The approach requires 11-16 times less computation than baselines while maintaining superior performance
- CapsFusion demonstrates greater scalability, with model performance continuing to improve as training data volume increases
- The resulting CapsFusion-120M dataset achieves state-of-the-art performance across multiple image captioning benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CapsFusion improves model performance by merging structured synthetic captions with knowledge-rich raw captions through LLM-guided fusion
- **Mechanism:** ChatGPT extracts real-world knowledge from noisy raw captions while maintaining synthetic caption syntax, producing integrated captions that outperform both sources
- **Core assumption:** ChatGPT can effectively parse and integrate information from both caption types when given clear instructions about their respective strengths
- **Evidence anchors:** 18.8 CIDEr improvement on COCO, 18.3 on NoCaps; ChatGPT instructions to integrate raw and synthetic captions
- **Break condition:** If ChatGPT cannot distinguish between raw and synthetic caption properties or fails to maintain syntactic structure while incorporating knowledge

### Mechanism 2
- **Claim:** Finetuning LLaMA with ChatGPT-generated fused captions creates a scalable alternative matching ChatGPT performance
- **Mechanism:** Finetuned LLaMA learns to replicate ChatGPT's fusion behavior, enabling large-scale caption generation without API constraints
- **Core assumption:** Fusion patterns learned from ChatGPT can be effectively transferred to LLaMA through finetuning
- **Evidence anchors:** ChatGPT-generated captions used as training data; 12-day training on 128 A100-40G GPUs
- **Break condition:** If finetuned model fails to generalize fusion behavior beyond training examples or produces significantly degraded quality

### Mechanism 3
- **Claim:** CapsFusion captions demonstrate superior scalability compared to synthetic captions, with performance continuing to improve with increased training volume
- **Mechanism:** Incorporating diverse real-world knowledge from raw captions prevents early saturation observed in synthetic caption-only models
- **Core assumption:** Including real-world knowledge prevents performance degradation from oversimplified synthetic captions
- **Evidence anchors:** CapsFusion captions show continued improvement with increased samples; synthetic captions typically saturate at 30 million pairs
- **Break condition:** If model performance plateaus or degrades despite increasing training data volume

## Foundational Learning

- **Concept:** Multimodal pretraining with image-text pairs
  - Why needed here: CapsFusion fundamentally addresses the quality and scalability of image-text data used in multimodal pretraining
  - Quick check question: How does the quality of image-text pairs affect the performance of large multimodal models?

- **Concept:** Large language model fine-tuning for specialized tasks
  - Why needed here: The approach relies on fine-tuning LLaMA to replicate ChatGPT's caption fusion capabilities
  - Quick check question: What are the key considerations when fine-tuning a large language model for a specialized task like caption fusion?

- **Concept:** Evaluation metrics for image captioning (CIDEr, SPICE, NoCaps)
  - Why needed here: CapsFusion performance is evaluated using multiple image captioning benchmarks
  - Quick check question: How do CIDEr and SPICE metrics differ in evaluating image captioning quality?

## Architecture Onboarding

- **Component map:** Image captioning model (BLIP) → ChatGPT → LLaMA-2 (finetuned) → Large multimodal model (LLaMA-2-7B + EVA-01-CLIP-g)
- **Critical path:** Raw image-text pairs → Synthetic caption generation → LLM-guided fusion → Finetuned caption generation → LMM training → Performance evaluation
- **Design tradeoffs:**
  - Quality vs scalability: ChatGPT provides higher quality but limited scalability; LLaMA finetuning sacrifices some quality for massive scale
  - Computation cost: Training on fused captions requires less data than synthetic captions (11-16x speedup) but involves additional fusion computation
  - Knowledge depth vs noise: Raw captions provide rich knowledge but are noisy; synthetic captions are clean but lack depth
- **Failure signatures:**
  - Model performance plateaus early (indicates synthetic caption limitations)
  - Generated captions lack specific details (indicates fusion instruction failure)
  - Finetuned model produces repetitive patterns (indicates overfitting during finetuning)
  - Performance degrades with increased training data (indicates scalability issues)
- **First 3 experiments:**
  1. Generate 1000 fused captions using ChatGPT and evaluate human preference vs synthetic captions
  2. Finetune LLaMA-2-13B on 10,000 fused caption examples and compare output quality to ChatGPT
  3. Train LMM on 1M synthetic vs 1M CapsFusion captions and measure performance difference on COCO validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CapsFusion's approach of using ChatGPT for caption fusion scale when dealing with multi-lingual datasets or captions in non-English languages?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on English-language captions and datasets, but doesn't address how the approach would work with multilingual data or whether the current prompting strategy would be effective across different languages
- What evidence would resolve it: Experimental results showing CapsFusion's performance on multilingual datasets or tests of the approach with translated captions in various languages would demonstrate its cross-lingual applicability

### Open Question 2
- Question: What are the computational and resource implications of scaling CapsFusion to larger datasets (e.g., 1 billion+ image-text pairs) compared to traditional synthetic caption generation approaches?
- Basis in paper: [inferred]
- Why unresolved: While the paper demonstrates CapsFusion's effectiveness on 120M pairs and compares computational costs to baselines, it doesn't address the practical challenges and resource requirements of scaling to much larger datasets
- What evidence would resolve it: Detailed analysis of computational costs, GPU requirements, and training time for CapsFusion at different scales (100M, 1B, 10B+ pairs) would provide insights into its scalability limitations and practical deployment considerations

### Open Question 3
- Question: How robust is CapsFusion to different types of raw caption noise and quality variations, and what are the failure modes when dealing with extremely noisy or irrelevant raw captions?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that raw captions are noisy but doesn't provide detailed analysis of how different levels or types of noise affect the caption fusion process
- What evidence would resolve it: Systematic experiments testing CapsFusion with captions of varying noise levels, including synthetic noise injection and real-world noisy datasets, would reveal its robustness and failure modes in challenging scenarios

## Limitations
- CapsFusion approach shows significant dependence on ChatGPT API availability and cost, creating potential bottlenecks
- Performance claims rely on comparisons with specific synthetic caption datasets that may not generalize to all approaches
- Quality assessment metrics primarily focus on CIDEr and SPICE scores, which may not fully capture semantic and factual accuracy improvements

## Confidence

**High confidence** in the core mechanism: The fusion of structured synthetic captions with knowledge-rich raw captions through LLM guidance represents a well-founded approach with clear theoretical justification and supported by 18.8 CIDEr score improvement.

**Medium confidence** in scalability claims: While improved performance with increased training data is demonstrated, the exact mechanisms preventing early saturation require further investigation across different model architectures.

**Medium confidence** in world knowledge depth: SEED-Bench evaluation suggests superior knowledge representation, but qualitative assessment means findings could vary depending on specific knowledge domains tested.

## Next Checks

1. **Knowledge integration validation**: Conduct a human evaluation study where annotators compare factual accuracy and detail richness of CapsFusion vs synthetic captions across 1000 diverse images, focusing on whether claimed knowledge integration is consistently achieved.

2. **Scalability stress test**: Train multimodal models on CapsFusion-120M dataset with progressively increasing subsets (10M, 30M, 60M, 120M) and measure whether performance improvement trend continues beyond reported results, examining whether new saturation points emerge.

3. **Generalization to other synthetic caption sources**: Apply CapsFusion framework to synthetic captions generated by alternative models (e.g., BLIP-2, Flamingo) rather than only LAION-COCO, and measure whether similar improvements in quality and scalability are observed across different synthetic caption generation approaches.