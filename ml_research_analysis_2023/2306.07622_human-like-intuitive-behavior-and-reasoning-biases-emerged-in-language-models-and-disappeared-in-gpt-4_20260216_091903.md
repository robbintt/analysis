---
ver: rpa2
title: Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models
  -- and Disappeared in GPT-4
arxiv_id: '2306.07622'
source_url: https://arxiv.org/abs/2306.07622
tags:
- cost
- take
- long
- takes
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how language models, particularly GPT-3, exhibit
  human-like intuitive reasoning biases and cognitive errors. The authors use the
  Cognitive Reflection Test (CRT) and semantic illusions to probe these models' decision-making
  processes.
---

# Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4

## Quick Facts
- arXiv ID: 2306.07622
- Source URL: https://arxiv.org/abs/2306.07622
- Reference count: 40
- Primary result: GPT-3 exhibits human-like intuitive reasoning biases on CRT tasks, but GPT-4 shows hyperrational behavior with minimal intuitive errors

## Executive Summary
This paper investigates whether large language models exhibit human-like intuitive reasoning biases using the Cognitive Reflection Test (CRT) and semantic illusions. The authors find that GPT-3 strongly exhibits intuitive but incorrect responses, mirroring human cognitive biases. However, more advanced models like ChatGPT and GPT-4 significantly outperform their predecessors, showing hyperrational behavior and largely avoiding intuitive errors. The study demonstrates that investigating LLMs with psychological methods can reveal emergent traits and suggests a remarkable increase in cognitive capabilities in newer models.

## Method Summary
The study tests multiple LLMs including GPT-3, GPT-4, ChatGPT, BLOOM, LLaMA, and others using 50 variants each of three CRT task types and 50 semantic illusions. Models are prompted with tasks and responses are categorized as correct, intuitive, or atypical. The researchers examine modified conditions including multiple-choice format, chain-of-thought prompts, and few-shot learning with training examples. All tasks are run with temperature=0 for reproducibility, and ambiguous responses are manually verified.

## Key Results
- GPT-3 exhibits 67% intuitive response rate on CRT tasks, closely mirroring human cognitive biases
- GPT-4 and ChatGPT show dramatically reduced intuitive errors (2% and 10% respectively), demonstrating hyperrational behavior
- Chain-of-thought prompting and few-shot learning significantly improve GPT-3's performance but have minimal effect on GPT-4

## Why This Works (Mechanism)

### Mechanism 1
Language models exhibit human-like intuitive biases because they are trained on human-generated text that encodes probabilistic patterns of cognitive ease and error. The model's probability distribution over tokens reflects the statistical prevalence of intuitive but incorrect responses in human language. When prompted with reasoning tasks, the model samples from these learned distributions, producing outputs that mirror human cognitive shortcuts.

### Mechanism 2
LLMs can overcome intuitive biases through chain-of-thought reasoning and few-shot learning, which enables multi-step deliberation. When prompted with suffixes like "Let's think step by step" or provided with training examples, the model decomposes complex reasoning tasks into sequential sub-problems. This process bypasses the immediate token sampling that produces intuitive errors.

### Mechanism 3
The decrease in intuitive errors from GPT-3 to GPT-4 is primarily due to architectural improvements, larger parameter counts, and better fine-tuning techniques rather than training data contamination. GPT-4's enhanced architecture allows for more sophisticated world knowledge representation and better integration of factual knowledge with reasoning processes.

## Foundational Learning

- Concept: Cognitive Reflection Test (CRT)
  - Why needed here: The CRT is the primary experimental tool used to measure intuitive vs. deliberative reasoning in both humans and LLMs. Understanding its structure and purpose is essential for interpreting the results.
  - Quick check question: What are the three types of CRT tasks and how do they each trigger intuitive but incorrect responses?

- Concept: Dual-process theory of cognition
  - Why needed here: The paper frames LLM behavior in terms of System 1 (intuitive) vs. System 2 (deliberative) thinking, which is central to understanding why models exhibit certain biases and how they can overcome them.
  - Quick check question: How does the paper characterize the difference between intuitive behavior and deliberate reasoning in LLMs?

- Concept: Semantic illusions
  - Why needed here: Semantic illusions provide a non-mathematical way to test intuitive reasoning biases, complementing the CRT and demonstrating that the observed phenomena are not specific to mathematical ability.
  - Quick check question: What makes semantic illusions effective at testing intuitive decision-making, and how do they differ from CRT tasks in their approach?

## Architecture Onboarding

- Component map: LLM <- Prompt <- Task dataset <- Evaluation script <- Statistical analysis
- Critical path: 1) Prepare prompt with task and evaluation suffix/training examples, 2) Feed prompt to LLM with temperature=0, 3) Capture model output, 4) Automatically categorize response based on presence of correct/intuitive keywords, 5) Manually verify ambiguous cases, 6) Aggregate results across all tasks, 7) Perform statistical significance tests
- Design tradeoffs: Using temperature=0 ensures reproducibility but may miss cases where the model's probability distribution could yield correct answers with some randomness. Manual verification of responses is time-consuming but necessary for accurate categorization, especially for semantic illusions where context matters.
- Failure signatures: High rates of "atypical" responses indicate the model lacks the necessary knowledge or reasoning capabilities. Persistent intuitive errors despite chain-of-thought prompting suggest the model's architecture cannot support the required deliberation. Inconsistent results across different prompt formulations may indicate sensitivity to prompt engineering.
- First 3 experiments:
  1. Run CRT tasks on GPT-3 with default settings to establish baseline intuitive error rate
  2. Apply chain-of-thought suffix ("Let's think step by step") to same tasks and measure improvement
  3. Provide 5 training examples of correct CRT solutions before testing to evaluate few-shot learning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural changes in GPT-4 led to its dramatic reduction in intuitive reasoning errors compared to GPT-3? The authors note that GPT-4 shows "a significant drop in intuitive responses to 2%" and hypothesize about architectural changes but OpenAI does not publish technical details. This remains unresolved because OpenAI has not released technical specifications for GPT-4, making it impossible to determine which architectural modifications caused the change in behavior.

### Open Question 2
Do LLMs exhibit genuine intuitive reasoning or are they merely mimicking human-like errors through pattern recognition? The authors state "intuition-like behavior in LLMs seems to be a systematic effect that is emerging when prompting the model with new, previously unseen tasks" and question whether models truly possess intuition. This remains unresolved because the paper demonstrates behavioral similarity to human intuition but cannot definitively determine whether LLMs experience genuine intuitive processes or are simply reproducing patterns from training data.

### Open Question 3
How does the ecological rationality of LLMs compare to humans when dealing with semantic illusions in natural communication? The authors discuss whether LLMs should "go with the conversational flow and just overlook small mistakes instead of correcting factually incorrect questions" and mention "ecological rationality" from the literature. This remains unresolved because while the paper shows LLMs can detect semantic illusions, it does not examine whether their behavior represents an optimal adaptation to real-world communication contexts.

## Limitations
- Potential training data contamination of GPT-4 and ChatGPT with CRT/semantic illusion datasets
- Limited testing of open-source alternatives like BLOOM and LLaMA
- Results may be heavily influenced by specific prompt formulations and evaluation protocols

## Confidence
- High Confidence: The empirical finding that GPT-3 exhibits human-like intuitive biases on CRT tasks is robust and well-supported by the data
- Medium Confidence: The claim that GPT-4 and ChatGPT show significantly reduced intuitive errors is supported but requires caution due to potential training data contamination
- Low Confidence: The broader interpretation that these findings demonstrate a "remarkable increase in cognitive capabilities" over model generations extends beyond what can be directly measured

## Next Checks
1. Conduct a comprehensive analysis of GPT-4's training corpus to determine whether CRT tasks or similar reasoning problems were present
2. Test the same CRT and semantic illusion tasks across a broader range of model architectures to determine whether the GPT-3â†’GPT-4 pattern is architecture-specific
3. Systematically vary prompt formulations, temperature settings, and evaluation suffixes to map the sensitivity of intuitive error rates to prompt engineering