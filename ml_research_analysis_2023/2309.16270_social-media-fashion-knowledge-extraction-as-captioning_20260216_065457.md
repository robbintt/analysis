---
ver: rpa2
title: Social Media Fashion Knowledge Extraction as Captioning
arxiv_id: '2309.16270'
source_url: https://arxiv.org/abs/2309.16270
tags:
- fashion
- knowledge
- post
- task
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of extracting structured fashion
  knowledge (occasion, person attributes, fashion item details) from multimodal social
  media posts. The key insight is to reframe this extraction as a captioning problem,
  generating a natural language description that can then be parsed into structured
  tuples.
---

# Social Media Fashion Knowledge Extraction as Captioning

## Quick Facts
- arXiv ID: 2309.16270
- Source URL: https://arxiv.org/abs/2309.16270
- Reference count: 40
- Primary result: Achieved state-of-the-art results on fashion knowledge extraction from social media posts using a captioning approach

## Executive Summary
This paper addresses the task of extracting structured fashion knowledge (occasion, person attributes, fashion item details) from multimodal social media posts. The key insight is to reframe this extraction as a captioning problem, generating a natural language description that can then be parsed into structured tuples. The authors propose a framework built on a multimodal pre-trained generative model (VL-Bart), enhanced with auxiliary tasks like visual question answering, image-text matching, and sentence reconstruction to improve fashion-specific knowledge capture. They also introduce a new dataset of social media posts with manual fashion knowledge annotations. The proposed method achieves state-of-the-art results, outperforming both classification-based and other generation-based baselines, particularly in complex multi-person and multi-fashion-item scenarios.

## Method Summary
The proposed method treats fashion knowledge extraction as a captioning problem using a VL-Bart-based encoder-decoder architecture. The model takes social media posts (images and text) as input and generates natural language captions describing fashion knowledge elements. These captions are then parsed into structured tuples containing occasion, person attributes, and fashion item details. The framework is enhanced with auxiliary pre-training tasks including sentence reconstruction, image-text matching, and visual question answering to improve fashion-specific knowledge capture. The model is fine-tuned on a newly collected dataset of Instagram fashion posts with manual annotations covering six occasion categories.

## Key Results
- The captioning approach significantly outperforms classification-based methods on fashion knowledge extraction tasks
- VL-Bart-based model achieves higher F1 scores than baselines, particularly for complex multi-person and multi-fashion-item scenarios
- Auxiliary tasks (SRC, ITM, VQA) provide substantial improvements in fashion knowledge extraction performance
- The generated captions can be effectively parsed back into structured fashion knowledge tuples with high accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming fashion knowledge tuples into natural language captions enables better modeling of modality interplay than classification-based approaches.
- Mechanism: The caption generation approach leverages the inherent dependencies between fashion knowledge elements (occasion → person attributes → fashion items) by generating them in sequence, allowing the model to condition each aspect on previous ones.
- Core assumption: Fashion knowledge aspects are not strictly independent but exhibit strong contextual dependencies.
- Evidence anchors:
  - [abstract]: "cast the task as a captioning problem to capture the interplay of the multimodal post information"
  - [section]: "Compared with existing classification-based models, our model incorporates the multimodal information from both the image and text as input and utilizes the natural language caption as the output, which can better capture the interactions between different modalities"
- Break condition: If fashion knowledge aspects were truly independent, the sequential generation would not provide additional benefit over parallel classification.

### Mechanism 2
- Claim: Auxiliary tasks (SRC, ITM, VQA) improve fashion knowledge extraction by pre-training the model with task-specific knowledge.
- Mechanism: Each auxiliary task focuses on different aspects of fashion knowledge - SRC for understanding fashion-related vocabulary, ITM for matching fashion captions to images, and VQA for answering fashion-specific questions about occasion, person attributes, and fashion items.
- Core assumption: Pre-training on related tasks improves performance on the main task by providing relevant prior knowledge.
- Evidence anchors:
  - [abstract]: "we design several auxiliary tasks including visual question answering (VQA), sentence reconstruction, and image-text matching to warm-up the model"
  - [section]: "These tasks are designed to equip the model with fashion-related knowledge via different formats but under the same model architecture"
- Break condition: If the auxiliary tasks were not sufficiently related to fashion knowledge extraction, they would not improve performance.

### Mechanism 3
- Claim: Multimodal pre-trained models (VL-Bart) provide better fashion knowledge extraction than non-pretrained models.
- Mechanism: VL-Bart's pre-training on image-text pairs enables it to understand the semantic relationships between visual and textual information, which is crucial for extracting fashion knowledge from social media posts.
- Core assumption: Pre-trained multimodal models have learned useful representations for understanding fashion-related content.
- Evidence anchors:
  - [abstract]: "we build our model based on a multimodal pre-trained generative model named VL-Bart [5] to utilize its rich knowledge of processing information from different modalities"
  - [section]: "Compared with existing classification-based models, our model incorporates the multimodal information from both the image and text as input"
- Break condition: If the pre-trained model's knowledge was not relevant to fashion knowledge extraction, fine-tuning would not provide significant benefits.

## Foundational Learning

- Concept: Multimodal learning and cross-modal attention mechanisms
  - Why needed here: The task requires integrating information from both images and text to extract fashion knowledge
  - Quick check question: Can you explain how cross-modal attention helps the model focus on relevant image regions when processing text descriptions?

- Concept: Sequence-to-sequence modeling and autoregressive generation
  - Why needed here: The caption generation approach requires generating text sequences that describe fashion knowledge
  - Quick check question: What is the difference between autoregressive and non-autoregressive generation in the context of fashion knowledge extraction?

- Concept: Pre-training and transfer learning
  - Why needed here: The model leverages pre-trained VL-Bart to improve performance on the fashion knowledge extraction task
  - Quick check question: How does pre-training on image-text pairs help the model extract fashion knowledge from social media posts?

## Architecture Onboarding

- Component map: Social media post (image + text) -> VL-Bart encoder (multimodal feature extraction) -> VL-Bart decoder (caption generation) -> Natural language caption -> Tuple recovery (structured fashion knowledge)

- Critical path:
  1. Image and text encoding using Faster-RCNN and text embedding layers
  2. Multimodal feature fusion in the transformer encoder
  3. Caption generation through the transformer decoder
  4. Tuple recovery from the generated caption

- Design tradeoffs:
  - Caption generation vs. direct tuple classification: Generation captures dependencies but is more complex
  - Pre-trained model vs. training from scratch: Pre-training provides better initialization but requires more computational resources
  - Pipeline-based vs. end-to-end: End-to-end avoids error propagation but requires more complex training

- Failure signatures:
  - Low precision in tuple extraction: Check caption generation quality and tuple recovery logic
  - Poor performance on multi-person posts: Verify person-attribute alignment in caption generation
  - Degradation with complex fashion items: Examine auxiliary task effectiveness for fashion-specific knowledge

- First 3 experiments:
  1. Test caption generation quality by comparing BLEU/METEOR scores with ground truth captions
  2. Evaluate tuple recovery accuracy by parsing generated captions and comparing with ground truth tuples
  3. Assess the impact of auxiliary tasks by training with and without each auxiliary task and measuring performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the fashion knowledge extraction model be extended to handle more complex fashion-related concepts beyond the predefined categories of occasion, person attributes, and fashion item details?
- Basis in paper: [explicit] The paper mentions that the format of fashion knowledge aspects is typically quite varied and that traditional classification-based models require manually determining a set of fashion knowledge categories in advance.
- Why unresolved: The paper focuses on extracting specific fashion knowledge elements but does not explore how to handle more nuanced or evolving fashion concepts that may not fit into predefined categories.
- What evidence would resolve it: Developing a method to dynamically learn and adapt to new fashion concepts without manual category definition, and evaluating its performance on a diverse set of fashion-related posts with varying complexity.

### Open Question 2
- Question: Can the captioning approach be further improved by incorporating additional contextual information beyond the post image and text, such as user profiles or historical fashion trends?
- Basis in paper: [inferred] The paper acknowledges the importance of multimodal information but primarily focuses on the image and text content of individual posts. It does not explore incorporating broader contextual information.
- Why unresolved: While the paper demonstrates the effectiveness of using image and text, it does not investigate whether incorporating additional contextual information could further enhance the fashion knowledge extraction performance.
- What evidence would resolve it: Designing and evaluating a model that incorporates user profiles, historical fashion trends, or other contextual information alongside the post image and text, and comparing its performance to the current approach.

### Open Question 3
- Question: How can the proposed framework be adapted to handle fashion knowledge extraction in languages other than English, considering the potential challenges of multilingual text processing and cultural differences in fashion expression?
- Basis in paper: [explicit] The paper mentions that the raw text in social media is often noisy and employs text cleaning methods, including translation to English using Google Translate API.
- Why unresolved: The paper primarily focuses on English-language posts and does not address the challenges and potential solutions for handling fashion knowledge extraction in other languages.
- What evidence would resolve it: Developing and evaluating a multilingual version of the framework, considering the specific challenges of different languages and cultural contexts, and comparing its performance to the English-only model.

## Limitations

- The Instagram-based dataset may not capture full diversity of social media fashion posts across different platforms and demographics
- The model's performance on non-Instagram fashion content (e-commerce, fashion blogs) remains untested
- Error propagation in the pipeline approach (caption generation → tuple extraction) could create cascading errors

## Confidence

**High confidence**: The experimental results showing VL-Bart outperforming classification baselines are well-supported by the provided metrics and comparisons. The architectural design choices are clearly justified.

**Medium confidence**: The claims about auxiliary tasks improving fashion-specific knowledge are supported but could benefit from more detailed ablation studies showing the individual contribution of each auxiliary task.

**Low confidence**: The claim that caption generation is fundamentally better than classification for capturing "interplay" lacks strong empirical support - the paper shows better results but doesn't prove this is due to the mechanism described rather than other factors like model capacity.

## Next Checks

1. **Ablation study on auxiliary tasks**: Train models with individual auxiliary tasks removed to quantify their specific contributions to performance gains.

2. **Cross-platform generalization test**: Evaluate the model on fashion posts from platforms other than Instagram (e.g., Twitter, Pinterest, or fashion blogs) to assess domain transferability.

3. **Error analysis pipeline**: Conduct detailed error analysis at each stage (caption generation, tuple recovery) to identify where errors originate and whether the pipeline approach creates compounding issues.