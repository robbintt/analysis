---
ver: rpa2
title: Order Matters in the Presence of Dataset Imbalance for Multilingual Learning
arxiv_id: '2312.06134'
source_url: https://arxiv.org/abs/2312.06134
tags:
- pre-training
- loss
- cross-entropy
- fine-tuning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-phase training method for multi-task
  learning in the presence of dataset imbalance. The method involves first pre-training
  on a high-resource task, then fine-tuning on a mixture of high- and low-resource
  tasks.
---

# Order Matters in the Presence of Dataset Imbalance for Multilingual Learning

## Quick Facts
- arXiv ID: 2312.06134
- Source URL: https://arxiv.org/abs/2312.06134
- Authors: 
- Reference count: 40
- Primary result: Two-phase training significantly improves low-resource task performance in multilingual learning while maintaining high-resource task performance.

## Executive Summary
This paper addresses the challenge of dataset imbalance in multi-task learning by proposing a two-phase training method. The approach involves pre-training on a high-resource task followed by fine-tuning on a mixture of high- and low-resource tasks with adjusted sampling rates. The method demonstrates superior performance on low-resource tasks compared to standard static weighting approaches, achieving better results while using only a fraction of the data. The authors validate their approach in both neural machine translation and multilingual language modeling settings.

## Method Summary
The method involves a two-phase training pipeline where a model is first pre-trained on a high-resource task to learn generalizable representations. After pre-training, the optimizer state and learning rate schedule are reset, and the model is fine-tuned on a mixture of high- and low-resource tasks with a higher sampling rate for the low-resource tasks. This approach enables positive transfer from high-resource to low-resource tasks while the controlled training duration provides implicit regularization. The method is compared against static sampling (scalarization) and restart baselines to demonstrate its effectiveness in improving low-resource task performance.

## Key Results
- Pre-training on high-resource tasks enables positive transfer and better initialization for low-resource tasks
- Higher sampling rates are more data-efficient for low-resource tasks but require pre-training to mitigate overfitting risks
- The two-phase method achieves superior low-resource task performance while using only a fraction of the data compared to static sampling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-training on high-resource tasks provides better initialization for low-resource tasks through positive transfer.
- **Mechanism**: High-resource task training learns generalizable representations that transfer beneficially to low-resource tasks, improving their optimization starting point.
- **Core assumption**: High-resource and low-resource tasks share underlying representations that enable positive transfer.
- **Evidence anchors**:
  - [abstract] "pre-training enables positive transfer and better initialization for fine-tuning"
  - [section] "Pre-training utilizes positive transfer between tasks, and initializes the fine-tuning phase at a better starting point than random initialization"
  - [corpus] "Multitask learning has gained increased attention in being able to learn many tasks in an efficient way due to parameter sharing and transfer between tasks."
- **Break condition**: Tasks are too dissimilar for meaningful transfer to occur.

### Mechanism 2
- **Claim**: Higher sampling rates are more data-efficient for low-resource tasks when training time is limited.
- **Mechanism**: When training duration is constrained, higher sampling rates provide more gradient updates for low-resource tasks, accelerating their convergence despite the risk of overfitting.
- **Core assumption**: Data efficiency benefits outweigh overfitting risks when training time is controlled.
- **Evidence anchors**:
  - [section] "Higher sampling rates are more data-efficient than lower sampling rates. Figure 4 shows how optimization (training set performance) gets more and more data-efficient as the sampling rate increases."
  - [section] "By design, pre-training joint fine-tuning has two separate training phases which allows the low-resource-training phase to be short. This in turn enables us to increase the low-resource sampling rate, resulting in faster training."
  - [corpus] "Prior works on intermediate training take advantage of cross-task transfer to improve downstream task performance."
- **Break condition**: Extended training duration causes severe overfitting despite pre-training benefits.

### Mechanism 3
- **Claim**: Pre-training provides a regularization effect that mitigates overfitting in low-resource tasks.
- **Mechanism**: The two-stage training process inherently limits exposure time for low-resource tasks during fine-tuning, reducing overfitting risk.
- **Core assumption**: The regularization effect comes primarily from controlled training duration rather than explicit regularization techniques.
- **Evidence anchors**:
  - [section] "Figure 2 shows that pre-training joint fine-tuning yields worse training set performance, and therefore, could be seen as having a regularization effect."
  - [section] "Figure 5 shows that regularization by itself does not explain the superior performance of our scheme."
  - [corpus] "Recent work [6] found that naive temperature sampling might lead to overfitting of low-count languages, and suggested epoch capping with a uniform distribution for high-count languages."
- **Break condition**: Explicit regularization techniques (e.g., dropout) become more effective than the implicit regularization.

## Foundational Learning

- **Concept**: Multi-task learning and task balancing
  - Why needed here: Understanding how to balance training across tasks with different data availability is central to the paper's approach
  - Quick check question: What is the primary challenge when training a single model on tasks with vastly different amounts of training data?

- **Concept**: Transfer learning and representation sharing
  - Why needed here: The paper's effectiveness relies on transfer from high-resource to low-resource tasks
  - Quick check question: What is the fundamental assumption behind using pre-training on high-resource tasks to benefit low-resource tasks?

- **Concept**: Pareto optimality in multi-objective optimization
  - Why needed here: The paper frames the problem as finding optimal trade-offs between task performances
  - Quick check question: What does it mean for a solution to be Pareto optimal in the context of multi-task learning?

## Architecture Onboarding

- **Component map**: Pre-training phase -> Joint fine-tuning phase -> Task sampler with configurable sampling rates -> Optimizer with resettable state and learning rate schedule -> Model checkpointing system for pre-trained weights

- **Critical path**: 
  1. Pre-train on high-resource task until convergence
  2. Reset optimizer state and learning rate schedule
  3. Fine-tune on mixed task distribution with higher low-resource sampling rate
  4. Monitor low-resource task performance to prevent overfitting

- **Design tradeoffs**:
  - Pre-training duration vs. fine-tuning time allocation
  - Sampling rate for low-resource tasks vs. overfitting risk
  - Model capacity vs. transfer effectiveness
  - Training budget vs. performance optimization

- **Failure signatures**:
  - Low-resource tasks overfitting despite pre-training
  - High-resource task performance degrading significantly
  - No improvement over static sampling despite pre-training
  - Unstable training after optimizer reset

- **First 3 experiments**:
  1. Replicate the two-task NMT experiment (En→Fr + En→Ro) to verify pre-training benefits
  2. Test different pre-training durations to find optimal balance
  3. Compare with baseline static sampling with and without optimizer reset to isolate pre-training effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the pre-training joint fine-tuning method improve the Pareto front in scenarios where there are multiple low-resource tasks?
- Basis in paper: [inferred] The paper demonstrates improvements in the Pareto front for scenarios with a single low-resource task in both NMT and multilingual language modeling settings. However, it does not explicitly explore the case with multiple low-resource tasks.
- Why unresolved: The paper focuses on scenarios with one high-resource and one low-resource task in NMT, and multiple tasks in multilingual language modeling but does not specifically address the case of multiple low-resource tasks.
- What evidence would resolve it: Conducting experiments with multiple low-resource tasks in both NMT and multilingual language modeling settings to compare the performance of the pre-training joint fine-tuning method against static sampling.

### Open Question 2
- Question: How does the pre-training joint fine-tuning method perform when the high-resource task is not linguistically related to the low-resource tasks?
- Basis in paper: [inferred] The paper shows improvements in low-resource tasks when pre-training on a high-resource task, but it does not explore the impact of linguistic relatedness between the tasks.
- Why unresolved: The paper does not provide evidence on the effectiveness of the method when there is a lack of linguistic similarity between the high-resource and low-resource tasks.
- What evidence would resolve it: Experiments comparing the performance of the pre-training joint fine-tuning method when the high-resource task is linguistically related versus unrelated to the low-resource tasks.

### Open Question 3
- Question: Is there an optimal length for the pre-training phase that balances the benefits of transfer learning and the risks of overfitting?
- Basis in paper: [explicit] The paper discusses the impact of pre-training length on performance but does not provide a definitive answer on the optimal length.
- Why unresolved: The paper suggests that longer pre-training can improve performance but also acknowledges that there might be a trade-off with the length of the fine-tuning phase and the risk of overfitting.
- What evidence would resolve it: A systematic study varying the length of the pre-training phase to determine the point at which additional pre-training no longer yields significant improvements or starts to negatively impact performance.

### Open Question 4
- Question: How does the pre-training joint fine-tuning method scale with model size and dataset size?
- Basis in paper: [inferred] The paper demonstrates the method's effectiveness on a 13B parameter model and discusses dataset imbalance, but does not explicitly analyze the scaling behavior with respect to model and dataset size.
- Why unresolved: The paper does not provide a detailed analysis of how the method's performance changes as the model size and dataset size increase.
- What evidence would resolve it: Experiments scaling both the model size and dataset size to observe the method's performance trends and identify any potential limitations or diminishing returns.

## Limitations

- The method's effectiveness depends on the assumption that high-resource and low-resource tasks share sufficient underlying representations for positive transfer, which may not hold for dissimilar tasks.
- The paper focuses on dataset imbalance but does not address other forms of imbalance such as label distribution skew or concept drift over time.
- The generalizability of the findings beyond the specific experimental setups (NMT and multilingual language modeling) is uncertain without further testing in other domains.

## Confidence

**High confidence**: The empirical results demonstrating superior low-resource task performance with the two-phase training method are well-supported by the experimental evidence. The comparison with baseline methods (static sampling and restart baseline) is thorough and convincing.

**Medium confidence**: The claim that pre-training provides a regularization effect through controlled training duration is supported but not definitively proven. The paper acknowledges that explicit regularization techniques were not explored, leaving open the possibility that other regularization methods could be equally or more effective.

**Low confidence**: The generalizability of the findings beyond the specific experimental setups (NMT and multilingual language modeling) is uncertain. The paper does not provide theoretical bounds or conditions under which the method would fail, making it difficult to assess applicability to other domains or task types.

## Next Checks

1. **Transferability test**: Apply the two-phase training method to a different domain (e.g., computer vision or reinforcement learning) with dataset imbalance to assess generalizability beyond NLP tasks.

2. **Task dissimilarity experiment**: Systematically vary the similarity between high-resource and low-resource tasks (e.g., using increasingly dissimilar language pairs or task types) to identify the boundary conditions where positive transfer breaks down.

3. **Ablation on regularization**: Implement and compare explicit regularization techniques (dropout, weight decay, early stopping) against the implicit regularization of the two-phase method to determine if the benefits are primarily due to regularization or other factors.