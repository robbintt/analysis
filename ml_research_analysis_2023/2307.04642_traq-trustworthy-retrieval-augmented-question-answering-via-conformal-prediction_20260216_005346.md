---
ver: rpa2
title: 'TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction'
arxiv_id: '2307.04642'
source_url: https://arxiv.org/abs/2307.04642
tags:
- prediction
- question
- conformal
- global
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of hallucinations in retrieval-augmented
  question answering systems, where large language models generate incorrect responses
  based on made-up facts. The authors propose TRAQ, a method that uses conformal prediction
  to provide statistical guarantees on the correctness of the retrieved and generated
  answers.
---

# TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction

## Quick Facts
- arXiv ID: 2307.04642
- Source URL: https://arxiv.org/abs/2307.04642
- Authors: [List of authors]
- Reference count: 25
- Key outcome: Provides statistical guarantees on answer correctness in retrieval-augmented QA systems while reducing prediction set size by 16.2% on average compared to ablation

## Executive Summary
This paper addresses the critical problem of hallucinations in retrieval-augmented question answering systems, where large language models generate incorrect responses based on made-up facts. The authors propose TRAQ, a method that leverages conformal prediction to provide statistical guarantees on the correctness of retrieved and generated answers. By constructing prediction sets for both retrieval and QA components and combining them using global testing, TRAQ ensures high-probability coverage of the correct answer while maintaining efficient prediction set sizes through Bayesian optimization.

## Method Summary
TRAQ combines conformal prediction with Bayesian optimization to create trustworthy retrieval-augmented QA systems. The method first constructs individual conformal predictors for retrieval and QA components using split conformal prediction. It then combines these using global hypothesis testing (Bonferroni Correction or Harmonic Mean p-value) to ensure end-to-end coverage guarantees. Bayesian optimization is employed to tune hyperparameters that affect prediction set size without compromising coverage guarantees. The method uses Dense Passage Retriever (DPR) for retrieval and gpt-3.5-turbo (ChatGPT) for question answering, with semantic clustering to group similar answers and improve prediction set quality.

## Key Results
- TRAQ provides statistical guarantees on answer correctness in retrieval-augmented QA systems
- Prediction set size reduced by 16.2% on average compared to ablation method
- Maintains desired coverage guarantee while optimizing prediction set efficiency
- Experiments conducted on Natural Questions dataset demonstrate effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction provides statistical guarantees on answer correctness in retrieval-augmented QA systems.
- Mechanism: TRAQ uses split conformal prediction to construct prediction sets that contain the correct answer with high probability. It creates individual conformal predictors for retrieval and QA components, then combines them using global hypothesis testing to ensure end-to-end coverage.
- Core assumption: The calibration data is representative of the test distribution and the nonconformity measures are appropriate for measuring answer correctness.
- Evidence anchors:
  - [abstract]: "TRAQ uses conformal prediction, a statistical technique for constructing prediction sets that are guaranteed to contain the semantically correct response with high probability."
  - [section]: "Theorem 3.1. We have Pr (B∼DN ,(x,y)∼D) (y ∈ C(x)) ≥ 1 − α."
- Break condition: The statistical guarantees break if the calibration data distribution differs significantly from the test distribution or if the nonconformity measures fail to capture semantic equivalence.

### Mechanism 2
- Claim: Bayesian optimization minimizes prediction set size while maintaining coverage guarantees.
- Mechanism: TRAQ uses Bayesian optimization to tune hyperparameters of the global test (such as αret and αQA for Bonferroni correction) to minimize prediction set size without affecting the correctness guarantee.
- Core assumption: The optimization objective correlates with prediction set size and the optimization space is smooth enough for Bayesian optimization to work effectively.
- Evidence anchors:
  - [abstract]: "Additionally, Bayesian optimization is employed to optimize hyperparameters and minimize the size of the prediction sets."
  - [section]: "Although these hyperparameters do not affect the correctness guarantee, they can significantly affect the resulting prediction set sizes."
- Break condition: Bayesian optimization fails if the cost function is noisy or if the relationship between hyperparameters and set size is non-smooth or discontinuous.

### Mechanism 3
- Claim: Semantic clustering enables more meaningful prediction sets for QA tasks.
- Mechanism: TRAQ uses Monte Carlo sampling and clustering to group semantically similar answers, then constructs nonconformity measures based on cluster membership rather than individual answer probabilities.
- Core assumption: Semantically similar answers cluster well using the chosen clustering method (entailment model or Rouge scores) and the cluster-based nonconformity measure captures true semantic equivalence.
- Evidence anchors:
  - [section]: "To address this limitation, we build on an idea proposed in Kuhn et al. (2023), and propose to use negative semantic confidence as the NCM, which we can estimate via Monte Carlo sampling and clustering."
  - [section]: "Given a prompt, the LLM predicts a set of answers guaranteed to include the correct answer with high probability."
- Break condition: The semantic clustering fails if the clustering algorithm doesn't capture true semantic equivalence or if semantically different answers are incorrectly grouped together.

## Foundational Learning

- Concept: Conformal prediction
  - Why needed here: Provides the statistical foundation for guaranteeing answer correctness without making distributional assumptions.
  - Quick check question: What is the main difference between split conformal prediction and standard conformal prediction in terms of computational complexity?

- Concept: Global hypothesis testing
  - Why needed here: Allows combining multiple statistical tests (retrieval and QA) while controlling the family-wise error rate.
  - Quick check question: How does the Bonferroni correction ensure family-wise error rate control when combining multiple hypothesis tests?

- Concept: Bayesian optimization
  - Why needed here: Optimizes hyperparameters that affect prediction set size without compromising coverage guarantees.
  - Quick check question: What is the main advantage of using Bayesian optimization over grid search for hyperparameter tuning in this context?

## Architecture Onboarding

- Component map: Data preprocessing → DPR retriever → ChatGPT QA → Semantic clustering → Conformal prediction → Bayesian optimization → Prediction set output
- Critical path: DPR retrieval → ChatGPT answer generation → Semantic clustering → Conformal prediction threshold computation → Prediction set construction → Bayesian optimization hyperparameter tuning
- Design tradeoffs:
  - Using DPR limits context retrieval quality but is efficient; more sophisticated retrievers could improve performance but increase computational cost
  - Restricting to top-20 contexts reduces API costs but may miss relevant contexts
  - Semantic clustering improves answer deduplication but adds computational overhead
  - Bayesian optimization requires additional held-out data for tuning
- Failure signatures:
  - Coverage below target (α) indicates distributional shift between calibration and test data
  - Very large prediction sets suggest suboptimal hyperparameter tuning or poor nonconformity measures
  - Inconsistent semantic clustering results indicate issues with the clustering method
- First 3 experiments:
  1. Verify coverage guarantees hold at various α levels using the calibration set
  2. Compare prediction set sizes with and without Bayesian optimization
  3. Test semantic clustering quality by manually evaluating a sample of clustered answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the optimization set impact the performance of Bayesian optimization in selecting hyperparameters for global tests?
- Basis in paper: [inferred] The paper mentions using Bayesian optimization to optimize hyperparameters but does not explore the effect of optimization set size.
- Why unresolved: The paper does not provide experiments or analysis on how varying the size of the optimization set affects the performance of Bayesian optimization.
- What evidence would resolve it: Experiments comparing the performance of Bayesian optimization with different optimization set sizes, showing the impact on prediction set sizes and coverage rates.

### Open Question 2
- Question: How do different nonconformity measures for the question answering model affect the coverage and prediction set sizes?
- Basis in paper: [explicit] The paper proposes using negative semantic confidence as the nonconformity measure for the question answering model.
- Why unresolved: The paper does not compare the proposed nonconformity measure with other potential measures, such as log probability or other semantic similarity metrics.
- What evidence would resolve it: Experiments comparing different nonconformity measures for the question answering model, showing their impact on coverage rates and prediction set sizes.

### Open Question 3
- Question: How does the performance of the proposed method change when using a different retriever or question answerer model?
- Basis in paper: [explicit] The paper uses Dense Passage Retriever (DPR) as the retriever and gpt-3.5-turbo (ChatGPT) as the question answerer.
- Why unresolved: The paper does not explore the impact of using different retriever or question answerer models on the performance of the proposed method.
- What evidence would resolve it: Experiments using different retriever and question answerer models, showing their impact on coverage rates, prediction set sizes, and overall performance of the proposed method.

### Open Question 4
- Question: How does the proposed method perform on datasets other than Natural Questions?
- Basis in paper: [explicit] The paper evaluates the proposed method on the Natural Questions dataset.
- Why unresolved: The paper does not provide results on other datasets, limiting the generalizability of the findings.
- What evidence would resolve it: Experiments evaluating the proposed method on other question answering datasets, showing its performance in terms of coverage rates and prediction set sizes across different domains and question types.

## Limitations
- The statistical guarantees rely heavily on the assumption that calibration data distribution matches test distribution, which may not hold in practice
- Semantic clustering approach may incorrectly group semantically similar but incorrect answers, potentially inflating prediction set size
- The method's performance across different domains and question types is not thoroughly validated

## Confidence

**High Confidence**: The core claim that conformal prediction can provide statistical guarantees for answer correctness is well-established in the literature and supported by the paper's theoretical analysis. The methodology for constructing prediction sets using split conformal prediction is clearly defined and follows established protocols.

**Medium Confidence**: The claim that Bayesian optimization effectively minimizes prediction set size while maintaining coverage guarantees is supported by the experimental results, but the optimization process could be sensitive to the choice of acquisition function and the smoothness of the objective function. The specific hyperparameters and optimization parameters used are not fully detailed in the paper.

**Low Confidence**: The semantic clustering approach for deduplicating answers shows promise but has unclear limitations. The paper mentions using entailment models or Rouge scores for clustering, but the robustness of these methods across different domains and question types is not thoroughly validated. The impact of clustering quality on overall system performance remains uncertain.

## Next Checks

1. **Distribution Shift Analysis**: Conduct experiments where the calibration and test sets are sampled from different distributions (e.g., different domains or time periods) to evaluate the robustness of the coverage guarantees under distribution shift.

2. **Ablation Study on Semantic Clustering**: Compare the performance of TRAQ with and without semantic clustering across different clustering methods (entailment models vs. Rouge scores) to quantify the impact of clustering quality on prediction set size and coverage.

3. **Error Analysis of Prediction Sets**: Manually analyze a sample of prediction sets from the test set to identify common patterns in incorrect answers, particularly focusing on cases where semantically similar but incorrect answers are included in the prediction sets.