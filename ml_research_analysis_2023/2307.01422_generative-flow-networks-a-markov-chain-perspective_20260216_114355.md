---
ver: rpa2
title: 'Generative Flow Networks: a Markov Chain Perspective'
arxiv_id: '2307.01422'
source_url: https://arxiv.org/abs/2307.01422
tags:
- markov
- state
- chain
- distribution
- measure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a unifying perspective on Generative Flow
  Networks (GFlowNets) as recurrent Markov chains, applicable to both discrete and
  general state spaces. The key contribution is demonstrating that GFlowNets, which
  treat sampling as a sequential decision-making problem, can be understood within
  the same theoretical framework as Markov chain Monte Carlo (MCMC) methods.
---

# Generative Flow Networks: a Markov Chain Perspective

## Quick Facts
- arXiv ID: 2307.01422
- Source URL: https://arxiv.org/abs/2307.01422
- Reference count: 40
- Primary result: Establishes GFlowNets as recurrent Markov chains applicable to both discrete and general state spaces

## Executive Summary
This paper presents a unifying perspective on Generative Flow Networks (GFlowNets) by connecting them to Markov chain theory. The authors demonstrate that GFlowNets can be understood as recurrent Markov chains that regenerate at initial states (in discrete spaces) or minorization sets (in general spaces). This framework reveals that while both GFlowNets and MCMC methods aim to sample from distributions defined up to normalization, they differ fundamentally: MCMC requires the Markov chain's invariant distribution to match the target, while GFlowNets only require the marginal distribution of a specific stopping time to match the target. The paper provides formal conditions for when the terminating state probability distribution induced by a GFlowNet is proportional to a given reward function, extending the framework to general state spaces through the splitting technique.

## Method Summary
The paper establishes GFlowNets as recurrent Markov chains through rigorous theoretical proofs. The method involves defining the state space (discrete or general), choosing or learning a transition kernel, verifying recurrence properties, ensuring boundary conditions are satisfied, and sampling via regeneration at initial states. For general state spaces, the authors introduce the splitting technique, which creates an artificial atom through a minorization condition, enabling the application of GFlowNets beyond discrete domains. The theoretical framework provides conditions for when the terminating state probability distribution is proportional to a given reward function.

## Key Results
- GFlowNets can be understood as recurrent Markov chains that regenerate at initial states (discrete spaces) or minorization sets (general spaces)
- GFlowNets differ from MCMC by only requiring the marginal distribution of a stopping time to match the target, not the entire invariant distribution
- The splitting technique enables GFlowNets to work in general state spaces by creating artificial atoms through minorization conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GFlowNets can be understood as recurrent Markov chains that regenerate at initial states
- Mechanism: The paper establishes that GFlowNets, which treat sampling as sequential decision-making, can be modeled as Markov chains that return to their initial state repeatedly. Each time the chain returns to the initial state, it effectively starts a new sample generation process.
- Core assumption: The Markov chain must be irreducible and positive recurrent
- Evidence anchors:
  - [abstract] "GFlowNets, which treat sampling as a sequential decision-making problem, can be understood within the same theoretical framework as Markov chain Monte Carlo (MCMC) methods"
  - [section 2.2] "we can view this process as being a single Markov chain that regenerates every time it reaches s0"
  - [corpus] Weak - neighboring papers discuss GFlowNets and MCMC but don't directly address the recurrent Markov chain perspective
- Break condition: If the Markov chain is not irreducible or positive recurrent, the regenerative property fails and the terminating state probability distribution may not be well-defined

### Mechanism 2
- Claim: GFlowNets differ from MCMC by only requiring the marginal distribution of a stopping time to match the target
- Mechanism: While MCMC requires the entire invariant distribution of the Markov chain to match the target distribution, GFlowNets only need the marginal distribution of the terminating state (the state right before returning to the initial state) to be proportional to the reward function
- Core assumption: The boundary conditions (7) are satisfied, meaning the flow at terminating states matches the reward function
- Evidence anchors:
  - [abstract] "MCMC requires the Markov chain's invariant distribution to match the target, while GFlowNets only require the marginal distribution of a specific stopping time to match the target"
  - [section 2.2] "The goal of a GFlowNet is to find a transition probability PF such that the corresponding terminating state probability P ⊤ F matches the reward function, up to normalization"
  - [corpus] Weak - neighboring papers discuss GFlowNets but don't directly compare their stopping time requirements to MCMC
- Break condition: If the boundary conditions are not satisfied, the terminating state probability will not be proportional to the reward function

### Mechanism 3
- Claim: The splitting technique enables GFlowNets to work in general state spaces by creating an artificial atom
- Mechanism: The paper introduces a minorization condition where a set X satisfies PF(s, B) ≥ ε(s)ν(B) for all states s. This allows the construction of a split chain where the state space is augmented with a binary variable indicating whether to use the remainder kernel or the probability measure ν
- Core assumption: The set X satisfies the minorization condition with a positive measurable function ε
- Evidence anchors:
  - [section 3.3] "Taking inspiration from the pointed DAG formulation... we will use a set X ∈ Σ that satisfies a minorization condition"
  - [section 3.3] "The important property to note is that the second kernel in this mixture, ν(B), is completely independent of x"
  - [corpus] Weak - neighboring papers discuss GFlowNets in general spaces but don't detail the splitting technique for creating atoms
- Break condition: If no set X satisfies the minorization condition, the splitting technique cannot create an artificial atom and the framework breaks down for general state spaces

## Foundational Learning

- Concept: Markov chain theory (irreducibility, positive recurrence, invariant measures)
  - Why needed here: The entire theoretical framework relies on understanding when Markov chains have invariant measures and how these relate to sampling distributions
  - Quick check question: What is the difference between positive recurrence and Harris recurrence, and why does each matter for different parts of the paper?

- Concept: Flow networks and flow-matching conditions
  - Why needed here: The paper builds on the flow network perspective of GFlowNets while transitioning to a Markov chain perspective, so understanding flow-matching is crucial
  - Quick check question: How do the flow-matching conditions in equation (1) relate to the invariance condition in equation (4)?

- Concept: Minorization conditions and atom construction
  - Why needed here: The splitting technique for general state spaces relies on minorization conditions to create artificial atoms, which is central to extending GFlowNets beyond discrete spaces
  - Quick check question: Why is it sufficient for the minorization condition to hold only on the set X rather than the entire state space?

## Architecture Onboarding

- Component map: State space S -> Transition kernel PF -> Minorization set X (for general spaces) -> Boundary conditions -> Terminating state probability distribution
- Critical path: Define state space → Choose/create transition kernel → Verify recurrence properties → Ensure boundary conditions are met → Sample via regeneration at initial state
- Design tradeoffs: 
  - Discrete vs general state spaces: Discrete spaces allow simpler atom construction at s0, while general spaces require the splitting technique
  - Learned vs handcrafted transition kernels: GFlowNets learn PF (typically with neural networks) while MCMC often uses handcrafted kernels like Metropolis-Hastings
  - Augmented state space: Using intermediate states enables better exploration of multi-modal distributions but increases complexity
- Failure signatures:
  - Poor mixing or getting stuck: Indicates the Markov chain is not exploring the state space effectively
  - Non-convergence of samples: Suggests the chain is not positive recurrent or boundary conditions are violated
  - Samples not proportional to rewards: Indicates boundary conditions are not properly satisfied
- First 3 experiments:
  1. Implement a simple discrete GFlowNet on a small DAG with known rewards and verify the terminating state distribution matches the reward function
  2. Test the splitting technique on a simple continuous state space (e.g., sampling from a Gaussian mixture) and verify the minorization condition holds
  3. Compare sampling efficiency of a GFlowNet vs Metropolis-Hastings on a multi-modal distribution to demonstrate the exploration advantage

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Complete absence of empirical validation - all results are theoretical with no numerical experiments or practical demonstrations
- The splitting technique for general state spaces may be computationally challenging to implement in practice
- Assumes knowledge of advanced Markov chain theory concepts that may not be familiar to all potential readers

## Confidence
- Confidence in the theoretical framework: High
- Confidence in practical implementation: Medium
- Confidence in the MCMC comparison claims: Medium-High

## Next Checks
1. Implement the splitting technique on a simple continuous distribution and verify the minorization condition holds
2. Test whether the terminating state distribution matches the reward function in a discrete GFlowNet example
3. Compare the mixing time of a GFlowNet vs Metropolis-Hastings on a multi-modal distribution to validate the exploration claims