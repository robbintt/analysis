---
ver: rpa2
title: Class-Adaptive Sampling Policy for Efficient Continual Learning
arxiv_id: '2311.16485'
source_url: https://arxiv.org/abs/2311.16485
tags:
- samples
- class
- casp
- classes
- buffer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Class-Adaptive Sampling Policy (CASP), a novel
  method to address catastrophic forgetting in continual learning. CASP dynamically
  allocates buffer space based on the difficulty and contribution of classes and samples,
  improving the retention of challenging and representative data.
---

# Class-Adaptive Sampling Policy for Efficient Continual Learning

## Quick Facts
- arXiv ID: 2311.16485
- Source URL: https://arxiv.org/abs/2311.16485
- Authors: 
- Reference count: 40
- Key outcome: CASP improves continual learning by dynamically allocating buffer space based on class and sample difficulty, achieving 3.55% accuracy gains on CIFAR100 and reducing forgetting by 2.29%.

## Executive Summary
This paper introduces Class-Adaptive Sampling Policy (CASP), a novel approach to address catastrophic forgetting in continual learning. CASP dynamically allocates buffer space based on the difficulty and contribution of classes and samples, prioritizing challenging data that the model struggles to classify confidently. Experiments on CIFAR10, CIFAR100, and Mini-ImageNet datasets demonstrate significant performance gains over existing methods, with improvements in accuracy and reductions in forgetting. CASP also enhances out-of-distribution generalization and maintains stability across varying epochs, proving its effectiveness and versatility in continual learning scenarios.

## Method Summary
CASP is a sampling policy for continual learning that addresses catastrophic forgetting by dynamically allocating buffer space based on class and sample difficulty. It uses a surrogate model to evaluate sample confidence across epochs, identifying challenging samples with high variance in confidence scores. Classes are prioritized for buffer allocation based on their vulnerability scores, which measure the standard deviation of average class confidence. Challenging samples from these prioritized classes are then selected for the buffer. CASP integrates with Experience Replay (ER) and demonstrates significant improvements in accuracy and forgetting reduction compared to baseline methods.

## Key Results
- CASP achieves 3.55% higher accuracy than ER on CIFAR100 with 5000 buffer size
- Reduces average forgetting by 2.29% compared to ER across all datasets
- Improves out-of-distribution generalization while maintaining in-distribution performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Challenging samples exhibit higher variance in model confidence across training epochs, making them more representative and harder to forget.
- Mechanism: CASP tracks the standard deviation of confidence scores for each sample over multiple epochs. Samples with high standard deviation are deemed "challenging" because they represent ambiguous or boundary cases that the model struggles to classify confidently.
- Core assumption: High variance in confidence scores correlates with sample difficulty and importance for retention.
- Evidence anchors:
  - [abstract]: "These samples exhibit a high standard deviation of confidence across epochs, highlighting their challenging nature. CASP prioritizes these challenging samples for retention because they represent the most significant obstacles encountered by the model."
  - [section 4]: "This study analyzes two scenarios to assess the contributions of classes and samples. In both scenarios, we initially train using the complete dataset. Subsequently, we train on a random subset comprising 10% of the dataset. In the first scenario, we focus on the samples' contribution to training...We conduct separate training sessions with the simplest 10% of samples, the hardest 10%, and the most challenging 10%...Fig. 4 illustrates that training on challenging samples yields superior performance compared to the other types."
- Break condition: If confidence variance does not correlate with sample difficulty, or if high-variance samples are outliers rather than representatives of the class distribution.

### Mechanism 2
- Claim: Classes with higher variance in average confidence across epochs are more prone to forgetting and require larger buffer allocation.
- Mechanism: CASP calculates the standard deviation of class confidence scores (average confidence of all samples in a class across epochs). Classes with higher standard deviation receive proportionally more buffer space.
- Core assumption: Classes that are harder to learn (more variable confidence) are more likely to be forgotten and thus need more representation in the buffer.
- Evidence anchors:
  - [abstract]: "In the next step, we assess the vulnerability of each class across E epochs by computing the standard deviation...This normalization enables us to proportionately allocate buffer space to each class, aligning it with the evaluated level of challenge."
  - [section 3.1]: "In the training of current classes during ongoing tasks, CASP considers the model's confidence in each sample across different epochs...These samples exhibit a high standard deviation of confidence across epochs, highlighting their challenging nature."
- Break condition: If class difficulty does not correlate with forgetting likelihood, or if uniform allocation performs equally well.

### Mechanism 3
- Claim: Prioritizing challenging samples and classes improves out-of-distribution (OOD) generalization compared to random or simple sample selection.
- Mechanism: CASP's sample and class selection strategies focus on retaining challenging samples from challenging classes, which represent the decision boundary and ambiguous cases better than simple or hard (outlier) samples.
- Core assumption: Challenging samples capture the true complexity of the data distribution, leading to better generalization.
- Evidence anchors:
  - [section 5.4]: "In Table 3, we explore the influence of different Sample/Class Strategies on the generalization capabilities of the learned model...selecting challenging samples not only yields superior accuracy on in-distribution samples compared to other sampling strategies but also surpasses them in accuracy on OOD distributions."
  - [corpus]: "Found 25 related papers...Top related titles: GRASP: A Rehearsal Policy for Efficient Online Continual Learning, Information-Theoretic Dual Memory System for Continual Learning..." (Weak evidence for OOD claims; requires more direct comparison)
- Break condition: If challenging samples do not improve OOD performance, or if they introduce noise that degrades overall accuracy.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why forgetting occurs is fundamental to appreciating why CASP's buffer allocation strategy is necessary.
  - Quick check question: What causes a model to lose performance on previous tasks when learning new ones?

- Concept: Experience Replay (ER) and buffer-based methods
  - Why needed here: CASP builds upon ER by modifying how samples are selected for the buffer. Understanding ER's limitations is key to grasping CASP's improvements.
  - Quick check question: How does ER typically select samples for its buffer, and what are the limitations of this approach?

- Concept: Variance as a measure of uncertainty/importance
  - Why needed here: CASP uses variance in confidence scores to identify challenging samples and classes. Understanding this statistical concept is crucial for implementing the method.
  - Quick check question: Why might high variance in model confidence across epochs indicate that a sample is challenging or important?

## Architecture Onboarding

- Component map:
  - Surrogate model (ˆfˆθ) -> Confidence tracker -> Vulnerability scorer -> Buffer allocator -> Sample selector

- Critical path:
  1. Train surrogate model on current task
  2. Track confidence scores for all samples/classes across epochs
  3. Compute vulnerability scores (standard deviation)
  4. Allocate buffer space to classes proportionally
  5. Select challenging samples for each class
  6. Update buffer with selected samples

- Design tradeoffs:
  - Training a surrogate model adds computational overhead but provides better sample selection
  - Focusing on challenging samples may miss simple but important cases
  - Class-adaptive allocation requires more buffer management complexity

- Failure signatures:
  - If variance in confidence does not correlate with actual difficulty, CASP may select poor samples
  - If the surrogate model architecture differs significantly from the main model, confidence scores may not transfer well
  - If buffer size is too small, even optimal allocation may not prevent forgetting

- First 3 experiments:
  1. Implement confidence tracking and verify that challenging samples show higher variance than simple samples
  2. Test class-adaptive buffer allocation on a simple dataset (e.g., Split CIFAR10) and compare to random allocation
  3. Evaluate the impact of different epoch counts in the surrogate model on sample selection quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal buffer size for maximizing CASP's performance across different dataset complexities?
- Basis in paper: [inferred] The paper mentions evaluating CASP with various buffer sizes (200, 500, 1000, 2000, 5000) but does not explicitly state an optimal size.
- Why unresolved: The paper only reports average performance across buffer sizes, not identifying a specific optimal size for different complexities.
- What evidence would resolve it: A detailed analysis showing CASP's performance peaks at specific buffer sizes for each dataset complexity.

### Open Question 2
- Question: How does CASP perform under different class ordering scenarios in real-world applications?
- Basis in paper: [explicit] The paper mentions setting 'Fixed Class Order' to 'False' to simulate real-world conditions but does not extensively evaluate performance under varied orderings.
- Why unresolved: The paper focuses on fixed ordering scenarios and does not explore the impact of different class orderings on CASP's effectiveness.
- What evidence would resolve it: Comparative experiments showing CASP's performance across multiple class ordering scenarios.

### Open Question 3
- Question: Can CASP's methodology be extended to non-image data types such as text or audio?
- Basis in paper: [inferred] The paper focuses on image datasets (CIFAR10, CIFAR100, Mini-ImageNet) without addressing applicability to other data types.
- Why unresolved: The methodology is demonstrated only on image data, leaving its generalizability to other modalities unexplored.
- What evidence would resolve it: Experiments applying CASP to text or audio datasets and comparing its performance with image data.

## Limitations
- CASP's performance gains are primarily demonstrated on small-scale image datasets, with limited validation on more complex domains
- The computational overhead of training surrogate models for sample selection may not scale well to larger datasets
- The paper does not extensively explore the impact of different class ordering scenarios on CASP's effectiveness

## Confidence
- Core mechanism validation: Medium-High (controlled experiments show consistent improvements over multiple baselines)
- OOD generalization claims: Low (indirect comparisons with related work, not direct experimental validation)
- Scalability to larger datasets: Low (limited testing on complex domains or larger buffer sizes)

## Next Checks
1. Test CASP with varying surrogate model architectures to verify robustness of confidence scores
2. Evaluate performance degradation when buffer size is severely constrained
3. Conduct experiments on larger-scale datasets like ImageNet-100 to assess practical scalability