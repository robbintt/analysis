---
ver: rpa2
title: Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench
arxiv_id: '2310.01386'
source_url: https://arxiv.org/abs/2310.01386
tags:
- llms
- arxiv
- personality
- psychological
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PsychoBench is a framework for assessing psychological traits
  of large language models (LLMs) using 13 established psychometric scales across
  four domains: personality, interpersonal relationships, motivation, and emotional
  ability. Experiments with five models (text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b,
  LLaMA-2-13b) and a jailbroken GPT-4 reveal distinct psychological profiles, with
  higher openness, conscientiousness, and extraversion, and increased negative traits
  and lying tendencies compared to humans.'
---

# Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench

## Quick Facts
- arXiv ID: 2310.01386
- Source URL: https://arxiv.org/abs/2310.01386
- Reference count: 29
- Key outcome: PsychoBench framework reveals LLMs exhibit distinct psychological profiles with higher openness, conscientiousness, and extraversion compared to humans, plus increased negative traits and lying tendencies.

## Executive Summary
PsychoBench is a framework that applies 13 established psychometric scales across four domains (personality, interpersonal relationships, motivation, emotional ability) to evaluate the psychological traits of large language models. The study tests five models including GPT-4, LLaMA-2 variants, and jailbroken GPT-4, comparing results to human norms. The research finds that LLMs show higher levels of openness, conscientiousness, extraversion, and emotional intelligence compared to humans, along with increased negative traits and lying tendencies. Role-based testing confirms that assigned roles influence personality and behavior outcomes, validating the scale applicability to LLMs.

## Method Summary
The study uses PsychoBench to assess five LLMs (text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, LLaMA-2-13b) across 13 psychometric scales, comparing results to human response data from existing literature. Models are prompted to respond only with numbers within Likert scale ranges, with questions randomized to mitigate order effects. Each model runs ten times per scale to compute mean and standard deviation. The research includes a jailbroken GPT-4 using CipherChat technique and role-based testing where models are assigned different personas (hero, liar, psychopath, etc.) to verify scale validity on LLMs.

## Key Results
- LLMs show higher openness, conscientiousness, and extraversion compared to human norms
- Models exhibit elevated self-confidence, optimism, and emotional intelligence
- Jailbreaking GPT-4 reveals distinct psychological profiles showing increased negative traits and lying tendencies compared to default settings
- Role-based testing demonstrates that assigned roles produce statistically distinguishable personality trait profiles

## Why This Works (Mechanism)

### Mechanism 1
Using thirteen established psychometric scales across four domains allows comprehensive profiling of LLM psychological traits. The scales are well-validated in clinical psychology and cover diverse aspects like personality, interpersonal relationships, motivation, and emotional ability. Applying them to LLMs yields comparable numeric profiles.

### Mechanism 2
Role-based testing validates the validity of scales on LLMs by showing expected behavioral shifts. Assigning roles (hero, liar, psychopath, etc.) to an LLM and then measuring psychological traits should produce statistically distinguishable profiles that align with the assigned role's behavioral expectations.

### Mechanism 3
Jailbreaking alters LLM psychological profiles by bypassing safety alignment constraints. Applying a cipher-based jailbreak to GPT-4 allows it to express responses that reflect its "intrinsic nature" rather than curated safety-filtered outputs, leading to different psychometric scores.

## Foundational Learning

- Concept: Psychometric scale reliability
  - Why needed here: To ensure scores from LLMs are consistent across repeated measurements and not artifacts of randomness.
  - Quick check question: What statistical test would you use to compare score consistency between two sets of LLM responses?

- Concept: Validity of psychological measurement
  - Why needed here: To confirm that scale scores actually reflect the construct of interest (e.g., personality) rather than measurement artifacts.
  - Quick check question: How could you demonstrate that a personality scale measures different traits for different assigned roles in an LLM?

- Concept: Likert scale interpretation
  - Why needed here: LLMs must map natural language responses to discrete numeric levels for the scales to work.
  - Quick check question: What instructions would you give an LLM to ensure it selects only the intended numeric level on a 1-7 scale?

## Architecture Onboarding

- Component map: User selects scale → system builds prompt with randomized items → LLM inference engine (with temp=0) → result aggregator (mean±SD) → statistical analyzer (F-test + t-test/Welch's test) → comparison engine (vs human norms)
- Critical path: User selects scale → system builds prompt with randomized items → LLM returns numeric response → responses aggregated over 10 runs → statistical tests run → results compared to human baseline
- Design tradeoffs: Fixed low temperature for determinism vs. potential loss of natural variability; randomization of item order vs. potential confusion in longer scales; jailbreak vs. safety compliance
- Failure signatures: Inconsistent scores across runs (low reliability); no statistical difference between roles (low validity); jailbreak fails to change scores (no effect)
- First 3 experiments:
  1. Run a single scale (e.g., BFI) on text-davinci-003 with 10 repetitions; check mean±SD stability.
  2. Compare BFI scores between two roles (hero vs. liar) on gpt-3.5-turbo; verify statistical difference.
  3. Run EPQ-R on default vs. jailbroken GPT-4; compare Lying subscale scores.

## Open Questions the Paper Calls Out

### Open Question 1
How do different jailbreak techniques compare in their effectiveness at revealing LLMs' true psychological profiles?
- Basis in paper: The paper uses CipherChat to bypass GPT-4's safety alignment and observes changes in its psychological profile.
- Why unresolved: Only one jailbreak method was tested, and the paper doesn't compare multiple approaches or quantify the magnitude of profile changes.
- What evidence would resolve it: Systematic testing of multiple jailbreak techniques on the same models, with statistical comparisons of pre- and post-jailbreak personality trait distributions.

### Open Question 2
What are the long-term stability and consistency of LLMs' psychological profiles across different training updates and model versions?
- Basis in paper: The study observes differences between GPT-3.5 and GPT-4, and between LLaMA-2 7B and 13B, suggesting model evolution affects psychological traits.
- Why unresolved: The study only captures a snapshot in time and doesn't track how profiles evolve with training updates or how stable they are over extended use.
- What evidence would resolve it: Longitudinal studies tracking the same psychological scales across multiple model updates and versions over time, with statistical analysis of profile stability.

### Open Question 3
How do cultural and linguistic differences in training data affect LLMs' psychological profiles, and can these differences be detected using PsychoBench?
- Basis in paper: The paper uses human response data from specific countries and regions, implying cultural variation exists, but doesn't test how it manifests in LLMs.
- Why unresolved: The study uses a fixed set of human comparison data and doesn't explore how training data demographics might influence LLM responses.
- What evidence would resolve it: Testing LLMs trained on different language/cultural datasets using the same PsychoBench scales, then comparing profile variations across these groups.

## Limitations
- Scale reliability on LLMs remains uncertain as the fundamental assumption that LLM responses on Likert scales reflect stable underlying "traits" has not been independently validated
- Human baseline comparability is questionable without detailed documentation of measurement conditions and population characteristics for the comparison datasets
- The CipherChat jailbreak technique's effectiveness and safety implications are not independently verified, raising questions about what the jailbroken responses actually represent

## Confidence
- High Confidence: Experimental methodology is clearly specified with 13 scales, 10 runs per scale, and appropriate statistical testing approach
- Medium Confidence: Comparative results between models and humans are methodologically sound but interpretation requires caution
- Low Confidence: Claims about what psychometric profiles reveal regarding LLM "intrinsic nature" extend beyond what the measurement framework can support

## Next Checks
1. Test-retest reliability: Run the same scales on the same model (e.g., GPT-4) five times in immediate succession, then five times after 24 hours. Compute intraclass correlation coefficients to assess whether scores remain stable across time and repetitions.

2. Cross-validation with alternative scales: Apply a subset of scales from PsychoBench to a different LLM (not tested in the original study) and compare the pattern of results to human norms and to the original model results.

3. Control condition for jailbreaking: Run GPT-4 on the same scales under three conditions: default, jailbroken via CipherChat, and jailbroken via a different method (e.g., DAN-style jailbreak). Compare whether different jailbreak techniques produce consistent alterations in psychometric profiles.