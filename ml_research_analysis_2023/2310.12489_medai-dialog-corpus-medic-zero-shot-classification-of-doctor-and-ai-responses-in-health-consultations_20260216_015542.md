---
ver: rpa2
title: 'MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses
  in Health Consultations'
arxiv_id: '2310.12489'
source_url: https://arxiv.org/abs/2310.12489
tags:
- text
- classification
- medical
- responses
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines zero-shot classification of doctor and AI responses
  in healthcare consultations. Three datasets are created containing doctor responses,
  ChatGPT responses, and rephrased doctor responses.
---

# MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations

## Quick Facts
- arXiv ID: 2310.12489
- Source URL: https://arxiv.org/abs/2310.12489
- Reference count: 40
- Primary result: Zero-shot classification models struggle to accurately distinguish between doctor and AI responses in healthcare consultations (F1 scores 0.18-0.58)

## Executive Summary
This study evaluates the performance of zero-shot classification models for distinguishing between doctor and AI-generated responses in healthcare consultations. The researchers created three datasets containing doctor responses, ChatGPT responses, and rephrased doctor responses, then tested five pre-trained language models (BART, BERT, XLM, XLM-R, DistilBERT) for binary and multi-class classification without any corpus training. The results demonstrate that zero-shot classification models perform poorly on this task, with F1 scores ranging from 0.18 to 0.58, indicating that zero-shot approaches alone are insufficient for reliable medical text classification.

## Method Summary
The study collected doctor and ChatGPT responses to healthcare questions from the QuestionDoctors dataset, created rephrased versions of doctor responses, and constructed three datasets: DC (Doctor-ChatGPT), DR (Doctor-Rephrased Doctor), and DCR (combined multi-class). Five pre-trained language models were evaluated for zero-shot classification without any fine-tuning or additional training. Performance was measured using accuracy, precision, recall, and F1 score for both binary and multi-class classification tasks.

## Key Results
- Zero-shot classification models achieved F1 scores between 0.18 and 0.58 when distinguishing doctor from AI responses
- BART performed best overall, followed by XLM and BERT, with XLM-R and DistilBERT showing lower performance
- Multi-class classification (DCR dataset) resulted in decreased performance across all models compared to binary classification
- Rephrased doctor responses were classified with higher accuracy than original doctor responses, suggesting paraphrasing affects model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot classification leverages pre-trained language models' general language understanding to map new text to unseen classes using semantic similarity.
- Mechanism: Models like BART, BERT, and XLM-R are trained on large corpora to capture semantic relationships between words and concepts. When classifying, they match the input text to provided class descriptions based on these learned relationships.
- Core assumption: The semantic patterns in medical consultation text are sufficiently similar to the patterns in the pre-training corpus for the model to generalize.
- Evidence anchors:
  - [abstract]: "Zero-shot classification has emerged as a widely accepted approach in various NLP tasks that lack sufficient training data."
  - [section]: "In zero-shot classification, a pre-trained language model is used as a starting point. These models are trained on a vast corpus of text from the internet and have learned to capture a rich understanding of language."
- Break condition: If medical text contains domain-specific jargon or structural patterns not present in the general training corpus, semantic similarity will fail to produce accurate mappings.

### Mechanism 2
- Claim: Performance differences between models (BART > BERT > XLM-R) stem from their architectural strengths in handling context and transfer learning efficiency.
- Mechanism: BART's denoising autoencoder architecture may better reconstruct corrupted medical text, while BERT's bidirectional attention captures nuanced context. XLM-R's cross-lingual training might dilute medical-specific features.
- Core assumption: Architectural differences translate directly to task-specific performance without fine-tuning.
- Evidence anchors:
  - [section]: "The models evaluated in this study are the Bidirectional Auto-Regressive Transformers (BART), Bidirectional Encoder Representations from Transformers (BERT), Cross-lingual Language Model (XLM), Cross-lingual Language Model - RoBERTa (XLM-R), and DistilBERT."
  - [section]: "BART performs the best when classifying the DC dataset, followed by XLM, according to F1 scores."
- Break condition: If performance differences are due to random initialization or dataset artifacts rather than architectural superiority.

### Mechanism 3
- Claim: Multi-class classification (DCR dataset) degrades performance more than binary classification due to increased decision complexity and class ambiguity.
- Mechanism: Zero-shot models struggle when multiple classes share semantic features, forcing the model to make finer distinctions without task-specific training.
- Core assumption: The model's semantic mapping capability scales sublinearly with the number of classes.
- Evidence anchors:
  - [section]: "The analysis performed in Table 7 reveals a notable decrease in the performance of the models when classifying the DCR dataset, with BERT having the highest F1 score."
  - [section]: "According to our findings, the zero-shot language models show a good understanding of language generally, but has limitations when trying to classify doctor and AI responses to healthcare consultations."
- Break condition: If degradation is caused by dataset imbalance or noise rather than inherent model limitations.

## Foundational Learning

- Concept: Transfer learning in NLP
  - Why needed here: The study relies on pre-trained models to perform zero-shot classification without domain-specific training.
  - Quick check question: What is the key difference between traditional supervised learning and transfer learning in NLP?

- Concept: Zero-shot vs few-shot learning
  - Why needed here: The paper contrasts zero-shot performance with potential few-shot approaches for future work.
  - Quick check question: How does zero-shot learning differ from few-shot learning in terms of required training data?

- Concept: Medical text classification challenges
  - Why needed here: Understanding why zero-shot approaches struggle with medical text requires knowledge of domain-specific features.
  - Quick check question: What unique characteristics of medical text might make it difficult for general-purpose language models?

## Architecture Onboarding

- Component map: QD website → Doctor responses, ChatGPT responses, rephrased responses → Three datasets (DC, DR, DCR) → Model evaluation with BART, BERT, XLM, XLM-R, DistilBERT → Metrics: Accuracy, Precision, Recall, F1 score

- Critical path: 1. Collect and preprocess medical consultation data 2. Create binary and multi-class datasets 3. Run zero-shot classification experiments 4. Evaluate performance metrics 5. Analyze results and identify limitations

- Design tradeoffs: Zero-shot approach vs. supervised training (Speed and flexibility vs. accuracy), Binary vs. multi-class classification (Simpler decision boundaries vs. more comprehensive analysis), Model selection (General-purpose models vs. domain-specific fine-tuning)

- Failure signatures: Low F1 scores (0.18-0.58 range) indicate fundamental limitations, Performance drop in multi-class setting suggests class ambiguity issues, Consistent underperformance across all models points to task difficulty rather than model-specific problems

- First 3 experiments: 1. Binary classification (DC dataset): Doctor vs ChatGPT responses 2. Binary classification (DR dataset): Original doctor vs rephrased doctor responses 3. Multi-class classification (DCR dataset): Doctor, ChatGPT, and rephrased doctor responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can few-shot learning techniques significantly improve the performance of zero-shot classification models for distinguishing between doctor and AI-generated medical responses?
- Basis in paper: [explicit] The authors explicitly state their intention to explore few-shot learning with different embedding techniques in future work, noting that the limited dataset generated by AI models at any given time may not be sufficient for accurate supervised training.
- Why unresolved: The paper only evaluates zero-shot classification models and demonstrates their limitations in accurately classifying doctor and AI-generated responses. The authors acknowledge the need for further research using few-shot learning approaches but do not provide empirical results for this method.
- What evidence would resolve it: Empirical evaluation of few-shot learning models on the same datasets used in this study, comparing their performance to the zero-shot models tested. Results showing significant improvements in classification accuracy would confirm the potential of few-shot learning for this task.

### Open Question 2
- Question: How do linguistic and stylistic differences between doctor-generated and AI-generated medical responses evolve as AI language models become more advanced and sophisticated?
- Basis in paper: [inferred] The study identifies linguistic features that distinguish ChatGPT-generated responses from doctor responses, such as ChatGPT's tendency to emphasize seeking professional medical help. However, it notes that rephrased doctor responses closely resemble human-generated text, posing challenges for classification.
- Why unresolved: The paper only analyzes responses from ChatGPT and does not account for potential improvements in AI language models over time. As these models become more advanced, the linguistic and stylistic differences between AI and human-generated text may become more subtle and difficult to detect.
- What evidence would resolve it: Longitudinal studies comparing linguistic and stylistic features of responses generated by different AI language models over time, including newer and more advanced models. Analysis of how these features change and become more similar to human-generated text would provide insights into the evolving nature of AI-generated medical responses.

### Open Question 3
- Question: What are the ethical implications of using AI-generated responses in healthcare consultations, and how can we ensure transparency and accountability in their deployment?
- Basis in paper: [explicit] The authors emphasize the importance of accurate text attribution and the need to distinguish between human and AI-generated responses to maintain trust in the healthcare system. They also mention the potential for inaccurate information generated by AI systems like ChatGPT to cause harm.
- Why unresolved: While the paper highlights the importance of transparency and accountability in using AI-generated responses, it does not provide specific guidelines or frameworks for addressing the ethical implications of their deployment in healthcare settings.
- What evidence would resolve it: Development and implementation of ethical guidelines and best practices for the use of AI-generated responses in healthcare consultations. This could include requirements for disclosure of AI involvement, mechanisms for human oversight and intervention, and protocols for handling cases where AI-generated responses may be inaccurate or misleading. Evaluation of the effectiveness of these guidelines in maintaining patient trust and ensuring the responsible use of AI in healthcare would provide evidence of their impact.

## Limitations

- Limited dataset size and potential lack of diversity across medical specialties may affect generalizability of findings
- Implementation details for zero-shot classification (tokenization, prompting strategies, hyperparameters) are not fully specified
- Quality control measures for rephrased doctor responses are unclear, potentially affecting dataset validity

## Confidence

**High Confidence**: The finding that zero-shot classification performs substantially worse than supervised approaches for medical text classification is well-supported. The consistent low F1 scores across all models (0.18-0.58) and the performance degradation in multi-class settings provide robust evidence that zero-shot learning alone is insufficient for reliable medical text classification.

**Medium Confidence**: The ranking of model performance (BART > BERT > XLM-R > DistilBERT) has moderate confidence. While the results show clear patterns, the absence of statistical significance testing and the potential influence of implementation details on model rankings limit the conclusiveness of these comparisons.

**Low Confidence**: The specific reasons for performance limitations (whether due to domain-specific vocabulary, complex sentence structures, or other factors) are speculative. The paper suggests multiple potential causes but doesn't conduct ablation studies or controlled experiments to isolate the primary factors affecting zero-shot performance.

## Next Checks

1. **Dataset Expansion and Diversity Test**: Create additional datasets from different medical specialties and consultation types to test whether zero-shot performance limitations are consistent across medical domains. Compare performance on datasets with varying levels of technical vocabulary and sentence complexity.

2. **Controlled Ablation Study**: Design experiments that systematically vary input characteristics (medical jargon density, sentence length, response type) while keeping other factors constant. This would help identify which specific features of medical text most challenge zero-shot classification models.

3. **Prompt Engineering Evaluation**: Test different zero-shot prompting strategies (chain-of-thought prompts, role-based prompts, multi-step reasoning prompts) to determine if performance improvements are possible through better prompt design rather than model selection or additional training data.