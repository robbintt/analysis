---
ver: rpa2
title: Scalable Acceleration for Classification-Based Derivative-Free Optimization
arxiv_id: '2309.11036'
source_url: https://arxiv.org/abs/2309.11036
tags:
- shrinking
- algorithm
- optimization
- race-cars
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the sequential classification-based derivative-free
  optimization (DFO) framework and introduces the concept of hypothesis-target shattering
  rate. The authors revisit the computational complexity upper bound of SRACOS, identifying
  deficiencies in the previous bound related to error-target dependence.
---

# Scalable Acceleration for Classification-Based Derivative-Free Optimization

## Quick Facts
- arXiv ID: 2309.11036
- Source URL: https://arxiv.org/abs/2309.11036
- Reference count: 40
- Key outcome: Introduces RACE-CARS algorithm with region-shrinking to accelerate sequential classification-based derivative-free optimization, achieving better theoretical bounds than SRACOS.

## Executive Summary
This paper addresses the computational complexity of sequential classification-based derivative-free optimization (DFO) by introducing the hypothesis-target η-shattering rate concept. The authors identify deficiencies in the previous SRACOS bound related to error-target dependence and propose a new algorithm, RACE-CARS, that adds a random region-shrinking step. Theoretical analysis shows RACE-CARS achieves acceleration through this mechanism, and empirical experiments on synthetic functions and black-box language model tuning demonstrate its efficiency.

## Method Summary
The paper studies the sequential classification-based DFO framework and introduces the hypothesis-target η-shattering rate to replace the debatable error-target θ-dependence assumption. RACE-CARS adds a random region-shrinking step that projects the current hypothesis onto a shrunken region around the best-so-far solution. The algorithm uses coordinate classification with a shrinking factor γ and frequency ρ to focus the search space. Theoretical results establish that under η-shattering conditions, RACE-CARS achieves better query complexity bounds than previous approaches. Empirical validation includes synthetic benchmark functions and a black-box language model tuning task.

## Key Results
- RACE-CARS achieves theoretical acceleration through region shrinking, with query complexity depending on η-shattering rate rather than error-target dependence
- Empirical experiments show RACE-CARS outperforms SRACOS on synthetic functions (Ackley, Levy, Rastrigin, Sphere) and black-box language model tuning
- Ablation study provides guidance on hyperparameter tuning, showing the relationship between γ, ρ and problem dimensionality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The hypothesis-target η-shattering rate improves upon the error-target θ-dependence by being independent of relative error P(Rt).
- **Mechanism:** Instead of requiring low relative error between the hypothesis and the true positive region, η-shattering directly measures the proportion of the target region Ω_ϵ covered by the hypothesis. This allows the algorithm to tolerate inaccurate hypotheses as long as they cover a significant fraction of the optimal set.
- **Core assumption:** Ω_ϵ is η-shattered by ht for all t ≥ r+1, meaning P(Ω_ϵ ∩ {x: ht(x) = 1}) ≥ η|Ω_ϵ|.
- **Evidence anchors:** [abstract] "By introducing a concept called hypothesis-target shattering rate, we revisit the computational complexity upper bound..."; [section 3.1] "To this end, we give a new concept called hypothesis-target η-shattering rate to replace the debatable assumption..."
- **Break condition:** If Ω_ϵ is not η-shattered by the learned hypotheses, the theoretical acceleration guarantee fails.

### Mechanism 2
- **Claim:** Region shrinking accelerates convergence by progressively focusing the search space toward the current best solution.
- **Mechanism:** At random intervals, the algorithm shrinks the sampling region around the best-so-far solution x_best by a factor γ, reducing the effective search volume and increasing the concentration of samples near promising areas.
- **Core assumption:** The region shrinking rate γ and frequency ρ are chosen such that the shrunken region still intersects with Ω_ϵ in expectation.
- **Evidence anchors:** [section 3.3] "Inspired by the counterexample (3), which is literally the most desirable hypothesis, we should focus more on the intersection of Ω_ϵ and active region of hypotheses..."; [section 3.3] "we propose the following Algorithm 4 'sequential RAndomized CoordinatE Classifying And Region Shrinking' (sequential 'RACE-CARS'), which shrink the active region of the sampling random vector Yt proactively and adaptively via a projection sub-procedure."
- **Break condition:** If γ is too small or ρ is too infrequent, the algorithm may lose coverage of the optimal region; if too aggressive, it may converge prematurely to a suboptimal region.

### Mechanism 3
- **Claim:** The combination of η-shattering and region shrinking yields a better query complexity bound than previous approaches.
- **Mechanism:** The theoretical analysis shows that under η-shattering, the query complexity depends on η and the shrinking factor γ^−ρ + γ^−(T−r)ρ, which can be made much smaller than the previous bound involving θ-dependence and γ-shrinking rate.
- **Core assumption:** The assumptions of η-shattering and bounded shrinking factor hold throughout the optimization process.
- **Evidence anchors:** [section 3.2] "Theorem 3.1. Consider sequential-mode classification-based DFO Algorithm 3, let Xt = Xht, assume that for ϵ > 0, Ω_ϵ is η-shattered by ht for all t = r+1 . . . , T..."; [section 3.2] "Theorem 3.2. Consider Algorithm 4. Assume that for ϵ > 0, Ω_ϵ is η-shattered by ˜ht for all t = r + 1 . . . , T..."
- **Break condition:** If either η or the shrinking factor becomes too small, the theoretical acceleration guarantee breaks down.

## Foundational Learning

- **Concept:** Sequential classification-based optimization framework
  - **Why needed here:** This paper builds on and extends the SRACOS algorithm, which uses sequential classification to guide derivative-free optimization.
  - **Quick check question:** What is the difference between batch-mode and sequential-mode classification-based optimization algorithms?

- **Concept:** Shattering rate and its relationship to query complexity
  - **Why needed here:** The paper introduces hypothesis-target η-shattering rate as a replacement for error-target θ-dependence, fundamentally changing how query complexity is analyzed.
  - **Quick check question:** How does hypothesis-target η-shattering rate differ from error-target θ-dependence in terms of what they measure?

- **Concept:** Region shrinking and its impact on search efficiency
  - **Why needed here:** The RACE-CARS algorithm adds a region-shrinking step to accelerate convergence, and understanding this mechanism is crucial for implementing and tuning the algorithm.
  - **Quick check question:** How does the region shrinking rate γ and frequency ρ affect the trade-off between exploration and exploitation?

## Architecture Onboarding

- **Component map:** Training set (size r) -> Classification model (ht) -> Region shrinking (optional, γ, ρ) -> Sampling mechanism (Yt, projected) -> Best-so-far tracking (xbest, ybest)

- **Critical path:**
  1. Initialize training set S from uniform distribution
  2. For each iteration t:
     - Classify current S into positive/negative sets
     - Train hypothesis ht using RACOS
     - With probability ρ, shrink region around xbest
     - Project ht onto shrunken region to get Xt
     - Sample new solution from Xt (or uniform with probability 1-λ)
     - Replace worst solution in S with new solution
     - Update best-so-far

- **Design tradeoffs:**
  - Larger r provides more stable training but slower adaptation
  - Higher λ increases exploitation but may miss global optimum
  - γ close to 1 provides gradual shrinking but slower convergence
  - ρ too high may cause premature convergence, too low may not accelerate enough

- **Failure signatures:**
  - Algorithm fails to converge: likely γ too small or ρ too low
  - Premature convergence to suboptimal solution: likely γ too large or ρ too high
  - Slow convergence: likely λ too low or r too large
  - High variance in results: likely insufficient training data (small r)

- **First 3 experiments:**
  1. Verify RACE-CARS on a simple convex function (e.g., Sphere) with varying γ and ρ to observe convergence behavior
  2. Test on a multimodal function (e.g., Ackley) to verify acceleration compared to SRACOS
  3. Implement ablation study on γ and ρ as described in section 5.1 to understand their relationship with problem dimensionality

## Open Questions the Paper Calls Out

- **Question:** What is the precise relationship between the hypothesis-target η-shattering rate and the error-target θ-dependence, and under what conditions can the former completely replace the latter in bounding the query complexity?
  - **Basis in paper:** [explicit] The paper states "The error-target dependence can be bounded by relative error and hypothesis-target shattering rate: θ ≤ max{P(Rt), |1 − P(Rt) − η|}" and discusses deficiencies of the error-target dependence.
  - **Why unresolved:** The paper introduces the η-shattering rate concept but does not fully explore its theoretical implications or prove its superiority over θ-dependence in all scenarios.
  - **What evidence would resolve it:** A rigorous mathematical proof comparing the two concepts under various conditions, and empirical studies showing consistent performance differences.

- **Question:** How can the training and replacing sub-procedures in sequential classification-based DFO algorithms be modified to achieve a higher hypothesis-target η-shattering rate, and what specific modifications would lead to the best empirical performance?
  - **Basis in paper:** [inferred] The paper mentions that "altering the Training and Replacing sub-procedures inherited from 'SRACOS', which may ideally lead to a bigger shattering rate, will be considered as another extension direction of the current study."
  - **Why unresolved:** The current sub-procedures are based on "SRACOS" and may not be optimal for maximizing η-shattering rate. The paper suggests this as a future research direction.
  - **What evidence would resolve it:** Experimental results comparing different sub-procedure modifications and their impact on η-shattering rate and overall algorithm performance.

- **Question:** What are the theoretical bounds on the dimensionally local Hölder continuity parameters (βi, Li) for real-world objective functions, and how do these bounds affect the selection of region shrinking rate γ and shrinking frequency ρ in practice?
  - **Basis in paper:** [explicit] The paper introduces the concept of dimensionally local Hölder continuity and provides a lower bound for γ and ρ based on these parameters.
  - **Why unresolved:** The paper acknowledges that βi, Li, and x* are generally unknown, making it difficult to determine the optimal γ and ρ. It suggests empirical tuning but does not provide theoretical guidance.
  - **What evidence would resolve it:** Empirical studies on various real-world functions to estimate these parameters and theoretical analysis of their impact on algorithm performance.

## Limitations
- The theoretical analysis assumes hypothesis-target η-shattering rate properties hold throughout optimization but lacks rigorous proofs for all function classes
- Empirical evaluation is limited to synthetic functions and one black-box tuning task, lacking diverse real-world problem testing
- Region-shrinking effectiveness appears sensitive to hyperparameter tuning with only preliminary ablation study guidance

## Confidence
- **High confidence:** The fundamental mechanism of hypothesis-target shattering rate as a replacement for error-target dependence is well-supported theoretically and the basic acceleration concept through region shrinking is sound.
- **Medium confidence:** The theoretical bounds on query complexity are mathematically rigorous but may not translate directly to practical performance across all problem types.
- **Low confidence:** The optimal setting of hyperparameters (γ, ρ) for different problem dimensions and characteristics is not well-established.

## Next Checks
1. Conduct comprehensive ablation studies across different problem dimensions (d ∈ [10, 100, 500]) to establish the relationship between γ, ρ and problem complexity.
2. Test RACE-CARS on a broader set of real-world black-box optimization problems beyond the language model tuning task, including hyperparameter tuning for different ML models and engineering design problems.
3. Perform runtime complexity analysis comparing RACE-CARS with SRACOS on high-dimensional problems (d > 1000) to verify scalability claims.