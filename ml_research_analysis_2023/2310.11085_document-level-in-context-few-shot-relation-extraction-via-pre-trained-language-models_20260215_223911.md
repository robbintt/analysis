---
ver: rpa2
title: Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained Language
  Models
arxiv_id: '2310.11085'
source_url: https://arxiv.org/abs/2310.11085
tags:
- replm
- relation
- context
- best
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REPLM, a novel framework for document-level
  in-context few-shot relation extraction via pre-trained language models. The key
  idea is to reformulate relation extraction as a triplet generation task using in-context
  few-shot learning, eliminating the need for named entity recognition and human annotation
  of documents.
---

# Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained Language Models

## Quick Facts
- arXiv ID: 2310.11085
- Source URL: https://arxiv.org/abs/2310.11085
- Reference count: 40
- Key outcome: Introduces REPLM, a framework that reformulates relation extraction as triplet generation using in-context few-shot learning, achieving state-of-the-art performance on DocRED without named entity recognition or human annotations.

## Executive Summary
This paper presents REPLM, a novel framework for document-level in-context few-shot relation extraction that leverages pre-trained language models to extract knowledge triplets directly from documents. The key innovation is reformulating relation extraction as a triplet generation task using distantly-supervised examples as in-context demonstrations, eliminating the need for named entity recognition and human annotations. The framework achieves state-of-the-art performance on the DocRED dataset, outperforming more than 30 baseline methods while also identifying missing annotations in the original dataset.

## Method Summary
REPLM reformulates document-level relation extraction as a triplet generation task using in-context few-shot learning. The framework creates L sets of K in-context examples from distantly-supervised documents, retrieves examples semantically similar to the target document using SBERT embeddings, and aggregates outputs from a pre-trained language model (GPT-JT) probabilistically. The method weights example sets based on cosine similarity and generates subject-object pairs for given relations, filtering results by probability thresholds. Crucially, no fine-tuning or human annotation is required, and the framework can be updated to new language models without retraining.

## Key Results
- REPLM achieves state-of-the-art F1-score on DocRED development set, outperforming 30+ baseline methods
- The framework identifies missing annotations in DocRED, with 85.5% precision on relations not present in original labels
- Performance improves with multiple set aggregation, with L=8 and K=16 providing optimal balance between precision and recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context few-shot learning via pre-trained language models can extract document-level relations without requiring named entity recognition or human annotations.
- Mechanism: The framework reformulates relation extraction as a triplet generation task, using distant supervision to create in-context examples that demonstrate the format and label space to the language model. The model generates subject-object pairs for a given relation directly from the document context.
- Core assumption: Pre-trained language models can generalize from distantly-supervised examples to extract correct relations from new documents, even with noisy labels.
- Evidence anchors:
  - [abstract] "We present a novel framework for in-context few-shot relation extraction via pre-trained language models... it eliminates the need for named entity recognition and human annotations of documents."
  - [section] "Our framework leverages the generalization capabilities of pre-trained LMs by reformulating the relation extraction task as a tailored in-context few-shot learning paradigm."
- Break Condition: If the language model cannot generalize from noisy distant supervision examples, or if the documents are too dissimilar from the in-context examples, the framework would fail to extract correct relations.

### Mechanism 2
- Claim: Aggregating multiple sets of in-context examples mitigates bias and improves relation extraction performance.
- Mechanism: The framework creates L sets of K in-context examples each, weights them based on semantic similarity to the target document, and aggregates their outputs probabilistically. This reduces the impact of individual example bias and improves robustness.
- Core assumption: The aggregated output from multiple semantically relevant example sets provides a more accurate estimate of the relation probabilities than a single set.
- Evidence anchors:
  - [abstract] "Our framework has three strengths: it eliminates the need (1) for named entity recognition and (2) for human annotations of documents, and (3) it can be updated to new LMs without re-training."
  - [section] "We calculate the joint probability of a subject-object pair as p(s, o | di, r) = Σ(p(Cl | di, r) · p(s, o | Cl, di, r)), where we aggregate the outputs from L sets of in-context examples."
- Break Condition: If the semantic similarity weighting fails to select relevant examples, or if the aggregation doesn't effectively combine the outputs, the framework's performance would degrade.

### Mechanism 3
- Claim: The framework can identify missing annotations in existing datasets by generating more comprehensive output than human-annotated labels.
- Mechanism: By comparing the framework's output against human-annotated labels, the authors found that many relations extracted by REPLM were correct but not annotated in the dataset. This suggests the framework can uncover missing knowledge triplets.
- Core assumption: Relations extracted by the framework that don't match human annotations are actually correct, not false positives, and can be validated against external knowledge bases.
- Evidence anchors:
  - [abstract] "Our framework allows us to identify missing annotations, and we thus show that our framework actually performs much better than the original labels from the development set of DocRED."
  - [section] "We conjecture that REPLM generates more comprehensive output than REBEL... we thus adopt a simple yet effective strategy to predict if a given document contains information about the given relation."
- Break Condition: If the framework generates many false positives that appear correct but aren't validated by external knowledge, or if the evaluation method incorrectly assumes all missing annotations should be added, the claim would be invalidated.

## Foundational Learning

- Concept: In-context few-shot learning
  - Why needed here: This is the core mechanism that allows the framework to extract relations without fine-tuning. Understanding how language models can learn from demonstrations in the prompt is essential.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are its limitations?

- Concept: Distant supervision
  - Why needed here: This is the source of training data for the in-context examples. Understanding how distant supervision works and its trade-offs with noise is crucial.
  - Quick check question: What is distant supervision, and how does it create noisy labels for relation extraction?

- Concept: Semantic similarity and embedding
  - Why needed here: The framework uses semantic similarity to retrieve relevant in-context examples. Understanding how document embeddings work and how similarity is calculated is important.
  - Quick check question: How are document embeddings created, and how does cosine similarity measure semantic similarity between documents?

## Architecture Onboarding

- Component map: Document -> SBERT Embedding -> Similarity Retrieval -> In-Context Examples -> GPT-JT -> Triplet Generation -> Aggregated Output

- Critical path:
  1. Encode input document and distantly-supervised documents
  2. Retrieve top-N semantically similar documents for the given relation
  3. Create L sets of K in-context examples from the top-N documents
  4. Weight each set based on average cosine similarity to input document
  5. Generate subject-object pairs for each weighted set using the language model
  6. Aggregate probabilities across all sets and rank subject-object pairs
  7. Filter by probability threshold and output knowledge triplets

- Design tradeoffs:
  - Using distant supervision eliminates the need for human annotations but introduces noise
  - Aggregating multiple sets of examples improves robustness but increases computational cost
  - Using a fixed prefix for output parsing simplifies post-processing but requires careful prompt design
  - Not fine-tuning the language model enables flexibility but may limit performance compared to specialized models

- Failure signatures:
  - Low precision: Many generated relations are incorrect or hallucinated
  - Low recall: Many correct relations are missed or filtered out by the probability threshold
  - High computational cost: Processing many sets of examples takes too long
  - Sensitivity to prompt format: Small changes in the prompt cause large performance drops

- First 3 experiments:
  1. Run REPLM on a single document with a known relation and verify the output format and basic functionality
  2. Compare REPLM performance with and without multiple set aggregation on a small validation set
  3. Test REPLM with different probability thresholds to find the optimal balance between precision and recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the REPLM framework to noise and adversarial examples in the in-context few-shot learning setup?
- Basis in paper: [explicit] The paper includes a sensitivity analysis where they introduce adversarial in-context examples and observe a significant drop in performance, indicating the framework's sensitivity to noise in the context examples.
- Why unresolved: While the paper shows that adversarial examples degrade performance, it doesn't explore the limits of this robustness or strategies to mitigate the impact of noisy in-context examples.
- What evidence would resolve it: Experiments varying the proportion of adversarial examples, testing different denoising techniques, and evaluating the framework's performance on datasets with varying levels of noise.

### Open Question 2
- Question: Can the REPLM framework be extended to handle more complex relation extraction tasks beyond document-level?
- Basis in paper: [inferred] The paper focuses on document-level relation extraction, but the framework's reliance on in-context few-shot learning and pre-trained language models suggests potential for extension to other NLP tasks.
- Why unresolved: The paper doesn't explore the framework's applicability to other relation extraction tasks, such as cross-document or cross-lingual relation extraction, or other NLP tasks altogether.
- What evidence would resolve it: Experiments applying the REPLM framework to different relation extraction tasks, evaluating its performance on cross-document and cross-lingual datasets, and exploring its potential for other NLP tasks like question answering or summarization.

### Open Question 3
- Question: How does the choice of pre-trained language model impact the performance of the REPLM framework?
- Basis in paper: [explicit] The paper uses GPT-JT as the pre-trained language model, but doesn't explore the impact of using different models on the framework's performance.
- Why unresolved: Different pre-trained language models have varying capabilities and biases, which could significantly impact the framework's ability to extract relations from context.
- What evidence would resolve it: Experiments comparing the performance of the REPLM framework using different pre-trained language models, analyzing the impact of model size, architecture, and training data on relation extraction performance.

## Limitations

- The framework's performance heavily depends on the quality of distantly-supervised examples, which may introduce significant noise that affects extraction accuracy
- Computational cost increases substantially with multiple set aggregation, potentially limiting practical deployment in resource-constrained environments
- The generalizability to domains outside biomedical and document-level relation extraction remains untested and may require substantial adaptation

## Confidence

**High confidence**: The core claim that in-context few-shot learning can eliminate the need for named entity recognition and human annotations for document-level relation extraction is well-supported by the framework's architecture and evaluation metrics.

**Medium confidence**: The claim about identifying missing annotations is supported by the framework's ability to extract more relations than human labels, but requires more rigorous external validation to confirm these as true positives versus false positives.

**Medium confidence**: The superiority claim over state-of-the-art methods is demonstrated on DocRED, but lacks detailed analysis of specific strengths and weaknesses against individual baseline methods.

## Next Checks

1. **External knowledge base validation**: Verify the missing annotation claims by systematically validating a random sample of unannotated relations extracted by REPLM against external knowledge bases like Wikidata or Wikipedia. Calculate precision, recall, and F1-score specifically for these potentially missing relations to quantify the actual discovery rate.

2. **Cross-domain generalization test**: Apply REPLM to a different document-level relation extraction dataset (e.g., CDR for chemical-disease relations or SciERC for scientific relations) to assess whether the framework's performance generalizes beyond the biomedical domain where DocRED resides. Document any prompt engineering or adaptation required.

3. **Computational efficiency analysis**: Measure the actual inference time and memory usage of REPLM with different values of L and K. Compare against a baseline that uses only single set of in-context examples to quantify the trade-off between performance gains and computational overhead. Identify the optimal configuration for practical deployment.