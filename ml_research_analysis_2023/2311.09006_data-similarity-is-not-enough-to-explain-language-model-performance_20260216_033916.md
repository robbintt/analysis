---
ver: rpa2
title: Data Similarity is Not Enough to Explain Language Model Performance
arxiv_id: '2311.09006'
source_url: https://arxiv.org/abs/2311.09006
tags:
- similarity
- pretraining
- dataset
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The relationship between pretraining data and downstream task performance
  in large language models is often assumed to be determined by data similarity. This
  paper tests whether various similarity metrics correlate with model performance.
---

# Data Similarity is Not Enough to Explain Language Model Performance

## Quick Facts
- arXiv ID: 2311.09006
- Source URL: https://arxiv.org/abs/2311.09006
- Reference count: 17
- One-line primary result: Data similarity metrics do not reliably predict language model performance on downstream tasks

## Executive Summary
This paper challenges the common assumption that similarity between pretraining data and downstream tasks determines language model performance. Through systematic testing of various similarity metrics including token distribution KL-divergence, embedding cosine similarity, and language model perplexity, the authors find no consistent correlation with accuracy on standard benchmarks like BIG-bench Lite and GLUE. While similarity does correlate with performance for multilingual datasets, this relationship breaks down for other common benchmarks, suggesting that data similarity alone is insufficient to explain language model behavior.

## Method Summary
The authors test whether distributional and example-specific similarity measures correlate with language model performance on downstream tasks. They use pretraining datasets (C4 and Pile), downstream benchmarks (BIG-bench Lite, GLUE, and multilingual datasets), and multiple similarity measures (KL-divergence, embedding cosine similarity, language model perplexity). The method involves evaluating language models on downstream tasks, computing similarity between pretraining and downstream data, and analyzing correlations using Spearman correlation coefficients. The study tests models up to 7B parameters and compares both aggregate and example-specific similarity measures.

## Key Results
- No correlation between similarity metrics and accuracy on BIG-bench Lite and GLUE benchmarks
- Similarity correlates with performance for multilingual datasets but not other benchmarks
- Different similarity measures (KL-divergence, MAUVE, cosine similarity, perplexity) are not significantly correlated with each other
- Examples more similar to pretraining data are not consistently easier for language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Similarity between pretraining data and downstream task data does not determine model performance
- Mechanism: Various similarity metrics (KL-divergence, embedding cosine similarity, language model perplexity) show no correlation with accuracy on common benchmarks
- Core assumption: The similarity metrics used are sufficient to capture relevant aspects of data similarity
- Evidence anchors:
  - [abstract] "similarity metrics are not correlated with accuracy or even each other"
  - [section 5] "Table 1 shows the lack of correlation between five aggregate similarity measures and few-shot performance on BIG-bench Lite tasks"
- Break condition: If similarity metrics are insufficient to capture relevant aspects of data similarity

### Mechanism 2
- Claim: Similarity correlates with performance for multilingual datasets but not for other benchmarks
- Mechanism: Performance tracks KL-divergence of unigram token distributions for multilingual datasets, but this correlation does not hold for other benchmarks
- Core assumption: The controlled setting of multilingual datasets allows for clearer relationship between similarity and performance
- Evidence anchors:
  - [abstract] "Similarity correlates with performance for multilingual datasets, but in other benchmarks...similarity metrics are not correlated with accuracy"
  - [section 3] "Figure 1 shows that performance tracks the KL-divergence of unigram token distributions between the Pile and Stack Exchange datasets"
- Break condition: If the controlled setting of multilingual datasets is not sufficient to establish clear relationship between similarity and performance

### Mechanism 3
- Claim: Different similarity measures are not correlated with each other
- Mechanism: Aggregate similarity measures like KL-divergence, MAUVE score, and cosine similarity are not significantly correlated when comparing BIG-bench Lite datasets with C4 and the Pile
- Core assumption: Similarity is not a monolithic concept and different measures capture different aspects
- Evidence anchors:
  - [abstract] "similarity metrics are not correlated with accuracy or even each other"
  - [section 6] "Table 2 shows that different measures of aggregate similarity...are not significantly correlated with each other"
- Break condition: If similarity measures used are not sufficient to capture relevant aspects of similarity

## Foundational Learning

- Concept: Statistical significance and multiple comparisons
  - Why needed here: The authors perform multiple statistical tests and need to control for false positives
  - Quick check question: What is the purpose of Bonferroni correction in the context of this study?

- Concept: Embedding similarity and cosine similarity
  - Why needed here: The authors use embedding similarity as one of the similarity measures to test the hypothesis
  - Quick check question: How is cosine similarity calculated between two embeddings?

- Concept: Language modeling and perplexity
  - Why needed here: The authors use language model perplexity as another similarity measure to test the hypothesis
  - Quick check question: What is perplexity in the context of language modeling?

## Architecture Onboarding

- Component map: Data preprocessing -> Similarity metric calculation -> Model training -> Model evaluation -> Statistical analysis
- Critical path: Calculating similarity metrics -> Training models -> Evaluating performance -> Performing statistical analysis
- Design tradeoffs: Using variety of similarity metrics and models increases robustness but computational complexity
- Failure signatures: Incorrect similarity metric implementation, insufficient computational resources, improper statistical analysis
- First 3 experiments:
  1. Calculate similarity metrics between pretraining data and downstream task data
  2. Train models on pretraining data and evaluate performance on downstream tasks
  3. Perform statistical analysis to test correlation between similarity metrics and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model scale affect the relationship between pretraining data similarity and downstream performance?
- Basis in paper: [explicit] The paper states "Our experiments are on models up to the 7B-parameter scale. Perhaps there is a model scale threshold at which similarity matters more for performance."
- Why unresolved: The authors only tested models up to 7B parameters and did not explore larger models that might show stronger relationship between similarity and performance
- What evidence would resolve it: Testing same similarity measures on larger models (e.g., 10B, 100B+ parameters) across various downstream tasks to see if correlation increases with model size

### Open Question 2
- Question: Are there specific types of downstream tasks or domains where pretraining data similarity is more predictive of performance?
- Basis in paper: [inferred] The paper found no correlation between similarity and performance on GLUE and BIG-bench Lite, but did not systematically test all task types or domains
- Why unresolved: Experiments focused on general-purpose benchmarks, but certain specialized domains might show stronger similarity-performance relationships
- What evidence would resolve it: Systematically testing similarity-performance relationships across diverse task categories (e.g., code generation, specialized scientific domains, domain-specific language tasks)

### Open Question 3
- Question: How do different similarity metrics capture distinct aspects of language model behavior, and can they be combined to better predict performance?
- Basis in paper: [explicit] The paper found that different similarity measures (KL-divergence, MAUVE, cosine similarity, perplexity) were not correlated with each other or with performance, suggesting they capture different notions of similarity
- Why unresolved: The paper only tested whether individual similarity measures predict performance, not whether combining them or understanding their distinct properties could yield better predictions
- What evidence would resolve it: Developing framework that combines multiple similarity measures or analyzes their individual strengths/weaknesses, then testing whether this combined approach better predicts performance

## Limitations
- The study relies on aggregate similarity measures that may not capture the full complexity of what makes data "similar" in ways that matter for learning
- The controlled multilingual experiments show correlation between similarity and performance, but this setting is highly artificial and may not generalize to more naturalistic pretraining scenarios
- The study focuses on few-shot learning settings, which may have different similarity-performance dynamics compared to full fine-tuning or in-context learning scenarios

## Confidence
Medium confidence in the core claim that data similarity alone does not explain language model performance, based on empirical evidence showing weak or inconsistent correlations across multiple similarity measures and benchmarks.

## Next Checks
1. Test whether similarity-performance relationships emerge when using task-specific similarity measures rather than general-purpose metrics
2. Replicate the analysis across different model sizes and architectures to determine if lack of correlation is consistent across the model spectrum
3. Examine whether intermediate checkpoints during pretraining show different similarity-performance relationships compared to fully trained models, which could reveal when and how similarity becomes relevant during learning