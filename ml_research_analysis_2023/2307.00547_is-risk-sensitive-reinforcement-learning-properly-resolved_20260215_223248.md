---
ver: rpa2
title: Is Risk-Sensitive Reinforcement Learning Properly Resolved?
arxiv_id: '2307.00547'
source_url: https://arxiv.org/abs/2307.00547
tags:
- risk
- policy
- learning
- risk-sensitive
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that existing risk-sensitive reinforcement learning
  (RSRL) methods suffer from biased optimization, as they optimize towards per-step
  risk measures instead of the global risk measure over the whole trajectory. To address
  this, the authors propose Trajectory Q-Learning (TQL), which learns an historical
  value function that models the conditional distribution of accumulated returns along
  the trajectory given the history.
---

# Is Risk-Sensitive Reinforcement Learning Properly Resolved?

## Quick Facts
- arXiv ID: 2307.00547
- Source URL: https://arxiv.org/abs/2307.00547
- Reference count: 37
- One-line primary result: TQL achieves unbiased optimization and converges to the optimal policy for various risk measures.

## Executive Summary
This paper addresses a fundamental flaw in existing risk-sensitive reinforcement learning (RSRL) methods: they optimize per-step risk measures instead of the global risk measure over the entire trajectory. The authors propose Trajectory Q-Learning (TQL), which uses a history-relied (HR) operator and non-Markovian policy to ensure unbiased optimization of risk measures over accumulated returns. TQL is theoretically proven to converge to the optimal policy under various risk measures and demonstrates superior performance compared to existing methods on both discrete and continuous control tasks.

## Method Summary
TQL is a distributional reinforcement learning algorithm that models the historical value function Zθ to represent the conditional distribution of accumulated returns along the trajectory given the history. It uses a non-Markovian policy and a history-relied (HR) operator for policy evaluation, and a risk-sensitive HR optimality operator for policy improvement. The algorithm employs a GRU to summarize the trajectory history and uses quantile regression with quantile Huber loss to update the value networks. Policy improvement is achieved by selecting actions that maximize the risk measure over the learned return distribution.

## Key Results
- TQL achieves unbiased optimization and converges to the optimal policy for various risk measures, including CVaR, POW, and Wang.
- On the risky MiniGrid task, TQL outperforms baselines like IQN and CVaR-DRL in optimizing CVaR of episodic returns.
- On the modified MountainCar task, TQL demonstrates superior performance across multiple risk measures (mean, CVaR, POW, Wang) compared to IQTD3 baseline.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TQL's history-relied (HR) operator converges in p-Wasserstein distance and enables unbiased optimization.
- Mechanism: The HR operator models the conditional distribution of accumulated returns along the whole trajectory given the history, using non-Markovian policy evaluation. This ensures that risk measures over the full trajectory are optimized rather than per-step.
- Core assumption: Deterministic dynamics and finite state-action spaces allow the HR operator to be a γ-contraction in maximum p-Wasserstein.
- Evidence anchors:
  - [abstract]: "TQL uses a non-Markovian policy and a history-relied (HR) operator for policy evaluation, and a risk-sensitive HR optimality operator for policy improvement."
  - [section]: "Theorem 4.1 (Policy Evaluation for T π h ). T π h : Z → Z is a γ-contraction in the metric of the maximum form of p-Wasserstein distance ¯dp."
  - [corpus]: No direct corpus evidence for this specific contraction property; assumed from deterministic dynamics.
- Break condition: Stochastic dynamics or continuous state-action spaces where contraction fails or becomes intractable.

### Mechanism 2
- Claim: Risk-sensitive Bellman optimality operator T ∗ β leads to biased optimization and fails to converge.
- Mechanism: T ∗ β maximizes risk measures locally at each state based on future returns only, ignoring past trajectory information. This causes the policy to optimize per-step risk rather than risk over entire trajectories.
- Core assumption: Risk measures other than mean are not additive, so local maximization does not align with global trajectory optimization.
- Evidence anchors:
  - [abstract]: "existing risk-sensitive reinforcement learning (RSRL) methods do not achieve unbiased optimization and can not guarantee optimality or even improvements regarding risk measures over accumulated return distributions."
  - [section]: "Theorem 3.2. Recursively applying risk-sensitive Bellman optimality operator T ∗ β w.r.t. risk measure β does not solve the RSRL objective Eq. (4) and β[Zk] is not guaranteed to converge to β[Z ∗] if β is not mean or an affine in mean."
  - [corpus]: Weak; no other papers directly address this specific biased optimization issue.
- Break condition: Risk measure β is mean or affine in mean, where local and global optimizations coincide.

### Mechanism 3
- Claim: TQL's policy improvement via HR optimality operator achieves unbiased optimization.
- Mechanism: By modeling history-action value distributions and improving policy based on full trajectory returns, TQL ensures that the policy optimizes risk measures over entire trajectories. The HR optimality operator guarantees monotonic improvement in risk-sensitive return.
- Core assumption: Deterministic dynamics and finite policy space allow HR optimality operator to yield strictly better policies unless already optimal.
- Evidence anchors:
  - [abstract]: "TQL achieves unbiased optimization and converges to the optimal policy for various risk measures."
  - [section]: "Theorem 4.2 (Policy Improvement for T ∗ h,β ). For two deterministic policies π and π′, if π′ is obtained by T ∗ h,β... then the following inequality holds β [Z π(ht, π(ht))] ≤ β [Z π′(ht, π′(ht))]."
  - [corpus]: No direct corpus evidence for this specific monotonicity property; assumed from deterministic HR operator.
- Break condition: Stochastic dynamics or non-finite policy spaces where monotonicity is not guaranteed.

## Foundational Learning

- Concept: Distortion risk measures and their properties.
  - Why needed here: TQL optimizes policies under arbitrary distortion risk measures, so understanding their definition and behavior is essential.
  - Quick check question: What distortion function hβ transforms the CDF for CVaR with η=0.1?

- Concept: Distributional reinforcement learning and Bellman operators.
  - Why needed here: TQL extends distributional RL by redefining Bellman operators to account for full trajectory risk.
  - Quick check question: How does the distributional Bellman operator differ from the scalar Bellman operator?

- Concept: Wasserstein metrics and their role in distributional RL.
  - Why needed here: Convergence guarantees for TQL rely on contraction in maximum p-Wasserstein distance.
  - Quick check question: What does it mean for two distributions to be close in Wasserstein distance?

## Architecture Onboarding

- Component map: History trajectory -> GRU -> History representation -> Zθ(history, action) -> Risk measure β -> Action selection -> Environment -> Reward -> Update Zθ and Zψ

- Critical path: At each step, encode history into reprt via GRU, compute Zθ(reprt, a) for all actions, select action maximizing β[Zθ], execute in env, store transition, update Zθ and Zψ with quantile regression loss, update πϕ by maximizing β[Zθ].

- Design tradeoffs: Using history increases representational capacity but inflates state space; rolling window L=10 reduces complexity but may lose long-term dependencies. Deterministic dynamics assumption simplifies theory but limits applicability.

- Failure signatures: If policy converges to sub-optimal actions, check whether Zθ is correctly modeling full trajectory returns or if β computation is misaligned. If training is unstable, verify quantile regression loss implementation and target network updates.

- First 3 experiments:
  1. Verify TQL learns optimal policy on 3-state MDP with CVaR objective by comparing learned return distribution to ground truth.
  2. Test TQL on MiniGrid risky task and compare CVaR of episodic returns to baseline IQN and CVaR-DRL.
  3. Evaluate TQL on continuous MountainCar with varying risk penalties and compare performance across risk measures (mean, CVaR, POW, Wang).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TQL be extended to handle stochastic environmental dynamics effectively?
- Basis in paper: [explicit] The paper explicitly states that TQL assumes deterministic dynamics and notes this as a limitation, suggesting future work on handling stochastic dynamics.
- Why unresolved: The paper only proposes a solution for deterministic dynamics and does not provide any empirical results or theoretical analysis for stochastic environments.
- What evidence would resolve it: Empirical results on benchmark environments with stochastic dynamics, or theoretical proofs showing how TQL can be adapted to handle stochasticity.

### Open Question 2
- Question: How does TQL perform on large-scale, real-world risk-sensitive reinforcement learning problems compared to other methods?
- Basis in paper: [inferred] The paper mentions that the current experiments are limited to simple and toy environments, and there is a lack of results on large-scale domains and real-world problem settings.
- Why unresolved: The paper does not provide any experimental results on complex or realistic tasks, only on simple discrete and continuous control environments.
- What evidence would resolve it: Empirical results comparing TQL to other methods on large-scale, real-world risk-sensitive RL problems.

### Open Question 3
- Question: What are the theoretical convergence properties of TQL under different risk measures, particularly those that are not affine in mean?
- Basis in paper: [explicit] The paper proves that TQL achieves unbiased optimization and converges to the optimal policy for various risk measures, but the proof of convergence for specific risk measures, especially non-affine ones, is not provided.
- Why unresolved: While the paper claims convergence for various risk measures, the detailed proof for each specific risk measure is not included in the main text.
- What evidence would resolve it: Detailed theoretical proofs of convergence for TQL under specific risk measures, particularly those that are not affine in mean.

## Limitations
- Theoretical analysis relies heavily on deterministic dynamics and finite state-action spaces, which may not generalize to stochastic environments.
- Convergence proof assumes the HR operator is a γ-contraction in maximum p-Wasserstein distance, but this property is assumed rather than empirically validated across different risk measures.
- Experiments are limited to relatively simple grid-world and continuous control tasks, leaving uncertainty about performance in more complex, high-dimensional environments.

## Confidence

- **High Confidence**: The mechanism explaining why existing RSRL methods fail (local vs. global optimization of risk measures) is well-established theoretically and supported by Theorem 3.2.
- **Medium Confidence**: The theoretical convergence guarantees for TQL are sound under stated assumptions, but the practical applicability to stochastic dynamics remains uncertain.
- **Low Confidence**: The empirical performance claims are based on limited experiments with specific risk measures and environments, requiring more extensive validation.

## Next Checks

1. **Stochastic Dynamics Test**: Implement TQL on a stochastic grid-world environment and measure whether the HR operator still contracts in Wasserstein distance and converges to optimal policy.
2. **Risk Measure Generalization**: Evaluate TQL across a broader spectrum of distortion risk measures (beyond CVaR, POW, Wang) to verify unbiased optimization holds universally.
3. **Scalability Analysis**: Test TQL on benchmark environments like Atari or Mujoco to assess performance degradation with increased state-action space complexity and whether the history representation remains effective.