---
ver: rpa2
title: Effective Distillation of Table-based Reasoning Ability from LLMs
arxiv_id: '2309.13182'
source_url: https://arxiv.org/abs/2309.13182
tags:
- reasoning
- data
- llms
- table
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel distillation framework to transfer
  table-based reasoning capabilities from large language models (LLMs) to smaller,
  specialized models for scientific table-to-text generation. The approach involves
  generating table-based reasoning data from LLMs using Chain-of-Thought prompting,
  then fine-tuning smaller models (e.g., Flan-T5-base) with this distilled data.
---

# Effective Distillation of Table-based Reasoning Ability from LLMs

## Quick Facts
- arXiv ID: 2309.13182
- Source URL: https://arxiv.org/abs/2309.13182
- Reference count: 11
- Key outcome: 0.22B parameter models fine-tuned with CoT data achieve 82.75% TAPEX-Acc and 78.72% TAPAS-Acc, surpassing gpt-3.5-turbo

## Executive Summary
This paper introduces a novel distillation framework that transfers table-based reasoning capabilities from large language models to smaller, specialized models for scientific table-to-text generation. The approach uses Chain-of-Thought prompting to generate reasoning data from LLMs, then fine-tunes smaller models (Flan-T5-base) with this distilled data. Experimental results show significant improvements over traditional fine-tuning baselines, with the distilled models surpassing the performance of the teacher model (gpt-3.5-turbo) on faithfulness metrics.

## Method Summary
The paper presents a two-stage distillation framework for transferring table-based reasoning abilities from LLMs to smaller models. First, LLMs generate reasoning-description pairs from scientific tables using Chain-of-Thought prompting with diverse reasoning generation. Second, smaller models are fine-tuned on these pairs, learning to generate similar reasoning and descriptions from tables directly. The approach employs data filtering using Self-Fine verification to ensure factual consistency and maximizes reasoning ability through diverse reasoning generation techniques.

## Key Results
- Flan-T5-base fine-tuned with CoT data achieves 82.75% TAPEX-Acc and 78.72% TAPAS-Acc
- Distilled models surpass gpt-3.5-turbo performance on faithfulness metrics
- Significant improvement compared to traditionally fine-tuned baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT prompting in LLMs enables structured reasoning that can be captured and transferred to smaller models through distillation.
- Mechanism: The LLM generates intermediate reasoning steps (CoT) for table-based tasks, which are then used as additional input context during fine-tuning of smaller models. This provides the smaller model with explicit reasoning pathways.
- Core assumption: The CoT data generated by LLMs is factually consistent with the tables and contains valid reasoning steps that smaller models can learn from.
- Evidence anchors: [abstract] "We utilise LLMs to generate table-based reasoning through CoT..."; [section] "We employ the diverse reasoning...to generate two different reasonings..."

### Mechanism 2
- Claim: The two-step pipeline (data generation + fine-tuning) effectively transfers reasoning ability from LLMs to smaller models.
- Mechanism: First, LLMs generate reasoning-description pairs from tables using CoT prompting. Second, smaller models are fine-tuned on these pairs, learning to generate similar reasoning and descriptions from tables directly.
- Core assumption: The fine-tuning process can effectively learn the mapping from table+reasoning to description, capturing the reasoning patterns from the LLM-generated data.
- Evidence anchors: [abstract] "Experimental results have shown that a 0.22 billion parameter model (Flan-T5-base) fine-tuned using distilled data..."; [section] "Our experimental results underscore that fine-tuning smaller models..."

### Mechanism 3
- Claim: Diverse reasoning generation captures different reasoning patterns, improving the robustness of the distilled model.
- Mechanism: By generating multiple reasoning paths for the same table, the framework captures diverse ways to approach table-based reasoning, which helps the smaller model generalize better.
- Core assumption: Different reasoning approaches for the same table are valid and beneficial for learning robust reasoning patterns.
- Evidence anchors: [section] "To maximize the reasoning ability distilled from LLMs, we employ the diverse reasoning...to generate two different reasonings..."; [section] "The table-to-text task enables the model to produce varied descriptions..."

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT enables LLMs to break down complex reasoning tasks into intermediate steps, which can then be captured and transferred to smaller models
  - Quick check question: What is the primary benefit of using CoT prompting in LLMs for table reasoning tasks?

- Concept: Knowledge distillation
  - Why needed here: Distillation transfers capabilities from larger models (LLMs) to smaller, more efficient models while maintaining performance
  - Quick check question: How does knowledge distillation differ from traditional supervised learning?

- Concept: Table-to-text generation
  - Why needed here: The task requires understanding structured table data and generating natural language descriptions with reasoning
  - Quick check question: What makes scientific table-to-text generation more challenging than general table-to-text tasks?

## Architecture Onboarding

- Component map: LLM (Teacher) → Data generator → Data filter → Student model → Evaluation pipeline
- Critical path: Table → LLM CoT generation → Data filtering → Student fine-tuning → Evaluation
- Design tradeoffs:
  - Model size vs. reasoning quality: Larger LLMs generate better CoT but are more expensive
  - Diversity vs. consistency: More diverse reasoning paths may improve robustness but risk inconsistency
  - Data filtering vs. data quantity: Stricter filtering ensures quality but reduces training data
- Failure signatures:
  - Low faithfulness metrics despite high surface metrics
  - Student model performance worse than traditional fine-tuning
  - Inconsistent reasoning patterns across similar tables
- First 3 experiments:
  1. Generate CoT data from a small subset of SciGen and verify consistency with tables
  2. Fine-tune a T5-base model with CoT data and compare to traditional fine-tuning on faithfulness metrics
  3. Test diverse reasoning generation by comparing performance with single vs. multiple reasoning paths per table

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the distilled models vary when using different types of table-based reasoning data (e.g., focusing on different table regions or performing different reasoning operations)?
- Basis in paper: [explicit] The paper mentions that the table-to-text task enables the model to produce varied descriptions by focusing on different table regions or performing various reasoning operations.
- Why unresolved: The paper does not explore the impact of different types of table-based reasoning data on the performance of the distilled models.
- What evidence would resolve it: Experiments comparing the performance of models fine-tuned with different types of table-based reasoning data, showing which types lead to better performance.

### Open Question 2
- Question: What is the impact of the size of the CoT data on the performance of the distilled models?
- Basis in paper: [explicit] The paper mentions that the maximal context limit of the LLMs and the average length of tables and descriptions of the SciGen dataset is larger than other table-to-text datasets, so they do not generate more reasoning-description pairs.
- Why unresolved: The paper does not investigate the impact of the size of the CoT data on the performance of the distilled models.
- What evidence would resolve it: Experiments varying the size of the CoT data and measuring the performance of the distilled models, showing the relationship between data size and performance.

### Open Question 3
- Question: How does the performance of the distilled models compare to other state-of-the-art models on table-based reasoning tasks?
- Basis in paper: [explicit] The paper compares the performance of the distilled models to baseline models and the teacher model, but does not compare to other state-of-the-art models on table-based reasoning tasks.
- Why unresolved: The paper does not provide a comprehensive comparison of the distilled models to other state-of-the-art models on table-based reasoning tasks.
- What evidence would resolve it: Experiments comparing the performance of the distilled models to other state-of-the-art models on table-based reasoning tasks, showing how they compare in terms of accuracy and efficiency.

## Limitations
- Limited evaluation to a single scientific table-to-text dataset (SciGen)
- Unspecified implementation details for CoT generation prompts and data filtering criteria
- No ablation studies on the impact of diverse reasoning generation

## Confidence
- Data generation reliability: Medium
- Generalization beyond SciGen: Low
- Teacher model dependency: Medium

## Next Checks
1. Cross-dataset validation: Test distilled model on multiple table reasoning datasets (TabFact, LogicNLG)
2. Teacher model ablation: Compare results using different LLM teachers (Claude, Gemini)
3. Long-term reasoning evaluation: Design test cases requiring multi-step reasoning across table segments