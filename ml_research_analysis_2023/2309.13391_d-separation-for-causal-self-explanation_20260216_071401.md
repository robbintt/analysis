---
ver: rpa2
title: D-Separation for Causal Self-Explanation
arxiv_id: '2309.13391'
source_url: https://arxiv.org/abs/2309.13391
tags:
- correlation
- methods
- rationale
- conference
- rationalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies two types of spurious correlations in rationalization:
  feature correlation (between input features) and mask correlation (introduced by
  the explainer). Using d-separation from probabilistic graphical models, it proves
  that if the unselected input and target label are d-separated by the selected rationale,
  then the rationale must contain all direct causal features.'
---

# D-Separation for Causal Self-Explanation

## Quick Facts
- arXiv ID: 2309.13391
- Source URL: https://arxiv.org/abs/2309.13391
- Reference count: 28
- Key outcome: MCD criterion achieves up to 13.7% higher F1 score than MMI by eliminating spurious correlations in rationalization

## Executive Summary
This paper addresses spurious correlations in self-explaining rationalization models by introducing the Minimum Conditional Dependence (MCD) criterion based on d-separation from probabilistic graphical models. The authors identify two types of spurious correlations—feature correlation between input features and mask correlation introduced by the explainer—and prove that causal rationales can be identified by ensuring d-separation between non-causal features and target labels. The MCD criterion minimizes the KL-divergence between predictor outputs conditioned on full input versus selected rationale, thereby selecting rationales that contain all direct causal features while excluding spurious correlations.

## Method Summary
The MCD method uses d-separation to identify causal rationales by minimizing the dependence between unselected input features and the target label given the selected rationale. The approach assumes an acyclic causal graph where the label has no causal effect on input features (Assumption 1). During training, the explainer generates a binary mask indicating rationale tokens, while the predictor computes distributions P(Ŷ|X) and P(Ŷ|XZ). The MCD loss minimizes KL-divergence between these distributions plus sparsity regularization, ensuring the selected rationale blocks all backdoor paths between non-causal features and the label. The method is validated on BeerAdvocate and HotelReviews datasets, showing improved F1 scores and robustness to mask correlation compared to maximum mutual information (MMI) baselines.

## Key Results
- MCD outperforms MMI-based methods by up to 13.7% in F1 score for rationale selection
- MCD demonstrates robustness to mask correlation (degeneration) where explainer implicitly learns category information
- MCD maintains performance with pretrained models and shows improved results when initialized with skewed explainer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCD selects rationales that block all backdoor paths between non-causal features and target labels, thereby isolating causal features.
- Mechanism: By minimizing the dependence between unselected input and target label conditioned on selected rationale, MCD forces inclusion of all direct causal features (Theorem 1).
- Core assumption: The causal graph has no cycles, and the target label has no causal effect on input features (Assumption 1).
- Evidence anchors:
  - [abstract]: "we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are d-separated by the causal rationale."
  - [section 4.2]: "If Assumption 1 holds, then all the direct causal features to Y within X will be included in XZ if and only if X−Z and Y are d-separated by XZ."
- Break condition: Assumption 1 fails (e.g., in cyclic causal graphs or when label influences input).

### Mechanism 2
- Claim: MCD outperforms MMI by avoiding spurious correlations introduced during both feature generation and mask selection.
- Mechanism: MMI selects features based on mutual information, which captures both causal and spurious correlations. MCD explicitly penalizes dependence between non-causal features and label, eliminating spurious correlations.
- Core assumption: The KL-divergence between P(Y|X) and P(Y|XZ) is a valid measure of dependence.
- Evidence anchors:
  - [abstract]: "Conventional work typically uses the maximum mutual information (MMI) criterion... However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label."
  - [section 4.3]: "we employ a simple and practical measure of dependence, the KL-divergence, to verify the effectiveness of the proposed criterion."
- Break condition: The KL-divergence fails to capture complex dependence structures between unselected features and target.

### Mechanism 3
- Claim: MCD is robust to mask correlation (degeneration) where the explainer implicitly learns category information.
- Mechanism: By conditioning on the selected rationale, MCD blocks backdoor paths created by mask correlation, forcing the predictor to rely on actual causal features rather than mask patterns.
- Core assumption: The explainer's mask selection creates backdoor paths between mask indicators and target labels.
- Evidence anchors:
  - [section 4.1]: "Another type of correlation stems from the rationale (mask) selection stage, and we call it mask correlation... the predictor only needs to determine whether the input rationale includes a '-' or not."
  - [section 5.3]: "MCD is much less affected, demonstrating its robustness in such scenarios."
- Break condition: The explainer fails to create any backdoor paths between masks and labels.

## Foundational Learning

- Concept: D-separation in probabilistic graphical models
  - Why needed here: D-separation provides the theoretical foundation for understanding when non-causal features become independent of target labels given causal features.
  - Quick check question: If A and B are d-separated by C in a causal graph, what does this imply about their conditional independence?

- Concept: Conditional independence vs. unconditional independence
  - Why needed here: The paper relies on conditional independence (Y ⫫ X−Z | XZ) rather than unconditional independence to select causal rationales.
  - Quick check question: Why is conditional independence more relevant than unconditional independence for identifying causal features?

- Concept: KL-divergence as dependence measure
  - Why needed here: The paper uses KL-divergence between P(Y|X) and P(Y|XZ) to measure dependence between unselected features and target label.
  - Quick check question: What does it mean when KL-divergence between two distributions equals zero?

## Architecture Onboarding

- Component map: Explainer -> Predictor -> MCD loss -> Regularization
- Critical path:
  1. Explainer generates rationale mask
  2. Predictor computes distributions P(Ŷ|X) and P(Ŷ|XZ)
  3. MCD loss minimizes KL-divergence between these distributions
  4. Regularization maintains rationale quality
  5. Joint training updates both components

- Design tradeoffs:
  - Using KL-divergence vs. other dependence measures (mutual information, HSIC)
  - Training stability when predictor tries to approximate two distributions
  - Computational cost of evaluating both full and partial inputs

- Failure signatures:
  - Predictor collapses to trivial patterns (mask correlation)
  - Explainer selects too few or too many tokens
  - MCD loss plateaus while regular training continues

- First 3 experiments:
  1. Verify MCD finds all causal features on synthetic dataset with known causal structure
  2. Compare MCD vs. MMI on BeerAdvocate dataset with known spurious correlations
  3. Test MCD robustness to initialized skewed explainer that induces degeneration

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but the limitations section implicitly raises several open research directions regarding Assumption 1's validity in real-world scenarios and the method's applicability to non-text domains.

## Limitations
- Assumption 1 (acyclic causal graph, no label-to-input effects) may not hold in many real-world NLP scenarios, making the theoretical guarantees conditional on a potentially unrealistic assumption.
- The MCD approach doubles computational cost by requiring predictor outputs for both full input and rationale, limiting scalability to larger models or longer sequences.
- The paper only validates MCD on two relatively small text classification datasets, raising questions about performance on larger-scale or non-text tasks.

## Confidence
- **High confidence**: The theoretical foundation using d-separation is sound and well-articulated. The KL-divergence formulation for measuring dependence is a standard and appropriate choice.
- **Medium confidence**: The empirical improvements (up to 13.7% F1 gain) are promising but reported only on two datasets with relatively small scale. The robustness claims to mask correlation are supported but could benefit from more extensive ablation studies.
- **Low confidence**: The claim that MCD "guarantees" inclusion of all direct causal features relies entirely on Assumption 1, which is not empirically verified and may not generalize to complex, real-world causal structures.

## Next Checks
1. **Assumption 1 validation**: Systematically test MCD performance on datasets with known label-to-input causal effects (e.g., feedback loops, selection bias) to quantify how assumption violations impact performance.

2. **Alternative dependence measures**: Compare MCD's KL-divergence approach against other dependence measures (mutual information estimators, HSIC) to assess whether the theoretical elegance translates to practical superiority.

3. **Scaling experiments**: Evaluate MCD on larger-scale datasets (e.g., multi-domain text classification) and with transformer-based encoders to assess computational feasibility and performance retention at scale.