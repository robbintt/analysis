---
ver: rpa2
title: E2E Spoken Entity Extraction for Virtual Agents
arxiv_id: '2302.10186'
source_url: https://arxiv.org/abs/2302.10186
tags:
- speech
- entity
- step
- extraction
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using a speech encoder to extract entities
  directly from speech in human-computer conversations. The core idea is to fine-tune
  a pre-trained speech encoder using Connectionist Temporal Classification (CTC) loss
  to map audio events to text events, generating only entity-relevant tokens instead
  of full transcriptions.
---

# E2E Spoken Entity Extraction for Virtual Agents

## Quick Facts
- arXiv ID: 2302.10186
- Source URL: https://arxiv.org/abs/2302.10186
- Authors:
- Reference count: 0
- 1-step approach (speech-to-entity) outperforms 2-step (ASR+NER) for spoken entity extraction

## Executive Summary
This paper presents a novel approach for extracting named entities directly from speech in human-computer conversations. The method fine-tunes a pre-trained speech encoder using CTC loss to map audio events directly to entity-relevant text tokens, bypassing intermediate transcription. When evaluated on enterprise virtual agent data, this 1-step approach demonstrated superior accuracy compared to the traditional 2-step approach of first transcribing speech and then extracting entities from text.

## Method Summary
The approach repurposes an E2E ASR model (Citrinet) by fine-tuning it with (speech, entity) pairs using CTC loss. Instead of generating full transcriptions, the model learns to output only entity-relevant tokens. Greedy CTC decoding is applied to obtain the final entity predictions, with confidence scores computed from posterior probabilities. The model is trained on audio recordings from an enterprise virtual agent system, where users respond to prompts like "What's your name?" or "What's your street address?"

## Key Results
- 1-step speech-based extraction outperformed 2-step ASR+NER for first names, last names, full names, street addresses, and email addresses
- The approach surpassed human annotators in a constrained environment for most entity types
- Joint modeling of all entity types improved performance for first-name, full-name, and email extraction
- The 1-step approach showed better robustness to ASR errors compared to cascaded systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CTC loss can be re-purposed to map audio events directly to text events, generating only entity-relevant tokens instead of full transcriptions.
- Mechanism: The CTC loss function, traditionally used in E2E ASR systems to align audio frames with text tokens, is adapted to optimize for entity extraction by fine-tuning the speech encoder with (Speech-input, Entity) pairs. This forces the model to learn to transcribe only the entity-relevant portions of speech, ignoring superfluous information like carrier phrases.
- Core assumption: The CTC loss function is flexible enough to learn a mapping from audio to entity tokens rather than general text tokens.
- Evidence anchors:
  - [abstract]: "We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech ignoring the superfluous portions such as carrier phrases, or spell name entities."
  - [section 3.1]: "We re-purpose an E2E ASR fine-tuned for standard transcription task and fine-tune it using(Speech-input, Entity) pairs optimized for CTC loss. Thus, extract spoken entities directly from speech in 1-step, instead of first transcribing text and running a second NL component for entity extraction."
- Break condition: If the CTC loss cannot effectively learn the mapping from audio to entity tokens, or if the model starts generating irrelevant tokens.

### Mechanism 2
- Claim: A non-autoregressive speech-based extraction model outperforms a cascaded ASR and NLU system for entity extraction.
- Mechanism: By directly extracting entities from speech using a single model, the proposed approach avoids the errors and information loss that can occur in the cascaded approach of first transcribing speech and then extracting entities from text. The model learns to perform intelligent entity extraction, understanding and ignoring speech patterns, rather than just literal lexical transcription.
- Core assumption: The single-step approach can learn to extract entities more accurately than the cascaded approach, even with noisy ASR output.
- Evidence anchors:
  - [abstract]: "In the context of dialogs from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step approach which first generates lexical transcriptions followed by text-based entity extraction for identifying spoken entities."
  - [section 3.2]: "We found that performance of cascade approach is better when human transcribed text is provided compared to E2E ASR output which has errors in it."
- Break condition: If the model cannot handle the variability in spoken input, or if the ASR errors are too severe for the model to recover.

### Mechanism 3
- Claim: Joint modeling of all entity types improves performance for certain entity types.
- Mechanism: By pooling training data for all entities and using a single model, the system can learn shared representations and improve performance for entity types that benefit from context, such as first names and full names.
- Core assumption: There is enough similarity between different entity types to justify joint modeling.
- Evidence anchors:
  - [section 5]: "Joint model which pools training data for all entities shows improved performance for first-name, full-name and email extraction."
- Break condition: If the entity types are too dissimilar, or if the model becomes too complex to train effectively.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: CTC loss is the core mechanism that allows the model to map audio events to text events without requiring frame-level alignment.
  - Quick check question: What is the key difference between CTC loss and traditional cross-entropy loss in ASR?

- Concept: Greedy CTC decoding
  - Why needed here: Greedy CTC decoding is used to obtain the final entity tokens from the model's output predictions by applying CTC decoding rules.
  - Quick check question: How does greedy CTC decoding differ from beam search decoding?

- Concept: Named Entity Recognition (NER)
  - Why needed here: Understanding NER is crucial for interpreting the task of extracting entities like names, addresses, and email addresses from speech.
  - Quick check question: What are the key challenges in NER from speech compared to text?

## Architecture Onboarding

- Component map: Speech input -> Speech encoder (Citrinet) -> Classification head -> CTC loss -> Greedy CTC decoder -> Entity output
- Critical path: Speech input -> Speech encoder -> Classification head -> CTC loss -> Greedy CTC decoder -> Entity output
- Design tradeoffs:
  - Using a pre-trained speech encoder vs. training from scratch
  - Fine-tuning the entire encoder vs. only the classification head
  - Using greedy CTC decoding vs. beam search decoding
  - Computing confidence scores vs. not computing them
- Failure signatures:
  - Model generates irrelevant tokens or fails to generate any tokens
  - Model generates incomplete or incorrect entities
  - Model performance degrades significantly on unseen data
- First 3 experiments:
  1. Evaluate the model on a small subset of the test data to ensure it is generating entities correctly.
  2. Compare the performance of the model with and without confidence scores to determine their impact on precision.
  3. Experiment with different decoding strategies (e.g., beam search) to see if they improve performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed 1-step approach perform on speech from different accents or non-native English speakers compared to the 2-step approach?
- Basis in paper: [inferred] The paper mentions that test participants were a mix of native and non-native speakers, but does not explicitly compare performance across different speaker groups or accents.
- Why unresolved: The paper does not provide a detailed analysis of performance variations across different speaker demographics or accents, which could be crucial for real-world deployment.
- What evidence would resolve it: Experimental results showing accuracy metrics for different speaker groups (e.g., native vs. non-native speakers, different regional accents) would clarify the robustness of the 1-step approach across diverse speech patterns.

### Open Question 2
- Question: Can the 1-step approach be extended to handle real-time spoken entity extraction with low latency requirements?
- Basis in paper: [explicit] The paper focuses on offline extraction and mentions the potential for real-time applications but does not address latency or real-time processing constraints.
- Why unresolved: Real-time processing introduces additional challenges such as computational efficiency and latency, which are not addressed in the current study.
- What evidence would resolve it: Benchmarking the 1-step approach on real-time speech streams with metrics for latency and computational load would determine its feasibility for live applications.

### Open Question 3
- Question: How does the proposed method handle code-switching or mixed-language speech inputs?
- Basis in paper: [inferred] The paper does not discuss handling multilingual or code-switched speech, which is common in many real-world scenarios.
- Why unresolved: The current model is trained on English speech data and may not generalize to multilingual contexts or code-switching without additional training or adaptation.
- What evidence would resolve it: Testing the model on code-switched datasets and comparing its performance to baseline methods would reveal its ability to handle multilingual inputs.

### Open Question 4
- Question: What is the impact of background noise or audio quality on the accuracy of the 1-step approach compared to the 2-step approach?
- Basis in paper: [inferred] The paper does not evaluate the robustness of the 1-step approach to varying audio qualities or noisy environments.
- Why unresolved: Real-world speech inputs often contain background noise, and the robustness of the proposed method to such conditions is not explored.
- What evidence would resolve it: Conducting experiments with noisy speech datasets (e.g., adding different levels of background noise) and comparing accuracy metrics would assess the robustness of the 1-step approach.

## Limitations

- Training dataset composition and size are not fully specified, making it difficult to assess generalizability
- No comparison against state-of-the-art text-based NER systems when given perfect transcripts
- Limited ablation studies on architectural choices (e.g., speech encoder type, CTC vs other losses)

## Confidence

- **High confidence**: The core finding that a single-step speech-based approach can outperform cascaded ASR+NER for entity extraction in constrained domains
- **Medium confidence**: Claims about surpassing human performance, given the specific constrained environment and limited evaluation metrics
- **Low confidence**: Claims about the general superiority of speech-based extraction without more diverse testing conditions

## Next Checks

1. Evaluate the model on an independently collected dataset with different prompts and entity distributions to assess true generalization beyond the original enterprise environment.

2. Conduct a detailed error analysis comparing model failures versus human annotator errors, focusing on systematic patterns (e.g., specific entity types, speech disfluencies, background noise) to identify true limitations.

3. Test alternative speech encoders (e.g., Wav2Vec2, HuBERT) and loss functions (e.g., cross-entropy, segmental CRF) to isolate the contribution of the specific Citrinet+CTC combination to performance improvements.