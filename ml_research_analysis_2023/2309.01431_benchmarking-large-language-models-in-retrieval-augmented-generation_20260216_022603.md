---
ver: rpa2
title: Benchmarking Large Language Models in Retrieval-Augmented Generation
arxiv_id: '2309.01431'
source_url: https://arxiv.org/abs/2309.01431
tags:
- llms
- documents
- information
- noise
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes RGB, a new benchmark for evaluating Retrieval-Augmented
  Generation (RAG) in large language models (LLMs). RGB evaluates four key abilities
  of RAG: noise robustness, negative rejection, information integration, and counterfactual
  robustness.'
---

# Benchmarking Large Language Models in Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2309.01431
- **Source URL:** https://arxiv.org/abs/2309.01431
- **Reference count:** 14
- **Primary result:** RGB benchmark evaluates LLMs on RAG abilities; current models struggle with negative rejection (45% rejection rate) and information integration (67% accuracy).

## Executive Summary
This paper introduces RGB, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs). The benchmark assesses four key abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. RGB consists of 800 English and Chinese instances constructed from latest news articles, with external documents retrieved using search engines. The authors evaluate six state-of-the-art LLMs on RGB and find that while they exhibit some noise robustness, they still struggle significantly with negative rejection, information integration, and counterfactual robustness. The results highlight limitations in current LLMs' RAG capabilities and suggest the need for further research to effectively apply RAG to LLMs.

## Method Summary
The RGB benchmark evaluates LLMs on four key RAG abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. It uses 800 English and Chinese instances constructed from latest news articles. The process involves collecting news articles, generating questions and answers using ChatGPT, manually verifying and filtering the data, retrieving relevant documents using Google's API and a dense retrieval model, constructing four testbeds for each ability, and evaluating six state-of-the-art LLMs on the RGB benchmark. The evaluation measures accuracy, rejection rate, error detection rate, and error correction rate.

## Key Results
- Best performing model on negative rejection achieves only a 45% rejection rate
- Best performing model on information integration achieves only a 67% accuracy rate
- Current LLMs exhibit some noise robustness but struggle significantly with negative rejection, information integration, and counterfactual robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves LLM accuracy by grounding responses in retrieved external knowledge.
- Mechanism: The system retrieves relevant documents for a query, then the LLM uses these documents to generate more accurate responses, reducing hallucination.
- Core assumption: Retrieved documents contain correct and relevant information that the LLM can effectively utilize.
- Evidence anchors:
  - [abstract]: "Incorporating external knowledge via information retrieval, i.e., Retrieval-Augmented Generation (RAG), has been regarded as a promising way to resolve the above challenges."
  - [section]: "With the help of external knowledge, LLMs can generate more accurate and reliable responses."
- Break condition: If retrieved documents are noisy or contain false information, the LLM may generate incorrect responses.

### Mechanism 2
- Claim: RGB evaluates four specific abilities required for effective RAG in LLMs.
- Mechanism: The benchmark tests noise robustness, negative rejection, information integration, and counterfactual robustness to assess LLM performance in different RAG scenarios.
- Core assumption: These four abilities comprehensively capture the challenges LLMs face when using RAG.
- Evidence anchors:
  - [abstract]: "RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case."
  - [section]: "Based on different compositions of query and document-set pairs, we expand the corpus and divided it into 4 testbeds to evaluate the following basic abilities of LLMs."
- Break condition: If there are other critical abilities not covered by these four, the evaluation may be incomplete.

### Mechanism 3
- Claim: The RGB benchmark uses latest news articles to avoid bias from LLMs' internal knowledge.
- Mechanism: By constructing questions and answers from recent news, the benchmark ensures that the correct answers are not already known by the LLMs, forcing them to rely on retrieved information.
- Core assumption: LLMs' internal knowledge is outdated and does not include very recent information.
- Evidence anchors:
  - [section]: "In order to ensure that the internal knowledge of LLMs does not introduce bias into the evaluation results, RGB chooses to aggregate the latest news information and constructs queries based on the news information."
- Break condition: If LLMs have been updated with recent information or have access to real-time data, this approach may not effectively test their RAG capabilities.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is the core technology being evaluated, so understanding how it works is essential.
  - Quick check question: How does RAG differ from standard LLM generation, and what problem does it aim to solve?

- Concept: Noise robustness
  - Why needed here: Evaluating how well LLMs can handle noisy documents is a key part of the RGB benchmark.
  - Quick check question: What strategies can LLMs use to extract useful information from noisy documents?

- Concept: Information integration
  - Why needed here: The ability to combine information from multiple documents is crucial for answering complex questions.
  - Quick check question: What challenges arise when LLMs need to integrate information from multiple sources?

## Architecture Onboarding

- Component map: News article collection and preprocessing -> QA instance generation using ChatGPT -> Document retrieval using search engines and dense retrieval models -> Testbed construction for each of the four abilities -> LLM evaluation and metric calculation

- Critical path: News article → QA instance generation → Document retrieval → Testbed construction → LLM evaluation → Results analysis

- Design tradeoffs:
  - Using latest news vs. general knowledge for evaluation
  - Manual vs. automatic evaluation of LLM responses
  - Focusing on specific abilities vs. overall RAG performance

- Failure signatures:
  - Low accuracy on noise robustness indicates LLMs struggle with irrelevant information
  - Low rejection rate suggests LLMs often generate answers even when information is insufficient
  - Poor information integration performance shows LLMs have difficulty combining multiple sources

- First 3 experiments:
  1. Test a simple RAG setup with clean documents to establish baseline performance
  2. Introduce varying levels of noise to documents and measure impact on accuracy
  3. Create complex questions requiring information from multiple documents and evaluate integration ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models (LLMs) be improved to effectively handle noise in external documents when performing retrieval-augmented generation (RAG)?
- Basis in paper: [explicit] The paper discusses the challenge of noise robustness in RAG and highlights the limitations of current LLMs in handling noisy documents.
- Why unresolved: The paper identifies the need for further enhancements in LLMs to effectively handle noise, but does not provide specific solutions or approaches.
- What evidence would resolve it: Research and development of techniques or models that can effectively filter out noise from external documents and improve the performance of LLMs in RAG tasks.

### Open Question 2
- Question: How can LLMs be trained to accurately reject answering questions when none of the external documents contain relevant information?
- Basis in paper: [explicit] The paper discusses the challenge of negative rejection in RAG and highlights the limitations of current LLMs in declining to answer when the required knowledge is not present in any retrieved document.
- Why unresolved: The paper identifies the need for LLMs to have the capability to reject answering, but does not provide specific methods or techniques to achieve this.
- What evidence would resolve it: Research and development of approaches or models that can accurately determine when to reject answering based on the absence of relevant information in external documents.

### Open Question 3
- Question: How can LLMs be enhanced to effectively integrate information from multiple external documents when answering complex questions?
- Basis in paper: [explicit] The paper discusses the challenge of information integration in RAG and highlights the limitations of current LLMs in summarizing from multiple documents.
- Why unresolved: The paper identifies the need for LLMs to have the ability to integrate information, but does not provide specific techniques or models to achieve this.
- What evidence would resolve it: Research and development of methods or models that can effectively combine information from multiple external documents and improve the performance of LLMs in answering complex questions.

## Limitations
- The evaluation framework's dependence on ChatGPT for instance generation introduces potential bias in question difficulty and answer phrasing that could affect model performance measurements.
- The reliance on latest news articles may not fully represent real-world RAG scenarios where document relevance and quality vary widely.
- The inability of top models to reject irrelevant queries (best at 45% rejection rate) and effectively integrate information from multiple documents (best at 67% accuracy) suggests fundamental architectural constraints.

## Confidence
- **High Confidence**: Claims about specific benchmark performance metrics (e.g., "best performing model on negative rejection only achieves a 45% rejection rate")
- **Medium Confidence**: Conclusions about LLM limitations in RAG abilities based on RGB results
- **Medium Confidence**: The effectiveness of using latest news to avoid internal knowledge bias

## Next Checks
1. Test RGB benchmark performance across different domains beyond news articles to assess generalizability of findings
2. Evaluate the impact of document retrieval quality on LLM performance by systematically varying document relevance levels
3. Compare RGB results with human performance on the same tasks to establish performance baselines and identify specific LLM weaknesses