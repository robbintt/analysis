---
ver: rpa2
title: 'DialogBench: Evaluating LLMs as Human-like Dialogue Systems'
arxiv_id: '2311.01677'
source_url: https://arxiv.org/abs/2311.01677
tags:
- dialogue
- llms
- evaluation
- arxiv
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DialogBench is a comprehensive benchmark for evaluating large language
  models as human-like dialogue systems. It contains 12 dialogue tasks that assess
  capabilities such as coherence, correctness, consistency, and safety.
---

# DialogBench: Evaluating LLMs as Human-like Dialogue Systems

## Quick Facts
- arXiv ID: 2311.01677
- Source URL: https://arxiv.org/abs/2311.01677
- Reference count: 14
- 28 LLMs evaluated on 12 dialogue tasks showing instruction tuning improves human-likeness to some extent

## Executive Summary
DialogBench is a comprehensive benchmark designed to evaluate large language models (LLMs) as human-like dialogue systems across 12 diverse tasks. The benchmark uses GPT-4 to generate high-quality evaluation instances with careful prompt design and bias mitigation. Experiments show that instruction tuning improves human-likeness, though most models still have room for improvement. The benchmark reveals that while LLMs excel at understanding context, they struggle with perceiving emotions and personality, and that positioning as "assistant AI" can actually weaken emotional perception.

## Method Summary
The method uses GPT-4 to generate evaluation instances for 12 dialogue tasks, then filters these instances through GPT-4's own evaluation to ensure quality. The benchmark balances domain distribution by manually specifying 20 domains to avoid the long-tail problem of GPT-4's default generation. 28 LLMs including both pre-trained and supervised instruction-tuning models are evaluated using accuracy metrics on the generated instances. Human evaluation data is used as a comparison baseline to assess human-likeness.

## Key Results
- Instruction tuning consistently improves LLM performance across most dialogue tasks
- GPT-4 achieves the highest overall performance but still shows gaps compared to human results
- LLMs demonstrate stronger context understanding than emotional perception and personality following
- Positioning LLMs as "assistant AI" can paradoxically weaken their human-like emotional capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4 can act as a surrogate human evaluator for dialogue tasks.
- **Mechanism:** GPT-4 generates evaluation instances that are later filtered by itself to ensure quality.
- **Core assumption:** GPT-4's outputs are of sufficient quality that self-filtering removes low-quality instances.
- **Evidence anchors:** [abstract] "The high human results indicate that the quality of the evaluation benchmark is very satisfactory" [section 4] "We further retain only those evaluation instances that GPT-4 considers correct" [corpus] Weak - no direct evidence GPT-4 filtering is reliable
- **Break condition:** If GPT-4's filtering is not sufficiently accurate, the benchmark will contain incorrect instances.

### Mechanism 2
- **Claim:** Instruction tuning improves human-likeness of LLMs.
- **Mechanism:** Supervised instruction-tuning models outperform their pre-trained counterparts across most tasks.
- **Core assumption:** Instruction tuning aligns model behavior with human preferences and capabilities.
- **Evidence anchors:** [abstract] "instruction fine-tuning benefits improve the human likeness of LLMs to a certain extent" [section 5.2] "instruction-tuning LLMs achieve higher scores than the corresponding pre-trained LLMs on most dialogue tasks" [corpus] Moderate - direct comparison of pre-trained vs instruction-tuned models
- **Break condition:** If instruction tuning does not consistently improve performance, the assumption fails.

### Mechanism 3
- **Claim:** Domain bias can be mitigated by external specification.
- **Mechanism:** Specifying 20 domains externally balances the distribution of evaluation instances.
- **Core assumption:** Manual domain specification is more balanced than GPT-4's default generation.
- **Evidence anchors:** [section 3.4] "we manually designate 20 domains... to balance the amount of instances in each domain" [corpus] Moderate - addresses long-tail domain distribution issue
- **Break condition:** If manually specified domains still show bias or miss important domains.

## Foundational Learning

- **Concept:** Prompt engineering principles
  - **Why needed here:** Effective prompts are crucial for generating high-quality evaluation instances
  - **Quick check question:** What are the four key ingredients of a prompt according to Zhao et al. [2023a]?

- **Concept:** Multi-choice question design
  - **Why needed here:** Unifies evaluation across generation and understanding tasks
  - **Quick check question:** How does accuracy as a metric help compare different task types?

- **Concept:** Bias mitigation techniques
  - **Why needed here:** Ensures evaluation instances are diverse and representative
  - **Quick check question:** What are the three types of bias addressed in the prompt optimization?

## Architecture Onboarding

- **Component map:** GPT-4 → Prompt generator → Bias mitigation → Data filter → DialogBench
- **Critical path:** Prompt design → Instance generation → Filtering → Task evaluation
- **Design tradeoffs:** Manual domain specification vs GPT-4 default generation
- **Failure signatures:** Domain imbalance, style bias, position bias in answers
- **First 3 experiments:**
  1. Generate instances with basic prompt and analyze domain distribution
  2. Test filtering mechanism by having GPT-4 evaluate its own outputs
  3. Compare pre-trained vs instruction-tuned models on a subset of tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the positioning of LLMs as assistant AI impact their performance on tasks related to human-like abilities such as emotional perception and personality following?
- Basis in paper: Explicit - The paper states "Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life."
- Why unresolved: The paper mentions this impact but does not provide a detailed analysis of why this occurs or how it specifically affects different tasks related to human-like abilities.
- What evidence would resolve it: A detailed study comparing the performance of LLMs with different positioning (e.g., assistant AI vs. dialogue system) on tasks related to emotional perception and personality following, along with an analysis of the underlying reasons for the observed differences.

### Open Question 2
- Question: How does the domain of the dialogue impact the performance of LLMs on tasks related to human-like abilities?
- Basis in paper: Explicit - The paper states "Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life."
- Why unresolved: The paper mentions this impact but does not provide a detailed analysis of why this occurs or how it specifically affects different tasks related to human-like abilities.
- What evidence would resolve it: A detailed study comparing the performance of LLMs with different positioning (e.g., assistant AI vs. dialogue system) on tasks related to emotional perception and personality following, along with an analysis of the underlying reasons for the observed differences.

### Open Question 3
- Question: What are the specific limitations of LLMs in perceiving emotions and personality, and how can these limitations be addressed?
- Basis in paper: Explicit - The paper states "LLMs are generally better at correctly understanding context, but relatively poor at perceiving emotions and personality."
- Why unresolved: The paper mentions these limitations but does not provide a detailed analysis of the specific challenges LLMs face in perceiving emotions and personality, or potential solutions to address these limitations.
- What evidence would resolve it: A comprehensive analysis of the specific challenges LLMs face in perceiving emotions and personality, along with potential solutions such as improved training data, novel architectures, or specialized fine-tuning techniques.

## Limitations

- GPT-4's self-filtering reliability is questionable with no direct evidence provided for its effectiveness
- Incomplete specification of bias mitigation techniques raises concerns about potential remaining biases
- Potential domain coverage gaps despite manual specification of 20 domains

## Confidence

- High: Instruction tuning consistently improves performance across tasks
- Medium: GPT-4 can generate quality evaluation instances and human-likeness gaps exist
- Low: GPT-4's self-filtering reliability and complete bias mitigation effectiveness

## Next Checks

1. **Cross-Evaluator Validation**: Have humans evaluate a random sample of GPT-4-generated instances to verify the filtering mechanism's accuracy and identify potential quality issues.

2. **Bias Analysis**: Conduct a systematic analysis of the 20 manually specified domains to check for coverage gaps, distribution balance, and potential blind spots in domain representation.

3. **Model Architecture Analysis**: Compare performance across different model architectures (not just instruction-tuning status) to determine if observed performance gaps reflect genuine capability differences versus architectural similarities to GPT-4.