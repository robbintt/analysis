---
ver: rpa2
title: Accelerating Machine Learning Algorithms with Adaptive Sampling
arxiv_id: '2309.14221'
source_url: https://arxiv.org/abs/2309.14221
tags:
- algorithm
- each
- dataset
- data
- banditmips
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis demonstrates how multi-armed bandit techniques can
  accelerate machine learning algorithms. The core method converts computationally
  intensive subroutines into statistical estimation problems, using adaptive sampling
  to avoid unnecessary computation.
---

# Accelerating Machine Learning Algorithms with Adaptive Sampling

## Quick Facts
- arXiv ID: 2309.14221
- Source URL: https://arxiv.org/abs/2309.14221
- Reference count: 0
- Primary result: Multi-armed bandit techniques accelerate ML algorithms by converting intensive subroutines into statistical estimation problems

## Executive Summary
This thesis demonstrates how multi-armed bandit techniques can accelerate machine learning algorithms by converting computationally intensive subroutines into statistical estimation problems. The core method uses adaptive sampling to avoid unnecessary computation, achieving significant speedups across multiple ML problems. The work applies this approach to k-medoids clustering (achieving O(n log n) instead of O(n²) runtime), tree-based model training (reducing complexity from O(n) to O(1) per node split), and maximum inner product search (eliminating explicit dependence on data dimension d).

## Method Summary
The method converts computationally intensive subroutines into statistical estimation problems using multi-armed bandit techniques. For each problem, candidate solutions are treated as "arms" in a bandit problem, and pulling an arm means sampling a small subset of data to estimate solution quality. Adaptive sampling allocates computational resources based on relative quality, using confidence intervals to eliminate poor candidates early while refining estimates for promising ones. This approach achieves comparable or better accuracy than state-of-the-art while providing significant speedups.

## Key Results
- Achieves O(n log n) runtime for k-medoids clustering instead of O(n²)
- Reduces tree-based model training complexity from O(n) to O(1) per node split
- Eliminates explicit dependence on data dimension d for maximum inner product search
- Provides up to 200x fewer distance computations in k-medoids clustering
- Achieves 20x faster MIPS in high dimensions while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting computationally intensive subroutines into statistical estimation problems enables O(n log n) scaling instead of O(n²).
- Mechanism: Each intensive computation is reformulated as a best-arm identification problem where arms represent candidate solutions and pulling an arm means sampling a small subset of data to estimate quality.
- Core assumption: The random variables representing arm rewards are σ-sub-Gaussian, allowing confidence intervals to be constructed and suboptimal arms to be eliminated early.
- Break condition: If the sub-Gaussianity assumption fails or gaps between arm parameters are very small, requiring nearly all data to be sampled.

### Mechanism 2
- Claim: Adaptive sampling allocates computational resources based on the relative quality of candidate solutions, not uniformly across all candidates.
- Mechanism: Uses confidence intervals to identify and eliminate poor candidates early while refining estimates for promising candidates, focusing computation where it matters most.
- Core assumption: The gap between the best and second-best solutions is sufficiently large to allow early elimination of suboptimal candidates.
- Break condition: When multiple candidates have similar quality, requiring nearly uniform sampling across all candidates.

### Mechanism 3
- Claim: The complexity of the adaptive sampling approach depends on the heterogeneity of solution qualities rather than dataset size.
- Mechanism: Sample complexity scales with the inverse square of gaps between candidate qualities, allowing O(1) or O(log n) scaling when gaps don't depend on n.
- Core assumption: The quality gaps between candidates are independent of the dataset size n.
- Break condition: When gaps between candidates scale with n, forcing nearly linear complexity.

## Foundational Learning

- Concept: Multi-armed bandit theory and best-arm identification algorithms
  - Why needed here: The entire approach relies on casting optimization problems as bandit problems and using bandit algorithms for solution
  - Quick check question: What is the difference between regret minimization and best-arm identification in multi-armed bandits?

- Concept: Sub-Gaussian random variables and concentration inequalities
  - Why needed here: These mathematical properties enable the construction of confidence intervals used to eliminate suboptimal candidates
  - Quick check question: What does it mean for a random variable to be σ-sub-Gaussian and why is this property important for the algorithm?

- Concept: Tree-based model construction and node-splitting optimization
  - Why needed here: The MABSplit algorithm specifically accelerates the node-splitting step in decision trees by finding optimal feature-threshold pairs
  - Quick check question: How does the impurity reduction calculation work for Gini impurity and entropy in decision tree splitting?

## Architecture Onboarding

- Component map: BanditPAM -> MABSplit -> BanditMIPS -> Successive elimination
- Critical path: For each problem, identify the intensive subroutine → reformulate as best-arm identification → implement adaptive sampling with confidence intervals → eliminate suboptimal candidates → return best solution
- Design tradeoffs: Accuracy vs. speed through error probability δ, sub-Gaussianity assumptions vs. generality, batch size vs. responsiveness to data
- Failure signatures: Algorithm degrades to near-linear complexity when gaps are small, incorrect results when distributional assumptions fail, performance loss when batch size is too small
- First 3 experiments:
  1. Run BanditPAM on small synthetic dataset (n=100) with known ground truth to verify correctness
  2. Profile MABSplit on MNIST to measure reduction in distance computations vs. exact solver
  3. Test BanditMIPS scaling with d on synthetic dataset where gaps are known to be constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific distributional assumptions on arm parameters does BanditPAM achieve optimal O(n log n) sample complexity, and can these assumptions be relaxed?
- Basis in paper: The paper states that the O(n log n) complexity relies on arm parameters following a sub-Gaussian distribution across steps, and discusses relaxing this assumption in Appendix A.2.1.
- Why unresolved: The paper provides empirical justification but does not prove optimality under the stated assumptions or provide a complete characterization of when these assumptions hold.
- What evidence would resolve it: A formal proof of optimality under sub-Gaussian assumptions, or empirical studies showing performance degradation when these assumptions are violated on various real-world datasets.

### Open Question 2
- Question: How does the choice of hyperparameter δ affect the clustering quality and runtime trade-off in BanditPAM, and can this trade-off be characterized theoretically?
- Basis in paper: The paper discusses δ as a hyperparameter governing error probability but notes this requires further investigation in Appendix A.2.3.
- Why unresolved: While the paper mentions this trade-off exists, it does not provide theoretical bounds or empirical studies characterizing how clustering quality degrades as δ increases.
- What evidence would resolve it: Theoretical bounds relating δ to clustering loss, or extensive empirical studies mapping δ values to clustering quality across diverse datasets.

### Open Question 3
- Question: What is the explicit dependence of BanditPAM on the number of clusters k, and under what conditions does this dependence become superlinear?
- Basis in paper: The paper notes that the dependence on k is not fully analyzed and mentions empirical observations of linear and quadratic scaling in different regimes.
- Why unresolved: The paper treats k as a constant in theoretical analysis but acknowledges this may not always be valid, without providing a complete characterization of when superlinear scaling occurs.
- What evidence would resolve it: Theoretical analysis of how k affects the complexity bound, or extensive empirical studies showing the relationship between k and runtime across diverse datasets and clustering scenarios.

## Limitations
- The adaptive sampling approach relies heavily on sub-Gaussian assumptions about the random variables representing candidate qualities
- The method requires sufficiently large gaps between candidate qualities to achieve optimal scaling
- Theoretical analysis is primarily asymptotic, and practical performance depends on proper hyperparameter tuning

## Confidence

- High confidence: The core mechanism of converting optimization subroutines to bandit problems is well-established in the literature and the empirical results showing speedups are compelling
- Medium confidence: The theoretical guarantees hold under stated assumptions, but real-world performance may vary with data distribution characteristics
- Low confidence: The scalability claims for BanditMIPS in extremely high dimensions (>10⁶) have limited empirical validation

## Next Checks

1. Test BanditPAM on datasets with heavy-tailed distance distributions to verify robustness when sub-Gaussian assumptions are violated
2. Profile the algorithm's behavior when multiple candidates have nearly identical quality to measure the impact of small gaps on sample complexity
3. Benchmark BanditMIPS on synthetic high-dimensional data (d > 10⁶) with varying sparsity patterns to validate the claimed independence from dimension d