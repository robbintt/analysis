---
ver: rpa2
title: Evaluating Large Language Models in Ophthalmology
arxiv_id: '2311.04933'
source_url: https://arxiv.org/abs/2311.04933
tags:
- medical
- ophthalmology
- language
- questions
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the performance of three large language models\
  \ (LLMs)\u2014GPT-3.5, GPT-4, and PaLM2\u2014in answering ophthalmology-specific\
  \ questions, comparing their accuracy with medical undergraduates, masters students,\
  \ and attending physicians. A 100-item single-choice test was used to assess performance\
  \ across three dimensions: average score, stability, and confidence."
---

# Evaluating Large Language Models in Ophthalmology

## Quick Facts
- arXiv ID: 2311.04933
- Source URL: https://arxiv.org/abs/2311.04933
- Reference count: 40
- GPT-4 matched attending physician accuracy on ophthalmology questions while outperforming GPT-3.5 and PaLM2

## Executive Summary
This study evaluated three large language models (GPT-3.5, GPT-4, PaLM2) on 100 ophthalmology single-choice questions, comparing their performance against medical undergraduates, masters students, and attending physicians. GPT-4 achieved accuracy levels comparable to attending physicians and demonstrated superior answer stability and confidence compared to the other models. While GPT-4 shows strong potential for medical education and clinical support applications, the study acknowledges limitations in handling complex clinical tasks and visual data, indicating these systems cannot fully replace human experts.

## Method Summary
The study administered 100 ophthalmology single-choice questions to three LLMs and three human groups (undergraduates, masters students, attending physicians) in a closed-book exam format. Performance was evaluated across three dimensions: average score, answer stability (measured through repeated trials), and confidence (percentage of questions answered correctly). Questions were drafted by experienced ophthalmologists and processed in batches to accommodate model limitations. Human subjects completed the exam in 3 hours, while LLMs were evaluated through multiple inference runs to assess consistency.

## Key Results
- GPT-4 matched attending physician accuracy levels on ophthalmology knowledge questions
- GPT-4 demonstrated significantly higher answer stability (mean correlation 0.83) compared to GPT-3.5 (0.59) and PaLM2 (0.61)
- GPT-4 showed superior confidence metrics, with 45% of answers correct and only 37% probability of confusion
- GPT-3.5 and PaLM2 performed slightly below masters student level on average

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms smaller models on ophthalmology knowledge recall and reasoning
- Mechanism: Larger parameter scale (~1T) and instruction tuning enable superior domain-specific fact retention and reasoning
- Core assumption: Larger models trained on diverse corpora internalize specialized knowledge without explicit medical fine-tuning
- Evidence anchors:
  - GPT-4 matched attending physician accuracy levels
  - GPT-4 outperformed GPT-3.5 and PaLM2 on average scores
- Break condition: Visual interpretation tasks expose text-only training limitations

### Mechanism 2
- Claim: GPT-4's answer stability improves with parameter count and alignment tuning
- Mechanism: Transformer layers learn coherent reasoning paths across repeated queries
- Core assumption: Parameter scaling and alignment reduce answer stochasticity in single-choice contexts
- Evidence anchors:
  - GPT-4 showed higher mean correlation (0.83) vs GPT-3.5 (0.59) and PaLM2 (0.61)
  - Superior stability demonstrated across multiple trials
- Break condition: Real-time updates beyond training cutoff erode stability advantage

### Mechanism 3
- Claim: GPT-4 exhibits higher confidence through sharper probability distributions
- Mechanism: Better alignment produces lower entropy predictions with higher consistency
- Core assumption: Sharpness of output distribution correlates with ground-truth accuracy
- Evidence anchors:
  - GPT-4 demonstrated superior confidence metrics
  - 45% correct answers with only 37% probability of confusion
- Break condition: Open-ended tasks reduce confidence-prediction correlation

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how GPT models process text sequences is critical for diagnosing answer inconsistencies
  - Quick check question: In a transformer, what operation allows tokens to "see" all other tokens in the sequence, and how does this relate to multi-hop reasoning?

- Concept: Few-shot and instruction tuning
  - Why needed here: GPT-4's performance relies on alignment with human preferences rather than task-specific fine-tuning
  - Quick check question: How does instruction tuning differ from standard fine-tuning, and why might it improve stability across repeated queries?

- Concept: Evaluation metrics for NLP models
  - Why needed here: The study uses mean score, stability, and confidence metrics that require proper interpretation
  - Quick check question: If a model answers 45 out of 100 questions correctly on average over five trials, what is its mean accuracy and how would you compute the standard deviation across trials?

## Architecture Onboarding

- Component map: Input preprocessing -> tokenizer -> transformer layers -> output head -> probability distribution -> argmax selection -> evaluation scoring
- Critical path: 1) Load model weights, 2) Tokenize 100 questions, 3) Run inference in fixed batches, 4) Collect answers and compute mean scores, 5) Repeat inference 5 times for stability, 6) Aggregate results and generate metrics
- Design tradeoffs: Batch size vs memory constraints, repetition count vs statistical power, single-choice format vs open-ended tasks
- Failure signatures: Low answer consistency across trials, wide score variance, low confidence despite high accuracy
- First 3 experiments: 1) Run each model on 10-question subset 5 times and compare mean scores and std dev, 2) Increase batch size to 50 and observe impact on completeness and runtime, 3) Replace single-choice with open-ended prompts and evaluate stability changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of LLMs in handling complex clinical tasks and visual data in ophthalmology, and how can these limitations be addressed?
- Basis in paper: The paper mentions GPT-4 cannot fully replace human experts due to limitations in handling complex clinical tasks and visual data
- Why unresolved: The paper does not provide detailed information on specific limitations or potential solutions
- What evidence would resolve it: Research studies identifying and addressing specific LLM limitations in handling complex clinical tasks and visual data

### Open Question 2
- Question: How can LLMs be integrated into the clinical workflow to enhance ophthalmologists' work without replacing their expertise?
- Basis in paper: The paper suggests LLMs can enhance work but does not provide specific integration examples
- Why unresolved: The paper lacks specific guidelines or examples on LLM integration into clinical workflow
- What evidence would resolve it: Case studies or pilot programs demonstrating successful LLM integration and impact on ophthalmologists' work

### Open Question 3
- Question: What are the potential ethical and legal considerations associated with using LLMs in ophthalmology, and how can these be addressed?
- Basis in paper: The paper mentions compliance with regulations but does not detail ethical and legal considerations
- Why unresolved: The paper does not provide specific information on ethical and legal considerations
- What evidence would resolve it: Research studies identifying and addressing ethical and legal considerations, including responsibility and accountability in medical AI

## Limitations
- Study limited to text-only evaluation without visual question assessment
- Performance gap between human experts and GPT-4 on complex clinical tasks remains unmeasured
- 100-question single-choice format may not capture full clinical reasoning complexity

## Confidence

- **High Confidence**: GPT-4's superior performance on knowledge recall and reasoning compared to GPT-3.5 and PaLM2
- **Medium Confidence**: GPT-4 matching attending physician accuracy levels (limited by unknown question difficulty distribution)
- **Low Confidence**: Extrapolation of results to clinical decision-making and medical education applications

## Next Checks
1. Evaluate all three models on ophthalmology questions requiring visual interpretation to assess real-world clinical limitations
2. Conduct longitudinal testing with regularly updated question sets to measure model performance against evolving medical knowledge
3. Implement open-ended clinical scenario testing to compare diagnostic reasoning patterns between LLMs and human experts