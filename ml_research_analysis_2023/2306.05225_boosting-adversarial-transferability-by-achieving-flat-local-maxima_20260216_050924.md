---
ver: rpa2
title: Boosting Adversarial Transferability by Achieving Flat Local Maxima
arxiv_id: '2306.05225'
source_url: https://arxiv.org/abs/2306.05225
tags:
- adversarial
- examples
- attack
- gradient
- transferability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes that adversarial examples at flat local regions
  tend to have better transferability. Based on this insight, it introduces Penalizing
  Gradient Norm (PGN), an attack method that approximates second-order Hessian matrices
  using first-order gradients and samples multiple data points to stabilize gradient
  updates.
---

# Boosting Adversarial Transferability by Achieving Flat Local Maxima

## Quick Facts
- arXiv ID: 2306.05225
- Source URL: https://arxiv.org/abs/2306.05225
- Reference count: 40
- Key outcome: PGN improves black-box attack success rates by up to 19.4% on defense models by locating adversarial examples in flat local regions

## Executive Summary
This paper proposes a novel method to improve the transferability of adversarial examples by optimizing them to reside in flat local regions of the loss landscape. The authors introduce Penalizing Gradient Norm (PGN), which approximates second-order Hessian matrices using first-order gradients and averages multiple samples to stabilize updates. Experiments demonstrate significant improvements over state-of-the-art methods on both normally and adversarially trained models, with compatibility for input transformation techniques. The work bridges the gap between flatness of loss surfaces and adversarial transferability.

## Method Summary
The PGN method introduces a gradient norm penalty into the loss function to encourage optimization toward flatter regions. It approximates second-order Hessian matrices using first-order gradients via finite difference interpolation, and averages gradients from multiple randomly sampled examples to stabilize updates. The method integrates with existing gradient-based attack frameworks like I-FGSM and MI-FGSM, requiring only minor modifications to the loss computation. PGN operates within the standard iterative attack framework but modifies the gradient direction to favor flat local maxima regions.

## Key Results
- PGN achieves up to 19.4% improvement in attack success rates on adversarially trained defense models
- Significant gains over state-of-the-art methods on normally trained models (5.3% and 7.2% improvements for I-FGSM and MI-FGSM respectively)
- Compatible with input transformation methods (DIM, TIM) for additional transferability boosts
- Visualization confirms PGN locates adversarial examples in flatter loss regions compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples at flat local regions have better transferability
- Mechanism: Gradient norm penalty encourages optimization toward flatter loss surfaces
- Core assumption: Flatness correlates with better generalization and transferability
- Evidence: Fig. 1 shows clear improvements with gradient regularization
- Break condition: Gradient penalty may push examples too far from decision boundary

### Mechanism 2
- Claim: First-order gradients can approximate second-order Hessian for flat region discovery
- Mechanism: Finite difference interpolation of two Jacobian matrices
- Core assumption: Accurate Hessian approximation possible with small neighborhood sampling
- Evidence: Corollary 1 provides mathematical formulation for approximation
- Break condition: High curvature regions may cause approximation errors

### Mechanism 3
- Claim: Multi-sample gradient averaging reduces variance and stabilizes flat region optimization
- Mechanism: Random sampling of N examples from neighborhood, averaging their gradients
- Core assumption: Multiple samples provide more stable gradient estimates
- Evidence: Random sampling shown to improve gradient stability
- Break condition: Too few samples leave high variance; too many increase computational cost

## Foundational Learning

- Concept: Gradient-based adversarial attacks (I-FGSM, MI-FGSM)
  - Why needed: PGN builds upon these methods by modifying gradient updates
  - Quick check: I-FGSM uses single-step momentum; MI-FGSM accumulates velocity over iterations

- Concept: Second-order optimization and Hessian matrices
  - Why needed: PGN approximates Hessian to find flat regions efficiently
  - Quick check: Direct Hessian computation requires O(n²) operations for n parameters

- Concept: Transfer-based black-box attacks
  - Why needed: PGN targets improved transferability across unknown target models
  - Quick check: Transferability measures success rate when attacking a different model than the surrogate

## Architecture Onboarding

- Component map: Loss function with gradient penalty -> Neighborhood sampling -> Hessian approximation -> Multi-sample averaging -> Gradient update -> Projection

- Critical path: Initialize adversarial example → Sample neighborhood examples → Compute gradients → Approximate Hessian → Average gradients → Update perturbation → Project to epsilon-ball

- Design tradeoffs: Approximation accuracy vs computational cost; neighborhood size vs stability; sample count vs variance reduction; compatibility vs specialized optimization

- Failure signatures: High white-box success but low transferability; gradient explosion/NaN values; adversarial examples too similar to clean inputs; performance degradation on defense models

- First 3 experiments: 1) Validate gradient norm penalty on single model (Inc-v3); 2) Test δ and ζ hyperparameters; 3) Combine PGN with input transformation (DIM, TIM)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PGN perform on datasets beyond ImageNet and CIFAR-10?
- Basis: Paper only evaluates on these two datasets
- Why unresolved: No results on other datasets provided
- Evidence needed: Testing on MNIST, SVHN, or other image datasets

### Open Question 2
- Question: Performance on larger models like ResNet-101, ResNet-152?
- Basis: Paper tests smaller models (VGG-16, ResNet-50, DenseNet-121)
- Why unresolved: No results on larger architectures
- Evidence needed: Evaluation on deeper models to test scalability

### Open Question 3
- Question: Performance on non-image datasets like text or audio?
- Basis: Paper focuses exclusively on image datasets
- Why unresolved: No results on non-image domains
- Evidence needed: Testing on text or audio adversarial examples

### Open Question 4
- Question: Performance on white-box or query-based attacks?
- Basis: Paper focuses on transfer-based black-box attacks
- Why unresolved: No results on other attack types
- Evidence needed: Evaluation on white-box and query-based scenarios

## Limitations

- Theoretical foundation linking flatness to transferability across diverse architectures remains unproven
- No rigorous error bounds provided for the first-order Hessian approximation method
- Computational overhead from neighborhood sampling and gradient averaging not fully quantified

## Confidence

- High confidence: PGN improves black-box attack success rates vs baselines
- Medium confidence: Flat regions inherently produce more transferable examples
- Medium confidence: First-order Hessian approximation is sufficiently accurate

## Next Checks

1. Cross-architecture validation: Test PGN transferability on transformers, vision-language models, and different training paradigms
2. Ablation on approximation accuracy: Systematically vary neighborhood size ζ and measure approximation error vs attack success tradeoffs
3. Defense robustness analysis: Evaluate PGN against gradient-based and certified defenses to test exploitability/mitigation potential