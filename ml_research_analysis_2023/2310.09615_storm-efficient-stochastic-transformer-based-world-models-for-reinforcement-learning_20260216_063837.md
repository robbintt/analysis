---
ver: rpa2
title: 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement
  Learning'
arxiv_id: '2310.09615'
source_url: https://arxiv.org/abs/2310.09615
tags:
- world
- learning
- storm
- agent
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents STORM, a stochastic transformer-based world
  model for model-based reinforcement learning. The method combines the strong sequence
  modeling capabilities of Transformers with the stochastic nature of variational
  autoencoders.
---

# STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.09615
- Source URL: https://arxiv.org/abs/2310.09615
- Reference count: 40
- Mean human normalized score of 126.7% on Atari 100k benchmark

## Executive Summary
STORM presents a stochastic transformer-based world model for model-based reinforcement learning that achieves state-of-the-art performance on the Atari 100k benchmark. The method combines a categorical variational autoencoder with a GPT-like Transformer to learn latent dynamics from limited data, enabling efficient agent training with minimal real-world interactions. By leveraging parallel computation and masked self-attention, STORM improves both modeling quality and training efficiency compared to previous recurrent approaches.

## Method Summary
STORM uses a categorical VAE to compress image observations into a 32×32 stochastic latent space, which is then fused with actions via MLPs to form tokens for a GPT-like Transformer sequence model. The Transformer predicts future latents, rewards, and continuation flags in parallel using masked self-attention. An actor-critic agent is trained on imagined trajectories generated by the world model, with the state representation combining latent samples and hidden states. Training alternates between world model updates using self-supervised losses and agent policy improvements, achieving high sample efficiency on the Atari 100k benchmark.

## Key Results
- Achieves mean human normalized score of 126.7% on Atari 100k, setting new state-of-the-art record
- Median human normalized score of 58.4% outperforms all previous methods without lookahead search
- Trains with only 1.85 hours of real-time interaction experience on a single RTX 3090 in 4.3 hours total

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of categorical VAE and Transformer enables robust latent state modeling with explicit stochasticity.
- Mechanism: The VAE maps high-dimensional image observations into a 32×32 categorical latent distribution Zt, which introduces stochasticity while preserving semantic structure. This latent representation, when combined with actions via the action mixer, provides the Transformer with compressed but informative tokens that capture both spatial and temporal dependencies.
- Core assumption: The categorical distribution with 32 categories and 32 classes per category is sufficient to represent the state space for Atari games.
- Evidence anchors:
  - [abstract] "STORM employs a categorical variational autoencoder (VAE) as the image encoder, enhancing the agent robustness and reducing accumulated autoregressive prediction errors."
  - [section 3.1] "We leverage a VAE to convert ot into latent stochastic categorical distributions Zt. Consistent with prior work, we set Zt as a stochastic distribution comprising 32 categories, each with 32 classes."
- Break condition: If the categorical representation becomes too coarse for environments with fine-grained visual distinctions, the latent space may fail to encode necessary details, leading to policy degradation.

### Mechanism 2
- Claim: The Transformer's masked self-attention architecture allows parallel computation and long-range dependency modeling without RNN's vanishing gradient issues.
- Mechanism: The GPT-like Transformer processes the fused observation-action tokens in parallel, using masked self-attention to predict future states autoregressively. The KV cache technique accelerates inference by reusing previously computed keys and values.
- Core assumption: The masked attention mechanism can effectively model the temporal dynamics of Atari environments without explicit recurrence.
- Evidence anchors:
  - [abstract] "STORM employs a GPT-like Transformer as the sequence model, improving modeling and generation quality while accelerating training."
  - [section 3.1] "We adopt a GPT-like Transformer structure for the sequence model, where the self-attention blocks are masked with a subsequent mask allowing et to attend to the sequence e1, e2, ..., et."
- Break condition: In highly stochastic environments where small perturbations cascade, the lack of explicit memory mechanisms may cause the model to lose track of important past states.

### Mechanism 3
- Claim: The separate treatment of observation-action fusion versus separate token modeling improves learning efficiency.
- Mechanism: By fusing zt and at into a single token et using MLPs before feeding to the Transformer, the model learns a joint representation that is more compact and contextually relevant than treating them as separate tokens.
- Core assumption: The fused representation captures the necessary interaction between observations and actions without losing discriminative power.
- Evidence anchors:
  - [section 2] "STORM follows a vanilla Transformer structure, while TWM adopts a Transformer-XL structure. In the sequence model of STORM, an observation and an action are fused into a single token, whereas TWM treats observation, action, and reward as three separate tokens of equal importance."
  - [section 3.1] "Before entering the sequence model, we combine the latent sample zt and the action at into a single token et using multi-layer perceptrons (MLPs) and concatenation."
- Break condition: If the MLP fusion layer cannot adequately capture complex action-observation interactions, the model may fail to learn effective policies in environments requiring precise action selection.

## Foundational Learning

- Concept: Variational Autoencoders and their role in stochastic latent space modeling
  - Why needed here: VAEs provide a principled way to introduce stochasticity into the latent space, which helps the agent handle uncertainty and reduces the impact of accumulated prediction errors during imagination.
  - Quick check question: What is the difference between a standard autoencoder and a variational autoencoder in terms of latent space properties?

- Concept: Transformer architecture and masked self-attention
  - Why needed here: The Transformer's parallel computation and attention mechanisms enable efficient modeling of long-range dependencies in visual observations and actions, which is critical for world modeling in reinforcement learning.
  - Quick check question: How does the subsequent mask in the Transformer ensure autoregressive generation without revealing future information?

- Concept: Reinforcement learning with world models and the role of imagined experience
  - Why needed here: The world model generates imagined trajectories that allow the agent to learn from simulated experiences, reducing the need for costly real-environment interactions while improving sample efficiency.
  - Quick check question: Why is it beneficial to train the policy using imagined trajectories rather than only real trajectories?

## Architecture Onboarding

- Component map:
  - Image encoder (VAE) -> latent distribution Zt -> latent sample zt
  - Action mixer -> fuses zt and at -> token et
  - Transformer sequence model -> processes et sequence -> predicts future states, rewards, continuation flags
  - Image decoder -> reconstructs observations from latent samples
  - Agent -> learns policy and value function from imagined trajectories

- Critical path:
  1. Observation → VAE → latent distribution Zt
  2. Sample zt → fuse with action at → token et
  3. Transformer processes et sequence → predicts future states and rewards
  4. Agent uses imagined trajectories to update policy and value function

- Design tradeoffs:
  - Categorical VAE vs continuous VAE: Categorical provides discrete semantic structure but may lose fine-grained details
  - Fused vs separate tokens: Fused is more compact but may limit expressiveness for complex action-observation interactions
  - Small vs large Transformer: Smaller is faster but may struggle with complex environments; larger needs more data

- Failure signatures:
  - Poor reconstruction quality in VAE → blurry or incorrect observations
  - High KL divergence in latent dynamics → unstable predictions
  - Policy collapse → agent exploits model inaccuracies
  - Slow training → inefficient architecture or hyperparameters

- First 3 experiments:
  1. Train VAE alone on random observations to verify reconstruction quality
  2. Train sequence model with fixed VAE to check latent dynamics prediction
  3. Train full system with simplified agent (e.g., random policy) to isolate world model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of STORM scale with larger amounts of training data and larger model sizes compared to other state-of-the-art methods?
- Basis in paper: [inferred] The paper mentions that the Atari 100k games provide limited samples and do not offer images from diverse domains. The authors suggest that training a larger model may require a substantial amount of data, which is not available in this benchmark. However, they do not explore the performance of STORM with larger datasets or model sizes.
- Why unresolved: The paper does not provide experimental results or analysis on how STORM performs with larger datasets or model sizes. This could be an interesting direction for future research to determine if STORM can maintain its performance advantage with more data and larger models.
- What evidence would resolve it: Conducting experiments with larger datasets and varying model sizes for STORM and comparing its performance with other state-of-the-art methods would provide insights into the scalability of STORM.

### Open Question 2
- Question: How does the inclusion of a demonstration trajectory impact the exploration efficiency and final performance of STORM in environments with sparse rewards?
- Basis in paper: [explicit] The paper mentions that adding a demonstration trajectory to the replay buffer can improve the robustness or performance of STORM in environments with sparse rewards, such as Pong and Freeway. However, the authors do not provide a detailed analysis of the impact of demonstration trajectories on exploration efficiency and final performance.
- Why unresolved: The paper does not provide a comprehensive study on the impact of demonstration trajectories on exploration efficiency and final performance in environments with sparse rewards. This could be an important aspect to investigate further to understand the benefits and limitations of using demonstration trajectories in STORM.
- What evidence would resolve it: Conducting experiments with and without demonstration trajectories in environments with sparse rewards and analyzing the exploration efficiency and final performance of STORM would provide insights into the impact of demonstration trajectories.

### Open Question 3
- Question: How does the choice of the agent's state representation (e.g., zt, ht, or [zt, ht]) affect the performance of STORM in different types of environments?
- Basis in paper: [explicit] The paper discusses the selection of the agent's state representation and mentions that the inclusion of ht in the state can lead to improved performance in environments that require contextual information, such as Ms. Pacman. However, the authors do not provide a detailed analysis of how the choice of state representation affects performance in different types of environments.
- Why unresolved: The paper does not provide a comprehensive study on the impact of different state representations on the performance of STORM in various types of environments. This could be an important aspect to investigate further to understand the role of state representation in the effectiveness of STORM.
- What evidence would resolve it: Conducting experiments with different state representations in various types of environments and analyzing the performance of STORM would provide insights into the impact of state representation on the effectiveness of STORM.

## Limitations

- Categorical VAE compression (32×32 latent space) may lose fine-grained visual details critical for precise control in complex environments
- Fixed mask attention pattern in Transformer may not capture all temporal dependencies needed for highly stochastic environments
- Evaluation on Atari 100k represents limited sample budget, raising questions about scalability to more data-intensive scenarios

## Confidence

- **High confidence** in core architectural contributions (VAE+Transformer combination, fused token representation) given clear implementation details and consistent results across multiple games
- **Medium confidence** in claimed efficiency improvements due to lack of direct comparison with identical hardware configurations across all baseline methods
- **Medium confidence** in robustness claims, as the paper demonstrates strong performance but doesn't extensively analyze failure modes or distribution shifts

## Next Checks

1. Test the VAE reconstruction quality specifically on small moving objects across different Atari games to quantify the visual information loss
2. Compare the policy performance when using separate action-observation tokens versus the fused representation to isolate the contribution of this design choice
3. Evaluate the model's performance when trained on 10× and 100× the sample budget to assess scalability and potential overfitting to the 100k constraint