---
ver: rpa2
title: Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?
arxiv_id: '2307.13136'
source_url: https://arxiv.org/abs/2307.13136
tags:
- progress
- benchmarks
- generalization
- timm
- geographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the gap between progress on standard ImageNet
  benchmarks and real-world generalization across geographies. It proposes using geographically
  diverse datasets like DollarStreet and GeoDE to measure real-world generalization,
  which capture natural distribution shifts without simulated environments or predefined
  variations.
---

# Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?

## Quick Facts
- arXiv ID: 2307.13136
- Source URL: https://arxiv.org/abs/2307.13136
- Authors: 
- Reference count: 40
- Primary result: Progress on ImageNet benchmarks exceeds real-world geographic generalization by up to 2.5x, with geographic disparities tripling over time.

## Executive Summary
This paper investigates whether improvements on standard object recognition benchmarks translate to better real-world generalization across geographic regions. The study evaluates nearly 100 vision models on geographically diverse datasets (DollarStreet and GeoDE) and finds a significant "Progress Gap" - models improve much faster on standard ImageNet benchmarks than on geographic generalization tasks. Notably, geographic disparities between regions have more than tripled over time, even for state-of-the-art foundation models like CLIP. The research demonstrates that common robustness interventions and scaling strategies fail to close these gaps, but simple last-layer retraining on geographically representative data offers a promising direction for reducing disparities.

## Method Summary
The study evaluates pretrained vision models (spanning 16 architectures including ResNet, ViT, CLIP, and DINOv2) on geographically diverse datasets DollarStreet and GeoDE. Models are tested using class mappings to ImageNet-1K, with preprocessing standardized across architectures. Key metrics include Progress Gap (difference in progress rates between standard and geographic benchmarks) and Geographic Disparity (maximum accuracy difference across regions). The evaluation tests various robustness interventions, scaling strategies, and last-layer retraining approaches to assess their impact on geographic generalization.

## Key Results
- Progress on ImageNet benchmarks exceeds real-world geographic generalization by up to 2.5x
- Geographic disparities between regions have more than tripled from early models to today's best models
- Simple last-layer retraining on geographically representative data reduces geographic disparity by over two-thirds while improving average accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard ImageNet benchmarks overestimate real-world generalization due to their focus on predefined or synthetic alterations rather than naturally occurring distribution shifts.
- **Mechanism:** Standard benchmarks (ImageNet-A, -C, -R) rely on artificially induced corruptions or predefined criteria that cannot adequately capture the rich diversity necessary for approximating real-world generalization. Geographic datasets like DollarStreet and GeoDE capture natural distribution shifts without simulated environments, preselected variations, or artificially injected transformations.
- **Core assumption:** Natural distribution shifts across geographies provide a more realistic measure of model generalization than synthetic benchmarks.
- **Evidence anchors:**
  - [abstract]: "standard benchmarks, which tend to focus on predefined or synthetic alterations of images, may not be sufficient for measuring real world generalization"
  - [section]: "Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks... This suggests standard benchmarks, which tend to focus on predefined or synthetic alterations of images, may not be sufficient for measuring real world generalization."
  - [corpus]: Weak - neighboring papers focus on different aspects of object recognition and do not directly address the synthetic vs natural shift distinction.
- **Break condition:** If synthetic benchmarks were redesigned to better capture the full spectrum of real-world variation, the gap between standard and geographic benchmarks would narrow.

### Mechanism 2
- **Claim:** Improvements on standard ImageNet benchmarks exacerbate geographic performance disparities rather than reducing them.
- **Mechanism:** As models improve on standard benchmarks, they tend to improve more on regions with abundant training data (like Europe) compared to underrepresented regions (like Africa). This creates a widening gap where geographic disparities more than triple between early models and today's best models.
- **Core assumption:** Model improvement rates differ across geographic regions, with overrepresented regions benefiting disproportionately from training on standard datasets.
- **Evidence anchors:**
  - [abstract]: "geographic disparities between the least performant models and today's best models have more than tripled"
  - [section]: "we discover, as shown in Figure 2, progress on ImageNet and its generalization benchmarks not only fails to resolve geographic disparities, but actually exacerbates disparities"
  - [corpus]: Weak - neighboring papers don't directly address the relationship between benchmark progress and geographic disparities.
- **Break condition:** If training datasets were balanced across all geographic regions, improvements would benefit all regions equally and disparities would not widen.

### Mechanism 3
- **Claim:** Simple last-layer retraining on more representative, curated data can significantly reduce geographic disparities while maintaining or improving standard benchmark performance.
- **Mechanism:** By retraining only the final classification layer on geographically diverse data (like DollarStreet), models can adapt to real-world distribution shifts without requiring full retraining. This approach reduces geographic disparity on both benchmarks by over two-thirds while dramatically improving average accuracy.
- **Core assumption:** The final classification layer is the primary bottleneck for geographic generalization, and can be effectively adapted using representative data.
- **Evidence anchors:**
  - [abstract]: "simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds"
  - [section]: "we observe improvements on GeoDE of 11.5% on average accuracy and 3.2% in geographic disparities" and "The average accuracy on DollarStreet's evaluation set improves by a dramatic 53.4% with geographic disparity also improving by 11.7%"
  - [corpus]: Weak - neighboring papers don't discuss last-layer retraining or its effectiveness for geographic generalization.
- **Break condition:** If the final layer were not the primary bottleneck, or if representative data were unavailable, this approach would be less effective.

## Foundational Learning

- **Concept:** Distribution shift and out-of-distribution generalization
  - Why needed here: Understanding how models perform on data that differs from their training distribution is central to evaluating real-world generalization
  - Quick check question: If a model achieves 95% accuracy on ImageNet but only 60% on DollarStreet, what type of distribution shift is occurring?

- **Concept:** Geographic bias in datasets
  - Why needed here: The paper shows that geographic representation in training data significantly affects model performance across different regions
  - Quick check question: Why might a model trained primarily on Western household objects perform worse on African household objects?

- **Concept:** Robustness interventions and their limitations
  - Why needed here: The paper evaluates common robustness techniques and finds they offer limited improvements to geographic disparities
  - Quick check question: If AugMix improves ImageNet-C accuracy but doesn't reduce DollarStreet geographic disparity, what does this tell us about the nature of geographic generalization?

## Architecture Onboarding

- **Component map:** ImageNet evaluation -> Standard robustness benchmarks (ImageNet-A, -C, -R) -> Geographic datasets (DollarStreet with 54 regions, GeoDE with 6 regions) -> Progress Gap and Geographic Disparity metrics

- **Critical path:** Load model weights → Preprocess input images (resize to 256, center crop to 224) → Run inference → Calculate accuracy on each benchmark → Compute geographic disparities by region → Analyze progress rates and disparity trends.

- **Design tradeoffs:** The evaluation uses class mappings between datasets to enable testing on 1K ImageNet-compatible models, but this mapping process may introduce approximation errors. Geographic disparity as a metric reveals performance differences but may oversimplify complex regional variations.

- **Failure signatures:** If geographic disparities remain high despite progress on standard benchmarks, this indicates the benchmarks don't capture real-world generalization. If last-layer retraining doesn't improve geographic performance, the issue may lie deeper in the model architecture.

- **First 3 experiments:**
  1. Load a ResNet50 model and evaluate it on ImageNet, ImageNet-A, DollarStreet, and GeoDE to establish baseline performance
  2. Compare progress rates by plotting model accuracy on standard benchmarks versus geographic benchmarks as a function of ImageNet accuracy
  3. Implement last-layer retraining on DollarStreet data and evaluate the retrained model on both DollarStreet and GeoDE to measure disparity reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does progress on standard benchmarks actually harm geographic generalization performance, or is it just uncorrelated with improvements in this area?
- Basis in paper: [explicit] The paper finds that progress on ImageNet benchmarks exacerbates geographic disparities between regions, tripling the gap between least and most performant models. However, the study doesn't definitively establish causation.
- Why unresolved: The observational nature of the study can't distinguish between progress actively harming geographic generalization versus simply failing to improve it.
- What evidence would resolve it: Controlled experiments where models are trained with explicit geographic diversity objectives, compared to standard training, could establish whether progress on standard benchmarks actively harms geographic generalization or just fails to address it.

### Open Question 2
- Question: What specific data characteristics in DINOv2 enable it to achieve smaller geographic disparities compared to other foundation models of similar size?
- Basis in paper: [explicit] DINOv2, despite being a mid-size model, achieved the smallest GeoDE region performance disparity in the study. The paper attributes this to its use of auto-curated video data.
- Why unresolved: The paper doesn't analyze what specific aspects of the auto-curation or video data contribute to better geographic generalization.
- What evidence would resolve it: Detailed analysis of DINOv2's training data distribution across geographies, compared to other foundation models, could identify key data characteristics that enable better geographic generalization.

### Open Question 3
- Question: Are there specific architectural features that correlate with better geographic generalization performance?
- Basis in paper: [inferred] The study evaluates models across 16 different architectures but doesn't find consistent patterns in how architecture affects geographic generalization. This suggests architecture might play a role, but the relationship isn't clear.
- Why unresolved: The large number of architectural variations and other confounding factors (like training data) make it difficult to isolate the impact of architecture on geographic generalization.
- What evidence would resolve it: Systematic ablation studies comparing models with identical training data but different architectural features could identify which architectural choices promote better geographic generalization.

## Limitations

- The study relies on class mappings between DollarStreet/GeoDE and ImageNet-1K, which may introduce approximation errors in geographic disparity measurements
- The geographic disparity metric (maximum accuracy difference across regions) simplifies complex regional variations into a single value
- The analysis focuses on pretrained models without investigating how training data composition specifically contributes to geographic performance gaps

## Confidence

- High confidence: The finding that progress on standard benchmarks exceeds progress on geographic benchmarks (Progress Gap up to 2.5x) - supported by systematic evaluation of nearly 100 models across multiple architectures
- Medium confidence: The claim that geographic disparities more than tripled - based on the disparity metric which may oversimplify regional variations
- Medium confidence: That last-layer retraining significantly reduces geographic disparities - demonstrated empirically but may not generalize to all model architectures

## Next Checks

1. Validate class mapping accuracy between DollarStreet/GeoDE and ImageNet-1K by manually checking a sample of mapped classes and their geographic distributions
2. Replicate the Progress Gap analysis using alternative geographic datasets to ensure findings are not dataset-specific
3. Test last-layer retraining across different architectures (beyond CLIP) to verify the approach generalizes to other vision models