---
ver: rpa2
title: 'Conformal Prediction in Multi-User Settings: An Evaluation'
arxiv_id: '2312.05195'
source_url: https://arxiv.org/abs/2312.05195
tags:
- prediction
- data
- conformal
- sets
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates conformal prediction in multi-user machine\
  \ learning settings where traditional methods overestimate performance due to inter/intra\
  \ user variance. The authors compare four evaluation strategies\u2014mixed, user-independent,\
  \ user-dependent, and user-calibrated models\u2014using conformal and traditional\
  \ metrics on four real-world datasets (WISDM, HAR70+, Opportunity, Smartwatch Gestures)."
---

# Conformal Prediction in Multi-User Settings: An Evaluation

## Quick Facts
- arXiv ID: 2312.05195
- Source URL: https://arxiv.org/abs/2312.05195
- Reference count: 8
- One-line primary result: User-calibrated conformal models outperform user-independent models despite using less calibration data

## Executive Summary
This paper evaluates conformal prediction methods in multi-user machine learning settings where traditional approaches overestimate performance due to inter- and intra-user variance. The authors compare four evaluation strategies—mixed, user-independent, user-dependent, and user-calibrated models—using both conformal and traditional metrics across four real-world datasets. Random Forest achieved the smallest prediction sets and highest performance overall. User-calibrated models, trained on general data but calibrated on target user data, demonstrated superior performance compared to user-independent models while using less calibration data, highlighting the importance of user-specific calibration. Visualizations including co-occurrence matrices, graphs, and multiset-enumerating charts were introduced to better analyze prediction set patterns.

## Method Summary
The paper evaluates four conformal prediction evaluation strategies on four real-world datasets (WISDM, HAR70+, Opportunity, Smartwatch Gestures) using Random Forest, Naive Bayes, SVM, and KNN classifiers. Data splits vary by strategy: mixed and user-dependent use 60% training, 20% calibration, 20% test; user-independent uses 60% training, 40% calibration; user-calibrated uses 60% training, 50% calibration, 50% test. The MAPIE library implements conformal prediction with α=0.95. Both traditional metrics (accuracy, sensitivity, specificity, F1) and conformal metrics (coverage, set size, Jaccard index) are calculated, with visualizations (co-occurrence matrices, graphs, confusion matrices, multiset-enumerating charts) generated to analyze prediction sets.

## Key Results
- Random Forest achieved the smallest prediction sets and highest performance across all evaluation strategies
- User-calibrated models outperformed user-independent models despite using less calibration data
- All hypotheses about performance differences were rejected except one, confirming significant differences across model types
- Conformal prediction sets provide more reliable uncertainty estimates than traditional metrics in multi-user settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction calibrates uncertainty estimates by wrapping any classifier with a marginal coverage guarantee.
- Mechanism: Uses a non-conformity score (e.g., 1 minus the score of the true label) to construct prediction sets such that the true label is contained with probability ≥ 1-ϵ.
- Core assumption: Exchangeability of the calibration set with the test set.
- Evidence anchors:
  - [abstract]: "Conformal prediction is a model agnostic method that provides confidence guarantees on the predictions"
  - [section]: "Conformal prediction is a general framework... that extends the capabilities of any classifier or regression model such that the uncertainty of the predictions can be quantified"
  - [corpus]: Weak. Corpus neighbors discuss robustness and dynamic settings, but none directly validate the coverage guarantee claim for multi-user settings.
- Break condition: If the exchangeability assumption is violated (e.g., in user-independent models where train and test users differ).

### Mechanism 2
- Claim: User-calibrated models outperform user-independent models despite less calibration data.
- Mechanism: Calibrating the conformal model with data from the target user improves the alignment of the non-conformity distribution, satisfying the coverage guarantee.
- Core assumption: Even small amounts of target user calibration data are enough to capture the relevant distribution shift.
- Evidence anchors:
  - [abstract]: "User-calibrated models, trained on general data but calibrated on target user data, outperformed user-independent models despite using less calibration data"
  - [section]: "The UCM is trained the same way as the User-independent model but it is calibrated only with data from the target user"
  - [corpus]: Weak. No direct corpus evidence; neighbors discuss calibration broadly but not in multi-user settings.
- Break condition: If the target user calibration data is too sparse or not representative.

### Mechanism 3
- Claim: Visualization methods (co-occurrence matrices, graphs, multiset-enumerating charts) reveal prediction set patterns that traditional confusion matrices obscure.
- Mechanism: These visualizations explicitly enumerate prediction sets and highlight class co-occurrences, making it easier to interpret multi-label uncertainty.
- Core assumption: Prediction sets are informative about classifier behavior and can be meaningfully visualized.
- Evidence anchors:
  - [section]: "We propose the use of matrix and graph based visualizations to analyze conformal prediction sets and compare the results between traditional and conformal models"
  - [section]: "We introduce the multiset-enumerating chart... which enumerates explicitly the powerset of the set of classes"
  - [corpus]: Weak. No corpus evidence; neighbors focus on conformal prediction theory, not visualization.
- Break condition: If prediction sets are too large or too sparse to visualize meaningfully.

## Foundational Learning

- Exchangeability assumption
  - Why needed here: Critical for validity of conformal coverage guarantees; violated in user-independent settings.
  - Quick check question: What happens to coverage if the calibration set is not exchangeable with the test set?
- Non-conformity scoring
  - Why needed here: Determines prediction set composition; affects set size and coverage.
  - Quick check question: How does changing the non-conformity score (e.g., using margin vs. softmax) affect the resulting prediction sets?
- Multi-user evaluation strategies
  - Why needed here: Determines how data splits affect performance estimates; relevant for interpreting conformal results.
  - Quick check question: Why does a mixed model overestimate performance compared to a user-independent model?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (MM, UDM, UIM, UCM) -> Conformal calibration -> Performance evaluation -> Visualization
- Critical path: Split data according to evaluation strategy -> Train base classifier -> Calibrate conformal model -> Evaluate coverage and set size -> Generate visualizations
- Design tradeoffs: More calibration data improves coverage but may violate exchangeability; smaller sets preferred but may reduce coverage.
- Failure signatures: Coverage < 1-ϵ: exchangeability assumption violated or miscalibration; large set sizes: poor non-conformity scores or model uncertainty.
- First 3 experiments:
  1. Run MM and UIM on WISDM; compare coverage and set sizes.
  2. Implement UCM; verify it achieves coverage ≥ 0.95 with less calibration data.
  3. Generate co-occurrence matrix and multiset chart for one dataset; interpret patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed user-calibrated model (UCM) compare to other user-adaptive models (UAM) in terms of performance and efficiency?
- Basis in paper: [explicit] The paper mentions user-adaptive models (UAM) as a potential future work, noting that UAMs try to achieve better performance than general models while minimizing the training data from the target user.
- Why unresolved: The paper only introduces the UCM as a new evaluation strategy and does not compare it to UAMs.
- What evidence would resolve it: Experimental results comparing the UCM to various UAMs on the same datasets and evaluation metrics.

### Open Question 2
- Question: How do the proposed visualization techniques (co-occurrence matrix, co-occurrence graph, zero diagonal confusion matrix, and multiset-enumerating chart) perform in terms of interpretability and effectiveness in conveying information about prediction sets compared to existing visualization methods?
- Basis in paper: [explicit] The paper introduces these visualization techniques as a way to better understand the results of conformal prediction sets, but does not compare their effectiveness to existing methods.
- Why unresolved: The paper does not provide a comparative analysis of the proposed visualization techniques with other methods.
- What evidence would resolve it: User studies or quantitative measures assessing the interpretability and effectiveness of the proposed visualization techniques compared to existing methods.

### Open Question 3
- Question: How does the choice of non-conformity measure (e.g., LAC method) affect the performance of conformal prediction models in multi-user settings?
- Basis in paper: [explicit] The paper uses the LAC method as the non-conformity function, but does not explore the impact of different non-conformity measures on model performance.
- Why unresolved: The paper only experiments with one non-conformity measure, limiting the understanding of its impact on model performance.
- What evidence would resolve it: Experimental results comparing the performance of conformal prediction models using different non-conformity measures on the same datasets and evaluation metrics.

## Limitations

- The analysis is based on only four datasets and four classifier types, which may limit generalizability to other domains.
- The paper assumes exchangeability within evaluation strategies, but this is explicitly violated in user-independent models, potentially explaining their poor performance.
- Computational complexity differences between evaluation strategies are not addressed, which could be significant in practical applications.

## Confidence

- High confidence: The mechanism of conformal prediction providing marginal coverage guarantees is well-established in the literature and the paper correctly implements this framework.
- Medium confidence: The claim that user-calibrated models outperform user-independent models despite less calibration data is supported by the experimental results, but the underlying mechanism (why small amounts of target user data are sufficient) could benefit from deeper analysis.
- Medium confidence: The visualization methods are described clearly and shown to reveal patterns, but their practical utility for model selection or improvement is not demonstrated.

## Next Checks

1. Test the evaluation strategies on additional datasets with different characteristics (e.g., more users, different feature types) to assess generalizability.
2. Implement a stratified sampling approach within the user-independent strategy to maintain class distribution across users and verify if this improves coverage.
3. Conduct a computational complexity analysis comparing the four evaluation strategies to identify potential scalability issues in larger applications.