---
ver: rpa2
title: Bridging the Sim-to-Real Gap from the Information Bottleneck Perspective
arxiv_id: '2305.18464'
source_url: https://arxiv.org/abs/2305.18464
tags:
- privileged
- policy
- learning
- knowledge
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sim-to-real gap in reinforcement learning
  by proposing a novel privileged knowledge distillation method called Historical
  Information Bottleneck (HIB). HIB learns a privileged knowledge representation from
  historical trajectories by capturing the underlying changeable dynamic information.
---

# Bridging the Sim-to-Real Gap from the Information Bottleneck Perspective

## Quick Facts
- arXiv ID: 2305.18464
- Source URL: https://arxiv.org/abs/2305.18464
- Reference count: 40
- Key outcome: HIB learns privileged knowledge from historical trajectories, reducing value discrepancy between oracle and learned policies while improving out-of-distribution generalization

## Executive Summary
This paper addresses the sim-to-real gap in reinforcement learning by proposing Historical Information Bottleneck (HIB), a privileged knowledge distillation method that learns representations from historical trajectories. The method captures changeable dynamic information through contrastive learning while applying information bottleneck regularization to improve generalization. HIB operates as a single-stage learning paradigm that outperforms two-stage policy distillation methods in sample efficiency and performance. Empirical results on simulated and real-world tasks demonstrate improved generalizability, particularly in out-of-distribution test environments.

## Method Summary
HIB is a privileged knowledge distillation method that learns representations from historical trajectories to bridge the sim-to-real gap. The method uses a two-stream architecture with online and target networks, processing historical data through a temporal convolutional network (TCN) encoder. Contrastive learning maximizes mutual information between historical representations and privileged states while information bottleneck regularization compresses irrelevant details. The method operates as a single-stage learning paradigm, eliminating the need for separate oracle policy training and student imitation phases typical in two-stage approaches.

## Key Results
- HIB outperforms previous methods in out-of-distribution test environments
- Single-stage HIB achieves better sample efficiency than two-stage policy distillation
- Reduces value discrepancy between oracle and learned policies through privileged knowledge representation
- Demonstrates improved generalizability across both simulated and real-world tasks

## Why This Works (Mechanism)

### Mechanism 1
Learning privileged state representation from history reduces value discrepancy between oracle and learned policies. HIB uses contrastive learning to maximize mutual information between historical representation and privileged state while compressing irrelevant history details. Core assumption: Historical trajectories contain sufficient information to reconstruct privileged states.

### Mechanism 2
Single-stage privilege distillation outperforms two-stage policy imitation in sample efficiency and performance. By learning privileged knowledge representation directly during RL training, HIB eliminates separate oracle policy training and student imitation phases. Core assumption: Joint optimization of representation learning and policy learning is more efficient than sequential training.

### Mechanism 3
Information bottleneck regularization improves generalization to out-of-distribution environments. HIB restricts representation complexity by minimizing I(Ht; Zt), removing decision-irrelevant information while preserving privileged knowledge. Core assumption: Compressing irrelevant history information makes representation more robust to distribution shifts.

## Foundational Learning

- Concept: Information Bottleneck (IB) principle
  - Why needed here: Provides theoretical foundation for learning representations that retain relevant information while compressing irrelevant details
  - Quick check question: What are the two mutual information terms being balanced in the IB objective, and what does each represent?

- Concept: Mutual Information (MI) estimation via contrastive learning
  - Why needed here: Enables practical approximation of MI between high-dimensional historical states and privileged information without explicit density estimation
  - Quick check question: How does the contrastive objective in HIB approximate the mutual information term I(Zt; Sp_t) without negative sampling?

- Concept: Partially Observable MDP (POMDP) relationship
  - Why needed here: Helps understand why historical information can substitute for missing privileged states in policy learning
  - Quick check question: How does the sim-to-real setting differ from POMDP in terms of privileged information availability during training?

## Architecture Onboarding

- Component map: History → TCN encoder → Contrastive loss → Representation update → Actor-Critic update → RL policy improvement
- Critical path: History → TCN encoder → Contrastive loss → Representation update → Actor-Critic update → RL policy improvement
- Design tradeoffs:
  - History length vs. computational cost: Longer histories provide more information but increase model complexity
  - IB regularization strength vs. representation expressiveness: Higher α improves generalization but may limit capacity
  - Contrastive vs. predictive objectives: Contrastive provides better representation quality but requires careful negative sampling
- Failure signatures:
  - Collapsed representations: Loss plateaus with minimal gradients across all components
  - Poor generalization: Performance matches baseline DR in out-of-distribution tests
  - Unstable training: High variance in returns across seeds or training runs
- First 3 experiments:
  1. Pendulum task with varying history lengths (k=10, 20, 50) to assess information sufficiency
  2. Ablation study removing IB regularization (α=0) to quantify its impact on generalization
  3. Comparison of cosine similarity vs. contrastive loss with negative sampling on quadruped walk task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of privileged knowledge representation affect the performance of HIB?
- Basis in paper: [inferred] The paper discusses the importance of privileged knowledge and how HIB learns a representation of it, but does not explore the impact of different representation choices.
- Why unresolved: The paper focuses on the effectiveness of HIB as a method, but does not investigate the sensitivity of the method to the choice of privileged knowledge representation.
- What evidence would resolve it: Experiments comparing the performance of HIB with different privileged knowledge representations, such as raw privileged state, predicted privileged state, or learned latent representation.

### Open Question 2
- Question: How does the choice of history length affect the performance of HIB?
- Basis in paper: [inferred] The paper mentions the use of a fixed-length history in the HIB method, but does not explore the impact of different history lengths.
- Why unresolved: The paper focuses on the effectiveness of HIB as a method, but does not investigate the sensitivity of the method to the choice of history length.
- What evidence would resolve it: Experiments comparing the performance of HIB with different history lengths, such as 10, 50, or 100 timesteps.

### Open Question 3
- Question: How does the choice of IB parameter alpha affect the performance of HIB?
- Basis in paper: [inferred] The paper mentions the use of the IB principle in HIB, but does not explore the impact of different IB parameter values.
- Why unresolved: The paper focuses on the effectiveness of HIB as a method, but does not investigate the sensitivity of the method to the choice of IB parameter.
- What evidence would resolve it: Experiments comparing the performance of HIB with different IB parameter values, such as 0.1, 1.0, or 10.0.

## Limitations
- Theoretical guarantees are conditional on privileged state being sufficiently predictable from history
- Ablation depth is limited - systematic analysis of hyperparameter sensitivity is lacking
- Generalizability claims need broader validation across different types of distribution shifts

## Confidence
- **High confidence**: Core mechanism of learning privileged representations from history to improve sim-to-real transfer
- **Medium confidence**: Claim that HIB outperforms two-stage methods in sample efficiency
- **Low confidence**: Assertion that HIB handles arbitrary out-of-distribution environments

## Next Checks
1. **Distribution shift sensitivity analysis**: Systematically vary the amount of distribution shift between training and test environments (e.g., terrain roughness, object masses) and measure HIB performance degradation vs. baseline DR
2. **History sufficiency study**: Evaluate performance across a range of history lengths (k=10, 20, 50, 100) to identify minimum information needed and test break conditions
3. **Two-stage vs. single-stage cost comparison**: Implement specific two-stage baseline and measure both sample efficiency and wall-clock training time to validate computational advantage claim