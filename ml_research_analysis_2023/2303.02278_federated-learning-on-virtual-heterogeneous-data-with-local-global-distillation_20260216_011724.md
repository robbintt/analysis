---
ver: rpa2
title: Federated Learning on Virtual Heterogeneous Data with Local-global Distillation
arxiv_id: '2303.02278'
source_url: https://arxiv.org/abs/2303.02278
tags:
- data
- virtual
- local
- global
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedLGD, a federated learning method that trains
  models on synthetic datasets generated via local and global distillation to address
  data heterogeneity. The key idea is to use iterative distribution matching for efficient
  local distillation and federated gradient matching to generate global virtual data
  as anchors for regularization.
---

# Federated Learning on Virtual Heterogeneous Data with Local-global Distillation

## Quick Facts
- arXiv ID: 2303.02278
- Source URL: https://arxiv.org/abs/2303.02278
- Authors: [Authors not provided in input]
- Reference count: 40
- Key outcome: FedLGD achieves 87.0% average accuracy on DIGITS with 50 images per class, outperforming the best baseline by 4.3%, and improves average accuracy by 2.1% and 7.2% over the best baseline on RETINA for ConvNet and ResNet18 respectively.

## Executive Summary
This paper introduces FedLGD, a federated learning method that addresses data heterogeneity by training models on synthetic datasets generated through local and global distillation. The approach uses iterative distribution matching for efficient local distillation and federated gradient matching to create global virtual data that serves as anchors for regularization. Experiments on DIGITS and RETINA datasets demonstrate that FedLGD outperforms state-of-the-art heterogeneous FL algorithms, particularly in mitigating domain shift among clients and defending against membership inference attacks.

## Method Summary
FedLGD proposes a novel approach to federated learning that addresses data heterogeneity by creating and training on synthetic datasets. The method consists of two main components: local distillation through iterative distribution matching and global distillation through federated gradient matching. Each client generates local virtual data by iteratively matching the distribution of their private data, while the server aggregates gradients from all clients to distill global virtual data. These global virtual data serve as anchor points to regularize local training, helping to mitigate domain shifts across heterogeneous client data. The method initializes distilled data using class-wise statistics to balance privacy preservation and distillation performance.

## Key Results
- On DIGITS dataset with 50 images per class, FedLGD achieves 87.0% average accuracy across clients, outperforming the best baseline (VHL) by 4.3%.
- On RETINA dataset, FedLGD improves average accuracy by 2.1% and 7.2% over the best baseline for ConvNet and ResNet18 architectures respectively.
- FedLGD demonstrates effectiveness in mitigating data heterogeneity and defending against membership inference attacks.
- The method shows consistent performance improvements across varying numbers of images per class (10, 50, and 100) on the DIGITS dataset.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative distribution matching using an evolving feature extractor improves local virtual data quality compared to using randomly initialized networks.
- Mechanism: By updating the feature extractor with the latest global model parameters during FL training, the synthetic data are distilled to better match the task-specific feature distribution of the private data.
- Core assumption: The empirical neural tangent kernel approximation holds, so updating the feature extractor during distillation improves the fidelity of the synthetic data for the classification task.
- Evidence anchors: [abstract] "We apply DM (Zhao & Bilen, 2023) to the baseline FL methods and demonstrate the effectiveness of our proposed iterative strategy in Sec. 4." [section 3.3.1] "Although such an efficient distillation strategy is inspired by DM (Zhao & Bilen, 2023), we highlight the following two key differences between our iterative distribution matching with theirs. First, DM uses randomly initialized deep neural networks to extract features, whereas we use trained FL models with task-specific supervised loss."
- Break condition: If the feature extractor diverges or the MMD loss fails to decrease over iterations, the synthetic data quality may not improve and could even degrade.

### Mechanism 2
- Claim: Global virtual data distilled via federated gradient matching acts as an anchor to regularize local training and mitigate domain shift among clients.
- Mechanism: The server aggregates gradients from all clients w.r.t. their local virtual data and uses this averaged gradient to update the global virtual data, which is then shared with clients. Clients use this global virtual data in a supervised contrastive loss to align their local features with the global feature space.
- Core assumption: The averaged gradient w.r.t. local virtual data is representative enough of the global data distribution to guide the distillation of useful global virtual data.
- Evidence anchors: [abstract] "We use federated gradient matching to distill global virtual data that serve as anchor points to rectify heterogeneous local training, without compromising data privacy." [section 3.3.2] "We propose to leverage local clients' averaged gradients to distill global virtual data. In addition, the distilled global virtual data could be used in Eq. (4)."
- Break condition: If the local virtual data are too dissimilar or the number of clients is too small, the averaged gradient may not represent the global distribution, leading to poor global virtual data quality.

### Mechanism 3
- Claim: Initializing distilled data using class-wise statistics (mean and std) from each client balances privacy preservation and distillation performance.
- Mechanism: Instead of sampling real images or using random Gaussian noise, each client computes per-class mean and standard deviation of their data and shares only these statistics with the server. The server aggregates these to initialize global virtual data, and clients use their own statistics to initialize local virtual data.
- Core assumption: Class-wise statistics preserve enough information to generate useful synthetic data without exposing individual samples.
- Evidence anchors: [section 3.3.3] "In this study, we propose to initialize the distilled data using statistics from the data set per class to take care of both privacy concerns and model performance. Specifically, each client calculates the statistics of its own data for each class, denoted as μc_i, σc_i, and then initializes the distillation images per class, x∼N(μc_i, σc_i), where c and i represent each client and categorical label."
- Break condition: If the class distributions are highly imbalanced or the statistics do not capture the data distribution well, the initialized synthetic data may be of low quality.

## Foundational Learning

- Concept: Federated Learning (FL) and its challenges with data heterogeneity.
  - Why needed here: Understanding FL is crucial because the paper proposes a method specifically to address heterogeneity in FL settings.
  - Quick check question: What is the main challenge that data heterogeneity poses in federated learning, and how does it affect model convergence?

- Concept: Dataset Distillation and its role in improving FL efficiency.
  - Why needed here: The paper uses dataset distillation to create smaller synthetic datasets for each client, which is central to the proposed method.
  - Quick check question: How does dataset distillation help in addressing the synchronization and efficiency issues in federated learning?

- Concept: Distribution Matching and Maximum Mean Discrepancy (MMD) for synthetic data generation.
  - Why needed here: The iterative distribution matching method uses MMD to measure and minimize the difference between real and synthetic data distributions.
  - Quick check question: What is the purpose of using MMD in the iterative distribution matching process, and how does it contribute to generating better synthetic data?

## Architecture Onboarding

- Component map: Server -> Global Virtual Data -> Clients -> Local Virtual Data -> Server (gradients)
- Critical path:
  1. Initialize global and local virtual data using class-wise statistics.
  2. Perform iterative distribution matching to refine local virtual data.
  3. Perform federated gradient matching to refine global virtual data.
  4. Clients train local models using local virtual data and regularize with global virtual data.
  5. Server aggregates client updates and shares updated global virtual data.

- Design tradeoffs:
  - Using iterative distribution matching vs. one-shot methods: iterative methods may yield better synthetic data but require more computation.
  - Sharing gradients w.r.t. virtual data vs. sharing model parameters: sharing gradients is more private but may provide less information.
  - Updating global virtual data vs. keeping it static: updating can better align with evolving local distributions but adds complexity.

- Failure signatures:
  - Poor convergence: local and global MMD losses not decreasing over iterations.
  - Degradation in test accuracy: models trained on synthetic data perform worse than those trained on real data.
  - Privacy leakage: membership inference attacks succeed on models trained with synthetic data.

- First 3 experiments:
  1. Benchmark on DIGITS dataset with varying IPC and architectures to compare with state-of-the-art methods.
  2. Ablation study on the choice of regularization loss (MMD vs. contrastive) and its coefficient.
  3. Test on RETINA dataset to evaluate performance on real-world medical data with limited samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of global virtual data generated by FedLGD compare to that of real data when used for training and regularization?
- Basis in paper: [explicit] The paper mentions that training with global virtual data only can achieve comparable results to training with real data.
- Why unresolved: The paper does not provide a direct comparison of the performance of models trained with global virtual data versus those trained with real data.
- What evidence would resolve it: A direct comparison of the performance of models trained with global virtual data and those trained with real data on the same tasks and datasets.

### Open Question 2
- Question: How does the performance of FedLGD vary with different local update frequencies?
- Basis in paper: [inferred] The paper mentions that aggregating at different frequencies is an important factor that affects FL behavior, and shows that increasing local epochs can degrade FedLGD's performance.
- Why unresolved: The paper does not provide a comprehensive study of the impact of local update frequencies on FedLGD's performance.
- What evidence would resolve it: A detailed study of the impact of different local update frequencies on FedLGD's performance across various datasets and tasks.

### Open Question 3
- Question: How does FedLGD's privacy preservation compare to other FL methods under various attack models?
- Basis in paper: [explicit] The paper mentions that FedLGD potentially defends against inference attacks better than FedAvg, and shows that models trained with synthetic data are harder to attack via membership inference.
- Why unresolved: The paper does not provide a comprehensive comparison of FedLGD's privacy preservation under various attack models compared to other FL methods.
- What evidence would resolve it: A comprehensive comparison of FedLGD's privacy preservation under various attack models compared to other FL methods on the same datasets and tasks.

## Limitations
- The method's performance on real-world datasets (RETINA) shows improvement, but the DIGITS results with only 50 images per class may not generalize to more complex tasks.
- The privacy guarantees through gradient sharing rather than parameter sharing are promising but not fully validated against advanced inference attacks.
- Claims about effectiveness rely heavily on synthetic data quality and the assumption that iterative distribution matching with evolving feature extractors improves distillation.

## Confidence

- High confidence: Local distillation via iterative distribution matching improves synthetic data quality when using task-specific feature extractors.
- Medium confidence: Global virtual data generated through federated gradient matching effectively regularizes local training and mitigates domain shift.
- Low confidence: The class-wise statistics initialization approach sufficiently balances privacy and distillation performance across diverse data distributions.

## Next Checks

1. Test FedLGD on larger-scale heterogeneous datasets (e.g., CIFAR-100 with non-IID splits) to evaluate scalability beyond the DIGITS benchmark.
2. Conduct rigorous privacy analysis including membership inference attacks on both synthetic and real data to verify the claimed privacy benefits of gradient-based sharing.
3. Compare FedLGD against state-of-the-art personalized FL methods that don't rely on synthetic data to isolate the contribution of the distillation approach versus personalization strategies.