---
ver: rpa2
title: 'Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning'
arxiv_id: '2306.08590'
source_url: https://arxiv.org/abs/2306.08590
tags:
- learning
- noise
- rate
- batch
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the prevailing view that stochastic gradient
  descent (SGD) noise improves implicit bias in deep learning. While prior work focused
  on offline learning (multiple epochs), the authors investigate online learning (single
  epoch).
---

# Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning

## Quick Facts
- arXiv ID: 2306.08590
- Source URL: https://arxiv.org/abs/2306.08590
- Authors: 
- Reference count: 40
- Primary result: In online learning, SGD noise does not provide implicit bias advantages; instead, it only affects traversal speed and stability of the optimal gradient flow path.

## Executive Summary
This paper challenges the widely-held belief that SGD noise improves implicit bias in deep learning. Through extensive experiments on CIFAR-5m, ImageNet, and C4 datasets, the authors demonstrate that in online learning (single epoch), smaller batch sizes and larger learning rates do not provide implicit bias advantages. Instead, lower noise consistently yields better performance after accounting for computational efficiency. The authors propose the "golden path" hypothesis: SGD noise in online learning only affects the speed and stability of traversing the optimal trajectory determined by gradient flow, rather than the path itself. This finding has significant implications for hyperparameter selection in online learning settings, suggesting practitioners should prioritize computational efficiency over implicit bias considerations.

## Method Summary
The paper investigates the impact of SGD noise (learning rate and batch size) on online learning performance by conducting controlled experiments across three datasets (CIFAR-5m, ImageNet, C4) with different model architectures (ResNet-18, ConvNext-T, GPT-2-small). The authors systematically vary batch sizes and learning rates in both online (single epoch) and offline (multiple epochs) settings, measuring test error and loss as functions of gradient steps. They conduct "golden path" hypothesis experiments by training with high noise parameters for T0 steps, then reducing noise and observing whether loss curves "snap" to match low-noise trajectories. The functional distance between models is measured using average total variation distance between softmax probabilities on test data.

## Key Results
- In online learning, lower SGD noise consistently yields better performance after accounting for computational efficiency
- Loss curves "snap" to match low-noise trajectories when SGD noise is reduced mid-training
- Functional distances between high-noise and low-noise models become indistinguishable after noise reduction
- SGD noise in online learning only affects traversal speed and stability rather than the optimization path itself

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SGD noise in online learning does not provide implicit bias advantages; instead, it only affects the speed and stability of traversing the optimal trajectory determined by gradient flow.
- **Mechanism:** In online learning, the noise introduced by finite batch sizes and non-negligible learning rates creates a stochastic approximation of the true gradient. This noise causes the optimization path to deviate from the deterministic gradient flow path, but does not fundamentally change the path itself. The "golden path" hypothesis suggests that SGD noise merely adds variance to the steps along this optimal trajectory, rather than steering optimization toward different minima.
- **Core assumption:** The loss landscape in online learning has a single optimal trajectory that both gradient flow and SGD follow, with SGD introducing noise but not fundamentally altering the path.
- **Evidence anchors:**
  - [abstract] "SGD noise in online learning only affects the speed and stability of traversing the optimal trajectory determined by gradient flow, rather than the path itself."
  - [section] "Our findings hint that the SGD path is just a noisy version of the underlying noiseless GF path as illustrated in Figure 2."
  - [corpus] Weak evidence - related papers focus on DP-SGD and batch size effects but don't directly address the golden path hypothesis in online learning.

### Mechanism 2
- **Claim:** When SGD noise is reduced mid-training, the loss curve "snaps" to match the trajectory of a model trained with low noise from initialization.
- **Mechanism:** The optimization process follows a deterministic path determined by the gradient flow dynamics. When noise is introduced, it causes temporary deviations from this path, increasing the loss. When noise is reduced, the optimization process quickly returns to the original path, causing the loss to decrease and align with the trajectory of a model that followed the low-noise path from the beginning.
- **Core assumption:** The optimization trajectory is primarily determined by the gradient flow dynamics, and noise only causes temporary deviations that can be corrected when noise is reduced.
- **Evidence anchors:**
  - [abstract] "SGD noise in online learning only affects the speed and stability of traversing the optimal trajectory determined by gradient flow, rather than the path itself."
  - [section] "After T0 steps, decrease the SGD noise by increasing the batch size of the second experiment to match the hyperparameters of the first one, and continue both runs. Under the golden path hypothesis, we expect that shortly after increasing the batch size (i.e., at T0 + τ for τ ≪ T0), the loss curve would 'snap' to the golden path, and continue following the same trajectory of the model that was trained with low SGD noise."
  - [corpus] Weak evidence - related papers discuss batch size effects but don't specifically address the snapping phenomenon.

### Mechanism 3
- **Claim:** The functional distance between models trained with different SGD noise levels becomes indistinguishable after noise reduction.
- **Mechanism:** Models trained with different noise levels explore different regions of the function space due to the stochastic nature of the optimization process. However, when noise is reduced, both models converge to similar functional representations, as they are both following the same underlying gradient flow path. This results in the functional distance between the models becoming comparable to the distance between two independent runs with low noise.
- **Core assumption:** The functional representation learned by the model is primarily determined by the gradient flow dynamics, and noise only causes temporary deviations in the parameter space that do not significantly affect the learned function.
- **Evidence anchors:**
  - [abstract] "SGD noise in online learning only affects the speed and stability of traversing the optimal trajectory determined by gradient flow, rather than the path itself."
  - [section] "A more refined notion of distance than the loss is to consider the functional distance of models (i.e., their pointwise behavior on test data points). The golden path hypothesis posits that once we reduce the amount of noise, the resulting functions will be similar."
  - [corpus] Weak evidence - related papers discuss functional similarity but don't specifically address the effect of noise reduction on functional distance.

## Foundational Learning

- **Concept:** Implicit bias in optimization
  - Why needed here: Understanding how optimization algorithms with different hyperparameters can lead to different solutions, even when minimizing the same loss function, is crucial for interpreting the results of this paper.
  - Quick check question: What is the difference between explicit and implicit bias in optimization, and how do they affect the final solution?

- **Concept:** Gradient flow and its relationship to SGD
  - Why needed here: The paper proposes that SGD in online learning follows a "golden path" determined by gradient flow dynamics. Understanding gradient flow and how it relates to SGD is essential for interpreting this hypothesis.
  - Quick check question: How does the continuous-time gradient flow algorithm relate to the discrete-time SGD algorithm, and what are the key differences between them?

- **Concept:** Online vs. offline learning regimes
  - Why needed here: The paper distinguishes between online and offline learning and shows that the effect of SGD noise differs between these two regimes. Understanding the differences between these learning paradigms is crucial for interpreting the results.
  - Quick check question: What are the key differences between online and offline learning, and how do these differences affect the optimization process and the final solution?

## Architecture Onboarding

- **Component map:** CIFAR-5m dataset -> ResNet-18 model -> SGD with momentum optimizer -> Test error measurement; ImageNet dataset -> ConvNext-T model -> SGD with momentum optimizer -> Test error measurement; C4 dataset -> GPT-2-small model -> AdamW optimizer -> Test error measurement

- **Critical path:**
  1. Train models with different SGD noise levels (varying learning rate and batch size) in both online and offline settings
  2. Measure the performance (loss and test error) as a function of the number of gradient steps or LR-normalized steps
  3. Conduct experiments where SGD noise is reduced mid-training to observe the "snapping" phenomenon
  4. Measure the functional distance between models trained with different noise levels before and after noise reduction

- **Design tradeoffs:**
  - Computational efficiency vs. implicit bias: The paper shows that in online learning, SGD noise only affects computational efficiency and not implicit bias. This suggests that practitioners should prioritize computational efficiency when choosing hyperparameters in online settings.
  - Model capacity vs. data size: The experiments use relatively large models (ResNet-18, ConvNext-T, GPT-2-small) and datasets (CIFAR-5m, ImageNet, C4). The results may not generalize to smaller models or datasets.

- **Failure signatures:**
  - If the loss curves do not "snap" back to the low-noise trajectory when noise is reduced mid-training, it would suggest that the golden path hypothesis is incorrect.
  - If the functional distance between models trained with different noise levels remains distinguishable even after noise reduction, it would suggest that noise introduces permanent structural changes to the learned function.

- **First 3 experiments:**
  1. Train ResNet-18 on CIFAR-5m with different batch sizes (e.g., 32, 128, 512) and learning rates (e.g., 0.01, 0.1, 0.2) in the online setting, measuring test error vs training steps
  2. Conduct the mid-training noise reduction experiment on CIFAR-5m by training two models with different batch sizes, then increasing the batch size of the smaller one to match the larger one mid-training, and observing the loss curve behavior
  3. Measure the functional distance between models trained with different noise levels on CIFAR-5m before and after noise reduction, comparing it to the distance between two independent low-noise models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "golden path" hypothesis extend to other optimization algorithms beyond gradient flow and SGD?
- Basis in paper: [explicit] The paper proposes the "golden path" hypothesis specifically for SGD and gradient flow in online learning, but doesn't explore other optimizers.
- Why unresolved: The paper only empirically tests the hypothesis for SGD and gradient flow, leaving open whether similar behavior exists for other optimizers like Adam or RMSprop.
- What evidence would resolve it: Comparative experiments testing whether other optimizers (Adam, RMSprop, etc.) also follow noisy versions of a "golden path" in online learning, or if they exhibit fundamentally different behavior.

### Open Question 2
- Question: How does the "golden path" hypothesis behave in the presence of different data distributions or domain shifts during online learning?
- Basis in paper: [inferred] The paper studies online learning with static data distributions (CIFAR-5m, ImageNet, C4) but doesn't address scenarios with changing data distributions.
- Why unresolved: The experiments use fixed datasets without any concept drift or distribution shifts, which are common in real-world online learning scenarios.
- What evidence would resolve it: Experiments showing how the "golden path" behavior changes when data distributions shift during training, or whether SGD continues to follow a noisy version of gradient flow under concept drift.

### Open Question 3
- Question: What is the theoretical mechanism behind why SGD noise in online learning only affects traversal speed and stability rather than the path itself?
- Basis in paper: [explicit] The paper observes empirically that SGD noise affects only speed and stability in online learning, but doesn't provide a theoretical explanation for this phenomenon.
- Why unresolved: While the paper provides strong empirical evidence for the "golden path" hypothesis, it doesn't explain why the implicit bias effects that appear in offline learning disappear in online learning.
- What evidence would resolve it: Theoretical analysis connecting the dynamics of online learning to the properties of gradient flow, potentially using techniques from stochastic approximation theory or dynamical systems analysis.

## Limitations

- The study focuses on online learning (single epoch), leaving the relationship between SGD noise and implicit bias in offline learning (multiple epochs) largely unexplored
- Experiments use relatively large batch sizes and learning rates, which may not capture the full spectrum of hyperparameter space
- The underlying mechanism of the "golden path" hypothesis remains somewhat speculative without rigorous theoretical analysis

## Confidence

- **High confidence**: The experimental evidence showing that in online learning, lower SGD noise consistently yields better performance after accounting for computational efficiency
- **Medium confidence**: The "golden path" hypothesis that SGD noise only affects speed and stability rather than the optimization path itself
- **Medium confidence**: The claim that implicit bias advantages from SGD noise are negligible in online learning settings

## Next Checks

1. **Expand hyperparameter range**: Test smaller batch sizes (8, 16) and learning rates (0.001, 0.01) to determine if the golden path hypothesis holds across the full optimization landscape, particularly for settings where noise might have stronger effects.

2. **Offline learning comparison**: Conduct parallel experiments in offline learning (multiple epochs) using the same models and datasets to quantify how the relationship between SGD noise and implicit bias differs between learning regimes.

3. **Theoretical analysis**: Develop rigorous mathematical analysis connecting the discrete SGD dynamics to continuous gradient flow in online settings, formally proving or disproving the conditions under which the golden path hypothesis holds.