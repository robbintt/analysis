---
ver: rpa2
title: Precursor-of-Anomaly Detection for Irregular Time Series
arxiv_id: '2306.15489'
source_url: https://arxiv.org/abs/2306.15489
tags:
- detection
- anomaly
- data
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Precursor-of-Anomaly (PoA) detection
  task, aiming to predict future anomalies before they occur in irregular time series
  data. The authors propose PAD, a unified framework combining two co-evolving neural
  controlled differential equations (NCDEs) with multi-task learning and knowledge
  distillation.
---

# Precursor-of-Anomaly Detection for Irregular Time Series

## Quick Facts
- arXiv ID: 2306.15489
- Source URL: https://arxiv.org/abs/2306.15489
- Reference count: 40
- Key outcome: PAD achieves >90% F1-scores on three datasets for both anomaly and precursor-of-anomaly detection in irregular time series

## Executive Summary
This paper introduces a novel Precursor-of-Anomaly (PoA) detection task that aims to predict future anomalies before they occur in irregular time series data. The authors propose PAD, a unified framework using two co-evolving neural controlled differential equations (NCDEs) with multi-task learning and knowledge distillation. Trained in a self-supervised manner using augmented data with artificial anomalies, PAD outperforms 17 baselines across three industrial datasets (MSL, SWaT, WADI), demonstrating high performance even with up to 70% missing observations.

## Method Summary
PAD is a unified framework for detecting both anomalies and their precursors in irregular time series using dual NCDEs with shared parameters. The method processes continuous-time paths created from irregular observations, with one NCDE dedicated to anomaly detection and another to PoA detection. The two networks co-evolve through shared parameters and knowledge distillation, where the anomaly detection network acts as a teacher for the PoA network. The entire framework is trained in a self-supervised manner by augmenting normal data to create artificial anomalies, eliminating the need for labeled anomaly data during training.

## Key Results
- PAD outperforms 17 baseline methods on three datasets (MSL, SWaT, WADI)
- Achieves F1-scores exceeding 90% for both anomaly and PoA detection tasks
- Maintains high performance with up to 70% missing observations in irregular time series
- Ablation studies confirm the effectiveness of the multi-task learning design

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dual NCDEs with shared parameters enable co-evolving feature extraction for both anomaly detection and PoA detection.
- **Mechanism**: Two NCDEs process the same time series path but maintain separate hidden states. Shared parameters (θc) force the networks to influence each other during training, allowing the PoA NCDE to learn from the anomaly detection task.
- **Core assumption**: The shared parameters provide sufficient coupling between the two tasks without overwhelming task-specific representations.
- **Evidence anchors**:
  - [abstract]: "We present a unified framework of detecting the anomaly and precursor-of-anomaly based on neural controlled differential equations (NCDEs) and its multi-task learning and knowledge distillation-based training algorithm"
  - [section 3.3]: "Therefore, our proposed architecture falls into the category of task-specific parameter sharing of the multi-task learning paradigm. We perform the anomaly detection task and the PoA detection tasks with h(T) and z(T), respectively. We ensure that the two NCDEs co-evolve by using the shared parameters θc that allow them to influence each other during the training process."
- **Break condition**: If the shared parameter size becomes too large relative to task-specific parameters, the tasks may become too entangled, harming individual task performance.

### Mechanism 2
- **Claim**: Knowledge distillation from the anomaly NCDE to the PoA NCDE transfers learned anomaly patterns to predict future anomalies.
- **Mechanism**: The anomaly NCDE acts as a teacher, producing outputs for both the current and next time window. The PoA NCDE, acting as a student, learns to mimic the anomaly NCDE's output for the next window while only seeing past information, effectively learning precursor patterns.
- **Core assumption**: The anomaly patterns detected by the teacher NCDE contain predictive information about future anomalies that the student can learn from.
- **Evidence anchors**:
  - [abstract]: "To train the PoA NCDE layer, we apply a knowledge distillation method where the anomaly NCDE layer becomes as a teacher and the PoA NCDE becomes a student."
  - [section 3.3]: "In the training progress, the anomaly NCDE gets two inputs, wi for the anomaly detection and wi+1 for the PoA detection."
- **Break condition**: If the temporal gap between current and future windows is too large, the knowledge transfer becomes ineffective as patterns may change significantly.

### Mechanism 3
- **Claim**: Self-supervised data augmentation creates synthetic anomalies to train both detection tasks without labeled data.
- **Mechanism**: Normal training sequences are augmented by randomly copying segments of normal data to create artificial anomalies with controlled anomaly ratios. This provides labeled data for training both anomaly detection and PoA detection.
- **Core assumption**: Artificially created anomalies preserve the statistical properties of real anomalies while providing sufficient training signal.
- **Evidence anchors**:
  - [abstract]: "Since our method is trained in a self-supervised manner, we resample the normal training dataset to create artificial anomalies"
  - [section 3.5]: "In general, the dataset for anomaly detection do not provide labels for training samples but have labels only for testing samples. Thus, we rely on the self-supervised learning technique."
- **Break condition**: If the augmentation method creates unrealistic anomaly patterns, the model may learn to detect only these synthetic patterns rather than genuine anomalies.

## Foundational Learning

- **Concept**: Neural Controlled Differential Equations (NCDEs)
  - Why needed here: NCDEs can handle irregular time series by processing continuous-time paths, unlike RNNs which require fixed intervals.
  - Quick check question: How does an NCDE differ from a standard neural ODE in processing time series data?

- **Concept**: Multi-task learning with task-specific parameter sharing
  - Why needed here: Allows two related detection tasks to share underlying representations while maintaining task-specific capabilities.
  - Quick check question: What is the role of the shared parameters (θc) in the dual NCDE architecture?

- **Concept**: Knowledge distillation in self-supervised learning
  - Why needed here: Enables training the PoA detection task using only unlabeled data by transferring knowledge from the anomaly detection task.
  - Quick check question: Why does the PoA NCDE need to mimic the anomaly NCDE's output for the next time window?

## Architecture Onboarding

- **Component map**: Time series → Path interpolation → Dual NCDE processing → Output classification → Knowledge distillation
- **Critical path**: Time series → Path interpolation → Dual NCDE processing → Output classification → Knowledge distillation loss
- **Design tradeoffs**: Shared vs. task-specific parameters balance generalization and specialization; knowledge distillation vs. direct supervision affects training efficiency
- **Failure signatures**: 
  - Poor anomaly detection performance indicates issues with the anomaly NCDE or shared parameters
  - Poor PoA detection performance suggests knowledge distillation is ineffective
  - Degraded performance on irregular data indicates interpolation or NCDE handling issues
- **First 3 experiments**:
  1. Train with only anomaly detection task (remove PoA task) to establish baseline performance
  2. Train with only PoA detection task (remove anomaly task) to measure standalone capability
  3. Train with both tasks but without knowledge distillation to measure distillation contribution

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Limited dataset diversity: All three datasets are from industrial process monitoring domains, raising concerns about generalizability to other irregular time series applications
- Missing computational analysis: No reporting of training times or computational complexity, which is critical for real-world deployment
- Synthetic anomaly validity: Artificially created anomalies may not fully capture the complexity of real-world anomalies, potentially limiting generalization

## Confidence
**High confidence**: The core architectural contributions are technically sound with clear mathematical formulations and proper specification of the training algorithm.

**Medium confidence**: Experimental results are compelling but limited by narrow dataset selection from the same domain. Ablation studies demonstrate multi-task learning importance but don't isolate individual contributions.

**Low confidence**: The claim about maintaining high performance with up to 70% missing observations lacks supporting evidence or controlled experiments.

## Next Checks
1. **Generalization testing**: Evaluate PAD on at least two datasets from different domains (e.g., healthcare monitoring or financial transactions) to verify effectiveness on diverse irregular time series data.

2. **Robustness to missing data**: Conduct controlled experiments varying the percentage of missing observations (0% to 70%) on a single dataset to quantify performance degradation and validate the 70% claim.

3. **Ablation of knowledge distillation**: Implement a variant of PAD without knowledge distillation but with all other components intact, then compare PoA detection performance to determine the specific contribution of the distillation mechanism.