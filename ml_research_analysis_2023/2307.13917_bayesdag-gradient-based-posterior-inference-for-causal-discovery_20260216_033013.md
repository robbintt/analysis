---
ver: rpa2
title: 'BayesDAG: Gradient-Based Posterior Inference for Causal Discovery'
arxiv_id: '2307.13917'
source_url: https://arxiv.org/abs/2307.13917
tags:
- inference
- causal
- posterior
- sg-mcmc
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BayesDAG, a scalable framework for Bayesian
  causal discovery that combines SG-MCMC and variational inference to jointly infer
  causal graphs and nonlinear model parameters. The method overcomes key limitations
  of existing approaches by avoiding explicit DAG regularization and enabling direct
  sampling from the posterior over DAGs.
---

# BayesDAG: Gradient-Based Posterior Inference for Causal Discovery

## Quick Facts
- **arXiv ID:** 2307.13917
- **Source URL:** https://arxiv.org/abs/2307.13917
- **Reference count:** 40
- **Key outcome:** BayesDAG combines SG-MCMC and variational inference to jointly infer causal graphs and nonlinear model parameters, enabling scalable Bayesian causal discovery without explicit DAG regularization.

## Executive Summary
BayesDAG introduces a novel scalable framework for Bayesian causal discovery that overcomes key limitations of existing approaches. The method combines stochastic gradient Markov Chain Monte Carlo (SG-MCMC) with variational inference to jointly infer causal graphs and nonlinear model parameters, enabling direct sampling from the posterior over DAGs without requiring explicit acyclicity constraints. By establishing theoretical equivalence between NoCurl's augmented space and permutation-based DAG learning, BayesDAG leverages differentiable sorting gradients to efficiently explore the posterior distribution. Empirical results demonstrate superior inference quality and scalability compared to state-of-the-art methods, successfully handling up to 100 variables in nonlinear settings while maintaining accurate uncertainty quantification.

## Method Summary
BayesDAG performs Bayesian causal discovery by combining SG-MCMC for continuous parameters (node potential vector p and model parameters Θ) with variational inference for discrete adjacency matrices W. The framework maps the problem to an augmented space (W, p) where p encodes topological order, enabling acyclicity through the Step function on potential differences rather than explicit regularization. The topological ordering implicit in p can be expressed as arg maxσ∈Σd pT(σo), which relaxes to a differentiable Sinkhorn operator providing informative gradients for SG-MCMC sampling. The method employs a Gibbs sampling procedure that iteratively samples p, Θ using SG-MCMC and updates the variational posterior qϕ(W|p) using a Bernoulli distribution parameterized by a neural network µϕ. This coupling between W and p through the variational network captures their joint dependence more effectively than independent sampling approaches.

## Key Results
- BayesDAG successfully handles up to 100 variables in nonlinear settings while maintaining accurate uncertainty quantification
- Outperforms state-of-the-art methods (DCDI, DAG-GNN, NOTEARS) on synthetic benchmarks with established ground truth
- Demonstrates superior scalability compared to DIBS, with O(BNp + Npd³) complexity versus DIBS' O(N²p + Npd³)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient-based MCMC can sample DAGs without explicit acyclicity constraints by working in an augmented space (W, p) where p encodes topological order.
- **Mechanism:** The mapping τ(W, p) = W ⊙ Step(grad p) implicitly enforces acyclicity by allowing edges only from higher potential nodes to lower potential nodes. This topological constraint emerges from the Step function on potential differences rather than being imposed through regularization.
- **Core assumption:** The node potential vector p can represent any topological ordering of the DAG without degeneracy (no two nodes share the same potential).
- **Evidence anchors:**
  - [abstract] "Our approach directly samples DAGs from the posterior without requiring any DAG regularization"
  - [section 3.1] "Theorem 3.1 (Equivalence of inference in (W, p) and binary DAG space)"
- **Break condition:** If multiple nodes have identical potentials, the Step function produces ties that break the acyclicity guarantee.

### Mechanism 2
- **Claim:** The (W, p) space is equivalent to permutation-based DAG learning, enabling use of differentiable sorting gradients.
- **Mechanism:** The topological ordering implicit in p can be expressed as arg maxσ∈Σd pT(σo), which relaxes to a differentiable Sinkhorn operator. This provides informative gradients for SG-MCMC sampling in the (W, p) space.
- **Core assumption:** The Sinkhorn approximation with entropy regularization converges to the permutation-based solution as temperature t→0.
- **Evidence anchors:**
  - [section 3.2] "We define L ∈ {0, 1}d×d as a matrix with lower triangular part to be 1, and vector o = [1, ..., d]"
  - [section 3.2] "Theorem 3.2 (Equivalence to NoCurl formulation)"
- **Break condition:** If the Sinkhorn approximation fails to converge or requires too many iterations, the gradient approximation becomes poor.

### Mechanism 3
- **Claim:** Combining SG-MCMC for continuous parameters (p, Θ) with VI for discrete W yields better inference quality than pure SG-MCMC approaches.
- **Mechanism:** SG-MCMC explores the continuous (p, Θ) space while VI efficiently approximates the posterior over discrete W given p. This avoids the slow mixing of discrete MCMC while maintaining theoretical guarantees.
- **Core assumption:** The coupling between p and W through the variational network µϕ captures their joint dependence better than independent sampling.
- **Evidence anchors:**
  - [section 4.2] "We employ a Gibbs sampling procedure [10], which iteratively applies (1) sampling p, Θ ∼ p(p, Θ|D, W) with SG-MCMC; (2) updating the variational posterior qϕ(W|p, D) ≈ p(W|p, Θ, D)"
  - [section E.2] "We hypothesize that coupling W, p through µϕ is important since changes in p results in changes of the permutation matrix σ(p), which should also influence W accordingly during posterior inference"
- **Break condition:** If the variational approximation qϕ becomes too restrictive, it may bias the overall posterior inference.

## Foundational Learning

- **Concept: Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC)**
  - Why needed here: Provides scalable sampling of continuous parameters (p, Θ) in the augmented space while maintaining the correct posterior distribution.
  - Quick check question: How does SG-MCMC differ from standard MCMC in terms of computational complexity and scalability?

- **Concept: Variational Inference (VI)**
  - Why needed here: Efficiently approximates the posterior over discrete DAG adjacency matrices W given the continuous parameters p.
  - Quick check question: Why is VI preferred over MCMC for the discrete W component in this framework?

- **Concept: Sinkhorn Operator and Differentiable Sorting**
  - Why needed here: Enables gradient-based optimization of the permutation matrix σ(p) that defines the topological ordering, which is otherwise non-differentiable.
  - Quick check question: What is the relationship between the Sinkhorn operator and the entropy-regularized optimal transport problem?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Neural network (ζi, li) -> Likelihood computation; SG-MCMC sampler (p, Θ) <- gradients from U(p, W, Θ); Variational network µϕ -> Bernoulli distribution over W; Sinkhorn operator -> Differentiable permutation matrix σ(p); Hungarian algorithm -> Discrete permutation matrix

- **Critical path:**
  1. Forward pass: Compute likelihood p(D|Θ, τ(W, p)) using current p, W, Θ
  2. Compute gradients ∇pU, ∇ΘU for SG-MCMC update
  3. Update p, Θ using SG-MCMC with preconditioning
  4. Update variational parameters ϕ by maximizing ELBO
  5. Sample new W from qϕ(W|p)

- **Design tradeoffs:**
  - SG-MCMC+VI vs pure SG-MCMC: SG-MCMC+VI provides better inference quality but requires designing the variational network; pure SG-MCMC is more flexible but computationally intensive
  - Rank-1 vs full-rank Sinkhorn initialization: Rank-1 (poT) requires fewer iterations but may be less expressive; full-rank is more general but computationally expensive

- **Failure signatures:**
  - Poor mixing in SG-MCMC: Check if injected noise levels are appropriate and if preconditioning is working
  - Variational approximation collapse: Monitor if qϕ becomes too peaked or fails to explore the W space
  - Sinkhorn divergence: If Sinkhorn iterations don't converge, check temperature t and iteration limits

- **First 3 experiments:**
  1. Verify topological ordering: Fix p to create known ordering, check if τ(W, p) produces valid DAGs
  2. Gradient check: Compare analytical gradients of U with finite differences for small d
  3. Ablation on noise levels: Run with different injected noise levels for p and Θ to find optimal balance between exploration and convergence

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Theoretical assumptions about unique node potentials may not hold in practice, potentially breaking the acyclicity guarantee
- Computational complexity grows quadratically with dimension for the adjacency matrix W and cubically for the Sinkhorn operator
- Assumes causal identifiability given the data distribution, which may not hold with latent confounders or selection bias

## Confidence
- **High confidence:** Empirical superiority over state-of-the-art methods (DCDI, DAG-GNN, NOTEARS) on synthetic benchmarks with established ground truth
- **Medium confidence:** Theoretical equivalence between NoCurl's augmented space and permutation-based DAG learning
- **Low confidence:** Claim that BayesDAG "efficiently scales to large graphs" - based only on results up to 100 variables

## Next Checks
1. **Edge case analysis:** Systematically test BayesDAG on DAGs where multiple valid topological orderings exist (e.g., disconnected components or parallel chains) to verify that the (W, p) formulation handles non-unique orderings correctly without degeneracy.

2. **Scalability stress test:** Evaluate BayesDAG on Erdős-Rényi graphs with increasing edge density (p=0.1 to p=0.9) for 50-500 variables to identify the precise scaling limits and computational bottlenecks, particularly focusing on Sinkhorn iteration counts and memory requirements.

3. **Posterior calibration:** Compare the posterior predictive distributions from BayesDAG against ground truth SCMs using stricter calibration metrics (e.g., expected calibration error) rather than just NLL to verify that the uncertainty quantification is well-calibrated across different graph structures and sample sizes.