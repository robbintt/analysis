---
ver: rpa2
title: Online Speculative Decoding
arxiv_id: '2310.07177'
source_url: https://arxiv.org/abs/2310.07177
tags:
- draft
- decoding
- speculative
- target
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Online speculative decoding accelerates LLM inference by adapting\
  \ draft models in real-time using user query data, achieving up to 3.06\xD7 latency\
  \ reduction. The method employs online knowledge distillation to continuously update\
  \ draft models during serving, improving token acceptance rates by 0.1-0.65 compared\
  \ to static approaches."
---

# Online Speculative Decoding

## Quick Facts
- arXiv ID: 2310.07177
- Source URL: https://arxiv.org/abs/2310.07177
- Reference count: 15
- Primary result: Achieves 1.22×-3.06× latency reduction by adapting draft models in real-time using online knowledge distillation

## Executive Summary
Online speculative decoding introduces a novel approach to accelerate large language model inference by continuously updating draft models during serving using observed user query data. The method leverages spare computational resources in serving clusters to perform online knowledge distillation, adapting the draft model to the actual query distribution rather than relying on static pre-training. This enables significant improvements in token acceptance rates (0.1-0.65) and latency reduction (1.22×-3.06×) across various benchmarks including Vicuna-7B and Flan-T5-XL target models.

## Method Summary
The approach uses online knowledge distillation to continuously update draft models during serving by leveraging observed user query data. The method employs a buffer to store recent queries where the draft model made incorrect predictions, along with the corresponding target model outputs. The draft model is updated at regular intervals using forward KL divergence loss on this buffered data. This enables the draft model to adapt to the query distribution in real-time, improving its ability to predict the target model's outputs accurately. The approach maintains computational cost-neutrality by utilizing spare FLOPs available in memory-bounded LLM inference serving clusters.

## Key Results
- Achieves 1.22×-3.06× latency reduction compared to baseline speculative decoding
- Improves token acceptance rates by 0.1-0.65 across multiple benchmarks
- Maintains cost-neutrality by leveraging spare computational resources in serving clusters
- Demonstrates effectiveness on diverse datasets including Spider, GSM8K, Code-search-Python, and Alpaca-finance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Online speculative decoding improves draft model accuracy by adapting to the query distribution in real-time.
- **Mechanism:** The method uses online knowledge distillation to continuously update the draft model using user query data, allowing it to better predict the target model's outputs for similar inputs.
- **Core assumption:** The query distribution of an LLM service is relatively simple and stable, allowing the draft model to learn and adapt effectively.
- **Evidence anchors:** [abstract], [section], [corpus] - Card: A Cache-Assisted Parallel Speculative Decoding Framework

### Mechanism 2
- **Claim:** Online knowledge distillation leverages the spare computational resources in serving clusters to make training cost-neutral.
- **Mechanism:** The method uses the surplus computational power available in an LLM serving cluster to continuously retrain the draft model on observed user query data, without incurring significant additional costs.
- **Core assumption:** LLM inference is memory-bounded, resulting in a substantial amount of unused compute in a typical LLM serving cluster.
- **Evidence anchors:** [abstract], [section], [corpus] - Card: A Cache-Assisted Parallel Speculative Decoding Framework

### Mechanism 3
- **Claim:** Online speculative decoding aligns the draft model with the target model on newly observed user query data, improving token acceptance rates.
- **Mechanism:** The method uses a new online learning algorithm based on Generalized Knowledge Distillation (GKD) to keep track of recent queries that the draft model has speculated incorrectly and forces the draft model to emulate the target model's outputs on these queries.
- **Core assumption:** The information about incorrect predictions is valuable for improving the draft model's accuracy.
- **Evidence anchors:** [abstract], [section], [corpus] - Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification

## Foundational Learning

- **Concept:** Speculative decoding
  - **Why needed here:** Understanding how speculative decoding works is crucial for grasping the core idea of online speculative decoding, which builds upon this technique.
  - **Quick check question:** What is the main goal of speculative decoding, and how does it achieve this goal?

- **Concept:** Knowledge distillation
  - **Why needed here:** Knowledge distillation is the key technique used in online speculative decoding to update the draft model based on the target model's outputs.
  - **Quick check question:** How does knowledge distillation work, and what are its main benefits in the context of online speculative decoding?

- **Concept:** Online learning
  - **Why needed here:** Online learning is the paradigm that enables the draft model to continuously adapt to the query distribution in real-time.
  - **Quick check question:** What are the main characteristics of online learning, and how does it differ from offline learning in the context of online speculative decoding?

## Architecture Onboarding

- **Component map:** User query -> Draft model -> Target model verification -> Buffer collection -> Online knowledge distillation updates
- **Critical path:**
  1. User sends a query to the serving cluster
  2. The draft model predicts the target model's outputs
  3. The target model verifies the draft model's predictions
  4. If predictions are correct, the draft model is rewarded; if incorrect, the error is recorded
  5. The online learning algorithm updates the draft model based on the recorded errors
- **Design tradeoffs:**
  - Using a larger draft model may improve accuracy but reduce the computational efficiency gains
  - Updating the draft model too frequently may incur additional costs, while updating too infrequently may slow down adaptation to the query distribution
  - Using different knowledge distillation techniques may have varying effects on the draft model's performance
- **Failure signatures:**
  - Low token acceptance rate: Indicates that the draft model is not accurately predicting the target model's outputs
  - High computational overhead: Suggests that the online learning algorithm is not efficiently utilizing the spare computational resources
  - Inconsistent performance: May indicate that the draft model is not effectively adapting to the query distribution
- **First 3 experiments:**
  1. Evaluate the token acceptance rate of the draft model on a held-out test set before and after online learning
  2. Measure the computational overhead of the online learning algorithm and compare it to the computational savings from speculative decoding
  3. Test the draft model's performance on different query distributions to assess its ability to adapt to varying data patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal frequency for updating draft models in online speculative decoding?
- **Basis in paper:** [inferred] The paper mentions update interval I as a dynamic parameter that can be adjusted based on system load and query distribution changes.
- **Why unresolved:** The paper does not provide specific guidelines or empirical results on the optimal update frequency. The choice of I is left to future exploration.
- **What evidence would resolve it:** Systematic experiments varying update intervals across different workloads and measuring token acceptance rates and latency improvements.

### Open Question 2
- **Question:** How does online speculative decoding perform on query distributions with high temporal variance?
- **Basis in paper:** [inferred] The paper mentions adapting to shifts in query distribution but does not explore scenarios with rapidly changing distributions over time.
- **Why unresolved:** The experiments focus on relatively stable distributions or gradual shifts. High-frequency distribution changes are not explicitly tested.
- **What evidence would resolve it:** Testing on real-world datasets with known temporal variance in query types and measuring performance degradation or adaptation speed.

### Open Question 3
- **Question:** What is the impact of online speculative decoding on draft model forgetting of previous knowledge?
- **Basis in paper:** [explicit] The paper mentions this concern in Appendix A.4 and conducts preliminary experiments on data mixing.
- **Why unresolved:** The experiments are limited to two datasets and do not comprehensively address long-term forgetting across diverse domains.
- **What evidence would resolve it:** Long-term experiments with multiple sequential distribution changes and comprehensive evaluation of draft model performance across all previously encountered distributions.

## Limitations

- The assumption that query distributions in LLM services are "relatively simple and stable" may not hold in all real-world scenarios, particularly those with highly diverse or rapidly changing query patterns.
- The computational cost-neutrality claim depends on consistent availability of spare FLOPs in serving clusters, which may vary significantly across different deployment scenarios and peak usage periods.
- The approach may struggle with distributional shifts between the initial draft model training data and specialized target models, particularly when target models have been fine-tuned on domain-specific data.

## Confidence

**High Confidence:** The mechanism of using online knowledge distillation to improve draft model accuracy is theoretically sound and supported by the experimental results showing 0.1-0.65 improvement in token acceptance rates.

**Medium Confidence:** The claim of computational cost-neutrality is plausible but depends heavily on serving cluster utilization patterns that may vary significantly across deployments.

**Low Confidence:** The assertion that query distributions in LLM services are "relatively simple and stable" lacks empirical validation across diverse real-world scenarios and may not generalize to all use cases.

## Next Checks

1. **Distribution Stability Test:** Conduct experiments with synthetically generated query distributions that vary in complexity and rate of change to empirically validate the assumption about distribution stability. Measure how quickly acceptance rates degrade under different distribution shift scenarios.

2. **Resource Utilization Analysis:** Implement monitoring to track actual computational overhead during online updates across different cluster utilization levels. Measure the break-even point where spare resources become insufficient for cost-neutral operation.

3. **Generalization Evaluation:** Test the approach on a broader range of target models including those fine-tuned on specialized domains (medical, legal, technical) to assess how well the online distillation handles distributional shifts between general-purpose draft models and specialized target models.