---
ver: rpa2
title: A Systematic Study on Quantifying Bias in GAN-Augmented Data
arxiv_id: '2308.13554'
source_url: https://arxiv.org/abs/2308.13554
tags:
- mode
- data
- datasets
- distribution
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates five state-of-the-art metrics
  for quantifying bias in GAN-augmented data, focusing on mode collapse. The metrics
  tested include Number of Statistically-Different Bins (NDB), Inception Score (IS),
  MODE Score, Fr\'echet Inception Distance (FID), and Jensen-Shannon divergence.
---

# A Systematic Study on Quantifying Bias in GAN-Augmented Data

## Quick Facts
- arXiv ID: 2308.13554
- Source URL: https://arxiv.org/abs/2308.13554
- Reference count: 1
- No single metric reliably quantifies bias exacerbation across different image domains

## Executive Summary
This study systematically evaluates five state-of-the-art metrics for quantifying bias in GAN-augmented data, focusing on mode collapse. The research tests NDB, IS, MODE Score, FID, and JS divergence on artificially mode-collapsed subsets of MNIST and CIFAR-10 datasets, as well as on GAN-generated images using DCGAN. Results show that while NDB, JS, and FID performed reliably on both datasets, they failed to detect variation in CIFAR-10 until at least six classes were removed. The IS showed inconsistent performance, failing to capture diversity loss in MNIST until only one class remained. MODE score demonstrated unreliable results across both datasets.

## Method Summary
The study created artificially mode-collapsed datasets by progressively removing classes from MNIST and CIFAR-10 (10 versions per dataset with 0-9 classes remaining). DCGAN was trained on both original and mode-collapsed datasets to generate 5000 samples each. Five metrics (NDB, IS, MODE Score, FID, JS divergence) were implemented and applied to compare diversity between reference and generated distributions. Results were scaled between 0 (diverse) and 1 (biased) for cross-metric comparison, with evaluation focusing on each metric's ability to detect mode collapse across varying levels of dataset bias.

## Key Results
- NDB, JS, and FID metrics performed reliably on both MNIST and CIFAR-10 datasets
- All three metrics failed to detect diversity loss in CIFAR-10 until at least six classes were removed
- IS showed inconsistent performance, failing on MNIST until only one class remained
- MODE score demonstrated unreliable results across both datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mode collapse occurs because GANs prioritize generator stability over output diversity during training
- Mechanism: During adversarial training, the discriminator rapidly improves, causing the generator to converge to a narrow set of high-scoring outputs that fool the discriminator rather than exploring the full data distribution
- Core assumption: The generator's loss function does not explicitly penalize reduced diversity, and the training dynamics favor quick wins over comprehensive coverage
- Evidence anchors: [abstract] states GANs "suffer from the so-called mode collapse failure mode, which makes them vulnerable to exacerbating biases on already skewed datasets"; [section] notes GANs have "an inherent failure mode called mode collapse (Goodfellow 2017)"

### Mechanism 2
- Claim: Evaluation metrics fail to reliably detect mode collapse because they measure different aspects of distribution similarity that don't always correlate with diversity
- Mechanism: Metrics like FID and IS compare feature distributions between real and generated data, but these comparisons can remain stable even when the generator produces limited modes if those modes happen to match real data statistics
- Core assumption: Distribution similarity metrics are not equivalent to diversity metrics, and matching summary statistics doesn't guarantee capturing all modes
- Evidence anchors: [section] shows "NDB, JS, and FID scores performed similarly well for both tested datasets; they all increase monotonically according to the amount of bias" but "fail to detect any variation in diversity for CIFAR-10 until at least six classes are removed"; [abstract] concludes "there is no single metric that quantifies bias exacerbation reliably over the span of different image domains"

### Mechanism 3
- Claim: Skewed training distributions amplify mode collapse because the generator learns to exploit the imbalance rather than learning the full distribution
- Mechanism: When training data is already biased toward certain classes or features, the generator finds it easier to focus on these overrepresented modes and ignore the sparse ones, effectively magnifying the existing bias
- Core assumption: The generator's optimization process treats all modes equally in terms of loss contribution, but the data distribution creates an implicit weighting that favors common modes
- Evidence anchors: [abstract] explains GANs "have been proven to not only perpetuate but also magnify the existing biases in the data"; [section] describes artificially creating "mode collapsed/skewed subsets" by sampling that "has classes 0, 1, ... i" to test bias detection

## Foundational Learning

- Concept: Generative Adversarial Networks architecture and training dynamics
  - Why needed here: Understanding how the generator and discriminator interact is essential to grasp why mode collapse occurs and why it's difficult to detect
  - Quick check question: What are the two competing networks in a GAN, and what are their respective objectives during training?

- Concept: Mode collapse as a failure mode in generative models
  - Why needed here: The paper's core focus is on detecting and quantifying this specific failure, so understanding its characteristics is fundamental
  - Quick check question: How does mode collapse manifest in the output of a GAN, and why is it particularly problematic for biased datasets?

- Concept: Statistical distance metrics (KL divergence, JS divergence, Fréchet distance)
  - Why needed here: The evaluation relies on multiple metrics that measure different aspects of distribution similarity, and understanding their properties is crucial for interpreting results
  - Quick check question: What is the key difference between how KL divergence and Fréchet distance measure the difference between two distributions?

## Architecture Onboarding

- Component map:
  Data preparation -> DCGAN training -> Metric evaluation -> Analysis pipeline

- Critical path:
  1. Create artificially mode-collapsed datasets by progressively removing classes
  2. Train DCGAN on both original and mode-collapsed datasets
  3. Generate 5000 samples from each trained GAN
  4. Apply all five metrics to both real and generated datasets
  5. Scale and compare metric results to assess detection capability

- Design tradeoffs:
  - Using artificially mode-collapsed datasets provides controlled testing but may not capture all real-world failure modes
  - DCGAN is chosen for its tendency to exacerbate biases but may not represent state-of-the-art GAN performance
  - Scaling metric results between 0 and 1 enables comparison but may obscure absolute differences

- Failure signatures:
  - Metrics showing inconsistent behavior across different datasets (e.g., IS failing on MNIST but working on CIFAR-10)
  - Metrics failing to detect diversity loss until extreme mode collapse occurs
  - Generated images showing clear artifacts or limited class diversity that don't correspond to metric scores

- First 3 experiments:
  1. Run the evaluation pipeline on MNIST with 1, 3, 5, 7, and 9 classes removed to observe metric behavior across the full range
  2. Test the same evaluation on CIFAR-10 to compare cross-dataset performance
  3. Train DCGAN on the original MNIST and CIFAR-10 datasets and evaluate the generated samples with all metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mode collapse be reliably quantified across diverse image domains using existing metrics?
- Basis in paper: [explicit] The study concludes that no single metric can reliably quantify bias exacerbation across different image domains, highlighting the need for improved evaluation methods
- Why unresolved: Current metrics (NDB, IS, MODE, FID, JS) show inconsistent performance across different datasets (MNIST vs CIFAR-10) and fail to detect bias until significant class removal occurs
- What evidence would resolve it: Development and validation of new metrics or metric combinations that consistently detect mode collapse across diverse image domains and benchmark datasets, with systematic evaluation showing reliable performance across varying dataset characteristics

### Open Question 2
- Question: How do noisy artifacts in GAN-generated data affect bias quantification metrics?
- Basis in paper: [explicit] The study notes that noisy artifacts generated by DCGAN affected the performance of the measures, particularly for MNIST results which showed varying scores
- Why unresolved: The impact of generation artifacts on metric reliability is not well understood, and the study suggests manual inspection of generated data as potential future work to better understand this relationship
- What evidence would resolve it: Controlled experiments isolating the effect of generation artifacts on metric performance, including quantitative analysis of how different types of artifacts influence each metric's ability to detect mode collapse

### Open Question 3
- Question: What are the limitations of current benchmark datasets for evaluating bias in GAN-generated data?
- Basis in paper: [explicit] The study suggests that adding labeled GAN-generated datasets already used in existing literature - especially those involving face-imagery or human data - would serve as benchmarks that more accurately reflect bias in society
- Why unresolved: Current evaluation relies on artificially mode-collapsed subsets of MNIST and CIFAR-10, which may not capture the complexity of real-world bias scenarios in human-data applications
- What evidence would resolve it: Creation and validation of higher-quality benchmark datasets that better represent societal biases, followed by systematic evaluation of existing metrics on these datasets to assess their real-world applicability

## Limitations

- Limited dataset diversity (only MNIST and CIFAR-10) restricts generalizability of findings
- Artificial mode collapse may not reflect natural bias development in real-world scenarios
- Single GAN architecture (DCGAN) used throughout experiments limits architectural sensitivity analysis

## Confidence

This study demonstrates **Low to Medium confidence** in its conclusions about metric reliability due to several limitations. The primary uncertainty stems from testing only two datasets and a single GAN architecture, which may not generalize to other domains or more sophisticated models.

## Next Checks

1. **Dataset Generalization Test**: Apply the same evaluation framework to additional diverse datasets (e.g., ImageNet, CelebA, medical imaging datasets) to assess whether metric performance patterns hold across different image domains and complexity levels.

2. **Architecture Sensitivity Analysis**: Repeat experiments using multiple GAN architectures (StyleGAN, BigGAN, diffusion models) to determine if metric reliability varies with model capacity and training methodology.

3. **Real-World Bias Validation**: Test the metrics on naturally biased datasets rather than artificially mode-collapsed versions, examining whether the observed metric behaviors translate to real-world bias detection scenarios.