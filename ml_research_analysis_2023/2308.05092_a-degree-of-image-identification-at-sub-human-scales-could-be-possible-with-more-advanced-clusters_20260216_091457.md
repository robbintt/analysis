---
ver: rpa2
title: A degree of image identification at sub-human scales could be possible with
  more advanced clusters
arxiv_id: '2308.05092'
source_url: https://arxiv.org/abs/2308.05092
tags:
- data
- image
- images
- scaling
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research investigates whether current self-supervised learning
  techniques can achieve human-level visual image comprehension using the same degree
  and amount of sensory input that humans receive. The study scales both data volume
  and image quality simultaneously, training vision transformers on up to 200,000
  images at 256 ppi resolution.
---

# A degree of image identification at sub-human scales could be possible with more advanced clusters

## Quick Facts
- arXiv ID: 2308.05092
- Source URL: https://arxiv.org/abs/2308.05092
- Reference count: 1
- Primary result: Human-level object detection at sub-human scales is achievable through simultaneous scaling of data volume and image resolution

## Executive Summary
This research investigates whether current self-supervised learning techniques can achieve human-level visual image comprehension using the same degree and amount of sensory input that humans receive. The study scales both data volume and image quality simultaneously, training vision transformers on up to 200,000 images at 256 ppi resolution. Using a polynomial function to model the relationship between data amount, model size, and image quality, the study predicts that human-level accuracy (defined as >90%) can be achieved with a 522B parameter Vision Transformer model trained on approximately 200,000 public images at 4096 ppi resolution.

## Method Summary
The research employs masked auto-encoders for self-supervised pretraining with large masking proportions (80%) across Vision Transformer models of varying sizes (Hybrid/S/L/B). The training dataset consists of 200,000 images from ImageNet (50%), CelebA (31.5%), CIFAR-10 (5%), and ADE20K (13.5%) at 256 ppi resolution. The study uses a polynomial scaling function in logarithmic space to model how increased model size and higher image resolution interact to improve accuracy, compensating for reduced data volume.

## Key Results
- Scaling both data volume and image resolution simultaneously enables human-level object detection performance at sub-human scales
- Masked auto-encoders with 80% masking ratio prove effective for self-supervised pretraining with minimal data augmentation
- The polynomial scaling function successfully models the relationship between data amount, model size, and image quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling both data volume and image resolution simultaneously enables human-level object detection at sub-human scales.
- Mechanism: The polynomial scaling function models how increased model size and higher image resolution interact to improve accuracy, compensating for reduced data volume.
- Core assumption: The relationship between data amount, model size, and image quality can be captured by a polynomial function in logarithmic space.
- Evidence anchors: The study predicts human-level accuracy can be achieved with a 522B parameter model on 200,000 images at 4096 ppi; the polynomial function Precision = (βi + logi αi)(logppi αppi + βppi) is used to model this relationship.

### Mechanism 2
- Claim: Masked auto-encoders are effective for self-supervised pretraining with large masking proportions.
- Mechanism: Masked auto-encoders require relatively little data augmentation, preserving the human-likeness of training data, and perform well with large masking proportions (80%).
- Core assumption: Masked auto-encoders are more effective than conventional self-supervised pictorial representational training methods for this task.
- Evidence anchors: The paper notes masked auto-encoders require less data augmentation than most existing visual self-supervised methods and selects 80% masking proportion based on their effectiveness.

### Mechanism 3
- Claim: Using a diverse dataset with varying image qualities and resolutions enables robust model performance.
- Mechanism: The dataset includes images from various sources (ImageNet, CelebA, CIFAR-10, ADE20K) with different resolutions and qualities, providing a rich training environment.
- Core assumption: A diverse dataset is necessary for achieving human-level image recognition performance.
- Evidence anchors: The training data combines 2,000 images from four different datasets with ImageNet contributing 50% of the total training data.

## Foundational Learning

- Concept: Vision Transformers (ViT)
  - Why needed here: The research uses Vision Transformers as the model architecture for image recognition tasks.
  - Quick check question: What are the advantages of using Vision Transformers over traditional convolutional neural networks for image recognition?

- Concept: Self-supervised Learning
  - Why needed here: The study employs self-supervised learning techniques to train the models without external labels.
  - Quick check question: How does self-supervised learning differ from supervised learning, and what are its advantages in the context of image recognition?

- Concept: Polynomial Scaling Functions
  - Why needed here: The research uses a polynomial function to model the relationship between data amount, model size, and image quality.
  - Quick check question: Why is a polynomial function in logarithmic space used to model the scaling relationship, and what are the implications of this choice?

## Architecture Onboarding

- Component map: Vision Transformers (ViT-Hybrid/S/L/B) → Masked auto-encoders (80% masking) → Combined dataset (ImageNet, CelebA, CIFAR-10, ADE20K) → Image resolution scaling (226-256 ppi)
- Critical path: Data preprocessing (resizing and augmenting images) → Model training (self-supervised pretraining) → Fine-tuning (if necessary) → Evaluation (accuracy measurement)
- Design tradeoffs: Larger models and higher image resolutions require more computational resources but may lead to better performance. The choice of self-supervised learning method and masking proportion also affects the training process and outcomes.
- Failure signatures: Poor performance on the validation set, overfitting on the training data, or failure to converge during training.
- First 3 experiments:
  1. Train a small ViT model on a subset of the dataset with a low resolution (e.g., 226 pixels) to establish a baseline performance.
  2. Increase the model size and image resolution incrementally to observe the effect on performance, following the scaling function.
  3. Experiment with different self-supervised learning methods and masking proportions to find the optimal configuration for the task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or additional techniques would be required to achieve human-level image recognition accuracy with the predicted 522B parameter Vision Transformer model?
- Basis in paper: The paper predicts that human-level accuracy can be achieved with a 522B parameter Vision Transformer model trained on approximately 200,000 public images at 4096 ppi resolution, but does not specify what architectural modifications or additional techniques would be necessary.
- Why unresolved: The paper only provides a scaling prediction based on the polynomial function and current results, but does not delve into the specific architectural or algorithmic changes needed to achieve the predicted performance.
- What evidence would resolve it: Experimental results showing the performance of different architectural modifications or additional techniques when scaling up to the predicted model size and data resolution.

### Open Question 2
- Question: How would the results change if different self-supervised learning methods, such as contrastive learning or generative models, were used instead of masked auto-encoders?
- Basis in paper: The paper states that masked auto-encoders were chosen for their benefits, but does not explore other self-supervised learning methods.
- Why unresolved: The paper only evaluates masked auto-encoders and does not compare their performance to other self-supervised learning methods.
- What evidence would resolve it: Experimental results comparing the performance of different self-supervised learning methods when scaling up data volume and image resolution.

### Open Question 3
- Question: What is the relationship between the model size, data amount, and image resolution when scaling up to achieve human-level accuracy? Is the polynomial function used in the paper the optimal representation of this relationship?
- Basis in paper: The paper uses a polynomial function to model the relationship between data amount, model size, and image quality, but does not explore alternative representations or validate the optimality of the chosen function.
- Why unresolved: The paper assumes the polynomial function is a suitable representation without exploring other possible functions or validating its optimality.
- What evidence would resolve it: Experimental results comparing the performance of different scaling strategies and their ability to achieve human-level accuracy with varying model sizes, data amounts, and image resolutions.

## Limitations
- Theoretical extrapolation from current model scales to extreme parameter counts (522B) without empirical validation
- Limited testing of the polynomial scaling function only on a narrow range of model sizes
- Dataset composition represents a limited subset of human visual experience and may not capture full complexity of real-world imagery

## Confidence
- Medium confidence: The core finding that simultaneous scaling of data volume and image resolution improves performance
- Low confidence: The specific prediction of human-level performance at 522B parameters with 200,000 images at 4096 ppi
- Medium confidence: The effectiveness of masked auto-encoders for this task

## Next Checks
1. Empirically validate the polynomial scaling function by training additional intermediate-sized models (e.g., 100B-200B parameters) to test whether the extrapolation holds before reaching the predicted 522B threshold
2. Conduct ablation studies comparing masked auto-encoders against other self-supervised methods (SimCLR, MoCo, DINO) using identical model scales and data to quantify the claimed advantages
3. Test model generalization on out-of-distribution datasets beyond ImageNet to verify whether the claimed human-level performance transfers to novel visual domains and whether the scaling relationship remains consistent across different data distributions