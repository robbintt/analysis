---
ver: rpa2
title: 'Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation'
arxiv_id: '2310.02304'
source_url: https://arxiv.org/abs/2310.02304
tags:
- solution
- utility
- language
- solutions
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STOP (Self-Taught Optimizer), a method for
  recursively improving code that uses language models as scaffolding. The key idea
  is to treat the design of scaffolding programs as an optimization problem itself,
  and use language models to recursively improve the scaffolding code that calls them.
---

# Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation

## Quick Facts
- arXiv ID: 2310.02304
- Source URL: https://arxiv.org/abs/2310.02304
- Reference count: 40
- Key outcome: STOP recursively improves code generation scaffolding, achieving better downstream task performance than seed improver

## Executive Summary
This paper introduces STOP (Self-Taught Optimizer), a method for recursively improving code generation scaffolding using language models. The approach treats scaffolding design as an optimization problem, using language models to recursively improve the code that calls them. STOP starts with a simple seed improver that prompts a language model to generate candidate improvements, then uses this seed improver to improve itself through multiple rounds of self-improvement. The resulting improved scaffolding generates programs with significantly better performance on downstream tasks. While not full recursive self-improvement (as the language model itself is not altered), the results demonstrate the potential for language models to write code that can call itself to improve itself.

## Method Summary
STOP is a recursive self-improvement algorithm that iteratively improves code generation scaffolding. It begins with a seed improver - a simple program that prompts a language model to generate candidate improvements to input programs based on a utility function. The algorithm then uses this seed improver to improve itself, running multiple rounds of self-improvement. In each iteration, the language model generates an improved version of the improver, which is evaluated on downstream tasks using a meta-utility function. The process continues until convergence or resource limits. The language model discovers various self-improvement strategies including beam search, genetic algorithms, and simulated annealing through this process.

## Key Results
- Improved improvers generated through STOP significantly outperform the seed improver on downstream tasks
- STOP-discovered scaffolding strategies include beam search, genetic algorithms, and simulated annealing
- Improved improvers demonstrate transfer to new downstream tasks not seen during self-improvement
- STOP successfully generates syntactically correct code improvements across multiple improvement iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive self-improvement works because the language model can propose and implement better scaffolding code that improves its own utility function.
- Mechanism: The model uses its own code generation ability to rewrite the scaffolding program that calls it, optimizing for higher downstream task performance.
- Core assumption: The language model can understand and improve code it generates, and the improved scaffolding leads to better downstream task results.
- Evidence anchors:
  - [abstract] "the resulting improved improver generates programs with significantly better performance than the seed improver"
  - [section] "the model refines this improver program" and "a variety of self-improvement strategies are proposed by the language model"
- Break condition: If the model cannot generate syntactically correct code improvements or if the utility function is misspecified, self-improvement fails.

### Mechanism 2
- Claim: The self-improvement process transfers across different downstream tasks because the model is not exposed to task-specific details during improvement.
- Mechanism: By optimizing only the meta-utility (average performance across tasks), the model learns general scaffolding strategies that apply to new problems.
- Core assumption: General scaffolding improvements benefit multiple tasks even when trained on a single task.
- Evidence anchors:
  - [section] "we select one of the better-performing improvers from Section 5.1...and evaluate its performance on five new downstream tasks...the improved improver...outperforms the seed improver on each new downstream task"
- Break condition: If the tasks are too dissimilar or if the model overfits to the training task distribution.

### Mechanism 3
- Claim: The language model can discover and implement various optimization strategies like beam search, genetic algorithms, and simulated annealing through self-improvement.
- Mechanism: The model proposes different meta-heuristic strategies in its code generations, and the improvement loop selects the most effective ones.
- Core assumption: The model has latent knowledge of these algorithms despite them not being in its training data.
- Evidence anchors:
  - [abstract] "A variety of self-improvement strategies are proposed by the language model, including beam search, genetic algorithms, and simulated annealing"
  - [section] "we qualitatively investigate the self-improvement strategies proposed by STOP, highlighting both the encouraging and novel approaches"
- Break condition: If the model cannot generate correct implementations of these strategies or if the strategies don't improve performance.

## Foundational Learning

- Concept: Utility functions and evaluation
  - Why needed here: The entire self-improvement process depends on having a well-defined utility function to measure and optimize performance
  - Quick check question: Can you write a utility function that returns a score for a given program's performance on a task?

- Concept: Recursive function calls and self-reference
  - Why needed here: The improvement process involves the model calling itself through improved scaffolding code
  - Quick check question: Can you trace through what happens when a function calls itself recursively with modified parameters?

- Concept: Meta-optimization and indirect optimization
  - Why needed here: Instead of optimizing the model directly, we optimize the scaffolding code that uses the model
  - Quick check question: Can you explain the difference between optimizing a model's weights versus optimizing the code that calls the model?

## Architecture Onboarding

- Component map:
  Seed improver -> Language model (GPT-4) -> Utility function -> Meta-utility -> Improvement loop

- Critical path:
  1. Start with seed improver code
  2. Use language model to generate improved version of the improver
  3. Evaluate improved improver on downstream tasks
  4. Repeat until convergence or resource limits

- Design tradeoffs:
  - Simpler seed improver vs. more complex initial scaffolding
  - Number of improvement iterations vs. computational cost
  - Using strong vs. weak language models for improvement
  - Sandbox safety vs. performance

- Failure signatures:
  - Code generation produces syntax errors
  - Improved scaffolding doesn't improve downstream performance
  - Model attempts to bypass constraints or "reward hack"
  - Performance degrades after multiple improvement iterations

- First 3 experiments:
  1. Run seed improver on a simple task like parity learning and measure baseline performance
  2. Apply one round of self-improvement and compare performance to seed improver
  3. Test transferability by applying the improved improver to a different task type

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of initial seed improver affect the self-improvement process and final results?
- Basis in paper: [explicit] The paper discusses designing the seed improver and mentions considering other variants but heuristically choosing one version.
- Why unresolved: The paper only evaluates one specific seed improver design, so it's unclear how sensitive the results are to this choice.
- What evidence would resolve it: Comparing STOP performance with multiple different seed improver designs.

### Open Question 2
- Question: Can STOP generate scaffolding code that is comparable to hand-engineered scaffolding by experts?
- Basis in paper: [explicit] The authors state "at this point, we do not believe that the scaffolding systems STOP creates are superior to those hand-engineered by experts."
- Why unresolved: The paper does not directly compare STOP-generated scaffolding to expert-designed scaffolding on the same tasks.
- What evidence would resolve it: Head-to-head comparison of STOP-generated vs expert-designed scaffolding on benchmark tasks.

### Open Question 3
- Question: How transferable are STOP-improved improvers to tasks outside the distribution seen during self-improvement?
- Basis in paper: [explicit] The paper shows some transfer to new tasks but notes it's plausible because the self-improver is not shown the downstream task or solution.
- Why unresolved: Only 5 new tasks are evaluated, which may not be representative of the full space of possible tasks.
- What evidence would resolve it: Extensive testing of STOP-improved improvers on a wide range of diverse tasks.

## Limitations
- The self-improvement process operates on scaffolding code rather than the language model itself, limiting the scope of improvement
- Computational costs of multiple improvement iterations with large language models raise practical deployment concerns
- The paper does not address potential safety issues that could arise from recursive self-improvement

## Confidence
- **High confidence**: The basic mechanism of recursive scaffolding improvement is well-demonstrated and reproducible
- **Medium confidence**: Transferability claims across different task types, as the paper shows promising but limited cross-task generalization
- **Medium confidence**: The diversity of self-improvement strategies discovered, though the specific implementations are not fully detailed

## Next Checks
1. Implement and test the seed improver on a simple parity learning task to establish baseline performance before attempting self-improvement
2. Run a controlled experiment comparing performance after 1, 2, and 4 rounds of self-improvement to determine optimal iteration count
3. Test the transferability claims by applying the improved improver to at least three new task types not used in the original training set, measuring performance degradation or improvement