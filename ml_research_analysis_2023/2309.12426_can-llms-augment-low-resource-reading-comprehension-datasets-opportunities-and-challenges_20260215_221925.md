---
ver: rpa2
title: Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities
  and Challenges
arxiv_id: '2309.12426'
source_url: https://arxiv.org/abs/2309.12426
tags:
- data
- synthetic
- linguistics
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of using GPT-4 to augment
  low-resource reading comprehension datasets by generating synthetic data. The core
  idea is to leverage GPT-4's few-shot learning capabilities to generate new contexts,
  questions, and answers that mimic the style and semantics of original training data.
---

# Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges

## Quick Facts
- arXiv ID: 2309.12426
- Source URL: https://arxiv.org/abs/2309.12426
- Reference count: 14
- Primary result: GPT-4 synthetic data augmentation improves low-resource MRC performance by 23% on CovidQA and 5% on PolicyQA

## Executive Summary
This paper investigates using GPT-4 to augment low-resource reading comprehension datasets through synthetic data generation. The authors employ GPT-4's few-shot learning capabilities with a two-stage approach: generating synthetic contexts followed by generating QA pairs conditioned on those contexts. A novel cycle-consistent filtration process ensures answer consistency across generations. Experiments on three low-resource datasets (CovidQA, PolicyQA, TechQA) demonstrate that synthetic data augmentation improves performance for CovidQA and PolicyQA, with one-shot generation plus filtration yielding the best results on CovidQA.

## Method Summary
The method uses GPT-4's few-shot learning to generate synthetic contexts and QA pairs from low-resource training data. In the first stage, GPT-4 generates synthetic contexts using one or two-shot prompts from the original dataset. In the second stage, it generates QA pairs conditioned on these synthetic contexts. An optional cycle-consistent filtration process checks if GPT-4 can consistently answer the generated question without seeing the original answer. The filtered synthetic data is combined with original data to train downstream MRC models. The authors experiment with generating 1x to 10x the original training data size and evaluate on three low-resource datasets using Exact Match and F1 metrics.

## Key Results
- Synthetic data augmentation improves BERT-based MRC performance by 23% on CovidQA and 5% on PolicyQA
- One-shot generation with cycle-consistent filtration yields best results on CovidQA
- Two-shot generation performs better than one-shot on PolicyQA
- TechQA shows no improvement from synthetic data augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's few-shot learning capabilities allow effective generation of synthetic data that mimics the style and semantics of original low-resource datasets.
- Mechanism: By priming GPT-4 with 1-2 examples from the training set, the model learns the domain-specific patterns and generates new contexts, questions, and answers that closely resemble human-authored content.
- Core assumption: GPT-4's few-shot learning is robust enough to capture the nuances of the original data distribution with minimal priming examples.
- Evidence anchors:
  - [abstract]: "We use in-context learning with passages, questions, and answers from the training set, ensuring minimal domain shift between the synthetically generated data and the original datasets"
  - [section]: "Providing just one or two examples allows GPT-4 to adapt from demonstrations due to the robust few-shot learning capabilities of LLMs"
  - [corpus]: Weak - no direct evidence found in corpus; this is a general LLM capability assumption
- Break condition: If GPT-4 fails to capture the domain-specific patterns with the given few-shot examples, the generated synthetic data will have high domain shift from the original dataset.

### Mechanism 2
- Claim: The round trip filtration process improves the quality of synthetic QA pairs by ensuring consistency in answers.
- Mechanism: After generating a synthetic question and answer, GPT-4 is prompted to answer the question again without seeing the original answer. If the newly generated answer matches the original, the QA pair is retained; otherwise, it's discarded.
- Core assumption: Consistency in answers across multiple generations indicates higher quality and more reliable synthetic QA pairs.
- Evidence anchors:
  - [section]: "If the model's newly generated answer matches the original synthetic answer, we retain this QA pair, as it indicates a high quality question with a consistent answer"
  - [abstract]: "We adopt cycle-consistent filtering to isolate high-quality training instances"
  - [corpus]: Weak - no direct evidence found in corpus; this is a proposed methodology
- Break condition: If GPT-4 generates inconsistent answers even for high-quality questions, the filtration process may discard useful synthetic data.

### Mechanism 3
- Claim: Augmenting low-resource datasets with synthetic data improves downstream task performance.
- Mechanism: The synthetic data expands the training set size and diversity, allowing models to learn better representations and generalize more effectively to unseen examples.
- Core assumption: More training data, even if synthetic, provides better learning signals for downstream models than smaller, purely original datasets.
- Evidence anchors:
  - [abstract]: "Experiments on three low-resource datasets... show that synthetic data augmentation improves performance for CovidQA and PolicyQA"
  - [section]: "Empirical evaluations conducted on three pertinent real-world low-resource datasets... reveal that our methodology improves the performance of BERT-based MRC on CovidQA by 23% and on PolicyQA by 5% in terms of exact match"
  - [corpus]: Weak - no direct evidence found in corpus; this is based on the paper's experimental results
- Break condition: If the synthetic data introduces noise or bias that misleads the learning process, performance may degrade instead of improve.

## Foundational Learning

- Concept: Few-shot learning in LLMs
  - Why needed here: Understanding how LLMs can learn from minimal examples is crucial for effectively priming GPT-4 to generate domain-specific synthetic data
  - Quick check question: How does in-context learning enable LLMs to perform tasks with only a few examples?

- Concept: Data augmentation techniques
  - Why needed here: Recognizing the principles behind data augmentation helps in understanding how synthetic data can improve model performance on low-resource tasks
  - Quick check question: What are the benefits and potential risks of using synthetic data to augment training datasets?

- Concept: Cycle-consistent filtering
  - Why needed here: Grasping the concept of self-consistency checks is important for implementing quality control in synthetic data generation
  - Quick check question: How does round trip consistency filtering help in identifying high-quality synthetic data?

## Architecture Onboarding

- Component map: GPT-4 context generation -> GPT-4 QA generation -> Cycle-consistent filtration -> RoBERTa-base training -> Evaluation
- Critical path:
  1. Prime GPT-4 with few-shot examples from original dataset
  2. Generate synthetic contexts using GPT-4
  3. Generate synthetic QA pairs using GPT-4
  4. Apply round trip filtration to synthetic QA pairs
  5. Combine original and filtered synthetic data
  6. Train downstream MRC model
  7. Evaluate performance
- Design tradeoffs:
  - One-shot vs. two-shot generation: One-shot provides more diversity but may have lower precision; two-shot may be more consistent but less diverse
  - Filtration vs. no filtration: Filtration improves precision but may reduce the amount of available data
  - GPT-4 vs. other LLMs: GPT-4 may offer better performance but at higher cost and potential availability issues
- Failure signatures:
  - Low diversity in generated synthetic data
  - High domain shift between synthetic and original data
  - Inconsistent answers in round trip filtration
  - No improvement or degradation in downstream task performance
- First 3 experiments:
  1. Generate synthetic data using one-shot priming and evaluate the diversity and quality of the generated contexts
  2. Implement round trip filtration and measure the percentage of synthetic QA pairs that pass the consistency check
  3. Train a downstream MRC model using the augmented dataset and compare performance metrics (EM, F1) with the baseline model trained on the original dataset only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4-based synthetic data augmentation compare to human-generated synthetic data for low-resource QA tasks?
- Basis in paper: [explicit] The paper compares GPT-4-generated synthetic data to a T5-based question generation model trained on SQUAD, but does not compare to human-generated data.
- Why unresolved: The paper only benchmarks against a T5-based model, not human-generated data. Human-generated synthetic data would provide a stronger baseline for comparison.
- What evidence would resolve it: Experiments comparing the performance of GPT-4-generated data vs human-generated synthetic data for low-resource QA tasks.

### Open Question 2
- Question: What is the optimal amount of synthetic data to generate for low-resource QA tasks? Does more data always lead to better performance?
- Basis in paper: [inferred] The paper experiments with generating 1x to 10x the size of the original training data, but does not determine an optimal amount.
- Why unresolved: The paper does not identify a point of diminishing returns or the amount of synthetic data that provides maximum performance gains.
- What evidence would resolve it: Experiments varying the amount of synthetic data generated and measuring performance to identify the optimal amount.

### Open Question 3
- Question: How does the performance of synthetic data augmentation vary across different low-resource QA datasets and domains?
- Basis in paper: [explicit] The paper experiments on three low-resource datasets (CovidQA, PolicyQA, TechQA) but does not compare performance across datasets.
- Why unresolved: The paper does not analyze how the effectiveness of synthetic data augmentation varies across different domains and dataset characteristics.
- What evidence would resolve it: Experiments comparing the performance of synthetic data augmentation across a wider variety of low-resource QA datasets and domains.

## Limitations
- Limited evaluation to three specific low-resource datasets without broader domain validation
- No ablation studies on minimum effective sample size for few-shot priming
- Cycle-consistent filtration effectiveness not empirically validated through human evaluation
- Only evaluates BERT-based MRC models, leaving other architectures untested

## Confidence
- Medium confidence in the effectiveness of one-shot generation with cycle-consistent filtration for CovidQA (23% improvement)
- Medium confidence in the effectiveness of two-shot generation for PolicyQA (5% improvement)
- Low confidence in the generalizability of these findings to other low-resource domains or MRC tasks
- Medium confidence in the round trip filtration methodology

## Next Checks
1. **Ablation study on few-shot examples**: Systematically vary the number of priming examples (0, 1, 2, 5, 10) to identify the minimum effective sample size and test whether GPT-4's few-shot capabilities truly enable domain adaptation with minimal examples.

2. **Cross-dataset generalization test**: Apply the best-performing augmentation pipeline (one-shot + cycle-consistent filtration) to additional low-resource MRC datasets from different domains to verify whether the 23% improvement on CovidQA is reproducible or dataset-specific.

3. **Quality analysis of filtered vs. unfiltered synthetic data**: Conduct human evaluation studies comparing synthetic QA pairs that pass vs. fail the cycle-consistent filter to empirically validate whether the filtration process actually improves data quality, or if it's merely removing a different type of noise.