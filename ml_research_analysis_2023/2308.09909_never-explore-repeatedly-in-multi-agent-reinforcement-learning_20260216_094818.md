---
ver: rpa2
title: Never Explore Repeatedly in Multi-Agent Reinforcement Learning
arxiv_id: '2308.09909'
source_url: https://arxiv.org/abs/2308.09909
tags:
- exploration
- learning
- multi-agent
- agents
- revisitation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of inefficient exploration in multi-agent
  reinforcement learning caused by the "revisitation" issue, where agents repeatedly
  explore previously visited areas due to limited expressive capability of neural
  statistics approximators. The authors propose a dynamic reward scaling approach,
  named NER, that modulates intrinsic rewards based on the similarity of joint observation
  distributions in a replay buffer.
---

# Never Explore Repeatedly in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2308.09909
- Source URL: https://arxiv.org/abs/2308.09909
- Reference count: 16
- Key outcome: NER reduces inefficient exploration in multi-agent RL by dynamically scaling intrinsic rewards based on joint observation similarity, achieving up to 100% win rates in sparse reward tasks.

## Executive Summary
This paper addresses the problem of inefficient exploration in multi-agent reinforcement learning caused by agents repeatedly revisiting previously explored areas. The authors identify that this "revisitation" issue stems from the limited expressive capability of neural statistics approximators used to estimate intrinsic rewards. Their proposed solution, NER (Never Explore Repeatedly), dynamically scales intrinsic rewards based on the similarity of joint observations to those stored in a replay buffer. This approach stabilizes rewards in explored regions while encouraging exploration of novel areas. Experiments on Google Research Football and StarCraft II micromanagement tasks demonstrate that NER significantly outperforms baseline methods in sparse reward settings.

## Method Summary
NER builds upon existing intrinsic motivation approaches like CDS by adding a dynamic reward scaling mechanism. The method maintains a replay buffer storing joint observations from previous episodes. For each new transition, NER computes the similarity between the current joint observation and its k-nearest neighbors in the replay buffer. This similarity score is used to calculate a scaling factor α that modulates the intrinsic reward - small α in well-explored regions reduces reward magnitude, while large α in novel regions encourages further exploration. The scaled intrinsic reward is then combined with environmental rewards for Q-learning updates using a QMIX architecture. The approach aims to prevent the exploration boundary from shrinking by continuously encouraging agents to venture into less familiar joint observation regions.

## Key Results
- NER achieved up to 100% win rates in sparse reward settings on Google Research Football scenarios.
- The method demonstrated superior exploration efficiency compared to baselines like CDS and MA VEN.
- NER showed consistent performance improvements across different SMAC maps and GRF scenarios in sparse reward conditions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intrinsic reward instability due to neural approximator generalization limits causes agents to forget previously explored regions, leading to redundant exploration cycles.
- Mechanism: Variational approximators estimate intrinsic rewards based on joint observation distributions. When agents explore new regions, estimation error increases for previously visited regions, causing high intrinsic rewards to decay and triggering revisitation.
- Core assumption: Neural approximators have limited generalization ability across disjoint joint observation sub-spaces.
- Evidence anchors:
  - [abstract] "While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators."
  - [section] "We posit that the scaling factor α should be minimized in well-explored sub-spaces. This reduction aims to counterbalance the pronounced variance of rI, which arises due to the increased error of its neural-network-based variational approximators after exploring other sub-spaces."

### Mechanism 2
- Claim: Dynamic reward scaling based on similarity to stored joint observations stabilizes intrinsic rewards in explored regions and promotes exploration of novel regions.
- Mechanism: NER computes a scaling factor α inversely proportional to the similarity of the current joint observation to k-nearest neighbors in a replay buffer. High similarity reduces intrinsic reward, while low similarity encourages exploration.
- Core assumption: The replay buffer contains representative joint observations from explored regions, and similarity correlates with visitation frequency.
- Evidence anchors:
  - [section] "Specifically, for the intrinsic reward associated with the transition (s, o, a, s′, re, o′), we adjust α according to the similarity of o to joint observations in a replay buffer D which stores joint observation every 10 episodes during learning without deletion."

### Mechanism 3
- Claim: NER prevents the exploration boundary from shrinking by continuously encouraging exploration of less familiar joint observation regions.
- Mechanism: Dynamic scaling ensures agents are consistently incentivized to explore novel regions. Large α in unfamiliar regions counteracts the tendency of variational approximators to underestimate intrinsic rewards in these areas.
- Core assumption: The intrinsic reward landscape, when properly scaled, can guide agents to unexplored regions effectively.
- Evidence anchors:
  - [section] "Our proposed solution, named NER (Never Explore Repeatedly), effectively reduces revisitation and fosters continuous exploration."

## Foundational Learning

- Concept: Variational inference and neural approximators in reinforcement learning.
  - Why needed here: Understanding how variational approximators estimate intrinsic rewards and their limitations is crucial for grasping the revisitation issue and NER's solution.
  - Quick check question: How do variational approximators estimate the posterior distribution of observations in multi-agent settings, and what are the main sources of estimation error?

- Concept: Multi-agent exploration and intrinsic motivation.
  - Why needed here: Familiarity with different intrinsic motivation approaches (e.g., CDS, MA VEN) and their role in multi-agent exploration is essential for contextualizing NER's contribution.
  - Quick check question: What are the key differences between individual, pairwise, and collective intrinsic motivation approaches in multi-agent exploration?

- Concept: Jensen-Shannon distance and distribution similarity.
  - Why needed here: Understanding how to measure the similarity between joint observation distributions is important for quantifying revisitation and evaluating NER's effectiveness.
  - Quick check question: How does the Jensen-Shannon distance compare to other divergence measures (e.g., KL divergence) in capturing the similarity between probability distributions?

## Architecture Onboarding

- Component map:
  - Local Q-networks (partially shared module) -> Trajectory encoding network (shared) -> Mixing network (QMIX-style) -> QMIX loss
  - Replay buffer stores joint observations
  - NER adjustment module computes scaling factor α
  - Intrinsic reward estimator (CDS) estimates intrinsic rewards

- Critical path:
  1. Agents interact with environment, generating transitions (s, o, a, s′, re, o′)
  2. Transitions are stored in replay buffer
  3. NER adjustment module computes scaling factor α based on current joint observation similarity to k-nearest neighbors in replay buffer
  4. Intrinsic reward is scaled by α and added to environmental reward
  5. Q-networks are updated using combined reward and QMIX loss function

- Design tradeoffs:
  - Replay buffer introduces memory overhead but provides stable reference for scaling
  - k (number of nearest neighbors) affects scaling granularity vs responsiveness
  - Kernel function choice determines similarity computation behavior

- Failure signatures:
  - Consistently small α leads to insufficient exploration and suboptimal performance
  - Consistently large α causes excessive exploration and sample waste
  - Non-representative replay buffer causes inaccurate scaling and unstable exploration

- First 3 experiments:
  1. Implement NER on 6×12 maze grid-world, comparing exploration trajectories and rewards to CDS and MA VEN baselines
  2. Evaluate NER on Google Research Football with win rate and sample efficiency metrics against baselines
  3. Conduct ablation study varying hyperparameters (k, kernel function) to assess impact on NER performance

## Open Questions the Paper Calls Out

- Question: How does the proposed NER method scale with increasing number of agents in terms of computational complexity and memory requirements?
- Question: Can the proposed NER method be extended to handle non-stationary environments where the dynamics of the environment change over time?
- Question: How does the choice of the kernel function K(x, y) and the number of nearest neighbors k affect the performance of NER?

## Limitations

- Computational overhead from maintaining replay buffer and computing k-nearest neighbors is not thoroughly characterized
- Choice of k=10 appears somewhat arbitrary without sensitivity analysis
- Limited quantitative evidence directly linking neural approximator error to revisitation patterns

## Confidence

- Revisitation mechanism due to neural approximator limitations: Medium
- Dynamic reward scaling effectiveness: High
- Experimental results validity: High
- Computational overhead characterization: Low

## Next Checks

1. Conduct ablation studies varying k (number of nearest neighbors) to assess sensitivity and optimal values for different environment complexities.
2. Implement direct visualization of exploration frontiers over training to empirically verify that NER prevents boundary shrinkage compared to baselines.
3. Measure and report the computational overhead introduced by NER's replay buffer and similarity computations relative to baseline methods.