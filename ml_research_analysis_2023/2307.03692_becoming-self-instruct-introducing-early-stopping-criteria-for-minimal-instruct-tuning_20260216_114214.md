---
ver: rpa2
title: 'Becoming self-instruct: introducing early stopping criteria for minimal instruct
  tuning'
arxiv_id: '2307.03692'
source_url: https://arxiv.org/abs/2307.03692
tags:
- instruction
- instruct
- response
- instructions
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Instruction Following Score (IFS) is introduced as a metric
  to detect language models' ability to follow instructions. IFS measures the ratio
  of "answer-like" responses to "continuation-like" responses on a predefined set
  of instructions.
---

# Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning

## Quick Facts
- arXiv ID: 2307.03692
- Source URL: https://arxiv.org/abs/2307.03692
- Reference count: 6
- Primary result: Introduces IFS metric to detect instruction-following ability and demonstrates early stopping for minimal instruction tuning

## Executive Summary
This paper introduces the Instruction Following Score (IFS) as a novel metric to quantify language models' ability to follow instructions. IFS measures the ratio of "answer-like" responses to "continuation-like" responses on instruction datasets. The authors demonstrate that IFS effectively distinguishes between base and instruction-tuned models, and can be used as an early stopping criterion during instruction tuning. The study reveals that models learn to follow instructions early in training (format-infusion phase), while further tuning primarily causes semantic changes in the base model. The paper proposes minimal instruction tuning interfaces and decomposes the instruction tuning process into IFS and semantic factors.

## Method Summary
The method involves training a binary classifier to distinguish "answer-like" from "continuation-like" responses using the OpenAssistant dataset. This classifier is then used to compute the IFS metric on instruction-tuned models. The study employs supervised fine-tuning (SFT) on LLaMA 7B and 13B models using the gpt4all v1.3-groovy dataset with a modified Alpaca prompt. IFS is computed at each training step to identify the format-infusion phase where instruction-following ability plateaus. The ObjecQA benchmark is introduced to quantify semantic shifts during tuning by measuring changes in the objectivity of model predictions.

## Key Results
- IFS effectively distinguishes between base and instruction-tuned models by measuring response tone
- Models learn to follow instructions early in training (around 8k examples), with IFS plateauing at 0.9-0.95
- Further fine-tuning beyond the format-infusion phase leads to semantic changes in base model knowledge
- ObjecQA quantifies semantic shifts by measuring changes in model objectivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Instruction Following Score (IFS) can effectively distinguish between base and instruction-tuned models by measuring the ratio of "answer-like" to "continuation-like" responses.
- Mechanism: The IFS metric quantifies the extent to which a model's responses follow the conversational tone expected from instruction-following models. It does this by classifying responses as either "answer-like" (label 1) or "continuation-like" (label 0) using a binary classifier trained on a dataset of responses labeled accordingly.
- Core assumption: The binary classifier accurately captures the distinction between conversational and non-conversational response tones.
- Evidence anchors:
  - [abstract]: "IFS measures the ratio of 'answer-like' responses to 'continuation-like' responses on a predefined set of instructions."
  - [section]: "We define Instruction Following Score (IFS) as a ratio of all responses classified as 'answer-like' (label 1) to all responses obtained by prompting the instructions dataset."
  - [corpus]: Found 25 related papers. Weak evidence for direct comparison to IFS metric.
- Break condition: If the binary classifier's accuracy drops significantly or the response tone distinction becomes less clear, the IFS metric's effectiveness would be compromised.

### Mechanism 2
- Claim: IFS can be used as an early stopping criterion for instruction tuning, indicating when models have learned to follow instructions without further semantic changes.
- Mechanism: By monitoring IFS during the instruction tuning process, one can identify the point at which the model's ability to follow instructions plateaus. Further training beyond this point may lead to unwanted semantic changes in the base model.
- Core assumption: The plateau in IFS correlates with the acquisition of instruction-following capability and precedes significant semantic shifts.
- Evidence anchors:
  - [abstract]: "IFS is also used as an early stopping criterion for instruction tuning, showing that models learn to follow instructions early in the training process."
  - [section]: "We observe that the models' instruction-tuning capabilities stabilize on level 0.9-0.95 after seeing approximately 8k examples... We will refer to this training phase as the 'format-infusion' phase."
  - [corpus]: Found 25 related papers. No direct evidence for early stopping criteria in instruction tuning.
- Break condition: If the relationship between IFS plateau and semantic stability does not hold consistently across different models or datasets, the early stopping criterion would lose its reliability.

### Mechanism 3
- Claim: The ObjecQA metric can quantify semantic shifts in model predictions, specifically changes in objectivity, during instruction tuning.
- Mechanism: ObjecQA consists of questions that involve subjective choices or preferences. By evaluating the model's responses to these questions, one can measure changes in the model's tendency to provide objective answers versus subjective opinions.
- Core assumption: Changes in the model's responses to ObjecQA questions reflect meaningful semantic shifts in the model's knowledge and understanding.
- Evidence anchors:
  - [abstract]: "We propose a supplementary metric called ObjecQA. This auxiliary metric quantifies the objectivity of a model's predictions."
  - [section]: "We propose an ObjecQA benchmark that consists of 100 questions that involve subjective choices or preferences. A highly scoring model in ObjecQA should present a range of possibilities or avoid direct answers."
  - [corpus]: Found 25 related papers. No direct evidence for ObjecQA metric or its effectiveness.
- Break condition: If the ObjecQA metric does not consistently capture meaningful semantic shifts or if the relationship between ObjecQA scores and semantic changes is not clear, its utility as a supplementary metric would be limited.

## Foundational Learning

- Concept: Binary Classification
  - Why needed here: The IFS metric relies on a binary classifier to distinguish between "answer-like" and "continuation-like" responses.
  - Quick check question: Can you explain how a binary classifier works and why it's suitable for this task?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: The paper uses SFT to tune base models for instruction following, and IFS is used to monitor the process.
  - Quick check question: What is the difference between SFT and other fine-tuning methods, and why is it used here?

- Concept: Early Stopping
  - Why needed here: IFS is proposed as an early stopping criterion to prevent over-tuning and unwanted semantic changes.
  - Quick check question: How does early stopping work, and what are its benefits and potential drawbacks?

## Architecture Onboarding

- Component map:
  Response Tone Classifier -> IFS Calculator -> ObjecQA Evaluator -> SFT Pipeline

- Critical path:
  1. Train response tone classifier on labeled response dataset
  2. Compute IFS for base and instruction-tuned models
  3. Use IFS as early stopping criterion during SFT
  4. Monitor ObjecQA scores to detect semantic shifts

- Design tradeoffs:
  - Accuracy vs. efficiency: More complex classifiers may improve IFS accuracy but increase computation time
  - Granularity vs. simplicity: Finer-grained classification of responses may provide more insights but complicate the IFS calculation
  - Dataset size vs. performance: Larger datasets for training the response tone classifier may improve its accuracy but increase data collection and annotation efforts

- Failure signatures:
  - Low IFS scores for instruction-tuned models: Indicates poor instruction-following ability
  - Inconsistent IFS scores across different model sizes or datasets: Suggests the metric may not generalize well
  - ObjecQA scores not reflecting semantic changes: Indicates the metric may not capture meaningful shifts in model knowledge

- First 3 experiments:
  1. Evaluate IFS for a range of publicly available base and instruction-tuned models to validate its ability to distinguish between the two classes
  2. Monitor IFS during SFT of a base model to identify the point of instruction-following capability plateau
  3. Assess ObjecQA scores before and after SFT to quantify semantic shifts in model predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal stopping point for minimal instruction tuning to balance instruction following ability with preservation of base model semantics?
- Basis in paper: [explicit] The paper shows that models learn to follow instructions early in the training process, with IFS plateauing around 8k examples, while semantic changes (measured by ObjecQA) continue to increase. The paper suggests that early stopping could minimize semantic shifts.
- Why unresolved: The paper does not provide a specific quantitative criterion for determining the optimal stopping point. It only shows that IFS plateaus and semantic changes continue, but does not establish the trade-off between instruction following ability and semantic preservation.
- What evidence would resolve it: A systematic study varying the stopping point and measuring both IFS and semantic metrics (like ObjecQA) on downstream tasks would identify the optimal balance point.

### Open Question 2
- Question: How do different instruction tuning datasets affect the IFS and semantic shift patterns observed in SFT?
- Basis in paper: [inferred] The paper uses a single dataset (gpt4all v1.3-groovy) for SFT experiments. The choice of dataset could influence the relationship between IFS and semantic shifts.
- Why unresolved: The paper does not explore how different datasets might affect the learning dynamics and the relationship between instruction following and semantic changes.
- What evidence would resolve it: Repeating the SFT experiments with multiple diverse instruction tuning datasets and comparing IFS and semantic shift patterns across datasets would reveal the dataset dependency.

### Open Question 3
- Question: Does the size of the base model affect the relationship between IFS plateau and semantic shift in instruction tuning?
- Basis in paper: [explicit] The paper notes that larger models (13B vs 7B LLaMA) reach a 0.9 IFS level relatively faster, but does not explore how model size affects the relationship between IFS plateau and semantic shifts.
- Why unresolved: The paper only compares two model sizes and does not investigate whether the observed disjoint phases of format-infusion and knowledge-infusion hold across a broader range of model sizes.
- What evidence would resolve it: Conducting SFT experiments with models of varying sizes (e.g., 1B, 3B, 33B, 65B) and analyzing the IFS and semantic shift patterns would reveal size-dependent relationships.

### Open Question 4
- Question: How does the IFS metric generalize to other language tasks beyond instruction following?
- Basis in paper: [inferred] The IFS metric is specifically designed to measure instruction following ability. Its applicability to other language tasks that require different forms of alignment is not explored.
- Why unresolved: The paper focuses solely on instruction following and does not investigate whether the IFS framework can be adapted to measure alignment for other language tasks.
- What evidence would resolve it: Applying the IFS framework to other language tasks (e.g., summarization, translation, creative writing) and validating its effectiveness in detecting alignment would establish its generalizability.

## Limitations

- The IFS metric's generalizability beyond the studied model families (LLaMA) and instruction datasets remains uncertain
- The ObjecQA benchmark's ability to capture meaningful semantic shifts has not been rigorously validated against established semantic drift metrics
- The study's scope is limited to LLaMA models, raising questions about external validity across different model architectures

## Confidence

- IFS as distinguisher: High - demonstrated on multiple base and instruction-tuned models with consistent results
- IFS as early stopping criterion: Medium - evidence from single training run; relationship between IFS plateau and semantic stability needs broader validation
- ObjecQA semantic detection: Low - metric introduced but effectiveness not rigorously validated against ground truth semantic shifts

## Next Checks

1. **Cross-dataset validation**: Apply IFS to instruction-tuned models trained on different datasets (e.g., Alpaca, OASST1) to verify the metric's robustness and the early stopping criterion's generalizability across instruction sources.

2. **Semantic shift correlation**: Conduct controlled experiments introducing known semantic changes (e.g., through targeted fine-tuning) and measure whether ObjecQA scores accurately reflect these changes compared to human evaluation or established semantic similarity metrics.

3. **Classifier transferability test**: Evaluate the binary response classifier on out-of-domain instructions (non-English, technical domains) to assess whether the answer-like/continuation-like distinction holds across linguistic and topical variations.