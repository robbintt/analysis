---
ver: rpa2
title: Channel-Wise Contrastive Learning for Learning with Noisy Labels
arxiv_id: '2308.06952'
source_url: https://arxiv.org/abs/2308.06952
tags:
- learning
- contrastive
- label
- noise
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning with noisy labels
  (LNL) in deep neural networks, where inaccurate labels can cause overfitting and
  poor generalization. The authors propose channel-wise contrastive learning (CWCL),
  which leverages contrastive learning across feature channels to distinguish authentic
  label information from noise.
---

# Channel-Wise Contrastive Learning for Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2308.06952
- Source URL: https://arxiv.org/abs/2308.06952
- Reference count: 40
- Key outcome: CWCL achieves up to 20 percentage points accuracy gains over state-of-the-art methods in high-noise settings (80% symmetric noise on CIFAR-100).

## Executive Summary
This paper addresses learning with noisy labels (LNL) by proposing channel-wise contrastive learning (CWCL), which performs contrastive learning across feature channels rather than instances. The method leverages the observation that individual channels in deep feature maps encode distinct visual patterns, allowing CWCL to extract more nuanced and noise-resistant features aligned with true labels. CWCL operates in two stages: first extracting clean label features and identifying high-confidence samples, then progressively fine-tuning the model using these samples with supervised contrastive loss. Experiments on CIFAR-10, CIFAR-100, Animal-10N, and Clothing-1M demonstrate significant improvements over state-of-the-art methods across diverse noise types and real-world datasets.

## Method Summary
CWCL implements channel-wise contrastive learning by computing contrastive loss across feature channels from L intermediate layers of ResNet18, combined with standard cross-entropy loss. The method first trains the model using this combined loss to extract discriminative features and identify high-confidence samples based on prediction probabilities. In the second stage, it performs progressive fine-tuning on these confident samples using supervised contrastive loss for 200 additional epochs. The approach uses class-balanced sampling to handle class imbalance and operates with batch size 128, SGD optimizer, and 300 total epochs.

## Key Results
- Achieves 20 percentage points higher accuracy than state-of-the-art methods on CIFAR-100 with 80% symmetric noise
- Outperforms existing methods by 5-10 percentage points on Animal-10N real-world noisy dataset
- Shows consistent improvements across both symmetric and asymmetric noise types on CIFAR-10 and CIFAR-100
- Maintains strong performance with small memory footprint and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-wise contrastive learning (CWCL) extracts clean label information from noisy training data by learning discriminative features from individual feature channels.
- Mechanism: CWCL performs contrastive learning across feature channels rather than instances. By pulling positive pairs (same channel under different augmentations) closer and pushing negative pairs (different channels) apart, it amplifies channel diversity and enhances the model's ability to mine fine-grained discriminative information from specific channels.
- Core assumption: Individual channels in deep feature maps encode distinct visual patterns and fine-grained discriminative information that align with authentic labels.
- Evidence anchors:
  - [abstract] "Unlike conventional instance-wise contrastive learning (IWCL), CWCL tends to yield more nuanced and resilient features aligned with the authentic labels."
  - [section] "Delving deeper into the anatomy of a DNN, it becomes apparent that the channels of its deep features often resonate with distinct visual patterns [33, 46, 48]."
  - [corpus] Weak - no direct evidence in neighbors about channel-wise contrastive learning.
- Break condition: If channels do not contain meaningful discriminative information or if noise corrupts all channels equally, CWCL would fail to extract clean label features.

### Mechanism 2
- Claim: Progressive confident-sample fine-tuning further refines the classifier by iteratively training on high-confidence samples identified in previous epochs.
- Mechanism: After initial CWCL training, the model identifies confident samples based on high prediction probabilities. These samples are treated as clean and used to fine-tune the model using supervised contrastive loss. This process repeats, progressively improving sample quality and model robustness.
- Core assumption: High-confidence predictions on noisy data often correspond to clean samples, and these samples can be iteratively refined to improve model performance.
- Evidence anchors:
  - [abstract] "Our strategy is twofold: firstly, using CWCL to extract pertinent features to identify cleanly labeled samples, and secondly, progressively fine-tuning using these samples."
  - [section] "Confident samples are characterized by high prediction probabilities concerning their associated labels... On gathering these high-confidence samples, the classifier is then trained by viewing them as pristine, noise-free data."
  - [corpus] Weak - no direct evidence in neighbors about progressive confident-sample fine-tuning.
- Break condition: If confident samples are predominantly noisy or if the iterative process amplifies noise rather than cleaning it, performance would degrade.

### Mechanism 3
- Claim: Combining CWCL with supervised contrastive loss in the fine-tuning stage maintains feature diversity while improving classification accuracy.
- Mechanism: The supervised contrastive loss encourages features of the same class to be close while keeping different classes apart. When applied to confident samples, this maintains the channel diversity learned by CWCL while providing class-specific supervision to improve accuracy.
- Core assumption: Supervised contrastive loss can effectively refine features learned by CWCL without collapsing the channel diversity that makes them noise-resilient.
- Evidence anchors:
  - [abstract] "This stage is orchestrated through a synergistic training paradigm employing both CE and supervised contrastive loss [23]."
  - [section] "Since we consider confident samples as clean data, we use supervised contrastive loss ( LSupCon ) [23] instead of LCW CL."
  - [corpus] Weak - no direct evidence in neighbors about combining contrastive losses for LNL.
- Break condition: If supervised contrastive loss collapses feature diversity or if it overfits to the limited confident samples, the method would fail.

## Foundational Learning

- Concept: Contrastive learning fundamentals (positive/negative pairs, InfoNCE loss)
  - Why needed here: CWCL builds directly on contrastive learning principles but applies them at the channel level instead of instance level
  - Quick check question: What is the difference between instance-wise and channel-wise contrastive learning in terms of positive/negative pair construction?

- Concept: Deep feature channel specialization
  - Why needed here: The method assumes that individual channels encode distinct visual patterns and discriminative information
  - Quick check question: How do feature channels in deep networks typically specialize to encode different visual patterns?

- Concept: Label noise types and their effects on learning
  - Why needed here: Understanding symmetric vs asymmetric noise and their impact on model memorization is crucial for applying CWCL effectively
  - Quick check question: How does symmetric label noise differ from asymmetric label noise in terms of how it affects model training?

## Architecture Onboarding

- Component map: Input images -> ResNet18 backbone -> Extract features from L intermediate layers -> Compute CWCL loss across channels + CE loss -> Update weights -> Generate predictions -> Select confident samples -> Fine-tune with supervised contrastive loss

- Critical path: Forward pass through network → extract channel features → compute CWCL loss → compute CE loss → backward pass → update weights → generate predictions → select confident samples → fine-tune with supervised contrastive loss

- Design tradeoffs: Channel-wise contrastive learning provides better noise resilience but requires careful tuning of the balance between CWCL and CE losses. Progressive fine-tuning improves accuracy but adds complexity and training time.

- Failure signatures: Poor performance on clean data suggests overfitting to noise, while degraded performance on noisy data indicates insufficient noise robustness. Large performance gaps between training and validation suggest memorization.

- First 3 experiments:
  1. Validate that CWCL improves feature diversity by comparing channel activation patterns with and without CWCL on a small noisy dataset.
  2. Test confident sample selection by measuring the noise rate in samples selected at different confidence thresholds.
  3. Evaluate the impact of progressive fine-tuning by comparing model performance after each iteration of the fine-tuning stage.

## Open Questions the Paper Calls Out
- Question: How does CWCL perform when integrated with other existing label noise mitigation methods beyond CTRR?
  - Basis in paper: [explicit] The paper states "CWCL promises to be a potent tool, either standalone or in conjunction with other techniques, providing an edge in crafting more resilient and accurate models."
  - Why unresolved: The paper only compares CWCL to baseline methods individually, without exploring synergistic combinations with other techniques.
  - What evidence would resolve it: Empirical results showing accuracy improvements when CWCL is combined with methods like Co-teaching, SL, or APL.

- Question: What is the optimal number of intermediate layers to extract representations from when applying CWCL?
  - Basis in paper: [explicit] The paper states "We extract representations from L intermediate layers of the network (e.g. layer{1, ... ,4} of ResNet18) and apply each to LCW CL" but does not provide ablation studies on the optimal value of L.
  - Why unresolved: The paper does not investigate how performance varies with different numbers of layers.
  - What evidence would resolve it: Comparative experiments showing accuracy across different values of L (e.g., 1, 2, 3, 4, 5 layers) on benchmark datasets.

## Limitations
- The assumption that individual channels encode meaningful discriminative information aligned with true labels needs empirical validation
- The reliability of confident sample selection as a mechanism for identifying truly clean labels in high-noise regimes is uncertain
- Limited exploration of computational overhead compared to instance-wise contrastive learning methods

## Confidence
- High: The overall two-stage framework design (CWCL + progressive fine-tuning) and its experimental implementation on benchmark datasets
- Medium: The effectiveness of channel-wise contrastive learning in extracting noise-resistant features from individual channels
- Low: The reliability of confident sample selection as a mechanism for identifying truly clean labels in high-noise regimes

## Next Checks
1. **Channel Specialization Validation**: Measure and compare feature channel activation patterns with and without CWCL on a small noisy dataset to empirically verify that CWCL enhances channel diversity and aligns channels with true labels.

2. **Confident Sample Noise Analysis**: Quantify the actual noise rate in samples selected at different confidence thresholds across multiple noise levels to determine if high-confidence predictions reliably indicate clean samples.

3. **Progressive Fine-tuning Stability**: Track model performance and confident sample quality across each iteration of the fine-tuning stage to verify that the iterative process consistently improves rather than degrades sample quality.