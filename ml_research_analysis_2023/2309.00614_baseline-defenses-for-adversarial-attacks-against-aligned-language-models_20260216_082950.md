---
ver: rpa2
title: Baseline Defenses for Adversarial Attacks Against Aligned Language Models
arxiv_id: '2309.00614'
source_url: https://arxiv.org/abs/2309.00614
tags:
- adversarial
- attack
- arxiv
- attacks
- defenses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates baseline defenses against adversarial attacks
  on aligned language models, focusing on detection (perplexity filtering), preprocessing
  (paraphrasing and retokenization), and adversarial training. The authors find that
  perplexity filtering and paraphrasing are effective even in white-box settings,
  with the attack optimizer struggling to minimize both perplexity and adversarial
  objectives simultaneously.
---

# Baseline Defenses for Adversarial Attacks Against Aligned Language Models

## Quick Facts
- arXiv ID: 2309.00614
- Source URL: https://arxiv.org/abs/2309.00614
- Authors: 
- Reference count: 26
- Primary result: Perplexity filtering and paraphrasing effectively defend against LLM adversarial attacks even in white-box settings

## Executive Summary
This paper evaluates baseline defenses against adversarial attacks on aligned language models, focusing on detection (perplexity filtering), preprocessing (paraphrasing and retokenization), and adversarial training. The authors find that perplexity filtering and paraphrasing are effective even in white-box settings, with the attack optimizer struggling to minimize both perplexity and adversarial objectives simultaneously. Retokenization using BPE-dropout degrades attack success rates. Adversarial training showed limited success due to the high computational cost of generating adversarial examples for LLMs. The paper highlights the unique challenges of LLM security compared to computer vision, particularly the higher computational complexity of attacks and the potential effectiveness of filtering and preprocessing defenses in this domain.

## Method Summary
The paper evaluates three categories of defenses against adversarial attacks on aligned language models: detection (perplexity filtering), preprocessing (paraphrasing and retokenization), and adversarial training. Attacks are generated using the Greedy Coordinate Gradient (GCG) optimizer from Zou et al. (2023) on models including Vicuna-7B, Guanaco-7B, Falcon-7B-Instruct, ChatGLM-6B, and MPT-7B-Chat. Perplexity filtering uses thresholds based on AdvBench prompts to detect adversarial inputs. Paraphrasing employs ChatGPT to rewrite prompts, while retokenization uses BPE-dropout with varying dropout rates. The evaluation measures Attack Success Rate (ASR), detection rates, and performance impact on benign inputs using AlpacaEval.

## Key Results
- Perplexity filtering achieves 100% detection of adversarial prompts while maintaining near-zero false positives on benign inputs
- Paraphrasing reduces attack success rates to near-baseline levels across tested models
- BPE-dropout retokenization with 0.4 dropout rate degrades attack success rates for Vicuna and Guanaco
- Adversarial training showed limited success due to computational constraints of generating adversarial examples for LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity filtering effectively detects adversarial attacks on LLMs even in white-box settings.
- Mechanism: Adversarial attacks on LLMs typically generate text that is unnatural or gibberish, resulting in high perplexity scores. By setting a threshold based on benign prompts, perplexity filtering can flag suspicious inputs.
- Core assumption: Attack strings have significantly higher perplexity than benign prompts due to the discrete nature of text optimization.
- Evidence anchors:
  - [abstract]: "We find that perplexity filtering and paraphrasing are promising, even if simple, as we discover that evading a perplexity-based detection system could prove challenging, even in a white-box scenario."
  - [section 4.1]: "From Table 1, we see that both perplexity and windowed perplexity easily detect all adversarial prompts generated by the optimizer, while letting all prompts in the AdvBench dataset through."
  - [corpus]: Weak evidence - only general references to perplexity-based defenses in other domains, not specifically LLM-focused.
- Break condition: If an adaptive attack can optimize for both low perplexity and adversarial objectives simultaneously, the defense would fail.

### Mechanism 2
- Claim: Retokenization using BPE-dropout degrades attack success rates by disrupting adversarial token combinations.
- Mechanism: BPE-dropout randomly drops some BPE merges during tokenization, resulting in more tokens and potentially breaking up token sequences that the attack relies on.
- Core assumption: Adversarial attacks exploit specific token combinations, and breaking these up disrupts the attack's effectiveness.
- Evidence anchors:
  - [section 4.3]: "To break up the text, we use BPE-dropout... which drops a random p% of the BPE merges during the tokenization of the text, resulting in a randomized tokenization with more tokens than a standard representation."
  - [section 4.3]: "From Figure 5 (left), we can see that the BPE-dropout data augmentation does degrade the attack success rate with the optimal dropout rate of 0.4 for Vicuna and Guanaco..."
  - [corpus]: No direct evidence found in corpus for BPE-dropout specifically applied to adversarial defense in LLMs.
- Break condition: If an adaptive attack optimizes using only characters with spaces to construct the attack, the defense would fail.

### Mechanism 3
- Claim: Paraphrasing can effectively defend against adversarial attacks by rewriting adversarial prompts into benign ones.
- Mechanism: A generative model paraphrases the input, potentially preserving benign instructions while failing to reproduce adversarial sequences accurately.
- Core assumption: Paraphrasing models are less likely to reproduce the exact adversarial token sequences while maintaining the semantic meaning of benign prompts.
- Evidence anchors:
  - [section 4.2]: "We evaluate this defense against attacks on the two models that the adversarial attacks were trained on... For paraphrasing, we follow the protocol described in Kirchenbauer et al. (2023) and use ChatGPT to paraphrase the prompt..."
  - [section 4.2]: "From Table 3, we present the Attack Success Rate when implementing the paraphrase defense... Vicuna and Guanaco return to near baseline success rates."
  - [corpus]: Weak evidence - only general references to paraphrase-based defenses in other domains, not specifically LLM-focused.
- Break condition: If an adaptive attack can find a paraphrase prompt that causes the paraphraser to return the adversarial prompt, the defense would fail.

## Foundational Learning

- Concept: Perplexity as a measure of text naturalness
  - Why needed here: Understanding perplexity is crucial for grasping how perplexity filtering detects adversarial attacks.
  - Quick check question: How does perplexity differ between natural text and adversarial text in the context of LLM attacks?

- Concept: Tokenization and subword units
  - Why needed here: BPE-dropout and retokenization defenses rely on manipulating tokenization, so understanding these concepts is essential.
  - Quick check question: How does BPE-dropout differ from standard BPE tokenization, and why might this disrupt adversarial attacks?

- Concept: Paraphrasing and semantic preservation
  - Why needed here: The effectiveness of paraphrasing as a defense depends on its ability to preserve benign semantics while disrupting adversarial sequences.
  - Quick check question: How might a paraphrasing model fail to reproduce an adversarial sequence while preserving the meaning of a benign prompt?

## Architecture Onboarding

- Component map: Input -> Preprocessing (filtering/rewriting) -> LLM -> Output
- Critical path: Input → Preprocessing (filtering/rewriting) → LLM → Output
  - Key decisions: Threshold setting for perplexity filter, choice of paraphrasing model, BPE-dropout rate
- Design tradeoffs:
  - Perplexity filtering: High false positive rate vs. effective attack detection
  - Paraphrasing: Performance degradation vs. attack success reduction
  - Retokenization: Potential loss of coherence vs. attack disruption
- Failure signatures:
  - High false positive rate in perplexity filtering
  - Significant performance degradation after paraphrasing or retokenization
  - Adaptive attacks successfully bypassing any of the defenses
- First 3 experiments:
  1. Implement perplexity filtering with varying thresholds and measure attack detection rate vs. false positive rate.
  2. Apply paraphrasing to a set of adversarial and benign prompts, measuring attack success rate and performance impact.
  3. Test retokenization with different BPE-dropout rates on adversarial prompts, measuring attack success rate and coherence of outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific types of adversarial training methods could be developed to improve the robustness of large language models against adversarial attacks?
- Basis in paper: [inferred] The paper mentions that adversarial training methods from computer vision are not directly transferable to the LLM domain due to the high computational cost of generating adversarial examples for LLMs.
- Why unresolved: The paper only explores a basic form of adversarial training and finds it ineffective due to the computational challenges involved. It suggests that stronger, more efficient optimizers are required for such tasks.
- What evidence would resolve it: Development and testing of novel adversarial training methods specifically designed for LLMs, with demonstrated improvements in robustness against adversarial attacks.

### Open Question 2
- Question: How can we theoretically bound or certify the minimal computational budget required for an attack against a given (gray-box) defense in the LLM domain?
- Basis in paper: [inferred] The paper suggests that computational cost is a major factor in defining a realistic threat model for LLMs, and that defenses increasing this budget have value. It also mentions the possibility of guaranteeing a level of safety based on computational complexity.
- Why unresolved: The paper does not provide a theoretical framework or method for bounding the computational budget required for attacks against specific defenses.
- What evidence would resolve it: Development of a theoretical framework that can estimate or certify the minimal computational resources needed for an attacker to successfully breach a given defense mechanism in the LLM domain.

### Open Question 3
- Question: Can discrete text optimizers be developed that significantly improve the effectiveness of adversarial attacks on LLMs, potentially returning LLM security to a state more like that of computer vision?
- Basis in paper: [explicit] The paper concludes by suggesting that the effectiveness of current discrete text optimizers is limited, and that the development of more powerful optimizers could change the landscape of LLM security.
- Why unresolved: The paper demonstrates that current optimizers struggle with complex adversarial objectives, and it remains an open question whether more effective optimizers can be developed.
- What evidence would resolve it: Creation and validation of new discrete text optimizers that can generate more effective adversarial attacks on LLMs, potentially matching the efficiency and success rates seen in computer vision attacks.

## Limitations

- The evaluation focuses on baseline defenses with limited scope, without exploring more sophisticated adaptive attacks
- The adversarial training experiments are particularly limited due to the high computational cost, with only one model trained on a restricted dataset
- The BPE-dropout defense has an identified vulnerability to character-level attacks that could bypass the mechanism entirely

## Confidence

**High Confidence:** The core observation that perplexity filtering effectively detects adversarial attacks on LLMs is well-supported by empirical results showing 100% detection rates on tested adversarial prompts while maintaining near-zero false positives on benign inputs.

**Medium Confidence:** The effectiveness of paraphrasing and retokenization defenses is demonstrated but with notable limitations. Paraphrasing shows strong performance against the specific attack optimized for Vicuna and Guanaco, but adaptive attacks that can find paraphrases producing adversarial outputs would defeat this defense. BPE-dropout shows promise but has a clear vulnerability to character-level attacks.

**Low Confidence:** The claim that adversarial training shows limited success is based on preliminary experiments with only one model trained on a limited dataset, without exploring alternative training approaches, more extensive datasets, or different model architectures that might show better robustness.

## Next Checks

1. Implement an adaptive attack that jointly optimizes for both adversarial objectives and perplexity minimization, testing whether the perplexity filtering defense can still maintain high detection rates when attackers have white-box access to the defense mechanism.

2. Develop character-level adversarial attacks specifically designed to bypass the BPE-dropout defense, and test whether this vulnerability can be exploited across different models and dropout rates to validate the identified break condition.

3. Conduct a more extensive adversarial training evaluation using multiple models, larger datasets of harmful prompts, and alternative training strategies (such as curriculum learning or multi-task training) to determine if the apparent limitation of adversarial training can be overcome with more comprehensive approaches.