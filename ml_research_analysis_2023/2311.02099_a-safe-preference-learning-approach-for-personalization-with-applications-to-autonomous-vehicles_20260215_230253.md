---
ver: rpa2
title: A Safe Preference Learning Approach for Personalization with Applications to
  Autonomous Vehicles
arxiv_id: '2311.02099'
source_url: https://arxiv.org/abs/2311.02099
tags:
- learning
- temporal
- problem
- weights
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a preference learning approach for autonomous
  vehicles that ensures adherence to traffic rules. The method incorporates priority
  ordering of Signal Temporal Logic (STL) formulas describing traffic rules into a
  learning framework.
---

# A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles

## Quick Facts
- arXiv ID: 2311.02099
- Source URL: https://arxiv.org/abs/2311.02099
- Reference count: 40
- Key outcome: Safe preference learning method for autonomous vehicles using PWSTL with priority ordering, showing competitive preference capture and superior safety performance in human subject studies.

## Executive Summary
This paper presents a preference learning approach for autonomous vehicles that ensures adherence to traffic rules. The method incorporates priority ordering of Signal Temporal Logic (STL) formulas describing traffic rules into a learning framework. By leveraging Parametric Weighted Signal Temporal Logic (PWSTL), the authors formulate the problem of safety-guaranteed preference learning based on pairwise comparisons. The approach finds feasible weight valuations for the PWSTL formula such that preferred signals have greater weighted quantitative satisfaction measures than non-preferred counterparts. In human subject studies involving two driving scenarios (stop sign and pedestrian crossing), the proposed method yields competitive results compared to existing preference learning methods in terms of capturing preferences and notably outperforms them when safety is considered.

## Method Summary
The paper proposes a safe preference learning framework for autonomous vehicles using Parametric Weighted Signal Temporal Logic (PWSTL). The method formulates the problem as an optimization task to find weight valuations for PWSTL formulas that ensure rule-following behaviors are preferred over rule-violating ones. Two approaches are proposed: random sampling and gradient-based optimization using a computation graph to calculate WSTL robustness. The framework ensures safety by design through priority ordering of STL formulas, guaranteeing that preferred signals have higher satisfaction measures than non-preferred counterparts. The method is validated through human subject studies in two driving scenarios, demonstrating competitive preference capture and superior safety performance compared to baseline methods.

## Key Results
- Proposed method ensures safety by design through priority ordering of STL formulas describing traffic rules
- Competitive performance in capturing user preferences while guaranteeing safety in human subject studies
- Outperforms baseline preference learning methods when safety is considered
- Validated on two driving scenarios: stop sign and pedestrian crossing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method guarantees safety by design through priority ordering of STL formulas.
- Mechanism: By incorporating the priority ordering of Signal Temporal Logic (STL) formulas describing traffic rules into the learning framework, the approach ensures that rule-following behaviors are always preferred over rule-violating ones, even when the latter are scarce in the training data.
- Core assumption: The priority ordering of STL formulas effectively captures the hierarchy of traffic rules, making rule compliance the highest priority.
- Evidence anchors:
  - [abstract] "Our approach incorporates the priority ordering of signal temporal logic (STL) formulas, describing traffic rules, into a learning framework."
  - [section] "Preference learning algorithms for safety critical operations must consider rule satisfaction."
- Break condition: If the priority ordering does not accurately reflect the true importance of traffic rules, the safety guarantee may fail.

### Mechanism 2
- Claim: The method uses a parametric extension of WSTL (PWSTL) to find feasible weight valuations.
- Mechanism: The approach formulates the problem of safety-guaranteed preference learning as an optimization problem, finding a feasible valuation for the weights of the PWSTL formula such that preferred signals have greater weighted quantitative satisfaction measures than their non-preferred counterparts.
- Core assumption: The PWSTL formula can effectively capture both the traffic rules and the user preferences through appropriate weight valuations.
- Evidence anchors:
  - [abstract] "By leveraging Parametric Weighted Signal Temporal Logic (PWSTL), we formulate the problem of safety-guaranteed preference learning based on pairwise comparisons..."
  - [section] "Starting with a parametric WSTL formula that specifies task objectives (traffic rules in autonomous vehicles) and a set of pairwise comparison preferences..."
- Break condition: If the PWSTL formula cannot adequately represent the complexity of the driving scenarios or user preferences, the optimization may not find suitable weights.

### Mechanism 3
- Claim: The method provides competitive results in capturing preferences while notably outperforming baselines in safety.
- Mechanism: Through human subject studies in two driving scenarios (stop sign and pedestrian crossing), the approach demonstrates its ability to capture individual preferences while ensuring safety, outperforming existing preference learning methods when safety is considered.
- Core assumption: The human subject studies accurately represent real-world driving scenarios and user preferences.
- Evidence anchors:
  - [abstract] "We demonstrate the performance of our method with a pilot human subject study in two different simulated driving scenarios involving a stop sign and a pedestrian crossing."
  - [section] "Our results verify the need for safety-aware preference learning by showing that baseline methods usually lead to unsafe selections."
- Break condition: If the human subject studies do not adequately represent the diversity of real-world driving scenarios or if the sample size is too small, the results may not generalize well.

## Foundational Learning

- Concept: Signal Temporal Logic (STL)
  - Why needed here: STL provides a formal framework for specifying and reasoning about the temporal properties of time series data, which is crucial for describing correct behaviors in autonomous systems and traffic rules.
  - Quick check question: Can you explain how STL can be used to specify that a vehicle must stop at a stop sign?

- Concept: Weighted Signal Temporal Logic (WSTL)
  - Why needed here: WSTL extends STL by incorporating weights that reflect the order of priority of preference, allowing for the representation of user preferences alongside traffic rules.
  - Quick check question: How does WSTL differ from STL in terms of expressing preferences?

- Concept: Parametric Weighted Signal Temporal Logic (PWSTL)
  - Why needed here: PWSTL is used to find suitable weight valuations that capture both the traffic rules and user preferences, enabling the learning of personalized driving behaviors while ensuring safety.
  - Quick check question: What is the role of PWSTL in the optimization problem formulation?

## Architecture Onboarding

- Component map: Preference data -> PWSTL formula specification -> Optimization engine (random sampling/gradient-based) -> Feasible weight valuations

- Critical path:
  1. Define PWSTL formula based on traffic rules and desired preferences
  2. Collect preference data through human subject studies
  3. Formulate optimization problem to find suitable weight valuations
  4. Solve optimization problem using random sampling and/or gradient-based approaches
  5. Validate results through simulation and comparison with baseline methods

- Design tradeoffs:
  - Random sampling vs. gradient-based optimization: Random sampling is simpler but may be less efficient, while gradient-based methods can be more efficient but may get stuck in local minima.
  - Formula complexity vs. optimization difficulty: More complex formulas can better capture nuanced preferences but may be harder to optimize.
  - Safety guarantee vs. preference satisfaction: Strict adherence to safety rules may limit the range of preferences that can be satisfied.

- Failure signatures:
  - Optimization does not converge or finds weights that do not satisfy safety constraints
  - Human subject study results show inconsistency or lack of preference
  - Simulation results indicate unsafe driving behaviors

- First 3 experiments:
  1. Validate the PWSTL formula specification on a simple driving scenario with known preferences
  2. Compare the performance of random sampling and gradient-based optimization on a mid-complexity scenario
  3. Conduct a small-scale human subject study to assess the method's ability to capture preferences while ensuring safety

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can preference learning for autonomous vehicles be made more efficient to handle larger datasets without overwhelming human subjects?
- Basis in paper: [explicit] The paper mentions that dealing with fifty pairs could be overwhelming and discusses the need for an active learning scheme that maximizes inference using minimum amount of question pairs.
- Why unresolved: The paper only mentions the upcoming focus on active learning without providing specific methods or results.
- What evidence would resolve it: A proposed active learning algorithm that reduces the number of required pairwise comparisons while maintaining or improving accuracy, validated with human subject studies.

### Open Question 2
- Question: How can the gradient-based optimization method be improved to ensure better convergence and avoid local minima in the loss surface?
- Basis in paper: [inferred] The paper notes that the gradient-based method can become stuck in local minima and suggests potential remedies like decreasing the softness coefficient or steepness of the logistic function, but these come with trade-offs.
- Why unresolved: The proposed remedies are not tested or validated in the paper, and the convergence issues remain a challenge.
- What evidence would resolve it: Experimental results showing improved convergence rates and robustness of the gradient-based method using alternative loss functions or optimization strategies.

### Open Question 3
- Question: How can the framework be extended to handle more complex temporal logic formulas with nested temporal operators and a larger number of predicates?
- Basis in paper: [inferred] The current experiments focus on relatively simple formulas, and the paper does not address scalability to more complex specifications.
- Why unresolved: The computational complexity of evaluating WSTL robustness increases with formula complexity, and the current methods may not scale well.
- What evidence would resolve it: Performance benchmarks of the method on more complex driving scenarios with nested temporal operators, showing scalability and maintained accuracy.

## Limitations

- The human subject study sample size (4 participants) is relatively small, potentially limiting generalizability of preference capture results
- Computational graph construction for gradient-based optimization may face scalability issues with more complex driving scenarios
- The assumption that priority ordering of STL formulas perfectly captures real-world traffic rule hierarchies may not hold in edge cases or ambiguous situations

## Confidence

**High Confidence**: The safety guarantee mechanism is well-supported by the theoretical framework and formal guarantees provided by the priority ordering of STL formulas. The method's ability to outperform baseline methods in safety is directly demonstrated through the human subject studies.

**Medium Confidence**: The effectiveness of PWSTL in capturing both traffic rules and user preferences is theoretically sound, but practical performance may vary depending on the complexity of driving scenarios and the quality of preference data. The optimization approaches are standard, but their relative performance in this specific application context needs further validation.

**Medium Confidence**: The human subject study results provide promising evidence, but the small sample size and limited number of scenarios suggest caution in generalizing these findings to broader driving contexts.

## Next Checks

1. **Scale Complexity**: Test the method on more complex driving scenarios with multiple interacting traffic rules and longer temporal horizons to evaluate the scalability of the PWSTL formulation and optimization approaches.

2. **Expand User Studies**: Conduct a larger-scale human subject study with diverse participants and additional driving scenarios to validate the generalizability of preference capture and safety guarantees across different driving contexts and user groups.

3. **Benchmark Against Real-World Data**: Compare the method's performance against real-world driving data to assess how well the learned preferences and safety guarantees translate from simulated to actual driving conditions.