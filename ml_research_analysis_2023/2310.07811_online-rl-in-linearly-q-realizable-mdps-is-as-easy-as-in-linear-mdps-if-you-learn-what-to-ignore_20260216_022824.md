---
ver: rpa2
title: "Online RL in Linearly $q^\u03C0$-Realizable MDPs Is as Easy as in Linear MDPs\
  \ If You Learn What to Ignore"
arxiv_id: '2310.07811'
source_url: https://arxiv.org/abs/2310.07811
tags:
- u1d458
- u1d43b
- u1d456
- u1d460
- u1d457
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of online reinforcement learning\
  \ (RL) in linearly $q^\u03C0$-realizable Markov decision processes (MDPs), a setting\
  \ more general than linear MDPs. The key insight is that low-range states (where\
  \ actions have nearly equal values) can be skipped, transforming the problem into\
  \ a linear MDP."
---

# Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore

## Quick Facts
- **arXiv ID**: 2310.07811
- **Source URL**: https://arxiv.org/abs/2310.07811
- **Reference count**: 40
- **Primary result**: Polynomial-sample-complexity online RL algorithm for linearly q^π-realizable MDPs by learning which states to skip

## Executive Summary
This paper tackles online reinforcement learning in linearly q^π-realizable MDPs, a setting more general than linear MDPs. The key insight is that low-range states (where actions have nearly equal values) can be skipped without affecting near-optimal policy performance. The proposed algorithm, SKIPPY ELEANOR, learns which states to skip while running a modified version of the ELEANOR algorithm on the hidden linear MDP. The method returns an ε-optimal policy after polylog(H,d)/ε² interactions with the MDP, where H is the time horizon and d is the dimension of the feature vectors. This is the first polynomial-sample-complexity online RL algorithm for this setting, and results hold under misspecification with graceful degradation.

## Method Summary
The method learns to skip low-range states (where action values are nearly equal) in linearly q^π-realizable MDPs, transforming the problem into a linear MDP. SKIPPY ELEANOR maintains an enclosing ellipsoid for the policy parameter space and uses ellipsoidal preconditioning for efficient estimation. The algorithm includes consistency checking with least-squares targets to detect and correct misestimation of low-range states. Data collection uses SKIPPY POLICY, which runs exploratory policies that skip low-range states probabilistically.

## Key Results
- First polynomial-sample-complexity online RL algorithm for linearly q^π-realizable MDPs
- Returns ε-optimal policy after polylog(H,d)/ε² interactions
- Results hold under misspecification with graceful degradation
- Transforms more general setting into tractable linear MDP through state skipping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: States with low action-value ranges can be skipped without affecting near-optimal policy performance.
- Mechanism: In linearly q^π-realizable MDPs, when all actions in a state have nearly equal values (range(state) < ε), the policy choice in that state is irrelevant. Skipping these states by executing a fixed action and summing rewards until reaching a high-range state yields a linear MDP hidden within the original MDP.
- Core assumption: Low-range states exist and can be detected; skipping them preserves action-value functions within ε.
- Evidence anchors:
  - [abstract]: "low-range states (where actions have nearly equal values) can be skipped, transforming the problem into a linear MDP."
  - [section 3]: "the choice of action in low-range states is not too important... these choices can affect transitions and rewards in a nonlinear way."
  - [corpus]: No direct corpus evidence; this is a novel contribution.
- Break condition: If no low-range states exist or detection fails, the conversion fails and the method degrades to general q^π-realizability hardness.

### Mechanism 2
- Claim: Ellipsoidal preconditioning enables efficient estimation of parameter spaces and range bounds.
- Mechanism: The algorithm maintains an enclosing ellipsoid for the policy parameter space Θ^ℎ at each stage ℎ. This ellipsoid shrinks as more vectors are added, tightening range estimates. Preconditioning with the ellipsoid inverse transforms the feature space to improve numerical stability and estimation accuracy.
- Core assumption: The parameter space Θ^ℎ can be bounded by an ellipsoid updated incrementally; valid preconditioning exists (Lemma 4.3).
- Evidence anchors:
  - [section 4.1]: "For any valid preconditioning /u1D444 and ℎ∈[ /u1D43B], /u1D444ℎ defines an enclosing ellipsoid for Θ ℎ."
  - [section 4.3]: "The whole estimation process leads to Optimization Problem 4.10... where ¯/u1D703 is in the confidence ellipsoid."
  - [corpus]: No corpus evidence; this is a novel algorithmic contribution.
- Break condition: If preconditioning becomes invalid (e.g., due to numerical errors or poor ellipsoid updates), range estimation fails and policy learning degrades.

### Mechanism 3
- Claim: Consistency checking with least-squares targets detects and corrects misestimation of low-range states.
- Mechanism: After collecting data, the algorithm solves Optimization Problem 4.12 to find the maximal discrepancy between predicted and measured action-value differences. If discrepancy exceeds a threshold, the discrepancy-maximizing direction is added to the ellipsoid, refining the estimate of which states are low-range.
- Core assumption: Realizability of certain auxiliary functions (Lemma 4.9) allows detection of inconsistency; discrepancy is small if estimates are correct.
- Evidence anchors:
  - [section 4.4]: "we check if the data collected is consistent with our estimates... by calculating the maximal discrepancy."
  - [section 4.2]: "We characterize an interesting set of such functions, whose (approximate) linearity plays a crucial role."
  - [corpus]: No corpus evidence; this is a novel detection/correction mechanism.
- Break condition: If consistency checking fails repeatedly without convergence, the algorithm may loop indefinitely or return a suboptimal policy.

## Foundational Learning

- Concept: Linear MDPs vs. linearly q^π-realizable MDPs
  - Why needed here: The algorithm transforms a more general class (q^π-realizable) into a tractable one (linear MDP) by exploiting low-range states. Understanding the difference is essential to grasp the conversion mechanism.
  - Quick check question: In a linear MDP, are the transition and reward functions required to be linear in the features, or only the action-values?

- Concept: Ellipsoidal potential and preconditioning
  - Why needed here: The algorithm uses an ellipsoid to bound the policy parameter space and precondition features for stable estimation. Without this, range estimation and consistency checking would be numerically unstable.
  - Quick check question: What property of the ellipsoid ensures that adding a discrepancy-maximizing vector tightens the bound on the parameter space?

- Concept: Least-squares estimation with auxiliary functions
  - Why needed here: The algorithm estimates policy parameters by regressing on auxiliary functions whose expected values are linearly realizable (Lemma 4.9). This is the core estimation mechanism.
  - Quick check question: Why does the realizability of auxiliary functions (not just action-values) enable consistency checking?

## Architecture Onboarding

- Component map: Data collection -> Parameter estimation -> Consistency checking -> Policy update
- Critical path:
  1. Initialize ellipsoid and policy parameters.
  2. Collect data with SKIPPY POLICY (Phase I skips low-range states, Phase II ensures coverage).
  3. Solve Optimization Problem 4.10 for optimistic parameters.
  4. Check consistency via Optimization Problem 4.12.
  5. If discrepancy > threshold, update ellipsoid and repeat from step 2; else check termination condition.
  6. Return policy if conditions met.
- Design tradeoffs:
  - Exploration vs. exploitation: Probabilistic skipping balances learning about low-range states vs. collecting useful data.
  - Computational vs. sample complexity: Ellipsoidal updates are efficient but may require many iterations; consistency checking adds overhead but ensures correctness.
  - Precision vs. robustness: Tighter ellipsoids improve estimation but risk numerical instability; looser bounds are safer but less informative.
- Failure signatures:
  - Algorithm loops indefinitely: Consistency check fails repeatedly; ellipsoid updates do not converge.
  - Suboptimal policy returned: Average uncertainty remains above threshold; termination condition not met.
  - Numerical instability: Ellipsoid becomes ill-conditioned; preconditioning fails.
- First 3 experiments:
  1. Verify low-range state detection: Create a simple MDP with known low-range states; check if algorithm correctly identifies and skips them.
  2. Test ellipsoid update: Simulate data collection; verify that adding discrepancy-maximizing vectors shrinks the ellipsoid appropriately.
  3. Check consistency mechanism: Introduce controlled misspecification; confirm that discrepancy detection and correction work as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a computationally efficient version of SKIPPY ELEANOR be designed for linearly q^π-realizable MDPs?
- Basis in paper: [explicit] The authors explicitly state this as an open problem, noting that their current algorithm is computationally inefficient.
- Why unresolved: The challenge lies in approximating the optimal solution for the parameter ɑ in Optimization Problem 4.10 with additive instead of multiplicative approximation error, as multiplicative errors increase exponentially with H.
- What evidence would resolve it: A proposed algorithm with polynomial time complexity that maintains the same sample complexity guarantees as the current method, or a proof that such an algorithm cannot exist under standard complexity assumptions.

### Open Question 2
- Question: Is the sample complexity bound of SKIPPY ELEANOR optimal for linearly q^π-realizable MDPs?
- Basis in paper: [inferred] The authors present a polynomial sample complexity bound but do not claim it is optimal, leaving room for improvement.
- Why unresolved: Without a matching lower bound, it's unclear if the polylog(H,d)/ε² dependence is tight or if it can be improved.
- What evidence would resolve it: A lower bound proof showing that any algorithm requires at least Ω(polylog(H,d)/ε²) samples, or an algorithm with improved sample complexity.

### Open Question 3
- Question: Can the realizability results for auxiliary functions (Section 4.2) be extended to more general function classes?
- Basis in paper: [explicit] The authors suggest their work on realizability of auxiliary functions may be of independent interest for related problem settings, such as batch RL with q^π-realizability.
- Why unresolved: The paper only establishes realizability for a specific class of functions (admissible functions), and it's unclear if the results extend to more general function classes.
- What evidence would resolve it: A proof that the realizability results hold for a broader class of functions, or a counterexample showing the limitations of the current approach.

## Limitations

- The algorithm's success critically depends on the presence of exploitable low-range states, which may not exist in all linearly q^π-realizable MDPs
- Numerical stability concerns arise from ellipsoidal preconditioning, particularly when parameter spaces become ill-conditioned
- Algorithm parameters (discrepancy thresholds, uncertainty bounds) are not fully specified, potentially affecting reproducibility
- The method may get stuck in infinite loops if consistency checking fails repeatedly without convergence

## Confidence

- **High confidence**: The polynomial sample complexity result under ideal conditions (linear MDP conversion succeeds, proper ellipsoid updates, consistent auxiliary functions)
- **Medium confidence**: The algorithm's behavior under misspecification and numerical stability claims, as these depend on implementation details not fully specified in the paper
- **Low confidence**: Claims about universal applicability across all linearly q^π-realizable MDPs, as the method's success critically depends on the presence of exploitable low-range states

## Next Checks

1. Test the algorithm on MDPs with varying densities of low-range states to quantify the impact on sample complexity and identify failure thresholds
2. Implement numerical stability tests for the ellipsoidal preconditioning under different parameter space geometries to validate robustness claims
3. Create controlled misspecification scenarios to evaluate the consistency checking mechanism's effectiveness and identify conditions under which it fails