---
ver: rpa2
title: 'Decision Knowledge Graphs: Construction of and Usage in Question Answering
  for Clinical Practice Guidelines'
arxiv_id: '2308.02984'
source_url: https://arxiv.org/abs/2308.02984
tags:
- cpgs
- data
- guidelines
- knowledge
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Clinical Practice Guidelines (CPGs) are complex medical documents
  that help doctors make treatment decisions, but they are difficult to search and
  navigate manually. Existing knowledge graph representations cannot efficiently handle
  CPGs due to their decision-based structure and frequent updates.
---

# Decision Knowledge Graphs: Construction of and Usage in Question Answering for Clinical Practice Guidelines

## Quick Facts
- arXiv ID: 2308.02984
- Source URL: https://arxiv.org/abs/2308.02984
- Reference count: 20
- Primary result: 40% accuracy improvement over fine-tuned BioBERT using Decision Knowledge Graphs for CPG question answering

## Executive Summary
Clinical Practice Guidelines (CPGs) are complex medical documents that help doctors make treatment decisions, but their decision-based structure and frequent updates make them difficult to represent in traditional knowledge graphs. This paper introduces Decision Knowledge Graphs (DKGs), which add a decision dimension to knowledge graphs to separate static treatment procedures from dynamic patient constraints. The DKG approach enables efficient updates by only modifying decision nodes while keeping static data unchanged, and demonstrates 40% higher accuracy in question answering compared to fine-tuned BioBERT models when tested on cancer guidelines.

## Method Summary
The method involves parsing CPG PDFs into structured CSV format, extracting patient constraints using constituency parsing and keyword-based approaches, and building a DKG in neo4j with static treatment triples and dynamic decision nodes. A transformer model converts natural language questions into Cypher queries that match decision nodes with specified constraints, then retrieves linked treatment nodes. The approach was evaluated on three types of cancer guidelines (ALL, bone, and kidney) using 8300 question-answer pairs, measuring accuracy, ROUGE, BLEU, and Jaccard scores.

## Key Results
- 40% accuracy improvement over fine-tuned BioBERT model in answering treatment recommendation questions
- Demonstrated effective handling of frequent CPG updates by isolating decision nodes
- Achieved improved ROUGE, BLEU, and Jaccard scores across three cancer guideline types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DKGs improve accuracy by 40% over fine-tuned BioBERT because they isolate dynamic decision nodes from static knowledge, enabling efficient updates without full reconstruction.
- Mechanism: Static data (treatment procedures, drug lists) is stored as traditional KG triples; dynamic data (patient constraints like age, comorbidities) is stored in separate decision nodes. Updates only modify decision nodes, reducing propagation complexity.
- Core assumption: Most guideline updates affect only patient constraints, not static treatment facts.
- Evidence anchors:
  - [abstract]: "Our proposed DKG has a decision dimension added to a Knowledge Graph (KG) structure... Using this DKG has shown 40% increase in accuracy compared to the deep learning model."
  - [section 6.1]: "The modifications that are made to guidelines, based on discussions, are mainly done on patients' constraints... Therefore, performing updates on DKG will be a more cost-effective task than updating a KG."
  - [corpus]: Weak corpus coverage; no direct neighbor papers cite DKG accuracy gains.
- Break condition: If guideline updates frequently change static treatment facts, the DKG advantage disappears.

### Mechanism 2
- Claim: DKG enables structured querying via Cypher, allowing precise extraction of treatment recommendations based on patient constraints.
- Mechanism: Natural language questions are converted to Cypher queries that match decision nodes with specified constraints, then retrieve linked treatment nodes.
- Core assumption: Constraint extraction from text is accurate enough to generate valid Cypher queries.
- Evidence anchors:
  - [section 6.2.3]: "We have used Cypher Query Language (CQL) to query DKG... MATCH (m: node-stratified=‘ph+’, MRD:‘rising’)-[:next_step]-> n RETURN n.treatments"
  - [section 7.3]: "Given a natural language question... using a transformer model, we convert the question to CQL query."
  - [corpus]: No corpus evidence directly supports Cypher-based querying effectiveness.
- Break condition: If constraint extraction fails, generated Cypher queries return no or incorrect results.

### Mechanism 3
- Claim: BioBERT fine-tuning without DKG underperforms because it lacks explicit representation of conditional logic and domain-specific embeddings.
- Mechanism: BioBERT trained only on question-answer pairs misses the structured decision logic present in CPGs; embeddings are augmented with MeSH and NCCN data to mitigate but not fully resolve.
- Core assumption: Transformer models need explicit decision structure to match human guideline interpretation.
- Evidence anchors:
  - [section 7.1]: "BioBERT has missing embeddings for words used in NCCN guidelines. We have created new embeddings using the architecture shown in Figure 4."
  - [section 8]: "Having DKG has improved accuracy (calculated as number of correct matches divided by total number of questions) by 40% compared to the deep learning model."
  - [corpus]: No corpus evidence on embedding quality impact.
- Break condition: If embeddings are sufficiently enriched, gap between DKG and non-DKG models narrows.

## Foundational Learning

- Concept: Knowledge Graph (KG) structure and triple representation
  - Why needed here: DKG extends KG; understanding nodes, edges, and triples is essential for grasping DKG construction.
  - Quick check question: What are the three components of a KG triple?

- Concept: Decision-based reasoning in clinical guidelines
  - Why needed here: CPGs are conditional; recognizing how patient constraints drive treatment decisions is core to DKG logic.
  - Quick check question: How do patient constraints like "age < 65" influence treatment selection in CPGs?

- Concept: Query languages for graph databases (Cypher)
  - Why needed here: DKG queries use Cypher; understanding MATCH, RETURN, and node/relationship patterns is required to build and test queries.
  - Quick check question: Write a Cypher query to find treatments for a patient with "ph+" stratification and "rising MRD".

## Architecture Onboarding

- Component map: PDF Parser -> CSV output (head entity, head constraints, tail entity, tail constraints) -> Constraint Extractor -> DKG Builder -> neo4j Graph Database -> Query Builder (Transformer) -> Cypher execution -> Treatment nodes

- Critical path:
  1. Parse CPG PDF -> CSV
  2. Extract constraints -> enrich CSV
  3. Build DKG in neo4j
  4. Convert user question -> Cypher
  5. Execute Cypher -> retrieve treatment

- Design tradeoffs:
  - Static vs dynamic data separation reduces update cost but increases schema complexity.
  - Constraint extraction accuracy directly impacts query correctness; errors propagate to wrong treatments.
  - Transformer-based query conversion is flexible but requires labeled training data.

- Failure signatures:
  - No results from Cypher -> constraint extraction failed or constraints not present in DKG.
  - Wrong treatments -> incorrect constraint mapping or misparsed guidelines.
  - Slow updates -> decision nodes not properly isolated; static data incorrectly marked dynamic.

- First 3 experiments:
  1. Parse a single guideline page -> verify CSV structure and constraint extraction output.
  2. Load CSV into neo4j -> run a simple MATCH query to retrieve a known treatment node.
  3. Feed a hand-crafted question -> verify transformer outputs syntactically correct Cypher that returns expected node.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term performance implications of Decision Knowledge Graphs (DKGs) as CPGs are updated over multiple years?
- Basis in paper: [inferred] The paper mentions DKGs enable efficient updates by only modifying decision nodes while keeping static data unchanged, but doesn't analyze long-term performance over multiple update cycles.
- Why unresolved: The paper only evaluates the current DKG performance without examining how it scales or performs after multiple years of CPG updates.
- What evidence would resolve it: Long-term studies tracking DKG performance, update efficiency, and query accuracy over 2-3 years of CPG updates across multiple cancer types.

### Open Question 2
- Question: How does the DKG approach compare to other knowledge graph representations specifically designed for handling conditional statements and frequent updates?
- Basis in paper: [explicit] The paper states "no existing representation is suitable to perform question-answering and searching tasks on CPGs" and claims DKGs are the first attempt at adding decision dimensions to KGs for CPGs.
- Why unresolved: The paper doesn't provide direct comparisons with other KG approaches that handle conditional statements or provide update efficiency metrics against alternative representations.
- What evidence would resolve it: Comparative analysis of DKG against other KG approaches (like Jiang et al. 2020's conditional KG) on the same CPG datasets measuring update efficiency and query performance.

### Open Question 3
- Question: What is the error rate in the constraint extraction module and how does it affect overall DKG accuracy?
- Basis in paper: [inferred] The paper mentions a hybrid constraint extraction model but doesn't provide error rates or evaluate how extraction errors propagate through the DKG.
- Why unresolved: The constraint extraction is a critical component of DKG construction, yet the paper doesn't quantify its accuracy or show how errors in this step affect downstream question-answering performance.
- What evidence would resolve it: Error analysis of the constraint extraction module showing precision/recall metrics and correlation between extraction errors and final question-answering accuracy.

## Limitations

- Constraint extraction accuracy is critical but not rigorously evaluated, creating uncertainty about the reliability of generated Cypher queries
- The 40% accuracy improvement claim lacks direct comparison to other knowledge graph-based approaches
- Scalability to larger guideline corpora and different medical domains remains unverified
- Specific regex rules and keyword lists for constraint extraction are not fully specified

## Confidence

- High confidence: The fundamental concept of separating static treatment knowledge from dynamic patient constraints is sound and well-justified by the guideline update patterns described.
- Medium confidence: The 40% accuracy improvement claim is credible based on the described methodology, but the lack of detailed comparison to other KG-based approaches and specific training configurations limits full confidence.
- Low confidence: The effectiveness of constraint extraction and query generation processes cannot be fully assessed without access to the specific regex rules, keyword lists, and transformer model hyperparameters used.

## Next Checks

1. Implement and test the constraint extraction process on a sample guideline section to verify the accuracy of extracted patient constraints against manual annotation.

2. Execute sample Cypher queries generated by the transformer model to confirm they correctly retrieve expected treatment nodes from the DKG.

3. Conduct a controlled experiment comparing DKG-based QA performance against other knowledge graph approaches on a standardized dataset to isolate the specific contribution of the decision dimension.