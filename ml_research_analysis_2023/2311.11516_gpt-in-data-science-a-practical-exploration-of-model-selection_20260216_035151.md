---
ver: rpa2
title: 'GPT in Data Science: A Practical Exploration of Model Selection'
arxiv_id: '2311.11516'
source_url: https://arxiv.org/abs/2311.11516
tags:
- data
- dataset
- regression
- selection
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of GPT-4 for model selection in data
  science, addressing the challenge of understanding its decision-making process.
  A variability model is employed to represent the factors influencing GPT-4's recommendations,
  and toy datasets are used to evaluate both the model and the implementation of identified
  heuristics.
---

# GPT in Data Science: A Practical Exploration of Model Selection

## Quick Facts
- arXiv ID: 2311.11516
- Source URL: https://arxiv.org/abs/2311.11516
- Reference count: 18
- Primary result: GPT-4 demonstrates satisfactory performance in model selection across tested datasets using a variability model approach

## Executive Summary
This paper explores the use of GPT-4 for model selection in data science, addressing the challenge of understanding its decision-making process. A variability model is employed to represent the factors influencing GPT-4's recommendations, and toy datasets are used to evaluate both the model and the implementation of identified heuristics. By comparing GPT-4's outcomes with heuristics from other platforms, the effectiveness and distinctiveness of its methodology are assessed. The research aims to advance the comprehension of AI decision-making processes, particularly in model selection, contributing to more transparent and comprehensible AI systems.

## Method Summary
The study employs a variability model to depict factors influencing GPT-4's model selection recommendations, using toy datasets for evaluation. The methodology combines top-down elicitation of GPT-4's decision criteria with bottom-up validation through practical applications. Feature models are constructed to represent the identified heuristics, and performance metrics including accuracy, precision, recall, and AUC-ROC are used to compare GPT-4's recommendations with established heuristics. GridSearchCV with cross-validation is implemented to automate the evaluation of model parameter combinations.

## Key Results
- GPT-4's suggestions achieve satisfactory performance across all tested datasets
- The feature model successfully captures key constraints influencing GPT-4's model selection process
- Comparison with Scikit-learn heuristics shows comparable effectiveness in certain scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's model selection process is effectively captured and represented through feature models that encode variability in dataset attributes, problem types, and evaluation criteria
- Mechanism: By using feature models, the authors reverse-engineer GPT-4's decision-making process into a structured representation that includes constraints and relationships among modeling assumptions, functional requirements, and non-functional requirements
- Core assumption: GPT-4's responses reflect its actual decision-making process and can be systematically decomposed into a feature model structure
- Evidence anchors:
  - [abstract] "We employ a variability model to depict these factors and use toy datasets to evaluate both the model and the implementation of the identified heuristics."
  - [section] "Based on the GPT output and on the feature model we designed for this application, the key constraints influencing feature modeling include: Dataset Constraints, Algorithm Type Constraints, Evaluation Criteria Constraints, Model-Specific Constraints, Computational Efficiency Consideration, Ranking and Transition Process Constraints."

### Mechanism 2
- Claim: The combination of top-down and bottom-up approaches provides a comprehensive understanding of GPT-4's model selection heuristics
- Mechanism: Top-down approach elicits explicit criteria from GPT-4, while bottom-up approach tests these criteria through practical applications with toy datasets, allowing for validation and refinement of the heuristics
- Core assumption: GPT-4's responses are consistent and can be validated through practical testing with controlled datasets
- Evidence anchors:
  - [section] "In the top-down approach, we directly posed a question to GPT to elicit its criteria for selecting a data science model... To complement the top-down analysis, we employed a bottom-up approach, which involved practical applications of GPT's decision-making process."

### Mechanism 3
- Claim: GPT-4's model selection suggestions can be evaluated and compared against established heuristics using performance metrics and cross-validation
- Mechanism: By applying GridSearchCV and evaluating models based on accuracy, precision, recall, and AUC-ROC, the authors can quantitatively compare GPT-4's recommendations with those from Scikit-learn heuristics
- Core assumption: Performance metrics and cross-validation provide a reliable basis for comparing different model selection approaches
- Evidence anchors:
  - [section] "The GridSearchCV from scikit-learn was employed to automate the evaluation of all model parameter combinations via cross-validation... Results including the top two models from GPT... the top two from Scikit-learn..."

## Foundational Learning

- Concept: Feature modeling and variability modeling
  - Why needed here: Feature models provide a structured way to represent the complex relationships between dataset attributes, problem types, and evaluation criteria that influence GPT-4's model selection decisions
  - Quick check question: How does a feature model differ from a simple decision tree in representing variability in model selection criteria?

- Concept: Top-down and bottom-up approaches in reverse engineering
  - Why needed here: The combination of eliciting explicit criteria from GPT-4 (top-down) and validating these criteria through practical testing (bottom-up) provides a comprehensive understanding of its decision-making process
  - Quick check question: What are the advantages and limitations of using only a top-down approach versus combining it with a bottom-up approach in reverse engineering AI decision-making processes?

- Concept: Cross-validation and performance metrics in model evaluation
  - Why needed here: Cross-validation and metrics like accuracy, precision, recall, and AUC-ROC provide an objective basis for comparing the effectiveness of different model selection heuristics
  - Quick check question: Why is cross-validation particularly important when evaluating model selection heuristics on small datasets?

## Architecture Onboarding

- Component map:
  - Data ingestion -> GPT-4 interaction -> Feature model construction -> Evaluation framework -> Comparison module

- Critical path:
  1. Elicit decision-making criteria from GPT-4 (top-down)
  2. Validate criteria through practical testing with toy datasets (bottom-up)
  3. Construct feature model representing identified heuristics
  4. Evaluate model suggestions using cross-validation and performance metrics
  5. Compare results with established heuristics

- Design tradeoffs:
  - Using toy datasets provides controlled testing but may not capture all real-world complexities
  - Feature models provide structured representation but may oversimplify complex decision-making processes
  - Performance metrics offer quantitative comparison but may not capture all aspects of model suitability

- Failure signatures:
  - Inconsistent GPT-4 responses across similar prompts
  - Feature model unable to predict GPT-4's recommendations consistently
  - Performance metrics showing no significant difference between GPT-4 and established heuristics

- First 3 experiments:
  1. Replicate the heart failure prediction dataset experiment, comparing GPT-4's top recommendations with Scikit-learn heuristics using the same evaluation framework
  2. Test the diabetes prediction dataset with modified prompts to assess the impact of prompt engineering on GPT-4's recommendations
  3. Create a new toy dataset with specific characteristics (e.g., highly imbalanced classes) to test the robustness of the feature model across diverse scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variability in dataset characteristics (e.g., size, quality, type) affect the performance of GPT-4's model selection recommendations?
- Basis in paper: [explicit] The paper discusses the influence of various factors in the model selection process, including the nature of the data, problem type, performance metrics, computational resources, interpretability vs accuracy, assumptions about data, and ethical considerations
- Why unresolved: While the paper explores GPT-4's decision-making process and compares its recommendations with established heuristics, it does not provide a detailed analysis of how different dataset characteristics specifically impact the performance of GPT-4's model selection
- What evidence would resolve it: Conducting experiments with datasets of varying characteristics and analyzing the correlation between these characteristics and the performance of GPT-4's model selection recommendations would provide evidence

### Open Question 2
- Question: To what extent can the feature models used to represent GPT-4's decision-making heuristics be generalized to other Large Language Models (LLMs)?
- Basis in paper: [inferred] The paper focuses on GPT-4's model selection process and uses feature models to represent the factors and assumptions guiding its recommendations. However, it does not explore the applicability of these models to other LLMs
- Why unresolved: The paper does not provide a comparative analysis of feature models across different LLMs, leaving the question of generalizability open
- What evidence would resolve it: Testing the feature models with other LLMs and comparing their decision-making processes would provide evidence for or against the generalizability of the models

### Open Question 3
- Question: How can the integration of ethical considerations into GPT-4's model selection process be improved to ensure more responsible AI decision-making?
- Basis in paper: [explicit] The paper mentions that while ethical considerations were raised in the top-down query, GPT did not consider these when proposing model selections for any of the three datasets
- Why unresolved: The paper acknowledges the gap in incorporating ethical considerations but does not provide a solution or methodology for improving this integration
- What evidence would resolve it: Developing and testing methods for explicitly incorporating ethical considerations into GPT-4's model selection process, followed by evaluating the impact on decision-making, would provide evidence for improvement

## Limitations

- Toy datasets may not fully capture the complexity of real-world data science scenarios
- Reliance on GPT-4's opaque internal decision-making process introduces uncertainty in result reproducibility
- Comparison with established heuristics limited to specific performance metrics, potentially overlooking other important factors

## Confidence

- Effectiveness of feature model in capturing all relevant factors: Medium
- Consistency of GPT-4's recommendations across varied prompts: Medium
- Generalizability of methodology to different domains: Low

## Next Checks

1. Test the feature model and methodology on a diverse set of real-world datasets to assess generalizability
2. Conduct a more extensive comparison of GPT-4's recommendations with multiple established heuristics across different domains
3. Perform a sensitivity analysis on GPT-4's recommendations by varying prompt engineering techniques to understand the robustness of its decision-making process