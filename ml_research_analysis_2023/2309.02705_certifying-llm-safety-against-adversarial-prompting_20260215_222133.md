---
ver: rpa2
title: Certifying LLM Safety against Adversarial Prompting
arxiv_id: '2309.02705'
source_url: https://arxiv.org/abs/2309.02705
tags:
- adversarial
- harmful
- prompts
- prompt
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adversarial attacks on large
  language models (LLMs) that cause them to generate harmful content by bypassing
  safety guardrails. The authors introduce "erase-and-check," the first framework
  for defending against such attacks with certifiable safety guarantees.
---

# Certifying LLM Safety against Adversarial Prompting

## Quick Facts
- arXiv ID: 2309.02705
- Source URL: https://arxiv.org/abs/2309.02705
- Authors: 
- Reference count: 17
- Primary result: Introduces "erase-and-check" framework with certifiable safety guarantees against adversarial prompting attacks

## Executive Summary
This paper addresses the critical problem of adversarial attacks on large language models (LLMs) that cause them to generate harmful content by bypassing safety guardrails. The authors introduce "erase-and-check," the first framework for defending against such attacks with certifiable safety guarantees. The core innovation is a token erasure mechanism that checks subsequences of input prompts against a safety filter, providing mathematical guarantees that harmful prompts will be detected even when adversarially modified up to a certain size.

## Method Summary
The erase-and-check framework works by erasing tokens individually from an input prompt and inspecting the resulting subsequences using a safety filter. If any subsequence or the original prompt is detected as harmful, the input prompt is labeled harmful. This approach provides a safety certificate guaranteeing that adversarial modifications up to a certain size are also labeled harmful. The method is implemented using Llama 2 as the safety filter and defends against three attack modes: adversarial suffix, adversarial insertion, and adversarial infusion. The authors also propose three efficient empirical defenses: RandEC, GreedyEC, and GradEC, which use randomized erasure, greedy erasure, and gradient-based erasure respectively.

## Key Results
- Against adversarial suffixes of length 20, erase-and-check certifiably detects 93% of harmful prompts
- Maintains 94% accuracy on labeling safe prompts as safe
- The safety certificate guarantees harmful prompts won't be mislabeled as safe under adversarial attack up to a certain size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Erasing tokens from harmful prompts ensures detection because at least one subsequence matches the original harmful prompt
- Mechanism: The procedure erases tokens individually from the input prompt, generating subsequences. If the original prompt is harmful and adversarial tokens are erased, one of the subsequences must be the original prompt, which the safety filter will detect
- Core assumption: Subsequences of harmful prompts remain harmful when detected by the safety filter
- Evidence anchors: [abstract]: "If any subsequences or the original prompt are detected as harmful, the input prompt is labeled harmful." [section]: "For an adversarial prompt P + α where P was originally detected as harmful by the safety filter is-harmful. If |α| ≤ d, the prompt P must be one of the subsequences checked by is-harmful."

### Mechanism 2
- Claim: The safety certificate guarantees that harmful prompts are not mislabeled as safe under adversarial attack up to a certain size
- Mechanism: The accuracy of the safety filter on harmful prompts is a lower bound on the accuracy of erase-and-check. By evaluating the filter's performance, we can certify the procedure's effectiveness
- Core assumption: The safety filter's accuracy on harmful prompts directly translates to the procedure's accuracy on adversarial modifications
- Evidence anchors: [abstract]: "Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size." [section]: "Therefore, to calculate the certified accuracy of erase-and-check on harmful prompts, we just need to evaluate the accuracy of the filter on such prompts."

### Mechanism 3
- Claim: The procedure can defend against three different adversarial attack modes: suffix, insertion, and infusion
- Mechanism: For each attack mode, the procedure generates subsequences by erasing tokens in a specific pattern (from end, anywhere, or arbitrary subsets) and checks them with the safety filter
- Core assumption: The erasure patterns cover all possible adversarial modifications for each attack mode
- Evidence anchors: [abstract]: "We defend against three attack modes: i) adversarial suffix, ii) adversarial insertion, and iii) adversarial infusion." [section]: Detailed description of how subsequences are generated for each attack mode and how the safety guarantee holds

## Foundational Learning

- Concept: Understanding of adversarial attacks on machine learning models
  - Why needed here: The work builds on existing research on adversarial attacks and defenses, particularly in the context of NLP and LLMs
  - Quick check question: What are the key differences between adversarial attacks on computer vision models and NLP models?

- Concept: Knowledge of safety filters and content moderation techniques
  - Why needed here: The procedure relies on a safety filter to detect harmful content in subsequences of the input prompt
  - Quick check question: How do safety filters for LLMs typically work, and what are their limitations?

- Concept: Familiarity with LLM architectures and token-based representations
  - Why needed here: The work operates at the token level, erasing individual tokens and generating subsequences
  - Quick check question: How do LLMs represent and process text at the token level?

## Architecture Onboarding

- Component map: Input prompt -> Token erasure module -> Safety filter -> Output label (harmful/safe) -> Configuration (maximum erase length)

- Critical path:
  1. Receive input prompt
  2. Generate subsequences by erasing tokens
  3. Check each subsequence and original prompt with safety filter
  4. If any detected as harmful, label input as harmful
  5. Otherwise, label as safe

- Design tradeoffs:
  - Maximum erase length vs. computational cost and accuracy
  - Choice of safety filter (LLM-based vs. dedicated classifier)
  - Tradeoff between certified guarantees and empirical performance

- Failure signatures:
  - High false positive rate on safe prompts
  - Low detection rate on harmful prompts with adversarial modifications
  - Excessive computational cost for large maximum erase lengths

- First 3 experiments:
  1. Test on a set of known harmful prompts with no adversarial modifications to establish baseline safety filter accuracy
  2. Test on harmful prompts with adversarial suffixes of varying lengths to evaluate certified guarantees
  3. Test on safe prompts to measure false positive rate and computational cost for different maximum erase lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational efficiency of the erase-and-check procedure be improved for larger adversarial sequence lengths while maintaining certified safety guarantees?
- Basis in paper: [explicit] The paper discusses that the running time of erase-and-check increases significantly with larger adversarial sequence lengths and more complex attack modes (e.g., adversarial infusion), limiting the practical applicability of the method for longer sequences
- Why unresolved: The paper acknowledges this as a limitation but does not propose concrete solutions to address it. The exponential growth in the number of subsequences to check makes it challenging to maintain efficiency for longer sequences
- What evidence would resolve it: A new algorithm or optimization technique that reduces the number of subsequences checked while still providing the same certified safety guarantees for longer adversarial sequences would resolve this question

### Open Question 2
- Question: Can a dedicated safety classifier be trained to improve the performance of erase-and-check on safe prompts, especially when dealing with erased subsequences?
- Basis in paper: [explicit] The paper suggests that training a safety classifier specifically on safe and harmful prompts, ensuring it recognizes erased subsequences of safe prompts as safe, could potentially resolve the issue of decreasing accuracy for larger adversarial sequences
- Why unresolved: The paper does not explore this direction and only uses an off-the-shelf LLM as the safety filter. The performance of a dedicated classifier on this task is unknown
- What evidence would resolve it: Training and evaluating a dedicated safety classifier on a dataset of safe and harmful prompts, including erased subsequences, and comparing its performance to the current approach would provide evidence to resolve this question

### Open Question 3
- Question: How does the performance of erase-and-check vary across different LLM architectures and safety filters?
- Basis in paper: [explicit] The paper uses Llama 2 as the safety filter and compares it with DistilBERT. However, it does not explore the impact of using different LLM architectures or custom-trained safety filters on the performance of erase-and-check
- Why unresolved: The paper only provides results for a limited set of safety filters and does not investigate the generalizability of the approach to other architectures or filters
- What evidence would resolve it: Conducting experiments with different LLM architectures (e.g., GPT-3.5, Claude) and custom-trained safety filters, and comparing their performance on the same set of harmful and safe prompts, would provide evidence to resolve this question

## Limitations
- Certified guarantees only apply to adversarial modifications up to a certain size (maximum erase length)
- Computational cost scales exponentially with maximum erase length for the most general attack mode
- Safety filter itself may have inherent limitations in detecting all forms of harmful content

## Confidence

**High Confidence Claims:**
- The mathematical proof that erase-and-check guarantees detection of harmful prompts when adversarial modifications are within the certified bound
- The empirical observation that accuracy on harmful prompts is a lower bound for the procedure's accuracy
- The effectiveness of the three attack mode defenses when tested against Greedy Coordinate Gradient attacks

**Medium Confidence Claims:**
- The generalizability of certified guarantees across different safety filter implementations
- The scalability of the approach to larger values of d in practice
- The robustness against more sophisticated adversarial attack strategies not tested in the evaluation

**Low Confidence Claims:**
- The effectiveness against emerging attack techniques that may circumvent token erasure detection
- The performance on harmful prompts requiring deep contextual understanding
- The long-term reliability as LLM capabilities and attack methodologies evolve

## Next Checks
1. **Stress Test Safety Filter Boundaries**: Evaluate the safety filter's performance on increasingly subtle variations of harmful prompts to determine the limits of detection capability, particularly focusing on context-dependent harms and semantically equivalent harmful content expressed with different tokens

2. **Extended Adversarial Robustness Testing**: Test the framework against more sophisticated attack strategies beyond GCG, including gradient-based attacks targeting the safety filter directly, black-box attacks, and human-generated jailbreak prompts to assess real-world robustness

3. **Scalability Analysis Under Realistic Constraints**: Measure the computational cost and detection accuracy across a range of d values (1-20) while simulating realistic deployment conditions, including latency requirements and resource constraints typical of production LLM safety systems