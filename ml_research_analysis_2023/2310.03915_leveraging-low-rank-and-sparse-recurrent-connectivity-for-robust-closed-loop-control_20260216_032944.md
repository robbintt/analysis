---
ver: rpa2
title: Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop
  Control
arxiv_id: '2310.03915'
source_url: https://arxiv.org/abs/2310.03915
tags:
- recurrent
- network
- networks
- which
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how low-rank and sparse recurrent connectivity
  affects the robustness of recurrent neural networks in closed-loop control settings.
  The authors propose a parameterization of recurrent connectivity that combines low-rank
  decomposition with sparsity via a binary mask, enabling control over both the rank
  and sparsity of the connectivity matrix.
---

# Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control

## Quick Facts
- **arXiv ID**: 2310.03915
- **Source URL**: https://arxiv.org/abs/2310.03915
- **Reference count**: 40
- **Key outcome**: Low-rank, sparse recurrent connectivity improves robustness under distribution shift in closed-loop control while using fewer parameters

## Executive Summary
This paper investigates how low-rank and sparse recurrent connectivity affects robustness in closed-loop control settings, particularly for offline-to-online generalization under distribution shift. The authors propose a parameterization combining low-rank decomposition with sparsity via a binary mask, enabling control over both rank and sparsity of recurrent weights. Through theoretical analysis and empirical evaluation across Atari and MuJoCo environments, they demonstrate that this approach yields more robust and memory-efficient agents compared to full-rank, fully-connected alternatives, while also providing interpretable insights into how network dynamics can be modulated through connectivity structure.

## Method Summary
The method parameterizes recurrent weights as Wrec(r, s) = W1(r)W2(r) ⊙ M(s), where r controls rank via singular value decomposition and s controls sparsity via a binary mask. This allows explicit control over the effective dimensionality and connectivity density of the recurrent matrix. Models are trained via imitation learning on expert trajectories from pretrained DQN agents, with evaluation both in-distribution and under various distribution shifts (occlusions, brightness changes, noise). The approach is tested across multiple architectures including CfCs, LSTMs, RNNs, and GRUs with varying rank/sparsity combinations.

## Key Results
- CfCs with low-rank, sparse connectivity outperform full-rank, fully-connected counterparts in online settings under distribution shift
- The proposed parameterization uses fewer parameters while maintaining or improving performance
- Low-rank and sparse connectivity induces interpretable priors on network dynamics that improve robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Parameterizing recurrent connectivity as a function of rank and sparsity induces interpretable priors on network dynamics that improve robustness under distribution shift.
- **Mechanism**: Low-rank decomposition reduces the effective dimensionality of recurrent state-space dynamics, while sparsity modulates the spectral radius to control the vanishing gradient. Together, these constraints simplify the model's dynamics and improve generalization.
- **Core assumption**: The task dimension gap between offline training and online closed-loop deployment means that models trained offline learn higher-rank connectivity changes, while robust deployment requires lower-rank dynamics.
- **Evidence anchors**:
  - [abstract] "The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs)."
  - [section] "We find that CfCs with fewer parameters can outperform their full-rank, fully-connected counterparts in the online setting under distribution shift."
  - [corpus] Weak – neighboring work discusses sparse and low-rank networks but does not address closed-loop robustness or the task dimension gap.
- **Break condition**: If the task requires high-rank recurrent dynamics (e.g., complex long-term dependencies), low-rank parameterization may hurt performance in-distribution.

### Mechanism 2
- **Claim**: CfCs are more amenable to low-rank and sparse connectivity than LSTMs because CfCs already express the vanishing gradient in their baseline form.
- **Mechanism**: LSTMs use gating mechanisms to maintain a more consistent error flow over time, so sparsity must be induced to push them into the vanishing gradient regime. CfCs inherently have low spectral radius due to their time-constant gating, so low-rank and sparse priors align naturally with their dynamics.
- **Core assumption**: A short temporal attention span is beneficial in closed-loop control tasks, making the vanishing gradient desirable.
- **Evidence anchors**:
  - [section] "CfCs in their baseline form already lie in the vanishing gradient regime: intuitively, inducing sparsity is less effective when the baseline network inherently possesses an affinity to selectively attend to past observations."
  - [section] "CfCs tend to express the vanishing gradient even in the baseline fully-connected, full-rank formulation as given by λmax < 1."
  - [corpus] Weak – neighboring work does not discuss temporal attention spans or the role of the vanishing gradient in closed-loop robustness.
- **Break condition**: If long-term dependencies are crucial for the task, CfCs may underperform relative to architectures that maintain error flow.

### Mechanism 3
- **Claim**: The time-constant network F in CfCs enables adaptive gating that dynamically interpolates between G and H trajectories, improving robustness under distribution shift.
- **Mechanism**: F learns interpolation weights based on the input, so under distribution shift it can adjust how much weight it places on G versus H. This dynamic adaptation compensates for corrupted inputs, whereas static gating in LSTMs and RNNs cannot.
- **Core assumption**: The time-constant network's ability to adjust interpolation weights based on input perturbations is key to robustness.
- **Evidence anchors**:
  - [section] "we interpret F as an adaptive gating mechanism that interpolates between the state-space trajectories of H and G on a per-element basis in the hidden vector h(t)."
  - [section] "Because the interpolation weights learned by F are a function of the input, the time constant network can dynamically adapt its interpolation weights under distribution shift by changing how much it weighs G versus H."
  - [corpus] Weak – neighboring work does not discuss time-constant networks or their role in robustness under distribution shift.
- **Break condition**: If the distribution shifts are too extreme or if the time-constant network cannot learn meaningful interpolation patterns, this mechanism may fail.

## Foundational Learning

- **Concept**: Singular Value Decomposition (SVD) and its role in low-rank approximation.
  - Why needed here: The low-rank parameterization is constructed via SVD of the recurrent weight matrix.
  - Quick check question: What is the Eckart-Young-Minsky theorem and how does it justify using SVD for low-rank approximation?

- **Concept**: Spectral radius and spectral norm as measures of network dynamics.
  - Why needed here: These metrics are used to analyze the vanishing gradient and temporal robustness of the network.
  - Quick check question: How do the spectral radius and spectral norm relate to the eigenvalues and singular values of a matrix, respectively?

- **Concept**: Distribution shift and its impact on closed-loop control.
  - Why needed here: The paper's main focus is on robustness under distribution shift in closed-loop settings.
  - Quick check question: What are the key differences between in-distribution and out-of-distribution performance in reinforcement learning tasks?

## Architecture Onboarding

- **Component map**: Input → Preprocessing → Recurrent network (Wrec(r, s)) → Output
- **Critical path**: Input → Preprocessing → Recurrent network (Wrec(r, s)) → Output
  - The recurrent weights Wrec(r, s) are the key component parameterized by rank and sparsity.
- **Design tradeoffs**:
  - Low rank: Fewer parameters, simpler dynamics, may hurt in-distribution performance if high-rank dynamics are needed.
  - High sparsity: Faster computation, modulated spectral radius, may hurt in-distribution performance if sparsity is too high.
  - CfC vs. LSTM/RNN/GRU: CfCs are more amenable to low-rank/sparse priors but may underperform if long-term dependencies are crucial.
- **Failure signatures**:
  - Poor in-distribution performance: Rank or sparsity too low for the task complexity.
  - Poor out-of-distribution performance: Rank or sparsity not low enough to induce robust dynamics.
  - Optimization difficulties: Very high sparsity (e.g., 0.95) may cause training instability, especially in LSTMs and GRUs.
- **First 3 experiments**:
  1. Compare in-distribution and out-of-distribution performance of CfC, LSTM, RNN, and GRU with varying (r, s) pairs on a simple ALE task (e.g., Seaquest).
  2. Analyze the spectral radius and spectral norm of trained networks to verify the effects of rank and sparsity on the vanishing gradient and temporal robustness.
  3. Decompose state-space trajectories into recurrently-driven and input-driven components to understand the impact of rank and sparsity on the effective dimensionality of dynamics.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the proposed low-rank and sparse connectivity parameterization perform in more complex tasks beyond the ALE and MuJoCo environments studied in the paper?
  - Basis in paper: The authors mention exploring the impact of their parameterization in the Arcade Learning Environment (ALE) and MuJoCo environments, but note that the efficacy of the parameterization in more complex tasks warrants further exploration.
  - Why unresolved: The paper only evaluates the proposed approach in a limited set of environments. It is unclear how well the low-rank and sparse connectivity would generalize to tasks with higher complexity or different characteristics.
  - What evidence would resolve it: Empirical results showing the performance of models with low-rank and sparse connectivity on a diverse set of more complex tasks, such as those with longer time horizons, higher-dimensional observations, or more complex action spaces. Comparing these results to models with full-rank, fully-connected connectivity would help quantify the benefits and limitations of the proposed approach.

- **Open Question 2**: What is the optimal balance between rank and sparsity for achieving the best performance in closed-loop control tasks?
  - Basis in paper: The authors investigate the impact of varying the rank and sparsity parameters on model performance, but do not provide a clear guideline for determining the optimal combination of these parameters for a given task.
  - Why unresolved: The paper shows that the performance of models with low-rank and sparse connectivity is sensitive to the choice of rank and sparsity parameters. However, the optimal values for these parameters likely depend on the specific characteristics of the task and the model architecture.
  - What evidence would resolve it: A systematic study exploring the performance of models with different combinations of rank and sparsity parameters across a range of tasks. This could involve techniques such as grid search, random search, or Bayesian optimization to identify the optimal parameter settings for each task.

- **Open Question 3**: How does the proposed connectivity parameterization affect the robustness of models to different types of distribution shifts beyond the ones studied in the paper?
  - Basis in paper: The authors evaluate the robustness of models with low-rank and sparse connectivity to a set of distribution shifts in the ALE and MuJoCo environments, but do not explore the effects of other types of shifts.
  - Why unresolved: The paper only considers a limited set of distribution shifts, such as occlusions, brightness changes, and noise. It is unclear how well the proposed connectivity parameterization would generalize to other types of shifts, such as those caused by changes in the dynamics of the environment or the agent's physical capabilities.
  - What evidence would resolve it: Empirical results showing the performance of models with low-rank and sparse connectivity on a diverse set of tasks with different types of distribution shifts. This could involve systematically varying the characteristics of the shifts and measuring the resulting impact on model performance. Comparing these results to models with full-rank, fully-connected connectivity would help quantify the benefits and limitations of the proposed approach in handling different types of distribution shifts.

## Limitations

- The theoretical analysis provides a parameterization framework but does not fully explain why low-rank sparse connectivity specifically improves robustness under distribution shift
- The relationship between the task dimension gap and the need for lower-rank dynamics during deployment is asserted but not empirically validated
- The mechanism by which the time-constant network F improves robustness is described but not rigorously proven

## Confidence

- **High confidence**: Empirical results showing improved out-of-distribution performance with low-rank sparse connectivity
- **Medium confidence**: Theoretical analysis linking rank/sparsity to spectral properties and dynamics
- **Low confidence**: Specific mechanism explaining why the task dimension gap necessitates lower-rank dynamics during deployment

## Next Checks

1. **Empirical validation of the task dimension gap hypothesis**: Design experiments that explicitly vary the gap between training and deployment task dimensions to test whether lower-rank dynamics consistently improve robustness.

2. **Ablation study on time-constant network**: Remove the time-constant network F from CfCs and compare robustness under distribution shift to verify its contribution to the observed improvements.

3. **Spectral analysis across architectures**: Conduct a systematic comparison of spectral radii and norms across CfCs, LSTMs, RNNs, and GRUs under identical rank/sparsity constraints to validate the claim that CfCs are inherently more amenable to these priors.