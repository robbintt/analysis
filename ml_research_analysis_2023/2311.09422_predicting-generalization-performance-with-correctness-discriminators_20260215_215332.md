---
ver: rpa2
title: Predicting generalization performance with correctness discriminators
arxiv_id: '2311.09422'
source_url: https://arxiv.org/abs/2311.09422
tags:
- accuracy
- bounds
- tasks
- predicted
- lower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method to estimate upper and lower bounds\
  \ on the accuracy of a sequence-to-sequence NLP model on unseen, unlabeled data,\
  \ without requiring gold annotations. The core idea is to train a discriminator\
  \ that predicts whether the model\u2019s output on a given input is correct or not,\
  \ and then use an ensemble of such discriminators with voting mechanisms to establish\
  \ the bounds."
---

# Predicting generalization performance with correctness discriminators

## Quick Facts
- arXiv ID: 2311.09422
- Source URL: https://arxiv.org/abs/2311.09422
- Reference count: 8
- Primary result: A method to estimate tight upper and lower bounds on sequence-to-sequence NLP model accuracy on unseen, unlabeled data using correctness discriminators

## Executive Summary
This paper introduces a novel approach to estimate accuracy bounds for sequence-to-sequence NLP models on unseen, potentially out-of-distribution data without requiring gold annotations. The method trains discriminators to predict whether a model's output is correct or incorrect, then uses an ensemble of these discriminators with voting mechanisms to establish upper and lower bounds on accuracy. Experiments across semantic parsing, tagging, and parsing tasks show that the gold accuracy reliably falls between the predicted bounds, which are often tight. The mean of the bounds also provides a strong point estimate of accuracy, outperforming prior confidence-based methods.

## Method Summary
The method trains a sequence-to-sequence parser (T5-base) on training data, then collects intermediate checkpoints before perfect accuracy is reached. These checkpoints are used to generate negative examples by sampling incorrect predictions on training data. A correctness discriminator (also T5-base) is trained to predict if a parser output is correct or incorrect given an input-output pair. Multiple discriminators are ensembled using two voting mechanisms: ensemble_incorrect (labels as Incorrect if any discriminator predicts Incorrect) and ensemble_correct (labels as Correct if any discriminator predicts Correct). These mechanisms produce upper and lower bounds on accuracy for unlabeled test data.

## Key Results
- The gold accuracy reliably falls between the predicted upper and lower bounds across in-distribution and out-of-distribution tasks
- Bounds are often tight, with the mean of bounds providing a strong point estimate of accuracy
- The approach outperforms earlier confidence-based methods for accuracy estimation
- Effectiveness is demonstrated on semantic parsing, tagging, and parsing tasks

## Why This Works (Mechanism)

### Mechanism 1
A correctness discriminator trained on a parser's intermediate checkpoints can generalize to distinguish correct outputs from errors on unseen test data. The discriminator learns error patterns from the distribution of mistakes made during training, particularly before the parser achieves perfect accuracy. Because these early errors reflect systematic mistakes the parser is prone to, the discriminator can recognize these patterns and generalize to new, unseen inputs. This assumes errors made on training data are representative of errors on out-of-distribution test data.

### Mechanism 2
Ensembling multiple discriminators using ensemble_incorrect and ensemble_correct voting mechanisms yields tight and reliable upper and lower bounds on accuracy. The ensemble_incorrect mechanism is conservative, ensuring the lower bound is not overestimated by labeling an instance as Incorrect if any discriminator predicts Incorrect. The ensemble_correct mechanism is optimistic, ensuring the upper bound is not underestimated by labeling as Correct if any discriminator predicts Correct. This assumes different discriminators capture complementary aspects of correctness with uncorrelated errors.

### Mechanism 3
The mean of the upper and lower bounds provides a strong point estimate of accuracy, outperforming confidence-based baselines. The midpoint between conservative lower and optimistic upper bounds balances their tendencies to over- or underestimate accuracy. This simple heuristic implicitly captures the distribution of discriminator uncertainty and leverages the fact that bounds are often tight. This assumes the true accuracy is roughly centered between the ensemble-predicted bounds.

## Foundational Learning

- Concept: Binary classification and ensemble methods
  - Why needed here: The discriminator is a binary classifier distinguishing correct from incorrect predictions, and the ensemble combines multiple such classifiers to improve reliability.
  - Quick check question: How does majority voting in an ensemble help reduce the variance of predictions compared to a single classifier?

- Concept: Sequence-to-sequence modeling and evaluation metrics
  - Why needed here: The parser is a sequence-to-sequence model, and the evaluation relies on exact match accuracy. Understanding how to train, decode, and evaluate such models is essential.
  - Quick check question: What is exact match accuracy and why is it used instead of BLEU or other metrics for semantic parsing?

- Concept: Generalization bounds and out-of-distribution detection
  - Why needed here: The method aims to predict generalization performance on OOD data, which requires understanding how models behave when test data distribution shifts from training data.
  - Quick check question: Why might a model's confidence on OOD data not reflect its actual accuracy, and how do calibration methods attempt to address this?

## Architecture Onboarding

- Component map: Parser (T5-base) -> Discriminator (T5-base) -> Ensemble (voting mechanisms) -> Bounds (upper/lower) -> Point estimate (mean of bounds)

- Critical path: 1) Train parser on training set. 2) Collect intermediate checkpoints before perfect accuracy. 3) Generate negative examples from checkpoint errors on training data. 4) Train discriminators on (input, predicted output, correctness label) triples. 5) Apply discriminators to parser outputs on unlabeled test set. 6) Combine predictions via ensemble_incorrect and ensemble_correct to produce bounds. 7) Compute mean of bounds as point estimate. 8) Evaluate against gold accuracy if available.

- Design tradeoffs: Using intermediate checkpoints for negative examples trades training time for more realistic error distributions but may miss rare or novel error types. Hard voting in ensembles is simpler and more interpretable than probabilistic averaging but may be less robust to noisy discriminators. Averaging bounds for point estimation is simple but may not be optimal if bounds are systematically biased.

- Failure signatures: Bounds are very wide or do not contain gold accuracy (indicates discriminators are not generalizing well or errors are too diverse). Bounds are consistently above or below gold accuracy (indicates systematic bias in discriminators or ensemble voting). Point estimate from mean bounds performs worse than confidence-based methods (indicates bounds are not tight or balanced).

- First 3 experiments: 1) Train parser and collect checkpoints; verify that intermediate checkpoints produce errors on training data. 2) Train a single discriminator on checkpoint data; evaluate on a small labeled subset of test data to check correctness prediction quality. 3) Ensemble multiple discriminators on unlabeled test data; compute upper and lower bounds and compare to oracle accuracy if possible.

## Open Questions the Paper Calls Out

### Open Question 1
How well do correctness discriminators generalize across different sequence-to-sequence tasks beyond those tested in the paper (e.g., machine translation, summarization, code generation)? The authors state "It will also be interesting to explore whether our method can be extended to other tasks by predicting different metrics (e.g. BLEU) instead of exact match accuracy." This remains unresolved as the paper only evaluates on semantic parsing, tagging, and constituency parsing tasks.

### Open Question 2
How does the performance of correctness discriminators vary with different sizes and compositions of training data, particularly when gold accuracy is not perfect? The authors collect negative examples from intermediate parser checkpoints, which may not reflect all types of errors the parser can make. This is unresolved as the paper only uses a specific method to collect training data for discriminators.

### Open Question 3
Can the ensemble voting mechanisms (ensemble_correct and ensemble_incorrect) be further optimized or replaced with more sophisticated methods to improve bound tightness without sacrificing reliability? The authors use simple hard voting mechanisms but mention that "Different from previous works where model predictions are combined by averaging predicted probabilities, we use two hard voting mechanisms to calculate the upper and lower bounds of our predicted accuracy." This remains unexplored as the paper does not explore alternative ensemble methods.

## Limitations

- The method relies on the assumption that errors from intermediate parser checkpoints on training data are representative of errors on out-of-distribution test data, which may break down with substantial domain shifts.
- The approach requires selecting K (checkpoint interval) and the number of checkpoints for collecting negative examples, but the paper does not provide guidance on optimal values for these hyperparameters.
- The method is evaluated only with T5-base models for both the parser and discriminator, making it unclear whether the approach generalizes to other architectures or model capacities.

## Confidence

- High confidence: The core mechanism of using correctness discriminators with ensemble voting to establish upper and lower bounds is well-supported by the experimental results.
- Medium confidence: The effectiveness of using intermediate checkpoints for generating negative examples is supported by results but relies on implicit assumptions about error distribution similarity between training and test data.
- Low confidence: The generalizability of specific hyperparameter choices (checkpoint frequency, ensemble size) and the method's behavior with non-T5 architectures or in extreme OOD scenarios.

## Next Checks

1. Evaluate the method on a dataset with a known, large domain shift (e.g., news headlines to legal documents) to measure how the bounds degrade as the distribution shift increases. Measure CR/IR scores and bound tightness across varying degrees of domain similarity.

2. Replace T5 with other transformer architectures (BERT, RoBERTa, smaller models) for both the parser and discriminator. Compare bound quality and point estimate accuracy to assess architecture dependence.

3. Systematically vary K (checkpoint frequency) and the number of checkpoints used for negative example generation. Plot bound quality metrics (CR, IR, AE) as functions of these hyperparameters to identify optimal ranges and potential failure modes.