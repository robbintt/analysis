---
ver: rpa2
title: 'Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval
  Augmented Generation'
arxiv_id: '2311.04177'
source_url: https://arxiv.org/abs/2311.04177
tags:
- retrieval
- figure
- system
- performance
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Retrieval Augmented Generation (RAG)
  to improve the problem-solving abilities of LLMs without costly retraining. The
  proposed ARM-RAG system stores and retrieves reasoning chains ("rationales") from
  past successes to inform future problem-solving attempts.
---

# Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation

## Quick Facts
- **arXiv ID**: 2311.04177
- **Source URL**: https://arxiv.org/abs/2311.04177
- **Reference count**: 4
- **Primary result**: ARM-RAG achieves 77.4% accuracy on GSM8K test set, outperforming baseline LLM-only approach (73.2%)

## Executive Summary
This paper introduces ARM-RAG, a Retrieval Augmented Generation system that enhances LLM problem-solving by storing and retrieving reasoning chains ("rationales") from past successes. The system addresses the challenge of improving LLM performance without costly retraining by leveraging a knowledge base of previously successful problem-solving approaches. Experiments on grade-school math problems demonstrate that ARM-RAG achieves 77.4% accuracy, showing modest but meaningful improvement over baseline approaches.

## Method Summary
The ARM-RAG system operates by storing successful problem-solving rationales in a knowledge base, then retrieving and using them as hints when solving new problems. The approach uses Pyserini with Faiss for dense vector retrieval, with an optional obfuscation step that replaces nouns and proper names with nonsensical words to focus retrieval on structural similarity rather than surface details. The system combines retrieved rationales with target questions in prompts sent to GPT-3.5-turbo for final answer generation.

## Key Results
- ARM-RAG achieves 77.4% accuracy on GSM8K test set
- Baseline LLM-only accuracy: 73.2%
- Multiple attempts at problem-solving (5 times per question) improved training accuracy to 91.9%
- Obfuscation technique provided modest additional improvement over non-obfuscated retrieval

## Why This Works (Mechanism)

### Mechanism 1
ARM-RAG improves LLM problem-solving by retrieving and reusing reasoning chains from past successes. When solving new problems, the system searches for previously successful rationales that match the problem structure, then incorporates them into prompts to guide reasoning. This assumes structural similarity in problems leads to similar solution approaches.

### Mechanism 2
Query obfuscation during retrieval helps focus on structural problem features rather than surface details. By replacing nouns and proper names with nonsensical words, the system forces retrieval models to match based on problem structure and operations rather than specific entities, reducing the influence of superficial similarity.

### Mechanism 3
Multiple attempts at problem-solving with the same question can yield different results due to LLM stochasticity. Since LLMs produce varied outputs on repeated prompts, asking the same question multiple times increases the chance of getting a correct answer by exploring different reasoning paths.

## Foundational Learning

- **Information Retrieval with Dense Representations**: Understanding how dense representations capture semantic similarity is crucial since ARM-RAG uses Pyserini with Faiss for dense vector retrieval of rationales. *Quick check: How does dense retrieval differ from traditional keyword-based retrieval, and why is it more suitable for matching reasoning structures?*

- **Chain-of-Thought Reasoning**: The system stores and retrieves step-by-step reasoning chains that led to correct answers. *Quick check: What makes a good rationale for retrieval purposes, and how can you distinguish between rationales that capture problem structure versus those that just describe surface features?*

- **Prompt Engineering with Examples**: ARM-RAG constructs prompts by combining target questions with retrieved rationales. *Quick check: How does the format and content of retrieved examples in a prompt affect the quality of LLM-generated responses?*

## Architecture Onboarding

- **Component map**: LLM (gpt-3.5-turbo) -> Pyserini + Faiss retrieval -> GSM8K dataset -> ARM-RAG system
- **Critical path**: 1) Problem posed to system, 2) Problem optionally obfuscated, 3) Obfuscated problem used as query to retrieve similar rationales, 4) Retrieved rationales formatted with original problem, 5) Formatted prompt sent to LLM, 6) LLM generates answer using retrieved reasoning guidance
- **Design tradeoffs**: Dense vs sparse retrieval (semantic similarity vs computational resources), Obfuscation vs direct retrieval (reduces surface matching vs may make retrieval too difficult), Number of retrieval results (more guidance vs irrelevant examples)
- **Failure signatures**: Low accuracy despite many retrieval hits (retrieval finds superficial matches), High training but low test accuracy (possible overfitting or answer leakage), No improvement from obfuscation (retrieval too dependent on specific nouns)
- **First 3 experiments**: 1) Run single question multiple times (5-10 attempts) to establish baseline stochasticity, 2) Build basic ARM-RAG with non-obfuscated retrieval on training set only, 3) Add obfuscation to ARM-RAG and compare performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the research:

1. Does ARM-RAG's performance improvement on GSM8K generalize to other math problem datasets or domains?
2. What is the optimal balance between retrieval accuracy and rationale quality for maximizing ARM-RAG performance?
3. Does ARM-RAG improve LLM performance on problems where the LLM initially performs poorly?
4. How does ARM-RAG performance scale with the size and diversity of the rationale memory?

## Limitations
- The 4.2% accuracy improvement over baseline may be partly due to multiple attempts rather than rationale retrieval alone
- Obfuscation technique's impact is modest and implementation details are sparse
- Results are only demonstrated on grade-school math problems, limiting generalizability

## Confidence
- **High confidence**: LLM stochasticity (Mechanism 3) - well-established in literature with straightforward experimental evidence
- **Medium confidence**: ARM-RAG performance improvement through rationale retrieval (Mechanism 1) - results show improvement but contribution of multiple attempts not clearly separated
- **Medium confidence**: Obfuscation technique improves structural matching (Mechanism 2) - modest improvement with sparse implementation details

## Next Checks
1. Run ablation study with only single attempts to isolate contribution of rationale retrieval versus stochastic exploration
2. Manually examine 50 random test cases to verify retrieved rationales are structurally relevant, not superficially similar
3. Test ARM-RAG on a different math problem dataset (like MATH) to verify generalization beyond GSM8K patterns