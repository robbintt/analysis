---
ver: rpa2
title: 'In Search of Lost Online Test-time Adaptation: A Survey'
arxiv_id: '2310.20199'
source_url: https://arxiv.org/abs/2310.20199
tags:
- adaptation
- data
- otta
- batch
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey benchmarks online test-time adaptation (OTTA) methods
  using Vision Transformers (ViTs) across diverse datasets, including CIFAR-10/100-C,
  ImageNet-C, CIFAR-10.1, and CIFAR-10-Warehouse. OTTA techniques are categorized
  into optimization-, model-, and data-based approaches.
---

# In Search of Lost Online Test-time Adaptation: A Survey

## Quick Facts
- arXiv ID: 2310.20199
- Source URL: https://arxiv.org/abs/2310.20199
- Reference count: 40
- Primary result: Benchmarks OTTA methods using Vision Transformers across diverse datasets, revealing batch size as critical factor and transformers show enhanced resilience to domain shifts

## Executive Summary
This survey provides a comprehensive benchmark of online test-time adaptation (OTTA) methods using Vision Transformers (ViTs) across five diverse datasets including CIFAR-10/100-C, ImageNet-C, CIFAR-10.1, and CIFAR-10-Warehouse. The study categorizes OTTA techniques into optimization-, model-, and data-based approaches, revealing that transformers exhibit enhanced resilience to domain shifts compared to traditional backbones. Key findings highlight the critical importance of batch size, with larger batches significantly improving adaptation performance while small batches (size=1) lead to instability. The research introduces novel efficiency metrics and demonstrates that methods like RoTTA and SAR perform consistently well across challenging domains.

## Method Summary
The study benchmarks seven state-of-the-art OTTA methods (Tent, CoTTA, SAR, Conjugate PL, MEMO, RoTTA, TAST) adapted for ViTs using LayerNorm instead of BatchNorm. Experiments evaluate performance across CIFAR-10-C, CIFAR-100-C, ImageNet-C, CIFAR-10.1, and CIFAR-10-Warehouse datasets with batch sizes of 1 and 16. The implementation uses pre-trained ViT-base models from the Timm repository with Adam optimizer (learning rate 1e-3). Computational efficiency is measured using GFLOPs, wall clock time, and GPU memory usage, with each method tested under controlled conditions to assess stability, robustness, and adaptation quality across varying domain shifts.

## Key Results
- Transformers show enhanced resilience to domain shifts compared to traditional backbones
- Larger batch sizes significantly improve adaptation performance, with small batches (size=1) causing instability
- RoTTA and SAR methods demonstrate consistent high performance across challenging domains
- LayerNorm-based adaptation requires careful batch size selection for stable optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch size directly controls the stability of LayerNorm-based adaptation
- Mechanism: Small batch sizes provide insufficient statistical estimates for LayerNorm normalization, causing unstable gradient updates and poor adaptation
- Core assumption: LayerNorm requires sufficient sample statistics to produce stable normalization
- Evidence anchors:
  - [section]: "we study the impact of varying batch sizes on Tent's performance when used with the CIFAR-10-C dataset. The results indicate that the performance of Tent is heavily influenced by the batch size within the range of 1 to 16"
  - [corpus]: Weak evidence - no corpus papers directly address LayerNorm batch size effects
- Break condition: When batch size exceeds ~32, performance plateaus and additional samples provide diminishing returns

### Mechanism 2
- Claim: Uncertainty reduction through entropy minimization can backfire on highly shifted domains
- Mechanism: Entropy minimization drives confident predictions, but on highly shifted domains this confidence may be misplaced, leading to model bias
- Core assumption: The model's initial predictions on shifted data are noisy but contain some signal for adaptation
- Evidence anchors:
  - [section]: "In certain domains, uncertainty-based methods may not work as expected. Specifically, in domain G-09, most uncertainty-based methods tend to fail"
  - [abstract]: "transformers exhibit heightened resilience to diverse domain shifts"
- Break condition: When domain shift magnitude exceeds the model's capacity to recover meaningful signal from noisy predictions

### Mechanism 3
- Claim: Memory bank strategies provide stability by maintaining global test distribution statistics
- Mechanism: By preserving high-quality samples across batches, memory banks prevent catastrophic forgetting and provide consistent reference points for adaptation
- Core assumption: Test data contains sufficient informative samples that can be identified and preserved
- Evidence anchors:
  - [section]: "RoTTA is insensitive to batch size as it maintains a memory bank to save fresh and reliable data"
  - [section]: "preserving valuable sample information is crucial, particularly for challenging domains"
- Break condition: When memory capacity is exhausted or when test data lacks sufficient diversity to maintain representative samples

## Foundational Learning

- Concept: Domain adaptation fundamentals
  - Why needed here: Understanding covariate shift and the difference between source-free and online adaptation
  - Quick check question: What distinguishes online test-time adaptation from traditional domain adaptation?

- Concept: Batch normalization vs layer normalization
  - Why needed here: OTTA methods need to adapt normalization strategies when moving from CNNs to ViTs
  - Quick check question: How does LayerNorm compute statistics differently from BatchNorm?

- Concept: Entropy-based optimization
  - Why needed here: Many OTTA methods use entropy minimization as their core adaptation objective
  - Quick check question: What is the relationship between entropy minimization and model confidence?

## Architecture Onboarding

- Component map: ViT backbone -> LayerNorm layers -> Adaptation modules (optional) -> Loss functions -> Optimizer
- Critical path: Input -> Feature extraction -> LayerNorm adaptation -> Prediction -> Loss computation -> Parameter update
- Design tradeoffs: Batch size vs adaptation stability, memory usage vs performance, complexity vs generalization
- Failure signatures: Unstable loss curves, degraded performance on known domains, sensitivity to hyperparameter choices
- First 3 experiments:
  1. Run baseline inference without adaptation to establish performance floor
  2. Implement Tent with LayerNorm updates and test on CIFAR-10-C with varying batch sizes
  3. Add memory bank component to Tent and compare performance on challenging domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can OTTA methods be effectively adapted for large-scale vision-language models like CLIP in dynamic, domain-shifting scenarios?
- Basis in paper: [explicit] The paper discusses the need for OTTA methods tailored for large-scale models and mentions the emergence of visual language models like CLIP.
- Why unresolved: Large-scale models introduce new challenges in terms of computational efficiency, memory usage, and the need for domain-specific adaptation strategies that haven't been thoroughly explored.
- What evidence would resolve it: Comparative studies evaluating OTTA methods on large-scale models, benchmarks showing performance across diverse domain shifts, and analysis of computational trade-offs would provide clarity.

### Open Question 2
- Question: What unsupervised proxies can simulate unseen distribution shifts to train OTTA methods effectively?
- Basis in paper: [explicit] The paper highlights the challenge of training OTTA methods without access to source data and the need for unsupervised proxies to simulate future shifts.
- Why unresolved: Designing effective unsupervised proxies that generalize across diverse and unpredictable shifts remains an open problem, as existing methods often rely on assumptions that may not hold in real-world scenarios.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of unsupervised proxies in improving OTTA performance across a wide range of shifts would provide insights into their utility.

### Open Question 3
- Question: How can OTTA methods achieve stable and robust optimization with limited unlabeled test samples?
- Basis in paper: [explicit] The paper identifies stability and robustness in optimization as critical challenges, especially when dealing with small batch sizes and noisy data.
- Why unresolved: Current OTTA methods often struggle with instability in optimization, particularly when batch sizes are small or the data is noisy, leading to suboptimal adaptation.
- What evidence would resolve it: Empirical studies comparing different optimization strategies, analysis of stability metrics, and benchmarks showing performance under varying batch sizes and noise levels would help address this question.

## Limitations

- Batch size dependency findings limited to range 1-16, optimal batch size may vary across architectures
- Results focused on Vision Transformers with LayerNorm may not transfer to CNN architectures using BatchNorm
- Memory bank trade-offs and computational overhead not extensively analyzed

## Confidence

- Transformers Show Enhanced Resilience: High confidence
- Batch Size Critical for Stability: High confidence
- RoTTA and SAR Performance: High confidence
- LayerNorm vs BatchNorm Adaptation: Medium confidence

## Next Checks

1. Test the same OTTA methods on CNN backbones with BatchNorm to determine if batch size effects and performance rankings hold across architectures

2. Conduct detailed analysis of memory bank methods' computational overhead, particularly for RoTTA, to quantify stability vs efficiency trade-offs

3. Evaluate OTTA performance across wider range of batch sizes (up to 64 or 128) to identify optimal operating points and potential saturation effects