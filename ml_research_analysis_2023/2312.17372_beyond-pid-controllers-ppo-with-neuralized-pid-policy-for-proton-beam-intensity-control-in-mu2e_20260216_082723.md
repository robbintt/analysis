---
ver: rpa2
title: 'Beyond PID Controllers: PPO with Neuralized PID Policy for Proton Beam Intensity
  Control in Mu2e'
arxiv_id: '2312.17372'
source_url: https://arxiv.org/abs/2312.17372
tags:
- spill
- mu2e
- beam
- policy
- intensity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel RL-enhanced spill control system that
  integrates a neuralized PID controller into the policy function to address the challenge
  of maintaining uniform proton beam intensity delivery in the Mu2e experiment at
  Fermilab. The approach treats the accelerator system as a Markov Decision Process
  and employs Proximal Policy Optimization (PPO) to regulate the spill process, with
  the goal of creating an automated controller capable of providing real-time feedback
  and calibration of the Spill Regulation System parameters on a millisecond timescale.
---

# Beyond PID Controllers: PPO with Neuralized PID Policy for Proton Beam Intensity Control in Mu2e

## Quick Facts
- arXiv ID: 2312.17372
- Source URL: https://arxiv.org/abs/2312.17372
- Reference count: 10
- Primary result: 13.6% improvement in Spill Duty Factor (SDF), surpassing PID controller baseline by 1.6%

## Executive Summary
This paper presents a novel reinforcement learning approach to proton beam intensity control for the Mu2e experiment at Fermilab. The proposed method integrates a neuralized PID controller into a Proximal Policy Optimization (PPO) framework to regulate spill processes in real-time. By treating the accelerator system as a Markov Decision Process and using a differentiable simulator, the approach achieves significant improvements in spill regulation compared to traditional PID controllers, with a 13.6% enhancement in Spill Duty Factor.

## Method Summary
The approach employs PPO with a neuralized PID policy that combines traditional PID control with adaptive neural network adjustments. The policy network processes state information including proportional, integral, and derivative components along with previous actions. Spill intensity data is generated through a differentiable Mu2e simulator, and rewards are computed using an exponential moving average (EMA) function to reduce noise impact. The system trains to maximize the Spill Duty Factor, which measures spill intensity consistency.

## Key Results
- Achieves 13.6% improvement in Spill Duty Factor compared to PID baseline
- Surpasses current PID controller performance by an additional 1.6%
- Demonstrates effectiveness of neuralized PID policy in spill regulation
- Shows EMA-based reward function helps reduce impact of short-term fluctuations

## Why This Works (Mechanism)

### Mechanism 1
The neuralized PID controller captures both traditional PID control signals and additional state-dependent adjustments, enabling superior spill regulation. The policy network combines a trainable PID component with learnable KP, KI, KD parameters and a linear projection of the previous action. This hybrid design leverages PID's inductive bias while allowing neural adaptation to system dynamics and noise patterns.

### Mechanism 2
The EMA-based reward function reduces short-term fluctuation impact and helps identify underlying spill trends. Using rt = -EMA(t, α) with α = 0.5 gives more weight to recent spills while smoothing older variations. This creates a stable training signal that guides the policy toward consistent spill intensity rather than overreacting to momentary noise.

### Mechanism 3
PPO algorithm with clipped surrogate objective provides stable training while optimizing the neuralized PID policy. PPO constrains policy updates to prevent destructive large changes while allowing efficient learning. This stability is crucial when training on the differentiable Mu2e simulator where policy actions directly affect the physical system state.

## Foundational Learning

- **Reinforcement Learning and Markov Decision Processes**: Needed to frame spill regulation as sequential decision-making where actions affect future states. Quick check: What are the key components of an MDP that must be defined for PPO to work?

- **PID Control Theory**: Essential for understanding the neuralized PID policy's architecture and how proportional, integral, and derivative terms contribute to spill regulation. Quick check: How do KP, KI, and KD parameters in PID control relate to the spill regulation objective?

- **Exponential Moving Average (EMA)**: Used to smooth the reward signal and reduce short-term spill fluctuation impact. Quick check: How does the α parameter in EMA affect the trade-off between responsiveness and stability?

## Architecture Onboarding

- **Component map**: Simulator → State Extraction (PID components + previous action) → Policy Network → Action → Simulator Update → Reward Calculation → PPO Update

- **Critical path**: The simulator generates spill data, the PPO agent processes states and generates actions, and rewards are computed based on spill intensity deviation from target

- **Design tradeoffs**: Neuralized PID vs. pure neural network (inductive bias vs. flexibility), EMA smoothing factor α (responsiveness vs. stability), PPO vs. alternative RL algorithms (stability vs. sample efficiency)

- **Failure signatures**: High variance in SDF improvement across seeds, poor convergence during training, actions that cause spill intensity to diverge from target

- **First 3 experiments**:
  1. Baseline comparison: Run PID controller alone vs. neuralized PID policy on same simulator seeds to verify the 1.6% improvement claim
  2. EMA sensitivity: Test different α values (0.1, 0.5, 0.9) to understand impact on training stability and final performance
  3. State ablation: Remove PID components from state vector to confirm their importance in learning process

## Open Questions the Paper Calls Out

- **Real-time hardware deployment**: How would the RL-enhanced spill control system perform when deployed on real-time hardware rather than just in offline simulations? The paper presents only preliminary offline results from a simulator.

- **Optimal EMA smoothing factor**: What is the optimal EMA smoothing factor (α) value for maximizing SDF performance in real-world conditions? While multiple α values were tested in simulation, optimal value likely depends on real-world noise characteristics.

- **Transformer-based approaches**: Would transformer-based approaches with pre-trained models outperform the neuralized PID approach in long-term spill regulation? The paper suggests this as a future direction without experimental validation.

## Limitations

- Validation based solely on differentiable simulator rather than real accelerator hardware
- Limited hyperparameter exploration for critical components like EMA smoothing factor
- Unclear generalizability to different spill intensity profiles and noise patterns

## Confidence

- **Neuralized PID effectiveness**: Medium confidence - theoretical framework sound but limited to simulator validation
- **EMA reward function**: High confidence - well-established smoothing technique with proven benefits
- **PPO algorithm stability**: High confidence - extensive literature support but implementation details unclear

## Next Checks

1. Deploy the trained neuralized PID policy on the actual Mu2e accelerator to verify simulator-trained performance translates to real-world spill regulation.

2. Conduct systematic ablation studies removing or modifying components (PID terms, EMA smoothing, PPO vs. alternatives) to quantify individual contributions to the 13.6% SDF improvement.

3. Evaluate policy performance across broader operating conditions including different spill intensity profiles and noise patterns not present in training simulator.