---
ver: rpa2
title: 'SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks'
arxiv_id: '2310.03684'
source_url: https://arxiv.org/abs/2310.03684
tags:
- smoothllm
- adversarial
- arxiv
- which
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SmoothLLM, the first algorithm designed to
  mitigate jailbreaking attacks on large language models (LLMs). SmoothLLM addresses
  the vulnerability of popular LLMs like GPT, Llama, and Claude to adversarial-prompting-based
  jailbreaks, where an attacker fools the LLM into generating objectionable content.
---

# SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks

## Quick Facts
- arXiv ID: 2310.03684
- Source URL: https://arxiv.org/abs/2310.03684
- Reference count: 40
- One-line primary result: SmoothLLM reduces jailbreak attack success rates on numerous popular LLMs to below one percentage point.

## Executive Summary
This paper introduces SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on large language models (LLMs). The defense exploits the brittleness of adversarially-generated prompts to character-level perturbations by randomly perturbing multiple copies of an input prompt and aggregating the corresponding predictions to detect adversarial inputs. Experiments show that SmoothLLM significantly reduces the attack success rate of various jailbreak attacks on numerous LLMs, including Llama2, Vicuna, GPT-3.5, GPT-4, Claude-1, Claude-2, and PaLM-2, to below one percentage point.

## Method Summary
SmoothLLM defends against jailbreaking attacks by randomly perturbing multiple copies of an input prompt and aggregating the corresponding predictions from the LLM. The algorithm uses perturbation functions (insert, swap, patch) to modify the input prompt with a specified percentage of character changes. The LLM's responses to these perturbed prompts are then aggregated to produce a final output that is more resistant to adversarial suffixes. The method is designed to be compatible with any LLM and maintains high nominal performance while significantly reducing the attack success rate.

## Key Results
- SmoothLLM reduces attack success rates on numerous popular LLMs to below one percentage point.
- The defense is effective against various attack variants, including those that use adversarial suffixes.
- SmoothLLM maintains high nominal performance on standard NLP benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random perturbations break the structure of adversarial suffixes.
- Mechanism: SmoothLLM randomly perturbs multiple copies of an input prompt and aggregates the corresponding predictions to detect adversarial inputs.
- Core assumption: Adversarially-generated prompts are brittle to character-level changes.
- Evidence anchors:
  - [abstract] "Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs."
  - [section] "Our algorithmic contribution is predicated on the following previously unobserved phenomenon: The adversarial suffixes generated by GCG are fragile to character-level perturbations."
- Break condition: If adversarial prompts are not brittle to character-level changes, this mechanism would not work.

### Mechanism 2
- Claim: Aggregation of predictions from perturbed prompts improves robustness.
- Mechanism: SmoothLLM obtains a collection of perturbed prompts and aggregates the predictions corresponding to this collection.
- Core assumption: On average, perturbed prompts tend to nullify jailbreaks.
- Evidence anchors:
  - [abstract] "SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point."
  - [section] "Rather than passing a single perturbed prompt through the LLM, we obtain a collection of perturbed prompts, and we then aggregate the predictions corresponding to this collection."
- Break condition: If aggregation of predictions does not improve robustness, this mechanism would not work.

### Mechanism 3
- Claim: High-probability guarantee of attack mitigation.
- Mechanism: Under a realistic model of perturbation stability, SmoothLLM admits a high-probability guarantee that mitigates suffix-based attacks.
- Core assumption: The input prompt is k-unstable, meaning the attack fails when one changes k or more characters in the suffix.
- Evidence anchors:
  - [abstract] "Under a realistic model of perturbation stability, we provide a high-probability guarantee that SmoothLLM mitigates suffix-based attacks."
  - [section] "Deriving an expression for the DSP requires a relatively mild, yet realistic assumption on the perturbation stability of the suffix S, which we formally state in the following definition."
- Break condition: If the input prompt is not k-unstable, this mechanism would not work.

## Foundational Learning

- Concept: Adversarial robustness and randomized smoothing
  - Why needed here: SmoothLLM is based on the idea of randomized smoothing, which is a technique used in adversarial robustness to improve the robustness of machine learning models.
  - Quick check question: What is the main idea behind randomized smoothing and how does it relate to SmoothLLM?

- Concept: Jailbreaking attacks and adversarial prompting
  - Why needed here: SmoothLLM is designed to defend against jailbreaking attacks, which involve adversarial prompting to fool LLMs into generating objectionable content.
  - Quick check question: What are jailbreaking attacks and how do they exploit the vulnerabilities of LLMs?

- Concept: Perturbation functions and their impact on robustness
  - Why needed here: SmoothLLM uses perturbation functions to randomly perturb the input prompts, and understanding the impact of these perturbations on robustness is crucial for designing effective defenses.
  - Quick check question: What are perturbation functions and how do they affect the robustness of machine learning models?

## Architecture Onboarding

- Component map: Input prompt -> Perturbation function -> LLM -> Aggregation -> Output response
- Critical path: Input prompt → Perturbation function → LLM → Aggregation → Output response
- Design tradeoffs:
  - Number of samples (N) vs. computational cost: Increasing N improves robustness but also increases computational cost.
  - Perturbation percentage (q) vs. nominal performance: Increasing q improves robustness but may degrade nominal performance.
- Failure signatures:
  - High attack success rate: Indicates that the perturbations are not effective in breaking the structure of adversarial suffixes.
  - Low nominal performance: Indicates that the perturbations are too aggressive and degrade the quality of generated text.
- First 3 experiments:
  1. Evaluate the impact of different perturbation functions (insert, swap, patch) on the attack success rate.
  2. Assess the tradeoff between the number of samples (N) and the attack success rate.
  3. Investigate the impact of the perturbation percentage (q) on the nominal performance of the LLM.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas for further research are implied:

1. What is the exact relationship between the perturbation percentage q and the attack success rate (ASR) for different perturbation types (insert, swap, patch) across various LLMs?
2. How does the choice of the number of samples N in SmoothLLM affect its performance, and what is the optimal value of N for different perturbation percentages q?
3. Can SmoothLLM be adapted to defend against adversarial-prompting-based jailbreaks that do not involve appending suffixes to the input prompt?
4. What is the impact of SmoothLLM on the computational efficiency of LLMs, and how does it compare to other defense mechanisms in terms of resource usage?
5. How does the choice of the alphabet size v affect the performance of SmoothLLM, and what is the optimal alphabet size for different LLMs and perturbation types?

## Limitations

- The theoretical guarantees rely on the assumption that input prompts are k-unstable, which may not hold for all attack scenarios.
- The paper does not fully explore the computational overhead implications of the defense, particularly for real-time applications.
- The exact implementation details of the GCG attack algorithm and the specific perturbation functions used could affect reproducibility.

## Confidence

- Effectiveness claims: High (extensive empirical evaluation across 7 different LLMs and multiple attack variants)
- Nominal performance maintenance: High (well-supported by experimental results)
- Adaptive attack resistance: Medium (claimed but not extensively tested)
- Computational efficiency: Low (not thoroughly explored)

## Next Checks

1. **Cross-perturbation function evaluation**: Test SmoothLLM with alternative perturbation strategies beyond the three mentioned (insert, swap, patch) to verify the robustness of the defense mechanism is not overly dependent on specific perturbation choices.

2. **Adaptive adversary response**: Design and implement an adaptive attack that specifically targets the SmoothLLM defense by learning to construct suffixes that are more resistant to the random perturbations, testing the claim that SmoothLLM is resistant to adaptive attacks.

3. **Real-world deployment assessment**: Evaluate SmoothLLM's performance in a practical setting with live user inputs, measuring both the attack success rate against real adversarial attempts and the impact on legitimate user interactions to quantify the practical utility of the defense.