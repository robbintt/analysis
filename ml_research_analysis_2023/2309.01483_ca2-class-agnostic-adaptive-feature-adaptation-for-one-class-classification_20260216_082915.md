---
ver: rpa2
title: 'CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification'
arxiv_id: '2309.01483'
source_url: https://arxiv.org/abs/2309.01483
tags:
- samples
- features
- one-class
- class
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adapting pre-trained features
  for one-class classification when the number of classes in the target data is unknown.
  Existing methods assume either a single class or a known number of classes, which
  limits their applicability in real-world scenarios.
---

# CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification

## Quick Facts
- arXiv ID: 2309.01483
- Source URL: https://arxiv.org/abs/2309.01483
- Reference count: 13
- One-line primary result: CA2 achieves 90.2% average AUROC on multi-class experiments, surpassing state-of-the-art methods

## Executive Summary
This paper addresses the challenge of one-class classification when the number of classes in target data is unknown. Existing methods struggle with this scenario, assuming either single-class or known-class settings. The authors propose CA2 (Class-Agnostic Adaptive Feature Adaptation), which generalizes center-based approaches to handle unknown classes by leveraging the prior that pre-trained features from the same class are adjacent. CA2 adaptively clusters features tighter by using the mean of k nearest neighbors as the center, without requiring knowledge of class boundaries.

## Method Summary
CA2 adapts pre-trained features for one-class classification by clustering each sample toward the mean of its k nearest neighbors, exploiting the property that features from the same class are adjacent in the pre-trained feature space. The method uses a k-NN classifier for OCC scoring and includes a hard weighted variant that mitigates catastrophic forgetting by preventing edge samples from being pulled too far toward the center. The approach is validated across datasets ranging from 2 to 1024 classes and consistently outperforms state-of-the-art methods.

## Key Results
- CA2 achieves an average AUROC of 90.2% on multi-class experiments
- Outperforms state-of-the-art methods in cases with large numbers of classes
- Demonstrates consistent performance across the full spectrum of training data classes (1 to 1024)

## Why This Works (Mechanism)

### Mechanism 1
CA2 leverages the pre-trained feature adjacency property to adaptively cluster features without knowing the number of classes. By using k nearest neighbors of each sample as its center for adaptation, CA2 exploits the prior that pre-trained features of the same class are adjacent.

### Mechanism 2
CA2's adaptive clustering improves linear separability of features while maintaining outlier detection capability. Tighter clustering of same-class features creates more distinct clusters, improving OCC performance.

### Mechanism 3
CA2's hard weighted variant mitigates catastrophic forgetting by preventing edge samples from being pulled too far toward the center. Samples near the center are given higher weights in the loss function, protecting against overlap with outliers.

## Foundational Learning

- **Pre-trained feature space properties**: Understanding how pre-trained models encode features, specifically the adjacency property of same-class features. Why needed: CA2 relies on this property for adaptation. Quick check: What is the key property of pre-trained feature space that CA2 exploits?

- **One-class classification fundamentals**: Understanding OCC basics is crucial to grasp why adapting pre-trained features matters. Why needed: To understand the OCC paradigm. Quick check: What distinguishes one-class classification from traditional binary/multiclass classification?

- **Catastrophic forgetting in OCC**: Understanding CF is essential to appreciate CA2's design choices. Why needed: CA2 addresses CF, a key challenge in OCC adaptation. Quick check: Why is catastrophic forgetting particularly problematic in OCC compared to supervised learning?

## Architecture Onboarding

- **Component map**: Pre-trained backbone (ResNet50/ViT) -> Nearest neighbor search module -> CA2 adaptation module (with optional hard weighting) -> OCC classifier (k-NN) -> Evaluation metrics (AUROC, TPR95FPR)

- **Critical path**: Extract features from pre-trained model → Perform k nearest neighbor search → Apply CA2 adaptation using nearest neighbor centers → Evaluate with OCC classifier

- **Design tradeoffs**: Choice of k in nearest neighbor search (larger k may include more noise but provides more stable centers); dynamic threshold β in hard weighting (balances between preventing overlap and allowing adaptation); pre-trained model selection (models with better initial discrimination may work better with CA2)

- **Failure signatures**: Performance degradation as training progresses (catastrophic forgetting); inconsistent improvement across different numbers of classes; poor performance when pre-trained features lack adjacency property

- **First 3 experiments**: Single class experiment on CIFAR-10 one-vs-rest to verify basic functionality; multi-class experiment with 2-4 classes to test class-agnostic capability; ablation study on k parameter to understand its impact on performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but raises several implicit ones regarding scalability, performance on different data modalities, and sensitivity to hyperparameters.

## Limitations

- Requires k nearest neighbors searches before training, which becomes time-consuming for large datasets
- Performance depends on the quality of pre-trained features and their adjacency property
- Limited exploration of extreme class count scenarios (very few or very many classes)

## Confidence

- **High confidence**: CA2's mechanism of clustering samples toward their k nearest neighbors as centers, and the mathematical formulation of the adaptation objective
- **Medium confidence**: The claim that pre-trained features from the same class are adjacent, and that this property generalizes across different pre-trained models and datasets
- **Low confidence**: The assertion that CA2 is "class-agnostic" without requiring any hyperparameter tuning or adaptation based on the number of classes present in the target data

## Next Checks

1. Validate adjacency property: Conduct experiments to measure the distribution of distances between same-class and different-class features in pre-trained models across multiple architectures (ResNet, ViT, etc.) and pre-training objectives to empirically confirm the adjacency assumption.

2. Test extreme class count scenarios: Evaluate CA2 performance on datasets with very few classes (1-2) and very many classes (>1000) to verify the claim of consistent performance across the full spectrum of class counts.

3. Analyze catastrophic forgetting mitigation: Perform ablation studies on the hard weighted variant by systematically varying β and analyzing its effect on preventing catastrophic forgetting, including visualizing feature distributions before and after adaptation.