---
ver: rpa2
title: New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking
arxiv_id: '2312.02382'
source_url: https://arxiv.org/abs/2312.02382
tags:
- watermarking
- text
- judger
- watermarked
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two new evaluation methods for assessing
  LLM watermarking algorithms: an LLM-based judge and a binary classifier. The judge
  uses GPT-3.5-Turbo to score watermarked and unwatermarked text completions across
  multiple criteria, while the classifier employs text embeddings to distinguish between
  the two.'
---

# New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking

## Quick Facts
- arXiv ID: 2312.02382
- Source URL: https://arxiv.org/abs/2312.02382
- Reference count: 21
- Key outcome: New LLM-based judge and binary classifier methods show current watermarking techniques degrade text quality and are detectable, with over 70% classification accuracy.

## Executive Summary
This paper introduces two novel evaluation methods for assessing LLM watermarking algorithms: an LLM-based judge using GPT-3.5-Turbo to score text quality across seven criteria, and a binary classifier that distinguishes watermarked from unwatermarked text using embeddings. Experiments on three datasets with soft-watermark and distortion-free watermark techniques demonstrate that watermarked text consistently receives lower quality scores, particularly in coherence and depth. The binary classifier achieves over 70% accuracy in detecting watermarked text, revealing that current watermarking methods are detectable and compromise text quality, highlighting a fundamental trade-off between watermark robustness and text quality.

## Method Summary
The study employs two evaluation approaches: an LLM-based judge and a binary classifier. The judge uses GPT-3.5-Turbo with a structured prompt to evaluate watermarked and unwatermarked text across seven quality dimensions (relevance, depth, clarity, coherence, originality, examples, accuracy), using randomization to avoid positional bias. The binary classifier trains on text embeddings (OpenAI's text-embedding-ada-002) using both neural network and logistic regression approaches to distinguish between watermarked and unwatermarked text. The experiments use three datasets (LongForm, C4-RealNewsLike, and Scientific Papers) and two watermarking techniques, varying watermark strength parameters to analyze the trade-off between robustness and quality degradation.

## Key Results
- The LLM-based judge consistently prefers unwatermarked text, particularly due to lower coherence and depth scores
- Binary classifier achieves over 70% accuracy in detecting watermarked text, with logistic regression reaching up to 60%
- Stronger watermarking parameters increase both detectability and quality degradation
- Scientific Papers dataset showed the highest quality degradation with 84% preference for unwatermarked samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-judger scores degrade for watermarked text across multiple quality dimensions
- Mechanism: LLM-judger uses a structured prompt to evaluate watermarked vs unwatermarked text on 7 criteria, with randomization to avoid positional bias
- Core assumption: GPT-3.5-Turbo can reliably judge text quality across dimensions like coherence and depth
- Evidence anchors: [abstract] "Our experiments...reveal that current watermarking methods are detectable by even simple classifiers, challenging the notion of watermarking subtlety." [section] "Across the judging criteria provided in the prompt, the GPT-judger, on average, gave a higher score to the unwatermarked text completions"

### Mechanism 2
- Claim: Text embeddings contain detectable patterns that distinguish watermarked from unwatermarked text
- Mechanism: Binary classifier trained on text embeddings (OpenAI's text-embedding-ada-002) learns to differentiate watermarked and unwatermarked text
- Core assumption: Watermarking introduces consistent changes to text embeddings that can be learned by a classifier
- Evidence anchors: [abstract] "Experiments on three datasets with two watermarking techniques...show that the judge consistently prefers unwatermarked text" [section] "MLP-based classifier achieved over 70% accuracy in detecting watermarked text"

### Mechanism 3
- Claim: Stronger watermarking parameters increase detectability and quality degradation
- Mechanism: Varying watermark strength parameters (δ) shows trade-off between watermark robustness and text quality
- Core assumption: Increasing watermark strength creates more pronounced changes in text that affect both detectability and quality
- Evidence anchors: [abstract] "These results demonstrate that current watermarking methods are detectable and degrade text quality" [section] "As the watermarking strength was increased, the difference between the judger's scores for the unwatermarked and watermarked outputs also increased"

## Foundational Learning

- Concept: Text embedding spaces and how they capture semantic meaning
  - Why needed here: Understanding how text embeddings can capture watermarking patterns
  - Quick check question: How would changes in token selection affect the resulting text embedding?

- Concept: Evaluation metrics for text quality (coherence, depth, relevance)
  - Why needed here: Understanding what dimensions the LLM-judger evaluates and why watermarking affects them
  - Quick check question: Why might coherence scores differ between watermarked and unwatermarked text?

- Concept: Binary classification and cross-validation
  - Why needed here: Understanding how the binary classifier works and why cross-validation is important
  - Quick check question: Why use k-fold cross-validation when testing classifier performance across datasets?

## Architecture Onboarding

- Component map: Dataset → Text generation (Llama-2) → Watermarking (soft-watermark/distortion-free) → LLM-judger evaluation → Embedding extraction → Binary classifier training → Results analysis

- Critical path:
  1. Generate text completions with and without watermarking
  2. Run LLM-judger evaluation on all samples
  3. Extract embeddings and train binary classifier
  4. Analyze results across datasets and watermarking parameters

- Design tradeoffs:
  - LLM-judger vs simpler evaluation methods: More nuanced but costlier vs faster but less detailed
  - Classifier complexity: MLP vs logistic regression (accuracy vs simplicity)
  - Watermark strength: Higher detectability but more quality degradation

- Failure signatures:
  - LLM-judger giving similar scores to both types of text (may indicate prompt issues)
  - Classifier accuracy near random chance (may indicate embedding issues or watermark invisibility)
  - Inconsistent results across datasets (may indicate dataset bias or watermark sensitivity)

- First 3 experiments:
  1. Generate 100 samples each from LongForm, RealNewsLike, and Scientific Papers datasets with and without watermarking
  2. Run LLM-judger evaluation on all samples and calculate preference statistics
  3. Train binary classifier on embeddings from all samples and evaluate accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of watermark detection change with larger language models (e.g., GPT-4-Turbo, Llama-3) compared to GPT-3.5-Turbo and Llama-2-7B?
- Basis in paper: [inferred] The paper notes that GPT-4 showed an even higher preference (75%) for unwatermarked completions compared to GPT-3.5-Turbo, suggesting larger models may be more discerning.
- Why unresolved: The experiments were primarily conducted with GPT-3.5-Turbo and Llama-2-7B. While GPT-4 was tested on a subset, the paper does not provide comprehensive results for the latest and most capable models.
- What evidence would resolve it: Conduct experiments using GPT-4-Turbo, Llama-3, and other state-of-the-art models with the same evaluation methods (LLM judger and binary classifier) to compare detection rates and quality assessment across different model sizes.

### Open Question 2
- Question: What specific linguistic features or patterns in the watermarked text are most responsible for degradation in coherence, depth, and use of examples?
- Basis in paper: [explicit] The paper explicitly states that "soft-watermarking tends to negatively affect these attributes of the generated text" and shows that the judger's decisions were "most influenced by the lack in coherence, depth, and usage of examples in the watermarked text."
- Why unresolved: While the paper identifies these quality attributes as being affected, it does not perform a detailed linguistic analysis to pinpoint the exact features causing these issues.
- What evidence would resolve it: Perform a detailed linguistic analysis comparing watermarked and unwatermarked texts, including examining token-level statistics, syntactic structures, semantic coherence measures, and example usage patterns to identify specific features that correlate with quality degradation.

### Open Question 3
- Question: How does watermark detectability vary across different domains and text types (e.g., technical writing, creative fiction, legal documents) beyond the tested datasets?
- Basis in paper: [explicit] The paper tested three datasets (LongForm, C4-RealNewsLike, and Scientific Papers) and notes that "On the ScientificPapers dataset, the judger preferred 84% of the unwatermarked samples, indicating an even larger degradation in quality for this class of academic text generation."
- Why unresolved: The paper only tested three datasets, which may not represent the full diversity of text types where watermarking could be applied.
- What evidence would resolve it: Conduct experiments with a broader range of text types including technical documentation, creative writing, legal texts, medical literature, and other specialized domains to measure watermark detectability and quality impact across diverse content types.

## Limitations
- The seven-point scoring rubric's inter-rater reliability between human evaluators and GPT-3.5-Turbo is unknown
- Classifier accuracy may not generalize to future watermarking techniques designed to evade embedding-based detection
- The study demonstrates degradation exists but doesn't fully explain which watermarking mechanisms cause which specific quality issues

## Confidence
- LLM-judger consistency: Medium confidence - consistent preferences shown but seven-point rubric reliability unknown
- Binary classifier effectiveness: High confidence based on >70% accuracy, but may not generalize to new techniques
- Quality degradation patterns: Medium confidence - shows degradation exists but doesn't fully explain mechanisms

## Next Checks
1. Run the LLM-judger prompt on GPT-4 and Claude to verify consistency in quality assessments across different models
2. Conduct blind human evaluations of watermarked vs. unwatermarked text on the same seven criteria to establish ground truth correlation with LLM-judger scores
3. Implement a simple adversarial watermarking technique that randomizes token selection patterns and measure its impact on both LLM-judger scores and classifier accuracy to identify detection vulnerabilities