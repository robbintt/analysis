---
ver: rpa2
title: 'ToViLaG: Your Visual-Language Generative Model is Also An Evildoer'
arxiv_id: '2312.11523'
source_url: https://arxiv.org/abs/2312.11523
tags:
- toxicity
- toxic
- generation
- text
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of toxic degeneration in large-scale
  Visual-Language Generative Models (VLGMs), which can produce offensive text and
  pornographic/violent images, raising significant ethical risks. To investigate this
  problem, the authors construct ToViLaG, a dataset of 32K co-toxic/mono-toxic text-image
  pairs and 1K innocuous but evocative text prompts.
---

# ToViLaG: Your Visual-Language Generative Model is Also An Evildoer

## Quick Facts
- **arXiv ID:** 2312.11523
- **Source URL:** https://arxiv.org/abs/2312.11523
- **Reference count:** 40
- **Primary result:** Proposes WInToRe toxicity metric and SMIB detoxification method for VLGMs, demonstrating significant toxicity reduction while maintaining generation quality.

## Executive Summary
This work addresses the critical problem of toxic degeneration in Visual-Language Generative Models (VLGMs), which can produce harmful text and offensive images. The authors construct the ToViLaG dataset with 32K toxic text-image pairs and develop WInToRe, a novel toxicity metric that captures both input and output toxicity. They benchmark diverse VLGMs and discover that toxicity scales with model size. To mitigate this, they propose SMIB, an information bottleneck-based detoxification method that reduces toxicity while preserving generation quality.

## Method Summary
The paper introduces WInToRe, a toxicity metric that simultaneously captures input and output toxicity through threshold sweeps over toxicity probabilities. The detoxification method SMIB uses an information bottleneck approach, optimizing a mapping layer to maximize task-relevant information retention while minimizing toxicity-relevant information. The method is trained by minimizing a squared-loss mutual information objective, theoretically guaranteed to reduce toxic information in intermediate representations without significantly degrading generation quality.

## Key Results
- VLGMs exhibit increasing toxicity levels as model scale grows, similar to patterns observed in language models
- WInToRe metric provides a unified measurement capturing different aspects of toxicity (EMT, TP, ATR)
- SMIB detoxification method effectively reduces toxicity while maintaining satisfactory generation quality across multiple VLGMs
- Some VLGMs are more vulnerable to producing toxic content than others, highlighting the need for model-specific detoxification approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** WInToRe captures both input and output toxicity simultaneously, avoiding inconsistent metric readings.
- **Mechanism:** The metric uses a threshold sweep (τm) over toxicity probabilities and computes a difference between input and output toxicity rates, normalized over multiple thresholds.
- **Core assumption:** The toxicity classifier provides calibrated probability estimates for both text and images.
- **Evidence anchors:** Abstract states WInToRe "theoretically reflects different aspects of toxicity considering both input and output."
- **Break condition:** If the toxicity classifier is miscalibrated or inconsistent across modalities, the metric will misrepresent toxicity levels.

### Mechanism 2
- **Claim:** The detoxification method (SMIB) reduces toxic information in intermediate representations without significantly hurting generation quality.
- **Mechanism:** SMIB optimizes a mapping layer fθ to maximize SMI(y,z) while minimizing SMI(z,a), using a squared-loss mutual information formulation.
- **Core assumption:** The classifier pϕ(a|z) accurately predicts toxicity from the intermediate representation and the prior toxicity distribution p(a) is estimable.
- **Evidence anchors:** Abstract states the method "mitigates toxicity while keeping the satisfactory quality of VLG."
- **Break condition:** If the classifier cannot be trained to separate toxic from non-toxic representations, the bottleneck fails to remove toxicity.

### Mechanism 3
- **Claim:** Toxicity increases with model scale and with contamination of training data.
- **Mechanism:** Larger models have more capacity to memorize and reproduce harmful patterns from training data.
- **Core assumption:** Model capacity and data scale are the dominant drivers of toxicity.
- **Evidence anchors:** Abstract notes some models "produce more toxic content than expected" and section 4.2 shows discernible increase in toxicity with parameters.
- **Break condition:** If model size does not correlate with toxicity after controlling for data, or if toxicity is dominated by architectural differences.

## Foundational Learning

- **Concept:** Information Bottleneck (IB)
  - Why needed here: The detoxification method is framed as an IB problem where toxic information is removed while preserving task-relevant information.
  - Quick check question: Can you explain the difference between mutual information and squared-loss mutual information in the context of representation learning?

- **Concept:** Toxicity detection via classifiers
  - Why needed here: Both metric calculation and detoxification depend on reliable toxicity probability estimates.
  - Quick check question: What properties must a toxicity classifier have to be useful for VLG tasks?

- **Concept:** Multimodal generation pipelines (T2I and I2T)
  - Why needed here: Understanding how different model architectures handle input-output mapping is key to diagnosing and mitigating toxicity.
  - Quick check question: How does the architecture of a text-to-image model differ from that of an image-to-text model, and why does that matter for detoxification?

## Architecture Onboarding

- **Component map:** Input encoder -> Mapping layer fθ -> Toxicity classifier pϕ(a|z) -> VLG model backbone -> Output decoder
- **Critical path:** 1) Input → encoder → fθ → toxicity classifier → feedback to optimize fθ 2) Input → encoder → fθ → backbone → output (generation)
- **Design tradeoffs:**
  - Placement of fθ: After encoder vs. inside backbone vs. before output
  - Training stability: Optimizing fθ while freezing backbone may cause mismatch
  - Classifier accuracy: Critical for effective detoxification
- **Failure signatures:**
  - Detoxification fails: Output toxicity remains high; fθ loss plateaus
  - Generation quality degrades: ROUGE/CLIPScore drops significantly
  - Mode collapse: Model produces bland, low-diversity outputs
- **First 3 experiments:**
  1. Ablation of fθ placement: Compare after image encoder vs. inside text decoder on I2T toxicity reduction
  2. Classifier accuracy impact: Train pϕ(a|z) with varying toxic vs. non-toxic data and measure detox efficacy
  3. Scaling study: Vary model size (base vs. large vs. huge) and measure both toxicity and generation quality changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the detoxification method be effectively adapted for text-to-image models like Stable Diffusion?
- Basis in paper: The paper mentions applying SMIB to Stable Diffusion showed reduction in toxicity, but highlights challenges in determining efficient mapping layers for T2I models.
- Why unresolved: Heterogeneity and high randomness in T2I model architectures make it difficult to determine optimal intervention strategies.
- What evidence would resolve it: Successful implementation and evaluation of SMIB on multiple T2I models demonstrating significant toxicity reduction without compromising image quality.

### Open Question 2
- Question: What is the underlying mechanism of toxicity generation in VLGMs, and how can it be addressed?
- Basis in paper: The authors plan to investigate the underlying mechanism of toxicity generation in VLGMs in future work.
- Why unresolved: Current research focuses on measuring and mitigating toxicity but does not explore root causes of why VLGMs generate toxic content.
- What evidence would resolve it: Comprehensive study analyzing factors contributing to toxicity generation including data, architecture, and training methods.

### Open Question 3
- Question: How can the proposed WInToRe metric be extended to unimodal generation tasks like NLG?
- Basis in paper: The authors mention both metric and detoxification method are theoretically suitable for unimodal generation tasks.
- Why unresolved: Applicability to NLG tasks is not explored, and authors suggest further experiments are needed.
- What evidence would resolve it: Application of WInToRe to various NLG tasks demonstrating effectiveness in measuring and comparing toxicity of different models.

## Limitations

- Toxicity classifier accuracy is critical but not independently validated on held-out data, creating uncertainty in both WInToRe metric reliability and SMIB detox effectiveness
- The detoxification method optimizes only a mapping layer while keeping the backbone frozen, which may limit its ability to address toxicity sources embedded deeper in the model architecture
- Controlled experiments isolating model scale effects from other factors (architecture, training procedure) are absent, making it difficult to attribute toxicity increases solely to parameter count

## Confidence

- WInToRe metric design and theoretical formulation: **Medium** - Mathematical framework is sound but practical calibration across modalities remains unverified
- SMIB detoxification effectiveness: **Low-Medium** - Shows promise in reducing toxicity scores but generation quality preservation and classifier dependency create uncertainty
- Toxicity scaling with model size: **Medium** - Empirical correlation is shown but causal attribution to parameter count alone is not established

## Next Checks

1. Conduct independent validation of toxicity classifier accuracy and calibration on held-out toxic/non-toxic examples across both text and image modalities
2. Perform ablation studies comparing SMIB detox performance when placing the mapping layer at different positions in the generation pipeline (after encoder vs. before decoder)
3. Design controlled experiments varying model architecture, training data composition, and parameter count separately to isolate their individual contributions to toxicity levels