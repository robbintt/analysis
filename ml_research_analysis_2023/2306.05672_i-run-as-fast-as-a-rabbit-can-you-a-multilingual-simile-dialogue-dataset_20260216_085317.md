---
ver: rpa2
title: I run as fast as a rabbit, can you? A Multilingual Simile Dialogue Dataset
arxiv_id: '2306.05672'
source_url: https://arxiv.org/abs/2306.05672
tags:
- simile
- dialogue
- data
- tenor
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multilingual simile dialogue (MSD) dataset
  to study similes in dialogue. The dataset is manually annotated with 20K similes
  in both English and Chinese.
---

# I run as fast as a rabbit, can you? A Multilingual Simile Dialogue Dataset

## Quick Facts
- arXiv ID: 2306.05672
- Source URL: https://arxiv.org/abs/2306.05672
- Authors: 
- Reference count: 20
- One-line primary result: Proposes MSD dataset with 20K similes for English and Chinese, enabling simile recognition, interpretation, and generation tasks in dialogue

## Executive Summary
This paper introduces the Multilingual Simile Dialogue (MSD) dataset, a large-scale manually annotated dataset containing approximately 20,000 similes in both English and Chinese. The dataset is designed to study similes in real-life dialogue contexts where simile components can span multiple turns, be mentioned by different speakers, or occur in reversed order. The authors define five tasks using MSD: simile recognition, interpretation, generation, dialogue retrieval, and dialogue generation. Experiments with strong pre-trained models demonstrate the challenging nature of these tasks, highlighting the need for further research in this area.

## Method Summary
The MSD dataset is constructed by extracting simile candidates from large dialogue datasets (Reddit Dialogue for English, PchatbotW and LCCC for Chinese) and manually annotating them with simile components (tenor, vehicle, shared property, comparator). The dataset is used to define five tasks: simile recognition (distinguishing similes from literals), interpretation (selecting the correct shared property), generation (selecting the appropriate vehicle), dialogue retrieval (ranking simile-rich responses), and dialogue generation (generating simile-containing responses). Strong pre-trained models (BERT, ChatGLM, DialoGPT, GODEL, T5, BART, GPT-2, CDialGPT) are fine-tuned on MSD for these tasks, with evaluation using metrics such as Precision, Recall, F1, Hit@1, BLEU, ROUGE, and Distinct.

## Key Results
- BERT fine-tuned on MSD outperforms zero-shot large language models on simile recognition tasks in both English and Chinese.
- ConceptNet is used to construct challenging distractors for the simile interpretation task, ensuring semantic relatedness while maintaining true negatives.
- Response generation models (DialoGPT, GODEL) show improved performance on MSD data compared to original datasets, indicating the value of the enriched dialogue context.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Simile recognition benefits from fine-tuning BERT on the MSD dataset, achieving higher precision and F1 than zero-shot large language models.
- **Mechanism**: Fine-tuning BERT aligns the model's learned representations with the specific linguistic patterns of similes in dialogue, enabling better discrimination between similes and literals.
- **Core assumption**: The MSD dataset captures sufficient diversity in simile patterns for effective fine-tuning.
- **Evidence anchors**:
  - [abstract]: "Experiments with strong pre-trained models show the challenge of the proposed tasks."
  - [section]: "Table 6 shows the simile recognition results. We can see that BERT(fine-tuned) performs much better on Precision and F1 than ChatGLM on both MSD-En and MSD-Ch."
  - [corpus]: Weak - no specific citation overlap found in corpus analysis.
- **Break condition**: If the MSD dataset lacks diversity in simile forms, fine-tuning may not generalize well.

### Mechanism 2
- **Claim**: Simile interpretation and generation tasks are improved by using ConceptNet to construct distractors, ensuring they are semantically related but not correct answers.
- **Mechanism**: ConceptNet provides a structured knowledge base to find related concepts, allowing the creation of challenging distractors that test the model's understanding of simile properties.
- **Core assumption**: ConceptNet contains relevant and accurate semantic relationships for the tenor and vehicle concepts.
- **Evidence anchors**:
  - [abstract]: "The MSD is the largest manually annotated simile data (∼20K) and it contains both English and Chinese data."
  - [section]: "We construct the distractors with ConceptNet... To ensure the distractors are true-negative, we randomly select 50 dialogue examples and manually check the quality of the distractors."
  - [corpus]: Weak - no specific citation overlap found in corpus analysis.
- **Break condition**: If ConceptNet lacks coverage for certain tenor or vehicle concepts, distractor quality may suffer.

### Mechanism 3
- **Claim**: Response generation models perform better on MSD data than on the original data used to extract it, due to the richer context provided by MSD.
- **Mechanism**: MSD dialogue contexts contain more informative content, allowing generation models to leverage this context to construct more relevant and potentially simile-rich responses.
- **Core assumption**: The added context in MSD dialogues directly contributes to improved response quality.
- **Evidence anchors**:
  - [abstract]: "The MSD data can also be used on dialogue tasks to test the ability of dialogue systems when using similes."
  - [section]: "Table 9 shows the generation and completion results. On most metrics of English data, DialoGPT and GODEL perform better on MSD-En than on Reddit-dialogue."
  - [corpus]: Weak - no specific citation overlap found in corpus analysis.
- **Break condition**: If the additional context in MSD does not significantly differ from the original data, the performance improvement may not materialize.

## Foundational Learning

- **Concept**: Simile structure (tenor, vehicle, shared property, comparator)
  - **Why needed here**: Understanding the components of a simile is essential for tasks like recognition, interpretation, and generation.
  - **Quick check question**: What are the four main components of a simile, and how do they relate to each other?

- **Concept**: Dialogue context and its role in simile interpretation
  - **Why needed here**: Similes in dialogue can span multiple turns and involve different speakers, requiring understanding of the broader context.
  - **Quick check question**: How does the dialogue context influence the interpretation of a simile compared to a single sentence?

- **Concept**: Multilingual data processing and annotation
  - **Why needed here**: MSD contains both English and Chinese data, requiring understanding of language-specific nuances and annotation standards.
  - **Quick check question**: What are the key differences in simile structure and annotation between English and Chinese data?

## Architecture Onboarding

- **Component map**: Data collection pipeline -> Annotation interface -> Task-specific models -> Evaluation metrics
- **Critical path**: Data collection → Annotation → Task model training → Evaluation
- **Design tradeoffs**:
  - Balancing dataset size with annotation quality.
  - Choosing between fine-tuning pre-trained models vs. training from scratch.
  - Tradeoff between model complexity and inference speed.
- **Failure signatures**:
  - Low inter-rater agreement during annotation.
  - Poor performance on test sets compared to validation sets.
  - Generated responses lacking simile structure.
- **First 3 experiments**:
  1. **Simile Recognition**: Fine-tune BERT on MSD-En/Ch data and evaluate on test sets.
  2. **Simile Interpretation**: Use ConceptNet to create distractors and evaluate BERT-Probe on SI task.
  3. **Response Generation**: Compare DialoGPT and GODEL on MSD-En data, focusing on simile inclusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can simile generation models be improved to produce more accurate and diverse similes in dialogue?
- Basis in paper: [inferred] The paper mentions that most generated responses are not similes, even when the comparator is included in the input. The authors suggest that the baselines are not designed for this task and that a specific model design is needed to capture the simile relations in context.
- Why unresolved: The paper does not provide a concrete solution or model design for addressing this issue. It only highlights the need for further research in this area.
- What evidence would resolve it: A new model design or approach that demonstrates improved simile generation performance on the MSD dataset, with a higher percentage of generated responses containing accurate and diverse similes.

### Open Question 2
- Question: How can the simile interpretation and generation tasks be adapted for the Chinese data in the MSD dataset?
- Basis in paper: [explicit] The paper mentions that simile interpretation and generation tasks were not conducted on the MSD-Ch data due to the lack of shared property annotations in the Chinese data.
- Why unresolved: The authors state that they are currently working on annotating the shared properties in the Chinese data but do not provide a timeline or methodology for doing so.
- What evidence would resolve it: A methodology or approach for automatically annotating shared properties in the Chinese data, along with the results of simile interpretation and generation tasks on the MSD-Ch dataset using this annotation.

### Open Question 3
- Question: How can the simile recognition and interpretation tasks be improved by incorporating syntactic structure information?
- Basis in paper: [explicit] The paper mentions that using syntactic structure information to locate simile components may help the simile recognition task.
- Why unresolved: The paper does not provide a concrete method or model for incorporating syntactic structure information into these tasks.
- What evidence would resolve it: A new model or approach that incorporates syntactic structure information into simile recognition and interpretation tasks, demonstrating improved performance on the MSD dataset compared to the baselines presented in the paper.

## Limitations
- The paper lacks specific hyper-parameter details and pre-trained model checkpoints, which could impact reproducibility and comparison with other works.
- The paper's evaluation metrics focus on precision and recall for simile recognition, but do not provide insights into the quality of generated similes in the generation tasks.
- The paper mentions that most generated results are not similes, indicating a potential gap between the model's understanding of similes and its ability to generate them.

## Confidence
- **High Confidence**: The paper's dataset collection and annotation process, as well as the task definitions, are well-specified and can be reliably reproduced.
- **Medium Confidence**: The experimental results and model performance are presented clearly, but the lack of hyper-parameter details and specific model checkpoints may limit direct comparison with other works.
- **Low Confidence**: The paper's claims about the challenge of the proposed tasks and the potential for improved simile generation are not fully substantiated by the experimental results.

## Next Checks
1. **Reproduce Simile Recognition**: Fine-tune BERT on the MSD dataset using the provided implementation details and evaluate its performance on the test sets. Compare the results with the paper's reported values to verify the model's effectiveness.
2. **Evaluate Simile Interpretation**: Use ConceptNet to create distractors for the SI task and evaluate BERT-Probe's performance. Assess the quality of the distractors and the model's ability to select the correct shared property.
3. **Analyze Generated Responses**: Examine the generated responses from the dialogue generation models (DialoGPT, GODEL) on the MSD data. Analyze the frequency and quality of similes in the generated responses to understand the models' capabilities and limitations in generating simile-rich dialogues.