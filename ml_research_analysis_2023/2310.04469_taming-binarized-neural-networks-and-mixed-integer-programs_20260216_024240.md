---
ver: rpa2
title: Taming Binarized Neural Networks and Mixed-Integer Programs
arxiv_id: '2310.04469'
source_url: https://arxiv.org/abs/2310.04469
tags:
- subadditive
- neural
- functions
- o-minimal
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges the gap between binarized neural networks (BNNs)
  and mixed-integer programming (MIP) through the lens of tame geometry. The authors
  reformulate BNN training as the subadditive dual of a mixed-integer program, and
  show that this dual is locally o-minimal, making it amenable to non-smooth optimization
  techniques.
---

# Taming Binarized Neural Networks and Mixed-Integer Programs

## Quick Facts
- arXiv ID: 2310.04469
- Source URL: https://arxiv.org/abs/2310.04469
- Reference count: 40
- One-line primary result: Reformulates BNN training as subadditive dual of a MIP, proving it's tame and enabling backpropagation

## Executive Summary
This paper bridges the gap between binarized neural networks (BNNs) and mixed-integer programming (MIP) through the lens of tame geometry. The authors reformulate BNN training as the subadditive dual of a mixed-integer program, and show that this dual is locally o-minimal, making it amenable to non-smooth optimization techniques. This result allows for the use of automatic differentiation and backpropagation in BNN training, overcoming the limitations of existing methods like the Straight-Through-Estimator.

## Method Summary
The paper reformulates BNN training as a mixed-integer program (MIP) and then constructs its subadditive dual. By proving that this dual is tame (locally o-minimal), the authors enable the use of non-smooth optimization techniques and conservative set-valued fields for automatic differentiation. The key insight is that "nice" conic MIPs with compact feasible sets and finite non-differentiable points have tame subadditive duals, which allows backpropagation through the training process.

## Key Results
- BNN training can be reformulated as the subadditive dual of a mixed-integer program
- The subadditive dual of a "nice" conic MIP is tame (locally o-minimal)
- Conservative set-valued fields enable backpropagation through non-smooth, tame functions
- Illustrates the approach with a simple BNN example with three final layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binarized neural networks can be trained using backpropagation by reformulating them as a subadditive dual of a mixed-integer program (MIP).
- Mechanism: The training of BNNs is cast as a MIP, and its subadditive dual is shown to be tame (locally o-minimal). Tame functions admit conservative set-valued fields that provide a chain rule, enabling backpropagation through non-smooth functions.
- Core assumption: The MIP formulation of the BNN has a compact feasible set and finite non-differentiable points (making it "nice").
- Evidence anchors:
  - [abstract]: "By reformulating the problem of training binarized neural networks as a subadditive dual of a mixed-integer program, we show that binarized neural networks admit a tame representation."
  - [section]: "We show that this dual problem is tame, or definable in an o-minimal structure. This, in turn, makes it possible for the use of powerful methods from non-smooth optimization when training the BNN..."
  - [corpus]: Weak evidence; corpus contains related works on backpropagation-free methods but not the tame MIP duality approach.
- Break condition: If the MIP formulation is not compact or has infinitely many non-differentiable points, the tame property fails.

### Mechanism 2
- Claim: The subadditive dual of a "nice" MIP is locally o-minimal, enabling the use of Bolte et al.'s framework for implicit differentiation.
- Mechanism: For nice MIPs (compact feasible set, finite non-differentiable points), the subadditive dual admits a tame reformulation. Local o-minimality plus compactness implies o-minimality, which allows conservative fields with chain rules to be applied.
- Core assumption: The objective function of the MIP corresponds to a finite union of smooth components, ensuring only finitely many non-differentiable points.
- Evidence anchors:
  - [section]: "Proposition 1. The conic MIP of Theorem 1 is nice...The objective function is a finite sum of loss functions for the original BNN, as such it has a finite number of non-differentiable points."
  - [section]: "Theorem 4. Let us consider a nice conic MIP...For this MIP, there exists an equivalent reformulation which is definable in an o-minimal structure."
  - [corpus]: Weak evidence; no direct mention of subadditive duality or o-minimal structures in related papers.
- Break condition: If the feasible region is unbounded or the objective function has infinitely many non-differentiable points, the tame reformulation fails.

### Mechanism 3
- Claim: Conservative set-valued fields provide a chain rule for non-smooth functions, enabling backpropagation through the subadditive dual.
- Mechanism: The conservative field D_f of a locally Lipschitz tame function f satisfies d/dt f(x(t)) = ⟨v, ẋ(t)⟩ for v ∈ D_f(x(t)), allowing gradient propagation in automatic differentiation frameworks.
- Core assumption: The subadditive dual function is locally Lipschitz and tame.
- Evidence anchors:
  - [section]: "Jérôme Bolte and Edouard Pauwels...introduced a generalized derivative, called a conservative set-valued field, for non-smooth functions...Having a chain rule is key for applications to backpropagation algorithms and automatic differentiation in machine learning."
  - [section]: "This gives a formal mathematical model for propagating derivatives which can be applied to guarantee local convergence of mini-batch stochastic gradient descent with backpropagation for a large number of machine learning problems."
  - [corpus]: No direct evidence; related papers discuss backpropagation pitfalls but not conservative fields.
- Break condition: If the function is not locally Lipschitz or not tame, conservative fields cannot be applied.

## Foundational Learning

- Concept: Tame geometry and o-minimal structures
  - Why needed here: The paper relies on the fact that the subadditive dual of a "nice" MIP is definable in an o-minimal structure, which ensures nice topological and differentiability properties.
  - Quick check question: What are the key properties of sets and functions in an o-minimal structure, and why are they important for optimization?

- Concept: Subadditive duality for mixed-integer programs
  - Why needed here: The reformulation of BNN training as a subadditive dual of a MIP is the core mechanism enabling the use of tame geometry and conservative fields.
  - Quick check question: How does the subadditive dual relate to the primal MIP, and what are the conditions for strong duality?

- Concept: Conservative set-valued fields and non-smooth automatic differentiation
  - Why needed here: These provide the chain rule necessary for backpropagation through non-smooth, tame functions.
  - Quick check question: What is the key property of conservative fields that allows them to be used in automatic differentiation?

## Architecture Onboarding

- Component map:
  - BNN training problem → MIP formulation (Theorem 1)
  - MIP → Subadditive dual (Definition 2)
  - Subadditive dual → Tame reformulation (Theorem 4)
  - Tame function → Conservative field (Bolte et al.)
  - Conservative field → Backpropagation with chain rule (Corollary 1)

- Critical path:
  1. Formulate BNN training as a MIP
  2. Verify the MIP is "nice" (compact feasible set, finite non-differentiable points)
  3. Construct the subadditive dual
  4. Prove the dual is tame
  5. Apply conservative fields for backpropagation

- Design tradeoffs:
  - Pros: Enables backpropagation through non-smooth BNN activations, potentially improving training efficiency
  - Cons: Requires MIP formulation and subadditive duality, which may be computationally expensive; relies on the "niceness" assumption

- Failure signatures:
  - MIP formulation fails (e.g., non-linear constraints)
  - Feasible set is unbounded or objective has infinitely many non-differentiable points
  - Subadditive dual cannot be constructed or is not tame
  - Conservative fields do not provide a usable chain rule

- First 3 experiments:
  1. Implement the MIP formulation of a simple BNN (e.g., one hidden layer) and verify it is "nice"
  2. Construct the subadditive dual and test its properties (compactness, finite non-differentiable points)
  3. Apply conservative fields to the dual and implement a simple backpropagation step

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we efficiently implement algorithms for constructing the subadditive dual of mixed-integer programs?
- Basis in paper: [explicit] The authors note that while seven different algorithms exist for constructing the subadditive dual, their computational complexity and relative merits are not well understood.
- Why unresolved: The authors state that efficient implementation of these algorithms is necessary for further empirical experiments, but no specific algorithm or implementation details are provided.
- What evidence would resolve it: A comparative study of the existing algorithms for constructing the subadditive dual, including their computational complexity and empirical performance on a range of problems, would help resolve this question.

### Open Question 2
- Question: What are the implications of the "nice" property of conic mixed-integer programs for other optimization problems?
- Basis in paper: [explicit] The authors define "nice" conic mixed-integer programs and show that their subadditive duals are tame, but they also note that this class could contain a number of other problems beyond binarized neural networks.
- Why unresolved: The authors suggest that the "nice" property may be applicable to other problems, but they do not provide specific examples or explore the implications of this property for other optimization problems.
- What evidence would resolve it: A systematic study of the "nice" property for a range of optimization problems, including an analysis of the implications of this property for the structure and solvability of these problems, would help resolve this question.

### Open Question 3
- Question: How can the results of this paper be applied to other types of neural networks beyond binarized neural networks?
- Basis in paper: [inferred] The authors show that binarized neural networks can be reformulated as mixed-integer programs with tame subadditive duals, which makes them amenable to non-smooth optimization techniques. This suggests that other types of neural networks with discrete or non-smooth components may also be reformulated in a similar way.
- Why unresolved: The authors do not provide specific examples or explore the implications of their results for other types of neural networks.
- What evidence would resolve it: A systematic study of the applicability of the results of this paper to other types of neural networks, including an analysis of the challenges and opportunities of applying these results to different types of neural networks, would help resolve this question.

## Limitations
- The "niceness" assumption for MIPs is crucial but not fully characterized in practice
- Computational overhead of MIP reformulation and subadditive duality is not discussed
- Implementation details for conservative fields and backpropagation are hand-waved

## Confidence
- High: The mathematical framework connecting tame geometry to BNN training is internally consistent and well-founded
- Medium: The specific example of a "nice" MIP for BNN training is correctly proven
- Low: Practical implications and computational feasibility for real-world BNN applications

## Next Checks
1. **Scalability test**: Implement the MIP formulation for a small BNN (2-3 layers) and measure training time vs. standard BNN methods
2. **Generalization test**: Apply the framework to a non-trivial BNN architecture (e.g., CNN on CIFAR-10) and verify "niceness" conditions hold
3. **Derivative verification**: Implement conservative field backpropagation and compare gradients against finite-difference approximations on simple test functions