---
ver: rpa2
title: Open, Closed, or Small Language Models for Text Classification?
arxiv_id: '2308.10092'
source_url: https://arxiv.org/abs/2308.10092
tags:
- llama
- political
- roberta
- task
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares three classes of language models (closed generative
  LLMs like GPT-3.5/GPT-4, open generative LLMs like Llama 2, and smaller supervised
  models like RoBERTa) on text classification tasks including NER, political ideology
  prediction, and misinformation detection. While larger LLMs often lead to improved
  performance, open-source models can rival their closed-source counterparts by fine-tuning.
---

# Open, Closed, or Small Language Models for Text Classification?

## Quick Facts
- arXiv ID: 2308.10092
- Source URL: https://arxiv.org/abs/2308.10092
- Authors: 
- Reference count: 40
- Key outcome: Open-source models can rival closed-source LLMs through fine-tuning, while smaller supervised models like RoBERTa can match or exceed generative LLMs on well-defined classification tasks.

## Executive Summary
This paper compares three classes of language models - closed generative LLMs (GPT-3.5, GPT-4), open generative LLMs (Llama 2), and supervised smaller models (RoBERTa) - on text classification tasks including named entity recognition, political ideology prediction, and misinformation detection. The study finds that while larger LLMs often lead to improved performance, open-source models can rival their closed-source counterparts when fine-tuned, and supervised smaller models can achieve similar or even greater performance in many datasets. However, closed models maintain an advantage in hard tasks that demand the most generalizability. The research emphasizes the importance of model selection based on task requirements and demonstrates that prompt engineering and fine-tuning techniques are critical for optimizing model performance.

## Method Summary
The study evaluates models using zero-shot, few-shot, and fine-tuning approaches across eight datasets spanning three tasks: named entity recognition (CoNLL 2003, WNUT 2017, WikiNER-EN), political ideology prediction (2020 Election, COVID-19, 2021 Election), and misinformation detection (LIAR, CT-FAN-22). Models are compared using F1-score for NER and political ideology prediction, accuracy for LIAR, and macro-F1 for CT-FAN-22. The research specifically examines prompt engineering differences between zero-shot and few-shot settings, and applies LoRA fine-tuning to open-source models. The evaluation framework systematically compares performance across model classes while considering cost and transparency trade-offs.

## Key Results
- Supervised smaller models like RoBERTa can match or exceed generative LLMs on well-defined classification tasks when task patterns are consistent
- Open-source models can rival closed-source LLMs through fine-tuning techniques like LoRA
- Prompt engineering significantly impacts generative LLM performance, with different prompt styles working better in different settings
- Closed models maintain advantages in highly generalizable tasks requiring broad reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised smaller models like RoBERTa can match or outperform larger generative LLMs in classification tasks when task patterns are well-defined.
- Mechanism: RoBERTa learns non-semantic patterns in training data that help it specialize on specific classification tasks, rather than relying on general reasoning abilities.
- Core assumption: The classification task has consistent patterns that can be learned from training data rather than requiring broad generalization.
- Evidence anchors:
  - [abstract]: "supervised smaller models, like RoBERTa, can achieve similar or even greater performance in many datasets compared to generative LLMs"
  - [section]: "Supervised smaller models can leverage non-semantic training data patterns, but become specialized and lose generalizability"
  - [corpus]: Weak - corpus papers focus on different model comparisons not specific to this mechanism
- Break condition: When the task requires broad generalization across diverse, evolving data where RoBERTa's learned patterns become less applicable.

### Mechanism 2
- Claim: Prompt engineering significantly impacts performance of generative LLMs, with different prompt styles working better in different settings.
- Mechanism: The format and structure of prompts affects how models process and respond to classification tasks, with structured formats like JSON working better for few-shot learning.
- Core assumption: The model's training included exposure to structured data formats that it can leverage during inference.
- Evidence anchors:
  - [abstract]: "Prompt engineering and other techniques are critical to getting strong results from generative LLMs"
  - [section]: "Prompts that work well in zero-shot setting do not necessarily work well for few-shot setting" and comparison of Serial vs JSON prompting results
  - [corpus]: Weak - corpus papers don't specifically address prompt engineering differences between zero-shot and few-shot settings
- Break condition: When prompt styles that work for one model (like GPT) don't transfer to similar models (like Llama 2), requiring task-specific prompt engineering.

### Mechanism 3
- Claim: Open-source models can be fine-tuned to outperform closed-source models in specific tasks.
- Mechanism: Fine-tuning allows adaptation of open-source models to task-specific patterns, providing advantages that closed models lack due to their fixed nature.
- Core assumption: The open-source model has sufficient capacity and the fine-tuning process can effectively adapt it to the task.
- Evidence anchors:
  - [abstract]: "open-source models can rival their closed-source counterparts by fine-tuning"
  - [section]: "With access to open-source models, we can further train the model to beat closed-source models" and LoRA fine-tuning results
  - [corpus]: Weak - corpus papers focus on different aspects of model evaluation not specifically on fine-tuning advantages
- Break condition: When the task is too complex or data-limited for effective fine-tuning, or when the computational cost of fine-tuning outweighs the benefits.

## Foundational Learning

- Concept: Named Entity Recognition (NER) fundamentals
  - Why needed here: The paper evaluates NER performance across multiple datasets, requiring understanding of entity types, evaluation metrics like F1-score, and common benchmark datasets
  - Quick check question: What are the four main entity types evaluated in the CoNLL 2003 dataset used in this study?

- Concept: Political ideology prediction methodology
  - Why needed here: The paper compares explicit (profile-based) and implicit (tweet-based) political classification, requiring understanding of different feature types and annotation approaches
  - Quick check question: How does the paper handle the difference between explicit and implicit political ideology prediction tasks?

- Concept: Misinformation detection approaches
  - Why needed here: The paper evaluates models on misinformation detection using different datasets and metrics, requiring understanding of truth verification and evaluation challenges
  - Quick check question: Why does the paper use different evaluation metrics (accuracy vs macro-F1) for the two misinformation datasets?

## Architecture Onboarding

- Component map: Closed LLMs (GPT-3.5, GPT-4) -> Open LLMs (Llama 2) -> Supervised models (RoBERTa) -> Three tasks (NER, Political Ideology, Misinformation) -> Eight datasets -> Three evaluation approaches (zero-shot, few-shot, fine-tuning)
- Critical path: Model selection → Prompt engineering / fine-tuning setup → Dataset evaluation → Performance comparison across model classes
- Design tradeoffs: Cost vs performance (GPT-4 expensive but potentially better), transparency vs capability (closed vs open models), generalization vs specialization (RoBERTa vs LLMs)
- Failure signatures: Poor performance when task requires generalization beyond training patterns, when prompt engineering is insufficient, or when computational resources limit model choice
- First 3 experiments:
  1. Compare zero-shot performance of all three model classes on a simple dataset like CoNLL 2003 NER
  2. Test different prompt styles (Serial vs JSON) on Llama 2 for the same task to observe prompt engineering impact
  3. Evaluate RoBERTa fine-tuning performance on a political ideology dataset to establish baseline for smaller models

## Open Questions the Paper Calls Out

- Question: Can LoRA fine-tuning be effectively applied to improve Llama 2's performance on other text classification tasks beyond NER?
- Question: How does the performance of open-source models like Llama 2 change with different fine-tuning strategies beyond LoRA?
- Question: What specific aspects of prompt engineering contribute most to performance differences between zero-shot and few-shot settings?
- Question: How do closed-source models like GPT-4 maintain their advantage in highly generalizable tasks compared to open-source alternatives?

## Limitations
- Performance differences may be partially attributed to dataset characteristics rather than fundamental model class advantages
- Evaluation focuses primarily on accuracy metrics without extensive analysis of computational costs or deployment considerations
- Limited exploration of model robustness to adversarial examples or out-of-distribution data
- Reliance on specific datasets and tasks that may not generalize to all text classification scenarios

## Confidence
- High: Supervised smaller models like RoBERTa can achieve comparable or superior performance to generative LLMs on well-defined classification tasks
- Medium: Open-source models can rival closed-source models through fine-tuning
- Medium: Prompt engineering significantly impacts generative LLM performance

## Next Checks
1. Test the same model comparisons on additional classification tasks with different characteristics (e.g., multi-label classification, long document classification) to assess generalizability of the findings
2. Conduct ablation studies on prompt engineering techniques by systematically varying prompt structures and measuring performance impact across all model classes
3. Analyze computational efficiency trade-offs by measuring inference times and costs for each model class while maintaining comparable performance levels