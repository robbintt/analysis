---
ver: rpa2
title: Comparing normalizing flows and diffusion models for prosody and acoustic modelling
  in text-to-speech
arxiv_id: '2307.16679'
source_url: https://arxiv.org/abs/2307.16679
tags:
- prosody
- acoustic
- diffusion
- speech
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares L1/L2-based approaches, normalizing flows, and
  diffusion models for prosody and acoustic modelling in text-to-speech synthesis.
  For acoustic modelling, a flow-based model outperforms equivalent diffusion and
  L1 models.
---

# Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech

## Quick Facts
- arXiv ID: 2307.16679
- Source URL: https://arxiv.org/abs/2307.16679
- Authors: 
- Reference count: 0
- Primary result: Flow-based acoustic model achieves best performance, with both flow and diffusion models significantly improving prosody expressiveness over L2 baselines

## Executive Summary
This paper compares L1/L2-based approaches with normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech synthesis. The authors evaluate three model architectures (L1/L2 loss, normalizing flows, diffusion) for both prosody prediction (log-f0 and duration) and acoustic prediction (mel-spectrograms). The flow-based acoustic model demonstrates the best overall performance, while both flow and diffusion models significantly improve prosody expressiveness compared to L2-based approaches. Objective metrics show that the proposed models produce more dynamic features with higher standard deviations and better match the distribution of oracle features.

## Method Summary
The paper evaluates three model architectures for TTS: L1/L2 loss-based, normalizing flow-based, and diffusion-based models for both prosody and acoustic components. All models share a common encoder architecture but differ in their decoder design and optimization strategy. Prosody models predict phoneme-level log-f0 and duration, while acoustic models predict mel-spectrograms. The flow models use invertible transformations to map complex distributions to simple priors, while diffusion models employ U-Net structures to capture long-term dependencies. Models are trained on an internal dataset of 200 speech hours from 116 native English speakers, with evaluation using both subjective MUSHRA tests and objective metrics including feature distribution matching.

## Key Results
- Flow-based acoustic model achieves the best performance for mel-spectrogram prediction, outperforming equivalent diffusion and L1 models
- Both diffusion and flow-based prosody predictors result in significant improvements over typical L2-trained prosody models
- Generated features from flow and diffusion models show higher standard deviations than L2 predictions, indicating more expressive prosody
- Flow-based acoustic model with temperature tuning τ=1.0 achieves optimal naturalness scores

## Why This Works (Mechanism)

### Mechanism 1
Flow-based acoustic models outperform diffusion and L1/L2 models because they learn exact likelihood estimation through invertible transformations. Normalizing flows transform complex target distributions into simple priors using invertible layers, allowing exact likelihood computation rather than approximating with strong distributional assumptions. The core assumption is that the target mel-spectrogram distribution can be effectively modeled by transforming it into a simpler distribution through invertible neural network layers. This works because the flow learns to map from an unknown complex distribution to a vector from a simpler prior distribution, optimizing for the exact log-likelihood of the data distribution.

### Mechanism 2
Diffusion models produce more expressive prosody features because they capture long-term dependencies better than L2-based approaches. The U-Net structure in diffusion models processes information across multiple scales, allowing better modeling of prosodic patterns that span longer time ranges compared to frame-level L2 regression. The core assumption is that prosody features like f0 and duration have long-term dependencies that benefit from multi-scale processing architectures. This is effective because the U-Net structure within the diffusion model is good at generating features considering long-term dependencies, a desirable trait for prosody modelling.

### Mechanism 3
L1/L2 loss approaches produce over-smoothed predictions because they assume Gaussian/Laplacian distributions for target features. Standard regression losses optimize for the mean of the assumed distribution, which for symmetric distributions like Gaussian/Laplacian results in averaged, less expressive outputs that lack variation. The core assumption is that the true distribution of speech features is multimodal or heavy-tailed, not well-represented by simple symmetric distributions. This strong assumption often results in an "over-smoothed" prediction, e.g., flat speech or a blurred image.

## Foundational Learning

- **Invertible neural network architectures**: Normalizing flows rely on invertible transformations to map between complex data distributions and simple priors, requiring understanding of coupling layers, affine transformations, and determinant computation. *Quick check*: What property must every layer in a normalizing flow maintain to ensure the entire network is invertible?

- **Score-based generative modeling**: Diffusion models learn to predict noise at various timesteps using score functions, requiring understanding of stochastic differential equations and noise schedule design. *Quick check*: How does the score function relate to the gradient of the log probability density in diffusion models?

- **Distribution matching metrics**: The paper uses Jensen-Shannon divergence to compare generated and oracle feature distributions, requiring understanding of probability metrics and their interpretation. *Quick check*: What does a lower Jensen-Shannon divergence value indicate about the similarity between two probability distributions?

## Architecture Onboarding

- **Component map**: Input text → phoneme sequence + style embedding → shared encoder → conditioning representation → prosody model → phoneme-level log-f0 and duration → upsampled conditioning → acoustic model → mel-spectrogram → vocoder → time-domain waveform

- **Critical path**: 1. Input text → phoneme sequence + style embedding 2. Shared encoder → conditioning representation 3. Prosody model → phoneme-level log-f0 and duration 4. Upsample conditioning → frame-level representation 5. Acoustic model → mel-spectrogram 6. Vocoder → time-domain waveform

- **Design tradeoffs**: Flow vs Diffusion: Flows offer exact likelihood but require invertible architecture constraints; Diffusion provides flexibility but needs careful noise scheduling and temperature tuning. Frame-level vs Phoneme-level features: Frame-level offers finer granularity but increases computational cost and may not capture joint duration-f0 relationships as effectively. Conditioning strategy: Providing oracle features during training vs generated features during inference creates train-test mismatch that must be managed.

- **Failure signatures**: Flow model collapse: Mode dropping in generated spectrograms, suggesting latent space insufficient to capture data distribution. Diffusion instability: Training divergence or poor sample quality, indicating score function estimation problems. Over-smoothing persistence: Generated speech still sounds flat despite using flow/diffusion approaches, suggesting conditioning information insufficient or model capacity inadequate.

- **First 3 experiments**: 1. Train L1 acoustic model with oracle prosody to establish baseline performance and identify any immediate architectural issues. 2. Implement flow-based prosody model and compare generated feature distributions against oracle using objective metrics before subjective evaluation. 3. Train flow-based acoustic model with oracle prosody and evaluate temperature sensitivity on naturalness to determine optimal sampling strategy.

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of normalizing flows versus diffusion models vary across different speaking styles and emotional content in text-to-speech synthesis? The paper notes that flow-PM and diff-PM significantly outperform L2-PM for high arousal styles like anger and happiness, but does not extensively analyze differences between flow and diffusion across various speaking styles. Comparative experiments measuring naturalness, style similarity, and expressiveness of flow-PM and diff-PM across a wide range of speaking styles and emotional content would resolve this.

### Open Question 2
What are the computational trade-offs between normalizing flows and diffusion models in terms of training time, inference speed, and memory usage for text-to-speech synthesis? The study focuses on the quality of generated speech but does not provide information on the computational resources required for training and inference of the different models. Comparative analysis of training time, inference speed, and memory usage for flow-based and diffusion-based models in text-to-speech synthesis would resolve this.

### Open Question 3
How does the choice of temperature parameter τ impact the trade-off between expressivity and quality in normalizing flows and diffusion models for different tasks in text-to-speech synthesis? While the paper identifies optimal temperatures for the investigated models, it does not explore the broader implications of temperature choice on the balance between expressivity and quality across different tasks and model architectures. Systematic exploration of temperature effects on expressivity and quality for various flow-based and diffusion-based models across different tasks in text-to-speech synthesis would resolve this.

## Limitations

- Evaluation relies heavily on subjective MUSHRA tests with relatively small listener pools
- Study focuses on a single internal dataset (200 hours, 116 speakers) without external validation
- Comparison between flow and diffusion models is limited to equivalent architectures, not exploring full design space
- Does not investigate computational efficiency or inference speed, critical for practical deployment

## Confidence

**High Confidence:**
- Flow-based acoustic models outperform L1/L2 baselines on this dataset
- Both flow and diffusion models improve prosody expressiveness over L2 baselines
- Generated features from flow/diffusion models show higher variance than L2 predictions

**Medium Confidence:**
- Flow models achieve "best overall performance" based on combined objective/subjective metrics
- U-Net structure specifically benefits prosody modeling through long-term dependency capture
- The temperature tuning methodology adequately addresses over-smoothing concerns

**Low Confidence:**
- Generalization claims to other datasets or languages
- Computational efficiency comparisons between approaches
- Long-term stability of flow-based acoustic models in production scenarios

## Next Checks

1. **Cross-dataset validation**: Train and evaluate the flow-based acoustic model on at least two external TTS datasets (e.g., LJ Speech, VCTK) to verify performance claims generalize beyond the internal corpus.

2. **Ablation study on conditioning strategy**: Systematically compare oracle vs predicted prosody features during acoustic model training and inference to quantify the train-test mismatch impact on final speech quality.

3. **Inference efficiency benchmarking**: Measure real-time factor and parameter counts for all three acoustic model variants (L1, flow, diffusion) to provide practical deployment guidance beyond quality metrics.