---
ver: rpa2
title: 'GreekT5: A Series of Greek Sequence-to-Sequence Models for News Summarization'
arxiv_id: '2311.07767'
source_url: https://arxiv.org/abs/2311.07767
tags:
- language
- greek
- which
- summarization
- abstractive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GreekT5, a series of Greek sequence-to-sequence
  models for abstractive news summarization, addressing the scarcity of high-quality
  Greek text summarization models. The authors finetune three multilingual mT5 variants
  (mt5-small, umT5-small, umT5-base) on the GreekSUM dataset, using a summarization
  prefix approach.
---

# GreekT5: A Series of Greek Sequence-to-Sequence Models for News Summarization

## Quick Facts
- arXiv ID: 2311.07767
- Source URL: https://arxiv.org/abs/2311.07767
- Reference count: 40
- Primary result: GreekT5 models outperform GreekBART on ROUGE metrics while achieving competitive BERTScore results

## Executive Summary
This paper introduces GreekT5, a series of Greek sequence-to-sequence models for abstractive news summarization. The authors address the scarcity of high-quality Greek text summarization models by finetuning three multilingual mT5 variants (mt5-small, umT5-small, umT5-base) on the GreekSUM dataset. Using a summarization prefix approach, GreekT5 models significantly outperform the state-of-the-art GreekBART model on ROUGE metrics (up to 26.67 ROUGE-1, 13.00 ROUGE-2, 22.42 ROUGE-L) while achieving competitive BERTScore results. The best-performing model, GreekT5 (umT5-base), demonstrates strong extractive and abstractive capabilities. The study shows that multilingual mT5 models finetuned for Greek outperform monolingual models, requiring fewer computational resources. Limitations include dataset domain specificity and model input/output length constraints.

## Method Summary
The authors fine-tune three mT5 variants (mt5-small, umT5-small, umT5-base) on the GreekSUM dataset using a text-to-text framework with "summarize:" prefixes. The models are trained with AdamW optimizer, learning rate 3e-4, batch sizes 6 (small) and 1 (base), max input length 1024 tokens, and max output length 128 tokens. Evaluation uses ROUGE-1/2/L and BERTScore metrics on the test split. The study compares these multilingual models against the monolingual GreekBART model to demonstrate the effectiveness of finetuning multilingual models for Greek summarization.

## Key Results
- GreekT5 models achieve significantly higher ROUGE scores than GreekBART (up to 26.67 ROUGE-1, 13.00 ROUGE-2, 22.42 ROUGE-L)
- umT5-base variant achieves the best overall performance across all evaluation metrics
- Multilingual mT5 models finetuned for Greek outperform monolingual GreekBART while requiring fewer computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GreekT5 models outperform GreekBART due to finetuning multilingual mT5/UMT5 variants on Greek-specific summarization data.
- Mechanism: Multilingual models pretrained on large multilingual corpora (mC4) include Greek tokens, enabling effective finetuning on GreekSUM dataset with task-specific prefixes ("summarize:").
- Core assumption: The multilingual pretraining of mT5/umT5 includes sufficient Greek language coverage to support downstream finetuning.
- Evidence anchors:
  - [abstract] "multilingual mT5 models finetuned for Greek outperform monolingual models"
  - [section 2.1] "mC4 dataset contains 43 billion Greek tokens, thus mT5 supports finetuning for Greek NLP"
  - [corpus] Weak - no direct comparison of Greek token coverage between mT5 and BART pretraining
- Break Condition: If Greek token coverage in pretraining is insufficient or GreekSUM dataset is too domain-specific, finetuning would fail to generalize.

### Mechanism 2
- Claim: The uniform sampling strategy (UNIMAX) in umT5 improves low-resource language performance compared to older strategies.
- Mechanism: UNIMAX allocates training tokens uniformly across underrepresented languages while avoiding overfitting, leading to better finetuning results for Greek.
- Core assumption: Uniform sampling prevents overfitting on high-resource languages while ensuring adequate representation of low-resource languages like Greek.
- Evidence anchors:
  - [section 2.1] "UNIMAX strategy... prevents overfitting on low-resource languages, without imposing any reprioritization on higher-resource languages"
  - [section 3.4] "umT5-small and umT5-base models outperform GreekBART on ROUGE metrics"
  - [corpus] Missing - no direct evidence showing umT5's sampling strategy impact vs. mt5 on Greek specifically
- Break Condition: If the uniform sampling doesn't significantly improve Greek representation or if the dataset size is too small, performance gains may not materialize.

### Mechanism 3
- Claim: The text-to-text framework with task prefixes enables zero-shot generalization across different summarization tasks.
- Mechanism: T5-based models use the same architecture for multiple tasks by prepending prefixes like "summarize:" which guide the model's output generation.
- Core assumption: The model can learn to interpret task prefixes and generate appropriate outputs based on the prefix-task mapping learned during pretraining.
- Evidence anchors:
  - [section 2.1] "mT5 follows the 'text-to-text' strategy, using task prefixes (e.g., 'summarize: <input_text>')"
  - [section 3.3] "the prefix 'summarize' was prepended at the start of each training example"
  - [corpus] Weak - no evidence of zero-shot performance or prefix effectiveness specifically for Greek
- Break Condition: If the model fails to properly interpret task prefixes or if the prefix-task mapping is not robust across different domains, performance will degrade.

## Foundational Learning

- **Transformer architecture and attention mechanisms**
  - Why needed here: Understanding how encoder-decoder attention enables sequence-to-sequence modeling for summarization
  - Quick check question: How does multi-head attention in the encoder help capture long-range dependencies in Greek text?

- **Pretraining vs. finetuning distinction**
  - Why needed here: Recognizing that mT5/umT5 are pretrained on multilingual data and finetuned on Greek-specific summarization task
  - Quick check question: What's the difference between pretraining on mC4 and finetuning on GreekSUM dataset?

- **Evaluation metrics for summarization (ROUGE, BERTScore)**
  - Why needed here: Understanding how ROUGE measures n-gram overlap while BERTScore captures semantic similarity
  - Quick check question: Why might BERTScore be more appropriate than ROUGE for evaluating abstractive summaries?

## Architecture Onboarding

- **Component map:**
  - Hugging Face Transformers library for mT5/umT5 implementations
  - GreekSUM dataset (train/test splits for finetuning/evaluation)
  - GPU/CPU hardware setup for training and inference
  - Evaluation framework (ROUGE-1/2/L, BERTScore implementations)

- **Critical path:**
  1. Load pretrained mT5/umT5 model from Hugging Face
  2. Prepare GreekSUM dataset with "summarize:" prefix
  3. Tokenize with max_length=1024, truncation enabled
  4. Finetune with AdamW optimizer, learning rate 3e-4
  5. Evaluate on test set using ROUGE and BERTScore

- **Design tradeoffs:**
  - Input length constraint (1024 tokens) vs. document completeness
  - Output length constraint (128 tokens) vs. summary informativeness
  - Model size (small vs. base) vs. computational resources
  - Exact match metrics (ROUGE) vs. semantic similarity (BERTScore)

- **Failure signatures:**
  - Low ROUGE scores with high BERTScore may indicate abstractive summaries that are semantically similar but lexically different
  - Memory errors during finetuning suggest input/output length constraints are too aggressive
  - Poor performance compared to baselines suggests insufficient Greek token coverage or dataset quality issues

- **First 3 experiments:**
  1. Finetune mt5-small on GreekSUM with default hyperparameters and evaluate ROUGE scores
  2. Compare finetuned mt5-small vs. umT5-small on same dataset to measure impact of sampling strategy
  3. Test different output lengths (64 vs. 128 tokens) to find optimal tradeoff between brevity and informativeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GreekT5 models perform on Greek news summarization datasets from domains other than general news (e.g., legal, medical, financial)?
- Basis in paper: [inferred] The authors note that GreekT5 models are finetuned on a news summarization dataset and mention that they "might not be suitable for other domains or language settings that do not resemble the language style of the training corpus."
- Why unresolved: The evaluation only tested GreekT5 on the GreekSUM news dataset, leaving performance on other domains unexplored.
- What evidence would resolve it: Finetuning GreekT5 models on domain-specific Greek summarization datasets (legal, medical, financial) and comparing their performance against GreekBART and baseline models using appropriate metrics.

### Open Question 2
- Question: Would increasing the maximum output length beyond 128 tokens significantly improve the quality of GreekT5-generated summaries?
- Basis in paper: [explicit] The authors state that "These models also exhibit certain limitations regarding their output length, notably when increasing the model output (e.g., from 128 â€“ 256 tokens). In this case, memory and training time requirements are increased substantially."
- Why unresolved: The evaluation used a maximum output length of 128 tokens, and the authors acknowledge this limitation without exploring the trade-offs of longer outputs.
- What evidence would resolve it: Training GreekT5 models with increased maximum output lengths (e.g., 256 tokens) and evaluating their performance on the GreekSUM dataset using ROUGE and BERTScore metrics.

### Open Question 3
- Question: How would GreekT5 models perform on long document summarization tasks (e.g., books, academic papers, financial reports)?
- Basis in paper: [explicit] The authors mention that "Current abstractive Seq2Seq TS models exhibit certain limitations regarding their input length. Specifically, most of these models support a context window of 1024 input tokens, which hinders long document summarization."
- Why unresolved: The evaluation used a maximum sequence length of 1024 tokens, which may not be sufficient for long documents, but this limitation was not tested.
- What evidence would resolve it: Testing GreekT5 models on long document summarization datasets and comparing their performance against models with longer context windows or hierarchical summarization approaches.

## Limitations
- Evaluation based on single Greek dataset (GreekSUM) that is relatively small for deep learning standards, limiting generalizability to other Greek text domains
- Lack of ablation studies on specific design choices like uniform sampling strategy or task prefix effectiveness
- Input/output length constraints (1024/128 tokens) may force aggressive truncation of longer documents or limit summary informativeness
- Limited comparison with GreekBART using only ROUGE and BERTScore metrics without considering other evaluation dimensions

## Confidence
- **High Confidence**: The claim that GreekT5 models achieve better ROUGE scores than GreekBART on the GreekSUM dataset. This is directly supported by the reported evaluation metrics and the experimental setup is clearly specified.
- **Medium Confidence**: The claim that multilingual mT5 models generally outperform monolingual models for low-resource languages like Greek. While the results support this for Greek specifically, the mechanism (Greek token coverage in mC4) is not directly verified, and the comparison is limited to one monolingual baseline.
- **Low Confidence**: The claim that the uniform sampling strategy (UNIMAX) specifically improves Greek performance compared to standard sampling. The paper shows umT5 outperforms GreekBART but does not provide direct evidence that this improvement is due to the sampling strategy rather than other factors like model size or pretraining data volume.

## Next Checks
1. **Ablation study on sampling strategy**: Train a version of mT5-small using standard sampling versus UNIMAX on the GreekSUM dataset to directly measure the impact of the uniform sampling strategy on Greek performance.

2. **Cross-domain evaluation**: Test the finetuned GreekT5 models on a different Greek text corpus (e.g., social media posts or scientific abstracts) to assess generalization beyond news articles and validate domain-specific limitations.

3. **Qualitative error analysis**: Conduct a detailed examination of generated summaries to identify systematic errors, hallucinations, or factual inconsistencies, complementing the automatic metrics with human evaluation of summary quality and faithfulness.