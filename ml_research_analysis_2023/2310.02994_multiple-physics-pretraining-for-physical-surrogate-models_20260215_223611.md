---
ver: rpa2
title: Multiple Physics Pretraining for Physical Surrogate Models
arxiv_id: '2310.02994'
source_url: https://arxiv.org/abs/2310.02994
tags:
- learning
- pretraining
- training
- systems
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multiple physics pretraining (MPP) is introduced as a pretraining
  approach for autoregressive prediction of spatiotemporal physical systems with transformers.
  MPP trains a single transformer model to predict the dynamics of multiple heterogeneous
  physical systems simultaneously by embedding them into a shared normalized space.
---

# Multiple Physics Pretraining for Physical Surrogate Models

## Quick Facts
- arXiv ID: 2310.02994
- Source URL: https://arxiv.org/abs/2310.02994
- Reference count: 31
- Primary result: MPP pretraining enables zero-shot and finetuned performance on physical systems without task-specific architectures

## Executive Summary
This paper introduces Multiple Physics Pretraining (MPP) for autoregressive prediction of spatiotemporal physical systems using transformers. The approach trains a single transformer model on multiple heterogeneous physical systems simultaneously by embedding them into a shared normalized space. Experiments demonstrate that MPP-pretrained models match or outperform task-specific baselines on pretraining tasks without finetuning, and finetuning MPP models results in more accurate predictions on new physics compared to training from scratch or using video foundation models.

## Method Summary
MPP uses an Axial Vision Transformer architecture with reversible instance normalization to project multiple physical systems into a shared embedding space. The model processes spatiotemporal data through axial attention blocks, enabling scalable training on high-resolution simulations. A scaled training objective balances gradients from different physical systems, while gradient accumulation with task sampling handles varying native resolutions efficiently.

## Key Results
- Single MPP-pretrained transformer matches or outperforms task-specific baselines on all pretraining sub-tasks without finetuning
- Finetuning MPP models on new physical systems yields better accuracy than training from scratch
- MPP-pretrained models outperform finetuned video foundation models on physical system prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
Shared embedding and normalization strategy enables effective pretraining across heterogeneous physical systems by projecting fields from multiple systems into a shared embedding space using reversible instance normalization. This captures common features across systems with different scales and resolutions.

### Mechanism 2
Axial attention enables scalable transformer training for spatiotemporal prediction by performing sequential attention operations over each axis rather than full attention, limiting computational complexity from O((HW T)^2) to O(H^2 + W^2 + T^2).

### Mechanism 3
Multiple physics pretraining enables zero-shot performance on pretraining tasks and improved transfer to new systems by learning broadly useful features across heterogeneous physical systems that facilitate transfer learning.

## Foundational Learning

- **Spatiotemporal dynamical systems and PDEs**: Understanding how these systems work is crucial for grasping the pretraining approach. Quick check: What is the difference between a time-dependent PDE and a steady-state PDE?

- **Transformer architectures and attention mechanisms**: Essential for comprehending the model architecture. Quick check: How does axial attention differ from standard self-attention in terms of computational complexity?

- **Transfer learning and pretraining**: Key to understanding the approach's benefits. Quick check: What is the main advantage of pretraining a model on a large, diverse dataset before finetuning on a specific task?

## Architecture Onboarding

- **Component map**: Input normalization layer → Field embedding layer → Axial ViT backbone → Output reconstruction layer → Position biases and boundary handling

- **Critical path**: 1) Normalize input fields using reversible instance normalization 2) Embed fields into shared representation using 1x1 convolutions 3) Process through axial ViT backbone with axial attention 4) Decode tokens back to field space using 1x1 convolutions 5) Denormalize outputs using saved normalization statistics

- **Design tradeoffs**: Axial attention vs. full attention (scalability vs. correlation capture), shared embedding vs. separate embeddings (transfer vs. optimality), reversible instance normalization vs. other normalizations (scale handling vs. complexity)

- **Failure signatures**: Poor performance on pretraining tasks (issues with shared embedding or attention), poor transfer to new systems (pretraining tasks too dissimilar), training instability (improper normalization or learning rate issues)

- **First 3 experiments**: 1) Train single physics model on Navier-Stokes and evaluate on same task 2) Train MPP model on Navier-Stokes and shallow water, evaluate zero-shot performance on both 3) Finetune MPP model on diffusion-reaction and compare to training from scratch

## Open Questions the Paper Calls Out

The paper mentions several related approaches in the literature but doesn't explicitly call out open questions for future work.

## Limitations

- Generalizability to physical systems outside fluid dynamics remains unproven
- Axial attention may miss important full spatiotemporal correlations
- Optimal number of physical systems for MPP pretraining is unknown

## Confidence

- **High confidence**: Architectural components (axial attention, transformer backbone) are well-established
- **Medium confidence**: Core claims about zero-shot performance and transfer learning benefits need additional ablation studies
- **Medium confidence**: Scaling of training objective is theoretically sound but optimal factors may be problem-dependent

## Next Checks

1. **Ablation study on attention mechanisms**: Compare axial attention against full attention and other efficient attention variants on the same pretraining tasks

2. **Cross-domain transfer evaluation**: Test pretrained model on physical systems outside fluid dynamics domain (e.g., heat transfer, structural mechanics)

3. **Normalization strategy comparison**: Implement and compare reversible instance normalization against alternative normalization approaches (batch normalization, layer normalization)