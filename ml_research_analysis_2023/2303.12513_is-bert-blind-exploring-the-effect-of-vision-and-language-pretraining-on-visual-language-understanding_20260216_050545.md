---
ver: rpa2
title: Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual
  Language Understanding
arxiv_id: '2303.12513'
source_url: https://arxiv.org/abs/2303.12513
tags:
- text
- tasks
- language
- visual
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a suite of visual language understanding (VLU)
  tasks to probe text encoder models for their visual reasoning capabilities. The
  tasks include concreteness prediction, color association prediction, and shape association
  prediction.
---

# Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding

## Quick Facts
- arXiv ID: 2303.12513
- Source URL: https://arxiv.org/abs/2303.12513
- Reference count: 40
- Primary result: Multimodal pretraining improves text-only visual reasoning performance

## Executive Summary
This paper investigates whether vision-and-language models like CLIP have better visual reasoning capabilities than unimodally trained models like BERT when evaluated on text-only tasks. The authors propose a suite of visual language understanding (VLU) tasks including concreteness prediction, color association prediction, and shape association prediction. They introduce a novel zero-shot probing method called Stroop probing that uses cosine similarity between text embeddings to predict masked tokens. The study compares multimodal models (CLIP, FLA V A) with unimodal models (BERT, RoBERTa) on these VLU tasks as well as non-visual NLU tasks for comparison. Results show that multimodal models consistently outperform unimodal models on VLU tasks while underperforming them on non-visual NLU tasks, suggesting that exposure to images during pretraining provides visual reasoning knowledge that benefits text-only visual reasoning tasks.

## Method Summary
The authors evaluate pretrained models on visual language understanding tasks using zero-shot probing methods. For BERT and RoBERTa, they use masked language modeling (MLM) probing, where models predict masked tokens using their MLM heads. For CLIP and FLA V A, they implement Stroop probing, which measures cosine similarity between masked prompts and potential completions. The evaluation includes three VLU tasks (concreteness prediction, color association, shape association) and three non-visual NLU tasks (factual knowledge, language proficiency, sentiment analysis). Models are evaluated without modification, and performance is measured using standard metrics like accuracy, correlation coefficients, and retrieval metrics.

## Key Results
- Multimodal models (CLIP, FLA V A) outperform unimodal models (BERT, RoBERTa) on all three visual language understanding tasks
- The performance gap is most pronounced for shape association tasks, where multimodal models show substantial improvements
- Multimodal models underperform unimodal models on non-visual NLU tasks, demonstrating they do not have a global advantage on language tasks
- The results hold across different probing methods (MLM vs. Stroop) and are consistent with different prompt formulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal pretraining improves text-only performance on visual reasoning tasks because the model learns to ground language in visual concepts during training
- Core assumption: Visual reasoning capabilities are inherently tied to having seen visual representations of concepts during training
- Evidence anchors: [abstract], [section 1]
- Break condition: If models achieve strong visual reasoning performance without multimodal pretraining

### Mechanism 2
- Claim: The Stroop probing method works because salient visual properties create stronger interference effects in the text encoding space
- Core assumption: The interference effect observed in human psychology has a parallel in how neural models encode information
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If cosine similarity does not correlate with visual salience

### Mechanism 3
- Claim: Multimodal models underperform on non-visual NLU tasks because their training creates representational biases toward visual reasoning
- Core assumption: Training data distribution and objective function create biases affecting performance on different task types
- Evidence anchors: [abstract], [section 6]
- Break condition: If multimodal models can be fine-tuned to match unimodal performance on non-visual tasks

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: The paper compares MLM probing with Stroop probing across different model types
  - Quick check question: What is the key difference between how BERT's MLM head and CLIP's text encoder would handle "The apple is [*]" when predicting the masked token?

- Concept: Visual grounding in language models
  - Why needed here: Central to the thesis that multimodal pretraining provides visual grounding for text-only tasks
  - Quick check question: How would a model's understanding of "banana" differ with only textual descriptions versus paired image-caption data?

- Concept: Cosine similarity in embedding spaces
  - Why needed here: Stroop probing relies on comparing cosine similarities between different text embeddings
  - Quick check question: What does a cosine similarity of 0.9 versus 0.3 tell you about semantic relationship between embeddings?

## Architecture Onboarding

- Component map: Text tokenizer -> Encoder -> Pooling layer -> Probing interface -> Prediction
- Critical path: Text → Tokenizer → Encoder → Pooling → Probing Method → Prediction
- Design tradeoffs:
  - BERT uses bidirectional attention capturing more context but requiring MLM pretraining
  - CLIP uses unidirectional attention optimized for contrastive learning with images
  - Stroop probing works without task-specific heads but may be less precise than MLM
  - Zero-shot evaluation isolates pretraining effects but may underestimate true capabilities
- Failure signatures:
  - Low correlation between predictions and ground truth suggests missing visual grounding
  - Inconsistent performance across different prompts indicates sensitivity to prompt engineering
  - Near-random performance on non-visual tasks suggests over-specialization on visual reasoning
  - High variance in results across different model checkpoints indicates instability
- First 3 experiments:
  1. Run Stroop probing on a simple color association task to verify basic mechanism
  2. Compare BERT MLM and CLIP Stroop probing on the same task to establish baseline differences
  3. Test effect of different prompt formulations on the same task to understand prompt sensitivity

## Open Questions the Paper Calls Out

- Question: How does the performance of CLIP compare to unimodally trained models on visual language understanding tasks when using a linear probing method instead of zero-shot probing?
  - Basis in paper: [explicit] The paper mentions using linear classifiers but does not compare CLIP on VLU tasks using this method
  - Why unresolved: The paper focuses on zero-shot probing for CLIP but does not explore linear probing performance
  - What evidence would resolve it: Running linear probing on CLIP and comparing to unimodally trained models on VLU tasks

- Question: What is the impact of reporting bias in the LAION dataset on the performance of multimodal models on visual language understanding tasks?
  - Basis in paper: [explicit] The paper analyzes reporting bias but cannot fully explain multimodal model performance
  - Why unresolved: While reporting bias is not the sole factor, its exact impact is not quantified
  - What evidence would resolve it: Experiments with multimodal models trained on datasets with varying levels of reporting bias

- Question: How does the size of the text encoder component of multimodal models affect their performance on visual language understanding tasks?
  - Basis in paper: [explicit] The paper compares models of different sizes but does not isolate text encoder size effects
  - Why unresolved: The paper shows multimodal models outperform unimodal models but does not isolate text encoder size effects
  - What evidence would resolve it: Experiments with multimodal models having text encoders of varying sizes but similar overall model sizes

## Limitations

- Weak corpus support: Retrieved papers discuss related vision-language topics but do not directly validate the paper's core mechanisms about multimodal pretraining improving visual reasoning
- Unknown model configurations: Exact HuggingFace model checkpoints are not specified for several models, which could affect performance comparisons
- Limited prompt examples: Only a few example prompts are provided, making it difficult to verify full prompt set implementation and assess prompt sensitivity

## Confidence

- High confidence: The empirical observation that multimodal models outperform unimodal models on visual reasoning tasks is robust
- Medium confidence: The interpretation that this stems from visual grounding acquired during multimodal pretraining is plausible but not definitively proven
- Low confidence: The Stroop probing method as a robust evaluation tool lacks sufficient external validation

## Next Checks

1. Verify prompt sensitivity by systematically varying prompts across tasks and measuring performance changes across models
2. Cross-validate results using traditional probing methods (supervised classifiers on frozen embeddings) alongside the proposed MLM and Stroop probing
3. Test fine-tuning effects by fine-tuning both multimodal and unimodal models on non-visual NLU tasks to determine if performance gaps narrow or reverse