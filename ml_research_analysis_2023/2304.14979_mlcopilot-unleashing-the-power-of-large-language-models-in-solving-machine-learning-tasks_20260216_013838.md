---
ver: rpa2
title: 'MLCopilot: Unleashing the Power of Large Language Models in Solving Machine
  Learning Tasks'
arxiv_id: '2304.14979'
source_url: https://arxiv.org/abs/2304.14979
tags:
- datasets
- tasks
- task
- learning
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MLCopilot is a framework that uses large language models to generate
  machine learning solutions for novel tasks by retrieving and reasoning over past
  experiences. It canonicalizes historical ML data into natural language, retrieves
  relevant experiences and knowledge, and prompts the LLM to produce interpretable
  solutions.
---

# MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks

## Quick Facts
- arXiv ID: 2304.14979
- Source URL: https://arxiv.org/abs/2304.14979
- Reference count: 40
- Key outcome: MLCopilot achieves state-of-the-art performance on ML task benchmarks by leveraging LLMs to retrieve and reason over historical experiences

## Executive Summary
MLCopilot is a novel framework that harnesses large language models to solve machine learning tasks by retrieving and reasoning over past experiences. The approach canonicalizes historical ML data into natural language, retrieves relevant experiences and knowledge, and prompts the LLM to produce interpretable solutions. Evaluated on three benchmarks (HPO-B, PD1, HyperFD), MLCopilot significantly outperforms traditional AutoML and zero/few-shot LLM baselines, demonstrating the potential of LLM-based approaches for automated ML problem solving.

## Method Summary
MLCopilot works by first converting structured ML solutions and metrics into natural language format through a process called canonicalization. It then retrieves the most relevant historical experiences using embedding-based cosine similarity, and elicits high-level knowledge from these experiences. This information is used to prompt an LLM (specifically GPT-3.5) to generate solutions for new ML tasks. The framework operates in two stages: an offline stage for canonicalization and knowledge elicitation, and an online stage for task-specific retrieval and prompting.

## Key Results
- MLCopilot achieves state-of-the-art performance on HPO-B, PD1, and HyperFD benchmarks
- Significantly outperforms traditional AutoML and zero/few-shot LLM baselines
- Key components like retrieval, canonicalization, and knowledge elicitation are essential for success
- Offers fast, interpretable, and human-like approach to solving ML tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can reason over heterogeneous ML experiences when data is canonicalized into natural language
- Mechanism: Converting structured ML solutions to natural language allows LLMs to process diverse data formats using their text understanding capabilities
- Core assumption: LLMs have sufficient text generation capacity to handle natural language representations of ML configurations
- Evidence anchors: Abstract mentions canonicalization to natural language; section 4.4 discusses converting raw data to well-formed natural language format
- Break condition: When LLM context window is exceeded or discretized values lose too much precision

### Mechanism 2
- Claim: Retrieval of relevant experiences improves LLM performance on novel ML tasks
- Mechanism: Embedding task descriptions and calculating cosine similarity retrieves the most relevant historical experiences to guide the LLM
- Core assumption: Task similarity measured by embedding distance correlates with solution relevance
- Evidence anchors: Section 4.3 describes cosine similarity retrieval; section 5.5 shows meta-feature and text embedding outperform random retrieval
- Break condition: When retrieved demonstrations are not sufficiently similar to the target task

### Mechanism 3
- Claim: Eliciting knowledge from experiences provides additional guidance beyond demonstrations alone
- Mechanism: LLM analyzes sampled experiences to generate concise, high-level knowledge that guides solution generation for new tasks
- Core assumption: High-level knowledge derived from experiences can be generalized to novel but related tasks
- Evidence anchors: Abstract mentions retrieving experiences and knowledge; section 4.5 proposes retrieve-and-prompt framework
- Break condition: When knowledge generation fails to generalize or post-validation reveals hallucination issues

## Foundational Learning

- Concept: Canonicalization of structured data to natural language
  - Why needed here: LLMs work best with text input, but ML solutions are typically structured configurations
  - Quick check question: If you have a JSON configuration with numerical hyperparameters, how would you convert it to natural language while preserving meaning?

- Concept: Embedding-based retrieval using cosine similarity
  - Why needed here: Need to find most relevant historical experiences for a new task based on task descriptions
  - Quick check question: If task A has embedding [0.1, 0.2, 0.3] and task B has [0.2, 0.3, 0.4], what is their cosine similarity?

- Concept: Discretization of continuous numerical values
  - Why needed here: LLMs struggle with precise numerical reasoning, so continuous values must be converted to discrete categories
  - Quick check question: If you have learning rates [0.001, 0.01, 0.1], how would you discretize them into "low", "medium", "high" categories?

## Architecture Onboarding

- Component map: User → Task Description → Text Embedding → Experience Retrieval → Knowledge Retrieval → LLM Prompt → Solution Generation
- Critical path: Task description → embedding retrieval → canonicalized demonstrations → LLM generation
- Design tradeoffs: Canonicalization loses some precision vs. LLM compatibility; retrieval accuracy vs. prompt length limitations
- Failure signatures: Poor performance on novel task types, inconsistent results across similar tasks, failure when numerical precision is critical
- First 3 experiments:
  1. Test LLM zero-shot performance on simple ML tasks without any retrieval or canonicalization
  2. Implement basic canonicalization and measure performance improvement on same tasks
  3. Add experience retrieval and evaluate impact on solution quality compared to random selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MLCopilot's performance scale with the size of the historical experience pool?
- Basis in paper: The paper discusses the importance of historical experiences but does not provide experiments varying the size of the experience pool
- Why unresolved: The paper focuses on demonstrating effectiveness with a fixed dataset, not exploring performance changes with different amounts of historical data
- What evidence would resolve it: Experiments showing performance on benchmarks as the size of the historical experience pool is systematically varied

### Open Question 2
- Question: How robust is MLCopilot to noisy or incomplete task descriptions?
- Basis in paper: The paper mentions task descriptions in natural language but does not test performance on imperfect or incomplete descriptions
- Why unresolved: Experiments use curated task descriptions, which may not reflect real-world variability and noise
- What evidence would resolve it: Experiments with intentionally degraded task descriptions to measure robustness

### Open Question 3
- Question: How does MLCopilot's performance compare to human experts on the same ML tasks?
- Basis in paper: The paper mentions aiming to assist human engineers and produce interpretable results
- Why unresolved: The paper focuses on comparing to traditional AutoML and LLM baselines, not human experts
- What evidence would resolve it: A study where human ML experts solve the same tasks as MLCopilot, with solutions evaluated using the same metrics

## Limitations

- Performance gains may not generalize to more complex ML tasks beyond hyperparameter optimization and feature selection
- Approach assumes natural language representations preserve sufficient information for LLM reasoning, which may not hold for tasks requiring high numerical precision
- Computational overhead of canonicalization and knowledge elicitation during offline stages is not discussed

## Confidence

- **High Confidence**: Claims about MLCopilot's performance improvements over baseline methods on tested benchmarks
- **Medium Confidence**: Claims about the essential role of individual components (canonicalization, retrieval, knowledge elicitation) supported by ablation studies
- **Medium Confidence**: Claims about producing interpretable solutions supported by methodology, though interpretability evaluation is not explicitly detailed

## Next Checks

1. Conduct detailed ablation study varying canonicalization format, discretization granularity, and retrieval methods to quantify individual impact on performance across different ML task types

2. Evaluate MLCopilot's performance on tasks requiring high numerical precision to assess limits of discretized value representations

3. Test MLCopilot on additional ML task benchmarks beyond HPO-B, PD1, and HyperFD to validate whether observed performance gains generalize to broader range of ML problems