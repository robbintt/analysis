---
ver: rpa2
title: Multimodality and Attention Increase Alignment in Natural Language Prediction
  Between Humans and Computational Models
arxiv_id: '2308.06035'
source_url: https://arxiv.org/abs/2308.06035
tags:
- attention
- human
- clip
- scores
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether multimodal generative AI models
  (mLLMs) process language in ways that align with humans when integrating visual
  and linguistic information. Researchers had 200 human participants watch 100 short
  audio-visual clips from two films, predicting upcoming words while eye movements
  were tracked.
---

# Multimodality and Attention Increase Alignment in Natural Language Prediction Between Humans and Computational Models

## Quick Facts
- arXiv ID: 2308.06035
- Source URL: https://arxiv.org/abs/2308.06035
- Authors: Not specified in source
- Reference count: 40
- Primary result: CLIP model's predictability scores significantly correlate with human estimates (r = 0.22, p < 0.001), especially for highly relevant scenes (r = 0.37, p < 0.001)

## Executive Summary
This study investigates whether multimodal generative AI models process language similarly to humans when integrating visual and linguistic information. Researchers had 200 human participants watch 100 short audio-visual clips from two films while predicting upcoming words, tracking their eye movements. They then ran the same clips through CLIP, a multimodal model that integrates visual and linguistic input via attention mechanisms, extracting predictability scores. The results show significant alignment between CLIP's predictions and human estimates, particularly for scenes rated as highly relevant for prediction. This alignment disappears when CLIP's attention mechanism is disrupted or absent, suggesting that attention-guided multimodal integration is crucial for human-like predictability processing.

## Method Summary
The study used 100 six-second audio-visual clips from two films ("The Prestige" and "The Usual Suspects"), with 200 human participants rating each clip's relevance for predicting upcoming words on a 0-100 scale while eye movements were tracked via webcam. CLIP-ViT-Base-Patch32 processed the same clips by encoding visual frames and text prompts into a shared embedding space, computing similarity scores via dot product, and applying softmax normalization to generate word probabilities. Spatial overlap between human eye-tracking data and CLIP's attention maps was quantified using Jaccard similarity after DBSCAN clustering. A GPT-2 baseline unimodal model of comparable size was also tested for comparison.

## Key Results
- CLIP's predictability scores significantly correlated with human estimates (r = 0.22, p < 0.001)
- Alignment was stronger for scenes rated as highly relevant for prediction (r = 0.37, p < 0.001)
- Spatial overlap between human eye-tracking data and CLIP's attention maps was significant (Jaccard index = 0.18, p < 0.001)
- Alignment dropped to chance when CLIP's attention mechanism was disrupted or absent
- Unimodal model of comparable size showed no meaningful alignment with human predictability scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's multimodal integration via visual-linguistic feature vector comparison leads to human-like predictability estimates
- Mechanism: CLIP encodes both visual frames and text prompts into a shared embedding space. The dot product of these vectors yields similarity scores, which are then softmax-normalized to compute word probabilities. This process mirrors how humans integrate visual and linguistic information when predicting upcoming words
- Core assumption: Humans use similar statistical integration of multimodal features when predicting upcoming semantic content
- Evidence anchors:
  - [abstract] "CLIP's scores correlated significantly with human estimates (r = 0.22, p < 0.001), especially for scenes rated as highly relevant for prediction (r = 0.37, p < 0.001)."
  - [section] "We find that human estimates of predictability align significantly with CLIP scores... based on a comparison of image and text feature vectors."
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism
- Break condition: If visual-linguistic integration is disrupted (e.g., via attention weight perturbation or removing attention), alignment drops to chance

### Mechanism 2
- Claim: The transformer attention mechanism is essential for achieving human-like predictability alignment
- Mechanism: Attention weights determine which visual features CLIP focuses on when processing each frame. These weights guide the model to attend to relevant regions that humans also focus on, creating spatial overlap in attention patterns
- Core assumption: Human visual attention during language processing is guided by similar statistical relevance patterns as CLIP's attention mechanism
- Evidence anchors:
  - [abstract] "This alignment dropped to chance when CLIP’s attention mechanism was disrupted or absent, and a unimodal model of comparable size showed no meaningful alignment."
  - [section] "Alignment vanished when CLIP’s visual attention weights were perturbed... The model’s attention patches and human eye tracking significantly overlapped."
  - [corpus] Weak evidence - no corpus support for this specific attention-human alignment mechanism
- Break condition: If attention weights are randomly shuffled or attention mechanism is removed, alignment with human predictability scores disappears

### Mechanism 3
- Claim: CLIP's attention patterns significantly overlap with human eye-tracking data in spatial regions critical for prediction
- Mechanism: Both CLIP and humans direct attention to similar visual features when predicting upcoming words. This overlap is quantified using Jaccard similarity between attention heatmaps
- Core assumption: Humans and CLIP share common visual features as relevant for language prediction, despite different attention mechanisms
- Evidence anchors:
  - [abstract] "Spatial overlap between human eye-tracking data and CLIP’s attention maps was significant (Jaccard index = 0.18, p < 0.001)."
  - [section] "We find a significant spatial overlap between CLIP’s visual attention weights and human eye-tracking data."
  - [corpus] Weak evidence - no corpus support for this specific spatial overlap finding
- Break condition: If visual context doesn't contain referents of upcoming words, overlap diminishes as humans make diverse judgments

## Foundational Learning

- Concept: Multimodal embedding spaces
  - Why needed here: CLIP maps both visual and linguistic inputs into a shared space for direct comparison
  - Quick check question: What mathematical operation is used to compare visual and text embeddings in CLIP?

- Concept: Transformer attention mechanism
  - Why needed here: Attention weights determine which visual features CLIP focuses on during prediction
  - Quick check question: What happens to CLIP's predictability alignment when visual attention weights are randomly shuffled?

- Concept: Visual world paradigm
  - Why needed here: Framework for understanding how humans integrate visual and linguistic information during language processing
  - Quick check question: What methodology did researchers use to collect human predictability estimates?

## Architecture Onboarding

- Component map: CLIP (image encoder + text encoder + transformer attention) → predictability scores; Human participants → eye-tracking + predictability ratings
- Critical path: Visual frames → CLIP image encoder → attention-weighted features → text encoder features → similarity scores → predictability; Human visual input → eye-tracking → attention patterns → predictability ratings
- Design tradeoffs: CLIP uses statistical attention vs. human attention influenced by broader cognitive factors; computational efficiency vs. cognitive plausibility
- Failure signatures: No alignment between model and human scores when attention is disrupted; weak correlations for unimodal models
- First 3 experiments:
  1. Compare CLIP predictability scores with human ratings across all clips
  2. Compare CLIP predictability scores with human ratings for high-relevance vs. low-relevance clips
  3. Compare CLIP attention heatmaps with human eye-tracking data using Jaccard similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do attention patterns between mLLMs and humans align under different types of visual stimuli and tasks beyond predictability rating?
- Basis in paper: [inferred] The paper notes that attention overlap is stronger when referents are present in visual input, but suggests this might not generalize across all stimuli types
- Why unresolved: The study focused specifically on predictability ratings for upcoming words in film clips, which represents a narrow range of visual-linguistic integration tasks
- What evidence would resolve it: Experiments testing alignment across diverse visual tasks (object recognition, scene understanding, action prediction) with varying complexity and presence of referents

### Open Question 2
- Question: What specific architectural features beyond attention mechanisms contribute to the alignment between mLLMs and human predictability processing?
- Basis in paper: [explicit] The paper shows alignment isn't solely due to attention, as a multimodal model without attention showed no alignment, suggesting other architectural factors matter
- Why unresolved: The study isolated attention effects but didn't systematically vary other architectural components like embedding dimensions, normalization methods, or layer configurations
- What evidence would resolve it: Ablation studies systematically removing or modifying different architectural components while measuring alignment with human judgments

### Open Question 3
- Question: How does the size and scale of mLLMs affect the degree of alignment with human predictability judgments?
- Basis in paper: [explicit] The paper notes that larger models might show increased alignment, as CLIP (with fewer parameters) achieved only 56% variance explained compared to human ceiling
- Why unresolved: The study used CLIP with a fixed, modest parameter count, leaving open whether scaling up would improve alignment substantially
- What evidence would resolve it: Systematic testing of progressively larger multimodal models on the same predictability tasks, measuring correlation with human judgments

## Limitations

- The moderate correlation strength (r = 0.22 overall, r = 0.37 for high-relevance scenes) suggests only partial alignment between CLIP and human predictive processing
- The study lacks direct evidence that humans actually use the same vector comparison mechanism as CLIP for multimodal integration
- Results may be specific to the film clips used and may not generalize to other domains or everyday conversational contexts

## Confidence

- Attention mechanism findings: Medium confidence - correlation drops when attention is disrupted, but lacks details about exact perturbation methods
- Spatial overlap results: Medium confidence - statistically significant but modest overlap (Jaccard = 0.18) raises questions about practical significance
- Generalizability: Low confidence - study focused on specific film clips without testing broader contexts

## Next Checks

1. Test CLIP's alignment with human predictability scores on a broader, more diverse dataset of everyday conversations and non-cinematic contexts to assess generalizability
2. Conduct controlled experiments where attention mechanisms are systematically varied (e.g., different perturbation strengths) to establish dose-response relationships
3. Compare CLIP's attention patterns with human eye-tracking data across different types of visual scenes to determine whether the overlap is consistent or context-dependent