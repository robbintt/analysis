---
ver: rpa2
title: 'Copy Suppression: Comprehensively Understanding an Attention Head'
arxiv_id: '2310.04625'
source_url: https://arxiv.org/abs/2310.04625
tags:
- heads
- l10h7
- attention
- token
- copy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive mechanistic explanation of
  the role of attention head L10H7 in GPT-2 Small, showing that its primary function
  is copy suppression across the entire training distribution. The authors demonstrate
  that L10H7 suppresses incorrect copying behavior, improving model calibration and
  explaining observations of "negative heads" that systematically favored wrong answers
  in prior work.
---

# Copy Suppression: Comprehensively Understanding an Attention Head

## Quick Facts
- **arXiv ID**: 2310.04625
- **Source URL**: https://arxiv.org/abs/2310.04625
- **Reference count**: 40
- **Key outcome**: This paper provides a comprehensive mechanistic explanation of the role of attention head L10H7 in GPT-2 Small, showing that its primary function is copy suppression across the entire training distribution.

## Executive Summary
This paper provides the most complete mechanistic description of a single component in a large language model to date. Through weights-based arguments and ablation experiments, the authors demonstrate that attention head L10H7 in GPT-2 Small primarily functions to suppress naive copying behavior by attending to previously predicted tokens and reducing their logits. This "copy suppression" mechanism accounts for 76.9% of the head's behavior and explains observations of "negative heads" that favored wrong answers in prior work. The analysis also reveals that copy suppression leads to self-repair behavior in specific tasks.

## Method Summary
The researchers used weights-based arguments to analyze L10H7's QK and OV circuits, showing that the head attends to tokens matching previously predicted ones and suppresses their probability through negative diagonal elements. They developed a Copy Suppression-Preserving Ablation (CSPA) technique that isolates the head's functional mechanisms by ablating all components except the OV and QK circuits. By measuring KL divergence between clean and ablated distributions across OpenWebText, they quantified the head's contribution to model behavior. The analysis also examined self-repair mechanisms through intervention experiments on the IOI task.

## Key Results
- Copy suppression accounts for 76.9% of L10H7's behavior on OpenWebText
- 84.70% of tokens in GPT-2 Small's vocabulary have their diagonal elements as top 10 most negative values in their columns
- 95.72% of diagonal values in the QK matrix were largest in their respective rows
- Copy suppression explains 39% of self-repair behavior in one narrow task
- L10H7 reduces model overconfidence and entropy while lowering overall loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: L10H7 suppresses naive copying behavior by attending to previously predicted tokens and reducing their logits.
- **Mechanism**: When early layers predict a token that already appears in context (e.g., "love" in "All's fair in love and..."), L10H7 attends to that earlier occurrence and suppresses its probability, increasing the chance of the correct token ("war").
- **Core assumption**: The head's OV circuit has large negative values on the diagonal, meaning it suppresses tokens it attends to.
- **Evidence anchors**:
  - [abstract]: "If components in earlier layers predict a certain token, and this token appears earlier in the context, the head suppresses it: we call this copy suppression."
  - [section 3.1]: "84.70% of the tokens in GPT-2 Small’s vocabulary have their diagonal elements as one of the top 10 most negative values in their columns."

### Mechanism 2
- **Claim**: L10H7's attention patterns are determined by the QK circuit matching predicted tokens to their earlier occurrences.
- **Mechanism**: The QK circuit ensures L10H7 attends strongly when the query token (predicted next) matches a key token (earlier in context), enabling targeted suppression.
- **Core assumption**: The QK matrix has high values on the diagonal, indicating attention to matching tokens.
- **Evidence anchors**:
  - [section 3.2]: "95.72% of diagonal values in this matrix were the largest in their respective rows."
  - [abstract]: "Copy suppression accounts for a majority of the head’s behavior and reduces the model’s overall loss."

### Mechanism 3
- **Claim**: Copy suppression explains 76.9% of L10H7's behavior through CSPA ablation experiments.
- **Mechanism**: By ablating all L10H7 functionality except the OV and QK mechanisms, researchers recover most of its effect on model outputs, measured via KL divergence.
- **Core assumption**: KL divergence between clean and ablated distributions accurately measures functional importance.
- **Evidence anchors**:
  - [section 3.3]: "CSPA explains 76.9% of L10H7’s behavior on OpenWebText."
  - [abstract]: "We uncover the mechanism that the Negative Heads use for copy suppression with weights-based evidence."

## Foundational Learning

- **Concept: Attention mechanisms in transformers**
  - Why needed here: Understanding how attention heads attend to context tokens is fundamental to grasping copy suppression.
  - Quick check question: What does it mean when an attention head has high attention weights from position A to position B?

- **Concept: Logit lens technique**
  - Why needed here: The logit lens allows researchers to see what tokens internal components are pushing for by applying the unembedding matrix to activations.
  - Quick check question: How can you use the logit lens to determine if a head is suppressing a particular token?

- **Concept: OV and QK circuits**
  - Why needed here: These circuits decompose attention heads into interpretable components that explain when and how suppression occurs.
  - Quick check question: What do the diagonal elements of an OV circuit represent in terms of token suppression?

## Architecture Onboarding

- **Component map**: Residual stream → Layer 10 → L10H7 → QK circuit → attention scores → OV circuit → suppressed logits → residual stream
- **Critical path**: Query → QK circuit → attention weights → key tokens → OV circuit → suppressed logits → residual stream
- **Design tradeoffs**: Copy suppression trades off between reducing overconfidence (which lowers loss) and potentially missing correct predictions if suppression is too aggressive.
- **Failure signatures**: If L10H7 fails, the model shows increased naive copying (repeating earlier tokens), higher overconfidence, and increased loss on copy-heavy prompts.
- **First 3 experiments**:
  1. Measure the diagonal elements of L10H7's OV circuit to confirm suppression behavior.
  2. Apply the logit lens to see if L10H7's output consistently reduces logits on previously predicted tokens.
  3. Perform CSPA ablation to verify that preserving only OV and QK mechanisms recovers most functional impact.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the complete mechanism by which copy suppression reduces model overconfidence?
- **Basis in paper**: The paper shows L10H7 reduces overconfidence and entropy, but doesn't fully explain why this happens or what causes negative heads to form.
- **Why unresolved**: The paper presents three theories but doesn't definitively test which is correct or if they're all true.
- **What evidence would resolve it**: Examining checkpointed models to see when negative heads form relative to copying behavior emergence.

### Open Question 2
- **Question**: How can we faithfully approximate the keyside input to L10H7 in practice?
- **Basis in paper**: The paper shows that projecting keyside onto effective embedding is not faithful, and that context-free residual state provides better approximation but still imperfect.
- **Why unresolved**: Current approximations cause large attention changes (median -0.09 and -0.06 respectively) suggesting key information is lost.
- **What evidence would resolve it**: Finding a projection method that preserves attention weights within acceptable tolerance.

### Open Question 3
- **Question**: What role does the perpendicular component to the unembedding direction play in self-repair?
- **Basis in paper**: Section 4.2 shows intervention experiments where perpendicular component seems more important than unembedding direction for both backup and negative heads.
- **Why unresolved**: The paper doesn't identify what this perpendicular component represents or how it enables self-repair.
- **What evidence would resolve it**: Decomposing this perpendicular component into interpretable subcomponents.

## Limitations

- The 76.9% attribution of L10H7's behavior to copy suppression is based on KL divergence measurements, which may not capture all functional aspects of the head.
- The analysis assumes OV and QK circuits fully explain the head's behavior, potentially missing higher-order interactions or context-dependent effects.
- The self-repair analysis showing 39% explanation is based on a narrow task and may not generalize to broader model behavior.

## Confidence

- **High confidence**: The basic mechanism of copy suppression through negative diagonal elements in the OV circuit.
- **Medium confidence**: The attribution of 76.9% of L10H7's behavior to copy suppression.
- **Low confidence**: The generalization of self-repair findings beyond the specific task studied.

## Next Checks

1. **Cross-validation with alternative metrics**: Replicate the 76.9% attribution using alternative measures of functional importance beyond KL divergence, such as direct loss reduction on copy-heavy prompts.

2. **Broader self-repair validation**: Test the 39% self-repair explanation on multiple diverse tasks beyond the narrow IOI task to establish generalizability.

3. **Interaction analysis**: Investigate whether L10H7's copy suppression interacts with other heads or layers in ways not captured by the OV/QK circuit analysis, potentially using activation patching or causal tracing methods.