---
ver: rpa2
title: 'ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees'
arxiv_id: '2304.08928'
source_url: https://arxiv.org/abs/2304.08928
tags:
- privacy
- progap
- graph
- training
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProGAP introduces a progressive training scheme for differentially
  private GNNs to address the high privacy costs of aggregation perturbation. It splits
  a GNN into overlapping submodels trained sequentially, caching and reusing perturbed
  embeddings from previous stages to limit privacy budget usage while maintaining
  expressive power.
---

# ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees

## Quick Facts
- arXiv ID: 2304.08928
- Source URL: https://arxiv.org/abs/2304.08928
- Authors: 
- Reference count: 40
- Key outcome: Up to 10.4% and 5.5% higher accuracy compared to state-of-the-art methods under edge- and node-level privacy, respectively

## Executive Summary
ProGAP introduces a progressive training scheme for differentially private GNNs that addresses the high privacy costs of aggregation perturbation. The method splits a GNN into overlapping submodels trained sequentially, caching and reusing perturbed embeddings from previous stages to limit privacy budget usage while maintaining expressive power. By combining progressive training with aggregation perturbation and DP-SGD, ProGAP provides both edge-level and node-level differential privacy guarantees with improved accuracy over existing methods.

## Method Summary
ProGAP implements progressive training by dividing a GNN into K+1 overlapping submodels, where each submodel is trained on privately aggregated node embeddings cached from previous stages. The Normalize-Aggregate-Perturb (NAP) module adds Gaussian noise to aggregations to provide edge-level differential privacy, while DP-SGD ensures node-level privacy. The key innovation is caching NAP outputs to reduce privacy budget consumption from O(KT) to O(K) queries to the adjacency matrix.

## Key Results
- Achieves up to 10.4% higher accuracy than state-of-the-art methods under edge-level privacy (epsilon = 1)
- Improves node-level privacy accuracy by 5.5% compared to existing approaches (epsilon = 8)
- Reduces privacy budget consumption by factor of T through caching mechanism
- Demonstrates effectiveness across five diverse graph datasets (Facebook, Reddit, Amazon, Facebook-100, WeNet)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive training reduces privacy costs by caching and reusing perturbed embeddings across stages
- Mechanism: By splitting the GNN into K+1 overlapping submodels and caching NAP outputs, ProGAP limits adjacency matrix queries from O(KT) to O(K), reducing accumulated privacy cost by factor of T
- Core assumption: Cached embeddings contain sufficient information for subsequent stages while maintaining privacy guarantees
- Evidence anchors:
  - [abstract]: "each submodel is trained over the privately aggregated node embeddings learned and cached by the previous submodels"
  - [section 4.1]: "The key point in this training strategy is that we immediately save the outputs of NAP modules on their first query and reuse them throughout the training"
- Break condition: If embeddings become stale or corrupted, subsequent stages may fail to learn effectively

### Mechanism 2
- Claim: Aggregation perturbation with Gaussian noise provides edge-level differential privacy
- Mechanism: NAP applies row-normalization, sum aggregation, and Gaussian noise addition to obfuscate individual edge contributions
- Core assumption: Gaussian noise with variance σ² is sufficient to mask individual edge contributions in aggregation
- Evidence anchors:
  - [section 3.3]: "we follow the aggregation perturbation technique proposed by Sajadmanesh et al. [39] and add noise to the output of the aggregation function"
  - [section 4.1]: "It can be easily shown that the resulting model provides edge-level DP as every query to the adjacency matrix A is immediately perturbed with noise"
- Break condition: If noise variance is too low relative to graph sensitivity, edge-level privacy guarantees may be violated

### Mechanism 3
- Claim: Progressive training with DP-SGD provides node-level differential privacy
- Mechanism: Each submodel trained using DP-SGD with gradient clipping and noise addition, while progressive structure ensures node features and labels remain protected
- Core assumption: DP-SGD with appropriate parameters can provide node-level privacy when combined with progressive structure
- Evidence anchors:
  - [abstract]: "node-level privacy achieved by combining the approach with DP-SGD"
  - [section 4.2]: "To ensure node-level DP, however, we must train every submodel using DP-SGD or its variants"
- Break condition: If DP-SGD parameters are not properly tuned, node-level privacy guarantees may be violated

## Foundational Learning

- Concept: Differential Privacy (DP) and its variants (edge-level vs node-level)
  - Why needed here: The entire paper is built on providing formal privacy guarantees for GNNs, so understanding DP fundamentals is essential
  - Quick check question: What's the difference between edge-level and node-level adjacency in graph DP?

- Concept: Graph Neural Networks (GNN) architecture and message passing
  - Why needed here: ProGAP modifies the standard GNN training process, so understanding how GNNs work is crucial
  - Quick check question: How does the aggregation function in GNNs differ from standard neural network layers?

- Concept: Progressive learning and its benefits
  - Why needed here: ProGAP's key innovation is progressive training, so understanding this concept is essential
  - Quick check question: What are the main advantages of progressive learning compared to standard training approaches?

## Architecture Onboarding

- Component map: Graph adjacency matrix A and node features X → NAP → MLP base → JK → MLP head (repeated across stages with caching)
- Critical path: NAP → MLP base → JK → MLP head (repeated across stages with caching)
- Design tradeoffs:
  - Depth vs privacy budget: Deeper models provide better accuracy but require more privacy budget
  - Noise variance vs accuracy: Higher noise provides better privacy but reduces accuracy
  - Progressive vs layerwise training: Progressive training provides better accuracy but is more complex to implement
- Failure signatures:
  - Training accuracy much higher than validation accuracy: Potential overfitting
  - Validation accuracy plateaus early: May need more stages or different hyperparameters
  - Privacy budget exhausted quickly: May need to reduce model depth or increase noise variance
- First 3 experiments:
  1. Implement NAP module and verify edge-level DP guarantees on a small synthetic graph
  2. Implement progressive training with 2 stages and compare against standard training on a benchmark dataset
  3. Add DP-SGD to implement node-level privacy and evaluate accuracy-privacy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProGAP's performance scale to extremely large graphs with millions of nodes and edges?
- Basis in paper: [inferred] The paper evaluates ProGAP on datasets with up to ~1.8 million nodes and ~46 million edges, but does not explore performance on graphs significantly larger than this.
- Why unresolved: The paper focuses on demonstrating ProGAP's effectiveness on moderate-sized datasets and does not investigate its scalability to massive graphs that are common in real-world applications.
- What evidence would resolve it: Experiments on graphs with orders of magnitude more nodes and edges than those used in the paper, along with analysis of memory usage, training time, and accuracy degradation as graph size increases.

### Open Question 2
- Question: Can ProGAP be extended to handle dynamic graphs where the structure changes over time?
- Basis in paper: [inferred] The paper assumes a static graph structure and does not discuss how ProGAP would handle graphs with evolving edges or node features.
- Why unresolved: Dynamic graphs are common in many real-world applications, but the paper does not address the challenges of maintaining privacy guarantees or adapting the progressive training scheme for changing graph structures.
- What evidence would resolve it: A modified version of ProGAP that can incrementally update the cached embeddings and adjust the privacy budget as the graph evolves, along with experiments showing its effectiveness on dynamic graph datasets.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the number of stages and the noise standard deviation, impact ProGAP's accuracy-privacy trade-off?
- Basis in paper: [explicit] The paper discusses the impact of the model depth (number of stages) on accuracy under different privacy budgets, but does not provide a systematic analysis of how other hyperparameters affect the trade-off.
- Why unresolved: While the paper shows that increasing the number of stages can improve accuracy under higher privacy budgets, it does not explore the sensitivity of ProGAP's performance to other key hyperparameters or provide guidelines for choosing them.
- What evidence would resolve it: A comprehensive sensitivity analysis of ProGAP's accuracy and privacy guarantees to various hyperparameters, along with recommendations for selecting these parameters based on the desired trade-off and dataset characteristics.

## Limitations

- Privacy validation relies primarily on theoretical RDP bounds rather than empirical privacy attacks
- Caching mechanism effectiveness depends on assumption that intermediate embeddings remain informative across stages
- Progressive training introduces complexity in hyperparameter tuning, particularly for number of stages and noise scale selection

## Confidence

- High confidence: Edge-level DP guarantees through aggregation perturbation
- Medium confidence: Node-level DP guarantees with DP-SGD integration
- Medium confidence: Accuracy improvements over baselines
- Low confidence: Generalization of progressive training benefits across diverse graph structures

## Next Checks

1. Conduct membership inference attacks on ProGAP's node representations to empirically verify node-level privacy guarantees beyond theoretical RDP bounds
2. Perform ablation studies systematically removing the caching mechanism to quantify its contribution to both accuracy gains and privacy budget savings
3. Test ProGAP on heterogeneous graph datasets (e.g., citation networks with varying density, social networks with different community structures) to evaluate robustness across graph types not covered in the original experiments