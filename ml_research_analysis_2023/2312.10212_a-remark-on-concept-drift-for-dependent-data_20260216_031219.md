---
ver: rpa2
title: A Remark on Concept Drift for Dependent Data
arxiv_id: '2312.10212'
source_url: https://arxiv.org/abs/2312.10212
tags:
- drift
- data
- time
- thus
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the gap between classical concept drift definitions\
  \ (for independent data) and the needs of learning from dependent data streams,\
  \ such as time series. It shows that stationarity\u2014commonly used in time series\
  \ analysis\u2014does not align with the notion of drift relevant to machine learning\
  \ and monitoring tasks."
---

# A Remark on Concept Drift for Dependent Data

## Quick Facts
- arXiv ID: 2312.10212
- Source URL: https://arxiv.org/abs/2312.10212
- Reference count: 24
- This work introduces consistency as a new framework for detecting drift in dependent data streams, showing that stationarity does not align with machine learning notions of drift.

## Executive Summary
This paper addresses a fundamental gap between classical concept drift definitions (designed for independent data) and the needs of learning from dependent data streams like time series. The authors demonstrate that stationarity—commonly used in time series analysis—does not align with the notion of drift relevant to machine learning and monitoring tasks. They introduce a new framework called consistency that better captures unexpected changes in single streams by measuring how well a path aligns with a predefined function class over time.

The key contribution is a practical definition and evaluation of drift for dependent data that aligns with observable learning behavior. Through theoretical examples and experiments, the paper shows that standard drift detection methods fail on time-shifted or slowed-down dependent data, whereas the proposed consistency-based approach works reliably across different synthetic scenarios.

## Method Summary
The paper introduces a consistency framework for detecting drift in dependent data streams. The method involves fitting a model from a predefined function class C to the observed path, then computing a k-local MSE (k-MSE) loss that compares the fitted model to local neighborhoods to detect structured residuals. Drift is flagged when the k-MSE exceeds noise levels, indicating the path contains information beyond what the function class can capture. The approach is evaluated against standard drift detection methods (ADF, KPSS, KCpD, MMD, KFRD) using ROC-AUC scores on synthetic datasets with controlled drift patterns.

## Key Results
- Standard drift detection methods (KCpD, MMD, KFRD) fail on dependent data with time shifts or slowdowns
- The consistency framework reliably detects drift across different synthetic scenarios including jump, lemniscate, and square wave patterns
- Stationarity does not align with machine learning notions of drift for single dependent paths
- The k-local MSE approach provides theoretical guarantees under ergodicity assumptions, though practical convergence requires careful hyperparameter selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency detects drift by measuring how well a path aligns with a predefined function class over time
- Mechanism: Consistency defines drift as unexpected changes in the path that cannot be captured by a fixed model class, using k-local MSE to compare a fitted model to local neighborhoods
- Core assumption: The underlying signal can be approximated by a function in the predefined class C, and deviation beyond local noise indicates drift
- Evidence anchors:
  - [abstract] "The paper introduces a new framework called consistency, which better captures unexpected changes in a single stream"
  - [section 3.2] "Consistency then refers to time-invariant losses"
  - [corpus] Weak: No direct citations, but aligns with general time series drift literature
- Break condition: If noise has strong local structure or if C is too restrictive/broad, consistency may fail

### Mechanism 2
- Claim: Standard drift detection methods fail on dependent data because they assume independent observations
- Mechanism: These methods detect distributional shifts across samples, but for a single dependent path, the distribution over time may remain stationary even when the path changes dramatically
- Core assumption: Methods are designed for independent streams, not single dependent paths
- Evidence anchors:
  - [abstract] "standard drift detection methods fail on time-shifted or slowed-down dependent data"
  - [section 3.1] Examples showing stationarity and drift do not align for single paths
  - [corpus] Weak: No direct citations, but consistent with limitations of classical methods in dependent settings
- Break condition: If data can be treated as multiple independent realizations, these methods may work

### Mechanism 3
- Claim: Uncertainty principle limits ability to estimate local distributions in dependent streams
- Mechanism: For single paths, estimating E[f(X)|T=t] requires averaging over time windows, but dependency limits local information; ergodicity ensures convergence but only asymptotically
- Core assumption: Real-time monitoring requires finite-time guarantees, which ergodicity does not provide
- Evidence anchors:
  - [section 2.3] "Due to the uncertainty principle we introduced before most make additional requirements that limit the non-stationarity"
  - [section 2.3] "Both ergodic and mixing processes usually require infinite temporal horizons to achieve convergence"
  - [corpus] Weak: No direct citations, but aligns with general limitations of ergodic theory
- Break condition: If process is truly ergodic with fast mixing, assumptions might hold

## Foundational Learning

- Concept: Ergodic processes and mixing
  - Why needed here: Understanding why ergodicity and mixing are insufficient for practical drift detection in dependent data streams
  - Quick check question: What is the key difference between ergodicity and mixing, and why does the paper argue neither is suitable for monitoring?

- Concept: Concept drift in independent vs. dependent data
  - Why needed here: Clarifying why classical drift definitions based on distributional changes fail for dependent data
  - Quick check question: How does the notion of drift differ between independent data streams and single dependent paths?

- Concept: Stationarity and its limitations
  - Why needed here: Recognizing why stationarity is not a suitable proxy for drift in dependent data
  - Quick check question: Why does the paper argue that stationarity does not align with the intuitive notion of drift for a single time series?

## Architecture Onboarding

- Component map: Data stream ingestion -> Consistency checker -> Local model fitting -> k-local MSE computation -> Drift decision
- Critical path:
  1. Fit model ˆf ∈ C to the observed path Xt
  2. Compute k-local MSE between ˆf and local neighborhoods of Xt
  3. If k-local MSE exceeds noise level, flag drift
- Design tradeoffs:
  - Choice of function class C: Too narrow → false positives; too broad → false negatives
  - Choice of k: Too small → noisy MSE; too large → loss of locality
  - Predefined vs. learned function class: Predefined is simpler but may miss unexpected drift patterns
- Failure signatures:
  - High false positive rate: C is too restrictive or k is too small
  - High false negative rate: C is too broad or k is too large
  - Slow detection: k is too large or model fitting is computationally expensive
- First 3 experiments:
  1. Test on synthetic data with known drift patterns (jump, shift, slowdown) to verify detection accuracy
  2. Compare ROC-AUC against baseline methods (KCpD, MMD, KFRD) on same synthetic data
  3. Stress-test with varying noise levels and function class choices to find failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How problematic is the discrepancy between stationarity and drift in practical monitoring setups?
- Basis in paper: [explicit] Authors state this is unclear after presenting theoretical and experimental evidence
- Why unresolved: Paper shows theoretical mismatch but doesn't empirically measure impact on real monitoring tasks
- What evidence would resolve it: Controlled experiments comparing stationarity-based vs consistency-based methods on real-world monitoring datasets

### Open Question 2
- Question: Is wide-sense stationarity a better alternative to strict stationarity for modeling drift in dependent data?
- Basis in paper: [explicit] Authors suggest this "might offer a well-suited choice for some cases" but haven't investigated it
- Why unresolved: Paper only analyzes strict stationarity; wide-sense stationarity requires different mathematical treatment
- What evidence would resolve it: Formal analysis comparing drift detection performance using wide-sense stationarity vs strict stationarity on benchmark datasets

### Open Question 3
- Question: Can consistency be formally defined for "dependent drift processes" as an alternative to stochastic processes?
- Basis in paper: [explicit] Authors state "A description of the phenomenon of dependent data streams by means of 'dependent drift process' and a suited generalization of the notion of consistency are still outstanding"
- Why unresolved: Paper only develops consistency for stochastic processes, not for drift processes with dependencies
- What evidence would resolve it: Mathematical framework defining consistency for dependent drift processes with proof of key properties

### Open Question 4
- Question: Does consistency-based drift detection fail for monitoring tasks in independent data settings as suggested by [14]?
- Basis in paper: [explicit] Authors acknowledge this is "very relevant" but haven't investigated it
- Why unresolved: Paper focuses on dependent data; [14]'s proof relies on independence assumptions
- What evidence would resolve it: Empirical evaluation comparing consistency-based methods with established independent-data methods on standard benchmark datasets

## Limitations

- The consistency framework relies on ergodicity and mixing assumptions for theoretical guarantees, but these are impractical for real-world monitoring due to slow convergence rates
- The predefined function class C is critical for detecting drift, but the paper does not specify how to choose C optimally
- The k-local MSE method depends on the choice of k(n), which must satisfy specific mathematical constraints while maintaining bounded variance

## Confidence

- **High confidence**: The observation that standard drift detection methods fail on dependent data streams is well-supported by theoretical arguments and experimental evidence
- **Medium confidence**: The proposed consistency framework is shown to work on synthetic examples, but real-world validation is limited and theoretical guarantees rely on assumptions that may not hold in practice
- **Low confidence**: The paper does not provide clear methodology for choosing the function class C or hyperparameters k(n) in practical applications

## Next Checks

1. Function class sensitivity analysis: Systematically test the consistency framework with different function classes (linear, polynomial, periodic) on synthetic data with known drift patterns to identify optimal choices for different drift types

2. Real-world application: Apply the consistency framework to real-world time series datasets (e.g., sensor data, financial markets) and compare performance against standard methods in terms of detection accuracy and false alarm rates

3. Theoretical refinement: Develop finite-time convergence bounds for the k-local MSE estimator under weaker assumptions than ergodicity/mixing, making the method more applicable to practical monitoring scenarios