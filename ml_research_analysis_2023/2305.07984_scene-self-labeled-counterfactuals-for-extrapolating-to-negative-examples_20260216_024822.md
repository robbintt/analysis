---
ver: rpa2
title: 'SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples'
arxiv_id: '2305.07984'
source_url: https://arxiv.org/abs/2305.07984
tags:
- examples
- scene
- squad
- negative
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCENE (Self-labeled Counterfactuals for Extrapolating
  to Negative Examples), a method to synthesize negative examples from positive-only
  training data. The key idea is to perturb positive examples using a mask infilling
  model (BART), then self-label the resulting examples as negative using a heuristic
  that combines model predictions and paraphrase detection.
---

# SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples

## Quick Facts
- arXiv ID: 2305.07984
- Source URL: https://arxiv.org/abs/2305.07984
- Authors: 
- Reference count: 38
- Primary result: Closes 69.6% of the performance gap on SQuAD 2.0 when training on positive-only SQuAD 1.1 data

## Executive Summary
This paper introduces SCENE (Self-labeled Counterfactuals for Extrapolating to Negative Examples), a method to synthesize negative examples from positive-only training data. The key idea is to perturb positive examples using a mask infilling model (BART), then self-label the resulting examples as negative using a heuristic that combines model predictions and paraphrase detection. This enables models to extrapolate from positive-only datasets to tasks with negative examples. On SQuAD 2.0, SCENE closes 69.6% of the performance gap compared to models trained on the full SQuAD 2.0 dataset. It also improves out-of-domain generalization to ACE-whQA and extends to boolean QA and RTE, demonstrating broad applicability.

## Method Summary
SCENE perturbs positive examples using BART's mask infilling capability, then self-labels the resulting examples as negative based on a filtering heuristic. The method combines retrieval-based negatives with self-labeled counterfactuals, using a weighted loss function during training. The filtering step employs a paraphrase detector to remove ambiguous or incorrect perturbations, improving the quality of synthetic training data.

## Key Results
- SCENE closes 69.6% of the performance gap on SQuAD 2.0 compared to models trained on the full dataset
- Improves out-of-domain generalization to ACE-whQA
- Extends to boolean QA (BoolQ) and RTE tasks
- Outperforms baselines of using only retrieval or only self-labeled examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method enables zero-shot extrapolation from positive-only data to negative examples by generating perturbed versions of positive examples and self-labeling them as negative.
- Mechanism: The perturbation (via BART mask infilling) alters the semantics of the question slightly, creating examples that are similar to but semantically distinct from the original. The self-labeling step uses model predictions combined with paraphrase detection to identify when these perturbations result in unanswerable questions.
- Core assumption: Perturbations generated by the mask infilling model are semantically meaningful and can create realistic unanswerable questions that differ only slightly from answerable ones.
- Evidence anchors:
  - [abstract]: "Given a positive example, SCENE perturbs it with a mask infilling model, then determines whether the resulting example is negative based on a self-training heuristic."
  - [section 3.2]: "Given an example (q,p,y) from our positive-only dataset DPositive, we use a generator G to synthesize a perturbed version G(q) of the question q. We then impute a label ˇy, which is often the 'unanswerable' label, and train the QA model to output ˇy given the input (G(q),p)."
  - [corpus]: Weak evidence; related papers discuss generating unanswerable questions but not the specific self-labeling heuristic.

### Mechanism 2
- Claim: The filtering step using paraphrase detection and prediction consistency improves the quality of self-labeled examples by removing ambiguous or incorrect perturbations.
- Mechanism: The paraphrase detector checks if the perturbed question is semantically similar to the original. If they are paraphrases but the model predictions differ, or if both predictions are wrong, the example is filtered out. This reduces noise in the training data.
- Core assumption: The paraphrase detector (RoBERTa on QQP) is effective at identifying semantic similarity, and the model's prediction consistency is a reliable signal for label correctness.
- Evidence anchors:
  - [section 3.2]: "To better determine whether the prediction ỹ is likely to be correct, we adopt the idea of rejection sampling by using a paraphrase detection model Γ... we discard it (i.e., set δPert(p,q,y) = 0) if one of the following two cases happen: (1) ambiguity: Γ(G(q),q) = Paraphrase but the perturbation changes the model prediction, i.e. ŷ ≠ ỹ. This suggests that we have contradictory conclusions from the paraphrase detector and the QA model, and we cannot self-label it with confidence. (2) bad prediction: if the perturbation doesn't change the QA model prediction but they're both wrong, i.e. ŷ = ỹ ≠ y."
  - [corpus]: No direct evidence; the effectiveness of this filtering is inferred from ablation results.

### Mechanism 3
- Claim: The combination of retrieval-based negatives and self-labeled counterfactuals provides a curriculum-like learning signal that helps the model gradually learn to detect more subtle unanswerable questions.
- Mechanism: Retrieval-based negatives (BM25) provide initial exposure to the concept of unanswerability by pairing questions with highly similar but answer-less passages. Self-labeled counterfactuals then build on this by creating more challenging examples that require finer-grained understanding.
- Core assumption: The model can learn from retrieval-based negatives and then generalize to more subtle examples generated by self-labeling.
- Evidence anchors:
  - [section 3.1]: "To create harder unanswerable examples, given a question q, we retrieve a passage with high word overlap from the pool P of all passages in the dataset... we create an unanswerable example (q,R(q), NoAns) where R is the retrieval operator."
  - [section 4.2]: "Our best method combines retrieval with self-labeled counterfactuals; this is 7.5 F1 points better than only using retrieval."
  - [corpus]: No direct evidence; the curriculum effect is inferred from performance improvements.

## Foundational Learning

- Concept: Counterfactual data augmentation
  - Why needed here: The method needs to generate examples that are semantically similar but have different labels (answerable vs. unanswerable) to teach the model fine-grained distinctions.
  - Quick check question: What is the difference between counterfactual data augmentation and standard data augmentation?

- Concept: Self-training and pseudo-labeling
  - Why needed here: The method uses the model's own predictions to label generated examples, which requires understanding how to balance confidence and noise in pseudo-labels.
  - Quick check question: How does self-training differ from standard supervised learning?

- Concept: Paraphrase detection
  - Why needed here: The filtering step relies on detecting whether a perturbed question is semantically similar to the original, which requires understanding paraphrase detection models and their limitations.
  - Quick check question: What are the challenges in building effective paraphrase detection models?

## Architecture Onboarding

- Component map: BART (G) -> Paraphrase Detector (Γ) -> QA Model (fΘ) -> Training Pipeline
- Critical path: Perturb (BART) -> Filter (paraphrase detector + model prediction) -> Self-label -> Train
- Design tradeoffs:
  - Using BART for perturbation vs. other LMs: BART is chosen for its mask infilling capability, but other models might generate different types of perturbations
  - Filtering with paraphrase detection vs. no filtering: Filtering reduces noise but may remove useful examples; ablation shows filtering improves performance
  - Combining retrieval and self-labeling vs. using one method: Combination provides a curriculum effect; using only one may be less effective
- Failure signatures:
  - Model fails to detect subtle unanswerable questions: Likely due to insufficient filtering or poor perturbation quality
  - Model over-predicts negative class: Likely due to lack of diversity in perturbations or over-reliance on self-labeling
  - Training is slow or unstable: Likely due to high computational cost of BART and paraphrase detection
- First 3 experiments:
  1. Train on SQuAD 1.1 with no augmentation and evaluate on SQuAD 2.0 to establish baseline
  2. Add retrieval-based negatives to the SQuAD 1.1 training and evaluate on SQuAD 2.0 to test curriculum effect
  3. Add self-labeled counterfactuals (with filtering) to the SQuAD 1.1 training and evaluate on SQuAD 2.0 to test the full method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SCENE perform on other tasks that require detecting challenging negatives beyond extractive QA, boolean QA, and RTE?
- Basis in paper: [inferred] The authors state "We only validated on extractive QA, boolean QA and RTE. Whether SCENE can be applied to other tasks that require detecting challenging negatives is unknown."
- Why unresolved: The paper only experiments with three specific tasks. Other NLP tasks like fact verification, natural language inference with more than two labels, or complex reasoning tasks might have different characteristics that could affect SCENE's performance.
- What evidence would resolve it: Testing SCENE on additional datasets/tasks like SciFact, MNLI, or other NLI datasets, and comparing performance against baselines and the gap between models trained on positive-only versus full data.

### Open Question 2
- Question: Can SCENE be used to extrapolate to classes other than just negative examples, such as multiple new classes simultaneously?
- Basis in paper: [inferred] The authors note "SCENE is also limited to extrapolating to one pre-defined new class, negative examples; whether SCENE can be used to extrapolate to other types of classes is also unknown."
- Why unresolved: The method was designed specifically for the binary case of positive vs. negative examples. Extending it to multi-class scenarios would require significant modifications to the perturbation, filtering, and self-labeling steps.
- What evidence would resolve it: Applying SCENE to multi-class problems like multi-choice QA or multi-label classification tasks, and demonstrating whether it can effectively generate and identify multiple new classes.

### Open Question 3
- Question: How can the computational cost of SCENE be reduced to make it more practical for real-world applications?
- Basis in paper: [explicit] The authors acknowledge "Though our method alleviates data collection cost by human annotators, its computational cost is higher than training with human annotated datasets for multiple reasons" and provide details in Appendix B.3.
- Why unresolved: While the authors identify sources of computational overhead (increased training time per epoch, BART generation, paraphrase detection), they do not propose solutions or optimizations to address this limitation.
- What evidence would resolve it: Experiments comparing different model architectures (smaller generators), efficient paraphrase detection methods, or approaches to reduce the number of SCENE examples needed while maintaining performance.

### Open Question 4
- Question: Can the filtering process in SCENE be made more adaptive or learnable rather than relying on a pre-trained paraphrase detector?
- Basis in paper: [inferred] The authors use a fixed paraphrase detector for filtering, but note in the ablation study that removing it decreases performance, suggesting it plays a crucial role.
- Why unresolved: The current approach uses a static paraphrase detector trained on a different task (QQP). It's unclear whether a more task-specific or adaptive filtering mechanism could improve performance or reduce computational overhead.
- What evidence would resolve it: Comparing SCENE with different filtering strategies, such as learning a task-specific filter during training, using a lighter-weight paraphrase model, or exploring alternative filtering criteria beyond paraphrase detection.

## Limitations
- Limited evaluation on diverse datasets with different characteristics
- Computational overhead from generating and filtering counterfactuals using large models
- Uncertainty about paraphrase detector performance on out-of-domain data

## Confidence
**High Confidence:**
- The SCENE method can improve performance on SQuAD 2.0 when trained on SQuAD 1.1 (as evidenced by the 69.6% performance gap closure)
- The combination of retrieval-based negatives and self-labeled counterfactuals is more effective than using either method alone (supported by ablation results)

**Medium Confidence:**
- The method generalizes to other tasks like boolean QA and RTE (based on limited evaluation on BoolQ-3L and RTE)
- The filtering step using paraphrase detection and prediction consistency improves the quality of self-labeled examples (inferred from ablation results)

**Low Confidence:**
- The method's effectiveness on datasets with significantly different characteristics from SQuAD (e.g., longer passages, different question types)
- The robustness of the paraphrase detector when applied to out-of-domain data
- The scalability of the method for large-scale applications due to computational overhead

## Next Checks
1. Evaluate SCENE on diverse datasets with different characteristics (e.g., longer passages, different question types) to assess generalization ability and identify potential limitations.

2. Analyze paraphrase detector performance on out-of-domain data to ensure robustness and identify potential failure modes.

3. Assess computational efficiency by measuring overhead of generating and filtering counterfactuals, and explore optimization techniques for large-scale applications.