---
ver: rpa2
title: 'Alt-Text with Context: Improving Accessibility for Images on Twitter'
arxiv_id: '2305.14779'
source_url: https://arxiv.org/abs/2305.14779
tags:
- text
- alt-text
- image
- tweet
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of generating descriptive alt-text
  for images shared on social media platforms like Twitter, where visual content is
  often inaccessible to users with visual impairments. Unlike standard image captioning,
  alt-text for social media must be highly descriptive and context-specific, often
  requiring background knowledge to capture the image's essence accurately.
---

# Alt-Text with Context: Improving Accessibility for Images on Twitter

## Quick Facts
- **arXiv ID**: 2305.14779
- **Source URL**: https://arxiv.org/abs/2305.14779
- **Reference count**: 7
- **Primary result**: Model achieves >2x improvement on BLEU@4 and is preferred by human annotators for both fluency and descriptiveness

## Executive Summary
This work tackles the problem of generating descriptive alt-text for images shared on social media platforms like Twitter, where visual content is often inaccessible to users with visual impairments. Unlike standard image captioning, alt-text for social media must be highly descriptive and context-specific, often requiring background knowledge to capture the image's essence accurately. To address this, the authors propose a multimodal model that leverages both visual information from the image and textual context from the associated tweet. The approach uses a CLIP image encoder combined with a mapping network to generate a "prefix" in word embedding space, which is then concatenated with an embedding of the tweet text. This multimodal prefix is fed into a pretrained language model (GPT-2) to autoregressively generate the alt-text. The authors collect a new dataset of 371k Twitter images paired with alt-text and tweets, and evaluate their model across multiple automated metrics (BLEU@4, METEOR, ROUGE-L, CIDEr) and human evaluation. Results show that their approach significantly outperforms prior work, achieving more than 2x improvement on BLEU@4, and is preferred by human annotators for both fluency and descriptiveness. The study demonstrates the effectiveness of incorporating tweet context into alt-text generation, highlighting its potential to improve accessibility for visually impaired users on social media.

## Method Summary
The authors propose a multimodal approach to alt-text generation that leverages both image content and associated tweet text. The method uses a CLIP image encoder to extract visual features, which are then projected into word embedding space using a mapping network. This image prefix is concatenated with an embedding of the tweet text to form a multimodal prefix, which is fed into a pretrained GPT-2 language model to generate descriptive alt-text. The model is trained on a dataset of 371k Twitter images paired with alt-text and tweets. During decoding, beam search is used with trigram blocking to prevent repetition, and an optional reranking step selects the caption most similar to the tweet text based on ROUGE-L. The model is evaluated using automated metrics (BLEU@4, METEOR, ROUGE-L, CIDEr) and human evaluation for fluency and descriptiveness.

## Key Results
- The proposed model achieves more than 2x improvement on BLEU@4 compared to prior work
- Human annotators prefer the generated alt-text for both fluency and descriptiveness
- Reranking based on ROUGE-L similarity to tweet text improves the quality of generated captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model leverages tweet text as a source of contextual knowledge that the image alone cannot provide.
- Mechanism: By concatenating a CLIP-generated prefix embedding of the image with an embedding of the tweet text, the language model can condition on both visual and textual context, effectively combining complementary information sources.
- Core assumption: Tweet text contains relevant contextual information that is not visually apparent in the image and can improve alt-text quality.
- Evidence anchors:
  - [abstract]: "images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative"
  - [section]: "Our central motivating idea is that despite the challenges associated with this domain, social media images come with additional contextual information that if properly leveraged can significantly improve the quality of automatically generated alt-text"
  - [corpus]: Weak evidence - only one neighboring paper mentions leveraging text context for alt-text, but lacks details on multimodal integration
- Break condition: If tweet text is irrelevant to the image content, or if the text contains no useful contextual information, the benefit of multimodal conditioning diminishes.

### Mechanism 2
- Claim: Mapping network translates image features into word embedding space, enabling multimodal prefix input to language model.
- Mechanism: CLIP image encoder produces visual features, which are then projected through a mapping network to a sequence of embeddings in the same dimensional space as word embeddings, allowing concatenation with tweet text embeddings.
- Core assumption: Image features can be meaningfully projected into word embedding space while preserving discriminative visual information.
- Evidence anchors:
  - [section]: "What we require is a way to project image features into text space (or more specifically word embedding sequence space) so that we can directly incorporate them into the tweet prefix. Fortunately, this is something that prior work has addressed"
  - [section]: "We pass an image x through a pretrained CLIP image encoder, and then project the output to word embedding space using a mapping network F"
  - [corpus]: No direct evidence found - corpus neighbors focus on alt-text generation but do not discuss CLIP-based prefix methods
- Break condition: If the mapping network fails to preserve critical visual information during projection, or if the projected features do not align well with word embedding semantics.

### Mechanism 3
- Claim: Reranking based on ROUGE-L similarity to tweet text ensures generated alt-text incorporates relevant contextual information from the post.
- Mechanism: Among top beam search candidates, the caption most similar to tweet text (measured by ROUGE-L) is selected, prioritizing inclusion of contextually relevant details mentioned in the tweet.
- Core assumption: High-quality alt-text will share significant n-gram overlap with tweet text when the tweet contains contextually relevant information.
- Evidence anchors:
  - [section]: "we also experiment with a reranking strategy to choose a new best caption among the top 5 candidates returned by beam search. Under this paradigm, we score each candidate based on its ROUGE-L similarity to the tweet text t and return the top caption"
  - [section]: "Furthermore, as with many language generation models we noticed that ours had a tendency to get stuck repeating words or short phrases towards the end of captions. We solve this by simply disallowing the model from producing a trigram it has already generated within that same path on the beam"
  - [corpus]: No evidence found - corpus neighbors do not discuss reranking strategies for alt-text generation
- Break condition: If tweet text is not relevant to image content, or if the best caption under ROUGE-L similarity does not actually describe the image well.

## Foundational Learning

- **Concept: CLIP (Contrastive Language-Image Pre-training)**
  - Why needed here: Provides strong visual feature representations that can be projected into text space for multimodal conditioning
  - Quick check question: What type of embeddings does CLIP output, and what is their dimensionality?

- **Concept: Prefix-based generation with pretrained language models**
  - Why needed here: Enables conditioning on both visual and textual information without modifying the decoder architecture
  - Quick check question: How does the mapping network transform CLIP embeddings to make them compatible with GPT-2 input format?

- **Concept: Beam search with repetition constraints**
  - Why needed here: Prevents degenerate generation while maintaining diversity among candidate captions
  - Quick check question: Why might beam search without repetition constraints still produce repetitive outputs in this task?

## Architecture Onboarding

- **Component map**: CLIP image encoder → Mapping network (MLP) → Prefix in word embedding space → GPT-2 decoder → Alt-text output; Tweet text embedding → Concatenated with image prefix → Reranking module (optional): Scores beam search candidates by ROUGE-L similarity to tweet text
- **Critical path**: Image → CLIP → Mapping Network → Prefix → GPT-2 → Alt-text
- **Design tradeoffs**:
  - Freezing CLIP for speed vs. fine-tuning for domain adaptation
  - Using beam search vs. sampling for decoding
  - Including vs. excluding reranking step
- **Failure signatures**:
  - Output contains verbatim tweet text (over-reliance on text context)
  - Generated captions are generic and lack image-specific details
  - Model produces repetitive phrases despite constraints
- **First 3 experiments**:
  1. Compare frozen CLIP mapping network vs. fine-tuned version on validation set
  2. Test different prefix lengths (k values) to find optimal balance between context and overfitting
  3. Evaluate reranking impact by comparing human preference scores with and without reranking

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the quality of alt-text generated by the proposed model compare to user-written alt-text when evaluated by blind or low-vision users?
  - Basis in paper: [inferred] The paper mentions limitations including that human evaluation was performed by sighted reviewers, and that a larger scale study using BLV users was beyond the scope of this work.
  - Why unresolved: The current evaluation relies on sighted annotators, which may not accurately reflect the accessibility needs and preferences of the target user group.
  - What evidence would resolve it: Conducting a user study with BLV participants to assess the usability and effectiveness of the generated alt-text in real-world scenarios.

- **Open Question 2**
  - Question: How does the proposed model perform on images from social media platforms other than Twitter, such as Instagram or Facebook?
  - Basis in paper: [inferred] The paper focuses on Twitter data and mentions that the distribution of images on Twitter differs from other captioning datasets, but does not evaluate performance on other platforms.
  - Why unresolved: The model is trained and evaluated specifically on Twitter data, and its generalization to other social media platforms with different image distributions and textual contexts is unknown.
  - What evidence would resolve it: Evaluating the model on alt-text generation tasks using images and associated text from other social media platforms.

- **Open Question 3**
  - Question: What is the impact of incorporating OCR (Optical Character Recognition) into the model to improve transcription of text found in images?
  - Basis in paper: [explicit] The paper mentions limitations including the inability to precisely transcribe text from images and suggests that incorporating an OCR system could be a potential improvement.
  - Why unresolved: The current model does not explicitly handle text within images, which can be crucial for generating accurate alt-text descriptions.
  - What evidence would resolve it: Integrating an OCR system into the model and comparing its performance on alt-text generation tasks involving images with embedded text.

- **Open Question 4**
  - Question: How does the model's performance change when trained on a dataset with more diverse and high-quality user-written alt-text references?
  - Basis in paper: [explicit] The paper acknowledges that the quality of user-written alt-text on Twitter is variable and can be noisy, which affects the training objective and evaluation metrics.
  - Why unresolved: The current dataset relies on user-generated alt-text, which may not always adhere to best practices or be consistently descriptive.
  - What evidence would resolve it: Collecting a dataset with more consistent and high-quality alt-text references, and retraining the model to assess improvements in performance and alignment with accessibility guidelines.

- **Open Question 5**
  - Question: How does the model handle images that appear without any accompanying textual context, or where the context is ambiguous?
  - Basis in paper: [explicit] The paper mentions limitations including the reliance on accompanying social media posts and the inability to adapt to settings where images lack textual context.
  - Why unresolved: The model's architecture is designed to leverage textual context from social media posts, and its performance on standalone images or images with ambiguous context is unclear.
  - What evidence would resolve it: Evaluating the model's performance on a dataset of images without accompanying text or with ambiguous context, and comparing it to models that do not rely on textual conditioning.

## Limitations

- The evaluation relies on automated metrics that may not fully capture the quality of descriptive alt-text, and human evaluation results are not detailed enough to assess inter-annotator agreement or potential biases
- The study's dataset is proprietary and only available via tweet IDs, which limits reproducibility and independent verification
- The model assumes that tweet text consistently provides relevant contextual information, which may not always be true

## Confidence

- **High Confidence**: The basic architecture of combining image and text modalities for alt-text generation is sound and well-supported by prior work in multimodal learning
- **Medium Confidence**: The specific implementation details (CLIP encoder + mapping network + GPT-2) and the claim of >2x improvement on BLEU@4 are supported by the paper's results, but lack sufficient detail for independent verification
- **Low Confidence**: Claims about the model's ability to capture complex contextual information from tweets and the generalizability of results to diverse Twitter content types are not fully substantiated

## Next Checks

1. **Reproduce the dataset**: Hydrate the tweet IDs and verify the preprocessing pipeline, particularly the deduplication steps using SSIM and ngram similarity. Assess the quality and representativeness of the resulting dataset.

2. **Implement ablation study**: Compare the full model against variants that exclude either the image prefix or tweet text prefix to quantify the contribution of each modality. This would validate whether tweet context genuinely improves alt-text quality.

3. **Conduct cross-platform evaluation**: Test the model on alt-text generation for images from other social media platforms (e.g., Reddit, Facebook) to assess generalizability beyond Twitter's specific context and user behavior patterns.