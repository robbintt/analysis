---
ver: rpa2
title: Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing
arxiv_id: '2310.01180'
source_url: https://arxiv.org/abs/2310.01180
tags:
- search
- architecture
- features
- proposed
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an evolutionary neural architecture search
  (ENAS) approach to improve Transformer-based knowledge tracing (KT) models. The
  key problems addressed are: 1) manually selected input features for fusion may miss
  valuable features and complex interactions, and 2) single global context modeling
  in Transformers fails to capture students'' forgetting behavior when related records
  are temporally distant.'
---

# Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing

## Quick Facts
- arXiv ID: 2310.01180
- Source URL: https://arxiv.org/abs/2310.01180
- Reference count: 40
- Primary result: ENAS-KT achieves AUC up to 0.8062 on knowledge tracing, outperforming state-of-the-art approaches

## Executive Summary
This paper introduces ENAS-KT, an evolutionary neural architecture search approach that automatically discovers optimal Transformer architectures for knowledge tracing. The method addresses two key challenges: manually selected input features may miss valuable interactions, and single global attention fails to capture students' forgetting behavior. ENAS-KT replaces the original global attention path with a sum of global and local paths (combining attention and convolutions), incorporates automated input feature selection, and uses an effective evolutionary algorithm with search space reduction. Experiments on EdNet and RAIEd2020 datasets demonstrate significant performance improvements over state-of-the-art KT approaches.

## Method Summary
ENAS-KT employs an evolutionary algorithm to search for optimal Transformer architectures in a novel search space. The search space includes a selective hierarchical input module for automated feature selection, and replaces the original global attention path with a sum of global (attention+FFN) and local (convolution) paths. The evolutionary algorithm uses binary tournament selection, single-point crossover, and bit-wise mutation, combined with a search space reduction strategy that iteratively removes poor-performing operations. The approach trains a super-Transformer using sandwich training with global, local, and random sub-models, then fine-tunes the best-found architecture on the full dataset.

## Key Results
- ENAS-KT achieves AUC up to 0.8062, significantly outperforming state-of-the-art KT approaches
- The approach successfully automates input feature selection, eliminating manual feature engineering
- Search space reduction strategy accelerates convergence without compromising performance
- Best-found architecture demonstrates good transferability between EdNet and RAIEd2020 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing single global attention with sum of global and local paths improves modeling of students' forgetting behavior
- Mechanism: The global path (attention) captures long-range dependencies while local path (convolution) captures short-term temporal patterns; their sum balances distant and recent context
- Core assumption: Student forgetting behavior is better captured by combining local temporal patterns with global attention
- Evidence anchors:
  - [abstract]: "replaces the original global path containing the attention module in Transformer with the sum of a global path and a local path that could contain different convolutions"
  - [section]: "split the original global path containing the MHSA module in each encoder block into the sum of two paths: one path is the original global path, and another path is the newly added local path containing convolution operations"

### Mechanism 2
- Claim: Automated input feature selection outperforms manual feature fusion in KT
- Mechanism: Evolutionary algorithm explores combinations of input features to find optimal feature set that captures complex interactions
- Core assumption: Manual feature selection may miss valuable features and complex interactions that automated search can discover
- Evidence anchors:
  - [abstract]: "manually selected input features for fusion may miss valuable features and complex interactions"
  - [section]: "design a novel search space for the Transformer, which has been presented in Fig. 1 and there exist three important designs: (1) A selective hierarchical input module"

### Mechanism 3
- Claim: Search space reduction strategy accelerates evolutionary algorithm convergence
- Mechanism: Iteratively removes worst-performing operations from search space based on fitness statistics
- Core assumption: Removing poor operations reduces search space without eliminating optimal architectures
- Evidence anchors:
  - [abstract]: "employ an effective evolutionary algorithm to explore the search space and also suggest a search space reduction strategy to accelerate the convergence"
  - [section]: "Search Space Reduction Strategy. This strategy aims to iteratively remove some worse operations from the search space based on all individuals' fitness statistics to speed up the convergence"

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper builds on Transformer by modifying its attention and adding convolutions
  - Quick check question: What is the difference between self-attention and cross-attention in Transformer?

- Concept: Knowledge tracing task formulation
  - Why needed here: Understanding the KT task is essential to grasp why the modifications address forgetting behavior
  - Quick check question: What information is available at prediction time versus what needs to be predicted in KT?

- Concept: Evolutionary algorithms and neural architecture search
  - Why needed here: The approach uses EA to search for optimal Transformer architectures
  - Quick check question: How does tournament selection work in evolutionary algorithms?

## Architecture Onboarding

- Component map: Encoder/decoder with selective hierarchical input module → global path (attention+FFN) + local path (convolution) → fusion → output
- Critical path: Input feature selection → hierarchical fusion → balanced global/local context modeling → prediction
- Design tradeoffs: Complex feature fusion vs. simple concatenation; balanced attention/convolution vs. pure attention; automated search vs. manual design
- Failure signatures: Poor performance on forgetting behavior; overfitting due to too many features; slow convergence in search
- First 3 experiments:
  1. Compare AUC of architecture with and without local convolution path on a validation set
  2. Test different kernel sizes for convolution operations in local path
  3. Evaluate impact of search space reduction strategy on convergence speed and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ENAS-KT approach perform when applied to datasets with significantly more input features (e.g., N_um > 20)?
- Basis in paper: [explicit] The paper mentions that the hierarchical feature fusion method requires a large number of parameters when N_um is large, leading to an exponential increase in feature pairs and network parameters.
- Why unresolved: The experiments in the paper were conducted on datasets with a limited number of input features (N_um = 12), so the approach's scalability to larger feature sets remains untested.

### Open Question 2
- Question: Can the search cost of ENAS-KT be further reduced without significantly compromising performance, and what techniques could achieve this?
- Basis in paper: [explicit] The paper discusses the high search cost of ENAS-KT and suggests that exploring surrogate models could help reduce it. It also mentions that ENAS-KT(f), a variant with reduced search cost, still performs better than other KT approaches.
- Why unresolved: While the paper introduces a variant (ENAS-KT(f)) with reduced search cost, it does not explore other potential techniques or optimizations to further reduce the search cost while maintaining performance.

### Open Question 3
- Question: How does the transferability of the best-found architecture from one dataset to another affect its performance, and under what conditions does this transfer fail?
- Basis in paper: [explicit] The paper demonstrates that the best architecture found on one dataset (EdNet) achieves similar performance on another dataset (RAIEd2020), indicating good transferability. However, it does not explore the conditions under which this transfer might fail.
- Why unresolved: The paper only provides a single example of successful transfer between two datasets, without investigating the factors that influence transferability or the scenarios where it might not work.

## Limitations
- Lack of extensive ablation studies demonstrating individual contributions of proposed components
- Search space reduction strategy's effectiveness may be domain-specific and not generalize to other tasks
- Missing statistical significance testing across experiments to validate performance differences

## Confidence

- **High confidence**: The experimental methodology and evaluation framework are sound, with appropriate use of large education datasets (EdNet and RAIEd2020) and standard metrics (AUC, ACC, RMSE)
- **Medium confidence**: The claim that automated input feature selection outperforms manual fusion is supported by empirical results, though the paper doesn't provide sufficient ablation studies to isolate this effect
- **Medium confidence**: The effectiveness of combining local convolutions with global attention for modeling forgetting behavior is demonstrated through performance improvements, but the theoretical justification and mechanism could be more rigorously established

## Next Checks

1. **Ablation study on component contributions**: Run experiments isolating each major component (input feature selection, local path addition, search space reduction) to quantify their individual impact on performance and determine which modifications are essential versus complementary

2. **Statistical significance analysis**: Perform paired t-tests or Wilcoxon signed-rank tests on model performance across multiple runs to establish whether the observed improvements over baselines are statistically significant rather than due to random variation

3. **Generalization testing**: Apply the ENAS-KT framework to a different domain (such as item response theory or sequential recommendation) to evaluate whether the local/global attention balance and automated feature selection provide similar benefits outside the education context