---
ver: rpa2
title: 'Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence
  Affect Language Models'
arxiv_id: '2302.13439'
source_url: https://arxiv.org/abs/2302.13439
tags:
- uncertainty
- expressions
- factive
- accuracy
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how expressions of certainty and uncertainty
  in prompts affect the accuracy of language model responses. The authors develop
  a typology of epistemic markers and inject 50 different markers into prompts for
  question answering.
---

# Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models

## Quick Facts
- arXiv ID: 2302.13439
- Source URL: https://arxiv.org/abs/2302.13439
- Reference count: 40
- Key outcome: Language model accuracy varies by over 80% depending on epistemic markers in prompts, with high certainty expressions decreasing accuracy by 7% compared to low certainty expressions.

## Executive Summary
This paper investigates how expressions of certainty and uncertainty in prompts affect the accuracy of language model responses. The authors develop a typology of epistemic markers and inject 50 different markers into prompts for question answering. They find that language models are highly sensitive to epistemic markers, with accuracies varying more than 80% depending on the marker used. Surprisingly, expressions of high certainty result in a 7% decrease in accuracy compared to low certainty expressions, and factive verbs hurt performance while evidentials benefit performance. Analysis of a pretraining dataset suggests that the behavior of language models may be based on mimicking observed language use rather than truly reflecting epistemic uncertainty.

## Method Summary
The study uses zero-shot prompting with expressions of uncertainty injected into QA questions across four datasets (TriviaQA, Natural Questions, CountryQA, and Jeopardy). The authors randomly sample 200 questions per dataset and generate prompts with all 50 expressions of uncertainty. They query GPT-3 with these prompts and record top 10 predictions. Accuracy and probability-on-gold are calculated for each expression, with bootstrap resampling used for confidence intervals. The linguistic features of expressions (strengtheners, weakeners, factive verbs, evidentials) are analyzed for their correlation with performance.

## Key Results
- Language model accuracy varies by over 80% depending on epistemic markers in prompts
- High certainty expressions decrease accuracy by 7% compared to low certainty expressions
- Factive verbs hurt performance while evidentials benefit performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models' accuracy is highly sensitive to epistemic markers in prompts, with accuracy varying more than 80%.
- Mechanism: The model generates text based on learned patterns in the pretraining data, where epistemic markers like "I'm certain" or "I think" have different statistical associations with correct answers.
- Core assumption: The pretraining data contains systematic associations between certain epistemic markers and the likelihood of correct answers.
- Evidence anchors:
  - [abstract] "we find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%."
  - [section 4.1] "we find that GPT3 is highly sensitive to uncertainty cues, and accuracy changes significantly depending on the uncertainty expression used."
  - [corpus] Weak evidence: Related papers like "Humans overrely on overconfident language models, across languages" suggest similar findings but don't directly address accuracy variation.

### Mechanism 2
- Claim: Expressions of high certainty result in decreased accuracy compared to low certainty expressions.
- Mechanism: Factive verbs (e.g., "realize," "know") presuppose truth, and when used in prompts, they create a mismatch between the model's actual uncertainty and the forced certainty, leading to worse performance.
- Core assumption: Factive verbs in the pretraining data are associated with contexts where the speaker has epistemic access to the truth.
- Evidence anchors:
  - [abstract] "expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance."
  - [section 4.1] "we find that weakeners perform significantly better than strengtheners... This effect is driven by the use of factive verbs in strengtheners... and the use of factive verbs consistently results in significant losses in accuracy."
  - [corpus] Moderate evidence: The corpus contains related work like "Revisiting Epistemic Markers in Confidence Estimation" that investigates similar phenomena.

### Mechanism 3
- Claim: Models place probability more evenly across answers when prompted with weakeners, leading to higher accuracy.
- Mechanism: Weakeners signal uncertainty, causing the model to generate a flatter probability distribution over possible answers rather than concentrating probability on a single answer.
- Core assumption: The pretraining data contains patterns where weakeners co-occur with multiple plausible answers rather than single correct answers.
- Evidence anchors:
  - [section 4.2] "we find that weakeners led to a flattening of the distribution of probability mass across answers... the entropy of the probability distribution of top tokens not counting the top prediction... is significantly higher among weakeners than strengtheners."
  - [table 1] "Average entropy of the probability distribution of alternative tokens among weakeners and strengtheners. Across all four datasets, entropy is higher among weakeners."
  - [corpus] Weak evidence: Limited direct evidence in the corpus about probability distribution changes.

## Foundational Learning

- Concept: Epistemic markers and their linguistic classification
  - Why needed here: Understanding the difference between strengtheners, weakeners, factive verbs, and evidentials is crucial for interpreting the experimental results and designing effective prompts.
  - Quick check question: What is the key difference between a factive verb like "realize" and a non-factive verb like "think" in terms of presupposition?

- Concept: Calibration and probability distribution in language models
  - Why needed here: The paper's findings about accuracy variation and entropy changes require understanding how language models assign probabilities to different answers and how this relates to their "confidence."
  - Quick check question: If a language model assigns 0.9 probability to the correct answer and 0.1 probability spread across other answers, is it well-calibrated if the correct answer is indeed the most likely?

- Concept: Pretraining data patterns and their influence on model behavior
  - Why needed here: The mechanisms proposed in the paper rely on the idea that patterns in the pretraining data (e.g., associations between certain epistemic markers and answer correctness) influence how the model responds to prompts.
  - Quick check question: How might the frequency of certain epistemic markers in question-answering contexts within the pretraining data affect a model's response to those markers in prompts?

## Architecture Onboarding

- Component map: Prompt generation -> Language model inference -> Answer extraction -> Accuracy calculation -> Analysis of linguistic features
- Critical path: Prompt generation → Language model inference → Answer extraction → Accuracy calculation → Analysis of linguistic features
- Design tradeoffs: Using zero-shot prompting isolates the effect of epistemic markers but may not reflect real-world usage where models are fine-tuned or use more complex prompting strategies
- Failure signatures: If accuracy doesn't vary significantly with different epistemic markers, or if factive verbs don't show the expected decrease in performance, the proposed mechanisms may not hold
- First 3 experiments:
  1. Replicate the accuracy variation finding across different epistemic markers using a held-out test set
  2. Measure the entropy of the probability distribution for different epistemic markers to confirm the flattening effect
  3. Analyze the pretraining data to identify associations between epistemic markers and answer correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do expressions of uncertainty improve absolute accuracy in question answering tasks, or do they only affect calibration?
- Basis in paper: Explicit
- Why unresolved: The paper found that certain expressions of uncertainty (e.g., evidential markers) improved accuracy in some datasets, but it's unclear if this is a general phenomenon or specific to certain types of questions. Additionally, the paper focused on calibration, but absolute accuracy could be an important factor for real-world applications.
- What evidence would resolve it: Further experiments on a wider range of question answering tasks, including those with different levels of difficulty and domains, could help determine if expressions of uncertainty consistently improve absolute accuracy.

### Open Question 2
- Question: How do expressions of uncertainty interact with model architecture and training data?
- Basis in paper: Inferred
- Why unresolved: The paper used GPT-3 as the language model, but it's unclear how expressions of uncertainty would affect other model architectures or models trained on different datasets. Additionally, the paper focused on zero-shot and in-context learning settings, but it's unclear how expressions of uncertainty would affect fine-tuned models.
- What evidence would resolve it: Experiments on different model architectures and training datasets, as well as fine-tuned models, could help determine how expressions of uncertainty interact with different model configurations.

### Open Question 3
- Question: How do expressions of uncertainty affect human trust and reliance on language models?
- Basis in paper: Inferred
- Why unresolved: The paper focused on how expressions of uncertainty affect model behavior, but it's unclear how these expressions affect human trust and reliance on language models. Additionally, the paper focused on question answering tasks, but it's unclear how expressions of uncertainty would affect other language tasks.
- What evidence would resolve it: User studies and experiments on different language tasks could help determine how expressions of uncertainty affect human trust and reliance on language models.

## Limitations

- The study relies on zero-shot prompting with GPT-3, which may not reflect real-world usage patterns where models undergo fine-tuning or use more sophisticated prompting strategies
- The analysis focuses on single-token answers, potentially missing nuances in multi-token or more complex question types
- The pretraining data analysis provides suggestive evidence about the origin of these effects but cannot definitively prove that the observed behavior stems from mimicking language use patterns rather than genuine uncertainty representation

## Confidence

- High confidence: Language models' accuracy varies significantly with epistemic markers (over 80% variation)
- Medium confidence: High certainty expressions decrease accuracy by 7% compared to low certainty expressions
- High confidence: Factive verbs hurt performance while evidentials benefit performance

## Next Checks

1. **Cross-model validation**: Replicate the experiments using different language models (e.g., GPT-4, Claude, LLaMA) to determine whether the observed effects are specific to GPT-3 or represent a broader phenomenon in language models

2. **Fine-tuning impact assessment**: Test whether fine-tuning language models on datasets with varied epistemic markers reduces or eliminates the sensitivity to these markers in prompts, addressing whether the effect is inherent to pretraining or can be mitigated through fine-tuning

3. **Multi-token answer analysis**: Extend the analysis to questions with multi-token answers to determine whether the observed effects on accuracy and probability distributions generalize beyond single-token responses, and whether different epistemic markers have varying effects on answer complexity