---
ver: rpa2
title: Group Distributionally Robust Knowledge Distillation
arxiv_id: '2311.00476'
source_url: https://arxiv.org/abs/2311.00476
tags:
- distillation
- knowledge
- student
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose GroupDistil, a method that incorporates group-specific
  weights into knowledge distillation to improve performance on minority groups under
  sub-population shifts. Our approach dynamically upweights domains with low student
  performance during optimization, using exponentiated gradient ascent on group weights
  and minibatch SGD on student weights.
---

# Group Distributionally Robust Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2311.00476
- **Source URL**: https://arxiv.org/abs/2311.00476
- **Reference count**: 12
- **Key outcome**: GroupDistil improves worst-group accuracy by dynamically upweighting domains with low student performance during knowledge distillation

## Executive Summary
GroupDistil is a knowledge distillation method that improves worst-group accuracy under sub-population shifts by incorporating group-specific weights into the distillation loss. The method dynamically upweights domains where the student model performs poorly by updating domain weights using exponentiated gradient ascent based on per-group distillation losses. Evaluated on Waterbirds and M&Ms datasets, GroupDistil consistently improves worst-group accuracy compared to standard knowledge distillation, achieving 81.7% worst-group accuracy on Waterbirds versus 78.0% for standard KD.

## Method Summary
GroupDistil extends knowledge distillation by adding domain weights that are updated during training based on per-group performance. The loss function is a weighted sum of distillation losses across domains, where weights are adjusted using exponentiated gradient ascent to emphasize domains with high loss (poor performance). The method uses a fixed pre-trained teacher and a trainable student, with weights normalized after each update. The approach is model-agnostic and can work with any distillation objective, though it requires access to domain labels for weight updates.

## Key Results
- Waterbirds dataset: GroupDistil achieves 81.7% worst-group accuracy vs 78.0% for standard KD
- M&Ms dataset, split 1: Improves accuracy from 55.8% to 62.8%
- M&Ms dataset, split 2: Improves accuracy from 64.2% to 66.5%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic weighting based on per-group performance improves worst-group accuracy
- Mechanism: GroupDistil updates group weights using exponentiated gradient ascent based on per-group distillation losses, amplifying domains with high loss
- Core assumption: Distillation loss reflects student's ability to match teacher's output for each group
- Evidence anchors: Abstract states method dynamically focuses on groups with low performance; section explains upweighting rare subgroups
- Break condition: If distillation loss doesn't correlate with actual group performance, weighting becomes arbitrary

### Mechanism 2
- Claim: Teacher-student knowledge transfer provides better initialization than training from scratch
- Mechanism: Student starts with ImageNet pretraining and learns from pre-trained teacher rather than learning from scratch
- Core assumption: Teacher has learned useful general features that transfer well and distillation preserves these while adapting to group shifts
- Evidence anchors: Abstract mentions consistent improvement over standard KD loss; section notes no restrictions on model architectures or teacher pre-training
- Break condition: If teacher is not sufficiently better on minority groups, distillation may transfer biases

### Mechanism 3
- Claim: Group weighting mechanism is orthogonal to choice of distillation objective
- Mechanism: Weighting mechanism separates from specific distillation loss, making it a general wrapper for improving robustness
- Core assumption: Weighting mechanism is independent of specific loss function and can improve worst-group performance regardless of base method
- Evidence anchors: Section notes main difference with groupDRO is choice of loss function; abstract describes updating weights based on per-group losses
- Break condition: If base distillation objective is already perfectly robust, additional weighting provides minimal benefit

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: GroupDistil is inspired by DRO techniques that optimize for worst-case performance across groups
  - Quick check question: What is the key difference between standard empirical risk minimization and DRO?

- Concept: Knowledge Distillation
  - Why needed here: Method uses teacher-student framework to transfer knowledge while improving robustness to group shifts
  - Quick check question: In standard knowledge distillation loss, what do H(y, σ(zS)) and DKL(σ(zT/τ), σ(zS/τ)) represent?

- Concept: Sub-population Shift
  - Why needed here: Paper addresses scenarios where training and test sets have same groups but different proportions
  - Quick check question: How does sub-population shift differ from domain shift?

## Architecture Onboarding

- Component map: Data -> Domain identification -> Teacher & Student forward pass -> Per-group loss computation -> Weight update -> Student SGD update -> Repeat

- Critical path: Data → Domain identification → Teacher & Student forward pass → Per-group loss computation → Weight update → Student SGD update → Repeat

- Design tradeoffs:
  - Fixed teacher vs. joint training: Fixed teacher simplifies optimization but may limit adaptation
  - Per-batch vs. per-epoch weight updates: Per-batch provides more dynamic weighting but may be noisier
  - Number of domains: More domains increase computational cost but provide finer-grained control

- Failure signatures:
  - High variance in results (as seen in KD baseline for M&Ms) may indicate instability in weighting mechanism
  - If worst-group accuracy improves but average accuracy drops significantly, weighting may be too aggressive
  - If all domain weights converge to similar values, weighting mechanism may not be learning meaningful differences

- First 3 experiments:
  1. Implement baseline KD on Waterbirds with ResNet-50→ResNet-18 and verify results match paper (78.0% worst-group accuracy)
  2. Add GroupDistil weighting mechanism with minimal hyperparameters (ηw=0.01) and verify improvement on Waterbirds
  3. Test on M&Ms with two different data splits to verify robustness across domains and dataset types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GroupDistil's performance compare to other group robustness methods like G-DRO or adaptive group DRO under same distillation framework?
- Basis in paper: Paper compares GroupDistil only to vanilla KD and groupDRO from scratch, not to other group robustness methods within distillation framework
- Why unresolved: Paper only evaluates GroupDistil against vanilla KD and groupDRO, missing comparison with other DRO methods adapted for distillation
- What evidence would resolve it: Empirical comparison showing GroupDistil vs G-DRO vs adaptive group DRO performance on same distillation tasks with identical student-teacher architectures

### Open Question 2
- Question: Can group weights in GroupDistil be learned in completely data-driven way without requiring explicit domain labels?
- Basis in paper: Paper states limitation is assuming access to fully labeled data with both label and domain annotations
- Why unresolved: Current method requires known domain/group labels, but paper acknowledges this as limitation without providing solutions
- What evidence would resolve it: Results demonstrating GroupDistil performance when group weights are learned using domain discovery methods or clustering approaches without ground truth domain labels

### Open Question 3
- Question: How does GroupDistil's worst-group accuracy scale with number of minority groups or increasing domain imbalance?
- Basis in paper: Paper tests on datasets with fixed group imbalances but doesn't systematically vary degree of imbalance or number of minority groups
- Why unresolved: Evaluation uses specific dataset splits but doesn't explore how performance changes as imbalance severity increases or with more minority groups
- What evidence would resolve it: Controlled experiments varying proportion of minority group samples while measuring worst-group accuracy degradation compared to baseline methods

## Limitations
- Requires access to domain/group labels, which is restrictive in practice
- Performance gains are most pronounced on datasets with clear group imbalances
- Computational overhead of maintaining and updating domain weights not quantified

## Confidence

**High confidence**: Worst-group accuracy improvements on Waterbirds (81.7% vs 78.0%) are statistically significant and consistent across seeds.

**Medium confidence**: M&Ms results show improvement but with high variance in KD baseline (standard deviation of 4.5%), suggesting small dataset size may be affecting result stability.

**Low confidence**: Claim that weighting mechanism is orthogonal to choice of distillation objective lacks strong empirical support, as paper only evaluates one specific distillation loss formulation.

## Next Checks
1. Perform paired t-tests on worst-group accuracy across multiple seeds to determine if improvements over KD are statistically significant, particularly for M&Ms dataset with its high variance.

2. Test sensitivity of results to weight update step size (ηw) by running experiments across a range of values (e.g., 0.001, 0.01, 0.1) to understand how robust weighting mechanism is to hyperparameter choice.

3. Evaluate GroupDistil on a dataset without strong group imbalances (e.g., CIFAR-10 or ImageNet) to test whether method provides benefits beyond subpopulation shift scenarios or if it potentially degrades performance when not needed.