---
ver: rpa2
title: 'Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making'
arxiv_id: '2305.17588'
source_url: https://arxiv.org/abs/2305.17588
tags:
- feature
- tasks
- clinical
- pre-training
- biobert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SUFO, a systematic framework to enhance interpretability
  of fine-tuned transformer feature spaces for clinical decision-making. SUFO combines
  Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier
  analysis to investigate pre-training data effects on pathology classification tasks.
---

# Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making

## Quick Facts
- arXiv ID: 2305.17588
- Source URL: https://arxiv.org/abs/2305.17588
- Reference count: 40
- Key outcome: SUFO framework reveals PubMedBERT overfits to minority classes while mixed-domain models resist overfitting, with in-domain pre-training accelerating feature disambiguation during fine-tuning.

## Executive Summary
This paper introduces SUFO, a systematic framework for interpreting fine-tuned transformer feature spaces in clinical decision-making. The framework combines supervised probing, unsupervised similarity analysis, feature dynamics, and outlier analysis to investigate how pre-training data distributions affect pathology classification tasks. The authors evaluate five 110M-sized transformer models across different pre-training corpora, revealing that PubMedBERT, while containing valuable information, can overfit to minority classes under class imbalance, whereas mixed-domain models show greater resistance to overfitting. The study also demonstrates that in-domain pre-training accelerates feature disambiguation during fine-tuning, and that fine-tuned feature spaces undergo significant sparsification, enabling identification of common outlier modes among models.

## Method Summary
The study evaluates five 110M-sized transformer models (BERT, TNLR, BioBERT, Clinical BioBERT, PubMedBERT) on 2907 prostate cancer pathology reports across four classification tasks. Models are fine-tuned using AdamW optimizer with fixed hyperparameters (learning rate 7.6 × 10^-6, weight decay 0.01, epsilon 1 × 10^-8, linear learning rate schedule with 0.2 warm-up ratio, batch size 8, max 25 epochs). The SUFO framework analyzes feature space changes through representational similarity analysis (RSA), principal component analysis (PCA) for sparsification, and cluster analysis for outlier detection. Performance is measured using macro F1 scores, with expert evaluation of outlier reports.

## Key Results
- PubMedBERT can overfit to minority classes under class imbalance, while mixed-domain models show greater resistance to overfitting
- In-domain pre-training accelerates feature disambiguation during fine-tuning, though the key determinants of performance are pre-training task closeness and data source diversity
- Fine-tuned feature spaces undergo significant sparsification, with first two principal components explaining on average 95% of variance
- SUFO successfully identifies common outlier modes among fine-tuned models, revealing reports that challenge model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-domain pre-training accelerates feature disambiguation during fine-tuning.
- Mechanism: Models pre-trained on biomedical data disambiguate class features faster because their initial representations are already closer to the target domain, reducing the number of epochs needed to separate classes.
- Core assumption: Feature disambiguation speed correlates with performance gains.
- Evidence anchors:
  - [abstract]: "in-domain pre-training accelerates feature disambiguation during fine-tuning"
  - [section]: "We find the benefit of in-domain pre-training is manifested in faster feature disambiguation; however, the key determinants of model performance are the closeness of pre-training and target tasks and a diverse pre-training data source enabling more robust textual modeling."
- Break condition: If the target task is far from the pre-training task, in-domain pre-training may not help or could even hurt.

### Mechanism 2
- Claim: Mixed-domain models are more resistant to overfitting on minority classes than domain-specific models.
- Mechanism: Mixed-domain models combine general-domain robustness with domain-specific knowledge, reducing sensitivity to class imbalance during fine-tuning.
- Core assumption: General-domain data provides regularization that domain-specific models lack.
- Evidence anchors:
  - [abstract]: "mixed-domain models show greater resistance to overfitting"
  - [section]: "We find the mixed-domain and domain-specific models exhibit faster feature disambiguation during fine-tuning. However, the domain-specific model, PubMedBERT, can overfit to minority classes when presented with class imbalance, while the mixed-domain models are more resistant to overfitting."
- Break condition: If class imbalance is not present, the difference in overfitting resistance may not manifest.

### Mechanism 3
- Claim: Fine-tuned feature spaces undergo significant sparsification, enabling identification of outlier modes.
- Mechanism: During fine-tuning, the feature space becomes concentrated in a few principal components, simplifying outlier detection by clustering in a low-dimensional space.
- Core assumption: Sparsification preserves discriminative information while reducing dimensionality.
- Evidence anchors:
  - [abstract]: "feature spaces undergo significant sparsification, enabling identification of common outlier modes among fine-tuned models"
  - [section]: "We observe that, for all the models across the 4 tasks, the first two PCs explain on average 95% of the variance in the dataset"
- Break condition: If the data distribution is too complex, sparsification may not capture sufficient variance.

## Foundational Learning

- Representational Similarity Analysis (RSA)
  - Why needed here: To quantify how much the feature space changes during fine-tuning by comparing pre-trained and fine-tuned representations.
  - Quick check question: What does a low RSA score between pre-trained and fine-tuned layers indicate about the model's adaptation?

- Principal Component Analysis (PCA) for sparsification
  - Why needed here: To reveal that fine-tuned feature spaces concentrate in few dimensions, enabling outlier detection.
  - Quick check question: If the first two principal components explain 95% of variance, what does this imply about the effective dimensionality of the feature space?

- Cluster analysis for outlier detection
  - Why needed here: To identify reports that do not conform to typical behavior, revealing challenging cases for the model.
  - Quick check question: How does projecting features onto principal components help in defining clusters for outlier detection?

## Architecture Onboarding

- Component map: Data preprocessing pipeline for pathology reports -> Pre-trained transformer models (BERT, TNLR, BioBERT, Clinical BioBERT, PubMedBERT) -> Fine-tuning module with class imbalance handling -> Feature extraction and analysis modules (RSA, PCA, clustering) -> Expert evaluation interface for outlier annotation

- Critical path: 1. Load and preprocess pathology reports 2. Fine-tune each pre-trained model on the 4 classification tasks 3. Extract activations from best checkpoints 4. Perform RSA to measure layer-wise changes 5. Apply PCA to analyze sparsification 6. Cluster features to identify outliers 7. Solicit expert feedback on outlier reports

- Design tradeoffs:
  - Using same-size models simplifies comparison but may limit performance
  - Random weighted sampling addresses class imbalance but may affect learning dynamics
  - Focusing on last layer classification token balances interpretability and performance

- Failure signatures:
  - Poor performance on minority classes indicates overfitting
  - High variance in validation F1 across runs suggests instability
  - No significant change in RSA scores implies ineffective fine-tuning

- First 3 experiments:
  1. Compare feature extraction vs. fine-tuning performance across all models
  2. Visualize feature dynamics through epochs to identify disambiguation patterns
  3. Test outlier detection sensitivity by varying number of principal components used

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does PubMedBERT's overfitting to minority classes persist across different biomedical classification tasks beyond prostate pathology?
- Basis in paper: [explicit] The paper demonstrates PubMedBERT's tendency to overfit to minority classes in prostate pathology tasks, but doesn't test other medical domains.
- Why unresolved: The study only examines four prostate pathology classification tasks, limiting generalizability to other clinical domains.
- What evidence would resolve it: Testing PubMedBERT on diverse biomedical classification tasks (e.g., radiology reports, EHR data, other cancer types) while systematically varying class imbalance ratios would clarify if this overfitting behavior is domain-specific or generalizable.

### Open Question 2
- Question: What specific linguistic or structural features in the pathology reports cause PubMedBERT to overfit while mixed-domain models resist this behavior?
- Basis in paper: [inferred] The paper observes this phenomenon but doesn't analyze the underlying linguistic or structural differences in the reports that trigger overfitting.
- Why unresolved: The authors focus on model-level analysis but don't examine the text characteristics that differentiate minority class reports from majority class reports.
- What evidence would resolve it: Detailed linguistic analysis comparing report features (terminology complexity, sentence structure, mention of specific biomarkers) between correctly and incorrectly classified minority class instances could identify problematic patterns.

### Open Question 3
- Question: Would incorporating clinical data during PubMedBERT's pre-training (creating a hybrid PubMed+clinical notes model) mitigate its overfitting issues while preserving its domain-specific advantages?
- Basis in paper: [explicit] The paper shows Clinical BioBERT (which includes clinical notes) outperforms PubMedBERT on pathology tasks, suggesting clinical data helps.
- Why unresolved: The study doesn't test whether PubMedBERT could be improved by adding clinical data during pre-training rather than switching to a mixed-domain approach entirely.
- What evidence would resolve it: Training and evaluating a PubMedBERT variant pre-trained on both PubMed abstracts and clinical notes, then comparing its performance and overfitting behavior to the original PubMedBERT and Clinical BioBERT.

## Limitations
- The study focuses on 110M-sized models, which may not represent the performance characteristics of larger clinical transformers used in production
- The pathological dataset represents a single specialty (prostate cancer), limiting generalizability to other medical domains
- Interpretability analyses rely on feature space visualizations that may not fully capture clinically relevant decision boundaries

## Confidence
- High confidence: The observation that in-domain pre-training accelerates feature disambiguation is well-supported by the RSA and PCA analyses showing faster variance concentration during fine-tuning
- Medium confidence: The claim about mixed-domain models being more resistant to overfitting requires further validation, as the study only tested one class-imbalanced scenario
- Medium confidence: The sparsification finding is statistically significant but may be an artifact of the specific pathology report domain rather than a universal property of fine-tuned transformers

## Next Checks
1. Apply SUFO to a different medical specialty (e.g., radiology reports) to test whether sparsification and outlier detection patterns generalize beyond prostate pathology
2. Repeat the analysis with larger transformer variants (340M-770M parameters) to determine if the interpretability patterns hold across model scales used in production clinical systems
3. Evaluate whether the identified outlier modes persist across different time periods of report collection, assessing whether they represent systematic modeling failures or dataset-specific anomalies