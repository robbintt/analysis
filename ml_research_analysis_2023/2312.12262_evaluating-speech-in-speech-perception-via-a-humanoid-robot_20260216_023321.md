---
ver: rpa2
title: Evaluating Speech-in-Speech Perception via a Humanoid Robot
arxiv_id: '2312.12262'
source_url: https://arxiv.org/abs/2312.12262
tags:
- test
- https
- robot
- speech
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study aimed to assess whether a humanoid NAO robot could serve
  as an effective auditory interface for speech-in-speech perception testing, offering
  a potentially more engaging experience than standard computer setups. The test used
  the Coordinate Response Measure (CRM) paradigm, presenting target sentences with
  color and number keywords amid competing speech under various target-to-masker ratios
  and voice conditions.
---

# Evaluating Speech-in-Speech Perception via a Humanoid Robot

## Quick Facts
- arXiv ID: 2312.12262
- Source URL: https://arxiv.org/abs/2312.12262
- Reference count: 40
- Key outcome: NAO robot provides equivalent speech-in-speech perception test results to computer interface while increasing participant engagement

## Executive Summary
This study evaluates whether a humanoid NAO robot can serve as an effective auditory interface for speech-in-speech perception testing. Using the Coordinate Response Measure paradigm, 27 normal-hearing young adults completed tests on both computer and robot interfaces under varying target-to-masker ratios and voice conditions. Speech intelligibility scores were comparable between setups, with Bayesian analysis supporting result equivalence. Participants showed more positive engagement behaviors during robot testing, though data collection duration was longer due to head movement feedback. The findings suggest the NAO robot is a viable alternative for speech-in-speech perception testing, offering potential benefits for engagement despite technical limitations.

## Method Summary
The study used a within-subjects design with 27 normal-hearing participants aged 19-36. Participants completed speech-in-speech perception testing using both a computer interface and a NAO robot ("Sam") with randomized order. The CRM paradigm presented target sentences with color and number keywords amid competing speech at three TMRs (-6 dB, 0 dB, +6 dB) and two voice cue conditions (F0/VTL manipulated). Each participant completed a training phase (4 trials) followed by data collection (91 trials per interface). Speech intelligibility was measured as percent correct responses, with additional NARS questionnaires and video recordings for human-robot interaction analysis.

## Key Results
- Speech intelligibility scores were functionally equivalent between computer and NAO robot interfaces
- Data collection duration was significantly longer with the robot (approximately 2 minutes) due to head movement feedback
- Participants displayed more positive engagement behaviors (smiling, laughing) during robot-based testing
- Bayesian analysis supported equivalence of results between the two setups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NAO robot provides comparable speech-in-speech perception test results to a standard computer setup.
- Mechanism: Both setups present identical acoustic stimuli and collect the same response data, ensuring equivalent measurement accuracy despite differences in hardware and user interface.
- Core assumption: The acoustic quality differences between the computer and NAO speakers are not large enough to affect the outcome of speech-in-speech perception testing.
- Evidence anchors:
  - [abstract] "Speech intelligibility results showed functional similarity between the computer and NAO setups."
  - [section] "Results of the classical RMANOVA showed no significant difference between the percent correct scores obtained when using the computer or Sam."
- Break condition: If the speaker quality differences introduce significant distortions or masking effects that alter the perception of the target speech relative to the masker speech.

### Mechanism 2
- Claim: The NAO robot increases participant engagement during speech-in-speech perception testing.
- Mechanism: The physical presence and social interaction capabilities of the robot, such as positive and negative feedback through head movements, create a more engaging experience compared to a computer interface.
- Core assumption: Participants find social robots more engaging than computer interfaces, leading to increased attention and motivation during repetitive tasks.
- Evidence anchors:
  - [abstract] "The presence of more positive backchannels when using NAO suggest higher engagement with the robot in comparison to the computer."
  - [section] "Behavioural coding results...showed...significantly more frequent 'smiling' when using Sam [t(1) = -13, p < 0.05]."
- Break condition: If the robot's social interaction features are perceived as artificial or distracting, leading to decreased focus on the task.

### Mechanism 3
- Claim: The NAO robot is a viable alternative auditory interface for speech-in-speech perception testing in research and clinical settings.
- Mechanism: The robot's speech-based communication and interactive features offer a low-cost tool for auditory perception evaluation that can improve participant experience without compromising data quality.
- Core assumption: The benefits of increased engagement and the potential for wider accessibility outweigh the increased data collection duration and technical limitations of the robot setup.
- Evidence anchors:
  - [abstract] "Overall, the study presents the potential of the NAO for presenting speech materials and collecting psychophysical measurements for speech-in-speech perception."
  - [section] "Our overall results show that the NAO robot shows promise to be used as an auditory interface for speech-in-speech testing."
- Break condition: If the technical limitations of the robot, such as processing speed or sound quality, significantly impact the accuracy or reliability of the test results.

## Foundational Learning

- Concept: Coordinate Response Measure (CRM) paradigm
  - Why needed here: The CRM is the specific speech-in-speech perception test used in the study, and understanding its structure is crucial for interpreting the results.
  - Quick check question: What are the key components of a CRM sentence, and how are they used in the test?

- Concept: Target-to-masker ratio (TMR) and voice cue manipulation
  - Why needed here: These are the experimental variables manipulated in the study to assess speech intelligibility under different listening conditions.
  - Quick check question: How do changes in TMR and voice cues (F0 and VTL) affect the perception of speech in background noise?

- Concept: Human-robot interaction (HRI) evaluation methods
  - Why needed here: The study uses both self-assessment questionnaires (NARS) and behavioral coding of video recordings to assess participant engagement with the robot.
  - Quick check question: What are the advantages and limitations of using NARS and behavioral coding to evaluate HRI in the context of psychophysical testing?

## Architecture Onboarding

- Component map: NAO robot -> Tablet (response logging) -> Website (researcher control) -> Speech-in-speech perception test software
- Critical path: Participant interacts with robot -> listens to stimuli -> logs responses on tablet -> data collected and analyzed
- Design tradeoffs: Robot offers increased engagement but has longer data collection duration and potential technical limitations compared to computer setup
- Failure signatures: Decreased speech intelligibility scores, increased data collection duration, or negative HRI metrics could indicate issues with robot's hardware, software, or interaction design
- First 3 experiments:
  1. Test the robot's audio output quality and compare it to the computer's to ensure it meets requirements for speech-in-speech perception testing
  2. Conduct a pilot study with small group of participants to assess the robot's interaction design and identify any usability issues
  3. Analyze the data collection duration and identify ways to optimize the robot's processing speed or feedback presentation to reduce overall test time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the speech intelligibility scores of normal-hearing participants using the NAO robot compare to those of hearing-impaired participants using the same setup?
- Basis in paper: [inferred] The study focuses on normal-hearing participants and shows that the NAO robot is a viable alternative for speech-in-speech perception testing. However, it does not address the potential benefits or limitations for hearing-impaired individuals.
- Why unresolved: The paper does not include data or analysis on hearing-impaired participants, and the impact of the NAO robot on this population is not explored.
- What evidence would resolve it: Conducting the same experiment with hearing-impaired participants and comparing their speech intelligibility scores to those of normal-hearing participants using the NAO robot.

### Open Question 2
- Question: How does the inclusion of speech recognition on the NAO robot affect the overall test duration and participant engagement compared to using a tablet for response logging?
- Basis in paper: [explicit] The paper suggests that using speech recognition on the NAO robot could enhance its social presence and functionality, but it does not provide data on the impact of this change on test duration or engagement.
- Why unresolved: The study does not implement speech recognition and therefore cannot provide insights into its effects on the test.
- What evidence would resolve it: Implementing speech recognition on the NAO robot and measuring the test duration and participant engagement compared to the current setup with a tablet.

### Open Question 3
- Question: What is the long-term impact of using a humanoid robot like NAO on participant engagement and test performance in repeated testing sessions?
- Basis in paper: [inferred] The study shows that participants have a more positive attitude towards robots and exhibit higher engagement during robot-based testing. However, it does not explore the effects of repeated interactions over time.
- Why unresolved: The paper only includes a single testing session, and the long-term effects of repeated interactions with the robot are not examined.
- What evidence would resolve it: Conducting multiple testing sessions with the same participants over an extended period and measuring changes in engagement and test performance.

## Limitations
- Study population limited to young, normal-hearing native English speakers (19-36 years), limiting generalizability
- Increased data collection duration (approximately 2 minutes) due to head movement feedback may impact practical implementation
- Technical limitations of NAO robot's speakers may introduce acoustic differences affecting speech perception testing outcomes

## Confidence
- **High Confidence**: The equivalence of speech intelligibility scores between robot and computer setups (supported by both classical and Bayesian ANOVA results showing no significant differences)
- **Medium Confidence**: The robot's ability to increase participant engagement (supported by behavioral coding and questionnaire data, though the engagement metrics were not directly compared to clinical utility)
- **Low Confidence**: The practical viability of the robot in clinical settings (limited by the small, specific participant pool and lack of long-term implementation data)

## Next Checks
1. Conduct a follow-up study with older adults and individuals with hearing impairments to assess the robot's effectiveness across diverse populations
2. Implement processing optimizations or alternative feedback mechanisms to reduce data collection duration and evaluate the impact on both engagement and test accuracy
3. Perform acoustic analysis comparing the robot's speaker output to calibrated clinical audiometry equipment to quantify any potential distortions or masking effects on speech perception