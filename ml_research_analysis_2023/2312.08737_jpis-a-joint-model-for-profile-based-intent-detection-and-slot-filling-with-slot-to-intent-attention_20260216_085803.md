---
ver: rpa2
title: 'JPIS: A Joint Model for Profile-based Intent Detection and Slot Filling with
  Slot-to-Intent Attention'
arxiv_id: '2312.08737'
source_url: https://arxiv.org/abs/2312.08737
tags:
- intent
- slot
- detection
- filling
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes JPIS, a joint model for profile-based intent
  detection and slot filling that incorporates supporting profile information into
  its encoder and introduces a slot-to-intent attention mechanism to transfer slot
  information representations to intent detection. Experiments on the Chinese benchmark
  dataset ProSLU show that JPIS substantially outperforms previous profile-based models,
  establishing a new state-of-the-art performance in overall accuracy at 86.67%.
---

# JPIS: A Joint Model for Profile-based Intent Detection and Slot Filling with Slot-to-Intent Attention

## Quick Facts
- **arXiv ID**: 2312.08737
- **Source URL**: https://arxiv.org/abs/2312.08737
- **Reference count**: 0
- **Primary result**: JPIS achieves 86.67% overall accuracy on ProSLU dataset, establishing new state-of-the-art performance

## Executive Summary
This paper introduces JPIS, a joint model for profile-based intent detection and slot filling that incorporates supporting profile information into its encoder and introduces a slot-to-intent attention mechanism. The model addresses the challenge of reducing utterance ambiguity by leveraging user-specific supporting profile information. Experiments on the Chinese benchmark dataset ProSLU demonstrate substantial improvements over previous profile-based models, achieving 87.95% intent accuracy, 85.76% slot F1 score, and 86.67% overall accuracy.

## Method Summary
JPIS is a joint model that processes input text and profile information through an utterance encoder with BiLSTM and self-attention layers. The model creates a profile representation matrix by projecting user profile and context awareness vectors, then combines this with word embeddings using multiplicative attention. A slot-to-intent attention mechanism computes label-specific representations for intents and slots, calculates a bilinear attention matrix between them, and uses this to transfer slot information to intent detection. The model jointly trains intent detection and slot filling tasks with a weighted sum loss function.

## Key Results
- Establishes new state-of-the-art performance on ProSLU with 86.67% overall accuracy
- Achieves 87.95% intent accuracy and 85.76% slot F1 score
- Outperforms previous profile-based models by 2.5-5.1% across all metrics
- Ablation analysis demonstrates effectiveness of slot-to-intent attention and supporting profile information

## Why This Works (Mechanism)

### Mechanism 1: Slot-to-Intent Attention
The slot-to-intent attention transfers slot information to intent detection by computing similarity between intent and slot label representations using a bilinear attention matrix. The model first computes label-specific representations for intents and slots using attention weights, then calculates a bilinear attention matrix C between these representations, and finally uses this matrix to compute weighted intent label representations for intent prediction. Core assumption: Slot label representations contain complementary information that can improve intent detection when properly aligned with intent representations.

### Mechanism 2: Profile Information Integration
Incorporating supporting profile information reduces utterance ambiguity by providing user-specific context that disambiguates similar utterances. Profile information (user preferences and context awareness vectors) is projected into the same space as word embeddings, then combined with word representations through multiplicative attention to create enriched token representations that include user context. Core assumption: Ambiguous utterances can be disambiguated when additional user-specific context is available and properly integrated.

### Mechanism 3: Joint Training with Weighted Loss
Joint training with weighted loss balances intent detection and slot filling to optimize overall accuracy. The model optimizes a weighted sum of intent detection loss and slot filling loss, allowing the model to learn task interactions while preventing one task from dominating training. Core assumption: Intent detection and slot filling are correlated tasks that benefit from joint optimization, and their relative importance can be balanced through loss weighting.

## Foundational Learning

- **Concept**: Multiplicative attention for profile information integration
  - Why needed here: To combine profile information with word representations in a way that allows different profile features to contribute differently to each word token
  - Quick check question: Why is multiplicative attention preferred over additive attention for combining profile information with word embeddings?

- **Concept**: Label-specific attention for intent and slot representations
  - Why needed here: To create separate representation spaces for each intent and slot label that capture their unique characteristics
  - Quick check question: How does label-specific attention differ from standard self-attention in terms of what it's attending to?

- **Concept**: BIO scheme for slot filling
  - Why needed here: To handle slot boundaries and enable structured prediction of slot values across token sequences
  - Quick check question: What are the three tag types in BIO scheme and how do they represent slot boundaries?

## Architecture Onboarding

- **Component map**: Utterance Encoder -> Slot-to-Intent Attention -> Intent Decoder, and simultaneously Utterance Encoder -> Slot Decoder
- **Critical path**: Utterance Encoder → Slot-to-Intent Attention → Intent Decoder, and simultaneously Utterance Encoder → Slot Decoder
- **Design tradeoffs**:
  - Joint training vs. separate training: Joint training allows task interaction but may cause gradient interference
  - Profile information integration: Early integration (in encoder) vs. late integration (in decoder)
  - Attention mechanism complexity: More complex attention (slot-to-intent) vs. simpler approaches
- **Failure signatures**:
  - High intent accuracy but low slot F1: Slot-to-intent attention may be transferring irrelevant information
  - Low overall accuracy despite good individual task performance: Profile information integration may be ineffective
  - Gradient explosion or vanishing: Joint training loss weighting may need adjustment
- **First 3 experiments**:
  1. Remove slot-to-intent attention component and measure performance drop
  2. Train without profile information to quantify its contribution
  3. Vary the loss weighting λ to find optimal balance between intent and slot tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of JPIS compare to other joint models on profile-based intent detection and slot filling tasks when evaluated on datasets other than ProSLU?
- **Basis in paper**: The paper evaluates JPIS on the Chinese benchmark dataset ProSLU, which is the only publicly available dataset with supporting profile information.
- **Why unresolved**: The evaluation is limited to a single dataset, and the effectiveness of JPIS on other datasets is not explored.
- **What evidence would resolve it**: Testing JPIS on multiple profile-based intent detection and slot filling datasets from different languages or domains would provide a comprehensive comparison of its performance.

### Open Question 2
- **Question**: How does the performance of JPIS vary with different types and amounts of supporting profile information?
- **Basis in paper**: The paper mentions two types of supporting profile information: User Profile and Context Awareness, but does not explore the impact of varying the types or amounts of this information.
- **Why unresolved**: The paper does not provide insights into how different combinations or quantities of profile information affect the model's performance.
- **What evidence would resolve it**: Conducting experiments with different combinations and quantities of supporting profile information would reveal how they influence JPIS's effectiveness.

### Open Question 3
- **Question**: What is the impact of incorporating knowledge graphs as additional supporting profile information in JPIS?
- **Basis in paper**: The paper briefly mentions the potential use of knowledge graphs as additional information to disambiguate mentions, but does not investigate its impact on the model's performance.
- **Why unresolved**: The role of knowledge graphs in enhancing the model's accuracy is not explored in the experiments.
- **What evidence would resolve it**: Integrating knowledge graphs into JPIS and evaluating its performance would demonstrate the contribution of this additional information.

## Limitations
- Evaluation is conducted on a single Chinese benchmark dataset (ProSLU), limiting generalizability to other languages or domains
- The ProSLU dataset has only 16 intent types and 22 slot types, which may not represent the complexity of real-world applications
- Long-term effectiveness depends heavily on data quality and availability of supporting user information, which is not addressed in the experimental setup

## Confidence
- **High Confidence**: The architectural design and mathematical formulations (Equations 9-18) are clearly specified and reproducible
- **Medium Confidence**: The ablation analysis demonstrates that slot-to-intent attention and profile information contribute to performance gains
- **Low Confidence**: The long-term effectiveness of profile-based approaches depends heavily on data quality and availability of supporting user information

## Next Checks
1. **Cross-domain validation**: Test JPIS on non-Chinese datasets (e.g., ATIS, SNIPS) to verify language and domain generalizability of the slot-to-intent attention mechanism
2. **Profile information sensitivity analysis**: Systematically vary the quality and relevance of supporting profile information to determine robustness thresholds and identify failure conditions
3. **Scalability assessment**: Evaluate JPIS performance as the number of intent and slot types increases beyond the current 16 intent and 22 slot types to assess practical deployment limits