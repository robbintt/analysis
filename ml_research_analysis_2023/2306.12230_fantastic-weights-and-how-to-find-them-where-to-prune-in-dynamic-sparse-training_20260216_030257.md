---
ver: rpa2
title: 'Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training'
arxiv_id: '2306.12230'
source_url: https://arxiv.org/abs/2306.12230
tags:
- pruning
- training
- sparse
- criteria
- criterion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of different pruning criteria
  on the performance of Dynamic Sparse Training (DST), a framework that trains sparse
  neural networks by iteratively pruning and regrowing weights. The paper focuses
  on the often-overlooked pruning criterion, which determines the importance score
  for each weight.
---

# Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training

## Quick Facts
- arXiv ID: 2306.12230
- Source URL: https://arxiv.org/abs/2306.12230
- Reference count: 40
- Primary result: Simple magnitude-based pruning outperforms more sophisticated criteria in low-density regimes of Dynamic Sparse Training

## Executive Summary
This study investigates the impact of different pruning criteria on Dynamic Sparse Training (DST) performance, focusing on the often-overlooked criterion component that determines weight importance. Through extensive empirical analysis across diverse models and datasets, the authors surprisingly find that simple magnitude-based pruning consistently outperforms more elaborate techniques, particularly in low-density regimes. The research reveals that most criteria yield similar results, with structural similarity analysis showing high overlap between different methods. The study challenges the assumption that complex pruning criteria are necessary for effective DST, suggesting that simpler approaches may be more effective than previously thought.

## Method Summary
The study evaluates Dynamic Sparse Training (DST) frameworks across various pruning criteria including magnitude-based, gradient-based, and hybrid methods. Models are trained with different initialization strategies (ER/ERK), cosine-decayed pruning fractions starting at 0.5, and update periods typically set to 800 iterations. The framework iteratively prunes and regrows weights while maintaining target density levels. Performance is evaluated across multiple architectures (MLPs, CNNs, ResNets) and datasets (Higgs, CIFAR10/100, Tiny-ImageNet, FashionMNIST) using both random and gradient-based growth modes.

## Key Results
- Magnitude-based pruning consistently outperforms more sophisticated criteria in low-density regimes
- Update frequency has minimal impact on performance, with infrequent updates (once per 16 epochs) still yielding excellent results
- Different pruning criteria produce structurally similar sparse networks with high Jaccard index overlap
- Complex gradient-based criteria often perform worse than or equal to simple magnitude scoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In DST, the weight's magnitude is a reliable indicator of its importance in sparse regimes
- Mechanism: Magnitude-based pruning directly uses the absolute value of weights as importance scores, selecting the smallest magnitudes for removal. This method remains effective even when the network's connectivity is dynamic and evolving
- Core assumption: The absolute value of a weight correlates with its contribution to model performance, even in evolving sparse networks
- Evidence anchors:
  - [abstract]: "differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based pruning"
  - [section 5.2]: "the more elaborate criteria using gradient-based approaches usually either perform worse than the simple magnitude score, or do not hold a clear advantage over it"
- Break condition: If the magnitude does not correlate with performance (e.g., in highly non-linear or adversarial settings), this method may fail

### Mechanism 2
- Claim: Frequent topology updates are not necessary for good DST performance
- Mechanism: The sparse connectivity can be adjusted periodically (e.g., every 800 iterations), and even infrequent updates (e.g., once every 16 epochs) still lead to competitive performance compared to dense models
- Core assumption: The sparse network can effectively adapt its structure with only a few connectivity updates during training
- Evidence anchors:
  - [section 5.3]: "performing just one topology update every âˆ†t = 6400 iterations (approximately once per 16 epochs) still gives excellent performance"
  - [section 5.3]: "not performing any topology updates at all (i.e., static sparse training) deteriorates performance"
- Break condition: If the dataset or task requires rapid adaptation of the network structure, infrequent updates may hinder performance

### Mechanism 3
- Claim: Different pruning criteria lead to similar sparse network topologies in DST
- Mechanism: The structural similarity of the pruned sets by each criterion, measured by the Jaccard index, shows high overlap, indicating that different criteria often select similar weights for removal
- Core assumption: The criteria incorporate similar information (e.g., weight magnitude) and thus make similar decisions about weight importance
- Evidence anchors:
  - [section 5.4]: "CMagnitude, CMEST and CSET criteria choose similar sets of weights to prune, while CSNIP and CRSensitivity are more diverse in their selections"
  - [section 5.4]: "the best-performing methods from Section 5.2 indeed make similar decision choices while having a smaller overlap with the less efficient criteria"
- Break condition: If the criteria use fundamentally different information or if the network's dynamics are highly sensitive to the pruning choices, the resulting topologies may diverge

## Foundational Learning

- Concept: Dynamic Sparse Training (DST)
  - Why needed here: DST is the framework being studied, where the sparse connectivity of a neural network is adapted during training by iteratively pruning and regrowing weights
  - Quick check question: What are the two key components of the DST framework that determine how weights are pruned and regrown?

- Concept: Pruning Criteria
  - Why needed here: Pruning criteria determine the importance score for each weight, which is crucial for deciding which weights to remove during DST
  - Quick check question: How does the choice of pruning criterion impact the performance of DST in low-density regimes?

- Concept: Structural Similarity Analysis
  - Why needed here: Analyzing the structural similarity of the pruned sets by each criterion helps understand how different methods impact the network topology
  - Quick check question: What metric is used to measure the similarity between the sets of weights selected for pruning by different criteria?

## Architecture Onboarding

- Component map:
  - DST Framework -> Pruning Criteria -> Growth Criteria -> Network Architectures -> Datasets
  - Sparse network initialization (ER/ERK) -> Training iterations -> Pruning selection -> Weight regrowth -> Density maintenance

- Critical path:
  1. Initialize sparse network with ER or ERK method
  2. Train network for a fixed number of iterations
  3. Apply pruning criterion to select weights for removal
  4. Regrow weights using growth criterion to maintain density
  5. Repeat steps 2-4 for a set number of iterations or until convergence

- Design tradeoffs:
  - Update Frequency vs. Performance: More frequent updates may lead to better adaptation but increase computational cost
  - Pruning Criterion Complexity vs. Effectiveness: Simpler criteria (e.g., magnitude) may be as effective as complex ones in sparse regimes
  - Global vs. Local Pruning: Global pruning can change sparsity distributions but risks disconnecting the model

- Failure signatures:
  - Poor Performance: If the chosen pruning criterion does not align with the network's needs, performance may degrade
  - Instability: Gradient-based criteria may cause instability, especially with small update periods
  - Disconnection: Global pruning may disconnect the model if not balanced across layers

- First 3 experiments:
  1. Compare the performance of different pruning criteria (e.g., magnitude, gradient-based) on a small MLP with CIFAR10
  2. Investigate the impact of update period on the performance of different pruning criteria using ResNet-56 on CIFAR10
  3. Analyze the structural similarity of the pruned sets by each criterion using the Jaccard index on a small-CNN with CIFAR10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pruning criterion in Dynamic Sparse Training (DST) affect the final sparse network topology in terms of parameter values and not just connectivity structure?
- Basis in paper: The paper focuses on the structural similarity of pruned sets and final masks but notes that the Jaccard index disregards parameter values. It suggests additional topographic insights could incorporate parameter values to analyze graph structure
- Why unresolved: The current analysis only considers the binary mask of active/inactive weights. The actual values of the weights in the final sparse network could differ significantly between pruning criteria, potentially affecting performance in ways not captured by the Jaccard index
- What evidence would resolve it: A detailed comparison of the parameter value distributions and correlation structures in the final sparse networks produced by different pruning criteria, along with an analysis of how these differences correlate with performance

### Open Question 2
- Question: How do the findings on pruning criteria in DST translate to large language models and other non-vision domains?
- Basis in paper: The paper explicitly states that the research was conducted mainly on computer vision datasets and one tabular dataset, noting that it would be interesting to verify how the findings translate to large language models
- Why unresolved: The effectiveness of pruning criteria might be domain-dependent. Language models have different architectures, training dynamics, and sparsity patterns compared to vision models, which could affect the relative performance of different pruning criteria
- What evidence would resolve it: Extensive empirical studies of various pruning criteria in DST applied to large language models and other non-vision domains, comparing their performance to dense models and static sparse training baselines

### Open Question 3
- Question: What is the relationship between the noisiness of gradients and the effectiveness of gradient-based pruning criteria in DST?
- Basis in paper: The paper loosely hypothesizes that the poor performance of gradient-based pruning criteria might be due to high variance in the gradient, especially in DST where gradients are computed on batches rather than the full dataset
- Why unresolved: While the paper presents some preliminary evidence by varying the batch size, it does not provide a comprehensive analysis of how gradient noise affects the reliability of gradient-based pruning scores in DST
- What evidence would resolve it: Controlled experiments varying the batch size, learning rate, and noise injection in gradients to systematically study their impact on the performance of gradient-based pruning criteria in DST. Additionally, theoretical analysis of how gradient noise propagates through the pruning process and affects the selection of weights

## Limitations

- Focus primarily on supervised image classification tasks, limiting generalizability to other domains
- Potential sensitivity to initialization and optimizer hyperparameters not fully specified in the study
- Simple magnitude pruning may overlook nuanced benefits of sophisticated criteria in specialized scenarios

## Confidence

- Magnitude pruning effectiveness: High confidence from extensive empirical validation across multiple architectures and datasets
- Update frequency tolerance: Medium confidence, as analysis covers periodic updates but not extreme frequencies or adaptive scheduling
- Structural similarity claims: High confidence from Jaccard index analysis, though functional similarity not fully explored

## Next Checks

1. Test magnitude pruning's robustness across different optimizers and learning rate schedules beyond the cosine decay used here
2. Evaluate whether structural similarity correlates with functional similarity by measuring performance when transferring pruned structures between models
3. Investigate adaptive update periods that respond to training dynamics rather than fixed intervals