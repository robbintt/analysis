---
ver: rpa2
title: Temporal Robustness against Data Poisoning
arxiv_id: '2302.03684'
source_url: https://arxiv.org/abs/2302.03684
tags:
- data
- temporal
- poisoning
- robustness
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Data poisoning attacks manipulate ML models through malicious training
  data. This paper introduces a temporal threat model using timestamps to define earliness
  (how far in advance an attack starts) and duration (how long it lasts).
---

# Temporal Robustness against Data Poisoning

## Quick Facts
- arXiv ID: 2302.03684
- Source URL: https://arxiv.org/abs/2302.03684
- Authors: 
- Reference count: 18
- Key outcome: Data poisoning attacks manipulate ML models through malicious training data. This paper introduces a temporal threat model using timestamps to define earliness (how far in advance an attack starts) and duration (how long it lasts). This enables meaningful robustness guarantees even with unbounded poisoned samples. The authors develop a temporal robustness benchmark simulating continuous data collection and periodic model updates, and propose temporal aggregation as a baseline defense. Experiments show the defense achieves provable temporal temporal robustness with minimal accuracy loss, offering protection against attacks starting up to 8-11 months in advance and lasting 6-9 months. The framework provides a novel approach to addressing data poisoning when attackers can easily inject many poisoned samples.

## Executive Summary
This paper addresses the fundamental challenge of data poisoning attacks in machine learning, where attackers can inject malicious samples into training data to manipulate model behavior. The key insight is that by leveraging timestamps to track when data is collected, we can define temporal constraints (earliness and duration) that bound the attacker's influence regardless of how many poisoned samples they inject. The authors introduce a temporal threat model that provides meaningful robustness guarantees even when attackers can poison unlimited amounts of data, as long as they cannot manipulate the timestamps themselves. They demonstrate that temporal aggregation—training multiple base models on different time windows and aggregating their predictions—provides provable temporal robustness while maintaining competitive clean accuracy.

## Method Summary
The method employs temporal aggregation as a defense against data poisoning attacks. Base learners are trained on sliding windows of training data, with each base learner trained on a specific time period. At prediction time, the system aggregates predictions from multiple base learners using majority voting. The temporal robustness is evaluated using a benchmark that simulates continuous data collection and periodic model updates, measuring the robust fraction of test samples that remain robust against attacks with bounded earliness and duration. The approach is evaluated on the News Category Dataset using RoBERTa features extracted from headlines.

## Key Results
- Temporal aggregation achieves provable temporal robustness against data poisoning attacks
- The defense maintains competitive clean accuracy while providing protection against attacks starting 8-11 months in advance
- Robustness guarantees hold even when attackers can inject unbounded amounts of poisoned data
- The approach successfully protects against poisoning attacks lasting 6-9 months

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal robustness is possible because poisoned samples cannot arbitrarily set their timestamps, constraining when attacks can be effective.
- Mechanism: By incorporating birth dates of data, the defense limits the temporal window in which poisoned samples can influence predictions. This creates a measurable "earliness" and "duration" that bounds the attacker's influence regardless of how many samples are poisoned.
- Core assumption: Attackers cannot set arbitrary timestamps for their poisoned samples (Assumption 2.4).
- Evidence anchors:
  - [abstract] "we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past"
  - [section 2.2] "Assumption 2.4 (Partial Reliability of Birth Dates). The attacker has no capability to directly set the birth dates of samples."
  - [corpus] Weak evidence - only one neighbor paper mentions timestamps in relation to poisoning attacks.
- Break condition: If attackers find a way to spoof timestamps or if the timestamp mechanism fails, the temporal robustness guarantee breaks down.

### Mechanism 2
- Claim: Temporal aggregation provides provable temporal robustness by limiting the influence of poisoned data to within certain time intervals.
- Mechanism: Temporal aggregation trains base models on data from specific time windows and aggregates predictions from multiple models. This dilutes the impact of poisoned samples because only a subset of base models can be affected by any given poisoning attack.
- Core assumption: Base learners are deterministic and the aggregation process is robust to temporal poisoning attacks within bounded earliness/duration.
- Evidence anchors:
  - [section 4.2] "temporal aggregation offers temporal robustness by limiting the influence of training data within certain intervals of time"
  - [section 4.2] Theorem 4.2 and 4.3 provide mathematical proof of temporal robustness
  - [corpus] No direct evidence - the neighbor papers focus on different aspects of poisoning defenses.
- Break condition: If the base learners become non-deterministic or if the aggregation size is too small to dilute the poisoned samples' influence.

### Mechanism 3
- Claim: The temporal threat model provides a meaningful sense of protection even with unbounded amounts of poisoned samples by focusing on when attacks occur rather than how many samples are poisoned.
- Mechanism: By shifting the focus from the number of poisoned samples to the temporal characteristics of attacks (earliness and duration), the defense can provide robustness guarantees that are independent of the attack scale.
- Core assumption: The temporal constraints (earliness and duration) are sufficient to bound the attacker's influence on the model's predictions.
- Evidence anchors:
  - [abstract] "providing a meaningful sense of protection even with unbounded amounts of poisoned samples when the attacks are temporally bounded"
  - [section 2.2] Definition 2.7 formalizes temporal robustness against data poisoning
  - [corpus] Weak evidence - the neighbor papers do not discuss temporal aspects of poisoning attacks.
- Break condition: If attackers can conduct attacks that span across multiple temporal boundaries or if the temporal model fails to capture the true nature of the attack.

## Foundational Learning

- Concept: Temporal threat modeling in security
  - Why needed here: Understanding how to incorporate time-based constraints into security threat models is crucial for developing robust defenses against data poisoning attacks.
  - Quick check question: What are the key differences between traditional and temporal threat models in data poisoning?

- Concept: Ensemble methods and aggregation techniques
  - Why needed here: Temporal aggregation relies on training multiple base models and combining their predictions to achieve robustness against poisoning attacks.
  - Quick check question: How does increasing the aggregation size affect the robustness and performance of temporal aggregation?

- Concept: Distribution shifts and their impact on model performance
  - Why needed here: The temporal robustness benchmark uses a dataset with natural distribution shifts over time, which is essential for evaluating the effectiveness of temporal defenses.
  - Quick check question: How do distribution shifts over time affect the performance of base learners and the overall effectiveness of temporal aggregation?

## Architecture Onboarding

- Component map:
  Data collection module with timestamp tracking -> Base learner training module (deterministic) -> Temporal aggregation module -> Robustness evaluation module -> Benchmark simulation module

- Critical path:
  1. Collect data with timestamps
  2. Train base learners on temporal windows
  3. Aggregate predictions from base learners
  4. Evaluate robustness against poisoning attacks
  5. Simulate continuous data collection and periodic updates

- Design tradeoffs:
  - Base coverage (n) vs. temporal robustness: Larger n may improve performance but reduce temporal robustness
  - Aggregation size (k) vs. clean accuracy: Larger k may improve robustness but reduce clean accuracy
  - Deterministic vs. non-deterministic base learners: Deterministic learners are required for provable robustness but may limit model choice

- Failure signatures:
  - Degradation in clean accuracy without corresponding improvement in temporal robustness
  - Inability to detect or mitigate poisoning attacks that span multiple temporal boundaries
  - Performance degradation due to distribution shifts over time

- First 3 experiments:
  1. Evaluate the impact of base coverage (n) on clean accuracy and temporal robustness
  2. Assess the effect of aggregation size (k) on performance and robustness
  3. Test the robustness of temporal aggregation against various poisoning attack scenarios with different earliness and duration constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do temporal robustness guarantees scale when considering continuous, real-world data streams with non-stationary distributions?
- Basis in paper: [explicit] The paper proposes temporal robustness but only evaluates on a fixed historical dataset with periodic updates, not truly continuous streams
- Why unresolved: The temporal robustness proofs assume periodic batch updates rather than continuous data flow, and the benchmark uses historical data with fixed timestamps
- What evidence would resolve it: Empirical validation on live data streams with varying arrival rates and non-stationary distributions, demonstrating that temporal aggregation maintains provable robustness

### Open Question 2
- Question: Can temporal robustness be achieved without relying on timestamps, or when timestamps can be manipulated by attackers?
- Basis in paper: [explicit] The defense assumes timestamps are reliable and cannot be forged by attackers (Assumption 2.4)
- Why unresolved: Many real-world scenarios involve timestamp manipulation (e.g., fake social media posts, data tampering) which would break the defense's core assumption
- What evidence would resolve it: Empirical studies showing temporal aggregation's vulnerability when attackers can modify timestamps, or development of timestamp-independent temporal defenses

### Open Question 3
- Question: What is the computational overhead of temporal aggregation compared to standard training methods, and how does it scale with dataset size?
- Basis in paper: [inferred] The paper focuses on theoretical guarantees but doesn't discuss computational costs of maintaining multiple base models
- Why unresolved: The defense requires training k base models per update period and performing majority voting at inference, which could be prohibitive for large-scale applications
- What evidence would resolve it: Benchmarking temporal aggregation's runtime and memory usage against standard training across varying dataset sizes and model architectures

## Limitations
- The defense critically depends on the reliability of timestamps, which may not hold in many real-world scenarios
- Evaluation is limited to a single dataset with relatively short temporal span, potentially limiting generalizability
- The approach requires training multiple base models, which increases computational overhead compared to standard training

## Confidence

**High Confidence Claims:**
- The temporal threat model (earliness/duration framework) provides a meaningful way to quantify attack timing constraints
- Temporal aggregation with majority voting achieves provable temporal robustness under the stated assumptions
- The defense maintains competitive clean accuracy compared to standard training

**Medium Confidence Claims:**
- The specific robustness thresholds (8-11 months earliness, 6-9 months duration) generalize to other datasets and attack scenarios
- The temporal aggregation configuration (n=9, k=5) represents an optimal balance across all evaluation metrics
- The theoretical bounds accurately predict empirical performance in all scenarios

## Next Checks

1. **Timestamp Manipulation Resistance**: Design experiments where attackers attempt to manipulate timestamps through various means (e.g., date spoofing, time zone manipulation) to assess whether the temporal aggregation defense can detect or mitigate such attacks. Measure the degradation in temporal robustness when timestamp reliability assumptions are violated.

2. **Cross-Dataset Generalization**: Evaluate the temporal aggregation approach on datasets with different temporal characteristics (e.g., longer time spans, different distribution shift patterns, different label dynamics) to assess whether the robustness guarantees hold across domains. Compare performance against the News Category Dataset baseline.

3. **Adaptive Attack Testing**: Implement adaptive poisoning attacks that specifically target the temporal aggregation mechanism by poisoning samples at strategic time points to maximize influence across multiple base learners. Measure whether the provable temporal robustness bounds still hold against sophisticated, adaptive adversaries.