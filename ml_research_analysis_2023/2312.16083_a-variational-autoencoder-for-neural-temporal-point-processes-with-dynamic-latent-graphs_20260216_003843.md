---
ver: rpa2
title: A Variational Autoencoder for Neural Temporal Point Processes with Dynamic
  Latent Graphs
arxiv_id: '2312.16083'
source_url: https://arxiv.org/abs/2312.16083
tags:
- latexit
- sha1
- base64
- event
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel variational auto-encoder (VAE) for
  neural temporal point processes with dynamic latent graphs. The proposed model partitions
  the whole time interval of an input event sequence into a set of sub-intervals,
  and assumes stationary event dynamics within each sub-interval but varying dynamics
  across sub-intervals.
---

# A Variational Autoencoder for Neural Temporal Point Processes with Dynamic Latent Graphs

## Quick Facts
- arXiv ID: 2312.16083
- Source URL: https://arxiv.org/abs/2312.16083
- Authors: 
- Reference count: 4
- Key outcome: Novel VAE for neural temporal point processes with dynamic latent graphs, demonstrating improved prediction accuracy on real-world datasets

## Executive Summary
This paper introduces a variational auto-encoder framework for modeling neural temporal point processes with dynamic latent graphs. The approach partitions event sequences into sub-intervals with stationary dynamics within each, while allowing the dependency graph among event types to evolve across intervals. A sequential latent variable model learns the dynamic graph structure, enabling efficient inference and prediction of future events. Experimental results demonstrate superior performance compared to existing state-of-the-art methods in predicting inter-event times and event types.

## Method Summary
The proposed method uses a VAE framework to learn dynamic dependency graphs among event types in temporal point processes. Event sequences are partitioned into regularly-spaced sub-intervals, with each sub-interval assumed to have stationary dynamics. A graph neural network encodes event embeddings into relation embeddings, which are then used to learn the latent graph structure through forward and backward RNNs. The decoder uses a graph recurrent neural network with log-normal mixture distributions to predict inter-event times, effectively filtering out non-contributing influences based on the learned graph.

## Key Results
- Improved accuracy in predicting inter-event times and event types compared to existing state-of-the-art neural point processes
- Learned dynamic graphs provide interpretable insights into evolving dependencies between event types
- Demonstrated effectiveness across multiple real-world datasets including New York Motor Vehicle Collisions, MathOF, AskUbuntu, and SuperUser

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model captures time-varying dependencies by partitioning the sequence into sub-intervals with stationary dynamics within each, enabling more accurate graph learning per interval.
- Mechanism: By splitting the continuous timeline into K regularly-spaced sub-intervals, the model assumes the latent dependency graph among event types remains fixed within each sub-interval but can evolve across intervals. This reduces the complexity of modeling the full dynamic graph at each event timestamp and allows the encoder to focus on learning the graph structure for each time segment separately.
- Core assumption: Event dynamics are approximately stationary within each sub-interval, and meaningful changes in dependency patterns occur only at sub-interval boundaries.
- Evidence anchors:
  - [abstract] "the whole time interval of the input sequence is partitioned into a set of sub-intervals. The event dynamics are assumed to be stationary within each sub-interval, but could be changing across those sub-intervals."
  - [section] "Given a sequence of events... the whole time-interval of the sequence is partitioned into K regularly-spaced sub-intervals... We assume the latent graph among the event types, is changing over states, but stationary within each sub-interval"
- Break condition: If event dynamics change significantly within a sub-interval, the stationary assumption fails and the learned graph becomes inaccurate.

### Mechanism 2
- Claim: The variational auto-encoder framework enables joint learning of the latent graph structure and efficient inference through the ELBO objective.
- Mechanism: The model defines a prior distribution pϕ(z | S) and a posterior qϕ(z | S) over the latent graph variables {zk(v,u)}. The prior evolves over sub-intervals using a forward RNN, while the encoder uses a backward RNN to incorporate information from future events. The ELBO objective balances reconstruction accuracy with regularization via the KL divergence, allowing gradient-based optimization of both graph structure and inter-event time parameters.
- Core assumption: The posterior distribution qϕ(z | S) can be approximated well by a neural network that conditions on the full event sequence.
- Evidence anchors:
  - [abstract] "We formulate the variational auto-encoder framework, by encoding a latent dynamic graph among event types, from observed sequences."
  - [section] "To learn the model parameters, we calculate the evidence lower bound (ELBO) as... We then sample {zk(v,u)} from the concrete reparameterizable approximation of the posterior distribution."
- Break condition: If the posterior cannot be well-approximated by the chosen neural network architecture, the learned graph will be poor.

### Mechanism 3
- Claim: The log-normal mixture decoder captures the complex inter-event time distribution while the dynamic graph allows removal of non-contributing influences.
- Mechanism: For each event type, the decoder uses a log-normal mixture distribution to model inter-event times. The hidden states {ˆhtiv} evolve through a graph recurrent neural network (GRNN) where messages can only pass through edges with non-zero weights zk(v,u). This allows the model to effectively "filter out" influences from event types that don't contribute to the current prediction, improving accuracy.
- Core assumption: The dependency structure between event types can be effectively captured by a sparse graph where zk(v,u) determines whether u influences v.
- Evidence anchors:
  - [abstract] "The model predicts the future event times, by using the learned dependency graph to remove the non-contributing influences of past events."
  - [section] "These hidden states {ˆhtiv} evolve through a graph recurrent neural network (GRNN), in which the messages can only pass through the non-zero edges hinted by {zk(v,u)}."
- Break condition: If the true dependency structure is not sparse or requires more complex interactions than edge weights can capture, the filtering will be ineffective.

## Foundational Learning

- Concept: Temporal point processes and their conditional intensity functions
  - Why needed here: The entire model builds on temporal point processes as the foundation for modeling asynchronous event sequences and their dependencies.
  - Quick check question: What is the key difference between a standard Hawkes process and a multivariate Hawkes process in terms of the conditional intensity function?

- Concept: Variational auto-encoders and the evidence lower bound (ELBO)
  - Why needed here: The model uses a VAE framework to learn the latent graph structure through the ELBO objective, which balances reconstruction accuracy and regularization.
  - Quick check question: In a VAE, what are the two main terms in the ELBO objective, and what does each term encourage the model to do?

- Concept: Graph neural networks for relational reasoning
  - Why needed here: The encoder uses a GNN to transform event embeddings into relation embeddings between event types, which are then used to learn the dynamic graph structure.
  - Quick check question: How does a graph neural network propagate information between nodes, and what is the typical form of the message passing equation?

## Architecture Onboarding

- Component map: Event sequence -> GNN embeddings -> RNN state evolution -> latent graph sampling -> GRNN hidden states -> inter-event time prediction
- Critical path: Event sequence → GNN embeddings → RNN state evolution → latent graph sampling → GRNN hidden states → inter-event time prediction
- Design tradeoffs:
  - Sub-interval length: Shorter intervals capture more granular dynamics but increase computational cost and parameter count
  - Number of graph edge types: More types allow richer dependency modeling but increase complexity
  - Mixture components in log-normal distribution: More components improve fit but risk overfitting
- Failure signatures:
  - Poor NLL on test set: Could indicate issues with the VAE objective, decoder distribution, or graph learning
  - Graph structure not evolving over time: May suggest the RNN state evolution is not learning meaningful dynamics
  - Unstable training: Could indicate issues with the reparameterization trick or ELBO optimization
- First 3 experiments:
  1. Train on synthetic data with known ground truth graph structure to verify the model can recover the true dependencies
  2. Ablation study: Compare performance with static vs dynamic graphs to quantify the benefit of learning time-varying structures
  3. Sensitivity analysis: Vary sub-interval length and number of graph edge types to find optimal configuration for a given dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed method automatically infer the optimal number and placement of regularly-spaced sub-intervals without requiring manual specification?
- Basis in paper: [explicit] The paper mentions that "an interesting yet challenging direction, is to automatically infer the regularly-spaced time intervals and the corresponding dynamic graph structure, which we leave to future research."
- Why unresolved: The current method requires the number of sub-intervals (K) to be specified a priori, which may not be optimal for all datasets and could limit the model's flexibility.
- What evidence would resolve it: Developing and testing a method that can automatically determine the optimal number and placement of sub-intervals based on the input data would resolve this question.

### Open Question 2
- Question: Can the proposed method be extended to capture non-stationary network dynamics, such as those studied in Yang and Koeppl (2018a,b) and Yang and Zha (2023)?
- Basis in paper: [explicit] The paper states "We plan to generalize the work to capture non-stationary network dynamics Yang and Koeppl (2018b,a, 2020); Yang and Zha (2023) in the future research."
- Why unresolved: The current method assumes stationary dynamics within each sub-interval, which may not capture more complex non-stationary patterns in some datasets.
- What evidence would resolve it: Extending the model to incorporate techniques from non-stationary network dynamics research and demonstrating improved performance on relevant datasets would resolve this question.

### Open Question 3
- Question: How does the performance of the proposed method compare to other state-of-the-art models on datasets with different characteristics, such as varying event densities or periodicity?
- Basis in paper: [inferred] The paper presents results on four real-world datasets but does not extensively explore how the method performs on datasets with different characteristics.
- Why unresolved: While the paper shows improved performance on the tested datasets, it is unclear how the method would perform on datasets with different event densities, periodicity, or other characteristics.
- What evidence would resolve it: Conducting extensive experiments on a wide range of datasets with varying characteristics and comparing the proposed method's performance to other state-of-the-art models would resolve this question.

## Limitations
- The assumption of stationary dynamics within sub-intervals may not hold for all real-world datasets
- The complexity of the model with multiple neural network components increases the risk of overfitting
- The evaluation is limited to a small number of datasets without extensive ablation studies

## Confidence
- **High confidence**: The core VAE framework and ELBO optimization procedure
- **Medium confidence**: The effectiveness of the sub-interval partitioning strategy and dynamic graph learning
- **Medium confidence**: The log-normal mixture decoder's ability to capture complex inter-event time distributions

## Next Checks
1. **Synthetic data evaluation**: Generate synthetic event sequences with known ground truth dynamic graphs and evaluate the model's ability to recover the true dependency structure across sub-intervals.

2. **Robustness to sub-interval length**: Systematically vary the sub-interval length and evaluate its impact on predictive performance and learned graph structures across multiple datasets.

3. **Ablation study**: Compare the full model against variants that remove the dynamic graph component, use a single log-normal distribution instead of mixture, or employ different neural network architectures for the encoder and decoder.