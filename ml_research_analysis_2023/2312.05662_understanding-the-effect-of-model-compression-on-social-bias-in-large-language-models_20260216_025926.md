---
ver: rpa2
title: Understanding the Effect of Model Compression on Social Bias in Large Language
  Models
arxiv_id: '2312.05662'
source_url: https://arxiv.org/abs/2312.05662
tags:
- bias
- social
- bert
- score
- roberta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work studies the effect of compression methods on social\
  \ bias in large language models. It shows that longer pretraining and larger models\
  \ increase social bias, while quantization acts as a regularizer that reduces bias\u2014\
  particularly around 20% of the original pretraining duration."
---

# Understanding the Effect of Model Compression on Social Bias in Large Language Models

## Quick Facts
- **arXiv ID**: 2312.05662
- **Source URL**: https://arxiv.org/abs/2312.05662
- **Reference count**: 24
- **Key outcome**: Model compression techniques like quantization and knowledge distillation can reduce social bias in large language models, with quantization providing the best tradeoff between bias reduction and language modeling performance.

## Executive Summary
This study examines how model compression techniques affect social bias in large language models (LLMs). The research reveals that longer pretraining and larger model sizes increase social bias, while compression methods—particularly quantization—act as regularizers that reduce bias. Dynamic Post-Training Quantization (PTQ) to int8 precision shows the most promising tradeoff, reducing bias while preserving language modeling performance, whereas knowledge distillation provides stronger bias reduction but at a significant cost to LM performance.

## Method Summary
The paper evaluates BERT, RoBERTa, and Pythia models of varying sizes and pretraining steps using the Bias Bench benchmark datasets (CrowS-Pairs, StereoSet, SEAT). Two compression methods are applied: dynamic Post-Training Quantization (PTQ) to int8 precision and knowledge distillation (DistilBERT, DistilRoBERTa). Models are assessed for social bias across GENDER, RACE, and RELIGION categories, along with language modeling performance on StereoSet. The study systematically examines how compression affects bias across different model families and sizes.

## Key Results
- Longer pretraining and larger models lead to higher social bias across all measured categories
- Quantization acts as a regularizer, showing optimal bias reduction tradeoff around 20% of original pretraining duration
- Knowledge distillation reduces bias but significantly degrades language modeling performance
- Dynamic PTQ provides better bias reduction while preserving LM scores compared to knowledge distillation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Longer pretraining and larger models increase social bias
- **Mechanism**: As models are trained longer and with more parameters, they fit more closely to the biases present in the training data, amplifying stereotypical patterns
- **Core assumption**: Pretraining data contains social biases that models learn to replicate
- **Evidence anchors**:
  - [abstract] "Longer pretraining and larger models led to higher social bias"
  - [section] "as the model size increases so does the LM model score and social bias"
- **Break condition**: If pretraining data is carefully curated to remove biases, this mechanism would not apply

### Mechanism 2
- **Claim**: Quantization acts as a regularizer that reduces social bias
- **Mechanism**: Reducing precision during quantization forces the model to compress information, which can smooth out fine-grained biased patterns while preserving general language capabilities
- **Core assumption**: Lower precision representations lose some of the subtle biased associations present in full-precision weights
- **Evidence anchors**:
  - [abstract] "quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time"
  - [section] "The application of dynamic PTQ shows a regularizer effect on all models"
- **Break condition**: If quantization is too aggressive, it may degrade language modeling performance beyond acceptable thresholds

### Mechanism 3
- **Claim**: Knowledge distillation reduces social bias at the cost of language modeling performance
- **Mechanism**: Training a smaller student model to mimic a larger teacher model can inherit the teacher's knowledge while the compression process filters out some fine-grained biases
- **Core assumption**: The student model cannot perfectly replicate all aspects of the teacher, leading to selective retention of knowledge
- **Evidence anchors**:
  - [abstract] "knowledge distillation also reduces bias but at the cost of language modeling performance"
  - [section] "dynamic PTQ shows a better trade-off in providing social bias reductions, while preserving LM score"
- **Break condition**: If the distillation process is too faithful to the teacher, biases may be preserved

## Foundational Learning

- **Concept**: Social bias measurement in language models
  - **Why needed here**: The paper evaluates multiple datasets (CrowS-Pairs, StereoSet, SEAT) to quantify bias across different categories
  - **Quick check question**: What are the three main identity categories measured in this study? (GENDER, RACE, RELIGION)

- **Concept**: Model compression techniques (quantization and distillation)
  - **Why needed here**: The study specifically examines how these two compression methods affect social bias in LLMs
  - **Quick check question**: What are the two model compression methods studied in this paper? (Quantization, Knowledge Distillation)

- **Concept**: Pretraining dynamics and model scaling
  - **Why needed here**: The paper investigates how longer pretraining and larger model sizes correlate with increased bias
  - **Quick check question**: According to the results, what relationship exists between model size and social bias? (Larger models show higher bias)

## Architecture Onboarding

- **Component map**: Pretrained models (BERT, RoBERTa, Pythia) → Bias evaluation → Compression methods (quantization, distillation) → Re-evaluation

- **Critical path**: 
  1. Load pretrained model
  2. Apply bias evaluation on original model
  3. Apply compression (quantization or distillation)
  4. Re-evaluate bias on compressed model
  5. Compare results

- **Design tradeoffs**:
  - Quantization offers better bias reduction while preserving language modeling performance
  - Distillation provides stronger bias reduction but at significant cost to language modeling
  - Larger models show higher bias but also better language modeling scores

- **Failure signatures**:
  - If quantization is too aggressive, LM scores drop significantly
  - If distillation is too faithful, bias reduction may be minimal
  - If evaluation metrics are misinterpreted, bias trends may appear reversed

- **First 3 experiments**:
  1. Apply dynamic PTQ to BERT Base and measure bias reduction across all three datasets
  2. Compare bias levels between BERT Base and BERT Large on CrowS-Pairs
  3. Apply knowledge distillation to RoBERTa Base and measure the tradeoff between bias reduction and LM score degradation

## Open Questions the Paper Calls Out
- Does the reduction in social bias through model compression persist across different downstream tasks beyond those evaluated in this study?
- How does model compression interact with different pretraining data distributions and their associated biases?
- What are the long-term stability and evolution of bias reductions achieved through compression during model fine-tuning and deployment?

## Limitations
- The exact mechanism by which reduced precision smooths out biased associations remains theoretically underspecified
- The study relies on three bias benchmark datasets but doesn't address potential limitations in these evaluation frameworks
- The generalizability of the optimal 20% quantization timing across different model architectures and domains remains unclear

## Confidence
- **High confidence**: The core finding that larger models and longer pretraining increase social bias, as this aligns with established scaling laws and is supported by consistent results across multiple model families
- **Medium confidence**: The claim that quantization acts as a regularizer for bias reduction, as the empirical evidence is strong but the theoretical mechanism requires further investigation
- **Medium confidence**: The assertion that knowledge distillation reduces bias at the cost of language modeling performance, as the tradeoff is demonstrated but the specific performance degradation varies by model family

## Next Checks
1. Conduct ablation studies varying quantization precision levels systematically (int4, int8, int16) to isolate the specific precision threshold where bias regularization effects begin and end
2. Evaluate compressed models on additional bias detection frameworks (e.g., BOLD, StereoSet MQM, or domain-specific benchmarks) to verify that observed bias reductions generalize beyond the Bias Bench suite
3. Test compressed models across multiple inference time points to assess whether bias reductions persist under varying computational loads and whether quantization-induced smoothing effects remain stable during extended deployment scenarios