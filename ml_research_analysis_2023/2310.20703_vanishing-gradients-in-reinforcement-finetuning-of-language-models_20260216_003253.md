---
ver: rpa2
title: Vanishing Gradients in Reinforcement Finetuning of Language Models
arxiv_id: '2310.20703'
source_url: https://arxiv.org/abs/2310.20703
tags:
- reward
- standard
- deviation
- small
- inputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies a fundamental optimization obstacle in reinforcement
  finetuning (RFT) of language models: the expected gradient for an input vanishes
  when its reward standard deviation under the model is small, even if the expected
  reward is far from optimal. Through experiments on an RFT benchmark and controlled
  environments, as well as a theoretical analysis, we demonstrate that vanishing gradients
  due to small reward standard deviation are prevalent and detrimental, leading to
  extremely slow reward maximization.'
---

# Vanishing Gradients in Reinforcement Finetuning of Language Models

## Quick Facts
- arXiv ID: 2310.20703
- Source URL: https://arxiv.org/abs/2310.20703
- Reference count: 40
- Primary result: Vanishing gradients occur in RFT when reward standard deviation is small, even if expected reward is far from optimal, and initial SFT phase can effectively mitigate this problem

## Executive Summary
This work identifies a fundamental optimization obstacle in reinforcement finetuning (RFT) of language models: the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.

## Method Summary
The paper uses Proximal Policy Optimization (PPO) for reinforcement finetuning and Adam optimizer for both RFT and supervised finetuning (SFT). Experiments are conducted on the GRUE benchmark with seven datasets (NarrativeQA, ToTTo, CommonGen, IWSLT 2017, CNN/Daily Mail, DailyDialog, and IMDB) as well as controlled environments using MNIST, CIFAR10, and STS-B datasets. The study measures reward achieved by RFT and SFT models, along with reward standard deviation of individual input samples. The authors explore the effect of increasing learning rate, applying temperature to logits, and entropy regularization as potential mitigation strategies for vanishing gradients.

## Key Results
- The expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal
- Vanishing gradients are prevalent in RFT and lead to extremely slow reward maximization
- An initial SFT phase can effectively mitigate vanishing gradients, with as few as 1% of samples requiring only a small number of optimization steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal.
- Mechanism: Small reward standard deviation causes the softmax operator to produce near-deterministic output distributions, which in turn causes the gradient contributions from those inputs to vanish during optimization.
- Core assumption: The vanishing gradient phenomenon is independent of the expected reward value - it depends solely on the reward standard deviation.
- Evidence anchors:
  - [abstract] "we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal"
  - [section] "The gradient ∇θV (x; θ) vanishes, when the reward standard deviation of x is small, due to a combination of the reward maximization objective and the use of softmax for producing distributions over output tokens"
  - [corpus] Weak - corpus neighbors focus on general RL finetuning but don't specifically address reward standard deviation gradient vanishing
- Break condition: If the reward standard deviation increases above a threshold where softmax produces sufficiently diverse distributions, the vanishing gradient effect will diminish.

### Mechanism 2
- Claim: PPO inherits the vanishing gradient problem from expected reward gradients when reward standard deviation is small.
- Mechanism: PPO's surrogate objective still depends on the probability ratio between current and reference policies, and when reward standard deviation is small, these ratios remain close to 1, causing gradient contributions to vanish.
- Core assumption: The PPO clipping mechanism doesn't fully address the vanishing gradient issue when reward standard deviation is small.
- Evidence anchors:
  - [abstract] "the same holds for PPO, and stems from a combination of the reward maximization objective and the softmax operator"
  - [section] "Proposition 1 establishes that, if the reward standard deviation for an input is small, then its expected gradient under PPO vanishes"
  - [corpus] Missing - corpus neighbors don't discuss PPO's specific behavior with respect to reward standard deviation
- Break condition: If the PPO clipping parameter δ is increased significantly or if KL regularization is used with appropriate coefficient, the effect might be mitigated.

### Mechanism 3
- Claim: Initial SFT phase reduces the number of inputs with small reward standard deviation, thereby alleviating vanishing gradients for subsequent RFT.
- Mechanism: SFT provides labeled data that helps the model learn to produce diverse outputs with larger reward standard deviation, which prevents the vanishing gradient problem during RFT.
- Core assumption: The benefits of SFT stem partially from reducing inputs with small reward standard deviation rather than just improving overall model performance.
- Evidence anchors:
  - [abstract] "we find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline"
  - [section] "a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice"
  - [corpus] Weak - corpus neighbors discuss curriculum learning and other RFT improvements but don't specifically address SFT's role in reward standard deviation
- Break condition: If SFT is performed on all inputs rather than a subset, the benefit might plateau or become computationally inefficient.

## Foundational Learning

- Concept: Policy gradient algorithms and their optimization landscape
  - Why needed here: Understanding how policy gradients work is crucial for grasping why small reward standard deviation causes vanishing gradients
  - Quick check question: Why does the softmax operator combined with reward maximization lead to vanishing gradients when reward standard deviation is small?

- Concept: Supervised vs reinforcement learning objectives
  - Why needed here: The key insight is that SFT doesn't suffer from the same vanishing gradient problem as RFT
  - Quick check question: What's the fundamental difference between the SFT cross-entropy loss and RFT reward maximization that makes them behave differently with respect to gradient vanishing?

- Concept: Exploration-exploitation tradeoff in RL
  - Why needed here: While exploration challenges exist in RFT, the paper shows that even with perfect exploration, vanishing gradients due to small reward standard deviation persist
  - Quick check question: How does the presence of inputs with small reward standard deviation create a different type of optimization challenge compared to exploration difficulties?

## Architecture Onboarding

- Component map: Pretrained model → Compute reward standard deviation per input → Identify inputs with vanishing gradients → Apply SFT or alternative mitigation strategies → Execute RFT
- Critical path: Pretrained model → Compute reward standard deviation per input → Identify inputs with vanishing gradients → Apply SFT or alternative mitigation strategies → Execute RFT
- Design tradeoffs: Full SFT vs partial SFT (fewer steps, fewer samples) vs alternative gradient modifications; computational cost vs effectiveness
- Failure signatures: Poor RFT performance despite good SFT results; high correlation between pretrain reward standard deviation and inability to improve rewards; slow optimization convergence
- First 3 experiments:
  1. Compute reward standard deviation for all training inputs under the pretrained model to identify vanishing gradient inputs
  2. Compare RFT vs SFT performance on inputs with small vs large pretrain reward standard deviation
  3. Test partial SFT (e.g., 40% steps, 1% samples) followed by RFT to verify if fewer resources still achieve good results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do vanishing gradients in reinforcement finetuning (RFT) manifest in larger language models, such as GPT-4 or Llama2, compared to smaller models like GPT-2 or T5-base?
- Basis in paper: [inferred] The paper acknowledges the limitation of not incorporating reward functions learned from human feedback and not considering larger models, suggesting this as an exciting next step.
- Why unresolved: The paper only experiments with GPT-2 and T5-base, which may not capture the full extent of the vanishing gradients phenomenon in more complex models.
- What evidence would resolve it: Conducting RFT experiments on larger language models with learned reward functions from human feedback would provide insights into the prevalence and impact of vanishing gradients in these more complex settings.

### Open Question 2
- Question: What is the optimal balance between the number of supervised finetuning (SFT) steps and the number of labeled inputs to effectively mitigate vanishing gradients in RFT?
- Basis in paper: [explicit] The paper demonstrates that a relatively small number of SFT steps on as few as 1% of the input samples can suffice to improve RFT performance.
- Why unresolved: While the paper shows the effectiveness of partial SFT, it does not provide a precise formula or guidelines for determining the optimal balance between SFT steps and labeled inputs.
- What evidence would resolve it: Systematic experiments varying the number of SFT steps and labeled inputs across different datasets and model sizes could identify the optimal trade-off for mitigating vanishing gradients in RFT.

### Open Question 3
- Question: How do different policy gradient algorithms, such as TRPO or SAC, compare to PPO in terms of susceptibility to vanishing gradients in RFT?
- Basis in paper: [inferred] The paper focuses on PPO as the most widely used policy gradient algorithm for RFT but acknowledges that other variants, like the KL regularized PPO, are also used.
- Why unresolved: The paper does not explore how other policy gradient algorithms might be affected by vanishing gradients or if they offer any advantages in this regard.
- What evidence would resolve it: Comparative experiments using different policy gradient algorithms on the same datasets and tasks as in the paper would reveal their relative susceptibility to vanishing gradients and potential benefits.

## Limitations
- The experiments primarily use relatively small models (T5-3B) and may not fully generalize to frontier models where other optimization challenges dominate
- The proposed solution - initial SFT phase - may not be universally applicable when high-quality supervised data is unavailable or prohibitively expensive to obtain
- The controlled environments simplify the complex distribution shifts present in real-world RFT scenarios

## Confidence

- Expected gradient vanishing mechanism: High confidence - The theoretical analysis proving gradient vanishing when reward standard deviation is small is mathematically rigorous and the experimental evidence strongly supports this claim across multiple datasets and model architectures.
- PPO inherits the vanishing gradient problem: Medium confidence - While the theoretical extension to PPO is sound, the practical severity of this issue in PPO implementations may be mitigated by other factors like the clipping mechanism and KL regularization that weren't fully isolated in experiments.
- SFT as the primary solution: Medium confidence - Experiments convincingly show SFT improves RFT outcomes, but the attribution of this improvement specifically to reduced vanishing gradients (rather than general better initialization or representation learning) could be more thoroughly validated.

## Next Checks

1. Test whether the vanishing gradient phenomenon persists at scale by running the same experiments with frontier models (e.g., 70B+ parameters) on the GRUE benchmark.

2. Design an ablation study that isolates the effect of reward standard deviation from other SFT benefits by comparing SFT on high-variance vs low-variance samples only.

3. Evaluate whether alternative reward shaping techniques (e.g., reward scaling, reward normalization) can achieve similar benefits to SFT in reducing vanishing gradients without requiring supervised data.