---
ver: rpa2
title: 'EHRTutor: Enhancing Patient Understanding of Discharge Instructions'
arxiv_id: '2310.19212'
source_url: https://arxiv.org/abs/2310.19212
tags:
- patient
- question
- discharge
- questions
- ehrtutor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EHRTutor, a multi-component framework that
  leverages large language models (LLMs) to educate patients about their discharge
  instructions through conversational question-answering. EHRTutor generates questions
  based on discharge instructions, interacts with patients by administering each question
  as a test, and provides a summary at the end of the conversation.
---

# EHRTutor: Enhancing Patient Understanding of Discharge Instructions

## Quick Facts
- arXiv ID: 2310.19212
- Source URL: https://arxiv.org/abs/2310.19212
- Reference count: 36
- Key outcome: EHRTutor outperforms GPT-4 with simple prompting in generating high-quality patient education conversations

## Executive Summary
This paper introduces EHRTutor, a multi-component framework leveraging large language models (LLMs) to educate patients about their discharge instructions through conversational question-answering. The system generates questions based on discharge instructions, interacts with patients by administering each question as a test, and provides a summary at the end of the conversation. Human and LLM evaluations demonstrate that EHRTutor outperforms GPT-4 with simple prompting, generating high-quality patient education conversations. The framework also offers potential for generating synthetic patient education dialogues for future in-house system training.

## Method Summary
EHRTutor is a multi-component framework designed to educate patients about their discharge instructions through conversational question-answering. The system consists of six key components: a keypoint chain, question chain, verification chain, response chain, agent, and summary chain. It uses GPT-4 for generating questions based on template-based instruction prompting, verifies question quality, interacts with patients using a ReAct framework, and provides conversation summaries. The framework was evaluated using discharge instructions from the MIMIC-III database, with human evaluation by medical practitioners and LLM evaluation using GPT-4 and Claude-2.

## Key Results
- EHRTutor outperforms GPT-4 with simple prompting in generating high-quality patient education conversations
- The template-based approach with verification improves question quality over zero-shot prompting
- The ReAct framework enables effective interactive conversation with patients

## Why This Works (Mechanism)

### Mechanism 1
The multi-component architecture improves question quality over zero-shot prompting through structured template-based generation with verification. EHRTutor uses question templates formulated by domain experts covering four categories (test, medication, complications & progress, follow-up) that constrain the LLM to generate clinically relevant questions. The verification chain checks whether generated questions can be answered using the discharge instruction, reducing irrelevant or unverifiable questions.

### Mechanism 2
The ReAct-based agent improves patient education through interactive conversation by reasoning about patient responses and deciding whether to provide hints or ask next questions. The agent can assess whether patient answers are correct, partially correct, or incorrect, and adapt its responses accordingly. This creates a more adaptive and engaging educational experience compared to direct answer provision.

### Mechanism 3
The summary chain reinforces learning by highlighting key points and areas of confusion. After the conversation, it generates a summary containing key self-care points and content the patient did not understand well, providing feedback to both patient and doctor. This summarization process helps reinforce learning and identify knowledge gaps.

## Foundational Learning

- **Large Language Models (LLMs)**: AI models capable of understanding and generating human language, used here for question generation, conversation, and summarization
  - Why needed: EHRTutor leverages LLMs as the core technology for all natural language processing tasks
  - Quick check: What are the key differences between GPT-3.5 and GPT-4, and how do they impact EHRTutor's performance?

- **Electronic Health Records (EHRs) and discharge instructions**: Digital records containing patient treatment information, including specific instructions for post-discharge care
  - Why needed: EHRTutor is designed specifically to help patients understand their discharge instructions, which are a critical component of EHRs
  - Quick check: What are the common challenges patients face when trying to understand discharge instructions?

- **Conversational AI and question-answering systems**: AI systems designed to engage in natural language dialogue and answer questions
  - Why needed: EHRTutor is fundamentally a conversational question-answering system for patient education
  - Quick check: How does EHRTutor's approach differ from traditional question-answering systems in healthcare?

## Architecture Onboarding

- **Component map**: keypoint chain → question chain → verification chain → agent → response chain → summary chain
- **Critical path**: Question generation (keypoint → question → verification) → conversation (agent → response) → summarization (summary)
- **Design tradeoffs**: Template-based approach improves question quality but may limit question diversity; ReAct framework enables interactive conversation but requires careful reasoning design
- **Failure signatures**: Poor question quality (irrelevant or unverifiable questions), ineffective conversation (incorrect hints or irrelevant follow-ups), inaccurate summaries
- **First 3 experiments**:
  1. Evaluate question quality generated by EHRTutor vs GPT-4 with simple prompting using human evaluation
  2. Test ReAct agent effectiveness in providing hints and guiding conversation using simulated patient responses
  3. Assess summary quality in capturing key points and areas of confusion using human evaluation

## Open Questions the Paper Calls Out

### Open Question 1
How does EHRTutor handle patients who are non-responsive or provide consistently incorrect answers during the conversation? The paper describes the decision-making process but doesn't explicitly address scenarios where patients struggle to provide correct answers or engage with the system. Empirical data showing performance with various patient response patterns would clarify how the system adapts to challenging interactions.

### Open Question 2
What is the impact of EHRTutor's patient education on actual patient outcomes post-discharge? While the paper evaluates conversation and summary quality, it doesn't provide data on how this education translates to improved patient understanding or adherence to treatment plans. Clinical studies measuring comprehension, adherence, and health outcomes would demonstrate practical impact.

### Open Question 3
How does EHRTutor's performance compare to human-led patient education in terms of patient satisfaction and understanding? The paper compares EHRTutor to GPT-4 but lacks comparison with traditional human-led education methods. Studies comparing patient satisfaction and understanding between EHRTutor and human educators would provide insight into relative effectiveness.

## Limitations
- Evaluation relies on simulated patient responses and proxy metrics rather than real patient testing or clinical outcomes
- Template-based approach may limit question diversity and adaptability across different medical scenarios
- Performance depends heavily on quality of question templates, which may not capture all clinically relevant scenarios

## Confidence

- **High confidence**: The multi-component architecture improves question quality over zero-shot prompting
- **Medium confidence**: The ReAct-based agent improves patient education through interactive conversation
- **Medium confidence**: The summary chain reinforces learning by highlighting key points and areas of confusion

## Next Checks

1. Conduct a randomized controlled trial with actual patients to measure knowledge retention and comprehension compared to standard discharge processes
2. Test the system across a broader range of discharge instructions from different medical specialties to evaluate template generalizability
3. Implement A/B testing comparing EHRTutor's performance against other patient education methods (e.g., video instruction, traditional nurse education) in real clinical settings