---
ver: rpa2
title: 'SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data'
arxiv_id: '2307.15870'
source_url: https://arxiv.org/abs/2307.15870
tags:
- uni00000013
- data
- training
- uni00000011
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Pseudo-Clustering Semi-SFL, a novel semi-supervised
  split federated learning system that addresses the challenges of unlabeled and non-IID
  data. The key idea is to incorporate clustering regularization to leverage the strengths
  of multiple clients by directing smashed data towards shared pseudo clusters.
---

# SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data

## Quick Facts
- arXiv ID: 2307.15870
- Source URL: https://arxiv.org/abs/2307.15870
- Reference count: 40
- Key outcome: 3.8x training speedup, 70.3% communication cost reduction, up to 5.8% accuracy improvement on non-IID data

## Executive Summary
This paper introduces Pseudo-Clustering Semi-SFL, a novel semi-supervised split federated learning system designed to address the challenges of unlabeled and non-IID data. The approach combines split learning with clustering regularization to leverage unlabeled data across multiple clients while mitigating the negative impacts of data heterogeneity. By directing smashed data toward shared pseudo clusters using contrastive loss, the system effectively utilizes unlabeled data and reduces communication costs compared to existing methods.

## Method Summary
The proposed system implements split federated learning where the model is divided into client-side bottom and server-side top components. Training alternates between supervised learning on labeled server data and split forward/backward propagation on unlabeled client data. The key innovation is clustering regularization that collects smashed data from all clients, filters by confidence threshold, and applies contrastive loss to align student and teacher features within shared pseudo clusters. An adaptive algorithm dynamically adjusts the global updating frequency based on supervised loss changes to balance training stages and maintain convergence stability.

## Key Results
- Achieves 3.8x speed-up in training time compared to state-of-the-art baselines
- Reduces communication cost by approximately 70.3%
- Improves accuracy by up to 5.8% under non-IID scenarios
- Demonstrates effectiveness across three benchmark datasets: SVHN, CIFAR-10, and IMAGE-100

## Why This Works (Mechanism)

### Mechanism 1
Clustering regularization mitigates non-IID data degradation by aligning client-side model predictions in a shared latent space. The method collects smashed data from all clients, filters by confidence, and uses contrastive loss to pull student features toward teacher features within the same pseudo cluster, reducing client-level skew. Core assumption: Smashed data from different clients contains shared semantic structure exploitable via contrastive alignment. Break condition: If pseudo-label confidence falls below threshold across all clients, clustering loss becomes ineffective.

### Mechanism 2
Global updating frequency adaptation balances supervised and split training stages to stabilize convergence under alternating optimization. The algorithm estimates supervised loss change every round and scales global steps up or down at predefined milestones based on thresholds. Core assumption: Supervised loss dynamics reflect relative need for labeled vs. unlabeled data training at each stage. Break condition: If thresholds are set too loosely, the system may oscillate and destabilize training.

### Mechanism 3
EMA-based teacher model provides stable pseudo-labels that reduce confirmation bias during contrastive regularization. Teacher parameters are updated as a moving average of global model parameters, used to generate both pseudo-labels and teacher features for clustering loss. Core assumption: EMA smoothing yields higher-quality pseudo-labels than single-step predictions. Break condition: If EMA decay is too high, teacher model lags too far behind and provides outdated pseudo-labels.

## Foundational Learning

- Concept: Split Learning (SL) framework and smashed data concept.
  - Why needed here: SemiSFL builds directly on SFL's split architecture; understanding smashed data flow is critical to implementing clustering regularization.
  - Quick check question: In SFL, what is transmitted from client to server during forward propagation?

- Concept: Semi-supervised learning with pseudo-labels and consistency regularization.
  - Why needed here: The method relies on generating and using pseudo-labels from unlabeled data, then enforcing consistency between augmented views.
  - Quick check question: What role does the confidence threshold τ play in filtering pseudo-labels?

- Concept: Contrastive loss and clustering in representation learning.
  - Why needed here: Clustering regularization is implemented via a multi-client contrastive loss that aligns student and teacher features in the same pseudo cluster.
  - Quick check question: How does the contrastive loss in clustering regularization differ from standard supervised contrastive loss?

## Architecture Onboarding

- Component map: Server (parameter server, top model ws, projection head wp, EMA models ˜ws/˜wp, memory queue Q, loss computation) -> Client (bottom model wc, local unlabeled data, smashed data generation, gradient computation) -> Communication (smashed data and model updates)

- Critical path: 1) Server performs supervised training steps on labeled data, 2) Server broadcasts bottom model to clients, 3) For split steps: clients forward propagate, send smashed data, server computes clustering loss, sends gradients back, clients update, 4) Server aggregates client bottom models, 5) Repeat

- Design tradeoffs: Split layer selection affects communication/computation balance; global vs. local updating frequency impacts convergence; confidence threshold controls cluster quality vs. coverage

- Failure signatures: Accuracy plateaus early (high τ filtering too many samples), loss divergence during split stage (Ks/Ku imbalance or poor EMA quality), communication bottleneck (smashed data size too large)

- First 3 experiments: 1) Baseline run with FedEMA (no clustering) to establish communication vs. accuracy baseline, 2) Vary τ (e.g., 0.8, 0.9, 0.95) to observe impact on clustering regularization quality, 3) Sweep Ks/Ku ratio (e.g., 10/50, 20/50, 50/50) to find optimal balance for convergence stability

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal global updating frequency (Ks) for balancing convergence speed and model performance in Pseudo-Clustering Semi-SFL? The paper discusses the impact of global updating frequency on model convergence and performance but does not provide a definitive answer, as it depends on various factors such as the dataset, model architecture, and data distribution. Conducting extensive experiments with different Ks values on various datasets and models could help determine the optimal Ks for different scenarios.

### Open Question 2
How does the proposed clustering regularization technique perform compared to other semi-supervised learning methods in terms of accuracy and training efficiency? While the paper compares Pseudo-Clustering Semi-SFL with several baselines and shows higher accuracy and lower communication cost, it does not compare with all existing semi-supervised learning methods. Conducting experiments comparing with a wide range of state-of-the-art semi-supervised learning methods on various datasets and models would provide a comprehensive evaluation.

### Open Question 3
How does the proposed system handle scenarios with a large number of clients and highly imbalanced data distributions? The paper mentions that data non-IIDness and the number of clients can affect model performance, and the proposed clustering regularization aims to mitigate these issues, but it does not extensively explore scenarios with a large number of clients or highly imbalanced data distributions. Conducting experiments with these challenging scenarios would help evaluate the scalability and robustness of the proposed system.

## Limitations
- The clustering regularization mechanism may break down when client data distributions are extremely divergent
- The relationship between supervised loss dynamics and training stage needs may not hold across all dataset/task combinations
- Limited ablation studies provided to quantify individual component contributions to accuracy improvements

## Confidence

High: Overall system architecture and communication cost reduction (well-specified and demonstrated)
Medium: Accuracy improvements on non-IID data (results show improvement but underlying mechanism needs more rigorous validation)
Low: Clustering regularization's robustness across diverse non-IID scenarios (limited ablation studies provided)

## Next Checks

1. **Robustness Test**: Run SemiSFL with increasingly severe non-IID partitions (e.g., one client per class) to identify the breaking point of the clustering regularization mechanism.

2. **Ablation Study**: Systematically disable components (contrastive loss, EMA teacher, dynamic frequency adaptation) to quantify each contribution to the reported accuracy improvements.

3. **Communication Analysis**: Measure actual network throughput and latency during smashed data transmission to validate the claimed 70.3% communication cost reduction under realistic bandwidth constraints.