---
ver: rpa2
title: 'RoME: A Robust Mixed-Effects Bandit Algorithm for Optimizing Mobile Health
  Interventions'
arxiv_id: '2312.06403'
source_url: https://arxiv.org/abs/2312.06403
tags:
- reward
- time
- algorithm
- baseline
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel contextual bandit algorithm, RoME, designed
  to optimize mobile health interventions. RoME addresses key challenges in mHealth
  by (1) modeling differential rewards with user- and time-specific random effects,
  (2) incorporating network cohesion penalties, and (3) employing debiased machine
  learning for flexible estimation of baseline rewards.
---

# RoME: A Robust Mixed-Effects Bandit Algorithm for Optimizing Mobile Health Interventions

## Quick Facts
- **arXiv ID**: 2312.06403
- **Source URL**: https://arxiv.org/abs/2312.06403
- **Reference count**: 40
- **Key outcome**: RoME achieves robust regret bounds that depend solely on the differential-reward model dimension, enabling strong performance even with complex baseline rewards in mobile health interventions.

## Executive Summary
This paper proposes RoME, a novel contextual bandit algorithm designed to optimize mobile health interventions. RoME addresses key challenges in mHealth by modeling differential rewards with user- and time-specific random effects, incorporating network cohesion penalties, and employing debiased machine learning for flexible estimation of baseline rewards. The algorithm achieves robust regret bounds that depend solely on the dimension of the differential-reward model, enabling strong performance even with complex baseline rewards. Empirical results demonstrate RoME's superior performance in simulation and two off-policy evaluation studies compared to existing methods.

## Method Summary
RoME combines three key innovations: (1) nearest-neighbor regularization (NNR) that pools information across similar individuals and time points, (2) Double Machine Learning (DML) debiasing that separates estimation of baseline rewards from differential rewards, and (3) Thompson sampling that balances exploration and exploitation. The algorithm constructs pseudo-rewards using inverse propensity weighting and baseline modeling, then performs weighted ridge regression with Laplacian regularization to estimate differential reward parameters. These estimates are used in Thompson sampling to select actions that maximize expected rewards.

## Key Results
- RoME achieves regret bounds that depend only on the dimension of the differential reward model, not the overall model complexity
- Superior performance demonstrated in synthetic simulations with both linear and nonlinear baseline rewards
- Outperforms existing methods in two off-policy evaluation studies on real mobile health datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double Machine Learning (DML) debiases the estimation of differential rewards by separating the estimation of the nuisance parameter (baseline reward) from the parameter of interest.
- Mechanism: DML uses sample splitting or stable estimators to prevent overfitting of the baseline reward model, which reduces the variance of the pseudo-reward used in Thompson sampling.
- Core assumption: The errors in the reward model are i.i.d. and the nuisance parameter (baseline reward) can be estimated consistently at a rate of oP(k^{-1/2}).
- Evidence anchors:
  - [abstract]: "leverages (1) nearest-neighbors to efficiently pool information on the differential reward function across users and time and (2) the Double Machine Learning (DML) framework to explicitly model baseline rewards and stay agnostic to the supervised learning algorithms used."
  - [section 3.1]: "Step 2: For each fold at each time t, use any supervised learning algorithm to estimate the working model for ri,t(s, a) denoted ˆf(m)i,t (s, a) using I ∁m."
- Break condition: If the errors are not i.i.d. or the nuisance parameter is not consistently estimable at the required rate, the debiasing will fail and the regret bound will not improve.

### Mechanism 2
- Claim: Network cohesion penalties improve estimation efficiency by pooling information across similar individuals and time points.
- Mechanism: The algorithm constructs a graph based on similarity (e.g., L2 distance) and applies a Laplacian penalty to encourage similar parameters for connected nodes.
- Core assumption: There exists a network structure (either known or estimable) that captures meaningful similarity between individuals and/or time points.
- Evidence anchors:
  - [abstract]: "incorporating network cohesion penalties"
  - [section 3.2]: "Define a graph G = (V, E) where each user represents a node, e.g., V := [N], and (i, j) is in the edge set E for the smallest M ≪ N distances."
- Break condition: If the network structure does not reflect true similarity (e.g., random or adversarial connections), the penalty will introduce bias rather than reduce variance.

### Mechanism 3
- Claim: The pseudo-reward construction via inverse propensity weighting and baseline modeling achieves double robustness, ensuring consistent estimation even if one component is misspecified.
- Mechanism: The pseudo-reward is unbiased if either the propensity weights or the baseline reward model is correctly specified.
- Core assumption: Either the propensity weights or the baseline reward model is correctly specified.
- Evidence anchors:
  - [section 3.1]: "Equation (3) presents a Doubly Robust estimator for the Differential Reward; i.e., if either πi,t or fi,t are correctly specified, (3) is a consistent estimator of the differential reward"
- Break condition: If both the propensity weights and the baseline reward model are misspecified, the pseudo-reward will be biased and the algorithm will fail to converge to optimal performance.

## Foundational Learning

- Concept: Sub-Gaussian random variables and concentration inequalities
  - Why needed here: The regret analysis relies on concentration inequalities for sub-Gaussian random variables to bound the deviation of the RLS and TS estimates from their true values.
  - Quick check question: Why is the pseudo-reward constructed in section 3.1 sub-Gaussian? (Hint: look at Lemma 5)

- Concept: Regularized least squares with Laplacian regularization
  - Why needed here: The algorithm uses weighted regularized least squares to estimate the differential reward parameters, with a Laplacian penalty to encourage network cohesion.
  - Quick check question: How does the Laplacian penalty in equation (5) encourage similarity between connected nodes? (Hint: consider the form tr(Θ⊤LΘ))

- Concept: Thompson sampling as randomized exploration
  - Why needed here: The algorithm uses Thompson sampling to balance exploration and exploitation by sampling from a distribution centered at the RLS estimate.
  - Quick check question: What are the two key properties (anti-concentration and concentration) that the sampling distribution DT S must satisfy according to Definition 1?

## Architecture Onboarding

- Component map:
  Data ingestion -> Nuisance estimation -> Pseudo-reward construction -> Parameter estimation -> Action selection

- Critical path:
  1. Observe context Si,t
  2. Estimate baseline reward fi,t
  3. Construct pseudo-reward ˜Rf i,t
  4. Update parameter estimates ˆΘk
  5. Sample ˜Θk from TS distribution
  6. Select action Ai,t = arg max x(Si,t, a)⊤(˜θ + ˜θind i + ˜θtime t)

- Design tradeoffs:
  - Sample splitting vs. stable estimators: Sample splitting is simpler but requires i.i.d. errors; stable estimators are more flexible but may have higher variance.
  - Number of neighbors in NNR: More neighbors increase smoothing but may introduce bias if the network is not well-structured.
  - Regularization parameters λ and γ: λ controls the smoothness of individual and time parameters; γ controls the shrinkage towards the global parameter.

- Failure signatures:
  - High regret: Could indicate misspecification of the baseline reward model, poor network structure, or incorrect regularization parameters.
  - Unstable estimates: Could indicate violation of the i.i.d. error assumption or poor performance of the nuisance estimator.
  - Slow convergence: Could indicate insufficient exploration (too low variance in the TS distribution) or poor initialization.

- First 3 experiments:
  1. Run the algorithm on a simple linear setting with known network structure and compare regret to standard TS.
  2. Vary the regularization parameters λ and γ and observe their effect on the estimates and regret.
  3. Introduce misspecification in the baseline reward model and verify that the algorithm still achieves low regret if the propensity weights are correct.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important areas remain unexplored based on the content.

## Limitations

- The network cohesion penalty assumes meaningful similarity structures exist, but the paper doesn't adequately address what happens when these structures are misspecified or absent.
- The theoretical guarantees assume either sample splitting or stable estimation for the nuisance parameter, but provide limited guidance on when each approach is preferable.
- Empirical validation relies heavily on synthetic data and retrospective studies rather than true prospective trials.

## Confidence

- **High Confidence**: The theoretical framework for DML-based debiasing and the basic regret bound structure
- **Medium Confidence**: The practical implementation details and parameter tuning recommendations
- **Low Confidence**: Claims about performance in real-world mHealth settings without extensive prospective validation

## Next Checks

1. **Network Structure Sensitivity Analysis**: Systematically test RoME's performance across varying degrees of network misspecification, from completely random to adversarial network structures, to quantify the robustness of the cohesion penalty.

2. **Nuisance Estimation Protocol**: Implement and compare both sample splitting and stable estimation approaches on real-world mHealth datasets, measuring the impact on convergence rates and regret when nuisance estimation fails to meet theoretical assumptions.

3. **Prospective Trial Validation**: Design and execute a small-scale prospective mHealth intervention trial (n=50-100 users) to validate whether RoME's retrospective evaluation performance translates to real-world deployment, measuring both statistical performance and practical implementation challenges.