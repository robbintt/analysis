---
ver: rpa2
title: Optimising Distributions with Natural Gradient Surrogates
arxiv_id: '2310.11837'
source_url: https://arxiv.org/abs/2310.11837
tags:
- natural
- parameters
- distribution
- distributions
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes surrogate natural gradient descent (SNGD),
  a technique for optimising probability distribution parameters by reframing the
  problem with respect to a surrogate distribution for which natural gradients are
  tractable. The method expands the set of distributions that can be efficiently targeted
  with natural gradients, is easy to implement using autodiff software, and does not
  require lengthy model-specific derivations.
---

# Optimising Distributions with Natural Gradient Surrogates

## Quick Facts
- **arXiv ID:** 2310.11837
- **Source URL:** https://arxiv.org/abs/2310.11837
- **Reference count:** 40
- **Key outcome:** SNGD extends natural gradient applicability by using tractable surrogate distributions, matching or outperforming existing methods in both iteration count and wall-clock time

## Executive Summary
This paper introduces surrogate natural gradient descent (SNGD), a technique for optimizing probability distribution parameters when natural gradients under the target distribution are intractable. The method reframes the optimization problem with respect to a surrogate distribution for which natural gradients are easy to compute, typically leveraging exponential family properties. SNGD is demonstrated across various maximum likelihood estimation and variational inference tasks, showing it typically matches or outperforms existing best-practice methods in both iteration count and wall-clock time.

## Method Summary
SNGD addresses the challenge of computing natural gradients for distributions where the Fisher information matrix is intractable. When faced with an optimization objective under a target distribution q, the method solves the problem by optimizing under a surrogate distribution Ëœq whose natural gradients can be efficiently computed. The approach uses exponential family duality to compute natural gradients without explicitly inverting the Fisher matrix, and employs a parameter mapping between target and surrogate parameters. The method is implemented using autodiff software, making it relatively easy to apply across different distribution families.

## Key Results
- SNGD converges faster than natural gradient descent under target distribution in negative binomial MLE task
- SNGD typically matches or outperforms existing best-practice methods in both iteration count and wall-clock time
- The method expands the set of distributions that can be efficiently targeted with natural gradients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Natural gradients are more efficient than standard gradients for distribution optimization because they follow the steepest descent direction on the statistical manifold, invariant to parameterization.
- **Mechanism:** The Fisher information matrix defines a Riemannian metric on the space of distributions, and natural gradients move along this metric, avoiding the pathologies of standard gradients that depend on arbitrary parameterizations.
- **Core assumption:** The objective function is smooth and the Fisher information matrix is well-defined and invertible for the distribution being optimized.
- **Evidence anchors:** [abstract], [section]
- **Break condition:** The Fisher information matrix becomes singular or ill-conditioned.

### Mechanism 2
- **Claim:** Surrogate natural gradient descent (SNGD) extends the applicability of natural gradients by using a tractable surrogate distribution whose natural gradients are easy to compute.
- **Mechanism:** When the natural gradient under the target distribution is intractable, SNGD reframes the optimization in terms of a surrogate distribution for which natural gradients can be efficiently computed, typically using exponential family properties.
- **Core assumption:** There exists a surrogate distribution and parameter mapping such that the optimization under the surrogate approximates the original problem, and the surrogate's natural gradients can be computed efficiently.
- **Evidence anchors:** [abstract], [section]
- **Break condition:** The surrogate distribution poorly approximates the target, or the parameter mapping is poorly chosen.

### Mechanism 3
- **Claim:** For exponential family distributions, natural gradients can be computed efficiently without explicitly inverting the Fisher matrix by exploiting duality between natural and mean parameters.
- **Mechanism:** The remarkable property that natural gradients in one parameterization are simply regular gradients in the dual parameterization allows efficient computation without matrix inversion.
- **Core assumption:** The distribution belongs to the exponential family and the dual parameterization can be computed efficiently.
- **Evidence anchors:** [section], [abstract]
- **Break condition:** The exponential family duality breaks down or the dual parameterization cannot be computed efficiently.

## Foundational Learning

- **Concept: Exponential Family Distributions**
  - **Why needed here:** The paper relies heavily on properties of exponential family distributions to efficiently compute natural gradients for the surrogate distributions.
  - **Quick check question:** What are the key components that define an exponential family distribution, and why does the minimal family assumption matter?

- **Concept: Fisher Information Matrix**
  - **Why needed here:** The Fisher information matrix defines the metric for natural gradients and is central to understanding why natural gradients are more efficient than standard gradients.
  - **Quick check question:** How is the Fisher information matrix defined for a probability distribution, and what role does it play in natural gradient descent?

- **Concept: KL Divergence and Statistical Manifolds**
  - **Why needed here:** Natural gradients move along the statistical manifold of distributions, where distance is measured by KL divergence, making this concept fundamental to understanding the motivation.
  - **Quick check question:** How does the KL divergence relate to the geometry of the space of probability distributions, and why is this relevant for optimization?

## Architecture Onboarding

- **Component map:** Core optimization loop -> Surrogate distribution selection -> Parameter mapping -> Exponential family duality computation -> Transformation back to target space
- **Critical path:**
  1. Choose appropriate surrogate distribution and parameter mapping
  2. Compute objective function and its gradient with respect to surrogate parameters
  3. Apply natural gradient update using exponential family duality if applicable
  4. Transform updated parameters back to target distribution space
  5. Check convergence and repeat
- **Design tradeoffs:**
  - Surrogate choice: Closer surrogates may be more accurate but harder to compute gradients for; more distant surrogates may be easier but less accurate
  - Parameterization: Mean vs natural parameters affect computational efficiency and convergence properties
  - Auxiliary parameters: Adding them increases flexibility but adds complexity and potential optimization issues
- **Failure signatures:**
  - Poor convergence or divergence when surrogate is a bad approximation
  - Numerical instability when Fisher matrix is ill-conditioned
  - Slow convergence when the objective doesn't behave like an MLE for the surrogate
  - Implementation errors in parameter mapping or dual parameter computation
- **First 3 experiments:**
  1. Simple negative binomial MLE with gamma surrogate to verify basic SNGD implementation
  2. Skew-normal density estimation to test more complex surrogate-target relationships
  3. Elliptical copula optimization to validate auxiliary parameter extension

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Under what specific conditions does SNGD consistently outperform NGD under the target distribution q?
- **Basis in paper:** Explicit - The paper discusses cases where SNGD outperforms NGD but doesn't provide a comprehensive characterization.
- **Why unresolved:** The paper mentions some intuitive guidelines but leaves a full characterization to future work.
- **What evidence would resolve it:** Systematic experiments across diverse distribution families and optimization landscapes, coupled with theoretical analysis of when the surrogate's Fisher metric approximates the target's.

### Open Question 2
- **Question:** Can we develop a principled method for choosing effective surrogate distributions beyond relying on intuition or case-by-case analysis?
- **Basis in paper:** Explicit - The paper acknowledges the main limitation of needing to find a suitable surrogate and reparameterization for a given target distribution.
- **Why unresolved:** The paper only begins to explore what it means for a surrogate to "behave like" the target with respect to parameters.
- **What evidence would resolve it:** A theoretical framework that characterizes the conditions under which a surrogate distribution will be effective.

### Open Question 3
- **Question:** How can we theoretically characterize the trade-off between the asymptotic efficiency of NGD under q and the early-to-mid-stage performance gains of SNGD?
- **Basis in paper:** Explicit - The paper discusses how SNGD loses the asymptotic efficiency of NGD under q but can make rapid progress in early stages.
- **Why unresolved:** While the paper provides some intuition, it doesn't provide a comprehensive theoretical analysis of this trade-off.
- **What evidence would resolve it:** Theoretical analysis that quantifies the performance of both methods across different stages of optimization.

## Limitations
- The method's performance heavily depends on appropriate surrogate distribution selection, with no systematic framework provided for choosing optimal surrogates
- The computational complexity of computing natural gradients via the Fisher information matrix remains a potential bottleneck for very high-dimensional problems
- The sample of experiments is limited and doesn't thoroughly explore failure modes or provide statistical significance testing

## Confidence

**High confidence:** The core theoretical mechanism of using exponential family duality to compute natural gradients efficiently is well-established in the literature and correctly applied here.

**Medium confidence:** The empirical demonstrations show SNGD typically matches or outperforms baselines, but the sample of experiments is limited, and the paper doesn't thoroughly explore failure modes or provide statistical significance testing across multiple random seeds.

**Medium confidence:** The claim that SNGD is "easy to implement using autodiff software" is supported by the JAX examples, but the implementation complexity for custom surrogate-target mappings may vary significantly depending on the specific distributions involved.

## Next Checks

1. **Surrogate Selection Framework:** Implement a systematic method for evaluating and selecting appropriate surrogate distributions for new target distributions, potentially using KL divergence between perturbed distributions as a similarity metric.

2. **High-Dimensional Scalability:** Test SNGD on a high-dimensional optimization problem (e.g., Bayesian neural network VI) to empirically validate scalability claims and identify potential computational bottlenecks.

3. **Failure Mode Analysis:** Design and run experiments specifically targeting known failure conditions (e.g., singular Fisher information, poor surrogate approximation) to characterize when and why SNGD might fail compared to standard methods.