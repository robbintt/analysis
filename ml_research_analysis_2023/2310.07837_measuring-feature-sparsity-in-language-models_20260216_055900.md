---
ver: rpa2
title: Measuring Feature Sparsity in Language Models
arxiv_id: '2310.07837'
source_url: https://arxiv.org/abs/2310.07837
tags:
- sparse
- linear
- sparsity
- features
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces quantitative metrics to assess the success
  of sparse coding techniques applied to language model activations. The authors propose
  and validate metrics including average coefficient norm and normalized loss, which
  measure the degree of sparsity in the sparse coding decomposition of activations.
---

# Measuring Feature Sparsity in Language Models

## Quick Facts
- arXiv ID: 2310.07837
- Source URL: https://arxiv.org/abs/2310.07837
- Reference count: 40
- Primary result: Introduces metrics to quantify feature sparsity in language models and finds activations are significantly more sparse than control datasets

## Executive Summary
This paper develops quantitative metrics to assess the success of sparse coding techniques applied to language model activations. The authors propose average coefficient norm and normalized loss metrics that measure sparsity in sparse coding decompositions. Through experiments on synthetic data and real language models, they demonstrate these metrics can accurately predict true sparsity levels and distinguish sparse linear data from other distributions. The work provides evidence that language model activations are well-modeled as sparse linear combinations of features, with particularly high sparsity in first and final layers.

## Method Summary
The method uses sparse coding with L1 regularization to decompose language model activations into feature directions and coefficients. The optimization alternates between updating the feature dictionary (Φ) using SGD and computing coefficients (α) via a greedy projection algorithm. The normalized loss metric Lnorm(Φ) = L(Φ)/(λ||α(Φ)||∞) quantifies sparsity by normalizing the reconstruction error by the average coefficient magnitude. This approach is validated on synthetic sparse linear data before being applied to token embeddings and intermediate activations of various language models, with results compared against control distributions.

## Key Results
- Average coefficient norm and normalized loss metrics accurately predict true sparsity levels in synthetic sparse linear data
- Language model activations show significantly lower normalized loss than Gaussian, heavy-tailed, and sparse non-linear control datasets
- Activation sparsity is greatest in first and final layers, with intermediate layers showing relatively lower sparsity
- Results support the hypothesis that language model activations can be modeled as sparse linear combinations of features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse coding with L1 regularization can accurately decompose language model activations into interpretable feature directions.
- Mechanism: The sparse coding objective L(Φ; α) = (1/n)(||X-Φα||²₂ + λ||α||₁) encourages solutions where α is sparse, representing only a small number of active features per activation vector. The iterative optimization alternates between minimizing L over Φ and finding α via a greedy procedure that projects onto the most active feature directions.
- Core assumption: Activations can be modeled as sparse linear combinations of underlying feature vectors (linearity and sparsity hypotheses).
- Evidence anchors: [abstract] "we develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions" [section 2] "The sparsity hypothesis says that a typical activation x only requires a small number of features to approximately represent it as a linear combination of those features"

### Mechanism 2
- Claim: The normalized loss metric Lnorm(Φ) = L(Φ)/(λ||α(Φ)||∞) accurately measures the degree of sparsity in the sparse coding decomposition.
- Mechanism: Lnorm normalizes the sparse coding objective by the average magnitude of the feature coefficients, making it invariant to scaling of activations. For perfectly reconstructed activations, Lnorm equals the average coefficient norm S1(Φ), which counts the average number of active features. For imperfect reconstruction, Lnorm includes a penalty term that makes it robust to noise.
- Core assumption: The normalized loss is a continuous, scale-invariant, and intuitive measure of sparsity.
- Evidence anchors: [section 3] "We define the normalized loss as Lnorm(Φ) = L(Φ)/(λ||α(Φ)||∞)" [section 3.1] "the normalized loss and average coefficient norm closely approximate the true sparsity"

### Mechanism 3
- Claim: Language model activations are more sparse than control datasets, supporting the sparsity hypothesis.
- Mechanism: By comparing the normalized loss of language model activations to that of various control datasets (Gaussian, heavy-tailed, sparse non-linear), we can quantify how much more sparse the language model activations are. Lower normalized loss indicates greater sparsity.
- Core assumption: Control datasets represent alternative models of activations that are less sparse than the true underlying sparse linear model.
- Evidence anchors: [abstract] "We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets" [section 4.1] "for all models the normalized loss of the token embeddings is much lower than that for a Gaussian"

## Foundational Learning

- Concept: Sparse coding and L1 regularization
  - Why needed here: Sparse coding with L1 regularization is the key technique used to decompose language model activations into interpretable feature directions. Understanding how it works is essential for interpreting the results.
  - Quick check question: What is the role of the L1 norm in the sparse coding objective, and how does it encourage sparsity in the feature coefficients?

- Concept: Linearity and sparsity hypotheses
  - Why needed here: These hypotheses form the theoretical foundation for why sparse coding should work on language model activations. They state that activations can be modeled as sparse linear combinations of underlying feature vectors.
  - Quick check question: What are the linearity and sparsity hypotheses, and how do they justify the use of sparse coding on language model activations?

- Concept: Normalized loss metric
  - Why needed here: The normalized loss metric is used to quantify the degree of sparsity in the sparse coding decomposition. Understanding how it is defined and why it is a good measure of sparsity is crucial for interpreting the results.
  - Quick check question: How is the normalized loss metric defined, and why is it a good measure of sparsity in the sparse coding decomposition?

## Architecture Onboarding

- Component map: Sparse coding algorithm -> Feature dictionary (Φ) and coefficients (α) -> Normalized loss computation -> Control dataset comparison
- Critical path: Run sparse coding on language model activations → Compute normalized loss → Compare to normalized loss of control datasets → Interpret results in terms of sparsity of language model activations
- Design tradeoffs: The main design tradeoff is between the expressiveness of the sparse coding model (controlled by dictionary size) and the computational cost of running the algorithm. Larger dictionary sizes allow more expressive decompositions but are more expensive to compute.
- Failure signatures: Failure modes include: sparse coding failing to converge, normalized loss not accurately measuring sparsity, control datasets not being representative of alternative activation models, and language model activations not being well-approximated by sparse linear combinations.
- First 3 experiments:
  1. Run sparse coding on a small subset of language model activations and visualize the resulting feature directions to sanity check that they look interpretable.
  2. Compute the normalized loss on a range of synthetic sparse linear datasets with known sparsity levels to validate that the metric accurately measures sparsity.
  3. Compare the normalized loss of language model activations to that of a few representative control datasets (e.g. Gaussian, heavy-tailed) to get an initial sense of how sparse the activations are.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed metrics (average coefficient norm and normalized loss) perform when applied to non-sparse linear data distributions beyond the three tested in the paper?
- Basis in paper: [explicit] The paper tests the metrics on Gaussian, heavy-tailed isotropic, and Rademacher distributions, but does not explore other non-sparse linear distributions.
- Why unresolved: The paper only considers a limited set of non-sparse linear distributions, leaving the question of whether the metrics can reliably distinguish sparse linear data from other non-sparse linear distributions unanswered.
- What evidence would resolve it: Applying the metrics to a wider range of non-sparse linear distributions and comparing their performance in distinguishing these distributions from sparse linear data.

### Open Question 2
- Question: How does the choice of dictionary size impact the performance of the sparse coding algorithm and the resulting feature sparsity measurements?
- Basis in paper: [explicit] The paper explores the effect of dictionary size on metric performance but does not provide a comprehensive analysis of the optimal dictionary size for different model architectures and layers.
- Why unresolved: While the paper demonstrates that the metrics are robust to dictionary size misspecification up to a factor of 4, it does not investigate the impact of dictionary size on the overall quality of the sparse coding decomposition or the accuracy of the feature sparsity measurements.
- What evidence would resolve it: Conducting experiments with varying dictionary sizes across different model architectures and layers, and analyzing the impact on the quality of the sparse coding decomposition and the resulting feature sparsity measurements.

### Open Question 3
- Question: How do the proposed metrics compare to other existing methods for measuring feature sparsity in language models?
- Basis in paper: [explicit] The paper introduces novel metrics for measuring feature sparsity but does not provide a comparison with other existing methods.
- Why unresolved: The paper does not discuss how the proposed metrics compare to other methods in terms of accuracy, computational efficiency, or robustness to different model architectures and layers.
- What evidence would resolve it: Conducting a comparative study of the proposed metrics against other existing methods for measuring feature sparsity in language models, evaluating their performance across different model architectures and layers.

## Limitations
- Analysis is primarily correlative rather than proving fundamental sparse feature combinations in activations
- Normalized loss shows some deviation from ground truth sparsity at higher sparsity levels
- Comparison to control datasets doesn't definitively rule out alternative explanations
- Study focuses on smaller models (BERT, TinyStories, GPT-Neo, GPT-2) and may not generalize to larger models

## Confidence
- **High confidence**: Core methodology for sparse coding decomposition and metric computation is technically sound and reproducible
- **Medium confidence**: Claim that activations are "significantly more sparse" than control datasets is supported but could be strengthened with additional controls
- **Medium confidence**: Layer-wise sparsity patterns are based on empirical measurements but require further validation across different model architectures

## Next Checks
1. Test normalized loss metric against additional control distributions including sparse autoencoders and random projections
2. Apply same sparsity analysis to larger language models (Llama, Claude, GPT-3/4) to verify generalizability
3. Conduct human evaluations to verify whether higher sparsity scores correspond to more interpretable or causally meaningful features