---
ver: rpa2
title: 'PFNs4BO: In-Context Learning for Bayesian Optimization'
arxiv_id: '2305.17535'
source_url: https://arxiv.org/abs/2305.17535
tags:
- prior
- optimization
- step
- bayesian
- hebo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that Prior-data Fitted Networks (PFNs)
  can serve as flexible surrogates for Bayesian Optimization (BO), outperforming standard
  GP-based methods on three diverse hyperparameter optimization benchmarks. The key
  contribution is showing that PFNs can approximate any prior distribution, including
  complex ones like Bayesian Neural Networks or HEBO's prior, enabling extensions
  such as incorporating user priors about optimum locations, ignoring irrelevant dimensions,
  and approximating non-myopic acquisition functions.
---

# PFNs4BO: In-Context Learning for Bayesian Optimization

## Quick Facts
- arXiv ID: 2305.17535
- Source URL: https://arxiv.org/abs/2305.17535
- Reference count: 40
- Key outcome: PFNs outperform standard GP-based BO methods on three HPO benchmarks using flexible priors including user-specified optimum locations

## Executive Summary
This paper introduces Prior-data Fitted Networks (PFNs) as flexible surrogates for Bayesian Optimization, demonstrating state-of-the-art performance on hyperparameter optimization benchmarks. PFNs are trained to approximate posterior predictive distributions through in-context learning on synthetic datasets drawn from chosen prior distributions. The approach enables Bayesian optimization with complex priors beyond traditional Gaussian processes, including user-specified information about optimum locations and handling of irrelevant dimensions.

## Method Summary
PFNs are neural processes trained offline on synthetic datasets sampled from a chosen prior distribution (GP, BNN, or HEBO-inspired). During inference, they map observations to posterior predictive distributions without requiring explicit analytical forms. The trained PFNs serve as surrogates in Bayesian optimization loops, using standard acquisition functions like Expected Improvement, Probability of Improvement, and Upper Confidence Bound. The method handles variable-length datasets through transformer architecture and incorporates user priors as additional conditioning inputs.

## Key Results
- PFNs with HEBO+ prior achieve state-of-the-art performance on HPO-B, Bayesmark, and PD1 benchmarks
- User priors provide additional performance gains on PD1 benchmark with expert-specified optimum locations
- Input warping enables handling of incorrectly scaled hyperparameters without prior knowledge
- PFNs outperform standard GP, DNGO, DGP, and HEBO baselines in average rank and regret metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PFNs approximate any prior distribution that can be efficiently sampled from, enabling Bayesian optimization with flexible priors beyond traditional GPs.
- Mechanism: By training on synthetic datasets drawn from a chosen prior, PFNs learn to map observations to posterior predictive distributions without requiring explicit analytical forms.
- Core assumption: The prior distribution can be efficiently sampled from and the PFN architecture is expressive enough to approximate its posterior.
- Evidence anchors:
  - [abstract] "PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) through in-context learning on any prior distribution that can be efficiently sampled from."
  - [section] "Prior-fitting is the training of a PFN to approximate the PPD and thus perform Bayesian prediction for a particular, chosen, prior."
  - [corpus] Weak - no direct corpus evidence of PFN posterior approximation quality.
- Break condition: If the prior cannot be efficiently sampled or the PFN architecture is insufficiently expressive.

### Mechanism 2
- Claim: PFNs enable direct incorporation of user priors about optimum locations without complex space warping.
- Mechanism: User beliefs about optimum locations are encoded as additional inputs to the PFN, modifying the prior during inference.
- Core assumption: User prior information can be encoded as simple interval and confidence inputs.
- Evidence anchors:
  - [abstract] "incorporate further information into the prior, such as allowing hints about the position of optima (user priors)"
  - [section] "PFNs, can integrate these priors in a more direct and sound way... let the user define an interval I ∈ I, in which they belief the optimum lies, and a weighting ρ ∈ [0, 1], indicating their confidence."
  - [corpus] Weak - no direct corpus evidence of user prior effectiveness.
- Break condition: If user prior information is too complex to encode as simple intervals.

### Mechanism 3
- Claim: PFNs naturally handle irrelevant dimensions by ignoring them during prior sampling.
- Mechanism: During prior fitting, irrelevant features are added at random and simply not fed to the GP/NN generating outcomes.
- Core assumption: Irrelevant features can be identified and excluded during the prior sampling process.
- Evidence anchors:
  - [abstract] "ignoring irrelevant dimensions"
  - [section] "PFNs, can integrate these priors in a more direct and sound way... We found that this improves performance, especially for large search spaces"
  - [corpus] Weak - no direct corpus evidence of irrelevant dimension handling.
- Break condition: If irrelevant features cannot be reliably identified during prior sampling.

## Foundational Learning

- Concept: Neural processes and transformer architecture
  - Why needed here: PFNs are built on neural process architecture using transformers, which enables handling variable-length datasets and in-context learning.
  - Quick check question: What is the key architectural difference between PFNs and standard neural networks?

- Concept: Bayesian inference and posterior predictive distributions
  - Why needed here: PFNs are trained to approximate posterior predictive distributions, which is the core of Bayesian optimization.
  - Quick check question: How does a PFN approximate a posterior predictive distribution during training?

- Concept: Acquisition functions in Bayesian optimization
  - Why needed here: PFNs are used as surrogates in Bayesian optimization, requiring understanding of how acquisition functions work with posterior distributions.
  - Quick check question: What is the difference between expected improvement and probability of improvement?

## Architecture Onboarding

- Component map:
  - Input layer: Variable-length training set and query set
  - Transformer encoder: Processes feature vectors and labels with attention
  - Riemann distribution head: Outputs discretized posterior predictive distribution
  - Style embeddings: Incorporates user priors and other conditioning information

- Critical path:
  1. Prior fitting: Train PFN on synthetic datasets from chosen prior
  2. Inference: Use trained PFN to approximate PPD for new observations
  3. Acquisition: Compute acquisition function values from PPD
  4. Suggestion: Select next query point based on acquisition values

- Design tradeoffs:
  - Training cost vs. inference speed: PFNs require expensive prior fitting but enable fast inference
  - Discretized vs. continuous output: Riemann distribution enables classification-style training but loses continuous information
  - Expressiveness vs. generalization: More complex priors may improve performance but reduce generalization

- Failure signatures:
  - Poor posterior approximation: PFN predictions don't match ground truth for simple priors
  - Overfitting: PFN performs well on training data but poorly on new tasks
  - Instability: PFN predictions vary wildly with small changes in inputs

- First 3 experiments:
  1. Train PFN on simple GP prior and compare predictions to ground truth posterior
  2. Test PFN with user priors on synthetic optimization problems with known optima
  3. Evaluate PFN performance on a small subset of HPO-B benchmark tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PFN performance scale with increasing dimensionality of the search space?
- Basis in paper: [inferred] The paper shows PFNs perform well on benchmarks with up to 18 dimensions, but explicitly mentions that "PFNs were only shown to perform well up to around 1000 training points" and that "we could not find a setting that recovers mislabeled log dimensions, but still performs very strong on correctly specified search spaces." 
- Why unresolved: The paper doesn't systematically study performance degradation with increasing dimensionality, and the authors acknowledge limitations with high-dimensional spaces and mislabeled hyperparameters.
- What evidence would resolve it: Systematic experiments showing PFN performance (rank, regret) across search spaces with varying dimensions (e.g., 5, 10, 50, 100 dimensions) while keeping other factors constant, including both well-specified and mislabeled hyperparameters.

### Open Question 2
- Question: Can PFNs effectively handle non-stationary and heteroscedastic functions without extensive input warping?
- Basis in paper: [explicit] The paper states "We plan to extend our work to contain more elaborate priors that can handle non-stationary functions" and "while this approximation works well for our prior, it might fail, for example with a heteroscedastic prior."
- Why unresolved: The authors acknowledge these are future research directions and demonstrate input warping as a workaround rather than a principled solution, noting that current PFNs have limitations with non-standard data distributions.
- What evidence would resolve it: Experiments comparing PFN performance with and without input warping on benchmarks containing non-stationary and heteroscedastic functions, showing whether principled prior extensions can eliminate the need for post-hoc input transformations.

### Open Question 3
- Question: What is the practical impact of the PFN's inability to draw joint samples from multiple data points?
- Basis in paper: [explicit] The authors state as a limitation: "Compared to Gaussian processes we cannot draw joint samples from multiple data points, which prohibits the straight-forward adaptation of some acquisition functions such as the noisy EI."
- Why unresolved: While identified as a limitation, the paper doesn't quantify how much this restriction affects real-world performance or whether alternative acquisition functions can compensate for this inability.
- What evidence would resolve it: Empirical comparison of PFN performance using acquisition functions that don't require joint sampling versus those that do (like noisy EI), measuring the performance gap on real-world optimization problems where noisy observations are common.

## Limitations

- Scalability concerns with high-dimensional problems and expensive prior sampling
- Computational cost of prior-fitting requiring training on synthetic datasets
- Discretization of posterior distributions may lose continuous information
- Limited testing on non-standard data distributions and misspecified priors

## Confidence

- **High confidence**: PFNs can effectively approximate GP posteriors and serve as surrogates for Bayesian optimization, as demonstrated across three benchmarks
- **Medium confidence**: PFNs can incorporate user priors about optimum locations and handle irrelevant dimensions, though empirical validation is limited
- **Low confidence**: The method's robustness to poorly specified priors or extreme hyperparameter scales, given limited testing scenarios

## Next Checks

1. Test PFN performance when the assumed prior is deliberately misspecified (e.g., fitting with GP prior but true function is highly non-stationary) to assess robustness to prior mismatch

2. Evaluate scaling behavior on high-dimensional problems (d > 50) to determine if PFNs maintain efficiency advantages over traditional GP-based methods as dimensionality increases

3. Compare acquisition function optimization quality between PFN-based and GP-based surrogates using identical optimization budgets and seeds to isolate the impact of the surrogate model itself