---
ver: rpa2
title: 'MAD: Modality Agnostic Distance Measure for Image Registration'
arxiv_id: '2309.02875'
source_url: https://arxiv.org/abs/2309.02875
tags:
- image
- registration
- distance
- measure
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAD introduces a modality-agnostic deep distance measure for multi-modal
  medical image registration. It uses random convolutions as data augmentation to
  generate synthetic image pairs with different appearances but preserved geometry,
  enabling training on mono-modal datasets.
---

# MAD: Modality Agnostic Distance Measure for Image Registration

## Quick Facts
- **arXiv ID:** 2309.02875
- **Source URL:** https://arxiv.org/abs/2309.02875
- **Reference count:** 30
- **Primary result:** Achieves lower mean absolute errors (e.g., 13.34 mm vs 17.09 mm for T1w-CT registration) and larger capture range than traditional metrics like NMI and NGF

## Executive Summary
MAD introduces a modality-agnostic deep distance measure for multi-modal medical image registration that eliminates the need for paired multi-modal training data. The method uses random convolutions as geometry-preserving data augmentation to generate synthetic image pairs with different appearances but preserved geometry, enabling training on mono-modal datasets. A CNN is trained to predict distances between patch centers from these synthetic pairs, learning a geometry-focused similarity measure. The method outperforms traditional metrics like NMI and NGF in large misalignment scenarios while demonstrating a larger capture range without requiring multi-resolution schemes.

## Method Summary
MAD uses random convolutions as geometry-preserving data augmentation to create synthetic multi-modal training data from mono-modal datasets. Random convolutions apply linear transformations to image intensities followed by clamping and leaky ReLU to maintain geometric consistency while altering appearance. A ResNet CNN is trained to predict Euclidean distances between patch centers from these synthetic modality pairs using MSE loss. For registration, the trained network aggregates patch-based similarity scores across the full image to compute a modality-agnostic distance measure, which is then used in optimization algorithms for affine registration of multi-modal medical images.

## Key Results
- MAD achieves lower mean absolute errors than NMI and NGF in T1w-CT registration (13.34 mm vs 17.09 mm)
- Demonstrates larger capture range than traditional measures without requiring multi-resolution schemes
- Successfully trained on mono-modal datasets and applied to multi-modal registration tasks
- Outperforms state-of-the-art registration methods in terms of accuracy and robustness

## Why This Works (Mechanism)

### Mechanism 1
Random convolutions preserve geometry while changing appearance, enabling synthetic multi-modal training without paired data. Random convolutions apply linear transformations to image intensities, followed by clamping and leaky ReLU. This maintains spatial structure while altering local intensity/texture patterns, creating aligned image pairs of different appearances.

### Mechanism 2
Training a CNN to predict distances between patch centers forces the network to learn geometry-focused similarity rather than appearance-based similarity. By supervising the network with ground truth distances between patch centers after synthetic geometric transformations, the CNN learns to estimate similarity based on spatial relationships rather than intensity patterns.

### Mechanism 3
MAD achieves larger capture range than traditional measures by learning complex appearance transformations rather than relying on hand-crafted assumptions. The network trained on randomly transformed appearance patches can handle larger misalignments because it has learned to recognize underlying geometry despite significant appearance changes, unlike NMI which relies on intensity histograms or NGF which uses edge maps with restrictive assumptions.

## Foundational Learning

- **Concept: Random convolutions as geometry-preserving data augmentation**
  - Why needed here: To create synthetic multi-modal training data without requiring aligned paired images from different modalities
  - Quick check question: How do random convolutions maintain geometric consistency while altering appearance?

- **Concept: Self-supervised learning from synthetic geometric transformations**
  - Why needed here: To train the distance measure without requiring ground truth multi-modal alignments
  - Quick check question: What supervision signal is used to train the distance prediction network?

- **Concept: Patch-based similarity aggregation for 3D medical images**
  - Why needed here: To create a computationally efficient and locally focused distance measure for large 3D volumes
  - Quick check question: How does the patch-based approach scale to full 3D image registration?

## Architecture Onboarding

- **Component map:** Mono-modal dataset → Random convolution augmentation → Synthetic geometric transformations → Patch sampling → ResNet distance prediction network → Aggregated distance measure → Registration optimization
- **Critical path:** The distance prediction network is the critical component; if it fails to learn meaningful geometry-focused similarity, the entire registration pipeline fails
- **Design tradeoffs:** Using random convolutions trades computational efficiency for the need to generate synthetic data, versus using real paired multi-modal data which is expensive to obtain
- **Failure signatures:** Large registration errors on multi-modal pairs, poor performance on large misalignments, overfitting to training appearance variations
- **First 3 experiments:**
  1. Verify random convolutions preserve geometry by applying known transformations and checking structural similarity
  2. Test distance prediction accuracy on synthetic patch pairs with varying appearance changes
  3. Evaluate registration performance on small synthetic misalignments before scaling to larger transformations

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but the discussion implies several areas for future work:
- Extension to deformable registration beyond affine transformations
- Application to other medical imaging modalities beyond MRI and CT
- Investigation of different network architectures for distance prediction
- Analysis of computational efficiency compared to traditional methods

## Limitations

- Limited evaluation to affine registration without testing deformable registration scenarios
- Performance validation only on MRI and CT modalities, not tested on other medical imaging combinations
- Reliance on synthetic data generation may not fully capture real-world multi-modal appearance variations
- No direct comparison with other deep learning registration methods trained on multi-modal data

## Confidence

- **High confidence:** The synthetic data generation approach and its application to single-domain training (backed by theoretical linear convolution properties)
- **Medium confidence:** The capture range improvement over traditional methods (supported by qualitative landscape visualizations but limited quantitative comparison)
- **Low confidence:** Generalization to arbitrary multi-modal pairs beyond the tested combinations (no ablation studies on augmentation diversity)

## Next Checks

1. Conduct a controlled experiment varying the diversity of random convolutions and measuring corresponding registration performance degradation to establish the relationship between augmentation diversity and generalization
2. Compare MAD against intensity-normalization baselines (e.g., histogram matching) to isolate whether improvements come from learned geometry versus appearance normalization
3. Test the method on modality pairs with known geometric vs. appearance discrepancies (e.g., same modality with different sequences) to validate the geometry-preserving claim of random convolutions