---
ver: rpa2
title: 'From Classification to Generation: Insights into Crosslingual Retrieval Augmented
  ICL'
arxiv_id: '2311.06595'
source_url: https://arxiv.org/abs/2311.06595
tags:
- language
- retrieval
- multilingual
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cross-lingual retrieval-augmented in-context
  learning (CREA-ICL) approach to improve the zero-shot performance of multilingual
  pre-trained language models (MPLMs) for low-resource languages like Bangla. The
  method uses a cross-lingual retriever to find semantically similar prompts from
  high-resource languages, which are then combined with the original input to create
  a retrieval-augmented prompt for the MPLM.
---

# From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL

## Quick Facts
- arXiv ID: 2311.06595
- Source URL: https://arxiv.org/abs/2311.06595
- Reference count: 18
- Primary result: Cross-lingual retrieval augmentation improves Bangla classification F1-scores by 5-10% over zero-shot baselines

## Executive Summary
This paper introduces a cross-lingual retrieval-augmented in-context learning (CREA-ICL) approach to improve zero-shot performance of multilingual pre-trained language models (MPLMs) for low-resource languages like Bangla. The method retrieves semantically similar prompts from high-resource languages and combines them with original inputs to create augmented prompts. While the approach yields significant improvements in classification tasks, it faces challenges in generation tasks, revealing important insights about the differential impact of cross-lingual retrieval across task types.

## Method Summary
The CREA-ICL approach uses a multilingual sentence transformer to map low-resource language inputs to a shared embedding space, retrieves the most semantically similar high-resource language samples, and constructs augmented prompts by combining these retrieved examples with the original input. The method evaluates BLOOM, BLOOMZ, mBERT, and mT5 models on Bangla classification (Vio-Lens and SentNoB) and generation (XLSum and XQuAD) tasks, comparing zero-shot baselines with retrieval-augmented settings using k=1 and k=3 retrieved prompts.

## Key Results
- CREA-ICL with k=3 retrieved prompts improves Bangla classification F1-scores by 5-10% over zero-shot baselines
- Classification tasks benefit more from cross-lingual retrieval augmentation compared to generation tasks
- Larger generative models like BLOOMZ-3B do not guarantee better results across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual retrieval provides semantically similar prompts from high-resource languages to improve zero-shot performance for low-resource languages like Bangla.
- Mechanism: The cross-lingual retriever maps a low-resource language input to a shared embedding space, retrieves the most semantically similar high-resource language samples, and uses them as augmented prompts to guide the MPLM's prediction.
- Core assumption: Semantic similarity in the shared embedding space translates to task-relevant guidance for the target low-resource language task.
- Evidence anchors:
  - [abstract] "By extracting semantically similar prompts from high-resource languages, we aim to improve the zero-shot performance of multilingual pre-trained language models (MPLM) across diverse tasks."
  - [section 3.1] "For a given input sentence q... the cross-lingual retriever maps it to a vector qembed in a shared embedding space using a function Embed"
  - [corpus] Weak: No direct corpus evidence, but methodologically aligned with PARC [Nie et al., 2023] which this work extends
- Break condition: If the shared embedding space poorly aligns semantic similarity across languages, retrieved samples may not provide relevant guidance.

### Mechanism 2
- Claim: Retrieval augmentation improves classification tasks more than generation tasks for low-resource languages.
- Mechanism: Classification tasks benefit from additional semantically similar examples that clarify task boundaries and class distinctions, while generation tasks require language consistency that may be disrupted by cross-lingual examples.
- Core assumption: Classification performance is more sensitive to semantic guidance than generation performance is to language consistency.
- Evidence anchors:
  - [abstract] "Though our approach yields steady improvements in classification tasks, it faces challenges in generation tasks."
  - [section 5.1] "Bloomz-3b demonstrated an improvement in F1-scores for both tasks by 5% and 10% respectively" for classification
  - [section 5.3] "classification tasks benefit more from cross-lingual retrieval augmentation compared to generative tasks, likely due to the language consistency requirements of generation"
- Break condition: If generation tasks have strong language transfer capabilities or classification tasks require strict language consistency, the observed pattern may reverse.

### Mechanism 3
- Claim: Larger generative models like BLOOMZ do not guarantee better results across all evaluation metrics.
- Mechanism: Model size and architecture interact with task requirements and retrieval augmentation in complex ways that don't always favor larger models.
- Core assumption: Model performance is determined by more than just parameter count, including instruction tuning, task alignment, and interaction with retrieval augmentation.
- Evidence anchors:
  - [section 5.1] "The striking null results from Bloom-3b, in contrast to Bloomz-3b, emphasize the pivotal role instruction tuning plays in retrieval-augmented in-context learning"
  - [section 5.2] "the additional retrieval data may be more of a distraction than an advantage for this model configuration in the summarization task"
  - [section 5.3] "while models such as Bloomz-3b demonstrate superior performance, larger models do not guarantee better results across all evaluation criteria"
- Break condition: If larger models have proportionally better instruction tuning or if retrieval augmentation quality scales with model capacity, larger models may show consistent improvement.

## Foundational Learning

- Concept: Cross-lingual semantic similarity in shared embedding spaces
  - Why needed here: The method relies on retrieving semantically similar prompts across languages, requiring understanding of how semantic similarity transfers across languages
  - Quick check question: How does a sentence transformer ensure that semantically similar sentences in different languages map to nearby vectors in the shared embedding space?

- Concept: In-context learning (ICL) mechanisms
  - Why needed here: The approach builds on ICL by augmenting prompts with retrieved examples, requiring understanding of how ICL works and how it can be enhanced
  - Quick check question: What distinguishes in-context learning from traditional fine-tuning, and how does prompt engineering affect ICL performance?

- Concept: Language consistency in generation tasks
  - Why needed here: The paper identifies language consistency as a key challenge for generation tasks, requiring understanding of why language consistency matters and how it can be maintained
  - Quick check question: Why might cross-lingual retrieval augmentation disrupt language consistency in generation tasks while benefiting classification tasks?

## Architecture Onboarding

- Component map: Input -> Cross-lingual retriever (multilingual sentence transformer) -> Retrieval corpus (high-resource language datasets) -> Prompt template engine -> MPLM (BLOOM, BLOOMZ, mBERT, mT5) -> Output post-processing and evaluation

- Critical path:
  1. Input processing and embedding via cross-lingual retriever
  2. Semantic similarity search in high-resource corpus
  3. Prompt template construction with retrieved examples
  4. MPLM prediction using augmented prompt
  5. Output post-processing and evaluation

- Design tradeoffs:
  - Retrieval sample count (k): More samples provide more context but increase noise and computational cost
  - Model selection: Masked models (mBERT) vs autoregressive models (BLOOMZ) have different strengths for classification vs generation
  - Prompt template design: Must balance task specificity with generality across languages

- Failure signatures:
  - No improvement over baseline: May indicate poor semantic alignment in shared embedding space
  - Degradation with retrieval augmentation: May indicate retrieval noise overwhelming useful signal
  - Language inconsistency in generation: May indicate cross-lingual contamination in autoregressive models

- First 3 experiments:
  1. Baseline zero-shot performance comparison across classification and generation tasks
  2. Retrieval augmentation with k=1 to establish minimal viable improvement
  3. Retrieval augmentation with k=3 to test scalability and noise tolerance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increasing the number of retrieved samples (k) not consistently improve performance in classification tasks?
- Basis in paper: [explicit] The authors note that "increasing k does not necessarily improve performance, suggesting strategic information filtering and noise reduction capabilities of the models."
- Why unresolved: The paper does not provide a detailed analysis of why the models struggle with selecting the most relevant information from a larger set of retrieved samples.
- What evidence would resolve it: Experiments comparing the quality of retrieved samples for different values of k, or ablation studies on the impact of noisy vs. clean retrieved samples.

### Open Question 2
- Question: How can language consistency be improved in generative tasks for low-resource languages?
- Basis in paper: [explicit] The authors discuss the challenge of "maintaining language consistency" in generative tasks, where models may output in unexpected languages.
- Why unresolved: The paper suggests that current metrics struggle to evaluate language-specific output quality, and modifying prompts to include language constraints only marginally improves results.
- What evidence would resolve it: Development and evaluation of new metrics that better account for language consistency, or experiments with different prompting strategies to encourage language adherence.

### Open Question 3
- Question: Why do generative models like BLOOMZ struggle more with minority classes compared to masked models like mBERT in classification tasks?
- Basis in paper: [explicit] The authors observe that "generative models struggle more with minority classes compared to masked models like mBERT."
- Why unresolved: The paper does not provide a detailed analysis of the reasons behind this difference in performance between generative and masked models.
- What evidence would resolve it: Experiments comparing the internal representations and decision boundaries of generative and masked models for minority classes, or ablation studies on the impact of different model architectures on minority class performance.

## Limitations
- The foundational assumption that semantic similarity in shared embedding spaces translates to task-relevant guidance across languages lacks direct empirical validation
- The analysis of why classification benefits more than generation from retrieval augmentation remains speculative, with the language consistency hypothesis needing more rigorous testing
- The observed pattern that larger models don't guarantee better results could be dataset-specific rather than a generalizable finding

## Confidence
**High Confidence Claims:**
- CREA-ICL improves Bangla classification F1-scores by 5-10% over zero-shot baselines (supported by direct experimental results)
- BLOOMZ outperforms BLOOM in retrieval-augmented settings (clearly demonstrated through comparative results)
- mBERT handles minority classes better than BLOOMZ (evidenced by confusion matrix analysis)

**Medium Confidence Claims:**
- Classification tasks benefit more from cross-lingual retrieval than generation tasks (supported by experimental trends but lacking deeper causal analysis)
- Language consistency requirements explain generation task challenges (plausible mechanism but not rigorously tested)
- Instruction tuning is pivotal for retrieval-augmented ICL success (inferred from BLOOM vs BLOOMZ comparison)

**Low Confidence Claims:**
- The specific mechanism by which semantic similarity translates to task performance (not directly tested)
- The generalizability of model size findings across different languages and tasks (based on limited experimental scope)
- The optimal k value for retrieval augmentation (tested only for k=1 and k=3)

## Next Checks
1. **Semantic Alignment Validation**: Conduct a human evaluation study to verify that retrieved samples are semantically similar and task-relevant across language pairs, measuring the correlation between embedding similarity and human-perceived relevance.

2. **Ablation on Language Consistency**: Design controlled experiments that systematically vary the language of retrieved examples (same-language vs cross-language) to quantify the impact of language consistency on generation task performance.

3. **Cross-Lingual Transfer Analysis**: Implement a controlled experiment using high-resource languages as target languages to test whether the observed pattern (classification > generation) holds when the "low-resource" condition is relaxed, helping isolate whether findings are due to language resource levels or task characteristics.