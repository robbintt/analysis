---
ver: rpa2
title: Applying Large Language Models for Causal Structure Learning in Non Small Cell
  Lung Cancer
arxiv_id: '2311.07191'
source_url: https://arxiv.org/abs/2311.07191
tags:
- causal
- https
- lung
- cancer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that Large Language Models (LLMs) can be
  effectively used to determine causal relationships in medical datasets, specifically
  in Non Small Cell Lung Cancer (NSCLC). Two LLM-based approaches for generating causal
  graphs were compared against traditional optimization methods (NOTEARS and PC algorithms),
  with LLMs achieving significantly better Bayesian Dirichlet (BDeu) scores (e.g.,
  -4150 vs -6092 for PC).
---

# Applying Large Language Models for Causal Structure Learning in Non Small Cell Lung Cancer

## Quick Facts
- arXiv ID: 2311.07191
- Source URL: https://arxiv.org/abs/2311.07191
- Reference count: 33
- Key outcome: LLM-based causal discovery achieved BDeu scores of -4150 vs -6092 for traditional PC algorithm on NSCLC data

## Executive Summary
This study demonstrates that Large Language Models (LLMs) can effectively determine causal relationships in medical datasets, specifically for Non Small Cell Lung Cancer (NSCLC). The researchers developed two LLM-based approaches for generating causal graphs and compared them against traditional optimization methods (NOTEARS and PC algorithms). LLM-generated models significantly outperformed traditional methods with better Bayesian Dirichlet (BDeu) scores and accurately identified causal relationships between clinical and genomic features. The models revealed that RET mutations had the strongest effect on both chemotherapy and targeted therapy outcomes, while immunotherapy showed poor performance across all biomarkers.

## Method Summary
The study used deidentified NSCLC patient data with EHR and genomic features to generate causal graphs using two LLM-based approaches: iterative prompting and single-prompt with corrections. LLMs were prompted to infer conditional dependencies between 18 feature nodes, creating adjacency matrices representing causal graphs. These graphs were validated using Bayesian Dirichlet equivalent uniform (BDeu) scores and converted into Bayesian Networks. The study also calculated Average Treatment Effect (ATE) to measure biomarker impact on treatment outcomes. The LLM-generated models were compared against traditional optimization algorithms (NOTEARS and PC) and included human-in-the-loop refinement for improved accuracy.

## Key Results
- LLM-based approaches achieved BDeu scores of -4150 compared to -6092 for PC algorithm
- RET mutations showed the strongest effect on both chemotherapy and targeted therapy outcomes
- Immunotherapy demonstrated poor performance across all biomarkers
- LLM-generated models accurately identified causal relationships between clinical and genomic features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate valid causal graph edges by leveraging domain knowledge encoded in their pretraining
- Mechanism: LLMs infer conditional dependencies between variables and return structured causal edge predictions
- Core assumption: LLM pretraining includes sufficient medical and causal domain knowledge
- Evidence anchors: "LLMs can accurately predict the directionality of edges in causal graphs, outperforming existing state-of-the-art methods"

### Mechanism 2
- Claim: LLM-generated graphs can be validated and scored using BDeu scores
- Mechanism: Generated DAGs are converted into Bayesian Networks and scored
- Core assumption: BDeu score reliably reflects plausibility of causal graph given data
- Evidence anchors: "LLM methods vastly outperform optimization based approaches"

### Mechanism 3
- Claim: Human-in-the-loop refinement improves causal discovery accuracy
- Mechanism: Domain experts provide corrective prompts to adjust adjacency matrix
- Core assumption: Expert knowledge can be effectively encoded as corrective prompts
- Evidence anchors: "Main benefits of the provided corrections are that treatment plan and survival directly cause the outcome"

## Foundational Learning

- Concept: Causal discovery and DAGs
  - Why needed here: Essential to interpret LLM outputs and validate models
  - Quick check question: What makes a graph a DAG, and why is acyclicity critical in causal modeling?

- Concept: Bayesian network scoring (BDeu)
  - Why needed here: Used to compare and validate different causal graphs
  - Quick check question: How does BDeu score penalize model complexity, and why is this useful?

- Concept: Average Treatment Effect (ATE)
  - Why needed here: Quantifies causal impact of treatments on outcomes
  - Quick check question: How is ATE calculated from a causal DAG, and what does it represent in medical decision-making?

## Architecture Onboarding

- Component map: Data Ingestion -> Preprocessing -> LLM-based Graph Generation -> Graph Validation (BDeu) -> Human-in-the-loop Refinement -> Causal Inference (ATE) -> Results
- Critical path: Prompt -> LLM Completion -> Adjacency Matrix -> Bayesian Network -> BDeu Score -> Interpretation
- Design tradeoffs:
  - Speed vs accuracy: Single-prompt faster but may be less accurate than iterative
  - LLM model choice: General-purpose vs specialized medical LLMs for domain relevance
  - Validation method: BDeu is computationally efficient but may not capture all nuances
- Failure signatures:
  - LLM outputs inconsistent or contradictory edges
  - BDeu scores fail to differentiate between models
  - ATE calculations yield implausible or extreme values
- First 3 experiments:
  1. Generate DAG using iterative prompts; validate with BDeu and compare to NOTEARS/PC
  2. Generate DAG using single-prompt method; refine with expert corrections and re-validate
  3. Repeat experiments with a specialized medical LLM; compare BDeu scores and ATE results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would specialized medical LLMs like Med-PaLM or BioGPT compare to general-purpose LLMs like GPT-4 in terms of accuracy for causal structure learning in medical domains?
- Basis in paper: [explicit] "Future work would include looking at models that have been specially trained on medical literature such as Med-PaLM or BioGPT"
- Why unresolved: Only tested general-purpose LLMs (GPT-4), no comparison with medical-specialized LLMs
- What evidence would resolve it: Direct comparison of BDeu scores and ATE calculations using both general-purpose and medical-specialized LLMs

### Open Question 2
- Question: What additional molecular markers would most significantly improve accuracy of causal structure learning in NSCLC treatment prediction?
- Basis in paper: [explicit] "Future work would expand the amount of genomic variations being examined but also include higher level genomic markers such as TMB and MSI"
- Why unresolved: Current model only includes 18 features, limited feature set may not capture full complexity
- What evidence would resolve it: Comparative analysis of BDeu scores and ATE calculations using models with different combinations of molecular markers

### Open Question 3
- Question: How does performance of LLM-based causal discovery compare to human expert-generated causal graphs in terms of clinical utility and accuracy?
- Basis in paper: [inferred] Authors mention using "human intervention (from in house pathological doctor)" to refine LLM-generated graphs
- Why unresolved: Compares LLM methods to optimization algorithms but not directly against human expert-generated graphs
- What evidence would resolve it: Side-by-side comparison of BDeu scores and ATE calculations between LLM-generated, human expert-generated, and optimization algorithm-generated causal graphs

## Limitations

- The study focuses exclusively on one cancer type with a fixed set of genomic markers, limiting generalizability
- Potential biases in LLM pretraining data that could affect causal inference are not addressed
- While BDeu scores indicate better fit, the absolute scores remain negative, suggesting room for improvement

## Confidence

- High Confidence: Comparative advantage of LLM methods over traditional optimization approaches is well-supported by BDeu score differences
- Medium Confidence: Accuracy of specific causal relationships identified is supported by clinical literature but requires independent validation
- Medium Confidence: Superiority of iterative prompt refinement over single-prompt approaches is demonstrated but optimal number of iterations is not established

## Next Checks

1. Apply the best-performing LLM approach to a separate NSCLC dataset with different patient demographics to verify generalizability
2. Have oncology specialists independently assess the clinical plausibility of the top 10 causal relationships identified by the LLM models
3. Repeat the experiments using a medical-specific LLM (such as Med-PaLM or BioGPT) to determine if domain-specific pretraining improves causal discovery accuracy compared to general-purpose LLMs