---
ver: rpa2
title: Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted
  Trees
arxiv_id: '2309.09968'
source_url: https://arxiv.org/abs/2309.09968
tags:
- data
- missing
- learning
- imputation
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for generating and imputing
  mixed-type tabular data using score-based diffusion and conditional flow matching
  with XGBoost, a popular Gradient-Boosted Tree (GBT) method. Unlike prior methods
  that rely on neural networks, this approach leverages the strengths of XGBoost,
  which can natively handle missing data and often outperforms neural networks for
  tabular data.
---

# Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees

## Quick Facts
- arXiv ID: 2309.09968
- Source URL: https://arxiv.org/abs/2309.09968
- Reference count: 40
- Primary result: Novel approach for generating and imputing mixed-type tabular data using score-based diffusion and conditional flow matching with XGBoost, outperforming deep-learning methods in data generation tasks

## Executive Summary
This paper introduces Forest-VP and Forest-Flow, two novel methods for generating and imputing mixed-type tabular data using score-based diffusion and conditional flow matching with XGBoost instead of neural networks. The approach leverages XGBoost's native ability to handle missing data and its strong performance on tabular datasets. A comprehensive benchmark with 27 diverse datasets and 9 metrics demonstrates that these methods outperform existing deep-learning generation methods while remaining competitive in imputation tasks. The methods can be trained entirely on CPUs without requiring a GPU.

## Method Summary
The approach uses score-based diffusion or conditional flow matching with XGBoost regressors instead of neural networks. For each noise level in the diffusion process, separate XGBoost models are trained for each variable, leveraging XGBoost's native missing value handling. Data is preprocessed with min-max normalization and dummy encoding for categorical variables. The Forward process adds noise to training data, and the Reverse process generates new samples or imputes missing values using the trained models. For imputation, the REPAINT algorithm is employed. The method trains nt=50 XGBoost models per variable and uses nnoise=100 noise samples per data point during training.

## Key Results
- Forest-VP and Forest-Flow outperform deep-learning generation methods in data generation tasks
- Methods remain competitive with state-of-the-art imputation approaches
- Can be trained entirely on CPUs without GPU requirements
- Shows strong performance across 27 diverse datasets with 9 different evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XGBoost's native handling of missing data enables direct training on incomplete datasets without imputation
- Mechanism: During tree construction, XGBoost learns optimal split directions for missing values, effectively modeling the conditional distribution of missingness
- Core assumption: The missing data mechanism is Missing Completely at Random (MCAR) or Missing at Random (MAR)
- Evidence anchors:
  - [section]: "Contrary to most generative models for tabular data, our method can be trained directly on incomplete data due to XGBoost, which learns the best split for missing values"
  - [abstract]: "our method can be trained directly on incomplete data due to XGBoost, which learns the best split for missing values"
- Break condition: If data is Missing Not at Random (MNAR), the learned splits may be biased and generate unrealistic samples

### Mechanism 2
- Claim: Training separate XGBoost models per noise level preserves fine-grained score/flow information
- Mechanism: By isolating each noise level into its own model, the trees can focus on learning the specific transformation at that level without interference from other noise scales
- Core assumption: The score/flow function varies significantly across noise levels and benefits from level-specific modeling
- Evidence anchors:
  - [section]: "we train a different model per noise level t. Although this makes training slower, in practice, we find that nt = 50 is enough for great performance"
  - [abstract]: "we train a different model per noise level t"
- Break condition: If noise levels are highly correlated or the function is smooth across levels, this approach may overfit and waste resources

### Mechanism 3
- Claim: Gradient boosting provides superior function approximation for tabular data compared to neural networks
- Mechanism: XGBoost's ensemble of shallow trees can capture complex interactions in tabular data more effectively than deep networks, which struggle with tabular sparsity and categorical encoding
- Core assumption: The data follows patterns that trees can capture better than neural networks for this domain
- Evidence anchors:
  - [section]: "GBTs tend to perform better than neural networks [Shwartz-Ziv and Armon, 2022, Borisov et al., 2022a, Grinsztajn et al., 2022]"
  - [abstract]: "GBTs tend to perform better than neural networks"
- Break condition: If the data has high-dimensional continuous interactions or requires smooth interpolations, neural networks may outperform

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs) and Ordinary Differential Equations (ODEs) in diffusion models
  - Why needed here: The paper alternates between SDE-based diffusion and ODE-based flow matching; understanding both is crucial for grasping the Forward and Reverse algorithms
  - Quick check question: What's the key mathematical difference between how noise is added in SDE diffusion versus ODE flow matching?

- Concept: Gradient boosting and tree-based missing value handling
  - Why needed here: XGBoost's ability to handle missing data natively is central to the method's ability to train on incomplete datasets
  - Quick check question: How does XGBoost determine the optimal split direction for missing values during tree construction?

- Concept: Score matching and conditional flow matching losses
  - Why needed here: The method minimizes either score-matching loss (3) or conditional flow matching loss (4), which are different objective functions requiring different model architectures
  - Quick check question: What's the fundamental difference between the target functions in score matching versus conditional flow matching?

## Architecture Onboarding

- Component map: Data preprocessing (normalization + dummy encoding) -> Forward diffusion/flow with noise addition -> XGBoost training (nt models per variable) -> Reverse sampling/imputation -> Generated/imputed data
- Critical path: Data → Forward diffusion/flow → XGBoost training → Reverse sampling → Generated/imputed data
- Design tradeoffs: Training separate models per noise level increases memory usage but improves performance; using XGBoost instead of neural networks reduces GPU dependency but may limit modeling capacity
- Failure signatures: Poor Wasserstein distance to training data indicates mode collapse; high MAD in imputation suggests lack of diversity; low R2 indicates poor utility for ML tasks
- First 3 experiments:
  1. Run on Iris dataset with nt=10, nnoise=10 to verify basic functionality and debug data flow
  2. Increase to nt=50, nnoise=100 on Iris to assess performance scaling
  3. Test imputation on a dataset with 20% MCAR missingness to verify REPAINT integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of noise levels (nt) and number of noise samples per data sample (nnoise) affect the performance of the Forest-VP and Forest-Flow methods?
- Basis in paper: [explicit] The paper mentions that nt = 50 and nnoise = 100 are used as default values, but also discusses the impact of varying these hyperparameters on performance.
- Why unresolved: The paper does not provide a comprehensive analysis of how different values of nt and nnoise affect the performance across all datasets and tasks.
- What evidence would resolve it: A systematic study varying nt and nnoise across different datasets and tasks, showing the impact on metrics like Wasserstein distance, coverage, and efficiency.

### Open Question 2
- Question: Can the Forest-VP and Forest-Flow methods be effectively adapted to handle more complex data types, such as time series or graph data?
- Basis in paper: [inferred] The paper focuses on tabular data and does not explore the applicability of the methods to other data types.
- Why unresolved: The paper does not provide any experiments or analysis on the performance of the methods on non-tabular data types.
- What evidence would resolve it: Experiments applying the methods to time series or graph data, comparing performance with existing methods designed for those data types.

### Open Question 3
- Question: How does the performance of the Forest-VP and Forest-Flow methods compare to deep learning methods when trained on larger datasets?
- Basis in paper: [explicit] The paper mentions that the methods can be trained on CPUs without a GPU, but does not compare performance with deep learning methods on large datasets.
- Why unresolved: The paper does not provide a comparison of the methods with deep learning methods on datasets with a large number of samples or features.
- What evidence would resolve it: Experiments comparing the methods with deep learning methods on large datasets, measuring metrics like training time, memory usage, and performance.

### Open Question 4
- Question: How does the Forest-VP and Forest-Flow methods handle missing data patterns that are not Missing Completely at Random (MCAR)?
- Basis in paper: [explicit] The paper mentions that the methods can handle missing data but does not explore different missing data patterns.
- Why unresolved: The paper does not provide experiments or analysis on the performance of the methods under different missing data mechanisms (e.g., Missing at Random, Missing Not at Random).
- What evidence would resolve it: Experiments simulating different missing data patterns and comparing the performance of the methods with existing imputation methods under those conditions.

## Limitations
- Limited theoretical justification for why XGBoost-based approach outperforms neural network alternatives
- Computational efficiency claims may not scale to very large datasets given 50 models per variable requirement
- Lack of direct corpus support for core mechanisms beyond empirical benchmarks

## Confidence
- **High confidence**: Empirical results showing state-of-the-art performance on data generation tasks; basic XGBoost missing value handling mechanism
- **Medium confidence**: Claims about XGBoost outperforming neural networks for tabular data; CPU-only training efficiency
- **Low confidence**: Theoretical explanations for why the approach works better than alternatives; scalability claims for large datasets

## Next Checks
1. **Mechanism validation**: Test the method on datasets with known MNAR missingness patterns to verify the break condition of Mechanism 1 - if performance degrades significantly, this confirms the MCAR/MAR assumption is critical.

2. **Computational scaling test**: Benchmark memory usage and training time on progressively larger datasets (10K → 100K → 1M rows) to empirically verify the CPU efficiency claims and identify scaling limits.

3. **Ablation study**: Compare performance using single XGBoost model across all noise levels versus separate models per level to quantify the actual benefit of Mechanism 2 and determine if the computational overhead is justified.