---
ver: rpa2
title: Open-source Large Language Models are Strong Zero-shot Query Likelihood Models
  for Document Ranking
arxiv_id: '2310.13243'
source_url: https://arxiv.org/abs/2310.13243
tags:
- zero-shot
- ranking
- abcdef
- llms
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the zero-shot ranking effectiveness of
  large language models (LLMs) when used as query likelihood models (QLMs) for document
  ranking. The study focuses on LLMs that are pre-trained solely on unstructured text
  data without supervised instruction fine-tuning.
---

# Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking

## Quick Facts
- arXiv ID: 2310.13243
- Source URL: https://arxiv.org/abs/2310.13243
- Authors: 
- Reference count: 21
- Pre-trained LLMs without instruction fine-tuning show strong zero-shot ranking capabilities

## Executive Summary
This paper investigates the zero-shot ranking effectiveness of large language models (LLMs) when used as query likelihood models (QLMs) for document ranking. The study focuses on LLMs that are pre-trained solely on unstructured text data without supervised instruction fine-tuning. Results show that these LLMs possess strong zero-shot ranking capabilities, and additional instruction fine-tuning may hinder effectiveness unless a question generation task is included in the fine-tuning dataset. The paper introduces a novel state-of-the-art ranking system that combines LLM-based QLMs with a hybrid zero-shot retriever, achieving exceptional effectiveness in both zero-shot and few-shot scenarios.

## Method Summary
The method involves harnessing state-of-the-art transformer decoder-only LLMs, such as LLaMA, which have undergone pre-training solely on unstructured text through unsupervised next token prediction. These models are used to estimate query likelihood scores for documents without requiring fine-tuning on relevance judgments. The approach can be combined with first-stage retrieval using interpolated BM25 and dense retrievers, and optionally interpolated with BM25 scores. The paper also introduces a "Guided by Bad Questions" (GBQ) prompt template to enhance few-shot ranking performance.

## Key Results
- Pre-trained-only LLMs outperform instruction-tuned versions in zero-shot ranking scenarios
- Instruction fine-tuning harms QLM effectiveness unless question generation tasks are included
- Hybrid zero-shot retriever (BM25 + HyDE) improves first-stage retrieval quality
- LLM-based QLM achieves state-of-the-art performance in zero-shot and few-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot QLM effectiveness emerges from LLMs' pre-training on unstructured text alone, without requiring supervised instruction fine-tuning.
- Mechanism: The LLM's next-token prediction training enables it to implicitly learn document-query relevance patterns, which can be leveraged for ranking through query likelihood estimation.
- Core assumption: The pre-training corpus contains sufficient document-query relevance signal for the LLM to capture without explicit supervision.
- Evidence anchors:
  - [abstract]: "Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset."
  - [section]: "Our approach involves harnessing the power of state-of-the-art transformer decoder-only LLMs, such as LLaMA (Touvron et al., 2023), which have undergone pre-training solely on unstructured text through unsupervised next token prediction."
  - [corpus]: Found 25 related papers. The neighbor papers focus on improving zero-shot LLM re-ranking efficiency and effectiveness, supporting the importance of this research direction.
- Break condition: If the pre-training corpus lacks diverse query-document pairs or relevance patterns, the zero-shot effectiveness would degrade significantly.

### Mechanism 2
- Claim: Instruction fine-tuning can harm QLM ranking effectiveness when the fine-tuning data lacks question generation tasks.
- Mechanism: Instruction tuning shifts the model's focus toward following task instructions rather than extracting relevance signals from document content.
- Core assumption: The instruction fine-tuning process changes the model's internal representation priorities in a way that conflicts with QLM ranking objectives.
- Evidence anchors:
  - [abstract]: "Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset."
  - [section]: "Our hypothesis to this unexpected finding is that instruction-tuned models tend to pay more attention to the task instructions and less attention to the input content itself."
  - [corpus]: The related work section includes papers on improving zero-shot LLM re-ranking, suggesting this is an active research area where instruction tuning effects are being investigated.
- Break condition: If instruction fine-tuning datasets include question generation tasks, the negative impact on QLM ranking could be mitigated or reversed.

### Mechanism 3
- Claim: Combining sparse (BM25) and dense (HyDE) retrievers through interpolation improves first-stage retrieval, which enhances downstream QLM re-ranking effectiveness.
- Mechanism: Sparse and dense retrievers capture complementary aspects of relevance - lexical matching and semantic similarity respectively - and their combination provides more diverse candidate documents for re-ranking.
- Core assumption: The interpolation weight (α=0.5) appropriately balances the contributions of both retriever types without requiring extensive tuning.
- Evidence anchors:
  - [section]: "Our results suggest that the effectiveness of zero-shot first-stage retrieval can be improved by simply interpolating sparse and dense retrievers."
  - [section]: "Moreover, after QLM re-ranking, the nDCG@10 values surpass those in Table 1. This indicates that zero-shot QLM re-rankers benefit from a stronger first-stage retriever."
  - [corpus]: The related papers focus on improving zero-shot re-ranking efficiency, supporting the importance of optimizing the retrieval-reranking pipeline.
- Break condition: If the interpolated retriever introduces significant noise or if the re-ranker cannot effectively distinguish between high-quality and low-quality candidates from the combined set.

## Foundational Learning

- Concept: Query Likelihood Models (QLMs)
  - Why needed here: The paper's core contribution is adapting LLMs as QLMs for document ranking, so understanding how QLMs work is fundamental.
  - Quick check question: How does a QLM estimate document relevance differently from traditional ranking models?

- Concept: Zero-shot learning
  - Why needed here: The paper specifically investigates zero-shot ranking capabilities, distinguishing it from transfer learning approaches.
  - Quick check question: What distinguishes zero-shot from few-shot learning in the context of language model ranking?

- Concept: Instruction fine-tuning effects
  - Why needed here: The paper reveals that instruction fine-tuning can harm effectiveness unless specific tasks are included, which is a non-intuitive finding.
  - Quick check question: Why might instruction fine-tuning that improves instruction-following ability actually reduce ranking effectiveness?

## Architecture Onboarding

- Component map: First-stage retriever (BM25 + HyDE interpolation) -> QLM re-ranker (LLM-based query likelihood estimation) -> Interpolation layer (linear combination of BM25 scores and QLM scores) -> Few-shot enhancement (Guided by Bad Questions prompt template)

- Critical path: 1. Retrieve top-k documents using interpolated BM25+HyDE 2. For each document, compute QLM score using LLM query likelihood estimation 3. Interpolate QLM scores with BM25 scores 4. Sort documents by final interpolated scores

- Design tradeoffs:
  - Model size vs. effectiveness: Larger LLMs generally perform better but require more resources
  - Instruction fine-tuning vs. zero-shot: Pre-trained-only models outperform instruction-tuned ones unless QG tasks are included
  - First-stage retrieval quality vs. computational cost: Better retrievers improve final rankings but increase latency

- Failure signatures:
  - Poor ranking effectiveness despite using large LLMs: Likely due to inappropriate instruction fine-tuning or insufficient document-query signal in pre-training data
  - High variance in ranking results: May indicate instability in LLM query likelihood estimation
  - Computational bottlenecks: LLM inference costs can be prohibitive for large-scale applications

- First 3 experiments:
  1. Compare zero-shot ranking effectiveness of LLaMA-7B vs LLaMA-13B on a small dataset to verify size-effectiveness relationship
  2. Test instruction-tuned vs pre-trained-only versions of the same model (e.g., Falcon-7B vs Falcon-7B-instruct) to confirm fine-tuning effects
  3. Evaluate different interpolation weights (α values) between BM25 and QLM scores to understand sensitivity to this parameter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of zero-shot LLM-based QLMs compare to fine-tuned PLM-based rankers in terms of ranking effectiveness and computational efficiency?
- Basis in paper: [explicit] The paper compares zero-shot LLM-based QLMs with fine-tuned PLM-based rankers like monoT5-3B and monoT5-3B-InPars-v2, showing that fine-tuned rankers consistently outperform zero-shot QLMs across all datasets except for T5-QLM-large.
- Why unresolved: The paper does not provide a comprehensive analysis of the computational efficiency of zero-shot LLM-based QLMs compared to fine-tuned PLM-based rankers. Additionally, the paper focuses on a subset of the BEIR dataset, which may not be representative of all information retrieval scenarios.
- What evidence would resolve it: Conducting experiments on a larger and more diverse set of datasets, as well as measuring the computational efficiency of both zero-shot LLM-based QLMs and fine-tuned PLM-based rankers, would provide a more comprehensive comparison.

### Open Question 2
- Question: How does the effectiveness of zero-shot LLM-based QLMs change when using different first-stage retrievers, such as sparse or dense retrievers, in the two-stage ranking pipeline?
- Basis in paper: [inferred] The paper combines BM25 and HyDE as the zero-shot first-stage retriever and observes improved effectiveness when using a stronger first-stage retriever. However, the paper does not explore the impact of using different first-stage retrievers on the effectiveness of zero-shot LLM-based QLMs.
- Why unresolved: The paper focuses on a specific combination of first-stage retrievers (BM25 + HyDE) and does not provide a comprehensive analysis of the impact of using different first-stage retrievers on the effectiveness of zero-shot LLM-based QLMs.
- What evidence would resolve it: Conducting experiments with different first-stage retrievers, such as sparse and dense retrievers, and comparing their impact on the effectiveness of zero-shot LLM-based QLMs would provide insights into the optimal combination of first-stage retrievers and zero-shot LLM-based QLMs.

### Open Question 3
- Question: How does the effectiveness of zero-shot LLM-based QLMs change when using different prompt templates for guiding the LLMs to generate more accurate query likelihood estimations?
- Basis in paper: [inferred] The paper uses a specific prompt template called "Guided by Bad Questions" (GBQ) to guide the LLM-based QLMs to produce more accurate query likelihood estimations. However, the paper does not explore the impact of using different prompt templates on the effectiveness of zero-shot LLM-based QLMs.
- Why unresolved: The paper focuses on a specific prompt template and does not provide a comprehensive analysis of the impact of using different prompt templates on the effectiveness of zero-shot LLM-based QLMs.
- What evidence would resolve it: Conducting experiments with different prompt templates and comparing their impact on the effectiveness of zero-shot LLM-based QLMs would provide insights into the optimal prompt templates for guiding LLMs to generate more accurate query likelihood estimations.

## Limitations

- The empirical evaluation is limited to a subset of BEIR datasets, which may not generalize to other domains or query distributions
- The paper doesn't explicitly examine what specific patterns in the pre-training corpus enable zero-shot ranking capability
- The interpolation weight (α=0.5) is presented as effective without systematic sensitivity analysis across different dataset characteristics

## Confidence

- **High confidence**: The empirical observation that pre-trained-only LLMs outperform instruction-tuned versions in zero-shot ranking scenarios
- **Medium confidence**: The mechanism that instruction fine-tuning shifts attention away from content toward instructions
- **Medium confidence**: The effectiveness of interpolating BM25 and HyDE retrievers for first-stage retrieval

## Next Checks

1. Cross-domain validation: Evaluate the zero-shot QLM approach on datasets outside the BEIR benchmark to test generalizability of the findings.

2. Attention pattern analysis: Use attention visualization techniques to empirically verify whether instruction-tuned models indeed attend less to document content during ranking compared to pre-trained-only models.

3. Pre-training corpus analysis: Analyze the frequency and distribution of query-document pairs in the pre-training corpora of the evaluated LLMs to quantify the availability of relevance signal that enables zero-shot ranking.