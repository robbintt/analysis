---
ver: rpa2
title: Large Language Models are Zero Shot Hypothesis Proposers
arxiv_id: '2311.05965'
source_url: https://arxiv.org/abs/2311.05965
tags:
- hypothesis
- background
- llms
- scientific
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  generate novel scientific hypotheses. A dataset of biomedical literature is constructed
  and split by publication date to ensure unseen test data.
---

# Large Language Models are Zero Shot Hypothesis Proposers
## Quick Facts
- arXiv ID: 2311.05965
- Source URL: https://arxiv.org/abs/2311.05965
- Reference count: 40
- Primary result: LLMs generate novel biomedical hypotheses in zero-shot settings, often outperforming few-shot and fine-tuning.

## Executive Summary
This paper investigates whether large language models (LLMs) can generate novel scientific hypotheses by proposing and evaluating them on biomedical literature. The authors construct a dataset split by publication date to ensure unseen test data, then evaluate zero-shot, few-shot, and fine-tuning settings across open-source and API-based LLMs, with and without multi-agent collaboration. Surprisingly, zero-shot generation often outperforms few-shot, and introducing uncertainty via multi-agent roles enhances hypothesis diversity and quality. However, automatic metrics show low correlation with human evaluations, highlighting the need for better evaluation methods.

## Method Summary
The study uses a temporally partitioned biomedical literature dataset (training: before Jan 2023; seen test: before Jan 2023; unseen test: Aug 2023). LLMs are evaluated in zero-shot, few-shot (similarity-retrieval or random), and fine-tuning settings. Multi-agent collaboration uses roles (Analyst, Engineer, Scientist, Critic) to iteratively refine hypotheses. Hypothesis quality is assessed via BLEU/ROUGE, four custom metrics (novelty, relevance, significance, verifiability), and human/ChatGPT evaluations.

## Key Results
- Zero-shot generation often outperforms few-shot and fine-tuning in novelty and verifiability.
- Multi-agent collaboration enhances hypothesis quality and diversity by introducing uncertainty.
- Automatic metrics (BLEU/ROUGE) poorly correlate with human evaluations, especially for zero-shot hypotheses.
- Temporal data partitioning ensures genuine zero-shot evaluation by preventing model exposure to test hypotheses.

## Why This Works (Mechanism)
### Mechanism 1
Zero-shot generation outperforms few-shot due to reduced model constraint by external examples. In zero-shot, LLMs rely solely on internal knowledge representations, avoiding overfitting to provided examples that may bias or limit creative hypothesis generation. Core assumption: Internal knowledge contains diverse and valid biomedical associations not explicitly shown in few-shot prompts.

### Mechanism 2
Multi-agent collaboration enhances hypothesis quality by introducing uncertainty and iterative refinement. Diverse agent roles simulate real-world research cycles, each adding perspectives and feedback that broaden hypothesis scope and rigor. Core assumption: Role-based task decomposition improves both creativity and validation compared to single-agent approaches.

### Mechanism 3
Temporal data partitioning ensures zero-shot evaluation by preventing model exposure to test hypotheses. Splitting datasets by publication date creates unseen test sets, guaranteeing that generated hypotheses are novel relative to model training data. Core assumption: Models are trained only on literature before a fixed cutoff date, making post-cutoff literature effectively invisible.

## Foundational Learning
- **Concept:** Temporal data partitioning for zero-shot evaluation
  - Why needed: Ensures that generated hypotheses are truly novel and not memorized, enabling genuine zero-shot assessment.
  - Quick check: How does splitting by publication date prevent models from seeing test hypotheses during training?

- **Concept:** Role-based task decomposition in multi-agent systems
  - Why needed: Simulates collaborative scientific discovery, enhancing both creative generation and critical evaluation of hypotheses.
  - Quick check: What distinct function does each agent role (Analyst, Engineer, Scientist, Critic) serve in the hypothesis generation loop?

- **Concept:** Evaluation metrics beyond word overlap
  - Why needed: Hypothesis generation is an open-ended task where word overlap metrics like BLEU/ROUGE fail to capture novelty, significance, and scientific validity.
  - Quick check: Why might high BLEU scores not correlate with high human ratings for scientific hypotheses?

## Architecture Onboarding
- **Component map:** Dataset builder -> Prompt generator -> LLM evaluator -> Multi-agent collaboration engine -> Evaluation pipeline
- **Critical path:**
  1. Construct temporally partitioned dataset.
  2. Generate prompts and evaluate across models.
  3. Fine-tune top performers if needed.
  4. Deploy multi-agent collaboration for further exploration.
  5. Evaluate outputs with both automatic and human-in-the-loop methods.
- **Design tradeoffs:** Zero-shot vs. few-shot (novelty for verifiability), single-agent vs. multi-agent (simplicity for richer refinement), automatic vs. human evaluation (speed for nuanced judgment).
- **Failure signatures:** Low novelty scores despite high BLEU (model overfitting), poor correlation between automatic and human metrics (need better evaluation design), multi-agent stagnation (feedback loops too narrow).
- **First 3 experiments:**
  1. Run zero-shot generation on unseen test set; compare novelty/relevance/significance scores.
  2. Apply similarity-retrieval few-shot prompts; measure changes in novelty vs. verifiability.
  3. Deploy basic two-agent (Scientist + Critic) collaboration; evaluate improvement in hypothesis coherence.

## Open Questions the Paper Calls Out
### Open Question 1
How does the correlation between ChatGPT-based evaluations and human evaluations vary across different scientific domains? The paper focuses on biomedical literature and does not investigate whether the correlation holds across other scientific domains.

### Open Question 2
What is the optimal balance between uncertainty and structure in multi-agent collaboration for hypothesis generation? The paper introduces multi-agent collaboration but does not systematically explore how varying levels of uncertainty and structure affect performance.

### Open Question 3
How do different types of external knowledge (e.g., few-shot examples vs. domain adaptation) differentially impact hypothesis generation quality? The paper compares few-shot examples and domain adaptation but does not deeply analyze their differential impacts on hypothesis generation.

### Open Question 4
What are the long-term effects of using LLMs for hypothesis generation on scientific discovery processes? The paper demonstrates that LLMs can generate novel hypotheses, but does not explore long-term implications.

## Limitations
- Single domain (biomedical literature) and specific temporal split limit generalizability.
- Evaluation relies on ChatGPT and human annotators, raising concerns about potential evaluator bias and subjectivity.
- Automatic metrics poorly correlate with human evaluations, but it remains unclear whether this reflects metric limitations or the nature of open-ended hypothesis generation.

## Confidence
- **High Confidence:** Temporal data partitioning effectively ensures zero-shot evaluation by preventing model exposure to test hypotheses.
- **Medium Confidence:** Zero-shot generation outperforms few-shot in novelty, though this may depend on domain and model architecture.
- **Low Confidence:** Multi-agent collaboration consistently enhances hypothesis quality across all settings and domains.

## Next Checks
1. Replicate the study using datasets from non-biomedical domains (e.g., physics, social sciences) to test generalizability of findings.
2. Conduct ablation studies on the multi-agent framework to isolate which agent roles contribute most to hypothesis quality improvements.
3. Develop and test alternative evaluation metrics specifically designed for open-ended scientific hypothesis generation tasks.