---
ver: rpa2
title: How to guess a gradient
arxiv_id: '2312.04709'
source_url: https://arxiv.org/abs/2312.04709
tags:
- gradient
- guess
- accuracy
- cosine
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that neural network gradients have structured,
  low-dimensional properties that can be exploited to make better gradient estimates
  without backpropagation. By analyzing the chain rule across layers, the authors
  demonstrate that gradients depend predictably on architecture and incoming features,
  allowing them to "guess" gradient directions with much higher cosine similarity
  to the true gradient than random guessing.
---

# How to guess a gradient

## Quick Facts
- arXiv ID: 2312.04709
- Source URL: https://arxiv.org/abs/2312.04709
- Authors: 
- Reference count: 39
- This paper shows neural network gradients have structured, low-dimensional properties that can be exploited to make better gradient estimates without backpropagation.

## Executive Summary
This paper introduces methods to estimate neural network gradients without backpropagation by exploiting the structured, low-dimensional properties of gradients. The authors demonstrate that gradients depend predictably on architecture and incoming features, allowing them to "guess" gradient directions with much higher cosine similarity to true gradients than random guessing. Their proposed methods achieve 100-1000× higher cosine similarity than directional descent and significantly improve training performance on MLPs and the LocalMixer architecture. While these advances narrow the gap with backpropagation, they still fall short of full gradient-based optimization performance.

## Method Summary
The paper proposes several gradient guessing methods that exploit the chain rule structure of neural networks. These methods use forward-mode differentiation (Jacobian-vector products) to estimate directional derivatives, then scale random perturbations by these measurements to generate gradient estimates. The key insight is that gradients lie in predictable low-dimensional subspaces determined by architecture and incoming features. Methods include activation perturbation (random perturbations constrained by architectural knowledge), activation mixing (random mixtures of activations as gradient guesses), W^T (confining guesses to the range space of W^T), and 1-layer downstream (propagating perturbations through one layer of the network). The methods trade off bias for variance reduction compared to unbiased but high-variance directional descent.

## Key Results
- Gradient guessing methods achieve 100-1000× higher cosine similarity to true gradients compared to directional descent
- W^T method with 1-layer downstream achieves 81.0% test accuracy on LocalMixer architecture (vs 80.6% for backpropagation)
- Methods show "self-sharpening" where gradient guesses improve over training due to decreasing weight matrix rank
- Gradient guessing generalizes better than backpropagation on LocalMixer despite lower training accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural network gradients lie in predictable low-dimensional subspaces determined by architecture and incoming features.
- Mechanism: By analyzing the chain rule across layers, gradients can be decomposed into outer products of future layer gradients and current layer activations, dramatically reducing the effective dimensionality from O(n²) to O(n).
- Core assumption: The future gradient ∂L/∂s_i is approximately low-dimensional and can be estimated using random perturbations constrained by architectural knowledge.
- Evidence anchors:
  - [abstract] "Gradients lie in a predictable low-dimensional subspace which depends on the network architecture and incoming features."
  - [section 3.2] "By 'unfolding' the computation, we can identify exploitable information to refine our gradient guesses. Considering the computation at layer i, where s_i = W_i x_i represents the pre-activations, applying the chain rule reveals a crucial insight: ∂L/∂W_i ∈ R^(n×n) is essentially the outer product of the (unknown) gradient at future layers ∂L/∂s_i ∈ R^n and the (known) incoming activations x_i ∈ R^n"
  - [corpus] Weak: No direct corpus support for this specific chain rule decomposition mechanism.
- Break condition: If the future gradient ∂L/∂s_i becomes high-dimensional (e.g., with complex loss functions or non-standard architectures), the dimensionality reduction benefit disappears.

### Mechanism 2
- Claim: Activation functions introduce structured sparsity in gradients that can be exploited for better guessing.
- Mechanism: ReLU activations create diagonal Jacobian matrices that zero out certain gradient components, allowing us to focus guessing only on surviving entries rather than the full gradient vector.
- Core assumption: The sparsity pattern introduced by ReLU activations remains consistent enough during training to provide useful constraints.
- Evidence anchors:
  - [section 3.2] "Our second insight is that by the very nature of ReLU activations, the Jacobian matrix ∂ReLU(s_i)/∂s_i ∈ R^(n×n) will be a sparse diagonal matrix. It is diagonal since each input controls one and only one output. Furthermore, this matrix will also typically 'zero out' some entries of the incoming gradient."
  - [section 3.2] "This suggests that we should 'guess' only the surviving entries of ∂L/∂x_i+1, as determined by that sparse and diagonal matrix (known at guess-time)."
  - [corpus] Weak: No corpus support for this specific ReLU-induced sparsity exploitation.
- Break condition: If activation functions change (e.g., using GELU or other non-sparse activations) or if the sparsity pattern becomes too irregular, this mechanism loses effectiveness.

### Mechanism 3
- Claim: Weight matrices often have low effective rank, allowing gradient guesses to be constrained to the image of W^T.
- Mechanism: By multiplying random guesses with W^T, we confine the guess to the range space of W^T, which is typically low-rank, further reducing the effective dimensionality of the guessing problem.
- Core assumption: The weight matrices W_i maintain low effective rank throughout training, or at least their singular value distributions remain skewed.
- Evidence anchors:
  - [section 3.2] "But we know that it will immediately be multiplied by W^T_i+1. While this does not necessarily give a 'hard' constraint on our guess, our third insight is that W^T_i+1 often effectively has low rank (Huh et al., 2023)."
  - [section 5] "For our proposed estimators, this is easily shown to be false. Activation mixing uses random mixtures of activations as the gradient guesses, and thus its covariance matrix is the same as the covariance matrix of the activations (and thus non-identity)."
  - [corpus] Weak: Only indirect support from cited work (Huh et al., 2023) which is not in the provided text.
- Break condition: If weight matrices become full-rank (e.g., with specific initialization schemes or regularization) or if the effective rank increases significantly during training, this mechanism loses its advantage.

## Foundational Learning

- Concept: Chain rule and automatic differentiation
  - Why needed here: The entire paper builds on decomposing gradients using the chain rule across multiple layers, which is fundamental to understanding how to "guess" gradients effectively.
  - Quick check question: Can you write out the chain rule decomposition for ∂L/∂W_i in terms of future layer gradients and current activations?

- Concept: Jacobian-vector products (JVP) and forward-mode differentiation
  - Why needed here: The paper relies on computing directional derivatives efficiently using JVPs, which is the computational primitive that enables gradient guessing without backpropagation.
  - Quick check question: How does a JVP computation differ from a standard forward pass, and why is it more efficient for gradient estimation?

- Concept: Dimensionality reduction and effective rank
  - Why needed here: The paper's core insight is that gradients lie in much lower-dimensional subspaces than their ambient parameter space, and understanding this requires familiarity with concepts like effective rank and principal component analysis.
  - Quick check question: Given a weight matrix W with singular values [10, 8, 2, 0.1], what is its approximate effective rank and why does this matter for gradient guessing?

## Architecture Onboarding

- Component map:
  - Input layer → Linear layer → ReLU activation → Linear layer → ReLU activation → ... → Loss function
  - Each linear layer has associated weight matrix W_i, pre-activation s_i = W_i x_i, and post-activation x_i+1 = ReLU(s_i)
  - The gradient guessing methods operate on each layer independently using knowledge from downstream layers

- Critical path:
  1. Forward pass to compute activations
  2. For each layer i, generate random perturbation vector
  3. Apply architectural constraints (W^T multiplication, ReLU mask application)
  4. Compute JVP to measure directional derivative
  5. Scale perturbation by JVP to get gradient estimate
  6. Convert pre-activation gradient to weight gradient using outer product

- Design tradeoffs:
  - Pre-activation perturbation vs. weight perturbation: Pre-activation allows batch-level parallelism but requires additional computation to convert to weight updates
  - Number of downstream layers to backpropagate: More layers reduce variance but increase bias
  - Random guessing vs. structured guessing: Random guessing is unbiased but high-variance; structured guessing reduces variance but introduces bias

- Failure signatures:
  - Very low cosine similarity (< 0.01) suggests architectural constraints aren't being properly applied
  - Unstable training with large gradient estimates suggests improper scaling or JVP computation errors
  - No improvement over directional descent suggests the bias-variance tradeoff isn't favorable for the current architecture

- First 3 experiments:
  1. Implement pre-activation perturbation for a 2-layer MLP on MNIST and measure cosine similarity vs. directional descent
  2. Add W^T constraint to the same setup and compare cosine similarity improvement
  3. Test 1-layer downstream method on the same architecture and compare training performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the bias in gradient guessing methods be mathematically corrected using the covariance structure of activations?
- Basis in paper: [explicit] The authors analyze that activation mixing and W^T methods have non-identity covariance matrices that introduce bias, and discuss this in Section 5 and the supplementary materials.
- Why unresolved: While the authors identify the source of bias and its mathematical form, they do not demonstrate practical methods to remove this bias or show whether de-biasing would significantly improve performance.
- What evidence would resolve it: Experimental results showing de-biased gradient guessing methods achieving comparable or better performance than backpropagation, along with theoretical analysis proving the bias can be fully or partially corrected.

### Open Question 2
- Question: What causes the self-sharpening phenomenon and can it be controlled to maintain good generalization?
- Basis in paper: [explicit] The authors observe that training with certain gradient guessing methods leads to increasing cosine similarity and training accuracy over time, but poor test performance, as shown in Figure 5 and Table 2.
- Why unresolved: The authors hypothesize this is due to decreasing rank of weight matrices but do not provide a complete theoretical explanation or methods to control this effect.
- What evidence would resolve it: A mechanistic understanding of how self-sharpening emerges from the optimization dynamics, plus experiments showing controlled self-sharpening that maintains both high training accuracy and good generalization.

### Open Question 3
- Question: Why do gradient guessing methods generalize better than backpropagation on the LocalMixer architecture?
- Basis in paper: [explicit] The authors note in Table 1 that their W^T method achieves higher test accuracy than backpropagation on LocalMixer despite lower training accuracy.
- Why unresolved: This is an unexpected finding that contradicts typical understanding of generalization, and the authors do not provide a theoretical explanation for why gradient guessing would lead to better generalization in this specific architecture.
- What evidence would resolve it: Analysis of the optimization trajectories and loss landscapes for both methods on LocalMixer, potentially revealing architectural features that make gradient guessing more conducive to generalization.

### Open Question 4
- Question: What is the fundamental limitation preventing gradient guessing methods from matching backpropagation performance on standard MLPs?
- Basis in paper: [explicit] The authors observe a consistent gap between gradient guessing and backpropagation across multiple experiments, particularly for complex datasets like CIFAR100.
- Why unresolved: While the authors identify bias as a major factor, they do not fully characterize whether bias is the only limitation or if there are other fundamental differences between the optimization dynamics of gradient guessing versus backpropagation.
- What evidence would resolve it: Comparative analysis of the effective learning rates, curvature information, and convergence properties of both methods, along with experiments testing whether removing bias alone closes the performance gap.

## Limitations

- The gradient guessing methods still fall short of backpropagation performance on standard MLPs, particularly for complex datasets like CIFAR100
- The self-sharpening phenomenon leads to poor generalization despite improving training performance
- The methods rely heavily on empirical observations rather than theoretical guarantees about gradient structure

## Confidence

- **High Confidence**: The empirical demonstrations of improved cosine similarity over directional descent, and the basic mechanism of using JVPs for gradient estimation are well-supported.
- **Medium Confidence**: The claims about dimensionality reduction and the effectiveness of architectural constraints are plausible but need more theoretical backing and broader empirical validation.
- **Low Confidence**: The self-sharpening phenomenon and its implications for long-term training dynamics are interesting but insufficiently explored.

## Next Checks

1. **Theoretical Analysis**: Prove bounds on the dimensionality of gradient subspaces for different architectures and loss functions to validate the O(n) vs O(n²) claim.

2. **Architecture Generalization**: Test the gradient guessing methods on more diverse architectures (Transformers, CNNs, ResNets) to determine if the observed improvements generalize beyond MLPs and LocalMixer.

3. **Loss Function Sensitivity**: Evaluate how different loss functions (cross-entropy, MSE, contrastive losses) affect the effectiveness of gradient guessing, particularly for the dimensionality reduction claims.