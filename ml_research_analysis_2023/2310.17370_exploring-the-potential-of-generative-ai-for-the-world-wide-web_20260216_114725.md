---
ver: rpa2
title: Exploring the Potential of Generative AI for the World Wide Web
arxiv_id: '2310.17370'
source_url: https://arxiv.org/abs/2310.17370
tags:
- image
- images
- diffusion
- webpages
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of generative AI, specifically stable
  diffusion, for image generation on the World Wide Web. The authors develop a tool
  called WebDiffusion to simulate a Web powered by stable diffusion from both client
  and server perspectives.
---

# Exploring the Potential of Generative AI for the World Wide Web

## Quick Facts
- **arXiv ID**: 2310.17370
- **Source URL**: https://arxiv.org/abs/2310.17370
- **Reference count**: 40
- **Key outcome**: Generative AI can produce high-quality, relevant web images using contextual information, though in-browser generation remains challenging for performance.

## Executive Summary
This paper explores using stable diffusion for AI-generated web images, examining both client-side and server-side approaches. The authors developed WebDiffusion to simulate an AI-powered web, collecting 409 AI-generated images from 60 webpages and evaluating them through crowdsourcing. Results show generative AI can produce pertinent, high-quality web images (70-95% scoring "fair" or higher) without requiring manual prompt input, simply by leveraging webpage contextual information. However, direct in-browser generation faces performance challenges, with only powerful GPUs able to compete with traditional downloads, making the approach currently viable mainly for applications like webpage repairing or handling private content.

## Method Summary
The researchers developed WebDiffusion, a tool that crawls webpages, generates annotations, simulates AI-powered webpages via proxy, and collects user feedback. They used two annotation approaches: client-based (extracting alt text and surrounding HTML context) and server-based (using Microsoft's GenerativeImage2Text transformer with the original image). Stable diffusion (SDXL) generated images with 20 inference steps. The team evaluated 409 images from 60 randomly selected webpages using crowdsourcing, measuring feasibility (annotation quality), quality (user scores), and performance (SpeedIndex, page load time, bandwidth) across different GPU types.

## Key Results
- 70-95% of AI-generated images scored "fair" or higher in quality
- Server-based annotations achieved 95% of images scoring "fair" or higher, compared to 70% for client-based
- 63% (A40 GPU) and 87% (A100 GPU) of websites showed SpeedIndex improvements with AI generation
- Client-based annotations successfully generated relevant annotations for 82% of images

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Stable diffusion can generate high-quality web images without manual prompt input by leveraging contextual webpage information.
- **Mechanism**: The system extracts contextual text (alt text, surrounding div content) and combines it with automated image captioning to create textual prompts that guide image generation.
- **Core assumption**: Webpage context contains sufficient semantic information to describe images accurately.
- **Evidence anchors**: 
  - [abstract] "Our findings suggest that generative AI is already capable of producing pertinent and high-quality Web images, even without requiring Web designers to manually input prompts, just by leveraging contextual information available within the webpages."
  - [section 4.1] "The output of the GenerativeImage2Text transformer is instead highly accurate – given their usage of BLIP image-captioning technique which is shown to outperform all state-of-the-art techniques"
- **Break condition**: If webpage context lacks descriptive text (common with ~50% of images missing alt text), the quality of generated images degrades significantly.

### Mechanism 2
- **Claim**: Server-based image generation outperforms client-based due to access to original image content.
- **Mechanism**: Server-side generation uses both contextual text and the actual image through GenerativeImage2Text, providing more accurate prompts than client-side which relies only on available webpage text.
- **Core assumption**: Access to original image content significantly improves prompt quality and thus image generation quality.
- **Evidence anchors**:
  - [section 4.1] "The server-based approach achieves the highest quality, thanks to the additional contextual information available."
  - [section 4.2] "95% of the images are scored as either 'fair' or higher... and 95% of webpages are scored as 'good' or higher" for server-based vs 70% for client-based
- **Break condition**: If original image content is unavailable or if server-based approach adds no additional contextual value, performance difference diminishes.

### Mechanism 3
- **Claim**: Powerful GPUs (A40, A100) can compete with traditional image downloads for SpeedIndex metrics.
- **Mechanism**: Image generation latency can be offset by transport protocol convergence time and the fact that only a few images are loaded above the fold, making local generation viable for visible content.
- **Core assumption**: The overhead of image generation can be hidden within the total page load time, especially for above-the-fold content.
- **Evidence anchors**:
  - [section 4.3] "63% (A40) and 87% (A100) of the websites benefit from a speedup even in presence of a very fast network" for SpeedIndex
  - [section 4.3] "This happens because, on average, only 2-5 images are loaded above the fold"
- **Break condition**: If the number of images to generate increases significantly or if GPUs are less powerful, the performance advantage disappears.

## Foundational Learning

- **Concept**: Stable diffusion and latent diffusion models
  - **Why needed here**: Understanding the image generation technology is crucial for optimizing the pipeline and knowing its limitations
  - **Quick check question**: What is the key difference between standard diffusion models and latent diffusion models?

- **Concept**: Web performance metrics (SpeedIndex, Page Load Time)
  - **Why needed here**: These metrics determine whether in-browser image generation is viable from a user experience perspective
  - **Quick check question**: Why does SpeedIndex show better performance for generated images compared to Page Load Time?

- **Concept**: Proxy-based traffic interception and MITM techniques
  - **Why needed here**: This is the current implementation approach for intercepting and modifying web traffic to inject generated images
  - **Quick check question**: What is the purpose of having two separate proxies for image and non-image requests?

## Architecture Onboarding

- **Component map**: Crawler -> Proxy System -> Stable Diffusion Server -> Web Interface
- **Critical path**: Webpage request → Proxy intercepts image request → Database lookup → Stable diffusion server generates image → Proxy returns generated image → Browser displays
- **Design tradeoffs**:
  - Using proxy vs. browser integration: Proxy approach is faster to implement but adds network overhead
  - Constant vs. dynamic inference steps: Constant steps (20) ensure fairness but may miss optimization opportunities
  - Client vs. server annotation: Client is more deployable but less accurate
- **Failure signatures**:
  - Images not loading: Proxy not intercepting requests or stable diffusion server unreachable
  - Poor image quality: Annotation scheme failing to capture image content
  - Slow performance: GPU insufficient or network bottleneck between proxy and stable diffusion server
- **First 3 experiments**:
  1. Verify proxy intercepts image requests and returns placeholder response
  2. Test stable diffusion server with sample prompts and verify image generation
  3. Run end-to-end with a single webpage and verify image replacement works

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would the quality of AI-generated Web images improve with dynamic stable diffusion settings based on the content of the textual prompt and additional webpage information?
- **Basis in paper**: [explicit] The paper mentions that "an interesting direction for future work is to explore some dynamic stable diffusion settings based on the content of a textual prompt, and potentially additional webpage information."
- **Why unresolved**: The current study uses a constant number of inference steps (20) for all images, regardless of their content. The impact of dynamic settings on image quality has not been evaluated.
- **What evidence would resolve it**: A user study comparing the quality of images generated with dynamic stable diffusion settings versus a constant number of inference steps, using the same evaluation methodology as in the paper.

### Open Question 2
- **Question**: How would the performance of AI-generated Webpages change with the evolution of hardware and AI models compared to network performance?
- **Basis in paper**: [inferred] The paper discusses that locally generating images is currently viable for applications operating on a handful of images, but acknowledges that this might change in the future depending on how hardware and AI models evolve compared to network performance.
- **Why unresolved**: The current study only benchmarks the performance of AI-generated Webpages with current hardware and AI models. The impact of future advancements on performance is unknown.
- **What evidence would resolve it**: A longitudinal study tracking the performance of AI-generated Webpages over time, comparing the improvements in hardware and AI models with advancements in network performance.

### Open Question 3
- **Question**: How would the Webcompat be impacted if generative AI is used to generate HTML, CSS, and JavaScript elements on the Web?
- **Basis in paper**: [explicit] The paper mentions that "image generation has no impact on Webcompat, differently from AI-based generation of HTML, JavaScript, etc." and suggests that extending WebDiffusion to support these elements is an interesting direction for future work.
- **Why unresolved**: The current study only focuses on image generation, which does not impact Web compatibility. The impact of generating other Web elements using generative AI has not been investigated.
- **What evidence would resolve it**: A study evaluating the impact of AI-generated HTML, CSS, and JavaScript on the functionality and compatibility of Webpages, using a similar methodology as the current study for image generation.

## Limitations
- Proxy-based architecture introduces network latency between image interception and stable diffusion server
- Constant 20 inference steps used for all images, potentially missing optimization opportunities
- Dataset of 60 webpages may not represent the full diversity of real-world web content

## Confidence

- **High confidence**: Quality assessment methodology and finding that server-based annotations produce higher quality images than client-based; comparative performance data across different GPUs
- **Medium confidence**: Performance conclusions about SpeedIndex improvements, dependent on specific network conditions and GPU capabilities
- **Low confidence**: Generalization to dynamic webpages with JavaScript-heavy content, as study primarily uses static webpage snapshots

## Next Checks

1. **Performance validation**: Replicate the SpeedIndex experiments across a broader range of network conditions (varying latency, bandwidth) and with different GPU configurations to verify the performance claims under diverse real-world scenarios.

2. **Content diversity testing**: Evaluate the annotation and generation pipeline on webpages with varying characteristics - minimal alt text, complex layouts, and different image types (graphs, diagrams, photographs) to identify failure modes.

3. **User experience study**: Conduct a longitudinal user study comparing traditional webpages with AI-generated counterparts, measuring not just image quality scores but also user task completion rates and subjective satisfaction across different use cases.