---
ver: rpa2
title: 'Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned
  Supervised Learning'
arxiv_id: '2310.19308'
source_url: https://arxiv.org/abs/2310.19308
tags:
- policy
- rcsl
- learning
- dataset
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes return-conditioned supervised learning (RCSL)
  for offline RL, showing it can avoid Bellman completeness but fails at trajectory
  stitching. To address this, the authors propose MBRCSL, which augments data coverage
  through model-based rollouts to enable trajectory stitching while retaining RCSL's
  benefits.
---

# Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning

## Quick Facts
- arXiv ID: 2310.19308
- Source URL: https://arxiv.org/abs/2310.19308
- Reference count: 40
- Key outcome: MBRCSL achieves higher returns than model-free/model-based offline RL and pure RCSL baselines across Point Maze and simulated robotic manipulation tasks

## Executive Summary
This paper analyzes return-conditioned supervised learning (RCSL) for offline reinforcement learning, demonstrating that while it avoids Bellman completeness requirements, it fundamentally cannot perform trajectory stitching. To address this limitation, the authors propose MBRCSL, which augments the offline dataset with model-based rollouts to enable trajectory stitching while retaining RCSL's benefits. Theoretically, they prove RCSL outperforms DP-based methods when Bellman completeness is hard to satisfy, and rigorously show RCSL cannot stitch trajectories. Empirically, MBRCSL achieves superior performance across tested domains.

## Method Summary
MBRCSL combines return-conditioned supervised learning with model-based data augmentation. The method first learns a dynamics model and behavior policy from the offline dataset using maximum likelihood estimation. It then generates rollouts by sampling from the learned behavior policy under the learned dynamics model, collecting trajectories with returns higher than the maximum in the original dataset. Finally, it trains a return-conditioned policy on this augmented rollout dataset using supervised learning, enabling trajectory stitching while avoiding Bellman completeness requirements.

## Key Results
- MBRCSL outperforms pure RCSL, model-free offline RL, and model-based offline RL methods on Point Maze and simulated robotic manipulation tasks
- Theoretical analysis proves RCSL cannot perform trajectory stitching even with full data coverage
- RCSL converges under significantly more relaxed assumptions than DP-based methods when Bellman completeness is difficult to satisfy
- MBRCSL successfully enables trajectory stitching while maintaining the benefits of avoiding Bellman completeness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RCSL avoids Bellman completeness requirements while still converging under relaxed assumptions inherited from supervised learning.
- **Mechanism**: RCSL learns a return-conditioned policy directly from data via maximum likelihood estimation without requiring iterative Bellman backups. Since it only needs to represent the optimal policy (realizability), it sidesteps the need for Bellman completeness which requires the function class to be closed under the Bellman operator.
- **Core assumption**: The dataset contains expert (optimal) trajectories that cover the optimal policy.
- **Evidence anchors**:
  - [abstract] "RCSL are able to circumvent these challenges of Bellman completeness, converging under significantly more relaxed assumptions inherited from supervised learning."
  - [section 3.1] "These techniques have seen recent prominence due to their surprisingly good empirical performance, simplicity and stability."
- **Break condition**: Without expert trajectories in the dataset, RCSL cannot guarantee optimality since it cannot perform trajectory stitching.

### Mechanism 2
- **Claim**: MBRCSL enables trajectory stitching by augmenting data coverage through model-based rollouts while retaining RCSL's benefits.
- **Mechanism**: MBRCSL learns a dynamics model and behavior policy from the offline dataset, then performs forward sampling to generate trajectories that combine sub-components from different original trajectories. These "stitched" trajectories are added to the dataset, enabling RCSL to learn policies that outperform any single trajectory in the original data.
- **Core assumption**: The learned dynamics model is sufficiently accurate to generate valid trajectories when combined with the behavior policy.
- **Evidence anchors**:
  - [abstract] "MBRCSL leverages learned dynamics models and forward sampling to accomplish trajectory stitching while avoiding the need for Bellman completeness."
  - [section 4] "The key idea behind MBRCSL is to augment the offline dataset with trajectories that themselves combine sub-components of many different trajectories."
- **Break condition**: If the dynamics model is highly inaccurate, generated trajectories may be invalid or misleading, preventing effective stitching.

### Mechanism 3
- **Claim**: RCSL cannot perform trajectory stitching even with full data coverage due to fundamental limitations in return-conditioned learning.
- **Mechanism**: RCSL policies condition only on current state and desired return, lacking the ability to distinguish between different optimal trajectories that reach the same state with different return requirements. This prevents combining sub-trajectories from different paths to form better trajectories.
- **Core assumption**: The policy architecture is Markovian (context length = 1), conditioning only on current state and desired return.
- **Evidence anchors**:
  - [abstract] "Furthermore, in order to learn from sub-optimal datasets, we propose a simple framework called MBRCSL, granting RCSL methods the ability of dynamic programming to stitch together segments from distinct trajectories."
  - [section 3.2] "Brandfonbrener et al. (2022) gives an intuitive counter-example, but without a rigorous theorem. In Section 3.2, we provide two different reasons and rigorously show the the inability of RCSL to do stitching."
- **Break condition**: Using non-Markovian policies with longer context windows might mitigate this limitation, though the paper focuses on Markovian RCSL.

## Foundational Learning

- **Concept**: Bellman Completeness
  - Why needed here: Understanding this concept is crucial because it's the key condition that RCSL avoids while still achieving convergence, and it's what makes traditional DP-based methods difficult to implement with function approximation.
  - Quick check question: Can you explain why Bellman completeness is harder to satisfy than realizability when using neural networks as function approximators?

- **Concept**: Return-to-Go (RTG)
  - Why needed here: RTG is the core conditioning variable in RCSL, representing the remaining return from each timestep. Understanding how RTG is computed and used is essential for grasping how RCSL works.
  - Quick check question: How is RTG computed from a trajectory, and why is it used instead of just total return in RCSL?

- **Concept**: Trajectory Stitching
  - Why needed here: This is the fundamental limitation of RCSL that MBRCSL addresses. Understanding what stitching means and why it's valuable helps appreciate the contribution of MBRCSL.
  - Quick check question: Can you provide an example where trajectory stitching would enable better performance than any single trajectory in the dataset?

## Architecture Onboarding

- **Component map**: Dataset preprocessing → RTG dataset creation → Dynamics model learning → Behavior policy learning → Rollout generation → Return-conditioned policy learning → Evaluation
- **Critical path**: The most critical components are the dynamics model accuracy and the behavior policy's ability to capture multimodal distributions. Poor performance in either will prevent effective stitching.
- **Design tradeoffs**:
  - Model complexity vs. sample efficiency: More complex dynamics models require more data but can generate better rollouts
  - Context length in RCSL: Longer context might help stitching but increases complexity
  - Rollout dataset size: Larger datasets improve coverage but increase training time
- **Failure signatures**:
  - If dynamics model is poor: Generated trajectories will have unrealistic transitions
  - If behavior policy is too narrow: Rollouts won't explore diverse actions needed for stitching
  - If RCSL policy is deterministic: It may not capture the stochasticity needed for exploration
- **First 3 experiments**:
  1. Test dynamics model accuracy on held-out validation data from the original dataset
  2. Verify that rollout dataset contains trajectories with returns higher than the original dataset maximum
  3. Compare RCSL performance with and without model-based rollouts on a simple stitching task

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the model-based rollout size in MBRCSL affect the trade-off between data coverage and computational efficiency?
  - Basis in paper: [explicit] The paper discusses using model-based rollouts to augment data coverage in MBRCSL, but doesn't provide detailed analysis on how different rollout sizes impact performance.
  - Why unresolved: The paper focuses on proving the theoretical benefits of MBRCSL and showing empirical results, but doesn't systematically explore the effects of varying rollout sizes.
  - What evidence would resolve it: A thorough empirical study varying the rollout dataset size in MBRCSL across different tasks, showing how performance changes with computational cost.

- **Open Question 2**: Can MBRCSL be extended to handle stochastic environments while maintaining its advantages over DP-based methods?
  - Basis in paper: [inferred] The paper mentions that a notable open problem is making MBRCSL work in stochastic environments, but doesn't provide a solution.
  - Why unresolved: The current theoretical analysis and empirical results are focused on deterministic environments, leaving the stochastic case unexplored.
  - What evidence would resolve it: A modified MBRCSL algorithm that can handle stochasticity, with theoretical guarantees and empirical results showing its effectiveness in stochastic environments.

- **Open Question 3**: How does the choice of behavior policy in MBRCSL affect the quality of trajectory stitching and overall performance?
  - Basis in paper: [explicit] The paper mentions using a learned behavior policy in MBRCSL, but doesn't explore how different choices of behavior policy impact performance.
  - Why unresolved: The paper focuses on the overall effectiveness of MBRCSL but doesn't investigate the role of the behavior policy in detail.
  - What evidence would resolve it: An empirical study comparing different behavior policy choices (e.g., learned vs. heuristic) in MBRCSL across various tasks, showing their impact on trajectory stitching and performance.

## Limitations
- The approach's effectiveness depends heavily on the quality of learned dynamics models, which can be challenging in complex environments
- Empirical validation is limited to only two domains (Point Maze and robotic manipulation), raising questions about generalizability
- The paper doesn't extensively explore failure modes or provide ablation studies showing the individual contributions of each component

## Confidence
- Medium confidence in core claims due to rigorous theoretical analysis but limited empirical validation across domains
- Theoretical analysis of RCSL's inability to perform trajectory stitching appears sound
- Empirical results demonstrate effectiveness but are limited in scope and domain diversity

## Next Checks
1. **Ablation Study**: Evaluate MBRCSL performance with varying levels of dynamics model accuracy to quantify the sensitivity to model quality and identify failure thresholds.

2. **Scalability Test**: Apply MBRCSL to a high-dimensional visual control task (e.g., vision-based robotic manipulation) to assess performance in more realistic settings.

3. **Baselines Comparison**: Compare against more diverse offline RL methods including conservative Q-learning variants and advantage-weighted regression to establish relative strengths and weaknesses.