---
ver: rpa2
title: 'SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape'
arxiv_id: '2311.13169'
source_url: https://arxiv.org/abs/2311.13169
tags:
- loss
- sigeo
- training
- warm-up
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SiGeo, a novel sub-one-shot neural architecture
  search (NAS) proxy that bridges the gap between zero-shot and one-shot NAS. The
  key innovation is a theoretical framework connecting the geometry of loss landscapes
  with the efficacy of NAS proxies during the warm-up phase of supernet training.
---

# SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape

## Quick Facts
- arXiv ID: 2311.13169
- Source URL: https://arxiv.org/abs/2311.13169
- Reference count: 40
- Key outcome: SiGeo achieves comparable performance to one-shot NAS methods while reducing computational costs by approximately 60%

## Executive Summary
SiGeo introduces a novel sub-one-shot neural architecture search (NAS) proxy that bridges zero-shot and one-shot NAS approaches. The method leverages information theory and geometric properties of the loss landscape, incorporating the Fisher-Rao norm, gradient statistics, and current training loss to predict architecture performance during the warm-up phase of supernet training. SiGeo demonstrates superior performance on both computer vision and recommendation system benchmarks, particularly when the supernet is warmed up with a small portion of training data.

## Method Summary
SiGeo is a NAS proxy that predicts architecture performance during the warm-up phase of supernet training. It combines three key components: Fisher-Rao norm (capturing curvature of the loss landscape), gradient variance (estimating generalization error), and current training loss (reflecting convergence progress). The method is designed to work with limited training data (1-10% of the full dataset) and uses a weighted combination of these components to rank candidate architectures. SiGeo is evaluated on recommendation system benchmarks (Criteo, Avazu, KDD Cup 2012) and computer vision tasks (CIFAR-10, CIFAR-100, ImageNet16-120), showing significant computational efficiency gains while maintaining competitive accuracy.

## Key Results
- SiGeo achieves log losses of 0.4396-0.4404 on CTR prediction tasks with search costs of only 0.1-0.12 GPU days
- Outperforms both one-shot and zero-shot baselines on recommendation system benchmarks
- Reduces computational costs by approximately 60% compared to traditional one-shot NAS methods
- Demonstrates superior performance particularly when supernet is warmed up with small portions of training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Fisher-Rao norm and current training loss become increasingly important predictors of performance as the supernet warms up.
- Mechanism: As the supernet undergoes limited training (warm-up), the geometry of the loss landscape around the local minimum becomes more informative. The Fisher-Rao norm captures the curvature of this landscape, while the current training loss reflects how close the model is to convergence.
- Core assumption: The Fisher-Rao norm and training loss remain correlated with test performance even when the supernet is only partially trained.
- Evidence anchors: [abstract] "The theoretical analysis shows that the Fisher-Rao norm and current training loss become increasingly important predictors of performance as the supernet warm-up progresses."

### Mechanism 2
- Claim: SiGeo leverages gradient variance to estimate generalization error, improving NAS proxy accuracy.
- Mechanism: The gradient variance across different training samples reflects the curvature of the loss landscape around the local minimum. Lower gradient variance indicates a flatter minimum, which is associated with better generalization.
- Core assumption: The gradient variance is a reliable estimator of generalization error, even when the supernet is only partially trained.
- Evidence anchors: [section 3.4] "The association of local flatness of the loss landscape with better generalization in DNNs has been widely embraced"

### Mechanism 3
- Claim: SiGeo balances the minimum achievable training loss and generalization error to predict test performance.
- Mechanism: The expected loss can be decomposed into the minimum achievable training loss and the generalization error. SiGeo estimates these components using the Fisher-Rao norm, gradient variance, and current training loss.
- Core assumption: The decomposition of expected loss into training loss and generalization error holds even when the supernet is only partially trained.
- Evidence anchors: [section 3.3] "The decomposition of expected test loss (Eq. 3) underscores the significance of generalization error."

## Foundational Learning

- Concept: Fisher-Rao norm
  - Why needed here: The Fisher-Rao norm is used to estimate the minimum achievable training loss, which is a component of the expected loss.
  - Quick check question: How does the Fisher-Rao norm relate to the curvature of the loss landscape?

- Concept: Gradient variance
  - Why needed here: The gradient variance is used to estimate the generalization error, which is another component of the expected loss.
  - Quick check question: How does gradient variance relate to the flatness of the loss landscape?

- Concept: Loss landscape geometry
  - Why needed here: Understanding the geometry of the loss landscape is crucial for interpreting the Fisher-Rao norm and gradient variance, and for designing effective NAS proxies.
  - Quick check question: What are the key geometric properties of the loss landscape that are relevant to NAS?

## Architecture Onboarding

- Component map: SiGeo consists of three main components: (1) Fisher-Rao norm, (2) gradient variance, and (3) current training loss. These components are combined using a weighted sum to form the SiGeo proxy.
- Critical path: The critical path for using SiGeo involves (1) warming up the supernet with a small portion of training data, (2) computing the SiGeo proxy for each candidate architecture, and (3) selecting the architecture with the best SiGeo score.
- Design tradeoffs: The key design tradeoff in SiGeo is the choice of weights for the three components. These weights can be tuned to optimize performance on specific tasks or datasets.
- Failure signatures: SiGeo may fail if the warm-up is insufficient, the supernet architecture is poorly designed, or the training data distribution is highly non-stationary.
- First 3 experiments:
  1. Validate the theoretical findings by computing the correlation between SiGeo scores and test performance on a small-scale dataset.
  2. Compare SiGeo against other zero-shot NAS proxies on a standard NAS benchmark (e.g., NAS-Bench-101).
  3. Evaluate SiGeo on a real-world recommendation system dataset (e.g., Criteo) under different warm-up levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the Fisher-Rao norm and generalization error in deep neural networks?
- Basis in paper: [explicit] The paper discusses the relationship between Fisher-Rao norm and generalization error, citing prior work by Liang et al. [2019] and Hochreiter and Schmidhuber [1997].
- Why unresolved: While the paper provides theoretical insights and empirical validation, a rigorous mathematical proof of the relationship between Fisher-Rao norm and generalization error in deep neural networks is not explicitly provided.
- What evidence would resolve it: A rigorous mathematical proof establishing the relationship between Fisher-Rao norm and generalization error in deep neural networks, supported by extensive empirical validation.

### Open Question 2
- Question: How does the proposed SiGeo proxy perform on other complex domains beyond computer vision and recommendation systems?
- Basis in paper: [inferred] The paper focuses on evaluating SiGeo's performance on computer vision and recommendation system benchmarks. However, the authors suggest that SiGeo could be applicable to other complex domains.
- Why unresolved: The paper does not provide empirical results or theoretical analysis on the performance of SiGeo in other complex domains.
- What evidence would resolve it: Empirical results demonstrating the effectiveness of SiGeo on other complex domains, such as natural language processing, graph neural networks, or time series analysis.

### Open Question 3
- Question: What are the limitations of the sub-one-shot NAS setting, and how can they be addressed?
- Basis in paper: [explicit] The paper introduces the sub-one-shot NAS setting as a bridge between zero-shot and one-shot NAS. However, the authors do not discuss potential limitations or challenges of this setting.
- Why unresolved: The paper focuses on the benefits of the sub-one-shot setting but does not explore its potential drawbacks or limitations.
- What evidence would resolve it: A comprehensive analysis of the limitations of the sub-one-shot NAS setting, along with potential solutions or mitigation strategies.

### Open Question 4
- Question: How does the choice of warm-up level affect the performance of SiGeo in different search spaces and tasks?
- Basis in paper: [explicit] The paper evaluates SiGeo under various warm-up levels on different search spaces and tasks. However, the authors do not provide a detailed analysis of the impact of warm-up level on performance.
- Why unresolved: The paper presents empirical results showing the impact of warm-up level on performance but does not provide a thorough analysis of the relationship between warm-up level and performance.
- What evidence would resolve it: A detailed analysis of the relationship between warm-up level and performance, considering different search spaces and tasks, along with guidelines for selecting the optimal warm-up level.

## Limitations
- The theoretical framework assumes that Fisher-Rao norm and gradient statistics remain reliable indicators of performance even with minimal warm-up, but this relationship could break down for architectures with highly non-convex loss landscapes.
- The specific NASRec search space used in RecSys experiments may not represent all practical deployment scenarios, limiting generalizability.
- The performance gains may not translate directly to all architectural search spaces or problem domains beyond computer vision and recommendation systems.

## Confidence
- **High Confidence**: The computational efficiency gains (approximately 60% reduction in search costs) are well-supported by experimental results and represent a straightforward empirical observation.
- **Medium Confidence**: The theoretical connection between loss landscape geometry and NAS proxy efficacy is conceptually sound but relies on assumptions about the stability of Fisher-Rao norm and gradient variance correlations that may not hold universally.
- **Medium Confidence**: The superiority of SiGeo over existing NAS proxies is demonstrated on specific benchmarks but may not translate directly to all architectural search spaces or problem domains.

## Next Checks
1. **Cross-Architecture Space Validation**: Test SiGeo on a broader range of NAS search spaces beyond NASRec, including DARTS-like and ENAS-like spaces, to verify the generalizability of the loss landscape geometry approach.

2. **Distribution Shift Robustness**: Evaluate SiGeo's performance when the warm-up data distribution differs from the full training distribution to assess its robustness to data shift scenarios.

3. **Theoretical Validation**: Conduct ablation studies that systematically vary the contributions of Fisher-Rao norm, gradient statistics, and training loss to quantify their individual impacts on proxy accuracy and identify potential failure modes.