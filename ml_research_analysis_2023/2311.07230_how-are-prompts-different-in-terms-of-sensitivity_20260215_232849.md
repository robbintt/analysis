---
ver: rpa2
title: How are Prompts Different in Terms of Sensitivity?
arxiv_id: '2311.07230'
source_url: https://arxiv.org/abs/2311.07230
tags:
- base
- sensitivity
- prompt
- prompts
- zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel analysis of prompt sensitivity in
  in-context learning (ICL), demonstrating that prompt sensitivity is a strong unsupervised
  proxy for model performance, exhibiting a strong negative correlation with accuracy.
  The authors use gradient-based saliency scores to show how different prompts affect
  token relevance, leading to varying sensitivity levels.
---

# How are Prompts Different in Terms of Sensitivity?

## Quick Facts
- arXiv ID: 2311.07230
- Source URL: https://arxiv.org/abs/2311.07230
- Reference count: 34
- Primary result: Sensitivity is a strong unsupervised proxy for ICL performance, with negative correlation to accuracy

## Executive Summary
This paper introduces a novel analysis of prompt sensitivity in in-context learning (ICL), demonstrating that prompt sensitivity is a strong unsupervised proxy for model performance, exhibiting a strong negative correlation with accuracy. The authors use gradient-based saliency scores to show how different prompts affect token relevance, leading to varying sensitivity levels. They propose sensitivity-aware decoding, which incorporates sensitivity estimation as a penalty term in greedy decoding, improving performance particularly when input information is scarce. Their comprehensive analysis across multiple models and tasks provides fresh insights into prompt engineering and ICL mechanisms.

## Method Summary
The paper analyzes prompt sensitivity in ICL by generating synthetic data through perturbations and calculating sensitivity using variation ratio. It employs gradient-based saliency scores to explain differences in sensitivity across prompts and introduces sensitivity-aware decoding that incorporates sensitivity estimation as a penalty term during greedy decoding. The approach is tested across multiple datasets (CoLA, RTE, MNLI, SST2, CSQA), different models (GPT-3, LLaMA, Flan-T5), and various prompts (base, CFP, CoT, APE, GKP), measuring sensitivity and accuracy as primary metrics.

## Key Results
- Sensitivity exhibits strong negative correlation with accuracy (r = -0.8764, p-value ≪ 0.01)
- Gradient-based saliency scores explain sensitivity differences across prompts
- Sensitivity-aware decoding improves performance, particularly when input information is scarce

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower sensitivity indicates better prompt effectiveness
- Mechanism: Prompts that reduce function sensitivity create more stable output mappings, leading to better generalization
- Core assumption: Sensitivity as defined by Hahn et al. (2021) directly correlates with model performance
- Evidence anchors: Strong correlation between sensitivity and accuracy; theoretical foundation from Hahn et al.

### Mechanism 2
- Claim: Gradient-based saliency scores explain sensitivity differences
- Mechanism: Prompts that make input tokens less relevant to output predictions reduce sensitivity
- Core assumption: Gradient norms accurately capture token relevance
- Evidence anchors: Correlation between gradient-based saliency and sensitivity; token relevance analysis

### Mechanism 3
- Claim: Sensitivity-aware decoding improves performance
- Mechanism: Penalizing high-sensitivity outputs during decoding reduces unstable predictions
- Core assumption: Sensitivity estimates can be calculated without labeled data
- Evidence anchors: Performance improvements with sensitivity-aware decoding; Table 8 results

## Foundational Learning

- Concept: Function sensitivity as defined by Hahn et al. (2021)
  - Why needed here: Forms the theoretical foundation for comparing prompt effectiveness
  - Quick check question: Can you explain what function sensitivity measures in terms of input-output relationships?

- Concept: Gradient-based saliency scoring
  - Why needed here: Provides empirical explanation for why different prompts lead to different sensitivity levels
  - Quick check question: How do you compute gradient-based saliency scores for a given input token?

- Concept: In-context learning mechanics
  - Why needed here: Understanding how ICL works helps explain why prompts affect sensitivity
  - Quick check question: What is the relationship between ICL and implicit gradient descent?

## Architecture Onboarding

- Component map: Sensitivity estimation module -> Gradient-based saliency calculator -> Sensitivity-aware decoding wrapper -> Prompt perturbation generator
- Critical path: 1. Generate synthetic data through perturbations 2. Calculate sensitivity using variation ratio 3. Compute gradient-based saliency scores 4. Apply sensitivity-aware decoding if desired
- Design tradeoffs: Accuracy vs computational cost in sensitivity estimation; Granularity of perturbations vs noise in synthetic data; Weighting factor α in sensitivity-aware decoding
- Failure signatures: Sensitivity estimates inconsistent across runs; Gradient-based saliency scores don't correlate with sensitivity; Sensitivity-aware decoding degrades performance
- First 3 experiments: 1. Compare sensitivity across different prompt types (base_a vs base_b) 2. Test correlation between sensitivity and accuracy across models 3. Evaluate gradient-based saliency scores for different prompt segments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of sensitivity-aware decoding vary with different decoding strategies beyond greedy decoding and Top-k sampling?
- Basis in paper: The paper mentions that decoding strategies influence the quality of LLM generations and that Top-k sampling leads to a higher level of sensitivity compared to greedy decoding. However, it does not explore other decoding strategies.
- Why unresolved: The paper only compares greedy decoding and Top-k sampling, leaving the potential effects of other decoding strategies unexplored.
- What evidence would resolve it: Experiments comparing the performance of sensitivity-aware decoding with various other decoding strategies, such as nucleus sampling or beam search, would provide insights into the generalizability of the approach.

### Open Question 2
- Question: What is the relationship between the sensitivity of a prompt and its ability to generalize across different tasks and domains?
- Basis in paper: The paper demonstrates that sensitivity is an unsupervised proxy for model performance and that different prompts lead to varying levels of sensitivity. However, it does not investigate how sensitivity relates to cross-task or cross-domain generalization.
- Why unresolved: The experiments are limited to a specific set of tasks and models, and the paper does not explore the broader implications of prompt sensitivity on generalization.
- What evidence would resolve it: Experiments testing the same prompts across a wider range of tasks and domains, along with an analysis of how sensitivity correlates with generalization performance, would provide insights into the broader implications of prompt sensitivity.

### Open Question 3
- Question: How does the sensitivity of a prompt change as the model size increases, and what are the implications for scaling up large language models?
- Basis in paper: The paper tests models with varying sizes but does not explicitly analyze how sensitivity changes with model size. The results show that sensitivity is a strong negative correlation with accuracy, but the relationship between sensitivity and model size is not explored.
- Why unresolved: The paper does not investigate the impact of model size on prompt sensitivity, which is crucial for understanding the behavior of larger models.
- What evidence would resolve it: Experiments testing the same prompts on a range of model sizes, along with an analysis of how sensitivity changes with model size, would provide insights into the implications of scaling up large language models.

## Limitations

- Generalizability concerns: Analysis primarily focused on classification tasks; perturbation-based sensitivity may not capture real-world variations
- Gradient-based method limitations: May be unstable for different model architectures; model-specific behavior observed with Flan-T5
- Sensitivity-aware decoding trade-offs: Optimal weighting factor α needs careful tuning; computational overhead during decoding

## Confidence

- High Confidence: Strong negative correlation between sensitivity and accuracy (r = -0.8764) across multiple models and datasets
- Medium Confidence: Gradient-based saliency scores explain sensitivity differences; causal relationship needs further validation
- Medium Confidence: Sensitivity-aware decoding effectiveness for scarce input information; benefits are conditional and task-dependent

## Next Checks

1. **Cross-Task Validation**: Evaluate the sensitivity-accuracy correlation and sensitivity-aware decoding performance on non-classification tasks such as question answering, text generation, or reasoning tasks to assess generalizability beyond the current experimental scope.

2. **Model Architecture Ablation**: Test the gradient-based saliency and sensitivity estimation methods across different model families (encoder-decoder, decoder-only, encoder-only) to identify architecture-specific limitations and validate the robustness of the proposed approach.

3. **Sensitivity Metric Robustness**: Compare the perturbation-based sensitivity metric against alternative sensitivity measures (e.g., input-output Jacobian, adversarial robustness) to assess whether the observed correlations and benefits are specific to the chosen metric or represent a more general phenomenon.