---
ver: rpa2
title: Assessment of Pre-Trained Models Across Languages and Grammars
arxiv_id: '2309.11165'
source_url: https://arxiv.org/abs/2309.11165
tags:
- computational
- parsing
- linguistics
- dependency
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-paradigm, multilingual probing framework
  to assess how multilingual LLMs encode syntax in terms of dependency and constituent
  structures. The approach casts parsing as sequence labeling using existing encodings,
  and probes frozen LLM representations with a linear layer.
---

# Assessment of Pre-Trained Models Across Languages and Grammars

## Quick Facts
- arXiv ID: 2309.11165
- Source URL: https://arxiv.org/abs/2309.11165
- Reference count: 21
- Primary result: Multi-paradigm, multilingual probing framework shows subword tokenization is needed for syntax representation and pretraining data presence is more important than task data size for syntax recovery.

## Executive Summary
This paper presents a comprehensive probing framework to assess how multilingual large language models (LLMs) encode syntactic information across different languages and grammatical formalisms. By treating parsing as sequence labeling and using frozen LLM representations with linear classifiers, the study evaluates syntax recoverability across 13 dependency treebanks and 10 constituent treebanks. The experiments reveal that subword tokenization is crucial for encoding syntax, in contrast to character-based models, and that the presence of a language in pretraining data matters more than the amount of task-specific training data when recovering syntactic structures.

## Method Summary
The approach casts syntactic parsing as sequence labeling by converting tree structures into sequences of labels using various encodings (head-selection, bracketing-based, transition-based). A frozen LLM encodes input sentences into contextual representations, which are then processed by a linear classifier to predict syntactic labels. The framework compares three setups: frozen weights (only classifier is trained), random weights (both LLM and classifier have random weights), and fine-tuned weights (both LLM and classifier are trained). Error reductions between frozen and random setups measure syntax recoverability, while fine-tuned performance provides an upper bound.

## Key Results
- The probing framework is consistent across different sequence labeling encodings
- Subword tokenization is necessary to represent syntax, unlike character-based models
- Language occurrence in pretraining data is more important than task data amount for syntax recovery

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The probing framework reliably estimates the amount of syntax encoded in LLM representations by comparing frozen weights performance against a randomized control baseline.
- Mechanism: By training only the output layer while freezing the LLM weights, the probe measures how much syntactic information can be extracted from the pre-trained representations alone. The difference between frozen and random setups quantifies syntax recoverability.
- Core assumption: The linear classifier layer cannot learn the syntactic parsing task itself without meaningful representations from the LLM.
- Evidence anchors:
  - [abstract] "The study provides insights into the capabilities and limitations of multilingual LLMs in encoding syntactic information."
  - [section] "We use a 1-layered feed-forward network on top of the LLMs to predict the labels. We propose three setups... Frozen weights (frz) The LLM weights are frozen..."
  - [corpus] Weak evidence - the corpus neighbors focus on syntax modeling but don't directly address probing methodology.
- Break condition: If the random baseline achieves similar performance to the frozen setup, it would indicate the probe is learning the task rather than measuring encoded syntax.

### Mechanism 2
- Claim: Subword tokenization is necessary for LLMs to effectively encode syntactic information, unlike character-based models.
- Mechanism: Subword tokenization creates meaningful linguistic units that can capture morphological and syntactic patterns, while character-level models struggle to represent higher-level syntactic structures.
- Core assumption: The granularity of subword tokens aligns better with linguistic units relevant for syntax than individual characters.
- Evidence anchors:
  - [abstract] "sub-word tokenization is needed to represent syntax, in contrast to character-based models"
  - [section] "mBERT (Devlin et al., 2019) It uses WordPiece tokenization... canine (-c and -s) (Clark et al., 2022) It uses char-based tokenization..."
  - [corpus] Moderate evidence - corpus includes papers on dependency parsing and syntax integration, supporting the importance of tokenization choices.
- Break condition: If character-based models performed equally well on syntactic tasks, this mechanism would be invalidated.

### Mechanism 3
- Claim: The presence of a language in pretraining data is more important than the amount of task-specific training data for recovering syntax.
- Mechanism: LLMs learn language-specific syntactic patterns during pretraining, and this knowledge transfers to parsing tasks even with limited task-specific data.
- Core assumption: Pretraining data provides sufficient exposure to language-specific syntactic patterns that transfer to parsing tasks.
- Evidence anchors:
  - [abstract] "occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors"
  - [section] "For larger treebanks, whose languages are supported by LLMs, the error reductions between the frz and rnd setups are large... For languages that are not supported by the LLMs, the error reductions are clearly smaller."
  - [corpus] Weak evidence - corpus doesn't directly address the pretraining data vs task data trade-off.
- Break condition: If larger task-specific datasets consistently outperformed pretraining data presence, this mechanism would be invalidated.

## Foundational Learning

- Concept: Sequence labeling encodings for syntactic parsing
  - Why needed here: The framework relies on converting tree structures into sequences of labels that can be predicted by a linear classifier
  - Quick check question: What information does the 2-planar bracketing encoding (2pb) capture for each token?

- Concept: Linear probing methodology
  - Why needed here: The approach uses frozen LLM weights with a linear classifier to measure what syntactic information is encoded in the representations
  - Quick check question: Why is a randomized control baseline necessary in probing experiments?

- Concept: Dependency vs constituent syntactic formalisms
  - Why needed here: The study compares how well LLMs encode different syntactic representations, requiring understanding of both paradigms
  - Quick check question: How do dependency relations differ from constituent structures in representing sentence syntax?

## Architecture Onboarding

- Component map:
  Input -> Pre-trained LLM encoder (frozen) -> Linear classifier -> Sequence of syntactic labels

- Critical path:
  1. Tokenize input text using LLM's tokenizer
  2. Generate contextual embeddings from frozen LLM
  3. Apply linear classifier to predict syntactic labels
  4. Decode labels back into syntactic structures
  5. Evaluate against gold standard parses

- Design tradeoffs:
  - Freezing vs fine-tuning LLM weights: Freezing isolates what's learned during pretraining, while fine-tuning maximizes performance but confounds pretraining vs task learning
  - Subword vs character tokenization: Subword generally performs better for syntax but may struggle with low-resource languages
  - Encoding choice: Different encodings capture different aspects of syntax; 2pb is most robust but others may be better for specific languages

- Failure signatures:
  - Random baseline performs as well as frozen setup: Probe is learning task rather than measuring encoded syntax
  - Character-based models perform well: Subword tokenization may not be as critical as hypothesized
  - Large treebanks don't improve frozen setup performance: Task data amount may not matter as much as assumed

- First 3 experiments:
  1. Run the probing framework on a single treebank with mBERT using 2pb encoding, comparing frozen vs random setups
  2. Test the same treebank with different encoding schemes (rh, ahtb, 2pb) to verify encoding consistency
  3. Run the same experiment on a treebank from a language not in mBERT's pretraining data to test pretraining data importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multilingual LLMs on syntactic probing tasks scale with model size and complexity, particularly for low-resource languages?
- Basis in paper: [inferred] The paper only tested a limited set of LLMs (mBERT, xlm-roberta, and canine) due to computational constraints and did not explore the impact of model size on syntactic probing performance.
- Why unresolved: The study did not investigate the effect of varying model sizes on the ability to recover syntactic structures, especially for languages not present in the pretraining data.
- What evidence would resolve it: Experiments comparing the probing performance of different-sized LLMs on a diverse set of languages, including low-resource ones, would provide insights into the scaling properties of syntactic encoding in multilingual models.

### Open Question 2
- Question: What specific linguistic features or patterns are encoded in the representations of multilingual LLMs that enable the recovery of syntactic structures, and how do these differ across languages and formalisms?
- Basis in paper: [inferred] While the paper demonstrated that LLMs can recover syntactic structures to some extent, it did not delve into the specific linguistic features or patterns that contribute to this ability.
- Why unresolved: The study focused on the overall performance of LLMs in recovering syntactic structures but did not investigate the underlying linguistic representations that facilitate this task.
- What evidence would resolve it: Detailed analyses of the linguistic features and patterns encoded in the representations of multilingual LLMs, using techniques like attention visualization, probing classifiers, or feature attribution methods, would shed light on the specific linguistic knowledge captured by these models.

### Open Question 3
- Question: How does the choice of syntactic formalism (dependency vs. constituency) impact the probing performance of multilingual LLMs, and what are the implications for downstream NLP tasks?
- Basis in paper: [explicit] The paper found no clear evidence that contextualized vectors encode constituent structures better than dependencies or vice versa, but did not explore the implications for downstream tasks.
- Why unresolved: The study compared the probing performance of LLMs on dependency and constituency parsing but did not investigate how these differences in encoding might affect the performance of LLMs on downstream NLP tasks that rely on syntactic information.
- What evidence would resolve it: Experiments evaluating the impact of syntactic formalism choice on the performance of LLMs in downstream NLP tasks, such as sentiment analysis, machine translation, or question answering, would provide insights into the practical implications of syntactic encoding differences.

## Limitations

- Probing methodology cannot definitively prove that encoded syntax is actually used by LLMs for language understanding
- Corpus-based evidence supporting key mechanisms is weak to moderate
- Study does not investigate the specific linguistic features encoded in LLM representations

## Confidence

**High confidence:** The probing framework is technically sound and the comparison between frozen and random setups is a well-established method for measuring syntax recoverability in LLM representations.

**Medium confidence:** The finding that subword tokenization is needed for syntax representation is supported by the experimental results, but the mechanism explaining why character-based models struggle with syntax is not fully validated.

**Low confidence:** The claim that pretraining data presence is more important than task data amount for recovering syntax is based on observed error reductions, but lacks direct evidence about the relationship between pretraining data and syntax encoding.

## Next Checks

1. **Probe control validation:** Run additional control experiments with randomized input tokens while keeping LLM weights frozen to verify that the probe is not exploiting spurious correlations in the data rather than measuring actual syntax encoding.

2. **Cross-linguistic generalization test:** Apply the probing framework to a held-out language family not present in any of the tested LLMs' pretraining data to better isolate the effect of pretraining data presence versus other factors.

3. **Fine-tuning ablation study:** Conduct a systematic ablation study comparing frozen, partially fine-tuned (only embedding layer), and fully fine-tuned setups to better understand how different types of pretraining data contribute to syntax encoding.