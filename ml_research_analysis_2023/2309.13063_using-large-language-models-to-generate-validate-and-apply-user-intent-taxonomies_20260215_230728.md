---
ver: rpa2
title: Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies
arxiv_id: '2309.13063'
source_url: https://arxiv.org/abs/2309.13063
tags:
- taxonomy
- user
- intent
- data
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a methodology for using large language models
  (LLMs) to generate, validate, and apply taxonomies for understanding user intent
  in log data. The key innovation is a human-in-the-loop approach where LLM-generated
  taxonomies are validated and refined by human experts to ensure quality and reliability.
---

# Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies

## Quick Facts
- arXiv ID: 2309.13063
- Source URL: https://arxiv.org/abs/2309.13063
- Reference count: 40
- Key outcome: LLM-generated taxonomies achieve substantial inter-coder reliability (kappa 0.72-0.76) with human annotations, enabling scalable user intent analysis

## Executive Summary
This paper presents a human-in-the-loop methodology for using large language models to generate, validate, and apply taxonomies for understanding user intent in log data. The approach leverages LLMs to create rich taxonomies while human experts validate and refine them to ensure quality and reliability. Through case studies on AI chat logs and search-vs-chat intent comparison, the method demonstrates substantial inter-coder reliability and reveals modality-specific intent patterns that inform system design.

## Method Summary
The methodology involves using GPT-4 to generate user intent taxonomies from log data, followed by human validation to ensure comprehensiveness, consistency, clarity, accuracy, and conciseness. Human assessors independently annotate sample data using the generated taxonomy, and inter-coder reliability (Cohen's kappa) is computed to assess agreement. The taxonomy is then refined based on feedback and applied to annotate test data, comparing LLM-generated labels against human annotations to measure reliability and accuracy.

## Key Results
- LLM-generated taxonomy achieved substantial inter-coder reliability (kappa 0.72-0.76) with human annotations
- GPT-4 annotations showed better objectivity and consistency compared to human-generated labels in this context
- Analysis revealed significant differences in intent distributions between search and chat modalities, with users favoring chat for create, learn, and leisure intents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-human collaboration produces taxonomies meeting quality criteria (comprehensiveness, consistency, clarity, accuracy, conciseness)
- Mechanism: LLMs generate initial taxonomy structures with rich examples; humans validate and refine to ensure external validity and eliminate feedback loops
- Core assumption: Human assessors can identify gaps and inconsistencies in LLM-generated taxonomies that the LLM cannot self-correct
- Evidence anchors:
  - [abstract] "using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents"
  - [section 3.3] "We asked the LLM to generate labels, descriptions, and examples for these categories"
- Break condition: If human assessors cannot agree on taxonomy categories, the validation loop fails

### Mechanism 2
- Claim: GPT-4 can annotate user intent categories with substantial inter-coder reliability compared to human coders
- Mechanism: Trained taxonomy is applied by GPT-4 using structured prompts that include category definitions and examples
- Core assumption: GPT-4's understanding of taxonomy definitions translates to consistent application across diverse log data
- Evidence anchors:
  - [section 3.5] "We computed Cohen's kappa to be 0.7212. This also indicates a substantial level of agreement"
  - [section 3.5] "we found that the labels generated by GPT-4 are better than those generated by humans in this case as they are more objectively and consistently assigned"
- Break condition: If GPT-4 generates "Other" category for significant portions of data, indicating poor fit

### Mechanism 3
- Claim: LLM-generated taxonomies reveal modality-specific intent patterns (search vs. chat) that inform system design
- Mechanism: Applied taxonomy to normalized query and chat data, revealing distribution differences across intent categories
- Core assumption: Intent distribution differences between modalities reflect genuine user behavior rather than annotation artifacts
- Evidence anchors:
  - [section 4.2] "Figure 4 shows the distribution of user intents for search and chat"
  - [section 4.2] "we found that while they could use either for their 'Information Retrieval' or 'Ask for Advice or Recommendation' needs, they are favoring (with a significant tilt toward) chat for their create, learn, and leisure intents"
- Break condition: If intent distributions show no meaningful differences between modalities

## Foundational Learning

- Concept: Inter-coder reliability (kappa statistic)
  - Why needed here: To validate that human and LLM annotations are consistent and reliable
  - Quick check question: What kappa value range indicates "substantial agreement" between coders?

- Concept: Taxonomy validation criteria
  - Why needed here: To ensure generated taxonomies are comprehensive, consistent, clear, accurate, and concise
  - Quick check question: Which validation criterion checks if all data can be reliably classified?

- Concept: Prompt engineering for LLMs
  - Why needed here: To guide LLM output toward desired taxonomy structure and quality
  - Quick check question: What prompt elements help ensure LLM generates appropriate category examples?

## Architecture Onboarding

- Component map: Log data preprocessing -> LLM interface (GPT-4) -> Human validation interface -> Annotation pipeline -> Analysis module

- Critical path: Training data → LLM taxonomy generation → Human validation → Test data annotation → Reliability measurement → Insights generation

- Design tradeoffs:
  - Single-level vs. hierarchical taxonomies (simplicity vs. granularity)
  - Category count limits (5-6 categories balances coverage and usability)
  - Human vs. full automation (quality vs. scalability)

- Failure signatures:
  - High "Other" category rates (>5%)
  - Low inter-coder reliability (<0.6 kappa)
  - Inconsistent LLM annotations across runs
  - Missing key intent categories in validation

- First 3 experiments:
  1. Test taxonomy comprehensiveness: Run LLM annotation on training data and measure "Other" category frequency
  2. Validate inter-coder reliability: Compare human coder annotations vs. LLM annotations on test set
  3. Compare modalities: Analyze intent distribution differences between search and chat data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated taxonomies compare to those created by human experts in terms of accuracy, completeness, and conciseness?
- Basis in paper: [inferred] The paper presents a methodology for using LLMs to generate taxonomies and validates their quality using human experts. However, it does not directly compare the quality of LLM-generated taxonomies to those created by humans.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the LLM-based approach rather than comparing it to human-generated taxonomies.
- What evidence would resolve it: A comparative study evaluating the quality of LLM-generated taxonomies against those created by human experts using the same validation criteria (comprehensiveness, consistency, clarity, accuracy, and conciseness).

### Open Question 2
- Question: How does the performance of LLMs in generating taxonomies vary across different domains or application contexts?
- Basis in paper: [inferred] The paper demonstrates the use of LLMs for generating taxonomies in the context of user intent analysis in AI chat logs. It does not explore the generalizability of this approach to other domains or contexts.
- Why unresolved: The paper focuses on a specific application domain and does not investigate the broader applicability of the LLM-based methodology.
- What evidence would resolve it: Experiments applying the LLM-based taxonomy generation approach to different domains or contexts and evaluating the quality and effectiveness of the resulting taxonomies.

### Open Question 3
- Question: What are the limitations of using LLMs for taxonomy generation, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper mentions that using LLMs to generate taxonomies can be problematic due to a lack of external validation and potential feedback loops. It proposes a methodology with human experts to address these issues.
- Why unresolved: The paper does not provide a comprehensive analysis of the limitations of LLM-based taxonomy generation or explore alternative approaches to address these limitations.
- What evidence would resolve it: A detailed analysis of the challenges and limitations of using LLMs for taxonomy generation, along with potential solutions or alternative approaches to mitigate these issues.

## Limitations
- Reliance on GPT-4's capabilities may not generalize to other LLM models or domains
- Validation process limited to small number of assessors (3-4), potentially missing diverse perspectives
- Assumes log data is already in English, doesn't address multilingual scenarios

## Confidence

**Major Limitations:**
The study's methodology relies heavily on GPT-4's capabilities, which may not generalize to other LLM models or domains. The validation process, while involving human experts, is limited to a small number of assessors (3-4), potentially missing diverse perspectives. The approach assumes log data is already in English and doesn't address multilingual scenarios. Additionally, the taxonomy generation process may be influenced by the specific prompt engineering used, making replication dependent on prompt quality.

**Confidence Assessment:**
- **High Confidence:** The inter-coder reliability results (kappa 0.72-0.76) are well-supported by the methodology and measurement approach. The comparison between search and chat modalities shows clear, interpretable patterns.
- **Medium Confidence:** The claim that LLM-generated taxonomies can replace or reduce human effort in taxonomy development is supported but requires further validation across different domains and LLM models.
- **Low Confidence:** The generalizability of the taxonomy generation approach to non-English log data and other domains remains untested and speculative.

## Next Checks
1. **Cross-LLM Validation:** Test the taxonomy generation methodology using different LLM models (e.g., Claude, LLaMA) to assess robustness and model dependency.
2. **Multi-language Extension:** Apply the methodology to multilingual log data to evaluate its effectiveness across language barriers.
3. **Domain Transferability:** Generate and validate taxonomies for different application domains (e.g., e-commerce, healthcare) to assess generalizability beyond chat and search contexts.