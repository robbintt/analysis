---
ver: rpa2
title: Bayesian CART models for insurance claims frequency
arxiv_id: '2303.01923'
source_url: https://arxiv.org/abs/2303.01923
tags:
- data
- tree
- terminal
- trees
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bayesian CART models for claims frequency
  modelling in non-life insurance pricing. The authors propose using Bayesian CART
  with three distributions - Poisson, negative binomial, and zero-inflated Poisson
  - to address imbalanced insurance claims data.
---

# Bayesian CART models for insurance claims frequency

## Quick Facts
- arXiv ID: 2303.01923
- Source URL: https://arxiv.org/abs/2303.01923
- Reference count: 40
- One-line primary result: Bayesian CART models with Poisson, negative binomial, and zero-inflated Poisson distributions improve prediction accuracy and interpretability for insurance claims frequency modeling

## Executive Summary
This paper introduces Bayesian CART models for claims frequency modeling in non-life insurance pricing, addressing the challenge of imbalanced insurance claims data. The authors propose using Bayesian CART with three distributions - Poisson, negative binomial, and zero-inflated Poisson - to improve upon standard CART models by using a global stochastic search rather than greedy local splitting. They develop a general MCMC algorithm with data augmentation and introduce DIC for model selection, demonstrating that the models can improve prediction accuracy while maintaining interpretability, which is essential for fair and transparent insurance premiums.

## Method Summary
The paper presents Bayesian CART models for insurance claims frequency with three distributions: Poisson, negative binomial, and zero-inflated Poisson. The approach uses a general MCMC algorithm with data augmentation to explore the space of possible tree structures through proposals like grow, prune, change, and swap. The models incorporate exposure time as an offset in the log-linear predictor and use DIC for model selection. The MCMC algorithm maintains a posterior distribution over tree structures and uses latent variables to enable Bayesian inference with non-Gaussian distributions.

## Key Results
- The ZIP model with exposure embedded in the zero mass component performs best on real insurance data
- BCART models improve over standard CART by using global stochastic search to find better tree partitions
- The DIC-based model selection effectively identifies trees with optimal complexity and accuracy
- The proposed models successfully classify policyholders into risk groups and select important risk factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian CART models improve over standard CART by using a global stochastic search rather than greedy local splitting.
- Mechanism: The model maintains a posterior distribution over tree structures and uses MCMC to explore the space of possible trees. Proposals like grow, prune, change, and swap allow it to correct earlier splits and find better partitions.
- Core assumption: The posterior distribution over trees is sufficiently peaked around good models that local exploration after convergence can find high-probability structures.
- Evidence anchors:
  - [abstract] "BCART model generates a much better tree by an effective Bayesian-motivated stochastic search algorithm"
  - [section 2.3] "This has been justified by simulation examples (with Gaussian distributed data) in the aforementioned papers"
  - [corpus] Weak - no direct citations supporting this mechanism
- Break condition: If the posterior is too flat or multimodal with distant modes, the MCMC may get stuck in suboptimal regions.

### Mechanism 2
- Claim: Data augmentation enables Bayesian CART to handle non-Gaussian distributions like Poisson, Negative Binomial, and Zero-Inflated Poisson.
- Mechanism: Latent variables are introduced to create a data-augmented likelihood where integration over parameters becomes tractable. The MCMC then alternates between sampling the latent variables and the tree structure.
- Core assumption: The augmented likelihood has a closed-form expression that allows efficient posterior computation.
- Evidence anchors:
  - [section 2.4] "a data augmentation method in implementing the MCMC algorithm"
  - [section 3.2.1] "we shall treat the parameter κt as known in the Bayesian framework which can be estimated upfront by using, e.g., the moment matching method"
  - [corpus] Weak - no direct citations supporting this mechanism
- Break condition: If the latent variable scheme is inefficient, convergence will be slow and the model may not capture the true distribution.

### Mechanism 3
- Claim: The Deviance Information Criterion (DIC) provides an effective model selection criterion for Bayesian CART models.
- Mechanism: DIC combines goodness of fit (deviance) with model complexity (effective number of parameters) to select trees that balance accuracy and parsimony.
- Core assumption: DIC is a reliable proxy for out-of-sample predictive performance in the context of tree models.
- Evidence anchors:
  - [section 2.5] "we propose to use the DIC for choosing appropriate γ, ρ, and thus introduce a three-step approach for selecting an 'optimal' tree"
  - [section 3.1] "The DIC of the tree T is obtained by using (10)"
  - [corpus] Weak - no direct citations supporting this mechanism
- Break condition: If the DIC is not well-calibrated for the specific distributions used, it may select overly complex or simple trees.

## Foundational Learning

- Concept: Posterior distribution over tree structures
  - Why needed here: The Bayesian framework requires specifying a prior over trees and updating it to a posterior given data
  - Quick check question: What are the two components of the tree prior p(T) as described in section 2.1?

- Concept: Data augmentation for non-Gaussian likelihoods
  - Why needed here: Standard conjugate priors don't exist for Poisson, NB, and ZIP distributions with tree parameters
  - Quick check question: What latent variable is introduced for the Poisson model in section 3.1?

- Concept: MCMC proposals for tree space
  - Why needed here: The tree space is discrete and high-dimensional, requiring specialized moves to explore efficiently
  - Quick check question: What are the four types of proposals used in Algorithm 1?

## Architecture Onboarding

- Component map: Tree prior specification -> Likelihood function (with data augmentation if needed) -> MCMC algorithm with proposals -> DIC-based model selection
- Critical path: 1) Specify tree prior and likelihood 2) Implement MCMC with appropriate proposals 3) Run multiple restarts to avoid local modes 4) Select optimal tree using DIC
- Design tradeoffs: Using data augmentation adds computational complexity but enables handling non-Gaussian distributions. The DIC-based selection is efficient but may not always select the true optimal model.
- Failure signatures: If the MCMC gets stuck in local modes, the DIC may select suboptimal trees. If the data augmentation is inefficient, convergence will be slow.
- First 3 experiments:
  1. Run P-BCART on a simple Poisson dataset with known structure to verify it can recover the true tree
  2. Compare P-BCART and P-CART on the chessboard data from section 4.1.1 to demonstrate the advantage of the Bayesian approach
  3. Test the DIC-based model selection by fitting trees with different numbers of terminal nodes and verifying DIC selects the one closest to the true structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Bayesian CART model compare in terms of stability and interpretability to other tree-based models like Random Forests or Gradient Boosting when applied to insurance claims data?
- Basis in paper: [explicit] The paper discusses the stability of the proposed BCART models and compares them to CART models, but does not compare them to other ensemble methods like Random Forests or Gradient Boosting.
- Why unresolved: The paper focuses on the advantages of BCART models over CART models and does not explore how they perform in comparison to other popular tree-based ensemble methods.
- What evidence would resolve it: Empirical studies comparing the performance of BCART models to Random Forests and Gradient Boosting on the same insurance claims datasets, focusing on stability and interpretability metrics.

### Open Question 2
- Question: What are the computational trade-offs between using more sophisticated proposals in the MCMC algorithm for tree exploration versus the four common proposals (Grow, Prune, Change, and Swap) in terms of mixing and convergence?
- Basis in paper: [explicit] The paper mentions that other proposals like those in [38, 39] could improve the mixing of simulated trees but suspects this will significantly increase computational time.
- Why unresolved: The paper does not provide empirical evidence on the trade-offs between using more sophisticated proposals and the four common proposals in terms of computational efficiency and model performance.
- What evidence would resolve it: Computational studies comparing the performance of the four common proposals with more sophisticated proposals on various insurance claims datasets, measuring mixing, convergence, and computational time.

### Open Question 3
- Question: How does the choice of prior for the tree topology (parameters γ and ρ) affect the performance of the BCART models in terms of model selection and prediction accuracy?
- Basis in paper: [explicit] The paper discusses the choice of prior for the tree topology and its effect on the expected number of terminal nodes but does not explore how this choice affects model selection and prediction accuracy.
- Why unresolved: The paper provides a general framework for choosing the prior for the tree topology but does not investigate the impact of this choice on the overall performance of the BCART models.
- What evidence would resolve it: Sensitivity analyses examining the effect of different choices of γ and ρ on the performance of BCART models in terms of model selection and prediction accuracy on various insurance claims datasets.

## Limitations
- Data augmentation schemes for NB and ZIP models lack implementation details needed for reproduction
- DIC-based model selection has limited empirical validation in this specific context
- Computational complexity of MCMC with data augmentation may be impractical for large-scale insurance datasets
- Prior specifications for terminal node parameters are only partially specified

## Confidence
- **High confidence**: The basic Bayesian CART framework and MCMC implementation for the Poisson model (sections 2.1-2.3)
- **Medium confidence**: The data augmentation approach for NB and ZIP models (section 2.4) - conceptually sound but implementation details are sparse
- **Medium confidence**: The DIC-based model selection criterion (section 2.5) - theoretically justified but limited empirical validation
- **Low confidence**: The practical performance on real insurance data - the real data example is limited to one dataset without extensive validation

## Next Checks
1. **Prior sensitivity analysis**: Test the impact of different prior specifications for terminal node parameters (λt for Poisson, λt and µt for ZIP) on model performance and selected tree structures.
2. **Convergence diagnostics**: Implement comprehensive convergence diagnostics for the MCMC algorithm, including multiple chains with different initializations, to verify the algorithm consistently finds high-probability tree structures.
3. **Cross-validation study**: Conduct k-fold cross-validation on multiple real insurance datasets to validate the out-of-sample predictive performance of the proposed models compared to standard CART and other benchmark methods.