---
ver: rpa2
title: An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models
arxiv_id: '2309.09958'
source_url: https://arxiv.org/abs/2309.09958
tags:
- llav
- performance
- data
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of scaling up large multimodal
  models (LMMs) by training LLaVA with larger language models (33B and 65B parameters)
  and exploring parameter-efficient tuning methods like LoRA and QLoRA. The study
  also examines the effects of higher image resolutions and mixing multimodal-language
  data.
---

# An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models

## Quick Facts
- arXiv ID: 2309.09958
- Source URL: https://arxiv.org/abs/2309.09958
- Reference count: 22
- Primary result: Scaling LMMs from 13B to 33B/65B consistently improves performance; LoRA/QLoRA offers comparable results to full fine-tuning at lower cost

## Executive Summary
This paper investigates the impact of scaling up large multimodal models (LMMs) by training LLaVA with larger language models (33B and 65B parameters) and exploring parameter-efficient tuning methods like LoRA and QLoRA. The study systematically examines how model size, image resolution, and data mixing affect LMM performance across multiple benchmarks. The findings demonstrate that scaling LMMs consistently enhances both multimodal and language capabilities, while LoRA/QLoRA tuning provides an efficient alternative to full-model fine-tuning. The research establishes stronger baselines for future LMM development by highlighting the importance of higher image resolutions and multimodal-language data mixing.

## Method Summary
The study employs a two-stage LLaVA training approach: Stage 1 aligns visual features with language embeddings using LAION-CC-SBU data, while Stage 2 performs visual instruction tuning with the LLaVA-80K dataset. Models ranging from 7B to 65B parameters are fine-tuned using full-model fine-tuning, LoRA (rank 8, 64), and QLoRA methods. The training leverages DeepSpeed ZeRO optimizers for distributed computation. The research systematically varies image resolutions (224×224 vs 336×336) and mixes multimodal data with language-only instruction data from ShareGPT. Performance is evaluated across multiple benchmarks including LLaVA-Bench, MM-VET, MM-Bench, Vicuna-80, and MMLU to assess multimodal understanding, reasoning capabilities, and language proficiency.

## Key Results
- Scaling LMMs from 13B to 33B and 65B parameters consistently improves performance across all benchmarks
- LoRA/QLoRA tuning achieves comparable results to full-model fine-tuning while reducing computational cost
- Higher image resolutions (336×336) provide 2-3 point improvements over 224×224 resolution
- Mixing language-only instruction data with multimodal data improves multimodal performance

## Why This Works (Mechanism)

### Mechanism 1
Larger language models improve both multimodal and pure language capabilities when fine-tuned with visual instruction data. Scaling up the LLM backbone increases capacity to encode richer representations of both visual and linguistic information, allowing better alignment of visual features with language embeddings during two-stage training. The increased parameter count enables the model to store more knowledge and maintain stronger language responding capability even when trained on multimodal data.

### Mechanism 2
LoRA/QLoRA tuning provides comparable performance to full-model fine-tuning at significantly lower cost. LoRA introduces low-rank updates to model weights, allowing efficient adaptation without modifying all parameters. QLoRA further compresses weights during training, reducing memory requirements while maintaining representational capacity. The low-rank decomposition captures essential changes needed for adaptation, and quantized representation doesn't lose critical information for the task.

### Mechanism 3
Higher image resolutions and mixing multimodal-language data improve LMM performance. Higher resolution images provide more visual detail for the vision encoder to process, leading to richer feature representations. Mixing language-only instruction data helps maintain and potentially enhance the model's language capabilities during multimodal training. The vision encoder can effectively process higher resolution inputs, and the language model benefits from additional language instruction data even when the primary task is multimodal.

## Foundational Learning

- **Two-stage training for LMMs**: Feature alignment followed by visual instruction tuning. Why needed: This approach allows efficient adaptation of large language models to multimodal tasks by first aligning visual features with the language embedding space, then fine-tuning on instruction data. Quick check: What are the two stages of training described in the paper, and what is the purpose of each stage?

- **Parameter-efficient fine-tuning methods (LoRA, QLoRA)**: These methods enable fine-tuning of very large models (33B, 65B parameters) with limited computational resources by only updating a small subset of parameters. Quick check: How do LoRA and QLoRA differ from full-model fine-tuning in terms of parameter updates and computational cost?

- **Multimodal evaluation benchmarks (LLaVA-Bench, MM-VET)**: These benchmarks provide standardized ways to evaluate LMM performance on real-world tasks requiring both visual understanding and language reasoning. Quick check: What are the main differences between LLaVA-Bench and MM-VET in terms of the capabilities they evaluate?

## Architecture Onboarding

- **Component map**: Image → Vision encoder → Visual features → Linear projection → Language embedding space → Language model → Response generation

- **Critical path**: Input image → Vision encoder → Visual features → Linear projection → Language embedding space → Multimodal input → Language model → Response generation

- **Design tradeoffs**: Model size vs. computational cost (larger models perform better but require more resources); Image resolution vs. feature richness (higher resolution provides better visual information but increases computational load); Fine-tuning method vs. performance (full-model fine-tuning gives best performance but LoRA/QLoRA offers good trade-offs)

- **Failure signatures**: Poor visual understanding (check vision encoder output and projection layer alignment); Weak language responses (verify language model initialization and instruction data quality); Memory issues during training (consider QLoRA or reduce batch size)

- **First 3 experiments**: 1) Test basic functionality by running inference on a simple image-text pair to verify the pipeline works; 2) Compare 7B vs 13B models on a small benchmark to observe scaling effects; 3) Test parameter-efficient tuning by comparing LoRA vs full fine-tuning on a subset of the data to validate the trade-off

## Open Questions the Paper Calls Out

### Open Question 1
How does scaling the vision encoder (beyond just using higher resolution inputs) impact the visual capabilities and performance of LMMs on vision recognition and understanding tasks? The paper focuses on scaling the language model size and data curation but does not explore architectural changes to the vision encoder itself. Experiments comparing different vision encoder architectures and their impact on LMM performance on vision benchmarks would resolve this.

### Open Question 2
How does the effectiveness of parameter-efficient methods like LoRA/QLoRA scale with model size and what are the optimal hyperparameters for very large models (e.g., 100B+ parameters)? The study is limited to models up to 65B parameters and does not systematically explore the full hyperparameter space or the scaling behavior for much larger models. Comprehensive ablation studies varying LoRA rank, learning rate, and alpha across a wider range of model sizes would resolve this.

### Open Question 3
How does the choice and mixing of training data (multimodal vs. language-only) impact the trade-off between multimodal and language capabilities in LMMs, and what are the optimal data curation strategies? The study uses a relatively small dataset and does not systematically explore different data mixing ratios or the impact on the balance between multimodal and language capabilities. Experiments with much larger and more diverse datasets, varying the ratio of multimodal to language-only data would resolve this.

## Limitations

- Scalability uncertainty: The effectiveness of LoRA/QLoRA at even larger scales (175B+ parameters) remains unknown, particularly for QLoRA where quantization artifacts may compound with scale
- Benchmark generalization: The study evaluates on established benchmarks but doesn't test on more challenging or diverse real-world scenarios
- Reproducibility challenges: The training relies on DeepSpeed with specific configurations for 33B-65B models, and without detailed hyperparameter specifications, reproducing these results may be challenging

## Confidence

- **High confidence**: Scaling LMMs from 13B to 33B/65B consistently improves performance on established benchmarks, and higher image resolutions provide measurable gains (2-3 points)
- **Medium confidence**: LoRA/QLoRA performance comparable to full fine-tuning at lower cost, though optimal rank configurations need more systematic exploration
- **Medium confidence**: Visual instruction tuning can improve pure language capabilities, but requires additional validation on language-only tasks

## Next Checks

1. **Ablation study on data mixing**: Systematically test the contribution of language-only instruction data versus multimodal data by training models with only language data, only multimodal data, and various mixing ratios to quantify the marginal benefit of each component

2. **Cross-benchmark generalization**: Evaluate the 33B and 65B models on additional benchmarks not used in the paper (e.g., GQA, NLVR2, or real-world visual question answering datasets) to assess whether performance gains generalize beyond the tested benchmarks

3. **Efficiency analysis at scale**: Conduct a comprehensive cost-performance analysis comparing LoRA, QLoRA, and full fine-tuning across different parameter counts (7B, 13B, 33B, 65B) including wall-clock time, GPU memory usage, and final task performance to establish clear scaling guidelines