---
ver: rpa2
title: 'Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:
  Optimal Day-Ahead Energy Scheduling'
arxiv_id: '2310.01758'
source_url: https://arxiv.org/abs/2310.01758
tags:
- linearization
- relu
- neural
- degradation
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes four linearization models for the ReLU activation
  function in neural networks embedded within optimization problems, specifically
  for day-ahead microgrid energy scheduling. The methods (BPWL, CTAR, P-CTAR, PCAR)
  replace the ReLU with linear constraints, making otherwise intractable problems
  solvable.
---

# Linearization of ReLU Activation Function for Neural Network-Embedded Optimization: Optimal Day-Ahead Energy Scheduling

## Quick Facts
- arXiv ID: 2310.01758
- Source URL: https://arxiv.org/abs/2310.01758
- Reference count: 24
- This paper proposes four linearization models for the ReLU activation function in neural networks embedded within optimization problems, specifically for day-ahead microgrid energy scheduling.

## Executive Summary
This paper addresses the challenge of integrating neural networks with non-linear activation functions into optimization problems by proposing four linearization models for the ReLU activation function. The methods enable practical optimization of neural-network-embedded systems by replacing non-linear ReLU functions with linear constraints. Applied to a day-ahead microgrid energy scheduling problem with a neural network-based battery degradation model, the methods demonstrate varying trade-offs between accuracy and computational efficiency.

## Method Summary
The paper proposes four linearization models (BPWL, CTAR, P-CTAR, PCAR) to replace ReLU activation functions in neural networks embedded within optimization problems. BPWL uses binary variables for exact linearization, CTAR uses linear triangular approximations, while P-CTAR and PCAR add penalty terms to improve approximation accuracy. These methods are applied to a neural network battery degradation model within a microgrid day-ahead scheduling optimization, solved using Gurobi with Pyomo.

## Key Results
- BPWL achieved zero linearization error but required longer solving time (28s)
- P-CTAR offered the best trade-off with low error (1.14%) and faster solving time (8.75s)
- P-CTAR was identified as the most effective model, balancing accuracy and computational efficiency
- The linearization methods enable practical integration of neural networks into optimization problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The BPWL method achieves zero linearization error by introducing binary variables to exactly represent ReLU activation states.
- Mechanism: BPWL uses big-M constraints and binary variables (δh_i) to split each neuron into two linear regions: one where the neuron is inactive (value = 0) and one where it is active (value = xh_i). This exact reformulation removes all approximation error.
- Core assumption: The big-M value is sufficiently large to cover all possible neuron pre-activation values.
- Evidence anchors:
  - [abstract] "BPWL achieved zero error but required longer solving time"
  - [section] "the BPWL model offers the distinct advantage of perfectly linearizing the ReLU activation function without any reformulation losses"
  - [corpus] No direct evidence in corpus; this is a novel contribution
- Break condition: If big-M is too small or too large, the formulation becomes either infeasible or numerically unstable.

### Mechanism 2
- Claim: CTAR approximates ReLU by bounding neuron values within linear triangular regions defined by lower and upper bounds.
- Mechanism: CTAR uses linear constraints to create a triangular feasible region that approximates the ReLU function. The approximation error depends on how well the triangle captures the true ReLU behavior.
- Core assumption: The bounds LB and UB can be estimated from the neural network training data.
- Evidence anchors:
  - [section] "The proposed CTAR approximate the ReLU function at each neuron with (4) and (6)-(7), which constrains the feasible solution set"
  - [section] "the blue area denotes the feasible region, delimited by the lower bound (LB) and upper bound (UB)"
  - [corpus] No direct evidence in corpus; this is a novel contribution
- Break condition: If LB and UB are poorly chosen, the approximation error becomes unacceptably large.

### Mechanism 3
- Claim: P-CTAR and PCAR add penalty terms to the objective function to push approximated solutions toward true ReLU behavior.
- Mechanism: Both methods introduce penalty costs that encourage the linear approximation to stay close to the actual ReLU activation values. P-CTAR uses triangular regions with penalties, while PCAR uses unbounded regions with penalties.
- Core assumption: The penalty constants are appropriately tuned for the specific optimization problem.
- Evidence anchors:
  - [section] "incorporating a penalty term ch into the objective function... This penalty encourages the nonnegative variable ah_i to be positioned closer to the lower two sides"
  - [section] "P-CTAR stands out as the best performance model in terms of linearization error among the rest of the models"
  - [corpus] No direct evidence in corpus; these are novel contributions
- Break condition: If penalty constants are too small, the approximation error remains high; if too large, the optimization becomes numerically unstable.

## Foundational Learning

- Concept: ReLU activation function properties
  - Why needed here: Understanding that ReLU(x) = max(0, x) creates piecewise linearity is crucial for developing linearization methods
  - Quick check question: What happens to the gradient of ReLU when x < 0?

- Concept: Mixed-integer linear programming (MILP)
  - Why needed here: BPWL requires binary variables and big-M constraints, which are MILP techniques
  - Quick check question: How does adding binary variables affect the complexity class of an optimization problem?

- Concept: Approximation theory in optimization
  - Why needed here: Understanding how to bound and minimize approximation errors is essential for evaluating CTAR, P-CTAR, and PCAR methods
  - Quick check question: What is the difference between an exact reformulation and an approximation in optimization?

## Architecture Onboarding

- Component map: Input layer (5 features) -> Hidden layer 1 (20 neurons) -> Hidden layer 2 (10 neurons) -> Output layer (1 neuron) -> Linearization wrapper (BPWL/CTAR/P-CTAR/PCAR) -> Optimization solver (Gurobi)
- Critical path: Neural network input → ReLU linearization → Optimization constraints → Battery degradation calculation → Objective function evaluation
- Design tradeoffs:
  - Accuracy vs. solving time: BPWL (high accuracy, slow) vs. PCAR (lower accuracy, fast)
  - Constraint complexity: CTAR (moderate) vs. P-CTAR (moderate + penalties)
  - Penalty tuning: P-CTAR and PCAR require careful selection of penalty constants
- Failure signatures:
  - Long solving times: Likely BPWL on large systems
  - High approximation error: Poor choice of bounds (CTAR) or penalty constants (P-CTAR/PCAR)
  - Numerical instability: Big-M values too large or penalty constants too extreme
- First 3 experiments:
  1. Compare solving times for BPWL vs. P-CTAR on a small microgrid case
  2. Test sensitivity of P-CTAR to different penalty constant values (ch)
  3. Evaluate linearization error when using CTAR with different LB/UB bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal penalty constant value (ch) for P-CTAR linearization across different neural network architectures and optimization problem types?
- Basis in paper: [explicit] The paper states "the selection of ch demands careful consideration and thorough preliminary testing to ensure the optimal performance of the P-CTAR model" and notes that the optimal value varies by problem.
- Why unresolved: The paper only tests one specific case (NNBD-integrated microgrid scheduling) and acknowledges that "if the optimization model undergoes any modifications, it would require recalibration to determine the ideal ch value."
- What evidence would resolve it: Systematic sensitivity analysis across diverse neural network architectures (different numbers of layers, activation functions) and optimization problem types (unit commitment, optimal power flow, etc.) with performance metrics to establish generalizable patterns or guidelines for selecting ch.

### Open Question 2
- Question: How does the computational complexity of the linearization methods scale with the size of the neural network and optimization problem?
- Basis in paper: [inferred] The paper notes BPWL "requires a relatively longer solving time due to the exponential addition of new binary variables" and mentions solving times for small-scale test cases, but doesn't analyze scaling behavior.
- Why unresolved: The paper only presents results for a single microgrid case and states that "it will increase significantly for larger systems" without providing quantitative analysis of how scaling affects each method's performance trade-offs.
- What evidence would resolve it: Empirical testing of the linearization methods on progressively larger neural networks and optimization problems with detailed computational complexity analysis, including how solution times and accuracy scale with network size and problem dimension.

### Open Question 3
- Question: How sensitive are the linearization methods to the choice of lower and upper bounds (LB and UB) in CTAR and P-CTAR?
- Basis in paper: [explicit] The paper states "different choices for LB and UB can impact the performance of the CTAR linearization method" and notes that "the chosen lower bound and upper bound values for CTAR are appropriate and effective in this testbed."
- Why unresolved: The paper only tests one specific LB/UB setting for the microgrid case and acknowledges that performance may vary, but doesn't provide guidance on how to select these bounds for different problems or analyze their sensitivity.
- What evidence would resolve it: Systematic sensitivity analysis varying LB and UB across a range of values for different types of neural networks and optimization problems, with performance metrics to identify robust bound selection strategies or rules of thumb.

## Limitations
- Exact neural network training data, weight/bias values, and normalization parameters are not provided
- Big-M constant value and penalty constant (ch) ranges are unspecified
- Results are only demonstrated on a single microgrid case without scaling analysis

## Confidence

- **High confidence**: The fundamental approach of replacing ReLU with linear constraints is well-established in optimization theory. The classification of methods (exact vs. approximate) and their general trade-offs (accuracy vs. solving time) are supported by the reported results.
- **Medium confidence**: The specific linearization formulations (BPWL, CTAR, P-CTAR, PCAR) appear novel and are not directly supported by existing literature in the corpus. While the mathematical descriptions are clear, the practical effectiveness depends heavily on parameter choices not specified in the paper.
- **Low confidence**: The microgrid system specifications, data sources (Pecan Street Dataport, ERCOT), and neural network architecture are described, but without access to the actual datasets and trained model parameters, full validation is impossible.

## Next Checks

1. **Parameter sensitivity analysis**: Test P-CTAR and PCAR methods across a range of penalty constant values (ch) to determine the sensitivity of linearization error and solving time to this critical parameter.

2. **Big-M value impact**: For BPWL, systematically vary the big-M constant to identify the threshold where numerical stability is compromised versus where the formulation remains exact but computationally tractable.

3. **Scaling study**: Evaluate how linearization error and solving time scale with microgrid size (number of nodes, time periods) for each method to identify the practical limits of each approach.