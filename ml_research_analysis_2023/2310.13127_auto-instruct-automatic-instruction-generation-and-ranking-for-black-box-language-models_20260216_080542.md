---
ver: rpa2
title: 'Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box
  Language Models'
arxiv_id: '2310.13127'
source_url: https://arxiv.org/abs/2310.13127
tags:
- instruction
- instructions
- tasks
- task
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Auto-Instruct is an automatic method for generating and ranking
  instructions for large language models (LLMs) in the true few-shot setting. The
  core idea is to first prompt the LLM to generate a diverse set of candidate instructions
  for a task, using different styles and demonstration tasks.
---

# Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models

## Quick Facts
- arXiv ID: 2310.13127
- Source URL: https://arxiv.org/abs/2310.13127
- Authors: 
- Reference count: 31
- Primary result: Auto-Instruct achieves 64.35 ROUGE-L on SuperNI and 52.04 accuracy on BBH in few-shot settings

## Executive Summary
Auto-Instruct is an automatic method for generating and ranking instructions for large language models (LLMs) in the true few-shot setting. The core idea is to first prompt the LLM to generate a diverse set of candidate instructions for a task, using different styles and demonstration tasks. Then, a ranking model trained on 575 NLP tasks is used to select the best instruction for each test example based on estimated downstream performance. This approach addresses the challenge of manual instruction engineering for black-box LLMs, which is laborious and subjective. Auto-Instruct significantly outperforms human-written instructions and existing baselines, achieving 64.35 ROUGE-L on SuperNI and 52.04 accuracy on BBH in few-shot settings. It also generalizes well to other LLMs like ChatGPT and GPT-4 without retraining, and shows consistent improvements in zero-shot settings.

## Method Summary
Auto-Instruct operates in three stages: (1) Generate diverse candidate instructions using 7 meta-prompts (4 style-specific plus 3 groups of human-written demonstrations) to prompt text-davinci-003, producing 22 instructions per task; (2) Train a FLAN-T5-Large instruction ranking model on 575 NLP tasks to predict scores for instruction-example pairs, optimized to align with downstream ROUGE-L performance; (3) For each test example, rank the 22 candidate instructions using the ranking model and select the top-ranked instruction for downstream inference with text-davinci-003. The method is designed for true few-shot and zero-shot settings where no model access or additional training data is available.

## Key Results
- Auto-Instruct achieves 64.35 ROUGE-L on SuperNI, outperforming human-written instructions and existing baselines
- Zero-shot generalization to ChatGPT and GPT-4 without retraining, showing consistent improvements
- 52.04 accuracy on BBH in few-shot settings, demonstrating strong performance across diverse tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse meta-prompts enable generation of instruction variations that better match unseen test examples.
- Mechanism: The LLM is prompted with 4 style-specific meta-prompts (one-sentence, one-paragraph, step-by-step, example-explaining) plus 3 groups of human-written instruction demonstrations. This exposes the model to different granularities and styles, increasing the likelihood that at least one instruction will align well with a given test example's complexity and format.
- Core assumption: Different downstream tasks and examples benefit from different instruction styles, and a single prompt cannot capture this diversity.
- Evidence anchors:
  - [abstract] "leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task"
  - [section 4.1] "We collect a diverse candidate set by specifying the expected style of each instruction"
  - [corpus] Weak: No direct corpus evidence; relies on controlled experiments.
- Break condition: If all generated instructions consistently fail on a task, indicating the meta-prompts do not cover the needed instruction style.

### Mechanism 2
- Claim: Instruction ranking model trained on 575 tasks generalizes to rank unseen instructions effectively.
- Mechanism: A FLAN-T5-Large model is trained to predict a score for each instruction-example pair, optimized to align with downstream ROUGE-L performance. By training on a large, diverse set of tasks, the model learns patterns linking instruction phrasing to effectiveness across domains.
- Core assumption: There exist transferable patterns between instruction phrasing and downstream performance that can be learned from diverse tasks.
- Evidence anchors:
  - [abstract] "ranks them using a scoring model trained on a variety of 575 existing NLP tasks"
  - [section 4.2.1] "train a generalizable instruction ranking model across a variety of NLP tasks"
  - [corpus] Weak: No direct citation; relies on empirical results in Table 1 and 3.
- Break condition: If the ranking model fails to outperform random selection on a held-out task set, indicating poor generalization.

### Mechanism 3
- Claim: Auto-Instruct improves over human-written instructions by finding instructions closer in embedding space to optimal ones.
- Mechanism: The ranking model selects instructions whose embeddings are more similar to the best-performing instruction embeddings (as measured by cosine similarity), thus refining the human seed instruction toward the ideal form for the task.
- Core assumption: Embedding similarity correlates with instruction effectiveness; the best instruction is nearby in embedding space to the selected one.
- Evidence anchors:
  - [section 5.4.5] "Auto-Instruct is able to provide instructions more similar to the optimal ones among the candidates"
  - [section 5.4.5] "the selected instruction refines the human-written seed instruction by progressing towards the ideal solution"
  - [corpus] Weak: No direct citation; based on internal analysis in Figure 6.
- Break condition: If selected instructions have low cosine similarity to optimal ones yet still perform well, invalidating the embedding-based refinement hypothesis.

## Foundational Learning

- Concept: In-context learning with black-box LLMs
  - Why needed here: Auto-Instruct operates entirely in the true few-shot setting where no model access or additional training data is available; understanding in-context learning mechanics is essential to design effective prompts and rank instructions.
  - Quick check question: What is the difference between few-shot and zero-shot prompting, and why does instruction quality matter more in the latter?

- Concept: Instruction tuning and its impact on LLM performance
  - Why needed here: The work builds on instruction-tuned models (FLAN-T5) and assumes that different instructions yield different downstream performances; understanding this relationship is key to the ranking objective.
  - Quick check question: How does the choice of instruction affect LLM output, and what evidence supports this sensitivity?

- Concept: Text embedding similarity and its use in instruction selection
  - Why needed here: The analysis uses cosine similarity between instruction embeddings (text-embedding-ada-002 and MPNet) to quantify how close the selected instruction is to the optimal one; familiarity with embeddings is needed to interpret results.
  - Quick check question: What does a high cosine similarity between two instruction embeddings indicate about their semantic content?

## Architecture Onboarding

- Component map:
  - Meta-prompt generator → LLM (text-davinci-003) → Candidate instruction set (22 per task)
  - Instruction ranking model (FLAN-T5-Large) → Scores for each instruction-example pair
  - Selection module → Best instruction per example
  - Downstream LLM → Final output

- Critical path: Meta-prompt → LLM generation → Ranking model scoring → Instruction selection → Downstream inference

- Design tradeoffs:
  - Instruction diversity vs. generation cost: More meta-prompts yield better coverage but increase API calls and cost.
  - Ranking model size vs. generalizability: Larger models may rank better but are slower and costlier to train.
  - Sampling vs. exhaustive evaluation: Sampling 8 instructions for training speeds up training but may miss rare high-performing instructions.

- Failure signatures:
  - Low variance in candidate instruction performance → Meta-prompts not generating diverse enough instructions.
  - Ranking model performance near random → Poor generalization from training tasks to test tasks.
  - Downstream performance worse than human instruction → Ranking model selecting suboptimal instructions.

- First 3 experiments:
  1. Generate candidate instructions for a held-out task using all meta-prompts; manually inspect diversity and quality.
  2. Train ranking model on a small subset (e.g., 50 tasks) and evaluate on a validation task; check if it outperforms random selection.
  3. Run end-to-end Auto-Instruct pipeline on a single test task; compare selected instruction performance to human and random baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Auto-Instruct perform on non-English language tasks?
- Basis in paper: The paper mentions that the scope of the study is limited to English language tasks and that non-English tasks are not part of the training data.
- Why unresolved: The authors explicitly state that the model might not perform satisfactorily for non-English tasks due to the lack of training data in other languages. They suggest that further investigation into generating cross-lingual instructions is left for future work.
- What evidence would resolve it: Experimental results on a diverse set of non-English language tasks would provide concrete evidence of the model's performance and limitations in multilingual settings.

### Open Question 2
- Question: How much does the phrasing of meta-prompts influence the quality of generated instructions?
- Basis in paper: The authors acknowledge that while they use a wide range of meta-prompts to mitigate reliance on prompt engineering, the phrasing of these meta-prompts could still influence the quality of the generated instructions.
- Why unresolved: The paper does not provide a systematic analysis of how different phrasings of meta-prompts affect the generated instructions' quality or downstream performance. The exploration of automatically diversifying the generated instructions is left for future work.
- What evidence would resolve it: A controlled experiment comparing the performance of Auto-Instruct using different phrasings of meta-prompts would quantify the impact of meta-prompt phrasing on instruction quality and downstream task performance.

### Open Question 3
- Question: What is the optimal number of candidate instructions to generate and rank for each task?
- Basis in paper: The authors generate 22 candidate instructions per task (3 for each of 7 meta-prompts, plus the original seed instruction) but do not explore the effect of varying this number on performance.
- Why unresolved: The paper does not investigate whether generating more or fewer candidate instructions would lead to better downstream performance. The computational cost of generating and ranking more instructions is also not discussed.
- What evidence would resolve it: An ablation study varying the number of candidate instructions (e.g., 5, 10, 15, 22, 30) and measuring the corresponding performance on downstream tasks would identify the optimal number of instructions to generate and rank.

## Limitations
- Reliance on OpenAI's text-davinci-003 for both instruction generation and downstream inference creates dependency on a specific model and API
- Instruction ranking model's performance is sensitive to the quality and diversity of the 575 training tasks, with no detailed analysis of task distribution or potential biases
- Evaluation focuses on ROUGE-L and accuracy metrics, which may not capture all aspects of instruction quality, particularly for tasks requiring reasoning or creativity

## Confidence

- **High confidence**: The claim that Auto-Instruct significantly outperforms human-written instructions and existing baselines (64.35 ROUGE-L on SuperNI, 52.04 accuracy on BBH) is supported by strong empirical evidence in Tables 1 and 3. The zero-shot generalization to ChatGPT and GPT-4 without retraining is also well-demonstrated in Table 2.
- **Medium confidence**: The claim that diverse meta-prompts enable generation of instruction variations that better match unseen test examples is plausible but relies on controlled experiments rather than direct evidence of mechanism. The assertion that the ranking model generalizes to rank unseen instructions effectively is supported by results but could be sensitive to the specific task distribution used for training.
- **Low confidence**: The claim that Auto-Instruct improves over human-written instructions by finding instructions closer in embedding space to optimal ones is based on internal analysis (Figure 6) and assumes that embedding similarity correlates with instruction effectiveness, which is not universally established.

## Next Checks

1. **Cross-LLM Robustness Test**: Evaluate Auto-Instruct's performance using a different instruction-tuned LLM (e.g., FLAN-T5 or LLaMA) for both instruction generation and downstream inference to verify that improvements are not specific to OpenAI's models.

2. **Ablation on Training Task Diversity**: Systematically reduce the number and diversity of training tasks for the ranking model and measure the impact on performance to quantify how much task variety is truly necessary for effective generalization.

3. **Cost-Benefit Analysis**: Measure the wall-clock time and API costs of the complete Auto-Instruct pipeline (22 instruction generations + ranking + downstream inference) versus human instruction engineering, and analyze whether the performance gains justify the computational overhead in practical applications.