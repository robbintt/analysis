---
ver: rpa2
title: Non-Cartesian Self-Supervised Physics-Driven Deep Learning Reconstruction for
  Highly-Accelerated Multi-Echo Spiral fMRI
arxiv_id: '2312.05707'
source_url: https://arxiv.org/abs/2312.05707
tags:
- fmri
- non-cartesian
- spiral
- multi-echo
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work developed a non-Cartesian version of self-supervised
  physics-driven deep learning (PD-DL) for efficient training of non-Cartesian PD-DL
  reconstructions, and showed its potential in highly-accelerated multi-echo spiral
  fMRI. The main challenge was the time and memory consuming of the non-uniform Fourier
  transform (NUFFT) operation for gridding and regridding in non-Cartesian MRI.
---

# Non-Cartesian Self-Supervised Physics-Driven Deep Learning Reconstruction for Highly-Accelerated Multi-Echo Spiral fMRI

## Quick Facts
- arXiv ID: 2312.05707
- Source URL: https://arxiv.org/abs/2312.05707
- Reference count: 0
- This work developed a non-Cartesian version of self-supervised physics-driven deep learning (PD-DL) for efficient training of non-Cartesian PD-DL reconstructions, and showed its potential in highly-accelerated multi-echo spiral fMRI.

## Executive Summary
This paper presents a novel non-Cartesian self-supervised physics-driven deep learning (non-Cartesian SSDU) reconstruction method for highly-accelerated multi-echo spiral fMRI. The method addresses the computational challenges of non-uniform Fourier transform (NUFFT) operations by employing Toeplitz decomposition for fast implementation of forward and adjoint operators. The approach demonstrates substantial improvements in image quality over conventional techniques while preserving meaningful BOLD activation patterns at 10-fold acceleration.

## Method Summary
The proposed method uses a physics-driven deep learning framework with self-supervised learning adapted for non-Cartesian trajectories. The approach employs Toeplitz decomposition to accelerate NUFFT operations, enabling efficient training of the unrolled optimization network. Training uses a multi-mask self-supervised strategy where k-space is split into disjoint sets for network input and loss calculation. The network consists of T=10 unrolled blocks with alternating data fidelity and learned proximal operators, trained on 10-fold accelerated single-shot spiral fMRI data from 5 subjects.

## Key Results
- Substantial improvements in image quality over conventional techniques for 10-fold accelerated multi-echo spiral fMRI
- Meaningful activation patterns preserved in BOLD analysis despite 4.7-fold lower baseline SNR
- Achieved high spatio-temporal resolution with improved in-plane resolution (2 × 2 mm²) compared to single-echo low resolution spiral fMRI (3 × 3 mm²)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Toeplitz decomposition accelerates non-Cartesian MRI reconstruction by replacing costly NUFFT operations with FFT-based convolutions
- Mechanism: Forward and adjoint NUFFT operations are decomposed into FFT, zero-padding/cropping, and a fast operator MΘj, which is precomputable and much faster than iterative NUFFT
- Core assumption: Toeplitz approximation introduces acceptable error for training convergence and reconstruction quality
- Evidence anchors: Algorithmic description of Toeplitz decomposition, speed-up claims in literature [19]

### Mechanism 2
- Claim: Multi-mask self-supervised learning enables training without fully-sampled reference data by creating surrogate targets from disjoint k-space subsets
- Mechanism: Acquired k-space locations are split into disjoint sets Θj (used in network forward pass) and Λj (used for loss calculation)
- Core assumption: Disjoint masking strategy preserves sufficient information in Θj to enable reconstruction of complementary Λj regions
- Evidence anchors: Self-supervised learning literature, algorithmic description of multi-mask strategy

### Mechanism 3
- Claim: Physics-driven unrolled optimization with learned proximal operators provides better generalization than purely data-driven CNNs for highly accelerated MRI
- Mechanism: Network unrolls variable splitting optimization with alternating data fidelity and learned regularizer, embedding physics constraints directly into training
- Core assumption: Physics-based data fidelity term constrains solutions sufficiently to prevent overfitting even with limited training data
- Evidence anchors: Physics-driven DL literature, reconstruction quality improvements over conventional techniques

## Foundational Learning

- Concept: Non-uniform Fourier Transform (NUFFT) and gridding operations
  - Why needed here: Non-Cartesian spiral trajectories require NUFFT for forward and adjoint operations, which are computationally expensive and memory-intensive
  - Quick check question: Why can't we simply use standard FFT for non-Cartesian k-space sampling?

- Concept: Variable splitting and proximal operators in optimization
  - Why needed here: Physics-driven approach uses variable splitting to separate data fidelity and regularization, with learned proximal operators replacing hand-designed regularizers
  - Quick check question: How does the learned proximal operator differ from traditional regularization approaches?

- Concept: Self-supervised learning and multi-mask strategies
  - Why needed here: Enables training without fully-sampled reference data by creating surrogate targets from disjoint k-space subsets
  - Quick check question: What guarantees that the disjoint masking strategy provides sufficient information for reconstruction?

## Architecture Onboarding

- Component map: Zero-filled gridded images (Θj) -> Unrolled network (T=10 blocks) -> Image-domain reconstruction (Λj) -> Loss calculation -> Parameter updates
- Critical path: Data flow from zero-filled input → unrolled network → image-domain reconstruction → loss calculation → parameter updates
- Design tradeoffs:
  - Memory vs speed: Toeplitz decomposition trades memory for computational speed
  - Reconstruction quality vs acceleration: 10-fold acceleration with acceptable BOLD sensitivity
  - Resolution vs SNR: 2×2mm vs 3×3mm with 4.7-fold SNR difference
- Failure signatures:
  - Poor reconstruction quality at high acceleration factors
  - Unstable training due to NUFFT computational bottlenecks
  - Inadequate BOLD sensitivity in activation patterns
- First 3 experiments:
  1. Validate Toeplitz decomposition accuracy vs full NUFFT on small test dataset
  2. Test multi-mask self-supervised training with varying mask ratios (60-40, 70-30, 80-20)
  3. Compare reconstruction quality at different unroll depths (T=5, T=10, T=15)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of spiral arms and acceleration factor for multi-echo spiral fMRI to balance spatial resolution, temporal resolution, and BOLD sensitivity?
- Basis in paper: The paper discusses using 10-fold acceleration with a single spiral arm for multi-echo spiral fMRI, achieving 2x2mm² in-plane resolution. However, it also mentions that further sequence changes are needed for whole-brain coverage, potentially via simultaneous multi-slice imaging or 3D acquisitions.
- Why unresolved: The study only tested one acceleration factor (10-fold) and did not explore the trade-offs between spatial resolution, temporal resolution, and BOLD sensitivity at different acceleration factors or numbers of spiral arms.
- What evidence would resolve it: Systematic comparison of multi-echo spiral fMRI at different acceleration factors and numbers of spiral arms, evaluating spatial resolution, temporal resolution, and BOLD sensitivity.

### Open Question 2
- Question: How does the performance of non-Cartesian self-supervised physics-driven deep learning (PD-DL) reconstruction vary with different mask ratios and numbers of masks in the training process?
- Basis in paper: The paper mentions using 7 masks with a 60%-40% separation for Θ-Λ in the non-Cartesian SSDU training, but does not explore the impact of different mask ratios or numbers of masks on reconstruction quality.
- Why unresolved: The study used a specific configuration for masks without investigating the sensitivity of the reconstruction quality to different mask ratios or numbers of masks.
- What evidence would resolve it: Comparative analysis of reconstruction quality using different mask ratios and numbers of masks in the non-Cartesian SSDU training.

### Open Question 3
- Question: Can the proposed non-Cartesian SSDU method be extended to 3D acquisitions for whole-brain coverage, and what are the computational challenges associated with this extension?
- Basis in paper: The paper mentions that 3D acquisitions may be needed for whole-brain coverage but would necessitate advances in large-scale processing as outlined in [19].
- Why unresolved: The study did not explore the feasibility or challenges of extending the non-Cartesian SSDU method to 3D acquisitions for whole-brain coverage.
- What evidence would resolve it: Demonstration of the non-Cartesian SSDU method applied to 3D acquisitions, along with an analysis of computational challenges and potential solutions.

## Limitations
- Computational resource constraints limit scaling to more complex trajectories or higher-dimensional data
- Self-supervised approach relies on assumption that disjoint k-space masking preserves sufficient information for reconstruction
- Generalizability from 5 subjects to broader populations remains unverified

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Toeplitz decomposition's computational benefits | High |
| Self-supervised learning efficacy | Medium |
| BOLD sensitivity preservation | Medium |

## Next Checks

1. Conduct ablation study varying mask ratios (60-40, 70-30, 80-20) to optimize the trade-off between reconstruction quality and training stability
2. Perform systematic comparison of Toeplitz decomposition approximation error versus full NUFFT across different trajectory densities and undersampling factors
3. Validate generalizability by testing the trained model on multi-center data with varying acquisition parameters and subject demographics