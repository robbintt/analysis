---
ver: rpa2
title: Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained
  Sequence-to-Sequence Models
arxiv_id: '2310.06374'
source_url: https://arxiv.org/abs/2310.06374
tags: []
core_contribution: This paper systematically analyzes the impact of model selection
  and decoding strategies on keyphrase generation (KPG) with pre-trained sequence-to-sequence
  (seq2seq) models. It demonstrates that attention heads in seq2seq decoders inherently
  encode phrase centrality, making them well-suited for KPG.
---

# Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models

## Quick Facts
- arXiv ID: 2310.06374
- Source URL: https://arxiv.org/abs/2310.06374
- Reference count: 39
- Key outcome: DESEL algorithm improves greedy search by 4.7% semantic F1 across five datasets

## Executive Summary
This paper presents a comprehensive analysis of model selection and decoding strategies for keyphrase generation (KPG) using pre-trained sequence-to-sequence models. The authors demonstrate that decoder attention heads inherently encode phrase centrality, making them well-suited for KPG tasks. They propose DESEL, a likelihood-based decode-select algorithm that combines greedy search's correlation knowledge with sampling diversity, achieving significant improvements over standard decoding approaches. The study systematically evaluates different pre-training and fine-tuning strategies, finding that in-domain pre-training combined with task adaptation provides the best parameter efficiency.

## Method Summary
The study fine-tunes sequence-to-sequence models (BART, T5) on the KP20k dataset using cross-entropy loss with a "One2Seq" formulation where keyphrases are joined by semicolons. The authors evaluate multiple pre-training strategies including in-domain pre-training (SciBART using S2ORC), task-adaptive pre-tuning (KeyBART, FLAN-T5), and combinations thereof. They propose DESEL, a decode-select algorithm that combines greedy decoding with sampling-based diversity selection using likelihood estimates. The evaluation uses five datasets (KP20k, Inspec, Krapivin, NUS, SemEval) with both lexical metrics (P@M, R@M, F1@M) and semantic metrics (SemP, SemR, SemF1) with Porter stemming and three random seeds.

## Key Results
- Attention heads in decoder implicitly encode phrase centrality, correlating with graph-based centrality scores
- In-domain pre-training combined with task adaptation is more parameter-efficient than simply increasing model size
- DESEL improves greedy search by an average of 4.7% semantic F1 across five datasets
- Greedy search achieves strong F1 scores but lags in recall compared to sampling-based methods

## Why This Works (Mechanism)

### Mechanism 1
Attention heads in seq2seq decoder implicitly encode phrase centrality, making them naturally suited for KPG. The decoder's self-attention weights capture global importance of phrases, correlating with graph-based centrality scores (MPRank).

Core assumption: Phrase centrality is a good proxy for keyphrase importance.
Evidence anchors:
- We correlate al,h_i with ci using Spearman correlation ρ and Kendall's Tau τ and present the best correlation for each model in Table 1. Surprisingly, BART and T5 decoders contain attention heads that encode phrase centrality similarly as MPRank.
- Simply ranking the noun phrase candidates by al,h_i achieves similar F 1@5 for present keyphrases or SemF 1@5 score as MPRank (appendix B).
Break condition: If attention weights fail to correlate with centrality, or if centrality is not a good indicator of keyphrase importance.

### Mechanism 2
In-domain pre-training combined with task adaptation is the most parameter-efficient strategy for KPG. In-domain pre-training provides domain-specific knowledge, while task adaptation adds task-specific supervision, leading to better performance than scaling or single adaptation.

Core assumption: KPG requires both domain knowledge and task-specific understanding.
Evidence anchors:
- In-domain pre-training consistently bolsters performance across both keyphrase types and can benefit from task adaptation.
- In-domain pre-training is more important for KPG and TAPT serves a complementary secondary role.
Break condition: If domain knowledge is not crucial for KPG, or if task adaptation does not provide additional benefits.

### Mechanism 3
DESEL improves greedy search by combining its correlation knowledge with diversity from sampling. DESEL uses greedy search's probability estimates as a baseline and selects from sampled phrases based on likelihood, filtering out low-quality candidates.

Core assumption: Greedy search captures label correlations but misses diverse high-quality phrases.
Evidence anchors:
- Greedy search captures the correlations in human-written labels but makes local decisions and has path dependency: high-quality keyphrases can be missed with improbable first tokens.
- Compared to the single-sequence decoding baselines, DESEL wins by bringing in the diversity.
Break condition: If greedy search already captures all relevant keyphrases, or if sampling does not provide additional diversity.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how attention weights encode phrase centrality is crucial for grasping why seq2seq models are suitable for KPG.
  - Quick check question: How do attention weights in transformer decoders capture global importance of phrases?

- Concept: Graph-based centrality measures
  - Why needed here: Understanding phrase centrality is essential for interpreting the correlation between attention weights and keyphrase importance.
  - Quick check question: What is the relationship between phrase centrality and keyphrase importance?

- Concept: Pre-training and fine-tuning strategies
  - Why needed here: Understanding the trade-offs between model size, in-domain pre-training, and task adaptation is crucial for selecting the best approach for KPG.
  - Quick check question: How do different pre-training and fine-tuning strategies affect the performance of KPG models?

## Architecture Onboarding

- Component map: Input document -> Encoder -> Decoder -> Attention weights -> Phrase centrality -> DESEL selection
- Critical path: 1. Input document → Encoder 2. Encoder output → Decoder 3. Decoder attention → Phrase centrality 4. Greedy search + Sampling → Candidate keyphrases 5. DESEL → Final keyphrase selection
- Design tradeoffs:
  - Model size vs. parameter efficiency: Larger models do not necessarily lead to better performance for KPG
  - In-domain vs. general domain: In-domain pre-training is crucial for KPG, but may reduce robustness to input perturbations
  - Greedy search vs. sampling: Greedy search captures label correlations but may miss diverse keyphrases, while sampling provides diversity but introduces noise
- Failure signatures:
  - Poor performance on absent keyphrases: May indicate insufficient domain knowledge or task adaptation
  - Sensitivity to input perturbations: May indicate overfitting to training data or insufficient robustness
- First 3 experiments:
  1. Compare attention-based keyphrase extraction with MPRank on a small dataset
  2. Evaluate the impact of in-domain pre-training on KPG performance using a general domain model
  3. Test DESEL's effectiveness in improving greedy search on a small subset of the KP20k dataset

## Open Questions the Paper Calls Out

### Open Question 1
How do model selection and data augmentation strategies interact for robust keyphrase generation?
Basis in paper: The paper shows that stronger PLMs (like in-domain and task-adapted models) are more vulnerable to input perturbations, suggesting a trade-off between domain specificity and generalization.
Why unresolved: The paper identifies this trade-off but doesn't explore methods to mitigate it or determine optimal combinations of model selection and data augmentation.
What evidence would resolve it: Experiments comparing various combinations of model selection (e.g., domain-specific vs. general PLMs) with different data augmentation techniques (e.g., synonym replacement, paraphrasing) to find optimal robustness-performance trade-offs.

### Open Question 2
What are the underlying mechanisms that make in-domain pre-training more effective than task-adaptive pre-training for keyphrase generation?
Basis in paper: The paper finds that in-domain pre-training (SciBART) significantly improves performance over general-domain models, while task-adaptive pre-training (TAPT) shows smaller gains and struggles with absent keyphrases.
Why unresolved: The paper observes these patterns but doesn't investigate why in-domain knowledge is more crucial than task-specific supervision for KPG.
What evidence would resolve it: Analysis of attention patterns and feature representations in in-domain vs. task-adapted models to identify which aspects of scientific knowledge are most beneficial for KPG.

### Open Question 3
How can decoding strategies be optimized specifically for keyphrase generation rather than adapting general text generation methods?
Basis in paper: The paper introduces DESEL, a likelihood-based decode-select algorithm that improves greedy search, suggesting that standard decoding strategies may not be optimal for KPG's unique characteristics.
Why unresolved: While DESEL shows promise, the paper doesn't explore whether KPG-specific decoding strategies could be developed that better capture keyphrase generation's unique requirements.
What evidence would resolve it: Development and evaluation of decoding algorithms that explicitly model keyphrase generation patterns, such as generating keyphrases as a set rather than sequence, or incorporating semantic similarity constraints.

## Limitations
- The correlation between attention weights and phrase centrality may not directly translate to improved keyphrase quality
- DESEL's performance gains lack comparison with other strong decoding strategies like diverse beam search
- Evaluation focuses primarily on automatic metrics with limited human evaluation
- In-domain pre-training may reduce model robustness to input perturbations

## Confidence

- **High Confidence**: The empirical findings about model size vs. adaptation efficiency are well-supported by systematic experiments across multiple datasets and model variants
- **Medium Confidence**: The correlation between decoder attention weights and phrase centrality is statistically demonstrated, but the practical implications for keyphrase quality remain partially validated
- **Medium Confidence**: The DESEL algorithm shows consistent improvements, but the exact contribution of each component (likelihood estimation vs. sampling diversity) is not fully decomposed

## Next Checks

1. **Attention-Centricity Validation**: Conduct human evaluation studies to verify whether attention-weighted phrases align with human judgments of keyphrase importance, testing the assumption that phrase centrality equals keyphrase quality
2. **DESEL Component Analysis**: Implement ablation studies isolating the likelihood estimation component from the sampling diversity component to quantify their individual contributions to DESEL's performance gains
3. **Robustness Testing**: Evaluate models on adversarially perturbed input texts (synonym replacement, sentence reordering) to assess sensitivity and robustness, particularly for in-domain-trained models that showed reduced robustness in the original study