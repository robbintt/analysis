---
ver: rpa2
title: 'RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning'
arxiv_id: '2305.14502'
source_url: https://arxiv.org/abs/2305.14502
tags:
- examples
- example
- reticl
- problem
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RetICL addresses the problem of selecting in-context examples for
  few-shot prompting of large language models, specifically focusing on sequential
  example selection that considers dependencies between examples and their order.
  The method frames example selection as a Markov decision process and trains a learnable
  example retriever using reinforcement learning with proximal policy optimization.
---

# RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.14502
- Source URL: https://arxiv.org/abs/2305.14502
- Authors: [Not specified in source]
- Reference count: 24
- Key outcome: Achieves 8.20% improvement over previous state-of-the-art on TabMWP math word problem dataset

## Executive Summary
RetICL addresses the challenge of selecting in-context examples for few-shot prompting of large language models by introducing a sequential retrieval approach that considers dependencies between examples and their order. The method frames example selection as a Markov decision process and trains a learnable retriever using reinforcement learning with proximal policy optimization. A novel confidence reward function based on inverse perplexity of generated solutions is introduced to improve training stability. Evaluated on TabMWP and GSM8K math word problem datasets, RetICL outperforms both heuristic and learnable baselines, achieving state-of-the-art accuracy on TabMWP.

## Method Summary
RetICL treats in-context learning example selection as a sequential decision-making problem using reinforcement learning. The approach uses an LSTM to maintain latent states representing the sequence of selected examples, combined with a bilinear transformation to efficiently rank example candidates from a corpus. The model is trained using proximal policy optimization with a novel confidence reward function based on inverse perplexity of generated solutions. The system operates on math word problem datasets, using OpenAI Codex as the underlying LLM, and is evaluated on test set accuracy for both TabMWP and GSM8K datasets.

## Key Results
- Achieves 8.20% improvement over previous state-of-the-art on TabMWP dataset
- Outperforms both heuristic and learnable baselines on math word problem datasets
- Demonstrates ability to implicitly learn problem-solving strategies through case studies
- Shows effectiveness of confidence reward function based on inverse perplexity

## Why This Works (Mechanism)

### Mechanism 1
- Sequential example selection improves performance by capturing dependencies between examples and their order
- Frames ICL example selection as a Markov decision process allowing conditioning on previously selected examples
- Core assumption: The order and combination of examples matters for eliciting desired responses from LLMs
- Evidence: Recent work shows choice of examples impacts task performance and finding optimal sets is non-trivial

### Mechanism 2
- Confidence reward based on inverse perplexity improves training stability and example selection quality
- Encourages selection of examples that lead to LLM generations with high probability (low perplexity)
- Core assumption: High-probability correct solutions indicate sound reasoning rather than lucky guessing
- Evidence: Ablation studies show removing confidence reward significantly reduces accuracy and example diversity

### Mechanism 3
- Bilinear transformation enables efficient ranking and generalization to unseen examples
- Learns mapping between model's latent space and example embedding space for maximum inner-product search
- Core assumption: Bilinear transformation can effectively capture relationships between states and examples
- Evidence: Enables efficient computation of policy over large corpus at inference time

## Foundational Learning

- Concept: Markov Decision Processes
  - Why needed here: Sequential example selection naturally frames as an MDP with states, actions, and rewards
  - Quick check question: What are the state, action, and reward components in the RetICL MDP formulation?

- Concept: Reinforcement Learning with Proximal Policy Optimization
  - Why needed here: PPO trains the example retriever by optimizing policy that selects examples to maximize expected reward
  - Quick check question: How does PPO differ from REINFORCE in terms of variance reduction and training stability?

- Concept: Bilinear Transformations and Maximum Inner Product Search
  - Why needed here: Bilinear transformation enables efficient ranking of examples using MIPS for scalability
  - Quick check question: Why is maximum inner product search equivalent to finding example with highest bilinear activation?

## Architecture Onboarding

- Component map: Problem statement → S-BERT encoder → LSTM hidden state → Bilinear transformation → Softmax → Example selection → LLM → Reward calculation → PPO update
- Critical path: Problem → S-BERT → LSTM hidden state → Bilinear → Softmax → Example selection → LLM → Reward calculation → PPO update
- Design tradeoffs:
  - LSTM vs Transformer for maintaining state: LSTM is simpler and sufficient for short sequences
  - Bilinear vs MLP for example ranking: Bilinear enables efficient MIPS but may be less expressive
  - Confidence reward vs only correctness: Confidence reward provides richer signal but requires likelihood computation
- Failure signatures:
  - Low diversity in selected examples indicates poor exploration or collapsed policy
  - Performance similar to random selection indicates poor learning signal
  - Large gap between training and validation performance indicates overfitting
  - Slow convergence indicates need for more exploration
- First 3 experiments:
  1. Remove LSTM to test if sequential conditioning is necessary
  2. Remove confidence reward to test if it improves training stability
  3. Vary corpus size at training time to test generalization to different inference corpus sizes

## Open Questions the Paper Calls Out

- How does performance vary when using dynamically determined number of examples T instead of fixed constant?
- Can RetICL be extended to handle other natural language generation tasks beyond math word problems?
- What is the relationship between confidence reward based on inverse perplexity and actual quality of solutions generated by LLM?

## Limitations

- Only validated on math word problem datasets, leaving applicability to other domains unexplored
- Uses fixed number of examples for all problems, not optimizing number per problem type
- Confidence reward assumption (low perplexity indicates sound reasoning) not fully validated
- Limited exploration of alternative architectures like Transformers for state maintenance

## Confidence

- Sequential decision-making framework: High
- Confidence reward mechanism: Medium
- Bilinear transformation architecture: Medium

## Next Checks

1. Test robustness of confidence reward assumption by comparing selected examples against human judgments of quality and relevance
2. Evaluate model's ability to generalize to different LLM architectures beyond Codex
3. Conduct ablation studies varying number of examples selected to understand sequential selection benefits with different context window sizes