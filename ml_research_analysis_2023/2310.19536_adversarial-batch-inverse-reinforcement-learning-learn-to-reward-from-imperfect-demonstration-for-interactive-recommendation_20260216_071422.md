---
ver: rpa2
title: 'Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect
  Demonstration for Interactive Recommendation'
arxiv_id: '2310.19536'
source_url: https://arxiv.org/abs/2310.19536
tags:
- learning
- reward
- recommendation
- which
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adversarial Batch Inverse Reinforcement Learning
  (Adversarial B-IRL) for interactive recommendation systems that learns reward functions
  directly from imperfect demonstration data. The key innovation is combining learning-to-reward
  with policy optimization through discounted stationary distribution correction,
  avoiding separate reward-learning pipelines.
---

# Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect Demonstration for Interactive Recommendation

## Quick Facts
- arXiv ID: 2310.19536
- Source URL: https://arxiv.org/abs/2310.19536
- Reference count: 24
- Primary result: Achieves 2.3% relative improvement in Hit Ratio and NDCG while reducing demonstration consumption by 11.53% on real-world datasets

## Executive Summary
This paper proposes Adversarial Batch Inverse Reinforcement Learning (Adversarial B-IRL) for interactive recommendation systems that learns reward functions directly from imperfect demonstration data. The key innovation is combining learning-to-reward with policy optimization through discounted stationary distribution correction, avoiding separate reward-learning pipelines. The method employs Bellman transformation for off-policy evaluation and KL regularization for conservatism when handling compositional demonstrations of unknown quality. Experiments on two real-world datasets (Kaggle and RecRec15) show the proposed method achieves 2.3% relative improvement in effectiveness metrics (Hit Ratio and NDCG) while reducing demonstration consumption by 11.53% compared to state-of-the-art baselines.

## Method Summary
Adversarial B-IRL integrates learning-to-reward (LTR) and policy optimization into a unified framework by parameterizing the stationary distribution with the policy and using discounted factors to handle distribution shift. The method employs Bellman transformation to enable off-policy evaluation of rewards, allowing learning from historical demonstration data without requiring new online interactions. A KL regularization term provides conservatism when dealing with compositional demonstrations of unknown quality, balancing exploration and exploitation. The approach uses an actor-critic architecture with shared encoders and Gumbel-Softmax for discrete action selection, trained adversarially to maximize cumulative rewards while minimizing divergence from demonstration data.

## Key Results
- Achieves 2.3% relative improvement in Hit Ratio and NDCG metrics compared to state-of-the-art baselines
- Reduces demonstration consumption by 11.53% while maintaining or improving performance
- Demonstrates effectiveness on two real-world datasets: Kaggle (195,523 interactions, 70,852 items) and RecSys15 (200,000 interactions, 26,702 items)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discounted stationary distribution correction unifies learning-to-reward and policy optimization into a single objective
- Mechanism: By parameterizing the stationary distribution with the policy, the reward can be computed directly from demonstration data rather than requiring a separate reward-learning pipeline. The discounted factor handles distribution shift between demonstrations and current policy
- Core assumption: The demonstration data provides sufficient coverage of the state-action space to estimate the stationary distribution
- Evidence anchors:
  - [abstract]: "utilizing discounted stationary distribution correction to combine LTR and recommender agent evaluation"
  - [section]: "discounted stationary distribution [13], and it induces discounted factor γ to tackle distribution shift [18]"
  - [corpus]: Weak evidence - related works focus on AIRL and IRL but don't directly address this specific unification approach
- Break condition: If demonstration data has poor coverage of the state-action space, the stationary distribution estimation becomes unreliable and the unified objective fails

### Mechanism 2
- Claim: Bellman transformation enables off-policy evaluation of the reward function
- Mechanism: The immediate reward is expressed as the difference between current state-action value and the next state-action value under the policy's Bellman operator. This converts the on-policy evaluation requirement into an off-policy computation using only demonstration data
- Core assumption: The Bellman operator accurately captures the transition dynamics between consecutive states
- Evidence anchors:
  - [section]: "We utilize Bellman operator as... Reward (5) is then computed as a temporal difference between consecutive tuples"
  - [abstract]: "Our method utilizes discounted stationary distribution correction to combine LTR and recommender agent evaluation"
  - [corpus]: Weak evidence - Bellman transformation is standard in RL but the specific application to reward learning from imperfect demonstrations is novel
- Break condition: If the state transitions in demonstrations are highly non-Markovian or the discount factor is poorly chosen, the Bellman approximation breaks down

### Mechanism 3
- Claim: KL regularization provides pessimism for compositional demonstrations
- Mechanism: The KL divergence between consecutive policies constrains updates to be conservative, preventing over-exploitation of uncertain regions in the compositional demonstration data. This balances exploration and exploitation when demonstration quality is unknown
- Core assumption: KL divergence between consecutive policies is a meaningful measure of policy change and conservatism
- Evidence anchors:
  - [section]: "we leverage KL conservation as a form of pessimism [17] to balance exploitation and exploration"
  - [abstract]: "incorporate the concept of pessimism through conservation"
  - [corpus]: Moderate evidence - pessimism in offline RL is established, but specific application to compositional demonstrations is novel
- Break condition: If the KL penalty is too strong, the policy becomes overly conservative and fails to improve; if too weak, it overfits to noisy demonstration data

## Foundational Learning

- Markov Decision Process
  - Why needed here: The entire framework is built on MDP formalism to model user-agent interactions in recommendation systems
  - Quick check question: What are the four components of an MDP and how do they map to the recommendation setting?

- Inverse Reinforcement Learning
  - Why needed here: The core innovation learns reward functions from demonstrations rather than requiring hand-designed rewards
  - Quick check question: What is the fundamental challenge of IRL that makes it "ill-posed" and how does this work address it?

- Off-policy evaluation
  - Why needed here: Enables learning from historical demonstration data without requiring new online interactions
  - Quick check question: What is the key difference between on-policy and off-policy evaluation in RL?

## Architecture Onboarding

- Component map: Encoder → Actor (recommendation) → Critic (reward estimation) → Bellman transformation (off-policy evaluation) → KL regularization (policy update)

- Critical path: Encoder → Actor (recommendation) → Critic (reward estimation) → Bellman transformation (off-policy evaluation) → KL regularization (policy update)

- Design tradeoffs:
  - Shared encoder vs separate encoders: Shared reduces parameters but may cause interference
  - Temperature in Gumbel-Softmax: Lower temperature gives more discrete samples but higher variance
  - KL penalty strength: Higher values increase conservatism but slow learning

- Failure signatures:
  - Poor performance on states not well-represented in demonstrations
  - High variance in reward estimates indicating insufficient data coverage
  - Slow convergence suggesting overly conservative KL regularization

- First 3 experiments:
  1. Train with perfect demonstrations to establish baseline performance
  2. Gradually introduce noise/uncertainty in demonstrations to test KL regularization
  3. Compare off-policy vs on-policy versions of the objective to verify efficiency gains

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the following questions arise from the methodology and results:

### Open Question 1
- Question: How does the performance of Adversarial B-IRL compare when applied to recommendation systems with different feedback mechanisms beyond clicks and purchases, such as time spent or ratings?
- Basis in paper: [inferred] The paper mentions "multi-types feedback signals with different valuations" and focuses on clicks and purchases, suggesting potential for other feedback types.
- Why unresolved: The paper only evaluates the method on click and purchase feedback, leaving the performance on other feedback mechanisms unexplored.
- What evidence would resolve it: Experiments comparing the method's performance across different feedback types would provide clarity.

### Open Question 2
- Question: What is the impact of varying the discount factor γ on the effectiveness and efficiency of Adversarial B-IRL in different recommendation scenarios?
- Basis in paper: [explicit] The paper discusses the discount factor γ in the context of user satisfaction and future reward accumulation, but does not explore its impact in depth.
- Why unresolved: The paper uses a fixed discount factor without investigating how changes affect performance across various recommendation tasks.
- What evidence would resolve it: Conducting experiments with different γ values across diverse recommendation datasets would reveal its impact.

### Open Question 3
- Question: How does the method handle scenarios where the demonstration data is significantly imbalanced across different user segments or item categories?
- Basis in paper: [inferred] The paper mentions "compositional demonstrations" and "unknown quality," suggesting potential challenges with imbalanced data.
- Why unresolved: The paper does not address how the method performs when demonstration data is unevenly distributed across user segments or items.
- What evidence would resolve it: Testing the method on datasets with known imbalances and analyzing its performance would provide insights.

### Open Question 4
- Question: Can the Adversarial B-IRL framework be extended to handle multi-agent recommendation systems where multiple agents interact with users simultaneously?
- Basis in paper: [inferred] The paper focuses on single-agent systems and does not explore multi-agent scenarios.
- Why unresolved: The method is designed for single-agent environments, and its applicability to multi-agent settings is unexplored.
- What evidence would resolve it: Implementing and testing the method in a multi-agent recommendation environment would determine its feasibility.

## Limitations

- The method assumes demonstration data provides sufficient coverage of the state-action space, which may not hold for sparse or highly personalized recommendation scenarios
- The KL regularization strength is fixed rather than adaptive, potentially leading to either overly conservative or insufficiently cautious policy updates depending on demonstration quality
- The evaluation focuses on traditional recommendation metrics (Hit Ratio, NDCG) without examining whether the learned reward functions capture meaningful user preferences beyond click patterns

## Confidence

- **Medium-High**: The unified objective combining learning-to-reward and policy optimization through discounted stationary distribution correction
- **Medium**: The off-policy evaluation claims using Bellman transformation, as extensive validation of correctness in significant distribution shift scenarios is lacking
- **Medium**: The effectiveness of KL regularization for handling compositional demonstrations, as the specific application to this context is novel

## Next Checks

1. **Coverage Analysis**: Measure the overlap between demonstration state-action pairs and states visited during policy optimization to quantify the risk of extrapolation beyond training distribution.

2. **Ablation Study on KL Regularization**: Systematically vary the KL penalty strength (δ) and demonstrate its effect on both performance and sample efficiency across different levels of demonstration noise.

3. **Reward Interpretability**: Analyze the learned reward function weights to verify they capture meaningful aspects of user preferences (e.g., diversity, novelty) rather than simply optimizing for click-through rates.