---
ver: rpa2
title: 'Forbidden Facts: An Investigation of Competing Objectives in Llama-2'
arxiv_id: '2312.08793'
source_url: https://arxiv.org/abs/2312.08793
tags:
- forbidden
- word
- query
- components
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how Llama-2 resolves competing objectives\
  \ (e.g., truthfulness vs. not saying forbidden words) using a \u201Cforbidden fact\u201D\
  \ task."
---

# Forbidden Facts: An Investigation of Competing Objectives in Llama-2

## Quick Facts
- arXiv ID: 2312.08793
- Source URL: https://arxiv.org/abs/2312.08793
- Reference count: 40
- This paper investigates how Llama-2 resolves competing objectives (e.g., truthfulness vs. not saying forbidden words) using a "forbidden fact" task.

## Executive Summary
This paper investigates how Llama-2 resolves competing objectives through residual stream components that implement suppression behavior. The authors decompose Llama-2 into ~1000 residual stream components and measure each component's importance for suppressing the correct answer using first-order patching. They find that ~35 components are sufficient to implement the full suppression behavior, though these components operate using heterogeneous and often faulty heuristics. One such heuristic enables a manually-designed adversarial attack called "The California Attack," which shows that the suppression mechanism can be exploited. The study suggests that interpreting complex AI systems may be more difficult than anticipated, as even simple behaviors are implemented via messy, heuristic-driven mechanisms rather than clean algorithms.

## Method Summary
The authors use first-order patching to decompose Llama-2 into ~1000 residual stream components (embedding, attention heads, and MLPs) and rank each by importance for suppressing correct answers when forbidden words are present. They analyze the top 35 components' attention patterns and OV circuits, finding that suppressor heads attend significantly to forbidden tokens and implement suppressive behavior through downweighting correct answers. The methodology includes adapting the CounterFact dataset to create the Forbidden Facts Dataset with 2634 prompts, computing log Bayes factors to quantify suppression effects, and manually designing adversarial attacks exploiting observed heuristics.

## Key Results
- Llama-2 resolves competing objectives through ~35 residual stream components that implement suppression behavior via attention head mechanisms
- Suppressor heads exhibit heterogeneous attention patterns, with the top 10 heads showing a mean attention of 19.64% to the forbidden token
- A manually-designed adversarial attack called "The California Attack" successfully exploits the suppression mechanism, demonstrating its heuristic nature
- The suppression mechanism relies on key semantic specificity rather than positional or semantic query specificity to identify which tokens to suppress

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Llama-2 resolves competing objectives through residual stream components that implement suppression behavior via attention head mechanisms.
- Mechanism: The model decomposes into ~1000 residual stream components, with approximately 35 components sufficient to implement full suppression behavior. These components operate using attention heads that attend to the forbidden token and apply suppressive OV circuits.
- Core assumption: The suppression behavior can be effectively decomposed into individual component contributions that can be measured through first-order patching.
- Evidence anchors:
  - [abstract] "We decompose Llama-2 into 1000+ components, and rank each one with respect to how useful it is for forbidding the correct answer."
  - [section 2] "Using first-order-patching based attribution, we rank all 1057 components."
  - [corpus] Weak - The corpus mentions related work on factual recall but doesn't directly support the decomposition mechanism described here.
- Break condition: If the residual stream components interact in non-linear ways that cannot be captured by first-order patching, the decomposition approach would fail to accurately represent the suppression mechanism.

### Mechanism 2
- Claim: The suppression mechanism relies on attention heads that pay significant attention to the forbidden token, with the top 10 heads showing a mean attention of 19.64% to the forbidden token.
- Mechanism: Important attention heads preferentially attend to the forbidden token on competing prompts, and their OV circuits implement suppressive behavior by downweighting the correct answer token.
- Core assumption: Attention patterns to the forbidden token are meaningful indicators of a head's role in the suppression circuit.
- Evidence anchors:
  - [section 3] "We find that the most important heads found using first-order patching attend significantly to the forbidden token, with the top 10 heads having a mean attention of 19.64% to the forbidden token."
  - [section 3] "The most important attention heads have suppressive OV circuits."
  - [corpus] Weak - The corpus mentions related work on attention mechanisms but doesn't directly validate the specific attention-to-forbidden-token hypothesis.
- Break condition: If attention to the forbidden token is a consequence rather than a cause of the suppression behavior, then this mechanism would be misidentified as causal.

### Mechanism 3
- Claim: The model uses key semantic specificity rather than positional or semantic query specificity to identify which tokens to suppress.
- Mechanism: Suppressor heads privilege correct answers to factual recall over all other keys, but their query enrichment is not semantically specific to the forbidden word.
- Core assumption: The model's mechanism for identifying tokens to suppress relies on key enrichment patterns rather than direct communication of the forbidden word.
- Evidence anchors:
  - [section 3] "Interestingly, we find that key attention is not positionally specific, and query attention is not semantically specific."
  - [section 3] "We find, surprisingly, that the only type of specificity the suppressor heads exhibit is key semantic specificity: the heads privilege correct answers to the factual recall over all other keys."
  - [corpus] Weak - The corpus mentions related work on factual recall mechanisms but doesn't specifically address key semantic specificity in suppression.
- Break condition: If the model uses a different mechanism entirely to communicate which tokens to suppress, such as direct embedding of the forbidden word instruction, this mechanism would be incorrect.

## Foundational Learning

- Concept: First-order patching and log Bayes factors
  - Why needed here: The paper uses first-order patching to measure component importance and log Bayes factors to quantify the effect size. Understanding these concepts is essential for interpreting the methodology.
  - Quick check question: What is the key difference between first-order patching and higher-order patching approaches in mechanistic interpretability?

- Concept: Transformer attention mechanisms and OV circuits
  - Why needed here: The paper extensively analyzes attention heads and their OV circuits to understand how they implement suppression behavior. Knowledge of how attention works in transformers is fundamental.
  - Quick check question: How does the OV circuit in an attention head transform the residual stream, and why is this important for understanding suppression behavior?

- Concept: Residual stream decomposition
  - Why needed here: The paper decomposes the model into 1000+ residual stream components. Understanding what these components represent (embedding, attention heads, MLPs) is crucial for following the analysis.
  - Quick check question: What are the three main types of residual stream components in Llama-2, and how many of each type are there?

## Architecture Onboarding

- Component map: Embedding layer -> 32 transformer layers (each with 32 attention heads and 1 MLP) -> Unembedding layer
- Critical path: For the forbidden fact task, the critical path involves: 1) Processing the system prompt and fact prefix through the layers, 2) Attention heads identifying the forbidden token and correct answer, 3) Suppressive heads downweighting the correct answer, 4) Final token prediction.
- Design tradeoffs: The model trades off between helpfulness (answering correctly) and harmlessness (avoiding forbidden words). The suppression mechanism implements this tradeoff through attention-based downweighting rather than explicit rule-based filtering.
- Failure signatures: If the forbidden word is semantically related to the correct answer (e.g., "California" for Golden Gate Bridge), the suppression mechanism may incorrectly suppress the correct answer. The model may also exhibit inconsistent behavior across different forbidden words.
- First 3 experiments:
  1. Replicate the first-order patching analysis on a smaller subset of components to verify the decomposition methodology.
  2. Test the California Attack on different factual recall prompts to understand the boundaries of the attack's effectiveness.
  3. Analyze the attention patterns of the top 10 suppressor heads across multiple competing prompts to identify common features in their behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms allow suppressor heads to communicate which tokens to suppress, given that query enrichment is not semantically specific to the forbidden word?
- Basis in paper: [explicit] The paper observes that suppressor heads exhibit heterogeneous attention patterns and that key semantic specificity (attending more to correct answers) is the primary mechanism, but questions remain about the communication mechanism.
- Why unresolved: The paper demonstrates that key attention is semantically specific but doesn't explain the underlying mechanism of how this semantic information is communicated to the suppressor heads.
- What evidence would resolve it: Experiments showing the information flow from the forbidden word specification through the model's layers to the suppressor heads, or evidence of alternative communication mechanisms.

### Open Question 2
- Question: Do larger language models (70b vs 7b/13b) have cleaner, more algorithmic suppression circuits, or does their increased complexity simply make analysis more difficult?
- Basis in paper: [explicit] The paper notes that the California Attack didn't work on the 70b model but suggests this might be due to either better heuristics or simply insufficient analysis effort.
- Why unresolved: The paper couldn't definitively determine whether the absence of successful attacks on 70b reflects genuine architectural improvements or just the difficulty of analyzing larger models.
- What evidence would resolve it: Systematic comparison of suppression mechanisms across model sizes using consistent analysis methods, or targeted adversarial attack experiments on larger models.

### Open Question 3
- Question: Is there a computational basis (representation or transformation) in which LLM behaviors become more interpretable and less reliant on complex heuristics?
- Basis in paper: [explicit] The paper suggests that the complexity of mechanisms might be due to "working in the wrong basis" and poses this as an important open question.
- Why unresolved: The paper identifies this as a fundamental challenge but doesn't propose specific alternative bases or demonstrate their existence.
- What evidence would resolve it: Discovery of alternative representations (e.g., through sparse autoencoders or other decomposition methods) that simplify the interpretation of complex behaviors.

## Limitations

- The paper's reliance on first-order patching assumes linear additivity of component effects, which may not capture non-linear interactions in the suppression mechanism.
- The adversarial attack demonstration was achieved through manual trial-and-error rather than systematic exploration, raising questions about generalizability.
- The analysis focuses on Llama-2 specifically and may not generalize to other model architectures or training approaches.

## Confidence

- **High confidence**: The basic empirical findings regarding component decomposition and the existence of heterogeneous suppression heuristics.
- **Medium confidence**: The interpretation that suppression behavior is implemented through "messy, heuristic-driven mechanisms" rather than clean algorithms.
- **Medium confidence**: The specific mechanism of the California Attack and its implications for model interpretability.

## Next Checks

1. **Systematic adversarial attack generation**: Rather than manual trial-and-error, implement an automated search over prompt variations to systematically identify the full space of words and contexts that can trigger the suppression vulnerability.

2. **Higher-order patching validation**: Apply second-order or higher-order patching techniques to the same component set to determine whether non-linear interactions significantly affect the suppression mechanism.

3. **Cross-model consistency testing**: Replicate the component decomposition and adversarial attack analysis on different model families (e.g., GPT models, Claude) and model sizes to determine whether the identified heuristics and vulnerabilities are specific to Llama-2 or represent more general phenomena in aligned language models.