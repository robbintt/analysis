---
ver: rpa2
title: 'HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings'
arxiv_id: '2310.09720'
source_url: https://arxiv.org/abs/2310.09720
tags:
- hicl
- learning
- contrastive
- training
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiCL improves sentence embeddings by using a hierarchical contrastive
  learning framework that models both local segment-level and global sequence-level
  relationships. The method divides sequences into shorter segments, encodes them
  separately for efficiency, and applies local and global contrastive objectives to
  learn richer representations.
---

# HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings

## Quick Facts
- arXiv ID: 2310.09720
- Source URL: https://arxiv.org/abs/2310.09720
- Authors: 
- Reference count: 25
- Key outcome: HiCL improves sentence embeddings by using a hierarchical contrastive learning framework that models both local segment-level and global sequence-level relationships, achieving new state-of-the-art performance on STS tasks.

## Executive Summary
HiCL introduces a hierarchical contrastive learning framework for unsupervised sentence embedding that addresses the limitations of traditional sequence-level contrastive learning. By dividing sequences into shorter segments and applying both local and global contrastive objectives, HiCL learns richer representations while maintaining computational efficiency. The method shows consistent improvements across multiple backbone models and evaluation tasks, particularly for longer texts and larger training corpora.

## Method Summary
HiCL divides input sequences into fixed-length segments, encodes each segment separately using a transformer encoder, and then aggregates these segment representations using weighted average pooling. The model employs a hybrid loss combining local contrastive learning at the segment level with global contrastive learning at the sequence level. This hierarchical approach reduces the quadratic computational complexity of transformers while capturing both local and global semantic relationships in the text.

## Key Results
- Improves state-of-the-art SNCSE model by +0.44% on RoBERTa-large and +0.2% on BERT-large for STS tasks
- Achieves new state-of-the-art performance on seven STS tasks (STS12-16, STS-B, SICK-R)
- Demonstrates consistent improvements across multiple backbone models and evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical approach improves efficiency by encoding short segments separately before aggregation, reducing the quadratic time complexity of transformers from O(|seqi|²) to O(L² × (li - 1) + |segli|²). This allows for more efficient training while maintaining representation quality through segment aggregation.

### Mechanism 2
Local contrastive learning helps the model better match sentence length distribution and focus on shorter sentences by learning segment-level relationships. This addresses the limitation where traditional sequence-level contrastive learning struggles with shorter texts.

### Mechanism 3
The hybrid loss combining local and global contrastive objectives prevents collapse while preserving both local and global information. The weighted combination balances these objectives, with lower weighting on local contrastive loss to account for potential information loss from truncation.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: HiCL builds directly on contrastive learning frameworks, using both local and global contrastive objectives to learn sentence embeddings
  - Quick check question: What is the difference between the local and global contrastive losses in HiCL, and how do they contribute to the final representation?

- Concept: Transformer attention complexity
  - Why needed here: Understanding why segment-level encoding is more efficient requires grasping the quadratic complexity of self-attention in transformers
  - Quick check question: Why does encoding shorter segments separately lead to computational efficiency gains compared to encoding full sequences?

- Concept: Sentence embedding evaluation metrics
  - Why needed here: HiCL is evaluated on semantic textual similarity tasks using Spearman correlation, requiring understanding of how sentence embeddings are assessed
  - Quick check question: What does a Spearman correlation of 0.8 mean in the context of STS tasks, and why is this metric appropriate for evaluating sentence embeddings?

## Architecture Onboarding

- Component map:
  - Input sequence → Slicer (fixed-length segmentation) → Segment encoder (transformer) → Segment representations → Aggregator (weighted pooling) → Sequence representation
  - Two parallel paths: Local contrastive (segment-level) and Global contrastive (sequence-level)
  - Combined loss function with tunable weights

- Critical path:
  1. Sequence slicing and segment encoding
  2. Local contrastive loss computation using segment pairs
  3. Sequence representation aggregation from segments
  4. Global contrastive loss computation using sequence pairs
  5. Combined loss backpropagation

- Design tradeoffs:
  - Fixed-length slicing vs. semantic-aware segmentation: Fixed-length is computationally simpler but may cut words inappropriately
  - Weighted vs. unweighted pooling: Weighted pooling performs better but requires length information
  - Batch size and truncation length: Larger batches improve contrastive learning but increase memory requirements

- Failure signatures:
  - Poor performance on shorter texts: Local contrastive loss may be too weak or segments too short
  - Inconsistent results across runs: Hyperparameter α may be poorly tuned or initialization sensitivity high
  - Memory issues: Batch size too large for available GPU memory given segment encoding requirements

- First 3 experiments:
  1. Verify segment encoding efficiency: Compare encoding time for full sequences vs. segmented approach with same truncation length
  2. Test pooling methods: Implement both weighted and unweighted pooling and measure impact on development set performance
  3. Validate segment relationships: Experiment with treating in-sequence segments as positive, negative, or neither, and measure impact on final embeddings

## Open Questions the Paper Calls Out

The paper suggests HiCL is a promising approach for further research in other natural language processing tasks beyond semantic textual similarity, though specific open questions are not explicitly enumerated in the provided content.

## Limitations

- The segment-level slicing strategy may introduce artifacts when cutting through semantically meaningful boundaries
- The choice of segment length L=32 appears somewhat arbitrary without thorough exploration of sensitivity
- Limited benchmarking against other recent contrastive learning approaches like UNSEE or SimCSE variants

## Confidence

**High Confidence**: The efficiency claims regarding quadratic complexity reduction are mathematically sound and well-supported by the analysis.

**Medium Confidence**: The empirical improvements are statistically significant and reproducible based on the described methodology, though absolute gains on some tasks are modest.

**Low Confidence**: The claim about effectiveness for longer texts and larger training corpora lacks sufficient empirical validation through systematic ablation studies.

## Next Checks

1. Ablation on Segment Length Sensitivity: Systematically vary the segment length L across different text length distributions and measure the impact on both efficiency and embedding quality.

2. Semantic Boundary Preservation Analysis: Measure how often HiCL's slicing strategy cuts through semantically coherent units and compare against semantic-aware segmentation approaches.

3. Cross-Domain Generalization Test: Evaluate HiCL's performance on specialized domains (medical, legal, technical) to validate effectiveness across diverse training corpora.