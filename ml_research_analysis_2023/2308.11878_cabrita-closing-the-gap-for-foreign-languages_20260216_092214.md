---
ver: rpa2
title: 'Cabrita: closing the gap for foreign languages'
arxiv_id: '2308.11878'
source_url: https://arxiv.org/abs/2308.11878
tags:
- language
- portuguese
- tokens
- tokenizer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Cabrita, a methodology for adapting Large\
  \ Language Models (LLMs) to foreign languages, specifically focusing on Portuguese.\
  \ The core idea involves adapting the tokenizer to the target language while preserving\
  \ the original model\u2019s knowledge, thereby improving performance and reducing\
  \ inference time."
---

# Cabrita: closing the gap for foreign languages

## Quick Facts
- arXiv ID: 2308.11878
- Source URL: https://arxiv.org/abs/2308.11878
- Reference count: 31
- Primary result: Adapting tokenizer to Portuguese while preserving English knowledge improves performance and reduces inference time.

## Executive Summary
Cabrita introduces a methodology for adapting Large Language Models to foreign languages, focusing on Portuguese. The approach involves modifying the tokenizer to better handle Portuguese text while preserving the original model's knowledge and embeddings. By performing continuous pre-training on Portuguese text, the openCabrita 3B model achieves competitive performance with larger English pre-trained models in few-shot learning tasks, demonstrating the effectiveness of this bilingual adaptation strategy.

## Method Summary
The Cabrita methodology adapts a pre-trained OpenLLaMA 3B model to Portuguese by first training a Portuguese-specific SentencePiece tokenizer, then merging it with the original English tokenizer to create a bilingual tokenizer. The embedding matrix is resized to accommodate the new Portuguese tokens while preserving the original English token embeddings. Continuous pre-training is then performed on the Portuguese subset of the mC4 corpus using AdamW optimization with a learning rate schedule and gradient clipping. This approach aims to improve Portuguese language performance while maintaining English capabilities.

## Key Results
- openCabrita 3B achieves competitive performance with larger English pre-trained models in few-shot learning tasks
- Cabrita tokenizer significantly reduces token count for Portuguese text, improving efficiency
- Methodology effective for both Portuguese and English tasks, demonstrating bilingual capabilities
- Achieves similar results to larger models with fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting tokenizer to target language while preserving original model's embeddings reduces token count and improves inference efficiency
- Mechanism: Merges original English tokenizer with Portuguese-specific tokenizer, preserving English embedding mapping while appending Portuguese tokens
- Core assumption: Original model's English embeddings remain valid for bilingual tasks
- Evidence anchors:
  - [abstract] "The core idea involves adapting the tokenizer to the target language while preserving the original model's knowledge, thereby improving performance and reducing inference time"
  - [section] "The result was a bilingual tokenizer, able to handle both English and Portuguese content efficiently"
  - [corpus] Weak evidence - no direct experimental comparison between different tokenizer strategies

### Mechanism 2
- Claim: Continuous pre-training on language-specific corpus improves performance without full-scale pre-training
- Mechanism: Initializes with pre-trained OpenLLaMA and fine-tunes on Portuguese corpus
- Core assumption: Knowledge from original pre-training transfers to target language
- Evidence anchors:
  - [abstract] "This model demonstrates competitive performance with larger English pre-trained models in few-shot learning tasks"
  - [section] "The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text"
  - [corpus] Weak evidence - no detailed breakdown of performance gains from continuous pre-training alone

### Mechanism 3
- Claim: Bilingual tokenizer enables effective handling of both English and Portuguese tasks
- Mechanism: Tokenizer efficiently represents both languages, reducing Portuguese tokens while maintaining English performance
- Core assumption: Well-designed bilingual tokenizer can bridge tokenization needs of two languages
- Evidence anchors:
  - [abstract] "The methodology is shown to be effective for both Portuguese and English tasks"
  - [section] "Furthermore, upon examining the token count in English, the Cabrita tokenizer even exhibited a slight improvement in the token count when compared with the original OpenLLaMA tokenizer"
  - [corpus] Weak evidence - no detailed comparison of English performance with/without bilingual tokenizer

## Foundational Learning

- Concept: Subword tokenization (e.g., SentencePiece)
  - Why needed here: Understanding how subword tokenizers work is crucial for grasping tokenizer adaptation process and embedding preservation
  - Quick check question: How does SentencePiece handle out-of-vocabulary words, and why is this important for multilingual tokenization?

- Concept: Continuous pre-training vs. full-scale pre-training
  - Why needed here: Understanding the difference helps appreciate cost-effectiveness and performance trade-offs of Cabrita methodology
  - Quick check question: What are the key differences in computational cost and performance outcomes between continuous pre-training and full-scale pre-training?

- Concept: Token efficiency and its impact on inference
  - Why needed here: Understanding how token count affects inference time is crucial for appreciating Cabrita tokenizer's benefits
  - Quick check question: How does the number of tokens required to represent a text sample impact the inference time and memory usage of a language model?

## Architecture Onboarding

- Component map: SentencePiece tokenizer training -> Tokenizer merging -> Embedding matrix resizing -> Continuous pre-training pipeline -> Evaluation
- Critical path:
  1. Train Portuguese-specific SentencePiece tokenizer
  2. Merge Portuguese tokenizer with original English tokenizer
  3. Resize model's embedding layer for new tokens
  4. Perform continuous pre-training on Portuguese corpus
- Design tradeoffs:
  - Tokenizer size vs. efficiency: Larger vocabulary improves tokenization but increases model size
  - Pre-training data size vs. performance: More data generally improves performance but increases computational cost
  - Bilingual vs. monolingual models: Bilingual models handle multiple languages but may not perform as well as monolingual models
- Failure signatures:
  - Degraded performance on English tasks indicates tokenizer adaptation may have negatively impacted English capabilities
  - Increased token count for Portuguese text suggests tokenizer adaptation was ineffective
  - Overfitting on pre-training data indicates poor generalization to unseen Portuguese text
- First 3 experiments:
  1. Compare token count and model performance on Portuguese text using original vs. adapted bilingual tokenizer
  2. Evaluate model performance on held-out Portuguese test set before and after continuous pre-training
  3. Assess bilingual capabilities by comparing performance on English and Portuguese tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Cabrita tokenizer's performance compare to other state-of-the-art tokenizers for languages other than Portuguese and English?
- Basis in paper: [inferred] Paper focuses on Portuguese/English but suggests methodology could apply to other languages, referencing successful Chinese experiment
- Why unresolved: No empirical data or experiments for languages beyond Portuguese and English
- What evidence would resolve it: Experiments with Cabrita tokenizer on various languages compared to other state-of-the-art tokenizers

### Open Question 2
- Question: What is the long-term impact on model performance when using Cabrita tokenizer for continuous pre-training on multilingual corpus?
- Basis in paper: [inferred] Discusses benefits for Portuguese/English but doesn't explore effects on multilingual datasets
- Why unresolved: Study limited to Portuguese/English with no data on multilingual corpus performance
- What evidence would resolve it: Evaluate model performance after continuous pre-training on multilingual corpus using Cabrita tokenizer

### Open Question 3
- Question: How does Cabrita tokenizer influence model's ability to understand and generate text with complex Portuguese linguistic structures or idioms?
- Basis in paper: [explicit] Claims tokenizer enhances monolingual performance but lacks detailed analysis of complex linguistic structures
- Why unresolved: No detailed analysis or specific tests for complex Portuguese linguistic structures or idioms
- What evidence would resolve it: Detailed analysis of model performance on tasks requiring understanding/generation of complex Portuguese linguistic structures

## Limitations

- Limited empirical evidence comparing tokenizer merging approach against alternatives like training new bilingual tokenizer from scratch
- Performance attribution unclear - doesn't isolate impact of tokenizer adaptation versus continuous pre-training
- Data quality and domain coverage analysis insufficient for mC4-pt corpus beyond basic quality thresholds

## Confidence

- High Confidence: Technical feasibility of tokenizer merging and embedding matrix resizing follows established practices
- Medium Confidence: Performance claims reasonable but lack ablation studies and direct comparisons with alternative approaches
- Low Confidence: Claims about achieving competitive performance "with fewer parameters" difficult to evaluate without specific model comparisons

## Next Checks

1. **Ablation Study on Tokenization Strategy**: Train three model versions - (a) original OpenLLaMA with no adaptation, (b) model with merged tokenizer but no continuous pre-training, and (c) complete Cabrita methodology. Compare Portuguese task performance across all three to isolate impact of tokenizer adaptation versus continuous pre-training.

2. **Cross-Lingual Transfer Evaluation**: Evaluate adapted model on English tasks using same benchmarks (AG News, IMDB, SST2, BoolQ) to verify bilingual tokenizer doesn't degrade English performance. Compare results against original OpenLLaMA model to quantify performance changes.

3. **Tokenizer Efficiency Analysis**: Measure actual token count reduction on diverse Portuguese corpus (news, social media, literature) and correlate token savings with inference time improvements to validate claimed efficiency gains.