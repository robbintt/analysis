---
ver: rpa2
title: Identification of Knowledge Neurons in Protein Language Models
arxiv_id: '2312.10770'
source_url: https://arxiv.org/abs/2312.10770
tags:
- neurons
- knowledge
- neuron
- language
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of interpreting protein language
  models by identifying knowledge neurons in the ESM model fine-tuned for enzyme sequence
  classification. Two methods, activation-based and integrated gradients-based selection,
  were compared against a random baseline for selecting important neurons from the
  query-key-value prediction networks in self-attention modules.
---

# Identification of Knowledge Neurons in Protein Language Models

## Quick Facts
- arXiv ID: 2312.10770
- Source URL: https://arxiv.org/abs/2312.10770
- Reference count: 4
- Key outcome: Activation-based and integrated gradients methods outperform random selection in identifying knowledge neurons, with key vector neurons being most important for enzyme classification

## Executive Summary
This study addresses the challenge of interpreting protein language models by identifying knowledge neurons in the ESM model fine-tuned for enzyme sequence classification. The researchers compared two interpretability methods—activation-based and integrated gradients-based selection—against a random baseline for selecting important neurons from the query-key-value prediction networks in self-attention modules. Results showed that both methods consistently outperformed random selection, with integrated gradients performing best, particularly for larger neuron subsets. The analysis revealed that neurons in key vector prediction networks were most important for knowledge expression, suggesting these neurons capture knowledge of different enzyme sequence motifs.

## Method Summary
The researchers fine-tuned the ESM-2 model on 11,731 enzyme sequences from PDB for binary classification. They then applied activation-based selection (preserving neurons with high activation magnitudes) and integrated gradients-based selection (measuring perturbation effects on model output) to identify knowledge neurons. Submodels were created by ablating neurons based on each method at different sparsity levels (50%, 25%, 10%, 1%) and evaluated against a random baseline. The integrated gradients method was implemented using the Captum library and computed once on a cloud GPU cluster due to memory constraints.

## Key Results
- Both activation-based and integrated gradients methods consistently outperformed random selection in identifying knowledge neurons
- Integrated gradients method performed best for larger subsets (50% and 25% of neurons)
- Key vector prediction networks contained the highest density of knowledge neurons, suggesting they capture enzyme sequence motifs
- Value vector neurons were least important for knowledge expression in this classification task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge neurons in key vector prediction networks are more critical for enzyme sequence classification
- Mechanism: Key vectors specialize in understanding different features of input sequences, allowing selective attention to specific aspects of enzyme sequences
- Core assumption: Neuron importance correlates with their role in self-attention, with key vectors more crucial for understanding input sequences
- Evidence anchors: Abstract mentions high density of knowledge neurons in key vector networks; Figure 3 shows value neurons are least important
- Break condition: If query or value neurons prove equally or more important for knowledge expression

### Mechanism 2
- Claim: Activation-based and integrated gradients methods effectively identify crucial knowledge neurons
- Mechanism: These methods evaluate each neuron's contribution to predictions through activation magnitude and perturbation effects
- Core assumption: Neurons with higher activation or significant gradient impact express relevant knowledge
- Evidence anchors: Abstract states both methods outperform random baseline; section confirms successful identification
- Break condition: If random selection or other methods outperform these approaches

### Mechanism 3
- Claim: Integrated gradients outperforms activation-based for larger neuron subsets
- Mechanism: Integrated gradients measures perturbation effects more sensitively for larger subsets
- Core assumption: Integrated gradients is more sensitive to neuron importance in larger subsets
- Evidence anchors: Abstract notes high density in key vector networks; section shows integrated gradients performs best for 50% and 25% submodels
- Break condition: If activation-based method performs better for larger subsets

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: Understanding self-attention is crucial to grasp why key neurons are more important
  - Quick check question: What is the role of key vectors in self-attention, and how do they differ from query and value vectors?

- Concept: Integrated gradients as an interpretability method
  - Why needed here: Knowing how integrated gradients works explains its effectiveness in identifying knowledge neurons
  - Quick check question: How does integrated gradients measure neuron contribution to model output, and why is this useful?

- Concept: Enzyme sequence classification
  - Why needed here: Understanding the task helps interpret why certain neurons are more important
  - Quick check question: What are enzyme sequence motifs, and why are they important for classification?

## Architecture Onboarding

- Component map: ESM-2 transformer model with pre-trained embeddings -> Fine-tuned classification head for enzyme sequence classification -> Self-attention modules with query-key-value prediction networks -> Activation-based and integrated gradients selection methods

- Critical path:
  1. Pre-train ESM-2 on protein sequences
  2. Fine-tune model for enzyme sequence classification
  3. Apply selection methods to identify knowledge neurons
  4. Evaluate submodel performance at different sparsity levels

- Design tradeoffs: Balancing knowledge neuron count versus performance; choosing between selection methods based on subset size; interpretability versus model complexity tradeoff

- Failure signatures: Poor submodel performance; no difference between random and proposed methods; inconsistent results across tasks or models

- First 3 experiments:
  1. Apply selection methods to identify knowledge neurons in fine-tuned ESM-2
  2. Evaluate submodel performance containing identified neurons on test set
  3. Analyze knowledge neuron distribution across layers and prediction network types

## Open Questions the Paper Calls Out

- How can we characterize the specific types of knowledge encoded by individual neurons beyond binary classification (knowledge vs non-knowledge)?
- Do findings about knowledge neuron importance generalize across different protein language models and tasks beyond enzyme classification?
- Can we develop more efficient methods for integrated gradients computation that reduce memory requirements while maintaining accuracy?

## Limitations

- Single model and task scope limits generalizability to other protein language models and computational biology tasks
- Small dataset size (11,731 sequences) may not capture full complexity of protein sequences
- Ablation approach may miss complex neuron interactions that emerge during full model operation

## Confidence

- High Confidence: Both interpretability methods outperform random selection in identifying knowledge neurons
- Medium Confidence: Integrated gradients performs better than activation-based for larger subsets
- Low Confidence: Generalizability of findings to other models, tasks, or domains

## Next Checks

1. Test interpretability methods and neuron importance patterns on ESM-2 models fine-tuned for different protein prediction tasks
2. Implement sequential ablation experiment (one neuron at a time) to verify parallel ablation methodology doesn't create artifacts
3. Evaluate transferability of identified knowledge neurons to improve or interpret different protein models or tasks