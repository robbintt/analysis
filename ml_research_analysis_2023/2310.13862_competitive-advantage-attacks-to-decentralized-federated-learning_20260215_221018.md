---
ver: rpa2
title: Competitive Advantage Attacks to Decentralized Federated Learning
arxiv_id: '2310.13862'
source_url: https://arxiv.org/abs/2310.13862
tags:
- clients
- selfish
- shared
- local
- non-selfish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces SelfishAttack, the first competitive advantage
  attack to decentralized federated learning (DFL). SelfishAttack targets colluding
  selfish clients aiming to achieve two goals: learning more accurate local models
  than when training alone (utility goal), and achieving higher accuracy than non-selfish
  clients (competitive-advantage goal).'
---

# Competitive Advantage Attacks to Decentralized Federated Learning

## Quick Facts
- arXiv ID: 2310.13862
- Source URL: https://arxiv.org/abs/2310.13862
- Reference count: 40
- Key outcome: SelfishAttack increases accuracy gaps between selfish and non-selfish clients by over 11% in most cases across CIFAR-10, FEMNIST, and Sent140 datasets.

## Executive Summary
This paper introduces SelfishAttack, the first competitive advantage attack targeting colluding selfish clients in decentralized federated learning (DFL). The attack aims to achieve two goals simultaneously: improving local model accuracy for selfish clients compared to training alone (utility goal), and achieving higher accuracy than non-selfish clients (competitive-advantage goal). SelfishAttack crafts carefully designed shared models sent from selfish to non-selfish clients in each training round, formulated as an optimization problem minimizing a weighted sum of loss terms quantifying both goals. The attack derives optimal shared models for FedAvg, Median, and Trimmed-mean aggregation rules and demonstrates effectiveness across multiple datasets.

## Method Summary
SelfishAttack operates in DFL settings where clients train locally and exchange model updates without central coordination. The attack exploits information asymmetry by having selfish clients craft adversarial shared models that, when aggregated with non-selfish clients' pre-aggregation models, produce post-aggregation models that preserve selfish client utility while degrading non-selfish client performance. The method formulates this as an optimization problem with two loss terms representing utility and competitive advantage goals. For coordinate-wise aggregation rules, the optimization is transformed to minimize over post-aggregation models, enabling analytical solutions. The attack includes timing mechanisms where selfish clients delay their attack until certain conditions are met, using loss monitoring to determine optimal attack timing.

## Key Results
- SelfishAttack increases accuracy gaps between selfish and non-selfish clients by over 11% in most cases across CIFAR-10, FEMNIST, and Sent140 datasets
- The attack successfully transfers to non-coordinate-wise aggregation rules (Krum, FLTrust, FLDetector, FLAME) using approximations derived from coordinate-wise solutions
- SelfishAttack outperforms baseline poisoning attacks including TrimAttack and GaussianAttack in both utility and competitive-advantage metrics
- Attack effectiveness varies with aggregation rule parameters, showing optimal performance under specific conditions for each aggregation method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SelfishAttack exploits the lack of central coordination in decentralized federated learning to craft model updates that simultaneously preserve selfish client utility and degrade non-selfish client performance.
- Mechanism: In each round, selfish clients send carefully crafted shared models to non-selfish clients such that the aggregation step produces a post-aggregation local model for the non-selfish client that is both close to its pre-aggregation model (preserving utility) and far from the model it would have received without selfish clients (creating competitive advantage).
- Core assumption: Selfish clients know the aggregation rule and have access to non-selfish clients' pre-aggregation models in each round.
- Evidence anchors:
  - [abstract]: "SelfishAttack targets colluding selfish clients aiming to achieve two goals: learning more accurate local models than when training alone (utility goal), and achieving higher accuracy than non-selfish clients (competitive-advantage goal)."
  - [section 4.2]: "The first loss term quantifies the utility goal, which aims to create shared models sent from selfish clients to a non-selfish client, ensuring that its post-aggregation local model remains close to its pre-aggregation local model."
  - [corpus]: No direct evidence; the corpus neighbors discuss Byzantine robustness and poisoning but do not address this specific competitive advantage attack.
- Break condition: If the aggregation rule becomes Byzantine-robust enough to detect and exclude the crafted models, or if clients use trusted execution environments to verify model authenticity.

### Mechanism 2
- Claim: For coordinate-wise aggregation rules (FedAvg, Median, Trimmed-mean), the attack can compute optimal shared models analytically by transforming the optimization problem to minimize over the post-aggregation model rather than the shared models.
- Mechanism: The optimization problem is reformulated to find the optimal post-aggregation model for each non-selfish client, then the shared models from selfish clients are derived to ensure the aggregation step produces this optimal post-aggregation model.
- Core assumption: The aggregation rule is coordinate-wise and deterministic, allowing closed-form solutions.
- Evidence anchors:
  - [section 4.3]: "We transform the optimization problem to one whose variable is the post-aggregation local model ˆwi. Then, we derive the optimal solution ˆw∗i to the transformed optimization problem."
  - [section 4.4-4.6]: Derive specific analytical forms for FedAvg, Median, and Trimmed-mean.
  - [corpus]: No direct evidence; the corpus neighbors focus on Byzantine robustness but not on deriving optimal adversarial updates.
- Break condition: If the aggregation rule is non-coordinate-wise or uses randomness, the analytical derivation fails and only approximate solutions are possible.

### Mechanism 3
- Claim: SelfishAttack can be transferred to non-coordinate-wise aggregation rules by using the shared models derived for coordinate-wise rules as approximations.
- Mechanism: When the aggregation rule is Krum, FLTrust, FLDetector, or FLAME, selfish clients use the optimal shared models computed for FedAvg, Median, or Trimmed-mean as their attack strategy, relying on the attack's effectiveness even without exact optimization.
- Core assumption: The approximate attack derived for coordinate-wise rules remains effective against more complex aggregation rules.
- Evidence anchors:
  - [section 4.7]: "When the aggregation rule is coordinate-wise FedAvg, Median, or Trimmed-mean, we can derive the optimal shared models... However, it is hard to derive the optimal shared models to attack other non-coordinate-wise aggregation rules such as Krum, FLTrust, FLDetector, and FLAME."
  - [section 5.2]: "Table 4 shows the results of our SelfishAttack when non-selfish clients use other aggregation rules... Selfish clients craft their shared models using the FedAvg based version of SelfishAttack... We observe that our SelfishAttack can transfer to these aggregation rules."
  - [corpus]: No direct evidence; the corpus neighbors discuss Byzantine robustness but not transfer attacks.
- Break condition: If the target aggregation rule has strong Byzantine-robust properties that effectively filter out the crafted models, the attack effectiveness drops significantly.

## Foundational Learning

- Concept: Federated Learning (FL) basics
  - Why needed here: The attack builds directly on the FL training process where clients train locally and aggregate models without a central server.
  - Quick check question: In standard FL, what happens in Step II and Step III of each global training round?

- Concept: Byzantine-robust aggregation rules
  - Why needed here: The attack's effectiveness depends on whether the aggregation rule can detect and filter out malicious updates from selfish clients.
  - Quick check question: How do Median and Trimmed-mean aggregation rules differ from FedAvg in handling outlier shared models?

- Concept: Optimization problem formulation
  - Why needed here: The attack is formulated as an optimization problem minimizing a weighted sum of two loss terms representing utility and competitive advantage goals.
  - Quick check question: In the transformed optimization problem, what are the bounds on the post-aggregation local model ˆwi[k]?

## Architecture Onboarding

- Component map: Non-selfish clients -> Train locally -> Send pre-aggregation models -> Receive aggregated models; Selfish clients -> Train locally -> Send crafted models to non-selfish clients -> Receive genuine models from other selfish clients -> Aggregate received models

- Critical path: 1) Non-selfish clients train and send pre-aggregation models, 2) Selfish clients receive these models, 3) Selfish clients compute optimal shared models based on aggregation rule, 4) Selfish clients send crafted models to non-selfish clients, 5) All clients aggregate received models

- Design tradeoffs: The attack balances utility (selfish clients still benefit from DFL) vs competitive advantage (selfish clients outperform non-selfish clients). Higher λ emphasizes competitive advantage but may reduce selfish client utility if non-selfish clients' models become too degraded.

- Failure signatures:
  - If selfish clients start attacking too early, non-selfish clients cannot learn good models, reducing the quality of shared models for selfish clients.
  - If λ is too high, non-selfish clients' post-aggregation models become poor quality, causing them to send bad models to selfish clients in subsequent rounds.
  - If the aggregation rule is Byzantine-robust enough, the crafted models may be detected and excluded.

- First 3 experiments:
  1. Implement the optimization problem transformation and verify that the derived optimal post-aggregation models satisfy the bounds constraints.
  2. Test the analytical shared model derivation for FedAvg on a simple dataset with known aggregation outcomes.
  3. Implement the attack timing mechanism using loss monitoring and test different ε and I parameter values on CIFAR-10.

## Open Questions the Paper Calls Out
- **Question 1**: How effective is SelfishAttack when the number of selfish clients is less than one-third of the total clients?
  - Basis in paper: [explicit] The paper states that N ≥ 3m + 1 is assumed, where m is the number of selfish clients and N is the total number of clients.
  - Why unresolved: The paper only evaluates SelfishAttack when the number of selfish clients is 30% of the total clients. It does not explore the attack's effectiveness when the number of selfish clients is less than one-third.
  - What evidence would resolve it: Evaluating SelfishAttack with different fractions of selfish clients, specifically when the number of selfish clients is less than one-third of the total clients.

- **Question 2**: How does SelfishAttack perform when the non-selfish clients use a different aggregation rule than the selfish clients?
  - Basis in paper: [explicit] The paper mentions that selfish clients can use different aggregation rules from non-selfish clients, but only evaluates SelfishAttack when selfish clients use the same aggregation rule as non-selfish clients.
  - Why unresolved: The paper does not provide experimental results for SelfishAttack when selfish clients use a different aggregation rule than non-selfish clients.
  - What evidence would resolve it: Evaluating SelfishAttack when selfish clients use different aggregation rules than non-selfish clients and comparing the results with the case when they use the same aggregation rule.

- **Question 3**: Can SelfishAttack be extended to other federated learning settings beyond decentralized federated learning?
  - Basis in paper: [inferred] The paper focuses on SelfishAttack in the context of decentralized federated learning, but the attack goals and methodology could potentially be applied to other federated learning settings.
  - Why unresolved: The paper does not explore the applicability of SelfishAttack to other federated learning settings beyond decentralized federated learning.
  - What evidence would resolve it: Evaluating SelfishAttack in other federated learning settings, such as cross-device federated learning or cross-silo federated learning, and comparing the results with the decentralized federated learning setting.

## Limitations
- The attack requires selfish clients to have access to non-selfish clients' pre-aggregation models, which may be difficult to obtain in real decentralized systems
- The analytical solutions for optimal shared models only work for coordinate-wise aggregation rules and may not generalize to more sophisticated Byzantine-robust methods
- The attack's success depends on carefully tuned timing parameters that require dataset-specific optimization

## Confidence
- **High Confidence**: The mechanism of exploiting information asymmetry between selfish and non-selfish clients to craft adversarial updates is well-established in the Byzantine-robustness literature and the mathematical formulation of the optimization problem is sound.
- **Medium Confidence**: The analytical derivations for optimal shared models under coordinate-wise aggregation rules (FedAvg, Median, Trimmed-mean) are correct, but their practical effectiveness depends on real-world aggregation implementation details.
- **Low Confidence**: The transferability of the attack to non-coordinate-wise aggregation rules (Krum, FLTrust, FLDetector, FLAME) relies on empirical observations rather than theoretical guarantees, and the effectiveness may vary significantly with different aggregation parameters.

## Next Checks
1. **Information Availability Validation**: Implement a communication channel simulation to verify whether selfish clients can realistically obtain non-selfish clients' pre-aggregation models in each round, and measure the attack effectiveness degradation when this information is partially available or delayed.

2. **Timing Mechanism Sensitivity Analysis**: Systematically vary the timing parameters ε and I across different datasets (CIFAR-10, FEMNIST, Sent140) and measure how the attack effectiveness changes, identifying optimal parameter ranges for different scenarios.

3. **Transferability Testing Under Different Aggregation Parameters**: For each non-coordinate-wise aggregation rule (Krum, FLTrust, FLDetector, FLAME), vary the aggregation-specific parameters (e.g., number of selected models in Krum, trust thresholds in FLTrust) and measure how this affects the transfer attack's effectiveness compared to the baseline methods.