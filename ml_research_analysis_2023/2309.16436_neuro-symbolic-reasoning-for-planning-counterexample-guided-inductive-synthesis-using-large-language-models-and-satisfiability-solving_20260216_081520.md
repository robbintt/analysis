---
ver: rpa2
title: 'Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis
  using Large Language Models and Satisfiability Solving'
arxiv_id: '2309.16436'
source_url: https://arxiv.org/abs/2309.16436
tags:
- state
- block1
- clear
- hand
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) combined
  with formal satisfiability modulo theory (SMT) solvers to iteratively generate and
  verify plans for block-world planning problems. The approach leverages the natural
  language capabilities of LLMs to encode planning problems and the deductive reasoning
  of SMT solvers to verify correctness.
---

# Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving

## Quick Facts
- arXiv ID: 2309.16436
- Source URL: https://arxiv.org/abs/2309.16436
- Reference count: 32
- Key outcome: The approach improves LLM success rates in generating correct block-world plans from 2/20 (GPT-3.5 Turbo) to 19/20 (GPT-4) by iteratively refining plans using SMT-based counterexamples.

## Executive Summary
This paper proposes a neuro-symbolic reasoning approach that combines large language models (LLMs) with satisfiability modulo theory (SMT) solvers to generate and verify plans for block-world planning problems. The method leverages the natural language processing capabilities of LLMs to encode planning problems and the deductive reasoning of SMT solvers to verify correctness. When an LLM-generated plan is incorrect, the SMT solver provides counterexamples that are used to refine the prompt for the LLM in an iterative process. Experiments demonstrate that this approach significantly improves the success rate of LLMs in generating correct plans compared to using LLMs alone, with GPT-4 achieving a 95% success rate on 3-block problems.

## Method Summary
The approach uses large language models (GPT-4, GPT-3.5 Turbo, Davinci, Curie, Babbage, and Ada) as learners to generate plans from natural language specifications of block-world planning problems. A formal verification engine (Z3 SMT solver) acts as an oracle to check plan correctness and generate counterexamples when plans are incorrect. The method employs Counterexample-Guided Inductive Synthesis (CEGIS), where natural language specifications and operations are translated into first-order logic constraints using the LLM. When the SMT solver finds an incorrect plan, it provides counterexamples that are incorporated back into the LLM prompt to guide subsequent plan generation. This iterative process continues until a correct plan is generated or a maximum number of iterations is reached.

## Key Results
- GPT-4 solved 19 out of 20 problems with 3 blocks, while other models had lower success rates (GPT-3.5 Turbo: 2/20, Davinci: 8/20, Curie: 5/20, Babbage: 4/20, Ada: 1/20)
- The number of iterations needed for the LLM to generate a correct plan decreases as the problem size increases
- The neuro-symbolic approach demonstrates improved performance over using LLMs alone for planning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM can be iteratively refined to generate correct plans using counterexamples from an SMT solver.
- Mechanism: When the LLM generates an incorrect plan, the SMT solver verifies it against the formal specification and produces counterexamples showing which parts of the plan violate constraints. These counterexamples are added back to the prompt, creating a feedback loop that guides the LLM away from incorrect solutions.
- Core assumption: The LLM can learn from counterexamples provided in the prompt context and adjust future outputs accordingly.
- Evidence anchors:
  - [abstract] "we use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generated solutions from the LLMs, produce counterexamples when the solutions are incorrect, and provide that feedback to the LLMs exploiting the dialog capability of instruct-trained LLMs"
  - [section 3] "we use the LLMs as the learner in our work and a formal verification engine as the oracle"
- Break condition: If the LLM cannot incorporate counterexamples effectively due to context window limitations or inability to generalize from the counterexamples.

### Mechanism 2
- Claim: Natural language can be automatically translated to formal constraints that an SMT solver can process.
- Mechanism: The LLM translates natural language descriptions of planning problems and operations (pick-up, put-down, stack, unstack) into first-order logic constraints using Z3 Python API syntax. This enables the SMT solver to verify plan correctness.
- Core assumption: The LLM has sufficient capability to accurately translate natural language specifications into formal logic.
- Evidence anchors:
  - [section 4] "The natural language specification of the plan in the BLOCKWORLD problem is also translated into formal (first-order) constraints using the LLM"
  - [section A] Provides detailed examples of natural language to formal logic translation for each operation
- Break condition: If the LLM mistranslates the natural language specification, leading to incorrect constraints and verification failures.

### Mechanism 3
- Claim: The combination of LLM and SMT solver provides a scalable yet trusted synthesis approach.
- Mechanism: The LLM handles the inductive generalization from examples and natural language, while the SMT solver provides deductive verification and counterexamples. This division of labor leverages the strengths of both approaches.
- Core assumption: The verification step is computationally cheaper than solving the planning problem from scratch.
- Evidence anchors:
  - [section 3] "the formal verifiers are very fast at checking the correctness of a single solution for a task such as planning, as compared to solving the planning problem using combinatorial search"
  - [section 4] Quantitative results show GPT-4 solving 19/20 problems with 3 blocks, demonstrating scalability
- Break condition: If verification becomes computationally expensive for larger problems, negating the scalability benefit.

## Foundational Learning

- Concept: Counterexample-Guided Inductive Synthesis (CEGIS)
  - Why needed here: This is the core paradigm that enables iterative refinement of LLM-generated plans using formal verification
  - Quick check question: What are the two main components in a CEGIS loop and what roles do they play?

- Concept: Satisfiability Modulo Theory (SMT) solving
  - Why needed here: Provides the formal verification engine that can check plan correctness and generate counterexamples
  - Quick check question: How does an SMT solver differ from a standard SAT solver in terms of the types of constraints it can handle?

- Concept: Block-world planning domain
  - Why needed here: The experimental domain used to evaluate the approach, requiring understanding of predicates like On, Clear, Arm-empty, and actions like Pick-up, Put-down, Stack, Unstack
  - Quick check question: What are the four primitive actions in the standard blocks world planning domain?

## Architecture Onboarding

- Component map: LLM (GPT-4, GPT-3.5 Turbo, etc.) → Natural language planner → SMT solver (Z3) → Counterexample feedback → LLM prompt update
- Critical path: Problem specification → LLM translation to constraints → LLM initial plan generation → SMT verification → Counterexample generation (if needed) → LLM prompt refinement → Repeat until correct
- Design tradeoffs: Larger context windows allow more counterexamples but increase computational cost; simpler verification is faster but may miss subtle errors
- Failure signatures: LLM repeatedly generates incorrect plans despite counterexamples; translation from natural language to formal constraints fails; verification takes too long
- First 3 experiments:
  1. Run the illustrative example from section 4 to verify the basic CEGIS loop works
  2. Test GPT-4 on a simple 3-block problem and measure the number of iterations needed
  3. Compare the performance of different GPT models (GPT-4, GPT-3.5 Turbo, etc.) on the same 3-block problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed approach scale with larger block-world problems beyond 10 blocks?
- Basis in paper: [inferred] The paper mentions testing with up to 10 blocks, but doesn't explore performance on larger problems.
- Why unresolved: The paper doesn't provide results or analysis for block-world problems larger than 10 blocks.
- What evidence would resolve it: Experiments and quantitative results showing success rates and iteration counts for problems with more than 10 blocks.

### Open Question 2
- Question: How does the proposed approach compare to other existing methods for automated planning in terms of success rate and computational efficiency?
- Basis in paper: [inferred] The paper doesn't compare its approach to other automated planning methods.
- Why unresolved: The paper only evaluates its own approach without benchmarking against alternative methods.
- What evidence would resolve it: Comparative experiments showing performance metrics of the proposed approach against other state-of-the-art automated planning methods.

### Open Question 3
- Question: How sensitive is the proposed approach to the quality and specificity of the natural language problem descriptions?
- Basis in paper: [explicit] The paper mentions that the approach allows users to communicate planning problems in natural language, but doesn't explore the impact of varying quality or specificity.
- Why unresolved: The paper doesn't investigate how different qualities or levels of specificity in natural language descriptions affect the approach's performance.
- What evidence would resolve it: Experiments testing the approach's performance with natural language descriptions of varying quality and specificity, measuring success rates and iteration counts.

## Limitations

- Evaluation is constrained to small block-world instances (≤ 3 blocks), so scalability to larger problems remains unverified
- Performance comparisons with pure formal planners or non-neuro-symbolic baselines are absent
- The claim that verification is "much cheaper" than planning is stated but not empirically supported with timing data

## Confidence

- **High confidence**: The core CEGIS loop integrating LLM-generated plans with SMT-based verification is correctly described and its basic operation is supported by the abstract and section 3
- **Medium confidence**: The reported performance differences between GPT-4 and other models are credible given GPT-4's documented strengths, though absolute numbers lack full experimental detail
- **Low confidence**: The scalability claims and the assertion that verification is computationally cheaper than planning lack quantitative backing and remain unverified beyond the small problem sizes tested

## Next Checks

1. Reproduce the iterative refinement loop using a simple 3-block block-world problem to verify that the LLM-SMT feedback mechanism works as described
2. Measure and compare verification times against planning times for the same problems to empirically assess the claim that verification is computationally cheaper
3. Test the approach on larger instances (e.g., 5 or 6 blocks) to evaluate scalability and identify at which point the method's performance degrades or becomes impractical