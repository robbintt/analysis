---
ver: rpa2
title: Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression
  Values
arxiv_id: '2306.06276'
source_url: https://arxiv.org/abs/2306.06276
tags:
- cancer
- data
- samples
- learning
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method that uses contrastive learning to
  improve cancer prognosis prediction from gene expression data. The approach learns
  feature representations in a low-dimensional space using supervised contrastive
  learning, which are then used to train Cox models for hazard ratio prediction or
  classifiers for risk group classification.
---

# Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values

## Quick Facts
- arXiv ID: 2306.06276
- Source URL: https://arxiv.org/abs/2306.06276
- Reference count: 30
- Primary result: CLCox models improved concordance index by 0.058-0.062 and AUC by >0.1 across multiple cancer types

## Executive Summary
This paper presents a novel method using supervised contrastive learning to improve cancer prognosis prediction from gene expression data. The approach learns low-dimensional feature representations that better capture survival-related patterns, which are then used to train Cox models for hazard ratio prediction or classifiers for risk group classification. When applied to 19 cancer types from TCGA data, the method significantly outperformed existing Cox-based approaches, with concordance index improvements of 0.058-0.062 and AUC increases exceeding 0.1 for risk classification across 14 cancer types.

## Method Summary
The method uses supervised contrastive learning with multi-layer perceptrons to learn low-dimensional feature representations from gene expression data and clinical information. These features are then input to Cox models (Cox-EN, Cox-XGB, Cox-nnet) or classifiers (XGBoost) for prognosis prediction. The approach includes data pooling across cancer types with similar gene expression patterns to improve generalization. Hyperparameter tuning is performed via 5-fold cross-validation, and the method is evaluated across 40 random train/test splits.

## Key Results
- CLCox models achieved concordance index improvements of 0.058-0.062 across all 19 cancer types compared to existing methods
- Risk classification with contrastive learning increased AUC by more than 0.1 for all 14 cancer types studied
- Data pooling improved concordance index by approximately 0.05 for LUSC and showed consistent improvements across cancer types in the same expression clusters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning learns better low-dimensional representations by aligning samples with similar survival outcomes and separating dissimilar ones
- Mechanism: The supervised contrastive loss function pulls together samples in the same risk group and pushes apart samples in different risk groups in the learned feature space
- Core assumption: Samples with similar progression-free intervals (PFI) share underlying biological patterns that can be captured in a lower-dimensional space
- Evidence anchors: [abstract] "we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space"; [section] "the low-dimensional features learned by the CL method are closely aligned for those data samples in the same group and more separated for those samples in different groups"

### Mechanism 2
- Claim: Pooling data across cancer types with similar gene expression patterns improves model generalization
- Mechanism: Training on pooled data from Hoadley et al.'s gene expression clusters provides more samples per risk group, improving the learned feature space
- Core assumption: Cancer types in the same cluster share common gene expression patterns that make them suitable for joint training
- Evidence anchors: [section] "we reasoned that if we pooled the data in the same cluster to train a model, we might improve prediction accuracy especially for those types of cancer with relatively small number of samples"; [section] "data pooling improved the c-index by about 0.05 for LUSC (SM)"

### Mechanism 3
- Claim: Reducing high-dimensional gene expression to lower-dimensional features helps Cox models learn better
- Mechanism: The contrastive learning module acts as a dimensionality reduction and feature engineering step before Cox model training
- Core assumption: The original 20,000+ gene expression features contain redundant information and nonlinear interactions that can be compressed into a smaller set of informative features
- Evidence anchors: [abstract] "we applied supervised contrastive learning to tumor transcriptomes and clinical data to learn feature representations in a low-dimensional space"; [section] "Contrastive learning (CL) is a self-supervised machine learning technique used to learn the general features of a data set without labels by teaching the model which data points are similar or different"

## Foundational Learning

- Concept: Survival analysis and the Cox proportional hazards model
  - Why needed here: The paper builds CLCox models that use learned features as input to Cox models for prognosis prediction
  - Quick check question: What is the difference between the partial likelihood used in Cox models and the contrastive loss used in CL modules?

- Concept: Supervised contrastive learning
  - Why needed here: The method uses supervised CL to learn feature representations that group samples by survival outcome
  - Quick check question: How does the supervised contrastive loss differ from the self-supervised contrastive loss used in image classification?

- Concept: Cross-validation and hyperparameter tuning
  - Why needed here: The paper uses 5-fold cross-validation to select hyperparameters for both CL modules and downstream Cox/classification models
  - Quick check question: Why is it important to use cross-validation rather than a single train/test split when selecting hyperparameters?

## Architecture Onboarding

- Component map: Input (gene expression vectors) → CL module (MLP with supervised contrastive loss) → Feature output → Cox model (for hazard prediction) OR Classifier (for risk group classification)
- Critical path: CL module training → Feature extraction → Downstream model training → Evaluation
- Design tradeoffs: The paper uses sigmoid activation and early stopping in the CL module to prevent overfitting; the choice of grouping samples (by PFI) is a critical design decision
- Failure signatures: If CL modules don't improve concordance index or AUC over baseline models, the feature learning is ineffective; if pooling across cancer types hurts performance, the assumption about shared patterns is wrong
- First 3 experiments:
  1. Train CL module with different numbers of hidden layers (2-5) and nodes (1,024-8,192) on a single cancer type, evaluate feature quality via downstream Cox model performance
  2. Test different grouping strategies for contrastive learning (e.g., PFI < 3 years vs ≥ 3 years vs finer-grained groups) and measure impact on classification accuracy
  3. Compare performance of CL-based models with and without data pooling for cancer types in the same gene expression cluster from Hoadley et al.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CLCox models compare when trained with different types of gene expression data preprocessing (e.g., different normalization methods, batch correction techniques)?
- Basis in paper: [inferred] The paper mentions that RNA-seq data has been normalized and batch corrected, but does not explore the impact of different preprocessing methods on model performance.
- Why unresolved: The study uses a single preprocessing pipeline without comparing alternative approaches.
- What evidence would resolve it: Systematic comparison of CLCox performance using different preprocessing methods on the same dataset.

### Open Question 2
- Question: Can contrastive learning be effectively applied to single-cell RNA-seq data for cancer prognosis prediction?
- Basis in paper: [inferred] The study uses bulk RNA-seq data, but does not explore the applicability of CL to single-cell data which has different characteristics and challenges.
- Why unresolved: Single-cell data has unique properties (e.g., higher dropout rates, different normalization needs) that may affect CL performance.
- What evidence would resolve it: Implementation and validation of CL-based models on single-cell RNA-seq datasets from cancer patients.

### Open Question 3
- Question: How does the performance of CL-based classifiers change when using different risk stratification criteria (e.g., different PFI thresholds)?
- Basis in paper: [explicit] The paper uses a specific PFI threshold (3 years) to define risk groups, but does not explore how different thresholds affect classifier performance.
- Why unresolved: The choice of risk stratification criteria can significantly impact model performance and clinical utility.
- What evidence would resolve it: Comparative analysis of CL-based classifier performance using multiple risk stratification criteria.

### Open Question 4
- Question: Can the CLCox models effectively predict prognosis for rare cancer types with limited sample sizes?
- Basis in paper: [inferred] The study focuses on cancer types with sufficient sample sizes, but does not address the challenge of rare cancers with limited data.
- Why unresolved: Rare cancers present unique challenges due to limited data availability, which may affect model generalization.
- What evidence would resolve it: Validation of CLCox models on rare cancer types with small sample sizes to assess performance and generalizability.

## Limitations

- The specific gene expression preprocessing pipeline and hyperparameter choices are not fully detailed, which could affect reproducibility
- The assumption that cancer types within the same gene expression cluster share survival dynamics may not hold universally, potentially limiting the effectiveness of data pooling strategies
- The method focuses on specific cancer types and may not generalize to rare cancers or different expression patterns not captured in the Hoadley et al. clusters

## Confidence

- **High Confidence**: The core claim that supervised contrastive learning improves feature representations for downstream survival models is well-supported by the experimental results across multiple cancer types and evaluation metrics
- **Medium Confidence**: The specific mechanisms by which contrastive learning improves performance (dimensionality reduction, better alignment of similar samples) are plausible but not directly validated through ablation studies
- **Medium Confidence**: The data pooling strategy showing consistent improvements across cancer types is supported by results but the generalizability to other cancer types or expression clusters remains to be tested

## Next Checks

1. Conduct ablation studies comparing CL-based models with and without dimensionality reduction to isolate the effect of feature learning versus sample alignment
2. Test the data pooling strategy on cancer types not included in the original Hoadley et al. clusters to assess generalizability
3. Validate the approach on independent cancer cohorts (e.g., from ICGC) to confirm that improvements in concordance index and AUC are not dataset-specific artifacts