---
ver: rpa2
title: Overview of the TREC 2023 Product Product Search Track
arxiv_id: '2311.07861'
source_url: https://arxiv.org/abs/2311.07861
tags:
- product
- baselines
- metadata
- queries
- collection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The TREC 2023 Product Search Track introduced a new reusable collection
  for evaluating retrieval methods in product search, incorporating contextual metadata
  and multi-modal data. The collection was based on the ESCI Shopping Queries dataset,
  enriched with additional metadata and images, and evaluated using a mix of synthetically
  generated and title-based queries.
---

# Overview of the TREC 2023 Product Product Search Track

## Quick Facts
- arXiv ID: 2311.07861
- Source URL: https://arxiv.org/abs/2311.07861
- Reference count: 21
- Primary result: Traditional retrieval systems like BM25 often outperform neural embedding models in product search tasks

## Executive Summary
The TREC 2023 Product Search Track introduced a new reusable collection for evaluating retrieval methods in product search, incorporating contextual metadata and multi-modal data. The collection was based on the ESCI Shopping Queries dataset, enriched with additional metadata and images, and evaluated using a mix of synthetically generated and title-based queries. The study found that traditional retrieval systems like BM25 often outperformed neural embedding models in both zero-shot and fine-tuned settings, with some models showing consistent gains when using enriched metadata. However, results were mixed for neural models, with larger models sometimes underperforming smaller variants. Fine-tuning improved performance for smaller models but degraded results for larger ones, suggesting potential issues with fine-tuning procedures. Overall, the study highlights the effectiveness of traditional methods and the nuanced impact of metadata in product search retrieval.

## Method Summary
The TREC 2023 Product Search Track evaluated various retrieval methods on a product search collection based on the ESCI Shopping Queries dataset, enriched with additional metadata and images. The study used 182 judged queries, including both synthetically generated and title-based queries, to evaluate retrieval performance. Traditional methods like BM25, hybrid retrieval systems, and neural embedding models were tested, with some models fine-tuned using the Tevatron library. The evaluation metrics included NDCG@10, NDCG@100, R@10, R@100, and InfAP. Fine-tuning was performed for 40 epochs using 4 A100 GPUs, batch size of 128, cross-device negatives, and learning rates ranging from 1e-5 to 1e-4.

## Key Results
- Traditional retrieval systems like BM25 often outperform neural embedding models in product search tasks
- Fine-tuning improved performance for smaller models but degraded results for larger ones
- Metadata enrichment provided inconsistent benefits across different retrieval approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional retrieval systems like BM25 outperform neural embedding models in product search tasks.
- Mechanism: BM25 leverages exact term matching and term frequency-inverse document frequency weighting, which aligns well with keyword-driven product queries and metadata-rich product descriptions.
- Core assumption: Product queries often contain specific keywords that directly match product titles and descriptions, making exact matching more effective than semantic similarity.
- Evidence anchors:
  - [abstract]: "traditional retrieval systems are highly effective and commonly outperform general-purpose pretrained embedding models."
  - [section]: "we see clear gains from hybrid retrieval systems that leverage multiple retrievers to improve performance (f_splade_bm25, cfdaclip_MR_A)."
- Break condition: If queries become more conversational or semantically complex, requiring understanding of intent beyond exact term matches.

### Mechanism 2
- Claim: Metadata enrichment does not consistently improve retrieval performance across all models.
- Mechanism: Additional metadata can introduce noise or irrelevant information, which may confuse neural models trained on clean, structured data but can be filtered effectively by traditional methods.
- Core assumption: Neural models are sensitive to the quality and relevance of input features, while traditional models can better handle noise through term-based filtering.
- Evidence anchors:
  - [abstract]: "additional metadata can improve retrieval performance at a macro level, extra information cannot guarantee performance."
  - [section]: "Despite this variability, our impact tends to be relatively small, as good retrievers models know an effect of less than 5%."
- Break condition: If metadata is carefully curated and relevant, or if models are specifically trained to leverage metadata features.

### Mechanism 3
- Claim: Fine-tuning large neural models can degrade performance in product search.
- Mechanism: Large models may overfit to the training data or lose general retrieval capabilities when fine-tuned on a specific domain without proper regularization.
- Core assumption: Large models have more parameters to fit, making them more prone to overfitting when fine-tuning data is limited or the fine-tuning procedure is suboptimal.
- Evidence anchors:
  - [section]: "larger models suffer from fine-tuning (indicating the fine-tuning recipe was incorrect), but smaller models see large gains."
  - [section]: "we see a consistent trend that the larger models suffer from fine-tuning."
- Break condition: If fine-tuning is performed with more data, better regularization, or more sophisticated techniques.

## Foundational Learning

- Concept: Term frequency-inverse document frequency (TF-IDF) weighting
  - Why needed here: Understanding how BM25 uses TF-IDF to score documents based on term importance is crucial for grasping why it performs well in product search.
  - Quick check question: How does BM25 adjust term frequency to prevent bias toward longer documents?

- Concept: Semantic similarity and dense embeddings
  - Why needed here: To understand the limitations of neural models that rely on semantic similarity rather than exact term matching in product search.
  - Quick check question: Why might semantic similarity be less effective for product queries that use specific product names or technical terms?

- Concept: Fine-tuning vs. zero-shot learning
  - Why needed here: To comprehend why fine-tuning affects large and small models differently and the implications for model selection.
  - Quick check question: What are the risks of fine-tuning a large model on a small, domain-specific dataset?

## Architecture Onboarding

- Component map: Query processing -> Retrieval engine (BM25 or neural) -> Ranking -> Result presentation
- Critical path:
  1. Receive user query
  2. Retrieve candidate products using BM25 or neural embeddings
  3. Apply ranking adjustments based on metadata
  4. Present top results to user
- Design tradeoffs:
  - BM25: Fast, interpretable, excels with keyword queries but lacks semantic understanding
  - Neural models: Can capture semantic similarity but may underperform with keyword-rich product data
  - Metadata usage: Can enhance relevance but may introduce noise or complexity
- Failure signatures:
  - BM25: Poor recall for semantically similar but lexically different queries
  - Neural models: Low precision when exact product names are used
  - Metadata issues: Inconsistent performance improvements or degradation
- First 3 experiments:
  1. Compare BM25 vs. neural retrieval on a sample of keyword-heavy product queries
  2. Test impact of adding/removing metadata features on retrieval performance
  3. Fine-tune a small neural model on a subset of data and evaluate performance changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of dense retrieval models compare to traditional methods like BM25 when fine-tuned on larger, domain-specific datasets with more diverse queries?
- Basis in paper: [explicit] The paper notes that traditional retrieval systems like BM25 often outperform neural embedding models, and fine-tuning improved performance for smaller models but degraded results for larger ones.
- Why unresolved: The study used a relatively small set of queries and a single fine-tuning procedure, which may not capture the full potential of dense retrieval models in different contexts or with more extensive training data.
- What evidence would resolve it: Comparative experiments using larger, domain-specific datasets with more diverse queries and multiple fine-tuning strategies could provide clearer insights into the relative performance of dense retrieval models versus traditional methods.

### Open Question 2
- Question: What is the impact of metadata enrichment on retrieval performance across different query types and product categories?
- Basis in paper: [explicit] The study found mixed results regarding the impact of metadata, with some models benefiting while others saw performance losses, and no clear trend in the impact of the expanded collection.
- Why unresolved: The analysis did not stratify the impact of metadata by query type or product category, leaving uncertainty about its effectiveness in specific contexts.
- What evidence would resolve it: Detailed analysis of metadata's impact across different query types and product categories, possibly using more granular evaluation metrics, could clarify its role in improving retrieval performance.

### Open Question 3
- Question: How do retrieval models perform when evaluated on multilingual product search queries, given the dataset's initial multilingual design?
- Basis in paper: [explicit] The dataset includes multilingual queries, but the study focused on English-only queries, leaving the performance on other languages unexplored.
- Why unresolved: The study did not evaluate the performance of retrieval models on non-English queries, which could reveal different effectiveness patterns or challenges.
- What evidence would resolve it: Evaluating retrieval models on multilingual queries using the full dataset could provide insights into their performance across different languages and cultural contexts.

## Limitations
- Findings based on a single annual product search collection may not generalize to all e-commerce contexts
- Reliance on synthetic queries generated by GPT-4 introduces uncertainty about real-world query behavior
- Fine-tuning procedures and model selection require further investigation due to mixed results

## Confidence

- **High**: Traditional retrieval systems (BM25) outperform neural models in product search contexts
- **Medium**: Metadata enrichment provides inconsistent benefits across different retrieval approaches
- **Low**: Fine-tuning procedures are optimal for neural models in product search tasks

## Next Checks

1. **Query Authenticity Test**: Evaluate retrieval performance using a dataset of genuine user queries from e-commerce platforms to assess whether synthetic query results hold in real-world scenarios.

2. **Fine-tuning Procedure Analysis**: Conduct ablation studies on fine-tuning hyperparameters (learning rates, batch sizes, early stopping criteria) to identify optimal configurations that prevent performance degradation in larger models.

3. **Metadata Quality Assessment**: Systematically vary the quality and relevance of metadata features to determine the conditions under which metadata enrichment provides consistent benefits versus introducing noise.