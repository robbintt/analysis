---
ver: rpa2
title: Efficient Online Data Mixing For Language Model Pre-Training
arxiv_id: '2312.02406'
source_url: https://arxiv.org/abs/2312.02406
tags:
- data
- mixing
- training
- weights
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an online data mixing method for large language
  model pretraining that leverages multi-armed bandit algorithms to adaptively adjust
  data sampling proportions during training. The method treats each data domain as
  a bandit arm and uses training loss as a reward signal to maximize information gain.
---

# Efficient Online Data Mixing For Language Model Pre-Training

## Quick Facts
- arXiv ID: 2312.02406
- Source URL: https://arxiv.org/abs/2312.02406
- Reference count: 22
- Primary result: Online data mixing using multi-armed bandit algorithms reduces training iterations by 19% and improves MMLU accuracy by 1.9% relative

## Executive Summary
This work introduces an online data mixing method for large language model pretraining that leverages multi-armed bandit algorithms to adaptively adjust data sampling proportions during training. The method treats each data domain as a bandit arm and uses training loss as a reward signal to maximize information gain. Experiments on a 1-billion parameter model trained on 50 billion tokens show that this approach reaches the final perplexity of the next best method with 19% fewer training iterations and improves 5-shot MMLU accuracy by 1.9% relative accuracy, while adding negligible computational overhead of 0.000007% to training time.

## Method Summary
The authors propose an online data mixing approach that treats each data domain as a multi-armed bandit arm and uses training loss as a reward signal. During pretraining, the algorithm continuously updates sampling probabilities based on recent losses, allowing it to adapt to domains that become more or less informative as training progresses. The method is implemented with a modified Exp3 algorithm using moving average rewards and requires no additional forward or backward passes beyond standard training procedures.

## Key Results
- Reaches final validation perplexity of next best method with 19% fewer training iterations
- Improves 5-shot MMLU accuracy by 1.9% relative accuracy
- Adds negligible computational overhead of 0.000007% to training time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The training loss per domain acts as an efficient reward signal that correlates with information gain.
- Mechanism: By treating each data domain as a multi-armed bandit arm and using the loss as reward, the algorithm increases sampling probability for domains with higher loss (higher perplexity), which indicates more information to be learned.
- Core assumption: Higher training loss on a domain indicates higher information gain potential.
- Evidence anchors:
  - [abstract]: "we utilize the training loss per domain as a reward for our multi-armed bandit algorithm"
  - [section]: "as discussed in section 1, perplexity (the exponentiated loss) is a measure of expected information gain from each token in a sequence"
  - [corpus]: Weak - no direct corpus evidence linking loss to information gain in this specific context.
- Break condition: If loss correlates poorly with actual information gain, the algorithm may sample suboptimal domains.

### Mechanism 2
- Claim: Online adaptation to changing training dynamics improves final model performance.
- Mechanism: The bandit algorithm continuously updates sampling probabilities based on recent losses, allowing it to adapt to domains that become more or less informative as training progresses.
- Core assumption: The relative importance of different data domains changes during training.
- Evidence anchors:
  - [abstract]: "our online approach optimizes the data mixing proportions during training"
  - [section]: "our method is the most effective, reaching the final validation perplexity of the next best method with 19% fewer iterations"
  - [corpus]: Weak - no direct corpus evidence of changing domain importance during training.
- Break condition: If domain importance remains static, online adaptation provides no benefit over static mixing.

### Mechanism 3
- Claim: The algorithm adds negligible computational overhead.
- Mechanism: Using training loss as reward requires no additional forward or backward passes, and the bandit algorithm itself has minimal computational cost.
- Core assumption: The overhead of the bandit algorithm is dominated by the loss calculation.
- Evidence anchors:
  - [abstract]: "while adding negligible wall-clock time during pretraining"
  - [section]: "we add no additional forward or backward passes through the model beyond standard training procedures"
  - [corpus]: Weak - no corpus evidence quantifying the actual overhead.
- Break condition: If the bandit algorithm's computational cost grows significantly with scale, overhead may become non-negligible.

## Foundational Learning

- Concept: Multi-armed bandit algorithms
  - Why needed here: Provides a principled framework for balancing exploration and exploitation when selecting data domains.
  - Quick check question: How does the exploration rate decay in the Exp3 algorithm, and why is this important?

- Concept: Information theory and perplexity
  - Why needed here: Perplexity as a measure of model uncertainty and expected information gain from learning the next token.
  - Quick check question: Why does higher perplexity (higher loss) indicate more information to be learned?

- Concept: Data mixing strategies
  - Why needed here: Understanding the difference between static and online data mixing is crucial for appreciating the contribution.
  - Quick check question: What are the limitations of fixed data mixing proportions during training?

## Architecture Onboarding

- Component map:
  Data domains (22 from The Pile) -> Model (1B parameter decoder-only transformer) -> Loss function (standard language modeling loss) -> Bandit algorithm (modified Exp3 with moving average rewards) -> Mixing distribution π(Di)

- Critical path:
  1. Sample domain Di according to current π
  2. Sample batch from Di
  3. Compute loss and update model
  4. Update rewards based on loss
  5. Update π for next iteration

- Design tradeoffs:
  - Exploration vs. exploitation balance
  - Homogeneity of micro-batches vs. diversity
  - Fixed vs. online data mixing proportions
  - Batch size and gradient accumulation steps

- Failure signatures:
  - High variance in domain losses leading to unstable π
  - Bandit algorithm failing to explore sufficiently
  - Computational overhead exceeding negligible threshold
  - Poor transfer of domain weights across different model architectures

- First 3 experiments:
  1. Compare validation perplexity of ODM vs. static mixing with The Pile Weights over first 10k steps.
  2. Measure computational overhead of ODM by comparing wall-clock time with baseline.
  3. Analyze the evolution of the sampling distribution π(Di) over training to verify adaptation to changing dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different tokenizer vocabularies (e.g., 256k vs 50k) affect the optimal data mixing proportions and domain weights across pretraining iterations?
- Basis in paper: [explicit] The authors found that DoReMi weights computed with a 256k vocabulary tokenizer led to worse perplexity than The Pile Weights, while 50k tokenizer weights performed better, suggesting tokenizer choice affects optimal weights.
- Why unresolved: The paper only compares two specific tokenizer sizes and doesn't systematically explore the relationship between tokenizer vocabulary size and optimal data mixing.
- What evidence would resolve it: A comprehensive study varying tokenizer vocabulary sizes while keeping all other factors constant, measuring resulting perplexity and MMLU performance to identify patterns.

### Open Question 2
- Question: Would mixing domains within micro-batches during the warm-up phase improve ODM's initial validation perplexity while maintaining its later advantages?
- Basis in paper: [explicit] The authors note that ODM's validation perplexity starts higher due to homogeneity of micro-batches, and suggest mixing domains during warm-up could alleviate this.
- Why unresolved: The authors only propose this as a potential solution but don't test it experimentally.
- What evidence would resolve it: An experimental comparison of ODM variants - one with domain-homogeneous micro-batches throughout, one with mixed-domain micro-batches only during warm-up, and one with mixed-domain micro-batches throughout.

### Open Question 3
- Question: What is the relationship between batch size, gradient accumulation steps, and ODM's performance in terms of both training efficiency and final model quality?
- Basis in paper: [explicit] The authors use specific batch size (480 total) and accumulation steps (8), but note that decreasing micro-batch size could reduce GPU utilization and wall-clock time.
- Why unresolved: The paper doesn't explore how different batch/accumulation configurations affect ODM's effectiveness.
- What evidence would resolve it: Experiments varying batch sizes and accumulation steps while measuring training time, validation perplexity trajectory, and final model performance across different configurations.

## Limitations
- The claim that training loss correlates with information gain remains theoretical without direct empirical validation through information-theoretic measurements
- The 19% improvement in iterations and 1.9% MMLU gain are demonstrated on a single model scale (1B parameters) and dataset (The Pile), limiting generalizability
- The negligible computational overhead claim (0.000007%) is asserted but not empirically verified through rigorous benchmarking

## Confidence

- High confidence: The fundamental approach of using multi-armed bandit algorithms for data mixing is sound and well-established in the literature
- Medium confidence: The experimental results showing improved efficiency and performance are convincing but limited to specific conditions
- Medium confidence: The theoretical justification linking training loss to information gain is reasonable but not directly validated

## Next Checks

1. Measure the actual computational overhead of ODM by running side-by-side timing experiments with and without the bandit algorithm across different hardware configurations
2. Validate the correlation between training loss and information gain by measuring mutual information or perplexity reduction when sampling from high-loss vs. low-loss domains
3. Test ODM's effectiveness across different model scales (e.g., 500M, 2B, 8B parameters) and alternative datasets to assess generalizability beyond the 1B parameter model on The Pile