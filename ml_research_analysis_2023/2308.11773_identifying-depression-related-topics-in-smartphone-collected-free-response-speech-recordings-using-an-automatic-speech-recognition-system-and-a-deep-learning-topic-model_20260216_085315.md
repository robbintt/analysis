---
ver: rpa2
title: Identifying depression-related topics in smartphone-collected free-response
  speech recordings using an automatic speech recognition system and a deep learning
  topic model
arxiv_id: '2308.11773'
source_url: https://arxiv.org/abs/2308.11773
tags:
- depression
- topics
- speech
- risk
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates the feasibility of identifying depression-related\
  \ topics in smartphone-collected free-response speech recordings using automatic\
  \ speech recognition (Whisper) and a deep learning topic model (BERTopic). The researchers\
  \ identified 29 topics in 3919 transcribed speech texts from 265 participants, with\
  \ six topics (No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework)\
  \ associated with higher depression severity (median PHQ-8 \u2265 10)."
---

# Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model

## Quick Facts
- arXiv ID: 2308.11773
- Source URL: https://arxiv.org/abs/2308.11773
- Reference count: 40
- Researchers used Whisper ASR and BERTopic to identify depression-related speech topics, finding six topics associated with higher depression severity and validated in a smaller dataset.

## Executive Summary
This study presents a novel approach to identifying depression-related topics in smartphone-collected free-response speech recordings by combining automatic speech recognition (Whisper) with a deep learning topic model (BERTopic). Analyzing 3,919 transcribed speech texts from 265 participants, researchers identified 29 topics, with six (No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework) associated with higher depression severity. The findings were validated in a smaller dataset (RAPID), showing consistent results for key topics. The study also found that participants discussing risk topics exhibited distinct behavioral patterns including higher sleep variability, later sleep onset, and fewer daily steps compared to those discussing non-risk topics.

## Method Summary
The researchers employed a three-step data-driven workflow to analyze depression-related speech topics. First, they used Whisper (Medium version) to automatically transcribe free-response speech recordings from smartphone-collected data. Second, they applied BERTopic to identify topics within the transcribed texts, manually reviewing and titling each topic based on keywords. Third, they compared depression severity (PHQ-8 scores), behavioral features (sleep variability, sleep onset, daily steps from Fitbit), and linguistic features (using LIWC-22) across topics to identify risk topics and understand their contextual characteristics. The pipeline was validated using a smaller dataset (RAPID) with PHQ-9 scores.

## Key Results
- Six topics (No Expectations, Sleep, Mental Therapy, Haircut, Studying, Coursework) showed median PHQ-8 ≥ 10, indicating depression risk
- Participants discussing risk topics had higher sleep variability, later sleep onset, and fewer daily steps than those discussing non-risk topics
- The topic model successfully identified depression-related themes without requiring predefined labels
- Findings were validated in a smaller dataset (RAPID), with consistent results for "No Expectations" and "Sleep" topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whisper transcription enables large-scale analysis of free-response speech data.
- Mechanism: Whisper's pre-trained ASR system provides near-human transcription accuracy, making it feasible to process thousands of speech recordings without manual effort.
- Core assumption: Whisper maintains acceptable accuracy across diverse accents and speech patterns in mental health contexts.
- Evidence anchors:
  - [abstract] "automatic speech recognition (Whisper) and a deep learning topic model (BERTopic)"
  - [section] "Whisper was trained on 680,000 hours of multilingual and multifunctional web-collected audio data, resulting in enhanced recognition of diverse accents, ambient noise, and specialized terminology"
  - [corpus] Weak evidence - no direct citations to Whisper validation studies in depression speech
- Break condition: Whisper's accuracy drops significantly for emotional speech, background noise, or non-native speakers.

### Mechanism 2
- Claim: BERTopic identifies depression-related speech topics without predefined labels.
- Mechanism: BERTopic uses transformer embeddings to cluster semantically similar speech content, revealing patterns linked to depression severity.
- Core assumption: Topic emergence reflects genuine mental health states rather than superficial content.
- Evidence anchors:
  - [abstract] "We identified 29 topics in 3919 transcribed speech texts from 265 participants"
  - [section] "By reviewing the keywords, we assigned an appropriate title to represent each topic"
  - [corpus] No direct validation of BERTopic performance on depression speech
- Break condition: Topics become too granular or fail to capture meaningful semantic relationships.

### Mechanism 3
- Claim: Behavioral and linguistic features validate topic-based depression associations.
- Mechanism: Sleep variability, sleep onset, daily steps, and linguistic markers (negation, negative emotion) provide objective context for interpreting speech topics.
- Core assumption: Behavioral patterns correlate with depression severity in ways that support linguistic findings.
- Evidence anchors:
  - [abstract] "participants mentioning risk topics had higher sleep variability, later sleep onset, and fewer daily steps"
  - [section] "We found that behavioral features derived from one-week Fitbit recordings prior to the speech task – Sleep Variability, Sleep Onset, and Daily Steps – were significantly different across topics"
  - [corpus] Strong - cited behavioral-depression associations from prior mHealth studies
- Break condition: Behavioral markers fail to differentiate between depression and other conditions with similar symptoms.

## Foundational Learning

- Concept: Automatic Speech Recognition
  - Why needed here: Enables processing of large volumes of speech data without manual transcription
  - Quick check question: What are the key limitations of Whisper when applied to clinical speech data?

- Concept: Topic Modeling with BERTopic
  - Why needed here: Identifies meaningful themes in unstructured speech without requiring labeled training data
  - Quick check question: How does BERTopic differ from traditional LDA in handling short, conversational texts?

- Concept: Behavioral Feature Extraction
  - Why needed here: Provides objective validation of depression severity through passive monitoring
  - Quick check question: Which Fitbit-derived features showed strongest associations with depression in this study?

## Architecture Onboarding

- Component map: Speech recordings -> Whisper ASR -> BERTopic -> Topic clusters -> PHQ-8 correlation -> Behavioral/Linguistic validation
- Critical path: Transcription quality -> Topic coherence -> Depression severity association -> Behavioral/linguistic validation
- Design tradeoffs: Whisper's general-purpose training vs. domain-specific fine-tuning for depression speech
- Failure signatures: Low topic coherence, weak depression correlations, inconsistent behavioral patterns
- First 3 experiments:
  1. Test Whisper transcription accuracy on a subset of speech recordings with manual verification
  2. Validate BERTopic topic stability using different random seeds and preprocessing parameters
  3. Correlate extracted topics with PHQ-8 scores using different statistical thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified speech topics perform as predictors of depression severity in a non-depressed population?
- Basis in paper: [explicit] The study recruited participants with a history of depression, so findings may not generalize to non-depressed populations.
- Why unresolved: The study only included participants with a history of depression, so it cannot assess generalizability to other populations.
- What evidence would resolve it: Testing the topic model on a dataset of non-depressed individuals and comparing topic frequencies and depression severity predictions.

### Open Question 2
- Question: What is the causation relationship between language use and depression severity, beyond correlation?
- Basis in paper: [explicit] The study investigates correlation between topic shifts and changes in depression severity, but notes that causation relationships need further investigation.
- Why unresolved: The study only explores correlations, not causation, between language use and depression.
- What evidence would resolve it: Longitudinal studies manipulating language use and measuring subsequent changes in depression severity, or randomized controlled trials testing interventions targeting language use.

### Open Question 3
- Question: How can the topic model be improved to distinguish between different contexts and dispositions for the same topic?
- Basis in paper: [explicit] The study notes that for a certain topic, different participants may mention it with different dispositions, which may explain some inconsistencies in topic-depression severity associations.
- Why unresolved: The current topic model treats all instances of a topic equally, without considering context or speaker disposition.
- What evidence would resolve it: Developing and testing a topic model that incorporates contextual features (e.g., behavioral, linguistic, acoustic) to better differentiate between different meanings and implications of the same topic.

## Limitations

- Absence of direct Whisper transcription validation in depression speech domain, potentially affecting accuracy
- Unspecified BERTopic hyperparameters making exact reproduction challenging
- Small validation dataset (RAPID) limiting generalizability of findings
- Subjective topic interpretation based on keyword review rather than systematic validation

## Confidence

**High Confidence**: The methodological framework for integrating speech transcription, topic modeling, and behavioral validation is sound and well-established. The correlations between identified risk topics and objective behavioral markers (sleep variability, sleep onset, daily steps) are robust, given the strong prior literature on these associations.

**Medium Confidence**: The identification of specific depression-related topics and their association with PHQ-8 scores. While the statistical approach is valid, the subjective nature of topic interpretation and the absence of direct transcription validation reduce confidence in the specific topic labels.

**Low Confidence**: The longitudinal analysis of topic shifts and depression severity changes. The study mentions investigating these correlations but provides limited detail on the methodology and results, making it difficult to assess the reliability of these findings.

## Next Checks

1. **Whisper Transcription Accuracy Validation**: Manually transcribe a random sample of 50 speech recordings and compare them to Whisper outputs to quantify error rates, particularly for emotional or depressed speech patterns.

2. **BERTopic Stability Testing**: Run the topic modeling with different random seeds and preprocessing parameters to assess the stability and reproducibility of the identified topics across multiple iterations.

3. **Cross-Dataset Validation**: Apply the complete pipeline to an independent depression speech dataset with known PHQ-9/8 scores to verify that the same risk topics (No Expectations, Sleep, Mental Therapy, Haircut, Studying, Coursework) emerge consistently.