---
ver: rpa2
title: 'COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language
  Models'
arxiv_id: '2306.05659'
source_url: https://arxiv.org/abs/2306.05659
tags:
- attack
- cover
- query
- arxiv
- template
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COVER, a black-box adversarial attack framework
  targeting prompt-based learning in language models. The core idea is to exploit
  heuristic destruction rules at both character and word levels to corrupt manually
  crafted templates without altering the original input text.
---

# COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models

## Quick Facts
- arXiv ID: 2306.05659
- Source URL: https://arxiv.org/abs/2306.05659
- Reference count: 21
- Key outcome: Achieves up to 100% attack success rate with fewer queries than baselines on prompt-based learning

## Executive Summary
COVER introduces a black-box adversarial attack framework targeting prompt-based learning in language models by corrupting manually crafted templates. The framework exploits heuristic destruction rules at character and word levels while maintaining the original input text. Using a greedy strategy that prioritizes previously successful attacks through an ordered dictionary, COVER demonstrates high effectiveness across eight classification datasets with three BERT variants, achieving attack success rates up to 100% while requiring significantly fewer queries than baseline methods.

## Method Summary
COVER operates by applying predefined heuristic destruction rules to corrupt prompt templates without altering the original input text. These rules include character-level modifications (insertion, deletion, replacement) and word-level alterations (mask position changes, negative word additions). The framework maintains an ordered dictionary tracking successful attacks and uses this information to greedily prioritize template modifications for new inputs. The approach is evaluated in black-box settings where only model decisions are available, using 8-shot few-shot tuning across multiple BERT variants and classification datasets.

## Key Results
- Achieves attack success rates up to 100% on various classification datasets
- Requires significantly fewer queries than baseline methods (rocket-prompt and COVE)
- Maintains effectiveness across different shot counts, template lengths, and query budgets
- Demonstrates robustness when applied to three different BERT variants (BERT-base, RoBERTa-base, RoBERTa-large)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy prioritization of previously successful templates increases attack success rate
- Mechanism: COVER maintains an ordered dictionary tracking which templates have successfully misled the model, then prioritizes these templates for new inputs. This exploits the assumption that templates causing errors once are likely to do so again on similar inputs.
- Core assumption: Templates that successfully attack one input have transferable effectiveness to similar inputs
- Evidence anchors:
  - [abstract]: "the greedy strategy by maintaining an ordered dictionary of previously successful attacks to prioritize template modifications for new inputs"
  - [section]: "our intuition is that the targeted templates that have been successfully attacked are more likely to sabotage the remaining data successfully"
- Break condition: If the input space has high diversity or the model adapts to specific template patterns, this transfer assumption breaks down

### Mechanism 2
- Claim: Heuristic destruction rules at character and word levels effectively corrupt prompt templates
- Mechanism: COVER applies predefined rules like inserting spaces, swapping characters, moving mask positions, and adding negative words to templates while keeping original text intact
- Core assumption: PLMs are sensitive to specific template formatting changes but robust to original text preservation
- Evidence anchors:
  - [abstract]: "exploit heuristic destruction rules at both character and word levels to corrupt manually crafted templates"
  - [section]: "we design character-level and word-level heuristic destruction rules against the manual template, which act to corrupt the template before each model's prediction"
- Break condition: If the PLM uses template-agnostic reasoning or has strong template normalization layers

### Mechanism 3
- Claim: Black-box setting without gradient access still enables effective attacks
- Mechanism: COVER achieves high attack success rates (up to 100%) using only model decision outputs, not internal gradients or parameters
- Core assumption: Model decisions contain sufficient signal to guide effective template corruption
- Evidence anchors:
  - [abstract]: "robust across varying shot counts, template lengths, and query budgets"
  - [section]: "the only information we know is the decision of the model"
- Break condition: If the model provides minimal decision information or implements decision-level obfuscation

## Foundational Learning

- Concept: Template-based prompting mechanics in PLMs
  - Why needed here: Understanding how templates interact with <mask> tokens and verbalizers is crucial for designing effective corruption strategies
  - Quick check question: How does changing template structure affect the model's prediction at the <mask> position?

- Concept: Greedy algorithm optimization principles
  - Why needed here: The ordered dictionary approach relies on greedy selection of most successful templates
  - Quick check question: What guarantees does greedy optimization provide in adversarial template selection?

- Concept: Black-box adversarial attack methodology
  - Why needed here: This work operates entirely in black-box settings, requiring understanding of query-based attack strategies
  - Quick check question: How can you distinguish between successful and unsuccessful attacks when you only receive final decisions?

## Architecture Onboarding

- Component map:
  - Template corruption engine (character/word level rules)
  - Success tracking system (ordered dictionary)
  - Greedy selection module (k-top template retrieval)
  - Black-box query interface (model decision retrieval)
  - Template generation module (clean templates creation)

- Critical path: Input → Template corruption → Model query → Decision check → Success tracking → Next input selection
- Design tradeoffs:
  - Rule complexity vs. attack success rate
  - Dictionary size vs. query efficiency
  - k value selection vs. coverage vs. speed
  - Template preservation vs. corruption effectiveness

- Failure signatures:
  - High ASR but low Query efficiency indicates rule overuse
  - Low ASR across all templates suggests model robustness
  - Dictionary stagnation suggests template diversity issues

- First 3 experiments:
  1. Baseline comparison: Implement rocket-prompt baseline and measure ASR/Query on SST2 dataset
  2. Template length sensitivity: Test COVER across different template length ranges on IMDB dataset
  3. k-value optimization: Sweep k values (2, 4, 8, 16, 32) on Amazon-LB dataset and measure performance curves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying causes of the observed decrease in attack success rate as the number of shots increases for certain datasets?
- Basis in paper: [explicit] The paper notes that for most domains, the attack success rate (ASR) tends to decrease when the shot count increases, but this trend does not hold for the sentiment domain.
- Why unresolved: The paper conjectures that this might be due to the short sentence length of sentiment datasets, which leads to poor model robustness, but this is not thoroughly investigated.
- What evidence would resolve it: Systematic experiments varying sentence length and analyzing model robustness in relation to attack success rates across different domains.

### Open Question 2
- Question: How does the effectiveness of COVER compare to other potential adversarial attack strategies in black-box scenarios?
- Basis in paper: [inferred] The paper focuses on COVER's performance against two baseline methods (rocket-prompt and COVE) but does not compare it to other advanced adversarial attack strategies.
- Why unresolved: The paper does not explore other adversarial attack techniques that could potentially be more effective or efficient.
- What evidence would resolve it: Comparative experiments with other advanced adversarial attack methods in black-box scenarios.

### Open Question 3
- Question: What is the impact of template length on the attack success rate and efficiency across different domains and model architectures?
- Basis in paper: [explicit] The paper conducts experiments to study the effect of template length on attack performance, finding that COVER maintains good ASR and query efficiency across different template lengths.
- Why unresolved: While the paper shows that template length does not significantly affect COVER's performance, it does not explore the underlying reasons or compare these findings across different domains and model architectures.
- What evidence would resolve it: Detailed analysis of how template length impacts attack success rates and efficiency across various domains and model architectures, possibly including qualitative analysis of template structures.

## Limitations

- Limited generalizability to more complex tasks beyond classification
- Potential overfitting to BERT-based architectures
- Unknown effectiveness against models with stronger template normalization or robustness mechanisms

## Confidence

**High confidence**: The effectiveness of heuristic destruction rules at character and word levels. The mechanism is clearly specified, the rules are implementable, and the experimental results across multiple datasets and model variants support this claim consistently.

**Medium confidence**: The greedy strategy using ordered dictionaries. While the concept is sound and well-implemented, the assumption that previously successful templates transfer to new inputs needs more validation, particularly across more diverse datasets or with models that have different sensitivities to template formatting.

**Low confidence**: The scalability of COVER to more complex tasks or larger models. The experiments focus on classification with relatively simple templates, and the approach may face challenges with more sophisticated reasoning tasks or extremely large parameter models where template sensitivity differs.

## Next Checks

1. **Transferability validation**: Test COVER's ordered dictionary strategy across datasets with varying similarity levels (e.g., SST2 vs. Amazon-LB vs. unrelated domains) to quantify how well template success transfers between different input distributions.

2. **Template diversity analysis**: Systematically vary template length, complexity, and linguistic features to identify which template characteristics make them more vulnerable to COVER's attacks versus naturally robust templates.

3. **Model architecture sensitivity**: Evaluate COVER against models with different pretraining objectives (causal vs. masked language modeling) and architectures (CNNs, RNNs, or newer transformer variants) to determine if the heuristic destruction rules have universal effectiveness or are BERT-specific.