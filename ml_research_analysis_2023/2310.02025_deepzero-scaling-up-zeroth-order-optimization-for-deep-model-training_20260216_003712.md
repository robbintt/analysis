---
ver: rpa2
title: 'DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training'
arxiv_id: '2310.02025'
source_url: https://arxiv.org/abs/2310.02025
tags:
- training
- optimization
- deepzero
- gradient
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of scaling zeroth-order (ZO)
  optimization to deep neural network (DNN) training, where first-order gradient information
  is unavailable. The authors propose DeepZero, a framework that combines three innovations:
  (1) coordinate-wise gradient estimation (CGE) over randomized vector-wise methods,
  (2) sparsity-induced training via ZO-based model pruning, and (3) feature reuse
  and forward parallelization for practical implementation.'
---

# DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training

## Quick Facts
- arXiv ID: 2310.02025
- Source URL: https://arxiv.org/abs/2310.02025
- Reference count: 40
- Primary result: Achieves 86.94% accuracy on ResNet-20/CIFAR-10 using only zeroth-order optimization, approaching first-order training performance

## Executive Summary
DeepZero addresses the challenge of training deep neural networks when gradient information is unavailable by scaling zeroth-order optimization techniques. The framework introduces coordinate-wise gradient estimation (CGE) to replace inefficient vector-wise methods, implements sparsity-induced training through model pruning, and leverages feature reuse with forward parallelization for practical implementation. The approach achieves 86.94% accuracy on ResNet-20/CIFAR-10, marking the first time ZO methods approach first-order training performance on deep models. DeepZero also demonstrates practical applications in adversarial defense and PDE simulation, all while operating in black-box settings where computational graphs are inaccessible.

## Method Summary
DeepZero combines three core innovations: coordinate-wise gradient estimation (CGE) for efficient gradient approximation, sparsity-induced training via ZO-based model pruning (ZO-GraSP), and feature reuse with forward parallelization for scalable implementation. The framework perturbs parameters element-wise to estimate gradients, identifies compressible parameter patterns to reduce query complexity, and parallelizes forward evaluations while eliminating redundant computations. This enables training ResNet-20 on CIFAR-10 to 86.94% accuracy without backpropagation, demonstrating that zeroth-order methods can match first-order training performance for the first time.

## Key Results
- Achieves 86.94% accuracy on ResNet-20/CIFAR-10, approaching first-order training performance
- Demonstrates 10-20% better certified adversarial defense compared to baseline methods
- Reduces PDE simulation error in solver-in-the-loop settings compared to existing approaches
- Shows significant improvements in query efficiency through sparsity-induced training protocols

## Why This Works (Mechanism)

### Mechanism 1
Coordinate-wise gradient estimation (CGE) outperforms randomized vector-wise gradient estimation (RGE) in both accuracy and computational efficiency when scaling to deep model training. CGE perturbs each parameter element-wise, allowing feature reuse and forward parallelization, while RGE requires generating and integrating high-dimensional perturbation vectors. The accuracy benefit of CGE increases with model depth due to better gradient estimation quality.

### Mechanism 2
Sparsity is crucial for realizing model training via CGE with finite differences, and can be obtained "for free" by extending model pruning techniques to the ZO learning paradigm. The proposed ZO-GraSP method identifies sparse parameter patterns through ZO oracle queries, reducing the query complexity of CGE from O(d) to O(|SZO-GraSP|). Neural network weights have inherent compressibility that can be exploited through pruning-at-initialization techniques.

### Mechanism 3
The parallelization-fit property inherent to CGE-based ZO optimization enables scalable distributed implementation through forward parallelization and feature reuse. CGE's alignment of parameter perturbations with forward passes allows decomposing the gradient estimation across multiple processes, while feature reuse eliminates redundant computations. The computational graph of the neural network can be effectively partitioned for parallel processing without significant synchronization overhead.

## Foundational Learning

- Concept: Zeroth-order optimization and gradient estimation techniques
  - Why needed here: DeepZero relies on finite differences to estimate gradients without explicit gradient information, making understanding ZO optimization fundamental to implementing the framework
  - Quick check question: How does coordinate-wise gradient estimation differ from randomized vector-wise gradient estimation in terms of query complexity and accuracy?

- Concept: Neural network pruning and model compression techniques
  - Why needed here: The sparsity-induced training protocol in DeepZero extends pruning-at-initialization methods to identify compressible parameter patterns that reduce query complexity
  - Quick check question: What is the key difference between weight sparsity and gradient sparsity in the context of ZO training?

- Concept: Parallel computing and distributed systems
  - Why needed here: Forward parallelization and feature reuse are essential for scaling DeepZero to larger models, requiring understanding of how to partition computational graphs and manage process synchronization
  - Quick check question: How does forward parallelization in DeepZero differ from conventional data parallelization used in first-order distributed training?

## Architecture Onboarding

- Component map:
  - ZO gradient estimation module (coordinate-wise perturbations)
  - Model pruning component (ZO-GraSP for sparsity pattern identification)
  - Feature reuse and forward parallelization engine
  - Sparse-CGE implementation for gradient estimation with dynamic sparsity patterns
  - Integration layer for connecting components and managing training loop

- Critical path:
  1. Initialize model and perform ZO-GraSP to identify layer-wise pruning ratios
  2. Generate dynamic sparsity patterns based on pruning ratios
  3. Perform parallelized forward evaluations with feature reuse
  4. Estimate sparse gradients using (Sparse-CGE)
  5. Update model weights and synchronize across processes
  6. Repeat steps 2-5 for each epoch

- Design tradeoffs:
  - Query efficiency vs. training accuracy: Higher sparsity reduces queries but may degrade performance
  - Parallelization granularity vs. synchronization overhead: More processes reduce time but increase communication costs
  - Feature reuse depth vs. memory consumption: Deeper reuse saves computation but requires more memory

- Failure signatures:
  - Training accuracy plateaus below expected levels: Indicates suboptimal sparsity patterns or insufficient query budget
  - Training time increases unexpectedly: Suggests parallelization inefficiency or feature reuse conflicts
  - Model divergence during training: Points to incorrect gradient estimation or poor learning rate selection

- First 3 experiments:
  1. Verify CGE vs RGE performance on a small CNN (as in Figure 2) to confirm accuracy and efficiency advantages
  2. Test ZO-GraSP pruning on a randomly initialized ResNet-20 to validate sparsity pattern identification
  3. Evaluate feature reuse impact by comparing training time with and without reuse on a ResNet-20 model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DeepZero's performance scale to larger neural network architectures (e.g., ResNet-50 or larger) beyond ResNet-20?
- Basis in paper: The paper states that while DeepZero achieves state-of-the-art accuracy on ResNet-20/CIFAR-10, "scalability remains a significant challenge when dealing with even larger models and more extensive datasets."
- Why unresolved: The experiments only tested ResNet-20. The authors explicitly acknowledge that further research is needed to scale to larger models.
- What evidence would resolve it: Empirical results showing training accuracy, convergence speed, and query efficiency when applying DeepZero to architectures like ResNet-50, VGG-16, or transformers on larger datasets.

### Open Question 2
- Question: What is the theoretical limit of gradient sparsity that DeepZero can exploit before performance degradation becomes prohibitive?
- Basis in paper: The paper mentions "increased sparsity leads to a reduced number of function queries" but also notes "as p increases, the convergence error of the proposed approach also grows," suggesting a tradeoff exists.
- Why unresolved: The experiments only tested sparsity levels up to 99%. The authors provide a theoretical convergence rate but don't empirically determine the practical limits of sparsity for different model architectures.
- What evidence would resolve it: Systematic experiments varying sparsity ratios (e.g., 80%-99.9%) across different network depths and architectures to identify the point where accuracy drops below acceptable thresholds.

### Open Question 3
- Question: Can DeepZero's forward parallelization be effectively combined with existing data parallelization techniques to achieve multiplicative speedups?
- Basis in paper: The paper states that forward parallelization is "different from the conventional data parallelization" and avoids certain limitations, but doesn't explore hybrid approaches.
- Why unresolved: The paper demonstrates benefits of forward parallelization but leaves open whether combining it with data parallelization could provide additional efficiency gains without introducing the local minima issues mentioned.
- What evidence would resolve it: Empirical results comparing training speed and accuracy when combining forward parallelization with data parallelization at various batch sizes, particularly on larger models and datasets.

## Limitations
- Scalability to larger models remains uncertain, as experiments were limited to ResNet-20
- Computational overhead of ZO-GraSP pruning and its impact on overall training efficiency are not fully characterized
- Limited validation across diverse architectures and datasets beyond ResNet-20/CIFAR-10

## Confidence
- CGE vs RGE performance: Medium - Strong empirical evidence on ResNet-20, but limited cross-architecture validation
- Sparsity benefits: Medium - Theoretical justification is sound, but real-world sparsity patterns may vary
- Parallelization efficiency: Low - The paper claims significant speedups, but implementation details are sparse

## Next Checks
1. Benchmark DeepZero on a transformer-based model (e.g., ViT) to assess cross-architecture performance
2. Measure the end-to-end training time of DeepZero vs first-order training on the same hardware to quantify practical overhead
3. Test the robustness of ZO-GraSP pruning across multiple random initializations to ensure stability of identified sparsity patterns