---
ver: rpa2
title: Sequential Planning in Large Partially Observable Environments guided by LLMs
arxiv_id: '2312.07368'
source_url: https://arxiv.org/abs/2312.07368
tags:
- state
- action
- space
- environment
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid agent, "neoplanner," that combines
  state space search with queries to a foundational large language model (LLM) to
  generate effective action plans in large partially observable environments. The
  agent builds a state space model of the environment, assigning values to states
  augmented with exploration terms.
---

# Sequential Planning in Large Partially Observable Environments guided by LLMs

## Quick Facts
- arXiv ID: 2312.07368
- Source URL: https://arxiv.org/abs/2312.07368
- Authors: 
- Reference count: 14
- Key outcome: Achieves 124% improvement in average reward over current best method in Scienceworld environment

## Executive Summary
This paper introduces NeoPlanner, a hybrid agent that combines state space search with large language model (LLM) queries to solve sequential planning problems in large partially observable environments. The agent builds a state space graph model where nodes store values updated via temporal difference learning, augmented with exploration terms from UCB1. When exploration is needed, NeoPlanner queries an LLM to generate action plans rather than using random exploration. Learnings from each trial are stored as entity relationships and used to improve future LLM queries, creating a continual learning cycle.

## Method Summary
NeoPlanner operates by constructing a state space graph where each node represents an environment state with associated value V(s) and visit count n_s. The agent uses UCB1 to balance exploration and exploitation, selecting actions that maximize V(s) + C√(ln N / n_s). When encountering leaf nodes or requiring exploration, the agent queries an LLM with the current state, objective, and memory of learned entity relationships to generate action plans. After executing plans, the agent updates the state space graph using TD(0) learning and processes the episode traces through a learner component to extract new entity relationships for future memory.

## Key Results
- Achieves 124% improvement in average reward over current best method across multiple tasks
- Demonstrates effectiveness in Scienceworld environment with large state and action spaces
- Shows continual improvement through memory-based bootstrapping across episodes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The state space graph exploration guided by UCB1 balances exploitation and exploration effectively.
- Mechanism: Each state node stores value V(s) updated via TD(0), augmented with exploration term C√(ln N / n_s). Actions maximizing this upper confidence bound are selected, ensuring both known-reward exploitation and unexplored-state exploration.
- Core assumption: Rewards are sparse but informative enough for TD learning to propagate values back through the graph.
- Evidence anchors:
  - [abstract] "A balance of exploration and exploitation is maintained by maximizing upper confidence bounds of values of states."
  - [section] Equation (1): V⊕(s) = V(s) + C√(ln N / n_s)
  - [corpus] No direct neighbor support for UCB1/POMDP hybrid; weak corpus evidence.
- Break condition: If reward signals are too sparse or non-Markovian, TD updates may fail to propagate meaningful values, making exploration dominate uselessly.

### Mechanism 2
- Claim: LLM-generated action plans reduce combinatorial explosion during exploration.
- Mechanism: When no informative state space information exists (leaf node or K_s * C√(ln N / n_s') > V_explore⊕(s)), the agent queries LLM with prompt including current state, objective, and actions-to-avoid list. LLM outputs a sequence of actions in one shot, reducing the number of LLM calls versus step-by-step generation.
- Core assumption: Natural language encoding of states/actions is sufficiently expressive for LLM to infer useful plans.
- Evidence anchors:
  - [abstract] "In places where random exploration is needed, the LLM is queried to generate an action plan."
  - [section] "Instead of random selection of actions for exploration, the LLM can provide a better educated guess on the actions required to reach the goal state, thereby damping the search space."
  - [corpus] Weak support; no neighbor papers mention LLM-guided action plan generation in POMDPs.
- Break condition: If state/action descriptions are ambiguous or the LLM cannot reason deeply, generated plans may be irrelevant or lead to dead ends.

### Mechanism 3
- Claim: Continuous text-based memory of entity relationships bootstraps future LLM performance.
- Mechanism: After each episode, the learner LLM processes action-observation traces plus feedback to produce relational triples "X Y Z". These are stored and prepended to future prompts, biasing the LLM toward learned correlations.
- Core assumption: The same relational memory format works across tasks without heavy task-specific tuning.
- Evidence anchors:
  - [abstract] "Learnings from each trial are stored as entity relationships in text format. Those are used in future queries to the LLM for continual improvement."
  - [section] Learner prompt schema: "[ <list of learnings. do not write redundant or contradicting statements> ]"
  - [corpus] No direct neighbor evidence; weak support.
- Break condition: If memory grows stale or contains contradictory facts from previous tasks, LLM outputs may degrade.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The environment has latent states derived from observations; POMDP formalism models this uncertainty.
  - Quick check question: What are the components of a POMDP tuple and why is the observation function necessary here?

- Concept: Upper Confidence Bound (UCB1) algorithm
  - Why needed here: UCB1's exploration-exploitation trade-off is applied to state values in the graph to avoid getting stuck in local optima.
  - Quick check question: How does the term C√(ln N / n_s) encourage exploration of rarely visited states?

- Concept: Temporal Difference (TD) learning
  - Why needed here: TD(0) updates propagate reward information back through the state space graph incrementally as episodes progress.
  - Quick check question: What is the update rule V(s) ← V(s) + α[r + γV(s') - V(s)] and how does it approximate future rewards?

## Architecture Onboarding

- Component map: Main loop -> selectplan() -> generateactionplan() -> executeactionplan() -> updategraph() -> learner()

- Critical path:
  1. selectplan() traverses graph until leaf or need exploration
  2. generateactionplan() queries LLM with current state + memory
  3. executeactionplan() runs actions, logs trace
  4. updatestatespacegraph() updates nodes/edges and values via TD
  5. learner() extracts new relational triples into memory

- Design tradeoffs:
  - UCB exploration constant C vs depth of search: high C → more random LLM calls, low C → premature convergence
  - Number of TD iterations i vs convergence: more iterations → more stable values but slower updates
  - Memory size: larger memory → better bootstrapping but higher LLM context costs
  - LLM prompt size: include full trace vs compressed summary: trade relevance vs cost

- Failure signatures:
  - Graph grows huge with low-reward nodes: indicates poor reward propagation
  - LLM returns invalid actions repeatedly: prompt or memory may be malformed
  - Agent cycles through same states: exploration term not strong enough or graph pruning needed
  - Memory contradictions: learner not filtering outdated relations

- First 3 experiments:
  1. Run with C=0 (pure exploitation) and observe if agent gets stuck quickly
  2. Disable LLM calls (force random exploration) and measure plan length vs reward
  3. Remove learner memory updates and see if performance degrades across episodes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NeoPlanner scale with increasing environment complexity, particularly in terms of state and action space size?
- Basis in paper: [inferred] The paper mentions that ScienceWorld has a huge number of possible actions and objects, making it a large state space. However, it doesn't provide experiments varying the complexity of the environment.
- Why unresolved: The paper only tests NeoPlanner on a fixed set of tasks in ScienceWorld. There's no systematic evaluation of how performance changes as the environment grows more complex.
- What evidence would resolve it: Experiments comparing NeoPlanner's performance across multiple environments of increasing size and complexity, with metrics like average reward and number of interactions needed.

### Open Question 2
- Question: What is the optimal balance between exploration and exploitation in NeoPlanner, and how does it vary across different task types?
- Basis in paper: [explicit] The paper mentions that NeoPlanner uses UCB1 to balance exploration and exploitation, but doesn't provide a detailed analysis of how this balance affects performance.
- Why unresolved: While the paper mentions the use of UCB1, it doesn't provide a thorough analysis of how the exploration-exploitation trade-off impacts performance across different tasks or environments.
- What evidence would resolve it: A systematic study varying the exploration-exploitation parameters (e.g., the constant C in UCB1) and measuring their impact on performance across different task types.

### Open Question 3
- Question: How does the quality and quantity of learned entity relationships affect the performance of NeoPlanner over time?
- Basis in paper: [explicit] The paper mentions that NeoPlanner builds a memory of learnings about the environment in the form of entity relationships, which are used in future LLM queries. However, it doesn't provide a detailed analysis of how this memory affects performance.
- Why unresolved: While the paper mentions the use of learned entity relationships, it doesn't provide a detailed analysis of how the quality and quantity of these relationships impact performance over time or across tasks.
- What evidence would resolve it: Experiments measuring the performance of NeoPlanner with varying amounts and qualities of learned entity relationships, and analyzing how these relationships evolve over time and across tasks.

## Limitations
- Heavy reliance on LLM quality without detailed analysis of failure cases when LLM produces suboptimal plans
- Memory-based bootstrapping mechanism not validated across diverse task domains beyond ScienceWorld
- Potential exponential growth of state space graph without clear pruning strategies for scalability

## Confidence
- High confidence: The core hybrid architecture combining state space search with LLM guidance is technically sound and the experimental improvement over baseline methods is clearly demonstrated with specific metrics (124% improvement in average reward)
- Medium confidence: The effectiveness of the UCB1-guided exploration in balancing exploitation and exploration within the state space graph is well-supported by the mathematical formulation, though the interaction with LLM-based exploration could benefit from more detailed analysis
- Low confidence: The generalizability of the memory-based bootstrapping mechanism across diverse task domains is claimed but not empirically validated beyond the ScienceWorld environment. The scalability of the approach to environments with significantly larger state and action spaces remains unproven

## Next Checks
1. Test the agent's performance when the LLM is replaced with a rule-based planner to isolate the contribution of LLM-guided exploration versus the state space search mechanism
2. Implement an ablation study removing the memory component to quantify its contribution to performance improvement across multiple episodes and tasks
3. Evaluate the agent on a new partially observable environment with different state and action representations to test the generalizability of the memory-based bootstrapping mechanism