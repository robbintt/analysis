---
ver: rpa2
title: 'Enhancing ML model accuracy for Digital VLSI circuits using diffusion models:
  A study on synthetic data generation'
arxiv_id: '2310.10691'
source_url: https://arxiv.org/abs/2310.10691
tags:
- data
- diffusion
- delay
- design
- gate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of diffusion models to generate synthetic
  data for enhancing the accuracy of machine learning models in VLSI circuit design
  tasks. The authors propose using a denoising diffusion probabilistic model to generate
  synthetic data for delay estimation in 22nm CMOS digital cells.
---

# Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation

## Quick Facts
- arXiv ID: 2310.10691
- Source URL: https://arxiv.org/abs/2310.10691
- Reference count: 28
- Primary result: Diffusion models generate high-quality synthetic circuit data that improves ML model accuracy for delay estimation

## Executive Summary
This paper explores the use of diffusion models to generate synthetic data for enhancing the accuracy of machine learning models in VLSI circuit design tasks. The authors propose using a denoising diffusion probabilistic model to generate synthetic data for delay estimation in 22nm CMOS digital cells. The model is trained on real data obtained from HSPICE simulations and evaluated using mean absolute percentage error (MAPE) compared to the simulator outputs. The results show low MAPE values across various circuit datasets, indicating close resemblance between the generated and original data distributions. Additionally, the authors demonstrate that using the generated synthetic data improves the performance of a gradient boosting regression model for predicting CMOS NOT gate delays, with significant improvements in R2 score and reductions in MSE, RMSE, MAE, and MAPE.

## Method Summary
The authors employ a denoising diffusion probabilistic model (DDPM) with a custom encoder-decoder architecture to generate synthetic circuit data for delay estimation in 22nm CMOS digital cells. The model is trained on real data from HSPICE simulations, with the forward diffusion process adding noise using a linearly increasing variance schedule (βt from 0.001 to 0.02). The reverse denoising process uses an encoder-decoder network with Leaky ReLU activation and batch normalization. The generated synthetic data is evaluated using MAPE against HSPICE outputs and used to augment training data for gradient boosting regression models to predict circuit delays.

## Key Results
- Diffusion models generate synthetic circuit data with low MAPE values compared to HSPICE simulations, indicating close resemblance to real data distributions.
- Using generated synthetic data improves gradient boosting regression model performance for CMOS NOT gate delay prediction, with significant improvements in R2 score and reductions in MSE, RMSE, MAE, and MAPE.
- A simpler encoder-decoder architecture is sufficient for the reverse denoising process in this circuit data context, as opposed to more complex UNET architectures used in image generation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models can generate high-quality synthetic circuit data that closely matches real HSPICE-simulated data.
- Mechanism: The forward diffusion process incrementally adds noise to real circuit data using a Markov chain. The reverse process learns to denoise via an encoder-decoder neural network, reconstructing data that statistically resembles the original.
- Core assumption: Circuit data is continuous and not as complex as images, allowing a simpler encoder-decoder architecture to learn the denoising function effectively.
- Evidence anchors:
  - [abstract] "Our results demonstrate the close resemblance of synthetic data using diffusion model to real data."
  - [section] "Diffusion models have two processes to follow: Forward Process... Reverse process – This phase involves predicting the noise added to each data point during the forward process."
  - [corpus] No direct citation for diffusion on circuit data; assumption based on image domain success (Dhariwal & Nichol 2021) and study context.
- Break condition: If the synthetic data's MAPE with respect to HSPICE exceeds a practical threshold (e.g., >10%) or if density plots diverge significantly from original data.

### Mechanism 2
- Claim: Synthetic data augmentation improves the performance of downstream ML models for circuit delay prediction.
- Mechanism: Generated synthetic samples expand the training set size and diversity, allowing gradient boosting regression models to better generalize across process, voltage, and temperature variations.
- Core assumption: The synthetic data preserves the statistical relationships between process parameters and delay outputs present in the real data.
- Evidence anchors:
  - [abstract] "using the generated synthetic data improves the performance of a gradient boosting regression model... with significant improvements in R2 score and reductions in MSE, RMSE, MAE, and MAPE."
  - [section] "Table 4 shows a significant improvement in a gradient-boosting regression (GBR) model using artificial data, to predict CMOS NOT gate delays."
  - [corpus] No direct citation for this specific augmentation benefit; assumption based on general ML data augmentation literature.
- Break condition: If the improvement in model metrics is negligible or if the model overfits to synthetic patterns not present in real circuits.

### Mechanism 3
- Claim: A simpler encoder-decoder architecture is sufficient for diffusion model reverse denoising in this circuit data context.
- Mechanism: Unlike image generation requiring UNETs with residual connections, the continuous circuit data's lower complexity allows effective denoising with a basic encoder-decoder, reducing computational cost.
- Core assumption: The data modality (continuous circuit parameters) is less complex than images, so the denoising task does not require deep residual architectures.
- Evidence anchors:
  - [section] "Since our target dataset is relatively less complex than images, we propose a simple encoder-decoder architecture [27] instead of a UNET for reverse denoising process."
  - [abstract] No direct statement on architecture choice impact; inferred from comparison to image models.
  - [corpus] No direct citation for encoder-decoder sufficiency on circuit data; assumption based on stated design choice.
- Break condition: If MAPE remains high or training becomes unstable with the simpler architecture, indicating the denoising task's complexity exceeds the model's capacity.

## Foundational Learning

- Concept: Diffusion models (DDPM)
  - Why needed here: To generate synthetic data that captures the statistical distribution of real circuit simulation data.
  - Quick check question: What are the two main processes in a diffusion model, and what does each accomplish?

- Concept: Mean Absolute Percentage Error (MAPE)
  - Why needed here: To quantitatively compare the similarity between synthetic data outputs and real HSPICE simulation results.
  - Quick check question: How is MAPE calculated, and why is it useful for evaluating synthetic circuit data?

- Concept: Gradient Boosting Regression (GBR)
  - Why needed here: To demonstrate that synthetic data improves predictive accuracy for circuit delay estimation tasks.
  - Quick check question: What are the key hyperparameters of a GBR model that might be tuned after data augmentation?

## Architecture Onboarding

- Component map: Forward diffusion process (adds noise) -> Reverse denoising model (encoder-decoder) -> Evaluation pipeline (MAPE vs HSPICE, GBR training)
- Critical path: Generate noisy data -> train reverse denoising model -> generate synthetic samples -> evaluate MAPE vs HSPICE -> train GBR on real+synthetic data -> compare performance metrics
- Design tradeoffs: Simpler encoder-decoder vs. UNET (computation vs. potential quality), variance schedule βt (noise injection vs. data fidelity), number of diffusion steps T (generation quality vs. speed)
- Failure signatures: High MAPE indicating poor data resemblance, GBR performance not improving with augmentation, training instability in reverse model
- First 3 experiments:
  1. Train reverse model on NOT gate dataset with varying βt schedules; evaluate MAPE vs HSPICE.
  2. Generate synthetic samples for all datasets; compare density plots to real data.
  3. Train GBR on real data only vs. real+synthetic data; measure improvement in R², MSE, RMSE, MAE, MAPE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic data generated by diffusion models compare to GANs or VAEs for other types of VLSI circuits beyond the twelve digital cells studied?
- Basis in paper: [explicit] The authors mention that for brevity, data generation with other generative models such as GANs or VAEs were not as effective, and hence not shown in results.
- Why unresolved: The paper does not provide a direct comparison of diffusion models with GANs or VAEs for the specific VLSI circuit datasets studied. It only states that other models were not as effective, but does not elaborate on the reasons or provide quantitative comparisons.
- What evidence would resolve it: A comprehensive study comparing the performance of diffusion models, GANs, and VAEs on a diverse set of VLSI circuit datasets, including the twelve digital cells studied in this paper, would provide insights into the relative effectiveness of each approach.

### Open Question 2
- Question: How sensitive is the performance of the diffusion model to variations in the variance schedule βt used in the forward process?
- Basis in paper: [explicit] The authors mention that they adopt a variance of βt, transitioning linearly from 0.001 to 0.02, following the approach by Ho et al. [4]. However, they do not explore the impact of different variance schedules on the quality of the generated synthetic data.
- Why unresolved: The paper does not provide a systematic investigation of how different variance schedules affect the performance of the diffusion model. It only mentions the specific variance schedule used in their experiments.
- What evidence would resolve it: Conducting experiments with different variance schedules, such as non-linear schedules or schedules with different ranges, and evaluating the impact on the quality of the generated synthetic data would provide insights into the sensitivity of the diffusion model to the variance schedule.

### Open Question 3
- Question: How does the size of the training dataset impact the quality of the generated synthetic data and the performance of the subsequent ML models?
- Basis in paper: [inferred] The authors mention that the diffusion model is trained using just 500 real data samples. However, they do not explore the impact of using larger or smaller training datasets on the quality of the generated synthetic data and the performance of the ML models.
- Why unresolved: The paper does not provide a systematic investigation of how the size of the training dataset affects the performance of the diffusion model and the subsequent ML models. It only mentions the specific training dataset size used in their experiments.
- What evidence would resolve it: Conducting experiments with different sizes of training datasets, such as 100, 500, 1000, and 5000 samples, and evaluating the impact on the quality of the generated synthetic data and the performance of the ML models would provide insights into the relationship between training dataset size and model performance.

## Limitations
- No quantitative threshold is provided for acceptable MAPE values, making it unclear if the reported improvements are practically significant.
- The encoder-decoder architecture details are minimal (only layers and learning rate specified), which may hinder exact reproduction of the results.
- The improvement in GBR performance is attributed to synthetic data augmentation, but no comparison is made to other augmentation techniques or baseline models trained on real data only.

## Confidence
- High confidence: Diffusion models can generate synthetic circuit data that statistically resembles real HSPICE-simulated data (supported by low MAPE and density plot comparisons).
- Medium confidence: Synthetic data augmentation improves GBR model performance for delay prediction (improvement metrics are shown but no ablation or baseline comparison).
- Low confidence: The simpler encoder-decoder architecture is sufficient for this task (no comparative analysis with UNET or other architectures provided).

## Next Checks
1. Replicate the diffusion model on a held-out test set and measure MAPE vs HSPICE; compare density plots and per-feature MAPE distributions to validate data resemblance.
2. Train GBR models with and without synthetic data augmentation; perform ablation by comparing to models trained on real data only and to models using other augmentation techniques (e.g., Gaussian noise, SMOTE).
3. Conduct hyperparameter sensitivity analysis for βt schedule, number of layers, and learning rate; evaluate model performance across different PVT corners and circuit types to test generalizability.