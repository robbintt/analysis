---
ver: rpa2
title: 'Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory'
arxiv_id: '2305.02437'
source_url: https://arxiv.org/abs/2305.02437
tags:
- memory
- generation
- linguistics
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of bounded memory in retrieval-augmented
  text generation, where the memory is retrieved from a fixed corpus and limited by
  its quality. The authors propose Selfmem, a framework that iteratively uses a retrieval-augmented
  generator to create an unbounded memory pool and a memory selector to choose one
  output as memory for the next generation round.
---

# Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory

## Quick Facts
- arXiv ID: 2305.02437
- Source URL: https://arxiv.org/abs/2305.02437
- Reference count: 40
- Primary result: Achieves state-of-the-art results on JRC-Acquis (4 directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1)

## Executive Summary
This paper introduces Selfmem, a framework addressing bounded memory limitations in retrieval-augmented text generation by iteratively using a generator's own output as memory. The approach leverages a retrieval-augmented generator to create an unbounded memory pool and a memory selector to choose high-quality candidates for subsequent generation rounds. The framework achieves state-of-the-art results across three text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation. Through systematic analysis, the authors identify the mechanisms enabling self-memory effectiveness and provide insights for future research in retrieval-augmented generation.

## Method Summary
Selfmem operates through a two-stage training process: first training a retrieval-augmented generator, then training a memory selector on generated candidates. The generator takes source text and retrieved memory as input, producing a candidate pool (beam size 50) using a Transformer-based architecture. The memory selector, implemented with XLM-Rbase for translation or RoBERTa base for other tasks, selects one candidate based on metrics like BLEU or ROUGE using KL divergence loss. The framework iteratively refines generation by using self-generated memory, with retrieval performed via BM25 over the training set datastore. Training uses NLL loss for the generator and KL divergence for the memory selector, with iterative selection and regeneration until convergence.

## Key Results
- Achieves SOTA on JRC-Acquis (4 translation directions) with significant BLEU improvements
- Sets new records on XSum summarization (50.3 ROUGE-1) and BigPatent (62.9 ROUGE-1)
- Demonstrates consistent performance gains across neural machine translation, abstractive summarization, and dialogue generation tasks

## Why This Works (Mechanism)

### Mechanism 1
The retrieval-augmented generator can already distinguish between "good" and "bad" memory without additional training, allowing self-memory to be effective. The generator trained on retrieved memory learns to leverage memory information effectively, as demonstrated by consistent performance improvements when using reference or random memory in experiments. Core assumption: The generator's learned ability to use memory is robust across different memory distributions.

### Mechanism 2
Self-memory is more similar in distribution to inference data than training data, making it more effective as memory. The model's own output, which is more similar to inference data, serves as better memory than the training data, leading to improved generation quality. Core assumption: The distribution of the model's output is closer to the inference data distribution than the training data distribution.

### Mechanism 3
The memory selector trained on model-free metrics can effectively select self-memory that improves generation quality. The memory selector uses metrics like BLEU or ROUGE to select candidates from the generator's output, which then serve as memory for the next generation round, leading to iterative improvement. Core assumption: Model-free metrics like BLEU or ROUGE are effective at identifying high-quality candidates for use as memory.

## Foundational Learning

- **Retrieval-augmented generation**: Understanding how memory is retrieved and used in the generation process is crucial for grasping the Selfmem framework. Quick check: How does the retrieval-augmented generation paradigm differ from traditional sequence-to-sequence models?
- **Memory selection and ranking**: The effectiveness of Selfmem relies on the ability to select high-quality candidates from the generator's output to serve as memory. Quick check: What are the advantages and disadvantages of using model-free metrics like BLEU or ROUGE for memory selection?
- **Iterative improvement and self-learning**: Selfmem relies on the idea of iterative improvement, where the model uses its own output as memory to improve future generations. Quick check: How does the iterative process in Selfmem contribute to the overall improvement in generation quality?

## Architecture Onboarding

- **Component map**: Retriever → Generator → Memory Selector → Memory → Generator (next round)
- **Critical path**: Retriever retrieves memory from datastore → Generator creates candidates → Memory selector chooses memory → Memory used in next generation round
- **Design tradeoffs**: Using self-memory vs. retrieved memory (self-memory may be more similar to inference data but could introduce biases); Model-free vs. model-based metrics for memory selection (simpler but may miss nuanced quality aspects)
- **Failure signatures**: Generator fails to leverage memory effectively; Memory selector fails to select high-quality candidates; Distribution of model's output not closer to inference data distribution
- **First 3 experiments**: 1) Evaluate generator's ability to use different types of memory (reference, random, retrieved) with fixed parameters; 2) Compare distribution of model's output to training data and inference data distributions; 3) Test effectiveness of different metrics (BLEU, ROUGE, etc.) for memory selection

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several areas for future research emerge from the work, including comprehensive comparisons of memory selection metrics across tasks, analysis of performance scaling with candidate pool size and iteration count, and evaluation of different backbone architectures beyond Transformer-based models.

## Limitations

- Performance on tasks beyond the three tested domains (translation, summarization, dialogue) remains unverified
- Computational overhead from multiple generation rounds and separate memory selector not fully characterized
- Distributional assumptions about self-memory effectiveness not rigorously proven across all domains

## Confidence

- **High Confidence**: Empirical results showing consistent improvements across all three tested tasks
- **Medium Confidence**: Mechanism explanations for why self-memory works, particularly distributional alignment hypothesis
- **Low Confidence**: Framework's performance on tasks beyond those tested and generalizability to other domains

## Next Checks

1. Implement Selfmem on a fourth generation task (e.g., long-form question answering or code generation) to assess cross-task generalization
2. Replace model-free metric-based memory selector with learned model-based selector trained on human judgments and compare performance
3. Conduct systematic analysis comparing distributional properties of training data, retrieved memory, and self-generated memory across multiple domains