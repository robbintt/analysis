---
ver: rpa2
title: 'CITB: A Benchmark for Continual Instruction Tuning'
arxiv_id: '2310.14510'
source_url: https://arxiv.org/abs/2310.14510
tags:
- tasks
- task
- learning
- methods
- stream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new benchmark for Continual Instruction Tuning
  (CIT) to study how models can continually learn new tasks without forgetting previous
  knowledge while leveraging rich natural language instructions. The benchmark includes
  two long task streams - InstrDialog with 19 dialogue tasks and InstrDialog++ with
  38 diverse tasks spanning 18 categories.
---

# CITB: A Benchmark for Continual Instruction Tuning

## Quick Facts
- arXiv ID: 2310.14510
- Source URL: https://arxiv.org/abs/2310.14510
- Reference count: 40
- The paper proposes CITB benchmark and shows existing CL methods don't effectively leverage instructions, while sequential fine-tuning of instruction-tuned models achieves competitive performance

## Executive Summary
This paper introduces CITB, a new benchmark for Continual Instruction Tuning (CIT) that studies how models can learn new tasks sequentially without forgetting previous knowledge while leveraging rich natural language instructions. The benchmark includes two long task streams: InstrDialog with 19 dialogue tasks and InstrDialog++ with 38 diverse tasks spanning 18 categories. Experiments reveal that conventional continual learning methods fail to effectively utilize instructions, while directly fine-tuning an instruction-tuned model sequentially yields competitive or better results. The findings suggest that rich instructions enable knowledge transfer and reduce forgetting, but current methods don't fully exploit these benefits.

## Method Summary
The CIT benchmark uses a two-stage training process: first, a base model is fine-tuned on 100 random tasks to create an instruction-tuned model (finit); second, this model is sequentially fine-tuned on task streams (InstrDialog or InstrDialog++) using various continual learning methods including L2, EWC, AGEM, Replay, and AdapterCL, along with instruction-based baselines (FT-init, FT-no-init). All tasks are transformed into text-to-text format using instruction templates containing task definitions and examples. Performance is evaluated on three test sets (sequential, initial, and unseen tasks) using ROUGE-L and CL-specific metrics (AR, FWT, BWT, FR).

## Key Results
- Existing continual learning methods (L2, EWC, AGEM, Replay, AdapterCL) perform similarly to or worse than naive fine-tuning baselines
- Fine-tuning an instruction-tuned model sequentially (FT-init) achieves competitive performance without specialized CL methods
- Task diversity in learning streams improves performance, especially for knowledge transfer metrics
- Rich natural language instructions enable knowledge transfer and reduce forgetting, but current CL methods don't fully exploit these benefits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rich natural language instructions enable knowledge transfer and reduce forgetting in CIT.
- **Mechanism:** Instructions containing task definitions and in-context examples help models learn task patterns rather than memorizing specific inputs, enabling generalization across similar tasks.
- **Core assumption:** Instructions encode sufficient task semantics that models can extract reusable patterns.
- **Evidence anchors:** [abstract] Existing CL methods don't effectively leverage rich instructions; [section 6.3] Remarkable performance of FT-init/no-init comes from rich instructions.
- **Break condition:** If instructions become too task-specific or lack semantic depth, models revert to pattern matching without understanding.

### Mechanism 2
- **Claim:** Task diversity in learning streams benefits continual instruction tuning.
- **Mechanism:** Learning diverse task types creates overlapping instruction patterns that reinforce general instruction-following capabilities.
- **Core assumption:** Diverse tasks share enough instruction structure that learning one type helps with others.
- **Evidence anchors:** [section 6.2] Improved performance with InstrDialog++'s diverse tasks; [section 6.3] Conventional CL methods don't fully leverage instructions.
- **Break condition:** If task stream becomes too homogeneous, models overfit to specific instruction patterns without developing general capabilities.

### Mechanism 3
- **Claim:** Initial multi-task instruction tuning creates a strong foundation that reduces forgetting.
- **Mechanism:** First learning to follow instructions across many tasks develops robust instruction-understanding capabilities that persist during sequential fine-tuning.
- **Core assumption:** Initial instruction tuning establishes parameter configurations less susceptible to overwriting.
- **Evidence anchors:** [abstract] Sequential fine-tuning of instruction-tuned models yields similar or better results; [section 5.2] Two-stage process with initial multi-task instruction tuning.
- **Break condition:** If initial instruction tuning is insufficient or subsequent task stream is too long, the foundation may degrade.

## Foundational Learning

- **Concept:** Catastrophic forgetting in neural networks
  - **Why needed here:** Understanding why models forget previous tasks is fundamental to designing CIT methods
  - **Quick check question:** What causes parameters learned for task A to be overwritten when training on task B?

- **Concept:** Zero-shot generalization in instruction-tuned models
  - **Why needed here:** CIT evaluation includes performance on unseen tasks, requiring understanding of how instruction tuning enables generalization
  - **Quick check question:** How does instruction tuning enable models to perform well on tasks they weren't explicitly trained on?

- **Concept:** Text-to-text format transformation
  - **Why needed here:** All tasks in CIT are transformed into unified text-to-text format, which is crucial for understanding the benchmark design
  - **Quick check question:** Why is converting diverse NLP tasks to text-to-text format beneficial for instruction tuning?

## Architecture Onboarding

- **Component map:** T5-small (LM-adapted) backbone -> Instruction templates (task definition + examples) -> Memory buffers (for replay-based methods) -> Adapter modules (for AdapterCL) -> Training pipeline (two-stage learning)

- **Critical path:** 1. Initial multi-task instruction tuning on 100 tasks; 2. Sequential single-task fine-tuning on InstrDialog or InstrDialog++ streams; 3. Evaluation on three test sets (seq, init, unseen); 4. Metric calculation (AR, FWT, BWT, FR)

- **Design tradeoffs:** Small model size (T5-small) vs. better performance with larger models; Fixed instruction template vs. task-specific templates; Limited memory size (10-50 instances) vs. better replay performance with larger memory; Random task ordering vs. curriculum learning approaches

- **Failure signatures:** Performance degradation on Tinit tasks indicates forgetting of initial instruction abilities; Low FWT scores suggest poor knowledge transfer between tasks; High variance across different task orders indicates sensitivity to learning sequence

- **First 3 experiments:** 1. Replicate FT-init baseline to verify competitive performance; 2. Test different instruction template variations (id only, def only, def+2p, def+2p+2n); 3. Compare performance with different memory sizes for replay-based methods (10 vs 50)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do task order and task similarity affect knowledge transfer in Continual Instruction Tuning (CIT)?
- **Basis in paper:** The paper states "Learned tasks transfer knowledge through the instructions to new tasks of the same type, therefore facilitating its learning" and shows that "all baselines are highly affected by task orders, fluctuating dramatically when different tasks are learned first."
- **Why unresolved:** The paper only shows task order affects performance but doesn't analyze which specific task characteristics drive this effect or provide a framework for predicting optimal task ordering.
- **What evidence would resolve it:** A systematic study varying task order and task types to quantify the relationship between task similarity and knowledge transfer, along with a metric for task difficulty that predicts optimal ordering.

### Open Question 2
- **Question:** Can continual learning methods be designed that fully leverage rich natural language instructions to prevent catastrophic forgetting and enhance knowledge transfer?
- **Basis in paper:** The paper concludes "we find that those conventional CL methods do not fully leverage the instructions to reduce forgetting and facilitate knowledge transfer while learning continuously because the naive FT-init and FT-no-init can also achieve the same."
- **Why unresolved:** While the paper shows current CL methods don't effectively use instructions, it doesn't propose new methods that do, leaving open the question of what instruction-specific mechanisms could improve performance.
- **What evidence would resolve it:** Development and evaluation of new CL methods that incorporate instruction-specific features and demonstrate improved performance over naive fine-tuning.

### Open Question 3
- **Question:** What is the optimal amount of training data per task for balancing knowledge transfer and catastrophic forgetting in CIT?
- **Basis in paper:** The paper shows "FWT and BWT gradually decrease when the number of training instances is scaled to large values" and "instruction-tuned models (FT-init) have better generalization to new tasks (FWT) than the model not fine-tuned on any instruction data (FT-no-init)."
- **Why unresolved:** The paper only tests a limited range of training instance counts (10-500) and doesn't determine the optimal trade-off point between sufficient task learning and maintaining generalization ability.
- **What evidence would resolve it:** A comprehensive study testing a wider range of training instance counts to identify the optimal amount that maximizes both knowledge transfer and minimizes forgetting.

## Limitations

- Limited model size (T5-small only) restricts generalizability to larger architectures where CL methods might perform differently
- Fixed instruction templates prevent assessment of how template quality affects method performance
- Small task streams (19-38 tasks) may not reveal long-term forgetting patterns that emerge over hundreds of tasks

## Confidence

**Medium** for claims about instruction-based methods' superiority - demonstrated with T5-small but untested on larger models
**Low** for claims about diversity benefits - confounds task diversity with task quantity and stream length
**Medium** for mechanism explanations - plausible but lacks definitive ablation studies

## Next Checks

1. **Template ablation study**: Test instruction-based methods with systematically varied templates (id only, def only, def+examples) across all CL methods to determine which instruction components drive performance gains

2. **Scale sensitivity analysis**: Repeat key experiments with T5-base and T5-large models to assess whether method rankings change with model size, particularly testing if CL methods become more effective on larger models

3. **Long-stream evaluation**: Extend sequential training beyond the 38 tasks in InstrDialog++ to 100+ tasks to observe whether instruction-based methods maintain their advantage over conventional CL methods in longer learning streams