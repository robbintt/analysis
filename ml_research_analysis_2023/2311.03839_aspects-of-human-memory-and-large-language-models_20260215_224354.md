---
ver: rpa2
title: Aspects of human memory and Large Language Models
arxiv_id: '2311.03839'
source_url: https://arxiv.org/abs/2311.03839
tags:
- memory
- language
- text
- list
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates memory characteristics in Large Language
  Models (LLMs) by examining how they recall facts presented in text. The research
  compares LLM memory properties to human memory phenomena including primacy/recency
  effects, the impact of elaborations, interference-based forgetting, and the benefits
  of spaced repetition.
---

# Aspects of human memory and Large Language Models

## Quick Facts
- arXiv ID: 2311.03839
- Source URL: https://arxiv.org/abs/2311.03839
- Reference count: 28
- One-line primary result: LLMs exhibit human-like memory properties including primacy/recency effects, improved recall with elaborations, interference-based forgetting, and spaced repetition benefits.

## Executive Summary
This study investigates memory characteristics in Large Language Models by examining how they recall facts presented in text. The research compares LLM memory properties to human memory phenomena including primacy/recency effects, the impact of elaborations, interference-based forgetting, and the benefits of spaced repetition. Key findings show LLMs exhibit human-like memory patterns despite lacking dedicated memory subsystems, suggesting that textual data used in training contains statistical imprints of human memory characteristics.

## Method Summary
The core method involves presenting LLMs with lists of facts about people (e.g., "Paul has a guitar") separated by intervening text, then querying them to test recall accuracy. The study uses GPT-J and various Pythia-family models, testing different relationships (has-a, is-a, lives-in) and manipulating list lengths and intervening text. Recall accuracy is measured by whether the highest probability noun matches the correct answer when querying with "N has X" format.

## Key Results
- LLMs show U-shaped recall curves demonstrating primacy and recency effects similar to human memory
- Improved recall accuracy when facts are elaborated with descriptive details
- Stronger forgetting through interference than decay-based mechanisms
- Better performance with spaced repetitions of facts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit human-like memory properties due to statistical imprints in training data rather than architectural design
- Mechanism: The transformer architecture lacks dedicated memory subsystems, so memory emerges from learned statistical correlations in text where humans naturally structure information according to biological memory constraints
- Core assumption: The conditional probability model captures not just grammar but also semantic and narrative structures from training data
- Evidence anchors:
  - [abstract] "We argue that the human-like memory properties of the Large Language Model do not follow automatically from the LLM architecture but are rather learned from the statistics of the training textual data."
  - [section] "The similarity of the characteristics of human biological memory with LLM's memory can be a-priori interpreted in two ways: (1) architectural features resemble human memory, (2) narratives are structured compatible with biological memory characteristics."
- Break condition: If different LLM architectures with similar performance but different internal mechanisms don't show these memory patterns, this mechanism would be invalid

### Mechanism 2
- Claim: Positional embeddings create architectural bias for recency effects
- Mechanism: Rotary positional embeddings encode token positions through decreasing similarity with distance, potentially creating architectural bias for recency effects where recent tokens have higher influence
- Core assumption: The mathematical properties of positional embeddings directly translate to behavioral biases in memory recall patterns
- Evidence anchors:
  - [section] "The position of the word/token in the LLM, is encoded by adding so-called (rotary) positional embedding [23], which is effectively the only proxy for time."
  - [section] "From this perspective, one could think that this provides a very clear architectural bias for the recency effect."
- Break condition: If smaller models with identical positional embeddings don't show recency effects, this mechanism is insufficient

### Mechanism 3
- Claim: "Memory formation time" reflects the need for sufficient context before information becomes accessible
- Mechanism: Initial low recall accuracy that improves with increasing distance from facts suggests a contextualization process where the model needs time (intervening text) to properly encode and make facts available for later recall
- Core assumption: The transformer's attention mechanism requires sufficient surrounding context to properly integrate new information into its probabilistic model
- Evidence anchors:
  - [section] "In Fig. 6, we observe that the performance initially strongly increases from very low levels, reaching a maximum for the intervening text being around 10 repetitions of Humpty Dumpty."
  - [section] "It therefore appears that there seems to be some effective 'memory formation time' necessary for prior information to be available for use later in the text."
- Break condition: If experiments show that adding more intervening text beyond a certain point doesn't improve recall, this mechanism would be incomplete

## Foundational Learning

- Concept: Conditional probability modeling of language
  - Why needed here: The entire memory experiment relies on the LLM's ability to compute P(token|preceding text), which is the foundation for defining and testing memory properties
  - Quick check question: How does the LLM's probabilistic model differ from simple next-word prediction in terms of capturing semantic relationships?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding that transformers have no dedicated memory subsystems and rely on attention between tokens at different positions is crucial for interpreting why memory emerges rather than being explicitly designed
  - Quick check question: Why does the transformer's simultaneous access to all preceding text make traditional memory concepts difficult to apply?

- Concept: Positional embeddings and their mathematical properties
  - Why needed here: The distinction between how positions are encoded (rotary embeddings with decreasing similarity) versus how humans perceive temporal sequences is key to understanding potential architectural biases
  - Quick check question: How do rotary positional embeddings mathematically encode distance between tokens, and what behavioral implications might this have?

## Architecture Onboarding

- Component map: Text input → tokenization → positional embedding → multi-head attention computation → feed-forward layers → output probability distribution → token selection for memory test

- Critical path: The critical performance bottleneck is typically the attention computation for long sequences

- Design tradeoffs: Larger models show more complete memory patterns but require significantly more computational resources. The choice of tokenizer affects which words become single versus multi-token representations, potentially biasing results. Using Humpty Dumpty as intervening text is convenient but may have unintended semantic effects.

- Failure signatures: If recency effects disappear, check model size and training regimen. If primacy effects are weak, verify that the facts list is properly constructed and tokenized. If "memory formation time" effects are absent, ensure intervening text is sufficiently long and semantically neutral.

- First 3 experiments:
  1. Replicate the basic primacy/recency effect experiment with a simple list of 20 facts and standard intervening text to verify the U-shaped curve
  2. Test the impact of varying intervening text length using n × HD repetitions to observe the memory formation time effect
  3. Implement the elaboration experiment by adding descriptive details to facts at specific positions and measuring recall improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the memory characteristics observed in GPT-J generalize to other LLM architectures, or are they specific to transformer-based models?
- Basis in paper: [explicit] The authors discuss architectural bias versus genuine statistical correlations in training data, noting that "currently all LLMs of comparable performance are based on transformers, so we have to confine ourselves to indirect arguments."
- Why unresolved: The paper primarily uses GPT-J and Pythia models, both transformer-based. No comparison is made with non-transformer architectures.
- What evidence would resolve it: Direct experiments comparing memory characteristics of transformer-based models with other architectures like recurrent neural networks or convolutional models.

### Open Question 2
- Question: How do different tokenization schemes affect the observed memory phenomena in LLMs?
- Basis in paper: [inferred] The authors note that "The Pythia tokenizer is, however, different and less convenient for these experiments" and mention issues with word tokenization affecting results.
- Why unresolved: The paper acknowledges tokenizer differences but doesn't systematically investigate how tokenization affects memory characteristics.
- What evidence would resolve it: Controlled experiments comparing memory performance across different tokenizers using the same base model.

### Open Question 3
- Question: What specific aspects of training data statistics cause LLMs to exhibit human-like memory characteristics?
- Basis in paper: [explicit] The authors conclude that "the properties of our biological memory leave an imprint on how we structure globally our textual narratives" but don't specify which textual features are responsible.
- Why unresolved: While the paper identifies that training data contains these patterns, it doesn't analyze what specific textual structures or patterns encode these memory characteristics.
- What evidence would resolve it: Detailed corpus analysis identifying specific linguistic features or narrative structures that correlate with observed memory patterns.

## Limitations

- The assumption that text-based experiments can fully capture "memory" properties when LLMs operate through probabilistic token prediction rather than biological memory systems
- Uncertainty regarding the role of training data composition and whether it contains sufficient narrative structures encoding human memory patterns
- Potential biases introduced by the tokenizer, particularly how composite versus single tokens affect experimental results

## Confidence

**High confidence**: The observed U-shaped recall curves demonstrating primacy and recency effects, and the improvement with elaborations are robust findings across multiple model sizes and are well-supported by the experimental data.

**Medium confidence**: The conclusion that interference-based forgetting is stronger than decay-based forgetting, while supported by the data, could be influenced by the specific experimental design and intervening text choice.

**Low confidence**: The mechanism explaining "memory formation time" through contextualization processes is the weakest claim, as the evidence shows correlation but doesn't establish causation for why this specific temporal pattern emerges.

## Next Checks

1. **Corpus Analysis**: Analyze the training corpora for statistical patterns of narrative structure that might encode human memory characteristics. Specifically, examine whether texts commonly present information in patterns that would create primacy/recency effects.

2. **Tokenizer Impact Study**: Conduct controlled experiments varying the tokenizer settings to see how composite versus single tokens affect the observed memory patterns, particularly focusing on whether certain word representations bias the results.

3. **Cross-Architecture Replication**: Test whether other LLM architectures with similar performance but different internal mechanisms (attention patterns, positional encodings) show the same memory characteristics to validate whether these patterns are truly learned from data rather than emerging from architectural constraints.