---
ver: rpa2
title: 'Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking
  Monitoring Systems'
arxiv_id: '2309.16495'
source_url: https://arxiv.org/abs/2309.16495
tags:
- parking
- dataset
- training
- pklot
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the deployment of a parking monitoring\
  \ system that operates across diverse scenarios without requiring target-specific\
  \ labeled data. It evaluates a global framework based on deep learning architectures\u2014\
  including MobileNetV3 and ResNet-50\u2014and compares single-model and ensemble\
  \ approaches (dynamic selection, stacking, majority vote)."
---

# Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems

## Quick Facts
- arXiv ID: 2309.16495
- Source URL: https://arxiv.org/abs/2309.16495
- Authors: 
- Reference count: 21
- Key outcome: Single models trained on diverse parking datasets achieve 95% cross-dataset accuracy, outperforming ensemble methods.

## Executive Summary
This study investigates the deployment of parking monitoring systems across diverse scenarios without requiring target-specific labeled data. The research compares single-model and ensemble approaches for parking space occupancy classification using deep learning architectures. Experiments on multiple public datasets show that a single model trained on diverse data achieves 95% accuracy, surpassing ensemble strategies in most cases. The findings suggest that deep representations learned from varied training data are sufficient for robust cross-dataset performance, eliminating the need for extensive retraining or annotation in new environments.

## Method Summary
The study evaluates deep learning architectures including MobileNetV3 and ResNet-50 for parking space classification across diverse scenarios. Models are trained on concatenated datasets from multiple parking lots (PKLot, CNRPark-EXT, NDISPark, BarryStreet) with data augmentation applied during training. The research compares single-model approaches against ensemble methods including dynamic selection, stacking, and majority vote. Cross-dataset testing is performed by training on one or more datasets and evaluating on completely unseen target datasets, with 10-fold random seeds used for statistical robustness.

## Key Results
- Single models trained on diverse datasets achieve 95% cross-dataset accuracy
- Ensemble strategies do not consistently outperform single models in this task
- Data augmentation through rotations and brightness/contrast changes improves generalization
- Cross-dataset accuracy degrades when training data lacks sufficient diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single deep learning model trained on diverse parking lot datasets achieves high cross-dataset accuracy without needing target-specific labeled data.
- Mechanism: Training a single model on data from multiple parking lots with varied camera angles, weather conditions, and backgrounds builds a robust feature representation that generalizes to unseen environments. The model learns invariant patterns of parking space occupancy across these variations.
- Core assumption: Diversity in training data across scenarios is sufficient to create a generalized model that performs well on new, unseen parking lots.
- Evidence anchors:
  - [abstract] "models trained on diverse datasets can achieve 95% accuracy without the burden of data annotation and model training on the target parking lot"
  - [section] "we found that models trained on diverse datasets can achieve 95% accuracy without the burden of data annotation and model training on the target parking lot"
- Break condition: If training data lacks sufficient diversity (e.g., all images from similar camera angles or lighting), the model may overfit to specific conditions and fail to generalize.

### Mechanism 2
- Claim: Ensemble methods do not consistently outperform a single well-trained model in cross-dataset parking space classification.
- Mechanism: While ensembles combine predictions from multiple models, the additional complexity doesn't necessarily translate to better generalization when the training data is already diverse. The single model may already capture the essential features needed for accurate classification across scenarios.
- Core assumption: The representational capacity of a single deep learning model is sufficient to handle the complexity of cross-dataset parking space classification.
- Evidence anchors:
  - [abstract] "a single model trained on diverse scenarios achieves 95% accuracy, outperforming ensemble strategies in most cases"
  - [section] "a single global model approach, trained over diverse scenarios, can better generalize the task and outperform any of the proposed ensemble frameworks, achieving an average rate of 95%"
- Break condition: In scenarios with highly specialized conditions or very limited training data, ensemble methods might provide marginal improvements by combining complementary strengths of different models.

### Mechanism 3
- Claim: Data augmentation improves model generalization by simulating environmental variations during training.
- Mechanism: Applying synthetic transformations like rotations, brightness, and contrast changes to training images creates a more robust model that can handle real-world variations in camera angles and lighting conditions without requiring additional labeled data.
- Core assumption: The augmented variations are representative enough of real-world differences between parking lots to improve generalization.
- Evidence anchors:
  - [section] "A synthetic data augmentation was applied for each training batch, improving the generalization through image rotations and changes in contrast and brightness. This approach mimics environmental issues such as different camera angles and lighting conditions."
- Break condition: If augmentation is too aggressive or unrealistic, it may introduce noise that confuses the model rather than improving its robustness.

## Foundational Learning

- Concept: Cross-dataset generalization in machine learning
  - Why needed here: Understanding how models trained on one dataset perform on unseen datasets is crucial for developing deployable parking monitoring systems that work across different environments without retraining.
  - Quick check question: What is the primary challenge when applying a model trained on Dataset A to Dataset B without any fine-tuning?

- Concept: Ensemble methods vs. single models
  - Why needed here: The paper compares different approaches to determine whether combining multiple models provides better results than a single, well-trained model for cross-dataset parking classification.
  - Quick check question: Under what circumstances might an ensemble of models outperform a single model, despite the additional complexity?

- Concept: Data augmentation techniques
  - Why needed here: Augmentation is used to improve model robustness to variations in real-world deployment conditions, which is essential for creating a "ready-to-use" system.
  - Quick check question: How does data augmentation help a model generalize to new environments without requiring additional labeled data?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture (MobileNetV3/ResNet-50) -> Training loop (Adam optimizer, cross-entropy loss, early stopping) -> Evaluation (cross-dataset testing, accuracy metrics)

- Critical path:
  1. Load and preprocess diverse training datasets (PKLot, CNRPark-EXT, NDISPark, BarryStreet)
  2. Apply data augmentation to simulate environmental variations
  3. Train model with early stopping based on validation accuracy
  4. Evaluate on completely unseen target datasets
  5. Compare single model vs. ensemble approaches

- Design tradeoffs:
  - Model complexity vs. inference speed: MobileNetV3 offers a good balance for real-time deployment
  - Training data diversity vs. model generalization: More diverse training data improves cross-dataset performance
  - Ensemble complexity vs. marginal accuracy gains: Ensembles may not justify their complexity in this use case

- Failure signatures:
  - High accuracy on training data but poor performance on target datasets (overfitting to specific conditions)
  - Consistently low accuracy across all datasets (underfitting or poor feature extraction)
  - Performance degradation on specific subsets (e.g., nighttime images) indicating insufficient representation in training data

- First 3 experiments:
  1. Train a MobileNetV3 model on PKLot dataset and evaluate on CNRPark-EXT to establish baseline cross-dataset performance
  2. Compare MobileNetV3 vs. ResNet-50 vs. custom CNN architectures on the same training/testing split to identify best model for this task
  3. Implement and evaluate a majority vote ensemble of all three architectures to determine if ensemble methods provide any advantage over the best single model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do heterogeneous ensemble methods (e.g., combining different architectures) perform compared to homogeneous ensembles in cross-dataset parking space classification?
- Basis in paper: [inferred] The paper mentions future work on exploring heterogeneous pools and ensemble methods, suggesting this remains unexplored.
- Why unresolved: The paper only evaluated homogeneous ensembles (same architecture across all models) and did not test mixing different architectures in the pool.
- What evidence would resolve it: Experiments comparing homogeneous vs. heterogeneous ensembles across the same cross-dataset scenarios, measuring accuracy and computational efficiency trade-offs.

### Open Question 2
- Question: What specific characteristics of the CNRPark-EXT dataset cause the performance drop in cross-dataset scenarios, and can targeted data augmentation or domain adaptation mitigate this?
- Basis in paper: [explicit] The paper notes that CNRPark-EXT's nine cameras capture the same scenario simultaneously, leading to lack of diversity in lighting, weather, and background textures, which deteriorates model generalization.
- Why unresolved: While the paper identifies this as a hypothesis, it does not empirically test data augmentation or domain adaptation techniques to address this issue.
- What evidence would resolve it: Experiments applying domain adaptation or targeted augmentation to CNRPark-EXT data and measuring cross-dataset performance improvements.

### Open Question 3
- Question: How does the proposed single model approach compare to state-of-the-art fine-tuned models when minimal target-domain data is available?
- Basis in paper: [inferred] The paper focuses on scenarios with no target-domain labeled data, but does not explore cases with small amounts of labeled data for fine-tuning.
- Why unresolved: The paper's evaluation protocol strictly excludes target-domain training samples, leaving the comparison with fine-tuning unexplored.
- What evidence would resolve it: Experiments comparing the single model's performance against fine-tuned versions using small amounts of target-domain data, measuring the trade-off between annotation effort and accuracy gains.

## Limitations
- Small number of parking lot datasets (4) may not capture full diversity of real-world scenarios
- Cross-dataset testing methodology may have overlapping characteristics that artificially inflate generalization performance
- Focus on accuracy metrics without reporting precision, recall, or F1-scores that could mask performance differences

## Confidence
- High confidence: The finding that single models outperform ensembles is well-supported by experimental results across multiple dataset combinations
- Medium confidence: The 95% accuracy claim, as it depends heavily on the specific datasets used and may not generalize to all parking scenarios
- Medium confidence: The assertion that ensemble methods add unnecessary complexity, as this conclusion is based on a limited set of ensemble strategies

## Next Checks
1. Test the proposed framework on additional parking datasets (e.g., PKLot-2, ParkNet) to verify generalization claims across a broader range of environments
2. Conduct ablation studies to determine the exact contribution of data augmentation to cross-dataset performance improvements
3. Evaluate ensemble methods with more sophisticated strategies (e.g., stacked generalization with meta-learning) to confirm that simpler approaches don't provide additional benefits