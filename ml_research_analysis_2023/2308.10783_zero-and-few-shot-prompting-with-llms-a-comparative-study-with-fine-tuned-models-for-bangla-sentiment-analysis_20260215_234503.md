---
ver: rpa2
title: 'Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
  Models for Bangla Sentiment Analysis'
arxiv_id: '2308.10783'
source_url: https://arxiv.org/abs/2308.10783
tags:
- sentiment
- bangla
- language
- dataset
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUBASE, a manually annotated Bangla sentiment
  analysis dataset with 33,606 social media posts. It evaluates zero- and few-shot
  in-context learning using LLMs like Flan-T5, GPT-4, and Bloomz, comparing them with
  fine-tuned models.
---

# Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis

## Quick Facts
- arXiv ID: 2308.10783
- Source URL: https://arxiv.org/abs/2308.10783
- Reference count: 22
- Key outcome: Fine-tuned monolingual BanglaBERT models outperformed both LLMs and fine-tuned multilingual models in Bangla sentiment analysis, with GPT-4 showing only slight performance gains in few-shot settings.

## Executive Summary
This paper introduces MUBASE, a manually annotated Bangla sentiment analysis dataset with 33,606 social media posts. The study comprehensively evaluates zero- and few-shot in-context learning using LLMs including Flan-T5, GPT-4, and Bloomz, comparing them against fine-tuned models. The research demonstrates that fine-tuned monolingual BanglaBERT models consistently outperform both LLMs and fine-tuned multilingual models across all evaluation metrics. While GPT-4 showed marginal improvements in few-shot settings, the gains were not statistically significant. The study also highlights the challenges LLMs face with neutral and positive class predictions, particularly Bloomz models.

## Method Summary
The study employed a multi-pronged approach to evaluate sentiment analysis performance. Researchers first created the MUBASE dataset through manual annotation of 33,606 Bangla social media posts. They then fine-tuned several transformer models including BanglaBERT, mBERT, XLM-RoBERTa, and Bloomz using standard hyperparameters (batch size 8, learning rate 2e-5, 10 epochs, max seq length 256). For LLM evaluation, zero-shot and few-shot prompts were designed in both Bangla and English for Flan-T5, GPT-4, and Bloomz models. The study also implemented a majority-based ensemble method combining predictions from different models, resulting in a 5.73% improvement in weighted F1 score.

## Key Results
- Fine-tuned monolingual BanglaBERT models achieved superior performance across all metrics compared to both LLMs and fine-tuned multilingual models
- GPT-4 showed only marginal performance improvements in few-shot settings that were not statistically significant
- Bloomz models struggled particularly with neutral and positive class predictions
- Ensemble methods combining multiple models improved weighted F1 by 5.73%

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned monolingual BanglaBERT models outperform both LLMs and fine-tuned multilingual models in Bangla sentiment analysis because monolingual models are pre-trained on larger and more representative Bangla text corpora, allowing them to capture language-specific linguistic patterns and cultural nuances better than multilingual models or LLMs with limited Bangla training data.

### Mechanism 2
Zero-shot and few-shot prompting with GPT-4 and Bloomz models can perform sentiment analysis without task-specific training because LLMs leverage their extensive pre-training on diverse text to understand sentiment-related instructions and generalize to new tasks through in-context learning.

### Mechanism 3
Ensemble methods combining predictions from different models can improve overall sentiment analysis performance because different models may make different types of errors, and combining their predictions through majority voting or weighted averaging can reduce the impact of individual model weaknesses.

## Foundational Learning

- **In-context learning**: Understanding how LLMs can perform tasks without parameter updates is crucial for evaluating zero- and few-shot prompting approaches. *Quick check*: How does in-context learning differ from traditional fine-tuning, and what are its limitations?

- **Multilingual vs. monolingual pre-training**: Comparing the performance of models trained on single vs. multiple languages helps explain the observed differences in sentiment analysis accuracy. *Quick check*: What are the trade-offs between multilingual and monolingual pre-training approaches in terms of model capacity and language coverage?

- **Sentiment analysis task formulation**: Understanding the specific challenges of Bangla sentiment analysis, such as class imbalance and cultural nuances, is essential for interpreting the results. *Quick check*: How do the characteristics of the Bangla language and its sentiment expression patterns affect the design of sentiment analysis models?

## Architecture Onboarding

- **Component map**: Data collection -> Preprocessing -> Model training (fine-tuning and in-context learning) -> Evaluation -> Ensemble combination
- **Critical path**: 1) Data collection and preprocessing, 2) Model selection and fine-tuning, 3) Prompt engineering for in-context learning, 4) Evaluation and comparison of different approaches, 5) Ensemble method implementation
- **Design tradeoffs**: Model complexity vs. computational resources, task-specific training vs. in-context learning flexibility, monolingual vs. multilingual model coverage, ensemble method complexity vs. performance gains
- **Failure signatures**: Poor performance on specific sentiment classes, overfitting to training data in fine-tuned models, ineffective in-context learning due to prompt design, ensemble method introducing additional errors
- **First 3 experiments**: 1) Fine-tune BanglaBERT on MUBASE dataset and evaluate performance on test set, 2) Design and test zero-shot prompts for GPT-4 and Bloomz models on MUBASE dataset, 3) Implement and evaluate ensemble method combining predictions from BanglaBERT and GPT-4 models

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLMs vary with different prompt engineering techniques beyond those tested in this study? The paper mentions that authors conducted extensive experiments with various prompts and instructions but did not explore all possible prompting strategies. Additional experiments testing a wider range of prompt engineering techniques and their impact on LLM performance would provide insights into optimal prompting strategies.

### Open Question 2
How does the performance of fine-tuned models compare to LLMs when using a larger and more diverse training dataset? The study used a single dataset (MUBASE) and did not explore the impact of dataset size and diversity on model performance. Experiments using larger and more diverse training datasets for both fine-tuned models and LLMs would provide insights into the impact of dataset size and diversity on model performance.

### Open Question 3
How does the performance of LLMs vary across different sentiment analysis tasks and domains? The study focused on a single sentiment analysis task (Bangla social media sentiment) and did not explore the generalizability of LLM performance across different tasks and domains. Experiments testing LLM performance on a variety of sentiment analysis tasks and domains would provide insights into the generalizability of LLM capabilities.

## Limitations

- Reliance on a single Bangla-specific dataset (MUBASE) without comprehensive cross-validation across multiple domains or time periods
- API access limitations for GPT-4 restricted comprehensive testing across different prompt variations and few-shot examples
- Limited exploration of the parameter space of prompt engineering techniques that could potentially bridge performance gaps

## Confidence

**High Confidence**: The superiority of fine-tuned monolingual BanglaBERT models over multilingual alternatives and LLMs in zero- and few-shot settings is well-supported by consistent experimental results across multiple metrics.

**Medium Confidence**: The observation that GPT-4 shows slight performance gains in few-shot settings compared to zero-shot, while not statistically significant, represents a plausible trend given the model's architecture and training methodology.

**Low Confidence**: The claim about Bloomz models' specific struggles with neutral and positive class predictions requires further validation, as the study provides limited analysis of failure patterns and potential mitigation strategies.

## Next Checks

1. **Cross-dataset validation**: Evaluate the trained models on additional Bangla sentiment analysis datasets from different domains (news, product reviews, etc.) to assess generalization beyond social media posts.

2. **Prompt engineering ablation study**: Systematically vary prompt structures, few-shot example selection methods, and temperature settings for GPT-4 and Bloomz to identify optimal configurations that might narrow the performance gap with fine-tuned models.

3. **Error analysis framework**: Conduct detailed misclassification analysis focusing on specific sentiment classes, particularly neutral and positive categories where LLMs showed weakness, to identify systematic failure patterns and potential architectural improvements.