---
ver: rpa2
title: Task-Based MoE for Multitask Multilingual Machine Translation
arxiv_id: '2308.15772'
source_url: https://arxiv.org/abs/2308.15772
tags:
- task
- tasks
- adapters
- experts
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of standard Mixture-of-Experts
  (MoE) models when applied to multi-task multilingual machine translation, where
  task-agnostic routing leads to interference between dissimilar tasks. To solve this,
  the authors introduce task-based MoE, which incorporates task information into the
  routing mechanism via shared dynamic adapters that direct tokens from similar tasks
  to the same expert groups.
---

# Task-Based MoE for Multitask Multilingual Machine Translation

## Quick Facts
- arXiv ID: 2308.15772
- Source URL: https://arxiv.org/abs/2308.15772
- Reference count: 6
- Task-based MoE improves BLEU scores on multilingual translation tasks compared to dense and standard MoE baselines

## Executive Summary
This paper addresses the inefficiency of standard Mixture-of-Experts (MoE) models when applied to multi-task multilingual machine translation, where task-agnostic routing leads to interference between dissimilar tasks. The authors introduce task-based MoE, which incorporates task information into the routing mechanism via shared dynamic adapters that direct tokens from similar tasks to the same expert groups. Experiments on multi-task multilingual translation (10 languages, 18 translation directions) show that task-based MoE models consistently outperform dense and standard MoE baselines in BLEU score across most language pairs.

## Method Summary
The method introduces task-based MoE for multitask multilingual translation, implementing three adapter variants: static (one adapter per task), dynamic (log2(n) adapters for n tasks), and shared-dynamic (shared embeddings between task and MoE gates). The architecture adds task adapter layers before MoE layers, using task embeddings for routing tokens to shared adapters, which then route to experts. The model is trained on WMT corpus with 10 language pairs using transformer architecture with 12 encoder/decoder layers, 1024 embedding dimension, 4096 feed-forward dimension, and 16 attention heads.

## Key Results
- Task-based MoE models consistently outperform dense and standard MoE baselines in BLEU score across most language pairs
- The shared-dynamic variant achieves the best results, particularly for low-resource languages
- Combining MoE with task adapters is superior to either component alone
- The model demonstrates flexibility in merging separately trained models to rapidly acquire new translation tasks with minimal additional training

## Why This Works (Mechanism)

### Mechanism 1
Task-based MoE reduces interference between dissimilar tasks by routing tokens from similar tasks to shared expert groups. The model uses task embeddings to inform routing decisions in the MoE layer, creating task-specific pathways through the model.

### Mechanism 2
Shared dynamic adapters reduce memory overhead while maintaining task-specific routing benefits. Instead of allocating one adapter per task, the model uses log2(n) adapters for n tasks, with the routing gate learning to map tasks to these shared adapters based on task similarity.

### Mechanism 3
The shared-dynamic variant improves performance by sharing embeddings between task and MoE gates. Both the task adapter gate and MoE gate use the same task embedding representation for routing decisions, creating consistency and reducing parameter overhead.

## Foundational Learning

- **Mixture-of-Experts (MoE) architecture**: Understanding MoE is fundamental to grasping how task-based MoE extends the basic concept with task-aware routing. Quick check: In standard MoE, how are tokens routed to experts, and what problem does task-based MoE solve?

- **Task embeddings and adapter-based routing**: The paper relies on task-specific embeddings to inform routing decisions, which is central to the task-based MoE approach. Quick check: How do task embeddings differ from token embeddings, and why are they important for multitask learning?

- **Load balancing in MoE systems**: The paper mentions using auxiliary loss for load balancing, which is important for practical MoE implementation. Quick check: What is the purpose of auxiliary loss in MoE training, and how does it relate to the routing mechanism?

## Architecture Onboarding

- **Component map**: Input tokens → Task embedding lookup → Task adapter routing → MoE routing → Expert computation → Output aggregation

- **Critical path**: Input tokens are processed through task embedding lookup, then routed through task adapters, followed by MoE routing to experts, and finally aggregated for output.

- **Design tradeoffs**: Static adapters offer simplicity but poor scalability; dynamic adapters reduce memory but require learning task similarities; shared embeddings reduce parameters but may create routing conflicts; task-based routing adds complexity but improves multitask learning.

- **Failure signatures**: Poor BLEU scores on low-resource languages may indicate insufficient task-specific training; high variance in expert utilization suggests routing problems; slow convergence on new tasks may indicate poor adapter design.

- **First 3 experiments**:
  1. Implement basic MoE layer with standard token-level routing and compare to dense baseline
  2. Add static task adapters and measure improvement on multitask setup
  3. Implement shared-dynamic adapters and evaluate against static and dynamic variants

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of task-based MoE models scale with the number of experts per layer compared to standard MoE models? The paper discusses that increasing the number of experts does not simply increase performance in standard MoE implementations, but does not provide a direct comparison of performance scaling with the number of experts between task-based MoE and standard MoE models.

### Open Question 2
What are the long-term effects of using task-based MoE on model generalization to unseen tasks? The paper mentions that task-based MoE models can generalize to new tasks efficiently, but does not explore the long-term effects of using task-based MoE on model generalization to unseen tasks beyond the initial acquisition.

### Open Question 3
How does the task similarity metric affect the routing decisions in task-based MoE models? The paper discusses the use of task information for routing tokens to experts and mentions the dynamic mode, but does not specify how task similarity is measured or how it directly influences routing decisions in the task-based MoE models.

## Limitations
- Claims about task-based MoE reducing interference lack direct mechanistic evidence
- The log2(n) dynamic adapter design has not been validated across different task similarity structures
- The shared embedding approach between task and MoE gates may introduce routing conflicts that are not explicitly measured
- Ablation studies focus on comparison with baselines rather than isolating specific contributions of each architectural component

## Confidence

- **High confidence**: Task-based MoE improves BLEU scores on multilingual translation tasks compared to dense and standard MoE baselines
- **Medium confidence**: Shared dynamic adapters achieve better performance than static or dynamic variants due to shared embeddings
- **Low confidence**: The log2(n) adapter design is optimal for all multitask scenarios, and task similarity can be effectively captured through shared embeddings

## Next Checks

1. **Routing analysis**: Analyze expert utilization patterns across different task groups to verify that similar tasks are indeed routed to the same expert groups, and measure the correlation between task similarity and expert routing decisions.

2. **Ablation of shared embeddings**: Implement variants where task and MoE gate embeddings are separate (non-shared) and compare performance to isolate the contribution of shared embeddings versus shared adapters.

3. **Task similarity sensitivity**: Systematically vary task groupings and measure how performance changes with different task similarity structures to validate whether the log2(n) adapter design is robust across different task configurations.