---
ver: rpa2
title: 'From Past to Future: Rethinking Eligibility Traces'
arxiv_id: '2312.12972'
source_url: https://arxiv.org/abs/2312.12972
tags:
- value
- function
- state
- learning
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses a critical flaw in TD(\u03BB) when combined\
  \ with non-linear function approximation: eligibility traces can use outdated gradient\
  \ information, leading to incorrect credit assignment to past states. To resolve\
  \ this, the authors introduce a novel bidirectional value function that incorporates\
  \ both future expected returns (standard discounted sum) and past expected returns\
  \ (reverse-time discounted sum)."
---

# From Past to Future: Rethinking Eligibility Traces

## Quick Facts
- arXiv ID: 2312.12972
- Source URL: https://arxiv.org/abs/2312.12972
- Reference count: 40
- Primary result: Introduces bidirectional value functions that combine forward and reverse temporal credit assignment, showing improved policy evaluation performance compared to TD(λ) on a synthetic chain domain

## Executive Summary
This paper identifies a critical flaw in TD(λ) when combined with non-linear function approximation: eligibility traces can use outdated gradient information, leading to incorrect credit assignment to past states. To resolve this, the authors introduce a novel bidirectional value function that incorporates both future expected returns (standard discounted sum) and past expected returns (reverse-time discounted sum). They derive principled Bellman equations and online update rules for learning this value function, proving convergence properties under standard assumptions. Experiments on a synthetic chain domain with highly irregular value functions show that methods learning bidirectional values (BiTD variants) outperform standard TD(λ) in policy evaluation speed and accuracy, particularly for intermediate λ values where TD(λ) struggles.

## Method Summary
The method introduces bidirectional value functions that learn both forward (discounted sum of future rewards) and backward (discounted sum of past rewards) value estimates simultaneously. The bidirectional value function is parameterized as the sum of forward and backward components, with shared weights enabling efficient learning. The authors derive new Bellman equations for these bidirectional values and prove convergence properties. They implement three BiTD variants (BiTD-FR, BiTD-BiR, BiTD-FBi) alongside standard TD(λ), using a single-layer neural network with ReLU activation on a 9-state chain domain. Experiments sweep learning rates and λ values across 100 seeds, comparing mean squared TD error (MSTDE) of forward value function approximation.

## Key Results
- Bidirectional value function methods (BiTD variants) outperform standard TD(λ) in policy evaluation speed and accuracy
- The improvement is most pronounced for intermediate λ values where TD(λ) struggles with irregular value functions
- All three BiTD parameterizations show consistent improvements over TD(λ) in the chain domain experiments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Eligibility traces using outdated gradients cause misaligned credit assignment in TD(λ) with non-linear function approximation.
- **Mechanism**: The backward view assumes that gradients of past states' value functions (stored in the eligibility trace) remain valid for credit assignment when the TD error is observed. However, in non-linear function approximation, the value function changes after each update, making the stored gradients outdated. This results in weight updates that are not aligned with the intended credit assignment direction.
- **Core assumption**: The value function's gradient changes significantly between updates, and the stored gradient in the eligibility trace does not reflect the current gradient.
- **Evidence anchors**:
  - [abstract] "eligibility traces can use outdated gradient information, leading to incorrect credit assignment to past states."
  - [section] "TD(λ) uses outdated gradients when performing updates... the effective update direction of TD(λ) points in the direction opposite to the intended/correct one."
  - [corpus] No direct evidence found in the 25 related papers. The closest is "Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning" which addresses off-policy bias but not outdated gradients specifically.
- **Break condition**: If the value function's gradient changes minimally between updates, or if the function approximation is linear (where gradients remain constant), this mechanism does not apply.

### Mechanism 2
- **Claim**: Bidirectional value functions provide complementary credit assignment signals that improve policy evaluation.
- **Mechanism**: By learning both forward and backward value functions (and their sum, the bidirectional value function), the agent captures credit assignment from both future and past perspectives. This dual perspective helps overcome the limitations of standard TD(λ) which only considers future returns.
- **Core assumption**: The backward value function (discounted sum of past rewards) contains useful information for evaluating the forward value function (discounted sum of future rewards).
- **Evidence anchors**:
  - [abstract] "bidirectional value functions account for both future expected returns... and past expected returns..."
  - [section] "the sum of the backward and forward value functions... is the summation of ←−v and →v at a given state"
  - [corpus] No direct evidence found in the 25 related papers. The concept of backward value functions is novel in this work.
- **Break condition**: If the backward value function does not provide meaningful additional information beyond the forward value function, or if learning both functions is computationally prohibitive.

### Mechanism 3
- **Claim**: The parameterization of bidirectional value functions as the sum of forward and backward components enables efficient learning.
- **Mechanism**: By parameterizing the bidirectional value function as the sum of the forward and backward value functions (with shared weights), learning one component helps learn the others. This shared parameterization reduces the number of parameters and creates interdependence between the value functions.
- **Core assumption**: The mathematical relationship ←→v = ←−v + →v holds and can be leveraged for parameterization.
- **Evidence anchors**:
  - [section] "We can leverage the mathematical property that ←→v = ←−v + →v to learn →v such that these two functions are interdependent"
  - [section] "θ = {w1, w2}, ϕ = {w1, w3}, and ψ = {w1, w2, w3}" (parameterization example)
  - [corpus] No direct evidence found in the 25 related papers. The specific parameterization strategy is novel.
- **Break condition**: If the interdependence between value functions does not improve learning, or if the shared parameterization introduces instability.

## Foundational Learning

- **Concept**: Temporal Difference (TD) learning and eligibility traces
  - **Why needed here**: The paper builds upon TD(λ) and identifies its limitations with non-linear function approximation, then proposes bidirectional value functions as an alternative.
  - **Quick check question**: What is the difference between TD(0) and TD(λ) in terms of credit assignment?

- **Concept**: Value function approximation and non-linear function approximation
  - **Why needed here**: The paper specifically addresses the limitations of TD(λ) when used with non-linear function approximators like neural networks.
  - **Quick check question**: Why does the problem of outdated gradients not occur with linear function approximation?

- **Concept**: Bellman equations and contraction mappings
  - **Why needed here**: The paper derives Bellman equations for the new bidirectional value functions and proves convergence properties using contraction mapping theory.
  - **Quick check question**: What is the significance of proving that the Bellman operator is a contraction mapping?

## Architecture Onboarding

- **Component map**: State features x(s) -> Single-layer neural network with ReLU -> Three value function heads (forward v, backward ←−v, bidirectional ←→v)
- **Critical path**:
  1. Forward pass through network to compute all three value functions
  2. Compute TD errors for each value function
  3. Compute gradients for each head's loss
  4. Update shared weights using the sum of gradients from all three heads

- **Design tradeoffs**:
  - Shared parameterization reduces parameters but may introduce coupling between value functions
  - Learning three value functions simultaneously increases computational cost but provides complementary credit assignment signals
  - The bidirectional approach may be more sample-efficient but requires more complex implementation

- **Failure signatures**:
  - If the bidirectional value function does not improve policy evaluation performance compared to standard TD(λ)
  - If the shared parameterization causes instability or divergence during training
  - If the computational overhead of learning three value functions is prohibitive

- **First 3 experiments**:
  1. Implement the baseline TD(λ) algorithm on the chain domain and verify it reproduces the results in Figure 5
  2. Implement the BiTD-FR parameterization and compare its performance to TD(λ) on the chain domain
  3. Test the sensitivity of BiTD-FR to different λ values and compare with TD(λ)'s sensitivity to λ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of bidirectional value functions scale with more complex environments and function approximators (e.g., deeper neural networks)?
- Basis in paper: [inferred] The paper focuses on a simple 9-state chain domain with a single-layer neural network. It mentions that the bidirectional approach could generalize to deeper networks but does not empirically test this.
- Why unresolved: The experiments are limited to a simple synthetic environment. There is no empirical evidence for the scalability of bidirectional methods to more complex domains.
- What evidence would resolve it: Empirical results on standard RL benchmarks (e.g., Atari, Mujoco) using deeper neural networks would demonstrate the scalability and potential advantages of bidirectional methods.

### Open Question 2
- Question: What are the theoretical convergence guarantees for bidirectional methods in the off-policy setting?
- Basis in paper: [inferred] The paper proves convergence for bidirectional methods in the on-policy tabular case but does not address the off-policy setting or function approximation.
- Why unresolved: The analysis focuses on the on-policy tabular case, leaving a gap in understanding the behavior of bidirectional methods in more general settings.
- What evidence would resolve it: Theoretical analysis extending convergence guarantees to the off-policy case with function approximation would address this gap.

### Open Question 3
- Question: How do bidirectional methods compare to other recent approaches for policy evaluation, such as distributional RL or model-based methods?
- Basis in paper: [explicit] The paper compares bidirectional methods to TD(λ) but does not compare them to other recent policy evaluation methods.
- Why unresolved: The paper focuses on a specific comparison but does not explore the broader landscape of policy evaluation techniques.
- What evidence would resolve it: Empirical comparisons of bidirectional methods to other state-of-the-art policy evaluation approaches would provide insights into their relative strengths and weaknesses.

## Limitations
- Experimental validation limited to a single synthetic domain (9-state chain), raising questions about generalization to complex environments
- Computational overhead of learning three value functions simultaneously not thoroughly analyzed
- Theoretical convergence proof relies on assumptions about Markov property and linear approximation that may not hold with non-linear function approximators

## Confidence

**High confidence**: The mechanism of outdated gradients in TD(λ) with non-linear function approximation
**Medium confidence**: The effectiveness of bidirectional value functions in improving policy evaluation
**Low confidence**: The scalability and generalization of the approach to complex, high-dimensional environments

## Next Checks

1. **Generalization to Complex Domains**: Test the BiTD variants on more complex, high-dimensional environments (e.g., Atari games or continuous control tasks) to assess scalability and generalization.

2. **Computational Overhead Analysis**: Conduct a thorough analysis of the computational overhead introduced by learning three value functions simultaneously, comparing it to standard TD(λ) in terms of training time and memory usage.

3. **Sensitivity to Function Approximation**: Investigate the sensitivity of BiTD variants to different function approximation architectures (e.g., deeper networks, convolutional networks) to understand the robustness of the approach.