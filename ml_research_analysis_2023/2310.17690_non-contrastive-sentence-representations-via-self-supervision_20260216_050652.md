---
ver: rpa2
title: Non-contrastive sentence representations via self-supervision
arxiv_id: '2310.17690'
source_url: https://arxiv.org/abs/2310.17690
tags:
- contrastive
- association
- pages
- computational
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares sample-contrastive (SimCSE) and dimension-contrastive
  (Barlow Twins, VICReg) training objectives for learning sentence embeddings. It
  finds that Barlow Twins can outperform SimCSE on most MTEB downstream tasks without
  auxiliary loss functions, while VICReg underperforms.
---

# Non-contrastive sentence representations via self-supervision

## Quick Facts
- arXiv ID: 2310.17690
- Source URL: https://arxiv.org/abs/2310.17690
- Reference count: 34
- Key outcome: Barlow Twins outperforms SimCSE on most MTEB tasks without auxiliary loss functions, while VICReg underperforms

## Executive Summary
This paper compares sample-contrastive (SimCSE) and dimension-contrastive (Barlow Twins, VICReg) training objectives for learning sentence embeddings. The study finds that Barlow Twins can outperform SimCSE on most MTEB downstream tasks without needing auxiliary loss functions, while VICReg underperforms due to optimization difficulties. The best results for Barlow Twins come from dropout augmentation with smaller dropout probability and larger shuffle augmentation. Supervised datasets like NLI do not consistently improve performance over unsupervised training.

## Method Summary
The study compares three self-supervised methods for learning sentence embeddings: SimCSE (sample-contrastive), Barlow Twins (dimension-contrastive), and VICReg (dimension-contrastive). Models are trained on Wikipedia using BERT-base and RoBERTa-base architectures with linear or multi-layer projectors. Three data augmentation strategies are evaluated: dropout, shuffling, and EDA. Models are trained for 2 epochs with batch size 256, evaluated every 60 steps, and the checkpoint with best STS-B dev set Spearman's correlation is selected for final evaluation on MTEB downstream tasks.

## Key Results
- Barlow Twins outperforms SimCSE on most MTEB tasks without auxiliary loss functions
- Dropout augmentation with smaller pdo (0.1) and larger pshuffle values work best for Barlow Twins
- VICReg underperforms due to optimization difficulties and larger parameter space
- Supervised datasets (NLI, WikiAuto) do not consistently improve performance over unsupervised training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Barlow Twins can outperform SimCSE on most MTEB downstream tasks without needing auxiliary loss functions
- Mechanism: Barlow Twins uses dimension-contrastive loss that decorrelates embedding dimensions while maximizing correlation between views, eliminating the need for negative samples that SimCSE requires
- Core assumption: The dimension-contrastive objective is sufficient to learn discriminative representations without explicit negative pairs
- Evidence anchors:
  - [abstract]: "We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions"
  - [section]: "Barlow Twins is competitive with unsupervised SimCSE as a standalone objective function and outperforms it on a majority of MTEB tasks with a RoBERTa based architectures"
  - [corpus]: Weak - only mentions related work on non-contrastive methods without direct comparison
- Break condition: If the dimension-decorrelation assumption fails (e.g., in tasks requiring very specific feature alignment), performance may degrade

### Mechanism 2
- Claim: Dropout augmentation with smaller dropout probability and larger shuffle augmentation yields best results for Barlow Twins
- Mechanism: Different augmentation strategies create complementary views that help the dimension-contrastive loss learn more robust representations
- Core assumption: The augmentation strategy significantly impacts the learned representations' quality
- Evidence anchors:
  - [section]: "Across models and loss functions, smaller pdo and larger pshuffle values are preferred, and the effect is more pronounced with BT"
  - [section]: "Barlow Twins (pdo = 0.1) 65.2 33.9 70.6 47.3 24.1 71.1 28.9 47.5" showing best performance with small dropout
  - [corpus]: Weak - no direct evidence about augmentation effectiveness in related works
- Break condition: If the augmentation becomes too strong (e.g., pshuffle too large), it may destroy semantic content and hurt downstream performance

### Mechanism 3
- Claim: VICReg underperforms Barlow Twins and SimCSE due to optimization difficulties
- Mechanism: VICReg's more complex loss function with three terms (variance, invariance, covariance) makes it harder to optimize than Barlow Twins' simpler two-term loss
- Core assumption: The optimization complexity of VICReg is a limiting factor in its performance
- Evidence anchors:
  - [section]: "VICReg underperforms Barlow Twins and SimCSE: we find it harder to optimize it and we cannot exclude that more hyperparameter exploration and better data augmentation would lead to better results"
  - [section]: "For more details about the scans we refer to Appendix A" suggesting extensive hyperparameter tuning was needed
  - [corpus]: Weak - no direct evidence about VICReg optimization challenges in related works
- Break condition: If proper hyperparameter tuning or better augmentation strategies are found, VICReg's performance could improve significantly

## Foundational Learning

- Concept: Contrastive learning vs dimension-contrastive learning
  - Why needed here: Understanding the fundamental difference between sample-contrastive (SimCSE) and dimension-contrastive (Barlow Twins, VICReg) methods is crucial for implementing and debugging these models
  - Quick check question: What is the key difference between sample-contrastive and dimension-contrastive methods?

- Concept: Data augmentation strategies for text
  - Why needed here: The paper shows that different augmentation strategies (dropout, shuffling, EDA) have varying impacts on performance, especially for Barlow Twins
  - Quick check question: Which augmentation strategy works best for Barlow Twins according to the paper?

- Concept: Alignment and uniformity metrics
  - Why needed here: These metrics are commonly used to analyze contrastive learning methods and provide insights into representation quality
  - Quick check question: How do alignment and uniformity metrics relate to downstream task performance?

## Architecture Onboarding

- Component map: BERT/RoBERTa base model -> Projector (linear or multi-layer) -> Embedding space

- Critical path:
  1. Prepare data and augmentations
  2. Pass through base model to get embeddings
  3. Project embeddings to high-dimensional space
  4. Compute loss based on the specific method
  5. Backpropagate and update weights

- Design tradeoffs:
  - SimCSE: Simpler loss but requires negative samples, sensitive to temperature parameter
  - Barlow Twins: No negative samples needed, simpler loss, but sensitive to augmentation strategy
  - VICReg: Most complex loss with three terms, harder to optimize, potentially more expressive

- Failure signatures:
  - Training collapse: All embeddings become identical
  - Poor downstream performance: Embeddings don't capture task-relevant features
  - Sensitivity to augmentation: Performance varies significantly with different augmentation strategies

- First 3 experiments:
  1. Train SimCSE with different dropout probabilities to understand its sensitivity
  2. Train Barlow Twins with different augmentation strategies (dropout vs shuffling) to identify optimal configuration
  3. Compare VICReg with different values of 位V and 位C to understand its optimization landscape

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Barlow Twins consistently outperform SimCSE on most MTEB tasks without auxiliary loss functions?
- Basis in paper: [explicit] The paper states "We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions."
- Why unresolved: The paper notes this finding is "partly at odds with" previous work, but does not provide a definitive explanation for why Barlow Twins succeeds where others have not.
- What evidence would resolve it: A detailed analysis comparing the mathematical properties of Barlow Twins versus SimCSE, particularly focusing on how they handle embedding collapse and feature decorrelation, would clarify this discrepancy.

### Open Question 2
- Question: What specific characteristics of Barlow Twins and VICReg loss functions lead to their different performances on downstream tasks?
- Basis in paper: [explicit] The paper compares Barlow Twins and VICReg, noting that "VICReg underperforms Barlow Twins and SimCSE" and "we find it harder to optimize it."
- Why unresolved: While the paper identifies performance differences, it does not deeply analyze the architectural or mathematical reasons for these differences.
- What evidence would resolve it: A comprehensive ablation study varying individual components of each loss function, combined with visualization of learned representations, would reveal which aspects contribute to the performance gap.

### Open Question 3
- Question: How do different data augmentation strategies (dropout vs. shuffling) affect the performance of dimension-contrastive methods across various NLP tasks?
- Basis in paper: [explicit] The paper experiments with dropout and shuffling augmentations, finding "smaller pdo and larger pshuffle values are preferred, and the effect is more pronounced with BT."
- Why unresolved: The paper provides empirical results but lacks theoretical explanation for why these augmentation parameters work better with Barlow Twins than with VICReg or SimCSE.
- What evidence would resolve it: Systematic testing of augmentation parameters across a wider range of NLP tasks and architectures, coupled with analysis of how augmentations affect embedding geometry, would explain these observations.

## Limitations

- VICReg's underperformance may be due to insufficient hyperparameter optimization rather than fundamental limitations
- Results rely heavily on MTEB downstream tasks which may not capture all aspects of representation quality
- NLI datasets do not consistently improve performance over unsupervised training, but reasons for this inconsistency are not fully explored

## Confidence

- **High confidence**: Barlow Twins outperforming SimCSE without auxiliary loss functions on MTEB tasks, and the effectiveness of small dropout probability with larger shuffle augmentation for Barlow Twins
- **Medium confidence**: VICReg's underperformance being primarily due to optimization difficulties, and the general viability of dimension-contrastive methods as alternatives to sample-contrastive techniques
- **Low confidence**: The consistency of NLI datasets not improving performance over unsupervised training, and whether the augmentation findings generalize to other model architectures

## Next Checks

1. Conduct more extensive hyperparameter sweeps for VICReg, particularly focusing on 位V and 位C values, to determine if its underperformance is truly due to optimization difficulties rather than fundamental limitations

2. Evaluate the dimension-contrastive methods on additional tasks beyond MTEB, including cross-lingual tasks and tasks requiring fine-grained semantic distinctions, to assess the generality of the findings

3. Test the augmentation strategy findings with different base models (e.g., larger variants like BERT-large or RoBERTa-large) to verify whether the observed patterns hold across model scales