---
ver: rpa2
title: Training Convolutional Neural Networks with the Forward-Forward algorithm
arxiv_id: '2312.14924'
source_url: https://arxiv.org/abs/2312.14924
tags:
- training
- layer
- algorithm
- image
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors extend the Forward-Forward (FF) algorithm to Convolutional
  Neural Networks (CNNs), addressing the limitation that previous FF implementations
  were restricted to fully connected networks. They introduce a spatially-extended
  labeling strategy using Fourier patterns to encode class information across all
  spatial positions, enabling CNNs to learn from labeled data without backpropagation.
---

# Training Convolutional Neural Networks with the Forward-Forward algorithm

## Quick Facts
- arXiv ID: 2312.14924
- Source URL: https://arxiv.org/abs/2312.14924
- Reference count: 23
- Primary result: Extends Forward-Forward algorithm to CNNs, achieving 99.00% test accuracy on MNIST without backpropagation

## Executive Summary
This paper presents a novel extension of the Forward-Forward (FF) algorithm to Convolutional Neural Networks (CNNs), addressing a fundamental limitation of previous FF implementations that were restricted to fully connected networks. The authors introduce a spatially-extended labeling strategy using Fourier patterns to encode class information across all spatial positions, enabling CNNs to learn from labeled data without backpropagation. Experiments on the MNIST dataset demonstrate that FF-trained CNNs achieve high classification accuracy (99.00% test accuracy), comparable to backpropagation-trained networks. The method avoids explicit gradient computation, making it suitable for neuromorphic hardware and biologically plausible learning.

## Method Summary
The authors extend the Forward-Forward algorithm to CNNs by introducing a spatially-extended labeling strategy using Fourier patterns. Each image is paired with a labeled image of the same size containing a characteristic gray value wave pattern superimposed with the original image. This Fourier-based label encoding is applied at every spatial position, allowing convolutional layers to access label information regardless of filter position. The network architecture consists of three FF-trained convolutional layers with 128 filters of size 7x7, followed by a fully connected layer. Layer normalization and a sigmoid-based goodness function are used to update weights during training. The method is evaluated on MNIST with 200 epochs, batch size 50, and Adam optimizer.

## Key Results
- FF-trained CNNs achieve 99.00% test accuracy on MNIST, comparable to backpropagation-trained networks
- Class Activation Maps reveal that different CNN layers learn complementary spatial features
- The method avoids explicit gradient computation, enabling potential neuromorphic hardware implementation
- Both linear classifier and goodness-based inference approaches yield similar high accuracy results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spatially-extended labeling strategy enables convolutional layers to access label information at every spatial position.
- Mechanism: By superimposing a Fourier wave pattern with fixed frequency, phase, and orientation across the entire image, the label information is preserved through convolution operations regardless of filter position.
- Core assumption: The Fourier wave pattern is distinguishable from image content and maintains consistent spatial structure across different images.
- Evidence anchors:
  - [abstract] "introduce a spatially-extended labeling strategy using Fourier patterns to encode class information across all spatial positions"
  - [section] "Each label corresponds to an image of the same size as the input, but with a characteristic gray value wave"
- Break condition: If the Fourier pattern interferes with or becomes indistinguishable from image features, the convolutional layers cannot learn to separate label information from content.

### Mechanism 2
- Claim: Layer normalization enables each subsequent layer to focus on pattern orientation rather than magnitude.
- Mechanism: Layer normalization scales activations so that subsequent layers only process the normalized orientation vector, preventing them from simply measuring the length of the previous layer's activity vector.
- Evidence anchors:
  - [section] "layer normalization involves the

## Foundational Learning
- **Forward-Forward Algorithm**: A gradient-free learning algorithm that uses positive and negative examples to update network weights. Why needed: Enables biologically plausible learning without backpropagation. Quick check: Verify that positive and negative examples are correctly distinguished and weighted.
- **Spatially-Extended Labeling**: Encoding label information across all spatial positions using Fourier patterns. Why needed: Allows convolutional layers to access label information regardless of filter position. Quick check: Visualize labeled images to ensure Fourier patterns are correctly superimposed and normalized.
- **Layer Normalization**: Normalizing activations across the feature dimension to stabilize training. Why needed: Prevents networks from learning trivial solutions based on activation magnitude. Quick check: Monitor activation statistics to ensure proper normalization.
- **Convolutional Neural Networks**: Deep learning architectures specialized for grid-like data processing. Why needed: Extract spatial hierarchies of features from images. Quick check: Verify that convolution operations preserve spatial relationships and receptive fields.

## Architecture Onboarding
- **Component Map**: Input Images -> Fourier Labeling -> CNN (Conv1 -> Conv2 -> Conv3 -> FC) -> Classification Output
- **Critical Path**: Fourier labeling strategy → Convolutional feature extraction → Goodness function evaluation → Weight updates
- **Design Tradeoffs**: 
  - Fourier labeling provides spatial label information but may interfere with image content
  - Fixed filter dimensions (7x7) balance receptive field size and computational efficiency
  - Layer normalization stabilizes training but adds computational overhead
- **Failure Signatures**: 
  - Low accuracy indicates poor Fourier pattern distinguishability or improper weight updates
  - Training instability suggests suboptimal learning rate or layer normalization parameters
  - High training but low validation accuracy indicates overfitting to Fourier patterns
- **First Experiments**: 
  1. Visualize Fourier-labeled images to verify pattern distinguishability from image content
  2. Monitor training curves to identify convergence patterns and potential instability
  3. Test different Fourier frequencies and orientations to optimize label encoding

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the Forward-Forward algorithm scale to larger, more complex datasets beyond MNIST?
- Basis in paper: [inferred] The paper demonstrates success on MNIST but mentions the need to explore larger datasets as a future direction.
- Why unresolved: The paper only tests the algorithm on the relatively simple MNIST dataset, leaving the performance on more complex datasets unknown.
- What evidence would resolve it: Testing the Forward-Forward algorithm on larger, more complex datasets like CIFAR-10, CIFAR-100, or ImageNet and comparing the results to backpropagation-trained networks.

### Open Question 2
- Question: What is the individual and synergistic contribution of positive and negative labels, and the locally defined goodness parameter, to the success of the Forward-Forward algorithm?
- Basis in paper: [explicit] The paper mentions this as an open research direction, stating the need to understand how these two innovations contribute to the algorithm's success.
- Why unresolved: While the paper demonstrates the effectiveness of the Forward-Forward algorithm, it does not provide a detailed analysis of how each component contributes to the overall performance.
- What evidence would resolve it: Ablation studies that isolate and test the impact of positive and negative labels, and the locally defined goodness parameter, on the algorithm's performance.

### Open Question 3
- Question: How does the Forward-Forward algorithm perform in unsupervised learning tasks?
- Basis in paper: [explicit] The paper mentions the exploration of the algorithm's capabilities for self-supervised learning as a future research direction.
- Why unresolved: The paper focuses on supervised learning tasks, leaving the algorithm's performance in unsupervised learning settings unexplored.
- What evidence would resolve it: Applying the Forward-Forward algorithm to unsupervised learning tasks, such as clustering or representation learning, and evaluating its performance compared to other unsupervised learning methods.

## Limitations
- The spatially-extended labeling strategy may not generalize well to natural images where Fourier patterns could interfere with image content
- Biological plausibility claims remain speculative without empirical validation in neuromorphic hardware
- The paper doesn't address computational efficiency comparisons between FF and backpropagation methods
- Results are limited to a single dataset (MNIST) without comparison to backpropagation baselines on larger, more complex datasets

## Confidence
- **High confidence** in the core FF-to-CNN extension mechanism and MNIST results
- **Medium confidence** in the biological plausibility claims without neuromorphic implementation
- **Medium confidence** in the generalizability of Fourier labeling to more complex datasets
- **Low confidence** in the computational efficiency claims without explicit benchmarking

## Next Checks
1. Implement the same FF-CNN architecture on CIFAR-10 or Fashion-MNIST to test generalization beyond simple digit classification
2. Benchmark computational efficiency and memory usage against standard backpropagation CNNs
3. Test the Fourier labeling strategy on natural images to evaluate interference effects and robustness to varying image statistics