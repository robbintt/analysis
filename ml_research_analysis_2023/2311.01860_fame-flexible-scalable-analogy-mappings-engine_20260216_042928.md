---
ver: rpa2
title: 'FAME: Flexible, Scalable Analogy Mappings Engine'
arxiv_id: '2311.01860'
source_url: https://arxiv.org/abs/2311.01860
tags:
- mapping
- relations
- entities
- fame
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FAME, a scalable analogy mapping engine that
  automatically identifies mappings between entities based on commonsense relational
  similarities. The method requires only names of entities as input, and uses state-of-the-art
  NLP and IR techniques to automatically infer commonsense relations between the entities
  from multiple sources like ConceptNet, Open IE, GPT-3, Quasimodo and Quasimodo++.
---

# FAME: Flexible, Scalable Analogy Mappings Engine

## Quick Facts
- arXiv ID: 2311.01860
- Source URL: https://arxiv.org/abs/2311.01860
- Authors: 
- Reference count: 19
- Key outcome: FAME correctly maps 81.2% of classical 2x2 analogy problems and achieves 77.8% accuracy on larger problems

## Executive Summary
This paper presents FAME, a scalable analogy mapping engine that automatically identifies mappings between entities based on commonsense relational similarities. The method requires only names of entities as input and uses state-of-the-art NLP and IR techniques to automatically infer commonsense relations between the entities from multiple sources like ConceptNet, Open IE, GPT-3, Quasimodo and Quasimodo++. FAME can handle partial mappings and suggest new entities to be added. Experiments show that FAME correctly maps 81.2% of classical 2x2 analogy problems and achieves 77.8% accuracy on larger problems.

## Method Summary
FAME is an analogy mapping engine that automatically identifies mappings between entities based on commonsense relational similarities. It requires only entity names as input and uses multiple knowledge sources (ConceptNet, Open IE, GPT-3, Quasimodo, Quasimodo++) to extract relations between entities. The extracted relations are clustered using sBERT embeddings and maximum-weight bipartite matching is applied to compute similarity between entity pairs. A beam search is then used to build the mapping, allowing for partial mappings and suggesting new entities for unmapped ones.

## Key Results
- FAME correctly maps 81.2% of classical 2x2 analogy problems with mean guess level of 50%
- FAME achieves 77.8% accuracy on larger analogy problems with mean guess level of 13.1%
- FAME outperforms human performance and generates suggestions for new entities similar to those suggested by humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FAME reduces input complexity by replacing hand-coded representations with automatically extracted commonsense relations
- Mechanism: Queries multiple knowledge sources to generate relation triples, then uses sBERT embeddings to compute similarity between relation clusters
- Core assumption: Commonsense relations between entities are sufficient to capture analogical structure
- Evidence: System eliminates need for manual curation of input relations

### Mechanism 2
- Claim: Clustering similar relations and applying maximum-weight bipartite matching enables robust analogy detection
- Mechanism: Clusters relations using hierarchical agglomerative clustering based on sBERT similarity, then applies maximum-weight bipartite matching
- Core assumption: Semantically similar relations should be clustered to avoid double-counting and improve matching quality
- Evidence: Weight of edges between clusters is the maximal weight of an edge between their nodes

### Mechanism 3
- Claim: Beam search with partial mapping allows flexible analogy construction beyond strict bijection
- Mechanism: Starts with promising pair mappings and iteratively expands using beam search, allowing entities to remain unmapped
- Core assumption: Relaxing bijection constraint and allowing partial mappings leads to more flexible analogy detection
- Evidence: Algorithm can generate new suggestions for non-mapped entities

## Foundational Learning

- Concept: Similarity metrics for relation sets
  - Why needed here: To determine how well relations between entities in different domains correspond
  - Quick check question: How would you compute the similarity between two sets of relations where each relation has multiple phrasings?

- Concept: Clustering of semantic embeddings
  - Why needed here: To group semantically similar relations and avoid double-counting in analogy mapping
  - Quick check question: What clustering distance threshold would you choose for relations with sBERT embeddings?

- Concept: Bipartite matching algorithms
  - Why needed here: To find optimal correspondences between relation clusters across domains
  - Quick check question: How does maximum-weight bipartite matching differ from simple one-to-one assignment?

## Architecture Onboarding

- Component map: Knowledge extraction layer -> Relation processing -> Mapping engine -> Suggestion module -> UI layer
- Critical path: Knowledge extraction → Relation processing → Mapping engine → UI output
- Design tradeoffs:
  - More knowledge sources provide better coverage but introduce noise
  - Lower clustering thresholds catch more similar relations but risk over-clustering
  - Larger beam search sizes find better solutions but increase computation time
- Failure signatures:
  - Low mapping accuracy: Knowledge source coverage gaps or poor relation extraction
  - Inconsistent mappings: Clustering threshold issues or embedding similarity problems
  - Slow performance: Large beam search size or inefficient relation processing
- First 3 experiments:
  1. Test knowledge extraction on a simple 2x2 analogy problem and verify relation coverage
  2. Test relation clustering with different thresholds on extracted relations
  3. Test beam search mapping on a small problem with known solution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FAME's framework be extended to handle ternary or higher-arity relations?
- Basis in paper: The paper mentions the need to improve coverage and extend the framework to more than just binary relations
- Why unresolved: Current implementation only considers binary relations between entities
- What evidence would resolve it: Developing and evaluating an extended version of FAME that can handle ternary or higher-arity relations on a benchmark dataset with such relations

### Open Question 2
- Question: Can FAME's entity suggestion method be improved to provide more diverse and creative suggestions?
- Basis in paper: While top suggestions resemble human suggestions, there is room for improvement
- Why unresolved: Current entity suggestion method relies on clustering and re-running the mapping algorithm
- What evidence would resolve it: Evaluating an improved entity suggestion method on a diverse set of analogy problems

### Open Question 3
- Question: How does FAME's performance vary across different domains and types of analogies?
- Basis in paper: Paper evaluates FAME on a limited set of analogy problems, primarily focusing on classical 2x2 problems
- Why unresolved: Current evaluation does not provide comprehensive understanding of FAME's strengths and weaknesses across different domains
- What evidence would resolve it: Conducting a large-scale evaluation of FAME on a diverse set of analogy problems from various domains and types

## Limitations

- Dependency on external knowledge sources with varying coverage across domains
- Performance drops significantly when knowledge sources lack relevant information for entities
- GPT-3 integration relies on few-shot prompting without specified exact prompts, making reproduction challenging

## Confidence

*High confidence* in core algorithmic approach and effectiveness for analogy mapping problems within tested scope
*Medium confidence* in claimed superiority over human performance and automatic entity suggestions
*Low confidence* in generalizability to domains beyond tested analogy problems

## Next Checks

1. **Knowledge Source Coverage Analysis**: Systematically test FAME on analogy problems where knowledge sources have varying levels of coverage for entities. Measure performance degradation as coverage decreases.

2. **Prompt Engineering Validation**: Design controlled experiments to test different GPT-3 prompt formulations for relation extraction. Compare performance across prompt variations to determine optimal configurations.

3. **Domain Transfer Test**: Apply FAME to analogy problems from domains not represented in original benchmark (e.g., technical domains, specialized fields). Measure performance drop and identify which algorithmic components contribute most to domain-specific challenges.