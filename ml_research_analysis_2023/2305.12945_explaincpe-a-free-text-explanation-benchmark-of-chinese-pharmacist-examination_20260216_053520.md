---
ver: rpa2
title: 'ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination'
arxiv_id: '2305.12945'
source_url: https://arxiv.org/abs/2305.12945
tags:
- chatgpt
- gpt-4
- question
- instruction
- drug
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExplainCPE, a Chinese medical QA benchmark
  for evaluating LLM interpretability, containing over 7k multiple-choice questions
  with explanations. The authors evaluate several LLMs (GPT-4, ChatGPT, GPT-3.5, ChatGLM-6B,
  BELLE-7B-2M, ChatYuan) on the dataset using accuracy and Rouge metrics.
---

# ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination

## Quick Facts
- arXiv ID: 2305.12945
- Source URL: https://arxiv.org/abs/2305.12945
- Reference count: 20
- Key outcome: GPT-4 achieves highest accuracy (75.7%) and Rouge-L score (0.247) on Chinese medical QA benchmark, outperforming ChatGPT (54.5% accuracy)

## Executive Summary
This paper introduces ExplainCPE, a Chinese medical QA benchmark containing over 7,000 multiple-choice questions with explanations designed to evaluate large language models' interpretability capabilities. The authors evaluate several LLMs including GPT-4, ChatGPT, GPT-3.5, ChatGLM-6B, BELLE-7B-2M, and ChatYuan using accuracy and Rouge metrics across different few-shot settings. GPT-4 demonstrates superior performance with 75.7% accuracy and 0.247 Rouge-L score, significantly outperforming other models. The study reveals that chat models perform better with instructions while non-chat models perform better without instructions, and identifies key limitations in current LLMs related to medical knowledge acquisition, calculation reliability, and text comprehension.

## Method Summary
The ExplainCPE dataset consists of 7,000+ Chinese medical multiple-choice questions with options and gold explanations. The evaluation involves prompting various LLMs (GPT-4, ChatGPT, GPT-3.5, ChatGLM-6B, BELLE-7B-2M, ChatYuan) using few-shot learning across zero-shot, one-shot, four-shot, and eight-shot settings with both instruction and non-instruction prompts. Models' answers and explanations are compared against gold answers and explanations using accuracy metrics and Rouge scores (ROUGE-1, ROUGE-2, ROUGE-L). Human evaluation is also conducted to assess explanation quality and coherence. The study systematically compares chat models (GPT-4, ChatGPT) with non-chat models (GPT-3.5, ChatGLM-6B, BELLE-7B-2M, ChatYuan) to understand their different prompt preferences and performance characteristics.

## Key Results
- GPT-4 achieves highest accuracy (75.7%) and Rouge-L score (0.247) among all evaluated models
- ChatGPT achieves 54.5% accuracy, significantly lower than GPT-4
- Chat models perform better with instructions while non-chat models perform better without instructions
- Error analysis reveals limitations in knowledge acquisition, calculation reliability, and text comprehension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms other models on medical question answering with explanations due to its larger parameter size and stronger dialogue ability.
- Mechanism: GPT-4's extensive pre-training on diverse data and fine-tuning through reinforcement learning from human feedback (RLHF) enables it to better understand complex medical questions and generate coherent explanations.
- Core assumption: Larger parameter size and RLHF training directly contribute to improved performance on specialized tasks.
- Evidence anchors:
  - [abstract]: "GPT-4 achieves the highest accuracy (75.7%) and Rouge-L score (0.247), outperforming ChatGPT (54.5% accuracy)."
  - [section 4.2]: "Chat models, which are pre-trained with vast amounts of data and fine-tuned through reinforcement learning from human feedback (RLHF), include GPT-4... Non-chat models, on the other hand, are typically pre-trained on unsupervised plain text and fine-tuned on code or instructional data but do not have sufficient RLHF to enable human-like conversation."

### Mechanism 2
- Claim: Different LLMs have varying preferences for in-context learning, with chat models performing better with instructions while non-chat models perform better without.
- Mechanism: Chat models like GPT-4 and ChatGPT are fine-tuned for conversational interactions and respond better to explicit instructions, while non-chat models like GPT-3.5 and BELLE-7B-2M are more task-oriented and perform better when given examples rather than instructions.
- Core assumption: The training methodology and fine-tuning process of different LLMs influence their optimal prompt format.
- Evidence anchors:
  - [section 5.1]: "When comparing the two figures, the models in the left picture have strong dialogue ability. Therefore, in the case of the same number of few-shot examples, providing instructions is better than not providing instructions. However, in the right picture, the models have weak dialogue ability. Therefore, in the case of the same number of few-shot examples, not providing instructions is better."
  - [section 4.2]: "In this section, we provide a brief introduction to the LLMs used in our experiments... Non-chat models, on the other hand, are typically pre-trained on unsupervised plain text and fine-tuned on code or instructional data but do not have sufficient RLHF to enable human-like conversation."

### Mechanism 3
- Claim: The ExplainCPE dataset's unique focus on Chinese medical examination questions with explanations presents a significant challenge for LLMs due to the specialized knowledge and language requirements.
- Mechanism: The dataset's combination of complex medical concepts, specialized terminology, and the requirement to generate explanations in Simplified Chinese pushes the limits of current LLMs' capabilities.
- Core assumption: The combination of medical expertise and language-specific requirements creates a higher barrier for LLMs compared to general knowledge question-answering tasks.
- Evidence anchors:
  - [abstract]: "To address the language bias and lack of medical resources in generating rationales QA datasets, we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese."
  - [section 3.4]: "At the third level, questions are classified into 14 categories based on their content: anti-inflammatory, infection, tumor, anesthesia, cardiovascular, weight loss, orthopedics, nervous system, respiratory system, digestive system, urinary system, endocrine, immune system, and others."

## Foundational Learning

- Concept: Medical knowledge and terminology
  - Why needed here: The ExplainCPE dataset focuses on Chinese medical examination questions, requiring a strong understanding of medical concepts and terminology.
  - Quick check question: Can you explain the difference between anti-inflammatory and anti-infective drugs?

- Concept: Question-answering with explanations
  - Why needed here: The dataset requires models to not only answer questions correctly but also generate coherent explanations for their answers.
  - Quick check question: How would you approach generating an explanation for a medical diagnosis?

- Concept: Natural language processing and language models
  - Why needed here: Understanding the strengths and limitations of current LLMs is crucial for interpreting the results and identifying areas for improvement.
  - Quick check question: What are the key differences between chat models and non-chat models in terms of their training and fine-tuning process?

## Architecture Onboarding

- Component map: Dataset -> Prompting -> LLM Response -> Evaluation (Accuracy/Rouge) -> Human Evaluation
- Critical path: Question + Options -> Prompt LLM -> Collect Answer+Explanation -> Compare to Gold -> Compute Metrics -> Analyze Results
- Design tradeoffs: Chat models benefit from instruction prompts while non-chat models benefit from example-only prompts; input length constraints limit shot size for some models
- Failure signatures: Incorrect medical knowledge, incoherent explanations, misunderstanding question requirements, inconsistent reasoning
- First 3 experiments:
  1. Evaluate models on small dataset subset using different prompt formats to identify optimal configuration
  2. Analyze performance across question types (positive vs negative, logical vs scenario) to identify strengths/weaknesses
  3. Conduct human evaluation of generated explanations for quality, coherence, and novelty compared to gold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key factors that contribute to the performance gap between GPT-4 and other LLMs on the ExplainCPE dataset, particularly in terms of knowledge acquisition and text comprehension?
- Basis in paper: [explicit] The authors note that GPT-4 outperforms other models in logical reasoning, scenario analysis, and mathematical calculations, and they identify limitations in current LLMs related to knowledge acquisition, reliability of calculations, and text comprehension.
- Why unresolved: The paper does not provide a detailed analysis of the specific architectural or training differences between GPT-4 and other models that lead to these performance gaps.
- What evidence would resolve it: Comparative analysis of the training data, model architectures, and fine-tuning strategies used for GPT-4 versus other LLMs, along with controlled experiments isolating the impact of these factors on performance.

### Open Question 2
- Question: How does the quality and diversity of explanations in the ExplainCPE dataset affect the interpretability performance of different LLMs, and what are the key characteristics of high-quality medical explanations?
- Basis in paper: [inferred] The authors discuss the importance of evaluating interpretability and note limitations in using automated metrics like ROUGE, but they do not provide a detailed analysis of what makes a high-quality medical explanation or how dataset characteristics affect model performance.
- Why unresolved: The paper does not provide a comprehensive framework for evaluating the quality of medical explanations or a detailed analysis of how dataset characteristics impact model performance.
- What evidence would resolve it: A systematic study of explanation quality using multiple evaluation metrics, including human judgment, and analysis of how different explanation characteristics (e.g., completeness, accuracy, coherence) correlate with model performance.

### Open Question 3
- Question: What are the optimal in-context learning strategies for different types of LLMs (chat vs. non-chat) when generating medical explanations, and how do these strategies vary across different question types?
- Basis in paper: [explicit] The authors find that chat models perform better with instructions while non-chat models perform better without, and they observe different performance patterns across few-shot settings.
- Why unresolved: The paper does not provide a detailed analysis of why these differences exist or how optimal strategies might vary for different question types or medical domains.
- What evidence would resolve it: Systematic experiments varying prompt structure, few-shot examples, and instructions across different question types and medical domains, combined with analysis of model behavior to identify optimal strategies for each LLM type.

## Limitations

- Limited human evaluation of explanation quality, relying primarily on automatic metrics (accuracy and Rouge scores)
- Varying input length constraints across models complicate direct comparison, particularly for eight-shot settings
- Lack of standardized few-shot examples across all models may introduce bias in comparative results
- Dataset difficulty attributed to medical knowledge and Chinese language requirements without comparative analysis on other languages/domains

## Confidence

**High Confidence**: The claim that GPT-4 outperforms other models on this benchmark is well-supported by the presented data, with statistically significant differences in accuracy (75.7% vs 54.5% for ChatGPT) and Rouge-L scores across multiple experimental conditions.

**Medium Confidence**: The finding that chat models perform better with instructions while non-chat models perform better without instructions is supported by the experimental results, though the mechanism could benefit from additional ablation studies to isolate the effect of RLHF fine-tuning from other architectural differences.

**Low Confidence**: The assertion that the dataset's difficulty stems specifically from the combination of medical knowledge and Chinese language requirements lacks direct evidence. While the dataset is challenging, the paper doesn't provide comparative results on similar benchmarks in other languages or domains to support this specific claim.

## Next Checks

1. **Human Evaluation Replication**: Conduct a human evaluation study where medical professionals rate the quality, coherence, and medical accuracy of explanations generated by top-performing models (GPT-4 and ChatGPT) to validate the automatic metric findings and assess whether high Rouge scores correlate with high-quality medical explanations.

2. **Cross-Domain Transferability Test**: Evaluate the same models on a comparable English-language medical QA benchmark (such as MedQA-USMLE) using identical prompting strategies to determine whether the observed performance differences are specific to the Chinese medical domain or reflect broader capabilities of the models.

3. **Prompt Sensitivity Analysis**: Systematically vary the instruction wording, example selection, and prompt structure for both chat and non-chat models to identify the specific aspects of prompt formatting that drive the observed performance differences, potentially revealing whether RLHF truly is the differentiating factor.