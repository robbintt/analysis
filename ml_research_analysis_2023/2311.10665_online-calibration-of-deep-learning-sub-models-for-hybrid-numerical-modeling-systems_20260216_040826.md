---
ver: rpa2
title: Online Calibration of Deep Learning Sub-Models for Hybrid Numerical Modeling
  Systems
arxiv_id: '2311.10665'
source_url: https://arxiv.org/abs/2311.10665
tags:
- learning
- online
- gradient
- sub-models
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of calibrating deep learning sub-models
  in hybrid numerical modeling systems, where the physical model is not differentiable.
  The authors propose an efficient and practical online learning approach called Euler
  Gradient Approximation (EGA), which assumes an additive neural correction to the
  physical model and an explicit Euler approximation of the gradients.
---

# Online Calibration of Deep Learning Sub-Models for Hybrid Numerical Modeling Systems

## Quick Facts
- **arXiv ID**: 2311.10665
- **Source URL**: https://arxiv.org/abs/2311.10665
- **Reference count**: 40
- **Key outcome**: Proposes Euler Gradient Approximation (EGA) for online calibration of deep learning sub-models in hybrid numerical systems where the physical solver is not differentiable, demonstrating significant improvements over offline learning.

## Executive Summary
This paper addresses the critical challenge of online calibration in hybrid numerical modeling systems where deep learning sub-models are integrated with traditional physical solvers that lack differentiability. The authors propose Euler Gradient Approximation (EGA), an efficient online learning approach that assumes an additive neural correction to the physical model and uses explicit Euler discretization to approximate gradients. The method converges to exact gradients as time steps approach zero, providing a practical solution for end-to-end learning in complex physical systems.

## Method Summary
The paper proposes Euler Gradient Approximation (EGA) for online calibration of deep learning sub-models in hybrid numerical systems. The method assumes an additive decomposition where the physical model is augmented by a neural correction term. Using explicit Euler discretization, EGA approximates the gradient of the solver with respect to neural parameters, enabling efficient online learning via SGD. The approach is shown to converge to exact gradients in the limit of infinitesimally small time steps, providing a practical alternative to costly offline training methods.

## Key Results
- EGA converges to exact gradients as time step h → 0, with O(h²) error when the number of integration steps n is fixed
- Static-EGA provides a simple approximation that maintains O(h²) convergence while avoiding complex Jacobian computations
- Ensemble Tangent Linear Model (ETLM) offers a data-driven alternative for Jacobian approximation when analytical methods are unavailable
- Numerical experiments on Lorenz 63 and Quasi-Geostrophic turbulence demonstrate significant improvements over offline learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EGA converges to exact gradients as h → 0
- Mechanism: Uses explicit Euler discretization to approximate solver gradients with additive neural correction
- Core assumption: Solver has order p ≥ 1 and integration steps n remain fixed
- Evidence: Abstract states EGA converges to exact gradients in limit of small time steps

### Mechanism 2
- Claim: Static-EGA achieves O(h²) convergence with simplified Jacobian approximation
- Mechanism: Approximates Jacobian as identity matrix, simplifying gradient computation
- Core assumption: Fixed number of integration steps n
- Evidence: Corollary shows gradient can be written with O(h²) error

### Mechanism 3
- Claim: ETLM provides data-driven Jacobian approximation
- Mechanism: Uses ensemble of perturbed initial conditions to approximate Jacobian via linear regression
- Core assumption: Ensemble size K << system dimension du
- Evidence: Section describes ensemble-based Jacobian approximation method

## Foundational Learning

- **Automatic Differentiation (AD)**: Needed to compute gradients of neural sub-model with respect to parameters. Quick check: Can you implement ∂Mθ/∂θ for a given input using PyTorch's autograd?

- **Order of convergence in numerical methods**: Required to understand how numerical solvers approximate continuous dynamics. Quick check: What is the order of explicit Euler method and how does it affect O(h²) error?

- **Tangent Linear Model (TLM) and adjoint methods**: Used in TLM-EGA variant for improved gradient approximation. Quick check: How would you compute TLM of a PDE solver and why is it useful for online learning?

## Architecture Onboarding

- **Component map**: Physical model solver Ψ -> Neural sub-model Mθ -> Gradient approximation module (EGA variants) -> Online learning loop (SGD) -> Dataset pipeline (historical states u†)

- **Critical path**: 1) Generate initial condition ut, 2) Propagate through Ψn to get ut+nh, 3) Approximate ∂Ψn/∂θ using EGA variant, 4) Compute ∂J/∂θ = v + wAl,p, 5) Update θ via SGD

- **Design tradeoffs**: Static-EGA vs TLM-EGA vs ETLM-EGA (simplicity vs accuracy vs data requirements), Fixed n (better O(h²)) vs variable n (worse O(h)) (convergence rate), Ensemble size K vs Jacobian accuracy (memory/compute trade-off)

- **Failure signatures**: Numerical instability (aggressive learning rates or poor Jacobian), Poor online performance (overfitting or inadequate Jacobian), Divergence in long simulations (missing physical constraints)

- **First 3 experiments**: 1) Reproduce Lorenz 63 convergence plot with varying h, 2) Implement Static-EGA on QG LES for vorticity PDF and spectra, 3) Test fine-tuning offline-trained model with online Static-EGA for 2 epochs

## Open Questions the Paper Calls Out

- **Open Question 1**: How does EGA performance scale with increasing model complexity and dimensionality? The paper doesn't address scalability to higher-dimensional systems, with authors noting future work will explore this area.

- **Open Question 2**: Can EGA be extended to handle non-additive sub-models directly? While the paper discusses converting non-additive to additive sub-models, it lacks a comprehensive approach for handling non-additive sub-models without conversion.

- **Open Question 3**: How does choice of Jacobian approximation method affect EGA performance? The paper introduces static, TLM, and ETLM methods but doesn't provide comprehensive comparison of their performance trade-offs across different hybrid model types.

## Limitations

- Convergence guarantees require restrictive conditions: solver order p ≥ 1, sufficiently small h, and fixed integration steps n
- Relies on additive decomposition assumption which may not capture complex physical-learned interactions
- Static-EGA sacrifices accuracy when integration steps vary with time step h
- Scalability to very high-dimensional systems remains an open question

## Confidence

- **High confidence** in theoretical convergence results and mathematical derivations
- **Medium confidence** in practical effectiveness across diverse physical systems
- **Low confidence** in scalability to very high-dimensional systems where Jacobian approximation becomes challenging

## Next Checks

1. **Convergence rate verification**: Systematically test EGA across different solvers with varying orders of convergence (p=1, 2, 3) to verify theoretical O(h²) error bounds hold empirically.

2. **Scalability analysis**: Evaluate performance on a high-dimensional system (e.g., 2D/3D Navier-Stokes) where du >> K to assess limitations of ensemble-based Jacobian approximations.

3. **Robustness to non-smooth dynamics**: Test the method on systems with discontinuities or non-differentiable points (e.g., shock capturing) to identify where smoothness assumptions break down.