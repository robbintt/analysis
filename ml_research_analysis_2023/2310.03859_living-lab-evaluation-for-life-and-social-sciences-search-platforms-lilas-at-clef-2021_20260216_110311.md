---
ver: rpa2
title: Living Lab Evaluation for Life and Social Sciences Search Platforms -- LiLAS
  at CLEF 2021
arxiv_id: '2310.03859'
source_url: https://arxiv.org/abs/2310.03859
tags:
- search
- evaluation
- academic
- data
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The LiLAS lab at CLEF 2021 focused on user-centric evaluation\
  \ of academic search systems through living labs. It integrated two real-world search\
  \ platforms\u2014LIVIVO (life sciences) and GESIS Search (social sciences)\u2014\
  and enabled participants to evaluate retrieval approaches via pre-computed results\
  \ or live Docker-based systems using the STELLA infrastructure."
---

# Living Lab Evaluation for Life and Social Sciences Search Platforms -- LiLAS at CLEF 2021

## Quick Facts
- arXiv ID: 2310.03859
- Source URL: https://arxiv.org/abs/2310.03859
- Reference count: 20
- Primary result: Living lab evaluation integrated real-world academic search systems with online user feedback to compare retrieval approaches

## Executive Summary
LiLAS at CLEF 2021 introduced a living lab evaluation framework for academic search platforms, combining two real-world systems: LIVIVO (life sciences) and GESIS Search (social sciences). The lab offered two evaluation approaches - pre-computed results and live Docker-based systems - using the STELLA infrastructure. Through interleaving and A/B testing with real users, the evaluation aimed to assess the validity of click-based metrics and reproducibility of online evaluations in academic search contexts, moving beyond traditional offline test collections.

## Method Summary
The evaluation used two submission types: Type A (pre-computed TREC-formatted runs) and Type B (Docker containers with retrieval systems). Submissions were integrated into LIVIVO and GESIS Search via the STELLA framework, which handled request routing and feedback collection. Interleaving was used for the ad-hoc search task in LIVIVO, while A/B testing evaluated research data recommendations in GESIS Search. The framework captured user interactions including clicks, dwell time, and explicit feedback to compare retrieval approaches in real user contexts.

## Key Results
- Living lab evaluation successfully integrated with real academic search platforms
- Docker container submissions enabled reproducible online evaluation of retrieval systems
- Interleaving and A/B testing provided user-centric comparison of different approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Living lab evaluation provides more realistic user behavior data than offline test collections by capturing actual user interactions in production systems.
- Mechanism: Users interact with deployed retrieval systems in real-time, generating implicit feedback (clicks, dwell time) and explicit feedback (ratings) that reflect true relevance judgments rather than artificially constructed relevance labels.
- Core assumption: The experimental setup preserves enough methodological control to ensure reliable comparisons while maintaining ecological validity through real users and live content.
- Evidence anchors:
  - [abstract] "Evaluations used interleaving and A/B testing to compare approaches in real user contexts."
  - [section] "This lab's unique selling point is that we offer two tasks to test this approach in two different academic search domains and evaluation setups."
  - [corpus] Weak evidence - corpus neighbors discuss living labs and platforms but do not directly address evaluation validity.
- Break condition: If the control over experimental conditions becomes too loose, introducing confounding factors that invalidate comparisons between systems.

### Mechanism 2
- Claim: Docker container submission enables reproducible and scalable deployment of custom retrieval approaches in production environments.
- Mechanism: Participants package their ranking algorithms into Docker containers that follow STELLA infrastructure rules, allowing automatic integration and parallel evaluation without manual system modifications.
- Core assumption: Docker provides sufficient isolation and resource management to run multiple participant systems concurrently without interference.
- Evidence anchors:
  - [section] "we now offer participants the possibility to submit Docker containers that can be run within our evaluation framework called STELLA."
  - [section] "participants encapsulate their retrieval system into a Docker container following some simple implementation rules inspired by the OSIRRC workshop at SIGIR 2019."
  - [corpus] No direct corpus evidence about Docker or container-based evaluation systems.
- Break condition: If Docker containers cannot handle the scale or complexity of real-time queries, causing performance degradation or system failures.

### Mechanism 3
- Claim: Pre-computed runs provide a baseline comparison while live system submissions test dynamic adaptability to real user queries.
- Mechanism: Type A submissions (pre-computed) allow participants to optimize for known queries, while Type B submissions (live systems) must handle arbitrary incoming queries, revealing differences in system flexibility and generalization.
- Core assumption: The candidate document sets provided are sufficiently representative of actual retrieval challenges in the domain.
- Evidence anchors:
  - [section] "For type A, participants pre-compute result files following TREC run file syntax and submit them for integration into the live systems."
  - [section] "For type B, participants encapsulate their retrieval system into a Docker container following some simple implementation rules."
  - [corpus] No corpus evidence directly addressing the pre-computed vs live system comparison methodology.
- Break condition: If the candidate sets are too narrow or biased, favoring certain approaches and invalidating cross-comparison between Type A and Type B submissions.

## Foundational Learning

- Concept: Interleaving evaluation methods
  - Why needed here: Understanding how Team Draft Interleaving works is crucial for interpreting the ad-hoc search task results where multiple systems are compared simultaneously.
  - Quick check question: How does Team Draft Interleaving ensure that each system has an equal chance of having its results shown to users while still allowing direct comparison?

- Concept: A/B testing in information retrieval
  - Why needed here: The research data recommendation task uses A/B testing to compare systems, requiring knowledge of how to split user sessions and collect unbiased performance metrics.
  - Quick check question: What are the key considerations for ensuring that A/B test groups are properly randomized and that results are statistically significant in the context of academic search?

- Concept: Docker containerization for ML systems
  - Why needed here: Understanding Docker fundamentals is essential for participants to successfully package their retrieval systems for Type B submissions.
  - Quick check question: What are the minimum requirements for a Docker container to be compatible with the STELLA infrastructure, and how does containerization benefit the reproducibility of online evaluations?

## Architecture Onboarding

- Component map: STELLA Server -> STELLA App -> LIVIVO/GESIS Search -> Participant Docker containers/Pre-computed results -> Dashboard

- Critical path:
  1. Participant submits Docker container or pre-computed results
  2. STELLA Server validates and deploys submission to STELLA App
  3. Real users interact with search systems, triggering retrieval requests
  4. STELLA App routes requests to participant systems and collects feedback
  5. Results are aggregated and displayed on the dashboard

- Design tradeoffs:
  - Control vs. realism: Tighter experimental control reduces ecological validity, while looser control introduces more noise
  - Scale vs. precision: Larger user samples provide more robust statistics but may dilute meaningful signals from specific user groups
  - Reproducibility vs. flexibility: Strict containerization rules ensure reproducibility but may limit algorithmic innovation

- Failure signatures:
  - Docker container failures: Submission errors, resource exhaustion, or network timeouts during evaluation
  - Data collection failures: Missing or incomplete user interaction logs, broken feedback mechanisms
  - Statistical anomalies: Unexpectedly high variance in results, systematic biases in interleaving or A/B test assignments

- First 3 experiments:
  1. Submit a simple baseline Docker container that returns the top-k documents from the provided candidate list for any query, verifying basic integration with STELLA infrastructure
  2. Run a controlled test with pre-computed results on a small subset of queries to validate the interleaving evaluation methodology and result aggregation
  3. Deploy two different retrieval approaches in parallel using Docker containers to test the concurrent execution and comparison capabilities of the STELLA framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are click-based evaluation metrics compared to traditional relevance assessments in academic search settings?
- Basis in paper: [explicit] The paper states: "We want to learn more about ... the validity and expressiveness of click-based and relevance assessment-based evaluation metrics on small to medium-scale academic platforms."
- Why unresolved: This is a direct research question posed by the authors, indicating they have not yet determined the effectiveness of click-based metrics versus traditional methods in this context.
- What evidence would resolve it: Comparative results from the LiLAS 2021 evaluation showing correlation between click-based metrics and traditional relevance assessments, along with analysis of their respective strengths and weaknesses in academic search.

### Open Question 2
- Question: Can living lab evaluations be reliably reproduced across different academic search platforms and domains?
- Basis in paper: [explicit] The paper states: "We want to learn more about ... the reproducibility of click-based evaluations in the academic domain."
- Why unresolved: The authors are explicitly investigating whether their living lab methodology can be consistently applied and yield reliable results across different academic search systems.
- What evidence would resolve it: Results from implementing the same evaluation methodology across multiple platforms (LIVIVO and GESIS Search) showing consistency in outcomes and challenges faced.

### Open Question 3
- Question: How do pre-computed results compare to live Docker-based system evaluations in terms of effectiveness and practicality for academic search?
- Basis in paper: [explicit] The paper states: "We would like to compare pre-computed results and those provided by our live evaluation framework STELLA which incorporates interactions with end-users."
- Why unresolved: This is a direct comparison the authors want to make but have not yet completed, as they are setting up the evaluation framework.
- What evidence would resolve it: Side-by-side results from the same tasks evaluated using both pre-computed results and live Docker-based systems, including analysis of user interaction data and system performance metrics.

## Limitations
- Living lab approach depends on real user participation, introducing variability in sample sizes and demographics
- STELLA infrastructure implementation details remain underspecified, limiting exact replication
- Candidate document sets for Type A submissions may bias results toward known query optimization

## Confidence
- Living lab evaluation validity: Medium - Strong theoretical foundation but limited empirical validation across multiple domains
- Docker-based deployment reproducibility: High - Well-established containerization technology with clear implementation guidelines
- Pre-computed vs. live system comparison: Low-Medium - Methodology appears sound but candidate set representativeness is uncertain

## Next Checks
1. Conduct a sensitivity analysis on candidate document set size and composition to determine their impact on Type A submission performance
2. Implement a pilot test with synthetic user interactions to validate the STELLA infrastructure's ability to handle concurrent system evaluations
3. Compare interleaving and A/B testing results against traditional offline metrics using the same retrieval approaches to quantify ecological validity gains