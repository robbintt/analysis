---
ver: rpa2
title: 'One Model for All: Large Language Models are Domain-Agnostic Recommendation
  Systems'
arxiv_id: '2310.14304'
source_url: https://arxiv.org/abs/2310.14304
tags:
- recommendation
- domain
- performance
- item
- multi-domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LLM-Rec, a multi-domain sequential recommendation
  framework based on pre-trained large language models (LLMs). The core idea is to
  concatenate user behavior sequences from different domains and feed them into LLMs
  to generate semantically rich user and item representations.
---

# One Model for All: Large Language Models are Domain-Agnostic Recommendation Systems

## Quick Facts
- arXiv ID: 2310.14304
- Source URL: https://arxiv.org/abs/2310.14304
- Reference count: 33
- One-line primary result: LLM-Rec achieves state-of-the-art multi-domain sequential recommendation by concatenating user behavior sequences across domains and using pre-trained LLMs to generate semantically rich representations

## Executive Summary
This paper introduces LLM-Rec, a framework that leverages pre-trained large language models (LLMs) for multi-domain sequential recommendation. The key innovation is concatenating user interaction sequences from different domains and using LLM representations instead of ID embeddings, enabling knowledge transfer and alleviating data sparsity. Extensive experiments on five real-world Amazon datasets demonstrate that LLM-Rec significantly outperforms strong baselines, with larger models showing particularly strong zero-shot performance on unseen domains.

## Method Summary
LLM-Rec mixes user behavior sequences across multiple domains and feeds them into pre-trained LLMs (BERT, OPT, FLAN-T5) to generate semantically rich user and item representations. Instead of using item IDs, the framework concatenates item titles into sentences and tokenizes them as LLM input. The model uses the [CLS] or </s> token representation as the final output, computing scores through inner product and optimizing with cross-entropy loss. The approach addresses data sparsity and cold-start problems by leveraging the world knowledge encoded in LLMs to bridge semantic gaps across domains.

## Key Results
- LLM-Rec outperforms six strong baselines on five real-world Amazon datasets
- Larger LLMs (1B+ parameters) significantly improve zero-shot performance on unseen domains
- Parameter-efficient fine-tuning methods like LoRA become viable only for models exceeding 1B parameters
- Increasing model size yields diminishing returns for in-domain performance beyond 100M parameters

## Why This Works (Mechanism)

### Mechanism 1
Mixing user behavior sequences across domains and using LLMs to generate representations enables knowledge transfer and alleviates data sparsity. By leveraging world knowledge encoded in pre-trained LLMs, the model bridges semantic gaps between domains, producing richer representations than ID-based approaches. This works when LLMs encode transferable knowledge and item titles provide sufficient semantic information.

### Mechanism 2
Increasing LLM parameter size improves multi-domain recommendation performance, with larger models showing better zero-shot generalization to unseen domains. This occurs because larger LLMs have greater capacity to encode and generalize world knowledge, enabling better understanding of semantic correlations across domains. The effect is strongest when model size increases from small to medium (40M to 100M parameters).

### Mechanism 3
Parameter-efficient fine-tuning methods like LoRA become viable only for LLMs exceeding ~1B parameters. Small models rely heavily on fine-tuning all parameters to adapt to recommendation-specific patterns, while large models can adapt effectively by updating only a small subset. This works because the base knowledge in large LLMs is already rich and robust.

## Foundational Learning

- Concept: Sequential recommendation and user behavior modeling
  - Why needed here: LLM-Rec builds on sequential modeling principles; understanding user-item interaction sequences is foundational
  - Quick check question: Can you explain how SASRec uses self-attention to model user interaction sequences and predict the next item?

- Concept: Cross-domain recommendation and data sparsity
  - Why needed here: The motivation for using multi-domain data stems from cross-domain recommendation's goal to alleviate data sparsity and cold-start
  - Quick check question: What are the main challenges of cross-domain recommendation when using ID-based item representations?

- Concept: Pre-trained language models and text-based representation
  - Why needed here: The core innovation replaces ID embeddings with text-derived LLM representations; familiarity with LLM input formatting is required
  - Quick check question: How does BERT's [CLS] token representation differ from OPT's </s> token usage in sequence modeling?

## Architecture Onboarding

- Component map: Item titles -> Tokenized sentence -> Pre-trained LLM -> [CLS]/</s> representation -> Inner product -> Score
- Critical path: 1) Build mixed multi-domain user sequences with item titles 2) Tokenize and feed into LLM backbone 3) Extract [CLS] or </s> representations 4) Compute scores and optimize with cross-entropy
- Design tradeoffs: Text vs. ID (semantic transfer vs. title availability), Model size (zero-shot vs. in-domain performance), Tuning method (FPFT vs. PEFT efficiency)
- Failure signatures: Performance collapses when item titles are missing/ambiguous, No improvement from multi-domain mixing suggests poor semantic alignment, PEFT failure for small models indicates insufficient base knowledge
- First 3 experiments: 1) Compare LLM-Rec vs. SASRec on single-domain sequences 2) Test multi-domain mixing (SPIAO) vs. domain-separated mixing (S2PIAO) 3) Vary model size (40M â†’ 6.7B) and measure in-domain vs. zero-shot performance

## Open Questions the Paper Calls Out

### Open Question 1
How does performance scale with increasing model size beyond 6.7B parameters, and is there a point of diminishing returns? The paper tests up to 6.7B parameters and observes limited in-domain improvement beyond 100M parameters, but leaves open whether even larger models would continue to improve or plateau.

### Open Question 2
Can LLM-Rec be effectively applied to domains with significantly different vocabulary and semantic structures, such as scientific literature or technical manuals? The paper tests e-commerce domains with similar vocabulary, limiting generalizability to domains with different characteristics.

### Open Question 3
How does LLM-Rec compare to other state-of-the-art multi-domain recommendation methods when using the same input representation (item titles)? The paper compares against baselines using item IDs, not directly isolating the contribution of the LLM architecture.

## Limitations
- Claims about semantic transfer rely on weak mechanistic evidence within the paper
- Effectiveness depends on availability and quality of item titles as semantic anchors
- Does not investigate whether gains come from semantic alignment or general regularization effects

## Confidence

- High confidence: Empirical observation that multi-domain mixing improves recommendation performance over single-domain baselines
- Medium confidence: Claim that larger models specifically improve zero-shot performance
- Medium confidence: Effectiveness of PEFT methods for large models
- Low confidence: Mechanistic claim that semantic transfer occurs through LLM-encoded world knowledge

## Next Checks

1. Conduct an ablation study removing the multi-domain mixing component while keeping the LLM backbone to isolate whether performance gains come from semantic transfer or increased data diversity.

2. Test the framework on domains where item titles are unavailable or of poor quality to validate whether the approach truly depends on semantic text representations.

3. Compare against specialized cross-domain recommendation methods (e.g., CDCF, HGD) using the same datasets to establish whether the LLM-based approach provides advantages beyond general multi-domain training.