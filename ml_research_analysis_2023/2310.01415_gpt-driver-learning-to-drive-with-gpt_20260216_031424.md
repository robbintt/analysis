---
ver: rpa2
title: 'GPT-Driver: Learning to Drive with GPT'
arxiv_id: '2310.01415'
source_url: https://arxiv.org/abs/2310.01415
tags:
- driving
- motion
- language
- planning
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GPT-Driver, a novel approach that transforms
  OpenAI's GPT-3.5 model into a reliable motion planner for autonomous vehicles. The
  key insight is to reformulate motion planning as a language modeling problem, where
  the planner inputs and outputs are represented as language tokens.
---

# GPT-Driver: Learning to Drive with GPT

## Quick Facts
- arXiv ID: 2310.01415
- Source URL: https://arxiv.org/abs/2310.01415
- Reference count: 5
- Primary result: GPT-Driver significantly outperforms state-of-the-art motion planners in L2 error while maintaining comparable collision rates on the nuScenes dataset

## Executive Summary
This paper presents GPT-Driver, a novel approach that transforms OpenAI's GPT-3.5 model into a reliable motion planner for autonomous vehicles. The key insight is to reformulate motion planning as a language modeling problem, where planner inputs and outputs are represented as language tokens. GPT-Driver leverages the strong reasoning capabilities and generalization potential of large language models to generate driving trajectories through natural language description of coordinate positions. A novel prompting-reasoning-finetuning strategy is proposed to stimulate the numerical reasoning potential of GPT-3.5, enabling it to describe highly precise trajectory coordinates and its internal decision-making process in natural language.

## Method Summary
GPT-Driver converts heterogeneous planner inputs (observations and ego-states) into unified language tokens using the GPT tokenizer, then feeds these prompts to GPT-3.5 to generate driving trajectories described in natural language. The approach employs a three-stage prompting-reasoning-finetuning strategy: first prompting the model with autonomous driving context, then having it perform chain-of-thought reasoning to identify critical objects and generate high-level driving decisions, and finally fine-tuning on human driving trajectories to align outputs with real-world behavior. The method is evaluated on the large-scale nuScenes dataset, demonstrating significant improvements in trajectory accuracy compared to state-of-the-art planners.

## Key Results
- GPT-Driver achieves significantly lower L2 error (0.72m) compared to state-of-the-art planner UniAD (0.84m) on the nuScenes test set
- Collision rate of 1.8% is on par with UniAD's 1.7%, demonstrating safety comparable to specialized planners
- The approach shows strong generalization ability, with fine-tuning performance improving consistently from 1% to 100% of training scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5 can be transformed into a reliable motion planner by reformulating motion planning as a language modeling problem
- Mechanism: The planner inputs (observations and ego-states) are converted into language tokens, and the LLM generates driving trajectories through natural language description of coordinate positions
- Core assumption: A commonly used language tokenizer (GPT tokenizer) has sufficient capability to estimate very precise numerical values needed for motion planning
- Evidence anchors:
  - [abstract] "The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem"
  - [section] "We propose to tackle the heterogeneous planner inputs by transforming them into unified language tokens"
- Break condition: If the tokenizer cannot accurately represent coordinate values or if the LLM cannot maintain precision when generating numerical outputs

### Mechanism 2
- Claim: The prompting-reasoning-finetuning strategy enables precise numerical reasoning and transparent decision-making
- Mechanism: GPT-3.5 is initially prompted with autonomous driving context, performs chain-of-thought reasoning to generate outputs, and is fine-tuned with human driving trajectories
- Core assumption: Chain-of-thought reasoning can be effectively applied to autonomous driving scenarios to improve interpretability
- Evidence anchors:
  - [abstract] "Furthermore, we propose a novel prompting-reasoning-finetuning strategy to stimulate the numerical reasoning potential of the LLM"
  - [section] "we propose a novel prompting-reasoning-finetuning strategy in the context of autonomous driving"
- Break condition: If chain-of-thought reasoning doesn't improve trajectory quality or if fine-tuning doesn't align outputs with human driving behaviors

### Mechanism 3
- Claim: Language modeling works for motion planning because tokenization decomposes coordinate values into manageable parts
- Mechanism: Through tokenization, a coordinate value is decomposed into integer part, decimal point, and decimal part, allowing estimation at different scales
- Core assumption: Tokenization naturally handles the multi-scale nature of trajectory coordinates (meter-level coarse location followed by centimeter-level fine location)
- Evidence anchors:
  - [section] "Take the coordinate value 23.17 as an example. Through tokenization, it is decomposed into '23' which is the integer part of this value, '.' and '17' which is the decimal part"
  - [section] "Hence, the process of predicting this waypoint coordinate is essentially first estimating a coarse location at the meter level ('23' here) and then estimating a fine-grained location at the centimeter level ('17' here)"
- Break condition: If the tokenizer cannot maintain precision for distant waypoints or if classification-based estimation is less accurate than direct regression

## Foundational Learning

- Concept: Tokenization and language modeling
  - Why needed here: Understanding how numerical values are converted to language tokens is crucial for implementing the core transformation of motion planning into language modeling
  - Quick check question: How does the GPT tokenizer convert a floating-point coordinate value into language tokens, and why is this decomposition beneficial for precise numerical estimation?

- Concept: Chain-of-thought reasoning
  - Why needed here: The reasoning process identifies critical objects, assesses their potential effects, and generates high-level driving decisions, making the system interpretable
  - Quick check question: What are the three steps of the chain-of-thought reasoning framework proposed for autonomous driving, and how do they contribute to transparent decision-making?

- Concept: Few-shot learning and generalization
  - Why needed here: The approach demonstrates strong generalization ability with limited training data, which is important for practical deployment in diverse driving scenarios
  - Quick check question: How does the GPT-Driver perform when trained on only 1%, 10%, or 50% of the training scenarios compared to state-of-the-art planners?

## Architecture Onboarding

- Component map: Input preprocessing -> GPT-3.5 model -> Output postprocessing -> Fine-tuning pipeline
- Critical path:
  1. Convert perception/prediction results and ego-states to language prompts
  2. Feed prompts to GPT-3.5 model
  3. Generate trajectory with reasoning process
  4. Convert trajectory from language to numerical format
  5. Fine-tune model using human driving trajectories

- Design tradeoffs:
  - Language modeling vs. direct regression: Language modeling provides better interpretability and multi-scale estimation but may have longer inference time
  - In-context learning vs. fine-tuning: Fine-tuning performs significantly better but requires training data, while in-context learning is more flexible but limited by context window
  - Perfect perception/prediction vs. learned perception/prediction: Perfect inputs provide better planning performance but are not realistic for deployment

- Failure signatures:
  - Poor L2 error indicates inaccurate trajectory generation
  - High collision rate suggests safety issues in planned trajectories
  - Inability to identify critical objects or assess their effects indicates reasoning failures
  - Inconsistent chain-of-thought outputs suggest instability in decision-making

- First 3 experiments:
  1. Test basic language modeling capability: Convert simple coordinate values to tokens and back to verify precision preservation
  2. Validate prompting strategy: Generate trajectories for simple scenarios using different prompt formulations to find optimal approach
  3. Evaluate reasoning outputs: Run model on scenarios with clear critical objects and verify chain-of-thought reasoning correctly identifies and responds to them

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-Driver achieve real-time performance for commercial driving applications?
- Basis in paper: [explicit] The paper explicitly states that "Due to the limitations of the OpenAI APIs, we are unable to obtain the inference time of our model. Thus it remains uncertain whether our approach can meet the real-time demands of commercial driving applications."
- Why unresolved: The authors could not measure the inference time of their model due to API limitations.
- What evidence would resolve it: Empirical measurements of inference time on a representative hardware platform, demonstrating whether the model can process data at the required frequency for autonomous driving.

### Open Question 2
- Question: How does GPT-Driver's performance scale with increasing complexity of driving scenarios?
- Basis in paper: [inferred] The paper demonstrates superior performance on the nuScenes dataset but does not explicitly test performance across varying scenario complexities or rare edge cases.
- Why unresolved: The evaluation focuses on average performance metrics rather than analyzing performance breakdown across different scenario types or complexity levels.
- What evidence would resolve it: Systematic testing across a taxonomy of driving scenarios with varying complexity, including edge cases and long-tailed events, with performance metrics for each category.

### Open Question 3
- Question: Can GPT-Driver incorporate additional sensor modalities beyond perception and prediction results?
- Basis in paper: [explicit] The authors mention in the conclusion that "Future works include optimizing the inference time and involving more sensor observations such as high-definition maps in input prompts."
- Why unresolved: The current implementation only uses language descriptions of detections and predictions, and the authors explicitly identify this as future work.
- What evidence would resolve it: Implementation and evaluation of a version of GPT-Driver that incorporates additional sensor modalities (e.g., HD maps, raw sensor data) and comparison of performance gains against the current version.

## Limitations
- The model's numerical precision may degrade for distant waypoints, potentially limiting planning horizon accuracy
- Dependence on perfect perception/prediction inputs means real-world performance may degrade with learned perception systems
- Computational overhead of language-based planning compared to traditional trajectory optimization could limit real-time deployment

## Confidence
- High Confidence: The fundamental insight that motion planning can be reformulated as a language modeling problem is well-supported by experimental results
- Medium Confidence: The prompting-reasoning-finetuning strategy shows promise but requires more extensive testing across diverse driving scenarios
- Low Confidence: The claimed strong generalization ability with limited training data needs more rigorous validation

## Next Checks
1. **Precision Degradation Test:** Evaluate the model's numerical precision across varying distances by measuring coordinate accuracy at waypoints from 10m to 100m ahead
2. **Chain-of-Thought Robustness:** Create adversarial scenarios with multiple conflicting critical objects and evaluate whether the chain-of-thought reasoning process remains stable
3. **Real Perception Integration:** Replace perfect perception/prediction inputs with outputs from a learned perception system and measure the impact on trajectory accuracy and collision rate