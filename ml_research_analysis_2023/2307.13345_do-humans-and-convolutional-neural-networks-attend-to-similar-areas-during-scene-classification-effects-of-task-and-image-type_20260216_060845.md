---
ver: rpa2
title: 'Do humans and Convolutional Neural Networks attend to similar areas during
  scene classification: Effects of task and image type'
arxiv_id: '2307.13345'
source_url: https://arxiv.org/abs/2307.13345
tags:
- image
- attention
- maps
- human
- areas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the similarity between human and CNN attention
  maps during scene classification, focusing on the influence of human task and image
  type. It compares spontaneous gaze, gaze-pointing, and manual drawing tasks on objects,
  indoor scenes, and landscapes.
---

# Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type

## Quick Facts
- arXiv ID: 2307.13345
- Source URL: https://arxiv.org/abs/2307.13345
- Authors: 
- Reference count: 12
- Primary result: Manual drawing tasks yield higher similarity to CNN attention maps than eye movement tasks, with image type strongly modulating similarity.

## Executive Summary
This study investigates the similarity between human and CNN attention maps during scene classification, focusing on the influence of human task (spontaneous gaze, gaze-pointing, manual drawing) and image type (objects, indoor scenes, landscapes). Results show that manual drawing tasks yield the highest similarity to CNN attention maps, while image type significantly affects similarity, with objects showing the highest similarity and landscapes the lowest. The interaction between task and image type reveals that manual selection is more suitable for images with clearly identifiable relevant areas, while eye tracking is better for images without distinct relevant areas.

## Method Summary
The study used 60 images (20 each of objects, indoor scenes, and landscapes) from the Places365 dataset. Twenty-five participants performed scene classification tasks using three different attention-elicitation methods: spontaneous gaze, gaze-pointing, and manual drawing. A ResNet-152 CNN trained on Places365 generated attention maps using Grad-CAM. Human attention maps were generated from eye-tracking data and manual drawings, then compared to CNN attention maps using Dice score and cross-correlation metrics.

## Key Results
- Manual drawing tasks showed significantly higher similarity to CNN attention maps than eye movement tasks.
- Image type strongly modulated similarity, with objects having the highest similarity and landscapes the lowest.
- The interaction between task and image type revealed that manual selection was more suitable for images with clearly identifiable relevant areas, while eye tracking was better for images without distinct relevant areas.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual drawing tasks yield higher similarity to CNN attention maps than eye movement tasks.
- Mechanism: When humans manually draw polygons around relevant image areas, they intentionally select regions that align more closely with the coarse, region-based attention maps produced by CNN Grad-CAM methods, which also emphasize broad image regions rather than fine-grained fixation locations.
- Core assumption: Participants' conscious selections better reflect the type of spatial information CNN models use for classification than unconscious eye movements, which are influenced by task-irrelevant biases (central fixation, salience capture).
- Evidence anchors:
  - [abstract] "manual drawing tasks yield the highest similarity to CNN attention maps"
  - [section] "attention maps derived from drawing were more similar to Grad-CAM than those derived from eye movements"
  - [corpus] Weak - corpus neighbors discuss saliency and attention in CNNs but do not directly compare manual vs eye-tracking elicitation.
- Break Condition: If participants' manual selections become inconsistent across trials or individuals (high variance), the similarity advantage may disappear, especially for images without clear relevant areas.

### Mechanism 2
- Claim: Image type strongly modulates the similarity between human and CNN attention maps.
- Mechanism: Different image types (objects, indoor scenes, landscapes) provide varying levels of diagnostic information and visual structure, which affects how both humans and CNNs allocate attention. Objects with a single salient feature lead to high similarity, while landscapes with uniform areas result in low similarity.
- Core assumption: Human attention is guided by the availability of diagnostic features and scene context, and CNN Grad-CAM similarly weights these based on learned features, so similarity is highest when both rely on the same type of information.
- Evidence anchors:
  - [abstract] "Image type significantly affects similarity, with objects showing the highest similarity and landscapes the lowest"
  - [section] "higher similarity between humans and CNN has been found for abnormal X-rays with a clearly discernible relevant area"
  - [corpus] Weak - corpus neighbors discuss saliency and attention in CNNs but do not directly compare different image types' effects on similarity.
- Break Condition: If CNN models are fine-tuned or architecture changes alter their reliance on global vs local features, the pattern of image-type effects may shift.

### Mechanism 3
- Claim: The interaction between task type and image type determines which elicitation method is most suitable.
- Mechanism: For images with clearly identifiable relevant areas (objects), manual drawing is more suitable because it reduces noise from task-irrelevant eye movement biases; for images without distinct relevant areas (landscapes), eye tracking is better because manual selection becomes arbitrary and inconsistent.
- Core assumption: The appropriateness of a task depends on whether the image content allows for a clear, consistent selection of relevant areas, and whether the task minimizes irrelevant variance.
- Evidence anchors:
  - [abstract] "the interaction between task and image type reveals that manual selection is more suitable for images with clearly identifiable relevant areas, while eye tracking is better for images without distinct relevant areas"
  - [section] "for objects, drawing generated much higher similarity to Grad-CAM than the two eye movement tasks, while for landscapes, drawing was descriptively least similar to Grad-CAM"
  - [corpus] Weak - corpus neighbors discuss saliency and attention in CNNs but do not directly compare task-image interactions.
- Break Condition: If the image set is modified to have more ambiguous category-defining features, the advantage of eye tracking over manual selection may reverse.

## Foundational Learning

- Concept: Scene gist perception and its role in rapid categorization
  - Why needed here: Understanding that humans can categorize scenes without eye movements (gist perception) explains why spontaneous gaze may not always reflect the areas used for categorization.
  - Quick check question: Can you explain why people might not move their eyes when categorizing a scene, even if eye tracking is used?

- Concept: Grad-CAM and its reliance on convolutional layer activations
  - Why needed here: Knowing how Grad-CAM generates attention maps from the last convolutional layer helps explain why it produces broad, region-based highlights that may align better with manual selections than with fine-grained fixations.
  - Quick check question: How does Grad-CAM differ from pixel-level saliency methods in terms of the spatial resolution of its attention maps?

- Concept: Cross-correlation vs. Dice score as similarity metrics
  - Why needed here: Understanding that cross-correlation favors broadly distributed attention (due to zero-padding) while Dice score penalizes area size differences explains why results differ depending on image type and task.
  - Quick check question: Why might cross-correlation yield higher similarity scores for drawing tasks on landscapes compared to eye movement tasks, even if the actual overlap is low?

## Architecture Onboarding

- Component map: CNN model (ResNet-152) -> Grad-CAM explainer -> Human attention maps (spontaneous gaze, gaze-pointing, drawing) -> Similarity metrics (Dice, cross-correlation) -> Statistical analysis (F2 ANOVA)
- Critical path: Human experiment data collection -> Attention map generation -> Similarity computation -> Statistical inference
- Design tradeoffs: Using summed participant data vs. individual participant maps (latter not feasible due to low fixation counts); choice of 5% threshold for binary masks; fixed task order vs. counterbalancing
- Failure signatures: High variance in manual selections for certain image types; low overlap between tasks and CNN for landscapes; negligible differences between spontaneous gaze and gaze-pointing
- First 3 experiments:
  1. Replicate the study with a counterbalanced task order to test for order effects.
  2. Compare similarity metrics using normalized vs. unnormalized fixation data.
  3. Test a broader set of image types (e.g., faces, medical images) to generalize findings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different methods of eliciting human attention maps (e.g., eye tracking vs. manual selection) affect the similarity between human and CNN attention maps across different image types?
- Basis in paper: [explicit] The study directly compares eye tracking tasks (spontaneous gaze and gaze-pointing) with manual selection (drawing) across different image types (objects, indoor scenes, landscapes).
- Why unresolved: While the study finds that manual selection produces higher similarity to CNN attention maps than eye tracking, it doesn't fully explain why this is the case. The authors speculate that manual selection avoids task-irrelevant biases and includes more context, but further research is needed to confirm these explanations.
- What evidence would resolve it: Future studies could manipulate the specific implementation details of each task (e.g., viewing time, number of categories, method of manual selection) and measure their impact on human-CNN similarity. Additionally, researchers could investigate the underlying cognitive processes involved in each task to understand why they produce different results.

### Open Question 2
- Question: How do image characteristics, such as complexity, ambiguity, and structural uniformity, influence the similarity between human and CNN attention maps?
- Basis in paper: [explicit] The study varies image types (objects, indoor scenes, landscapes) and discusses how these differences might affect human-CNN similarity. However, it doesn't systematically investigate the impact of specific image characteristics within each type.
- Why unresolved: The authors acknowledge that image types are not homogenous and can vary in complexity, ambiguity, and structural uniformity. They suggest that future research should investigate which differentiations between image types exert the most influence on human-CNN similarity, but this remains an open question.
- What evidence would resolve it: Future studies could manipulate specific image characteristics (e.g., number of objects, scene complexity, presence of distractors) and measure their impact on human-CNN similarity. Additionally, researchers could use computational methods to quantify image features and relate them to human attention patterns.

### Open Question 3
- Question: How do individual differences in human observers affect the similarity between human and CNN attention maps?
- Basis in paper: [inferred] The study acknowledges that individual differences in humans could affect the results, but it doesn't explicitly investigate this factor. The authors suggest that future research should consider individual differences, especially in application areas where CNN should be similar to particular humans with specific characteristics.
- Why unresolved: The study uses a within-participants design and sums fixations and drawings over all participants, which obscures individual differences. The authors suggest that individual differences could be important, but they don't provide any evidence or specific hypotheses about how they might affect human-CNN similarity.
- What evidence would resolve it: Future studies could use a between-participants design and compare human-CNN similarity across different groups of observers (e.g., experts vs. novices, different age groups, different cultural backgrounds). Additionally, researchers could investigate the relationship between individual differences in cognitive abilities (e.g., working memory, attention span) and human-CNN similarity.

## Limitations
- The study relies on summed participant attention maps, which may obscure individual differences in attention strategies.
- The analysis does not account for potential order effects from the fixed task sequence.
- The generalizability of findings to other CNN architectures or training regimes is unknown.

## Confidence
- Mechanism 1: Medium - Plausible but not directly tested
- Mechanism 2: High - Clear statistical differences, underlying reasons inferred
- Mechanism 3: Medium - Observed interaction relies on untested assumptions

## Next Checks
1. Conduct a counterbalanced experiment to rule out task-order effects on similarity scores.
2. Test the similarity analysis using individual participant attention maps (if sufficient fixations per trial can be obtained).
3. Compare results using different CNN architectures (e.g., Vision Transformers) to assess robustness across model types.