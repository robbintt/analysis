---
ver: rpa2
title: 'Discriminative Feature Attributions: Bridging Post Hoc Explainability and
  Inherent Interpretability'
arxiv_id: '2307.15007'
source_url: https://arxiv.org/abs/2307.15007
tags:
- attributions
- feature
- mask
- vert
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges post hoc explainability and inherent interpretability
  by proposing VerT (Verifiability Tuning), which converts black-box models into verifiably
  interpretable ones that yield faithful and verifiable feature attributions. The
  key innovation is enforcing model robustness to distractor erasure through distillation-based
  training, ensuring explanations remain consistent when unimportant features are
  masked.
---

# Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability

## Quick Facts
- arXiv ID: 2307.15007
- Source URL: https://arxiv.org/abs/2307.15007
- Authors: 
- Reference count: 40
- Key outcome: VerT converts black-box models into verifiably interpretable ones through distillation-based training, achieving faithfulness >0.995 while producing verifiable attributions with IOU 0.461-0.821

## Executive Summary
This paper addresses the fundamental limitation of post hoc explainability methods - their inability to verify feature attributions. The authors propose VerT (Verifiability Tuning), a method that converts black-box models into verifiably interpretable models through iterative distillation-based training. By enforcing model robustness to distractor erasure, VerT produces feature attributions that are both faithful to the original model and verifiable through counterfactual tests. Experiments demonstrate that VerT models maintain high accuracy on original data while producing attributions that outperform baseline methods in pixel perturbation tests and achieve high IOU with ground truth across multiple datasets.

## Method Summary
VerT bridges post hoc explainability and inherent interpretability by converting black-box models into models from a Q-verifiable model class through iterative optimization. The method alternates between mask optimization to find the sparsest Q-feature attributions and model training via distillation to ensure both faithfulness to the original model and verifiability of explanations. The training objective consists of data distillation (ensuring predictions remain consistent after masking unimportant features) and model distillation (ensuring the interpretable model approximates the original black-box model). The approach is theoretically grounded in the existence of signal-distractor decomposition in datasets and validated across semi-synthetic and real-world datasets including Hard MNIST, Chest X-ray, and CelebA.

## Key Results
- VerT models achieve faithfulness >0.995 to original black-box models while maintaining >0.87 accuracy on simplified datasets
- VerT attributions achieve IOU 0.461-0.821 with ground truth compared to baseline methods
- Pixel perturbation tests show VerT attributions consistently outperform baseline methods across all tested datasets
- Method remains robust even when applied to models with adversarially manipulated gradients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VerT enforces model robustness to distractor erasure, ensuring explanations remain consistent when unimportant features are masked.
- Mechanism: The method uses distillation-based training to project a black-box model into a verifiably interpretable model class by optimizing for both data distillation (ensuring predictions remain consistent after masking unimportant features) and model distillation (ensuring the interpretable model approximates the original black-box model).
- Core assumption: There exists a signal-distractor decomposition in the dataset where the optimal model relies only on a subset of features (signal) while ignoring others (distractor).
- Evidence anchors:
  - [abstract] "The key innovation is enforcing model robustness to distractor erasure through distillation-based training, ensuring explanations remain consistent when unimportant features are masked."
  - [section] "We perform extensive experiments on semi-synthetic and real-world datasets (Hard MNIST, Chest X-ray, CelebA) demonstrate that VerT models achieve high faithfulness to original models (accuracy >0.995 on original data) while producing verifiable attributions..."
  - [corpus] Weak evidence - corpus lacks direct mention of distractor erasure or distillation-based training for explainability.
- Break condition: If the dataset does not have a meaningful signal-distractor decomposition (e.g., when the entire input is signal), the method becomes inapplicable.

### Mechanism 2
- Claim: VerT produces verifiable feature attributions by finding the counterfactual distribution Q that leads to the sparsest feature attributions.
- Mechanism: The method alternates between optimizing masks to find the sparsest Q-feature attribution (QFA) and distilling the black-box model to ensure it produces consistent predictions when unimportant features are masked with the optimal Q distribution.
- Core assumption: For datasets with signal-distractor decomposition, the optimal counterfactual distribution Q equals the ground truth distractor distribution.
- Evidence anchors:
  - [abstract] "The key innovation is enforcing model robustness to distractor erasure through distillation-based training, ensuring explanations remain consistent when unimportant features are masked."
  - [section] "Theorem 1. For datasets which are signal-distractor decomposable, QFA recovers the signal distribution when applied to the optimal predictor fv* ∈ Fv(Q)."
  - [corpus] Weak evidence - corpus lacks direct discussion of counterfactual distributions or Q-feature attributions.
- Break condition: If the model cannot be distilled effectively to maintain faithfulness while achieving robustness to distractor erasure, the method fails to produce verifiable attributions.

### Mechanism 3
- Claim: VerT bridges post hoc explainability and inherent interpretability by producing models that yield both faithful and verifiable explanations.
- Mechanism: The method converts black-box models into models from a Q-verifiable model class Fv(Q), where QFA produces the sparsest masks, making the explanations inherently interpretable while maintaining the performance of the original black-box model.
- Core assumption: Models from the Q-verifiable model class Fv(Q) can approximate the performance of the original black-box model while producing verifiable explanations.
- Evidence anchors:
  - [abstract] "Our contributions are the following: 1. We first formalize a framework for understanding feature attribution verification, and show theoretically that feature attributions applied to black-box models are unverifiable."
  - [section] "We perform extensive experiments on semi-synthetic and real-world datasets and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain."
  - [corpus] Weak evidence - corpus lacks direct discussion of bridging post hoc and inherent interpretability methods.
- Break condition: If the Q-verifiable model class Fv(Q) cannot achieve comparable performance to the original black-box model, the method fails to bridge the two approaches effectively.

## Foundational Learning

- Concept: Signal-distractor decomposition in datasets
  - Why needed here: This concept is fundamental to understanding how VerT identifies and isolates important features from unimportant ones. The method relies on the existence of a clear separation between signal (discriminative features) and distractor (non-discriminative features).
  - Quick check question: Given an image classification task, how would you determine whether a particular feature belongs to the signal or distractor category?

- Concept: Counterfactual distributions and Q-feature attributions
  - Why needed here: VerT uses counterfactual distributions Q to determine feature replacement during the masking process. Understanding Q-feature attributions is crucial for grasping how the method identifies the sparsest masks.
  - Quick check question: If Q is the dataset mean, what does this imply about how unimportant features are replaced during the attribution process?

- Concept: Model distillation and knowledge transfer
  - Why needed here: VerT uses distillation-based training to transfer knowledge from the black-box model to the verifiably interpretable model while maintaining both faithfulness and verifiability.
  - Quick check question: What is the difference between data distillation and model distillation in the context of VerT's training objective?

## Architecture Onboarding

- Component map:
  - Black-box model (fb) - the original model to be explained
  - Verifiable model (fv) - the interpretable model produced by VerT
  - Mask optimization module - finds the sparsest Q-feature attributions
  - Distillation training loop - alternates between mask optimization and model training
  - Counterfactual distribution Q - determines how unimportant features are replaced

- Critical path:
  1. Initialize fv = fb and masks to all ones
  2. Optimize masks to find sparsest QFA using LQFA objective
  3. Round masks to binary values
  4. Train fv using Ltrain objective to ensure faithfulness and verifiability
  5. Repeat steps 2-4 until convergence

- Design tradeoffs:
  - Mask granularity vs interpretability: Smaller superpixels provide finer-grained attributions but may be less visually interpretable
  - Faithfulness vs verifiability: Stronger emphasis on verifiability may reduce faithfulness to the original model
  - Computational cost vs accuracy: More iterations of alternating optimization improve results but increase computation time

- Failure signatures:
  - Poor performance on simplified data (fv(Ds) ≠ fb(Dd)): Indicates failure in distillation process
  - High IOU with ground truth but poor pixel perturbation test results: Suggests attributions are correct but model is not robust to masking
  - Low accuracy on original data (fv(Dd) ≠ fb(Dd)): Indicates loss of model faithfulness during distillation

- First 3 experiments:
  1. Run VerT on a simple synthetic dataset with known signal-distractor decomposition and verify that recovered masks match ground truth
  2. Test pixel perturbation performance on VerT attributions vs baseline methods on Hard MNIST
  3. Evaluate model faithfulness by comparing accuracy of fv on original data vs fb on same data

## Open Questions the Paper Calls Out

- How does VerT handle cases where no clear signal-distractor decomposition exists in the dataset?
  - Basis in paper: [explicit] The paper states "the utility of these attributions, and indeed, all feature attributions, relies on the existence of a non-trivial signal distractor decomposition of the dataset."
  - Why unresolved: The paper acknowledges this limitation but doesn't provide solutions or workarounds for datasets lacking this decomposition.
  - What evidence would resolve it: Experiments testing VerT on datasets without clear signal-distractor decompositions, along with proposed modifications to handle such cases.

## Limitations
- The method fundamentally requires signal-distractor decomposition in datasets, making it inapplicable when such decomposition doesn't exist
- Experimental validation relies on semi-synthetic data with known ground truth, which may not reflect real-world complexity
- The practical significance of quantitative improvements (e.g., IOU increases from 0.455 to 0.615) requires domain-specific interpretation

## Confidence
- High confidence in the theoretical framework and mechanism description
- Medium confidence in experimental methodology and results interpretation
- Low confidence in generalizability to arbitrary real-world datasets without signal-distractor decomposition

## Next Checks
1. Test VerT on a real-world dataset without ground truth signal-distractor decomposition to evaluate performance in practical scenarios
2. Conduct ablation studies removing either the data distillation or model distillation components to quantify their individual contributions
3. Evaluate model performance degradation when applied to datasets where signal and distractor features are not clearly separable