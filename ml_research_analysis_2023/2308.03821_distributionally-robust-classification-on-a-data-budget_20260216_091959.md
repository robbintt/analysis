---
ver: rpa2
title: Distributionally Robust Classification on a Data Budget
arxiv_id: '2308.03821'
source_url: https://arxiv.org/abs/2308.03821
tags:
- robustness
- dataset
- data
- trained
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates distributional robustness in image classification
  under limited data budgets. It introduces JANuS, a new dataset with images, labels,
  and captions, to enable controlled experiments comparing models trained with cross-entropy
  (CE) loss and vision-language (VL) loss objectives.
---

# Distributionally Robust Classification on a Data Budget

## Quick Facts
- arXiv ID: 2308.03821
- Source URL: https://arxiv.org/abs/2308.03821
- Authors: 
- Reference count: 40
- Primary result: CE-loss models can match VL-loss models in distributional robustness when trained on sufficient data

## Executive Summary
This paper investigates distributional robustness in image classification under limited data budgets. The authors introduce JANuS, a new dataset with images, labels, and captions, to enable controlled experiments comparing models trained with cross-entropy (CE) loss and vision-language (VL) loss objectives. Through ablation studies and a large-scale meta-analysis, they evaluate factors affecting robustness including dataset size, model architecture, and parameter count. The key finding challenges conventional wisdom by demonstrating that CE-loss models can achieve comparable robustness to VL-loss models like CLIP when trained on sufficient data, rather than requiring massive pretraining on vision-language pairs.

## Method Summary
The authors conduct controlled experiments using the JANuS dataset, which contains four subsets (ImageNet-100, OpenImages-100, LAION-100, YFCC-100) with images, labels, and captions. They train models from scratch using either CE or VL loss objectives, comparing different architectures (ResNet, ViT) and scaling approaches (parameter count, data budget). Models are evaluated on ImageNet distribution shifts (V2, Sketch, Rendition, Adversarial) to measure distributional robustness. The experimental design controls for confounding factors to isolate the effects of loss function, architecture, and data scale on robustness.

## Key Results
- CE-loss ResNet-50 trained on 2.4 million samples matches the robustness of CLIP ResNet-50 trained on 400 million samples
- Architecture choice has substantial impact on distributional robustness independent of data quantity
- Larger models and convolutional architectures show better robustness, but effects are limited on small data budgets
- Input image resolution scaling improves distributional robustness by providing more spatial context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CE models can match VL models in distributional robustness when trained on sufficient data
- Mechanism: CE models learn robust features from large-scale supervised data without needing language supervision, closing the robustness gap previously attributed to VL pre-training
- Core assumption: Robustness emerges primarily from data quantity and diversity rather than the specific training objective
- Evidence anchors:
  - [abstract]: "standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples"
  - [section]: "CE-loss models can achieve comparable robustness to VL-loss models (such as CLIP) when trained on sufficient data"

### Mechanism 2
- Claim: Architecture choice has substantial impact on distributional robustness independent of data quantity
- Mechanism: Certain architectures (like VOLO) inherently learn more robust representations than others (like VGG) even when controlling for training data
- Core assumption: Architectural inductive biases and attention mechanisms can directly improve robustness without requiring massive data scaling
- Evidence anchors:
  - [section]: "we see that increasing the parameter count has a positive effect on average robustness, but the effect is limited on small data budgets"
  - [section]: "we find that architecture can strongly impact distributional robustness, controlling for all other factors"

### Mechanism 3
- Claim: Input image resolution scaling improves distributional robustness
- Mechanism: Higher resolution inputs provide more spatial context and detail that helps models handle distribution shifts involving fine-grained visual features
- Core assumption: Distribution shifts often involve subtle visual differences that require higher resolution to capture effectively
- Evidence anchors:
  - [section]: "we plot average robustness against image resolution... we find that increasing input image resolution leads to gains in robustness on IN1000 and IN100-Dogs"
  - [corpus]: "No direct evidence in corpus about resolution-robustness relationship, suggesting this is a novel finding"

## Foundational Learning

- Concept: Distributional robustness
  - Why needed here: The paper's core contribution is showing how to achieve robust classification under distribution shifts with limited data
  - Quick check question: What distinguishes distributional robustness from standard generalization in machine learning?

- Concept: Vision-language (VL) models
  - Why needed here: The paper compares CE-loss models to VL-loss models like CLIP to understand what drives robustness
  - Quick check question: How do VL models like CLIP use text captions during training differently from standard image classifiers?

- Concept: Cross-entropy loss vs InfoNCE loss
  - Why needed here: The paper directly compares these two loss functions to determine their impact on robustness
  - Quick check question: What is the fundamental difference between how CE loss and InfoNCE loss optimize model parameters?

## Architecture Onboarding

- Component map: Input image → model backbone (ResNet/ViT/VOLO) → linear classification head (CE) OR text transformer (VL) → robustness evaluation on distribution shifts
- Critical path: Data loading → augmentation → model forward pass → loss computation → backpropagation → parameter update
- Design tradeoffs: Larger models improve robustness but require more data; higher resolution improves robustness but increases computation
- Failure signatures: CE models underperforming VL models on small datasets; ViTs underperforming ResNets on limited data
- First 3 experiments:
  1. Train CE ResNet-50 and VL ResNet-50 on IN100 at 1x scale, compare robustness
  2. Scale both models to 2x and 10x data, observe robustness trends
  3. Replace ResNet-50 with ViT-S-16, compare robustness across same data scales

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do vision-language models trained with fewer than 400 million samples compare to cross-entropy models trained on limited data in terms of distributional robustness?
- Basis in paper: [explicit] The paper shows that CE-loss models can achieve comparable robustness to VL-loss models when trained on sufficient data, but the comparison is limited to specific data scales (e.g., 2.4 million vs. 400 million samples).
- Why unresolved: The paper does not explore the robustness of VL models trained on data scales between 15 million and 400 million samples, leaving a gap in understanding their performance at intermediate scales.
- What evidence would resolve it: Training and evaluating VL models on datasets with 15 million to 400 million samples and comparing their robustness to CE models trained on similar scales.

### Open Question 2
- Question: What is the impact of fine-tuning on the distributional robustness of vision-language models, and how does it compare to training from scratch?
- Basis in paper: [explicit] The paper notes that fine-tuning VL models can erode distributional robustness, even as base accuracy improves, but does not explore the extent of this trade-off or alternative fine-tuning strategies.
- Why unresolved: The paper does not provide a comprehensive analysis of fine-tuning methods or their effects on robustness across different data scales and architectures.
- What evidence would resolve it: Systematic experiments comparing fine-tuned VL models to models trained from scratch across various data scales and architectures.

### Open Question 3
- Question: How does the choice of label set size (e.g., 100 classes vs. 1000 classes) affect the distributional robustness of cross-entropy models?
- Basis in paper: [explicit] The paper demonstrates that reducing the label set size improves robustness, but the analysis is limited to specific label sets and does not explore the full range of possible label set sizes.
- Why unresolved: The paper does not investigate the robustness of CE models on label sets with sizes between 100 and 1000 classes or on fine-grained classification tasks.
- What evidence would resolve it: Training and evaluating CE models on label sets of varying sizes (e.g., 200, 500, 800 classes) and analyzing their robustness on distribution shifts.

### Open Question 4
- Question: What is the role of dataset diversity in achieving distributional robustness, and how does it compare to dataset size?
- Basis in paper: [inferred] The paper highlights the importance of dataset size but does not explicitly address the role of diversity, though it is implied in the discussion of JANuS and other datasets.
- Why unresolved: The paper does not isolate the effects of dataset diversity from dataset size, leaving uncertainty about their relative contributions to robustness.
- What evidence would resolve it: Experiments comparing models trained on datasets with similar sizes but different levels of diversity, and vice versa, to quantify their impact on robustness.

### Open Question 5
- Question: How do recent advancements in vision transformer architectures (e.g., DeiT, VOLO) perform on distributional robustness when trained on limited data?
- Basis in paper: [explicit] The paper mentions that recent ViT architectures like DeiT and VOLO show improved robustness, but the analysis is limited to specific data scales and does not explore their performance on limited data.
- Why unresolved: The paper does not provide a detailed comparison of these architectures on limited data budgets or explore their potential to close the gap with convolutional models.
- What evidence would resolve it: Training and evaluating DeiT, VOLO, and other advanced ViT architectures on limited data budgets and comparing their robustness to convolutional models.

## Limitations

- The JANuS dataset covers only four domains and may not represent the diversity needed to claim universal principles about robustness
- Focus on ImageNet-based distribution shifts may not generalize to other domains like medical imaging or robotics
- Does not explore the impact of different optimization algorithms or learning rate schedules on robustness conclusions

## Confidence

**High Confidence**: Architecture choice significantly impacts distributional robustness (well-supported by ablation studies across multiple model families and data scales)
**Medium Confidence**: CE-loss models can match VL-loss models in robustness when trained on sufficient data (compelling but based on limited comparison between ResNet-50 architectures)
**Low Confidence**: Input resolution scaling improves distributional robustness (observed positive trends but lacks controlled experiments varying resolution independently)

## Next Checks

1. **Cross-Domain Validation**: Evaluate the CE-vs-VL robustness findings on non-ImageNet datasets (medical imaging, satellite imagery) to test domain generalization of the core claims

2. **Resolution Control Experiments**: Design experiments that isolate resolution effects by keeping model capacity and training data constant while varying only input resolution, to verify the claimed relationship

3. **Optimization Ablation**: Test different optimizers (AdamW, SGD with momentum) and learning rate schedules across the CE and VL training setups to determine if optimization choices confound the robustness comparisons