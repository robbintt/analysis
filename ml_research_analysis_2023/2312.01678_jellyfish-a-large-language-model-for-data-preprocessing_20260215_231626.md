---
ver: rpa2
title: 'Jellyfish: A Large Language Model for Data Preprocessing'
arxiv_id: '2312.01678'
source_url: https://arxiv.org/abs/2312.01678
tags:
- data
- tasks
- knowledge
- product
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Jellyfish is an open-source large language model designed as a
  universal task solver for data preprocessing (DP). Built on Llama 2-13B, it is instruction-tuned
  on datasets covering four typical DP tasks: error detection, data imputation, schema
  matching, and entity matching.'
---

# Jellyfish: A Large Language Model for Data Preprocessing

## Quick Facts
- **arXiv ID**: 2312.01678
- **Source URL**: https://arxiv.org/abs/2312.01678
- **Reference count**: 40
- **Primary result**: Jellyfish is an open-source LLM for data preprocessing that outperforms non-LLM methods and rivals GPT models on seen tasks while showing good generalization to unseen tasks.

## Executive Summary
Jellyfish is an open-source large language model designed as a universal task solver for data preprocessing. Built on Llama 2-13B and instruction-tuned on datasets covering four typical DP tasks (error detection, data imputation, schema matching, and entity matching), Jellyfish demonstrates strong performance on seen DP tasks while generalizing to unseen tasks like column type annotation and attribute value extraction. The model operates on a single GPU, ensuring data security and enabling further tuning, while its interpreter provides enhanced reasoning capabilities compared to GPT-3.5.

## Method Summary
Jellyfish was developed through a two-stage process: pre-training on FLAN and Open-Platypus datasets to enhance reasoning and STEM capabilities, followed by DP-tuning on four representative DP tasks using instruction tuning with knowledge injection. The model employs instance serialization to convert raw data into structured prompts and uses LoRA for efficient fine-tuning. A separate interpreter model was developed to provide reasoning explanations for DP results, leveraging techniques like few-shot prompting and chain-of-thought reasoning.

## Key Results
- Jellyfish outperforms non-LLM methods and rivals GPT-3.5/4 on seen DP tasks (error detection, data imputation, schema matching, entity matching)
- Demonstrates good generalization to unseen tasks like column type annotation and attribute value extraction
- Interpreter model provides enhanced reasoning capabilities compared to GPT-3.5
- Operates efficiently on a single GPU while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning on diverse DP tasks enables Jellyfish to generalize to unseen tasks
- Mechanism: The model learns general DP patterns and reasoning strategies during instruction tuning, which can be applied to new tasks without task-specific training
- Core assumption: DP tasks share underlying reasoning patterns that can be transferred
- Evidence anchors: [abstract] "Jellyfish showcases several notable features: a universal model design, moderate size, assurance of data security, feasibility for further tuning, natural language instruction handling, optional specification of prior knowledge, and model interpretability" [section] "We select a collection of datasets across four representative DP tasks and construct instruction tuning data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP"
- Break condition: If unseen tasks require fundamentally different reasoning that wasn't present in the training tasks

### Mechanism 2
- Claim: Instance serialization converts raw data into effective LLM prompts
- Mechanism: The instance serializer automatically translates raw data into structured prompts that include task descriptions, instance content, and injected knowledge
- Core assumption: LLMs can effectively process serialized data instances as prompts
- Evidence anchors: [section] "Jellyfish uses an instance serializer that iterates through all the instances and transforms each instance to a prompt" [section] "The prompt contains the task description, the instance content, and any injected knowledge"
- Break condition: If prompt length exceeds model token limits or if serialization loses critical information

### Mechanism 3
- Claim: Knowledge injection enhances model performance on both seen and unseen datasets
- Mechanism: Task- and dataset-specific knowledge is infused into instruction data during tuning and can be optionally specified during inference
- Core assumption: Adding domain-specific knowledge improves model understanding of task requirements
- Evidence anchors: [section] "Jellyfish is equipped with an instance serializer, which automatically translates raw data into model prompts, and a knowledge injector, which optionally introduces task- and dataset-specific knowledge to enhance DP performance" [section] "Unlike many existing methods that rely heavily on prior knowledge [71, 69], Jellyfish acquires domain knowledge during its tuning process and uses optional knowledge injection during inference" [section] "Table 10 report the results. Comparing the base model with those with pre-tuning, it can be seen that OpenOrcaxOpenChat-Preview2-13B, pre-tuned with augmented FLAN, roughly performs better"
- Break condition: If injected knowledge is incorrect or conflicts with model's learned understanding

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: Converts base LLM into a DP task solver by training on (instruction, output) pairs
  - Quick check question: What is the difference between instruction tuning and traditional fine-tuning?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables efficient fine-tuning by modifying weight matrices without updating all parameters
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning?

- Concept: Prompt engineering
  - Why needed here: Improves model performance through techniques like few-shot prompting and chain-of-thought
  - Quick check question: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting?

## Architecture Onboarding

- Component map: Llama 2-13B -> Instance Serializer -> Knowledge Injector -> Jellyfish-13B -> DP result -> Jellyfish-13B-Interpreter -> Explanation

- Critical path: 1. Raw data → Instance serializer → Prompt 2. Prompt + optional knowledge → Jellyfish-13B → DP result 3. DP result → Jellyfish-13B-interpreter → Explanation

- Design tradeoffs:
  - Model size vs. inference speed: 13B parameters balance performance and GPU requirements
  - Token limits vs. comprehensive context: Single-instance processing avoids context loss
  - Open-source vs. proprietary: Local deployment enables data security

- Failure signatures:
  - Poor performance on unseen tasks: Check if training tasks cover necessary reasoning patterns
  - Hallucinations in outputs: Verify knowledge injection and prompt quality
  - Slow inference: Monitor GPU memory usage and consider quantization

- First 3 experiments:
  1. Test instance serialization on sample data to verify prompt quality
  2. Validate knowledge injection effectiveness on one seen dataset
  3. Compare performance with and without few-shot prompting on unseen task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Jellyfish's performance on unseen tasks compare to task-specific models fine-tuned on those tasks?
- Basis in paper: [inferred] The paper shows Jellyfish performs well on unseen tasks like CTA and AVE, but doesn't compare to models specifically fine-tuned on these tasks.
- Why unresolved: The paper focuses on comparing Jellyfish to non-LLM methods and LLM methods like GPT-3/4, but doesn't benchmark against specialized models for the unseen tasks.
- What evidence would resolve it: Experiments comparing Jellyfish's performance on CTA and AVE to state-of-the-art models specifically designed and fine-tuned for these tasks.

### Open Question 2
- Question: How does the size of Jellyfish (13B parameters) impact its performance compared to larger models like GPT-4 (1.76T parameters) on DP tasks?
- Basis in paper: [explicit] The paper mentions Jellyfish has 13B parameters and outperforms GPT-3/3.5 but not GPT-4 on some tasks.
- Why unresolved: The paper doesn't analyze how model size affects performance, especially since Jellyfish is much smaller than GPT-4 but achieves comparable results on some tasks.
- What evidence would resolve it: A detailed analysis of performance vs. model size across different DP tasks, including scaling experiments with larger Jellyfish variants.

### Open Question 3
- Question: How does Jellyfish's reasoning capability (via the interpreter model) compare to other LLM-based interpretability methods for DP tasks?
- Basis in paper: [explicit] The paper introduces a separate interpreter model and shows it outperforms GPT-3.5 in providing explanations for DP results.
- Why unresolved: The paper doesn't compare Jellyfish's interpretability approach to other LLM interpretability techniques or evaluate the quality of explanations objectively.
- What evidence would resolve it: A comprehensive comparison of Jellyfish's interpretability to other LLM-based explanation methods, including human evaluations of explanation quality.

## Limitations

- Limited evaluation of generalization capabilities to unseen tasks (only 2 tasks tested)
- Insufficient detail on knowledge injection methodology and knowledge selection criteria
- Absence of direct comparisons with GPT-4 despite claims of rivaling GPT models

## Confidence

**High Confidence**: The core mechanism of instruction tuning on DP tasks is well-established and the technical implementation details are sufficiently specified for reproduction. The performance improvements on seen tasks are reliably demonstrated through comprehensive comparisons with both traditional and LLM-based methods.

**Medium Confidence**: The generalization capabilities to unseen tasks are supported by experimental evidence but require further validation across a broader range of DP scenarios. The knowledge injection benefits are observed but the specific mechanisms and knowledge selection criteria remain underspecified.

**Low Confidence**: Claims about reasoning capabilities and interpretability require additional validation, as the comparison methodology and evaluation criteria for these aspects are not fully detailed in the paper.

## Next Checks

1. **Knowledge Injection Validation**: Conduct ablation studies comparing Jellyfish performance with varying levels of knowledge injection (none, task-specific, dataset-specific) on a subset of seen tasks to quantify the actual contribution of knowledge injection versus instruction tuning alone.

2. **Generalization Stress Test**: Evaluate Jellyfish on at least 5-10 additional unseen DP tasks spanning different domains and complexity levels, including tasks requiring multi-step reasoning and those involving unstructured data formats beyond the current scope.

3. **Interpretability Benchmark**: Develop a standardized evaluation framework for comparing reasoning capabilities between Jellyfish-interpreter and GPT-3.5, including metrics for explanation quality, consistency, and computational efficiency across diverse DP scenarios.