---
ver: rpa2
title: 'SpaDen : Sparse and Dense Keypoint Estimation for Real-World Chart Understanding'
arxiv_id: '2308.01971'
source_url: https://arxiv.org/abs/2308.01971
tags:
- chart
- data
- line
- loss
- learnt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extracting chart data from
  images, specifically focusing on reconstructing the data used to generate various
  chart types (line, bar, scatter, box plots). The authors propose a novel bottom-up
  approach called SpaDen (Sparse and Dense Keypoint Estimation) that learns to detect
  keypoints in chart images, which are then used to reconstruct the chart components.
---

# SpaDen : Sparse and Dense Keypoint Estimation for Real-World Chart Understanding

## Quick Facts
- arXiv ID: 2308.01971
- Source URL: https://arxiv.org/abs/2308.01971
- Reference count: 25
- Key outcome: SpaDen achieves F1 score of 0.83 for element detection and 0.69 for data extraction on line charts.

## Executive Summary
This paper introduces SpaDen, a bottom-up approach for extracting chart data from images by detecting and fusing sparse and dense keypoints. The method learns to identify discrete points (data points, corners) and continuous elements (lines, bars) through combined heatmap predictions, then uses contrastive learning to cluster related keypoints and match them to legend elements. SpaDen demonstrates state-of-the-art performance on real-world chart data extraction tasks, outperforming existing methods on both element detection and data series extraction across multiple chart types.

## Method Summary
SpaDen combines sparse keypoint heatmaps for discrete elements with dense directional heatmaps for continuous elements, using a self-attention-based fusion layer. The model employs a multi-head architecture with five different views (binary reconstruction, foreground/background regression and classification) and is trained with a weighted loss combining keypoint localization (70%), contrastive learning for clustering (20%), and chart type classification (10%). Text invariance augmentation is applied during training to improve robustness to varying text content. Post-processing uses clustering algorithms to group keypoints from the same chart elements and ROI-align for legend mapping.

## Key Results
- F1 score of 0.83 for element detection (Task 6a) and 0.69 for data extraction (Task 6b) on line charts
- Outperforms baseline HGN model with only sparse keypoints (F1 of 0.76 for element detection)
- Demonstrates superior performance across multiple chart types (line, bar, scatter, box plots) on the Chart-Infographics Challenge Dataset
- Shows robustness to text variations through ablation studies with easy/hard samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusion of sparse and dense keypoint predictions improves accuracy over single modality.
- Mechanism: Dual representation captures discrete points (exact locations) and continuous contours (complete element shapes). Sparse heatmaps provide classification for discrete points while dense heatmaps enable regression for contour prediction.
- Core assumption: Chart elements can be represented as discrete points and continuous contours, with joint learning providing complementary information.
- Evidence anchors: Abstract states novelty in detecting fusion of continuous and discrete keypoints; section describes creating dense directional masks by interpolating between inflection points.

### Mechanism 2
- Claim: Multi-task learning with multiple loss functions provides robust unconstrained keypoint detection.
- Mechanism: Weighted combination of L2 custom loss for keypoints (70%), L1 distance loss for offset, and contrastive loss for clustering (20%) with chart type classification (10%).
- Core assumption: Different loss functions capture different aspects of chart structure, with combination providing more complete learning signal.
- Evidence anchors: Section details final loss as alpha blend of 0.7× aggregated KP loss, 0.2× contrastive loss, and 0.1× chart type classification.

### Mechanism 3
- Claim: Text invariance augmentation improves robustness to text variations in real-world charts.
- Mechanism: 25% chance of replacing text boxes with median chart color and 25% chance of adding skewed contextual text during training forces model to focus on chart elements rather than text.
- Core assumption: Real-world charts have varying text content and placement; model needs to learn text-invariant keypoint features.
- Evidence anchors: Section describes augmentation strategy with 25% probability for text replacement and addition.

## Foundational Learning

- Concept: Keypoint estimation and heatmap regression
  - Why needed here: Core task requires detecting specific points corresponding to data elements, fundamentally a keypoint estimation problem.
  - Quick check question: How does heatmap regression differ from direct coordinate regression in keypoint estimation tasks?

- Concept: Contrastive learning and metric learning
  - Why needed here: Uses contrastive loss to cluster keypoints from same chart element and separate different elements, essential for reconstructing chart data.
  - Quick check question: What is the difference between push-pull loss and multi-similarity loss in contrastive learning?

- Concept: Object detection as keypoint detection
  - Why needed here: Treats chart element detection as keypoint detection rather than bounding box detection for more precise localization.
  - Quick check question: How does the anchor-free object detection paradigm apply to chart data extraction?

## Architecture Onboarding

- Component map: Input chart image → Backbone feature extraction (HGN/CPN/SPN variants) → Chart type classification + Keypoint localization (5 heads) → Post-processing and keypoint clustering → Legend mapping (ROI-align) → Data extraction output
- Critical path: Input → Backbone → Classification + Localization → Post-processing/Clustering → Legend mapping → Output
- Design tradeoffs: Choice between backbone architectures involves accuracy vs computational efficiency tradeoffs; dual sparse/dense predictions increase complexity but improve accuracy for different element types.
- Failure signatures: Poor line chart performance indicates dense keypoint regression issues; legend matching failures suggest ROI-align or contrastive learning problems; text sensitivity indicates inadequate augmentation.
- First 3 experiments:
  1. Test baseline HGN backbone with only sparse keypoints on simple line chart dataset
  2. Add dense directional keypoint masks and evaluate improvement on continuous elements
  3. Implement contrastive learning component and test clustering effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SpaDen be further optimized for different chart types?
- Basis in paper: Explicit - authors mention HGN+CP+DLA performed best for line charts while IIT CVIT model performed best for box charts, but don't provide detailed optimization analysis per chart type.
- Why unresolved: Paper provides comparative analysis but not specific optimization strategies for each chart type.
- What evidence would resolve it: Further research with SpaDen focusing on optimizing architecture, loss functions, and post-processing per chart type.

### Open Question 2
- Question: How can SpaDen handle more complex chart types and layouts like multi-panel charts or overlapping elements?
- Basis in paper: Inferred - authors mention Chart-Infographics Challenge Dataset includes multi-panel charts but focus on single-panel charts; don't discuss overlapping elements or complex layouts.
- Why unresolved: Paper primarily focuses on single-panel charts without exploring more complex chart types.
- What evidence would resolve it: Developing and testing SpaDen on complex chart types and layouts, evaluating performance with overlapping elements and multi-panel charts.

### Open Question 3
- Question: How can SpaDen adapt to charts with varying noise, distortion, or low-quality images?
- Basis in paper: Inferred - authors mention easy/hard samples by adding/removing text boxes but don't discuss varying noise, distortion, or image quality.
- Why unresolved: Paper doesn't provide comprehensive analysis of model performance under different image quality conditions.
- What evidence would resolve it: Conducting experiments with SpaDen using charts with varying noise, distortion, or low-quality images and evaluating performance.

## Limitations
- Performance metrics reported on specific dataset without clear test set composition or cross-validation details
- Limited ablation studies make it difficult to isolate contribution of individual components
- Text invariance augmentation lacks detailed analysis of impact across different chart types and text densities

## Confidence
- **High**: Fundamental approach of combining sparse/dense keypoints is well-grounded; overall architecture clearly described
- **Medium**: Specific performance metrics (F1 scores of 0.83 and 0.69) likely accurate but may not generalize beyond tested dataset
- **Low**: Relative importance of individual architectural choices (backbone selection, loss weightings, augmentation) not rigorously validated

## Next Checks
1. **Ablation Study**: Systematically test SpaDen with only sparse keypoints, only dense keypoints, and various loss weight combinations to quantify contribution of each component.
2. **Generalization Test**: Evaluate model on charts from different domains (scientific papers vs business presentations) with varying text densities to assess robustness claims.
3. **Failure Analysis**: Conduct detailed error analysis on charts where SpaDen underperforms, examining whether failures stem from backbone architecture, loss function imbalance, or post-processing thresholds.