---
ver: rpa2
title: Risk-sensitive Actor-free Policy via Convex Optimization
arxiv_id: '2307.00141'
source_url: https://arxiv.org/abs/2307.00141
tags:
- policy
- learning
- risk
- risk-sensitive
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a risk-sensitive actor-free policy for reinforcement
  learning that uses Conditional Value at Risk (CVaR) as a safety criterion. The method
  models the risk-sensitive objective function using an input-convex neural network
  (PICNN), which ensures convexity with respect to the actions and enables identification
  of globally optimal actions through gradient descent.
---

# Risk-sensitive Actor-free Policy via Convex Optimization

## Quick Facts
- arXiv ID: 2307.00141
- Source URL: https://arxiv.org/abs/2307.00141
- Reference count: 7
- This paper proposes a risk-sensitive actor-free policy for reinforcement learning using CVaR as a safety criterion

## Executive Summary
This paper introduces an actor-free policy method for risk-sensitive reinforcement learning that uses Conditional Value at Risk (CVaR) as a safety criterion. The method models the risk-sensitive objective function using an input-convex neural network (PICNN), which ensures convexity with respect to actions and enables identification of globally optimal actions through gradient descent. The policy eliminates the need for incremental actor learning, reducing training instability. Experiments on a water tank level control task demonstrate that the proposed method maintains better risk control compared to CVaR-TD3, with lower variance in training and more conservative behavior in evaluation.

## Method Summary
The method parameterizes the risk-sensitive objective function using an input-convex neural network (PICNN), which guarantees convexity with respect to actions. This enables finding globally optimal actions through simple gradient descent without requiring a separate actor network. The approach models the cost-return distribution as Gaussian and uses CVaR to incorporate risk sensitivity, penalizing infrequent catastrophic events. The policy is trained using a distributional critic that estimates the mean and variance of cost-to-go, with a penalty formulation to handle safety constraints.

## Key Results
- Maintains better risk control compared to CVaR-TD3 in water tank level control task
- Lower variance in training process compared to actor-critic methods
- More conservative behavior in evaluation, keeping water level farther from critical levels
- Eliminates need for incremental actor learning and associated hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The actor-free policy enables globally optimal actions by leveraging convexity in the input space.
- Mechanism: The method parameterizes the risk-sensitive objective function using an input-convex neural network (PICNN), which guarantees convexity with respect to the actions. This ensures that gradient-following methods can identify the globally optimal action directly.
- Core assumption: The risk-sensitive objective function can be modeled as a convex function of the actions when using a PICNN architecture.
- Evidence anchors:
  - [abstract] "The risk-sensitive objective function is modeled using an input-convex neural network ensuring convexity with respect to the actions and enabling the identification of globally optimal actions through simple gradient-following methods."
  - [section 2.3] "With our actor-free approach, the policy aligns optimally with the approximated criterion. Key to our approach is to parameterize the risk-sensitive objective function using an input-convex neural network[Amos et al., 2017], ensuring convexity with respect to the actions (inputs). Consequently, simple gradient-following techniques can be used to find a globally optimal action."
- Break condition: If the PICNN architecture fails to maintain convexity with respect to actions, the globally optimal action cannot be guaranteed.

### Mechanism 2
- Claim: CVaR provides a risk-sensitive criterion that penalizes infrequent catastrophic events.
- Mechanism: The method replaces the expectation in safety violation costs with the Conditional Value at Risk (CVaR), which quantifies the amount of tail risk by emphasizing the worst α-percentile cases.
- Core assumption: The distribution of cost-return can be modeled as a Gaussian distribution, allowing for a closed-form CVaR measure.
- Evidence anchors:
  - [section 2.2] "To incorporate risk, we replace the expectation in the safety violation cost with the Conditional Value at Risk (CVaR) [Artzner et al. , 1999 ], a widely recognized risk measure that quantifies the amount of tail risk. More precisely, CVaRα is defined as the expected reward of the worst α-percentile cases..."
  - [section 2.2] "Instead, we follow [Tang et al., 2020] and model the distribution of cost-return C(s, a) as a Gaussian distribution N(Qπc(s, a), σ²c(s, a)) where Qπc(s, a) = Eπ[Σ∞i=t γi−tc(si, ai) | st = s, at = a] is the expected future cost-return and σ²c(s, a) its variance."
- Break condition: If the Gaussian assumption for cost-return distribution is invalid, the closed-form CVaR calculation may be inaccurate.

### Mechanism 3
- Claim: The actor-free architecture eliminates the need for incremental actor learning, reducing training instability.
- Mechanism: By defining the action as the solution to a convex optimization problem, the method avoids the need for a separate actor network that requires hyperparameter tuning and stabilization tricks.
- Core assumption: The optimal action can be directly computed as the solution to a convex optimization problem without requiring a separate actor network.
- Evidence anchors:
  - [abstract] "This eliminates the need for incremental actor learning, which often necessitates hyperparameter tuning and tricks to stabilize the training process."
  - [section 2.3] "However, if the optimal action w.r.t the critic can be easily identified, the need for modeling the actor is eliminated. This can be achieved through parameterization of the reward with Partially Input Convex Neural Networks (PICNNs) [Amos et al., 2017]."
- Break condition: If the convex optimization problem becomes computationally intractable or the PICNN fails to approximate the risk-sensitive criterion accurately, the actor-free approach may not be feasible.

## Foundational Learning

- Concept: Input-convex neural networks (PICNNs)
  - Why needed here: PICNNs ensure convexity with respect to the actions, which is crucial for finding globally optimal actions through gradient descent.
  - Quick check question: What property of PICNNs allows for globally optimal action identification?
- Concept: Conditional Value at Risk (CVaR)
  - Why needed here: CVaR provides a risk-sensitive criterion that penalizes infrequent catastrophic events, making the policy more robust to tail risks.
  - Quick check question: How does CVaR differ from the expected value in terms of risk sensitivity?
- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: CMDPs provide a framework for incorporating safety constraints into the reinforcement learning problem, ensuring compliance with safety thresholds.
  - Quick check question: What is the primary goal of the agent in a CMDP framework?

## Architecture Onboarding

- Component map: State and action inputs -> PICNN -> Risk-sensitive objective value -> Gradient descent -> Optimal action
- Critical path:
  1. State and action inputs to PICNN
  2. PICNN outputs the risk-sensitive objective value
  3. Gradient descent on the PICNN output to find optimal action
  4. Distributional critic updates to estimate cost-to-go distribution
- Design tradeoffs:
  - Convexity vs. expressiveness: PICNNs ensure convexity but may limit the expressiveness of the model
  - Gaussian assumption vs. accuracy: Modeling cost-return as Gaussian allows for closed-form CVaR but may not capture all distributions accurately
  - Actor-free vs. stability: Eliminating the actor network reduces training instability but may increase computational complexity
- Failure signatures:
  - Poor convergence or suboptimal actions may indicate that the PICNN is not maintaining convexity
  - High variance in training may suggest issues with the distributional critic or Gaussian assumption
  - Inability to find optimal actions may point to problems with the gradient descent algorithm or PICNN architecture
- First 3 experiments:
  1. Verify convexity of PICNN with respect to actions using a simple test case
  2. Compare CVaR-based policy with expected value-based policy on a toy environment
  3. Evaluate the impact of different risk levels (α) on the learned policy's behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform with other distributions beyond the Gaussian distribution for modeling the cost-return distribution?
- Basis in paper: [explicit] The authors mention that future research could explore a more general distribution beyond the Gaussian distribution used in their current work.
- Why unresolved: The paper only evaluates the method using a Gaussian distribution for the cost-return distribution, so the performance with other distributions is unknown.
- What evidence would resolve it: Empirical results comparing the performance of the proposed method using different distributions (e.g., t-distribution, mixture of Gaussians) for modeling the cost-return distribution on various tasks.

### Open Question 2
- Question: What is the impact of the hyperparameter κ (penalty coefficient) on the performance and safety of the learned policy?
- Basis in paper: [inferred] The paper uses a penalty formulation to handle the safety constraint, with κ as a hyperparameter. The impact of κ on the trade-off between performance and safety is not explored in detail.
- Why unresolved: The paper does not provide a sensitivity analysis of the performance and safety of the learned policy with respect to different values of κ.
- What evidence would resolve it: Empirical results showing the performance and safety of the learned policy with different values of κ on various tasks, and an analysis of the trade-off between performance and safety.

### Open Question 3
- Question: How does the proposed method scale to high-dimensional state and action spaces?
- Basis in paper: [inferred] The paper only evaluates the method on a relatively simple continuous control task (water tank level control). The scalability of the method to high-dimensional state and action spaces is not addressed.
- Why unresolved: The paper does not provide any results or analysis on the scalability of the method to high-dimensional state and action spaces.
- What evidence would resolve it: Empirical results comparing the performance of the proposed method with other state-of-the-art methods on tasks with high-dimensional state and action spaces, such as robotic manipulation or locomotion tasks.

## Limitations

- The method relies on the assumption that the risk-sensitive objective function can be accurately modeled as convex with respect to actions using PICNNs, which may limit expressiveness.
- The Gaussian assumption for modeling cost-return distributions may not capture all real-world risk scenarios accurately.
- The actor-free approach may introduce computational overhead during policy execution due to the need for convex optimization at each time step.

## Confidence

- High confidence: The theoretical framework of using PICNNs for actor-free policy and the CVaR-based risk criterion are well-established in the literature and mathematically sound.
- Medium confidence: The experimental results showing improved risk control in the water tank task are promising but limited to a single benchmark problem.
- Low confidence: The scalability of this approach to high-dimensional continuous control tasks and the practical computational efficiency compared to traditional actor-critic methods remain uncertain.

## Next Checks

1. Test the PICNN's ability to maintain convexity across a range of different reward landscapes and risk scenarios to verify the robustness of the convexity guarantee.
2. Evaluate the policy's performance on additional continuous control benchmarks (e.g., OpenAI Gym MuJoCo tasks) to assess scalability and generalizability.
3. Compare the computational efficiency of the actor-free approach against traditional actor-critic methods in terms of wall-clock training time and policy evaluation latency.