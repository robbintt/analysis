---
ver: rpa2
title: Learning Action Embeddings for Off-Policy Evaluation
arxiv_id: '2305.03954'
source_url: https://arxiv.org/abs/2305.03954
tags:
- embeddings
- mips
- action
- reward
- learned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores learning action embeddings for marginalized
  inverse propensity scoring (MIPS) in off-policy evaluation. The key challenge addressed
  is the high variance of IPS estimators in large action spaces, particularly when
  certain actions are under-explored by the logging policy.
---

# Learning Action Embeddings for Off-Policy Evaluation

## Quick Facts
- arXiv ID: 2305.03954
- Source URL: https://arxiv.org/abs/2305.03954
- Reference count: 36
- Primary result: Learned action embeddings reduce variance in marginalized IPS by grouping similar actions

## Executive Summary
This work addresses the high variance problem in inverse propensity scoring (IPS) for off-policy evaluation when dealing with large action spaces. While marginalized IPS (MIPS) can reduce variance by using predefined action embeddings, defining such embeddings is often difficult in practice. The authors propose learning action embeddings directly from logged data using intermediate outputs of a trained reward model. These learned embeddings are then used in MIPS to estimate policy performance. Experiments on synthetic and real-world datasets show consistent improvements over standard baselines including IPS, direct method (DM), doubly robust (DR), and MIPS with predefined embeddings.

## Method Summary
The method learns action embeddings from logged contextual bandit data by training a reward model and extracting intermediate outputs as action representations. These embeddings are used in marginalized IPS (MIPS) by estimating propensity weights based on embedding similarity rather than exact action identity. The approach can use any model class for reward prediction and extends MIPS to applications where predefined embeddings are unavailable. The method is theoretically connected to kernel regression when using linear discriminant analysis or logistic regression for propensity estimation.

## Key Results
- Learned MIPS consistently outperforms standard IPS, DM, DR, and MIPS with predefined embeddings
- The method reduces variance in high-dimensional action spaces by grouping similar actions
- Performance is robust to reward model misspecification - useful embeddings can be learned even when the model cannot accurately predict rewards
- Experiments show optimal embedding dimensionality between 8-16 dimensions for the tested problems

## Why This Works (Mechanism)

### Mechanism 1
Learning action embeddings from reward model intermediate outputs reduces variance in marginalized IPS by grouping similar actions. The method learns a mapping from actions to low-dimensional embeddings using a reward model's intermediate outputs. These embeddings are then used in MIPS, where propensity weights are calculated based on embedding similarity rather than exact action identity. Core assumption: Actions that are "similar" in terms of their reward impact can be grouped in a lower-dimensional embedding space without significant loss of information.

### Mechanism 2
The learned embeddings allow kernel regression interpretation, connecting MIPS to kernelized direct method. When using linear discriminant analysis or logistic regression for propensity estimation, the MIPS estimator becomes equivalent to kernel regression in the learned embedding space. Core assumption: The classifier used for propensity estimation (LDA or logistic regression) recovers the embedding distribution structure.

### Mechanism 3
Learning embeddings from reward signal is more robust to model misspecification than direct reward prediction. Even when the reward model cannot accurately predict rewards end-to-end, the learned embeddings capture the relative ordering of contexts for each action, which is sufficient for MIPS. Core assumption: The embedding learning captures the structure of the reward function even when absolute predictions are inaccurate.

## Foundational Learning

- Concept: Inverse Propensity Scoring (IPS)
  - Why needed here: IPS is the baseline method that MIPS extends, and understanding its variance issues is crucial for appreciating the proposed solution.
  - Quick check question: Why does IPS have high variance when certain actions are under-explored by the logging policy?

- Concept: Marginalized IPS (MIPS)
  - Why needed here: The proposed method builds directly on MIPS by learning embeddings instead of using predefined ones.
  - Quick check question: How does MIPS reduce variance compared to standard IPS?

- Concept: Kernel Regression
  - Why needed here: The paper shows the proposed method can be interpreted as kernel regression in learned embedding space.
  - Quick check question: What is the relationship between kernel regression and the propensity weighting in MIPS?

## Architecture Onboarding

- Component map: Logging data → Reward model training → Embedding extraction → Propensity estimation → MIPS estimation
- Critical path: Reward model training → Embedding extraction → Propensity estimation → MIPS estimation
- Design tradeoffs: Embedding dimensionality vs. variance reduction, Model complexity vs. embedding quality, Whether to use predefined embeddings vs. learning from scratch
- Failure signatures: High variance in estimates suggests poor embedding quality, High bias suggests embedding space doesn't capture reward-relevant action differences, Performance similar to IPS suggests embeddings not improving upon direct action identity
- First 3 experiments: 1) Compare Learned MIPS with IPS on synthetic data with varying action space sizes, 2) Test performance with different embedding dimensionalities on synthetic data, 3) Evaluate on real-world dataset with different sample sizes and compare to baselines

## Open Questions the Paper Calls Out
- What is the optimal number of dimensions for learned action embeddings to minimize the bias-variance trade-off in MIPS?
- How do learned action embeddings perform in contextual bandits with continuous action spaces?
- How sensitive is the method to the choice of model class for reward prediction when learning action embeddings?
- How does the method perform in list-wise recommendation settings where the action is a ranked list of items?
- What is the impact of using different classifiers (besides logistic regression and LDA) to estimate action embedding propensities?

## Limitations
- Theoretical analysis connecting Learned MIPS to kernel regression relies on specific assumptions about the propensity classifier
- Experimental validation is limited to relatively simple synthetic settings and one real-world dataset
- The method still requires sufficient exploration of action space by logging policy to learn useful embeddings

## Confidence
- **High Confidence**: The core empirical finding that Learned MIPS outperforms standard IPS in high-dimensional action spaces
- **Medium Confidence**: The interpretation of Learned MIPS as kernel regression is mathematically sound but relies on specific classifier choices
- **Medium Confidence**: The claim that Learned MIPS provides an alternative to DR for combining low variance of DM with low bias of IPS

## Next Checks
1. Systematically vary the complexity gap between true reward function and reward model architecture to test the claimed robustness of Learned MIPS to model misspecification
2. Evaluate Learned MIPS on additional real-world datasets with different characteristics to assess generalization beyond the fashion recommendation dataset
3. Conduct a more thorough analysis of the trade-off between embedding dimensionality and estimation quality, particularly examining when the learned embeddings stop providing benefits over standard MIPS