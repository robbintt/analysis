---
ver: rpa2
title: 'VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation'
arxiv_id: '2312.14867'
source_url: https://arxiv.org/abs/2312.14867
tags:
- image
- human
- editing
- generation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VIEScore, a visual instruction-guided explainable
  metric for evaluating conditional image generation tasks. VIEScore leverages multimodal
  large language models (MLLMs) to provide both a score and rationale for image evaluation
  without requiring training or fine-tuning.
---

# VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation

## Quick Facts
- arXiv ID: 2312.14867
- Source URL: https://arxiv.org/abs/2312.14867
- Authors: 
- Reference count: 22
- Primary result: VIEScore uses MLLMs to provide explainable evaluation scores for conditional image synthesis tasks with correlation to human judgments

## Executive Summary
VIEScore introduces a novel approach to evaluating conditional image synthesis by leveraging multimodal large language models (MLLMs) as general-purpose evaluators. The framework generates both quantitative scores and qualitative rationales for synthetic images across seven different image synthesis tasks without requiring task-specific training or fine-tuning. By using instruction-guided MLLMs, VIEScore aims to provide explainable evaluations that align with human judgment while offering interpretable reasoning for each score component.

The method was evaluated on the ImagenHub human evaluation dataset, comparing GPT-4v against open-source MLLMs across tasks including text-guided generation, image editing, and subject-driven synthesis. Results show that while GPT-4v achieves reasonable correlation with human ratings (0.3 Spearman), open-source alternatives perform significantly worse. The framework demonstrates potential for explainable evaluation but reveals challenges particularly in image editing tasks where subtle changes are harder to detect.

## Method Summary
VIEScore processes synthetic images and their associated conditions through MLLMs using carefully crafted task-specific prompts. The method requires only one pass through the MLLM to generate both scores (for semantic consistency, perceptual quality, and overall) and natural language rationales. The framework uses in-context learning but observes that this can sometimes decrease performance as MLLMs may focus on example images rather than the target evaluation. The approach is evaluated across seven image synthesis tasks using the ImagenHub human evaluation dataset with approximately 100-200 conditional inputs per task.

## Key Results
- GPT-4v achieved Spearman correlation of 0.3 with human evaluations (human-to-human correlation: 0.45)
- Open-source MLLMs performed significantly worse than GPT-4v in all tasks
- VIEScore showed comparable correlation with human ratings in generation tasks but struggled in editing tasks
- The framework provides natural language rationales alongside scores, offering explainability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VIEScore leverages MLLMs' general knowledge to evaluate image synthesis without task-specific training.
- Mechanism: The framework uses instruction-guided MLLMs to provide both scores and rationales by interpreting images and conditions, replacing the need for task-specific metric design.
- Core assumption: MLLMs possess sufficient visual reasoning capability to assess synthetic images across diverse tasks.
- Evidence anchors:
  - [abstract] "VIEScore leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning."
  - [section 4] "Our work uses MLLM to access all the discussed tasks on synthetic image evaluation."
- Break condition: If MLLMs cannot reliably interpret task-specific instructions or produce consistent scores across different tasks, the method's generality fails.

### Mechanism 2
- Claim: VIEScore provides explainability through natural language rationales alongside scores.
- Mechanism: The MLLM generates detailed reasoning in natural language explaining the basis for each score component, enhancing interpretability compared to traditional metrics.
- Core assumption: Natural language explanations are sufficient for humans to understand and trust the evaluation process.
- Evidence anchors:
  - [abstract] "The function should produce the intermediate rationale in the form of natural language before generating the final score according to the prompt instruction"
  - [section 1] "VIES CORE can offer the rationale in the form of natural languages to help humans understand the reasoning process."
- Break condition: If the rationales are inconsistent, vague, or don't align with the scores, the explainability benefit diminishes.

### Mechanism 3
- Claim: VIEScore's correlation with human evaluations demonstrates its effectiveness as an evaluator.
- Mechanism: By measuring Spearman correlation between VIEScore and human ratings across multiple tasks, the paper shows GPT-4v achieves correlations around 0.3, indicating reasonable alignment with human judgment.
- Core assumption: Human evaluations serve as a reliable ground truth for image synthesis quality assessment.
- Evidence anchors:
  - [section 5.1] "GPT4v and LLaV A were able to follow our instructions clearly while other MLLMs were not able to produce any meaningful results"
  - [section 5.1] "From overall performance, we found that GPT-4v reports a significantly higher correlation than LLaV A, while LLaV A's correlation is much less than human raters."
- Break condition: If human-to-human correlation is high but VIEScore correlation is consistently low across tasks, the method's validity as a human-like evaluator is questionable.

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: VIEScore depends on MLLMs' ability to process both images and text instructions simultaneously.
  - Quick check question: What are the key differences between traditional language models and multimodal models in terms of input capabilities?

- Concept: Spearman correlation for evaluation
  - Why needed here: The paper uses Spearman correlation to measure alignment between automated metrics and human judgments, which is appropriate for ordinal data.
  - Quick check question: Why would Spearman correlation be preferred over Pearson correlation for comparing ranking-based evaluations?

- Concept: Task-specific prompt engineering
  - Why needed here: Different conditional image generation tasks require carefully crafted prompts to elicit appropriate evaluations from MLLMs.
  - Quick check question: How does prompt structure need to change when evaluating text-guided generation versus image editing tasks?

## Architecture Onboarding

- Component map: Input processor -> MLLM backend -> Output parser -> Correlation calculator
- Critical path:
  1. Input formatting (conditions + images + task-specific instructions)
  2. MLLM evaluation execution
  3. Output parsing and validation
  4. Score aggregation and correlation calculation

- Design tradeoffs:
  - Single-pass evaluation vs. multi-step refinement: Single-pass is faster but may miss nuances
  - Open-ended prompts vs. structured templates: Structured templates ensure consistency but may limit flexibility
  - MLLM choice: GPT-4v performs better but is more expensive than open-source alternatives

- Failure signatures:
  - MLLM fails to follow format requirements → Output parsing errors
  - Inconsistent scores across similar inputs → Reliability issues
  - Low correlation with human judgments → Alignment problems

- First 3 experiments:
  1. Validate MLLM format compliance: Test with simple inputs and verify structured output generation
  2. Measure consistency: Run same evaluation multiple times to check score stability
  3. Compare correlation: Run evaluations on a small subset of ImagenHub data and compute initial Spearman correlations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do VIEScore's performance and reliability compare across different types of conditional image synthesis tasks, such as text-to-image generation, image editing, and subject-driven tasks?
- Basis in paper: [explicit] The paper evaluates VIEScore across seven tasks but notes varying performance, particularly lower correlations in editing tasks.
- Why unresolved: The study provides initial findings but does not deeply analyze why certain tasks are more challenging or how VIEScore could be adapted for better performance in weaker areas.
- What evidence would resolve it: A detailed comparative analysis of VIEScore's performance across tasks, including qualitative insights into the types of errors made in each task, would clarify the model's strengths and weaknesses.

### Open Question 2
- Question: What specific aspects of image editing tasks make them more challenging for VIEScore and other evaluation metrics compared to image generation tasks?
- Basis in paper: [explicit] The paper notes that MLLMs often fail to detect minor changes in image editing, leading to lower correlation scores in these tasks.
- Why unresolved: While the paper identifies the issue, it does not explore the underlying reasons or potential solutions for improving VIEScore's performance in editing tasks.
- What evidence would resolve it: An in-depth investigation into the specific challenges of image editing, such as the ability to detect subtle changes or the impact of overediting, would provide insights into how VIEScore could be improved.

### Open Question 3
- Question: How does the use of In-Context Learning (ICL) impact the performance of VIEScore and other MLLMs in evaluating synthetic images?
- Basis in paper: [explicit] The paper observes that using ICL can lead to decreased performance due to MLLMs focusing on example images rather than the task at hand.
- Why unresolved: The study notes the negative impact of ICL but does not explore alternative prompting strategies or the reasons behind this behavior.
- What evidence would resolve it: Comparative experiments testing different prompting methods, including ICL, zero-shot, and few-shot approaches, would clarify the conditions under which ICL is beneficial or detrimental to VIEScore's performance.

### Open Question 4
- Question: How do the subjective elements of human evaluation, such as personal preferences and visual acuity, affect the correlation between VIEScore and human ratings?
- Basis in paper: [inferred] The paper acknowledges that human evaluators display diverse perspectives on perceptual quality, which may impact the correlation with VIEScore.
- Why unresolved: While the paper notes the diversity in human ratings, it does not quantify the impact of subjective factors or explore how VIEScore can account for these differences.
- What evidence would resolve it: A study analyzing the variability in human ratings and its correlation with VIEScore's performance would provide insights into the role of subjectivity in evaluation metrics.

## Limitations
- VIEScore achieves only moderate correlation (0.3 Spearman) with human evaluations compared to human-to-human correlation (0.45)
- Performance heavily depends on using GPT-4v, with open-source MLLMs performing significantly worse
- Method shows notably weaker performance on image editing tasks compared to generation tasks

## Confidence
- High confidence: The correlation results are accurately reported and the methodology for computing Spearman correlation is sound
- Medium confidence: The claim that MLLMs can serve as generalizable evaluators across diverse image synthesis tasks is supported but limited by the performance gap with open-source alternatives
- Medium confidence: The explainability benefits through natural language rationales are demonstrated but the quality and consistency of these rationales across tasks needs further validation

## Next Checks
1. **Cross-task consistency validation**: Test VIEScore on a held-out set of synthetic images across all seven tasks to verify consistent performance patterns and identify specific task types where correlation drops significantly.

2. **Human evaluation of rationales**: Conduct a user study where human evaluators assess the quality, consistency, and usefulness of the generated rationales to determine if explainability translates to actual interpretability benefits.

3. **Open-source MLLM optimization**: Systematically test different prompt engineering strategies and output formatting approaches with leading open-source MLLMs to identify whether the performance gap with GPT-4v is due to model capability or implementation details.