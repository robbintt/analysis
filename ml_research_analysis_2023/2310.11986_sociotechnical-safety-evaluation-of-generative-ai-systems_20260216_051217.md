---
ver: rpa2
title: Sociotechnical Safety Evaluation of Generative AI Systems
arxiv_id: '2310.11986'
source_url: https://arxiv.org/abs/2310.11986
tags:
- evaluation
- systems
- arxiv
- generative
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a three-layered sociotechnical framework for
  evaluating generative AI systems' safety, recognizing that context determines whether
  capabilities may cause harm. The framework includes capability evaluation (technical
  components), human interaction evaluation (user experience and externalities), and
  systemic impact evaluation (broader societal, economic, and environmental effects).
---

# Sociotechnical Safety Evaluation of Generative AI Systems

## Quick Facts
- arXiv ID: 2310.11986
- Source URL: https://arxiv.org/abs/2310.11986
- Reference count: 40
- One-line primary result: Three-layered sociotechnical framework for evaluating generative AI systems' safety, identifying coverage, context, and multimodal evaluation gaps.

## Executive Summary
This paper introduces a three-layered sociotechnical framework for evaluating the safety of generative AI systems, recognizing that context determines whether capabilities may cause harm. The framework includes capability evaluation (technical components), human interaction evaluation (user experience and externalities), and systemic impact evaluation (broader societal, economic, and environmental effects). The authors conduct a comprehensive review of current safety evaluations, identifying three main gaps: 1) coverage gap (evaluations for several risks are lacking), 2) context gap (human interaction and systemic evaluations are rare), and 3) multimodal gap (evaluations are missing for multimodal AI systems). They propose practical steps to close these gaps, including repurposing existing evaluations for new modalities, transcribing non-text output for text-based evaluation, and using model-driven evaluation methods.

## Method Summary
The authors developed a sociotechnical framework for evaluating generative AI safety by conducting a comprehensive review of existing evaluations, mapping them across six harm areas, three evaluation layers, and multiple output modalities. They systematically analyzed the current evaluation landscape to identify gaps in coverage, context, and multimodality. Based on this analysis, they proposed practical approaches to close these gaps, including repurposing existing evaluations, translating non-text outputs to text, and using pre-trained models as evaluation tools. The framework operationalizes system safety principles by progressively adding context from technical capabilities to human interaction to systemic impacts.

## Key Results
- Three main evaluation gaps identified: coverage gap (several risks lack evaluations), context gap (human interaction and systemic evaluations are rare), and multimodal gap (evaluations missing for non-text modalities)
- Current evaluations are heavily clustered at the capability layer and primarily assess text outputs
- Proposed practical steps to close gaps include repurposing existing evaluations, transcribing non-text output, and using model-driven evaluation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-layered sociotechnical framework enables systematic safety evaluation by progressively adding context from technical capabilities to human interaction to systemic impacts.
- Mechanism: By separating evaluation targets into capability, human interaction, and systemic impact layers, the framework ensures that risks are not just assessed at the technical component level but also in the contexts where they manifest and their broader societal effects.
- Core assumption: Risks from generative AI systems cannot be comprehensively understood without evaluating them across multiple levels of context.
- Evidence anchors:
  - [abstract] "This framework encompasses capability evaluations... It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm."
  - [section 2.1] "Capability evaluation can indicate whether an AI system is likely to produce factually incorrect output... However, the risk of people being deceived or misled by that output may depend on factors such as the context in which an AI system is used..."
  - [corpus] Weak evidence - the related papers mention "system safety" and "sociotechnical" but don't provide concrete evidence for this specific mechanism.

### Mechanism 2
- Claim: The framework identifies specific evaluation gaps by mapping current evaluations against the taxonomy of harms across modalities and layers.
- Mechanism: By systematically categorizing existing evaluations by risk area, output modality, and evaluation layer, the framework reveals coverage gaps (certain risks lack evaluations), context gaps (human interaction and systemic evaluations are rare), and multimodal gaps (evaluations missing for non-text modalities).
- Core assumption: The current evaluation landscape can be effectively mapped and analyzed to identify systematic gaps.
- Evidence anchors:
  - [abstract] "Three salient evaluation gaps emerge from this analysis."
  - [section 3.3] "First, we observe that evaluations are scarce for several previously identified risks... Second, our second main observation is that insofar as evaluation tools exist... they are mainly clustered at the capability layer... Third observation is that the vast majority of evaluations exclusively assess text."
  - [corpus] Weak evidence - related papers discuss evaluations but don't provide specific evidence for this gap analysis mechanism.

### Mechanism 3
- Claim: Practical steps to close evaluation gaps include repurposing existing evaluations, transcribing non-text output, and using model-driven evaluation methods.
- Mechanism: The framework provides actionable approaches to address gaps: reusing components from existing evaluations (with caution about context differences), translating non-text outputs to text for evaluation using existing tools, and leveraging pre-trained models as evaluation tools to cover more risk-modality combinations quickly.
- Core assumption: Existing evaluation methods can be adapted or extended to address current gaps in generative AI safety evaluation.
- Evidence anchors:
  - [abstract] "We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors."
  - [section 4.3] "One way to address gaps in the evaluation landscape is to repurpose components of existing evaluation methods... Another way to address the uneven distribution of evaluations across modalities is to translate outputs from one modality into another..."
  - [corpus] Weak evidence - related papers mention evaluation methods but don't provide specific evidence for these adaptation mechanisms.

## Foundational Learning

- Concept: Sociotechnical systems thinking
  - Why needed here: The framework is built on the premise that AI systems are sociotechnical systems where both technical and social components determine whether risks manifest.
  - Quick check question: Why does the framework include human interaction and systemic impact layers beyond just technical capability evaluation?

- Concept: Operationalization of latent constructs
  - Why needed here: To make complex, multifaceted risks of harm measurable, the framework requires translating them into observable metrics at each evaluation layer.
  - Quick check question: How does the framework suggest measuring the risk of misinformation differently at the capability layer versus the human interaction layer?

- Concept: Evaluation validity and reliability
  - Why needed here: The framework acknowledges that evaluations have inherent limitations and must be designed carefully to ensure valid and reliable results across different contexts and risk areas.
  - Quick check question: What are some