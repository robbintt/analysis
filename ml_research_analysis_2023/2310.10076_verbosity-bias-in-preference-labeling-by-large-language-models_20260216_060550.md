---
ver: rpa2
title: Verbosity Bias in Preference Labeling by Large Language Models
arxiv_id: '2310.10076'
source_url: https://arxiv.org/abs/2310.10076
tags:
- llms
- bias
- human
- verbosity
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates verbosity bias in large language models
  (LLMs), where models tend to prefer longer responses even when quality is similar.
  Using GPT-4 to evaluate pairs of responses, the authors found that GPT-4 generally
  favors longer answers in creative writing tasks.
---

# Verbosity Bias in Preference Labeling by Large Language Models

## Quick Facts
- **arXiv ID**: 2310.10076
- **Source URL**: https://arxiv.org/abs/2310.10076
- **Reference count**: 2
- **Key outcome**: GPT-4 exhibits verbosity bias with a value of 0.328, preferring longer responses even when humans prefer shorter ones

## Executive Summary
This study investigates verbosity bias in large language models, where models tend to prefer longer responses even when quality is similar. Using GPT-4 to evaluate pairs of responses, the authors found that GPT-4 generally favors longer answers in creative writing tasks. To quantify this bias, they propose a metric based on accuracy parity, measuring the difference in prediction accuracy between cases where the longer answer is correct versus incorrect. Experiments on the HH-RLHF dataset show that GPT-4 exhibits verbosity bias with a value of 0.328, indicating lower human alignment when humans prefer shorter answers. This suggests that RLAIF may reinforce verbosity bias in LLMs.

## Method Summary
The study evaluates response pairs using GPT-4 with chain-of-thought and few-shot prompting. Vicuna-7b-v1.5 generates responses for creative writing prompts, which are then evaluated by GPT-4. The HH-RLHF dataset provides human preference labels for comparison. The verbosity bias metric is calculated using accuracy parity, comparing GPT-4's prediction accuracy when the longer response is correct versus when the shorter response is correct. Position swapping is used to account for position bias in the evaluations.

## Key Results
- GPT-4 exhibits verbosity bias with a value of 0.328 in the HH-RLHF dataset
- Human alignment is lower when humans prefer shorter answers (45.8%) compared to when humans prefer longer answers (71.7%)
- Other LLMs like Claude and GPT-3.5 also show some verbosity bias, but to a lesser extent than GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 exhibits verbosity bias by preferring longer responses even when human feedback indicates shorter responses are more helpful.
- Mechanism: When evaluating response pairs, GPT-4's preference shifts toward longer answers as word count differences increase, with accuracy parity metrics quantifying this bias through lower alignment when humans prefer shorter answers.
- Core assumption: Longer responses are not inherently better, and the observed preference is a bias rather than an accurate quality assessment.
- Evidence anchors:
  - [abstract] "We see that in our problem setting, GPT-4 prefers longer answers more than humans."
  - [section] "We use the same prompt template as the previous experiment but asked GPT-4 to evaluate the whole conversation... when human feedback chose the answer with fewer words, human alignment was low."
  - [corpus] Weak evidence - corpus mentions similar bias studies but doesn't directly support this specific mechanism.
- Break condition: The bias would break if longer responses were consistently rated higher quality by independent evaluators, suggesting the preference reflects genuine quality rather than verbosity bias.

### Mechanism 2
- Claim: The proposed accuracy parity metric effectively quantifies verbosity bias by measuring prediction accuracy differences across word count conditions.
- Mechanism: The metric compares GPT-4's prediction accuracy when the longer response is correct versus when the shorter response is correct, with the difference indicating the degree of verbosity bias.
- Core assumption: Accuracy parity is a valid measure of bias because it isolates verbosity effects from other quality factors.
- Evidence anchors:
  - [abstract] "To quantify this bias, they propose a metric based on accuracy parity, measuring the difference in prediction accuracy between cases where the longer answer is correct versus incorrect."
  - [section] "Accuracy parity is satisfied if the accuracy of prediction is equal among both demographics... The deviance from accuracy parity can be calculated with the following equation."
  - [corpus] Weak evidence - corpus mentions related metrics but doesn't validate this specific approach.
- Break condition: The metric would break if other confounding factors (like response formatting or content complexity) correlate with word count and affect accuracy independently of verbosity.

### Mechanism 3
- Claim: RLAIF trained with verbosity-biased LLMs will reinforce verbose output generation in downstream applications.
- Mechanism: When LLMs trained via RLAIF use verbosity-biased reward models, they learn to generate longer responses even when conciseness would be more appropriate, as the reward signal favors verbosity.
- Core assumption: Reward models trained on biased LLM feedback will propagate that bias to the target model during RL training.
- Evidence anchors:
  - [abstract] "This suggests that RLAIF may reinforce verbosity bias in LLMs."
  - [section] "Training with RLAIF with verbosity bias present can lead to LLMs generating excessively long responses, when in reality a much more concise response would suffice."
  - [corpus] Weak evidence - corpus mentions related RLHF/RLAIF studies but doesn't directly support this causal claim.
- Break condition: This mechanism would break if reward models could be explicitly trained to ignore word count when evaluating response quality.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is essential because verbosity bias affects how reward models are trained and subsequently influence model behavior.
  - Quick check question: What are the two main phases of RLHF and how does verbosity bias affect each phase?

- Concept: Preference labeling and evaluation metrics
  - Why needed here: The study relies on understanding how LLMs evaluate response pairs and how metrics like accuracy parity quantify biases.
  - Quick check question: How does accuracy parity differ from equal opportunity in measuring verbosity bias?

- Concept: Chain-of-thought prompting and bias mitigation
  - Why needed here: The paper mentions these techniques as potential methods to reduce evaluation biases in LLMs.
  - Quick check question: How might chain-of-thought prompting help reduce verbosity bias in LLM evaluations?

## Architecture Onboarding

- Component map: Vicuna-7b-v1.5 -> Response generation -> Pair creation -> GPT-4 evaluation -> Human alignment comparison -> Bias quantification
- Critical path: Response generation → Pair creation → LLM evaluation → Human alignment comparison → Bias quantification
- Design tradeoffs:
  - Using Vicuna-7b-v1.5 for generation vs. other models (affects response diversity)
  - Position swapping to account for position bias vs. simpler evaluation
  - Accuracy parity metric vs. other potential bias measures
- Failure signatures:
  - Low variance in word count differences → inability to detect bias
  - Contradictory position swap results → position bias contamination
  - High human alignment across all conditions → no observable bias
- First 3 experiments:
  1. Generate response pairs with controlled word count differences and evaluate with GPT-4 to confirm verbosity preference
  2. Compare GPT-4 evaluations against HH-RLHF human labels to measure human alignment across verbosity conditions
  3. Calculate accuracy parity metrics for multiple LLMs to compare verbosity bias levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a more general metric for verbosity bias that accounts for bias within each group (cases where one answer is barely longer versus significantly longer)?
- Basis in paper: [inferred] from limitations section discussing the current metric's inability to detect bias within groups
- Why unresolved: The current formulation only accounts for bias between two groups divided by which answer is longer, missing potential bias patterns within these groups
- What evidence would resolve it: Experimental data showing cases where the model has high human alignment with large length differences but low alignment with small differences, and a corresponding mathematical formulation that captures this

### Open Question 2
- Question: Does verbosity bias vary across different types of tasks (creative writing, summarization, question answering) and how does this affect the appropriate bias metric?
- Basis in paper: [explicit] from Huang et al. (2023) finding GPT-4 prefers short responses in summarization tasks
- Why unresolved: The paper's experiments focus on creative writing tasks, but different task types may exhibit different verbosity preferences
- What evidence would resolve it: Systematic experiments across multiple task types measuring verbosity bias for each, and task-specific metrics that account for these differences

### Open Question 3
- Question: What is the underlying cause of verbosity bias in LLMs - is it learned behavior from training data or an inherent characteristic of the model architecture?
- Basis in paper: [inferred] from discussion suggesting models may learn to mimic human behavior heuristically by choosing longer answers
- Why unresolved: The paper observes the phenomenon but does not investigate whether it stems from training data patterns or model architecture
- What evidence would resolve it: Comparative experiments with different training datasets and model architectures, or controlled experiments manipulating training data verbosity patterns

## Limitations

- The study is limited to creative writing tasks and may not generalize to other task domains
- Only GPT-4 is used as the judge model, limiting understanding of whether verbosity bias is specific to this model or a broader LLM phenomenon
- The causal link between RLAIF training and downstream verbosity reinforcement is presented as theoretical rather than empirically validated

## Confidence

- **High confidence**: The existence of verbosity bias in GPT-4's evaluation behavior (supported by consistent experimental results across multiple analyses)
- **Medium confidence**: The accuracy parity metric as a valid measure of verbosity bias (the metric is theoretically sound but requires broader validation)
- **Medium confidence**: The claim that RLAIF may reinforce verbosity bias (plausible but not empirically demonstrated in this study)

## Next Checks

1. Replicate the verbosity bias measurement across different task domains (e.g., question answering, summarization) to assess generalizability beyond creative writing
2. Test multiple judge models (Claude, GPT-3.5, LLaMA) to determine if verbosity bias is specific to GPT-4 or a broader LLM phenomenon
3. Conduct an empirical study training a reward model with and without verbosity bias mitigation to directly observe downstream effects on generated response lengths