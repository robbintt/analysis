---
ver: rpa2
title: Emergent inabilities? Inverse scaling over the course of pretraining
arxiv_id: '2305.14681'
source_url: https://arxiv.org/abs/2305.14681
tags:
- scaling
- performance
- inverse
- training
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether inverse scaling\u2014a phenomenon\
  \ where larger language models perform worse at certain tasks\u2014can occur not\
  \ only as a function of model size but also as a function of training data. The\
  \ authors test seven Pythia models (ranging from 70M to 6.9B parameters) on four\
  \ tasks from the Inverse Scaling Challenge at eight different checkpoints during\
  \ training."
---

# Emergent inabilities? Inverse scaling over the course of pretraining

## Quick Facts
- arXiv ID: 2305.14681
- Source URL: https://arxiv.org/abs/2305.14681
- Authors: 
- Reference count: 5
- Key outcome: Larger language models show decreased performance on specific tasks (QUOTE-REPETITION and REDEFINE-MATH) as training progresses, despite overall improvements in general performance.

## Executive Summary
This paper investigates whether inverse scaling—where larger models perform worse at certain tasks—can occur not only as a function of model size but also as a function of training data. The authors test seven Pythia models (ranging from 70M to 6.9B parameters) on four tasks from the Inverse Scaling Challenge at eight different checkpoints during training. They find that for two tasks (QUOTE-REPETITION and REDEFINE-MATH), the largest models show decreased performance as training progresses, despite overall improvements in general performance. This demonstrates inverse scaling over the course of pretraining, highlighting the importance of continuous evaluation even after initial task success. The findings suggest that model performance at specific tasks can unexpectedly degrade during training, even as general capabilities improve.

## Method Summary
The study evaluates seven Pythia models of varying sizes (70M to 6.9B parameters) on four tasks from the Inverse Scaling Challenge at eight training checkpoints ranging from 4B to 300B tokens. For each model-task-checkpoint combination, accuracy is computed as the proportion of correct responses. Least-squares linear regression is then performed with log10(model parameters), log10(training tokens), and their interaction as predictors to test for inverse scaling effects over both model size and training data. The analysis reveals that while general performance improves with more training data, specific tasks (QUOTE-REPETITION and REDEFINE-MATH) show inverse scaling—performance degradation—for larger models as training progresses.

## Key Results
- Two tasks (QUOTE-REPETITION and REDEFINE-MATH) show inverse scaling over training for larger Pythia models
- Performance degradation begins after approximately 32 billion training tokens for the largest three models
- Inverse scaling over training is stronger for larger models, suggesting capacity-related factors
- General model performance continues to improve even as specific task performance degrades

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverse scaling over training data is stronger for larger models because more parameters require more training data to maintain performance on specific tasks.
- Mechanism: Larger models have higher capacity and thus need proportionally more training data to maintain performance. When training data is insufficient relative to model capacity, performance on specific tasks degrades.
- Core assumption: Model capacity scales with parameter count, and performance on specific tasks is sensitive to the ratio of training data to model capacity.
- Evidence anchors:
  - [abstract] "we find that for Pythia models with a higher number of parameters, performance decreases over the course of training at these two tasks"
  - [section] "inverse scaling over the course of training is stronger for larger models"
  - [corpus] "Large language models (LLMs) have been shown to exhibit emergent abilities in some downstream tasks, where model performance stagnates at first and then improves sharply and unpredictably with scale beyond a threshold" (supports the general concept of non-linear scaling)
- Break condition: If performance degradation is due to other factors like catastrophic forgetting or task-specific overfitting, this mechanism would not hold.

### Mechanism 2
- Claim: Tasks that are already subject to parameter-based inverse scaling are more likely to show inverse scaling over training.
- Mechanism: These tasks contain patterns that are difficult for models to learn correctly, and as models become larger, they may overfit or misinterpret these patterns more severely during extended training.
- Core assumption: The same underlying task properties that cause inverse scaling with parameter size also cause inverse scaling with training data quantity.
- Evidence anchors:
  - [abstract] "a key finding in this study is that inverse scaling over the course of training is stronger for larger models"
  - [section] "whether this is always the case or whether it only occurs when the selected tasks—like those selected in the present study—already show parameter-based inverse scaling remains to be determined"
  - [corpus] "Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale" (provides context for standard scaling vs. inverse scaling)
- Break condition: If inverse scaling over training occurs for tasks that don't show parameter-based inverse scaling, this mechanism would be incomplete.

### Mechanism 3
- Claim: Inverse scaling over training occurs when the model's learned representations for a task become less optimal as training progresses beyond a certain point.
- Mechanism: During training, the model may initially learn task-relevant patterns, but continued training causes these representations to shift in ways that reduce task performance, even while general language modeling improves.
- Core assumption: The optimization process for general language modeling can sometimes move away from optimal task-specific representations.
- Evidence anchors:
  - [abstract] "performance at QUOTE-REPETITION and REDEFINE-MATH appears to be a genuine case of inverse scaling—a decrease in performance at a specific task, even as the models' general performance increases"
  - [section] "the general performance of these models generally improves with pretraining data" (contrasts with specific task degradation)
  - [corpus] "advocates of 'emergence' view breakthroughs as unlocked capabilities, others attribute them to thresholding effects on noncontinuous metrics" (highlights the complexity of scaling phenomena)
- Break condition: If performance degradation is due to data quality issues or distribution shift rather than representation drift, this mechanism would not be the primary cause.

## Foundational Learning

- Concept: Understanding of scaling laws in language models
  - Why needed here: The paper investigates how performance scales with both model size and training data, requiring understanding of how these factors typically relate to performance
  - Quick check question: What is the general relationship between model size, training data, and performance in language models?

- Concept: Inverse scaling and emergent abilities
  - Why needed here: The paper explores a novel form of inverse scaling and contrasts it with standard scaling and emergent abilities
  - Quick check question: How does inverse scaling differ from the typical positive scaling observed in language models?

- Concept: Statistical significance testing in machine learning experiments
  - Why needed here: The paper uses regression analysis to test the reliability of observed patterns in model performance
  - Quick check question: Why is it important to use statistical tests when analyzing performance trends across multiple model sizes and training checkpoints?

## Architecture Onboarding

- Component map:
  - Pythia models (70M to 6.9B parameters) -> Inverse Scaling Challenge tasks -> Eight training checkpoints (4B to 300B tokens) -> Evaluation framework -> Regression analysis

- Critical path:
  1. Load model checkpoints at specified training tokens
  2. Run inference on all four tasks
  3. Calculate accuracy for each model-task-checkpoint combination
  4. Store results in structured format
  5. Perform regression analysis on results
  6. Visualize performance trends

- Design tradeoffs:
  - Using fewer checkpoints would reduce computation but might miss non-linear trends
  - Testing more tasks would provide broader insights but increase computational cost
  - Using different model architectures could validate findings but would complicate comparisons

- Failure signatures:
  - Inconsistent results across different random seeds
  - Performance degradation due to catastrophic forgetting rather than inverse scaling
  - Issues with task formatting or evaluation code

- First 3 experiments:
  1. Verify that baseline performance on a standard benchmark (like ARC Easy) shows standard positive scaling with both model size and training tokens
  2. Test the QUOTE-REPETITION task on a single model size across all checkpoints to confirm the non-linear degradation pattern
  3. Perform ablation by testing with different random seeds to ensure results are reproducible and not due to initialization effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does inverse scaling over training tokens consistently occur for tasks that already exhibit inverse scaling with model size parameters?
- Basis in paper: [inferred] The paper notes that "inverse scaling over the course of training is stronger for larger models" and that the tested tasks (QUOTE-REPETITION and REDEFINE-MATH) already showed parameter-based inverse scaling in the Inverse Scaling Challenge.
- Why unresolved: The study only tested four tasks from the Inverse Scaling Challenge, limiting generalizability. It's unclear whether the relationship between parameter-based and training-based inverse scaling holds across a broader range of tasks.
- What evidence would resolve it: Testing a larger, more diverse set of inverse scaling tasks on models with varying parameter counts and training checkpoints would establish whether the pattern consistently holds.

### Open Question 2
- Question: What is the mechanism behind inverse scaling during pretraining when general performance continues to improve?
- Basis in paper: [explicit] The paper states that "performance at QUOTE-REPETITION and REDEFINE-MATH appears to be a genuine case of inverse scaling—a decrease in performance at a specific task, even as the models' general performance increases."
- Why unresolved: The paper identifies the phenomenon but does not explain why specific task performance degrades while overall capabilities improve. This suggests competing optimization pressures during training.
- What evidence would resolve it: Detailed analysis of model internal representations and training dynamics during the periods where inverse scaling occurs could reveal whether specific task-relevant features are being degraded by broader training objectives.

### Open Question 3
- Question: At what point during pretraining does inverse scaling typically begin, and can it be predicted?
- Basis in paper: [explicit] The results show "an interesting, non-linear version of this phenomenon—for the largest three models, performance is relatively stable until 32 billion training tokens, at which point it begins to drop sharply."
- Why unresolved: The study only tested specific checkpoints and two tasks showing this effect. The threshold appears to vary by task and model size, but the paper doesn't establish systematic patterns.
- What evidence would resolve it: Testing models at more frequent checkpoints across a wider range of inverse scaling tasks would help identify whether there are predictable triggers or patterns for when inverse scaling begins during training.

## Limitations
- The study is limited to four tasks from the Inverse Scaling Challenge, which may not represent the full diversity of inverse scaling phenomena
- The analysis cannot definitively distinguish between different potential mechanisms for inverse scaling over training
- Statistical significance testing assumes certain distributional properties that may not fully capture the complexity of scaling behavior in LLMs

## Confidence
- High confidence: The observation that QUOTE-REPETITION and REDEFINE-MATH tasks show inverse scaling over training for larger Pythia models (70M to 6.9B parameters). The statistical analysis and visual trends strongly support this finding.
- Medium confidence: The claim that inverse scaling over training is stronger for larger models. While supported by the data, this relationship could be influenced by other factors not controlled for in the study.
- Medium confidence: The interpretation that inverse scaling over training is a distinct phenomenon from parameter-based inverse scaling. The paper provides evidence for this distinction but acknowledges that the mechanisms may overlap.

## Next Checks
1. Test additional tasks that exhibit parameter-based inverse scaling to determine if they consistently show inverse scaling over training as well.
2. Evaluate the same models on a standard benchmark (like ARC Easy) to confirm that general performance continues to improve while specific task performance degrades.
3. Conduct ablation studies by training models on subsets of the training data to isolate whether the inverse scaling effect is due to insufficient data relative to model capacity or other factors.