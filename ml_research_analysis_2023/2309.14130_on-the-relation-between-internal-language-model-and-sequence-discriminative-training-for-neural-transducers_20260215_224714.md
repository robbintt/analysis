---
ver: rpa2
title: On the Relation between Internal Language Model and Sequence Discriminative
  Training for Neural Transducers
arxiv_id: '2309.14130'
source_url: https://arxiv.org/abs/2309.14130
tags:
- training
- sequence
- subtraction
- recognition
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes a theoretical and empirical connection between
  internal language model (ILM) subtraction and sequence discriminative training for
  neural transducers. The authors derive that the global optimum of MMI training shares
  a similar formula as ILM subtraction.
---

# On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers

## Quick Facts
- arXiv ID: 2309.14130
- Source URL: https://arxiv.org/abs/2309.14130
- Reference count: 0
- This work establishes a theoretical and empirical connection between internal language model (ILM) subtraction and sequence discriminative training for neural transducers.

## Executive Summary
This paper investigates the relationship between internal language model (ILM) subtraction and sequence discriminative training methods (MMI and MBR) for neural transducers. The authors derive a theoretical connection showing that the global optimum of MMI training shares a similar formula with ILM subtraction. Empirically, they demonstrate that both MMI and MBR training achieve similar performance improvements as ILM subtraction across various transducer configurations on the Librispeech dataset. The study also reveals that sequence discriminative training jointly affects both the encoder and prediction+joint network for posterior probability reshaping, including both ILM and blank suppression.

## Method Summary
The authors train conformer-based RNN-Transducer models on Librispeech 960h using sentence-level cross-entropy as a baseline. They then apply sequence discriminative training with MMI and MBR criteria, using both N-best-list and lattice-free approximations for MMI. For ILM subtraction, they employ zero-encoder, mini-LSTM, and density ratio methods during decoding. The models use gammatone features and acoustic data-driven subword modeling (ADSM) units. Performance is evaluated using word error rate (WER) and perplexity (PPL) of extracted ILMs.

## Key Results
- MMI training's global optimum shares a similar formula as ILM subtraction, both approximating division by an ILM-like term in log space
- MMI and MBR training achieve similar WER improvements as ILM subtraction across various setups including full and limited context transducers
- Sequence discriminative training jointly affects both encoder and prediction+joint network for posterior probability reshaping, including ILM and blank suppression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MMI training reshapes the RNN-T posterior to approximate ILM subtraction
- Mechanism: The global optimum of MMI training yields a posterior formula proportional to (prior/LM)^1/α, which is equivalent to dividing by an ILM-like term in log space
- Core assumption: The renormalized probability Pseq(aS1|X) captures the text prior distribution modeled by the LM
- Evidence anchors:
  - [abstract] "Theoretically, we derive that the global optimum of maximum mutual information (MMI) training shares a similar formula as ILM subtraction"
  - [section 3.1.2] "ˆPRNNT(aS1|X) ∝ (P r(aS1|X)/P β LM(aS1))1/α" and subsequent derivation showing Z(X) is constant
- Break condition: If the renormalized probability doesn't capture the text prior accurately, or if the α scaling distorts the relationship

### Mechanism 2
- Claim: MBR training achieves similar ILM suppression effects as MMI and ILM subtraction
- Mechanism: MBR finds the label sequence that minimizes Bayes risk, which implicitly learns to suppress the ILM component through the cost function and LM integration
- Core assumption: The edit distance cost function combined with LM-integrated posteriors drives the model toward ILM suppression
- Evidence anchors:
  - [abstract] "Empirically, we show that ILM subtraction and sequence discriminative training achieve similar performance across a wide range of experiments on Librispeech, including both MMI and minimum Bayes risk (MBR) criteria"
  - [section 3.2.2] Discussion of global optimum where optimal label sequence minimizes Bayes risk with Pseq
- Break condition: If the Bayes risk optimization doesn't converge to the ILM-suppressed optimum, or if approximations in hypothesis space generation are too coarse

### Mechanism 3
- Claim: Sequence discriminative training has joint effects on both encoder and prediction+joint network for posterior reshaping
- Mechanism: The training criterion affects all network components, not just the prediction network where ILM is typically estimated, leading to coordinated suppression of both ILM and blank probabilities
- Core assumption: The cross-component parameter dependencies mean changes in one part of the network affect the others during discriminative training
- Evidence anchors:
  - [abstract] "We also provide an in-depth study to show that sequence discriminative training has a minimal effect on the commonly used zero-encoder ILM estimation, but a joint effect on both encoder and prediction + joint network for posterior probability reshaping including both ILM and blank suppression"
  - [section 4.4.2] Table 4 results showing swapping encoder and prediction+joint components from different trained models yields improved performance
- Break condition: If the components are too independent or if the training criterion doesn't properly backpropagate through all components

## Foundational Learning

- Concept: Neural transducer architecture (encoder, prediction network, joint network)
  - Why needed here: Understanding how ILM is implicitly modeled in the prediction network and how discriminative training affects all components
  - Quick check question: What are the three main components of an RNN-T model and what is the role of each in generating output probabilities?

- Concept: Language model integration methods (shallow fusion, ILM subtraction)
  - Why needed here: The paper compares how different integration methods perform with and without discriminative training
  - Quick check question: How does ILM subtraction differ from simple shallow fusion in terms of the decision rule?

- Concept: Sequence discriminative training criteria (MMI, MBR)
  - Why needed here: These are the core training methods being compared to ILM subtraction
  - Quick check question: What is the key difference between MMI and MBR training objectives in terms of what they optimize for?

## Architecture Onboarding

- Component map: Input features → Encoder → Joint network → Softmax → Output sequence
- Critical path: Input → Encoder → Joint → Softmax → Output sequence
- Design tradeoffs:
  - Using full-context vs context-1 transducers affects ILM strength and integration performance
  - N-best-list vs lattice-free methods for hypothesis generation in discriminative training
  - Choice of LM (full-context vs bigram) for training vs recognition
- Failure signatures:
  - If discriminative training doesn't improve over CE baseline: likely issues with hypothesis generation or training stability
  - If ILM subtraction doesn't help CE model but does help discriminative models: indicates successful ILM suppression during training
  - If component swapping doesn't yield improvements: suggests training didn't properly affect all components
- First 3 experiments:
  1. Train CE baseline transducer and evaluate with/without ILM subtraction to establish baseline performance
  2. Train MMI model with full-context LM and compare to CE+ILM subtraction performance
  3. Perform component swapping experiment (encoder from CE, prediction+joint from MMI) to verify joint training effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of hypothesis generation method (N-best vs lattice-free) affect the correlation between MMI training and ILM subtraction in practice?
- Basis in paper: [explicit] The paper mentions using both N-best-list and lattice-free methods for hypothesis space generation in MMI training, noting that the global optimum is not guaranteed due to approximations.
- Why unresolved: The paper does not provide empirical comparisons of how different hypothesis generation methods impact the ILM subtraction effects observed in MMI training.
- What evidence would resolve it: Comparative experiments showing WER improvements and ILM subtraction benefits across different hypothesis generation methods (N-best, lattice-free, etc.) under identical MMI training setups.

### Open Question 2
- Question: What is the precise mechanism by which sequence discriminative training jointly affects both the encoder and prediction+joint network for posterior probability reshaping?
- Basis in paper: [explicit] The paper states that sequence discriminative training has a "joint effect on both encoder and prediction + joint network for posterior probability reshaping including both ILM and blank suppression," but does not detail the mechanism.
- Why unresolved: The paper provides empirical evidence of the joint effect but does not explain how or why this joint effect occurs at a technical level.
- What evidence would resolve it: Detailed analysis of gradient flows, attention weights, or internal representations showing how sequence discriminative training simultaneously modifies both components during training.

### Open Question 3
- Question: Why does the h'zero ILM estimation perform better than the density ratio (DR) approach for ILM subtraction?
- Basis in paper: [explicit] The paper notes that "h'zero ILM performs better than the DR ILM approach" but does not explain the reason.
- Why unresolved: The paper does not investigate or theorize about the relative performance differences between these ILM estimation methods.
- What evidence would resolve it: Comparative analysis of the statistical properties, computational efficiency, or estimation accuracy of h'zero vs DR ILM methods across various transducer architectures and datasets.

## Limitations

- Theoretical generalization may be limited as the analysis assumes Z(X) is constant across hypotheses, which may not hold for longer sequences
- Empirical validation is primarily focused on Librispeech with conformer-based RNN-T models, limiting generalizability to other datasets and architectures
- The specific mechanism of joint effects on encoder and prediction+joint network is not fully characterized, lacking detailed analysis of gradient patterns

## Confidence

**High Confidence**: The empirical observation that MMI and MBR training achieve similar performance improvements as ILM subtraction across various transducer configurations. The component-swapping experiments provide clear evidence of joint training effects.

**Medium Confidence**: The theoretical derivation connecting MMI's global optimum to ILM subtraction formula. While mathematically sound, the practical implications depend on training convergence and approximation quality.

**Low Confidence**: The specific mechanism by which discriminative training coordinates blank suppression across components. The paper suggests this occurs but doesn't provide detailed analysis of the gradient patterns or architectural reasons.

## Next Checks

1. **Cross-Dataset Validation**: Replicate the main experiments on a different dataset (e.g., Switchboard or TED-LIUM) to verify that the ILM-subtraction equivalence holds across acoustic conditions and speaking styles.

2. **Ablation of LM Integration**: Systematically test the performance of ILM subtraction vs sequence discriminative training with different LM integration strengths (varying β parameters) to quantify the exact relationship between LM influence and performance gains.

3. **Gradient Analysis**: Perform detailed gradient flow analysis during MMI/MBR training to verify that the prediction network indeed learns ILM suppression patterns and that the encoder receives appropriate updates for joint optimization.