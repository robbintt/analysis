---
ver: rpa2
title: 'Active Instruction Tuning: Improving Cross-Task Generalization by Training
  on Prompt Sensitive Tasks'
arxiv_id: '2311.00288'
source_url: https://arxiv.org/abs/2311.00288
tags:
- tasks
- task
- uncertainty
- instruction
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Active Instruction Tuning, a framework to
  improve the cross-task generalization ability of instruction-tuned models by actively
  selecting informative tasks. The key idea is to use prompt uncertainty - measuring
  disagreement in model outputs across perturbed versions of task instructions - to
  identify tasks that are most beneficial for training.
---

# Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks

## Quick Facts
- arXiv ID: 2311.00288
- Source URL: https://arxiv.org/abs/2311.00288
- Reference count: 40
- Key outcome: Training on prompt-uncertain tasks consistently outperforms random sampling, achieving better zero-shot generalization with fewer training tasks

## Executive Summary
This paper introduces Active Instruction Tuning, a framework that improves cross-task generalization by actively selecting informative tasks based on prompt uncertainty. The method measures disagreement in model outputs across perturbed versions of task instructions to identify tasks that are most beneficial for training. Experiments on NIV2 and Self-Instruct datasets demonstrate that training on prompt-uncertain (ambiguous) tasks leads to better zero-shot performance compared to random sampling or training on difficult tasks. The authors also propose a Task Map tool that categorizes tasks based on prompt uncertainty and prediction probability, revealing that ambiguous tasks improve generalization while difficult tasks do not.

## Method Summary
Active Instruction Tuning iteratively selects tasks for instruction tuning by measuring prompt uncertainty - the disagreement in model outputs when instructions are perturbed. The method perturbs task instructions (using random word dropout), measures prediction probability disagreement across perturbations, and selects tasks with highest uncertainty scores. Starting from an initial seed set of tasks, the framework iteratively adds new tasks, trains updated models, and evaluates zero-shot generalization on held-out test tasks. The approach is evaluated on NIV2 (with T5-770M) and Self-Instruct (with LLaMA-7B) datasets, comparing against random sampling and other baselines.

## Key Results
- Prompt-uncertain task selection consistently outperforms random sampling on NIV2 and Self-Instruct datasets
- Training on ambiguous (high-uncertainty) tasks improves zero-shot generalization while training on difficult (low-uncertainty, low-probability) tasks offers no benefit
- Task Map categorization successfully identifies which task types are most beneficial for instruction tuning
- The method achieves better generalization with fewer training tasks compared to random sampling baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt uncertainty measures how consistently a model maps perturbed instructions to the same latent task concept
- Mechanism: By perturbing task instructions and measuring disagreement in predicted likelihoods, the method estimates the model's epistemic uncertainty about the task's underlying concept
- Core assumption: A model that cannot robustly map varied surface forms of an instruction to the same latent concept will generalize poorly to unseen tasks
- Evidence anchors:
  - [abstract] "We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts."
  - [section 2.2] "From the robustness of the in-context learning perspective, if a model cannot robustly map task instructions to specific latent concepts, which is reflected by the sensitivity regarding perturbations in instructions, its generalization ability to the corresponding task is limited."
- Break condition: If perturbations alter instruction meaning significantly, the disagreement score conflates semantic drift with uncertainty

### Mechanism 2
- Claim: Training on prompt-uncertain tasks improves the model's ability to generalize by forcing it to learn robust instruction-to-concept mappings
- Mechanism: The model is exposed to ambiguous tasks where its current mapping is inconsistent, forcing it to refine its understanding of how instructions relate to tasks
- Core assumption: Tasks that are prompt-uncertain are those where the model's current instruction-to-concept mapping is weak and can be strengthened through training
- Evidence anchors:
  - [abstract] "We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit."
  - [section 2.2] "We hypothesize that training the model on prompt-uncertain tasks will improve its ability to associate prompts with specific latent concepts, leading to a better zero-shot performance on unseen instructions."
- Break condition: If ambiguous tasks are too noisy or contradictory, training may degrade rather than improve generalization

### Mechanism 3
- Claim: Task Map categorization based on prompt uncertainty and prediction probability reveals which task types are most beneficial for instruction tuning
- Mechanism: By mapping tasks into Ambiguous, Easy, and Difficult categories, the method identifies that Ambiguous tasks improve generalization while Difficult tasks do not
- Core assumption: The two-dimensional space of prompt uncertainty and prediction probability captures meaningful distinctions in task characteristics relevant to instruction tuning
- Evidence anchors:
  - [abstract] "We further propose Task Map, a task diagnosing tool that categorizes tasks based on their prompt uncertainty and prediction probability, providing insights into task characteristics and quality."
  - [section 5] "We further follow the above intuition to categorize the tasks into three types: Ambiguous tasks, where models fail to recognize them and have high prompt uncertainty; Easy and Difficult tasks..."
- Break condition: If the task map categories do not align with actual generalization performance, the diagnostic tool loses predictive value

## Foundational Learning

- Concept: Epistemic uncertainty vs aleatoric uncertainty
  - Why needed here: The method specifically targets epistemic uncertainty (model's lack of knowledge) rather than inherent task ambiguity
  - Quick check question: What is the difference between a task being inherently ambiguous versus the model being uncertain about it?

- Concept: In-context learning and latent concept mapping
  - Why needed here: The method assumes models implicitly map instructions to latent task concepts during in-context learning
  - Quick check question: How does instruction perturbation affect the model's ability to map to the same latent concept?

- Concept: Task-level uncertainty aggregation
  - Why needed here: The method must aggregate instance-level uncertainty measures to obtain task-level informativeness scores
  - Quick check question: Why can't we simply use average instance uncertainty as the task-level score?

## Architecture Onboarding

- Component map:
  - Task pool management (remaining tasks, selected tasks, training tasks)
  - Prompt perturbation module (instruction dropout with configurable rate)
  - Uncertainty computation module (likelihood comparison across perturbations)
  - Model training pipeline (instruction tuning with selected tasks)
  - Evaluation framework (zero-shot generalization metrics)

- Critical path:
  1. Initialize with random seed tasks
  2. Compute prompt uncertainty for remaining tasks
  3. Select top-k uncertain tasks
  4. Train new model on updated task set
  5. Evaluate generalization
  6. Repeat until convergence or task pool exhaustion

- Design tradeoffs:
  - Perturbation rate: Higher rates increase uncertainty sensitivity but risk semantic drift
  - Number of perturbations (k): More perturbations give more stable estimates but increase computation
  - Number of instances per task (n): More instances provide better task-level estimates but increase cost

- Failure signatures:
  - Prompt uncertainty scores become uniformly low (task pool exhausted)
  - Model performance plateaus despite adding tasks
  - Uncertainty scores correlate with task difficulty rather than informativeness
  - Training on ambiguous tasks degrades rather than improves performance

- First 3 experiments:
  1. Baseline comparison: Random sampling vs prompt uncertainty on a small task pool
  2. Perturbation sensitivity: Vary dropout rate and number of perturbations to find optimal settings
  3. Category validation: Train on Ambiguous vs Easy vs Difficult tasks to confirm Task Map predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prompt uncertainty measurement change when using different perturbation techniques (e.g., paraphrasing vs. word dropping) on various instruction tuning datasets?
- Basis in paper: [explicit] The authors mention trying several perturbation methods but do not compare their effectiveness across datasets
- Why unresolved: The paper focuses on word dropping for simplicity but doesn't systematically compare perturbation methods
- What evidence would resolve it: Experiments comparing different perturbation techniques (paraphrasing, token addition, word dropping) on multiple instruction tuning datasets showing which perturbation method best captures task novelty

### Open Question 2
- Question: What is the relationship between prompt uncertainty and the number of training instances per task? Does prompt uncertainty become less effective when tasks have many training examples?
- Basis in paper: [inferred] The paper contrasts NIV2 (many instances per task) with Self-Instruct (one instance per task) but doesn't analyze how instance count affects prompt uncertainty effectiveness
- Why unresolved: The authors don't explore how varying the number of training instances per task affects the correlation between prompt uncertainty and task informativeness
- What evidence would resolve it: Controlled experiments varying the number of training instances per task while measuring prompt uncertainty effectiveness across different task selection methods

### Open Question 3
- Question: How does prompt uncertainty perform in noisy or poorly constructed instruction tuning datasets compared to clean datasets like NIV2 and Self-Instruct?
- Basis in paper: [explicit] The authors acknowledge this limitation, stating their experiments are on well-constructed datasets and extreme scenarios may require additional techniques
- Why unresolved: The paper doesn't test prompt uncertainty on datasets with noisy or poorly constructed tasks
- What evidence would resolve it: Experiments applying prompt uncertainty to noisy instruction tuning datasets, comparing performance with and without noisy task filtering techniques

## Limitations
- The method's effectiveness depends on perturbations preserving semantic meaning, which isn't validated
- The Task Map framework's predictive validity isn't rigorously tested across different domains or model sizes
- The method assumes training on ambiguous tasks always improves generalization, without exploring potential harmful noise effects

## Confidence
- **High confidence**: Empirical results showing prompt-uncertain task selection outperforms random sampling on both NIV2 and Self-Instruct datasets
- **Medium confidence**: The mechanism explanation linking prompt uncertainty to improved cross-task generalization
- **Low confidence**: The Task Map diagnostic tool's predictive validity across different experimental conditions

## Next Checks
1. **Perturbation Semantic Stability Test**: Systematically evaluate how different perturbation rates affect both uncertainty scores and semantic integrity of instructions
2. **Category Transferability Validation**: Apply the Task Map categorization framework to a completely different task distribution and verify predicted training benefits hold
3. **Uncertainty Source Decomposition**: Design experiments to disentangle whether prompt uncertainty captures model epistemic uncertainty versus task inherent ambiguity