---
ver: rpa2
title: 'Justifiable Artificial Intelligence: Engineering Large Language Models for
  Legal Applications'
arxiv_id: '2311.15716'
source_url: https://arxiv.org/abs/2311.15716
tags:
- language
- available
- legal
- https
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper advocates for a shift from Explainable AI to Justifiable
  AI for legal applications of large language models (LLMs). It argues that rather
  than requiring full interpretability of LLMs, which is currently infeasible, we
  should focus on providing evidence that supports or refutes the LLM's output.
---

# Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications

## Quick Facts
- arXiv ID: 2311.15716
- Source URL: https://arxiv.org/abs/2311.15716
- Authors: 
- Reference count: 40
- One-line primary result: Shifts focus from Explainable AI to Justifiable AI for legal LLM applications by providing evidence that supports or refutes LLM outputs

## Executive Summary
This paper proposes Justifiable AI as an alternative to Explainable AI for legal applications of large language models (LLMs). Rather than requiring full interpretability of LLMs, which remains infeasible, the approach focuses on providing evidence that supports or refutes LLM outputs. The method combines retrieval of relevant legal documents with entailment classification to determine whether documents support or contradict LLM claims. This allows legal experts to make informed decisions while maintaining human decision-making sovereignty.

## Method Summary
The paper advocates for fact-checking techniques to provide evidence-based justification for LLM outputs in legal domains. The core method involves a retrieval module to find relevant legal documents and an entailment classifier to assess whether documents support or contradict the LLM's claims. Two pipeline architectures are proposed: one that prompts the LLM to extract evidence from documents and another that uses a separate fact-checking module to validate the LLM's output. The approach aims to help legal experts trust LLM outputs while maintaining human decision-making sovereignty.

## Key Results
- Fact-checking evidence provides trustworthiness for LLM legal outputs through retrieval and entailment classification
- Separating fact-checking from LLM generation creates more reliable legal outputs
- Human-in-the-loop validation maintains legal decision sovereignty while leveraging AI assistance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fact-checking evidence provides trustworthiness for LLM legal outputs
- Mechanism: Retrieve relevant legal documents and use entailment classification to determine whether the documents support or contradict the LLM's claims
- Core assumption: Legal domain users will trust outputs more when shown evidence, even if the underlying model remains opaque
- Evidence anchors:
  - [abstract] "Instead of focusing on Explainable Artificial Intelligence, we should focus on providing evidence that supports or refutes the LLM's output."
  - [section 3] "Providing evidence to the user, on the base of real documents, shall justify the AI's output."
  - [corpus] Found 25 related papers with average FMR=0.393
- Break condition: If retrieval fails to find relevant documents or entailment classification is unreliable

### Mechanism 2
- Claim: Separating fact-checking from LLM generation creates more reliable legal outputs
- Mechanism: Use the LLM to generate a claim, then employ a separate fact-checking pipeline that retrieves supporting/contradicting documents and classifies entailment
- Core assumption: LLM generation and fact-checking are distinct tasks that can be modularized without introducing cascading errors
- Evidence anchors:
  - [section 3] "The justification does not necessarily have to be performed by the LLM itself... Justifiable AI follows the motto: If you cannot explain yourself, at least justify your opinion."
  - [section 2.4] "Fact-checking requires a collection of trusted sources, a retrieval module and a way to compare the claim and the fact (e.g., via entailment classification)."
  - [corpus] Found relevant papers on software engineering methods for AI-driven deductive legal reasoning
- Break condition: If the LLM's claim is poorly formed or the fact-checking module has low precision

### Mechanism 3
- Claim: Human-in-the-loop validation maintains legal decision sovereignty while leveraging AI assistance
- Mechanism: Present both supporting and contradicting evidence to legal experts, allowing them to make final decisions based on comprehensive information
- Core assumption: Legal experts value having multiple perspectives and will use evidence appropriately to validate or reject claims
- Evidence anchors:
  - [abstract] "The key outcome is that Justifiable AI, by providing supporting and contradicting evidence, can help legal experts trust LLM outputs while maintaining human decision-making sovereignty."
  - [section 3] "In that way, the user does not get influenced by a confirmation bias, but can look into different perspectives, when the answer may not be straightforward."
  - [corpus] Found surveys on LLM applications in law showing current limitations
- Break condition: If legal experts are overwhelmed by evidence or lack the expertise to interpret it correctly

## Foundational Learning

- Concept: Entailment classification
  - Why needed here: To determine whether retrieved legal documents support or contradict the LLM's claims
  - Quick check question: Can you explain the difference between entailment and semantic similarity in the context of legal document validation?

- Concept: Information retrieval for legal documents
  - Why needed here: To find relevant legal precedents, statutes, or case law that can serve as evidence for or against the LLM's claims
  - Quick check question: What are the key challenges in retrieving relevant legal documents compared to general web search?

- Concept: Transformer-based architectures and their limitations
  - Why needed here: Understanding why LLMs lack explainability helps justify the shift to Justifiable AI
  - Quick check question: Why are attention visualizations insufficient for explaining complex legal reasoning performed by LLMs?

## Architecture Onboarding

- Component map: Retriever module -> LLM -> Entailment classifier -> User interface
- Critical path: LLM generates claim → Retriever finds relevant documents → Entailment classifier labels documents as supporting/contradicting → Evidence displayed to user for decision
- Design tradeoffs: Full transparency vs. practical usefulness (showing raw evidence vs. summary), computational cost of retrieval vs. accuracy benefits, breadth of document collection vs. precision of relevant results
- Failure signatures: False positive evidence (documents incorrectly labeled as supporting), missing relevant evidence (retrieval failure), overwhelming user with too many documents, user distrust due to low-quality evidence
- First 3 experiments:
  1. Implement simple BM25 retrieval with a small legal document collection and test entailment classification accuracy on legal claims
  2. Compare user trust ratings between raw LLM output vs. LLM output with supporting evidence from legal documents
  3. Measure the impact of document relevance quality on user acceptance of LLM-generated legal claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are different justifiable AI architectures in improving legal experts' trust in LLM outputs compared to standard XAI approaches?
- Basis in paper: [explicit] The paper proposes two pipeline architectures for justifiable AI and discusses their potential benefits over XAI approaches
- Why unresolved: The paper is a position paper that outlines the concept of justifiable AI but does not provide empirical comparisons
- What evidence would resolve it: Empirical studies comparing user trust levels, accuracy of outputs, and adoption rates between justifiable AI systems and traditional XAI methods

### Open Question 2
- Question: What are the optimal strategies for selecting and maintaining trustworthy knowledge bases for justifiable AI systems in legal domains?
- Basis in paper: [inferred] The paper discusses using fact-checking techniques and knowledge bases but does not address the challenges of maintaining up-to-date legal information
- Why unresolved: The paper acknowledges the need for trustworthy sources but does not explore the practical challenges of curating and updating legal knowledge bases
- What evidence would resolve it: Case studies of successful knowledge base implementations in different legal systems, and analyses of the cost-benefit trade-offs

### Open Question 3
- Question: How can justifiable AI systems effectively balance the presentation of supporting and contradicting evidence?
- Basis in paper: [explicit] The paper proposes showing both supporting and contradicting evidence but does not address the user interface and cognitive load challenges
- Why unresolved: While the concept of showing both types of evidence is introduced, the paper does not explore the optimal ways to present this information
- What evidence would resolve it: User studies comparing different interface designs for presenting evidence, and analyses of how different levels of evidence detail affect user decision-making

## Limitations

- The proposed Justifiable AI approach remains largely theoretical with limited empirical validation
- The paper does not address key implementation challenges or provide quantitative performance metrics
- Critical aspects like optimal user interface design for presenting evidence are not explored

## Confidence

- Claim: "showing evidence improves trust" - Medium confidence
- Claim: "separating fact-checking from LLM generation creates more reliable outputs" - Low confidence
- Claim: "human-in-the-loop validation maintains legal decision sovereignty" - Medium confidence

## Next Checks

1. Implement the proposed pipeline and measure fact-checking accuracy (precision/recall) on a benchmark legal document corpus
2. Conduct user studies comparing trust levels between raw LLM outputs, summarized explanations, and evidence-based justifications
3. Test the system's robustness when retrieval returns no relevant documents or when entailment classification produces ambiguous results