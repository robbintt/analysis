---
ver: rpa2
title: Borges and AI
arxiv_id: '2310.01425'
source_url: https://arxiv.org/abs/2310.01425
tags:
- language
- machine
- more
- ction
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes understanding large language models through
  the lens of Jorge Luis Borges' fiction, rather than the typical science fiction
  imagery. It describes language models as machines that generate plausible continuations
  of text, constrained by narrative necessity rather than truth.
---

# Borges and AI

## Quick Facts
- arXiv ID: 2310.01425
- Source URL: https://arxiv.org/abs/2310.01425
- Authors: 
- Reference count: 12
- Primary result: Proposes understanding LLMs through Borges' fiction as "fiction machines" generating plausible text continuations constrained by narrative necessity rather than truth

## Executive Summary
This paper proposes understanding large language models through the lens of Jorge Luis Borges' fiction rather than typical science fiction imagery. The authors argue that language models are fundamentally "fiction machines" that generate plausible continuations of text based on narrative necessity rather than truth. They highlight concerns about the potential for these models to shape human thought and culture, and advocate for careful consideration of their implications along with the need for verification mechanisms to distinguish fact from fiction.

## Method Summary
This paper presents a conceptual analysis rather than empirical research. It draws analogies between Jorge Luis Borges' fictional concepts (particularly "The Garden of Forking Paths" and "The Library of Babel") and the behavior of large language models. The analysis is primarily philosophical, examining how the infinite space of possible text continuations in LLMs parallels Borges' infinite libraries and forking narrative paths.

## Key Results
- Language models generate text based on narrative coherence and plausibility rather than factual accuracy
- Fine-tuning and alignment techniques cannot fundamentally change the model's nature as a narrative generator
- The outputs of language models are shaped by both user input and invisible participants (trainers, alignment teams)
- There is a need for verification mechanisms to distinguish factual information from fictional outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models function as "fiction machines" that generate plausible text continuations constrained by narrative necessity rather than truth.
- Mechanism: The model encodes a vast space of plausible texts and, given an input sequence, randomly samples from possible continuations based on learned statistical patterns, creating narrative coherence without requiring factual accuracy.
- Core assumption: The training data contains sufficient patterns of narrative coherence that the model can learn to generate contextually appropriate continuations.
- Evidence anchors:
  - [abstract] "This paper proposes understanding large language models through the lens of Jorge Luis Borges' fiction... language models are 'fiction machines' that can be manipulated to produce desired outputs"
  - [section 1] "Call this a perfect language model" describing the random sampling from plausible text continuations
  - [section 2] "The ability to recognize the demands of a narrative is a flavour of knowledge distinct from the truth"
- Break condition: If the model lacks sufficient narrative patterns in training data or if the sampling mechanism fails to maintain coherence, the fiction generation breaks down.

### Mechanism 2
- Claim: Fine-tuning and alignment techniques attempt to reshape the fiction machine's outputs but fundamentally cannot change its nature as a narrative generator.
- Mechanism: Human feedback and additional training dialogues create constraints on the model's output space, steering it toward certain narrative paths while suppressing others, but the underlying mechanism of narrative generation remains intact.
- Core assumption: The model's core architecture and learned representations remain stable during fine-tuning, allowing only surface-level modifications to output behavior.
- Evidence anchors:
  - [section 2] "Both groups want to reshape the garden of forking paths against its nature, severing the branches that lead to stories they deem undesirable"
  - [section 2] "Although there are countless ways to foil these attempts to reshape the fiction machine"
  - [corpus] Related papers discuss evaluation protocols and alignment challenges, indicating ongoing difficulties with controlling model outputs
- Break condition: If fine-tuning fundamentally alters the model's representations or if the model learns to subvert alignment constraints, the intended steering mechanism fails.

### Mechanism 3
- Claim: The model's outputs are shaped by both the user's input narrative demands and invisible participants (trainers, alignment teams) who influence what the machine says.
- Mechanism: The dialogue history creates narrative constraints that the model follows, while fine-tuning and alignment create implicit preferences in the model's response generation, combining to produce outputs that reflect multiple narrative influences.
- Core assumption: The model can effectively integrate multiple narrative influences from different sources (user input, training data, alignment) into coherent outputs.
- Evidence anchors:
  - [section 2] "When our part of the dialog with the machine evokes a professor correcting a mediocre student, its plausible completions make the machine assume the role of the student"
  - [section 2] "Both groups want to reshape the garden of forking paths against its nature"
  - [corpus] Papers on alignment and evaluation protocols suggest ongoing research into managing these multiple influences
- Break condition: If the model cannot reconcile conflicting narrative demands or if alignment constraints create incoherent outputs, the multi-influence mechanism breaks down.

## Foundational Learning

- Concept: Narrative necessity vs. truth
  - Why needed here: Understanding that language models generate text based on what makes narrative sense rather than factual accuracy is crucial for interpreting their outputs correctly.
  - Quick check question: If a language model is asked "What is 2+2?" and responds "2+2 equals 4, which is the number of wheels on a car," is this response factually correct, narratively coherent, or both?

- Concept: Fine-tuning and alignment limitations
  - Why needed here: Recognizing that alignment techniques cannot fundamentally change the model's nature as a fiction generator helps set realistic expectations about what can be achieved through post-training modifications.
  - Quick check question: If a model is fine-tuned to avoid harmful outputs, can it still generate harmful content if the narrative context makes such content narratively necessary?

- Concept: The infinite space of plausible texts
  - Why needed here: Understanding that the model operates within a vast space of possible narratives helps explain both its creative potential and its unreliability as a factual source.
  - Quick check question: If every possible continuation of a text exists in the model's learned space, how does it decide which path to take when generating output?

## Architecture Onboarding

- Component map: Input text → token embedding → transformer layers → attention refinement → output distribution over next tokens → sampling mechanism → generated text
- Critical path: Input text → token embedding → transformer layers → attention refinement → output distribution over next tokens → sampling mechanism → generated text
- Design tradeoffs: The model balances computational efficiency with representation capacity, using fixed-size context windows and learned embeddings rather than explicit knowledge bases.
- Failure signatures: Hallucinations (confabulations), inconsistent responses across similar prompts, inability to maintain long-term coherence, susceptibility to prompt injection attacks.
- First 3 experiments:
  1. Test narrative coherence by providing partial stories and evaluating whether generated continuations maintain internal consistency and narrative logic.
  2. Evaluate alignment effectiveness by attempting to elicit prohibited content through carefully crafted prompts that create narrative necessity for such content.
  3. Measure context window limitations by testing how well the model maintains coherence when generating very long continuations of a given text.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models be effectively aligned to produce consistently truthful outputs rather than fictional narratives?
- Basis in paper: [explicit] The authors discuss efforts to "fine-tune" models and use "reinforcement learning with human feedback" to steer outputs, but question whether this can fundamentally change the nature of language models as fiction machines.
- Why unresolved: The paper argues that language models fundamentally generate plausible continuations based on narrative necessity, not truth. Attempts to align them may be constantly evaded by users finding new ways to game the system.
- What evidence would resolve it: Empirical studies showing whether alignment techniques can consistently produce factually accurate outputs across diverse topics and user interactions, or whether users can always find ways to elicit fictional responses.

### Open Question 2
- Question: How can we design effective verification mechanisms to distinguish factual information from fictional outputs of language models?
- Basis in paper: [explicit] The authors suggest the need for "more mundane verification machines to check these stories against the cold reality" but do not propose specific methods.
- Why unresolved: While the paper identifies the problem of distinguishing fact from fiction in language model outputs, it does not offer concrete solutions for verification.
- What evidence would resolve it: Development and testing of practical verification systems that can reliably identify and flag fictional or inaccurate information in language model outputs across various domains and use cases.

### Open Question 3
- Question: What are the long-term societal implications of widespread use of language models as "fiction machines" on human cognition and culture?
- Basis in paper: [explicit] The authors warn about the potential for language models to shape human thought and culture, possibly leading to "insanity" if we mistake them for artificial intelligence that can spare us the burden of thinking.
- Why unresolved: The paper raises concerns about the cultural impact of language models but does not explore the specific long-term effects on human cognition, decision-making, or cultural development.
- What evidence would resolve it: Longitudinal studies on how frequent use of language models affects individual and collective reasoning, information processing, and cultural production over extended periods.

## Limitations

- The analysis relies heavily on philosophical interpretation rather than empirical validation
- Limited concrete evidence demonstrating how well the Borgesian metaphor maps to actual LLM behavior
- Abstract discussion of alignment limitations without addressing specific technical mechanisms
- No specific examples of LLM outputs or experimental results to ground theoretical claims

## Confidence

- **High confidence**: The core observation that LLMs generate text based on statistical patterns rather than factual knowledge is well-established in the literature.
- **Medium confidence**: The extension of this observation to frame LLMs as "fiction machines" in the specific Borgesian sense is philosophically compelling but requires more empirical grounding.
- **Low confidence**: The assertions about alignment limitations and the inability to fundamentally reshape the model's nature are speculative without concrete technical analysis.

## Next Checks

1. **Narrative Coherence Test**: Collect a dataset of LLM-generated story continuations and human-written continuations of the same prompts. Use both automated metrics (perplexity, repetition) and human evaluation to measure narrative coherence, then compare whether LLM outputs consistently follow narrative logic even when departing from factual accuracy.

2. **Alignment Evasion Experiment**: Systematically test whether carefully crafted prompts can elicit prohibited content from aligned models by creating narrative contexts where such content appears necessary or inevitable. Measure the success rate across different alignment techniques (RLHF, constitutional AI, etc.).

3. **Multi-Influence Analysis**: Design experiments where the same prompt is given to multiple instances of the same model under different "invisible influences" (different fine-tuning versions, different system prompts). Analyze whether and how the outputs reflect these multiple narrative sources, testing the claim that outputs represent a synthesis of user demands and trainer preferences.