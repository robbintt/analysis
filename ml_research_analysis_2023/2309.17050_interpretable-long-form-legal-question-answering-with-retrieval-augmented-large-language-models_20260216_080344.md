---
ver: rpa2
title: Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large
  Language Models
arxiv_id: '2309.17050'
source_url: https://arxiv.org/abs/2309.17050
tags:
- legal
- question
- linguistics
- language
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LLeQA, a novel expert-annotated dataset for
  long-form legal question answering in French, comprising 1,868 questions with detailed
  answers and references to 27,942 statutory articles. It proposes a retrieve-then-read
  framework using dense retrieval and instruction-tuned large language models to generate
  interpretable long-form answers.
---

# Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2309.17050
- Source URL: https://arxiv.org/abs/2309.17050
- Authors: 
- Reference count: 17
- Primary result: LLeQA dataset with 1,868 questions and 27,942 statutory articles, achieving R@5: 48.6%, R@10: 60.6%, MRR@10: 60.0% in retrieval and revealing hallucination issues in generated answers.

## Executive Summary
This paper introduces LLeQA, a novel dataset for long-form legal question answering in French, and proposes a retrieve-then-read framework using dense retrieval and instruction-tuned LLMs to generate interpretable answers with rationales. The system retrieves relevant statutory articles, conditions LLM generation on this context, and extracts paragraph-level justifications to support factual accuracy. Experimental results demonstrate moderate retrieval performance and reveal significant challenges with factual accuracy and hallucination in generated answers.

## Method Summary
The approach employs a retrieve-then-read pipeline where a dense retriever (CamemBERT bi-encoder) first identifies relevant statutory articles from a large corpus, then an instruction-tuned LLM generates comprehensive answers with rationales. The system uses context window extension via NTK-aware RoPE scaling to handle long legal articles and parameter-efficient finetuning (QLoRA) to adapt LLMs to the task. Evaluation combines automatic metrics (METEOR for answer quality, F1 for rationale extraction) with qualitative human assessment to identify hallucination issues.

## Key Results
- Retrieval performance: R@5: 48.6%, R@10: 60.6%, MRR@10: 60.0%
- Automatic metrics indicate reasonable answer quality but fail to capture factual accuracy issues
- Qualitative analysis reveals significant hallucination problems in generated justifications
- LLeQA dataset contains 1,868 expert-annotated questions with references to 27,942 statutory articles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation grounds LLM answers in statutory text, reducing hallucination.
- Mechanism: The dense retriever first fetches relevant articles; the LLM then conditions its output on this context, making its response traceable to legal provisions.
- Core assumption: Retrieved articles contain sufficient evidence to answer the question accurately.
- Evidence anchors:
  - [abstract] "Our experimental results show retrieval performance of R@5: 48.6%, R@10: 60.6%, and MRR@10: 60.0%"
  - [section] "To support training and evaluating such systems, we collect and release the Long-form Legal Question Answering (LLeQA) dataset"
- Break condition: If retriever recall is low, the LLM must rely on its parametric memory, increasing hallucination risk.

### Mechanism 2
- Claim: Instruction tuning and parameter-efficient finetuning improve LLM's ability to follow answer format and rationale extraction.
- Mechanism: By training on expert-annotated Q-A pairs, the LLM learns to produce structured answers with "ANSWER" and "RATIONALES" sections, mimicking the expected format.
- Core assumption: The model can generalize from training examples to unseen questions.
- Evidence anchors:
  - [abstract] "We use an instruction-tuned large language model (LLM) that we adapt to our task via two distinct learning strategies"
  - [section] "Our generator aims at formulating a comprehensive answer to a short legal question, leaning on corroborative data"
- Break condition: If training data is too narrow, the model may fail to generalize to novel legal topics.

### Mechanism 3
- Claim: Context window extension via NTK-aware RoPE scaling allows the LLM to process long legal articles without perplexity collapse.
- Mechanism: Dynamic interpolation preserves positional information for longer sequences, enabling the model to attend to more evidence.
- Core assumption: Scaling RoPE positions linearly preserves relative distances for the model's attention mechanism.
- Evidence anchors:
  - [section] "Recent efforts have aimed to extend the context window sizes of pretrained LLMs employing rotary position embedding...by interpolating positional encoding"
  - [section] "Preliminary results suggest this approach substantially mitigates perplexity degradation for sequences exceeding the maximum window size"
- Break condition: If the interpolation is too aggressive, attention weights may become unreliable.

## Foundational Learning

- Concept: Dense retrieval with bi-encoders
  - Why needed here: Efficiently finds relevant statutory articles from a large corpus before expensive LLM inference.
  - Quick check question: What similarity function is used in the bi-encoder to compute relevance scores between queries and articles?

- Concept: Contrastive learning for retrieval
  - Why needed here: Trains the retriever to distinguish relevant from irrelevant provisions, improving recall.
  - Quick check question: What sampling strategy is used to select hard negatives for training the retriever?

- Concept: Parameter-efficient finetuning (LoRA)
  - Why needed here: Allows adaptation of massive LLMs without full finetuning, saving compute and memory.
  - Quick check question: What is the rank parameter (r) set to in the LoRA adapters used in this work?

## Architecture Onboarding

- Component map: Question → Dense Retriever → Top-k Articles → LLM Generator → Rationale Extractor → Answer + Rationales

- Critical path: Question → Retriever → Top-k Articles → LLM Context → Answer + Rationales → Evaluation

- Design tradeoffs:
  - Retriever recall vs. inference speed
  - Context window size vs. computational cost
  - Automatic metrics vs. qualitative human judgment

- Failure signatures:
  - Low R@5: Retriever not finding relevant articles
  - High METEOR but poor rationale F1: LLM fabricates justifications
  - Zero-shot better than finetuned: Domain shift or overfitting

- First 3 experiments:
  1. Compare retrieval performance (R@5, MRR) with and without contrastive hard negatives.
  2. Ablate LLM context length (top-3 vs top-5 articles) and measure answer quality.
  3. Test different rationale extraction prompts (free-text vs paragraph ID) for factual accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using longer context windows (beyond 8192 tokens) on the performance of retrieval-augmented LLMs for long-form legal question answering?
- Basis in paper: [inferred] The paper mentions that LLMs typically come with a predefined context window limit and that recent efforts have aimed to extend the context window sizes of pretrained LLMs. However, the paper only experiments with context window sizes up to 8192 tokens.
- Why unresolved: The paper does not explore the performance of retrieval-augmented LLMs with context window sizes larger than 8192 tokens, which could potentially improve the system's ability to handle longer answers and more extensive legal provisions.
- What evidence would resolve it: Experimental results comparing the performance of retrieval-augmented LLMs with context window sizes larger than 8192 tokens against the current setup.

### Open Question 2
- Question: How does the quality of synthetic paragraph-level rationales generated by LLMs compare to those annotated by legal experts, and what is the impact on the overall system performance?
- Basis in paper: [explicit] The paper mentions that only 10.4% of the collected questions come with paragraph-level references, and synthetically generated paragraph-level rationales are used for the remaining samples. The paper evaluates the performance of the synthetic annotation method but does not compare it to expert annotations.
- Why unresolved: The paper does not provide a direct comparison between the quality of synthetic paragraph-level rationales and those annotated by legal experts, which could impact the system's interpretability and trustworthiness.
- What evidence would resolve it: A comparison of the performance of the system using synthetic paragraph-level rationales versus expert-annotated rationales, along with a qualitative analysis of the differences between the two.

### Open Question 3
- Question: What are the most effective strategies for mitigating hallucinations in the generated long-form answers and associated rationales in retrieval-augmented LLMs for legal question answering?
- Basis in paper: [explicit] The paper mentions that despite seemingly addressing the question, many facts, dates, sources, and conditions in the generated answers appear to be fabricated, indicating a tendency for hallucinations.
- Why unresolved: The paper does not explore specific strategies for mitigating hallucinations in the generated long-form answers and associated rationales, which is a critical issue for ensuring the trustworthiness and reliability of the system.
- What evidence would resolve it: Experimental results comparing the performance of different hallucination mitigation strategies, such as fine-tuning with additional data, using more robust evaluation metrics, or incorporating external knowledge sources, against the current setup.

## Limitations
- Moderate retrieval performance (R@5: 48.6%) indicates retriever finds relevant articles for less than half of queries in top-5
- Automatic metrics fail to capture significant hallucination issues in generated answers and rationales
- Dataset domain-specific to French civil law, limiting generalizability to other legal systems or languages

## Confidence
- **High confidence**: The retrieval-augmentation mechanism reduces hallucination when relevant articles are retrieved (supported by R@10: 60.6% and qualitative analysis)
- **Medium confidence**: Instruction tuning and parameter-efficient finetuning improve answer format adherence (based on METEOR scores and comparison with other LLMs)
- **Low confidence**: Context window extension via NTK-aware RoPE scaling effectively prevents perplexity collapse for long legal articles (preliminary results mentioned but not quantified)

## Next Checks
1. **Retrieve-then-generate ablation**: Test whether the retrieve-then-read pipeline produces more factual answers than direct LLM prompting without retrieval augmentation.
2. **Cross-domain generalization**: Evaluate the trained models on legal questions from other jurisdictions (e.g., US case law) to assess domain transfer.
3. **Human evaluation of rationales**: Conduct expert review of generated rationales to quantify hallucination frequency and assess the quality of paragraph-level justifications.