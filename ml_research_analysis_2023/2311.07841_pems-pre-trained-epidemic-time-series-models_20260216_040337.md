---
ver: rpa2
title: 'PEMS: Pre-trained Epidemic Time-series Models'
arxiv_id: '2311.07841'
source_url: https://arxiv.org/abs/2311.07841
tags:
- epidemic
- forecasting
- time-series
- tasks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles epidemic forecasting by introducing a self-supervised
  pre-training approach that learns from multiple heterogeneous disease datasets.
  PEM uses a transformer-based architecture with segmented inputs and positional encoding
  to capture local temporal patterns.
---

# PEMS: Pre-trained Epidemic Time-series Models

## Quick Facts
- arXiv ID: 2311.07841
- Source URL: https://arxiv.org/abs/2311.07841
- Reference count: 40
- Primary result: Pre-trained transformer achieves 11-24% lower RMSE and 9-12% better peak prediction accuracy than state-of-the-art epidemic forecasting models

## Executive Summary
This paper introduces PEM (Pre-trained Epidemic Time-series Models), a self-supervised pre-training approach for epidemic forecasting that learns from heterogeneous disease datasets. PEM uses a transformer-based architecture with segmented inputs and positional encoding to capture local temporal patterns. The model is pre-trained on 11 different diseases using four self-supervised learning tasks: random masking, last-segment masking, peak masking, and seasonal detection. PEM significantly outperforms previous state-of-the-art methods in forecasting and peak prediction across diseases like influenza, cryptosporidiosis, and typhoid, and demonstrates efficient adaptation to novel diseases like COVID-19.

## Method Summary
PEM employs a transformer encoder that processes time-series data as fixed-size segments rather than individual time steps. The model uses reversible instance normalization to handle varying magnitudes across heterogeneous disease datasets. Pre-training is performed on 11 diseases using four self-supervised learning tasks designed to capture epidemic dynamics: random masking (reconstructing randomly masked segments), last-segment masking (recovering future dynamics), peak masking (modeling peak behavior), and seasonal detection (identifying seasonal patterns). After pre-training, task-specific output layers are added for downstream fine-tuning on specific forecasting and peak prediction tasks. The architecture uses 6 transformer layers with 8 attention heads, segment size of 4, and learns transferable representations that enable better performance with smaller training data fractions.

## Key Results
- PEM achieves 11-24% lower RMSE in forecasting compared to previous state-of-the-art models across multiple diseases
- Peak prediction accuracy improves by 9-12% with PEM compared to baselines
- Pre-training enables faster convergence and better performance with smaller training data fractions
- PEM adapts efficiently to novel diseases like COVID-19 unseen during pre-training
- Ablation studies confirm segmentation, instance normalization, and SSL tasks are critical for strong performance

## Why This Works (Mechanism)

### Mechanism 1
Pre-training on heterogeneous epidemic datasets improves downstream forecasting by capturing generalizable epidemic dynamics. SSL tasks train the model to reconstruct masked segments, recover future dynamics, model peak behavior, and detect seasonal patterns across multiple diseases, thereby learning transferable structural priors. Core assumption: The same temporal patterns recur across different epidemic diseases and can be generalized. Evidence: PEM outperforms previous methods with 11-24% lower RMSE and 9-12% better peak prediction accuracy. Break condition: If diseases exhibit fundamentally incompatible temporal dynamics, pre-training gains will be limited.

### Mechanism 2
Segmenting time series into fixed-size chunks and using them as transformer tokens improves local temporal pattern learning compared to single-step tokens. Uniform segmentation transforms the continuous series into discrete tokens, allowing the transformer to model local trends and periodicity more effectively. Core assumption: Each segment carries meaningful semantic information about local epidemic dynamics. Evidence: Without segmentation, performance decreases by about 75% in forecasting. Break condition: If segments are too short to capture meaningful patterns or too long to fit into GPU memory, performance degrades.

### Mechanism 3
Instance normalization followed by linear projection and positional encoding stabilizes learning across heterogeneous disease datasets with varying magnitudes. Instance normalization standardizes each time series independently before feeding into the transformer, preventing scale-induced bias; positional encoding restores temporal order. Core assumption: Normalization does not erase disease-specific signal patterns and can be reversed without loss. Evidence: Using reversible instance normalization has the most impact on peak intensity prediction at 31% whereas only decreases forecasting performance by about 8%. Break condition: If normalization removes critical disease-specific scaling cues, model loses ability to distinguish outbreak severity.

## Foundational Learning

- **Self-supervised learning (SSL)**: Enables learning from unlabeled, heterogeneous epidemic time series without needing task-specific labels across diseases. Quick check: What is the difference between supervised and self-supervised learning in the context of time series?
- **Transformer attention mechanisms**: Captures long-range dependencies and temporal relationships in epidemic curves better than RNNs or CNNs. Quick check: How does multi-head attention help the model learn different aspects of temporal patterns?
- **Instance normalization vs. batch normalization**: Instance normalization handles per-sample scaling differences across diseases, while batch normalization would mix signals across diseases. Quick check: When would instance normalization fail to preserve useful information?

## Architecture Onboarding

- **Component map**: Input segmentation layer → Instance normalization → Linear projection → Positional encoding → Transformer encoder (6 layers, 8 heads) → SSL task-specific output heads → Fine-tuning heads
- **Critical path**: Data → Segmentation → Normalization → Projection → Positional encoding → Encoder → Output layer → Loss → Backpropagation
- **Design tradeoffs**: Segment size P=4 balances local trend capture vs. sequence length; larger P reduces resolution, smaller P increases computational load. γ values control SSL task difficulty; higher γ forces stronger reconstruction, but risks task collapse. Using a single shared PEM across SSL tasks saves parameters but may dilute task-specific learning.
- **Failure signatures**: Performance collapse when training on unseen diseases; poor convergence with small γ; overfitting to training diseases.
- **First 3 experiments**: 1) Train PEM on Dpre with RAND MASK only; evaluate on held-out diseases; measure RMSE vs baseline. 2) Replace segmentation with raw time steps; compare forecasting accuracy drop. 3) Remove instance normalization; retrain; quantify loss in peak intensity prediction.

## Open Questions the Paper Calls Out

### Open Question 1
How well does PEM generalize to diseases with fundamentally different seasonal patterns than those seen during pre-training, such as tropical diseases with irregular seasonality or diseases with multiple peaks per year? The paper evaluates PEM on diseases with relatively simple seasonal patterns (single peak per year), leaving its performance on more complex seasonal patterns untested. Testing PEM on diseases with irregular seasonality or multiple peaks per year and comparing its performance to baseline models would provide evidence of its ability to generalize to diverse seasonal patterns.

### Open Question 2
What is the impact of the pre-training dataset size on PEM's performance, and is there a point of diminishing returns where adding more pre-training data doesn't significantly improve downstream performance? The paper uses a large pre-training dataset but doesn't explore the relationship between dataset size and performance. Conducting experiments with varying sizes of pre-training datasets and analyzing the relationship between dataset size and downstream performance would provide insights into the optimal amount of pre-training data needed for PEM.

### Open Question 3
How does PEM perform in scenarios where the pre-training data and the downstream task data have significant distributional shifts, such as different geographical regions or time periods? The paper evaluates PEM on diseases from the same geographical region and time period as the pre-training data, but doesn't explore its performance in scenarios with significant distributional shifts. Evaluating PEM's performance on diseases from different geographical regions or time periods than the pre-training data would provide insights into its robustness to distributional shifts.

## Limitations

- Pre-training datasets are limited to diseases available in Project Tycho (US, up to 1980) and CDC/NIID (Japan, up to 2012), which may not represent the full diversity of epidemic dynamics globally
- The claim of generalizability to COVID-19 is based on a single case study without systematic evaluation across diverse disease types
- The study focuses on three diseases for downstream evaluation, which may not be sufficient to establish robust generalization across the vast diversity of epidemic diseases
- Effectiveness on diseases with non-seasonal patterns or different temporal scales remains untested

## Confidence

- **High confidence**: The core architectural innovations (segmentation, instance normalization, SSL tasks) and their individual contributions are well-supported by ablation studies and quantitative comparisons with baselines
- **Medium confidence**: The claim of improved generalization across diseases is supported but limited by the narrow set of downstream diseases tested and the historical nature of pre-training data
- **Low confidence**: The assertion that PEM will generalize effectively to fundamentally different epidemic patterns (e.g., COVID-19, vector-borne diseases) is based on a single case study without systematic evaluation across diverse disease types

## Next Checks

1. Evaluate PEM on a systematically diverse set of diseases including non-seasonal (e.g., Ebola), vector-borne (e.g., dengue), and respiratory pandemic diseases to test true generalization capabilities beyond the pre-training distribution

2. Assess PEM's performance when pre-training data comes from significantly different time periods than downstream tasks, particularly for diseases where surveillance and reporting practices have evolved substantially over time

3. Quantify the relationship between pre-training dataset diversity and downstream performance by systematically varying the number and types of diseases included in pre-training, measuring the marginal benefit of each additional disease type