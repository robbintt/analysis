---
ver: rpa2
title: On the Robustness of Split Learning against Adversarial Attacks
arxiv_id: '2307.07916'
source_url: https://arxiv.org/abs/2307.07916
tags:
- layers
- learning
- split
- input
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of split learning against
  adversarial attacks, a critical security concern that has been largely overlooked
  in prior research. The authors propose a novel attack method called SLADV, which
  exploits the characteristics of split learning to craft adversarial examples.
---

# On the Robustness of Split Learning against Adversarial Attacks

## Quick Facts
- arXiv ID: 2307.07916
- Source URL: https://arxiv.org/abs/2307.07916
- Reference count: 22
- This paper investigates the robustness of split learning against adversarial attacks and proposes a novel attack method called SLADV.

## Executive Summary
This paper addresses the critical security vulnerability of split learning to adversarial attacks, an area that has been largely overlooked in prior research. The authors propose SLADV, a two-stage attack that exploits the unique characteristics of split learning architectures. The attack demonstrates that even with minimal resources and unlabeled data, an attacker can significantly degrade the performance of target models while remaining undetected by clients.

## Method Summary
SLADV consists of two stages: shadow model training and local adversarial attack. In the shadow model training stage, the server trains shadow input layers using a similarity loss with the target model's input layers, requiring only unlabeled non-IID data. In the local adversarial attack stage, the server crafts adversarial examples by perturbing intermediate outputs of the shadow model. The attack exploits the split learning architecture where the server has access to intermediate outputs but not labels, enabling gradient-based optimization without detection.

## Key Results
- SLADV achieves significant accuracy drops in target models (up to 80% degradation) with minimal attack resources
- The attack is highly effective even with proxy data from different distributions than the target data
- SLADV remains undetected by clients as it operates within normal split learning communication patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shadow input layers trained with similarity loss can approximate the behavior of original input layers even without access to labels.
- Mechanism: The server leverages the output of the target model's input layers as a supervisory signal to train shadow input layers by minimizing L2 distance between Fθ1(x) and Fθ'1(x').
- Core assumption: Early layers extract general features shared across datasets, so shadow input layers can learn useful representations even from non-IID data.
- Evidence anchors:
  - [abstract]: "The first stage only requires a few unlabeled non-IID data"
  - [section]: "shadow input layers can be trained. It is stressed that this way does not alert clients"
- Break condition: If the input layers capture task-specific features rather than general ones, the shadow model won't generalize effectively.

### Mechanism 2
- Claim: Adversarial examples crafted by perturbing intermediate outputs can transfer to the target model.
- Mechanism: By reducing the cosine similarity between server layer outputs (o2 and o'2), the attack creates perturbations that degrade target model performance.
- Core assumption: Server layers are deterministic functions of their inputs, so similar inputs produce similar outputs, enabling attack transfer.
- Evidence anchors:
  - [abstract]: "The overall cost of the proposed attack process is relatively low, yet the empirical attack effectiveness is significantly high"
  - [section]: "SLADV crafts adversarial examples by perturbing the intermediate outputs"
- Break condition: If server layers have significant non-linearities or normalization layers that amplify small input differences.

### Mechanism 3
- Claim: Training shadow models concurrently with target models exploits split learning's communication pattern without detection.
- Mechanism: The server fuses gradients from original task loss and similarity loss during backpropagation, making additional computation invisible to clients.
- Core assumption: Clients cannot distinguish between normal and tampered gradients as long as the mathematical form appears consistent.
- Evidence anchors:
  - [abstract]: "SLADV follows the latter type of attacks, as it is more practical than the former type"
  - [section]: "The fused gradients are returned to clients. Clients cannot detect gradient tampering by only observing the returned gradients"
- Break condition: If clients implement gradient inspection or differential privacy mechanisms that detect unusual gradient patterns.

## Foundational Learning

- Concept: Split Learning Architecture
  - Why needed here: Understanding how models are partitioned between clients and servers is essential for identifying attack vectors and designing shadow models.
  - Quick check question: What layers does the server have access to in the U-shaped configuration?

- Concept: Adversarial Example Transferability
  - Why needed here: The attack relies on examples crafted on shadow models transferring to target models, requiring understanding of when and why transfer occurs.
  - Quick check question: What factors influence adversarial example transferability between models?

- Concept: Gradient-based Optimization
  - Why needed here: Both shadow model training and adversarial example generation use gradient descent, requiring understanding of how gradients flow through split architectures.
  - Quick check question: How does the server compute gradients when it only has partial model access?

## Architecture Onboarding

- Component map:
  - Client side: Input layers (Fθ1) and output layers (Fθ3)
  - Server side: Server layers (Fθ2) and shadow input layers (Fθ'1)
  - Communication: Intermediate outputs between input and server layers

- Critical path:
  1. Clients send input layer outputs to server
  2. Server computes server layer outputs and similarity loss
  3. Server fuses gradients and returns to clients
  4. Clients update input layers using fused gradients
  5. Server uses shadow model to craft adversarial examples

- Design tradeoffs:
  - Data volume vs. attack effectiveness: More data improves shadow model quality but increases detection risk
  - Similarity constraint strength vs. model performance: Stronger constraints improve attack but may degrade target model accuracy
  - Shadow model architecture vs. input layer depth: Deeper shadow models better approximate deeper input layers

- Failure signatures:
  - Low attack effectiveness despite proper implementation
  - Target model accuracy degradation when similarity constraints are too strong
  - Detection by clients through unusual communication patterns

- First 3 experiments:
  1. Verify shadow model training works by comparing output similarity with and without similarity loss
  2. Test adversarial example transferability by crafting on shadow model and testing on target
  3. Measure attack effectiveness vs. similarity constraint strength to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can gradient decomposition techniques be effectively implemented to identify and eliminate attacker-added gradients in split learning scenarios?
- Basis in paper: [explicit] The paper suggests gradient decomposition as a potential defense mechanism, stating "we think gradient decomposition may be a promising direction. It may be possible to identify and eliminate attacker-added gradients so as to alleviate the threat of the proposed attack."
- Why unresolved: While gradient decomposition is mentioned as a promising direction, the paper does not provide specific implementation details or evaluate its effectiveness against SLADV.
- What evidence would resolve it: A detailed description of a gradient decomposition algorithm, its implementation in a split learning framework, and experimental results demonstrating its effectiveness in mitigating SLADV attacks.

### Open Question 2
- Question: How does the vulnerability of split learning to adversarial attacks compare across different types of data (e.g., medical images vs. general images) and tasks (e.g., classification vs. regression)?
- Basis in paper: [inferred] The paper focuses on image classification tasks and does not explore the impact of data type or task on vulnerability. The authors mention "medical diagnosis systems" as a potential application, suggesting interest in medical data.
- Why unresolved: The paper's experiments are limited to image classification tasks using standard datasets (CIFAR-10, CIFAR-100, TinyImageNet, SVHN), leaving the vulnerability of split learning in other domains unexplored.
- What evidence would resolve it: Experiments comparing SLADV's effectiveness against split learning in different domains (e.g., medical imaging, natural language processing) and tasks (e.g., object detection, regression).

### Open Question 3
- Question: What are the long-term effects of adversarial training on the performance of split learning models, especially when considering the trade-off between robustness and accuracy?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of SLADV but does not explore potential defenses such as adversarial training. The authors mention that increasing similarity constraints can lead to "greater loss in model performance," hinting at potential trade-offs.
- Why unresolved: While the paper shows that SLADV can significantly degrade model performance, it does not investigate how adversarial training might affect the model's long-term performance or the balance between robustness and accuracy.
- What evidence would resolve it: Experiments evaluating the impact of adversarial training on split learning models' performance over time, including metrics for both accuracy and robustness against various attacks.

## Limitations

- The paper provides limited empirical validation of the shadow model's approximation quality to the target model's behavior.
- The attack's stealthiness claims assume clients don't implement gradient-based detection mechanisms, which remains untested.
- Experimental results are limited to specific dataset combinations and may not generalize across different data distributions or model architectures.

## Confidence

- **Medium** for SLADV attack effectiveness: Experiments show significant accuracy drops but are limited to specific dataset combinations.
- **Low** for shadow model training mechanism: Claims about unlabeled data sufficiency lack rigorous empirical evidence.
- **Medium** for stealth claims: Assumes clients don't detect gradient anomalies, but this remains untested.

## Next Checks

1. **Shadow model fidelity evaluation**: Measure the output similarity between the target model's input layers and shadow input layers across different proxy datasets to quantify how well the shadow model approximates the target behavior.

2. **Cross-architecture transferability test**: Evaluate whether SLADV's effectiveness transfers to different model architectures (e.g., ResNet, VGG, MobileNet) to assess the attack's generalizability beyond the tested configurations.

3. **Gradient inspection vulnerability assessment**: Implement client-side gradient anomaly detection mechanisms to test whether the fused gradients can be distinguished from normal training gradients, validating or challenging the stealth claims.