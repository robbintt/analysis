---
ver: rpa2
title: Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation
arxiv_id: '2310.14025'
source_url: https://arxiv.org/abs/2310.14025
tags:
- retrieval
- what
- image
- phrase
- describe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores Visual Word Sense Disambiguation (VWSD), a
  novel task of retrieving an image that best represents the meaning of an ambiguous
  word within a given context. The authors approach VWSD as a multimodal retrieval
  problem and propose several methods: using large language models (LLMs) as knowledge
  bases to enhance phrases, converting VWSD to unimodal problems (text-to-text, image-to-image
  retrieval, and question-answering), and training a learn-to-rank (LTR) model to
  combine different modules.'
---

# Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation

## Quick Facts
- **arXiv ID**: 2310.14025
- **Source URL**: https://arxiv.org/abs/2310.14025
- **Reference count**: 24
- **Primary result**: Achieves state-of-the-art performance on VWSD task using LLM-based phrase enhancement and multimodal retrieval

## Executive Summary
This paper introduces a novel approach to Visual Word Sense Disambiguation (VWSD), which involves retrieving images that best represent ambiguous words in context. The authors propose a multimodal retrieval framework that leverages large language models (LLMs) as knowledge bases to enhance phrases and resolve target word ambiguity. By combining LLM-based phrase enhancement with various retrieval methods (text-to-image, image-to-image, and question-answering), the system achieves competitive ranking results. The approach also utilizes Chain-of-Thought prompting to generate explainable answers and reveal intermediate reasoning steps. Experimental results demonstrate that LLM-based phrase enhancement significantly improves retrieval performance, with GPT-3 achieving the best results.

## Method Summary
The VWSD approach treats the task as a multimodal retrieval problem and proposes six methods: (1) baseline VL retrieval using transformers like CLIP/ALIGN/BLIP, (2) LLM-based phrase enhancement using various LLMs (GPT-2/3/3.5, BLOOMZ, OPT, Galactica), (3) image captioning for text retrieval, (4) Wikipedia/Wikidata image retrieval, (5) learn-to-rank model combining features from previous approaches, and (6) question-answering with Chain-of-Thought prompting. The LLM-as-KB paradigm enriches short phrases with additional context to help VL models disambiguate rare or ambiguous target words. Features from different approaches are combined through gradient boosting to create a robust ranking system. The entire pipeline processes input phrases through LLM enhancement, VL retrieval, feature extraction, LTR ranking, and output generation.

## Key Results
- LLM-based phrase enhancement significantly improves VWSD retrieval performance across all tested LLMs
- GPT-3 achieves the best results among LLM variants for phrase enhancement
- The LTR model combining features from multiple approaches achieves competitive ranking results
- Chain-of-Thought prompting successfully reveals intermediate reasoning steps for explainable VWSD decisions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-based phrase enhancement significantly improves VWSD retrieval performance by resolving target word ambiguity through contextual enrichment.
- **Mechanism**: The LLM-as-KB paradigm enriches short phrases with additional context, helping the VL model disambiguate rare or ambiguous target words by providing broader semantic context beyond the minimal context word.
- **Core assumption**: LLMs contain sufficient knowledge to disambiguate rare target words and that this enrichment transfers effectively to downstream VL retrieval.
- **Evidence anchors**:
  - [abstract] "We utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word."
  - [section] "Large Language Models (LLMs) as knowledge bases (LLM-as-KB) is a novel paradigm, presenting some interesting capabilities compared to traditional knowledge graphs."
  - [corpus] FMR=0.635 for related papers suggests moderate relevance of LLM-approaches to VWSD, but citation count=0 indicates limited validation in literature.
- **Break condition**: LLMs fail to provide relevant context for rare words, or the enrichment doesn't transfer to improved VL retrieval due to semantic mismatch.

### Mechanism 2
- **Claim**: The LTR model combining features from multiple approaches achieves competitive ranking results by leveraging complementary information from different modalities.
- **Mechanism**: Features extracted from VL retrieval, LLM-enhanced retrieval, text-to-text retrieval, and image-to-image retrieval are combined through gradient boosting to create a more robust ranking system that captures information missed by individual approaches.
- **Core assumption**: Different approaches capture complementary information about phrase-image relationships, and combining them improves overall performance.
- **Evidence anchors**:
  - [abstract] "On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results."
  - [section] "We implement a lightweight Learning to Rank (LTR) model that harnesses features extracted from our aforementioned experiments."
  - [corpus] FMR=0.635 suggests moderate relevance of multi-approach VWSD solutions, though no citations limit validation.
- **Break condition**: Features from different approaches are highly correlated or contradictory, leading to overfitting or degraded performance.

### Mechanism 3
- **Claim**: Chain-of-Thought prompting reveals intermediate reasoning steps that help explain VWSD decisions and improve disambiguation.
- **Mechanism**: CoT prompting guides the LLM through a step-by-step reasoning process, making the decision-making explicit and helping identify fine-grained differences between semantically similar candidates.
- **Core assumption**: LLMs can perform explicit reasoning when prompted appropriately, and this reasoning correlates with correct VWSD answers.
- **Evidence anchors**:
  - [abstract] "To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation."
  - [section] "Chain-of-Thought (CoT) prompting is leveraged to guide answer generation, while revealing intermediate reasoning steps that act as explanations for retrieval."
  - [corpus] FMR=0.635 for related papers suggests moderate relevance of CoT to multimodal tasks, but citation count=0 indicates limited validation.
- **Break condition**: CoT fails when the LLM lacks knowledge about the ambiguous term, or when the reasoning process doesn't align with visual semantics.

## Foundational Learning

- **Concept**: Multimodal retrieval fundamentals
  - **Why needed here**: VWSD is fundamentally a text-image retrieval task requiring understanding of how VL models encode and compare cross-modal representations.
  - **Quick check question**: How do CLIP and ALIGN differ in their approach to learning text-image embeddings?

- **Concept**: LLM prompting techniques
  - **Why needed here**: The paper relies heavily on prompting LLMs for phrase enhancement and CoT reasoning, requiring understanding of prompt engineering best practices.
  - **Quick check question**: What's the difference between zero-shot and few-shot prompting, and when would each be appropriate?

- **Concept**: Learning-to-rank fundamentals
  - **Why needed here**: The LTR component combines features from multiple approaches, requiring understanding of rank-based learning objectives and feature engineering.
  - **Quick check question**: What's the difference between pointwise, pairwise, and listwise LTR approaches, and when would you use each?

## Architecture Onboarding

- **Component map**: Input phrase -> LLM enhancement -> VL retrieval -> feature extraction -> LTR ranking -> output
- **Critical path**: Input phrase → LLM enhancement → VL retrieval → feature extraction → LTR ranking → output
- **Design tradeoffs**: Heavy LLM usage vs. computational cost, multiple approaches vs. simplicity, explainability vs. performance
- **Failure signatures**: Low accuracy despite high MRR (retrieves relevant images but wrong ranking), CoT failures indicating knowledge gaps, LTR overfitting on training features
- **First 3 experiments**:
  1. Run baseline VL retrieval with CLIP to establish performance floor
  2. Apply LLM enhancement with GPT-3 and measure improvement
  3. Implement LTR with baseline and LLM-enhanced features to test combination benefits

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do LLM-based phrase enhancements impact the robustness and explainability of VWSD models?
- **Basis in paper**: [explicit] The paper discusses using LLMs as knowledge bases to enhance given phrases and resolve ambiguity, as well as leveraging Chain-of-Thought prompting to guide explainable answer generation.
- **Why unresolved**: While the paper demonstrates the benefits of LLM-based phrase enhancement and CoT prompting, it does not thoroughly investigate their impact on model robustness and explainability. Additionally, the paper acknowledges limitations of LLMs, such as hallucinations and untruthful generation, which could negatively impact the reliability of explanations.
- **What evidence would resolve it**: Conducting extensive experiments to evaluate the robustness of LLM-enhanced VWSD models against various types of adversarial attacks and perturbations. Additionally, performing human evaluations to assess the quality and usefulness of explanations generated by CoT prompting.

### Open Question 2
- **Question**: How does the scale of LLMs impact the performance of VWSD models?
- **Basis in paper**: [explicit] The paper experiments with LLMs of varying scales, from 1.5B to 175B parameters, and observes that larger models generally perform better. However, it also notes that smaller models can sometimes outperform larger ones, depending on the specific task and prompt used.
- **Why unresolved**: The paper does not provide a comprehensive analysis of the relationship between LLM scale and VWSD performance. It is unclear whether the observed performance differences are due to the model's capacity, its pre-training data, or other factors.
- **What evidence would resolve it**: Conducting controlled experiments with LLMs of different scales, trained on the same data and using the same prompts, to isolate the impact of model size on VWSD performance. Additionally, analyzing the internal representations of LLMs of different scales to understand how they capture and process information relevant to VWSD.

### Open Question 3
- **Question**: How can the limitations of LLMs, such as hallucinations and untruthful generation, be mitigated in the context of VWSD?
- **Basis in paper**: [explicit] The paper acknowledges the risks of hallucinations and untruthful generation associated with LLMs and suggests combining LLM knowledge with knowledge graphs as a potential solution.
- **Why unresolved**: The paper does not provide a detailed investigation of methods to mitigate LLM limitations in VWSD. It is unclear how effective knowledge graph integration would be and what other techniques could be employed.
- **What evidence would resolve it**: Developing and evaluating various methods to detect and correct hallucinations and untruthful generation in LLM-enhanced VWSD models. This could involve techniques such as fact-checking, consistency checking, and uncertainty estimation. Additionally, exploring the use of hybrid models that combine LLMs with other knowledge sources, such as knowledge graphs or rule-based systems.

## Limitations

- The effectiveness of LLM-based phrase enhancement depends on LLMs possessing sufficient knowledge about rare target words, which may not hold for specialized domains.
- The LTR model's success assumes different approaches capture complementary information, but feature correlation analysis wasn't thoroughly conducted.
- Chain-of-Thought prompting provides explanations but lacks rigorous validation of whether the reasoning correlates with retrieval accuracy.

## Confidence

- **High confidence**: Baseline VL retrieval results using CLIP and ALIGN models are well-established and reproducible.
- **Medium confidence**: LLM-based phrase enhancement shows consistent improvements across multiple LLMs, but exact prompt templates and their impact remain unclear.
- **Medium confidence**: LTR model achieves competitive ranking results, though specific feature combinations and individual contributions aren't fully analyzed.
- **Low confidence**: Chain-of-Thought prompting explanations provide insights but lack rigorous evaluation of correlation with retrieval accuracy.

## Next Checks

1. **Prompt template ablation study**: Systematically test different prompt templates for LLM-based phrase enhancement (beyond the basic ones mentioned) to identify which structures yield the most consistent improvements across different target word types and ambiguity levels.

2. **Feature correlation analysis**: Conduct detailed correlation analysis between features extracted from different approaches (VL retrieval, LLM-enhanced retrieval, text-to-text, image-to-image) to empirically validate the assumption that they capture complementary information, and identify any redundant or contradictory features.

3. **CoT reasoning evaluation**: Implement a human evaluation study to assess whether the Chain-of-Thought explanations actually help humans understand the retrieval decisions, and analyze cases where CoT reasoning succeeds or fails to identify patterns that could improve the prompting strategy.