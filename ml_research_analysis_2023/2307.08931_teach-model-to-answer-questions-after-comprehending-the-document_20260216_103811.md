---
ver: rpa2
title: Teach model to answer questions after comprehending the document
arxiv_id: '2307.08931'
source_url: https://arxiv.org/abs/2307.08931
tags:
- student
- teacher
- comprehension
- distillation
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a two-stage knowledge distillation method
  for multi-choice Machine Reading Comprehension (MRC) tasks, addressing the limitations
  of traditional single-stage approaches that often neglect document comprehension.
  By dividing the MRC task into semantic comprehension and question-answering stages,
  the method enables the student model to focus on understanding document semantics
  before selecting correct answers.
---

# Teach model to answer questions after comprehending the document

## Quick Facts
- arXiv ID: 2307.08931
- Source URL: https://arxiv.org/abs/2307.08931
- Reference count: 36
- Accuracy of 71.8% on ReCO test set using proposed two-stage knowledge distillation method

## Executive Summary
This paper addresses the limitation of traditional single-stage approaches in multi-choice Machine Reading Comprehension (MRC) tasks that often neglect document comprehension. The authors propose a two-stage knowledge distillation method that first aligns semantic representations between teacher and student models, then aligns answer predictions. By enhancing the teacher model with both evidence and document context, the method reduces information gaps and improves performance. The approach achieves state-of-the-art results on the ReCO dataset, with the student model outperforming previous approaches by focusing on document understanding before answer selection.

## Method Summary
The method divides the MRC task into two stages: semantic comprehension and question-answering. In the first stage, the teacher model (BERT-base with evidence and document input) generates semantic representations that are aligned with the student model (BERT-base with document-only input) using MSE loss on specific tokens. In the second stage, the softmax predictions for candidate answers are aligned using KL divergence and cross-entropy losses. This sequential distillation process forces the student to first understand document semantics before learning answer selection, addressing the shortcut learning problem common in single-stage approaches.

## Key Results
- Achieves 71.8% accuracy on ReCO test set, outperforming previous state-of-the-art methods
- Student model trained with two-stage distillation significantly outperforms single-stage distillation baselines
- Teacher model with both evidence and document input demonstrates improved semantic representation quality compared to evidence-only input

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dividing the MRC task into two stages (semantic comprehension and answer selection) reduces shortcut learning by forcing the model to first understand the document before answering.
- Mechanism: The two-stage knowledge distillation method aligns semantic representations from the teacher model in the first stage and aligns answer predictions in the second stage, ensuring the student model learns document understanding independently of answer selection.
- Core assumption: Semantic understanding and answer selection are separable and can be effectively learned in sequence without loss of performance.
- Evidence anchors:
  - [abstract]: "The MRC task has traditionally been viewed as a process of answering questions based on the given text. This single-stage approach has often led the network to concentrate on generating the correct answer, potentially neglecting the comprehension of the text itself."
  - [section 3.3]: "To enhance the student model's ability to understand documents, we align the output representations of the token <2> as well as the token <3>."
  - [corpus]: Weak evidence - no direct mention of multi-stage approaches in neighbor papers.

### Mechanism 2
- Claim: Adding document context to the teacher model's input improves the quality of knowledge distillation by reducing information gaps between teacher and student.
- Mechanism: By providing both evidence and document context to the teacher model, it generates richer semantic representations and more accurate predictions, which the student can then learn from during the two-stage distillation process.
- Core assumption: The teacher model benefits from additional context and can leverage this to provide better guidance to the student model.
- Evidence anchors:
  - [abstract]: "The teacher model is enhanced by incorporating both evidence and documents as input, reducing information gaps and improving performance."
  - [section 3.1]: "we constructed a teacher model with input derived from concatenating the candidate answers, question, evidence, and document."
  - [corpus]: Weak evidence - no direct mention of document context enhancement in neighbor papers.

### Mechanism 3
- Claim: Sequential distillation (first semantic, then answer selection) is more effective than single-stage distillation with combined losses.
- Mechanism: The two-stage approach allows the student to focus on learning document comprehension without the distraction of answer selection, and then learn answer selection using the already-understood document context.
- Core assumption: Learning complex tasks sequentially is easier than learning them simultaneously, especially when the tasks have different requirements.
- Evidence anchors:
  - [section 4.4]: "the performance of the student trained by our proposed method gradually decreases during the first distillation stage's training process" - this shows the student is actually learning comprehension rather than shortcuts.
  - [section 4.5]: "The single-stage method involves only one training stage and aligns the representations of tokens related to semantics and predictions simultaneously... The performance of this method was underwhelming"
  - [corpus]: Weak evidence - no direct mention of sequential vs simultaneous learning in neighbor papers.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The method relies on transferring knowledge from a teacher model to a student model through distillation techniques.
  - Quick check question: What is the difference between hard labels and soft labels in knowledge distillation, and why might soft labels be preferred?

- Concept: Multi-choice Machine Reading Comprehension
  - Why needed here: The task involves selecting correct answers from multiple choices based on document comprehension, which is the core problem being addressed.
  - Quick check question: How does multi-choice MRC differ from span extraction tasks in terms of required model capabilities?

- Concept: Semantic Representation Alignment
  - Why needed here: The first stage of distillation aligns semantic representations between teacher and student to improve document comprehension.
  - Quick check question: What are some common methods for aligning representations between different models, and what loss functions are typically used?

## Architecture Onboarding

- Component map: BERT-base teacher model (evidence+document input) -> Two-stage distillation -> BERT-base student model (document-only input)
- Critical path:
  1. Train teacher model on ReCO dataset with full input
  2. Train student model through two-stage distillation
  3. Evaluate student performance on test set
- Design tradeoffs:
  - Using BERT-base for both models keeps architecture simple but may limit performance compared to larger models
  - Two-stage approach increases training complexity but improves semantic understanding
  - Adding document context to teacher improves quality but increases computational requirements
- Failure signatures:
  - Student performance plateaus early in training
  - Student performs well on evidence-based tasks but poorly on document-only tasks
  - Loss values become unstable during training
- First 3 experiments:
  1. Compare student performance with and without teacher guidance on document-only tasks
  2. Test different alignment strategies for semantic representations (different tokens, layers)
  3. Evaluate impact of document context in teacher model by comparing with evidence-only teacher

## Open Questions the Paper Calls Out

- How does the two-stage knowledge distillation method perform on other multi-choice MRC datasets beyond ReCO?
- What is the impact of varying the hyper-parameters α, β, and γ in the single-stage distillation loss function on the model's performance?
- How does the inclusion of documents in the teacher model's input affect its performance compared to using only evidence?

## Limitations

- Method relies heavily on teacher model quality and may create mismatch between training (with evidence) and inference (document-only)
- Two-stage approach increases training complexity and may introduce instability
- Performance evaluated only on ReCO dataset, limiting generalizability to other MRC tasks
- Uses BERT-base rather than larger models, potentially limiting performance ceiling

## Confidence

- **High Confidence**: The method's core architecture (two-stage knowledge distillation with semantic representation alignment followed by answer prediction alignment) is well-defined and technically sound. The experimental results showing 71.8% accuracy on the ReCO test set are clearly presented and verifiable.
- **Medium Confidence**: The claim that adding document context to the teacher model improves knowledge distillation quality is supported by the results but could benefit from ablation studies comparing different teacher model configurations. The effectiveness of the two-stage approach versus single-stage methods is demonstrated but could be strengthened with more extensive comparisons.
- **Low Confidence**: The claim about the specific percentage of paraphrased evidence (46%) in the ReCO dataset is mentioned but not independently verified. The generalizability of the method to other datasets and tasks is not thoroughly explored.

## Next Checks

1. Conduct ablation studies comparing teacher models with different input configurations (evidence-only, document-only, evidence+document) to verify that the combined input truly provides better guidance than either component alone.

2. Evaluate the method on additional multi-choice MRC datasets beyond ReCO to assess its robustness and generalizability across different domains and task formats.

3. Perform a more detailed comparison between single-stage and two-stage approaches, including different alignment strategies and loss function combinations, to better understand the optimal training procedure.