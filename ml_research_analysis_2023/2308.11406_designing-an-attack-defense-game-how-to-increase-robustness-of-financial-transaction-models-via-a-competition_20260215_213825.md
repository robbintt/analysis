---
ver: rpa2
title: 'Designing an attack-defense game: how to increase robustness of financial
  transaction models via a competition'
arxiv_id: '2308.11406'
source_url: https://arxiv.org/abs/2308.11406
tags:
- attack
- attacks
- data
- boosting
- competition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel attack-defense competition designed
  to enhance the robustness of financial transaction models. The competition framework
  pits participants against each other, challenging them to develop attacks that degrade
  model performance and defenses that protect against such attacks.
---

# Designing an attack-defense game: how to increase robustness of financial transaction models via a competition

## Quick Facts
- **arXiv ID**: 2308.11406
- **Source URL**: https://arxiv.org/abs/2308.11406
- **Reference count**: 27
- **Key outcome**: A novel competition framework that pits participants against each other to develop attacks and defenses, demonstrating superior performance over existing literature approaches for enhancing financial transaction model robustness.

## Executive Summary
This paper introduces an attack-defense competition framework designed to enhance the robustness of financial transaction models. The competition pits participants against each other, challenging them to develop attacks that degrade model performance and defenses that protect against such attacks. Through this framework, the authors analyze competition dynamics to answer key questions about model concealing, attack break times, and robustness techniques. The results show that the developed attacks and defenses outperform existing alternatives from the literature, validating the competition as an effective tool for uncovering and mitigating vulnerabilities in machine learning models across various domains.

## Method Summary
The authors create a competition framework where participants develop attacks to degrade financial transaction model performance and defenses to protect against such attacks. The setup uses anonymized financial transaction data with credit default labels, split into cohorts for attack development and defense testing. Attackers can modify up to 10 transactions per client by changing merchant category codes and amounts within specified bounds. Defenses employ strategies like model ensembling, feature filtering, and distillation from base GRU models. Performance is measured using ROC AUC scores, with defenses evaluated on their harmonic mean across clean and attacked data.

## Key Results
- The competition framework successfully identifies vulnerabilities in financial transaction models that existing literature approaches miss
- Gradient boosting models demonstrate superior robustness to adversarial attacks compared to neural networks when combined with effective feature engineering
- Model concealing significantly impacts robustness, with the paper finding that models can be completely broken within approximately two weeks of model leakage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks can degrade model performance by altering up to 10 transactions per client, making the model unusable.
- Mechanism: The attack modifies a small number of transactions (up to 10) within each client's sequence, changing their merchant category codes (MCC) and amounts to values that significantly impact the model's predictions.
- Core assumption: The model is sensitive to small changes in input transactions, and these changes can be made without being detected as fraudulent by the bank's systems.
- Evidence anchors:
  - [abstract]: "The participants compete directly against each other, proposing attacks and defenses -- so they are examined in close-to-real-life conditions."
  - [section]: "The goal of the attack track is to develop an attack that significantly changes the output of a given model after a minor change of input."
  - [corpus]: Weak - the corpus contains papers on fraud detection and graph-based approaches, but none directly address the specific attack mechanism of altering transactions in sequences.
- Break condition: If the bank has robust fraud detection models that can identify and ignore altered transactions, the attack would fail.

### Mechanism 2
- Claim: Gradient boosting models are more robust to adversarial attacks than neural networks.
- Mechanism: Gradient boosting models, when combined with filtering and ensembling, can withstand attacks that significantly degrade the performance of neural networks.
- Core assumption: Gradient boosting models have inherent properties that make them less susceptible to adversarial attacks compared to neural networks.
- Evidence anchors:
  - [abstract]: "The experiments demonstrate that the developed attacks and defenses outperform existing alternatives from the literature, proving the validity of the competition as a tool for uncovering vulnerabilities of machine learning models and mitigating them in various domains."
  - [section]: "Finally, our investigation suggests that with limited amounts of training data and possibilities of using effective feature engineering approaches, gradient boosting models can be efficiently distilled from neural networks with similar or even better quality on unstructured data, while being significantly more robust to adversarial attacks."
  - [corpus]: Weak - the corpus does not provide direct evidence for the robustness of gradient boosting models against adversarial attacks.
- Break condition: If the attack strategies are specifically designed to target the weaknesses of gradient boosting models, their robustness could be compromised.

### Mechanism 3
- Claim: Concealing the model from attackers is crucial for maintaining its robustness.
- Mechanism: By not revealing the model's architecture and parameters to potential attackers, it becomes significantly harder for them to craft effective attacks.
- Core assumption: Attackers need detailed knowledge of the model to develop successful attacks, and without this knowledge, their attacks are less effective.
- Evidence anchors:
  - [abstract]: "Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break it, and what techniques one should use to make it more robust."
  - [section]: "Dynamic of the score suggest that after obtaining a model it is enough to spent about two weeks to completely break the model, so model owners should take actions after the model leakage in a timely manner."
  - [corpus]: Weak - the corpus does not provide direct evidence for the importance of model concealing in adversarial settings.
- Break condition: If attackers can obtain information about the model through other means (e.g., through queries or model extraction attacks), the effectiveness of model concealing would be reduced.

## Foundational Learning

- Concept: Understanding the structure and characteristics of financial transaction data.
  - Why needed here: Financial transaction data has unique properties, such as dependence on macroeconomic situations and higher diversity of available features, which require specialized algorithms for attacks and defenses.
  - Quick check question: What are the key differences between financial transaction data and other types of sequential data, such as natural language or event sequences?

- Concept: Knowledge of adversarial attacks and defenses in machine learning.
  - Why needed here: The competition involves participants developing attacks to degrade model performance and defenses to protect against such attacks, requiring a solid understanding of these concepts.
  - Quick check question: What are the main types of adversarial attacks, and how do they differ in their approach to compromising machine learning models?

- Concept: Familiarity with gradient boosting models and their properties.
  - Why needed here: Gradient boosting models are shown to be more robust to adversarial attacks than neural networks, and understanding their properties is crucial for developing effective defenses.
  - Quick check question: How do gradient boosting models differ from neural networks in terms of their architecture and training process, and how might these differences contribute to their robustness?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Attack generation -> Defense development -> Competition scoring
- Critical path: 1. Data preprocessing and feature engineering 2. Model training and initial evaluation 3. Attack generation and application to the model 4. Defense development based on the attack results 5. Defense evaluation and comparison to baseline
- Design tradeoffs: Balancing the complexity of the attack generation process with the computational resources available; choosing between more sophisticated attacks that may be harder to defend against but also more computationally expensive; deciding on the level of model transparency during the competition to balance between fairness and security
- Failure signatures: Attack generation module fails to produce meaningful changes to the transaction sequences; Defense development module fails to improve the model's robustness against the attacks; Competition management module fails to accurately score and rank the participants' submissions
- First 3 experiments: 1. Implement a simple attack that randomly changes a small number of transactions and measure its impact on the model's performance 2. Develop a basic defense that averages the predictions of the model over multiple permutations of the transaction sequence and evaluate its effectiveness against the simple attack 3. Compare the performance of a neural network model and a gradient boosting model on the financial transaction data, both with and without adversarial training

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but it raises several important considerations regarding the scalability of attack effectiveness, transferability of attacks across different model architectures, and the optimal balance between model robustness and predictive performance.

## Limitations
- The attack mechanism's effectiveness depends on the assumption that banks cannot detect altered transactions, which may not hold in practice
- The evidence base for specific mechanisms (gradient boosting robustness, model concealing importance) is weak, with no direct external validation
- Reproducibility is hindered by unknown implementation details, particularly around preprocessing steps and exact defense strategy configurations

## Confidence

- **High confidence**: The general framework of using attack-defense competitions to test model robustness is well-established in the literature and the paper's methodology for implementing such a competition is sound.
- **Medium confidence**: The claim that concealing models is important for robustness, based on the observed score dynamics and attack patterns in the competition.
- **Low confidence**: The specific claims about gradient boosting models being more robust than neural networks and the attack mechanism's effectiveness in real-world scenarios, due to lack of external validation and weak evidence anchors.

## Next Checks

1. **Attack Detection Feasibility**: Test whether financial institutions' existing fraud detection systems can identify the specific transaction modifications proposed in the attack mechanism, as this would invalidate the attack's effectiveness.

2. **Cross-Domain Robustness Testing**: Apply the competition framework to a different sequential data domain (e.g., clickstream data) to verify whether the observed patterns of gradient boosting robustness and attack dynamics generalize beyond financial transactions.

3. **Model Extraction Vulnerability**: Evaluate whether attackers can extract useful information about the model architecture through query-based methods, which would test the actual security benefits of model concealing as proposed in the competition.