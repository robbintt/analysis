---
ver: rpa2
title: Merging Vision Transformers from Different Tasks and Domains
arxiv_id: '2312.16240'
source_url: https://arxiv.org/abs/2312.16240
tags:
- merging
- tasks
- different
- task
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of merging multiple Vision
  Transformer (ViT) models trained on different tasks or domains into a single unified
  model. Existing model merging methods are explored and found to have limitations
  in handling the whole ViT model and suffer from performance drops when the merging
  task becomes more difficult.
---

# Merging Vision Transformers from Different Tasks and Domains

## Quick Facts
- arXiv ID: 2312.16240
- Source URL: https://arxiv.org/abs/2312.16240
- Reference count: 40
- This paper investigates merging multiple ViT models trained on different tasks or domains into a single unified model.

## Executive Summary
This paper addresses the challenge of merging Vision Transformer (ViT) models trained on different tasks or domains into a single unified model while maintaining good performance on each task. The authors identify limitations in existing model merging methods when applied to whole ViT models and propose a novel solution using a gating network combined with a weight similarity metric. Their approach enables merging beyond 10 ViT models from different vision tasks with negligible performance degradation on each task.

## Method Summary
The method involves training a lightweight gating network (MobileNetV2) on unlabeled datasets from different tasks to predict task probabilities for each input. This gating network controls a weighted averaging of corresponding layers across models during inference. Additionally, the authors design a novel weight similarity metric based on cosine similarity between corresponding layers across models. Layers with high similarity are merged using simple averaging methods (AvgMean/RegMean), while layers with low similarity are merged using the gating network approach. The controllable parameter m determines how many layers use the gating network versus simple averaging.

## Key Results
- Successfully merges up to 12 ViT models from different tasks while maintaining performance
- Achieves 4.5% average improvement over existing merging methods on benchmark tasks
- Can merge beyond 10 ViT models with negligible effect on performance of each task
- The method fails when models are trained from scratch rather than fine-tuned from a common initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gating network enables task-specific layer merging and classifier selection by predicting input-task probabilities
- Mechanism: A lightweight CNN (e.g., MobileNetV2) is trained on unlabeled datasets from different tasks to output a probability distribution over tasks for each input. This probability is used to perform weighted averaging of corresponding layers across models and to select the appropriate classifier.
- Core assumption: Task correlation can be predicted from input features without labels, and this correlation directly indicates which model weights to emphasize.

### Mechanism 2
- Claim: Weight similarity metric identifies layers that are incompatible under simple averaging and should be handled by the gating network
- Mechanism: For each layer, pairwise cosine similarity between corresponding weights across models is computed and summed to get a total similarity score. Layers with low similarity are merged using the gating network, while high-similarity layers use AvgMean/RegMean.
- Core assumption: Weight similarity reflects the functional compatibility of layers across tasks; low similarity indicates that a task-specific merging strategy is needed.

### Mechanism 3
- Claim: Pretraining initialization enables linear mode connectivity, making model merging effective
- Mechanism: All models are fine-tuned from the same ViT-B/16 pretrained on ImageNet-21K, placing them in a common optimization basin. This ensures that the linear combination of weights stays within a low-loss region.
- Core assumption: Models sharing a common pretraining initialization belong to a common loss basin and are thus linearly connected with low loss barrier.

## Foundational Learning

- **Linear mode connectivity (LMC)**
  - Why needed here: LMC explains why models from the same pretraining initialization can be merged effectively; it underpins the assumption that weight averaging stays in a low-loss region.
  - Quick check question: What does LMC imply about the relationship between two models that share a common initialization?

- **Cosine similarity as a weight similarity metric**
  - Why needed here: Used to quantify how aligned corresponding layers are across tasks; guides which layers should use gating vs. averaging.
  - Quick check question: If two weight vectors have cosine similarity close to 1, what does that imply about their directional alignment?

- **Task arithmetic and model interpolation**
  - Why needed here: Provides baseline merging strategies; the paper builds on and improves these by adding gating and similarity-based selection.
  - Quick check question: In task arithmetic, what does the task vector represent?

## Architecture Onboarding

- **Component map**: Unlabeled dataset samples -> Gating Network (MobileNetV2) -> Weight Similarity Calculator -> Merging Controller -> Classifier Selector -> Merged Model

- **Critical path**: 1. Train gating network on unlabeled data, 2. Compute weight similarities for all layers, 3. For each input, gating network outputs probabilities, 4. Use probabilities to merge layers (weighted avg for low-similarity layers, simple avg for high-similarity), 5. Select classifier based on highest probability, 6. Forward pass through merged model

- **Design tradeoffs**: Gating network size vs. parameter overhead (larger gating nets improve accuracy but add parameters and FLOPs), m selection vs. performance (higher m increases merged model accuracy but also parameter count), similarity threshold vs. generalization (tighter thresholds may overfit to task differences, looser thresholds may underperform)

- **Failure signatures**: Low accuracy across tasks (likely due to poor gating network training or similarity metric misestimation), overfitting to few tasks (may indicate m is too high or gating network is overtrained), high FLOPs/params despite small m (check if gating network architecture is unnecessarily large)

- **First 3 experiments**: 1. Sanity check merging: Merge two models from the same task (should match individual performance) to verify basic merging logic, 2. Gating network ablation: Merge with and without the gating network on a simple 2-task setup; compare per-task accuracy, 3. Similarity threshold sweep: Vary the similarity threshold and observe impact on merged model accuracy to find optimal m

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture and initialization strategy for the gating network to maximize performance across different tasks and domains?
- Basis in paper: [explicit] The paper states "Since the fusion process is performed by the gating network, the architecture and initialization of the gating network may have a significant impact on the merged model's performance."
- Why unresolved: The paper only compares a few gating network architectures (CLIP ViT-B/16, ResNet-50, MobileNetV2) and initialization strategies (fine-tuned, from scratch). The impact of deeper or more complex gating networks, or alternative initialization methods, is not explored.

### Open Question 2
- Question: How does the proposed method scale to merging a larger number of models (e.g., 50+) or models from more diverse tasks (e.g., object detection, segmentation)?
- Basis in paper: [inferred] The paper successfully merges up to 12 models but does not explore scenarios with significantly more models or different vision tasks beyond classification.
- Why unresolved: The paper's experiments are limited to 16 datasets and focus on image classification. Scaling to hundreds of models or different vision tasks may introduce new challenges related to computational cost, model compatibility, and performance degradation.

### Open Question 3
- Question: Can the weight similarity metric be improved to better handle cases where models are fine-tuned from different initializations or pretrained models?
- Basis in paper: [explicit] The paper mentions that the method "may easily fail" when applied to ViTs trained from scratch instead of fine-tuned from a pretrained model.
- Why unresolved: The weight similarity metric is designed based on the assumption that all models are fine-tuned from the same initialization. It is unclear how well this metric generalizes to models with different initializations or pretraining.

## Limitations
- Method's effectiveness depends heavily on all models sharing a common pretraining initialization (ImageNet-21K)
- Gating network architecture and training procedure for unlabeled data is not fully specified
- Weight similarity metric's effectiveness for identifying incompatible layers is assumed but not rigorously validated across diverse model architectures
- The controllable parameter m requires manual tuning and optimal values may vary across tasks
- Evaluation focuses on ViT-B/16 models; generalization to other ViT variants or architectures is not explored

## Confidence

- **High**: The basic premise that pretraining initialization enables effective model merging is well-supported by experimental results showing poor performance when merging independently trained models
- **Medium**: The gating network approach for task-specific layer merging shows consistent improvements, but the optimal architecture and training strategy remain somewhat arbitrary
- **Medium**: The weight similarity metric provides a useful heuristic for layer selection, but its correlation with actual merging difficulty is not systematically validated

## Next Checks

1. **Cross-initialization validation**: Test the merging method on models fine-tuned from different pretraining checkpoints (not just ImageNet-21K) to assess the robustness of the weight similarity metric and gating network approach

2. **Gating network ablation study**: Systematically compare different gating network architectures (CLIP, ResNet-50, MobileNetV2 from scratch vs fine-tuned) and initialization strategies on a subset of tasks

3. **Architectural generalization test**: Apply the method to merge ViT models of different sizes (e.g., ViT-Small, ViT-Base, ViT-Large) to evaluate scalability beyond a single architecture variant