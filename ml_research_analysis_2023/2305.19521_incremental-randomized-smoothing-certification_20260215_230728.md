---
ver: rpa2
title: Incremental Randomized Smoothing Certification
arxiv_id: '2305.19521'
source_url: https://arxiv.org/abs/2305.19521
tags:
- certification
- confidence
- samples
- cifar10
- smoothing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Incremental Randomized Smoothing (IRS) provides a method to efficiently\
  \ certify robustness of smoothed neural networks after model approximation (e.g.,\
  \ quantization or pruning). It leverages cached certification information from the\
  \ original model to estimate a disparity parameter \u03B6x and compute a certified\
  \ radius for the approximate model with very few samples."
---

# Incremental Randomized Smoothing Certification

## Quick Facts
- arXiv ID: 2305.19521
- Source URL: https://arxiv.org/abs/2305.19521
- Reference count: 40
- Key outcome: IRS achieves up to 3x certification speedup over standard randomized smoothing while maintaining strong robustness guarantees for quantized and pruned models.

## Executive Summary
Incremental Randomized Smoothing (IRS) addresses the computational bottleneck of certifying robustness for approximate deep neural networks by reusing certification information from the original model. The method estimates a disparity parameter ζx between the original and approximate classifiers using cached certification data and very few samples, then computes a certified radius without re-sampling the entire certification process. Experiments demonstrate significant speedup (up to 3x) while maintaining robustness guarantees, with effectiveness increasing for larger smoothing parameters and quantization-based approximations.

## Method Summary
IRS builds on randomized smoothing by reusing cached certification information (top class index, lower confidence bound pA) from the original model to certify an approximate model with very few samples. The method estimates a disparity parameter ζx that bounds the probability of disagreement between classifiers on noisy inputs, then computes the certified radius using Theorem 2: radius = σ·Φ⁻¹(pA - ζx). This approach leverages the observation that ζx is typically small for practical approximations like quantization and pruning, making binomial proportion estimation sample-efficient. When pA - ζx < 0.5 or pA ≥ γ, IRS falls back to standard sampling.

## Key Results
- IRS achieves up to 3x certification speedup over standard randomized smoothing
- Speedup increases with larger smoothing parameters σ and is more effective for quantization than aggressive pruning
- ζx remains below 0.01 for int8 quantization across multiple large networks
- IRS maintains strong robustness guarantees while significantly reducing computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IRS can reuse certification guarantees from the original smoothed model to certify a smoothed approximate model with very few samples.
- Mechanism: IRS estimates a disparity parameter ζx, which bounds the probability that the original and approximate classifiers disagree on noisy inputs. This ζx is combined with cached confidence bounds from the original model to compute a certified radius for the approximate model.
- Core assumption: The disparity ζx between the original and approximate classifiers is small for common approximations like quantization and pruning.
- Evidence anchors: [abstract] "IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees." [section 3.1] "Common approximations yield small ζx values – for instance, it is below 0.01 for int8 quantization for multiple large networks in our experiments."
- Break condition: If ζx becomes large (e.g., with aggressive pruning or approximation), the efficiency gains diminish and the certified radius may shrink significantly.

### Mechanism 2
- Claim: Estimating ζx requires fewer samples than recomputing full confidence bounds because ζx is close to zero for practical approximations.
- Mechanism: The number of samples needed for binomial proportion estimation is smallest when the true parameter is near 0 or 1. Since ζx is small, IRS can estimate it efficiently using the Clopper-Pearson method.
- Core assumption: ζx is small enough that binomial proportion estimation is sample-efficient, and the estimation of ζx and the original pA/pB can be done independently.
- Evidence anchors: [section 3.1] "Estimating ζx through binomial confidence interval requires fewer samples as it is close to 0 – it is, therefore, less expensive to certify with this probability than directly working with lower and upper probability bounds in the original RS algorithm."
- Break condition: If ζx is not small (e.g., high-uncertainty approximations), the sample efficiency gain disappears and IRS may become slower than baseline.

### Mechanism 3
- Claim: IRS can compute a certified radius for the approximate model by combining the cached pA/pB bounds with ζx without re-sampling.
- Mechanism: Using Theorem 2, IRS transforms the original certified radius (based on pA and pB) into a new radius for the approximate model by adjusting with ζx: radius = σ·Φ⁻¹(pA - ζx).
- Core assumption: The original model's confidence bounds (pA, pB) remain valid proxies for the approximate model when adjusted by ζx, and pA - ζx ≥ 0.5 ensures the conservative radius formula applies.
- Evidence anchors: [section 3.2] "We can leverage ζx alongside the bounds in the certified radius of g around x to compute the certified radius for gp – thus soundly reusing the samples from certifying the original model."
- Break condition: If pA - ζx < 0.5, IRS falls back to standard sampling, losing the speedup benefit.

## Foundational Learning

- Concept: Randomized Smoothing (RS) certification
  - Why needed here: IRS builds directly on RS; understanding how RS computes certified radii and uses statistical sampling is essential to grasp IRS's reuse strategy.
  - Quick check question: In RS, what condition must hold for a classifier to certify a class with radius R = σ·Φ⁻¹(pA)?

- Concept: Binomial proportion confidence intervals
  - Why needed here: IRS uses Clopper-Pearson and related methods to estimate both pA/pB and ζx; knowing how these intervals behave for different true probabilities explains IRS's sample efficiency.
  - Quick check question: Why does binomial proportion estimation require fewer samples when the true probability is close to 0 or 1?

- Concept: Model approximation (quantization/pruning)
  - Why needed here: IRS targets smoothed classifiers built from quantized or pruned models; understanding how these approximations affect model behavior is key to interpreting ζx values.
  - Quick check question: How does aggressive pruning affect the disparity ζx between the original and approximate classifiers?

## Architecture Onboarding

- Component map: Original classifier f -> Cached certification info (pA, top class, seeds) -> Approximate classifier f_p -> IRS algorithm (ζx estimation, radius computation)
- Critical path:
  1. Load cached certification data for input x from original model f
  2. If pA < γ, estimate ζx using binomial sampling with cached seeds
  3. If pA - ζx ≥ 0.5, return radius σ·Φ⁻¹(pA - ζx)
  4. Else, sample f_p directly and compute radius using standard RS
- Design tradeoffs:
  - Using a higher γ reduces fallback sampling but may miss some IRS speedups
  - Larger np improves ζx estimation accuracy but reduces speedup
  - IRS is most effective when ζx is small (e.g., quantization) and σ is large (pA tends to be smaller)
- Failure signatures:
  - ζx close to 0.5 → IRS loses efficiency and certified radius shrinks
  - pA close to 1 → IRS falls back to standard sampling, losing speedup
  - Aggressive pruning → ζx increases, reducing IRS effectiveness
- First 3 experiments:
  1. Run IRS on a quantized ResNet-20 on CIFAR-10 with σ=1.0; measure speedup vs baseline
  2. Test IRS with increasing pruning ratios on ResNet-110; observe ζx growth and speedup decay
  3. Vary np from 1% to 10% of n; plot ACR vs certification time to find optimal np range

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions arise from the research:

### Open Question 1
- Question: How does IRS perform when certifying other types of network approximations beyond quantization and pruning, such as knowledge distillation or low-rank factorization?
- Basis in paper: [inferred] The paper focuses on quantization and pruning but mentions IRS can work with "most of these approximations" without characterizing the full set.
- Why unresolved: The paper only empirically evaluates quantization and pruning, leaving open whether IRS would be effective for other approximation methods.
- What evidence would resolve it: Experiments showing IRS speedups (or lack thereof) when certifying smoothed networks after knowledge distillation, low-rank factorization, or other approximation techniques.

### Open Question 2
- Question: What is the theoretical relationship between the approximation level of a network and the value of ζx, and how does this relationship affect IRS performance?
- Basis in paper: [inferred] The paper observes empirically that more aggressive pruning leads to larger ζx values and lower IRS speedups, but does not provide a theoretical characterization.
- Why unresolved: The paper provides empirical observations about ζx values for different approximations but does not derive theoretical bounds or relationships.
- What evidence would resolve it: Theoretical analysis or mathematical proofs establishing bounds on ζx as a function of approximation level, and experimental validation of these bounds across various approximation techniques.

### Open Question 3
- Question: How does the choice of confidence parameters α and αζ affect the trade-off between IRS certification accuracy and speedup, and what is the optimal setting?
- Basis in paper: [explicit] The paper uses fixed values α = 0.001 and αζ = 0.001, but mentions these are hyperparameters without exploring their impact.
- Why unresolved: The paper fixes these parameters without exploring how different settings affect the balance between certification accuracy and computational efficiency.
- What evidence would resolve it: Systematic experiments varying α and αζ to quantify their impact on certification accuracy, speedup, and the overall trade-off, potentially leading to guidelines for optimal parameter selection.

## Limitations
- IRS effectiveness depends critically on ζx remaining small; aggressive pruning or poor approximations can break this assumption and eliminate speedup benefits
- The method requires caching certification information from the original model, adding memory overhead and complexity to the certification pipeline
- Fallback to standard sampling occurs when pA - ζx < 0.5 or pA ≥ γ, potentially losing the incremental advantage

## Confidence
- **High Confidence**: IRS's core mechanism of reusing cached certification information and the mathematical formulation of ζx estimation
- **Medium Confidence**: The sample efficiency claim for ζx estimation, supported by the observation that binomial proportion estimation is more efficient for probabilities near 0
- **Medium Confidence**: The certified radius computation combining cached bounds with ζx, though the fallback conditions need clearer characterization

## Next Checks
1. Systematically measure ζx across a broader range of approximation ratios (pruning percentages, quantization bit-widths) to establish when the small-disparity assumption breaks.
2. Experimentally determine the optimal γ threshold by measuring the tradeoff between speedup and certified radius across different datasets and model architectures.
3. Validate the sample complexity claim by comparing the actual number of samples needed for ζx estimation versus direct confidence bound computation across different true probability values.