---
ver: rpa2
title: 'ChapGTP, ILLC''s Attempt at Raising a BabyLM: Improving Data Efficiency by
  Automatic Task Formation'
arxiv_id: '2310.11282'
source_url: https://arxiv.org/abs/2310.11282
tags:
- language
- training
- data
- tasks
- chapgtp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the ILLC's submission to the BabyLM Challenge's
  strict-small track, which aims to train efficient language models on a limited dataset.
  The core contribution is a novel data augmentation technique called Automatic Task
  Formation (ATF), which generates textual formulations from the existing training
  data based on predefined templates to improve performance on specific tasks like
  question answering and sentiment classification.
---

# ChapGTP, ILLC's Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation

## Quick Facts
- **arXiv ID**: 2310.11282
- **Source URL**: https://arxiv.org/abs/2310.11282
- **Reference count**: 18
- **Primary result**: ATF improves GLUE performance, especially for paraphrase detection and multi-sentence reading comprehension

## Executive Summary
This paper presents ChapGTP, the ILLC's submission to the BabyLM Challenge's strict-small track, which trains a masked language model on a limited 10M token dataset. The key innovation is Automatic Task Formation (ATF), a data augmentation technique that generates synthetic question-answer pairs and sentiment-labeled sentences from existing training data using predefined templates. ChapGTP uses a DeBERTa-small architecture trained for 200 epochs with ATF, achieving improved performance on GLUE tasks and outperforming baseline models provided by the challenge.

## Method Summary
The method trains a DeBERTa-small masked language model on the BabyLM Challenge's 10M token dataset using Automatic Task Formation (ATF) data augmentation. ATF generates synthetic QA pairs and sentiment-carrying sentences that mimic GLUE task formats, aligning pre-training data distribution with fine-tuning tasks. The model uses BPE tokenization with 10K vocabulary, is trained for 200 epochs with AdamW optimizer and cosine learning rate scheduling, and employs masked token prediction with 15% mask probability. The approach aims to improve data efficiency by exposing the model to task-relevant patterns during pre-training.

## Key Results
- ATF improved GLUE task performance, particularly for paraphrase detection and multi-sentence reading comprehension
- Prolonged training up to 200 epochs increased performance for most evaluation benchmarks
- ChapGTP outperformed baseline models provided by the BabyLM challenge
- Inverse scaling behavior observed on Irregular Forms BLiMP task with extended training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ATF improves GLUE performance by aligning pre-training data distribution with fine-tuning task distribution
- Mechanism: ATF generates synthetic question-answer pairs and sentiment-labeled sentences that mimic GLUE task formats, exposing the model to task-relevant patterns during pre-training
- Core assumption: The model can learn transferable representations from synthetic task formulations that generalize to actual GLUE evaluation tasks
- Evidence anchors:
  - [abstract] "ATF improved performance on GLUE tasks, especially for paraphrase detection and multi-sentence reading comprehension"
  - [section 5] "We hoped that if the training data were augmented with patterns that resembled data found in GLUE, the model could already start learning representations useful for GLUE tasks during pre-training"
- Break condition: If synthetic patterns are too dissimilar from actual GLUE data distributions, or if the model overfits to synthetic patterns without learning transferable representations

### Mechanism 2
- Claim: Prolonged training up to 200 epochs leads to monotonic improvement on most linguistic tasks except for specific exceptions like Irregular Forms
- Mechanism: Extended training allows the model to gradually refine and stabilize learned representations, improving performance across linguistic phenomena except where overfitting to surface patterns occurs
- Core assumption: More training epochs provide additional opportunity for representation refinement without catastrophic forgetting of earlier learned patterns
- Evidence anchors:
  - [abstract] "Prolonged training up to 200 epochs led to increased performance for most evaluation benchmarks"
  - [section 6] "increasing the amount of epochs has a positive effect on almost all tasks" and "Inverse scaling behavior for the Irregular Forms BLiMP task"
- Break condition: When training duration causes overfitting to specific patterns (like irregular forms) or when learning rate scheduling prevents effective long-term optimization

### Mechanism 3
- Claim: Pre-pretraining on bracketed constituency-labeled text improves hierarchical generalization on MSGS tasks
- Mechanism: Explicit syntactic structure markers provide additional inductive bias during pre-pretraining, helping the model learn hierarchical representations that transfer to generalization tasks
- Core assumption: Models benefit from explicit syntactic structure information during early training phases, even when this information is later removed
- Evidence anchors:
  - [section 7.4] "BRAK ChapGTP performs considerably better on the MSGS tasks" and "BRAK's main gains stem from two tasks: 'Main Verb Lexical Control The', and 'Main Verb Relative Token Position'"
  - [abstract] mentions BRAK as one of several approaches explored but not included in final model
- Break condition: If the additional computational overhead of pre-pretraining outweighs the performance benefits, or if the unsupervised parser introduces noise that harms downstream performance

## Foundational Learning

- Concept: Masked Language Modeling objective
  - Why needed here: ChapGTP uses a DeBERTa-based masked LM architecture, requiring understanding of how token prediction from context works
  - Quick check question: How does the model handle predicting masked tokens when multiple tokens are masked in the same sequence?

- Concept: Tokenization and subword units
  - Why needed here: ChapGTP uses BPE tokenization with a vocabulary size of 10,000 tokens, affecting how the model processes and represents language
  - Quick check question: What happens when the tokenizer encounters out-of-vocabulary words or rare morphological variants?

- Concept: Learning rate scheduling and optimization
  - Why needed here: ChapGTP uses cosine learning rate scheduler with AdamW optimizer, requiring understanding of how learning rate decay affects training dynamics
  - Quick check question: How does the learning rate schedule interact with the 200-epoch training duration and batch size?

## Architecture Onboarding

- Component map: DeBERTa-small architecture (6 layers, 12 attention heads, 768 hidden size) + BPE tokenizer + ATF data augmentation pipeline + 200-epoch training schedule
- Critical path: Data preprocessing → Tokenizer training → ATF augmentation → Model initialization → 200-epoch training with ATF → Evaluation
- Design tradeoffs: Smaller vocabulary size (10K) vs. compact model and faster training vs. potential loss of linguistic nuance
- Failure signatures: Poor GLUE performance despite ATF (suggests misalignment between synthetic and real task distributions), inverse scaling on specific tasks (suggests overfitting to surface patterns)
- First 3 experiments:
  1. Train baseline DeBERTa-small without ATF for 20 epochs to establish performance floor
  2. Add ATF augmentation and compare GLUE task performance gains
  3. Extend training to 200 epochs and analyze inverse scaling behavior on Irregular Forms task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does prolonged training on the BabyLM dataset lead to improved performance on all tasks, or are there tasks that are negatively affected by extended training?
- Basis in paper: [explicit] The paper reports that while prolonged training generally led to increased performance for most evaluation benchmarks, there was inverse scaling behavior for the Irregular Forms BLiMP task, where performance worsened with increased training epochs
- Why unresolved: The paper only examined a limited range of tasks (BLiMP, GLUE, and MSGS) and a specific inverse scaling pattern (Irregular Forms). It is unclear whether other tasks or different types of inverse scaling behavior exist
- What evidence would resolve it: Further experiments with a wider range of tasks and longer training durations would be needed to determine the full extent of prolonged training's effects on model performance

### Open Question 2
- Question: Can the Automatic Task Formation (ATF) technique be effectively applied to other language modeling tasks beyond question answering and sentiment classification?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of ATF in improving performance on specific tasks (question answering and sentiment classification) within the GLUE benchmark. However, it is unclear whether ATF can be generalized to other tasks
- Why unresolved: The paper only explored a limited set of ATF templates and did not investigate its applicability to other tasks. The effectiveness of ATF may depend on the specific characteristics of the target task and the available training data
- What evidence would resolve it: Experiments applying ATF to a wider range of language modeling tasks, with different types of templates and training data, would be needed to assess its generalizability

### Open Question 3
- Question: What is the impact of incorporating prosodic information on language model training and performance, particularly in low-resource settings?
- Basis in paper: [inferred] The paper discusses the potential benefits of incorporating prosodic information into language model training, citing its importance in human language learning. However, it does not provide empirical evidence of its effectiveness
- Why unresolved: The paper only briefly mentions the idea of using prosodic information and does not conduct any experiments to evaluate its impact. The potential benefits and challenges of incorporating prosody into language models remain unexplored
- What evidence would resolve it: Experiments comparing the performance of language models trained with and without prosodic information, particularly in low-resource settings, would be needed to assess its impact on model training and performance

## Limitations
- Limited ablation studies on ATF patterns prevent isolating which specific synthetic data contributes most to performance gains
- Inverse scaling behavior on Irregular Forms is observed but not thoroughly analyzed to understand underlying mechanisms
- Comparison with baseline models limited to BabyLM-provided baselines rather than state-of-the-art approaches
- No analysis of potential biases introduced by ATF's sentiment patterns or QA templates

## Confidence
- **High confidence**: ATF improves GLUE performance is well-supported by empirical results showing consistent gains across multiple tasks
- **Medium confidence**: Prolonged training up to 200 epochs improves performance on most benchmarks, but inverse scaling exceptions need further investigation
- **Low confidence**: Pre-pretraining on bracketed constituency-labeled text significantly improves MSGS performance is based on experiments not included in final model

## Next Checks
1. **Ablation study of ATF patterns**: Systematically remove different categories of ATF-generated synthetic data and retrain models to isolate which specific patterns contribute most to GLUE performance improvements

2. **Inverse scaling mechanism investigation**: Conduct controlled experiments varying training duration and learning rate schedules to determine whether inverse scaling on Irregular Forms represents genuine overfitting or optimization artifacts

3. **Bias and distribution analysis**: Analyze synthetic data generated by ATF to quantify potential biases introduced by sentiment patterns and QA templates, comparing synthetic vs. natural data distributions across linguistic phenomena