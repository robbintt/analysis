---
ver: rpa2
title: Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models
arxiv_id: '2312.09211'
source_url: https://arxiv.org/abs/2312.09211
tags:
- outlier
- fine-tuning
- low-precision
- language
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses outlier activation challenges in low-precision
  fine-tuning of language models. The authors propose representing outlier activation
  values in 8-bit integers using operator tiling to avoid 16-bit integer matrix multiplication.
---

# Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models

## Quick Facts
- arXiv ID: 2312.09211
- Source URL: https://arxiv.org/abs/2312.09211
- Authors: [Not provided in input]
- Reference count: 6
- This paper addresses outlier activation challenges in low-precision fine-tuning of language models, improving performance on GLUE and SQuAD datasets.

## Executive Summary
This paper tackles the challenge of outlier activations in low-precision fine-tuning of language models. The authors propose a method to represent outlier activation values in 8-bit integers using operator tiling to avoid 16-bit integer matrix multiplication. By separating outlier activations from regular activations and representing them in higher precision (INT12), the approach reduces quantization noise and preserves information. The method demonstrates improved fine-tuning performance on GLUE and SQuAD datasets compared to standard 8-bit fine-tuning with untreated outliers.

## Method Summary
The authors propose representing outlier activation values in 8-bit integers using operator tiling to avoid 16-bit integer matrix multiplication. They separate outlier activations (identified using a threshold) from regular activations and represent them in higher precision (INT12) while keeping the rest in INT8. This reduces the variance of the quantized representation, thereby increasing informativeness. The approach uses integer arithmetic and operator tiling for outlier activations to avoid expensive 16-bit matrix multiplication operations. The mixture distribution model of activations (outliers vs non-outliers) allows for variance reduction through separate treatment, improving information preservation in low-precision.

## Key Results
- Improved fine-tuning performance on GLUE and SQuAD datasets compared to standard 8-bit fine-tuning with untreated outliers
- Gains ranging from 2.1% to 9.2% in accuracy and F1 scores across multiple tasks
- Enables efficient fine-tuning while maintaining model performance, suitable for resource-constrained deployment scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating outlier activations separately in low-precision integer fine-tuning improves performance by reducing quantization noise and preserving information.
- Mechanism: The authors separate outlier activations from regular activations and represent them in higher precision (INT12) while keeping the rest in INT8. This reduces the variance of the quantized representation, thereby increasing informativeness.
- Core assumption: Outlier activations significantly affect the scaling factor and information preservation in low-precision quantization.
- Evidence anchors:
  - [abstract]: "The outlier values in the activation can negatively affect the performance of fine-tuning language models in the low-precision regime since they affect the scaling factor and thus make representing smaller values harder."
  - [section]: "The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively."
  - [corpus]: Weak evidence - the related papers discuss outlier handling but don't provide direct evidence for the specific integer tiling approach.

### Mechanism 2
- Claim: Using integer arithmetic and operator tiling for outlier activations avoids expensive 16-bit matrix multiplication operations.
- Mechanism: By representing outlier activations as integers and using operator tiling, the approach enables efficient computation without resorting to 16-bit integer matrix multiplication.
- Core assumption: Integer arithmetic can effectively replace floating-point operations for the specific computations needed in handling outlier activations.
- Evidence anchors:
  - [abstract]: "The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively."
  - [section]: "Using the advantage of integer number formats, we present a tiling strategy that enables the possibility of using int8 GEMM for all the computation of linear layers."
  - [corpus]: Weak evidence - related papers discuss quantization and outlier handling but don't provide direct evidence for the specific integer tiling approach.

### Mechanism 3
- Claim: The mixture distribution model of activations (outliers vs non-outliers) allows for variance reduction through separate treatment, improving information preservation in low-precision.
- Mechanism: By treating outlier activations as a separate distribution, the approach reduces the overall variance of the quantized representation, which increases sensitivity and informativeness according to the Hammersley-Chapman-Robbins bound.
- Core assumption: The outlier activations can be effectively modeled as a mixture distribution separate from the main activation distribution.
- Evidence anchors:
  - [section]: "Treating outlier activations separately as explained in Section 3.2 closely resembles having a mixture distribution as shown in Figure 3."
  - [section]: "The inequality (11) shows that weighted average of variances of distributions is less than total variance of a mixture distribution. Therefore, treating outlier separately reduces the variance and hence it increases the informativeness."
  - [corpus]: Weak evidence - the related papers discuss outlier handling but don't provide direct evidence for the specific mixture distribution model.

## Foundational Learning

- Concept: Dynamic fixed-point representation (block floating-point)
  - Why needed here: It allows efficient mapping of floating-point values to integers while preserving information, which is crucial for low-precision fine-tuning.
  - Quick check question: How does the dynamic fixed-point representation differ from standard fixed-point representation, and why is it beneficial for handling outliers in low-precision fine-tuning?

- Concept: Sensitivity analysis and informativeness measures
  - Why needed here: These concepts help quantify the impact of low-precision representation on information preservation and guide the design of quantization strategies.
  - Quick check question: Explain how the leverage and sensitivity measures defined by Tukey (1965) relate to information preservation in low-precision number formats.

- Concept: χ2-divergence and Hammersley-Chapman-Robbins bound
  - Why needed here: These distribution distance measures provide a theoretical foundation for understanding the information loss in low-precision quantization and guide the design of quantization strategies.
  - Quick check question: How does the χ2-divergence between the original and quantized distributions relate to the informativeness of the low-precision representation?

## Architecture Onboarding

- Component map:
  Linear layers with integer-only forward and backward propagation -> Dynamic fixed-point conversion units -> Outlier detection and separation logic -> Operator tiling engine for efficient computation -> Integration with standard fine-tuning pipeline

- Critical path:
  Forward pass: Input activation -> Outlier detection -> Separate processing (INT12 for outliers, INT8 for others) -> Tiled matrix multiplication -> Output
  Backward pass: Gradients computed in INT8 format, no special treatment for outliers

- Design tradeoffs:
  - Precision vs. efficiency: Using INT12 for outliers increases precision but also memory usage
  - Complexity vs. performance: The additional logic for outlier detection and separate processing adds complexity but improves fine-tuning performance
  - Hardware compatibility: Integer-only operations are more widely supported but may have different performance characteristics on various hardware platforms

- Failure signatures:
  - Unexpected drop in fine-tuning performance
  - Increased memory usage without corresponding performance gain
  - Longer training times due to additional outlier processing overhead
  - Numerical instability in edge cases

- First 3 experiments:
  1. Implement basic outlier detection and separation, measure impact on a simple linear layer fine-tuning task
  2. Integrate the full integer-only linear layer with outlier handling into a BERT fine-tuning pipeline on a single GLUE task
  3. Compare the proposed approach with standard 8-bit fine-tuning on multiple GLUE tasks and SQuAD datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to larger models beyond BERT-base, such as GPT-3 or other LLMs with significantly more parameters?
- Basis in paper: [inferred] The paper focuses on BERT-base fine-tuning, but doesn't address scalability to larger models.
- Why unresolved: The paper only provides experimental results on BERT-base, leaving questions about the method's effectiveness on larger, more complex models.
- What evidence would resolve it: Experimental results showing the method's performance on larger models like GPT-3 or other contemporary LLMs, including memory usage and accuracy metrics.

### Open Question 2
- Question: What is the impact of different threshold values (γ) on the outlier detection and overall performance of the fine-tuning process?
- Basis in paper: [explicit] The paper uses a fixed threshold γ = 5 for all experiments but doesn't explore the sensitivity to this parameter.
- Why unresolved: The choice of threshold significantly impacts which activations are treated as outliers, yet the paper doesn't investigate optimal threshold selection or sensitivity analysis.
- What evidence would resolve it: A comprehensive study varying the threshold value and measuring its impact on model performance, memory usage, and computational efficiency across different tasks.

### Open Question 3
- Question: How does the proposed integer-only approach compare to other low-precision methods like FP8 in terms of computational efficiency and accuracy?
- Basis in paper: [explicit] The paper claims advantages of integer arithmetic but doesn't provide direct comparisons with other low-precision formats like FP8.
- Why unresolved: The paper discusses benefits of integer representation but lacks comparative analysis with other emerging low-precision formats.
- What evidence would resolve it: Direct experimental comparisons between the proposed integer approach and FP8-based methods on identical tasks, measuring both accuracy and computational efficiency.

### Open Question 4
- Question: What is the impact of outlier treatment on model convergence speed and stability during fine-tuning?
- Basis in paper: [inferred] The paper focuses on final performance metrics but doesn't analyze training dynamics or convergence behavior.
- Why unresolved: While the paper shows improved final performance, it doesn't examine how outlier treatment affects the training process itself, such as convergence speed or stability.
- What evidence would resolve it: Analysis of training curves comparing models with and without outlier treatment, including metrics like loss convergence rate and gradient stability during training.

## Limitations
- The paper assumes that outlier activations can be effectively modeled as a separate distribution, which may not hold for all tasks or model architectures.
- The choice of threshold γ=5 for outlier detection is heuristic and may require task-specific tuning.
- The approach adds complexity to the fine-tuning pipeline, which could introduce additional points of failure or optimization challenges.

## Confidence

- **High Confidence**: The empirical results showing improved performance on GLUE and SQuAD datasets are well-supported by the reported metrics. The approach of separating outliers and using integer arithmetic for efficient computation appears sound.
- **Medium Confidence**: The theoretical justifications, particularly around mixture distribution modeling and variance reduction, are plausible but rely on assumptions about the activation distributions that may not always hold in practice.
- **Low Confidence**: The specific implementation details of the integer tiling strategy and the exact threshold selection for outlier detection are not fully specified, which could impact reproducibility and generalizability.

## Next Checks

1. **Cross-task threshold sensitivity analysis**: Systematically evaluate the impact of different outlier detection thresholds (γ values) across all GLUE tasks and SQuAD datasets to identify optimal threshold ranges and task-specific variations.
2. **Scaling factor stability analysis**: Conduct ablation studies on different scaling factor calculation methods and their impact on both outlier and non-outlier activation representations to understand the trade-offs between precision and efficiency.
3. **Hardware performance benchmarking**: Measure actual runtime performance and memory usage on different hardware platforms (e.g., CPUs, GPUs, TPUs) to validate the claimed efficiency gains from integer-only operations and identify any platform-specific optimizations or limitations.