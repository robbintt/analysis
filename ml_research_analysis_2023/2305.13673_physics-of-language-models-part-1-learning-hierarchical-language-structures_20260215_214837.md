---
ver: rpa2
title: 'Physics of Language Models: Part 1, Learning Hierarchical Language Structures'
arxiv_id: '2305.13673'
source_url: https://arxiv.org/abs/2305.13673
tags:
- cut50
- attention
- cut0
- data
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how pre-trained GPT-2 language models learn
  context-free grammars (CFGs), which are hierarchical language systems capable of
  generating complex and ambiguous sentences. The authors construct synthetic CFGs
  of varying difficulty and demonstrate that GPT-2 can achieve near-perfect accuracy
  in generating strings that adhere to these grammars, even for challenging CFGs with
  long sequences and high ambiguity.
---

# Physics of Language Models: Part 1, Learning Hierarchical Language Structures

## Quick Facts
- arXiv ID: 2305.13673
- Source URL: https://arxiv.org/abs/2305.13673
- Authors: 
- Reference count: 40
- Pre-trained GPT-2 can learn complex context-free grammars, achieving near-perfect accuracy even for challenging hierarchical structures.

## Executive Summary
This paper investigates how transformer language models learn hierarchical language structures by training GPT-2 on synthetic context-free grammars (CFGs). The authors demonstrate that GPT-2 can achieve near-perfect accuracy in generating valid strings from complex CFGs, even those with long sequences and high ambiguity. Through detailed analysis of hidden states and attention patterns, they discover that transformers implicitly encode CFG structure, with hidden states precisely capturing nonterminal ancestor information at boundaries and attention resembling dynamic programming. The study also explores implicit CFGs and model robustness against corrupted input.

## Method Summary
The authors construct seven synthetic CFGs of varying difficulty and pre-train GPT-2 small (12 layers, 12 heads, 768 dims) on strings generated from these grammars using standard auto-regressive language modeling. They evaluate generation accuracy and diversity, implement linear probes to extract nonterminal ancestor information from hidden states, analyze attention patterns with and without position bias, and test robustness by training on corrupted input with grammar errors.

## Key Results
- GPT-2 achieves near-perfect generation accuracy (>95%) on challenging CFGs with up to 729 tokens and high ambiguity
- Hidden states linearly encode nonterminal ancestor information at boundary positions with high accuracy
- Attention patterns show boundary-based behavior resembling dynamic programming algorithms
- Pre-training with corrupted data significantly improves robustness, even with 100% grammar errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer hidden states linearly encode nonterminal (NT) ancestor information at NT boundaries with high accuracy.
- Mechanism: After pre-training, each token's hidden state contains a linearly extractable vector representing its NT ancestor symbol at each CFG level, with the encoding localized near NT boundary positions.
- Core assumption: The auto-regressive language modeling task implicitly learns syntactic structure without explicit supervision of NT symbols.
- Evidence anchors:
  - [abstract] "We discover that the hidden states within the transformer implicitly and precisely encode the CFG structure (such as putting tree node information exactly on the subtree boundary)"
  - [section] "We devise a method to verify that the hidden states of the trained model linearly encode the hidden NT information (in fact, encoding the 'NT symbol' exactly at 'NT boundary') almost perfectly"
- Break condition: If NT ancestor prediction accuracy drops below ~90% on challenging CFGs like cfg3f, suggesting the model doesn't fully encode hierarchical structure.

### Mechanism 2
- Claim: Boundary-based attention allows tokens to attend to most adjacent NT boundaries across different CFG levels, resembling dynamic programming.
- Mechanism: After removing position bias, attention weights show strong preference for tokens at NT boundaries attending to other NT boundaries that are "adjacent" in the CFG tree structure, with distance measured by ancestor index differences.
- Core assumption: The attention mechanism learns to implement parsing operations similar to dynamic programming for CFG verification.
- Evidence anchors:
  - [abstract] "its attention patterns resemble the information passing in a dynamic programming algorithm"
  - [section] "We show that GPT learns two types of attentions... Boundary-based attention enables the model to learn the hierarchical and recursive structure of the CFG, and to generate tokens based on the NT symbols and rules"
- Break condition: If attention patterns don't show decreasing weights with increasing distance between NT boundaries, suggesting the model doesn't learn parsing-like attention.

### Mechanism 3
- Claim: Pre-training with corrupted data improves robustness by teaching the model a "mode switch" between correct and incorrect generation.
- Mechanism: When trained on data with grammar errors, the model learns to detect whether input prefixes are grammatically correct and adjusts its generation accordingly, with low temperatures favoring correct mode.
- Core assumption: The model can learn to distinguish between clean and corrupted input patterns during pre-training.
- Evidence anchors:
  - [abstract] "Adding as little as 10% data perturbation, or even allowing all the training samples to have grammar mistakes, the robust accuracy significantly improves"
  - [section] "Using CFGs, we show when pre-trained with perturbed data, transformer learns a 'mode switch' between intentionally writing and not writing grammar mistake"
- Break condition: If robust accuracy doesn't improve with corrupted training data, or if the model doesn't show temperature-dependent behavior.

## Foundational Learning

- Concept: Context-Free Grammars (CFGs) and their relationship to pushdown automata
  - Why needed here: The paper investigates how transformers learn to generate and parse CFGs, which are fundamental to understanding hierarchical language structures
  - Quick check question: What class of automata is equivalent in power to context-free grammars?

- Concept: Dynamic programming for CFG parsing
  - Why needed here: The paper argues that transformer attention patterns resemble dynamic programming algorithms used for CFG verification
  - Quick check question: How does dynamic programming verify whether a string belongs to a CFG language?

- Concept: Transformer attention mechanisms and positional embeddings
  - Why needed here: The paper analyzes how different attention types (position-based vs boundary-based) and positional embedding variants affect CFG learning
  - Quick check question: What's the key difference between absolute, relative, and rotary positional embeddings?

## Architecture Onboarding

- Component map: GPT-2 small (12 layers, 12 heads, 768 dims) -> variants (GPTrel, GPTrot, GPTpos, GPTuni) -> CFG generation -> evaluation (accuracy, diversity) -> linear probes (NT ancestor/boundary prediction) -> attention analysis
- Critical path: Pre-training on CFG-generated data → evaluate generation accuracy and diversity → probe hidden states for NT ancestor/boundary encoding → analyze attention patterns → test robustness with corrupted data
- Design tradeoffs: Using synthetic CFG data allows controlled experiments but may not capture all natural language complexities; weaker attention variants help isolate mechanisms but sacrifice performance
- Failure signatures: Low generation accuracy (<90%) on challenging CFGs indicates poor learning; low NT ancestor prediction accuracy suggests hidden states don't encode structure; uniform attention patterns indicate lack of hierarchical understanding
- First 3 experiments:
  1. Pre-train GPT-2 on cfg3f dataset and evaluate generation accuracy on both clean and corrupted prefixes
  2. Implement linear probing to extract NT ancestor information from hidden states and measure prediction accuracy
  3. Visualize attention patterns with and without position bias to identify boundary-based attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transformers learn and generalize from context-free grammars (CFGs) with overlapping or ambiguous rules?
- Basis in paper: [explicit] The paper demonstrates that transformers can learn CFGs with overlapping rules, but does not fully explain the mechanisms behind this generalization.
- Why unresolved: While the paper shows transformers can handle ambiguous CFGs, the underlying learning process for such complex structures remains unclear.
- What evidence would resolve it: Detailed analysis of the attention patterns and hidden states during training on ambiguous CFGs, and comparison with CFGs without ambiguity.

### Open Question 2
- Question: Can transformers effectively learn and generalize from context-sensitive grammars, which are more complex than CFGs?
- Basis in paper: [inferred] The paper focuses on CFGs, suggesting an extension to more complex grammars could be a future research direction.
- Why unresolved: The paper does not explore context-sensitive grammars, leaving their learnability by transformers an open question.
- What evidence would resolve it: Experiments training transformers on context-sensitive grammars and evaluating their performance compared to CFGs.

### Open Question 3
- Question: How does the robustness of transformers against grammar errors in real-world data relate to their ability to learn CFGs?
- Basis in paper: [explicit] The paper discusses model robustness against corrupted input and suggests that including low-quality data during pre-training improves robustness.
- Why unresolved: The paper does not fully explore the connection between CFG learning and real-world grammar error robustness.
- What evidence would resolve it: Comparative studies of transformer performance on real-world data with grammar errors, trained on CFGs with varying levels of noise.

## Limitations

- The analysis of attention patterns relies on manually defined distance metrics that may not capture the model's internal reasoning
- CFG generation represents a highly synthetic domain that may not generalize to natural language processing
- The "mode switch" mechanism for robustness is inferred from behavior patterns rather than directly observed

## Confidence

**High confidence**: GPT-2 can learn to generate strings from CFGs with high accuracy (>95% even on challenging CFGs like cfg3f)

**Medium confidence**: Hidden states linearly encode NT ancestor information and attention patterns resemble dynamic programming, though these findings rely on specific probe architectures and distance metrics

**Medium confidence**: Robustness improves with corrupted training data through a proposed "mode switch" mechanism, but the underlying mechanism is inferred rather than directly observed

## Next Checks

1. **Intervention Study on Attention**: Conduct targeted attention masking experiments where specific attention heads or attention patterns between NT boundaries are selectively removed during inference. Measure how this affects generation accuracy to directly test whether boundary-based attention is essential for CFG learning.

2. **Cross-Domain Generalization**: Test whether models trained on CFGs can successfully parse or generate strings from more complex formal languages or simplified natural language structures (such as simple context-sensitive grammars). This would validate whether the learned mechanisms generalize beyond the synthetic domain.

3. **Alternative Probe Architectures**: Implement and compare multiple different linear probe architectures (varying in position weighting, layer selection, and attention mechanisms) for extracting NT information from hidden states. Analyze whether the high accuracy reported is robust across different probe designs or specific to the chosen architecture.