---
ver: rpa2
title: Module-wise Training of Neural Networks via the Minimizing Movement Scheme
arxiv_id: '2309.17357'
source_url: https://arxiv.org/abs/2309.17357
tags:
- training
- modules
- module-wise
- table
- trgl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the stagnation problem in module-wise training
  of neural networks, where early layers overfit and deeper layers stop improving
  test accuracy. The proposed method, TRGL (Transport-Regularized Greedy Learning),
  introduces a module-wise regularization inspired by the minimizing movement scheme
  for gradient flows in distribution space.
---

# Module-wise Training of Neural Networks via the Minimizing Movement Scheme

## Quick Facts
- arXiv ID: 2309.17357
- Source URL: https://arxiv.org/abs/2309.17357
- Reference count: 40
- Primary result: TRGL improves test accuracy of module-wise trained networks while using 10-60% less memory

## Executive Summary
This paper addresses the stagnation problem in module-wise training where early layers overfit and deeper layers stop improving test accuracy. The authors propose Transport-Regularized Greedy Learning (TRGL), a method that introduces a regularization term inspired by the minimizing movement scheme for gradient flows in distribution space. By minimizing the kinetic energy of each module, TRGL forces modules to preserve the geometry of their input as much as possible, preventing early overfitting and allowing deeper layers to continue improving accuracy.

## Method Summary
TRGL adds a kinetic energy regularization term to the loss function of each module during module-wise training. The method is based on the idea that each module should act as a proximal step in a minimizing movement scheme for optimizing the loss over probability distributions in Wasserstein space. This regularization ensures that modules behave like optimal transport maps, inheriting regularity properties that improve generalization. The approach is applicable to various architectures including ResNets, VGG, and Transformers, and works with both sequential and parallel module-wise training paradigms.

## Key Results
- TRGL consistently improves test accuracy of module-wise trained networks across multiple architectures and datasets
- The method often outperforms end-to-end training while using 10-60% less memory
- TRGL is particularly effective at reducing overfitting and improving accuracy in smaller datasets and when using fewer modules
- The regularization is effective across a wide range of τ values (0.03 to 100)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TRGL avoids the stagnation problem by minimizing the kinetic energy of each module, thereby forcing modules to preserve the geometry of their input and not destroy task-relevant information.
- Mechanism: Adding a regularization term 1/(2τ) * ||T(x) - x||² to the loss penalizes large displacements caused by each module, which in practice limits overfitting in early layers and allows deeper layers to continue improving accuracy.
- Core assumption: The residual form T = id + r makes the kinetic energy term equal to ||r||², and that minimizing this term encourages the module to behave like an optimal transport map, which is regular and stable.
- Evidence anchors:
  - [abstract] "we minimize the kinetic energy of the modules along with the training loss... forces them to preserve the geometry of the problem as much as possible"
  - [section] "To keep greedily-trained modules from overfitting and destroying information needed later, we penalize their kinetic energy to force them to preserve the geometry of the problem as much as possible"
  - [corpus] No direct evidence for transport map claim; only indirect from optimal transport background.
- Break condition: If residual assumption fails (non-residual architectures with mismatched dimensions), the kinetic energy regularization is undefined or less effective.

### Mechanism 2
- Claim: TRGL modules act as proximal steps in the minimizing movement scheme for minimizing the loss over probability distributions, ensuring that each module progressively improves the data representation.
- Mechanism: By solving min_T,F L(F,T∘ρ) + (1/2τ)W²(T∘ρ,ρ), each module performs a proximal gradient step in Wasserstein space toward a minimizer of the loss functional Z(μ) = min_F L(F,μ), where ρ is the data distribution.
- Core assumption: The data distribution ρ is absolutely continuous and the network has sufficient representational power to approximate optimal transport maps between successive module outputs.
- Evidence anchors:
  - [section] "Solving Problems (3) is equivalent to following a minimizing movement scheme (MMS) in distribution space for minimizing Z(μ) := min_F L(F, μ)"
  - [section] "the modules progressively minimize the loss and explains why the method avoids the accuracy collapse observed in module-wise training"
  - [corpus] No direct experimental evidence for Wasserstein-space convergence; only theoretical.
- Break condition: If the loss functional Z is not lower semi-continuous or not λ-geodesically convex, the MMS convergence guarantees fail.

### Mechanism 3
- Claim: The transport regularization improves module regularity, leading to better generalization and reduced overfitting.
- Mechanism: By forcing T to be an optimal transport map between successive distributions, the module inherits Hölder continuity and stability properties from OT theory, which empirically reduces overfitting.
- Core assumption: The optimization algorithm returns an approximate solution that is an ϵ-optimal transport map, so that stability bounds of the form ||T(x) - T(y)|| ≤ 2ε + C||x - y||^η hold almost everywhere.
- Evidence anchors:
  - [section] "T^τ_k is an optimal transport map between its input and output distributions, which means that it comes with some regularity... these networks generalize better and overfit less in practice"
  - [section] "T^τ_k is η-Hölder continuous almost everywhere and if the optimization algorithm we use to return an approximate solution pair... then we have the following stability property"
  - [corpus] No direct experimental measurement of Hölder continuity or OT map property.
- Break condition: If the optimization returns a poor approximation to the OT map, the regularity benefits vanish.

## Foundational Learning

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: TRGL's theoretical justification relies on viewing each module as performing an optimal transport step in distribution space; understanding W₂ distance is key to grasping the regularization's effect.
  - Quick check question: What is the dual formulation of the Wasserstein-2 distance between two probability measures?

- Concept: Gradient flows in metric spaces
  - Why needed here: The minimizing movement scheme interpretation of TRGL depends on viewing the loss as a functional over P(Ω) and following its gradient flow in the Wasserstein space.
  - Quick check question: How does the minimizing movement scheme differ from standard gradient descent in Euclidean space?

- Concept: Residual network architectures
  - Why needed here: TRGL's kinetic energy regularization is naturally defined for residual blocks (T = id + r), so understanding how residual connections work is essential to applying the method.
  - Quick check question: In a ResNet block, how is the output computed from the input and the residual function?

## Architecture Onboarding

- Component map:
  - Input -> Module T₁ -> Module T₂ -> ... -> Module Tₙ -> Output
  - Each module Tₖ: One or more layers (often residual blocks) implementing transformation T
  - Each module has auxiliary classifier Fₖ producing predictions from Tₖ's output
  - Regularization term: (1/2τ) * sum of squared residual norms within each Tₖ

- Critical path:
  1. For each batch, feed input through modules in order
  2. At module k, compute local loss L(Fₖ, Tₖ(Gₖ₋₁(x))) + (1/2τ) * kinetic energy
  3. Backpropagate only within Tₖ and Fₖ to update their parameters
  4. Pass Tₖ's output to next module; repeat

- Design tradeoffs:
  - Memory vs accuracy: More modules reduce memory but can hurt accuracy without regularization; TRGL mitigates this
  - Regularization weight τ: Smaller τ enforces stricter geometry preservation but may slow convergence; larger τ reduces effect
  - Parallel vs sequential: Parallel offers better accuracy but higher memory; sequential saves memory but may need more epochs

- Failure signatures:
  - Early modules overfit: If test accuracy drops after early modules, τ may be too large or regularization missing
  - Later modules stall: If deeper modules do not improve accuracy, modules may be too shallow or τ too small
  - Training instability: If loss oscillates, τ may be too small or learning rate too high

- First 3 experiments:
  1. Implement TRGL on a small ResNet (e.g., 10-block) with 2 modules on CIFAR10; compare VanGL vs TRGL accuracy per module
  2. Vary τ across [0.01, 0.1, 1, 10] on the same setup; plot accuracy vs τ to find sweet spot
  3. Test sequential vs parallel TRGL on TinyImageNet with 4 modules; measure memory usage and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of regularization weight τ affect the performance of TRGL across different architectures and datasets?
- Basis in paper: [explicit] The authors mention varying τ along the depth k and across SGD iterations i, but only find one setting where this works best. They also show in Figure 4 that TRGL performs better than VanGL for values of τ from 0.03 to 100.
- Why unresolved: The paper does not provide a systematic study of the impact of τ on different architectures and datasets. The sensitivity analysis in Figure 4 is limited to one specific experiment.
- What evidence would resolve it: A comprehensive ablation study varying τ across a range of architectures (ResNet, VGG, Transformer) and datasets (TinyImageNet, CIFAR10, CIFAR100, STL10) to determine optimal τ values and their impact on performance.

### Open Question 2
- Question: Can the theoretical guarantees of TRGL (regularity of modules, progressive task solving) be extended to other gradient flow formulations beyond the minimizing movement scheme?
- Basis in paper: [explicit] The authors prove that TRGL leads to greedy modules that are regular and progressively solve the task by leveraging connections with gradient flows in distribution space. They also mention in Section 7 that future work can experiment with working in Wasserstein space W_p for p≠2.
- Why unresolved: The paper only explores the minimizing movement scheme for gradient flows. Other gradient flow formulations might offer different theoretical guarantees or computational advantages.
- What evidence would resolve it: Theoretical analysis and experimental validation of TRGL when using different gradient flow formulations, such as gradient flows in other Wasserstein spaces or using different metrics.

### Open Question 3
- Question: How does TRGL perform when combined with other module-wise training methods like DGL, InfoPro, or Sedona?
- Basis in paper: [explicit] The authors state that "being a regularization, the method can easily be combined with other layer-wise training methods." They also mention that Sedona applies architecture search to decide on where to split the network into modules and what auxiliary classifier to use before module-wise training.
- Why unresolved: The paper does not experimentally investigate the combination of TRGL with other state-of-the-art module-wise training methods. It only compares TRGL against these methods individually.
- What evidence would resolve it: Experiments combining TRGL with other module-wise training methods, such as using TRGL's regularization in conjunction with Sedona's architecture search or InfoPro's information-theoretic losses, and comparing the results to each method individually.

## Limitations
- The regularization's effectiveness depends heavily on the residual parameterization, limiting applicability to non-residual architectures
- The claim that TRGL modules act as optimal transport maps lacks direct experimental validation
- The optimal regularization strength τ appears dataset-dependent, requiring extensive tuning

## Confidence
- Mechanism 1: Medium - The kinetic energy regularization is clearly implemented, but the OT map properties are theoretical
- Mechanism 2: Medium - The MMS interpretation is mathematically sound but unverified experimentally
- Mechanism 3: Low - The claimed regularity improvements are not directly measured in experiments

## Next Checks
1. Measure actual OT map properties (e.g., Lipschitz continuity) of TRGL modules vs baseline modules to verify the claimed regularity improvements
2. Test TRGL on non-residual architectures (e.g., DenseNet) to assess generalizability beyond the residual parameterization assumption
3. Conduct extensive ablation studies varying τ across more values and architectures to identify consistent optimal regularization strategies