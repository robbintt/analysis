---
ver: rpa2
title: 'InGram: Inductive Knowledge Graph Embedding via Relation Graphs'
arxiv_id: '2305.19987'
source_url: https://arxiv.org/abs/2305.19987
tags:
- graph
- relations
- relation
- ingram
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes INGRAM, an inductive knowledge graph embedding
  method that can generate embeddings for new relations as well as new entities at
  inference time. The key idea is to define a relation graph where each node corresponds
  to a relation and edge weights indicate affinity between relations.
---

# InGram: Inductive Knowledge Graph Embedding via Relation Graphs

## Quick Facts
- arXiv ID: 2305.19987
- Source URL: https://arxiv.org/abs/2305.19987
- Reference count: 38
- Primary result: INGRAM achieves up to 0.31 MRR on link prediction tasks, outperforming 14 state-of-the-art methods on 12 datasets

## Executive Summary
The paper proposes INGRAM, an inductive knowledge graph embedding method that can generate embeddings for new relations and new entities at inference time. The key innovation is defining a relation graph where nodes represent relations and edge weights indicate affinity between relations. INGRAM uses attention mechanisms to aggregate neighboring embeddings to generate both relation and entity embeddings, enabling full inductive reasoning without retraining.

## Method Summary
INGRAM constructs a relation graph with affinity-weighted edges between relations, then uses attention-based aggregation at two levels: first aggregating over neighboring relations to generate relation embeddings, then aggregating over neighboring entities and connected relations to generate entity embeddings. The method employs a dynamic split and random re-initialization strategy during training to improve generalization to unseen graphs. The final embeddings are used with a DistMult-like scoring function for link prediction tasks.

## Key Results
- Achieves up to 0.31 MRR on link prediction tasks
- Outperforms 14 different state-of-the-art methods across 12 datasets
- Shows especially strong performance when the ratio of new relations is high
- Demonstrates effectiveness with varied ratios of new relations and entities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The relation graph aggregation allows inductive generation of embeddings for new relations by leveraging affinity-weighted neighborhoods.
- **Mechanism**: The method defines a relation graph where nodes are relations and edge weights indicate affinity. During training, it learns attention weights to aggregate neighbor relation embeddings, which are then used at inference to generate embeddings for unseen relations based on their local neighborhood structure.
- **Core assumption**: Relations that frequently co-occur with the same entities share structural similarity, and their embeddings can be meaningfully aggregated to approximate a new relation's embedding.
- **Evidence anchors**:
  - [abstract] "Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them."
  - [section 4] "To represent which relations are associated with which entities, we create two matrices Eh ∈ Rn×m and Et ∈ Rn×m where the subscripts h and t indicate head and tail, respectively."
  - [section 5.1] "We update each relation's representation by aggregating its own and neighbors' representation vectors... where the attention value α(l)ij is defined by... incorporating the affinity between the relations."
- **Break condition**: If new relations are structurally dissimilar to any existing relations in the training graph, the aggregation may produce poor approximations.

### Mechanism 2
- **Claim**: The dual aggregation (relation-level + entity-level) enables full inductive reasoning for both new entities and new relations simultaneously.
- **Mechanism**: Relation embeddings are first computed by aggregating over neighboring relations in the relation graph. Entity embeddings are then computed by aggregating over neighboring entities and the relations connected to them, using the relation embeddings from the first step. This hierarchical approach propagates structural information from known relations to new ones via the entity graph.
- **Core assumption**: The structure of the knowledge graph encodes sufficient relational patterns that can be transferred to new entities and relations through neighborhood aggregation.
- **Evidence anchors**:
  - [abstract] "INGRAM learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism."
  - [section 5.2] "We update a representation vector of vi by aggregating the representation vectors of its neighbors, its own vector, and the representation vectors of the relations adjacent to vi."
  - [section 5.3] "Given the representation vectors provided in Section 5.1 and Section 5.2, we compute the final embedding vectors: zk := M z(L)k (k = 1, · · ·, m) for relations and hi := cM h(bL)i (i = 1, · · ·, n) for entities."
- **Break condition**: If the new entities and relations form completely disconnected or highly sparse subgraphs, the aggregation may fail to capture meaningful patterns.

### Mechanism 3
- **Claim**: The dynamic split and random re-initialization strategy improves generalization to unseen graphs.
- **Mechanism**: During training, the triplet set is randomly split into known facts and targets each epoch, and all feature vectors are re-initialized. This forces the model to learn how to generate embeddings from scratch for each epoch, improving its ability to handle random initializations at inference time.
- **Core assumption**: Models that learn to compute embeddings from random initializations are more robust when applied to entirely new graphs with no prior entity/relation embeddings.
- **Evidence anchors**:
  - [section 5.4] "For every epoch, we randomly re-split Ftr and Ttr... At the beginning of each epoch, we initialize all feature vectors using Glorot initialization."
  - [section 5.4] "This dynamic split and re-initialization strategy allows INGRAM to robustly learn the model parameters, which makes the model more easily generalizable to an inference graph."
  - [section 6.4] "Removing each module leads to a noticeable degradation in the performance of INGRAM... while INGRAM achieves the best performance when all modules come together."
- **Break condition**: If the random re-initialization is too aggressive or the splits are not balanced, the model may fail to converge or learn stable patterns.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their aggregation mechanisms
  - Why needed here: INGRAM is fundamentally a GNN-based approach that aggregates neighbor information for both relations and entities. Understanding how GNNs propagate and aggregate information is essential to grasp how INGRAM generalizes to new entities and relations.
  - Quick check question: What is the difference between mean, sum, and attention-based aggregation in GNNs, and why might attention be preferred for knowledge graphs?

- **Concept**: Inductive vs. transductive learning
  - Why needed here: The paper contrasts INGRAM's inductive approach with traditional transductive methods. Knowing the distinction is crucial to understanding why existing methods fail when new relations appear and how INGRAM addresses this limitation.
  - Quick check question: In what way does inductive learning differ from transductive learning in the context of knowledge graph completion, and why is this distinction important for real-world applications?

- **Concept**: Knowledge graph embedding scoring functions
  - Why needed here: INGRAM uses a variant of DistMult to score triplets. Understanding how scoring functions work (e.g., DistMult, TransE, RotatE) is important for understanding how the model evaluates link prediction quality and how it is trained.
  - Quick check question: How does the DistMult scoring function f(h, r, t) = h^T diag(r) t differ from TransE's scoring function, and what are the implications for modeling relational patterns?

## Architecture Onboarding

- **Component map**: Relation Graph Construction -> Relation-level Aggregation -> Entity-level Aggregation -> Scoring Layer -> Training Loop
- **Critical path**: Relation Graph → Relation Aggregation → Entity Aggregation → Scoring → Loss Computation. The relation graph and its aggregation are the core innovations enabling full inductive reasoning.
- **Design tradeoffs**:
  - Pros: Full inductive capability for both entities and relations; no need for retraining on new graphs; scalable to large graphs.
  - Cons: Requires constructing a relation graph, which may be memory-intensive; performance depends on structural similarity of new relations to known ones; no explicit modeling of textual or attribute information.
- **Failure signatures**:
  - Poor performance on datasets with very sparse relation co-occurrence patterns.
  - Degraded performance when new relations are structurally dissimilar from any training relations.
  - Overfitting to training relation patterns if dynamic split is disabled.
- **First 3 experiments**:
  1. Run INGRAM on a small synthetic dataset with known relation patterns to verify basic functionality.
  2. Compare INGRAM with a transductive baseline (e.g., TransE) on a dataset with new entities but no new relations to isolate the effect of entity-level aggregation.
  3. Evaluate INGRAM on a dataset where new relations are semantically similar to training relations vs. dissimilar ones to test the limits of relation-level aggregation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can INGRAM be extended to incorporate known-relation-specific patterns into its inferences when known relations are dominant?
- **Basis in paper**: Explicit. The paper states "We will investigate the ways in which INGRAM can incorporate known-relation-specific patterns into inferences when known relations are dominant."
- **Why unresolved**: The paper acknowledges this as an open research direction but does not provide solutions.
- **What evidence would resolve it**: Experimental results showing improved performance on datasets where known relations are dominant after incorporating known-relation-specific patterns into INGRAM.

### Open Question 2
- **Question**: What are the theoretical bounds on the expressiveness of INGRAM compared to other GNN-based methods for knowledge graph embedding?
- **Basis in paper**: Inferred. The paper mentions "We also plan to do the theoretical analysis of INGRAM as done in (Xu et al., 2019), (Barcelo et al., 2022) and (Hamilton et al., 2017)" and discusses how INGRAM learns to compute embeddings using random features.
- **Why unresolved**: The paper does not provide theoretical analysis of INGRAM's expressiveness.
- **What evidence would resolve it**: Formal proofs or theoretical results showing the expressive power of INGRAM and comparing it to other methods.

### Open Question 3
- **Question**: How can INGRAM be made robust and reliable to noisy information in a given knowledge graph?
- **Basis in paper**: Explicit. The paper states "Finally, we will look into how we can make the predictions of INGRAM robust and reliable to possibly noisy information in a given knowledge graph."
- **Why unresolved**: The paper acknowledges this as an open research direction but does not provide solutions.
- **What evidence would resolve it**: Experimental results showing improved robustness of INGRAM to noisy data, possibly through modifications to the model or training procedure.

## Limitations
- Performance degradation when new relations are structurally dissimilar to any training relations is not extensively quantified
- Computational complexity of constructing and maintaining the relation graph for very large knowledge graphs is not addressed
- Lacks explicit evaluation of INGRAM's ability to handle new relations that are semantically related but structurally dissimilar to training relations

## Confidence
- **High**: The core mechanism of relation graph aggregation and dual-level attention is well-specified and supported by experimental results across multiple datasets.
- **Medium**: The claim that INGRAM outperforms 14 state-of-the-art methods is supported by extensive experiments, but the exact implementation details of the attention mechanism and multi-head configuration are not fully specified.
- **Low**: The paper's claims about INGRAM's ability to handle completely new graphs with no prior entity/relation embeddings are plausible but not thoroughly validated on entirely unseen knowledge graphs.

## Next Checks
1. **Structure Similarity Test**: Evaluate INGRAM on a dataset where new relations are semantically related but structurally dissimilar to training relations to test the limits of relation-level aggregation.
2. **Memory Efficiency Analysis**: Measure the memory usage and computational time of INGRAM on large knowledge graphs to assess scalability and identify potential bottlenecks.
3. **Hyperparameter Sensitivity**: Conduct a thorough sensitivity analysis of INGRAM's performance to key hyperparameters like the number of attention heads, layer dimensions, and learning rates to provide more robust implementation guidelines.