---
ver: rpa2
title: Skew-Probabilistic Neural Networks for Learning from Imbalanced Data
arxiv_id: '2312.05878'
source_url: https://arxiv.org/abs/2312.05878
tags:
- data
- imbalanced
- datasets
- class
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SkewPNNs, a classifier using probabilistic
  neural networks with skew-normal kernel functions to address imbalanced data distribution.
  SkewPNNs leverage the skew-normal distribution's flexibility for non-symmetric data
  and use the Bat optimization algorithm for hyperparameter tuning.
---

# Skew-Probabilistic Neural Networks for Learning from Imbalanced Data

## Quick Facts
- arXiv ID: 2312.05878
- Source URL: https://arxiv.org/abs/2312.05878
- Reference count: 40
- Key outcome: SkewPNNs demonstrate higher AUC-ROC and F1-scores across multiple benchmark datasets compared to state-of-the-art methods.

## Executive Summary
This paper introduces Skew-Probabilistic Neural Networks (SkewPNNs), a novel classifier that addresses imbalanced data distribution by using probabilistic neural networks with skew-normal kernel functions. The approach leverages the flexibility of skew-normal distributions to better represent non-symmetric data distributions, particularly useful for minority class densities that are often skewed. The method employs a Bat optimization algorithm for hyperparameter tuning, achieving substantial performance improvements over existing methods on both balanced and imbalanced datasets.

## Method Summary
SkewPNNs modify traditional probabilistic neural networks by replacing Gaussian kernels with skew-normal kernels, allowing better modeling of asymmetric distributions common in imbalanced datasets. The model architecture consists of an input layer, pattern layer with neurons computing PDFs using skew-normal kernels, a summation layer that aggregates outputs by class, and an output layer providing classification probabilities. The Bat optimization algorithm is employed to tune hyperparameters, specifically the smoothing parameter and skew shape parameter, through an iterative search process inspired by echolocation behavior. The approach is validated through 10-fold cross-validation on multiple benchmark datasets from UCI and KEEL repositories.

## Key Results
- SkewPNNs achieved higher AUC-ROC scores than baseline classifiers (DT, NN, AB, RF, HDDT, HDRF, XGB) on all tested datasets
- The proposed method showed improved F1-scores across imbalanced datasets with varying class imbalance ratios
- BA-SkewPNNs variant performed best overall, demonstrating the effectiveness of Bat algorithm optimization for hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SkewPNNs improve classification of minority classes by using a skew-normal kernel instead of Gaussian kernel.
- Mechanism: The skew-normal kernel introduces asymmetry to better fit data distributions with non-zero skewness, allowing the model to better represent minority class densities that are often skewed.
- Core assumption: The true data distribution has significant skewness that a symmetric Gaussian kernel cannot adequately capture.
- Evidence anchors:
  - [abstract] "By leveraging the skew-normal distribution, which offers increased flexibility, particularly for imbalanced and non-symmetric data, our proposed Skew Probabilistic Neural Networks (SkewPNNs) can better represent underlying class densities."
  - [section 3.1] "The skew normal distribution can be used as a kernel function in PNNs for the situation when the data exhibits skewed distributions with a long tail."
- Break condition: If the underlying data distribution is approximately symmetric, the skew-normal kernel may not provide significant advantages over Gaussian.

### Mechanism 2
- Claim: Bat algorithm optimization finds better hyperparameters for SkewPNNs than manual or grid search methods.
- Mechanism: The Bat algorithm explores the hyperparameter space more effectively by using echolocation-inspired search, finding optimal smoothing parameters for each pattern layer.
- Core assumption: The hyperparameter space is complex enough that heuristic search outperforms exhaustive or random search methods.
- Evidence anchors:
  - [abstract] "Hyperparameter fine-tuning is imperative to optimize the performance of the proposed approach on imbalanced datasets. To this end, we employ a population-based heuristic algorithm, the Bat optimization algorithm, to explore the hyperparameter space effectively."
  - [section 2.3] "This versatility and effectiveness render the bat algorithm a valuable and potent tool across a range of problem domains"
- Break condition: If the hyperparameter space is relatively smooth and convex, simpler optimization methods might be sufficient.

### Mechanism 3
- Claim: The statistical consistency proof ensures SkewPNNs will converge to true distributions as sample size increases.
- Mechanism: By proving that the skew-normal kernel satisfies conditions (A1)-(A4) for consistency, we guarantee that density estimates improve with more data.
- Core assumption: The skew-normal distribution is appropriate for modeling the true underlying distribution of the data.
- Evidence anchors:
  - [section 3.4] "Theorem 1. Suppose K(x; ξ, ω2, α) (as in Eqn. 9) is a Borel function satisfying the conditions... then the estimate fn(x) is consistent in quadratic mean"
  - [section 3.4] "Parzen [52] proved that the estimate fn(x) is consistent in quadratic mean if conditions (A1)-(A4) are met which satisfies in this case."
- Break condition: If the true distribution is not well-approximated by skew-normal distributions, consistency guarantees may not translate to good performance.

## Foundational Learning

- Concept: Probability density estimation using kernel methods
  - Why needed here: SkewPNNs are built on Parzen window estimation, which requires understanding how kernel functions estimate probability densities
  - Quick check question: What property must a kernel function satisfy to be a valid probability density estimator?

- Concept: Statistical consistency of estimators
  - Why needed here: The paper proves consistency of SkewPNN density estimates, which requires understanding what consistency means in statistical estimation
  - Quick check question: What does it mean for an estimator to be consistent in quadratic mean?

- Concept: Metaheuristic optimization algorithms
  - Why needed here: The Bat algorithm is used for hyperparameter optimization, requiring understanding of how population-based search algorithms work
  - Quick check question: How does the Bat algorithm balance exploration and exploitation during the search process?

## Architecture Onboarding

- Component map:
  Input -> Pattern layer (skew-normal kernels) -> Summation layer (class aggregation) -> Output layer (classification probabilities) -> Bat optimizer (hyperparameter tuning)

- Critical path:
  1. Normalize input data
  2. Compute pattern layer PDFs using skew-normal kernel with current hyperparameters
  3. Aggregate through summation layer
  4. Apply Bat algorithm to optimize hyperparameters
  5. Output classification probabilities

- Design tradeoffs:
  - Skew-normal kernel vs Gaussian: More flexible but requires additional parameter (α) to estimate
  - Bat algorithm vs grid search: More efficient exploration but introduces stochasticity
  - Single smoothing parameter vs per-pattern parameters: Simpler vs more expressive

- Failure signatures:
  - Poor minority class performance despite skew-normal kernel: May indicate data isn't actually skewed
  - Slow convergence of Bat algorithm: May need adjusted parameters or different optimization method
  - Inconsistent results across runs: May need to address stochastic elements

- First 3 experiments:
  1. Compare SkewPNNs vs standard PNNs on a synthetic skewed dataset with known parameters
  2. Test sensitivity of Bat algorithm to its own hyperparameters (population size, max iterations)
  3. Evaluate performance degradation as data becomes more symmetric to understand when skew-normal kernel loses advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SkewPNN method compare to deep learning approaches on tabular imbalanced data?
- Basis in paper: [explicit] The paper states "Since advanced deep learning models do not work well on tabular data, we did not consider them in this study [58]."
- Why unresolved: The authors deliberately excluded deep learning models from their comparison due to known limitations with tabular data, leaving a gap in understanding whether SkewPNN could outperform modern deep learning approaches on this data type.
- What evidence would resolve it: A systematic comparison between SkewPNN and state-of-the-art deep learning models (such as TabNet, NODE, or other tabular-specific architectures) on a common set of imbalanced tabular datasets, measuring standard metrics like AUC-ROC and F1-score.

### Open Question 2
- Question: How can the SkewPNN approach be extended to handle imbalanced regression problems where targets are continuous?
- Basis in paper: [explicit] The conclusion section mentions "future research avenues could focus on working on imbalanced regression problems [71] where hard boundaries between classes do not exist."
- Why unresolved: While the paper demonstrates success on classification tasks, regression problems with imbalanced continuous targets present different challenges that the current framework does not address.
- What evidence would resolve it: A modified SkewPNN framework that incorporates continuous density estimation techniques and demonstrates improved performance on benchmark imbalanced regression datasets compared to existing regression methods.

### Open Question 3
- Question: What is the theoretical limit of the computational efficiency gains from using SkewPNN compared to other probabilistic neural network approaches?
- Basis in paper: [explicit] The authors claim SkewPNN is "computationally inexpensive" and provide theoretical complexity analysis, but don't quantify the practical efficiency gains.
- Why unresolved: While theoretical complexity is analyzed, the actual runtime performance and scalability limits compared to Gaussian PNN and other methods on large-scale datasets remains unmeasured.
- What evidence would resolve it: Systematic benchmarking of SkewPNN against Gaussian PNN and other classifiers on datasets of increasing size, measuring both training and prediction times, with analysis of how performance scales with dimensionality and sample size.

## Limitations
- The experiments are limited to 10 datasets, which may not fully represent the diversity of imbalanced data scenarios
- The Bat optimization algorithm introduces stochastic elements that may lead to inconsistent results across different runs
- While the skew-normal kernel provides flexibility, it adds an additional parameter (α) that must be estimated, potentially increasing computational complexity

## Confidence

- High confidence: The theoretical foundation for skew-normal kernels and consistency proofs
- Medium confidence: The empirical results showing improved performance on benchmark datasets
- Low confidence: Claims about the superiority of Bat algorithm over other hyperparameter optimization methods

## Next Checks

1. Test SkewPNNs on datasets with varying degrees of skewness to validate the claim that skew-normal kernels are superior to Gaussian kernels
2. Compare Bat algorithm optimization with grid search and random search on the same hyperparameter space to quantify efficiency gains
3. Evaluate class-wise performance metrics to understand if improvements come from better minority class predictions specifically