---
ver: rpa2
title: On the Reliability of Watermarks for Large Language Models
arxiv_id: '2306.04634'
source_url: https://arxiv.org/abs/2306.04634
tags:
- text
- detection
- human
- watermark
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Watermarking is effective for detecting machine-generated text
  even after human and AI paraphrasing, but requires sufficient token length. We study
  watermark reliability under realistic scenarios where text is paraphrased by humans
  or LLMs, mixed with human text, or paraphrased by other models.
---

# On the Reliability of Watermarks for Large Language Models

## Quick Facts
- **arXiv ID**: 2306.04634
- **Source URL**: https://arxiv.org/abs/2306.04634
- **Reference count**: 40
- **Primary result**: Watermarking remains effective for detecting machine-generated text even after human and AI paraphrasing, but requires sufficient token length.

## Executive Summary
This study evaluates the reliability of watermarking as a detection method for machine-generated text under realistic attack scenarios. The authors systematically test watermark robustness against human paraphrasing, machine paraphrasing using GPT and Dipper models, and copy-paste attacks where watermarked text is mixed with human text. Across all attack types, watermarking demonstrates superior performance compared to retrieval and loss-based detection methods, primarily due to its favorable sample complexity - detection accuracy improves steadily with more observed tokens.

The key finding is that while human paraphrasing poses the strongest challenge to watermark detection, the method still succeeds with sufficient token length (approximately 800 tokens). Machine paraphrasing causes smaller performance drops, and watermark strength recovers as more tokens are observed. The study concludes that watermarking provides a robust detection mechanism because it enforces a persistent statistical difference between machine and human text distributions that accumulates over sequence length.

## Method Summary
The researchers generated watermarked text using llama-7B and vicuna-7B models with token coloring schemes (γ=0.25, δ=2.0, h=1) on C4 and LFQA datasets. They then applied three types of attacks: machine paraphrasing using GPT-3.5-turbo and Dipper models, human paraphrasing from a controlled study, and copy-paste attacks where watermarked spans were embedded in human text. Detection performance was evaluated using ROC-AUC, true positive rates at fixed false positive rates, and z-scores. The study compared watermarking against retrieval methods (BM25) and DetectGPT, measuring performance across varying token lengths from 100 to 2000 tokens.

## Key Results
- Watermarking achieves AUC > 0.99 without attacks, dropping to ~0.8-0.9 under human paraphrasing but recovering with longer texts (~800 tokens needed)
- Machine paraphrasing (GPT, Dipper) causes smaller AUC drops than human paraphrasing, with watermark strength recovering more quickly as token count increases
- Watermarking outperforms retrieval and DetectGPT methods under attack due to favorable sample complexity - detection improves steadily with more tokens while other methods plateau or degrade

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Watermarking remains detectable after human and AI paraphrasing because the original watermarked text leaks n-grams or longer fragments that preserve the statistical watermark signal.
- **Mechanism**: Even when paraphrased, tokens or sequences from the original watermarked text are often recycled or remain unchanged. Since watermarking biases certain tokens (green tokens) to appear more frequently, these recycled fragments maintain the statistical deviation from human text distributions.
- **Core assumption**: Paraphrased text still contains substrings or token sequences from the original watermarked text, either verbatim or with minor changes.
- **Evidence anchors**: [abstract] "paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed"
- **Break condition**: If paraphrasing completely eliminates all traces of original n-grams or uses a model trained to specifically avoid watermark signals, detection could fail until sufficient new tokens are generated.

### Mechanism 2
- **Claim**: Detection strength improves with more observed tokens because watermarking enforces a consistent statistical difference between machine and human text distributions that accumulates over sequence length.
- **Mechanism**: Watermarking increases the probability of green tokens beyond what occurs in natural human text. As more tokens are observed, the cumulative statistical evidence (e.g., z-score) grows, eventually surpassing detection thresholds even if the watermark is diluted.
- **Core assumption**: The statistical difference between watermarked and unwatermarked text is persistent and additive over longer sequences.
- **Evidence anchors**: [abstract] "detection succeeds after ~800 tokens" and "watermark strength recovers with more tokens"
- **Break condition**: If the attack completely neutralizes the green token bias or the text length is insufficient to accumulate detectable statistical evidence.

### Mechanism 3
- **Claim**: Watermarking outperforms retrieval and loss-based detection under attack because it has favorable sample complexity—detection accuracy improves steadily with token count, while other methods plateau or degrade.
- **Mechanism**: Watermarking's detection power scales with the amount of text observed, leveraging the compounding statistical signal. Retrieval methods suffer when the proportion of watermarked text in mixed documents decreases, and loss-based methods like DetectGPT are sensitive to text perturbations and model-specific factors.
- **Core assumption**: The watermark's statistical signal is robust to certain types of text corruption and accumulates linearly with token count.
- **Evidence anchors**: [abstract] "watermark evidence compounds the more examples are given, and the watermark is eventually detected"
- **Break condition**: If an attack method can maintain high similarity to human text while completely erasing watermark patterns, or if detection thresholds are set too high relative to accumulated signal.

## Foundational Learning

- **Concept**: Statistical hypothesis testing (z-test, ROC-AUC)
  - **Why needed here**: The paper uses z-scores to measure the statistical significance of green token counts and ROC-AUC to evaluate detection performance across varying thresholds.
  - **Quick check question**: If a sequence has 600 tokens and the expected green token count under no watermark is 150, what z-score corresponds to observing 210 green tokens (assuming γ=0.25)?
    - **Answer**: z = (210 - 150) / sqrt(0.25 * 0.75 * 600) ≈ 4.9

- **Concept**: Text watermarking and token coloring schemes
  - **Why needed here**: Understanding how watermarks are embedded by biasing token selection (green/red lists) is essential to grasp why detection works and how attacks attempt to evade it.
  - **Quick check question**: In a vocabulary of size 50,000 with γ=0.25, how many tokens are in the green list at each generation step?
    - **Answer**: 0.25 * 50,000 = 12,500 tokens

- **Concept**: Paraphrasing and text similarity metrics (P-SP, n-gram overlap)
  - **Why needed here**: The paper evaluates attack strength by measuring how much original text remains in paraphrased versions and how this affects detection.
  - **Quick check question**: If a paraphrased text shares 80% of its 4-grams with the original, what does this imply about watermark leakage?
    - **Answer**: High n-gram overlap suggests significant watermark signal retention, aiding detection.

## Architecture Onboarding

- **Component map**: Watermark Generator -> Watermark Detector -> Attack Simulator -> Evaluation Pipeline -> Baseline Detectors
- **Critical path**: 
  1. Generate watermarked text using LLM with watermark parameters (γ, δ, hash scheme)
  2. Apply attack (paraphrase or copy-paste) to produce test samples
  3. Run detection (z-score, WinMax, retrieval, DetectGPT) on attacked text
  4. Compute ROC curves and AUC at different token lengths T
  5. Compare performance across attacks and baselines
- **Design tradeoffs**:
  - Hash scheme complexity vs. robustness: Simpler schemes (LeftHash) are less secure but may suffice if API access is controlled; complex schemes (SelfHash, MinHash) resist reverse engineering
  - Token length vs. detection speed: Longer texts improve detection but increase latency; windowed tests (WinMax) help detect short spans in long documents
  - Watermark strength (δ) vs. text quality: Higher δ increases detectability but may degrade output diversity
- **Failure signatures**:
  - Low AUC across all T: Indicates watermark is effectively removed or attack is too strong
  - AUC improves slowly or plateaus: Suggests watermark is weakened but not eliminated; more tokens needed
  - High false positive rate: Detector threshold too low or human text coincidentally matches watermark statistics
  - Retrieval fails on copy-paste: When watermarked fragments are small relative to context, semantic similarity drops
- **First 3 experiments**:
  1. Baseline watermark detection: Generate watermarked text, test detection without attack, confirm high AUC (>0.99)
  2. Machine paraphrasing attack: Apply GPT or Dipper paraphrasing, measure AUC drop and recovery as T increases
  3. Copy-paste attack: Embed watermarked spans into human text, test detection with varying span counts and lengths

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do watermarking schemes with larger context widths (h > 4) perform in terms of both robustness to attack and text quality compared to the schemes evaluated in this study?
- **Basis in paper**: [explicit] The authors discuss that increasing context width improves security against reverse engineering but note trade-offs with text quality. They evaluate up to h=8 but only report results for h=1 and h=4.
- **Why unresolved**: The study only explores a limited range of context widths, leaving open questions about optimal width choices for different applications.
- **What evidence would resolve it**: Empirical evaluation of watermarking schemes with context widths h=8, 16, 32 showing ROC-AUC values and text diversity metrics across various attack types.

### Open Question 2
- **Question**: Can the "curse of recursion" phenomenon (Shumailov et al., 2023) be mitigated by watermarking when LLMs are trained on data containing watermarked text?
- **Basis in paper**: [inferred] The authors mention that watermarking could help "future-proof" models by documenting machine-generated text for removal from training data, but don't investigate this application.
- **Why unresolved**: The paper focuses on detection of machine-generated text, not on the downstream effects of watermarking on model training and performance.
- **What evidence would resolve it**: Experiments training LLMs on datasets with varying proportions of watermarked text and measuring performance degradation compared to control groups.

### Open Question 3
- **Question**: How does the performance of watermarking detection methods compare to black-box learned detectors when tested on out-of-distribution data or adversarial attacks?
- **Basis in paper**: [explicit] The authors note that black-box learned detectors are vulnerable to out-of-distribution problems and adversarial attacks, but only compare watermarking to retrieval and DetectGPT methods.
- **Why unresolved**: The study doesn't include evaluation of other detection paradigms that could potentially perform better under certain attack scenarios.
- **What evidence would resolve it**: Comprehensive comparison of watermarking detection accuracy against multiple black-box learned detectors across diverse datasets and attack strategies, measuring ROC-AUC and false positive rates.

## Limitations

- The study's conclusions about watermark robustness under human paraphrasing rely on controlled laboratory conditions that may not generalize to real-world scenarios with diverse motivations and paraphrasing strategies.
- The evaluation focuses primarily on technical detection performance metrics without thoroughly examining potential false positive risks in deployment scenarios.
- The attack simulation methodology, while comprehensive, may not capture all realistic attack vectors, particularly sophisticated evasion techniques that operate at the token level or combine multiple attack strategies.

## Confidence

**High Confidence**: The claim that watermark detection improves with token count and outperforms retrieval-based methods under attack is well-supported by the experimental results.

**Medium Confidence**: The assertion that human paraphrasing represents the strongest attack against watermarking is supported by controlled experiments, but generalizability to real-world conditions requires additional validation.

**Low Confidence**: The paper's claims about the practical security of watermarking against determined adversaries lack sufficient real-world validation beyond simulated attacks.

## Next Checks

1. **Real-world human paraphrasing study**: Conduct a large-scale study with diverse participants paraphrasing text under realistic conditions (varying time constraints, motivations, and contexts) to validate whether controlled laboratory results generalize to practical scenarios.

2. **False positive rate assessment**: Evaluate the watermark detection system on large corpora of known human-generated text from various domains and writing styles to establish false positive rates and identify potential edge cases that could trigger incorrect detections.

3. **Adaptive attack evaluation**: Test the watermarking scheme against adaptive attacks where adversaries have access to the detection mechanism and can iteratively refine their paraphrasing strategies to minimize detection while maintaining semantic similarity.