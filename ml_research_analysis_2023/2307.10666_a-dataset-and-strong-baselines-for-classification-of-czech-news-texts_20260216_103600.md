---
ver: rpa2
title: A Dataset and Strong Baselines for Classification of Czech News Texts
arxiv_id: '2307.10666'
source_url: https://arxiv.org/abs/2307.10666
tags:
- tasks
- czech
- news
- dataset
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CZEch NEws Classification dataset (CZE-NEC),
  a large dataset of Czech news articles for text classification tasks. The dataset
  covers 6 news sources over 20 years and includes metadata like author gender and
  publication date.
---

# A Dataset and Strong Baselines for Classification of Czech News Texts

## Quick Facts
- arXiv ID: 2307.10666
- Source URL: https://arxiv.org/abs/2307.10666
- Authors: 
- Reference count: 23
- The paper introduces CZE-NEC, a large dataset of Czech news articles for text classification, and presents strong baselines using pre-trained transformer models that outperform humans on 3 of 4 tasks.

## Executive Summary
This paper introduces the CZE-NEC dataset, a large collection of Czech news articles spanning 20 years from six major Czech media outlets. The dataset includes rich metadata enabling four classification tasks: predicting news source, category, inferred author gender, and publication day. Human evaluation confirms these tasks are challenging, yet strong baselines using pre-trained Czech transformer models (RobeCzech and Fernet-News) significantly outperform human performance on three tasks. The work provides both a challenging benchmark for Czech NLP and demonstrates the effectiveness of fine-tuning pre-trained models for complex classification tasks.

## Method Summary
The authors created CZE-NEC by crawling and filtering news articles from six Czech media outlets (2000-2022), extracting metadata including author gender, publication date, and categories. They defined four classification tasks based on this metadata and evaluated multiple pre-trained Czech transformer models (RobeCzech and Fernet-News) using fine-tuning with grid-searched learning rates, linear decay, 0.1 warmup, 48 effective batch size, and AdamW optimizer. Models were trained with text truncated to 510 tokens. Performance was measured using F1 Macro scores and compared against a logistic regression baseline with TF-IDF features and text statistics.

## Key Results
- CZE-NEC dataset contains news articles from six Czech outlets spanning 20 years with rich metadata
- Pre-trained Czech transformer models outperform humans on 3 of 4 classification tasks
- The best model (RobeCzech) outperforms selected large language models on 2 tasks
- Fine-tuning on a smaller dataset (50k samples) still yields strong performance, especially for in-domain pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using pre-trained Czech transformer encoders (RobeCzech, Fernet-News) provides strong performance on classification tasks.
- Mechanism: Fine-tuning leverages general linguistic knowledge encoded in the models to adapt to specific downstream tasks.
- Core assumption: The transformer models have learned representations that generalize well to Czech news text classification.
- Evidence anchors:
  - [abstract] states "Strong baselines are presented using pre-trained transformer models. The best model outperforms humans on 3 tasks and selected large language models on 2 tasks."
  - [section 3] describes training and evaluating transformer models on the classification tasks.
  - [corpus] shows related work on Czech transformer models for text classification.
- Break condition: If the pre-trained models are not well-aligned with the domain (news text) or language (Czech), fine-tuning may not provide strong performance.

### Mechanism 2
- Claim: The CZE-NEC dataset provides challenging tasks that are difficult for humans but achievable by machine learning models.
- Mechanism: The tasks are defined using metadata like news source, category, author gender, and publication day. These provide discriminative features that models can learn to exploit.
- Core assumption: The metadata is correlated with the content in ways that models can discover, even if humans struggle to consistently make the same associations.
- Evidence anchors:
  - [abstract] states "Human evaluation showed these tasks are challenging."
  - [section 2.3] describes the classification tasks and how they are defined using metadata.
  - [section 3.3] shows model performance exceeding human performance on all tasks.
- Break condition: If the metadata does not provide meaningful signal for the tasks, or if the signal is too subtle for models to learn, the tasks will remain challenging for both humans and machines.

### Mechanism 3
- Claim: Fine-tuning on a smaller dataset (50k samples) still yields strong performance, especially for in-domain pre-trained models.
- Mechanism: The pre-training provides a strong prior that allows effective learning even with limited data. In-domain pre-training (Fernet-News) is particularly helpful.
- Core assumption: The knowledge gained during pre-training is transferable to the downstream tasks, and in-domain pre-training further aligns the model with the task domain.
- Evidence anchors:
  - [abstract] states "Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models."
  - [section 3.2] describes the small dataset setup and the performance of models trained on limited data.
  - [section 3.2] notes the advantage of in-domain pre-training for Fernet-News.
- Break condition: If the pre-training tasks and data are not sufficiently aligned with the downstream tasks, or if the tasks are too different from the pre-training, fine-tuning on limited data may not yield strong performance.

## Foundational Learning

- Concept: Transformer models and fine-tuning
  - Why needed here: The paper uses pre-trained transformer encoders and fine-tunes them on the classification tasks.
  - Quick check question: What is the key architectural difference between transformer encoders and decoders, and how does this impact their suitability for classification tasks?

- Concept: Czech language processing
  - Why needed here: The dataset and models are specific to Czech language news text.
  - Quick check question: What are some unique challenges in processing Czech language text compared to English, and how might this impact the design of NLP models for Czech?

- Concept: Metadata-based classification
  - Why needed here: The classification tasks are defined using metadata like news source, category, author gender, and publication day.
  - Quick check question: How can metadata be used to define classification tasks, and what are the potential limitations or biases of relying on metadata for task definitions?

## Architecture Onboarding

- Component map: News crawling and filtering -> Metadata extraction and processing -> Classification task definition -> Fine-tuning pre-trained models -> Model evaluation
- Critical path: Crawling and filtering news articles -> Extracting and processing metadata -> Defining classification tasks -> Fine-tuning pre-trained models -> Evaluating model performance
- Design tradeoffs: Using pre-trained models provides strong performance but may limit the ability to learn task-specific features. Defining tasks based on metadata is convenient but may introduce biases or limitations.
- Failure signatures: If the models fail to outperform baselines or human performance, it may indicate issues with the dataset quality, task definitions, or model architectures.
- First 3 experiments:
  1. Fine-tune RobeCzech on the full dataset for the source classification task and evaluate performance.
  2. Compare the performance of RobeCzech and Fernet-News on the category classification task using the small dataset setup.
  3. Analyze the model predictions to understand which metadata features are most influential for each task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pre-trained Czech language models (e.g., RobeCzech vs. Fernet-News) compare in performance on the four classification tasks when trained on the full dataset?
- Basis in paper: [explicit] The paper compares RobeCzech and Fernet-News models on the full dataset, showing RobeCzech outperforming Fernet-News despite Fernet-News's same-domain pre-training.
- Why unresolved: While the paper provides initial comparisons, further investigation into the specific reasons for performance differences (e.g., model architecture, pre-training data, hyperparameters) is needed.
- What evidence would resolve it: Detailed ablation studies comparing different aspects of the models, such as varying pre-training data, architecture modifications, and hyperparameter tuning, would provide insights into the factors contributing to performance differences.

### Open Question 2
- Question: How does the performance of the best model compare to human performance on the inferred gender classification task, and what factors contribute to the difference?
- Basis in paper: [explicit] The paper states that the best model outperforms human performance on all tasks except inferred gender classification, where human performance is higher.
- Why unresolved: The paper does not provide a detailed analysis of the factors contributing to the difference in performance between the model and humans on the inferred gender task.
- What evidence would resolve it: Analyzing the specific errors made by the model and humans on the inferred gender task, as well as investigating the features used by both, would help understand the reasons for the performance difference.

### Open Question 3
- Question: How does the size of the training dataset impact the performance of the models on the four classification tasks, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper presents results using both a large dataset and a smaller subset of 50k instances, showing that further training on the full dataset is beneficial.
- Why unresolved: The paper does not explore the relationship between dataset size and model performance in detail, such as investigating the point at which additional data no longer significantly improves performance.
- What evidence would resolve it: Conducting experiments with varying dataset sizes and analyzing the corresponding model performance would help determine the impact of dataset size and identify potential points of diminishing returns.

## Limitations
- Dataset focuses exclusively on six major Czech outlets, potentially missing diversity in Czech news media
- Metadata-based task definitions rely on inference rather than ground truth, introducing potential noise
- Human performance evaluation involved only three raters with limited context

## Confidence

**High Confidence Claims:**
- The CZE-NEC dataset creation methodology and its general characteristics (size, timespan, metadata) are well-documented and reproducible
- The transformer fine-tuning approach and baseline results for the four classification tasks are clearly specified
- The general finding that pre-trained Czech transformer models outperform commercial LLMs on these tasks

**Medium Confidence Claims:**
- The assertion that tasks are "challenging for humans" based on the limited human evaluation study
- The specific performance metrics achieved by different models, which may be sensitive to random initialization and implementation details
- The conclusion that in-domain pre-training (Fernet-News) provides advantages, based on comparison with a single alternative model

**Low Confidence Claims:**
- Generalizability of results to other Czech news sources or domains beyond the six included outlets
- The robustness of metadata-based task definitions to changes in data distribution or collection methods
- Claims about model interpretability and the specific features being learned

## Next Checks

1. **Cross-validation with alternative Czech news sources**: Test model performance on news articles from outlets not included in the original dataset to assess generalization beyond the six major sources.

2. **Metadata quality assessment**: Conduct a systematic evaluation of the accuracy and reliability of the inferred author gender and other metadata labels, including comparison with ground truth where available.

3. **Robustness to data perturbations**: Evaluate model performance when training data is subjected to realistic perturbations such as paraphrasing, sentence reordering, or introduction of adversarial examples to assess the stability of learned representations.