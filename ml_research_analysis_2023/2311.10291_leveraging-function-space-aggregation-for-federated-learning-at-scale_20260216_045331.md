---
ver: rpa2
title: Leveraging Function Space Aggregation for Federated Learning at Scale
arxiv_id: '2311.10291'
source_url: https://arxiv.org/abs/2311.10291
tags:
- local
- training
- client
- performance
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FedFish, a federated learning algorithm that
  aggregates local client updates by matching their output functions on client data.
  Unlike FedAvg, which directly averages parameters, FedFish uses an estimate of the
  Fisher information to weight client parameters, aiming to preserve functional relationships
  across heterogeneous client data distributions.
---

# Leveraging Function Space Aggregation for Federated Learning at Scale

## Quick Facts
- arXiv ID: 2311.10291
- Source URL: https://arxiv.org/abs/2311.10291
- Reference count: 40
- Key outcome: FedFish, a federated learning algorithm using Fisher-weighted parameter averaging, achieves up to 7% better next-token prediction accuracy than FedAvg in large-scale language model pretraining and fine-tuning

## Executive Summary
This paper introduces FedFish, a federated learning algorithm that aggregates local client updates by matching their output functions on client data using Fisher information weighting. Unlike traditional FedAvg which averages parameters directly, FedFish uses an estimate of the Fisher information matrix to weight parameters during aggregation, aiming to preserve functional relationships across heterogeneous client data distributions. The authors demonstrate improved global and personalization performance across image classification and language modeling tasks, particularly as the number of local training epochs increases.

## Method Summary
FedFish addresses federated learning by aggregating local client updates through a function space perspective rather than parameter space averaging. The method computes diagonal approximations of the Fisher information matrix during local training to weight parameters according to their importance for each client's data distribution. This Fisher-weighted averaging aims to produce a global model that better represents the combined functional relationships learned by all clients. The approach is compatible with heterogeneous datasets, longer local training epochs, and multiple rounds of federated learning, making it robust to client drift and reducing communication costs when comparable performance is achieved in fewer rounds.

## Key Results
- FedFish achieves up to 7% better next-token prediction accuracy than FedAvg in large-scale federated pretraining on C4 followed by fine-tuning on Stack Overflow
- The method shows improved global and personalization performance across EMNIST, CIFAR100, and language modeling tasks
- FedFish demonstrates robustness to longer local training epochs without suffering from client drift
- The Client-Server Barrier metric shows FedFish generally exhibits lower barriers throughout training compared to FedAvg

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FedFish preserves functional relationships between client models better than FedAvg by weighting parameters according to their Fisher information
- **Mechanism:** The Fisher-weighted averaging accounts for parameter importance with respect to client data distributions, giving higher weight to parameters that contribute more to prediction accuracy
- **Core assumption:** The diagonal Fisher information matrix approximates parameter importance for function matching across heterogeneous data distributions
- **Evidence anchors:** Abstract mentions "Fisher information" and "functions learned by clients"; section 2.2 discusses parametric aggregation accounting for client data distributions
- **Break condition:** When diagonal Fisher approximation becomes inaccurate due to highly correlated parameters or deep architectures

### Mechanism 2
- **Claim:** FedFish is more robust to longer local training epochs because it doesn't assume client models are optimal
- **Mechanism:** Deriving from a function space perspective allows aggregation of intermediate client models without requiring convergence
- **Core assumption:** Matching intermediate function representations is more effective than averaging parameter updates
- **Evidence anchors:** Abstract notes robustness to longer local training; section 2.1 mentions compatibility with longer local training
- **Break condition:** When local training becomes so long that client models diverge significantly in function space

### Mechanism 3
- **Claim:** FedFish produces models more amenable to personalization because the global model better represents client functions
- **Mechanism:** The global model serves as better initialization for local fine-tuning since it incorporates functional relationships from all clients
- **Core assumption:** A global model that better represents client functions provides superior initialization for personalization
- **Evidence anchors:** Abstract mentions efficient personalization; section 5.2.2 observes higher post-personalization performance
- **Break condition:** When personalization requires learning entirely new functions not represented in client data

## Foundational Learning

- **Concept: Function space perspective in federated learning**
  - Why needed here: This work fundamentally reframes FL from parameter space averaging to function space matching, which is the core innovation
  - Quick check question: What is the key difference between parameter space averaging and function space matching in the context of federated learning?

- **Concept: Fisher Information Matrix and its diagonal approximation**
  - Why needed here: The diagonal Fisher information is used to weight parameters in FedFish, making it crucial to understand what it represents and how it's approximated
  - Quick check question: Why does using only the diagonal of the Fisher Information matrix make FedFish computationally feasible for large models?

- **Concept: Client drift phenomenon in federated learning**
  - Why needed here: Understanding why FedAvg suffers from longer local training helps explain why FedFish's approach is beneficial
  - Quick check question: What causes client drift in federated learning, and why does it become more problematic with longer local training epochs?

## Architecture Onboarding

- **Component map:** Global model -> Client broadcast -> Local training (SGD) -> Fisher diagonal computation -> Client update transmission -> Server aggregation (Fisher-weighted averaging) -> Global model update -> Client broadcast

- **Critical path:**
  1. Global model broadcast to clients
  2. Clients perform local training (compute gradients, update parameters, compute Fisher diagonals)
  3. Clients send model deltas and Fisher diagonals to server
  4. Server aggregates using Fisher-weighted averaging
  5. Server broadcasts updated global model

- **Design tradeoffs:**
  - Computational overhead: FedFish requires computing Fisher diagonals during local training (additional pass over data)
  - Communication overhead: Sends Fisher diagonals along with model deltas (approximately doubles communication per round)
  - Robustness vs. efficiency: More robust to heterogeneity and longer training, but at increased computational cost

- **Failure signatures:**
  - Poor personalization performance: May indicate Fisher approximation is not capturing true parameter importance
  - Degraded performance with very long local training: Suggests client models are diverging too much in function space
  - Inconsistent Client-Server Barrier: May indicate instability in aggregation or issues with Fisher computation

- **First 3 experiments:**
  1. Reproduce the toy regression experiment with two clients and varying data overlap to verify FedFish preserves functional relationships better than FedAvg
  2. Run EMNIST with 4, 8, and 16 local epochs comparing FedAvg vs FedFish global and personalization performance to verify robustness to local training length
  3. Implement and test the Client-Server Barrier metric on Stack Overflow data to understand how aggregation quality changes over training rounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FedFish perform relative to FedAvg under heterogeneous computational budgets across clients (e.g., variable local training steps per client)?
- Basis in paper: [explicit] The paper shows FedFish is robust to longer local training and reduces communication rounds, but does not test client-specific local epoch counts or varying compute budgets per client.
- Why unresolved: Client-device heterogeneity is a core characteristic of federated settings, yet all experiments assume homogeneous local training steps and equal compute per client.
- What evidence would resolve it: Experiments comparing FedFish vs. FedAvg when clients are allowed to train for different numbers of epochs (e.g., based on availability or device capability), measuring global accuracy, personalization, and CSB.

### Open Question 2
- Question: What is the impact of using higher-order approximations to the Fisher Information matrix (e.g., K-FAC or block-diagonal) on FedFish's performance?
- Basis in paper: [inferred] The paper uses a diagonal Fisher approximation for scalability and notes that full or block-diagonal Fisher matrices could better capture parameter correlations, potentially improving aggregation quality.
- Why unresolved: While diagonal Fisher is computationally efficient, it ignores inter-parameter correlations. The paper does not explore richer approximations or compare their trade-offs in performance vs. computational cost.
- What evidence would resolve it: Benchmarking FedFish with diagonal, block-diagonal, and K-FAC-style Fisher approximations on the same datasets/tasks, measuring accuracy, personalization, and communication overhead.

### Open Question 3
- Question: How does the choice of global optimizer (beyond Adam) influence FedFish's convergence and final performance?
- Basis in paper: [explicit] The authors use Adam for the global optimizer and note FedFish can be combined with adaptive optimizers, but do not test alternative choices (e.g., SGD, AdamW, or second-order methods).
- Why unresolved: FedFish's theoretical derivation is agnostic to the global optimizer; empirical performance may vary significantly depending on optimizer choice, especially under different task/heterogeneity regimes.
- What evidence would resolve it: Systematic comparison of FedFish using Adam, SGD, AdamW, and other optimizers across multiple federated datasets, tracking convergence speed, final accuracy, and robustness to hyperparameters.

### Open Question 4
- Question: Does the Client-Server Barrier (CSB) correlate with downstream task performance, and can it be used as a reliable early stopping criterion?
- Basis in paper: [explicit] The paper introduces CSB as a metric to assess aggregation quality and shows its trajectory varies across datasets/epochs, but does not validate whether low CSB correlates with high global or personalization performance.
- Why unresolved: While CSB is proposed as an interpretable in-training metric, the paper does not empirically demonstrate its predictive value for final model quality or its utility for early stopping.
- What evidence would resolve it: Correlation analysis between CSB trends and final task performance (accuracy, perplexity, personalization gains) across multiple federated experiments, plus ablation studies using CSB-based early stopping.

## Limitations

- The paper's core claims rely heavily on the diagonal Fisher approximation, which may not capture true parameter importance in deep architectures with correlated parameters
- Additional computational overhead of computing Fisher diagonals during local training is not fully characterized
- Communication overhead approximately doubles per round, which could become prohibitive at scale
- The Client-Server Barrier metric lacks extensive validation across diverse scenarios
- The paper does not thoroughly explore failure cases where FedFish might underperform, particularly in highly heterogeneous settings

## Confidence

- **High confidence**: FedFish improves robustness to longer local training epochs compared to FedAvg (supported by multiple experiments and ablation studies)
- **Medium confidence**: Fisher-weighted averaging preserves functional relationships better than parameter averaging (supported by toy experiments but limited empirical validation on deep networks)
- **Medium confidence**: FedFish produces better global models for personalization (supported by post-hoc fine-tuning experiments but not comprehensive personalization benchmarks)

## Next Checks

1. **Break condition analysis**: Systematically test FedFish performance as client model divergence increases, measuring when the diagonal Fisher approximation fails to properly weight parameters
2. **Overhead characterization**: Measure and compare computational and communication costs of FedFish vs FedAvg across different model architectures and dataset sizes, particularly for large language models
3. **Cross-task generalization**: Evaluate FedFish on non-image, non-language tasks (e.g., tabular data or reinforcement learning) to verify the generality of function space aggregation benefits beyond the presented benchmarks