---
ver: rpa2
title: 'Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics'
arxiv_id: '2302.02092'
source_url: https://arxiv.org/abs/2302.02092
tags:
- data
- robustness
- arxiv
- distributions
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve the robustness of machine
  learning models by interpolating training data distributions using Wasserstein geodesics.
  The key idea is to augment data by finding the worst-case Wasserstein barycenter
  on the geodesic connecting subpopulation distributions of different categories.
---

# Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics

## Quick Facts
- arXiv ID: 2302.02092
- Source URL: https://arxiv.org/abs/2302.02092
- Reference count: 40
- Up to 7.7% improvement in certifiable robustness on CIFAR10 and 16.8% on empirical robustness on CIFAR-100

## Executive Summary
This paper proposes a novel data augmentation method for improving machine learning model robustness by interpolating training data distributions using Wasserstein geodesics. The key insight is that augmenting data by finding worst-case Wasserstein barycenters on geodesics connecting subpopulation distributions can improve model robustness. The method also incorporates regularization for smoother model performance along these continuous geodesic paths. Extensive experiments on CIFAR-100, ImageNet, MNIST, and CIFAR-10 demonstrate significant improvements in both certifiable and empirical robustness metrics.

## Method Summary
The method involves computing Wasserstein geodesics between class distributions using optimal transport maps, then generating worst-case interpolation samples along these geodesics. A VAE embedding is used to reduce dimensionality before computing OT maps via Sinkhorn algorithm. During training, the model is trained with both the original data and augmented worst-case samples from the interpolation, plus a regularization term that enforces smooth performance along the geodesic path. The approach is compatible with existing robust training methods and can be applied to various network architectures.

## Key Results
- Achieves up to 7.7% improvement in certifiable robustness on CIFAR10 (ℓ₂, σ = 0.25, 0.5, 1.0)
- Achieves 16.8% improvement in empirical robustness on CIFAR-100 (ℓ∞)
- Method works across multiple datasets including CIFAR-100 and ImageNet (64×64)
- Can be easily combined with existing robust training methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interpolating along Wasserstein geodesics creates worst-case adversarial distributions that improve model robustness.
- Mechanism: By finding the Wasserstein barycenter on the geodesic between subpopulation distributions, the method generates synthetic samples at decision boundaries, forcing the model to learn smoother decision surfaces.
- Core assumption: The worst-case adversarial distribution lies on the Wasserstein geodesic between class distributions.
- Evidence anchors: [abstract] "we augment the data by finding the worst-case Wasserstein barycenter on the geodesic connecting subpopulation distributions of different categories" - [section] "we find the worst-case interpolation distributions that lie on the decision boundary and improve the model's smoothness with samples from these distributions"
- Break condition: If the true worst-case adversarial distribution does not lie on the geodesic between class distributions, this mechanism fails.

### Mechanism 2
- Claim: The geodesic-based regularization promotes smoother model performance along continuous paths between distributions.
- Mechanism: Regularizing the model's performance along the Wasserstein geodesic path prevents abrupt changes in predictions, improving robustness to perturbations.
- Core assumption: Smoother decision boundaries along geodesics correlate with better robustness to adversarial perturbations.
- Evidence anchors: [abstract] "We regularize the model for smoother performance on the continuous geodesic path connecting subpopulation distributions" - [section] "the geodesic provides a new protocol to assess one model's robustness beyond the local area surrounding individual data points"
- Break condition: If smoothness along geodesics doesn't translate to better local robustness near individual samples, regularization becomes ineffective.

### Mechanism 3
- Claim: The OT map estimation enables efficient computation of interpolation without requiring intractable barycenter optimization.
- Mechanism: Using McCann's interpolation and OT maps allows direct computation of geodesic samples via closed-form expressions, making the method scalable.
- Core assumption: Optimal transport maps between Gaussian distributions have closed-form solutions that can be efficiently computed.
- Evidence anchors: [section] "Thanks to the celebrated properties (Brenier, 1991; McCann, 1997), which connects the Monge pushforward map with the coupling, we can avoid the intractable optimization over µt" - [section] "the transport map can be estimated solely on data and stored regardless of the supervised learning task"
- Break condition: If OT map estimation becomes intractable for non-Gaussian distributions or high-dimensional data, scalability breaks down.

## Foundational Learning

- Concept: Wasserstein distance and optimal transport theory
  - Why needed here: The method fundamentally relies on Wasserstein geodesics and barycenters for distribution interpolation
  - Quick check question: What is the relationship between Wasserstein distance and optimal transport maps for Gaussian distributions?

- Concept: Adversarial training and distributional robustness
  - Why needed here: The method builds on adversarial training principles but extends them to distribution-level interpolation rather than point-level perturbations
  - Quick check question: How does distributional robustness differ from standard adversarial training in terms of the perturbation set?

- Concept: Neural network calibration and robustness metrics
  - Why needed here: Understanding different robustness metrics (empirical vs certified) is crucial for evaluating the method's effectiveness
  - Quick check question: What is the relationship between standard accuracy, robust accuracy, and smoothed accuracy in the context of ℓ∞ perturbations?

## Architecture Onboarding

- Component map: Data preprocessing -> VAE embedding -> OT map estimator -> Geodesic interpolation generator -> Training loop with augmentation and regularization -> Evaluation pipeline
- Critical path: Data preprocessing -> OT map computation -> Interpolation generation -> Model training with augmentation and regularization -> Robustness evaluation
- Design tradeoffs: Computational efficiency vs. approximation quality in OT estimation; regularization strength vs. standard accuracy
- Failure signatures: Poor OT coupling convergence -> noisy interpolation; excessive regularization -> underfitting; insufficient augmentation batch size -> inadequate coverage
- First 3 experiments:
  1. Run OT map estimation on simple 2D Gaussian distributions to verify closed-form solutions match theoretical expectations
  2. Generate interpolated samples on a simple binary classification task and visualize decision boundaries before/after augmentation
  3. Test different OT regularization parameters (entropic coefficient) on a small dataset to find sweet spot between quality and speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the worst-case interpolation distribution location along the Wasserstein geodesic affect model robustness?
- Basis in paper: [explicit] The paper mentions that the geodesic location contributes to robustness improvement and provides theoretical analysis on this relationship.
- Why unresolved: While the paper provides theoretical analysis, it doesn't experimentally investigate how different locations along the geodesic affect robustness.
- What evidence would resolve it: Experiments comparing robustness across different interpolation weights (t values) along the geodesic path.

### Open Question 2
- Question: Can the proposed method be extended to multi-marginal Wasserstein barycenters for robustness against more than two classes simultaneously?
- Basis in paper: [explicit] The paper mentions this as a potential future work direction, stating "considering multi-marginal adversarial Wasserstein barycenter on a simplex."
- Why unresolved: The paper focuses on pairwise interpolation and doesn't explore the multi-class scenario.
- What evidence would resolve it: Extension of the method to handle multiple classes simultaneously and comparison of robustness results.

### Open Question 3
- Question: How does the proposed method perform on real-world datasets with complex, non-Gaussian distributions?
- Basis in paper: [inferred] The paper's theoretical analysis is based on Gaussian distributions, but experiments are conducted on CIFAR-100 and ImageNet.
- Why unresolved: The paper doesn't provide a detailed analysis of how well the method performs when the data distribution deviates significantly from the Gaussian assumption.
- What evidence would resolve it: Experiments on datasets with known complex distributions and comparison of results with the theoretical predictions.

## Limitations

- The method's theoretical guarantees are primarily established for Gaussian distributions, with limited empirical validation on complex real-world data distributions
- Computational complexity of OT map estimation may become prohibitive for very high-dimensional data without efficient embedding techniques
- Exact network architectures for VAE and main classifier are not specified, potentially affecting reproducibility

## Confidence

- Mechanism 1 (adversarial interpolation): Medium - The theoretical framework is well-established, but empirical validation beyond synthetic Gaussian distributions is limited
- Mechanism 2 (geodesic regularization): Low - While intuitively appealing, the connection between geodesic smoothness and local robustness is not rigorously proven
- Mechanism 3 (OT map scalability): Medium - The approach is theoretically sound for Gaussian distributions, but scalability to complex real-world data depends heavily on VAE quality

## Next Checks

1. Validate OT map estimation quality on CIFAR-100 class distributions by comparing geodesic interpolation results with ground-truth interpolated samples
2. Test geodesic regularization ablation - compare model robustness with and without the regularization term while keeping all other factors constant
3. Evaluate computational scaling by measuring interpolation generation time as a function of dataset dimensionality and class separation