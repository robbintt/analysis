---
ver: rpa2
title: Contrastive Initial State Buffer for Reinforcement Learning
arxiv_id: '2309.09752'
source_url: https://arxiv.org/abs/2309.09752
tags:
- states
- learning
- state
- which
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sample inefficiency in reinforcement
  learning (RL) for robotic tasks. The authors propose the Contrastive Initial State
  Buffer (CL-Buffer), which strategically selects states from past experiences to
  initialize the agent in the environment, guiding it toward more informative states.
---

# Contrastive Initial State Buffer for Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.09752
- Source URL: https://arxiv.org/abs/2309.09752
- Reference count: 32
- This paper proposes the Contrastive Initial State Buffer (CL-Buffer), which strategically selects states from past experiences to initialize the agent in the environment, guiding it toward more informative states.

## Executive Summary
This paper addresses the challenge of sample inefficiency in reinforcement learning (RL) for robotic tasks. The authors propose the Contrastive Initial State Buffer (CL-Buffer), which strategically selects states from past experiences to initialize the agent in the environment, guiding it toward more informative states. The CL-Buffer uses a learned projection network to map observations to an embedding space, clustering states with similar task-relevant experiences. The authors evaluate their approach on two complex robotic tasks: quadruped robot locomotion on challenging terrains and quadcopter drone racing. The results show that the CL-Buffer achieves 18.3% higher task performance for the quadruped walking task compared to the baseline, and a success rate of 0.9 for the drone racing task, compared to 0.2 for the baseline without an initial state buffer.

## Method Summary
The Contrastive Initial State Buffer (CL-Buffer) improves sample efficiency in RL by strategically selecting initial states for agent episodes. The method maintains a visited states buffer (V) that collects states during rollouts, and an initial state buffer (ISB) that stores selected states for episode initialization. A projection network maps observations to an embedding space where K-means clustering groups states with similar task-relevant experiences based on value function changes. The CL-Buffer continuously updates this embedding space during training, allowing it to focus on states that remain challenging for the agent. The authors compare three variants: Random-Buffer (uniform sampling), Obs-Buffer (clustering in observation space), and CL-Buffer (contrastive learning in embedding space).

## Key Results
- CL-Buffer achieves 18.3% higher task performance for quadruped walking task compared to baseline
- CL-Buffer achieves success rate of 0.9 for drone racing task vs 0.2 for baseline without initial state buffer
- CL-Buffer shows faster training convergence compared to nominal baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing the agent from strategically selected past states reduces the number of initializations in already mastered sub-tasks, thus speeding up learning.
- Mechanism: The Initial State Buffer (ISB) stores states that represent diverse task-relevant experiences. By sampling initial states from this buffer, the agent avoids repeatedly starting in situations where it already has learned effective behavior, focusing instead on new or challenging scenarios.
- Core assumption: The buffer contains a representative sample of states that span the relevant state space, and the sampling strategy does not bias toward trivial or redundant states.
- Evidence anchors:
  - [abstract] "our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training convergence."
  - [section] "It is thus important to modify the sampling algorithm to increase the diversity of states, which is addressed by the next ISB variant."
  - [corpus] Weak evidence; neighboring papers focus on replay buffer strategies, not initial state initialization.
- Break condition: If the ISB is dominated by states from simple or repetitive scenarios, the agent will waste rollouts on already-learned behaviors, reducing the benefit.

### Mechanism 2
- Claim: Clustering states in an embedding space based on task-relevant experience similarity leads to more diverse and informative initial states than clustering in observation space.
- Mechanism: The Contrastive Learning Buffer (CL-Buffer) learns a projection network that maps observations to an embedding space where states with similar value function changes after policy updates are close together. K-means clustering in this space selects initial states that represent different skills or challenges.
- Core assumption: The embedding space captures task-relevant similarity better than raw observation space, and the contrastive loss successfully trains the projection network to group states with similar experiences.
- Evidence anchors:
  - [abstract] "Our method uses a network to project observations... in which we apply K-means clustering."
  - [section] "Intuitively, we want the network to map states to close-by embeddings if they provide similar experiences to the RL agent."
  - [corpus] Weak evidence; no direct support from corpus neighbors.
- Break condition: If the projection network fails to learn meaningful embeddings, clustering in the embedding space will not improve diversity and may even hurt performance.

### Mechanism 3
- Claim: The CL-Buffer adapts its clustering over time, shifting focus from states that no longer change in value function to newly challenging states.
- Mechanism: The projection network is continually trained throughout RL training using a contrastive loss based on the change in value function after policy updates. This dynamic adaptation ensures that the ISB reflects the agent's current learning needs.
- Core assumption: The value function change is a reliable signal of task-relevant experience, and the contrastive loss can effectively update the embedding space as the agent learns.
- Evidence anchors:
  - [section] "This type of clustering thus adds a dynamic behavior to the ISB."
  - [section] "the network is continually trained throughout RL training, which means that it can learn to shift the focus from states as they no longer change in terms of the value function."
  - [corpus] No direct evidence; assumption relies on the paper's claims.
- Break condition: If the value function changes are noisy or uninformative, the embedding space may become unstable, leading to poor initial state selection.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The RL framework in the paper is based on MDPs, where states, actions, transitions, rewards, and policies are defined. Understanding MDPs is essential to grasp how the ISB and CL-Buffer modify the learning process.
  - Quick check question: In an MDP, what does the transition probability p(s'|s,a) represent?

- Concept: Experience Replay and Replay Buffers
  - Why needed here: The paper builds on the idea of reusing past experiences, similar to experience replay, but applies it to initial state selection rather than policy updates. Knowing how replay buffers work helps understand the motivation for the ISB.
  - Quick check question: How does experience replay improve sample efficiency in RL?

- Concept: Contrastive Learning
  - Why needed here: The CL-Buffer uses contrastive learning to train the projection network. Understanding how contrastive losses work (e.g., InfoNCE) is key to grasping how the embedding space is constructed.
  - Quick check question: In contrastive learning, what is the purpose of the positive and negative sets?

## Architecture Onboarding

- Component map:
  - RL agent with policy Ï€ and value function V
  - Visited States Buffer (V): FIFO buffer collecting states during rollouts
  - Initial State Buffer (ISB): Buffer of selected states for initializing episodes
  - Projection Network f: MLP mapping observations to embedding space
  - K-means Clustering: Groups states in embedding space
  - Contrastive Loss: Trains f based on value function changes

- Critical path:
  1. Agent interacts with environment, states added to V
  2. After each rollout, N states sampled from V and added to ISB
  3. For CL-Buffer, states projected via f, clustered, and nearest neighbors selected for ISB
  4. When starting new episodes, initial states sampled from ISB with probability p, otherwise from original start distribution

- Design tradeoffs:
  - Random-Buffer: Simple but may oversample redundant states
  - Obs-Buffer: Uses observation space clustering, but may group uncorrelated states
  - CL-Buffer: More complex, requires training projection network, but captures task-relevant similarity

- Failure signatures:
  - Random-Buffer: Training plateaus early, similar to baseline
  - Obs-Buffer: Performance similar to Random-Buffer, indicating poor clustering in observation space
  - CL-Buffer: If projection network fails, performance drops to Random-Buffer level

- First 3 experiments:
  1. Implement Random-Buffer and Obs-Buffer, compare training curves to baseline on a simple locomotion task
  2. Train CL-Buffer projection network, visualize embedding space clustering, verify diversity of selected states
  3. Run full CL-Buffer on quadruped and drone racing tasks, measure success rate and convergence speed compared to baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CL-Buffer compare to other advanced exploration strategies like intrinsic motivation methods?
- Basis in paper: [explicit] The authors mention that their approach can be combined with intrinsic motivation methods, but do not compare directly to them.
- Why unresolved: The paper focuses on comparing different ISB strategies and a nominal baseline, but does not include comparisons to other exploration enhancement methods.
- What evidence would resolve it: Experimental results comparing CL-Buffer to RL agents using intrinsic motivation techniques like curiosity-driven exploration or count-based exploration on the same tasks.

### Open Question 2
- Question: Can the CL-Buffer's projection network generalize to entirely new tasks without retraining?
- Basis in paper: [inferred] The paper shows that the projection network clusters states into task-relevant skills, but does not test transfer to new tasks.
- Why unresolved: The paper demonstrates task-specific clustering but does not explore the network's ability to generalize or transfer knowledge to unseen environments.
- What evidence would resolve it: Experiments showing the performance of a CL-Buffer trained on one task when applied to a different task, either with or without fine-tuning.

### Open Question 3
- Question: What is the impact of different state filtering strategies on the CL-Buffer's performance in environments with varying reward structures?
- Basis in paper: [explicit] The authors mention pre-selection of states for the visited state buffer and show ablation experiments for the quadruped task, but do not explore different filtering strategies across diverse environments.
- Why unresolved: The paper only evaluates one specific filtering approach for each task, without investigating how different reward structures might benefit from alternative filtering methods.
- What evidence would resolve it: Comparative experiments using multiple state filtering strategies (e.g., based on reward magnitude, episode length, or state novelty) across a variety of tasks with different reward structures.

## Limitations
- Evaluation restricted to two specific robotic tasks without ablation studies on CL-Buffer components
- Minimal specification of projection network architecture and hyperparameters
- Reliance on value function changes as similarity signal may not generalize to sparse reward tasks

## Confidence

- Mechanism 1 (ISB reduces redundant rollouts): Medium - supported by performance gains but lacks ablation evidence
- Mechanism 2 (Embedding space clustering): Low-Medium - theoretical justification exists but limited empirical validation
- Mechanism 3 (Dynamic adaptation): Low - claimed but not directly demonstrated

## Next Checks
1. Implement ablation studies comparing Random-Buffer, Obs-Buffer, and CL-Buffer on a simpler control task to isolate the contribution of each component
2. Test CL-Buffer performance across tasks with varying reward structures (dense vs sparse) to evaluate the robustness of using value function changes as the similarity signal
3. Conduct experiments varying the projection network architecture and training schedule to determine sensitivity to these design choices