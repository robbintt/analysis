---
ver: rpa2
title: 'DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models
  at All Scales'
arxiv_id: '2308.01320'
source_url: https://arxiv.org/abs/2308.01320
tags:
- training
- rlhf
- system
- deepspeed
- deepspeed-chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepSpeed-Chat, a system for efficient and
  scalable RLHF training of large language models. It addresses the lack of accessible,
  cost-effective end-to-end RLHF training pipelines, particularly for models with
  billions of parameters.
---

# DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales

## Quick Facts
- **arXiv ID**: 2308.01320
- **Source URL**: https://arxiv.org/abs/2308.01320
- **Reference count**: 16
- **Primary result**: Trains OPT-13B in 9 hours and OPT-30B in 18 hours on Azure Cloud for under $300 and $600 respectively

## Executive Summary
DeepSpeed-Chat is a system designed for efficient and scalable RLHF (Reinforcement Learning from Human Feedback) training of large language models, particularly those with billions of parameters. It introduces a unified Hybrid Engine that seamlessly combines DeepSpeed's training and inference capabilities, enabling smooth transitions between inference and training modes during RLHF. The system replicates the InstructGPT RLHF pipeline and provides robust optimizations for both training and inference, achieving significant speedups and cost reductions compared to existing solutions.

## Method Summary
DeepSpeed-Chat uses a three-step RLHF pipeline: Supervised Fine-Tuning (SFT), Reward Model Fine-tuning, and RLHF with PPO. The system's Hybrid Engine dynamically switches between training and inference modes to optimize memory and compute usage for each phase. It leverages DeepSpeed's training engine with ZeRO and LoRA optimizations, and the inference engine with tensor parallelism and high-performance kernels. The system supports model sizes up to 175B parameters and has been validated on Azure Cloud, achieving training times of 9 hours for OPT-13B and 18 hours for OPT-30B at a cost of under $300 and $600 respectively.

## Key Results
- Trains OPT-13B in 9 hours and OPT-30B in 18 hours on Azure Cloud for under $300 and $600 respectively
- Achieves 6-19x speedup over Colossal-AI and 1.4-10.5x over HuggingFace DDP on multi-GPU setups
- Supports model sizes up to 175B parameters with up to 15x higher throughput than existing solutions by optimizing the generation phase of RLHF training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepSpeed-HE achieves high efficiency by seamlessly switching between training and inference modes during RLHF.
- Mechanism: The Hybrid Engine dynamically configures memory management and parallelism strategies to match the computational demands of each phase (generation vs. RL training). During generation, it prioritizes memory bandwidth and uses tensor parallelism; during training, it leverages ZeRO-based sharding to reduce memory overhead.
- Core assumption: The transition between inference and training modes can be done with minimal overhead and without disrupting the RLHF pipeline.
- Evidence anchors:
  - [abstract] "The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost."
  - [section] "The Hybrid-Engine is capable of seamlessly transitioning between inference and training modes within RLHF, allowing it to leverage various optimizations from DeepSpeed-Inference such as tensor-parallelism and high-performance transformer kernels for generation, while also benefiting from the multitude of ZeRO- and LoRA [9]-based memory optimization strategies for RL training."
- Break condition: If the switching overhead exceeds the performance gains from optimized modes, or if the model partitioning logic introduces significant latency.

### Mechanism 2
- Claim: DeepSpeed-HE maximizes throughput by optimizing the memory- and compute-intensive generation phase.
- Mechanism: It uses high-performance transformer kernels and tensor parallelism to maximize GPU memory bandwidth utilization during generation, while also supporting larger batch sizes by reconfiguring memory management per mode.
- Core assumption: The generation phase is the bottleneck in RLHF training and can be accelerated significantly with specialized kernels and parallelism.
- Evidence anchors:
  - [section] "During its inference execution for experience generation phase of RLHF training, DeepSpeed Hybrid Engine uses a light-weight memory management system to handle the KV-cache and intermediate results, together with highly optimized inference-adapted kernels and tensor parallelism implementation, to achieve significant boost in throughput (tokens-per-second) compared to the existing solutions."
  - [section] "DeepSpeed-HE can achieve up to 9x throughput improvement during this phase over HuggingFace and 15x over Colossal-AI allowing it to achieve unparallel end-to-end efficiency."
- Break condition: If the generation phase is not the primary bottleneck or if specialized kernels do not provide expected speedup due to hardware constraints.

### Mechanism 3
- Claim: DeepSpeed-HE enables training of very large models (up to 175B parameters) by combining memory optimizations across training and inference.
- Mechanism: It composes ZeRO-based sharding, LoRA, and tensor parallelism in a unified way, allowing the system to partition model states and reduce memory consumption per GPU, while still maintaining high throughput.
- Core assumption: The combination of memory optimizations is compatible and does not introduce conflicting behaviors or excessive communication overhead.
- Evidence anchors:
  - [abstract] "The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost."
  - [section] "DeepSpeed-HE supports models with hundreds of billions of parameters and can achieve excellent scalability on multi-node multi-GPU systems."
- Break condition: If the combined optimizations introduce significant communication overhead or degrade performance due to complex memory management.

## Foundational Learning

- Concept: RLHF pipeline (SFT → Reward Model → RL fine-tuning)
  - Why needed here: Understanding the three-step InstructGPT pipeline is essential to grasp why DeepSpeed-HE optimizes differently for each phase and how memory/compute demands shift across steps.
  - Quick check question: What are the three main stages of the RLHF pipeline, and why does each require different system optimizations?

- Concept: Memory optimizations (ZeRO, LoRA, tensor parallelism)
  - Why needed here: These are the core technologies enabling DeepSpeed-HE to scale to large models and switch efficiently between training and inference modes.
  - Quick check question: How does ZeRO sharding reduce memory usage per GPU, and why is tensor parallelism preferred during the generation phase?

- Concept: Inference vs. training mode differences in transformers
  - Why needed here: The generation (inference) phase has different memory and compute characteristics than the RL training phase, which DeepSpeed-HE exploits for optimization.
  - Quick check question: Why is the generation phase more memory bandwidth bound, and how does this affect the choice of optimizations?

## Architecture Onboarding

- Component map:
  - Hybrid Engine -> Training Optimizer (DeepSpeed training engine with ZeRO and LoRA) -> Inference Optimizer (DeepSpeed inference engine with tensor parallelism and high-performance kernels) -> Memory Manager (light-weight system for KV-cache and intermediate results) -> Pipeline Scheduler (coordinates execution order and batch sizing for each RLHF phase)

- Critical path:
  1. Initialize model and pipeline.
  2. Execute SFT (standard training).
  3. Execute reward model training (standard training).
  4. Enter RLHF fine-tuning loop:
     - Switch to inference mode for generation.
     - Generate experiences (prompt + response).
     - Switch to training mode for RL update.
     - Update actor and reward models.
  5. Repeat until convergence or max steps.

- Design tradeoffs:
  - Memory vs. throughput: Larger batches improve efficiency but require more memory; Hybrid Engine balances this by switching modes.
  - Complexity vs. performance: Unified engine adds code complexity but avoids redundant memory copies and transitions.
  - Hardware utilization: Specialized kernels and parallelism maximize throughput but may limit portability across GPU types.

- Failure signatures:
  - Out-of-memory (OOM) errors during generation suggest batch size or model partitioning needs adjustment.
  - Low throughput in generation phase indicates suboptimal kernel or parallelism configuration.
  - Training instability may arise from improper switching logic or mismatched memory states.

- First 3 experiments:
  1. Run a single RLHF iteration on a small model (e.g., OPT-1.3B) with fixed batch size, measure throughput in both modes, and verify seamless switching.
  2. Increase batch size until OOM, then enable tensor parallelism and retest generation throughput.
  3. Profile memory usage and GPU utilization across both modes to identify bottlenecks and confirm expected mode-specific optimizations.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, some potential open questions are:

### Open Question 1
- Question: What is the impact of different reward model sizes on the overall training efficiency and final model quality in the DeepSpeed-RLHF pipeline?
- Basis in paper: [explicit] The paper mentions using OPT-350M as the reward model for both OPT-13B and OPT-66B actor models, but does not explore the effect of varying reward model sizes.
- Why unresolved: The paper does not provide experiments or analysis on how different reward model sizes affect training efficiency or final model quality.
- What evidence would resolve it: Experiments comparing training efficiency and final model quality using different reward model sizes (e.g., OPT-175B, OPT-66B, OPT-30B, OPT-13B) for the same actor model would provide insights into the optimal reward model size.

### Open Question 2
- Question: How does the choice of dataset size and composition affect the performance of ChatGPT-like models trained using DeepSpeed-Chat?
- Basis in paper: [inferred] The paper mentions using multiple datasets for training but does not provide a detailed analysis of how dataset size and composition impact model performance.
- Why unresolved: The paper does not explore the relationship between dataset characteristics and model performance in depth.
- What evidence would resolve it: Experiments varying dataset sizes and compositions while keeping other factors constant would help understand their impact on model performance.

### Open Question 3
- Question: What is the optimal batch size for training ChatGPT-like models using DeepSpeed-Chat, considering both training efficiency and model quality?
- Basis in paper: [inferred] The paper mentions using a maximum global batch size of 0.5M tokens but does not explore the impact of different batch sizes on training efficiency and model quality.
- Why unresolved: The paper does not provide a systematic study of how different batch sizes affect training efficiency and model quality.
- What evidence would resolve it: Experiments varying batch sizes while keeping other factors constant would help identify the optimal batch size for balancing training efficiency and model quality.

## Limitations
- The claimed cost and time efficiency metrics are based on Azure Cloud pricing and specific hardware configurations, which may not be directly reproducible on other cloud providers or on-premise setups.
- The system's performance gains are heavily dependent on the generation phase being the bottleneck, which may not hold for all RLHF workloads or model architectures.
- While the paper claims support for models up to 175B parameters, the scalability to "hundreds of billions" is not empirically demonstrated.

## Confidence
- **High confidence** in the Hybrid Engine's architectural design and the reported efficiency improvements for generation-heavy RLHF pipelines, supported by direct evidence from the paper's performance comparisons.
- **Medium confidence** in the reproducibility of the exact cost and time metrics, as these depend on specific cloud pricing, hardware, and dataset details not fully disclosed.
- **Low confidence** in the system's universal applicability beyond OPT models and RLHF, given the lack of broader benchmarking across different model families or tasks.

## Next Checks
1. Reproduce the throughput and memory usage measurements on a smaller model (e.g., OPT-1.3B) across both training and inference modes, verifying the seamless mode-switching claim.
2. Conduct a scalability test by incrementally increasing model size and batch size, observing the point of OOM or performance degradation to assess the limits of the Hybrid Engine's optimizations.
3. Benchmark DeepSpeed-Chat against alternative RLHF systems (e.g., Colossal-AI, HuggingFace DDP) on a fixed workload, using identical hardware and datasets, to independently verify the reported 6-19x and 1.4-10.5x speedups.