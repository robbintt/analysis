---
ver: rpa2
title: NetHack is Hard to Hack
arxiv_id: '2305.19240'
source_url: https://arxiv.org/abs/2305.19240
tags:
- policy
- learning
- lstm
- training
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance gap between neural and
  symbolic agents in the challenging game of NetHack, where symbolic approaches outperform
  neural ones by a significant margin. To understand the reasons behind this gap,
  the authors analyze the winning symbolic agent, AutoAscend, and extend its codebase
  to generate a large demonstration dataset with hierarchical strategy labels.
---

# NetHack is Hard to Hack

## Quick Facts
- arXiv ID: 2305.19240
- Source URL: https://arxiv.org/abs/2305.19240
- Reference count: 40
- Key outcome: Neural agents improve by 127% (offline) and 25% (online) over previous methods but still lag symbolic models by a wide margin.

## Executive Summary
This paper investigates why neural agents struggle to match symbolic approaches in NetHack, a complex dungeon-crawling game. The authors analyze the winning symbolic agent AutoAscend, extend its codebase to generate a large demonstration dataset with hierarchical strategy labels (HiHack), and use it to train neural policies via hierarchical behavioral cloning. They systematically evaluate the benefits of hierarchy, Transformer architectures, and reinforcement learning fine-tuning. While their state-of-the-art neural agent significantly outperforms prior neural policies, the performance gap with symbolic models and human players remains large, indicating that scaling alone is insufficient.

## Method Summary
The authors generate the HiHack dataset by extending AutoAscend to track its internal strategy selection, producing 109 demonstrations with hierarchical labels. They train neural policies using hierarchical behavioral cloning (HBC), comparing LSTM and Transformer-LSTM architectures with and without hierarchy. Offline training runs for 48 hours on a single GPU, with optional RL fine-tuning using APPO. Evaluation uses 1024 withheld NLE game instances, measuring median and mean NLE scores across 6 random seeds.

## Key Results
- Hierarchical behavioral cloning (HBC) significantly outperforms non-hierarchical behavioral cloning when model capacity is sufficient.
- Large Transformer models show considerable improvements over LSTM baselines, but power-law scaling indicates diminishing returns.
- Online RL fine-tuning further enhances performance, with hierarchy proving beneficial for exploration.

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical behavioral cloning (HBC) significantly outperforms non-hierarchical behavioral cloning (BC) when model capacity is sufficient. AutoAscend uses a directed acyclic graph of explicitly defined strategies, each responsible for accomplishing a specific subgoal. By extending the codebase to track internal strategy selection, the HiHack dataset provides hierarchical labels. Training a neural policy to predict both the high-level strategy and low-level actions mirrors this decomposition, allowing the model to focus on context-specific action spaces per strategy rather than the full 121-dimensional action space. Core assumption: The hierarchical decomposition used by AutoAscend is meaningful and aligns with the structure of the NetHack task, and the model has sufficient capacity to learn both levels of the hierarchy. Evidence anchors: Abstract showing 127% offline and 25% online improvements; section stating HBC significantly outperforms BC with adequate capacity; weak corpus support. Break condition: If the strategy hierarchy is not well-aligned with the task structure, or if the model capacity is insufficient, the benefits of hierarchy may disappear or even hurt performance.

### Mechanism 2
Large Transformer models exhibit considerable improvements over LSTM baselines, but scaling alone is insufficient to close the performance gap with symbolic models. Transformers, with their self-attention mechanisms, are better at capturing long-range dependencies and complex patterns in the multi-modal observations (pixel, text messages, stats) compared to LSTms. This leads to better representation learning from the complex NetHack observations. However, the power-law scaling of performance with model size has a shallow slope, indicating diminishing returns. Core assumption: The complex, multi-modal nature of NetHack observations benefits more from the representation power of Transformers than from LSTMs. Evidence anchors: Abstract noting considerable improvements but shallow power-law slope; section showing Transformer-LSTM outperforms LSTM but performance gap remains; weak corpus support. Break condition: If the task does not require capturing long-range dependencies or complex patterns, or if the data is insufficient for the Transformer to learn effective representations, the advantage may diminish.

### Mechanism 3
Online fine-tuning with RL further enhances performance, with hierarchy proving beneficial for exploration. Behavioral cloning alone can lead to overfitting or fail to correct for mistakes. RL fine-tuning allows the policy to learn from its own interactions, correcting errors and improving robustness. The hierarchical structure, where only the low-level action decoders are updated with RL, helps guide exploration by focusing on context-specific action spaces, making the RL process more efficient. Core assumption: The initial policy learned via HBC provides a good starting point, and the hierarchical structure can guide exploration in the RL phase. Evidence anchors: Abstract stating RL fine-tuning enhances performance with hierarchy benefiting exploration; section showing RL fine-tuning offers significant performance boost; weak corpus support. Break condition: If the initial BC policy is poor, or if the hierarchical structure does not align with the RL dynamics, the benefits of RL fine-tuning may be limited or even harmful.

## Foundational Learning

- **Imitation Learning (Behavior Cloning)**: The paper relies heavily on learning from demonstrations (HiHack dataset) rather than from scratch with RL. Understanding the strengths and limitations of BC is crucial. Quick check: What is the main limitation of vanilla behavior cloning that might necessitate techniques like RL fine-tuning?

- **Hierarchical Reinforcement Learning (HRL)**: The paper introduces hierarchy at the policy level, similar to HRL concepts. Understanding temporal abstraction and skill hierarchies is relevant to interpreting the results. Quick check: How does the fixed hierarchy used in this paper differ from the learned hierarchies typically explored in HRL?

- **Transformer Architectures**: The paper uses a Transformer-LSTM hybrid architecture. Understanding self-attention, causal masking, and context length is important for interpreting the architectural choices and results. Quick check: What is the purpose of using a frozen LSTM as a recurrent encoder in the Transformer-LSTM model?

## Architecture Onboarding

- **Component map**: Observation → Encoders (Conv2D for pixels, MLP for messages, Conv1D+MLP for stats) → Core (LSTM or Transformer-LSTM) → Decoder (Hierarchical: strategy decoder + multiple action decoders, or Non-hierarchical: single action decoder) → Action/Strategy Prediction → Environment Interaction
- **Critical path**: Observation → Encoders → Core → Decoder → Action/Strategy Prediction → Environment Interaction
- **Design tradeoffs**: LSTM vs Transformer: LSTMs are recurrent and can handle variable-length sequences but may struggle with long-range dependencies. Transformers excel at capturing global context but require fixed context length during training. Hierarchical vs Non-hierarchical: Hierarchy can simplify the action space per strategy but adds complexity and potential error accumulation. BC vs RL: BC is simple and data-efficient but can overfit. RL can correct mistakes but requires exploration and may be unstable.
- **Failure signatures**: Overfitting: High training accuracy but poor test performance, especially for Transformer models. Underfitting: Both training and test performance are poor, especially for LSTM models. Hierarchical failure: Poor performance of strategy decoder leading to incorrect action decoder selection.
- **First 3 experiments**: 1. Train a non-hierarchical LSTM model with BC on HiHack and evaluate on withheld NLE instances to establish a baseline. 2. Train a hierarchical LSTM model with HBC on HiHack and compare performance to the non-hierarchical baseline to assess the impact of hierarchy. 3. Train a Transformer-LSTM model with HBC on HiHack and compare performance to both LSTM models to evaluate the benefit of the Transformer architecture.

## Open Questions the Paper Calls Out

### Open Question 1
Would increasing the context length of the transformer model improve performance on NetHack? Basis in paper: The paper suggests that the transformer models may be overfitting due to their limited context length. It mentions that exploring alternate methods for increasing the transformer context length could potentially give the agent a more effective long-term memory. Why unresolved: The paper does not conduct experiments to test the impact of increasing the context length of the transformer model. What evidence would resolve it: Conducting experiments with transformer models that have increased context lengths and comparing their performance to the current models would provide evidence for this question.

### Open Question 2
How would using distributional behavioral cloning (e.g., GAIL, BeT) affect the performance of neural policies in NetHack? Basis in paper: The paper mentions that distributional BC methods might help alleviate the issue of multi-modal demonstration data, where different trajectories can lead to the same reward. Why unresolved: The paper does not experiment with distributional BC methods. What evidence would resolve it: Implementing and comparing the performance of neural policies trained with distributional BC methods to those trained with standard BC methods would provide evidence for this question.

### Open Question 3
Would further increasing the scale of hierarchically-informed, fixed behavioral factorization improve the performance of neural policies in NetHack? Basis in paper: The paper hypothesizes that the quality of neural policies trained with imitation learning on complex, long-horizon tasks like NetHack may be further improved with increases in the scale of hierarchically-informed, fixed behavioral factorization. Why unresolved: The paper does not explore this hypothesis by implementing and testing neural policies with larger hierarchical structures. What evidence would resolve it: Developing and evaluating neural policies with larger hierarchical structures and comparing their performance to the current models would provide evidence for this question.

## Limitations

- Neural approaches, even with hierarchy and Transformers, still lag significantly behind symbolic agents and human players in NetHack.
- The power-law scaling of Transformer performance has a shallow slope, indicating that achieving human-level performance would require impractical computational resources.
- The study relies on demonstrations from a single symbolic agent (AutoAscend), which may limit the diversity of strategies learned by neural models.

## Confidence

- **High Confidence**: The effectiveness of hierarchical behavioral cloning when model capacity is sufficient, supported by clear quantitative improvements over non-hierarchical baselines.
- **Medium Confidence**: The comparative advantage of Transformer over LSTM architectures, as the paper provides strong evidence but acknowledges that scaling alone is insufficient.
- **Medium Confidence**: The benefits of RL fine-tuning with hierarchical structure for exploration, though the magnitude of improvement varies across model classes.

## Next Checks

1. **Architecture Ablation**: Systematically vary Transformer depth, attention heads, and context length to map the exact relationship between capacity and performance, particularly focusing on the point where hierarchical benefits diminish.

2. **Strategy Diversity Analysis**: Compare the strategy distributions learned by neural HBC models against AutoAscend's explicit strategy graph to quantify how well neural models internalize the hierarchical decomposition.

3. **Transfer to New Symbolic Teachers**: Evaluate whether HBC-trained models can effectively learn from demonstrations generated by alternative symbolic agents (not just AutoAscend) to test the robustness of hierarchical imitation learning.