---
ver: rpa2
title: 'FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion
  Models'
arxiv_id: '2312.03517'
source_url: https://arxiv.org/abs/2312.03517
tags:
- diffusion
- image
- arxiv
- ddim
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of diffusion models,
  primarily due to repeated denoising steps needed for high-quality image generation.
  The authors propose FRDiff, a feature reuse (FR) based zero-shot acceleration method
  that leverages temporal redundancy in diffusion models.
---

# FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models

## Quick Facts
- **arXiv ID**: 2312.03517
- **Source URL**: https://arxiv.org/abs/2312.03517
- **Reference count**: 40
- **Primary result**: Achieves up to 1.8x acceleration on diffusion models while maintaining high-quality output

## Executive Summary
FRDiff addresses the high computational cost of diffusion models by introducing a zero-shot acceleration method that leverages temporal redundancy in feature maps. The method combines feature reuse (FR) with reduced number of function evaluations (NFE) through a score mixing technique. By reusing highly similar feature maps across timesteps and selectively mixing scores from different computational paths, FRDiff achieves significant latency improvements while maintaining generation quality across multiple tasks including text-to-image generation, super-resolution, and image inpainting.

## Method Summary
FRDiff is a zero-shot acceleration method that exploits temporal redundancy in diffusion models by reusing feature maps with high cosine similarity across timesteps. The approach targets specific layers (first convolution in residual blocks and attention block outputs in transformer blocks) where feature maps exhibit high temporal similarity. A score mixing technique combines outputs from feature reuse paths and keyframe paths using a sigmoid scheduling function λ(t), allowing the method to balance low-frequency component preservation from reduced NFE with high-frequency detail preservation from feature reuse. The method is universally applicable to various diffusion model architectures without requiring retraining.

## Key Results
- Achieves up to 1.8x acceleration while maintaining high-quality output on diffusion models
- Significantly lower FID scores compared to existing zero-shot acceleration methods within the same latency budget
- Demonstrates effectiveness across multiple tasks: text-to-image generation, super-resolution, and image inpainting
- Transformer blocks show higher computational savings (up to 90% latency reduction) compared to residual blocks (less than 50%) due to architectural differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal redundancy in diffusion model feature maps enables selective skipping of computations without degrading output quality.
- Mechanism: Diffusion models generate feature maps with high cosine similarity across adjacent timesteps. By identifying and reusing these highly similar feature maps, the proposed method can skip redundant computations in residual and transformer blocks while preserving generation quality.
- Core assumption: Feature maps within specific layers exhibit high temporal similarity within small timestep windows (< 5 timesteps).
- Evidence anchors:
  - [abstract] "Reusing feature maps with high temporal similarity opens up a new opportunity to save computation resources without compromising output quality."
  - [section] "According to our extensive analysis, specific modules within diffusion models show high similarity in their feature maps across adjacent frames."
  - [corpus] Weak - corpus neighbors focus on cache-based acceleration but don't directly validate temporal similarity claims.
- Break condition: If feature similarity degrades significantly across timesteps, the reuse strategy would introduce noticeable artifacts and quality loss.

### Mechanism 2
- Claim: Score mixing compensates for the limitations of both reduced NFE and feature reuse by combining their complementary strengths.
- Mechanism: The method linearly interpolates between scores from feature reuse paths and keyframe paths using a sigmoid scheduling function λ(t). This combines the fine-detail preservation of feature reuse with the low-frequency component maintenance of reduced NFE.
- Core assumption: Early denoising stages generate coarse low-frequency components while later stages generate fine high-frequency details, allowing complementary use of different acceleration strategies.
- Evidence anchors:
  - [abstract] "FRDiff is designed to harness the advantages of both reduced NFE and feature reuse, achieving a Pareto frontier that balances fidelity and latency trade-offs"
  - [section] "Our empirical findings suggest that FR is not consistently better than Jump; they possess distinct characteristics and strengths and weaknesses."
  - [corpus] Missing - corpus neighbors don't discuss score mixing as a complementary mechanism.
- Break condition: If the mixing schedule λ(t) is poorly tuned, the method could degrade both low and high-frequency components simultaneously.

### Mechanism 3
- Claim: Feature reuse achieves higher computational savings in transformer blocks compared to residual blocks due to architectural differences.
- Mechanism: In transformer blocks, the method can skip attention and embedding operations, retaining only normalization with addition, achieving up to 90% latency reduction. Residual blocks only skip the initial convolution, achieving <50% reduction.
- Core assumption: Transformer block computations are more amenable to selective skipping than residual block computations.
- Evidence anchors:
  - [section] "In the residual block, we solely skip the initial convolution operation, resulting in a latency reduction of less than 50%. On the other hand, in the case of the transformer block, we can skip the majority of computations, including attention and embedding operations"
  - [section] "In Fig. 4, we depict the latency profiles of individual blocks within two Diffusion U-Net structures: LDM-4 and stable diffusion."
  - [corpus] Weak - corpus neighbors discuss caching mechanisms but don't analyze architectural differences in skip potential.
- Break condition: If transformer block architecture changes or if skip operations introduce instability, the computational savings could diminish.

## Foundational Learning

- Concept: Diffusion models as iterative denoising processes
  - Why needed here: Understanding the iterative nature and temporal redundancy is fundamental to grasping why feature reuse works
  - Quick check question: What is the relationship between timesteps in diffusion models and the generation of coarse vs fine image details?

- Concept: Feature map similarity and cosine distance metrics
  - Why needed here: The method relies on measuring and leveraging temporal similarity between feature maps across timesteps
  - Quick check question: How would you compute the temporal similarity between feature maps from adjacent timesteps?

- Concept: ODE solvers and NFE reduction techniques
  - Why needed here: The method builds on existing NFE reduction approaches and introduces score mixing to combine their benefits
  - Quick check question: What is the primary trade-off when reducing NFE in diffusion models?

## Architecture Onboarding

- Component map: Input noisy image → U-Net processing → Feature map extraction → Conditional reuse/skip → Score mixing → Denoising output

- Critical path: Input noisy image → U-Net backbone with residual and transformer blocks → Feature map extraction points (first conv in residual blocks, attention block outputs in transformer blocks) → Conditional reuse/skip → Score mixing layer for combining FR and keyframe outputs → Denoising output

- Design tradeoffs:
  - Reuse interval vs quality: Longer intervals increase speedup but may degrade quality
  - Skip percentage vs computational savings: More aggressive skipping yields higher speedup but risks artifacts
  - Score mixing schedule vs generation fidelity: Poorly tuned mixing can harm both frequency components

- Failure signatures:
  - Color shifts or detail loss indicating inappropriate reuse intervals
  - Blurry outputs suggesting insufficient NFE compensation
  - Computational overhead if reuse logic is improperly implemented

- First 3 experiments:
  1. Measure feature map similarity across adjacent timesteps in a pretrained diffusion model to validate temporal redundancy
  2. Implement basic feature reuse with fixed intervals and measure FID-latency tradeoff
  3. Add score mixing with varying schedules to evaluate frequency component preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal skip interval vary across different diffusion model architectures and tasks?
- Basis in paper: [inferred] The paper mentions that the skip interval can be adjusted to maximize output quality within a given latency target, but it is uniformly allocated across experiments. It also states that "The search for the optimal skip interval is a topic left for future work."
- Why unresolved: The paper does not explore how the optimal skip interval might differ based on model architecture, task type, or dataset characteristics. It only uses a uniform allocation of keyframes.
- What evidence would resolve it: Experiments varying the skip interval across different architectures (e.g., pixel-space vs latent-space models), tasks (e.g., text-to-image, super-resolution, inpainting), and datasets to identify optimal intervals for each scenario.

### Open Question 2
- Question: How sensitive are different layers within the U-Net structure to feature reuse, and could selective layer reuse improve performance?
- Basis in paper: [explicit] The paper mentions that "the network components having low similarity are computed as usual" and suggests in the discussion that "considering the sensitivity of layers when applying feature reusing and reusing different layers over time may lead to improved performance."
- Why unresolved: The current implementation applies feature reuse uniformly to specific block types (residual and transformer blocks) without exploring layer-level sensitivity or selective reuse strategies.
- What evidence would resolve it: Detailed analysis of feature map similarity at different layers within the U-Net, followed by experiments implementing selective feature reuse based on layer sensitivity to identify performance improvements.

### Open Question 3
- Question: How would FRDiff perform when combined with other optimization techniques like model compression or architectural modifications?
- Basis in paper: [explicit] The paper states "please note that the proposed method could apply along with other learning-based or backbone optimization studies" in the related works section, but does not conduct experiments combining FRDiff with these techniques.
- Why unresolved: The paper focuses solely on demonstrating FRDiff as a standalone zero-shot acceleration method and does not explore its compatibility or synergistic effects with other optimization approaches.
- What evidence would resolve it: Experiments combining FRDiff with techniques like model pruning, quantization, or architectural modifications to evaluate whether performance benefits can be further enhanced through combined approaches.

## Limitations

- The method's effectiveness depends heavily on the temporal similarity of feature maps, which may vary across different diffusion model architectures and datasets
- The claimed 1.8x acceleration with maintained quality is supported by experimental results but may be architecture-dependent
- The optimal skip interval and score mixing schedule require empirical tuning and may not generalize across all tasks and models

## Confidence

- **High confidence**: The fundamental mechanism of feature reuse leveraging temporal redundancy is theoretically sound and supported by empirical measurements of feature similarity
- **Medium confidence**: The score mixing approach combining FR and NFE reduction benefits is promising but relies on empirical tuning of the mixing schedule λ(t)
- **Medium confidence**: The claimed 1.8x acceleration with maintained quality is supported by experimental results but may be architecture-dependent

## Next Checks

1. **Architecture Generalization Test**: Apply FRDiff to a non-U-Net diffusion architecture (e.g., a diffusion transformer) to verify if temporal similarity patterns hold and if the method provides similar benefits
2. **Temporal Similarity Analysis**: Systematically measure feature map cosine similarity across timesteps for different layers and different diffusion models to quantify the scope of temporal redundancy
3. **Ablation Study on Score Mixing**: Conduct a comprehensive ablation study varying the mixing schedule λ(t) and interval parameters to identify the Pareto-optimal configurations across different tasks and models