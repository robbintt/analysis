---
ver: rpa2
title: A New Interpretable Neural Network-Based Rule Model for Healthcare Decision
  Making
arxiv_id: '2309.11101'
source_url: https://arxiv.org/abs/2309.11101
tags:
- neural
- datasets
- tt-rules
- learning
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TT-rules, a neural network framework that
  transforms a deep learning model into an interpretable rule-based model for healthcare
  decision-making. The method extracts necessary and sufficient rules from a trained
  Truth Table network (TTnet) to achieve both global and exact interpretability.
---

# A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making

## Quick Facts
- arXiv ID: 2309.11101
- Source URL: https://arxiv.org/abs/2309.11101
- Reference count: 25
- Key outcome: TT-rules transforms neural networks into interpretable rule-based models, achieving high performance on healthcare datasets while reducing input dimensions by half or more.

## Executive Summary
This paper introduces TT-rules, a novel framework that combines the interpretability of rule-based models with the high performance of deep neural networks (DNNs) for healthcare decision-making. The framework builds upon Truth Table networks (TTnet), transforming a trained neural network into an interpretable rule-based model by extracting necessary and sufficient rules. TT-rules supports binary classification, multi-label classification, and regression tasks on both small and large tabular datasets, including real-life DNA datasets with over 20K features. The method demonstrates superior performance compared to existing interpretable methods while simultaneously acting as an effective feature selection technique.

## Method Summary
TT-rules is a neural network framework that transforms a deep learning model into an interpretable rule-based model by extracting necessary and sufficient rules from a trained Truth Table network (TTnet). The process involves training a TTnet model, then extracting rules that faithfully reproduce the model's behavior (global and exact interpretability). The framework optimizes the extracted rules through two steps: integrating human logic into truth tables to reduce individual rule sizes, and analyzing correlations to decrease the total number of rules. TT-rules scales to large tabular datasets while maintaining high performance and acts as a feature selection method, reducing input dimensions by half or more without sacrificing accuracy.

## Key Results
- Achieved 0.029 RMSE for regression tasks on TCGA lung cancer dataset
- Attained 0.835 AUC for binary classification on DNA datasets with 23,689 features
- Reached 0.986 accuracy for multi-classification on Diabetes 130 US-Hospitals dataset
- Generated 9,472 rules from 23,689 features, effectively halving input size while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TT-rules transforms a deep neural network into a rule-based model by extracting necessary and sufficient rules from a trained Truth Table network (TTnet).
- Mechanism: The Truth Table network (TTnet) is a family of deep neural networks that can be represented as a Boolean formula. After training, TT-rules extracts the necessary and sufficient rules (R) from the TTnet model to create an interpretable rule-based model.
- Core assumption: The trained TTnet model can be accurately represented as a Boolean formula and the extracted rules can faithfully reproduce the model's behavior.
- Evidence anchors:
  - [abstract]: "By extracting the necessary and sufficient rules R from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model."
  - [section 2]: "This paper introduces a novel neural network framework that effectively combines the interpretability of rule-based models with the high performance of DNNs. Our framework, called TT-rules, builds upon the advancements made by Benamira et al. [4] who introduced a new Convolutional Neural Network (CNN) filter function called the Learning Truth Table (LTT) block. The LTT block has the unique property of its complete distribution being computable in constant and practical time, regardless of the architecture. This allows the transformation of the LTT block from weights into an exact mathematical Boolean formula. Since an LTT block is equivalent to a CNN filter, the entire neural network model, known as Truth Table net (TTnet), can itself be represented as a Boolean formula."

### Mechanism 2
- Claim: TT-rules optimizes the extracted rules through human logic integration and correlation analysis to reduce rule set size.
- Mechanism: TT-rules integrates human logic into the truth tables to reduce the size of each rule in the set R. Then, it analyzes the correlation to decrease the number of rules in R. These optimizations automatically and efficiently transform the set R into an optimized set in constant time.
- Core assumption: Human logic can be effectively integrated into the truth tables, and correlation analysis can meaningfully reduce the rule set size without compromising accuracy.
- Evidence anchors:
  - [section 2]: "We then optimize our formula set R in two steps. We automatically integrate human logic into the truth tables. This reduces the size of each rule in the set R. Then we analyze the correlation to decrease the number of rules in R. These optimizations, specific to the TT-rules framework, automatically and efficiently transform the set R into an optimized set in constant time."

### Mechanism 3
- Claim: TT-rules scales to large tabular datasets with over 20K features while maintaining high performance.
- Mechanism: TT-rules leverages the scalability of the TTnet architecture, which has been demonstrated on large datasets like ImageNet. Additionally, TT-rules acts as a feature selection method, reducing the input feature set while maintaining high performance.
- Core assumption: The TTnet architecture can scale to large datasets, and the feature selection process can effectively reduce the input feature set without compromising accuracy.
- Evidence anchors:
  - [abstract]: "Notably, TT-rules presents the first accurate rule-based model capable of fitting large tabular datasets, including two real-life DNA datasets with over 20K features."
  - [section 3.3]: "Our TT-rules framework demonstrated excellent scalability to real-life datasets with up to 20K features. This result is not surprising, considering the original TTnet paper [4] showed the architecture's ability to scale to ImageNet. Furthermore, our framework's superiority was demonstrated by outperforming other rule-based models that failed to converge to such large datasets (GL [24]). Regarding performance, the TT-rules framework outperforms all other methods. Our approach not only scales but also reduces the input features set, acting as a feature selection method. We generated a set of 1064 rules out of 20530 features for the regression problem, corresponding to a drastic reduction in complexity. For the binary classification dataset, we generated 9472 rules, which more than halved the input size from 23689 to 9472."

## Foundational Learning

- Concept: Truth Table networks (TTnet)
  - Why needed here: TT-rules is built upon the TTnet architecture, which is crucial for understanding how the framework works and its scalability.
  - Quick check question: What is the key property of the Learning Truth Table (LTT) block that enables the TTnet architecture to be represented as a Boolean formula?

- Concept: Rule extraction and optimization
  - Why needed here: TT-rules extracts rules from the trained TTnet model and optimizes them through human logic integration and correlation analysis. Understanding this process is essential for implementing and improving the framework.
  - Quick check question: How does TT-rules integrate human logic into the truth tables, and how does it analyze correlation to reduce the rule set size?

- Concept: Interpretability and performance trade-offs
  - Why needed here: TT-rules aims to combine the interpretability of rule-based models with the high performance of deep neural networks. Understanding the trade-offs between interpretability and performance is crucial for applying the framework in healthcare decision-making.
  - Quick check question: How does TT-rules balance the need for interpretability with the requirement for high performance in healthcare applications?

## Architecture Onboarding

- Component map: Truth Table network (TTnet) -> Rule extraction module -> Rule optimization module -> Feature selection module -> Interpretable rule-based model

- Critical path:
  1. Train a TTnet model on the target dataset
  2. Extract necessary and sufficient rules from the trained TTnet model
  3. Optimize the extracted rules through human logic integration and correlation analysis
  4. Apply feature selection to reduce input feature set
  5. Generate the interpretable rule-based model

- Design tradeoffs:
  - Balancing interpretability and performance - more interpretable models may sacrifice some performance, while high-performance models may be less interpretable
  - Rule set size vs. accuracy - larger rule sets may improve accuracy but reduce interpretability, while smaller rule sets may improve interpretability but reduce accuracy
  - Feature selection vs. completeness - aggressive feature selection may reduce model complexity but miss important features, while conservative feature selection may preserve important features but increase model complexity

- Failure signatures:
  - Poor performance on the target dataset - may indicate issues with the TTnet model or rule extraction process
  - Overly complex or large rule sets - may indicate issues with the rule optimization or feature selection process
  - Inability to scale to large datasets - may indicate issues with the TTnet architecture or implementation

- First 3 experiments:
  1. Train a TTnet model on a small tabular dataset (e.g., Breast Cancer Wisconsin) and extract rules to verify the basic functionality of the TT-rules framework.
  2. Apply the rule optimization process to the extracted rules and compare the performance and interpretability of the optimized rule set with the original rule set.
  3. Scale the TT-rules framework to a larger tabular dataset (e.g., DNA dataset) and evaluate its performance and scalability compared to other rule-based methods.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions. However, based on the content, several implicit open questions emerge regarding the framework's generalizability, interpretability validation, and applicability to diverse healthcare scenarios.

## Limitations
- Implementation details of the TT-rules framework, including specific architecture and optimization process, are not fully disclosed
- Hyperparameters and training configurations used for the framework are not explicitly mentioned
- The framework's performance on multi-modal data (combining tabular, text, and image data) remains untested

## Confidence
- High: The general concept of transforming neural networks into rule-based models for interpretability
- Medium: The specific implementation details and performance claims of TT-rules
- Low: The scalability and generalizability of the approach to diverse healthcare datasets and applications

## Next Checks
1. Implement and test TT-rules on a wider range of healthcare datasets, including those with different data types and characteristics, to assess its generalizability and robustness.
2. Conduct a thorough computational complexity analysis to determine the runtime and resource requirements of the TT-rules framework, especially for large-scale healthcare applications.
3. Collaborate with domain experts to evaluate the interpretability and clinical relevance of the generated rules, ensuring that the extracted knowledge aligns with established medical practices and insights.