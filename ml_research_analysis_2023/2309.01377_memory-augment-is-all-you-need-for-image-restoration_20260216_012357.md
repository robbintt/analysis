---
ver: rpa2
title: Memory augment is All You Need for image restoration
arxiv_id: '2309.01377'
source_url: https://arxiv.org/abs/2309.01377
tags:
- image
- memory
- removal
- shadow
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MemoryNet, a network that combines a memory
  augmentation module and contrastive learning for image restoration tasks. The memory
  augmentation module uses learnable latent variables to remember representative structural
  patterns, while contrastive learning constrains features to balance positive, negative,
  and actual samples.
---

# Memory augment is All You Need for image restoration

## Quick Facts
- arXiv ID: 2309.01377
- Source URL: https://arxiv.org/abs/2309.01377
- Reference count: 40
- This paper proposes MemoryNet, a network that combines a memory augmentation module and contrastive learning for image restoration tasks

## Executive Summary
This paper introduces MemoryNet, a novel architecture for image restoration tasks that leverages memory augmentation and contrastive learning. The method combines learnable memory prototypes to capture structural patterns with a three-class contrastive learning framework to improve feature discrimination. MemoryNet demonstrates significant improvements over state-of-the-art methods on three challenging tasks: image de-shadowing, real image deraining, and image deblurring, achieving substantial gains in PSNR and SSIM metrics while maintaining computational efficiency.

## Method Summary
MemoryNet integrates a memory augmentation module with contrastive learning for image restoration. The memory module stores learnable prototypes that capture representative structural patterns, using cosine similarity-based attention to retrieve relevant information at multiple semantic levels. The architecture employs a three-stage encoder-decoder network with supervised attention modules between stages for feature realignment. A three-class contrastive learning framework treats clean, shadowed, and de-shadowed samples as distinct classes to improve feature discrimination. The network is trained with a combined loss function that balances reconstruction accuracy with contrastive feature learning.

## Key Results
- Achieves 28.03 PSNR and 0.952 SSIM on the ISTD shadow removal dataset
- Achieves 25.38 PSNR and 0.84 SSIM on the Raindrop real rain removal dataset
- Achieves 30.76 PSNR and 0.953 SSIM on the GoPro image deblurring dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The memory augmentation module captures global representative structural patterns through learnable latent variables
- Mechanism: The memory module stores N prototypes with fixed feature dimension C. During processing, each feature is compared to all memory items using cosine similarity, generating attention weights that combine the most relevant prototypes. This creates a multi-level semantic representation that preserves structural information across different scales.
- Core assumption: Representative structural patterns can be effectively captured and stored in a fixed memory dimension, and these patterns are transferable across different image restoration tasks
- Evidence anchors:
  - [abstract] "The memory augmentation module uses learnable latent variables to remember representative structural patterns"
  - [section] "The memory module contains N prototypes recorded by a metric M ∈ RN ×C with a fixed feature dimension C"
- Break condition: If the memory prototypes become too generic or too specific, they lose transferability. If the cosine similarity metric fails to capture meaningful relationships between features and prototypes, the memory module becomes ineffective.

### Mechanism 2
- Claim: Contrastive learning constrains features to balance positive, negative, and actual samples
- Mechanism: The network treats shadow removal as a three-class classification problem with positive (clean), negative (shadowed), and standard (de-shadowed) samples. Global features act as anchor points, enabling the model to discriminate whether features come from the same image type. This forces the network to learn discriminative features that separate different image states.
- Core assumption: The three-class formulation effectively captures the semantic differences between clean, shadowed, and de-shadowed images, and the model can learn meaningful boundaries between these classes
- Evidence anchors:
  - [abstract] "dividing the samples into positive, negative, and actual three samples for contrastive learning"
  - [section] "We define the image de-shadowing task as a three-class classification problem, corresponding to positive (clean samples), standard (de-shadowed samples), and negative samples (shadowed samples)"
- Break condition: If the three-class distinction becomes ambiguous (e.g., semi-shadowed regions), or if the contrastive loss overwhelms the reconstruction objective, the model may prioritize classification over accurate restoration.

### Mechanism 3
- Claim: Multi-scale feature fusion with cross-stage attention modules improves restoration quality
- Mechanism: The network uses a three-stage encoder-decoder architecture with supervised attention modules between stages. These modules realign features based on ground truth supervision before passing them to the next stage. Additionally, intermediate multi-scale features from earlier stages are fused into later stages to consolidate contextual information.
- Core assumption: Feature realignment based on ground truth and cross-stage fusion provides meaningful improvements over simple cascading of stages
- Evidence anchors:
  - [section] "Instead of simply cascading multiple stages, we add a supervised attention module between each two stages"
  - [section] "Under the supervision of the real image, our module readjusts the feature maps of the previous stage before passing them on to the next stage"
- Break condition: If the attention modules become too dependent on ground truth supervision and fail to generalize, or if feature fusion introduces noise rather than useful information, performance may degrade.

## Foundational Learning

- Concept: Memory networks and prototype-based learning
  - Why needed here: Understanding how to design and implement memory modules that can store and retrieve representative patterns is crucial for implementing the memory augmentation component
  - Quick check question: How does the cosine similarity-based attention mechanism in the memory module differ from traditional attention mechanisms in transformers?

- Concept: Contrastive learning and metric learning
  - Why needed here: The contrastive learning component requires understanding how to formulate appropriate positive/negative pairs and design loss functions that encourage feature discrimination
  - Quick check question: What are the key differences between triplet loss, contrastive loss, and the three-class contrastive approach used in this paper?

- Concept: Multi-scale feature fusion and skip connections
  - Why needed here: The architecture relies on cross-stage feature fusion and attention modules, requiring understanding of how to effectively combine features from different scales and stages
  - Quick check question: How does the cross-stage feature fusion in this architecture compare to traditional U-Net skip connections?

## Architecture Onboarding

- Component map:
  - Memory Augmentation Module: Contains learnable memory prototypes, cosine similarity-based attention, and hierarchical semantic levels (part, instance, semantic)
  - Three-Stage Encoder-Decoder Network: Each stage processes features at increasing semantic levels
  - Supervised Attention Modules: Realign features between stages based on ground truth
  - Contrastive Learning Discriminator: Three-class classification for shadow/clean image discrimination
  - Feature Fusion Mechanism: Combines intermediate features across stages

- Critical path:
  1. Input image → Memory Augmentation (stage 1) → Supervised attention realignment
  2. Memory features + stage 1 output → Encoder 2 → Memory Augmentation (stage 2) → Feature fusion with stage 1 decoder output
  3. Memory features + stage 2 output → Encoder 3 → Memory Augmentation (stage 3) → Feature fusion with stage 2 decoder output
  4. Final decoder output → Contrastive learning head for three-class discrimination
  5. Total loss = Reconstruction loss + Contrastive loss

- Design tradeoffs:
  - Memory size (N) vs. computational cost: Larger memory captures more patterns but increases computation
  - Number of semantic levels vs. abstraction: More levels provide finer-grained representation but may overfit
  - Weight of contrastive loss vs. reconstruction quality: Higher contrastive weight improves discrimination but may hurt pixel-level accuracy

- Failure signatures:
  - Memory module produces nearly uniform attention weights → prototypes are too generic or feature similarity metric is ineffective
  - Contrastive loss dominates training → network prioritizes classification over restoration quality
  - Feature fusion introduces artifacts → stage alignment is incorrect or fusion weights are improperly balanced

- First 3 experiments:
  1. Baseline test: Run the network without memory augmentation and contrastive learning to establish performance baseline
  2. Memory ablation: Test with only one semantic level in memory (remove hierarchical structure) to evaluate impact of multi-level representation
  3. Contrastive analysis: Visualize feature embeddings with t-SNE to verify that the three-class structure is being learned effectively

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Memory augmentation module implementation details are not fully specified, particularly regarding how prototypes are updated during training
- Three-class contrastive learning may struggle with gradual shadow boundaries that don't fit neatly into discrete categories
- The method's performance on other image restoration tasks beyond the three evaluated tasks is unknown

## Confidence
- Memory Augmentation Mechanism (Medium): While the concept of using learnable prototypes is well-established, the specific implementation details and effectiveness of the cosine similarity-based attention mechanism require further validation
- Contrastive Learning Framework (High): The three-class formulation is clearly defined and the training objective is explicitly stated, making this component relatively straightforward to implement and verify
- Multi-scale Feature Fusion (Low): The paper provides limited details about how the cross-stage attention modules work and how features are effectively combined across different semantic levels

## Next Checks
1. **Memory Prototype Visualization**: Visualize the learned memory prototypes across different semantic levels to verify they capture meaningful structural patterns and show diversity in their representations
2. **Attention Weight Analysis**: Examine the cosine similarity-based attention weights to ensure they are not collapsing to uniform distributions, which would indicate ineffective prototype matching
3. **Contrastive Embedding Verification**: Use t-SNE or similar visualization techniques to confirm that the three-class contrastive learning successfully separates clean, shadowed, and de-shadowed image features in the embedding space