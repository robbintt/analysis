---
ver: rpa2
title: Deep Neural Networks Tend To Extrapolate Predictably
arxiv_id: '2310.00873'
source_url: https://arxiv.org/abs/2310.00873
tags:
- neural
- network
- inputs
- distribution
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether neural network predictions on high-dimensional
  out-of-distribution (OOD) inputs tend to revert towards the optimal constant solution
  (OCS), i.e., the prediction that minimizes the average loss over the training data
  without observing the input. The authors present empirical evidence across 8 datasets,
  3 loss functions, and both CNNs and transformers showing that as inputs become more
  OOD, neural network predictions move closer to the OCS.
---

# Deep Neural Networks Tend To Extrapolate Predictably

## Quick Facts
- arXiv ID: 2310.00873
- Source URL: https://arxiv.org/abs/2310.00873
- Reference count: 40
- Primary result: Neural networks' predictions on OOD inputs converge toward the optimal constant solution

## Executive Summary
This paper investigates whether neural network predictions on high-dimensional out-of-distribution (OOD) inputs tend to revert towards the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. The authors present empirical evidence across 8 datasets, 3 loss functions, and both CNNs and transformers showing that as inputs become more OOD, neural network predictions move closer to the OCS. They provide both empirical and theoretical analyses to explain this behavior, showing that OOD inputs yield smaller-magnitude representations in the network, causing the output to be dominated by model constants that approximate the OCS.

## Method Summary
The authors train models using various architectures (CNNs and transformers) on multiple datasets with three different loss functions (cross entropy, MSE, Gaussian NLL). They evaluate these models on OOD datasets with varying levels of distributional shift, measuring the distance between predictions and the OCS. The theoretical analysis focuses on homogeneous networks with ReLU activations, using gradient flow dynamics to explain why OOD representations shrink in magnitude and cause outputs to be dominated by bias terms. The framework is applied to enable risk-sensitive decision-making in selective classification scenarios.

## Key Results
- Neural network predictions consistently move closer to the OCS as inputs become more OOD across all tested datasets and architectures
- OOD inputs produce smaller-magnitude representations in later network layers, causing outputs to be dominated by bias terms
- The accumulated model constants (biases) in trained networks approximate the OCS for the loss function used
- Risk-sensitive decision-making based on OCS distance improves OOD selective classification performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural network predictions on OOD inputs move toward the optimal constant solution (OCS).
- Mechanism: As inputs become more OOD, network representations in later layers shrink in magnitude because they lie increasingly outside the subspace spanned by the weight matrices. This causes outputs to be dominated by bias terms that approximate the OCS.
- Core assumption: The OCS is meaningful and computable for the loss function used.
- Evidence anchors:
  - [abstract] "As inputs diverge further from the training distribution, a neural network's predictions often converge towards a fixed constant value."
  - [section 3.1] "As the likelihood of samples from POOD(x) under Ptrain(x) decreases, fθ(x) for x ∼ POOD(x) tends to approach f*constant."
- Break condition: If OOD inputs yield large-magnitude representations or if biases do not align with OCS, the mechanism fails.

### Mechanism 2
- Claim: Representations of OOD inputs have less overlap with weight matrices than in-distribution inputs.
- Mechanism: OOD features project poorly onto the top singular vectors of weight matrices, causing a drop in activation magnitude through the network.
- Core assumption: Weight matrices in trained networks occupy low-dimensional subspaces that align well with in-distribution data.
- Evidence anchors:
  - [section 4.1] "The middle plots of Fig. 4 show the ratio of the representation's norm at layer j that is captured by projecting the representation onto Vtop... This tends to decrease as distributional shift increases."
- Break condition: If OOD data happens to align with weight subspaces or if networks are shallow, the mechanism weakens.

### Mechanism 3
- Claim: The accumulated model constants (biases) in a trained network approximate the OCS.
- Mechanism: Gradient flow on homogeneous networks with ReLU converges to solutions where the bias term is proportional to the sum of labels on margin points, which matches the OCS for the loss.
- Core assumption: The label distribution of margin points mimics the overall training label distribution.
- Evidence anchors:
  - [section 4.2] Proposition 4.2: "If gradient flow on F̃ converges directionally to Ŵ, ˆb, then ˆb ∝ Σk yk for margin points."
- Break condition: If margin points have label distribution very different from training set, bias will not approximate OCS.

## Foundational Learning

- Concept: Out-of-distribution (OOD) inputs and covariate shift
  - Why needed here: The paper studies how networks behave on inputs drawn from a distribution different from training data, a common real-world scenario.
  - Quick check question: What is the difference between covariate shift and label shift, and which one does this paper assume?

- Concept: Optimal constant solution (OCS)
  - Why needed here: OCS is the target toward which predictions revert when inputs are OOD; understanding it is key to interpreting the results.
  - Quick check question: How is the OCS defined for cross-entropy loss versus MSE loss?

- Concept: Gradient flow and implicit bias in deep homogeneous networks
  - Why needed here: The theoretical analysis relies on gradient flow convergence to low-rank solutions that explain OOD behavior.
  - Quick check question: What is the relationship between gradient flow and gradient descent with infinitesimally small step size?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model (ResNet/VGG/DistilBERT) -> Loss function (CE/MSE/Gaussian NLL) -> Evaluation (OOD score, distance to OCS)
  Theoretical analysis: Homogeneous networks with ReLU -> Gradient flow dynamics -> Low-rank bias -> OCS alignment

- Critical path:
  1. Train model on in-distribution data
  2. Measure OOD score for evaluation datasets
  3. Compute distance between predictions and OCS
  4. Analyze representation norms and weight alignment
  5. Apply risk-sensitive decision-making if desired

- Design tradeoffs:
  - Using homogeneous networks simplifies theory but may not capture all practical architectures
  - OCS-based decision-making requires choosing a loss whose OCS matches desired cautious behavior
  - OOD detection via linear classifier on featurized data is simple but may not capture all types of distribution shift

- Failure signatures:
  - OOD score increases but distance to OCS does not decrease
  - Representation norms do not shrink for OOD inputs
  - Biases do not align with OCS in experiments
  - Performance degrades on adversarial or highly structured OOD data

- First 3 experiments:
  1. Train a ResNet on CIFAR10 and evaluate on CIFAR10-C with varying corruption levels; plot OOD score vs. distance to OCS.
  2. For a trained MNIST model, visualize representation norms at different layers for in-distribution and adversarial OOD inputs.
  3. Implement selective classification using MSE loss; compare action selection frequency on in-distribution vs. OOD inputs.

## Open Questions the Paper Calls Out

- What specific properties of an OOD distribution determine whether and to what extent "reversion to the OCS" occurs?
- How does the "reversion to the OCS" phenomenon extend to more complex, multi-step decision-making problems beyond selective classification?
- Why does "reversion to the OCS" sometimes fail, as observed with adversarial inputs and impulse noise?

## Limitations

- The theoretical analysis is limited to homogeneous networks with ReLU activations, which may not capture all practical architectures
- The paper focuses primarily on synthetic and natural distribution shifts that maintain the same input space as training data
- The OCS concept has limitations for cross-entropy loss, as it corresponds to the marginal label distribution which may not be meaningful for all classification tasks

## Confidence

- Empirical results consistency: High
- Theoretical mechanism validity: Medium
- Cross-architecture generalization: Medium

## Next Checks

1. Test the OCS convergence phenomenon on non-homogeneous architectures like residual networks with batch normalization or attention-based transformers to verify if the theoretical mechanism generalizes beyond the current analysis scope.

2. Evaluate network behavior on inputs from completely different modalities or domains (e.g., testing a CIFAR10 model on medical imaging data) to determine if the OCS convergence phenomenon holds for truly novel input spaces.

3. Test the framework's effectiveness against adversarial OOD inputs designed to exploit the OCS convergence behavior, examining whether the proposed risk-sensitive decision-making strategy provides meaningful protection against such attacks.