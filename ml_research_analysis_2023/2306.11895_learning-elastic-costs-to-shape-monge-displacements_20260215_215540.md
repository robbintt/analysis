---
ver: rpa2
title: Learning Elastic Costs to Shape Monge Displacements
arxiv_id: '2306.11895'
source_url: https://arxiv.org/abs/2306.11895
tags:
- optimal
- transport
- cost
- maps
- estimator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies learning ground-truth optimal transport maps
  using elastic costs, which include a regularization term that shapes the structure
  of displacements. The authors propose a method to compute h-transforms and ground-truth
  OT maps for general structured costs using proximal gradient descent, enabling the
  generation of synthetic problems with known OT maps.
---

# Learning Elastic Costs to Shape Monge Displacements

## Quick Facts
- arXiv ID: 2306.11895
- Source URL: https://arxiv.org/abs/2306.11895
- Reference count: 40
- One-line primary result: Learning elastic costs with regularization terms improves optimal transport map estimation by capturing structured displacements.

## Executive Summary
This paper addresses the problem of learning ground-truth optimal transport maps when the underlying cost structure is not the standard squared-Euclidean distance. The authors propose using elastic costs that incorporate a regularization term to shape the structure of displacements, enabling more accurate estimation of transport maps. They develop methods to compute h-transforms and ground-truth OT maps for general structured costs using proximal gradient descent, and introduce a learning framework to tune the parameters of the regularizer through bilevel optimization. The paper provides theoretical guarantees for the MBO estimator under subspace-structured costs and demonstrates through experiments that using the correct structured cost significantly improves estimation performance compared to the standard cost.

## Method Summary
The paper proposes computing h-transforms for elastic costs using proximal gradient descent, where the proximal operator of the regularized cost can be decomposed into the proximal operator of the regularizer followed by scaling. For learning the regularizer parameters, the authors employ a bilevel optimization approach, where the inner problem solves the optimal transport problem with the current cost parameters, and the outer problem minimizes a loss function based on the optimal transport coupling. The gradient for the outer problem is computed using implicit differentiation of the Sinkhorn solution. The method is applied to learn low-dimensional subspace structures for displacements, optimizing over the Stiefel manifold using Riemannian gradient descent.

## Key Results
- The MBO estimator under subspace-structured costs achieves improved sample complexity depending on the subspace dimension p rather than the full ambient dimension d.
- Learning the correct cost structure (e.g., ℓ1 or subspace-structured) significantly outperforms using the standard squared-Euclidean cost in estimating transport maps.
- As the regularization parameter γ approaches infinity, the subspace-structured cost recovers the spiked transport model, providing theoretical insight into the limiting behavior.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The h-transform of an arbitrary potential function f can be computed using proximal gradient descent, enabling ground-truth optimal transport maps for structured costs.
- **Mechanism:** For a cost h = ½||·||² + γτ, the proximal operator of λh can be decomposed into the proximal operator of λγ/(λ+1)τ followed by a scaling. This allows the use of proximal gradient descent to minimize h(x-y) - f(x), yielding the h-transform.
- **Core assumption:** The potential function f is concave and smooth, and the proximal operator of τ is available.
- **Evidence anchors:**
  - [abstract]: "For any elastic cost, we propose a numerical method to compute Monge maps that are provably optimal."
  - [section 3.1]: "We define implicitly the h-transform of an arbitrary concave potential f as the minimizer (obtained using proximal gradient descent) of h(x, ·) - f(·)."
  - [corpus]: Weak - corpus neighbors do not discuss proximal gradient descent or h-transform computation.
- **Break condition:** If f is not smooth or the proximal operator of τ is not available, the proximal gradient descent will not converge or cannot be applied.

### Mechanism 2
- **Claim:** Learning the parameters of a regularizer τθ through bilevel optimization enables adaptive cost structures that capture suitable regularity in displacements.
- **Mechanism:** The cost function L(θ) = ⟨P⋆(θ), M(θ)⟩ is minimized, where P⋆(θ) is the optimal transport coupling for cost h = ½||·||² + γτθ, and M(θ) is the matrix of regularization values τθ(zij). The gradient ∇L(θ) is computed using implicit differentiation of the Sinkhorn solution.
- **Core assumption:** The Sinkhorn algorithm can be differentiated through, and the implicit function theorem applies to the optimization problem defining P⋆(θ).
- **Evidence anchors:**
  - [abstract]: "We propose a learning framework to tune the parameters of the regularizer by minimizing a bilevel optimization objective."
  - [section 4.1]: "We consider all other parameters constant and re-write (8) as: P⋆(θ) := P⋆(X, a, Y, b; ½ℓ²₂ + γτθ, ε). The matrix P⋆(θ) contains n × m weights that each quantify the association strength between a pair (xi, yj)."
  - [corpus]: Weak - corpus neighbors do not discuss bilevel optimization or implicit differentiation of Sinkhorn solutions.
- **Break condition:** If the Sinkhorn solution is not differentiable or the implicit function theorem conditions are not met, the gradient computation will fail.

### Mechanism 3
- **Claim:** The subspace structured cost ½||·||² + γτA⊥ promotes displacements in a low-dimensional subspace, and as γ → ∞, the optimal transport map recovers the spiked transport model.
- **Mechanism:** The regularizer τA⊥(z) = ½||A⊥z||²₂ penalizes displacements outside the subspace spanned by A. As γ increases, the optimal coupling concentrates on the subspace, recovering the spiked transport model where T(x) = x - AT(Ax - S(Ax)).
- **Core assumption:** The spiked transport model is the limit of the subspace structured cost as γ → ∞.
- **Evidence anchors:**
  - [abstract]: "We also introduce a learning framework to tune the parameters of the regularizer by minimizing a bilevel optimization objective, with a specific application to learning low-dimensional subspaces for displacements."
  - [section 5.2]: "The authors studied the estimation of the Wasserstein distance in the setting where the Brenier map between µ and ν takes the form, Tspiked(x) = x - AT(Ax - S(Ax)), where A ∈ S p,d and S : Rp → Rp is the gradient of a convex function on Rp."
  - [corpus]: Weak - corpus neighbors do not discuss spiked transport models or subspace structured costs.
- **Break condition:** If the spiked transport model does not accurately describe the limit as γ → ∞, the theoretical guarantees may not hold.

## Foundational Learning

- **Concept:** Optimal transport and the Monge problem
  - **Why needed here:** The paper is fundamentally about computing and learning optimal transport maps, which requires understanding the Monge problem and its dual formulations.
  - **Quick check question:** What is the Monge problem, and how does it relate to the Kantorovich dual formulation?

- **Concept:** Proximal operators and proximal gradient descent
  - **Why needed here:** The computation of h-transforms and ground-truth optimal transport maps relies on proximal gradient descent, which requires knowledge of proximal operators.
  - **Quick check question:** What is the proximal operator of a function, and how is it used in proximal gradient descent?

- **Concept:** Bilevel optimization and implicit differentiation
  - **Why needed here:** The learning framework for tuning the regularizer parameters involves bilevel optimization, which requires implicit differentiation of the Sinkhorn solution.
  - **Quick check question:** What is bilevel optimization, and how can implicit differentiation be used to compute gradients in this setting?

## Architecture Onboarding

- **Component map:** Sinkhorn solver -> Proximal gradient descent -> Implicit differentiation -> Riemannian gradient descent
- **Critical path:**
  1. Compute optimal transport coupling using Sinkhorn algorithm.
  2. Compute h-transform using proximal gradient descent.
  3. Compute gradient of learning objective using implicit differentiation.
  4. Update parameters using Riemannian gradient descent.
- **Design tradeoffs:**
  - Computational efficiency vs. statistical accuracy: Using entropic regularization makes the problem more tractable but introduces bias.
  - Flexibility vs. interpretability: Learning the regularizer parameters allows for more flexible cost structures but may make the resulting maps harder to interpret.
- **Failure signatures:**
  - Sinkhorn algorithm does not converge: May indicate ill-conditioned cost matrix or insufficient regularization.
  - Proximal gradient descent does not converge: May indicate non-smooth potential function or inappropriate step size.
  - Implicit differentiation fails: May indicate non-differentiability of Sinkhorn solution or violation of implicit function theorem conditions.
- **First 3 experiments:**
  1. Implement Sinkhorn algorithm for computing optimal transport couplings and verify convergence on simple test cases.
  2. Implement proximal gradient descent for computing h-transforms and compare with ground truth on synthetic examples.
  3. Implement implicit differentiation for computing gradients in bilevel optimization and verify correctness using finite differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the regularization parameter $\gamma$ impact the convergence rate of the MBO estimator for structured costs?
- Basis in paper: [inferred] The paper discusses the impact of $\gamma$ on the performance of the MBO estimator but does not provide a detailed analysis of its effect on the convergence rate.
- Why unresolved: The paper focuses on the theoretical aspects of the MBO estimator and its performance in experiments but does not delve into the specific impact of $\gamma$ on convergence rates.
- What evidence would resolve it: A detailed analysis showing the relationship between $\gamma$ and the convergence rate of the MBO estimator, possibly through additional experiments or theoretical derivations.

### Open Question 2
- Question: Can the MBO estimator be extended to handle non-convex regularizers, and if so, how does this affect its theoretical guarantees?
- Basis in paper: [explicit] The paper discusses the use of convex regularizers like the $\ell_1$ norm and orthogonal projection norms but does not explore non-convex regularizers.
- Why unresolved: The paper does not address the potential extension of the MBO estimator to non-convex regularizers, which could be relevant in certain applications.
- What evidence would resolve it: A theoretical framework or experimental results demonstrating the extension of the MBO estimator to non-convex regularizers, along with an analysis of how this affects its performance and guarantees.

### Open Question 3
- Question: How does the dimensionality of the subspace $p$ affect the sample complexity of the MBO estimator for subspace-structured costs?
- Basis in paper: [explicit] The paper mentions that the sample complexity depends on the subspace dimension $p$ but does not provide a detailed analysis of this relationship.
- Why unresolved: While the paper touches on the relationship between $p$ and sample complexity, it does not provide a comprehensive analysis or theoretical bounds.
- What evidence would resolve it: A detailed theoretical analysis or empirical study showing how the dimensionality of the subspace $p$ affects the sample complexity of the MBO estimator, possibly through additional experiments or mathematical derivations.

### Open Question 4
- Question: How does the MBO estimator perform in high-dimensional settings where the number of dimensions $d$ is much larger than the number of samples $n$?
- Basis in paper: [inferred] The paper discusses the performance of the MBO estimator in various settings but does not specifically address high-dimensional scenarios where $d \gg n$.
- Why unresolved: The paper does not provide insights into the performance of the MBO estimator in high-dimensional settings, which is a common scenario in many applications.
- What evidence would resolve it: Empirical results or theoretical analysis demonstrating the performance of the MBO estimator in high-dimensional settings, possibly with a focus on how it scales with $d$ and $n$.

### Open Question 5
- Question: Can the MBO estimator be adapted to handle dynamic or time-varying transport problems?
- Basis in paper: [inferred] The paper focuses on static optimal transport problems and does not explore dynamic or time-varying scenarios.
- Why unresolved: The paper does not address the potential extension of the MBO estimator to dynamic or time-varying transport problems, which could be relevant in applications like tracking or video analysis.
- What evidence would resolve it: A theoretical framework or experimental results showing how the MBO estimator can be adapted to handle dynamic or time-varying transport problems, along with an analysis of its performance in such scenarios.

## Limitations
- The theoretical guarantees for the MBO estimator rely on assumptions about the structure of the ground-truth transport map, which may not hold in all practical scenarios.
- The sample complexity bounds depend on constants that are not explicitly quantified, making it difficult to assess their tightness or practical implications.
- The learning framework requires solving a bilevel optimization problem, which can be computationally expensive and sensitive to hyperparameter choices.

## Confidence
- **High Confidence:** The mechanism of using proximal gradient descent to compute h-transforms for structured costs, given the explicit decomposition of the proximal operator and the availability of the proximal operator of the regularizer term.
- **Medium Confidence:** The learning framework for tuning the regularizer parameters through bilevel optimization, assuming the differentiability of the Sinkhorn solution and the validity of the implicit function theorem.
- **Medium Confidence:** The theoretical connection between subspace-structured costs and the spiked transport model in the limit as γ → ∞, based on the cited work on spiked transport models.

## Next Checks
1. **Convergence Verification:** Implement and test the proximal gradient descent algorithm for computing h-transforms on a variety of concave potential functions and elastic cost structures. Verify convergence by monitoring the objective function value and checking for oscillations or divergence.

2. **Learning Performance:** Evaluate the learning framework for subspace-structured costs on synthetic data with known low-dimensional transport maps. Assess the quality of the learned orthogonal basis by examining the squared-residual error when projecting ground-truth basis vectors onto the learned basis, and compare the performance with the standard ℓ2 cost.

3. **Sample Complexity Assessment:** Conduct experiments to empirically evaluate the sample complexity of the MBO estimator under subspace-structured costs. Generate synthetic data with varying sample sizes and subspace dimensions, and estimate the rate at which the MSE decreases with the sample size. Compare the empirical rates with the theoretical bounds provided in the paper.