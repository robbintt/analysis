---
ver: rpa2
title: 'Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels'
arxiv_id: '2303.16296'
source_url: https://arxiv.org/abs/2303.16296
tags:
- dice
- soft
- labels
- loss
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dice Semimetric Losses (DMLs), a new class
  of loss functions designed to optimize the Dice score in medical image segmentation
  while being fully compatible with soft labels. Unlike the standard Soft Dice Loss
  (SDL), which pushes predictions towards binary values when given soft labels, DMLs
  preserve soft label information and maintain better calibration.
---

# Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels

## Quick Facts
- arXiv ID: 2303.16296
- Source URL: https://arxiv.org/abs/2303.16296
- Reference count: 40
- Key outcome: DMLs outperform SDL with soft labels, achieving up to 2.5 Dice point gains and 0.2-0.3 ECE reduction

## Executive Summary
This paper introduces Dice Semimetric Losses (DMLs), a new class of loss functions designed to optimize the Dice score in medical image segmentation while being fully compatible with soft labels. Unlike the standard Soft Dice Loss (SDL), which pushes predictions towards binary values when given soft labels, DMLs preserve soft label information and maintain better calibration. The authors derive two variants of DMLs that are semimetrics and identical to SDL when using hard labels, making them safe to use as a drop-in replacement. Extensive experiments on three public medical imaging datasets (QUBIQ, LiTS, KiTS) demonstrate that DMLs outperform SDL when trained with soft labels from label smoothing, knowledge distillation, and label averaging. Models trained with DMLs achieve higher Dice scores and lower expected calibration error (ECE) compared to those trained with SDL.

## Method Summary
The paper proposes Dice Semimetric Losses (DMLs) as an alternative to Soft Dice Loss (SDL) that is compatible with soft labels while maintaining identical behavior with hard labels. DMLs are derived by transforming IoU-based losses that satisfy reflexivity and positivity into a Dice-compatible form, ensuring that minimizing the loss aligns with matching soft predictions to soft labels. The method involves replacing the denominator of SDL from (|x| + |y|) to (|x| + |y| - |x - y|) to preserve soft label information. The paper also explores knowledge distillation with kernel density estimation (KDE) for calibrated teacher signals. The training procedure uses a mixture of Cross-Entropy and DMLs weighted 0.25/0.75 with SGD optimizer and learning rate decay.

## Key Results
- DMLs achieve up to 2.5 Dice point improvements over SDL when using soft labels
- Models trained with DMLs show 0.2-0.3 reduction in Expected Calibration Error (ECE)
- DMLs maintain identical behavior to SDL when using hard labels, making them a safe drop-in replacement
- Knowledge distillation with KDE improves performance beyond simple soft label averaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DMLs preserve soft label information while maintaining identical behavior to SDL with hard labels.
- Mechanism: The Dice semimetric losses are derived by transforming IoU-based losses that satisfy reflexivity and positivity into a Dice-compatible form, ensuring that minimizing the loss aligns with matching soft predictions to soft labels.
- Core assumption: The set of conditions required for a semimetric (reflexivity, positivity, symmetry, and relaxed triangle inequality) are both sufficient for compatibility with soft labels and consistent with Dice optimization.
- Evidence anchors:
  - [abstract]: "Unlike the standard Soft Dice Loss (SDL), which pushes predictions towards binary values when given soft labels, DMLs preserve soft label information and maintain better calibration."
  - [section 2.2]: "DMLs have properties that are similar to JMLs... Theorem 2 indicates that we can safely replace the existing implementation of SDL with DMLs and no change will be incurred since they are identical when only hard labels are presented."
- Break condition: If the triangle inequality relaxation bound ρ is too large, the loss may not properly optimize Dice; if too small, it may fail to be a semimetric.

### Mechanism 2
- Claim: Training with soft labels improves both Dice score and model calibration compared to hard labels.
- Mechanism: Soft labels (from label averaging, smoothing, or distillation) provide richer supervisory signals that reduce overfitting and over-confidence, leading to more calibrated and accurate segmentations.
- Core assumption: The additional information in soft labels meaningfully reflects true uncertainty or inter-rater variability, and the model can learn from this signal without collapsing to hard targets.
- Evidence anchors:
  - [abstract]: "Models trained with DMLs achieve higher Dice scores and lower expected calibration error (ECE) compared to those trained with SDL, with improvements up to 2.5 Dice points and 0.2–0.3 ECE reduction."
  - [section 3.4]: "Generally, models trained with soft labels not only become more accurate, but also more calibrated. The rich multi-rater information can be captured by simply averaging their annotations."
- Break condition: If soft labels are poorly estimated or overly noisy, the regularization benefit may be lost or reversed.

### Mechanism 3
- Claim: Knowledge distillation from a calibrated teacher improves student performance beyond simple soft label averaging.
- Mechanism: A calibrated teacher provides a low-bias estimate of Bayes class-probabilities, and the kernel density estimator (KDE) approximates E[y|f(x)], which bounds the calibration error and improves student accuracy.
- Core assumption: The teacher's calibration error is sufficiently low for KDE to provide a useful distillation target; the KDE bandwidth h is appropriately tuned.
- Evidence anchors:
  - [section 3.5]: "Wang and Blaschko [47] empirically found that a calibrated teacher can distill a more accurate student... Inspired by this, we apply a recent kernel density estimator (KDE)... we adopt it as a post-hoc calibration method to replace the temperature scaling to calibrate the teacher."
  - [section D]: "Theorem 4 implies the bias of the estimation is bounded above by the calibration error. Thus, if the teacher's calibration error is reduced, it can provide a closer estimation of Bayes class-probabilities."
- Break condition: If the teacher is poorly calibrated or the KDE bandwidth is mis-set, distillation may degrade performance.

## Foundational Learning

- Concept: Semimetric spaces and their role in loss design.
  - Why needed here: DMLs are designed to be semimetrics so they can handle soft labels while still optimizing Dice; understanding semimetric properties explains why they differ from SDL.
  - Quick check question: What is the relaxed triangle inequality, and how does it differ from the standard triangle inequality in metric spaces?

- Concept: Expected Calibration Error (ECE) and its importance in segmentation.
  - Why needed here: The paper claims DMLs improve both Dice score and ECE; understanding ECE helps interpret calibration gains.
  - Quick check question: How is ECE computed for segmentation tasks, and why is it a useful complement to Dice score?

- Concept: Label smoothing and knowledge distillation.
  - Why needed here: These are the two main sources of soft labels evaluated in the experiments; knowing their mechanics is essential to understand the results.
  - Quick check question: How does label smoothing regularize model predictions, and what role does the temperature parameter play in knowledge distillation?

## Architecture Onboarding

- Component map: Predicted segmentation (soft) -> DML loss function -> loss value -> backpropagation
- Critical path:
  1. Forward pass: Compute DML loss between predictions and soft labels
  2. (Optional) For KD: Compute KDE-based soft target from teacher
  3. Backward pass: Gradient descent updates
- Design tradeoffs:
  - SDL vs DML: SDL collapses soft labels to hard; DML preserves them but is slightly more complex
  - KDE bandwidth: Higher bandwidth = smoother distillation, but may lose detail
  - Focal terms: Can focus loss on hard examples but add hyperparameters
- Failure signatures:
  - Loss values plateau at suboptimal Dice despite convergence
  - Calibration metrics (ECE) worsen when using soft labels with SDL
  - KD performance degrades if teacher is poorly calibrated or KDE bandwidth is wrong
- First 3 experiments:
  1. Replace SDL with DML in a baseline UNet; verify identical behavior with hard labels and improved results with soft labels
  2. Compare soft label averaging vs hard majority voting on QUBIQ; check Dice and ECE gains
  3. Apply KDE-based KD from a calibrated teacher; tune bandwidth and compare against temperature scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting scheme for averaging annotations in multi-rater scenarios beyond uniform and Dice-score based weighting?
- Basis in paper: [explicit] The paper mentions that weighted averaging with uniform weights obtains the highest BDice, while Dice-score based weighting achieves the highest Dice score, but suggests more sophisticated weighting schemes like learned weights could improve results.
- Why unresolved: The paper only tested two simple weighting schemes and did not explore more complex methods for determining optimal weights.
- What evidence would resolve it: Comparative experiments testing different weighting schemes (e.g., learned weights, uncertainty-based weighting) on the same multi-rater datasets to determine which achieves the best balance of Dice score and calibration.

### Open Question 2
- Question: How does the performance of Dice Semimetric Losses (DMLs) compare to other soft-label compatible loss functions like Jaccard Metric Losses (JMLs) in various segmentation tasks?
- Basis in paper: [explicit] The paper introduces DMLs as an alternative to SDL that is compatible with soft labels, but only compares DMLs to SDL, not to other soft-label compatible losses like JMLs.
- Why unresolved: The paper does not provide a direct comparison between DMLs and other loss functions that have been designed to work with soft labels.
- What evidence would resolve it: Head-to-head comparisons of DMLs, JMLs, and other soft-label compatible loss functions on the same datasets with soft labels to determine which performs best.

### Open Question 3
- Question: What is the impact of using different bandwidth parameters in the kernel density estimator (KDE) for knowledge distillation on the final model performance?
- Basis in paper: [explicit] The paper mentions that the performance of KDE is sensitive to the bandwidth parameter, but does not provide an in-depth analysis of how different bandwidths affect the results.
- Why unresolved: The paper only briefly mentions the sensitivity to bandwidth and does not explore the relationship between bandwidth choice and model performance.
- What evidence would resolve it: A systematic study varying the bandwidth parameter in KDE across a range of values and evaluating the impact on model performance metrics like Dice score and calibration error.

## Limitations

- The KDE-based knowledge distillation approach introduces additional complexity with the bandwidth parameter, and its sensitivity to hyperparameter tuning is not thoroughly explored
- The paper focuses primarily on binary segmentation tasks, leaving open questions about DMLs' effectiveness in multi-class settings
- While DMLs show superior performance with soft labels in controlled settings, it's unclear how these improvements translate to real-world clinical scenarios where soft labels may be noisier or less consistent

## Confidence

- High Confidence: The theoretical derivation of DMLs as semimetrics and their equivalence to SDL with hard labels is mathematically sound and well-supported by proofs
- Medium Confidence: The experimental results showing improved Dice scores and ECE with soft labels are convincing within the paper's controlled settings, but may not generalize to all medical imaging scenarios
- Medium Confidence: The claim that DMLs preserve soft label information while SDL pushes predictions toward binary values is supported by empirical evidence but could benefit from more ablation studies on different soft label distributions

## Next Checks

1. **Generalization to Multi-Class Segmentation**: Test DMLs on multi-class medical imaging datasets to verify if the theoretical advantages extend beyond binary segmentation
2. **Robustness to Noisy Soft Labels**: Evaluate DMLs' performance when soft labels contain varying degrees of noise or inconsistency, simulating real-world clinical label quality
3. **Ablation Study on KDE Bandwidth**: Conduct a comprehensive sensitivity analysis of the KDE bandwidth parameter in knowledge distillation to understand its impact on performance and identify optimal settings across different datasets