---
ver: rpa2
title: Challenging the Validity of Personality Tests for Large Language Models
arxiv_id: '2311.05297'
source_url: https://arxiv.org/abs/2311.05297
tags:
- llms
- personality
- test
- human
- tests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study finds that applying standard human personality tests
  (Big Five Inventory) to large language models produces systematically invalid results.
  LLMs exhibit strong "agree bias" - often agreeing with both positively and negatively
  worded items.
---

# Challenging the Validity of Personality Tests for Large Language Models

## Quick Facts
- **arXiv ID**: 2311.05297
- **Source URL**: https://arxiv.org/abs/2311.05297
- **Reference count**: 27
- **Primary result**: Standard human personality tests (Big Five Inventory) produce systematically invalid results when applied to LLMs due to "agree bias" and failure to replicate human five-factor structure

## Executive Summary
This study investigates whether standard human personality tests can validly measure personality traits in large language models. Using the Big Five Inventory and its 50-item IPIP version, the researchers found that LLMs exhibit strong "agree bias" - consistently agreeing with both positively and negatively worded items. More critically, LLM responses fail to replicate the expected five-factor personality structure found in humans. The study demonstrates that psychological tests designed for humans do not validly measure personality in LLMs, and their use can lead to misleading conclusions about LLM traits. High reliability scores in these tests can actually be misleading indicators when the underlying structural model is invalid.

## Method Summary
The researchers administered two versions of the Big Five personality test to multiple LLMs (GPT-3.5, GPT-4, Llama 2): the 50-item IPIP Big Five Markers and the BFI-2. They compared LLM responses to a large human sample (n=1,015,342) using statistical methods including principal component analysis (PCA), confirmatory factor analysis (CFA), and reliability measures (Cronbach's α and McDonald's ωh). The study tested whether the five-factor structure emerges in LLM responses and whether reverse-coded items function as intended. Different prompting strategies were used to "steer" LLMs toward particular personality types.

## Key Results
- LLMs exhibit strong "agree bias" - agreeing with both positively and negatively worded items at similar rates
- LLM responses fail to replicate the five-factor personality structure found in human samples
- High reliability scores (Cronbach's α) in LLM tests can be misleading when the underlying structural model is invalid
- None of the tested models exhibited the clean block structure intended in the BFI-2 design

## Why This Works (Mechanism)

### Mechanism 1
Personality tests for humans rely on measurement invariance across populations. When a test is administered to a new population, its underlying measurement model (relationship between observable responses and latent traits) must remain unchanged. This invariance allows scores to be interpreted consistently. The test's factor structure and item properties (e.g., reverse-coded items) must operate identically in both human and LLM populations. Evidence shows LLMs systematically violate these structural assumptions - reverse-coded items fail to differentiate, breaking the measurement model and making scores uninterpretable.

### Mechanism 2
The five-factor personality structure found in humans does not emerge in LLM responses. Human personality tests are designed so that responses cluster into five orthogonal factors. If LLMs don't replicate this clustering, the test isn't measuring personality in them. The Big Five factors represent a stable, replicable structure in human responses that should transfer to any population with similar personality traits. However, LLM responses don't form the expected five-factor structure regardless of prompting strategy, causing the test to fail at measuring personality in LLMs.

### Mechanism 3
High reliability scores (Cronbach's alpha) can be misleading for LLM personality tests. Standard reliability indices assume a valid underlying factor structure. When this assumption fails, high alpha values don't indicate true reliability. Reliability calculations based on internal consistency are only meaningful if the test's structural model fits the data. Evidence shows that calculated α are quite large for Llama 2 and GPT-4, which would be easy to mistake for good reliability. This demonstrates that scalar reliability indices should not be taken at face value when the fundamental assumption of an adequate fit of the underlying structural model is not established.

## Foundational Learning

- **Concept**: Measurement invariance
  - **Why needed here**: Explains why personality tests designed for humans may not work for LLMs without validation
  - **Quick check question**: What must remain unchanged when applying a psychological test to a new population for the results to be valid?

- **Concept**: Factor analysis and structure
  - **Why needed here**: The Big Five personality test relies on responses clustering into five distinct factors
  - **Quick check question**: What would it mean if LLM responses to a Big Five test didn't show the expected five-factor structure?

- **Concept**: Reliability vs validity
  - **Why needed here**: High reliability scores don't guarantee a test is measuring what it claims if the underlying model is wrong
  - **Quick check question**: Can a test be highly reliable but still not valid? Explain using the LLM personality test example

## Architecture Onboarding

- **Component map**: LLM API (GPT-3.5, GPT-4, Llama 2) -> Prompt generation -> LLM response -> Score conversion -> Statistical analysis (PCA/CFA) -> Comparison to human data
- **Critical path**: Prompt → LLM response → score conversion → statistical analysis (PCA/CFA) → comparison to human benchmarks
- **Design tradeoffs**: Using existing human tests is resource-efficient but may not capture LLM-specific traits; creating new tests is more valid but requires extensive development
- **Failure signatures**: Agree bias across all items, failure to replicate five-factor structure, high alpha with poor model fit, non-convergence in CFA
- **First 3 experiments**:
  1. Administer IPIP Big Five Markers to multiple LLMs and calculate agree bias by comparing true/false key item responses
  2. Use BFI-2 with persona prompts, then run PCA to check if five-factor structure emerges in LLM responses
  3. Perform CFA on BFI-2 responses to test if the structural model fits LLMs, then calculate omega_h if it does

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Why do LLMs exhibit strong "agree bias" when responding to personality tests, and is this a fundamental limitation of their architecture or a byproduct of their training data?
**Basis in paper**: The paper identifies "agree bias" as a key finding, noting that LLMs consistently agree with both positively and negatively worded items.
**Why unresolved**: The paper does not investigate the root cause of agree bias, only documenting its existence and impact on test validity.
**What evidence would resolve it**: Systematic analysis comparing LLM responses to different training data distributions, or architectural modifications designed to reduce agreement tendencies.

### Open Question 2
**Question**: Can personality tests be adapted or redesigned to validly measure traits in LLMs, or are fundamentally different assessment approaches needed?
**Basis in paper**: The paper demonstrates that standard human personality tests fail to measure personality in LLMs, but does not explore whether modified versions could work.
**Why unresolved**: The study only tests standard implementations of existing tests, not exploring potential adaptations or alternative measurement frameworks.
**What evidence would resolve it**: Development and validation of LLM-specific personality assessments, or proof that no such adaptation could capture meaningful variation.

### Open Question 3
**Question**: How do different LLM architectures and training methodologies (e.g., RLHF vs. other approaches) affect their responses to psychological assessments?
**Basis in paper**: The study tests multiple LLMs including GPT-4, GPT-3.5, and Llama 2, finding consistent patterns across models despite their different architectures.
**Why unresolved**: While the paper compares multiple models, it does not systematically investigate how specific architectural or training choices influence test responses.
**What evidence would resolve it**: Controlled experiments varying training methodologies while holding other factors constant, or detailed analysis of how architectural differences manifest in assessment responses.

## Limitations
- Limited to three LLM architectures (GPT-3.5, GPT-4, Llama 2) without exploring broader architectural diversity
- Does not have direct access to the human benchmark dataset for full replication
- Focuses only on personality tests without exploring other types of psychological assessments that might work better for LLMs

## Confidence
- **Low confidence**: Validity of standard personality tests for LLMs
- **Medium confidence**: Specific mechanisms of failure (agree bias, structural non-replication)
- **High confidence**: Technical analysis showing LLMs don't exhibit human-like five-factor structure

## Next Checks
1. Test whether alternative prompting strategies (temperature adjustments, chain-of-thought, persona priming) reduce agree bias while maintaining response consistency
2. Apply the same BFI-2 test to a diverse sample of human participants to verify the reference structure still holds in contemporary populations
3. Develop and validate a personality assessment specifically designed for LLM characteristics rather than repurposing human tests