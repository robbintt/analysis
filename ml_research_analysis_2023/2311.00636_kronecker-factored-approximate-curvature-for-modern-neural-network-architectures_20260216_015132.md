---
ver: rpa2
title: Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures
arxiv_id: '2311.00636'
source_url: https://arxiv.org/abs/2311.00636
tags:
- k-fac
- linear
- layer
- have
- weight-sharing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework to apply Kronecker-Factored Approximate
  Curvature (K-FAC) to modern neural network architectures with linear weight-sharing
  layers, such as transformers, convolutions, and graph neural networks. The authors
  identify two settings of linear weight-sharing layers that motivate two K-FAC variations
  - expand and reduce.
---

# Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures

## Quick Facts
- **arXiv ID**: 2311.00636
- **Source URL**: https://arxiv.org/abs/2311.00636
- **Reference count**: 40
- **Key outcome**: K-FAC-reduce is generally faster than K-FAC-expand for modern neural network architectures with weight-sharing layers, achieving 50-75% fewer training steps to reach fixed validation targets

## Executive Summary
This paper presents a framework for applying Kronecker-Factored Approximate Curvature (K-FAC) to modern neural network architectures that employ linear weight-sharing layers, such as transformers, convolutions, and graph neural networks. The authors identify two settings of linear weight-sharing layers—expand and reduce—that motivate two distinct K-FAC variations. They demonstrate that K-FAC-reduce is exact for deep linear networks with convolutions and average pooling, while K-FAC-expand is not. Empirically, both K-FAC variations achieve similar optimization performance, but K-FAC-reduce is generally faster due to lower computational complexity, making it more suitable for memory-constrained applications and automatic hyperparameter selection.

## Method Summary
The paper extends K-FAC to modern architectures by recognizing that layers like attention, convolution, and GNNs can be expressed as linear weight-sharing operations. The authors identify two settings: expand (where weight-sharing dimension is aggregated after per-example loss) and reduce (where it's aggregated before). For each setting, they derive appropriate K-FAC approximations—K-FAC-expand uses sum of Kronecker products, while K-FAC-reduce uses Kronecker product of sums. They prove theoretical exactness properties for deep linear networks and demonstrate empirically that both variations can reduce training steps by 50-75% compared to first-order methods while maintaining similar validation performance.

## Key Results
- K-FAC-reduce achieves 50-75% fewer training steps than first-order reference runs while maintaining similar validation performance
- K-FAC-reduce is generally faster than K-FAC-expand due to lower computational complexity (O(NP_{in}(P_{in}+R)) vs O(NRP^2_{in}))
- K-FAC-reduce enables faster automatic hyperparameter selection via marginal likelihood optimization
- Both K-FAC variations perform similarly on practical tasks despite different theoretical properties

## Why This Works (Mechanism)

### Mechanism 1
K-FAC-reduce is faster than K-FAC-expand due to lower computational and memory complexity. K-FAC-reduce uses a Kronecker product of sums approximation with scaling by 1/R, while K-FAC-expand uses a sum of Kronecker products approximation with scaling by 1/(NR). This leads to O(NRP^2_{in}) vs O(NP_{in}(P_{in}+R)) complexity for A_ℓ computation, making K-FAC-reduce more efficient when the weight-sharing dimension R is large.

### Mechanism 2
Both K-FAC variations reduce training steps by 50-75% compared to first-order methods by providing second-order curvature information that better conditions the optimization landscape. This allows faster convergence to the same validation metric target, with both variations performing similarly on practical tasks despite different theoretical exactness properties.

### Mechanism 3
K-FAC-reduce enables faster automatic hyperparameter selection via marginal likelihood optimization because its lower computational complexity reduces wall-clock time per marginal likelihood update step, making hyperparameter search more efficient compared to K-FAC-expand.

## Foundational Learning

- **Concept**: Kronecker-Factored Approximate Curvature (K-FAC)
  - Why needed here: K-FAC is the core optimization method being extended to modern architectures with weight-sharing layers
  - Quick check question: What is the computational complexity difference between exact Fisher approximation and K-FAC for a single layer?

- **Concept**: Linear weight-sharing layers
  - Why needed here: Understanding how attention, convolution, and GNN layers can be expressed as linear operations with shared weights
  - Quick check question: How does the forward pass of a 2D convolution relate to a linear weight-sharing layer?

- **Concept**: Expand vs Reduce settings
  - Why needed here: Determines which K-FAC variant (expand or reduce) is theoretically appropriate for a given architecture
  - Quick check question: In which setting is the weight-sharing dimension aggregated before the per-example loss?

## Architecture Onboarding

- **Component map**: Input layer -> Linear weight-sharing layers -> Aggregation functions -> Loss function -> K-FAC computation -> Gradient updates
- **Critical path**: 1) Forward pass through linear weight-sharing layers, 2) Compute Jacobians and loss gradients, 3) Calculate K-FAC approximation (expand or reduce), 4) Precondition gradient using K-FAC, 5) Update parameters
- **Design tradeoffs**:
  - K-FAC-expand: More general, exact for deep linear networks in expand setting, but higher computational cost
  - K-FAC-reduce: Lower computational cost, exact for deep linear networks in reduce setting, but may be inexact for attention mechanisms
  - Frequency of K-FAC updates: Balancing computational overhead vs optimization benefits
- **Failure signatures**:
  - Memory errors: K-FAC-expand may run out of memory with large batch sizes or sequence lengths
  - Slow training: K-FAC overhead dominates when data loading is fast or update frequency is high
  - Poor convergence: Inappropriate K-FAC variant for the architecture setting
- **First 3 experiments**:
  1. Implement K-FAC-expand on a simple transformer with sequence length 8, compare to standard AdamW
  2. Implement K-FAC-reduce on a CNN with average pooling, compare to K-FAC-expand
  3. Apply K-FAC to a GraphNetwork on ogbg-molpcba, measure step reduction vs reference optimizer

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of K-FAC-expand compare to K-FAC-reduce when applied to larger-scale models like GPT-3 or GPT-4? The paper suggests the discrepancy would be more pronounced for larger models based on timing experiments with nanoGPT, but the experiments are limited to smaller models.

### Open Question 2
What are the specific conditions under which K-FAC-reduce significantly outperforms K-FAC-expand in terms of wall-clock time? The paper indicates K-FAC-reduce has lower computational complexity but doesn't comprehensively analyze all possible scenarios and model architectures.

### Open Question 3
How does the choice of damping parameter affect the performance of K-FAC-expand and K-FAC-reduce? The paper mentions damping is typically used with K-FAC and provides examples of tuning, but doesn't explore the impact of different damping values on performance.

### Open Question 4
How does the frequency of K-FAC updates affect optimization performance and wall-clock time for both K-FAC-expand and K-FAC-reduce? The paper mentions overhead can be amortized by reducing update frequency but doesn't provide detailed analysis of this trade-off.

## Limitations
- Limited empirical validation across diverse architectures and tasks
- No ablation studies on K-FAC update frequency or damping parameters
- Theoretical analysis focuses on deep linear networks, which may not fully capture nonlinear behavior

## Confidence
- **High confidence**: Computational complexity analysis comparing K-FAC-expand vs K-FAC-reduce is rigorous and clearly presented
- **Medium confidence**: Empirical claims about step reduction and wall-clock time improvements are supported by experiments on three architecture types
- **Medium confidence**: Marginal likelihood optimization speedups from K-FAC-reduce are demonstrated on one architecture

## Next Checks
1. Verify the O(NRP²in) vs O(NPin(Pin+R)) complexity claims empirically by measuring actual runtime and memory usage across different values of the weight-sharing dimension R
2. Test K-FAC-expand and K-FAC-reduce on a broader range of architectures including RNNs with recurrence, attention mechanisms without weight-sharing, and hybrid architectures
3. Conduct detailed ablation studies on K-FAC damping parameters and update frequencies to understand their impact on convergence speed and final performance across different K-FAC variants