---
ver: rpa2
title: 'Towards Understanding Neural Collapse: The Effects of Batch Normalization
  and Weight Decay'
arxiv_id: '2309.04644'
source_url: https://arxiv.org/abs/2309.04644
tags:
- weight
- batch
- neural
- normalization
- cosine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of batch normalization (BN) and
  weight decay (WD) in the emergence of Neural Collapse (NC) in deep neural networks.
  NC is a geometric structure observed at the terminal phase of training, where features
  of the same class collapse to a single point, while features of different classes
  become equally separated.
---

# Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay

## Quick Facts
- **arXiv ID**: 2309.04644
- **Source URL**: https://arxiv.org/abs/2309.04644
- **Reference count**: 40
- **Primary result**: Batch normalization and weight decay are sufficient conditions for Neural Collapse emergence when regularized cross-entropy loss is near optimal

## Executive Summary
This paper investigates how batch normalization (BN) and weight decay (WD) contribute to Neural Collapse (NC) in deep neural networks. NC is characterized by feature vectors of the same class collapsing to a single point while features of different classes become equally separated. The authors propose a cosine similarity measure to quantify NC and provide theoretical guarantees under the layer-peeled model. They demonstrate that BN combined with WD forces features into a low-variance, high-symmetry configuration that drives the emergence of NC geometry. Empirically, they show that NC is most significant in models with BN and high WD values across various architectures and datasets.

## Method Summary
The authors train neural networks with and without batch normalization across various weight decay values, measuring intra-class and inter-class cosine similarities to quantify Neural Collapse. They use synthetic datasets (Conic Hull, MLP3, MLP6) and real-world datasets (MNIST, CIFAR-10, CIFAR-100) with models including MLPs and VGG architectures. The theoretical analysis is based on the layer-peeled model, where regularization of the last BN layer's gamma vector is shown to be equivalent to regularizing the quadratic average of feature norms. Training proceeds with gradient descent while recording cosine similarity metrics to analyze the impact of BN and WD on NC emergence.

## Key Results
- BN combined with WD forces features into the Neural Collapse geometry, characterized by class feature collapse and inter-class equiangular separation
- Cosine similarity serves as a direct geometric proxy for NC, with intra-class similarity → 1 and inter-class similarity → -1/(C-1) indicating collapse
- Proposition 2.1 establishes mathematical equivalence between γ regularization and feature norm control under specific BN placement conditions

## Why This Works (Mechanism)

### Mechanism 1
BN normalizes feature variance per layer while WD penalizes large weight norms. Together they restrict feature and weight spaces, pushing the cross-entropy loss surface toward a basin where class features concentrate at a single point per class and inter-class separation stabilizes. This works when the last-layer features are close to optimal so regularization dominates geometry.

### Mechanism 2
Cosine similarity measures directional alignment only, making it sensitive to class-mean concentration (NC1) and equiangular tightness (NC2) without being affected by norm drift. This provides a direct geometric proxy for NC when classes are balanced.

### Mechanism 3
In the layer-peeled model, regularization of the last BN layer's gamma vector is mathematically equivalent to regularizing the quadratic average of feature norms. WD on γ therefore penalizes large feature norms, tightening the feature distribution and enabling collapse when BN is immediately before the final linear layer.

## Foundational Learning

- **Jensen's inequality for strongly convex functions**: Used to bound deviations of subset means from the global mean, which underpins the cosine similarity bounds in the theorem. Quick check: If f is λ-strongly-convex and 1/N Σ f(xᵢ) ≤ f( x̄ ) + ε, what is the maximum possible deviation of the mean of any δ-fraction of the data from x̄?

- **Equiangular Tight Frame (ETF) geometry**: NC2 states that class means form an ETF; understanding this structure is necessary to interpret inter-class cosine similarity values. Quick check: For C class means in d dimensions forming an ETF, what is the exact value of inter-class cosine similarity between any two normalized means?

- **Cross-entropy loss landscape and regularization paths**: The theory relies on the interplay between loss minimization and norm penalties; knowing how WD and BN alter the loss surface is essential. Quick check: In a two-class softmax, what is the minimal achievable cross-entropy loss when the feature norms are constrained by L2 regularization?

## Architecture Onboarding

- **Component map**: Input → Conv/FC layers → BN layers (trainable γ) → Final linear classifier (trainable W) → Softmax → Loss

- **Critical path**: 
  1. Forward pass to extract last-layer features
  2. Compute BN-normalized features
  3. Compute intra/inter-class cosine similarities
  4. Update W and γ via gradient descent with WD

- **Design tradeoffs**: 
  - BN placement: immediate before final layer gives strongest theoretical guarantees; earlier BN layers require tracking of subsequent weight norms
  - WD magnitude: too low → no collapse; too high → underfitting, loss of near-optimal regime
  - Learning rate: must be low enough to allow fine-grained collapse; high LR can overshoot

- **Failure signatures**: 
  - Intra-class cosine similarity plateauing below 1 → insufficient regularization or non-optimal loss
  - Inter-class cosine similarity rising above -1/(C-1) at high WD → underfitting, loss of near-optimal regime
  - Vanishing gradients in early layers → learning rate too high or BN improperly scaled

- **First 3 experiments**: 
  1. Train a 3-layer MLP on a synthetic conic-hull dataset with BN before final layer; sweep WD from 0.0001 to 0.1; plot intra/inter cosine similarities
  2. Remove BN from step 1; repeat sweep; compare collapse magnitude
  3. Replace synthetic data with CIFAR-10; use VGG-11+BN vs VGG-11; sweep WD; verify cosine similarity trends match synthetic case

## Open Questions the Paper Calls Out

- **Does BN alone induce NC?**: The authors show BN and WD are both necessary but don't explicitly test whether BN alone is sufficient. An experiment training with BN but without WD would resolve this.

- **How does depth influence NC?**: The paper mentions experiments with different depths but lacks detailed analysis of depth's impact on NC. A comprehensive study comparing NC across varying depths would help.

- **Can theoretical guarantees extend to deeper layers?**: The analysis assumes BN and WD only on the final layer, which doesn't represent practical training. A theoretical framework incorporating earlier layers would extend the theory.

## Limitations
- Theoretical results rely heavily on the layer-peeled model, which abstracts away many practical considerations of deep network training
- The empirical results are limited to standard vision benchmarks without testing on imbalanced or noisy datasets
- The equivalence between γ regularization and feature norm control holds exactly only under specific BN placement conditions

## Confidence

- **High confidence**: Mathematical relationship between BN gamma regularization and feature norm control (Proposition 2.1), theoretical framework for cosine similarity as NC proxy under balanced classes
- **Medium confidence**: Empirical observations that BN and WD together promote NC across architectures under controlled conditions
- **Low confidence**: Claims about sufficiency of BN and WD for NC emergence in all practical scenarios, particularly for imbalanced datasets

## Next Checks
1. Test the cosine similarity metric on imbalanced datasets (varying class sizes) to verify robustness of the NC quantification
2. Implement experiments with BN layers placed at different depths in the network to determine sensitivity of NC emergence to BN positioning
3. Conduct ablation studies varying learning rate schedules while keeping WD constant to isolate the interaction between optimization dynamics and regularization effects on NC