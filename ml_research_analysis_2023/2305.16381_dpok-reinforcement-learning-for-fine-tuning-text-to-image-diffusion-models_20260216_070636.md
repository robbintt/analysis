---
ver: rpa2
title: 'DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models'
arxiv_id: '2305.16381'
source_url: https://arxiv.org/abs/2305.16381
tags:
- fine-tuning
- diffusion
- supervised
- regularization
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DPOK, an online reinforcement learning approach
  to fine-tune text-to-image diffusion models using human feedback rewards. It formulates
  diffusion sampling as a multi-step MDP and applies policy gradient to optimize the
  reward.
---

# DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2305.16381
- Source URL: https://arxiv.org/abs/2305.16381
- Reference count: 40
- Primary result: RL fine-tuning with human feedback rewards outperforms supervised fine-tuning on image-text alignment and quality

## Executive Summary
This paper proposes DPOK, an online reinforcement learning approach for fine-tuning text-to-image diffusion models using human feedback rewards. The method formulates diffusion sampling as a multi-step Markov Decision Process and applies policy gradient optimization to maximize reward. KL regularization with respect to the pre-trained model is incorporated to stabilize training and prevent reward over-optimization. The approach demonstrates superior performance compared to supervised fine-tuning on both single- and multi-prompt settings, while also mitigating common failure modes like oversaturation and reducing biases in pre-trained models.

## Method Summary
DPOK treats diffusion sampling as a multi-step MDP where each denoising step is a policy action. The method applies policy gradient to optimize the expected reward of generated images while incorporating KL regularization to maintain proximity to the pre-trained model. The fine-tuning is performed using LoRA adapters for parameter efficiency. The approach evaluates the reward model online during training on dynamically updated distributions, enabling better adaptation to the reward model's generalization behavior compared to supervised fine-tuning which evaluates on fixed pre-training data.

## Key Results
- DPOK outperforms supervised fine-tuning in both image-text alignment (measured by ImageReward) and image quality (measured by aesthetic scores)
- The method successfully mitigates failure modes like oversaturation in fine-tuned models
- RL fine-tuning with DPOK reduces biases present in pre-trained models while maintaining high reward and visual fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion sampling can be formulated as a multi-step Markov Decision Process where each denoising step is treated as a policy action, enabling direct policy gradient optimization of reward.
- Mechanism: By treating the denoising transitions pθ(xt-1|xt,z) as a policy in an MDP with horizon T, the expected reward of the final image can be maximized using standard policy gradient, with each timestep's action being the noise prediction.
- Core assumption: The continuity of pθ(px0:T|z) and ∇θpθ(px0:T|z) in θ, x0:T, and z holds, ensuring the validity of the policy gradient derivation.
- Evidence anchors:
  - [abstract] "We show that optimizing the expected reward of a diffusion model’s image output is equivalent to performing policy gradient on a multi-step diffusion model under certain regularity assumptions."
  - [section 4.1] "We obtain the following gradient of this objective function: Lemma 4.1 ... If pθ(px0:T|z)r(x0,z) and ∇θpθ(px0:T|z)r(x0,z) are continuous functions of θ, x0:T, and z, then we can write the gradient of the objective in (4) as..."
- Break condition: If the continuity assumptions fail (e.g., discontinuous noise schedules or reward functions), the gradient derivation is invalid and policy gradient optimization would not converge.

### Mechanism 2
- Claim: KL regularization with respect to the pre-trained model acts as an implicit reward that stabilizes online RL fine-tuning and prevents reward over-optimization.
- Mechanism: The KL divergence KL[pθ(xt-1|xt,z)||ppre(xt-1|xt,z)] is treated as an additional reward term with weight β, encouraging the fine-tuned model to stay close to the pre-trained model in distribution while optimizing the main reward.
- Core assumption: The KL divergence between output image distributions is upper-bounded by the sum of KL divergences at each diffusion step (Lemma 4.2), justifying the use of step-wise KL regularization.
- Evidence anchors:
  - [abstract] "Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning."
  - [section 4.1] "We incorporate the Kullback–Leibler (KL) divergence between the fine-tuned model and the pre-trained model as a regularizer... Lemma 4.2. Assume that ppre(px0:T|z) and pθ(px0:T|z) are both Markov chains starting from xT ~ N(0,I) given text prompt z..."
- Break condition: If the upper bound in Lemma 4.2 is too loose or the KL weight β is poorly chosen, the regularization may either be ineffective or overly constrain the model's ability to optimize the main reward.

### Mechanism 3
- Claim: Online RL fine-tuning evaluates the reward model on the dynamically updated distribution, leading to better reward optimization compared to supervised fine-tuning which evaluates on fixed pre-training data.
- Mechanism: In online RL, the model generates new samples at each update step, and the reward model is evaluated on these fresh samples, allowing the model to adapt to the reward model's generalization behavior. In supervised fine-tuning, the reward is only evaluated on the fixed dataset generated by the pre-trained model.
- Core assumption: The reward model generalizes well to samples from the fine-tuned distribution, not just the pre-trained distribution.
- Evidence anchors:
  - [abstract] "Crucially, online training allows evaluation of the reward model and conditional KL divergence beyond the (supervised) training dataset."
  - [section 4.3] "Online RL fine-tuning evaluates the reward model using the updated distribution, while supervised fine-tuning evaluates the reward on the fixed pre-training data distribution. As a consequence, our online RL optimization should derive greater benefit from the generalization ability of the reward model."
- Break condition: If the reward model's generalization is poor or its predictions are unstable on samples from the fine-tuned distribution, online evaluation may lead to noisy or misleading gradients.

## Foundational Learning

- Concept: Reinforcement Learning Policy Gradient
  - Why needed here: The core optimization of expected reward in diffusion sampling requires gradient-based methods that can handle the multi-step nature of the denoising process.
  - Quick check question: What is the policy gradient theorem and how does it apply to optimizing expected reward in a Markov Decision Process?

- Concept: KL Divergence and Regularization
  - Why needed here: KL regularization is used both as a theoretical justification for the method (bounding output distribution divergence) and as a practical tool to stabilize training by preventing the model from drifting too far from the pre-trained distribution.
  - Quick check question: How does KL divergence between two distributions relate to their mutual information, and why is this relevant for regularization?

- Concept: Diffusion Models and Denoising
  - Why needed here: The entire approach is built on the diffusion model framework, where the forward noising process and reverse denoising process are key components that enable the MDP formulation.
  - Quick check question: What is the role of the variance schedule βt in the forward noising process, and how does it affect the reverse denoising process?

## Architecture Onboarding

- Component map:
  Pre-trained text-to-image diffusion model (Stable Diffusion v1.5) -> LoRA adapter -> Reward model (ImageReward) -> KL regularizer -> Policy gradient optimizer

- Critical path:
  1. Sample text prompt from distribution
  2. Generate image latents through diffusion process using current model
  3. Compute reward from reward model on final image
  4. Compute KL divergence at each diffusion step
  5. Calculate policy gradient from reward and KL terms
  6. Update LoRA weights using AdamW optimizer
  7. Repeat until convergence

- Design tradeoffs:
  - LoRA vs full fine-tuning: LoRA is more parameter-efficient but may have lower capacity
  - KL weight β: Higher values stabilize training but may limit reward optimization
  - Reward weight α: Higher values prioritize reward but risk over-optimization
  - Sampling batch size m: Larger batches provide better gradient estimates but increase compute

- Failure signatures:
  - Over-saturated or non-photorealistic images: likely under-regularization (low β)
  - Poor reward improvement: likely insufficient exploration or poor reward model generalization
  - Training instability: likely too high learning rate or improper KL weight

- First 3 experiments:
  1. Run with only reward term (α > 0, β = 0) to observe reward optimization without stabilization
  2. Run with only KL term (α = 0, β > 0) to verify regularization effect without reward influence
  3. Run with small α and β values to find stable operating point before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the efficiency of fine-tuning text-to-image models on multiple prompts simultaneously?
- Basis in paper: [explicit] The paper discusses challenges in multi-prompt training, noting that fine-tuning on multiple prompts requires longer training time, hyperparameter tuning, and engineering efforts.
- Why unresolved: The paper identifies the need for more efficient training methods but does not provide specific solutions or techniques to address this challenge.
- What evidence would resolve it: Empirical results demonstrating a new training method or algorithm that significantly reduces training time and resource usage while maintaining or improving performance on multiple prompts.

### Open Question 2
- Question: What are the potential benefits and drawbacks of incorporating advanced policy gradient methods in the fine-tuning process?
- Basis in paper: [inferred] The paper mentions exploring advanced policy gradient methods as a potential future direction for performance improvement, but does not provide detailed analysis or results.
- Why unresolved: The paper acknowledges the possibility of using advanced policy gradient methods but does not explore or evaluate their impact on the fine-tuning process.
- What evidence would resolve it: Comparative studies showing the performance of different policy gradient methods, including traditional and advanced approaches, on text-to-image fine-tuning tasks.

### Open Question 3
- Question: How can we systematically evaluate and mitigate biases in text-to-image models beyond using reward models like ImageReward?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of RL fine-tuning in reducing bias using ImageReward but suggests that thorough documentation of biases in publicly available reward models is critical.
- Why unresolved: While the paper shows success in bias reduction with ImageReward, it does not explore other methods or comprehensive strategies for bias evaluation and mitigation.
- What evidence would resolve it: Development and validation of additional bias evaluation frameworks and techniques that can be integrated into the fine-tuning process, along with empirical results showing their effectiveness.

## Limitations

- The analysis assumes continuity conditions that may not hold in practice, particularly for complex reward functions or non-standard noise schedules
- The paper does not provide extensive hyperparameter sensitivity analysis for the KL weight β and reward weight α
- Limited discussion of computational overhead compared to supervised fine-tuning methods

## Confidence

- High confidence: Core mechanism of treating diffusion sampling as multi-step MDP and applying policy gradient optimization
- High confidence: Mathematical framework for integrating KL regularization as implicit reward term
- Medium confidence: Empirical claims about performance improvements and online evaluation benefits

## Next Checks

1. Conduct ablation studies varying the KL regularization weight β to quantify its impact on both reward optimization and image quality preservation
2. Test the method on out-of-distribution prompts not seen during fine-tuning to evaluate reward model generalization
3. Compare training stability and convergence speed with alternative RL fine-tuning approaches mentioned in related work section