---
ver: rpa2
title: Do Language Models Know When They're Hallucinating References?
arxiv_id: '2305.18248'
source_url: https://arxiv.org/abs/2305.18248
tags:
- match
- exact
- references
- hallucination
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether language models (LMs) can detect their
  own hallucinated references. Hallucinations are defined as fabricated text not grounded
  in the LM's training data.
---

# Do Language Models Know When They're Hallucinating References?

## Quick Facts
- arXiv ID: 2305.18248
- Source URL: https://arxiv.org/abs/2305.18248
- Reference count: 40
- Key outcome: Indirect queries asking for ancillary details like authors can partially detect LM hallucinations, with ensemble methods improving accuracy

## Executive Summary
This paper investigates whether language models can detect their own hallucinated references by using indirect queries that ask for ancillary details rather than direct yes/no questions. The authors find that indirect queries asking for author lists show consistent results for real references but inconsistent results for hallucinated ones across multiple sessions. GPT-4 particularly demonstrates this pattern, generating different author lists for hallucinated references in independent sessions while maintaining consistency for real references. The study suggests that hallucinations stem more from generation techniques than from the underlying LM representation.

## Method Summary
The method generates candidate reference titles using language models (GPT-4, ChatGPT, Davinci) for 200 randomly sampled ACM topics, producing 5 titles per topic. For each title, the approach runs both direct queries (binary yes/no questions about reference existence) and indirect queries (asking for author details across 3 independent sessions with 10 samples each). Results are compared for consistency, and ground truth labels are established using Bing search API. The study evaluates performance using ROC curves and AUC metrics, finding that ensemble methods combining direct and indirect queries improve detection accuracy.

## Key Results
- GPT-4 generates inconsistent author lists for hallucinated references across independent sessions but consistent lists for real references
- Indirect queries (AUC: 0.878) outperform direct queries for GPT-4 hallucination detection
- Ensemble methods combining direct and indirect queries achieve the best performance across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models encode partial knowledge of reference titles and their associated authors, allowing indirect queries to detect inconsistencies.
- Mechanism: When an LM generates a hallucinated reference, the generated author list is based on patterns and correlations in the training data rather than specific grounding. This leads to inconsistent author lists across independent sessions for hallucinated references. Conversely, for real references, the LM retrieves consistent author lists because they are grounded in the training data.
- Core assumption: The LM's internal representations contain sufficient information about real references (authors, titles) to produce consistent outputs when queried indirectly.
- Evidence anchors:
  - [abstract] "We posit that if a language model cites a particular reference in its output, then it should ideally possess sufficient information about its authors and content"
  - [section 4.3] "GPT-4 hallucinated the title Introduction to Operations Research and Decision Making, but there is a real book called Introduction to Operations Research. In all three indirect queries, it hallucinated the authors of the existing book, Hillier Frederick S., Lieberman Gerald J."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.409" - indicates some related work on LM knowledge detection, though not directly on indirect queries.
- Break condition: If the LM's training data does not contain sufficient information about the referenced work, indirect queries will not be reliable. This could occur for very new or obscure references not present in the training corpus.

### Mechanism 2
- Claim: Direct queries (asking "Is this reference real?") are less reliable than indirect queries for detecting hallucinations because LMs can be biased towards affirming the existence of a reference.
- Mechanism: Direct queries are binary and may be influenced by the LM's tendency to be agreeable or by the phrasing of the prompt. Indirect queries, by asking for ancillary details like authors, force the LM to generate more specific information, and inconsistencies across multiple queries indicate hallucination.
- Core assumption: LMs are biased in answering direct existence questions, especially when the reference is plausible-sounding.
- Evidence anchors:
  - [abstract] "Consistency checks done with direct queries about whether the generated reference title is real... are compared to consistency checks with indirect queries which ask for ancillary details such as the authors of the work."
  - [section 4.2] "For the Davinci and ChatGPT models, the IQ procedure performs best as quantified via the area under the ROC curve (AUC). For GPT-4... both the IQ and DQ approaches work well... with the IQ (AUC: 0.878) and DQ1 (AUC: 0.887) performing the best."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.409" - suggests some research on direct query methods, but indirect queries appear novel.
- Break condition: If the indirect query prompt is not specific enough, or if the LM can generate consistent but fabricated author lists for hallucinated references, the method will fail.

### Mechanism 3
- Claim: Ensemble methods combining direct and indirect queries improve hallucination detection accuracy.
- Mechanism: Different query types capture different aspects of the LM's knowledge. Direct queries assess existence, while indirect queries assess consistency of ancillary details. Combining them leverages the strengths of each approach.
- Core assumption: The errors made by direct and indirect queries are not perfectly correlated, so combining them reduces overall error.
- Evidence anchors:
  - [section 4.2] "We find that classification performance increases when we take ensemble of different approaches... The ensemble of IQ and DQ (computed using the 50-50 mean of IQ and the DQ mean), referred to as IQ+DQ performs the best for every model."
  - [section 3.3] "For creating the ensemble of the approaches, we simply compute the mean of the scores and use them as thresholds."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.409" - indicates some related work on ensemble methods for LM confidence estimation.
- Break condition: If the direct and indirect queries are highly correlated in their errors, the ensemble will not provide significant improvement.

## Foundational Learning

- Concept: Groundedness vs. Correctness
  - Why needed here: The paper distinguishes between whether a reference is grounded in the training data (existence) and whether it is factually correct. This distinction is crucial for understanding the nature of hallucinations.
  - Quick check question: If an LM generates a reference to a real paper but cites the wrong authors, is this a hallucination? (Answer: Yes, because it's not grounded in the training data, even if the paper exists.)

- Concept: Consistency Checks
  - Why needed here: Both direct and indirect queries rely on consistency checks. Direct queries check consistency across multiple binary judgments, while indirect queries check consistency across multiple author lists.
  - Quick check question: If an LM generates the same author list for a reference across 5 independent queries, does this guarantee the reference is real? (Answer: No, it suggests high probability but is not a guarantee, as the LM could have learned a consistent pattern for a common hallucination.)

- Concept: ROC Curves and AUC
  - Why needed here: The paper uses ROC curves and AUC to evaluate the performance of different hallucination detection methods. Understanding these metrics is essential for interpreting the results.
  - Quick check question: What does an AUC of 0.9 mean for a hallucination detection method? (Answer: It means the method has high accuracy in distinguishing between grounded and hallucinated references.)

## Architecture Onboarding

- Component map: Data Generation -> Direct Query Module -> Indirect Query Module -> Ensemble Module -> Labeling Module

- Critical path:
  1. Generate candidate reference titles.
  2. For each title, run direct and indirect queries.
  3. Compute scores and ensemble if applicable.
  4. Compare scores to threshold to classify as grounded or hallucinated.
  5. Evaluate against ground truth labels.

- Design tradeoffs:
  - Number of queries (j for direct, i for indirect): More queries increase accuracy but also cost and latency.
  - Prompt wording: Different phrasings can significantly affect LM responses.
  - Threshold selection: Balancing precision and recall based on application needs.

- Failure signatures:
  - High false positive rate: Method incorrectly flags grounded references as hallucinated. Could be due to overly strict thresholds or biased prompts.
  - High false negative rate: Method fails to detect hallucinated references. Could be due to insufficient queries or inconsistent indirect query results for real references.
  - Inconsistent results across runs: Could indicate sensitivity to prompt wording or random sampling.

- First 3 experiments:
  1. Run direct queries (j=1) on a small set of generated references and compare to ground truth.
  2. Run indirect queries (i=1) on the same set and compare performance to direct queries.
  3. Run ensemble of direct and indirect queries and evaluate improvement in accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the effectiveness of indirect queries compare across different types of hallucinations beyond reference citations?
- Basis in paper: [explicit] The authors note that "it would be very interesting to see if the methods we employ could be used to identify other types of open-domain hallucinations beyond references" and discuss how "one way to aim to discover general hallucinations would be to query the LM for 'notable, distinguishing details' about the item in question."
- Why unresolved: The study only examined hallucinated references. Other types of hallucinations may have different characteristics that affect the performance of indirect queries.
- What evidence would resolve it: Empirical studies applying indirect queries to various hallucination types (e.g., factual claims, code generation, creative writing) with comparative performance metrics.

### Open Question 2
- Question: What is the optimal balance between direct and indirect queries for hallucination detection?
- Basis in paper: [explicit] The authors found that "combining multiple methods led to further improvements in accuracy" and discuss ensemble approaches, but note this was preliminary.
- Why unresolved: The study only tested simple averaging of scores. The optimal weighting, selection of query types, and combination strategies remain unexplored.
- What evidence would resolve it: Systematic experiments testing various ensemble configurations, query selection strategies, and weighting schemes across different LM models and hallucination types.

### Open Question 3
- Question: How do prompt engineering and contextual framing affect the reliability of direct and indirect hallucination detection?
- Basis in paper: [explicit] The authors note that "LMs are notoriously sensitive to prompt wording" and that "some of our findings comparing direct and indirect queries may be sensitive to the specific wording in the prompt."
- Why unresolved: The study used a limited set of prompts. The sensitivity to prompt variations could significantly impact detection reliability.
- What evidence would resolve it: Systematic prompt ablation studies testing variations in wording, context, and framing across different hallucination detection approaches and LM models.

## Limitations
- The method relies heavily on Bing search API for ground truth labeling, which may miss real references due to search limitations or formatting differences
- The approach only evaluates English-language references and may not generalize to other languages or domains with different citation conventions
- Prompt templates used for queries are shortened in the paper, and subtle wording changes could significantly impact results

## Confidence
- High Confidence: GPT-4 shows inconsistent author generations across independent sessions for hallucinated references while maintaining consistency for real references
- Medium Confidence: Indirect queries generally outperform direct queries for hallucination detection, though superiority varies across models
- Medium Confidence: Hallucination stems more from generation techniques than LM representation, though alternative explanations are not fully ruled out

## Next Checks
1. **Prompt Sensitivity Analysis:** Systematically vary the wording of direct and indirect query prompts across 5-10 variants to quantify how sensitive detection performance is to prompt engineering, particularly for indirect queries where prompt specificity is crucial.

2. **Cross-Lingual Validation:** Apply the same indirect query methodology to non-English reference titles generated by multilingual LMs to test whether the consistency detection mechanism generalizes across languages and cultural citation norms.

3. **Temporal Stability Test:** Generate reference titles and run indirect queries across multiple time periods (e.g., monthly intervals) to determine if the consistency patterns persist over time or degrade as LM training data ages.