---
ver: rpa2
title: 'BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization'
arxiv_id: '2305.18666'
source_url: https://arxiv.org/abs/2305.18666
tags:
- step
- learning
- size
- upper
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of tuning two correlated learning\
  \ rates (\u03B1, \u03B2) in gradient-based bi-level optimization. The authors propose\
  \ variants of stochastic Polyak step size (SPSB) and stochastic line search (SLSB)\
  \ for single-level optimization, which converge without requiring interpolation\
  \ or monotonicity of the step size."
---

# BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization

## Quick Facts
- **arXiv ID**: 2305.18666
- **Source URL**: https://arxiv.org/abs/2305.18666
- **Reference count**: 40
- **Key outcome**: BiSLS-Adam/SGD converges faster than fine-tuned Adam/SGD for bi-level optimization with minimal tuning, robust to search starting points.

## Executive Summary
This paper addresses the challenge of tuning two correlated learning rates in gradient-based bi-level optimization. The authors propose variants of stochastic Polyak step size (SPSB) and stochastic line search (SLSB) that converge in non-interpolating settings without requiring monotonic step sizes. They unify these under an envelope-type step-size framework and extend the analysis to bi-level optimization, designing SPS-based (BiSPS) and line-search-based (BiSLS-Adam/SGD) algorithms. The proposed methods demonstrate faster convergence and better generalization than standard methods requiring fine-tuning, while being highly robust to initialization.

## Method Summary
The authors develop single-level algorithms (SPSB, SLSB) using an envelope-type step-size structure that allows non-monotonic updates while maintaining convergence. For bi-level optimization, they extend this framework to create BiSPS, which uses stochastic hypergradient estimates with envelope step sizes at both levels. They also propose BiSLS-Adam/SGD, which employs a stochastic Armijo line-search condition at the upper level with periodic resets to larger values to enable exploration of larger learning rates. Both approaches achieve faster convergence than fine-tuned baselines while requiring minimal parameter tuning.

## Key Results
- BiSPS achieves convergence rates matching SGD (up to logarithmic factors) for bi-level optimization with envelope-type step sizes
- BiSLS-Adam/SGD finds large learning rates with minimal tuning and demonstrates superior convergence compared to fine-tuned Adam/SGD
- The algorithms show high robustness to search starting point initialization across various bi-level problems
- Empirical results demonstrate better generalization performance on hyper-representation learning and data distillation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SPSB and SLSB converge in non-interpolating settings without requiring monotonicity of the step size.
- **Mechanism**: Both algorithms use an envelope-type step-size structure: γk = min{max{γl,k, ˜γk}, γb,k}, where γb,k is a non-increasing upper bound. This allows large initial steps for fast convergence while regularizing with γb,k to prevent instability.
- **Core assumption**: The step-size upper bound γb,k must be non-increasing to ensure convergence.
- **Evidence anchors**:
  - [abstract] "We propose simple variants of SLS and SPS that converge in the non-interpolating setting while not requiring the step size to be monotonic."
  - [section] "Unlike DecSPS in which both the step size and the upper bound are non-increasing... we simplify the recursive structure and do not require the step-size to be monotonic."
- **Break condition**: If γb,k is not non-increasing, the regularization effect is lost and the algorithm may diverge.

### Mechanism 2
- **Claim**: BiSPS achieves the same convergence rate as SGD for bi-level optimization by using envelope-type step sizes.
- **Mechanism**: BiSPS applies the envelope-type step-size framework to both upper and lower-level objectives. The upper-level uses a stochastic hypergradient estimate with a non-increasing upper bound αb,k, while the lower-level uses SPSmax with a non-increasing bound βb,k.
- **Core assumption**: The bias in the stochastic hypergradient estimate must decay exponentially with the number of samples N.
- **Evidence anchors**:
  - [abstract] "This unified envelope strategy allows for the extension of the algorithms and their convergence guarantees to BO settings."
  - [section] "Based on step size of the form (10) and (11)... BiSPS achieves the rate: 1/K Σ E[∥∇F (xk)∥²] ≤ ˜O(κ³/√K + κ² log K/√K)."
- **Break condition**: If the bias in the hypergradient estimate does not decay exponentially, the convergence guarantee may not hold.

### Mechanism 3
- **Claim**: BiSLS-Adam/SGD finds large learning rates with minimal tuning and is robust to initialization.
- **Mechanism**: BiSLS uses a stochastic Armijo line-search condition that allows the learning rate to grow until a sufficient decrease condition is violated. The algorithm resets the search starting point to larger values periodically to avoid always starting from a small initial value.
- **Core assumption**: The line-search condition can be satisfied with a non-zero slack δ to account for stochastic approximation errors.
- **Evidence anchors**:
  - [abstract] "The new algorithms... can find large learning rates with minimal tuning and converge faster than corresponding vanilla SGD or Adam BO algorithms that require fine-tuning."
  - [section] "The key algorithmic challenge we are facing is that during the backtracking process... we need to compute ˆxk := xk − αkhk f and approximate y∗(ˆxk) with ˆyk+1."
- **Break condition**: If δ is set too small, the line-search may fail to find a feasible step size due to approximation errors.

## Foundational Learning

- **Concept**: Stochastic Polyak step size (SPS)
  - Why needed here: SPS is the foundation for SPSB and BiSPS, providing adaptive step sizes based on gradient information.
  - Quick check question: How does SPS differ from standard SGD in terms of step-size adaptation?

- **Concept**: Stochastic line search (SLS)
  - Why needed here: SLS is the foundation for SLSB and BiSLS, providing adaptive step sizes based on sufficient decrease conditions.
  - Quick check question: What is the key difference between SLS and standard backtracking line search?

- **Concept**: Bi-level optimization
  - Why needed here: The paper extends single-level optimization algorithms to bi-level optimization, which involves two coupled learning rates.
  - Quick check question: How does the bi-level optimization problem differ from standard optimization in terms of gradient computation?

## Architecture Onboarding

- **Component map**: Single-level algorithms (SPSB, SLSB) → Bi-level algorithms (BiSPS, BiSLS-Adam/SGD). Each algorithm has components for step-size computation, gradient estimation, and line-search (for SLS variants).
- **Critical path**: For BiSLS-Adam/SGD, the critical path is: compute stochastic hypergradient → perform line-search to find learning rate → update variables x and y.
- **Design tradeoffs**: The tradeoff between exploration (finding large learning rates) and exploitation (staying within a stable range) is managed by the envelope-type step-size structure and the line-search condition with slack δ.
- **Failure signatures**: If the learning rates grow too large or too small, the algorithm may diverge or converge too slowly. If the line-search fails to find a feasible step size, the algorithm may get stuck.
- **First 3 experiments**:
  1. Implement SPSB on a simple convex problem (e.g., logistic regression) and compare its convergence to standard SGD.
  2. Implement BiSPS on a bi-level problem (e.g., hyperparameter optimization) and compare its convergence to standard bi-level optimization methods.
  3. Implement BiSLS-Adam on a bi-level problem and investigate its sensitivity to the initialization of the search starting points αb,0 and βb,0.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the envelope-type step size framework for bi-level optimization (BiSPS) provide provable convergence guarantees for non-convex upper-level objectives?
- Basis in paper: [explicit] The paper states that BiSPS matches the optimal rate of SGD up to a logarithmic factor for convex upper-level objectives (Theorem 3). However, it does not explicitly address the non-convex case.
- Why unresolved: The analysis relies on the convexity of the upper-level objective function, which is a simplifying assumption not always satisfied in practice.
- What evidence would resolve it: Extending the convergence analysis of BiSPS to non-convex upper-level objectives, either through a new theoretical proof or by providing strong empirical evidence of convergence in such settings.

### Open Question 2
- Question: How sensitive are the BiSLS algorithms to the choice of the initial search starting points (αb,0 and βb,0) in high-dimensional problems?
- Basis in paper: [explicit] The paper shows that BiSLS-Adam/SGD is highly robust to different search starting points (αb,0 and βb,0) in the experiments. However, the sensitivity analysis is limited to relatively low-dimensional problems.
- Why unresolved: The robustness of the algorithms to initialization may not generalize to high-dimensional settings where the search space is more complex.
- What evidence would resolve it: Conducting experiments with BiSLS-Adam/SGD on high-dimensional problems and analyzing the impact of different initial search starting points on convergence and performance.

### Open Question 3
- Question: Can the envelope-type step size framework be extended to other optimization algorithms beyond SGD and Adam, such as momentum-based methods or adaptive gradient methods?
- Basis in paper: [inferred] The paper focuses on extending the envelope-type step size framework to SGD and Adam for bi-level optimization. However, it does not explore its applicability to other popular optimization algorithms.
- Why unresolved: The effectiveness of the envelope-type step size framework may depend on the specific characteristics of the optimization algorithm it is applied to.
- What evidence would resolve it: Adapting the envelope-type step size framework to other optimization algorithms and evaluating their performance on bi-level optimization problems.

## Limitations

- The convergence guarantees for BiSLS-Adam/SGD rely on a modified Armijo condition with slack δ, but the sensitivity to δ selection is not fully characterized and may require practical tuning.
- The extension from single-level to bi-level optimization introduces additional complexity through the Neumann series approximation for hypergradients, with exponential decay assumptions that may not hold for all problem classes.
- The envelope-type step-size framework requires careful tuning of non-increasing upper bounds (αb,k, βb,k), and extreme initialization choices could still impact performance despite the framework's design for robustness.

## Confidence

- **High confidence**: The single-level SPSB and SLSB algorithms are well-grounded in existing stochastic optimization theory, with clear convergence proofs for non-interpolating settings.
- **Medium confidence**: The bi-level extension through BiSPS follows logically from single-level results but introduces additional assumptions about hypergradient estimation that require validation.
- **Medium confidence**: BiSLS-Adam/SGD shows promising empirical results, but theoretical analysis depends on Armijo condition modification and stochastic approximation quality which may vary across problems.

## Next Checks

1. **Step-size sensitivity analysis**: Systematically vary the initialization and decay rates of αb,0 and βb,0 in BiSLS-Adam/SGD to quantify the impact on convergence speed and final accuracy across different bi-level problems.

2. **Bias decay verification**: Empirically verify the exponential decay of hypergradient bias in BiSPS by measuring the error between the Neumann series approximation and exact hypergradients as a function of sample size N.

3. **Comparison to baseline initialization**: Compare BiSLS-Adam/SGD's performance when using the periodic reset strategy (option 3) versus always starting from the last feasible step size to quantify the benefit of aggressive exploration.