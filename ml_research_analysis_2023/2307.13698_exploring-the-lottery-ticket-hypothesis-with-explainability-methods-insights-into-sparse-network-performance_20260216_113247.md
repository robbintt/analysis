---
ver: rpa2
title: 'Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights
  into Sparse Network Performance'
arxiv_id: '2307.13698'
source_url: https://arxiv.org/abs/2307.13698
tags:
- color
- grey
- pattern
- black
- tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the Lottery Ticket Hypothesis (LTH) using
  explainability methods, specifically Grad-CAM for pixel-level and PCBM for concept-level
  explanations. LTH identifies sparse subnetworks within larger neural networks that
  achieve comparable performance when trained in isolation.
---

# Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance

## Quick Facts
- arXiv ID: 2307.13698
- Source URL: https://arxiv.org/abs/2307.13698
- Reference count: 40
- This work investigates the Lottery Ticket Hypothesis (LTH) using explainability methods to understand how sparse network performance relates to changes in pixel and concept-level explanations.

## Executive Summary
This paper investigates how iterative magnitude-based pruning, as described by the Lottery Ticket Hypothesis, affects both the performance and explainability of neural networks. Using Grad-CAM for pixel-level and PCBM for concept-level explanations, the authors analyze how pruning alters the network's reliance on specific concepts and pixels for classification. The study reveals that as pruning progresses, the discovered concepts and pixels become inconsistent with the original network, potentially explaining the performance degradation observed in pruned networks.

## Method Summary
The method combines LTH-based iterative pruning with two explainability techniques: Grad-CAM for pixel-level saliency mapping and PCBM for concept extraction from network embeddings. The authors apply this approach to vision and medical imaging datasets, specifically CUB-200 for bird species classification and HAM10000 for skin lesion classification. They use ResNet50 for CUB-200 and Inception-V3 for HAM10000, pruning iteratively (15 iterations, removing 10% weights each iteration) while analyzing changes in both performance metrics and explainability outputs.

## Key Results
- As pruning progresses, network performance degrades in both vision and medical imaging datasets
- The discovered concepts and pixels from pruned networks become inconsistent with the original network
- Magnitude-based iterative pruning alters the network's representation, leading to decreased reliance on relevant concepts and pixels for classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning removes weights based on magnitude, which may eliminate components critical for maintaining explainability in terms of both concepts and pixels.
- Mechanism: The Lottery Ticket Hypothesis (LTH) identifies sparse subnetworks by iteratively removing low-magnitude weights. However, these weights, while small, may still be important for preserving the network's reliance on relevant concepts and pixels for classification.
- Core assumption: Magnitude-based pruning effectively removes redundant parameters without affecting the network's core decision-making process.
- Evidence anchors:
  - [abstract] "As more weights are pruned, the performance of the network degrades. The discovered concepts and pixels from the pruned networks are inconsistent with the original network"
  - [section 3.1] "Tab. 3 and 2 demonstrate that different pruned networks rely on different concepts for classifying the same class labels"
  - [corpus] Weak evidence - related papers focus on pruning performance but not specifically on explainability degradation
- Break condition: If the network can be pruned without affecting concept and pixel relevance, this mechanism would not hold.

### Mechanism 2
- Claim: The carved interpretable models (PCBMs) from pruned networks rely on different concepts than the original network, leading to decreased performance.
- Mechanism: PCBM extracts human-interpretable concepts from network embeddings. As pruning removes weights, the embedding changes, causing the PCBM to identify different concepts as important for classification.
- Core assumption: The PCBM accurately captures the network's concept representation, and changes in this representation reflect changes in the network's decision-making process.
- Evidence anchors:
  - [abstract] "The discovered concepts and pixels from the pruned networks are inconsistent with the original network"
  - [section 3.1] "Tab. 3 and 2 reports the top-3 concepts for the different interpretable models g for each pruned networks"
  - [corpus] Weak evidence - related papers focus on LTH but not on PCBM concept extraction
- Break condition: If the PCBM consistently identifies the same concepts regardless of pruning level, this mechanism would not hold.

### Mechanism 3
- Claim: Grad-CAM heatmaps from pruned networks focus on different pixels than the original network, indicating a shift in the network's attention.
- Mechanism: Grad-CAM identifies important pixels by calculating gradients. As pruning removes weights, the gradient flow changes, causing the network to focus on different pixels for classification.
- Core assumption: Grad-CAM accurately captures the network's attention on pixels, and changes in this attention reflect changes in the network's decision-making process.
- Evidence anchors:
  - [abstract] "The discovered concepts and pixels from the pruned networks are inconsistent with the original network"
  - [section 3.1] "Fig. 8 shows similar inconsistencies in the Grad-CAM outputs for different networks"
  - [corpus] Weak evidence - related papers focus on LTH but not on Grad-CAM pixel attention
- Break condition: If Grad-CAM heatmaps remain consistent regardless of pruning level, this mechanism would not hold.

## Foundational Learning

- Concept: Neural network pruning and the Lottery Ticket Hypothesis
  - Why needed here: The paper investigates how LTH-based pruning affects network explainability
  - Quick check question: What is the main idea behind the Lottery Ticket Hypothesis?
- Concept: Model explainability methods (Grad-CAM and PCBM)
  - Why needed here: The paper uses these methods to analyze how pruning affects the network's decision-making process
  - Quick check question: How do Grad-CAM and PCBM differ in their approach to explaining neural networks?
- Concept: Concept-based vs. pixel-based explanations
  - Why needed here: The paper examines explainability at both the concept and pixel levels to provide a comprehensive understanding of how pruning affects the network
  - Quick check question: What is the difference between concept-based and pixel-based explanations in neural networks?

## Architecture Onboarding

- Component map: LTH pruning -> Network pruning -> Grad-CAM and PCBM analysis
- Critical path: The pruning process is the critical path, as it directly affects the performance of both the concept extractor and the pixel attention analyzer.
- Design tradeoffs: The main tradeoff is between network sparsity (achieved through pruning) and explainability (maintained through concept and pixel analysis).
- Failure signatures: Decreased performance in either the concept extractor or the pixel attention analyzer may indicate that the pruning process is removing critical components of the network.
- First 3 experiments:
  1. Apply LTH pruning to a small neural network and analyze the changes in concept importance using PCBM.
  2. Use Grad-CAM to visualize how pixel attention shifts as the network is pruned.
  3. Compare the performance of the pruned network with the original network in terms of both accuracy and explainability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do concept-based explanations change across different pruning iterations for medical imaging datasets compared to natural images?
- Basis in paper: [explicit] The paper analyzes concept-based explanations using PCBM on both CUB-200 (natural images) and HAM10000 (medical images), showing inconsistent concepts across pruning iterations.
- Why unresolved: The paper provides examples for both datasets but doesn't systematically compare how concept instability differs between medical and natural image domains.
- What evidence would resolve it: A comprehensive comparison of concept consistency across pruning iterations between medical imaging datasets (like HAM10000) and natural image datasets (like CUB-200), including statistical measures of concept stability.

### Open Question 2
- Question: Can incorporating Grad-CAM saliency maps as regularization during pruning improve concept consistency in Lottery Ticket Hypothesis?
- Basis in paper: [inferred] The paper observes that pruned networks with fewer weights highlight different pixels and concepts than the original network, suggesting potential for regularization.
- Why unresolved: The paper identifies the problem of inconsistent concepts/pixels but doesn't explore solutions to maintain consistency during pruning.
- What evidence would resolve it: Experimental results showing improved concept and pixel consistency in pruned networks when Grad-CAM-based regularization is applied during the pruning process.

### Open Question 3
- Question: How does the difficulty level of samples affect concept-based explanations in pruned networks identified through LTH?
- Basis in paper: [explicit] The paper mentions wanting to "rank the samples based on the difficulty" and investigate concept differences for "harder" samples in future work.
- Why unresolved: The paper identifies this as a future direction but doesn't provide any analysis of how sample difficulty correlates with concept consistency in pruned networks.
- What evidence would resolve it: Analysis showing how concept-based explanations vary between easy and hard samples across different pruning iterations, potentially revealing patterns in concept stability.

## Limitations

- The study relies on two specific explainability methods (Grad-CAM and PCBM) which may not capture all aspects of network decision-making
- The experimental scope is limited to two datasets and specific network architectures, which may not generalize to all vision tasks
- The paper demonstrates correlation between explainability changes and performance degradation but cannot definitively prove causation

## Confidence

- High confidence: Core finding that pruning affects network explainability at both pixel and concept levels
- Medium confidence: Claim that explainability changes correlate with performance degradation
- Low confidence: Specific mechanisms by which pruning alters concept extraction

## Next Checks

1. Replicate the study using different pruning strategies (e.g., structured vs unstructured, different pruning ratios) to isolate the effect of pruning methodology on explainability.
2. Apply additional explainability methods (e.g., LIME, SHAP) to verify that Grad-CAM and PCBM findings are consistent across different explanation approaches.
3. Test the findings on additional datasets and network architectures to assess generalizability of the observed patterns between pruning, performance, and explainability changes.