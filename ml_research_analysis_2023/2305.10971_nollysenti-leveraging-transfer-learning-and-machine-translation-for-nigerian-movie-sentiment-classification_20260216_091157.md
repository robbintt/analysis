---
ver: rpa2
title: 'NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian
  Movie Sentiment Classification'
arxiv_id: '2305.10971'
source_url: https://arxiv.org/abs/2305.10971
tags:
- languages
- sentiment
- language
- reviews
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of developing sentiment analysis\
  \ tools for under-represented African languages by creating NollySenti, a multilingual\
  \ sentiment dataset for five Nigerian languages (English, Hausa, Igbo, Nigerian-Pidgin,\
  \ and Yor\xF9b\xE1) based on Nollywood movie reviews. The authors compare transfer\
  \ learning approaches (cross-domain from Twitter and cross-lingual from English)\
  \ and leverage machine translation to improve sentiment classification performance."
---

# NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification

## Quick Facts
- arXiv ID: 2305.10971
- Source URL: https://arxiv.org/abs/2305.10971
- Reference count: 20
- Key outcome: Transfer learning from English movie reviews improves sentiment classification accuracy by 5%+ compared to Twitter transfer, and machine translation adds 7% improvement despite low BLEU scores.

## Executive Summary
This paper addresses the challenge of sentiment analysis for under-represented African languages by creating NollySenti, a multilingual sentiment dataset for five Nigerian languages based on Nollywood movie reviews. The authors demonstrate that cross-lingual transfer from English movie reviews outperforms cross-domain transfer from Twitter by more than 5% in accuracy. They further show that machine translation, despite low automatic quality scores, improves performance by 7% while preserving sentiment as confirmed by human evaluation. Africa-centric pre-trained language models (AfriBERTa, AfroXLMR) outperform massively multilingual models for these languages.

## Method Summary
The authors collect and annotate movie reviews in five Nigerian languages (English, Hausa, Igbo, Nigerian-Pidgin, and Yorùbá), creating the NollySenti dataset. They employ transfer learning approaches including cross-domain adaptation from Twitter and cross-lingual adaptation from English movie reviews. To improve performance on low-resource languages, they use machine translation (NLLB and MAFAND) to generate synthetic training data. The models are evaluated using accuracy metrics and human assessment of translation quality and sentiment preservation. Africa-centric pre-trained language models are fine-tuned for sentiment classification.

## Key Results
- Cross-lingual transfer from English movie reviews yields >5% accuracy improvement over cross-domain Twitter transfer
- Machine translation improves performance by 7% over cross-lingual evaluation alone
- Africa-centric PLMs (AfriBERTa, AfroXLMR) outperform massively multilingual models for these languages
- Human evaluation confirms high sentiment preservation in translated reviews despite low BLEU scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual transfer from English in the same target domain yields better performance than cross-domain transfer from Twitter.
- Mechanism: The model learns domain-specific linguistic patterns and sentiment cues in English movie reviews, which are more transferable to other languages' movie reviews than Twitter-style sentiment patterns.
- Core assumption: Movie review domains share similar sentiment expression patterns across languages, while Twitter domains differ significantly.
- Evidence anchors:
  - [abstract] "Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in the same language."
  - [section 5.2] "Our result shows that the adaptation of imdb has similar performance as the cross-domain adaptation, while the NollySenti (eng) exceeded the performance by over +6%!"

### Mechanism 2
- Claim: Machine translation improves sentiment classification performance despite low BLEU scores.
- Mechanism: MT preserves the sentiment-bearing content of reviews, even when translation quality metrics are low, allowing the classifier to leverage this sentiment information.
- Core assumption: Sentiment is preserved in translation even when lexical and syntactic quality is lower, and the classifier can still extract sentiment from imperfect translations.
- Evidence anchors:
  - [abstract] "Machine translation further improves performance by 7% over cross-lingual evaluation."
  - [section 5.2] "Native speakers also rated their output to preserve more sentiment (i.e. achieving at least of 90%) even for some translated texts with low adequacy ratings."

### Mechanism 3
- Claim: Africa-centric PLMs (AfriBERTa, AfroXLMR) outperform massively multilingual PLMs for these languages.
- Mechanism: PLMs pre-trained on African languages capture language-specific features and linguistic patterns that are more relevant for the target languages than general multilingual models.
- Core assumption: The pre-training data for Africa-centric PLMs includes sufficient representative samples of the target languages to learn useful representations.
- Evidence anchors:
  - [section 5.1] "In general, we find Africa-centric PLMs (AfriBERTa-large and AfroXLMR-base) have better accuracy than massively multilingual PLMs pre-trained on around 100 languages."
  - [section 5.1] "Overall, AfriBERTa achieves the best result on average, but slightly worse for English and Nigerian-Pidgin (an English-based creole language) since it has not been pre-trained on the English language."

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: To leverage labeled data in high-resource languages (English) to build models for low-resource languages (Hausa, Igbo, Nigerian-Pidgin, Yorùbá) where labeled data is scarce.
  - Quick check question: How does the model handle language differences during transfer, and what assumptions are made about the similarity of sentiment expression across languages?

- Concept: Domain adaptation
  - Why needed here: To handle the difference between the source domain (Twitter or English movie reviews) and the target domain (Nigerian movie reviews) to ensure the model learns relevant patterns.
  - Quick check question: What are the key differences between the source and target domains, and how might these differences affect the transferability of learned features?

- Concept: Machine translation evaluation
  - Why needed here: To assess the quality of MT outputs for sentiment classification, as traditional metrics like BLEU may not capture sentiment preservation.
  - Quick check question: How do we measure whether MT preserves sentiment, and what are the limitations of using automatic metrics like BLEU for this task?

## Architecture Onboarding

- Component map:
  Data collection and preprocessing (scraping movie reviews, translation) -> Model training (fine-tuning PLMs, training classifiers) -> Zero-shot evaluation (cross-domain and cross-lingual transfer) -> Machine translation integration -> Human evaluation of MT quality and sentiment preservation

- Critical path:
  1. Collect and preprocess movie review data in English and other languages
  2. Fine-tune PLMs on English movie review data
  3. Evaluate zero-shot performance on other languages
  4. Generate synthetic data via MT
  5. Evaluate performance with MT-translated data
  6. Conduct human evaluation of MT quality and sentiment preservation

- Design tradeoffs:
  - Using professional translators vs. automatic MT for data creation
  - Choosing between different PLMs (Africa-centric vs. multilingual)
  - Balancing the amount of MT-translated data vs. original data
  - Deciding on evaluation metrics (accuracy vs. human evaluation)

- Failure signatures:
  - Poor zero-shot performance indicating domain or language mismatch
  - MT outputs with low adequacy but high sentiment preservation
  - Classifier performance not improving with additional MT data
  - Human evaluation revealing significant sentiment alteration in MT outputs

- First 3 experiments:
  1. Evaluate zero-shot cross-lingual transfer from English movie reviews to other languages using different PLMs
  2. Evaluate zero-shot cross-domain transfer from Twitter to movie reviews in the same language
  3. Generate MT-translated data and evaluate its impact on classifier performance compared to zero-shot transfer

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several implicit questions emerge from the research:

### Open Question 1
- Question: What specific linguistic features or phenomena in Nigerian languages cause the most significant challenges for sentiment preservation in machine translation?
- Basis in paper: Explicit - The paper mentions that despite low BLEU scores, human evaluators found high sentiment preservation rates, and provides examples of translation errors in Table 6
- Why unresolved: The paper identifies that translations have issues but doesn't systematically analyze which linguistic features (e.g., tone markers, agglutinative morphology, or cultural references) contribute most to sentiment preservation failures
- What evidence would resolve it: A detailed linguistic analysis comparing successful vs. failed translations, identifying specific features that correlate with sentiment preservation or loss

### Open Question 2
- Question: How does the performance of sentiment classification models vary across different genres of Nollywood movies beyond the general movie review domain?
- Basis in paper: Inferred - The paper uses Nollywood movie reviews as a domain but doesn't explore genre-specific variations in sentiment classification
- Why unresolved: The paper establishes that Nollywood reviews can be used for sentiment analysis but doesn't investigate whether performance differs for genres like comedy, drama, action, or cultural films
- What evidence would resolve it: Experiments evaluating sentiment classification models across multiple Nollywood genres with genre-specific datasets

### Open Question 3
- Question: What is the optimal balance between synthetic data generated through machine translation and human-translated data for training sentiment classifiers in low-resource languages?
- Basis in paper: Explicit - The paper shows that machine-translated data improves performance but doesn't explore the trade-off between synthetic and human-translated data
- Why unresolved: While the paper demonstrates that MT-generated reviews improve performance, it doesn't investigate whether combining different ratios of synthetic and human-translated data yields better results
- What evidence would resolve it: Systematic experiments varying the proportion of human-translated vs. machine-translated training data and measuring performance impact

## Limitations
- The evaluation focuses primarily on accuracy metrics, which may not fully capture nuanced aspects of sentiment classification performance
- The study relies on English as the source language for machine translation, potentially introducing biases for languages with different typological structures
- The research does not address dialectal variations within Nigerian languages or how they might affect sentiment classification performance

## Confidence
- **High Confidence:** Cross-lingual transfer from English movie reviews outperforms cross-domain Twitter transfer (5%+ improvement)
- **Medium Confidence:** Machine translation improves performance by 7% despite low BLEU scores
- **Low Confidence:** Africa-centric PLMs consistently outperform massively multilingual PLMs for these specific languages

## Next Checks
1. **Linguistic Feature Analysis:** Conduct a systematic analysis of sentiment expression patterns across the five Nigerian languages to identify language-specific features that may affect cross-lingual transferability.

2. **Controlled Translation Study:** Perform a controlled experiment comparing MT-translated data against professionally translated data for a subset of the dataset to quantify the impact of translation quality on sentiment classification performance.

3. **Domain Transfer Generalization:** Evaluate the model's performance on a different domain (e.g., news articles or social media posts) within the same languages to assess whether the domain-specific patterns learned from movie reviews transfer effectively.