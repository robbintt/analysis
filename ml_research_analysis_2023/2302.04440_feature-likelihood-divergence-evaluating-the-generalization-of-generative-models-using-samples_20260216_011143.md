---
ver: rpa2
title: 'Feature Likelihood Divergence: Evaluating the Generalization of Generative
  Models Using Samples'
arxiv_id: '2302.04440'
source_url: https://arxiv.org/abs/2302.04440
tags:
- samples
- generative
- training
- tting
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Feature Likelihood Score (FLS), a sample-based
  metric for evaluating generative models that captures sample quality, diversity,
  and generalization. FLS estimates the perceptual density of generated samples using
  a mixture of Gaussians in a feature space (Inception-v3 or CLIP) and compares it
  to the density of real data.
---

# Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples

## Quick Facts
- arXiv ID: 2302.04440
- Source URL: https://arxiv.org/abs/2302.04440
- Reference count: 26
- Key outcome: Introduces FLS, a sample-based metric that captures sample quality, diversity, and generalization while detecting overfitting in generative models.

## Executive Summary
This paper introduces the Feature Likelihood Score (FLS), a novel metric for evaluating generative models that simultaneously assesses sample quality, diversity, and generalization while detecting overfitting. FLS estimates the perceptual density of generated samples using a mixture of Gaussians in a feature space (Inception-v3 or CLIP) and compares it to the density of real data. The method identifies overfitting by fitting bandwidths using training samples and evaluating the likelihood of test samples, revealing models that are overrated or underrated by traditional metrics like FID.

## Method Summary
FLS uses a mixture of Gaussians density estimator where each generated sample is a Gaussian center in a perceptual feature space. Bandwidths are optimized to maximize the likelihood of training samples, causing memorized samples to collapse into Dirac deltas. The method computes a log-likelihood ratio between the generated MoG and a baseline MoG (using half the training data) on the test set, yielding a single interpretable score that balances quality, diversity, and generalization.

## Key Results
- FLS correlates with FID on standard datasets while detecting overfitting cases FID misses
- Experiments on CIFAR10, ImageNet, and LSUN show FLS identifies models overrated or underrated by FID
- FLS reveals synthetic overfitting cases where bandwidth collapse indicates memorization
- Code implementation available at https://github.com/marcojira/fld

## Why This Works (Mechanism)

### Mechanism 1
FLS detects overfitting by collapsing bandwidths for memorized samples. During optimization, bandwidths are fitted to maximize training sample likelihood. If a generated sample exactly matches a training sample, its bandwidth collapses to zero (Dirac delta), causing test samples far from memorized points to have very low likelihood, revealing overfitting.

### Mechanism 2
FLS provides a unified score balancing quality, diversity, and generalization by computing the log-likelihood ratio between generated and baseline MoGs on test samples. The exponential of the normalized difference gives a single interpretable score where 100 indicates perfect generalization, simultaneously penalizing poor quality, poor diversity, and overfitting.

### Mechanism 3
FLS correlates with FID while detecting overfitting cases FID misses by evaluating samples in perceptual feature space (Inception-v3 or CLIP). Unlike FID, FLS uses density estimation with bandwidth fitting to detect overfitting. Empirical results show strong correlation with FID on standard datasets but reveal models overrated or underrated by FID due to their generalization performance.

## Foundational Learning

- **Concept: Mixture of Gaussians density estimation** - Why needed: FLS uses MoG where each generated sample is a Gaussian center to model generated sample density without assuming parametric form. Quick check: Why does using each generated sample as a Gaussian center help in modeling the distribution without assuming a parametric form?

- **Concept: Kernel Density Estimation (KDE) bandwidth selection** - Why needed: FLS optimizes individual bandwidths for each Gaussian using training data likelihood, generalizing KDE bandwidth selection. Quick check: How does optimizing bandwidths on training data (rather than generated data) help detect overfitting?

- **Concept: Feature space embeddings for perceptual similarity** - Why needed: FLS maps images to Inception-v3 or CLIP feature spaces to compute meaningful ℓ2 distances that correlate with perceptual similarity. Quick check: Why is it important to use a pre-trained network's feature space rather than raw pixel space for evaluating generative models?

## Architecture Onboarding

- **Component map:** Feature extractor (Inception-v3 or CLIP) -> MoG density estimator with individual Gaussian centers -> Bandwidth optimization module -> Score computation

- **Critical path:** 1. Map training, test, and generated samples to feature space 2. Split training set into two halves 3. Fit MoG on generated samples with bandwidths optimized on first half 4. Fit MoG on baseline samples (second half) with bandwidths optimized on first half 5. Compute log-likelihood of test set under both MoGs 6. Calculate FLS as exponential of normalized likelihood difference

- **Design tradeoffs:** MoG vs other density estimators (simplicity vs scalability); individual vs shared bandwidths (precision vs complexity); feature space choice (standard vs robustness)

- **Failure signatures:** Poor FLS with high FID indicates overfitting; high FLS with low FID suggests good generalization but quality/diversity issues; FLS near 50 means no overfitting detected but potential quality/diversity problems

- **First 3 experiments:** 1. Implement FLS on Two Moons synthetic dataset with known overfitting model to verify bandwidth collapse 2. Apply FLS to pre-trained GAN on CIFAR10 and compare with FID 3. Test FLS with different feature extractors (Inception vs CLIP) on same model

## Open Questions the Paper Calls Out

### Open Question 1
How can FLS be adapted for conditional generative models? The paper mentions extending FLS to conditional models as future work but provides no framework or experimental results.

### Open Question 2
How does the choice of feature extractor (Inception-v3 vs CLIP) affect FLS performance? The paper experiments with both but lacks detailed comparison of their impact on results.

### Open Question 3
Can FLS be extended to handle other data modalities beyond natural images, such as text, audio, or time series? The paper suggests this as future work but provides no theoretical framework or experimental validation.

## Limitations

- Computational scalability concerns due to O(n²) distance computations in MoG density estimation
- Sensitivity to feature space choice and bandwidth optimization hyperparameters
- Limited empirical validation across diverse overfitting patterns and datasets

## Confidence

**Medium** for overfitting detection reliability across diverse generative models. **Medium** for FID correlation claims. **Low** for computational scalability assessment.

## Next Checks

1. Test FLS on models with varying degrees of mode collapse and memorization across multiple datasets to validate sensitivity to different overfitting patterns.

2. Compare FLS scores using different feature extractors (Inception-v3, CLIP, alternatives) on same generative models to quantify feature space impact.

3. Evaluate FLS performance with sample sizes from 1K to 100K to establish practical limits and test approximate MoG methods for scalability.