---
ver: rpa2
title: Evaluation of Active Feature Acquisition Methods for Static Feature Settings
arxiv_id: '2312.03619'
source_url: https://arxiv.org/abs/2312.03619
tags:
- feature
- data
- semi-offline
- assumption
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends semi-offline reinforcement learning (RL) to
  the static feature setting for active feature acquisition (AFA) performance evaluation
  (AFAPE). The approach simulates an AFA agent's feature acquisition process under
  a missing-at-random (MAR) assumption, then corrects for this simulation using inverse
  probability weighting (IPW), direct method (DM), and double reinforcement learning
  (DRL) estimators.
---

# Evaluation of Active Feature Acquisition Methods for Static Feature Settings

## Quick Facts
- arXiv ID: 2312.03619
- Source URL: https://arxiv.org/abs/2312.03619
- Authors: 
- Reference count: 40
- This paper extends semi-offline reinforcement learning (RL) to the static feature setting for active feature acquisition (AFA) performance evaluation (AFAPE).

## Executive Summary
This paper develops a semi-offline RL framework for evaluating active feature acquisition (AFA) performance in static feature settings. The approach simulates an AFA agent's feature acquisition process under missing-at-random (MAR) assumptions and corrects for this simulation using inverse probability weighting (IPW), direct method (DM), and double reinforcement learning (DRL) estimators. The framework relaxes the strong positivity assumptions required by missing data approaches and improves data efficiency. Experiments show that semi-offline RL estimators outperform biased alternatives and achieve better data efficiency than missing data IPW, particularly for data-hungry AFA agents.

## Method Summary
The method extends semi-offline RL to static feature settings for AFAPE by simulating trajectories under a blocked version of the AFA policy. The framework uses inverse probability weighting (IPW), direct method (DM), and double reinforcement learning (DRL) estimators to correct for the simulation. A key innovation is the hybrid semi-offline RL + missing data view that handles missing-not-at-random (MNAR) scenarios by first identifying p(Xadj,(1)) using missing data methods, then treating Xadj,(1) like Xo and applying semi-offline RL for the remaining variables.

## Key Results
- Semi-offline RL estimators outperform biased alternatives (mean imputation, blocking, complete case analysis) in estimating acquisition and misclassification costs
- DRL estimator maintains consistency even with model misspecification due to double robustness
- Semi-offline RL approach achieves better data efficiency than missing data IPW, especially for data-hungry AFA agents
- Hybrid semi-offline RL + missing data approach successfully handles MNAR scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The semi-offline RL framework relaxes the strong positivity requirement of complete cases required by the missing data view.
- Mechanism: In semi-offline RL, the simulation policy is restricted to sampling only features available in the retrospective data, allowing trajectory simulations from datapoints where R ≥ R′ (features needed for acquisition are observed), rather than requiring complete cases (R = ⃗1).
- Core assumption: The retrospective dataset contains support for at least as many feature acquisitions as desired by the AFA policy.
- Evidence anchors:
  - [abstract]: "The semi-offline RL framework relaxes positivity assumptions compared to missing data approaches and improves data efficiency."
  - [section 5.2]: "The semi-offline RL viewpoint drastically reduces positivity requirements (i.e. requirements for which patterns of missingness exist in the data) compared to the missing data view."
- Break condition: If the retrospective dataset lacks support for even minimal feature acquisition patterns required by the AFA policy.

### Mechanism 2
- Claim: The DRL estimator maintains consistency even with model misspecification.
- Mechanism: The DRL estimator combines IPW and DM estimators, achieving double robustness - it remains consistent if either the propensity score model πβ or the Q-function QSemi is correctly specified.
- Core assumption: At least one of the nuisance models (propensity score or Q-function) is correctly specified.
- Evidence anchors:
  - [abstract]: "The DRL estimator maintains consistency even with model misspecification."
  - [section 5.4]: "Theorem 7. (Double robustness of JDRL-Semi). The estimator JDRL-Semi is doubly robust, in the sense that it is consistent if either the Q-function QSemi or the propensity score model ˆπβ is correctly specified."
- Break condition: Both the propensity score model and Q-function are misspecified.

### Mechanism 3
- Claim: The hybrid semi-offline RL + missing data view handles MNAR scenarios where pure approaches fail.
- Mechanism: For MNAR scenarios, first identify p(Xadj,(1)) using missing data methods (which can handle MNAR), then treat Xadj,(1) like Xo and apply semi-offline RL for the remaining variables.
- Core assumption: The MNAR missingness mechanism πβ(R|Xadj,(1)) is identified using missing data techniques.
- Evidence anchors:
  - [section 6.1]: "We propose a new hybrid semi-offline RL / missing data viewpoint, that lies in between the pure semi-offline RL view and the missing data view."
  - [section 6.3]: "One can in general apply any combination of estimators from the semi-offline RL and missing data viewpoints to estimate the respective parts of the reformulated AFAPE problem under the hybrid semi-offline RL + missing data view."
- Break condition: The MNAR missingness mechanism cannot be identified using available missing data techniques.

## Foundational Learning

- Concept: Semi-parametric theory and influence functions
  - Why needed here: Understanding how to construct consistent and asymptotically linear estimators for the target parameter J without fully specifying the data-generating distribution p.
  - Quick check question: What is the key property that makes an estimator asymptotically linear, and why is this useful for inference?

- Concept: Missing data mechanisms (MCAR, MAR, MNAR)
  - Why needed here: Different missingness assumptions lead to different identification and estimation strategies for AFAPE.
  - Quick check question: Under which missingness assumption can the semi-offline RL approach be directly applied without combining with missing data methods?

- Concept: Counterfactual reasoning and causal inference
  - Why needed here: AFAPE involves estimating what would have happened under a different feature acquisition policy, requiring causal inference concepts.
  - Quick check question: What is the key assumption needed to relate the counterfactual feature values X(1) to the observed features X and missingness indicators R?

## Architecture Onboarding

- Component map:
  - Retrospective dataset (X, R, Y) -> AFA policy πα -> Classifier g(Y*|X, A) -> Simulation policy πsim -> Nuisance models (ˆπβ, QSemi) -> Estimators (IPW-Semi, DM-Semi, DRL-Semi)

- Critical path:
  1. Load retrospective dataset
  2. Train nuisance models (ˆπβ, QSemi)
  3. Simulate trajectories under blocked policy
  4. Apply estimator to simulated data
  5. Compute performance estimate

- Design tradeoffs:
  - Missing data view: Simpler estimation but requires complete cases and strong positivity
  - Semi-offline RL view: More data-efficient but requires correct simulation policy and Q-function
  - Hybrid view: Handles MNAR but adds complexity of combining approaches

- Failure signatures:
  - Bias in estimates: Incorrect nuisance models or violated assumptions
  - High variance: Insufficient data or extreme weights in IPW estimators
  - Poor convergence: Mismatch between simulation policy and AFA policy

- First 3 experiments:
  1. Synthetic MAR experiment with random AFA policies (10% and 90% acquisition probability)
  2. Synthetic MNAR experiment with similar random policies
  3. Real-world Heloc dataset under MAR missingness with DQN AFA agent

## Open Questions the Paper Calls Out

- Question: What is the most efficient influence function for the AFAPE problem?
  - Basis in paper: [inferred] The authors explicitly state that "we have not delved into the efficiency of this derived influence function" and leave it as future work.
  - Why unresolved: The paper derives an influence function but does not analyze its efficiency compared to other possible influence functions.
  - What evidence would resolve it: A formal proof demonstrating whether the derived influence function is efficient or identifying an alternative efficient influence function.

- Question: How does the AFAPE framework extend to optimize active feature acquisition methods rather than just evaluate them?
  - Basis in paper: [explicit] The authors state "we plan to extend the developed semi-offline RL framework to be able to also address the AFA optimization problem."
  - Why unresolved: The paper focuses on evaluation but only briefly mentions optimization in Section 3.5 without providing implementation details.
  - What evidence would resolve it: A concrete algorithm or framework showing how semi-offline RL can be applied to optimize AFA policies rather than just evaluate them.

- Question: What are the effects of partial static feature assumptions (features that are static during appointments but change between them) on the AFAPE problem?
  - Basis in paper: [explicit] The authors discuss "scenarios where the static feature assumption only holds partially" and suggest combining methods from this paper with their companion paper.
  - Why unresolved: The paper only briefly mentions this scenario without analyzing its impact on identification or estimation.
  - What evidence would resolve it: Experimental results comparing different estimation methods under various partial static feature scenarios.

## Limitations

- The approach relies on the MAR assumption for the semi-offline RL framework, which may not hold in many practical AFA scenarios
- Performance is sensitive to the quality of nuisance models, and misspecification can lead to bias even with the double-robust DRL estimator
- Computational complexity of simulating trajectories may become prohibitive for large-scale problems with many features

## Confidence

- **High confidence**: The theoretical foundations of the semi-offline RL framework and the double robustness property of the DRL estimator
- **Medium confidence**: The empirical results showing improved performance over biased baselines, as these depend on specific experimental conditions and hyperparameter choices
- **Medium confidence**: The claim about data efficiency improvements, as this is demonstrated on synthetic data and may vary with real-world data characteristics

## Next Checks

1. **Sensitivity analysis to MAR assumption violations**: Systematically evaluate estimator performance under various degrees of MAR assumption violations to quantify robustness to model misspecification.

2. **Scaling study**: Test the computational efficiency and statistical performance of the approach on high-dimensional feature spaces (e.g., 50+ features) to assess practical scalability limits.

3. **Comparison with online evaluation**: Validate the semi-offline RL estimates against ground truth performance measured through online A/B testing in a controlled environment to assess estimation accuracy in practice.