---
ver: rpa2
title: 'ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language
  Models to CTR Prediction'
arxiv_id: '2310.09234'
source_url: https://arxiv.org/abs/2310.09234
tags:
- prompt
- prediction
- language
- arxiv
- clickprompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ClickPrompt, a model-agnostic framework that
  uses CTR models to generate interaction-aware soft prompts for pretrained language
  models (PLMs) to improve CTR prediction. The core idea is to explicitly align and
  interact the collaborative knowledge from CTR models and semantic knowledge from
  PLMs through soft prompt interfaces.
---

# ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction

## Quick Facts
- **arXiv ID**: 2310.09234
- **Source URL**: https://arxiv.org/abs/2310.09234
- **Reference count**: 40
- **Primary result**: ClickPrompt significantly outperforms existing baseline models in CTR prediction by using CTR models to generate interaction-aware soft prompts for PLMs

## Executive Summary
This paper proposes ClickPrompt, a model-agnostic framework that leverages CTR models to generate soft prompts for pretrained language models (PLMs) in click-through rate (CTR) prediction tasks. The framework addresses the limitations of traditional CTR models (shallow feature interactions) and PLM-based CTR models (weak collaborative knowledge incorporation) by explicitly aligning collaborative knowledge from CTR models with semantic knowledge from PLMs through soft prompt interfaces. ClickPrompt adopts a pretrain-finetune scheme with a prompt-augmented masked language modeling (PA-MLM) task, demonstrating significant performance improvements across four real-world datasets.

## Method Summary
ClickPrompt processes multi-field categorical data by converting tabular features into one-hot ID features and corresponding textual features. A CTR model (e.g., DCNv2) processes the ID features through embedding and feature interaction layers, then generates soft prompts via a prompt generation layer. These prompts serve as prefix states at each transformer layer of a PLM (e.g., RoBERTa-base). During PA-MLM pretraining, the PLM must recover masked tokens using both text context and soft prompts from the CTR model, creating fine-grained alignments between collaborative and semantic knowledge. The framework supports two finetuning strategies: with PLM (combining outputs) or without PLM (using only CTR output).

## Key Results
- ClickPrompt significantly outperforms existing baseline models in terms of AUC and Log Loss across four real-world datasets
- Layerwise prompting consistently outperforms single-layer prompting, with improvements of 0.25%-2.36% in AUC
- PA-MLM pretraining demonstrates substantial performance gains compared to direct finetuning
- The model-agnostic framework works effectively with different CTR model backbones (FM, DCN, DCNv2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft prompts generated by CTR models enable explicit alignment between collaborative knowledge (from ID features) and semantic knowledge (from text).
- Mechanism: The CTR model processes one-hot ID features through embedding and feature interaction layers to produce compact representations. These representations are then projected into soft prompt vectors that serve as prefix states at each transformer layer of the PLM. During forward propagation, collaborative signals flow into the PLM via these prompts. During backpropagation, semantic knowledge from the PLM flows back to the CTR model, enabling bidirectional knowledge transfer.
- Core assumption: The feature interaction patterns learned by CTR models contain meaningful collaborative signals that can be effectively encoded into soft prompts and aligned with PLM semantic understanding.
- Evidence anchors:
  - [abstract]: "The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface."
  - [section]: "With the soft prompts as the bridges, the ID-based collaborative knowledge will be passed to PLM through forward propagation, and the text-based semantic knowledge would flow back into the CTR model via backpropagation."
  - [corpus]: Weak - No direct citations found, but the mechanism is novel and distinct from existing CTRL and FLIP approaches.
- Break condition: If the CTR model's feature interaction patterns don't capture meaningful collaborative signals, or if the prompt generation layer fails to preserve this information during projection.

### Mechanism 2
- Claim: Prompt-augmented masked language modeling (PA-MLM) pretraining task creates fine-grained alignments between CTR and PLM parameters.
- Mechanism: PA-MLM applies token masking to textual features while preserving original ID features. The PLM must recover masked tokens using both text context and soft prompts from the CTR model. This forces the PLM to extract and incorporate collaborative information embedded in the prompts, creating fine-grained alignments between the two modalities.
- Core assumption: The masked token recovery task provides sufficient supervision for the PLM to learn to leverage collaborative signals from the CTR model's soft prompts.
- Evidence anchors:
  - [abstract]: "PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated from ID features."
  - [section]: "To complete such a cloze task over masked tokens, PLM has to extract and incorporate the corresponding 'right answer' embedded in the soft prompts, resulting in fine-grained alignments between the CTR model and PLM towards the same input."
  - [corpus]: Weak - This specific pretraining task is novel; related work focuses on contrastive alignment rather than masked language modeling with prompts.
- Break condition: If the masking ratio is too high or too low, or if the prompt generation layer produces noisy/meaningless vectors that mislead the PLM during recovery.

### Mechanism 3
- Claim: The layerwise prompt strategy enables more effective interaction between collaborative and semantic knowledge than shallow prompting.
- Mechanism: Instead of placing prompts only at the first layer, ClickPrompt maintains prompt vectors at each transformer layer. This allows collaborative signals to be incorporated at multiple levels of semantic abstraction, preventing the CTR model's knowledge from being overwhelmed during deep PLM processing.
- Core assumption: Feature interactions and semantic representations evolve differently across transformer layers, requiring collaborative signals at multiple abstraction levels.
- Evidence anchors:
  - [section]: "If the prompt vectors are only placed at the shallow input layer, the collaborative knowledge from CTR model might be overwhelmed during the PLM forwarding, thus leading to unbalanced interactions with semantic knowledge and consequently inferior performance."
  - [section]: Table 4 shows layerwise prompting consistently outperforms single-layer prompting across different CTR models and datasets.
  - [corpus]: Weak - This specific architectural choice is not well-documented in related literature.
- Break condition: If the projection networks cannot generate distinct meaningful prompts for each layer, or if deeper layers become saturated with prompt information.

## Foundational Learning

- Concept: Feature interaction modeling in CTR prediction
  - Why needed here: Understanding how traditional CTR models capture collaborative signals through feature crossing patterns is essential to grasp why CTR models can generate meaningful soft prompts.
  - Quick check question: What is the difference between explicit and implicit feature interaction modeling, and how do FM, DeepFM, and DCN approach this differently?

- Concept: Masked language modeling and pretraining objectives
  - Why needed here: The PA-MLM task builds upon BERT's masked language modeling but adds the prompt generation component, requiring understanding of both standard MLM and how prompts modify the objective.
  - Quick check question: How does the standard BERT MLM objective differ from the PA-MLM objective in ClickPrompt, and what additional supervision does the prompt component provide?

- Concept: Prompt tuning and prefix language modeling
  - Why needed here: ClickPrompt uses soft prompts as prefix states, which is related to but distinct from standard prompt tuning. Understanding prefix LM helps explain how prompts modify transformer behavior.
  - Quick check question: What is the key difference between prompt tuning as a parameter-efficient finetuning method versus ClickPrompt's use of soft prompts as bridges between modalities?

## Architecture Onboarding

- Component map:
  Input layer (one-hot ID features + textual features) -> CTR model (embedding + feature interaction + prompt generation) -> PLM (transformer layers with layerwise soft prompts) -> Output layer (sigmoid classification)

- Critical path:
  1. ID features flow through CTR model to generate soft prompts
  2. Soft prompts serve as prefix states in each PLM layer
  3. PLM processes textual features with prompt-enhanced representations
  4. Outputs from both CTR and PLM (if used) are combined for final prediction

- Design tradeoffs:
  - Using soft prompts vs. hard templates: Soft prompts allow learned, context-aware bridging but require more parameters and training complexity
  - Layerwise prompting vs. shallow prompting: Layerwise provides better signal propagation but increases parameter count and computational cost
  - Finetuning with vs. without PLM: With PLM gives better performance but higher inference cost; without PLM is more efficient but relies entirely on pretraining for semantic knowledge transfer

- Failure signatures:
  - Performance worse than CTR baseline: Likely indicates poor prompt generation or misalignment between CTR and PLM representations
  - Slow convergence during pretraining: May suggest suboptimal prompt generation or insufficient supervision from the PA-MLM task
  - High variance across runs: Could indicate sensitivity to initialization or instability in the prompt generation layer

- First 3 experiments:
  1. Implement the basic architecture with a simple CTR model (e.g., FM) and small PLM (e.g., TinyBERT), using shallow prompting only
  2. Add layerwise prompting and compare performance to validate the importance of multi-layer signal propagation
  3. Implement PA-MLM pretraining and compare pretraining vs. no-pretraining scenarios to demonstrate the value of fine-grained alignment

## Open Questions the Paper Calls Out
The paper mentions future work directions including exploring ClickPrompt on other recommendation tasks beyond CTR prediction, suggesting this is a potential area for extension.

## Limitations
- Architecture scalability remains uncertain, as experiments were conducted on relatively small-scale datasets and model configurations
- Generalization beyond categorical features is unexplored, with reliance on simple template-based text generation potentially limiting broader applicability
- Prompt generation layer stability lacks thorough investigation across different random seeds and initialization strategies

## Confidence
**High Confidence**: The core claim that CTR models can generate soft prompts for PLMs is well-supported by experimental results across four datasets; the superiority of layerwise prompting over shallow prompting is consistently demonstrated with statistical significance; the PA-MLM pretraining task effectively improves performance compared to direct finetuning.

**Medium Confidence**: The mechanism of bidirectional knowledge transfer through soft prompts is theoretically sound but relies on assumptions about feature interaction patterns; the framework's model-agnostic nature is demonstrated with multiple CTR models but limited PLM variations; the computational efficiency claims are supported by inference-only PLM usage but lack detailed complexity analysis.

**Low Confidence**: Claims about ClickPrompt's effectiveness on industrial-scale datasets are extrapolated from experimental results; the framework's robustness to different data distributions and noise patterns is not thoroughly investigated; long-term stability and maintenance requirements for production deployment are not addressed.

## Next Checks
1. **Architecture Stress Test**: Implement ClickPrompt with larger PLMs (e.g., BERT-large, T5-base) and evaluate performance on datasets with 10x more samples and features. Measure memory consumption, training time, and inference latency to assess scalability limits.

2. **Prompt Generation Robustness Analysis**: Conduct experiments varying the prompt generation layer architecture (different projection dimensions, normalization techniques, and initialization strategies). Measure performance variance across 10 random seeds to quantify stability and identify failure modes.

3. **Cross-Domain Generalization Test**: Apply ClickPrompt to datasets from different domains (e.g., financial transactions, healthcare records, e-commerce) with varying feature distributions and noise levels. Compare performance against domain-specific baselines to evaluate generalization capability beyond the original experimental domains.