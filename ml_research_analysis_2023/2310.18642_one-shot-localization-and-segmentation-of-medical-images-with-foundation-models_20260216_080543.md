---
ver: rpa2
title: One-shot Localization and Segmentation of Medical Images with Foundation Models
arxiv_id: '2310.18642'
source_url: https://arxiv.org/abs/2310.18642
tags:
- segmentation
- images
- image
- tasks
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that pre-trained Vision Transformers and
  Stable Diffusion models, despite being trained only on natural images, can effectively
  perform medical image localization and segmentation across diverse modalities (CT,
  MR, Ultrasound) and anatomical regions. The authors propose a one-shot method that
  uses dense feature extraction from these foundation models to establish correspondence
  with a template image, followed by prompting a Segment Anything model for segmentation.
---

# One-shot Localization and Segmentation of Medical Images with Foundation Models

## Quick Facts
- **arXiv ID**: 2310.18642
- **Source URL**: https://arxiv.org/abs/2310.18642
- **Reference count**: 14
- **Key outcome**: Foundation models trained on natural images achieve Dice scores of 62-90% for medical image segmentation without in-domain fine-tuning

## Executive Summary
This paper presents a one-shot method for medical image localization and segmentation that leverages pre-trained Vision Transformers and Stable Diffusion models trained exclusively on natural images. The approach uses dense feature extraction from these foundation models to establish correspondence with a template image, then prompts a Segment Anything model to generate segmentation masks. The method demonstrates strong performance across diverse medical imaging modalities (CT, MR, Ultrasound) and anatomical regions, achieving Dice scores ranging from 62% to 90% without requiring any fine-tuning on medical data.

## Method Summary
The method consists of three main stages: (1) Dense feature extraction from pre-trained foundation models (DINO variants, Stable Diffusion, SAM, CLIP) applied to both template and target medical images using overlapping patch methods and neighborhood log binning; (2) Correlation computation between template and target features to establish pixel-level correspondence and generate region prompts; (3) Prompting the Segment Anything model with the generated prompts to produce segmentation masks. The approach eliminates the need for in-domain fine-tuning while achieving competitive performance against few-shot methods.

## Key Results
- Achieves Dice scores of 62-90% across seven different medical segmentation tasks
- Outperforms the few-shot UniverSeg method on six out of seven segmentation tasks
- Self-supervised learning models (DINO v2) show better performance than supervised counterparts (SAM, CLIP)
- Normalized Euclidean distance below 0.05 threshold considered acceptable for clinical usage

## Why This Works (Mechanism)

### Mechanism 1
- Vision Transformers and Stable Diffusion models trained on natural images can extract semantically meaningful features from medical images across diverse modalities
- Dense feature extraction followed by pixel-level interpolation and correlation computation establishes correspondence between template and target images
- Core assumption: Learned visual representations from natural images capture generalizable semantic structure applicable to medical imaging domains

### Mechanism 2
- Chaining dense feature correspondence with SAM-based segmentation produces effective one-shot medical image segmentation without fine-tuning
- Template-based landmark correspondence establishes region prompts that guide SAM to segment target images accurately
- Core assumption: SAM's prompt-based segmentation can generalize from natural image masks to medical image structures when provided appropriate region prompts

### Mechanism 3
- Self-supervised learning models (DINO/DINOv2) outperform supervised models (SAM, CLIP) for medical image feature extraction
- SSL models trained on natural images develop more generalizable feature representations that transfer better to unseen domains
- Core assumption: Self-supervised learning captures more fundamental visual patterns that transfer across domains better than supervised classification tasks

## Foundational Learning

- **Vision Transformer architecture and patch-based feature extraction**: Understanding how ViT models tokenize images into patches and extract dense features is critical for implementing the feature extraction pipeline
  - Quick check: How does the patch size affect the spatial resolution of extracted features?

- **Feature correspondence and similarity metrics**: The method relies on computing correlation between template and target image features to establish correspondence
  - Quick check: What correlation metric is most appropriate for comparing high-dimensional feature vectors across images?

- **Prompt-based segmentation with SAM**: The segmentation stage uses SAM with prompts derived from feature correspondence to generate masks
  - Quick check: How do positive and negative prompts affect SAM's segmentation output?

## Architecture Onboarding

- **Component map**: Template image → Feature extraction (DINO/SD) → Dense feature interpolation → Pixel-level correlation → Prompt generation → SAM segmentation → Output mask
- **Critical path**: Feature extraction → Correlation computation → SAM prompting → Mask generation
- **Design tradeoffs**: Larger models (d2g14) vs smaller models (d2s14) for feature extraction - larger models may capture more semantic detail but require more computation
- **Failure signatures**: Poor landmark correspondence indicated by high normalized Euclidean distance; inaccurate segmentation indicated by low Dice scores
- **First 3 experiments**:
  1. Test feature extraction on a simple medical image pair (template and target) to verify dense features capture anatomical structures
  2. Implement correlation computation between template and target features to verify landmark correspondence
  3. Chain feature correspondence with SAM prompting on a known segmentation task to verify end-to-end pipeline functionality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How well would this one-shot segmentation approach scale to 3D volume segmentation compared to its current 2D slice-based implementation?
- **Basis in paper**: The authors explicitly state "Our approach is currently based on single slice, and it would be interesting to see how does this scale to 3D volume segmentation" and note this as a shortcoming
- **Why unresolved**: The paper only evaluates performance on 2D slices across all tasks. No experiments or analysis were conducted on volumetric data, leaving the question of how the dense feature extraction and SAM prompting would perform when extended to 3D space
- **What evidence would resolve it**: Systematic evaluation of the proposed method on 3D medical imaging datasets (e.g., volumetric CT, MRI) with comparison to both 2D slice-by-slice processing and existing 3D segmentation methods, reporting metrics like Dice score and computational efficiency

### Open Question 2
- **Question**: Which specific model configurations (DINO variant, SD version, SAM, CLIP) are optimal for different anatomical regions and imaging modalities, and can these be predicted without exhaustive testing?
- **Basis in paper**: The authors observe that "there is no single model which outperforms all other models across different tasks" and note that "we have to choose the model optimal for the task at hand"
- **Why unresolved**: While the paper provides performance comparisons across various models and tasks, it doesn't establish a systematic framework for predicting which model would work best for a given anatomical region or modality. The variability in performance across tasks suggests underlying patterns that remain unexplored
- **What evidence would resolve it**: Analysis correlating model performance with image characteristics (contrast, texture, organ shape regularity) or anatomical features, potentially leading to a decision tree or predictive model for selecting the optimal foundation model based on task-specific image properties

### Open Question 3
- **Question**: How does the performance of this one-shot method compare to supervised, in-domain task-specific models trained on medical imaging data?
- **Basis in paper**: The authors explicitly state "We have not compared the performance of such single shot approach with supervised, in-domain task-specific models" as a shortcoming
- **Why unresolved**: All comparisons in the paper are against few-shot methods (UniverSeg) and other unsupervised approaches. The fundamental question of whether the benefit of avoiding in-domain fine-tuning comes at the cost of reduced accuracy compared to traditional supervised methods remains unanswered
- **What evidence would resolve it**: Direct performance comparison of the one-shot FM-based method against state-of-the-art supervised segmentation models (e.g., U-Net variants, nnUNet) on identical datasets, using metrics like Dice score, sensitivity, specificity, and inference time

## Limitations

- The method requires template images with positive and negative prompts marked, which are not provided in the paper
- No comparison was made against supervised, in-domain task-specific models to assess the trade-off between avoiding fine-tuning and potential accuracy loss
- Current implementation is limited to 2D slices, with 3D volume segmentation scalability remaining unexplored

## Confidence

- **High confidence**: The overall methodology of using dense feature extraction from foundation models followed by SAM prompting is sound and well-implemented
- **Medium confidence**: The quantitative results showing Dice scores of 62-90% across tasks are valid for the specific datasets tested
- **Low confidence**: The claim that SSL models (DINO v2) consistently outperform supervised models (SAM, CLIP) across all medical imaging tasks lacks strong empirical support in the paper

## Next Checks

1. **Cross-domain robustness test**: Apply the method to a medical imaging dataset with substantially different characteristics (e.g., histopathology or X-ray) to validate generalization beyond the tested CT, MR, and Ultrasound modalities
2. **Ablation study on prompt design**: Systematically vary the positive/negative prompt placement and measure the impact on segmentation accuracy to quantify the sensitivity to prompt design
3. **Statistical significance testing**: Perform paired t-tests comparing Dice scores between the proposed method and UniverSeg across all seven tasks to determine if performance differences are statistically significant