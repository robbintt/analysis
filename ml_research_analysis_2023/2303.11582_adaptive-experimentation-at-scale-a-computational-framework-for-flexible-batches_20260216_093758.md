---
ver: rpa2
title: 'Adaptive Experimentation at Scale: A Computational Framework for Flexible
  Batches'
arxiv_id: '2303.11582'
source_url: https://arxiv.org/abs/2303.11582
tags:
- sampling
- gaussian
- batch
- experiment
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a new adaptive experimentation framework for
  scenarios where reallocation of measurement effort is costly and feedback is delayed.
  The core idea is to use normal approximations of aggregate rewards to formulate
  a Gaussian sequential experiment, which can be modeled as a Markov decision process
  over posterior beliefs.
---

# Adaptive Experimentation at Scale: A Computational Framework for Flexible Batches

## Quick Facts
- arXiv ID: 2303.11582
- Source URL: https://arxiv.org/abs/2303.11582
- Reference count: 40
- Key outcome: RHO policy significantly improves statistical power over standard methods in adaptive experimentation with delayed feedback and costly reallocation

## Executive Summary
This paper develops a computational framework for adaptive experimentation when reallocation of measurement effort is costly and feedback is delayed. The key insight is using normal approximations of aggregate rewards to formulate a tractable Gaussian sequential experiment as a Markov decision process over posterior beliefs. The authors propose Residual Horizon Optimization (RHO), an iterative planning method that solves an open-loop problem using current information, which outperforms exact dynamic programming in both theory and practice. Empirical results demonstrate substantial performance gains over standard methods like Thompson sampling, particularly in underpowered experiments with high measurement noise or many treatment arms.

## Method Summary
The framework formulates adaptive experimentation as a Gaussian sequential experiment using normal approximations of aggregate rewards via the central limit theorem. This enables tractable Bayesian updates and policy optimization through an MDP formulation over posterior beliefs. The core contribution is Residual Horizon Optimization (RHO), which iteratively solves an open-loop planning problem using stochastic gradient ascent. RHO only uses currently available information to plan future sampling allocations, making it computationally efficient while achieving strong empirical performance. The method uses a sample average approximation with Sobol sequence draws for variance reduction and employs Adam optimizer with softmax parameterization.

## Key Results
- RHO achieves the largest performance gains among Bayesian policies using batch Gaussian approximations across diverse settings
- Significant improvements over Thompson sampling, especially in underpowered experiments with high measurement noise
- Performance gains particularly large with many treatment arms and limited reallocation epochs
- Theoretical analysis shows RHO converges to a posterior sampling procedure in the large horizon limit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normal approximations of aggregate rewards allow a Gaussian sequential experiment to replace intractable exact models in adaptive experimentation.
- Mechanism: The central limit theorem ensures that scaled sample means converge to a Gaussian distribution regardless of the underlying reward distribution. This normal approximation can be used to model the likelihood over average rewards, enabling tractable Bayesian updates without knowing the full reward distribution.
- Core assumption: The batch size n is large enough for the CLT to provide a good approximation; the differences in average rewards scale as Θ(1/√n).
- Evidence anchors:
  - [abstract] "By deriving a Gaussian sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards."
  - [section 2.1] "Using successive normal approximations (2) at each epoch, we arrive at a Gaussian sequential experiment that provides an asymptotic approximation to the adaptive experimentation problem."
- Break condition: When the batch size is too small or the reward distributions are extremely heavy-tailed, the normal approximation may break down and lead to poor policies.

### Mechanism 2
- Claim: The Markov decision process formulation enables gradient-based optimization of sampling allocations by making state transitions differentiable.
- Mechanism: The posterior updates over average rewards follow closed-form Gaussian updates (11), making the value function differentiable with respect to sampling allocations. This allows the use of stochastic gradient descent to optimize policies without having to enumerate the combinatorially large action space.
- Core assumption: The posterior distributions remain Gaussian (conjugate prior), and the sampling allocations are continuous functions of the current posterior state.
- Evidence anchors:
  - [section 3.1] "State transitions in the MDP are governed by posterior updates (11) which depend on the continuous actions πt ∈ ∆K. Given this Markovian structure... we restrict attention to sampling allocations πt(µt,σt) that only depend on the current state (µt,σt)."
  - [section 3.2] "Since the states (µt,σt) and actions πt are both continuous, using dynamic programming to directly solve the policy optimization problem... is computationally intractable even for a moderate number of arms and reallocation epochs."
- Break condition: If the prior is non-Gaussian or the likelihood is non-conjugate, the posterior updates may not have a closed form, breaking differentiability.

### Mechanism 3
- Claim: Residual Horizon Optimization (RHO) outperforms exact dynamic programming by solving an open-loop problem that only uses current information.
- Mechanism: RHO iteratively solves a planning problem assuming future allocations are non-adaptive, which is computationally cheaper than full dynamic programming. The resulting policy is guaranteed to outperform any static allocation and performs well empirically across various settings.
- Core assumption: The open-loop planning problem provides a good approximation to the optimal adaptive policy, especially when the number of reallocation epochs is small.
- Evidence anchors:
  - [section 3.2] "Our main proposed method, Residual Horizon Optimization, solves an open-loop problem that optimize future sampling allocations that only depend on currently available information."
  - [section 4] "Among Bayesian policies that utilize batch Gaussian approximations,Residual Horizon Optimization achieves the largest performance gain across a wide variety of settings."
- Break condition: When the number of reallocation epochs is very large, the open-loop assumption may become a poor approximation and exact dynamic programming could outperform RHO.

## Foundational Learning

- Concept: Central Limit Theorem and normal approximations
  - Why needed here: To justify replacing the exact (intractable) distribution of aggregate rewards with a Gaussian distribution, enabling tractable Bayesian updates and policy optimization.
  - Quick check question: If you have a batch of 100 samples from a Bernoulli distribution, what is the approximate distribution of the sample mean?

- Concept: Markov decision processes and dynamic programming
  - Why needed here: To formulate the adaptive experimentation problem as an MDP where states are posterior beliefs and actions are sampling allocations, allowing the use of reinforcement learning and approximate DP methods.
  - Quick check question: In the MDP formulation, what are the state variables and how do they evolve over time?

- Concept: Bayesian updating with conjugate priors
  - Why needed here: To maintain tractable posterior distributions over average rewards, which are needed for the Gaussian MDP formulation and for computing the value function.
  - Quick check question: If you have a Gaussian prior over average rewards and observe a Gaussian likelihood, what is the form of the posterior?

## Architecture Onboarding

- Component map: Normal approximation module -> MDP solver -> RHO planner -> Policy evaluator -> Experiment runner
- Critical path:
  1. Compute normal approximations of aggregate rewards
  2. Set up the MDP over posterior beliefs
  3. Solve the MDP approximately using RHO or other methods
  4. Evaluate the policy using the Bayes simple regret
  5. Deploy the policy in the finite batch problem
- Design tradeoffs:
  - Exact vs. approximate DP: Exact DP is intractable for large problems, so approximate methods like RHO are needed. RHO trades some optimality for computational efficiency.
  - Gaussian vs. non-Gaussian approximations: Gaussian approximations enable tractable updates but may not capture heavy-tailed reward distributions well. Non-Gaussian approximations could be more accurate but less tractable.
  - Prior strength: Stronger priors lead to more confident initial beliefs but may be misspecified. Weaker priors are more robust but may require more data to learn.
- Failure signatures:
  - Poor performance on small batch sizes: If the normal approximation is not accurate for small batches, the policy may perform poorly.
  - Over-exploration in low-noise settings: If the RHO planning problem is not solved accurately, the policy may explore too much when the signal-to-noise ratio is high.
  - Under-exploration in high-noise settings: If the policy does not explore enough in high-noise settings, it may get stuck in suboptimal arms.
- First 3 experiments:
  1. Compare RHO to uniform allocation on a simple problem with known Gaussian rewards and a small number of arms and batches. Verify that RHO outperforms uniform allocation as expected.
  2. Test the normal approximation on a problem with non-Gaussian rewards (e.g., Bernoulli or Gamma-Gumbel). Verify that the approximation is accurate enough for the policy to work well even with non-Gaussian rewards.
  3. Evaluate RHO on a harder problem with many arms and high measurement noise. Verify that RHO still outperforms uniform allocation and other adaptive methods in this more challenging setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the asymptotic results for RHO in the large horizon limit (Propositions 4-6) translate to finite horizon settings with practical batch sizes?
- Basis in paper: The paper analyzes RHO's behavior as the residual horizon T-t grows large and shows it converges to a posterior sampling procedure (Density Thompson Sampling). However, it acknowledges that practical experiments involve only a handful of reallocation epochs.
- Why unresolved: The theoretical analysis focuses on the asymptotic regime where the residual sampling budget grows large, which is not representative of typical experimental settings. The paper doesn't quantify how quickly RHO approaches the limiting behavior or how well the asymptotic predictions match finite horizon performance.
- What evidence would resolve it: Numerical experiments comparing RHO's performance and exploration-exploitation trade-off across different finite horizons, especially focusing on the transition region between small and large horizons.

### Open Question 2
- Question: What is the impact of misspecified priors on the performance of RHO compared to other Bayesian methods like Thompson Sampling?
- Basis in paper: The authors mention that RHO uses a Gaussian prior to approximate the true prior distribution and achieves larger performance gains compared to Thompson sampling policies that use the true prior. However, they don't explore how robust RHO is to prior misspecification.
- Why unresolved: While the paper shows RHO can effectively incorporate prior information, it doesn't investigate how sensitive the method is when the assumed prior distribution differs from the true distribution. This is crucial for practical applications where prior information may be uncertain.
- What evidence would resolve it: Empirical comparisons of RHO and Thompson sampling under various degrees of prior misspecification, including scenarios where the true prior has different moments or distribution family than the assumed Gaussian prior.

### Open Question 3
- Question: How does RHO perform in settings with contextual information or when treatment effects vary across subgroups?
- Basis in paper: The paper focuses on a simplified setting with K treatment arms and no contextual information. The authors mention that their framework can be extended to incorporate additional objectives and constraints, but don't explore more complex experimental designs.
- Why unresolved: Modern experimentation platforms often involve contextual bandit problems where treatment effects vary across subgroups or depend on observed features. The paper's MDP formulation and RHO algorithm would need to be extended to handle these more complex scenarios.
- What evidence would resolve it: Extensions of the MDP framework to handle contextual information, with empirical validation showing how RHO performs in contextual bandit settings with varying treatment effects across subgroups.

## Limitations

- Performance critically depends on normal approximations, which may break down for small batch sizes or heavy-tailed distributions
- Computational cost scales poorly with number of arms and epochs, limiting large-scale applicability
- Theoretical analysis focuses on asymptotic behavior with limited finite-horizon guarantees
- Assumes known measurement variances which may not hold in practice

## Confidence

- High confidence in theoretical framework and asymptotic analysis
- Medium confidence in empirical performance claims given specific experimental settings
- Medium confidence in generalizability across different reward distributions

## Next Checks

1. Test the sensitivity of RHO to batch size by systematically varying n and measuring performance degradation
2. Evaluate the framework on reward distributions with heavy tails (e.g., Pareto or Cauchy) to assess the limits of normal approximations
3. Compare the computational efficiency of RHO against exact dynamic programming for small-scale problems to quantify the tradeoff between optimality and tractability