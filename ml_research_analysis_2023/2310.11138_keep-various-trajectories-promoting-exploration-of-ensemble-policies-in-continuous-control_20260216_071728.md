---
ver: rpa2
title: 'Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous
  Control'
arxiv_id: '2310.11138'
source_url: https://arxiv.org/abs/2310.11138
tags:
- teen
- ensemble
- learning
- policy
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TEEN, a new ensemble reinforcement learning
  algorithm that promotes diverse exploration by maximizing the expected return while
  encouraging diverse trajectories. The key idea is to measure the diversity of trajectories
  using the discrepancy between the state-action visit distributions of sub-policies.
---

# Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control

## Quick Facts
- arXiv ID: 2310.11138
- Source URL: https://arxiv.org/abs/2310.11138
- Reference count: 40
- Primary result: Achieves 41% improvement over baseline ensemble DRL algorithms across tested continuous control environments

## Executive Summary
This paper introduces TEEN, an ensemble reinforcement learning algorithm that improves exploration by promoting diversity among sub-policies. TEEN addresses a key limitation in existing ensemble methods where sub-policies converge to similar behaviors, limiting their exploration potential. The approach uses KL-divergence to measure behavioral differences between sub-policies and regularizes them to maintain diverse trajectories. Experimental results show TEEN significantly outperforms baseline ensemble DRL algorithms, achieving a 41% improvement in performance on average across tested MuJoCo and DMControl environments.

## Method Summary
TEEN extends TD3 by maintaining an ensemble of N sub-policies and Q-functions, with each sub-policy trained to maximize both expected return and diversity from other sub-policies. The algorithm uses KL-divergence between state-action visit distributions to measure diversity, implements bias control in ensemble Q-learning through minimum-over-M and mean-over-N operators, and employs recurrent training where individual sub-policies are updated periodically to prevent premature convergence. A discriminator network estimates the posterior distribution over sub-policy identity given state-action pairs, enabling regularization based on behavioral dissimilarity.

## Key Results
- Achieves 41% improvement in performance over baseline ensemble DRL algorithms
- Demonstrates significantly more diverse trajectory sampling compared to standard ensemble methods
- Shows consistent performance gains across multiple MuJoCo and DMControl environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ensemble policy diversity is bounded by the entropy of the ensemble's state-action visit distribution, which decomposes into sub-policy discrepancy and sub-policy diversity components.
- **Mechanism:** TEEN enforces diversity by maximizing the KL-divergence between each sub-policy's state-action visit distribution and the ensemble's overall distribution, ensuring that sub-policies visit distinct regions of the state-action space.
- **Core assumption:** The KL-divergence between conditioned state-action visit distributions directly measures behavioral diversity and correlates with improved exploration efficiency.
- **Evidence anchors:**
  - [abstract] "Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be."
  - [section] "Lemma 1 (Ensemble Sample Diversity Decomposition) Given the state-action visit distribution of the ensemble policy ρ. The entropy of this distribution is H(ρ). Notice that this term can be decomposed into two parts: H(ρ) = Ez[DKL(ρ(s, a|zk)||ρ(s, a))] + H(ρ|z)"
  - [corpus] Weak - no direct citations in corpus neighbors about KL-divergence decomposition of policy diversity.
- **Break condition:** If the KL-divergence measure fails to capture meaningful behavioral differences (e.g., due to similar reward landscapes), diversity enforcement may not translate to exploration gains.

### Mechanism 2
- **Claim:** Controlling overestimation bias in ensemble Q-learning improves performance by combining minimum over a subset of Q-functions with averaging across sub-policies.
- **Mechanism:** TEEN computes the target Q-value as the minimum over M randomly selected Q-functions of the mean across N sub-policies, reducing both bias and variance in value estimation.
- **Core assumption:** The order statistics of Q-value ensembles follow predictable patterns that allow bias-variance tradeoffs through parameter tuning (M and N).
- **Evidence anchors:**
  - [section] "Theorem 1. Let X1, X2, ..., XN be an infinite sequence of i.i.d. random variables with a probability density function (PDF) of f(x) and a cumulative distribution function (CDF) of F (x). Denote µ = E[Xi] and σ2 = V[Xi]. Let X1:N ≤ X2:N ≤ X3:N ... ≤ XN:N be the order statistics corresponding to {Xi}N. Denote PDF and CDF of the k-th order statistic Xk:N as fk:N and Fk:N respectively."
  - [abstract] "TEEN outperforms the baseline ensemble DRL algorithms by 41% in performance on the tested representative environments."
  - [corpus] Weak - corpus neighbors focus on exploration but not specifically on ensemble Q-learning bias control mechanisms.
- **Break condition:** If the Q-value distribution is heavy-tailed or non-stationary, the order statistics-based bias control may become unreliable.

### Mechanism 3
- **Claim:** Recurrent training of individual sub-policies prevents exploration degradation by avoiding premature convergence to similar behaviors.
- **Mechanism:** Instead of updating all sub-policies simultaneously, TEEN randomly selects one sub-policy every T episodes for regularization, maintaining diversity over long training horizons.
- **Core assumption:** Parallel updates of multiple sub-policies lead to correlated behavior patterns that reduce ensemble diversity over time.
- **Evidence anchors:**
  - [section] "Recurrent Optimization. Updating all sub-policies every time step in parallel may suffer from the exploration degradation problem [35]."
  - [section] "To tackle this challenge, we employ a recurrent training method for our sub-policies. In particular, given an ensemble of N sub-policies, we randomly select a single sub-policy, denoted as πk, every T episodes."
  - [corpus] Weak - no direct corpus citations about recurrent training for ensemble diversity maintenance.
- **Break condition:** If T is set too large, the selected sub-policy may drift too far from optimal performance before the next update cycle.

## Foundational Learning

- **Concept:** KL-divergence as a measure of distribution dissimilarity
  - **Why needed here:** TEEN uses KL-divergence to quantify behavioral differences between sub-policies' state-action visit distributions
  - **Quick check question:** What property of KL-divergence makes it suitable for measuring policy diversity in continuous control tasks?

- **Concept:** Order statistics and their application to ensemble Q-learning
  - **Why needed here:** Theorem 1 provides theoretical justification for how minimum and mean operators control bias and variance in ensemble Q-value estimation
  - **Quick check question:** How does the expected value of the minimum order statistic relate to the overall mean in the context of Q-value estimation?

- **Concept:** Shannon entropy decomposition in information theory
  - **Why needed here:** Lemma 1 shows how the entropy of ensemble state-action distribution decomposes into sub-policy discrepancy and diversity components
  - **Quick check question:** What does the term H(ρ|z) represent in the context of ensemble policy entropy decomposition?

## Architecture Onboarding

- **Component map:** Sub-policies (TD3 actors) -> Q-function networks -> Discriminator network -> Ensemble policy selector -> Replay buffer -> Target networks
- **Critical path:**
  1. Sample action using randomly selected sub-policy
  2. Store transition in replay buffer
  3. Sample minibatch and compute target Q-values using min-over-M and mean-over-N
  4. Update Q-functions via gradient descent
  5. Update selected sub-policy with trajectory-aware regularization
  6. Update discriminator to approximate posterior behavior distribution
  7. Update target networks
- **Design tradeoffs:**
  - Ensemble size N vs computational cost (TEEN uses N=10)
  - Target value subset size M vs bias-variance tradeoff (TEEN uses M=2)
  - Regularization weight α vs exploration-exploitation balance (environment-dependent)
  - Recurrent interval T vs diversity maintenance vs convergence speed
- **Failure signatures:**
  - Performance plateaus with low diversity metrics (KL-divergence stagnation)
  - High variance in Q-value estimates despite bias control
  - Sub-policies converge to similar behaviors despite regularization
  - Discriminator fails to distinguish sub-policy behaviors (uniform qζ predictions)
- **First 3 experiments:**
  1. Ablation study: Compare TEEN with TD3 using identical hyperparameters except for the KL-divergence regularization term
  2. Sensitivity analysis: Vary α parameter across [0.0, 0.2, 0.5, 1.0] to identify optimal exploration-exploitation balance
  3. Order statistics validation: Plot empirical Q-value distributions against theoretical bounds from Theorem 1 to verify bias control effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TEEN perform in more complex continuous control environments beyond those tested?
- Basis in paper: [inferred] The paper only tests TEEN on a limited set of MuJoCo and DMControl environments. It would be valuable to see performance on more challenging environments like Humanoid-v3 or harder DMControl tasks.
- Why unresolved: The authors only evaluate TEEN on a small subset of benchmark environments. Performance on more complex tasks could reveal limitations or advantages not apparent in the tested environments.
- What evidence would resolve it: Additional experimental results showing TEEN's performance on a wider range of continuous control environments, including very complex ones like Humanoid-v3, would help assess its broader applicability.

### Open Question 2
- Question: How sensitive is TEEN's performance to its hyperparameters, particularly the weight parameter α and the number of target values M?
- Basis in paper: [explicit] The paper discusses hyperparameter sensitivity, especially the effect of α and M on performance, but only provides results for a limited set of values.
- Why unresolved: The paper only explores a narrow range of hyperparameter values and doesn't provide a comprehensive sensitivity analysis. It's unclear how TEEN's performance would be affected by other choices of these parameters.
- What evidence would resolve it: A thorough ablation study varying α and M across a wider range of values, along with analysis of how these choices affect performance, would clarify TEEN's sensitivity to its hyperparameters.

### Open Question 3
- Question: How does TEEN's sample efficiency compare to other state-of-the-art exploration methods in terms of the number of samples required to reach a certain performance threshold?
- Basis in paper: [inferred] While the paper claims TEEN improves sample efficiency, it doesn't directly compare the number of samples needed to reach specific performance levels against other methods.
- Why unresolved: The paper focuses on final performance but doesn't provide a detailed comparison of sample efficiency. This makes it difficult to assess TEEN's practical advantages in terms of data requirements.
- What evidence would resolve it: Direct comparison of sample efficiency metrics (e.g., samples needed to reach 90% of final performance) between TEEN and other exploration methods would provide a clearer picture of its practical advantages.

## Limitations

- Computational overhead: Requires maintaining 10 sub-policies and additional discriminator network, increasing computational cost by approximately 10× compared to standard TD3
- Hyperparameter sensitivity: Performance depends on careful tuning of multiple parameters (α, M, T, N) which may not transfer well across different environments
- Limited scalability: The ensemble approach may not scale effectively to very high-dimensional state-action spaces or extremely complex environments

## Confidence

- KL-divergence mechanism for exploration: **Medium** - mathematically sound decomposition but empirical validation relies heavily on performance metrics rather than direct exploration quality measurement
- Bias-variance control through order statistics: **High** - rigorous theoretical analysis provided in Theorem 1 with clear mathematical foundation
- Recurrent training approach: **Medium** - theoretical motivation is sound but limited ablation studies on the interval parameter T

## Next Checks

1. Conduct a scaling study to measure how TEEN's performance improvement changes with ensemble size N, determining if the 10-policy ensemble is optimal or if smaller ensembles achieve comparable results with reduced computational cost
2. Perform direct measurement of exploration quality by analyzing the state visitation entropy of TEEN versus baseline methods across different training stages, rather than relying solely on final performance metrics
3. Test TEEN on sparse-reward environments where exploration is critical to determine if the trajectory diversity benefits translate to scenarios where standard exploration bonuses fail