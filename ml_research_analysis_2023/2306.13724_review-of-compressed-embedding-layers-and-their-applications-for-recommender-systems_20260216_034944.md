---
ver: rpa2
title: Review of compressed embedding layers and their applications for recommender
  systems
arxiv_id: '2306.13724'
source_url: https://arxiv.org/abs/2306.13724
tags:
- embedding
- accuracy
- layers
- samples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews methods for compressing large embedding layers
  in recommender systems and proposes a new decomposed embedding layer approach. The
  authors introduce a trainable compressed embedding layer that replaces enormous
  embedding layers with significantly smaller, decomposed ones without changing network
  hyperparameters, preserving accuracy while reducing memory usage.
---

# Review of compressed embedding layers and their applications for recommender systems

## Quick Facts
- arXiv ID: 2306.13724
- Source URL: https://arxiv.org/abs/2306.13724
- Reference count: 21
- Key outcome: Proposes decomposed embedding layers that reduce memory usage by ~80% while maintaining accuracy

## Executive Summary
This paper addresses the challenge of deploying large recommender systems by introducing a compressed embedding layer approach. The authors propose replacing traditional embedding layers with a Factorized Parameterized Decomposition (FPD) layer that uses tensor contractions to achieve significant memory savings. Experiments on DLRM with the Criteo-1TB dataset demonstrate that replacing the five largest embedding layers reduces total size from 15.4GB to around 3GB while maintaining similar accuracy (0.8033 vs 0.8030 AUC). The approach also enables much higher throughput on modern GPUs compared to the uncompressed model.

## Method Summary
The paper introduces a trainable compressed embedding layer that replaces traditional embedding tables with decomposed representations using tensor contractions. The decomposed embedding layer uses tensor decomposition to reduce storage from O(d × n) to O(d × r × p), where r and p are decomposition ranks. The FPD layer is integrated into the DLRM architecture by replacing the five largest embedding layers. The model is trained end-to-end using the Criteo-1TB dataset, with the decomposed layers learning optimal decompositions that preserve task-relevant information while significantly reducing memory footprint.

## Key Results
- Memory reduction from 15.4GB to ~3GB by replacing five largest embedding layers
- Minimal accuracy impact (0.8033 vs 0.8030 AUC) with decomposed layers
- 78-83 million samples/s throughput on 8xH100 GPUs vs 3.7 million samples/s for uncompressed model
- No hyperparameter changes required for the rest of the network

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing large embedding layers with decomposed layers reduces memory footprint while preserving accuracy
- Mechanism: The decomposed embedding layer uses tensor contraction instead of storing full embedding tables, reducing storage from O(d × n) to O(d × r × p) where r and p are decomposition ranks
- Core assumption: The embedding matrix can be approximated by low-rank decomposition without significant loss of information relevant to the recommendation task
- Evidence anchors:
  - [abstract] "replacing enormous embedding layers with significantly smaller, decomposed ones without changing network hyperparameters, preserving accuracy while reducing memory usage"
  - [section] "we have replaced some of the embedding layers in both NCF and DLRM... replacing the five largest embedding layers in DLRM with the FPD layer"
  - [corpus] Weak corpus evidence - related papers focus on general recommender system topics, not compressed embeddings specifically
- Break condition: When the embedding matrix has low intrinsic dimensionality or when r and p are set too small to capture necessary relationships

### Mechanism 2
- Claim: The decomposed approach enables better GPU utilization by converting memory-bound operations to compute-bound operations
- Mechanism: Instead of memory-heavy lookups from large embedding tables, the decomposed layer performs tensor contractions that utilize GPU compute units more efficiently
- Core assumption: Modern GPUs are more compute-bound than memory-bound for typical recommendation workloads
- Evidence anchors:
  - [abstract] "achieving much higher throughput compared to the original model, with 78-83 million samples/s throughput on 8xH100 GPUs versus 3.7 million samples/s for the uncompressed model"
  - [section] "it gives us huge throughput gains when using modern GPUs, since recommenders typically leave efficient GPUs starving for compute tasks"
  - [corpus] No direct corpus evidence for this specific GPU utilization claim
- Break condition: When GPU memory bandwidth becomes the bottleneck or when the tensor contractions become too complex relative to the gains

### Mechanism 3
- Claim: The FPD layer can be trained end-to-end without accuracy degradation
- Mechanism: The decomposed layer is trained alongside the rest of the network, allowing it to learn an optimal decomposition that preserves task-relevant information
- Core assumption: The network can learn to compensate for any information loss from decomposition through continued training
- Evidence anchors:
  - [abstract] "with minimal impact on accuracy (0.8033 vs 0.8030 AUC)"
  - [section] "we have introduced our FPD layer as a replacement of native PyTorch embeddings" and "it is possible to replace enormous embedding layers with significantly smaller... without changing the hyperparameters of the network"
  - [corpus] No corpus evidence for this specific training approach
- Break condition: When the decomposition cannot capture essential features of the data, or when training becomes unstable due to the additional complexity

## Foundational Learning

- Concept: Embedding layers in recommender systems
  - Why needed here: Understanding that these layers map sparse categorical features to dense vectors is fundamental to grasping why they become memory bottlenecks
  - Quick check question: Why do recommender systems typically have such large embedding tables compared to other deep learning applications?

- Concept: Tensor decomposition and low-rank approximations
  - Why needed here: The core innovation relies on representing large embedding matrices as products of smaller matrices
  - Quick check question: What mathematical property of embedding matrices makes them amenable to low-rank decomposition?

- Concept: GPU memory hierarchy and compute-bound vs memory-bound operations
  - Why needed here: The performance gains come from shifting from memory-bound to compute-bound operations
  - Quick check question: How does converting memory lookups to tensor contractions change the GPU's bottleneck?

## Architecture Onboarding

- Component map: Input features → FPD layer (tensor contractions) → Interaction layers → Output prediction
- Critical path: Input features → FPD layer (tensor contractions) → Interaction layers → Output prediction
- Design tradeoffs: Memory vs compute (larger r,p increases accuracy but also memory and compute), training time vs inference speed, complexity of implementation vs performance gains
- Failure signatures: Accuracy degradation when r,p too small, training instability, unexpected memory usage patterns, latency increases despite theoretical speedups
- First 3 experiments:
  1. Replace one small embedding layer with FPD using default r=8, p=1 and compare accuracy/memory
  2. Vary r parameter while keeping p=1 to find sweet spot for a specific embedding layer
  3. Benchmark inference latency and throughput on a single GPU before and after replacement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different tensor train rank values affect accuracy and memory usage in practice?
- Basis in paper: [explicit] The paper mentions that the size of the tensor-train embedding and the extent of the accuracy degradation depend on the hyperparameters of the tensor-train algorithm, but does not discuss any kind of heuristic for choosing the hyperparameters.
- Why unresolved: The paper states that choosing the right hyperparameters is left to the user and does not provide guidance on how to select them.
- What evidence would resolve it: Experimental results showing accuracy and memory usage for various tensor train rank values across different datasets and recommender system architectures.

### Open Question 2
- Question: How does the proposed decomposed embedding layer compare to other compression methods in terms of training speed and convergence?
- Basis in paper: [inferred] The paper mentions that their method requires more compute power but comes with more guarantees, leading to faster training with fewer epochs. However, it does not provide a direct comparison to other compression methods.
- Why unresolved: The paper does not compare the training speed and convergence of their method to other compression techniques.
- What evidence would resolve it: A comprehensive comparison of training time, convergence rate, and final accuracy across multiple compression methods on the same datasets and recommender system architectures.

### Open Question 3
- Question: Can the proposed decomposed embedding layer be effectively used with other neural network architectures beyond DLRM?
- Basis in paper: [inferred] The paper mentions that their design choices were made to future-proof their technology and that it might work well with other neural networks, but this is not experimentally verified.
- Why unresolved: The paper only tests their method on DLRM and does not explore its applicability to other architectures.
- What evidence would resolve it: Experimental results showing the performance of the decomposed embedding layer on various neural network architectures (e.g., transformers, LSTMs) for different tasks (e.g., NLP, computer vision) and datasets.

## Limitations
- Implementation details of the decomposed embedding layer are not fully specified
- Limited hyperparameter exploration for decomposition ranks (r, p)
- No ablation studies showing performance impact of different r, p combinations
- No statistical significance testing on accuracy differences

## Confidence
- **High Confidence**: Memory reduction claims (15.4GB → 3GB) based on clear mathematical principles
- **Medium Confidence**: Accuracy preservation claims, as results show minimal AUC drop but no statistical significance testing
- **Medium Confidence**: Throughput improvement claims, though the mechanism is sound and GPU utilization data supports this

## Next Checks
1. **Ablation study**: Systematically vary r and p parameters across multiple embedding layers to identify optimal configurations and understand accuracy/compute tradeoffs
2. **Statistical validation**: Perform significance testing on AUC differences between original and compressed models using confidence intervals
3. **GPU profiling**: Conduct detailed profiling of memory bandwidth utilization and compute utilization comparing original vs compressed implementations across different GPU architectures