---
ver: rpa2
title: Testing the Segment Anything Model on radiology data
arxiv_id: '2312.12880'
source_url: https://arxiv.org/abs/2312.12880
tags:
- segmentation
- masks
- data
- segment
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We tested the Segment Anything Model (SAM) on zero-shot segmentation
  of anatomical structures in MRI data. We ran SAM inference on 2D slices from 8 segmentation
  tasks, including prostate, heart, spleen, and colon.
---

# Testing the Segment Anything Model on radiology data

## Quick Facts
- arXiv ID: 2312.12880
- Source URL: https://arxiv.org/abs/2312.12880
- Authors: 
- Reference count: 40
- We tested the Segment Anything Model (SAM) on zero-shot segmentation of anatomical structures in MRI data. We ran SAM inference on 2D slices from 8 segmentation tasks, including prostate, heart, spleen, and colon. Using different heuristics to select predicted masks, we found that SAM achieves acceptable performance in limited cases (e.g. Dice=65% for left atrium), but overall underperforms, with Dice scores as low as 4.2% for colon cancer. Seeding SAM with points did not improve results. While SAM may provide acceptable segmentations for a few specific slices, it is insufficient for robust zero-shot segmentation across entire MRI volumes.

## Executive Summary
The Segment Anything Model (SAM) was evaluated on zero-shot segmentation of anatomical structures in MRI data across 8 tasks. The model was tested on 2D slices from prostate, heart, spleen, colon, and other segmentation datasets. Using various heuristics to select predicted masks, SAM achieved acceptable performance only in limited cases (Dice=65% for left atrium), with most tasks showing poor results (Dice as low as 4.2% for colon cancer). Even with seeded prompting using points, performance did not improve. While SAM may work for specific slices, it is insufficient for robust zero-shot segmentation across entire MRI volumes.

## Method Summary
SAM was applied to 2D MRI slices from 8 segmentation tasks including prostate, heart, spleen, and colon. MRI data was normalized to 0-255 range and converted from single-channel to three-channel format by repeating values across RGB channels. SAM inference was run on each slice using different heuristics to select predicted masks: all masks, best masks, masks with IoU>0.25, and masks with IoU>0.5. Performance was evaluated using Dice scores, intersection over union (IoU), and fraction of detected objects, with comparisons made to ground truth segmentations.

## Key Results
- SAM achieved Dice=65% for left atrium segmentation but Dice as low as 4.2% for colon cancer
- Median Dice scores ranged from 5.7% (prostate) to 89.3% (spleen)
- Seeded prompting with points did not improve segmentation performance
- Performance varied significantly across different anatomical structures and MRI protocols

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM's poor performance on MRI segmentation is due to domain shift between natural images and medical imaging.
- Mechanism: SAM was trained on natural images with RGB channels and specific intensity distributions, while MRI data uses single-channel intensity encodings specific to acquisition parameters. This mismatch prevents SAM from recognizing anatomical structures.
- Core assumption: The visual features SAM learned from natural images do not transfer well to the intensity-based representations in MRI data.
- Evidence anchors:
  - [abstract] "MRI data is three-dimensional data with a single channel" vs. "SAM is a two-dimensional model trained on three channel (RGB) data"
  - [section] "individual voxel values in radiology data are encoded as values which depend on the particular image acquisition process"
  - [corpus] "How to build the best medical image segmentation algorithm using foundation models" - indicates need for adaptation
- Break condition: If SAM were trained on diverse medical imaging modalities, the domain shift would be reduced.

### Mechanism 2
- Claim: SAM's seed-based prompting does not improve performance on MRI segmentation because the seeds do not provide meaningful spatial context.
- Mechanism: When SAM uses seeds (point prompts), it still relies on learned visual features to expand from those points. If those features don't match MRI characteristics, the expansion fails regardless of seed placement.
- Core assumption: Seeds alone cannot compensate for fundamental feature mismatches between training and target domains.
- Evidence anchors:
  - [section] "Using these points, we prompt SAM to obtain prostate gland masks. We observe that this fails to improve Dice scores"
  - [section] "seeded SAM (Table 2). This suggests that, for radiology, SAM benefits more from having a more aggressive seeding mechanism"
  - [corpus] "Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM)" - suggests limited success in medical imaging
- Break condition: If seeds were combined with modality-specific feature adaptation, performance might improve.

### Mechanism 3
- Claim: SAM's zero-shot performance is acceptable only when there is high structural contrast and simple geometry in the target anatomy.
- Mechanism: SAM can identify objects in MRI when they have distinct boundaries and simple shapes that happen to align with features SAM learned from natural images (like circular/elliptical shapes).
- Core assumption: Some anatomical structures may coincidentally match patterns SAM learned, even in a different domain.
- Evidence anchors:
  - [section] "the best performance is observed for the segmentation of the left atrium in heart MRI data (Dice = 65%)"
  - [section] "SAM achieves relatively good median performances (Dicepos = 95% and Dicepos = 90% for spleen and left atrium segmentation, respectively)"
  - [corpus] "Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected" - suggests SAM works better on objects with clear boundaries
- Break condition: If applied to complex anatomical structures with subtle boundaries, performance would degrade significantly.

## Foundational Learning

- Concept: Domain adaptation in computer vision
  - Why needed here: Understanding why foundation models trained on natural images fail on medical imaging requires knowledge of how domain shifts affect model performance.
  - Quick check question: What are the key differences between natural image distributions and medical image distributions that could cause a foundation model to fail?

- Concept: Zero-shot learning evaluation metrics
  - Why needed here: The paper uses specific metrics (Dice score, IoU) to evaluate SAM's performance, requiring understanding of how these metrics work and their limitations.
  - Quick check question: How does the Dice coefficient differ from IoU, and why might one be preferred over the other for medical image segmentation?

- Concept: Foundation model limitations and capabilities
  - Why needed here: To understand the gap between SAM's impressive performance on natural images and its poor performance on MRI data, one needs to understand the theoretical limits of foundation models.
  - Quick check question: What factors determine whether a foundation model can successfully transfer to a new domain without fine-tuning?

## Architecture Onboarding

- Component map:
  - SAM model (image encoder, prompt decoder, mask decoder)
  - MRI preprocessing pipeline (normalization, channel stacking)
  - Inference wrapper (handling 3D volumes as 2D slices)
  - Evaluation metrics calculator (Dice, IoU)
  - Heuristics selector (choosing masks based on IoU thresholds)

- Critical path:
  1. Load MRI volume
  2. Normalize and convert to compatible format
  3. Iterate through slices and run SAM inference
  4. Collect predicted masks
  5. Apply heuristics to select relevant masks
  6. Calculate evaluation metrics

- Design tradeoffs:
  - Processing 3D volumes as 2D slices loses spatial context but enables use of 2D model
  - Using all three RGB channels with identical data preserves compatibility but doesn't add information
  - Generous evaluation with ground truth information doesn't reflect real-world use but helps understand potential

- Failure signatures:
  - Very low Dice scores across all tasks
  - High fraction of slices with no detections
  - Poor performance even with seeded prompts
  - Performance doesn't improve with different heuristics

- First 3 experiments:
  1. Run SAM on a simple MRI dataset with clear anatomical structures (like left atrium) to establish baseline
  2. Test different preprocessing approaches (histogram equalization, contrast enhancement) to see if image quality affects performance
  3. Compare SAM's performance with a simple U-Net trained on the same MRI data to quantify the domain gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to SAM would enable effective zero-shot segmentation on medical imaging modalities like MRI?
- Basis in paper: [explicit] The paper notes that "SAM-based foundational models by others have shown potential in this field [27, 25]" but performance is still lacking compared to domain-specific models, suggesting that modifications are needed.
- Why unresolved: While the paper demonstrates SAM's limitations on MRI data, it does not explore what specific architectural changes or training strategies could overcome these limitations.
- What evidence would resolve it: Experiments comparing different SAM modifications (e.g., domain-specific pre-training, architectural changes, prompt engineering) on the same MRI datasets used in this paper, showing which modifications lead to improved performance.

### Open Question 2
- Question: Are there specific MRI imaging characteristics or anatomical structures that SAM can segment effectively in a zero-shot manner?
- Basis in paper: [explicit] The paper notes that SAM achieved relatively good performance (Dice=65%) for left atrium segmentation in heart MRI, suggesting some success cases exist.
- Why unresolved: The paper only tested SAM on 8 segmentation tasks and found mostly poor performance, but did not systematically investigate which specific conditions might enable better results.
- What evidence would resolve it: A comprehensive study testing SAM on a much larger variety of MRI datasets, anatomical structures, and imaging parameters to identify specific conditions where zero-shot performance is acceptable.

### Open Question 3
- Question: How does SAM's performance on MRI data compare to other foundation models or zero-shot segmentation approaches specifically designed for medical imaging?
- Basis in paper: [inferred] The paper notes that "other recently developed adaptations of SAM, such as those by Ma and Wang [27] or by Zhang and Liu [25] have shown potential in this field" and mentions other zero-shot methods in the literature, but does not directly compare them.
- Why unresolved: The paper focuses solely on SAM's performance and does not benchmark it against alternative approaches that might be better suited for medical imaging.
- What evidence would resolve it: Head-to-head comparisons of SAM versus other foundation models and zero-shot segmentation methods on the same MRI datasets, using the same evaluation metrics.

## Limitations
- The evaluation uses ground truth information for mask selection, which inflates performance beyond practical zero-shot scenarios
- The single-channel to three-channel conversion (repeating MRI intensity values across RGB channels) is a pragmatic but potentially suboptimal approach
- Only 2D slice-based inference was tested, missing potential 3D context that could improve segmentation

## Confidence
- High Confidence: SAM's poor zero-shot performance on MRI segmentation is primarily due to domain shift between natural images and medical imaging data
- Medium Confidence: SAM's performance is acceptable only for specific anatomical structures with high structural contrast and simple geometry
- Low Confidence: Seeded prompts do not improve performance because seeds lack meaningful spatial context

## Next Checks
1. Test SAM on MRI data from different scanners and protocols to determine if performance correlates with image quality or acquisition parameters rather than anatomical structure
2. Fine-tune SAM on a small subset of MRI data (e.g., 10-20 cases) to establish an upper bound on performance and quantify the domain adaptation gap
3. Test alternative preprocessing strategies including contrast-limited adaptive histogram equalization and modality-specific normalization to determine if image enhancement can bridge the domain gap without model adaptation