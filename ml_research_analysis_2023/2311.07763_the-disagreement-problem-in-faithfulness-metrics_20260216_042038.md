---
ver: rpa2
title: The Disagreement Problem in Faithfulness Metrics
arxiv_id: '2311.07763'
source_url: https://arxiv.org/abs/2311.07763
tags:
- explanations
- metrics
- faithfulness
- methods
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of choosing faithfulness metrics
  for post-hoc feature attribution methods in explainable AI (XAI). The authors compare
  three existing metrics (PGI, PGU, and area under the ablation curve) with two novel
  metrics (bottleneck distance and ablation) on synthetic and real tabular classification
  datasets.
---

# The Disagreement Problem in Faithfulness Metrics

## Quick Facts
- arXiv ID: 2311.07763
- Source URL: https://arxiv.org/abs/2311.07763
- Authors: 
- Reference count: 35
- Key outcome: Faithfulness metrics for post-hoc feature attribution methods in explainable AI disagree on which explanations are most faithful, even on synthetic datasets with ground truth.

## Executive Summary
This paper investigates the critical problem of choosing faithfulness metrics for post-hoc feature attribution methods in explainable AI (XAI). The authors systematically compare three existing metrics (PGI, PGU, and area under the ablation curve) with two novel metrics (bottleneck distance and ablation) across synthetic and real tabular classification datasets. Their experiments reveal that these metrics produce significantly different rankings of explanation faithfulness, with low rank correlations between them. The findings suggest that current faithfulness metrics do not provide reliable guidance for practitioners, and that more research is needed to understand the assumptions and contexts behind each metric.

## Method Summary
The authors evaluate faithfulness metrics using both synthetic datasets (where ground truth explanations are known) and four real tabular datasets (Adult, German Credit, HAR, Spambase). They generate explanations using Deep SHAP, KernelSHAP, and Integrated Gradients methods with multiple baselines (constant median, training, opposite class, nearest neighbor) plus random explanations. The faithfulness metrics compared include PGI, PGU, area under ablation curve, bottleneck distance, and ablation. Rank correlations between metrics are computed using Spearman's ρ, weighted Kendall-τ, and Kendall's τ to assess agreement. The synthetic dataset allows controlled experiments where explanations can be generated with known levels of faithfulness.

## Key Results
- On the synthetic dataset, different faithfulness metrics disagree on which explanations are most faithful, even when ground truth is available
- PGI ranks random explanations as more faithful than the constant median baseline, while ablation and bottleneck distance rank them correctly
- Rank correlations between metrics on real datasets are generally low, indicating fundamental disagreement
- ABC is the only metric showing strictly monotonic behavior in controlled experiments with varying explanation quality
- The choice of perturbation method and parameters significantly impacts metric rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different faithfulness metrics capture different conceptual notions of "faithfulness," leading to disagreement in rankings.
- Mechanism: Faithfulness is not a single well-defined property but a composite of multiple desiderata. PGI focuses on prediction gap when important features are perturbed, ABC measures area under the ablation curve to assess sensitivity to perturbation ordering, and BND uses topological similarity of explanation manifolds. Each metric optimizes for a different aspect of explanation quality.
- Core assumption: The underlying ground truth for explanation faithfulness is multi-dimensional rather than uni-dimensional.
- Evidence anchors:
  - [abstract] "We have many faithfulness metrics, but they do not correlate well with one another."
  - [section 4.1] "Of the three metrics, only the ABC metric matches our expectation. The BND metric is the next closest to expectations—but it is not strictly monotonic for fractions up to 0.5; while the PGI metric is not monotonic."
  - [corpus] Weak evidence - related papers discuss disagreement but don't provide empirical evidence of metric divergence.
- Break condition: If a unified theoretical framework for faithfulness is developed that encompasses all metrics, or if one metric consistently outperforms others across all contexts.

### Mechanism 2
- Claim: The choice of perturbation method and parameters significantly impacts the resulting faithfulness rankings, causing metrics to disagree.
- Mechanism: PGI and ABC both use perturbation-based approaches but differ in how they measure the effect. PGI uses Gaussian perturbation for continuous features and a flip percentage for discrete features, while ABC uses area under the ablation curve. The choice of perturbation method (Gaussian vs marginal), the number of features to perturb (k), and treatment of discrete features all affect the outcome.
- Core assumption: The perturbation process itself introduces variability that is not accounted for in the metric design.
- Evidence anchors:
  - [section 4.3] "In Figure 4, we contrast PGI calculated with a Gaussian perturbation (left) and with a marginal perturbation (right). For a fixed value of k (shown with a vertical dashed line), higher values of PGI indicate higher faithfulness. It is evident that the ranking of the baselines between the two figures are dissimilar."
  - [section 4.3] "Choice of top k: how to select k is not theoretically motivated. We see in Figure 4 that choice of k can impact the faithfulness rankings of a set of explanations based on PGI."
  - [corpus] No direct evidence - related papers don't discuss perturbation parameter sensitivity.
- Break condition: If perturbation parameters are standardized across all metrics, or if non-perturbation metrics consistently align with the best perturbation-based approach.

### Mechanism 3
- Claim: The topological data analysis (TDA) approach measures a fundamentally different property than perturbation-based metrics, leading to disagreement.
- Mechanism: BND uses mapper networks and bottleneck distance to measure topological similarity between explanation manifolds. This approach doesn't rely on perturbations and instead treats explanations as geometric objects. The metric measures how similar the topological structure of explanations is, not how well they predict model behavior when features are changed.
- Core assumption: Topological similarity is a valid proxy for faithfulness, even though it doesn't directly measure prediction gap or ablation effects.
- Evidence anchors:
  - [section 3.3] "Bottleneck distance (BND) is a similarity metric to compare two manifolds, with origins in topological data analysis (TDA) and persistent homology. It does not rely on perturbations—a characteristic which makes its possible use appealing."
  - [section 4.1] "The BND metric is the next closest to expectations—but it is not strictly monotonic for fractions up to 0.5."
  - [corpus] Weak evidence - related papers discuss topological methods but don't provide empirical evidence of their disagreement with other metrics.
- Break condition: If TDA methods are shown to be equivalent to perturbation-based approaches under certain conditions, or if TDA is proven to be measuring a different property entirely.

## Foundational Learning

- Concept: Spearman's rank correlation coefficient
  - Why needed here: The paper uses Spearman's ρ, weighted Kendall-τ, and Kendall's τ to measure agreement between metrics. Understanding rank correlation is essential to interpret the heatmap results in Figure 3.
  - Quick check question: If two metrics always rank explanations in exactly opposite order, what would their Spearman's ρ correlation be?

- Concept: Mapper algorithm and topological data analysis
  - Why needed here: BND relies on mapper networks and persistence diagrams. Understanding how mapper works (resolution, gain, clustering) is crucial for interpreting why BND might disagree with other metrics.
  - Quick check question: What role does the filter function play in constructing mapper networks, and why did the authors choose model predictions as their filter?

- Concept: Feature attribution methods (SHAP, Integrated Gradients)
  - Why needed here: The paper evaluates three feature attribution methods (Deep SHAP, KernelSHAP, Integrated Gradients) with different baselines. Understanding how these methods work and how baseline choice affects outputs is essential for understanding the experimental setup.
  - Quick check question: How does the choice of baseline affect SHAP values, and why might this impact faithfulness metric results?

## Architecture Onboarding

- Component map: Dataset → Model → Explanations → Faithfulness metrics → Rank correlations
- Critical path: Synthetic and real tabular datasets → Model training (logistic regression, dense neural network) → Feature attribution methods (Deep SHAP, KernelSHAP, Integrated Gradients) with multiple baselines → Faithfulness metrics (PGI, PGU, ABC, BND, ablation) → Rank correlations between metrics
- Design tradeoffs: The paper chose to focus on tabular classification problems and local explanations rather than global explanations or other data types. This limits generalizability but allows for controlled experiments with ground truth on synthetic data.
- Failure signatures: Low rank correlations between metrics suggest fundamental disagreements in what faithfulness means. Non-monotonic behavior in controlled experiments (Figure 2) indicates metric design flaws or inappropriate parameter choices.
- First 3 experiments:
  1. Reproduce Figure 2: Generate explanations with controlled levels of faithfulness on the synthetic dataset and verify that ABC is the only metric that shows strictly monotonic behavior.
  2. Test perturbation sensitivity: Calculate PGI using different perturbation methods (Gaussian vs marginal) on the spambase dataset and verify that rankings change significantly.
  3. Compare TDA vs perturbation: Calculate both BND and ABC on the same set of explanations from the German credit dataset and verify that they produce different rankings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which faithfulness metrics should practitioners use in different application contexts?
- Basis in paper: [explicit] The authors conclude that more work is needed to understand the contexts and assumptions behind each metric, and to develop a unified framework for evaluating explanations.
- Why unresolved: The paper demonstrates that current faithfulness metrics disagree significantly, but does not provide clear guidance on how to choose appropriate metrics for specific use cases.
- What evidence would resolve it: Systematic studies comparing metric performance across different types of models, datasets, and explanation methods, along with guidelines for metric selection based on specific application requirements.

### Open Question 2
- Question: How do different perturbation methods and parameters affect the reliability of faithfulness metrics?
- Basis in paper: [explicit] The authors discuss how the choice of perturbation method (Gaussian vs marginal), the selection of top k features, and treatment of discrete features can significantly impact the rankings of explanations.
- Why unresolved: The paper shows that these parameters can lead to different rankings but does not provide a comprehensive analysis of their effects or recommendations for choosing appropriate values.
- What evidence would resolve it: Extensive experiments systematically varying perturbation methods and parameters across multiple datasets and explanation methods to determine their impact on metric reliability and provide guidelines for parameter selection.

### Open Question 3
- Question: Can topological data analysis (TDA) be improved to better assess explanation faithfulness?
- Basis in paper: [explicit] The authors note that TDA treats explanations as manifolds and uses bottleneck distance to compare them, but this approach has limitations, such as not having a notion of "good" or "faithful" baked into it.
- Why unresolved: The paper shows that TDA is less sensitive to changes in explanation quality compared to other metrics, but does not explore potential improvements to the method.
- What evidence would resolve it: Research into modifying TDA techniques, such as incorporating additional topological features or combining TDA with other metrics, to improve its ability to assess explanation faithfulness.

## Limitations
- The study focuses exclusively on tabular classification problems, limiting generalizability to other data types and tasks
- The disagreement problem is demonstrated empirically but the root causes remain speculative without deeper theoretical analysis
- The paper does not provide clear guidelines for practitioners on which metrics to use in different contexts

## Confidence
- Medium: The empirical findings about metric disagreement are well-supported by experimental results, but the explanations for why disagreement occurs remain speculative
- Low: The paper does not fully explore the theoretical foundations of faithfulness or provide a unified framework to resolve metric disagreements

## Next Checks
1. **Mechanism isolation test**: Systematically vary perturbation parameters (k values, noise magnitude) while keeping all else constant to quantify their contribution to metric disagreement
2. **Metric behavior characterization**: Test each metric's sensitivity to explanation quality on a spectrum from random to ground truth to identify specific failure modes and breakpoints
3. **Cross-domain validation**: Apply the same experimental framework to image or text classification tasks to determine if the disagreement problem is domain-specific or universal across XAI