---
ver: rpa2
title: Emergent Modularity in Pre-trained Transformers
arxiv_id: '2305.18390'
source_url: https://arxiv.org/abs/2305.18390
tags:
- experts
- neurons
- function
- each
- functional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the presence of modularity in pre-trained Transformers,
  a feature commonly found in human brains. It evaluates whether each neuron is mainly
  specialized in a certain function, and finds that the answer is yes.
---

# Emergent Modularity in Pre-trained Transformers

## Quick Facts
- arXiv ID: 2305.18390
- Source URL: https://arxiv.org/abs/2305.18390
- Reference count: 28
- Key outcome: Pre-trained Transformers exhibit emergent modularity where neurons specialize in specific functions and group into functional experts within Mixture-of-Experts structures

## Executive Summary
This paper investigates modularity in pre-trained Transformers, examining whether neurons specialize in specific functions and whether these neurons cluster into functional experts. Through systematic analysis of neuron predictivities for semantic, knowledge, and task sub-functions, the study demonstrates that pre-trained models show significantly greater functional specialization than randomly initialized ones. The research reveals that Mixture-of-Experts structures naturally group functionally similar neurons, creating "functional experts" that can be identified through statistical hypothesis testing. The emergence of modularity is traced during pre-training, showing that modular structure stabilizes early in the training process, preceding individual neuron stabilization.

## Method Summary
The study analyzes pre-trained Transformers (T5 and Switch Transformer) by computing neuron-level predictivities for three function categories: semantic, knowledge, and task sub-functions. Functional experts are identified through statistical hypothesis testing to determine if neurons specialized in a function are concentrated in specific experts beyond random chance. Perturbation experiments validate the causal importance of functional experts by measuring performance drops when expert activations are masked. The emergence of modularity is tracked across pre-training checkpoints by computing functional expert metrics and Spearman's rank correlation for stabilization analysis. The MoEfication technique is applied to convert T5 into its MoE version for comparative analysis.

## Key Results
- Pre-trained Transformer neurons show significantly higher functional predictivity than randomly initialized neurons
- Functional experts exist in MoE structures, with specialized neurons concentrating in specific experts
- Perturbing functional experts significantly degrades corresponding function performance
- Modular structure stabilizes early during pre-training (around 15% of total steps), faster than individual neuron stabilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Functional specialization of neurons emerges during pre-training, enabling modular organization
- Mechanism: Self-supervised learning on large-scale corpora causes individual neurons in Transformer feedforward layers to develop distinct functional identities through gradient-based optimization
- Core assumption: Neurons can develop distinct functional identities through gradient-based optimization without explicit architectural constraints
- Evidence anchors:
  - Experimental results show neurons in pre-trained Transformers become much more specialized than those in randomly-initialized ones
  - Average best predictivities of pre-trained neurons are significantly higher than those of randomly-initialized neurons
- Break condition: Insufficient diverse data or optimization dynamics preventing functional differentiation

### Mechanism 2
- Claim: MoE structure naturally reflects emergent modularity by grouping functionally similar neurons into experts
- Mechanism: The Mixture-of-Experts architecture partitions neurons into experts where each expert concentrates neurons specialized in the same function
- Core assumption: Natural distribution of functionally specialized neurons will align with MoE expert partitions
- Evidence anchors:
  - Experimental results show functional experts exist where clustered neurons are specialized in certain functions
  - Both pre-MoE and post-MoE Transformers show tendency to distribute neurons excelling in certain functions concentratively into some experts
- Break condition: If neuron distributions are truly random or MoE partitioning doesn't align with functional boundaries

### Mechanism 3
- Claim: Modularity emerges through a coarse-to-fine mechanism during pre-training
- Mechanism: Pre-training first establishes stable modular structure at early stages, then gradually refines individual neuron functions within those modules
- Core assumption: Model prioritizes learning high-level modular organization before fine-tuning individual components
- Evidence anchors:
  - Modular structure stabilizes at early stage, faster than neuron stabilization
  - Experts are stabilized to large extent at early stage of pre-training (around 15% of training steps)
- Break condition: If neuron specialization precedes or occurs simultaneously with modular structure formation

## Foundational Learning

- Concept: Neuron predictivity measurement using Average Precision (AP)
  - Why needed here: To quantify how well individual neurons capture specific sub-functions, enabling identification of functional specialization
  - Quick check question: How is AP calculated for neuron activations in this framework?

- Concept: Statistical hypothesis testing for functional expert identification
  - Why needed here: To determine whether neurons specialized in a function are significantly concentrated in certain experts beyond random chance
  - Quick check question: What statistical test is used to identify functional experts?

- Concept: Spearman's rank correlation for stabilization analysis
  - Why needed here: To measure how consistently neurons and experts maintain their predictivity rankings across training checkpoints
  - Quick check question: What does a high Spearman correlation between checkpoints indicate about stabilization?

## Architecture Onboarding

- Component map:
  Transformer encoder with FFN layers -> Mixture-of-Experts (MoE) layers (pre-MoE and post-MoE variants) -> Neuron-level analysis framework -> Expert-level perturbation mechanisms -> Pre-training pipeline with checkpoint saving

- Critical path:
  1. Load pre-trained model and identify FFN neurons
  2. Compute neuron predictivities for semantic, knowledge, and task sub-functions
  3. Apply statistical hypothesis testing to identify functional experts
  4. Conduct perturbation experiments to validate expert importance
  5. Analyze pre-training checkpoints to study emergence patterns

- Design tradeoffs:
  - Neuron vs. expert-level analysis: finer granularity vs. computational efficiency
  - Dense vs. sparse MoE: interpretability vs. model capacity
  - Pre-partitioning vs. post-partitioning: training stability vs. flexibility

- Failure signatures:
  - No significant difference between pre-trained and random model neuron predictivities
  - Uniform distribution of sub-functional neurons across experts
  - Expert predictivities fail to stabilize during pre-training

- First 3 experiments:
  1. Compare neuron predictivities between pre-trained and randomly initialized models to establish functional specialization
  2. Apply statistical hypothesis testing to identify functional experts in MoE structures
  3. Conduct perturbation experiments on functional experts to validate their causal importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other modular structures beyond MoE, such as hierarchical or variable-sized modules, compare to MoE in revealing functional specialization in Transformers?
- Basis in paper: The paper mentions that MoE is not the only possible modular structure and suggests exploring other types of structures
- Why unresolved: The paper focuses primarily on MoE as a promising candidate for modularity
- What evidence would resolve it: Comparative studies of different modular structures in Transformers to evaluate their effectiveness in revealing functional specialization

### Open Question 2
- Question: How does the categorization of functions, such as semantic, knowledge, and task functions, need to be adapted for pre-trained Transformers, considering the overlap between these functions?
- Basis in paper: The paper acknowledges overlap between studied functions and suggests new Transformer-based function categorization may be needed
- Why unresolved: The paper uses typical categorization but recognizes limitations and potential need for more suitable categorization
- What evidence would resolve it: Development and validation of new function categorization scheme specifically tailored for pre-trained Transformers

### Open Question 3
- Question: How does the modularity in other dense pre-trained Transformers, such as BERT, compare to the modularity observed in MoE-fied T5 and Switch Transformer?
- Basis in paper: The paper mentions adopted MoEfication technique can only transform ReLU-based Transformers
- Why unresolved: The paper focuses on studying modularity in MoE-fied T5 and Switch Transformer but doesn't extend analysis to other dense pre-trained Transformers
- What evidence would resolve it: Application of MoEfication technique to other dense pre-trained Transformers and comparison of their modular structures

## Limitations
- Reliance on pre-defined sub-functions may constrain discovery of emergent modular structures
- Analysis focuses exclusively on feedforward network neurons, ignoring attention mechanisms
- Perturbation experiments show correlation but don't establish primary causality of functional experts
- Emergence analysis tracks only three functional dimensions during pre-training

## Confidence
- High Confidence: Pre-trained Transformer neurons show greater functional specialization than randomly initialized neurons
- Medium Confidence: Mixture-of-Experts structures concentrate functionally similar neurons into experts
- Low Confidence: Coarse-to-fine emergence mechanism during pre-training

## Next Checks
1. Apply functional specialization and expert identification framework to additional Transformer architectures (BERT, RoBERTa) and scales to test universality of modularity patterns
2. Conduct unsupervised functional clustering analysis of neurons without pre-defined sub-functions to identify whether MoE structure reveals modular organization that diverges from hypothesized functional groupings
3. Extend modular analysis framework to include attention heads and cross-attention patterns to determine whether modularity exists beyond FFN layers and how it relates to observed feedforward modularity