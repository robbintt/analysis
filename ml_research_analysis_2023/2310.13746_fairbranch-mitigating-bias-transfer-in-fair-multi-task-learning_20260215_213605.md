---
ver: rpa2
title: 'FairBranch: Mitigating Bias Transfer in Fair Multi-task Learning'
arxiv_id: '2310.13746'
source_url: https://arxiv.org/abs/2310.13746
tags:
- fairness
- transfer
- tasks
- accuracy
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairBranch addresses negative and bias transfer in fair multi-task
  learning by grouping tasks based on parameter similarity and correcting fairness
  gradient conflicts within branches. It uses kernel alignment-based similarity to
  form task groups, creating parameter branches to avoid accuracy conflicts, then
  applies fairness conflict correction within branches using gradient rejection.
---

# FairBranch: Mitigating Bias Transfer in Fair Multi-task Learning

## Quick Facts
- **arXiv ID**: 2310.13746
- **Source URL**: https://arxiv.org/abs/2310.13746
- **Reference count**: 38
- **Primary result**: FairBranch reduces bias transfer in fair MTL while maintaining accuracy through parameter-similarity-based task grouping and within-branch fairness conflict correction

## Executive Summary
FairBranch addresses negative and bias transfer in fair multi-task learning by grouping tasks based on parameter similarity rather than gradient similarity. The method uses kernel alignment to assess parameter similarity, creating branches of related tasks to avoid accuracy conflicts, then applies fairness conflict correction within branches using gradient rejection. Experiments on tabular (ACS-PUMS) and visual (CelebA) datasets demonstrate that FairBranch outperforms state-of-the-art MTL methods on both fairness and accuracy metrics while scaling better than alternatives by limiting conflict resolution to task groups.

## Method Summary
FairBranch is a fair multi-task learning method that addresses both negative transfer (accuracy conflicts) and bias transfer (fairness conflicts) through a two-stage approach. First, it trains a model for accuracy to establish parameter similarity, then groups tasks using kernel alignment-based similarity with a threshold τ (0.7 for tabular, 0.8 for visual datasets). These groups form parameter branches that share only within-task-group parameters. Second, it trains for fairness with conflict detection, applying fairness gradient rejection within branches when conflicts exceed 90 degrees. The method uses ACS-PUMS 2018-2019, ACS-PUMS 2019-2021, and CelebA datasets with gender and age protection attributes, optimizing for Average Relative Accuracy (ARA) and Average Relative Fairness (ARF) metrics.

## Key Results
- FairBranch achieves lower bias transfer than competitors while maintaining strong accuracy performance
- The method excels at reducing fairness conflicts during training through effective gradient rejection
- FairBranch scales better than alternatives by limiting conflict resolution to task groups rather than all possible task pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FairBranch addresses negative transfer by grouping tasks based on parameter similarity rather than gradient similarity.
- **Mechanism**: Tasks are grouped using kernel alignment-based similarity of learned parameters, forming branches that share only within-task-group parameters to avoid accuracy gradient conflicts.
- **Core assumption**: Parameters of tasks that update with similar gradients will have similar learned parameters after training, making parameter similarity a stable proxy for task relatedness.
- **Evidence anchors**:
  - [abstract] "branches the MTL model by assessing the similarity of learned parameters, thereby grouping related tasks to alleviate negative transfer"
  - [section] "We choose to use parameter similarity instead. The parameters... when changes/updates its parameters with similar (different) gradients then they must be similar (different) after such updates"
  - [corpus] Weak - no direct corpus neighbors discuss parameter similarity for task grouping in MTL
- **Break condition**: If tasks have similar parameters but conflicting objectives (e.g., accuracy vs fairness), parameter similarity alone may incorrectly group them, potentially preserving bias transfer.

### Mechanism 2
- **Claim**: FairBranch addresses bias transfer by performing fairness conflict correction only within task-group branches rather than across all task pairs.
- **Mechanism**: Within each branch, conflicting fairness gradients are resolved using vector rejection, reducing the number of potential conflicts from T(2T-1) to a manageable number.
- **Core assumption**: Limiting fairness conflict resolution to smaller task groups reduces computational complexity while still addressing most harmful conflicts.
- **Evidence anchors**:
  - [abstract] "it incorporates fairness loss gradient conflict correction between adjoining task-group branches to address bias transfer within these task groups"
  - [section] "limiting the focus of fairness gradient conflict correction within only the task-group branches helps scale to a large number of tasks for the fair-MTL problem"
  - [corpus] Weak - corpus lacks specific discussion of fairness conflict resolution in MTL, though it mentions gradient conflicts generally
- **Break condition**: If harmful fairness conflicts exist between tasks in different branches, this mechanism won't resolve them, potentially allowing bias transfer to persist across branches.

### Mechanism 3
- **Claim**: FairBranch achieves both fairness and accuracy gains by resolving conflicts in a specific order - first accuracy conflicts, then fairness conflicts within branches.
- **Mechanism**: The algorithm first optimizes for accuracy to form task groups, then applies fairness gradient correction within those groups, ensuring parameter updates don't harm accuracy while improving fairness.
- **Core assumption**: Accuracy conflict resolution at the parameter level creates a stable foundation for subsequent fairness optimization without reintroducing accuracy conflicts.
- **Evidence anchors**:
  - [abstract] "incorporates fairness loss gradient conflict correction between adjoining task-group branches"
  - [section] "we infer that for any two tasks we can tackle the accuracy conflict and fairness conflict separately in any order"
  - [corpus] Weak - corpus mentions multi-task learning but doesn't discuss ordered conflict resolution between accuracy and fairness
- **Break condition**: If fairness gradients significantly conflict with accuracy gradients even after grouping, the sequential approach may create suboptimal tradeoffs that the algorithm cannot resolve.

## Foundational Learning

- **Concept**: Gradient conflict and its impact on MTL performance
  - Why needed here: Understanding how conflicting gradients cause negative and bias transfer is essential for grasping why FairBranch's grouping and conflict resolution strategies work
  - Quick check question: What mathematical condition defines a gradient conflict between two tasks?

- **Concept**: Fairness measures in ML (Equal Opportunity and Equalized Odds)
  - Why needed here: FairBranch optimizes for specific fairness metrics, and understanding these definitions is crucial for interpreting experimental results
  - Quick check question: How do Equal Opportunity and Equalized Odds differ in what they measure regarding protected attributes?

- **Concept**: Parameter similarity measures (kernel alignment)
  - Why needed here: FairBranch uses kernel alignment to group tasks, so understanding this similarity measure is key to understanding the algorithm's behavior
  - Quick check question: What does kernel alignment measure between two parameter matrices, and why might it be more stable than gradient similarity?

## Architecture Onboarding

- **Component map**:
  Shared parameter layers (θsh) at multiple depths -> Task-specific parameter layers (θt) for each task -> Branch-specific parameters (θbr) created during task grouping -> Conflict detection and correction modules for both accuracy and fairness conflicts

- **Critical path**:
  1. Train model for accuracy to establish parameter similarity
  2. Compute pairwise parameter similarities using kernel alignment
  3. Group tasks based on similarity threshold and form branches
  4. Train for fairness with conflict detection
  5. Apply fairness gradient rejection within branches as needed

- **Design tradeoffs**:
  - Parameter vs. gradient similarity: More stable but potentially less responsive to dynamic task relationships
  - Branch granularity: Tighter grouping reduces conflicts but may miss beneficial knowledge transfer
  - Conflict resolution scope: Limiting to within-branch conflicts improves scalability but may miss some harmful cross-branch conflicts

- **Failure signatures**:
  - Persistent accuracy degradation despite grouping (negative transfer not resolved)
  - Bias transfer indicated by increasing fairness violations in specific tasks
  - Branching creates too many small groups, increasing model complexity without benefit

- **First 3 experiments**:
  1. Compare parameter similarity vs. gradient similarity for task grouping on a simple MTL dataset
  2. Test FairBranch with different similarity thresholds (τ) to find optimal balance between group size and conflict reduction
  3. Evaluate fairness vs. accuracy tradeoffs on datasets with varying numbers of tasks and different protected attributes

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal threshold τ for parameter similarity-based task grouping across different types of datasets?
  - Basis in paper: [explicit] The authors performed an ablation study testing τ values of 0.4, 0.5, 0.6, 0.7, and 0.8 on tabular and visual datasets, finding different optimal values (0.7 for tabular, 0.8 for visual) with varying parameter efficiency
  - Why unresolved: The optimal τ appears to depend on dataset characteristics, and the paper only tested a limited range of values on two datasets. The relationship between τ, dataset properties, and model efficiency remains unclear
  - What evidence would resolve it: Systematic testing of FairBranch across diverse datasets (different domains, task numbers, and data characteristics) to identify patterns in optimal τ selection and understand the trade-offs between accuracy, fairness, and parameter efficiency

- **Open Question 2**: How does FairBranch scale with very large numbers of tasks, particularly regarding the exponential growth of potential conflicts?
  - Basis in paper: [explicit] The authors note that "a complete solution remains a challenge for the future to address" because with T tasks, there are T(2T-1) possible conflicts, making it "nearly impossible to scale performance in scenarios with a very large number of tasks"
  - Why unresolved: The paper addresses this by limiting task groups to binary links, but doesn't demonstrate performance on datasets with hundreds or thousands of tasks, nor does it propose a complete solution to the conflict scaling problem
  - What evidence would resolve it: Empirical testing of FairBranch on datasets with hundreds of tasks, along with theoretical analysis of how conflict resolution complexity grows with task count and proposed strategies for handling large-scale fair-MTL

- **Open Question 3**: What is the relationship between the remaining unaddressed conflicts (∇θL1, ∇θF2) and (∇θL2, ∇θF1) and the observed bias transfer in visual datasets?
  - Basis in paper: [explicit] The authors state that their method "still leaves us with two more conflicts (∇θL1, ∇θF2) and (∇θL2, ∇θF1) unaddressed" and note that FairBranch "still suffers from bias transfer in certain tasks" on visual data setups
  - Why unresolved: The paper doesn't investigate whether these unaddressed conflicts are the cause of the remaining bias transfer, nor does it explore methods to address them
  - What evidence would resolve it: Analysis showing whether addressing these specific conflict pairs reduces bias transfer, or empirical evidence demonstrating that these conflicts are not the primary cause of the remaining bias transfer issues

## Limitations

- The stability of parameter similarity as a proxy for task relatedness over training epochs has not been validated experimentally
- Cross-branch fairness conflicts may persist unaddressed, though this tradeoff is acknowledged
- The kernel alignment implementation details are underspecified, which could affect reproducibility

## Confidence

- **High confidence**: The core algorithmic approach of parameter-based task grouping and within-branch fairness conflict correction
- **Medium confidence**: The empirical improvements shown on both tabular and visual datasets
- **Medium confidence**: The scalability claims regarding computational complexity reduction

## Next Checks

1. **Parameter stability test**: Track parameter similarity trajectories across training epochs to verify that initial grouping remains valid
2. **Cross-branch conflict audit**: Design experiments to measure potential fairness conflicts between branches that FairBranch cannot address
3. **Hyperparameter sensitivity**: Systematically vary the similarity threshold τ and kernel alignment parameters to assess robustness of task grouping