---
ver: rpa2
title: Multi-point Feedback of Bandit Convex Optimization with Hard Constraints
arxiv_id: '2310.10946'
source_url: https://arxiv.org/abs/2310.10946
tags:
- convex
- constraint
- where
- have
- violation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies bandit convex optimization with hard constraints,
  where the goal is to minimize cumulative loss while also minimizing cumulative constraint
  violation. The authors adopt a cumulative hard constraint violation metric, which
  is more suitable for safety-critical systems than the conventional soft constraint
  violation metric.
---

# Multi-point Feedback of Bandit Convex Optimization with Hard Constraints

## Quick Facts
- arXiv ID: 2310.10946
- Source URL: https://arxiv.org/abs/2310.10946
- Reference count: 40
- This paper proposes a penalty-based proximal gradient descent method for bandit convex optimization with hard constraints, achieving sub-linear growth of both regret and cumulative hard constraint violation.

## Executive Summary
This paper addresses the challenging problem of bandit convex optimization with hard constraints, where the goal is to minimize cumulative loss while also minimizing cumulative constraint violation. Unlike previous work that focuses on soft constraints (allowing temporary violations with penalties), this paper adopts a cumulative hard constraint violation metric that is more suitable for safety-critical systems. The authors propose a novel algorithm that uses two-point function evaluations to estimate gradients and updates decision variables and penalty parameters iteratively. The algorithm achieves sub-linear regret and constraint violation bounds for both convex and strongly convex loss functions, with the latter case providing even better performance guarantees.

## Method Summary
The proposed algorithm uses a penalty-based proximal gradient descent approach with two-point function evaluation for gradient estimation. At each round, the algorithm samples a perturbation, queries two function values symmetrically around the current decision point, computes an unbiased gradient estimate, and solves a regularized optimization problem that includes a penalty term proportional to constraint violation. The penalty parameter is updated based on observed constraint violations, ensuring that cumulative hard constraint violations remain sublinear. The algorithm parameters (learning rate and penalty coefficient) are chosen as power functions of the iteration number with a user-determined parameter c in [1/2, 1).

## Key Results
- Achieves O(d^2 T^max{c,1-c}) regret bound for convex loss functions
- Achieves O(d^2 T^(1-c/2)) cumulative hard constraint violation bound
- For strongly convex loss functions, both bounds can be further reduced
- Uses only two function evaluations per round for gradient estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gradient estimator using two-point function evaluations is an unbiased estimator of the smoothed gradient.
- Mechanism: By querying two points symmetrically around the decision point and averaging the resulting directional derivatives, the estimator recovers the expected value of the smoothed gradient over the uniform distribution on the unit sphere.
- Core assumption: The function is Lipschitz continuous and the perturbation is symmetric.
- Evidence anchors:
  - [abstract] "The algorithm uses a two-point function evaluation to estimate the gradient"
  - [section] Lemma 1 states "For any convex function f : X → R, define its smoothed version function ˆf (x) = Ev∈ Bd [f (x + δv)]... Then, for any δ > 0, we have Eu∈ Sd [ d/δ f (x + δu)u ] = ∇ ˆf (x)"
  - [corpus] No direct evidence; only general bandit optimization context
- Break condition: If the function is not smooth enough or the perturbation is not symmetric, the estimator becomes biased.

### Mechanism 2
- Claim: The penalty-based proximal gradient descent balances regret minimization and hard constraint violation control.
- Mechanism: The algorithm solves a regularized optimization problem at each step that includes a penalty term proportional to the constraint violation. The penalty parameter is updated to ensure that constraint violations do not accumulate excessively.
- Core assumption: The constraint functions are convex and Lipschitz continuous.
- Evidence anchors:
  - [abstract] "We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation"
  - [section] "We update the penalty parameter (line 9) as λ t+1 = max{λ t + γt+1[gt+1(xt)]+, η t+1}"
  - [corpus] No direct evidence; only general constrained optimization context
- Break condition: If the penalty parameter grows too slowly relative to constraint violations, hard constraint violations may accumulate linearly.

### Mechanism 3
- Claim: The strongly convex loss case achieves faster convergence rates than the convex case.
- Mechanism: When loss functions are strongly convex, the optimization problem becomes more "well-behaved," allowing tighter bounds on both regret and constraint violation. The algorithm exploits this structure by adjusting the learning rate based on the strong convexity parameter.
- Core assumption: Loss functions are Lipschitz continuous and strongly convex with known modulus.
- Evidence anchors:
  - [abstract] "We also extend the result for the case where the loss functions are strongly convex and show that both regret and constraint violation bounds can be further reduced"
  - [section] "Under Assumption 4, the function ht : X → R defined as Eq. (7) is also strongly convex with modulus σt"
  - [corpus] No direct evidence; only general convex optimization context
- Break condition: If the strong convexity parameter is too small or unknown, the algorithm cannot achieve the improved bounds.

## Foundational Learning

- Concept: Gradient estimation in bandit optimization
  - Why needed here: The algorithm cannot access gradients directly and must estimate them from function values
  - Quick check question: What is the unbiased gradient estimator when only function values at perturbed points are available?

- Concept: Strong convexity and its implications
  - Why needed here: Strong convexity allows faster convergence rates and tighter bounds
  - Quick check question: How does strong convexity affect the regret bound in online convex optimization?

- Concept: Hard constraint violation vs soft constraint violation
  - Why needed here: The algorithm must ensure safety-critical constraints are never violated in a cumulative sense
  - Quick check question: What is the difference between cumulative hard constraint violation and cumulative soft constraint violation?

## Architecture Onboarding

- Component map: Gradient estimator -> Proximal gradient update -> Penalty parameter update
- Critical path: At each round: sample perturbation → query two function values → compute gradient estimate → solve regularized optimization → update decision and penalty parameter.
- Design tradeoffs: The choice of learning rate and penalty parameter affects the balance between regret and constraint violation. Too aggressive updates may lead to large constraint violations, while too conservative updates may result in high regret.
- Failure signatures: If constraint violations grow linearly, the penalty parameter update is too slow. If regret grows superlinearly, the gradient estimation or optimization step is not properly calibrated.
- First 3 experiments:
  1. Verify the unbiasedness of the gradient estimator by checking if the expected gradient estimate equals the true gradient over multiple trials.
  2. Test the algorithm on a simple convex optimization problem with known constraints to ensure it achieves sublinear regret and constraint violation.
  3. Evaluate the algorithm's performance on a safety-critical simulation to ensure hard constraints are not violated.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, potential open questions include:
1. How does the proposed algorithm perform in non-stationary environments where the loss and constraint functions change over time?
2. Can the proposed algorithm be extended to handle multiple constraint functions simultaneously?
3. How does the choice of the user-determined parameter c affect the algorithm's regret and constraint violation bounds?

## Limitations
- The paper does not provide explicit implementation details for solving the proximal gradient optimization step (Problem 6), which could significantly impact practical performance.
- The initialization strategy for the decision variable x1 and penalty parameter λ1 is not specified, which may affect the algorithm's convergence behavior.
- The algorithm assumes knowledge of Lipschitz constants for loss and constraint functions, which may not be available in practice.

## Confidence

- High confidence: The theoretical regret and constraint violation bounds (O(d^2 T^max{c,1-c}) and O(d^2 T^(1-c/2)) are mathematically derived and follow standard analysis techniques.
- Medium confidence: The two-point gradient estimation mechanism is well-established in bandit optimization literature, but its unbiasedness depends critically on the smoothness of the objective functions.
- Medium confidence: The penalty-based approach for handling hard constraints is theoretically sound, but practical performance may vary depending on the choice of penalty update parameters.

## Next Checks

1. Implement the proximal gradient optimization step using standard convex optimization solvers and verify that it can be solved efficiently in practice.
2. Conduct empirical validation comparing cumulative hard constraint violations against cumulative soft constraint violations on benchmark problems to quantify the safety benefits.
3. Test the algorithm's sensitivity to initialization by running multiple trials with different starting points and analyzing the variance in regret and constraint violation performance.