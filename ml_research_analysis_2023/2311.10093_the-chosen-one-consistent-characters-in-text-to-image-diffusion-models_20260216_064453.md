---
ver: rpa2
title: 'The Chosen One: Consistent Characters in Text-to-Image Diffusion Models'
arxiv_id: '2311.10093'
source_url: https://arxiv.org/abs/2311.10093
tags:
- prompt
- consistent
- character
- identity
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fully automated method for consistent character
  generation in text-to-image diffusion models. The approach iteratively clusters
  generated images based on semantic similarity, extracts a coherent identity from
  the most cohesive cluster, and refines the model parameters to improve consistency
  while maintaining prompt alignment.
---

# The Chosen One: Consistent Characters in Text-to-Image Diffusion Models

## Quick Facts
- **arXiv ID:** 2311.10093
- **Source URL:** https://arxiv.org/abs/2311.10093
- **Reference count:** 40
- **Primary result:** Automated iterative method for consistent character generation that outperforms baselines in balancing prompt similarity and identity consistency

## Executive Summary
This paper addresses the challenge of generating consistent characters in text-to-image diffusion models, where characters often change appearance across different prompts or contexts. The proposed method introduces an iterative clustering approach that generates multiple images from the same prompt, clusters them based on semantic similarity using DINOv2 embeddings, and extracts the most cohesive cluster to refine the character representation. By combining textual inversion with LoRA-based fine-tuning and using a convergence criterion based on pairwise Euclidean distances between embeddings, the method achieves superior balance between prompt adherence and identity consistency compared to existing approaches like TI, BLIP-diffusion, IP-adapter, LoRA DreamBooth, and ELITE.

## Method Summary
The method generates N images from a given prompt and embeds them using DINOv2. It then applies K-MEANS++ clustering to identify groups of semantically similar images, selects the most cohesive cluster, and uses this cluster to refine the model's parameters via LoRA and textual inversion. This iterative process continues until the average pairwise Euclidean distance between image embeddings falls below a predefined threshold. The approach leverages two text encoders (CLIP and OpenCLIP) in Stable Diffusion XL, adding new textual tokens for each while updating model weights via LoRA, providing a more expressive parameter space than textual inversion alone.

## Key Results
- Achieves better balance between prompt similarity and identity consistency compared to baselines
- User study confirms perceptually significant improvements in identity consistency
- Enables applications such as story illustration, local image editing, and pose-controlled generation
- Converges in approximately 20 minutes on an A100 GPU

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative clustering refines the representation of the target character by progressively funneling generated images into a consistent identity.
- **Mechanism:** The method generates a large set of images from the same prompt, clusters them semantically using DINOv2 embeddings, selects the most cohesive cluster, and uses this cluster to refine the model's parameters via LoRA and textual inversion. This process repeats until convergence.
- **Core assumption:** A sufficiently large set of images generated from the same prompt will contain clusters of images with shared characteristics, and the most cohesive cluster will represent a more consistent version of the target character.
- **Evidence anchors:** [abstract] "We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set."
- **Break condition:** If the generated images are too diverse or the prompt is too general, the clustering may not find a cohesive enough cluster to extract a consistent identity, leading to poor convergence or inconsistent results.

### Mechanism 2
- **Claim:** Using a combination of textual inversion and LoRA-based fine-tuning allows the model to capture a more expressive and consistent identity representation.
- **Mechanism:** The method adds new textual tokens for each text encoder and updates the model weights via LoRA, optimizing the denoising loss on the cohesive cluster images. This dual approach provides a richer parameter space than textual inversion alone.
- **Core assumption:** The combination of new textual tokens and LoRA updates is more expressive than either method alone, enabling better capture of the character's identity.
- **Evidence anchors:** [section 3.2] "we also update the model weights Î¸ via a low-rank adaptation (LoRA) of the self- and cross-attention layers of the model."
- **Break condition:** If the representation is not expressive enough (e.g., using only textual inversion without LoRA), the method may underfit and fail to capture the character's identity adequately.

### Mechanism 3
- **Claim:** The convergence criterion based on the average pairwise Euclidean distance between image embeddings ensures the method stops when a consistent identity is achieved.
- **Mechanism:** After each iteration, the method calculates the average pairwise Euclidean distance between all image embeddings. If this distance is smaller than a predefined threshold, the method stops, indicating that the generated images are sufficiently similar and the identity is consistent.
- **Core assumption:** When the average pairwise distance between image embeddings is small, the generated images are consistent enough to represent the same character identity.
- **Evidence anchors:** [section 3.3] "we apply a convergence criterion that enables early stopping. After each iteration, we calculate the average pairwise Euclidean distance between all N embeddings of the newly-generated images, and stop when this distance is smaller than a predefined threshold dconv."
- **Break condition:** If the convergence threshold is set too low, the method may stop prematurely before achieving a consistent identity. If set too high, the method may overfit and generate images that are too similar, losing diversity.

## Foundational Learning

- **Concept: Text-to-image diffusion models**
  - Why needed here: Understanding how diffusion models work is crucial for grasping the method's approach to generating and refining images.
  - Quick check question: What is the main difference between a standard diffusion model and a text-to-image diffusion model?

- **Concept: Textual inversion**
  - Why needed here: Textual inversion is a key component of the method, allowing the addition of new textual tokens to capture the character's identity.
  - Quick check question: How does textual inversion differ from fine-tuning the entire model?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: LoRA is used in conjunction with textual inversion to provide a more expressive parameter space for capturing the character's identity.
  - Quick check question: What is the main advantage of using LoRA over full fine-tuning?

## Architecture Onboarding

- **Component map:** Text-to-image diffusion model (SDXL) -> DINOv2 feature extractor -> K-MEANS++ clustering -> LoRA and textual inversion modules -> Convergence criterion module

- **Critical path:** 1. Generate images from the input prompt 2. Embed images using DINOv2 3. Cluster embeddings and select most cohesive cluster 4. Refine model parameters using LoRA and textual inversion 5. Check convergence criterion 6. Repeat steps 1-5 until convergence

- **Design tradeoffs:**
  - Using a large number of generated images (N=128) ensures sufficient diversity for clustering but increases computational cost.
  - Combining textual inversion with LoRA provides a more expressive representation but adds complexity compared to using either method alone.
  - The convergence criterion based on pairwise distance ensures consistency but may lead to overfitting if the threshold is too low.

- **Failure signatures:**
  - Inconsistent identity: If the clustering stage fails to find a cohesive cluster, the method may not converge to a consistent identity.
  - Spurious attributes: The method may add attributes not present in the input prompt if they appear frequently in the most cohesive cluster.
  - High computational cost: Each iteration involves generating a large number of images and optimizing the model parameters, which can be time-consuming.

- **First 3 experiments:**
  1. Generate a small set of images (N=32) from a simple prompt (e.g., "a cat") and visualize the embeddings to understand the clustering process.
  2. Apply the method to a prompt with a clear identity (e.g., "a specific celebrity") and evaluate the consistency of the generated images.
  3. Test the method with different convergence thresholds and analyze the trade-off between consistency and diversity of the generated images.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence criterion dconv affect the quality and consistency of the generated characters across different types of prompts (e.g., abstract vs. concrete)?
- Basis in paper: [explicit] The paper mentions using a convergence criterion dconv and stopping when the average pairwise Euclidean distance between embeddings is smaller than dconv, but does not explore its impact on different prompt types.
- Why unresolved: The paper uses a fixed dconv threshold but does not analyze how varying this threshold or its effectiveness might differ based on prompt complexity or subject matter.
- What evidence would resolve it: Experiments varying dconv for different prompt categories (abstract concepts vs. specific objects) and measuring the resulting consistency and prompt adherence would clarify optimal convergence settings.

### Open Question 2
- Question: What is the impact of using different feature extractors (DINOv2, DINOv1, CLIP) on the identity clustering and final consistency quality across diverse character types?
- Basis in paper: [explicit] The paper compares DINOv2, DINOv1, and CLIP as feature extractors and shows quantitative differences in prompt similarity and identity consistency, but does not deeply analyze their impact on different character types.
- Why unresolved: While the paper shows general performance differences between feature extractors, it does not explore how these differences manifest for specific character categories or complex prompts.
- What evidence would resolve it: Systematic testing of each feature extractor across various character categories (humans, animals, objects, abstract concepts) with detailed analysis of clustering quality and final output consistency.

### Open Question 3
- Question: How does the computational cost of the iterative refinement process scale with different prompt complexities and character types?
- Basis in paper: [explicit] The paper mentions the method takes about 20 minutes to converge on an A100 GPU but does not analyze how this varies with different inputs or provide optimization strategies.
- Why unresolved: The paper provides a general runtime estimate but does not explore how computational requirements change based on prompt complexity, character diversity, or potential optimization opportunities.
- What evidence would resolve it: Detailed profiling of the method's runtime components across various prompt types and character complexities, along with exploration of potential optimizations like early stopping criteria or adaptive iteration counts.

## Limitations

- The method's performance heavily depends on prompt quality and diversity of generated images, potentially failing with overly general or ambiguous prompts.
- Computational cost is significant, requiring generation of 128 images per iteration and multiple optimization steps, making it less practical than single-shot approaches.
- The method may introduce spurious attributes not present in the original prompt if they appear frequently in the most cohesive cluster.

## Confidence

**High Confidence:** The core iterative clustering mechanism (Mechanism 1) has strong empirical support from the quantitative results showing improved identity consistency over baselines. The user study validation provides additional perceptual evidence for the method's effectiveness in maintaining character identity across different contexts.

**Medium Confidence:** The combination of textual inversion and LoRA (Mechanism 2) shows promise based on the reported improvements in prompt similarity while maintaining identity consistency. However, the ablation study comparing this combination against either method alone is not provided, making it difficult to isolate the contribution of each component.

**Medium Confidence:** The convergence criterion based on pairwise Euclidean distance (Mechanism 3) is theoretically sound but lacks comprehensive validation. The paper reports convergence in most cases but does not analyze scenarios where the method might fail to converge or overfit to the training distribution.

## Next Checks

1. **Ablation Study on Iterative Clustering Thresholds:** Systematically vary the convergence threshold (dconv) and analyze the trade-off between identity consistency and prompt adherence. Generate scatter plots showing the relationship between average pairwise distance, CLIP similarity scores, and perceptual quality across different threshold values.

2. **Stress Test with Challenging Prompts:** Evaluate the method on prompts with ambiguous or multiple possible interpretations (e.g., "a person with glasses" vs. "a specific celebrity"). Compare the clustering stability and identity consistency across these prompt types to identify failure modes and their frequency.

3. **Runtime and Resource Analysis:** Measure the actual computational cost per iteration, including image generation time, embedding computation, and model training steps. Compare this against alternative methods like DreamBooth and ELITE to quantify the practical limitations and identify optimization opportunities.