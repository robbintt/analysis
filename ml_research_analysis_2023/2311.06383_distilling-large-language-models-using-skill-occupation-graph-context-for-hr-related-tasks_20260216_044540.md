---
ver: rpa2
title: Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related
  Tasks
arxiv_id: '2311.06383'
source_url: https://arxiv.org/abs/2311.06383
tags:
- skills
- description
- resumes
- graph
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RJDB, a multi-task benchmark for HR applications
  that includes resume-job description matching, skill extraction, and resume editing.
  The benchmark is created by distilling knowledge from GPT-4 using a curated skill-occupation
  graph as context.
---

# Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks

## Quick Facts
- arXiv ID: 2311.06383
- Source URL: https://arxiv.org/abs/2311.06383
- Reference count: 26
- Student models achieve near/better performance than GPT-4 on HR tasks through knowledge distillation

## Executive Summary
This paper introduces RJDB, a multi-task benchmark for HR applications that includes resume-job description matching, skill extraction, and resume editing. The benchmark is created by distilling knowledge from GPT-4 using a curated skill-occupation graph as context. The graph is constructed by combining data from Dice and the US Bureau of Labor Statistics, with GPT-4 generating skills for general occupations. The generation pipeline ensures diverse, realistic documents with task-specific annotations. Student models (Flan-T5 base) trained on RJDB achieve near/better performance than GPT-4 on matching and explaining tasks, and comparable performance on extraction and editing tasks. RJDB also demonstrates utility on out-of-distribution data through zero-shot and weak supervision approaches.

## Method Summary
The paper constructs a skill-occupation graph by combining Dice data with US Bureau of Labor Statistics occupations and GPT-4-generated skills. Using random walks with GPT-4 guidance, subgraphs are sampled as context for generating 52,000 triples of job descriptions, matched resumes, and unmatched resumes with task-specific annotations. Student models (Flan-T5 base) are then fine-tuned on this RJDB benchmark for matching, explaining, extracting, and editing tasks. The models are evaluated against GPT-4 on held-out data and out-of-distribution data using zero-shot and weak supervision approaches.

## Key Results
- Student models achieve near/better performance than GPT-4 on matching and explaining tasks
- Comparable performance to GPT-4 on skill extraction and resume editing tasks
- RJDB demonstrates utility on out-of-distribution data through zero-shot and weak supervision approaches

## Why This Works (Mechanism)

### Mechanism 1
Using skill-occupation subgraphs as contextual guidance enables LLMs to generate more accurate and diverse HR documents. The skill-occupation graph provides domain-specific context that helps LLMs understand the relationship between occupations and required skills, reducing hallucinations and improving factuality.

### Mechanism 2
Knowledge distillation from GPT-4 to smaller student models (Flan-T5) can achieve comparable or better performance on HR tasks. The distillation process transfers the reasoning capabilities and task-specific knowledge from the large teacher model to smaller, more deployable student models.

### Mechanism 3
The combination of random walks with GPT-4 guidance for subgraph sampling ensures realistic document generation. Random walks explore the graph structure while GPT-4 provides real-world statistics on skill counts per occupation, balancing diversity with authenticity.

## Foundational Learning

- **Bipartite graphs and their properties**: Understanding how skill-occupation graphs work as bipartite structures is crucial for sampling subgraphs effectively. Quick check: How does a bipartite graph differ from a general graph, and why is this structure suitable for representing skills and occupations?

- **Knowledge distillation principles**: The paper's core contribution relies on transferring knowledge from GPT-4 to smaller models. Quick check: What are the key differences between teacher-student training and traditional supervised learning?

- **Zero-shot and few-shot learning**: The paper explores using RJDB for out-of-distribution data through zero-shot adaptation. Quick check: How does zero-shot learning differ from traditional supervised learning, and what are its limitations?

## Architecture Onboarding

- **Component map**: Teacher model (GPT-4) → Graph curation system → Document generation pipeline → Student model training → Evaluation and benchmarking
- **Critical path**: Graph curation → Document generation → Student training → Performance evaluation
- **Design tradeoffs**: Larger teacher models provide better quality but are expensive to use; smaller student models are more deployable but may sacrifice some capabilities
- **Failure signatures**: Poor performance on out-of-distribution data suggests insufficient graph diversity; low student model performance suggests inadequate distillation process
- **First 3 experiments**: 1) Test the quality of generated documents with different subgraph sampling strategies, 2) Compare student model performance on in-distribution vs out-of-distribution data, 3) Evaluate the impact of different amounts of RJDB data on student model performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the diversity of the skill-occupation graph impact the quality and usefulness of the RJDB benchmark? The paper mentions the importance of diversity but does not provide a detailed analysis of how this diversity affects the benchmark's quality or usefulness in practice.

### Open Question 2
Can the RJDB benchmark be effectively used for HR tasks in domains outside of technology, social, product, finance, manual labor, healthcare, and administrative? The paper focuses on the mentioned categories but does not explore its potential use in other HR domains.

### Open Question 3
How does the performance of student models trained on RJDB compare to models trained on proprietary, in-house HR data? The paper does not address its relative performance compared to models trained on such data.

### Open Question 4
What are the limitations of using GPT-4 as the teacher model for knowledge distillation in the HR domain? The paper does not critically examine its potential shortcomings or biases in the HR context.

### Open Question 5
How can the RJDB benchmark be further improved to better support HR tasks that require a higher level of reasoning or domain-specific expertise? The paper does not discuss potential enhancements to better support tasks that require advanced reasoning or domain-specific knowledge.

## Limitations

- Evaluation methodology lacks transparency with unspecified human evaluation criteria and inter-rater reliability
- Comparison between student models and GPT-4 may be biased since student models are trained on GPT-4-generated data
- Out-of-distribution evaluation is limited to initial experiments without comprehensive validation

## Confidence

- **High Confidence**: The technical approach of using skill-occupation graphs as context for document generation is well-defined and reproducible
- **Medium Confidence**: Claims about student model performance relative to GPT-4 due to potential evaluation bias
- **Medium Confidence**: The assertion that RJDB enables zero-shot and weak supervision adaptation for out-of-distribution data due to limited evidence

## Next Checks

1. Have an external team evaluate the student models on a held-out test set that was not generated by GPT-4, using standardized evaluation metrics for each task
2. Test RJDB-trained models on real-world HR datasets (e.g., from job boards or company ATS systems) to assess generalization beyond synthetic data
3. Systematically vary the skill-occupation graph quality to quantify the impact on generated document quality and downstream task performance