---
ver: rpa2
title: An Empirical Study of Using ChatGPT for Fact Verification Task
arxiv_id: '2311.06592'
source_url: https://arxiv.org/abs/2311.06592
tags:
- chatgpt
- claim
- answer
- prompts
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines ChatGPT's performance on the
  fact verification task using three distinct prompts on the FEVER dataset. The experiments
  show that a conversational prompt requiring ChatGPT to generate multiple sub-prompts
  significantly outperforms both direct and evidence-augmented prompts, achieving
  over 72% accuracy on the full label set and over 85% on binary labels.
---

# An Empirical Study of Using ChatGPT for Fact Verification Task

## Quick Facts
- arXiv ID: 2311.06592
- Source URL: https://arxiv.org/abs/2311.06592
- Reference count: 10
- Key outcome: Conversational prompt requiring ChatGPT to generate multiple sub-prompts significantly outperforms direct and evidence-augmented prompts, achieving over 72% accuracy on full labels and over 85% on binary labels.

## Executive Summary
This study systematically examines ChatGPT's performance on the fact verification task using three distinct prompts on the FEVER dataset. The experiments show that a conversational prompt requiring ChatGPT to generate multiple sub-prompts significantly outperforms both direct and evidence-augmented prompts, achieving over 72% accuracy on the full label set and over 85% on binary labels. Error analysis reveals that the most frequent mistakes stem from incorrect logical reasoning and misunderstanding the context of claims, accounting for nearly 70% of errors. The findings demonstrate that while ChatGPT is capable of high accuracy in fact verification, its performance is highly sensitive to prompt design, and careful prompting can substantially improve results.

## Method Summary
The study evaluates ChatGPT's zero-shot performance on the FEVER 1.0 dataset using three prompt designs: a direct query (Prompt-1), an evidence-augmented query (Prompt-2), and a conversational approach requiring multiple sub-prompts (Prompt-3). The model (gpt-3.5-turbo, March 23, 2023 version) is evaluated on two settings: three-way classification (Support, Refute, Not Enough Information) and binary classification (Support/Refute only). Performance is measured using Label Accuracy on both development and test splits, with error analysis conducted on incorrect predictions.

## Key Results
- Prompt-3 (conversational multi-prompt) significantly outperforms Prompt-1 and Prompt-2, achieving over 72% accuracy on full labels and over 85% on binary labels
- Evidence-augmented prompts (Prompt-2) do not improve performance over direct prompts, suggesting ChatGPT's internal knowledge is sufficient
- Error analysis shows 48% of errors stem from incorrect logical reasoning and 21% from misunderstanding context, totaling nearly 70% of all mistakes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple conversational sub-prompts improve reasoning performance.
- Mechanism: Breaking a complex fact verification task into smaller, answerable sub-prompts creates a reasoning chain that leverages ChatGPT's conversational context and iterative refinement.
- Core assumption: ChatGPT's reasoning improves when guided through step-by-step sub-prompts rather than a single monolithic prompt.
- Evidence anchors:
  - [abstract] "a conversational prompt requiring ChatGPT to generate multiple sub-prompts significantly outperforms both direct and evidence-augmented prompts, achieving over 72% accuracy"
  - [section] "Prompt-3 asks ChatGPT to generate more prompts related to the claim and answers them. Based on the answers to the generated prompts, ChatGPT is asked to provide the label for the claim. ChatGPT is known to change its response based on the conversation, and in most scenarios, the change is positive."
- Break condition: If sub-prompts are too narrow or irrelevant, the reasoning chain may fail to cover the claim's essential context, leading to incorrect conclusions.

### Mechanism 2
- Claim: Evidence-augmented prompts do not improve performance over direct prompts.
- Mechanism: ChatGPT's internal knowledge base is sufficient for fact verification without external evidence retrieval, making additional evidence prompts redundant.
- Core assumption: ChatGPT's pretrained knowledge contains enough context to verify claims without explicit evidence lookup.
- Evidence anchors:
  - [abstract] "ChatGPT achieved > 72% for both the development and test splits for evaluation setting-1... indicating that ChatGPT can achieve good performance when verifying claims."
  - [section] "From the results of ChatGPT in Table 2, we can observe that Prompt-3 significantly outperforms both Prompt-1 and Prompt-2... asking for additional evidence for the claim does not improve the performance of ChatGPT."
- Break condition: If the claim requires very recent or specialized knowledge not in ChatGPT's training data, evidence-augmented prompts might become necessary.

### Mechanism 3
- Claim: ChatGPT's performance is highly sensitive to prompt design.
- Mechanism: Small variations in prompt phrasing, structure, and task framing significantly affect the model's output quality and accuracy.
- Core assumption: The model's reasoning process is highly dependent on the framing and specificity of the input prompt.
- Evidence anchors:
  - [abstract] "its performance is highly sensitive to prompt design, and careful prompting can substantially improve results."
  - [section] "From the results on evaluation setting-2, we can observe that Prompt-2 outperforms Prompt-1 for the development set, but the performance is still unsatisfactory. From the results of ChatGPT in Table 2, we can observe that Prompt-3 significantly outperforms both Prompt-1 and Prompt-2 for both evaluation settings."
- Break condition: If prompt design is optimized beyond a certain point, further improvements may plateau or even degrade due to prompt complexity or confusion.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The experiments evaluate ChatGPT without any fine-tuning or training data, relying entirely on the model's pretrained capabilities.
  - Quick check question: What does "zero-shot" mean in the context of language model evaluation?

- Concept: Fact verification task structure
  - Why needed here: Understanding the three-label (Support, Refute, NEI) structure is essential for designing appropriate prompts and interpreting results.
  - Quick check question: What are the three possible labels in the FEVER fact verification task?

- Concept: Error categorization in NLP systems
  - Why needed here: The error analysis categorizes mistakes into logical reasoning, context misunderstanding, factual errors, etc., which helps identify model weaknesses.
  - Quick check question: Why is categorizing errors important for improving model performance?

## Architecture Onboarding

- Component map: ChatGPT (gpt-3.5-turbo) -> Prompt engineering interface -> FEVER dataset loader -> Evaluation metrics (Label Accuracy) -> Error analysis pipeline
- Critical path: Prompt design → ChatGPT API call → Response parsing → Label extraction → Accuracy calculation → Error analysis
- Design tradeoffs: Non-binary prompts require more complex reasoning but yield better accuracy; binary prompts are simpler but less informative
- Failure signatures: Incorrect logical reasoning (48% of errors), misunderstanding context (21%), factual errors in generated prompts (16%)
- First 3 experiments:
  1. Test Prompt-1 vs Prompt-2 on a small sample to confirm no performance difference
  2. Test Prompt-3 on a small sample to confirm performance improvement
  3. Run error analysis on Prompt-3's incorrect predictions to identify dominant error categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ChatGPT's performance on fact verification tasks vary across different versions of the model (e.g., GPT-3.5 vs. GPT-4)?
- Basis in paper: [inferred] The paper explicitly states that conclusions are limited to the specific version (gpt-3.5-turbo March 2023) used in experiments.
- Why unresolved: The study only tested one version, so comparative performance across versions is unknown.
- What evidence would resolve it: Systematic testing of multiple ChatGPT versions on the same FEVER dataset using identical prompts.

### Open Question 2
- Question: Can the prompt-3 conversational approach generalize to other fact verification datasets beyond FEVER?
- Basis in paper: [explicit] The authors state that "our conclusions are based on empirical results on FEVER 1.0 shared task and cannot be extended to other fact verification datasets."
- Why unresolved: The study only used one benchmark dataset, leaving generalization uncertain.
- What evidence would resolve it: Replicating the prompt-3 methodology on diverse fact verification datasets (e.g., HoVer, FAKT, Wiki-FactCheck-English).

### Open Question 3
- Question: What is the impact of temporal knowledge gaps (e.g., Wikipedia 2017 vs. current data) on ChatGPT's fact verification accuracy?
- Basis in paper: [explicit] The authors note that "The evidence for the claims in the FEVER 1.0 dataset are obtained from Wikipedia’s June 2017 dump, whereas ChatGPT may or may not have access to Wikipedia or may have access to additional websites resulting in incorrect responses."
- Why unresolved: The study identified this as a limitation but didn't quantify the impact of temporal knowledge gaps.
- What evidence would resolve it: Controlled experiments comparing ChatGPT's performance on FEVER claims using only 2017 data versus claims where ground truth knowledge changed after 2017.

## Limitations
- Results are specific to ChatGPT's March 2023 knowledge cutoff and may not generalize to other model versions or more recent information
- Error analysis shows nearly 70% of mistakes stem from fundamental reasoning and context understanding failures that prompt engineering alone cannot fully address
- The study only tested one fact verification dataset (FEVER), limiting conclusions about generalizability to other domains or claim types

## Confidence
- **High confidence**: The comparative performance differences between prompt types (Prompt-3 outperforming Prompt-1 and Prompt-2) are well-supported by the presented results and consistent across both evaluation settings
- **Medium confidence**: The mechanism explaining why conversational sub-prompts improve performance (reasoning chain hypothesis) is plausible but not definitively proven by the study design
- **Medium confidence**: The claim that evidence-augmented prompts provide no benefit assumes ChatGPT's internal knowledge is sufficient, but this may not hold for specialized or recent information beyond the training cutoff

## Next Checks
1. Replicate the study using a different fact verification dataset (e.g., FEVEROUS or SciFact) to test generalizability of prompt design findings across domains
2. Conduct a controlled experiment varying only the number and specificity of sub-prompts in Prompt-3 to identify the optimal reasoning chain length and complexity
3. Implement a human evaluation study where annotators review ChatGPT's intermediate responses to determine whether logical reasoning failures stem from the model's synthesis step or earlier sub-prompt generation