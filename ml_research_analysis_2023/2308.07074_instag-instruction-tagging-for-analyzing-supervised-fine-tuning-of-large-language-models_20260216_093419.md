---
ver: rpa2
title: '#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large
  Language Models'
arxiv_id: '2308.07074'
source_url: https://arxiv.org/abs/2308.07074
tags:
- data
- tags
- datasets
- tagging
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes InsTag, an open-set fine-grained tagger, to
  analyze the diversity and complexity of SFT datasets for LLMs. InsTag uses ChatGPT
  to automatically assign tags to queries in SFT datasets and then normalizes the
  tags to ensure consistency.
---

# #InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models

## Quick Facts
- arXiv ID: 2308.07074
- Source URL: https://arxiv.org/abs/2308.07074
- Authors: 
- Reference count: 40
- Primary result: Fine-tuning models on 6K diverse and complex samples selected via InsTag outperforms models trained on larger amounts of SFT data

## Executive Summary
This paper introduces InsTag, an open-set fine-grained tagger that uses ChatGPT to automatically assign semantic tags to instructions in SFT datasets. The framework normalizes these tags to ensure consistency and uses them to analyze dataset diversity and complexity. The authors demonstrate that model performance increases with more diverse and complex data, and propose a data selector that samples 6K diverse, complex samples from open-source datasets. Fine-tuning models on this InsTag-selected data results in better performance than models trained on larger amounts of SFT data. The framework is open-sourced to enable deeper insights into LLM alignment beyond data selection.

## Method Summary
InsTag uses ChatGPT to automatically tag queries in SFT datasets with fine-grained semantic labels. The tags undergo normalization through frequency filtering, rule aggregation, semantic aggregation, and association aggregation to remove inconsistencies. The framework then analyzes dataset diversity (unique tag coverage) and complexity (average tags per instruction). A complexity-first diverse sampling algorithm selects high-value instruction subsets by sorting by tag count and applying coverage constraints. Models are fine-tuned on these selected subsets and evaluated on MT-Bench to demonstrate performance improvements over larger but less curated datasets.

## Key Results
- Models fine-tuned on InsTag-selected 6K diverse and complex samples outperform those trained on 125K larger SFT datasets
- MT-Bench performance increases with higher average tag numbers, indicating complexity benefits
- Open-set tagging captures more diverse instruction types than closed-set approaches
- InsTag achieves 96.1% precision and 86.6% consistency when evaluated against GPT-4 and human annotators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tagging enables quantitative measurement of SFT dataset diversity and complexity
- Mechanism: Tagging assigns fine-grained semantic labels to each instruction, enabling calculation of unique tag coverage (diversity) and average tags per instruction (complexity)
- Core assumption: Tags accurately capture the semantic intentions behind user queries
- Evidence anchors:
  - [abstract] "We propose using a tagging system to feature and categorize samples in SFT datasets"
  - [section 3.4] "we introduce the diversity and complexity attributes of SFT datasets induced by our tagging result"
  - [corpus] Weak - no direct citation found for tag-based diversity/complexity measurement in literature
- Break condition: If tags fail to capture semantic nuances or are too coarse to differentiate instruction types

### Mechanism 2
- Claim: More diverse and complex SFT data improves model alignment performance
- Mechanism: Diverse data exposes models to broader semantic contexts while complex data requires multi-step reasoning, both improving instruction-following capability
- Core assumption: Model performance scales with instruction diversity and complexity rather than just data volume
- Evidence anchors:
  - [abstract] "model performance increases with more diverse and complex data"
  - [section 4.3] "the overall trend of performance on MT-BENCH is increasing along with the growth of average tag numbers"
  - [corpus] Assumption: This finding aligns with established scaling laws but direct evidence for diversity/complexity effects is limited

### Mechanism 3
- Claim: Complexity-first diverse sampling selects high-value instruction subsets
- Mechanism: Sorting by tag count prioritizes complex instructions while the coverage constraint ensures diversity, selecting samples that maximize both metrics
- Core assumption: Tag count correlates with instruction complexity and diverse tag coverage ensures broad semantic representation
- Evidence anchors:
  - [section 4.2] "we sample an SFT data subset of 6K samples from the pooled dataset with the highest sample complexity"
  - [algorithm 1] Describes the sampling methodology prioritizing complexity then diversity
  - [corpus] Assumption: Similar sampling strategies exist but tag-based complexity measurement appears novel

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is the core training methodology being analyzed and optimized
  - Quick check question: What distinguishes SFT from other LLM alignment methods like RLHF?

- Concept: Instruction following evaluation
  - Why needed here: MT-BENCH is used to measure alignment performance of fine-tuned models
  - Quick check question: How does MT-BENCH differ from other LLM evaluation benchmarks?

- Concept: Corpus analysis using semantic tags
  - Why needed here: Tag-based analysis provides the quantitative framework for understanding dataset properties
  - Quick check question: What are the limitations of using tag-based metrics for dataset analysis?

## Architecture Onboarding

- Component map: ChatGPT-based tagger (InsTag) -> Tag normalization pipeline -> Dataset analysis module -> Data selector -> Model training pipeline -> Evaluation module

- Critical path:
  1. Tag dataset instructions using ChatGPT
  2. Normalize tags to remove inconsistencies
  3. Analyze diversity and complexity metrics
  4. Select diverse, complex subset using complexity-first sampling
  5. Fine-tune model on selected data
  6. Evaluate on MT-BENCH

- Design tradeoffs:
  - Tag granularity vs. annotation consistency (fine-grained tags provide better analysis but harder normalization)
  - Open-set vs. closed-set tagging (open-set captures more diversity but requires more normalization)
  - Complexity vs. diversity sampling balance (too much complexity focus reduces diversity and vice versa)

- Failure signatures:
  - Poor tag precision/consistency indicates annotation quality issues
  - Low correlation between tag metrics and performance suggests metrics don't capture relevant properties
  - Random sampling outperforms structured sampling indicates sampling methodology flaws

- First 3 experiments:
  1. Compare MT-BENCH scores of models trained on random vs. complexity-prioritized subsets
  2. Test sensitivity of performance to different diversity thresholds in sampling
  3. Evaluate tag consistency across different annotators (human vs. GPT-4 vs. fine-tuned tagger)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of InsTag compare to human annotators when dealing with ambiguous or complex instructions?
- Basis in paper: [explicit] The paper evaluates InsTag's precision and consistency using both GPT-4 and human annotators, finding that GPT-4 achieves 96.1% precision and 86.6% consistency, while human annotators achieve 100% on both metrics.
- Why unresolved: The evaluation is based on a limited sample size (4,000 cases for GPT-4 and 40 cases for humans). It's unclear how InsTag would perform on a larger scale or with more nuanced instructions.
- What evidence would resolve it: A larger-scale study comparing InsTag's performance to human annotators across diverse and complex instruction sets would provide more robust evidence.

### Open Question 2
- Question: Can InsTag be effectively adapted for multilingual instruction tagging?
- Basis in paper: [inferred] The paper mentions that the analysis of SFT datasets is primarily focused on English, suggesting potential limitations in multilingual contexts.
- Why unresolved: The paper does not explore InsTag's performance on non-English datasets or provide evidence of its adaptability to other languages.
- What evidence would resolve it: Testing InsTag on multilingual datasets and evaluating its tagging accuracy and consistency across different languages would demonstrate its adaptability.

### Open Question 3
- Question: How does the choice of tag normalization techniques impact the quality of InsTag's annotations?
- Basis in paper: [explicit] The paper describes three normalization techniques: frequency filtering, rule aggregation, and semantic aggregation. However, it does not provide a detailed analysis of how each technique individually affects the final tag set.
- Why unresolved: The paper combines all normalization techniques in sequence without isolating their individual effects, making it difficult to determine which techniques are most critical for improving tag quality.
- What evidence would resolve it: Conducting ablation studies where each normalization technique is applied separately would reveal their individual contributions to tag quality and help optimize the overall normalization process.

## Limitations
- Reliance on ChatGPT for tag generation introduces potential consistency issues requiring extensive normalization
- Correlation between tag-based metrics and model performance improvements remains observational rather than causal
- Open-set tagging may miss nuanced instruction types or introduce semantic drift during normalization

## Confidence
- High confidence in the tag normalization methodology and its ability to reduce noise in instruction labels
- Medium confidence in the correlation between tag-based metrics and model performance improvements
- Medium confidence in the complexity-first diverse sampling approach, though alternative sampling strategies were not thoroughly explored

## Next Checks
1. Conduct ablation studies testing whether the specific complexity-first sampling strategy outperforms simpler diversity-based or random sampling approaches
2. Evaluate tag consistency and performance impact when using different annotators (human vs. GPT-4 vs. fine-tuned tagger) on the same instruction sets
3. Test whether models trained on InsTag-selected subsets maintain performance gains when evaluated on unseen instruction types not well-represented in the original datasets