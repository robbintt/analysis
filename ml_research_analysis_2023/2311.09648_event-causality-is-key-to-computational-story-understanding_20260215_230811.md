---
ver: rpa2
title: Event Causality Is Key to Computational Story Understanding
arxiv_id: '2311.09648'
source_url: https://arxiv.org/abs/2311.09648
tags:
- event
- story
- causal
- causality
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using large language models, specifically ChatGPT,
  to extract event causality from stories. The key idea is to prompt ChatGPT to generate
  event graphs representing causal relationships between story events.
---

# Event Causality Is Key to Computational Story Understanding

## Quick Facts
- arXiv ID: 2311.09648
- Source URL: https://arxiv.org/abs/2311.09648
- Reference count: 30
- Primary result: ChatGPT-extracted event causality improves story quality evaluation and video-text alignment tasks

## Executive Summary
This paper proposes using large language models, specifically ChatGPT, to extract event causality from stories through prompt engineering. The extracted event graphs representing causal relationships between story events are shown to improve performance on story quality evaluation and video-text alignment tasks. The approach leverages the idea that event causality provides critical information for story understanding, mirroring how humans process narratives. The method demonstrates that causal context can be more effective than temporal context for certain story understanding tasks.

## Method Summary
The paper uses ChatGPT with carefully designed prompts to generate event graphs that represent causal relationships between story events. The method involves prompting ChatGPT to extract event pairs and their causal relationships, then converting these into structured event graphs. This extracted causality is then integrated into downstream story understanding tasks - specifically story quality evaluation (measuring correlation with human judgments) and video-text alignment (improving alignment accuracy by replacing temporal context with causal context). The approach is evaluated against supervised baselines using metrics like BLEU, BertScore, and F1 scores.

## Key Results
- ChatGPT achieves 75-82% F1 score on event causality extraction compared to supervised models
- Story quality evaluation improves by 1.4% average across correlation metrics with causal context
- Video-text alignment improves by 8.7% Clip Accuracy and 8.0% Sentence IoU when using causal context instead of temporal context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Event causality provides critical information for story understanding by aligning with human cognitive processes.
- Mechanism: The paper leverages research showing that humans process and remember stories based on causal relationships between events. When ChatGPT identifies these causal structures, it mirrors how humans understand narratives.
- Core assumption: The causal structures identified by ChatGPT accurately reflect the true causal relationships in stories.
- Evidence anchors:
  - [abstract] "Cognitive science and symbolic AI research suggest that event causality provides vital information for story understanding."
  - [section 2.1] "Cognitive science research indicates that event causality is crucial in human narrative comprehension... story events involved in more causal connections are better memorized."
  - [corpus] Weak evidence - the corpus shows related work on event causality identification but doesn't directly validate the cognitive claim.
- Break condition: If ChatGPT's extracted causal structures don't align with actual story understanding, the performance gains in downstream tasks would disappear.

### Mechanism 2
- Claim: Prompt engineering with ChatGPT can effectively extract event causality from stories.
- Mechanism: By structuring prompts to generate event graphs with causal relationships, ChatGPT leverages its training data that likely contains representations of causality as directional relationships.
- Core assumption: ChatGPT's training data contains sufficient representations of causal relationships to extract meaningful event causality.
- Evidence anchors:
  - [section 3.3] "We believe this prompt is suitable for extracting event causality with ChatGPT because of the event graphs format and the representation of event causality as arrows"
  - [section 3.3] "we often find that the causal statements extracted by ChatGPT, while different from the annotations, are perfectly reasonable"
  - [corpus] Moderate evidence - related papers show prompt-based approaches for event causality but don't specifically validate this mechanism.
- Break condition: If ChatGPT's understanding of causality differs significantly from human understanding, the extracted structures would be less useful for story understanding tasks.

### Mechanism 3
- Claim: Incorporating event causality into story understanding tasks improves performance.
- Mechanism: Adding causal context to models (like in video-text alignment) provides additional semantic information beyond temporal relationships, leading to better alignment accuracy.
- Core assumption: Causal relationships between events provide complementary information to temporal relationships for understanding stories.
- Evidence anchors:
  - [section 4.2] "replacing part of the temporal context with causal context yielded improvements across the board... 8.7% on Clip Accuracy and 8.0% Sentence IoU"
  - [section 4.1] "the simple change improved dataset-level performance by an average of 1.4% on three correlation metrics"
  - [corpus] Moderate evidence - related work on video-text alignment shows temporal context helps, but causal context is novel.
- Break condition: If causal relationships don't provide additional useful information beyond what temporal relationships offer, the performance improvements would not materialize.

## Foundational Learning

- Concept: Event causality identification
  - Why needed here: The entire approach depends on accurately extracting causal relationships between story events
  - Quick check question: Can you explain the difference between temporal ordering and causal relationships in stories?

- Concept: Prompt engineering with LLMs
  - Why needed here: The method relies on carefully crafted prompts to guide ChatGPT in extracting event causality
  - Quick check question: What are the key components of an effective prompt for extracting structured information from LLMs?

- Concept: Video-text alignment and multimodal understanding
  - Why needed here: One of the downstream tasks demonstrates how causal context improves multimodal story understanding
  - Quick check question: How does incorporating contextual information (temporal or causal) improve video-text alignment performance?

## Architecture Onboarding

- Component map:
  ChatGPT with event graph prompt -> Event causality extraction -> Downstream task models (story evaluation, video-text alignment) -> Performance metrics

- Critical path:
  1. Story text input
  2. ChatGPT prompt generation
  3. Event graph extraction
  4. Integration with downstream model
  5. Performance evaluation

- Design tradeoffs:
  - Using few-shot learning with ChatGPT vs. supervised fine-tuning
  - Simple prompt vs. complex multi-prompt ensemble approach
  - Causal context vs. temporal context in multimodal tasks

- Failure signatures:
  - Poor correlation between extracted causality and human annotations
  - No improvement or degradation in downstream task performance
  - ChatGPT generating inconsistent or illogical causal relationships

- First 3 experiments:
  1. Test ChatGPT event graph extraction on GLUCOSE dataset and compare to supervised baselines
  2. Apply extracted causality to story quality evaluation task and measure correlation improvements
  3. Integrate causal context into video-text alignment model and measure performance changes against temporal-only baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the generalizability of ChatGPT-extracted event causality compare to supervised models when applied to stories from different domains or cultures?
- Basis in paper: [inferred] The paper states that ChatGPT can generalize to stories of different types and lengths, but it doesn't explicitly compare its performance to supervised models on stories from different domains or cultures.
- Why unresolved: The paper only tests the models on a dataset of children's stories and does not explore how they would perform on stories from different domains or cultures.
- What evidence would resolve it: A direct comparison of the performance of ChatGPT and supervised models on a diverse set of stories from different domains and cultures.

### Open Question 2
- Question: What is the impact of using different definitions of causality (e.g., multifactorial, interventionist, probabilistic, counterfactual) on the performance of ChatGPT in extracting event causality?
- Basis in paper: [explicit] The paper mentions that they experimented with different definitions of causality but does not report the results of these experiments.
- Why unresolved: The paper does not provide any quantitative or qualitative analysis of how different definitions of causality affect the performance of ChatGPT.
- What evidence would resolve it: A detailed analysis of the performance of ChatGPT using different definitions of causality, including both quantitative metrics and qualitative examples.

### Open Question 3
- Question: How does the quality of the event causality extracted by ChatGPT compare to human-annotated causality in terms of capturing the nuances and complexities of causal relationships in stories?
- Basis in paper: [inferred] The paper mentions that ChatGPT can extract event causality with high accuracy, but it doesn't explicitly compare the quality of its output to human-annotated causality.
- Why unresolved: The paper only evaluates the accuracy of ChatGPT in extracting event causality and does not assess the quality of the extracted causality in terms of capturing the nuances and complexities of causal relationships.
- What evidence would resolve it: A qualitative analysis of the event causality extracted by ChatGPT and human-annotated causality, focusing on the nuances and complexities of the causal relationships captured.

## Limitations

- Limited empirical validation of cognitive claims: While citing cognitive science research, there's no direct experimental validation that extracted causal structures align with human cognitive processing.
- Prompt engineering details: The paper doesn't provide specific prompt templates or comprehensive ablations showing which prompt components are most critical.
- Dataset bias concerns: Evaluation relies heavily on the GLUCOSE dataset, which may not represent the full diversity of narrative structures found in real-world stories.

## Confidence

**High confidence**: The basic claim that event causality can be extracted from stories using ChatGPT with reasonable accuracy (75-82% F1 score) is well-supported by the results.

**Medium confidence**: The claim that extracted causality improves story quality evaluation is moderately supported, showing consistent though modest improvements across correlation metrics.

**Medium confidence**: The claim that causal context improves video-text alignment performance is supported by the 8% improvements, but the evaluation only considers one specific multimodal dataset and task.

## Next Checks

1. **Cross-dataset validation**: Test the event causality extraction method on multiple narrative datasets (not just GLUCOSE) and measure consistency in extraction quality across domains like news articles, literature, and social media stories.

2. **Human alignment study**: Conduct a human evaluation study comparing the ChatGPT-extracted causal structures against human annotations specifically measuring whether the extracted causality aligns with how humans understand story comprehension and memory.

3. **Ablation on prompt components**: Systematically vary prompt components (e.g., graph format requirements, causality specification, examples) to identify which elements are most critical for extraction quality and downstream task performance.