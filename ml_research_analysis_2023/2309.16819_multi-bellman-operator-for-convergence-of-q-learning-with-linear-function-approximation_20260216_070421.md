---
ver: rpa2
title: Multi-Bellman operator for convergence of $Q$-learning with linear function
  approximation
arxiv_id: '2309.16819'
source_url: https://arxiv.org/abs/2309.16819
tags:
- q-learning
- learning
- approximation
- function
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the convergence issues in Q-learning with
  linear function approximation, a problem that has persisted despite decades of research.
  The authors introduce a novel multi-Bellman operator that extends the traditional
  Bellman operator and show that under certain conditions, the projected multi-Bellman
  operator becomes contractive, providing improved fixed-point guarantees compared
  to the Bellman operator.
---

# Multi-Bellman operator for convergence of $Q$-learning with linear function approximation

## Quick Facts
- **arXiv ID**: 2309.16819
- **Source URL**: https://arxiv.org/abs/2309.16819
- **Reference count**: 39
- **Primary result**: Introduces multi-Bellman operator enabling Q-learning convergence with linear function approximation

## Executive Summary
This paper addresses the long-standing convergence issues in Q-learning with linear function approximation. The authors introduce a novel multi-Bellman operator that extends the traditional Bellman operator, showing that under certain conditions, the projected multi-Bellman operator becomes contractive. They propose the multi Q-learning algorithm which converges to the fixed-point of this projected operator, yielding solutions of arbitrary accuracy. The algorithm is validated on classic control environments.

## Method Summary
The method introduces a multi-Bellman operator Hn defined recursively by applying the Bellman operator n times. Under certain conditions (particularly when the feature covariance matrix is invertible), the projected multi-Bellman operator becomes contractive in the μ-norm for sufficiently large n. The multi Q-learning algorithm uses n-step targets computed by simulating n-step trajectories and trying all actions at each step. The algorithm updates parameters using stochastic approximation, and convergence is proven through Lyapunov analysis and stochastic approximation theory.

## Key Results
- Multi-Bellman operator Hn with sufficient n becomes contractive under projection
- Multi Q-learning converges to the unique fixed-point of projected multi-Bellman operator
- As n increases, multi Q-learning solution approaches optimal Q-values arbitrarily closely

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The multi-Bellman operator Hn with sufficiently large n becomes contractive under projection, enabling a unique fixed-point solution.
- **Mechanism**: By recursively applying the Bellman operator n times, Hn accumulates discount factors γn, which dominate the contraction behavior. The projection step then ensures the operator maps back into the linear function approximation space while preserving contraction.
- **Core assumption**: The covariance matrix of the features Eµ[ϕ(x,a)ϕᵀ(x,a)] is invertible and the data distribution has no shift (i.i.d. samples from µ).
- **Evidence anchors**:
  - [abstract] "By exploring the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes contractive"
  - [section 3.1] Proposition 1 proves that for N ∈ ℕ, Proj Hn is a contraction in the µ-norm for all n ≥ N
- **Break condition**: If the feature covariance is singular, or if the data distribution shifts during training (violating Assumption 2), the contraction property may fail and convergence breaks.

### Mechanism 2
- **Claim**: The multi Q-learning update with n-step targets converges to the unique fixed-point of the projected multi-Bellman operator.
- **Mechanism**: The algorithm builds n-step targets that sample all actions along an n-step trajectory, effectively planning with fixed depth and full breadth. This targets the true multi-step return under the optimal policy, reducing bias from single-step approximation.
- **Core assumption**: The ordinary differential equation derived from the expected update has a unique globally asymptotically stable equilibrium.
- **Evidence anchors**:
  - [abstract] "demonstrate that this algorithm converges to the fixed-point of the projected multi-Bellman operator"
  - [section 4] Theorem 1 shows convergence under Assumptions 1-3 via a Lyapunov argument and stochastic approximation theory
- **Break condition**: If the maximum depth n is insufficient for contraction (n < N), or if learning rates don't satisfy ∑αt = ∞ and ∑αt² < ∞, the update may diverge.

### Mechanism 3
- **Claim**: As n increases, the multi Q-learning solution approaches the projected optimal Q-values arbitrarily closely.
- **Mechanism**: The error bound in Proposition 2 shows that ∥q* − q̃ωn∥ decays exponentially with n, scaled by the contraction factor γn and inversely with the minimum visit probability µmin.
- **Core assumption**: The multi-Bellman projection remains well-behaved as n grows, and the approximation error shrinks with deeper lookaheads.
- **Evidence anchors**:
  - [section 3.1] Proposition 2 and Corollary 2 formalize the error decay and convergence of ωn to ω*
  - [abstract] "solutions of arbitrary accuracy"
- **Break condition**: If γ is too close to 1 or if the feature norm ϕmax is large relative to the smallest eigenvalue of the covariance, the error bound may not shrink fast enough for practical n.

## Foundational Learning

- **Concept**: Bellman operator contraction in ∞-norm with factor γ
  - Why needed here: Provides the baseline for understanding why Hn contracts faster (∝γn) and when projection can restore contractivity in µ-norm
  - Quick check question: Why does H contract in ∞-norm but not necessarily in µ-norm without projection?

- **Concept**: Stochastic approximation convergence conditions (Lipschitz updates, martingale noise, stable ODE equilibrium)
  - Why needed here: Theorem 0 (reproduced from Borkar) is the theoretical backbone proving multi Q-learning converges; all four conditions must be verified
  - Quick check question: What role does the martingale difference condition play in preventing bias accumulation in the update sequence?

- **Concept**: Linear function approximation and projection operator properties
  - Why needed here: Ensures the projection step exists (invertibility of feature covariance) and characterizes how close the projected solution is to the true optimal Q-values
  - Quick check question: How does the matrix E[ϕϕᵀ]⁻¹ relate to the stability of the projected operator?

## Architecture Onboarding

- **Component map**: (state, action) -> ϕ -> ℝᵏ -> ω -> Q-values -> n-step targets -> update ω

- **Critical path**:
  1. Sample (x,a) from replay buffer
  2. Simulate n-step trajectory to build τⁿ target
  3. Compute TD error: τⁿ − ϕᵀω
  4. Update ω ← ω + α[ϕ(τⁿ − ϕᵀω)]
  5. Repeat until convergence

- **Design tradeoffs**:
  - Larger n → better convergence but exponential computational cost in action space size
  - Fixed vs. adaptive depth: fixed depth simplifies theory but may waste computation
  - On-policy vs. off-policy data: i.i.d. assumption is strict; replay buffers help but may slow distribution shift

- **Failure signatures**:
  - Oscillating or diverging parameters → n too small, or data not i.i.d.
  - Slow convergence → learning rate too small, or n barely above threshold N
  - Poor final performance → insufficient feature richness, or γ close to 1 weakening contraction

- **First 3 experiments**:
  1. Replicate ω → 2ω counter-example: verify Q-learning (n=1) diverges, multi Q-learning (n≥4) converges
  2. Vary n on CartPole: plot return vs. n to see performance plateau or improvement
  3. Stress-test Assumption 2: introduce data distribution shift, measure impact on convergence stability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Under what conditions can Multi Q-learning be adapted to use a learned model of the environment rather than requiring a known model?
- **Basis in paper**: [explicit] The paper mentions that Multi Q-learning can be combined with learned models, stating "it is even possible to use non-parametric models, such as replay buffers, to sample transitions and rewards, without prejudice of the convergence established."
- **Why unresolved**: While the paper acknowledges the possibility of using learned models, it does not provide specific conditions or guarantees for convergence when using such models.
- **What evidence would resolve it**: A theoretical analysis showing under what conditions a learned model (parametric or non-parametric) can be used without compromising the convergence guarantees of Multi Q-learning.

### Open Question 2
- **Question**: How does the performance of Multi Q-learning with non-linear function approximation compare to relevant policy-free reinforcement learning algorithms like DQN?
- **Basis in paper**: [inferred] The paper suggests this as a future work direction, stating "It would also be interesting to analyze multi Q-learning with non-linear function approximation, theoretically and empirically."
- **Why unresolved**: The paper does not provide any empirical or theoretical analysis of Multi Q-learning with non-linear function approximation.
- **What evidence would resolve it**: Empirical comparisons between Multi Q-learning with non-linear function approximation and DQN on benchmark tasks, along with theoretical analysis of convergence conditions.

### Open Question 3
- **Question**: What is the trade-off between computational efficiency and theoretical guarantees when not performing every action on every state along n-step trajectories in Multi Q-learning?
- **Basis in paper**: [explicit] The paper acknowledges this trade-off, stating "Multi Q-learning does not hold so benign properties when we are unable to perform every action on every state along n step trajectories."
- **Why unresolved**: The paper does not provide a detailed analysis of the computational benefits versus the loss of theoretical guarantees when relaxing the requirement to try every action on every state.
- **What evidence would resolve it**: A study comparing the computational efficiency and performance of Multi Q-learning with and without the requirement to try every action on every state, along with an analysis of the impact on convergence guarantees.

## Limitations

- Strong assumptions about i.i.d. data sampling limit applicability to online reinforcement learning
- Multi-step target computation scales exponentially with action space size, making it impractical for domains with many actions
- The paper doesn't provide practical guidance on determining the threshold n required for contractivity in specific MDPs

## Confidence

**High Confidence**: The mechanism by which multi-step Bellman operators become contractive (Mechanism 1) is well-supported by the theoretical analysis and Proposition 1. The convergence proof for multi Q-learning (Mechanism 2) follows standard stochastic approximation theory and is rigorously established in Theorem 1.

**Medium Confidence**: The error bound analysis showing that solutions approach optimal values with increasing n (Mechanism 3) is mathematically sound, but its practical significance depends heavily on the relationship between γ, feature properties, and the required n for acceptable accuracy.

**Low Confidence**: The practical performance claims for classic control environments lack quantitative results and comparisons to baselines, making it difficult to assess real-world effectiveness beyond theoretical guarantees.

## Next Checks

1. **Contractivity threshold verification**: Systematically vary n and measure the spectral radius of the projected multi-Bellman operator to empirically identify the minimum n required for contraction in different MDPs.

2. **Distribution shift robustness**: Modify the replay buffer to introduce controlled data distribution shifts and measure the impact on convergence speed and stability compared to the i.i.d. case.

3. **Computational scaling analysis**: Implement the multi-step target computation for MDPs with varying action space sizes and measure the exponential computational cost to quantify the practical limitations of the approach.