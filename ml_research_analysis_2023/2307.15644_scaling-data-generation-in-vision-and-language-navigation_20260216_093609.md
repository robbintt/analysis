---
ver: rpa2
title: Scaling Data Generation in Vision-and-Language Navigation
arxiv_id: '2307.15644'
source_url: https://arxiv.org/abs/2307.15644
tags:
- navigation
- data
- agent
- learning
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ScaleVLN, a large-scale data generation paradigm
  for vision-and-language navigation (VLN) training. ScaleVLN applies 1200+ photo-realistic
  environments from HM3D and Gibson datasets, builds fully traversable navigation
  graphs, and generates 4.9 million instruction-trajectory pairs using a simple LSTM-based
  model.
---

# Scaling Data Generation in Vision-and-Language Navigation

## Quick Facts
- arXiv ID: 2307.15644
- Source URL: https://arxiv.org/abs/2307.15644
- Reference count: 40
- Primary result: Achieves 80% success rate on R2R test split, a new state-of-the-art

## Executive Summary
This paper introduces ScaleVLN, a large-scale data generation paradigm for training vision-and-language navigation (VLN) agents. By leveraging 1200+ photo-realistic environments from HM3D and Gibson datasets, building fully traversable navigation graphs, and generating 4.9 million instruction-trajectory pairs, ScaleVLN significantly improves VLN agent performance. The method achieves state-of-the-art results across multiple VLN tasks including R2R, CVDN, REVERIE, and R2R-CE, while reducing the generalization gap between seen and unseen environments to less than 1%.

## Method Summary
ScaleVLN generates large-scale training data for VLN by first building fully traversable navigation graphs in 1200+ photo-realistic environments from HM3D and Gibson datasets. The method uses an excessive viewpoint sampling and aggregation algorithm to create dense graphs, then applies image recovery using Co-Modulated GAN to fix corrupted rendered images. Trajectories are sampled from these graphs and paired with instructions generated by an LSTM-based model. Agents are trained using a combination of pre-training with proxy tasks (MLM, MRM, SAP) and fine-tuning on the augmented data.

## Key Results
- Achieves 80% success rate on R2R test split, a new state-of-the-art
- Reduces generalization gap between seen and unseen environments to less than 1%
- Sets new state-of-the-art results on CVDN, REVERIE, and R2R-CE tasks
- Demonstrates that adding more scenes and data brings steady performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing visual data diversity from new environments improves generalization more than increasing data quantity from the same environments.
- Mechanism: Diverse environments expose the agent to varied visual features, reducing overfitting to the fixed distribution of the original training scenes.
- Core assumption: The visual features from new environments are sufficiently different from the original ones to prevent overfitting.
- Evidence anchors:
  - [abstract]: "learning from additional scenes helps agents to generalize better to unseen environments than simply learning from more data."
  - [section]: "Compare Method#4 to Method#5, #6, and #7; it is clear that it is very beneficial to keep the data augmented from the addition environments (HM-E) in fine-tuning (+2.51% SR in Val-Unseen)."
- Break condition: If new environments have similar visual features to original ones, the benefit of diversity is lost.

### Mechanism 2
- Claim: High-quality navigation graphs with dense viewpoints and fully traversable edges improve the agent's ability to learn effective trajectories.
- Mechanism: Dense graphs provide more varied training samples and prevent the agent from learning to navigate through obstacles, which would be impossible in real environments.
- Core assumption: The navigation graph accurately reflects the traversable space in the environment.
- Evidence anchors:
  - [abstract]: "a fully traversable navigation graph is crucial to improve the agent's performance for downstream tasks with detailed instructions like R2R."
  - [section]: "We argue that a high-quality navigation graph needs to satisfy a number of criteria, including high coverage of the space to maximize visual diversity and fully traversable edges..."
- Break condition: If the graph contains many non-traversable edges or misses key regions, the agent will learn incorrect navigation strategies.

### Mechanism 3
- Claim: Recovering high-quality images from faulty rendered images improves agent performance, especially in low-quality environments.
- Mechanism: Better image quality provides clearer visual cues for the agent to learn visual-language grounding and navigation policies.
- Core assumption: The image recovery model (Co-Modulated GAN) can generate visually realistic images from corrupted ones.
- Evidence anchors:
  - [abstract]: "recovering photo-realistic images from the rendered images is very beneficial, especially for the low-quality 3D scans from the Gibson environments."
  - [section]: "We formulate this task as an image-to-image translation problem, where the model takes a rendered image as input and learns to recover the broken, distorted, or missing regions."
- Break condition: If the image recovery model introduces artifacts or unrealistic features, it may confuse the agent.

## Foundational Learning

- Concept: Vision-and-Language Navigation (VLN)
  - Why needed here: Understanding the VLN task is crucial to grasp the problem this paper addresses and the solutions it proposes.
  - Quick check question: What is the main challenge in VLN that this paper aims to solve?

- Concept: Navigation Graphs
  - Why needed here: Navigation graphs are the foundation for sampling trajectories in discrete environments, which are used to train the agent.
  - Quick check question: How does the quality of a navigation graph affect the quality of the sampled trajectories?

- Concept: Imitation Learning
  - Why needed here: The paper uses imitation learning to train the agent on the augmented data, so understanding this method is key.
  - Quick check question: What is the main advantage of using imitation learning for training VLN agents?

## Architecture Onboarding

- Component map: 3D Environments (HM3D, Gibson) -> Navigation Graph Construction -> Image Recovery (Co-Modulated GAN) -> Trajectory Sampling -> Instruction Generation (LSTM-based model) -> Agent Training (Pre-training + Fine-tuning) -> Downstream VLN Tasks (R2R, CVDN, REVERIE, R2R-CE)

- Critical path: 3D Environments → Navigation Graph Construction → Image Recovery → Trajectory Sampling → Instruction Generation → Agent Training → Downstream VLN Tasks

- Design tradeoffs:
  - Dense vs. sparse navigation graphs: Dense graphs provide more training samples but increase computational cost.
  - Image recovery vs. raw images: Image recovery improves visual quality but adds complexity and potential for introducing artifacts.

- Failure signatures:
  - Poor agent performance: Could indicate issues with navigation graph quality, image recovery, or agent training.
  - Agent getting stuck or navigating through obstacles: Likely indicates issues with navigation graph construction.

- First 3 experiments:
  1. Build a navigation graph for a single environment and visualize it to check coverage and traversability.
  2. Apply the image recovery model to a set of rendered images and visually inspect the results.
  3. Sample a small set of trajectories from the graph and generate instructions to check the quality of the augmented data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the generated navigational instructions impact the performance of the VLN agent in downstream tasks?
- Basis in paper: [explicit] The paper mentions that the generated instructions are of low quality but show a large influence on learning to navigate, implying that pairing the augmented trajectories with better instructions could be promising future work.
- Why unresolved: The paper only experiments with two simple models for generating instructions and does not explore the impact of higher quality instructions on the agent's performance.
- What evidence would resolve it: Conducting experiments with different instruction generation models, including more advanced models like GPT-2, and comparing their impact on the agent's performance in various VLN tasks.

### Open Question 2
- Question: What is the optimal balance between the quantity of augmented scenes and the quantity of training data in improving the agent's performance?
- Basis in paper: [inferred] The paper shows that adding more scenes and data can bring a steady performance gain to the agent, but also suggests that having more diverse environments for learning VLN is important.
- Why unresolved: The paper does not provide a clear answer on the optimal balance between the quantity of augmented scenes and the quantity of training data, and how this balance affects the agent's performance.
- What evidence would resolve it: Conducting experiments with different ratios of augmented scenes to training data and evaluating the agent's performance to find the optimal balance.

### Open Question 3
- Question: How does the ScaleVLN approach generalize to other VLN tasks with different types of instructions and action spaces?
- Basis in paper: [explicit] The paper mentions that the ScaleVLN approach achieves new state-of-the-art results on various VLN tasks, including CVDN and R2R-CE, which have different styles of instructions and action spaces.
- Why unresolved: The paper does not provide a detailed analysis of how the ScaleVLN approach performs on other VLN tasks with different types of instructions and action spaces, and what factors contribute to its generalization ability.
- What evidence would resolve it: Conducting experiments on additional VLN tasks with different types of instructions and action spaces, and analyzing the factors that contribute to the ScaleVLN approach's generalization ability.

## Limitations
- The method relies on existing datasets (HM3D and Gibson) which may limit the novelty of environments
- Image recovery model may introduce artifacts or unrealistic features that could confuse the agent
- Navigation graphs are discrete approximations of continuous spaces, which may not fully capture real-world navigation complexity
- The method requires significant computational resources for building graphs, recovering images, and training agents

## Confidence
- High confidence in the overall effectiveness of ScaleVLN, given consistent improvements across multiple VLN tasks
- Medium confidence in the claim that increasing visual diversity from new environments is more beneficial than increasing data quantity from the same environments
- Low confidence in the long-term stability of the image recovery model's performance

## Next Checks
1. Evaluate agent performance on truly novel, never-before-seen environments to confirm generalization beyond the HM3D and Gibson datasets
2. Conduct a user study to assess the quality and naturalness of the generated instructions, ensuring they are interpretable by humans
3. Test the scalability of the method to even larger numbers of environments and instruction-trajectory pairs to determine if there is a point of diminishing returns