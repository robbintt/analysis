---
ver: rpa2
title: Benchmarking Cognitive Biases in Large Language Models as Evaluators
arxiv_id: '2309.17012'
source_url: https://arxiv.org/abs/2309.17012
tags:
- each
- bias
- system
- human
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can be used as automatic evaluators
  for text generation tasks, but their evaluations may be influenced by cognitive
  biases that reduce their reliability. This paper introduces COBBLEr, a benchmark
  to measure six cognitive biases in LLM evaluations, including order bias, compassion
  fade, egocentric bias, salience bias, bandwagon effect, and attentional bias.
---

# Benchmarking Cognitive Biases in Large Language Models as Evaluators

## Quick Facts
- arXiv ID: 2309.17012
- Source URL: https://arxiv.org/abs/2309.17012
- Authors: 
- Reference count: 35
- Primary result: Most LLMs exhibit strong cognitive biases in evaluation tasks, with only 49.6% alignment with human preferences

## Executive Summary
This paper introduces COBBLEr, a benchmark designed to measure six cognitive biases in large language models used as automatic evaluators for text generation tasks. The study evaluates 15 LLMs across varying sizes using 50 question-answering examples from ELI5 and BigBench datasets. Results demonstrate that most models exhibit significant biases, with an average of 40% of comparisons showing bias effects. The findings reveal only 49.6% agreement between machine and human preferences, suggesting current LLMs are not reliable as unbiased automatic evaluators and highlighting the need for improved bias mitigation in future models.

## Method Summary
The study generates responses from 15 LLMs for 50 QA examples from ELI5 and BigBench datasets. These responses are then evaluated using pairwise comparisons where each LLM evaluator compares two responses and indicates preference. The benchmark measures both implicit biases (order bias, compassion fade, egocentric bias, salience bias) and induced biases (bandwagon effect, attentional bias) through specific prompt modifications. Human preference annotations from 6 AMT workers serve as a baseline for comparison. The evaluation outputs are processed to identify biased responses and calculate Rank-Biased Overlap (RBO) scores for measuring alignment between human and machine preferences.

## Key Results
- 40% of evaluations across all models showed strong indications of cognitive biases
- Only 49.6% agreement between human and machine preferences (RBO score)
- 11 out of 15 models were heavily influenced by irrelevant statistics in bandwagon effect experiments
- Most models showed position-based preferences, with significant order bias detected

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise comparison amplifies existing biases in LLM evaluators.
- Mechanism: When LLMs are asked to choose between two outputs, they do not evaluate absolute quality but relative preference, which exposes and magnifies biases like order bias and egocentric bias. The benchmark shows that in 40% of comparisons, models exhibit strong bias signals.
- Core assumption: Bias in pairwise comparison is stronger than in absolute scoring because relative positioning makes implicit preferences explicit.
- Evidence anchors:
  - [abstract]: "exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models)"
  - [section 5.1]: "we observe that most models (11/15) tend to be drawn towards either the first- or last-ordered model"
  - [corpus]: Weak. Only mentions "pairwise" in neighbor titles, no direct mechanism evidence.

### Mechanism 2
- Claim: Real model names trigger compassion fade and egocentric bias more strongly than anonymous aliases.
- Mechanism: When evaluators see recognizable model names (e.g., Vicuna, Alpaca), they are more likely to exhibit self-preference and bias based on identity rather than output quality. This is shown by comparing aliased vs. named evaluation conditions.
- Core assumption: Named identity cues activate self-referential processing in LLMs, increasing egocentric and identity-based biases.
- Evidence anchors:
  - [section 5.1]: "we see a large drop in self-preference for models in the largest size group ( >100B) models, which see a large increase in bias for each position"
  - [section 4.2]: "We then prompt the models to evaluate their own and other models’ responses"
  - [corpus]: Weak. No corpus mention of compassion fade or naming effects.

### Mechanism 3
- Claim: Induced biases (bandwagon effect, attentional bias) are introduced through adversarial prompt modifications.
- Mechanism: By adding false majority preference statements or irrelevant context, the benchmark induces evaluators to deviate from content-based judgment, revealing susceptibility to external influence.
- Core assumption: LLMs lack robust filtering of irrelevant or false contextual cues when making pairwise judgments.
- Evidence anchors:
  - [section 3.2]: "We add an additional sentence after the initial instruction stating a fake statistic"
  - [section 5.1]: "we observe that almost all models (11/15) are heavily influenced by irrelevant statistics"
  - [corpus]: Weak. Only mentions "cognitive biases" generically, no specific adversarial prompt evidence.

## Foundational Learning

- Concept: Cognitive bias benchmarking
  - Why needed here: To measure whether LLM evaluators make consistent, quality-based judgments or are swayed by irrelevant factors.
  - Quick check question: If a model prefers its own output 60% of the time in anonymized comparisons, is that evidence of bias?

- Concept: Pairwise vs. listwise evaluation
  - Why needed here: Pairwise is simpler and reduces complexity, but may amplify bias; listwise allows full ranking but is harder for small models.
  - Quick check question: Why does the paper switch to pairwise for most experiments?

- Concept: Rank-Biased Overlap (RBO)
  - Why needed here: To compare ranked lists where top positions are more important, matching how humans prioritize quality.
  - Quick check question: What parameter p=0.8 means in RBO?

## Architecture Onboarding

- Component map:
  Data pipeline: ELI5 + BigBench → 50 QA examples
  Generation module: 15 models → 750 responses
  Evaluation engine: Pairwise prompts → 42K comparisons
  Bias detection: Implicit (order, compassion, egocentric, salience) + Induced (bandwagon, attentional)
  Human study: AMT annotations → 300 rankings
  Scoring: RBO for human-LLM alignment

- Critical path:
  1. Generate responses from all models
  2. Create all unique pairs for pairwise evaluation
  3. Run each pair twice (swap order) to detect order bias
  4. Apply bias-specific prompt modifications for induced biases
  5. Aggregate results and compute bias proportions

- Design tradeoffs:
  - Pairwise reduces task complexity but doubles evaluation calls
  - Real model names increase bias detection sensitivity but may skew results
  - AMT workers provide human baselines but have lower IAA than expected

- Failure signatures:
  - Low valid response rate → model did not understand prompt
  - High proportion of "invalid" outputs → prompt format issue
  - RBO near 0.5 → human and machine preferences largely misaligned

- First 3 experiments:
  1. Run ORDER bias benchmark with GPT-4 as evaluator on 10 pairs, check for first-order bias > random threshold
  2. Test COMPASSION FADE by comparing aliased vs. named evaluations for Vicuna
  3. Induce BANDWAGON EFFECT on a small subset and verify >70% conformity to false majority

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific cognitive biases in LLM evaluators affect the overall quality and reliability of automated text evaluation across different domains?
- Basis in paper: [explicit] The paper introduces COBBLEr, a benchmark to measure six cognitive biases in LLM evaluations, and finds that most LLMs exhibit strong cognitive biases affecting their evaluation quality.
- Why unresolved: The paper focuses on a QA setting and does not explore how these biases impact evaluations in other domains such as mathematical reasoning or creative writing.
- What evidence would resolve it: Conducting similar bias evaluations across diverse domains and comparing the results to identify domain-specific bias patterns.

### Open Question 2
- Question: Can chain-of-thought reasoning or other de-biasing techniques effectively mitigate the cognitive biases identified in LLM evaluators?
- Basis in paper: [inferred] The paper mentions potential de-biasing methods as an area of future exploration but does not test their effectiveness.
- Why unresolved: The paper identifies biases but does not propose or evaluate solutions to mitigate them.
- What evidence would resolve it: Implementing and testing de-biasing techniques, such as chain-of-thought reasoning, on the same benchmark to measure improvements in evaluation quality.

### Open Question 3
- Question: How does the alignment between human and machine preferences change as LLM evaluators are fine-tuned or improved to reduce biases?
- Basis in paper: [explicit] The paper finds low agreement (49.6% RBO) between human and machine preferences, suggesting misalignment.
- Why unresolved: The paper does not investigate whether fine-tuning or improving LLMs can enhance alignment with human preferences.
- What evidence would resolve it: Fine-tuning LLMs using COBBLEr data to reduce biases and re-evaluating their alignment with human preferences using the same RBO metric.

## Limitations

- The study does not directly compare bias rates between pairwise and absolute scoring methods, leaving uncertainty about whether simpler evaluation formats might reduce bias amplification.
- Effect sizes and statistical significance of compassion fade and egocentric bias across different model sizes are not fully quantified in the results.
- The induced bias experiments rely on adversarial prompt modifications, but lack baseline comparisons to determine current LLM robustness without fine-tuning.

## Confidence

- **High Confidence**: The methodology for detecting implicit biases (order bias, compassion fade, egocentric bias, salience bias) is well-grounded in established cognitive bias literature and clearly implemented.
- **Medium Confidence**: The claim that pairwise evaluation amplifies bias is supported by observed results, but lacks direct experimental comparison to absolute scoring methods.
- **Low Confidence**: The effectiveness of induced bias manipulations (bandwagon effect, attentional bias) depends heavily on prompt engineering details that are not fully specified in the paper.

## Next Checks

1. Conduct a controlled experiment comparing bias rates in pairwise vs. absolute scoring evaluations to verify the amplification hypothesis.
2. Test the robustness of bias detection by anonymizing all model names consistently and measuring changes in egocentric and compassion fade bias.
3. Validate the induced bias results by creating a no-manipulation control condition and measuring the delta in bias rates.