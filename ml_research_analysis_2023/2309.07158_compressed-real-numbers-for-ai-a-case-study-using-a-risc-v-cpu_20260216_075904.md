---
ver: rpa2
title: 'Compressed Real Numbers for AI: a case-study using a RISC-V CPU'
arxiv_id: '2309.07158'
source_url: https://arxiv.org/abs/2309.07158
tags:
- vector
- format
- instruction
- risc-v
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that using 16-bit compressed numerical formats
  like bfloat16 as storage format for deep learning weights and activations can significantly
  improve memory bandwidth and cache utilization in vector-capable RISC-V processors.
  The proposed approach performs decompression directly within vector registers using
  RISC-V vector instructions, avoiding the need to load 32-bit values.
---

# Compressed Real Numbers for AI: a case-study using a RISC-V CPU

## Quick Facts
- arXiv ID: 2309.07158
- Source URL: https://arxiv.org/abs/2309.07158
- Reference count: 27
- Using 16-bit compressed formats like bfloat16 for deep learning weights and activations can significantly improve memory bandwidth and cache utilization in vector-capable RISC-V processors

## Executive Summary
This paper demonstrates that using 16-bit compressed numerical formats like bfloat16 as storage format for deep learning weights and activations can significantly improve memory bandwidth and cache utilization in vector-capable RISC-V processors. The proposed approach performs decompression directly within vector registers using RISC-V vector instructions, avoiding the need to load 32-bit values. Experiments on GEMM kernels show up to 97% performance improvement when using compressed storage compared to uncompressed 32-bit storage, depending on cache size and memory bandwidth.

## Method Summary
The approach involves storing deep learning weights and activations in compressed 16-bit formats (bfloat16), then decompressing them directly within vector registers using RISC-V vector instructions (vwmul for decompression, vnsrl for compression) just before computation. This avoids loading full 32-bit values from memory, reducing memory bandwidth usage. The method was evaluated using GEMM kernels with varying matrix sizes (128×128 to 512×512) and vector register lengths (4096 to 16384 bits) across different cache sizes and memory bandwidths using RISC-V simulation tools (Vehave, MUSA, Paraver).

## Key Results
- Up to 97% performance improvement in GEMM kernels when using compressed 16-bit storage vs uncompressed 32-bit storage
- Maximum benefit achieved with 512KB cache and 1GB/s memory bandwidth
- Improvement is highly dependent on cache size and memory bandwidth characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decompressing 16-bit bfloat16 values inside vector registers reduces memory bandwidth usage and improves cache efficiency.
- Mechanism: By loading compressed 16-bit operands instead of 32-bit values, twice as many elements fit in the same memory transfer, and decompression happens within vector registers using RISC-V vector instructions, avoiding full 32-bit loads.
- Core assumption: The decompression overhead (instruction latency) is less than the memory bandwidth savings from transferring fewer bits.
- Evidence anchors:
  - [abstract] "using 16-bit compressed numerical formats like bfloat16 as storage format for deep learning weights and activations can significantly improve memory bandwidth and cache utilization"
  - [section IV] "we propose a way to decompress a tensor of bfloat/posits just before computations, i.e., after the compressed operands have been loaded within the vector registers"
  - [corpus] Weak match - neighboring papers discuss mixed-precision but not the in-register decompression approach specifically.
- Break condition: If decompression instruction latency exceeds the time saved by reduced memory transfers, the approach loses its advantage.

### Mechanism 2
- Claim: In-register compression/decompression enables instruction overlapping in out-of-order vector processors.
- Mechanism: Independent processing phases for different data pieces can overlap (e.g., loading 16-bit data for piece n+1 while decompressing piece n), reducing idle time compared to 32-bit loads which take twice as long.
- Core assumption: The processor's out-of-order execution and vector register structure allow meaningful overlap between load, decompress, compute, compress, and store phases.
- Evidence anchors:
  - [section IV] "the interleaving will start with an increased latency" and Figure 7 shows overlapping sequences.
  - [section V] Results show up to 97% improvement with 512KB cache and 1GB/s bandwidth.
  - [corpus] No direct evidence - neighboring papers don't discuss instruction overlapping in this context.
- Break condition: If the processor is in-order or has limited vector register resources, overlapping cannot be effectively exploited.

### Mechanism 3
- Claim: The bfloat16 to binary32 conversion is computationally cheap due to bit-shift operations.
- Mechanism: Converting bfloat16 to binary32 requires a left shift of 16 bits with zero padding, and converting back requires a right shift of 16 bits - both are simple ALU operations.
- Core assumption: These conversions can be implemented as single vector instructions (vwmul for decompression, vnsrl for compression) with minimal latency.
- Evidence anchors:
  - [section III.A] "conversion between IEEE binary32 number and bfloat16 can be performed with a right-shift or a right zero padding"
  - [section IV] "The two steps have an inherent cost for the conversion we need to take into account the computation effort spent on and decompression and compression."
  - [corpus] No direct evidence - neighboring papers don't discuss the specific instruction costs.
- Break condition: If the processor lacks dedicated vector instructions for these conversions or if the instruction latency exceeds acceptable thresholds (as shown in Figure 12), the approach becomes less beneficial.

## Foundational Learning

- Concept: RISC-V vector extension (RVV) architecture and vector length agnosticism
  - Why needed here: Understanding how vector registers, vector length settings, and vector instructions work is crucial to grasp how in-register decompression operates and why it's beneficial.
  - Quick check question: What is the difference between a vector register and a scalar register in RISC-V, and how does the vector length agnostic approach affect portability?

- Concept: Floating-point number formats (binary32, bfloat16, posits)
  - Why needed here: The paper relies on understanding the structure of these formats to explain why bfloat16 can be efficiently compressed/decompressed and how posits differ in complexity.
  - Quick check question: How many bits are allocated to the sign, exponent, and fraction in bfloat16 compared to binary32, and why does this enable easy conversion?

- Concept: Memory hierarchy and bandwidth constraints in deep learning workloads
  - Why needed here: The motivation for compression stems from memory bandwidth limitations, so understanding cache sizes, memory latency, and bandwidth impacts is essential to evaluate when the approach works.
  - Quick check question: Why does a larger cache size and lower memory bandwidth lead to greater improvements when using compressed formats?

## Architecture Onboarding

- Component map: RISC-V processor with V extension -> Vector registers (4096-16384 bits) -> L1 cache (512KB-2.5MB) -> Memory subsystem (1-100GB/s bandwidth, 50-150ns latency) -> Simulation tools (Vehave, MUSA, Paraver)

- Critical path:
  1. Load compressed 16-bit data into vector registers
  2. Decompress to 32-bit using vector instructions (vwmul)
  3. Perform computation (GEMM in this case)
  4. Compress results back to 16-bit (vnsrl)
  5. Store compressed data

- Design tradeoffs:
  - Compressed storage reduces memory bandwidth but adds decompression overhead
  - Larger vector registers enable more data parallelism but increase register pressure
  - Higher cache sizes improve benefit but may not be available on all platforms
  - Instruction latency for compression/decompression must be balanced against memory savings

- Failure signatures:
  - Performance degradation when decompression instruction latency is high (>20ns in experiments)
  - Minimal improvement or regression with large caches (2.5MB) and high bandwidth (100GB/s)
  - Failure when using posit format without dedicated hardware instructions due to complex bit manipulations

- First 3 experiments:
  1. Run GEMM with 128x128 matrices using 4096-bit vector registers, compare compressed vs uncompressed performance
  2. Vary cache size from 512KB to 2.5MB with fixed 1GB/s bandwidth to identify sweet spot
  3. Test with different vector register lengths (4096, 8192, 16384 bits) to see impact on instruction count and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance improvement scale with different matrix sizes and vector register lengths when using compressed numerical formats?
- Basis in paper: [explicit] The paper mentions that experiments were conducted with matrices of sizes 128×128, 256×256, and 512×512, and vector register lengths of 4096, 8192, and 16384 bits.
- Why unresolved: The paper provides results for specific configurations but does not explore a wider range of matrix sizes and vector register lengths to determine the scalability of performance improvements.
- What evidence would resolve it: Comprehensive performance data for a broader range of matrix sizes and vector register lengths would help understand the scalability of the approach.

### Open Question 2
- Question: What is the impact of using posit formats instead of bfloat16 for compression in terms of performance and accuracy?
- Basis in paper: [explicit] The paper mentions that posit formats are considered but focuses primarily on bfloat16. It states that the conversion between posit and floating-point has a higher cost in terms of instructions.
- Why unresolved: The paper does not provide detailed experimental results comparing the performance and accuracy of posit formats with bfloat16 in the context of the proposed approach.
- What evidence would resolve it: Experimental results comparing the performance and accuracy of posit formats and bfloat16 when used for compression in the proposed approach would provide insights into the viability of posit formats.

### Open Question 3
- Question: How does the proposed approach perform with different memory bandwidth and cache size configurations?
- Basis in paper: [explicit] The paper discusses the impact of memory bandwidth and cache size on performance, showing that lower bandwidth and smaller cache sizes yield higher improvements.
- Why unresolved: The paper provides results for specific memory bandwidth and cache size configurations but does not explore a wide range of configurations to determine the robustness of the approach.
- What evidence would resolve it: Performance data for a broader range of memory bandwidth and cache size configurations would help understand the robustness and applicability of the approach in different scenarios.

## Limitations
- Performance benefits highly dependent on specific hardware configuration (cache size and memory bandwidth)
- Assumes availability of dedicated vector instructions for bfloat16 conversion which may not be available on all RISC-V implementations
- Based on simulation rather than real hardware, which may not capture all performance nuances

## Confidence
- **High Confidence**: The core claim that compressed formats reduce memory bandwidth usage is well-supported by the data and follows directly from the 2:1 compression ratio of bfloat16 to binary32.
- **Medium Confidence**: The claim about instruction overlapping providing additional benefits is plausible but relies on assumptions about out-of-order execution capabilities that weren't explicitly validated.
- **Medium Confidence**: The specific performance numbers (97% improvement) are based on simulation and apply to the specific test configuration; real-world benefits may vary significantly.

## Next Checks
1. **Hardware Validation**: Implement and benchmark the approach on real RISC-V hardware with V extension to verify simulation results and measure actual instruction latencies for compression/decompression operations.

2. **Energy Efficiency Analysis**: Measure power consumption and energy efficiency improvements alongside performance gains, as memory bandwidth reduction should also reduce energy usage, particularly in edge computing scenarios.

3. **Generalization to Other Kernels**: Test the compressed format approach with different deep learning workloads beyond GEMM (such as convolution operations and activation functions) to evaluate whether the memory bandwidth benefits translate across various neural network operations.