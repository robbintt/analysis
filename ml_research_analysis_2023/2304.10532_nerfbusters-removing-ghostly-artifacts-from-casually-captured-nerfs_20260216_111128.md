---
ver: rpa2
title: 'Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs'
arxiv_id: '2304.10532'
source_url: https://arxiv.org/abs/2304.10532
tags:
- evaluation
- training
- scene
- loss
- nerf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of floaters and poor geometry
  in casually captured NeRFs, which often occur when rendering novel views far from
  the training trajectory. To better evaluate this, the authors propose a new dataset
  and evaluation protocol where two camera trajectories are captured for each scene:
  one for training and one for evaluation.'
---

# Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs

## Quick Facts
- arXiv ID: 2304.10532
- Source URL: https://arxiv.org/abs/2304.10532
- Reference count: 40
- PSNR improvement: 17.00 → 17.99, SSIM improvement: 0.5267 → 0.6060 on evaluation split

## Executive Summary
Nerfbusters addresses a critical limitation in NeRFs trained from casual video captures - the prevalence of ghostly artifacts and poor geometry when rendering novel views far from the training trajectory. The authors propose a new evaluation protocol using separate training and evaluation camera trajectories, revealing artifacts that standard benchmarks miss. Their solution leverages a 3D diffusion model trained on synthetic ShapeNet data to provide geometric regularization during NeRF optimization through a novel Density Score Distillation Sampling (DSDS) loss, significantly improving both geometry quality and artifact removal compared to existing handcrafted regularizers.

## Method Summary
Nerfbusters combines a 3D diffusion model with a modified NeRF training pipeline. The diffusion model is trained on voxelized cubes from ShapeNet meshes to learn geometric priors. During NeRF optimization, the method samples local density cubes using importance sampling based on density values, converts them to binary occupancy, and passes them through the diffusion model. The denoised output serves as a target for the DSDS loss, which regularizes NeRF densities by encouraging occupied voxels where the diffusion model predicts surfaces and penalizing densities where it predicts empty space. Additionally, a visibility loss removes densities in regions not seen by training views, targeting floaters outside the training frustum.

## Key Results
- PSNR improvement from 17.00 to 17.99 on the proposed evaluation dataset
- SSIM improvement from 0.5267 to 0.6060 on the evaluation split
- Significant reduction in floaters and ghostly artifacts compared to vanilla Nerfacto
- Better depth and normal estimation accuracy in challenging novel view scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local 3D diffusion prior regularizes NeRF densities by enforcing geometric plausibility learned from synthetic data
- Mechanism: A 3D U-Net diffusion model is trained on voxelized local cubes extracted from ShapeNet meshes. During NeRF optimization, the method samples 32³ density cubes from non-empty regions, binarizes them, and passes them through the diffusion model to denoise. The denoised output acts as a target for the Density Score Distillation Sampling (DSDS) loss, which penalizes NeRF densities where the diffusion model predicts empty voxels and increases densities where it predicts occupied voxels
- Core assumption: The distribution of local 3D surface patches in synthetic ShapeNet data is representative enough of real-world geometry to provide useful regularization
- Evidence anchors:
  - [abstract] "we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization"
  - [section] "Given a cube of NeRF densities σ, we discretize the densities into binary occupancy... The Nerfbusters diffusion model then predicts the denoised cube x0"
  - [corpus] Weak evidence - no direct corpus papers address diffusion-based density regularization for NeRFs
- Break condition: If real-world geometry significantly deviates from synthetic ShapeNet distributions, the prior may introduce incorrect geometric constraints

### Mechanism 2
- Claim: Importance sampling with density-based weights focuses regularization on meaningful regions
- Mechanism: Instead of uniformly sampling cubes from the entire NeRF volume (which would be inefficient due to empty space), the method maintains a low-resolution grid of densities that tracks which regions contain non-empty content. This grid is updated with exponential moving average during training, and cubes are sampled proportionally to these density values
- Core assumption: Regions with higher densities in the NeRF are more likely to correspond to actual scene geometry that needs regularization
- Evidence anchors:
  - [section] "Sampling these cubes uniformly from the NeRF volume is inefficient... To enable more efficient sampling, we store a low-resolution grid of either accumulation weights or densities"
  - [section] "This importance sampling method comes with almost no added cost since we store the densities or weights along the rays already used for volume rendering"
  - [corpus] Weak evidence - no direct corpus papers address importance sampling strategies for diffusion-based NeRF regularization
- Break condition: If the density grid becomes biased toward artifacts or false positives, importance sampling may focus on the wrong regions

### Mechanism 3
- Claim: Visibility loss removes floaters by penalizing densities in regions not observed by any training view
- Mechanism: The method renders rays from a tight sphere around the training camera positions through the scene center and into the distance. For each 3D location along these rays, it checks whether that location is visible from at least one training view (using frustum checks). Densities at locations not visible from any training view are penalized
- Core assumption: Floaters typically appear in regions outside the convex hull of training views, and these regions can be identified by checking visibility from training cameras
- Evidence anchors:
  - [section] "We propose a simple loss that penalizes densities at 3D locations that are not seen by multiple training views"
  - [section] "This loss penalizes densities in regions not seen by training images"
  - [corpus] Weak evidence - while visibility-based regularization exists in NeRF literature, the specific implementation with sphere-based ray shooting is novel
- Break condition: If training views have significant occlusions or the scene has complex geometry that extends beyond simple visibility checks, some valid geometry might be incorrectly penalized

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: The method uses a diffusion model as the learned prior to denoise local 3D cubes and provide guidance for NeRF density regularization
  - Quick check question: How does the forward diffusion process in DDPMs differ from the reverse denoising process, and why is this distinction important for the DSDS loss implementation?

- Concept: Score Distillation Sampling (SDS)
  - Why needed here: The DSDS loss is inspired by SDS but adapted for unconditional density regularization rather than conditional image generation
  - Quick check question: What is the key difference between how SDS and DSDS handle the domain gap between continuous densities and discrete occupancy predictions?

- Concept: Importance Sampling
  - Why needed here: The method uses importance sampling to efficiently query local 3D cubes from non-empty regions of the NeRF volume
  - Quick check question: How does the exponential moving average update of the density grid help maintain sampling efficiency throughout training?

## Architecture Onboarding

- Component map:
  3D U-Net diffusion model (7.2M parameters, 28MB) trained on synthetic ShapeNet cubes -> Low-resolution density grid for importance sampling -> NeRF model (Nerfacto implementation) -> Volume rendering pipeline with modified sampling strategy -> Loss computation combining RGB reconstruction, visibility loss, and DSDS

- Critical path: NeRF training → Density grid update → Cube sampling → Diffusion denoising → DSDS loss computation → Backpropagation to NeRF

- Design tradeoffs:
  - Using local cubes (32³) vs global priors: Local cubes are simpler and more category-agnostic but require more samples to cover the scene
  - Binarizing densities vs continuous values: Binarization simplifies the diffusion model but loses density magnitude information
  - Importance sampling vs uniform sampling: Importance sampling is more efficient but requires maintaining and updating the density grid

- Failure signatures:
  - Persistent floaters in regions with sparse training views indicate the visibility loss isn't working effectively
  - Loss of fine geometric details suggests the diffusion prior is over-regularizing
  - Inconsistent geometry across rendered views indicates the local prior isn't maintaining 3D consistency

- First 3 experiments:
  1. Test the diffusion model's denoising capability on synthetic cubes with known ground truth to verify it learns meaningful geometric priors
  2. Validate the importance sampling strategy by comparing density distributions with and without the density grid
  3. Evaluate the visibility loss alone on a simple scene to confirm it can identify and remove floaters outside the training view frustum

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is Nerfbusters at handling transparent objects compared to regular objects, and what modifications might improve its performance with transparency?
- Basis in paper: [explicit] The paper notes that Nerfbusters removes transparent objects because they behave similarly to floaters, and the method cannot distinguish between the two due to lack of semantic information.
- Why unresolved: The paper does not provide experimental results or quantitative comparisons specifically for scenes with transparent objects, leaving the effectiveness of Nerfbusters in this domain unclear.
- What evidence would resolve it: Conducting experiments on datasets containing transparent objects (e.g., glass, liquids) and comparing Nerfbusters' performance to baselines and human perception of transparency would provide insights. Analyzing failure cases and developing semantic-aware modifications to the diffusion model could improve results.

### Open Question 2
- Question: Can the local 3D diffusion prior learned by Nerfbusters be extended to handle texture hallucination and inpainting for regions with low confidence or missing data?
- Basis in paper: [explicit] The paper acknowledges that Nerfbusters improves geometry but cannot edit texture, as it operates on densities. It suggests that colorization and inpainting low-confidence regions could be future work.
- Why unresolved: The paper does not explore methods for combining the geometric improvements of Nerfbusters with texture synthesis or inpainting techniques, leaving the potential for a more complete solution unaddressed.
- What evidence would resolve it: Integrating Nerfbusters with existing texture synthesis methods like 2D diffusion priors or 3D-consistent inpainting techniques and evaluating the combined approach on scenes with missing or low-quality textures would demonstrate the feasibility and effectiveness of this extension.

### Open Question 3
- Question: How does the performance of Nerfbusters scale with the complexity and size of the captured scene, and what are the limitations in terms of computational resources and memory usage?
- Basis in paper: [inferred] The paper mentions that the cube sampling strategies and cube sizes are important factors in the method's performance, but does not provide a comprehensive analysis of how the method scales with scene complexity or discuss its computational limitations.
- Why unresolved: The paper focuses on evaluating Nerfbusters on a specific dataset with relatively small scenes and does not explore its performance on larger, more complex scenes or discuss the computational requirements and memory usage in detail.
- What evidence would resolve it: Conducting experiments on scenes with varying complexity and size, measuring the computational time and memory usage, and analyzing the trade-offs between performance and resource consumption would provide insights into the scalability and limitations of Nerfbusters.

## Limitations
- The diffusion model is trained on synthetic ShapeNet data, raising questions about generalization to real-world geometry
- Cannot handle transparent objects effectively, as they are treated similarly to floaters
- Limited to 20 scenes in the proposed evaluation dataset, which may not represent the full diversity of casual captures
- No quantitative analysis of runtime overhead introduced by the 3D diffusion model

## Confidence

| Claim | Confidence |
|-------|------------|
| Visibility loss effectively removes floaters | High |
| Importance sampling strategy improves efficiency | Medium |
| ShapeNet diffusion priors generalize to real-world geometry | Low |

## Next Checks
1. **Cross-dataset generalization test**: Evaluate Nerfbusters on scenes from different 3D object datasets (like CO3D or ScanNet) to verify the ShapeNet prior doesn't introduce category-specific artifacts.
2. **Ablation of diffusion timestep**: Systematically vary the number of diffusion timesteps used during DSDS to determine the optimal tradeoff between denoising quality and computational cost.
3. **Runtime profiling**: Measure the actual wall-clock time overhead of running the 3D diffusion model during NeRF training across different scene scales to assess practical viability.