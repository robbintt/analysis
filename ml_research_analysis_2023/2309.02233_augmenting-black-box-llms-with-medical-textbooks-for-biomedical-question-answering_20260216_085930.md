---
ver: rpa2
title: Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering
arxiv_id: '2309.02233'
source_url: https://arxiv.org/abs/2309.02233
tags:
- medical
- query
- arxiv
- retrieval
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-AMT, a system that enhances black-box
  large language models with domain-specific medical knowledge from textbooks. The
  approach uses a pipeline with query augmentation, hybrid textbook retrieval, and
  a knowledge-refining reader module to improve medical question-answering accuracy.
---

# Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering

## Quick Facts
- arXiv ID: 2309.02233
- Source URL: https://arxiv.org/abs/2309.02233
- Authors: 
- Reference count: 6
- Improves medical QA accuracy by 11.6%-16.6% over GPT-3.5 baseline using domain-specific textbook knowledge

## Executive Summary
This paper introduces LLM-AMT, a system that enhances black-box LLMs with domain-specific medical knowledge from textbooks to improve biomedical question answering. The approach uses a pipeline with query augmentation, hybrid textbook retrieval, and a knowledge-refining reader module to boost accuracy. Experiments on three medical QA datasets show LLM-AMT improves accuracy by 11.6%-16.6% over GPT-3.5 baselines, and outperforms specialized Med-PaLM 2 by 2-3%. Despite being 100x smaller, medical textbooks outperform Wikipedia as a knowledge source, boosting performance by 7.8%-13.7%. Human evaluation shows a 16% reduction in hallucinations.

## Method Summary
The LLM-AMT system uses a three-stage pipeline: query augmentation, hybrid textbook retrieval, and knowledge-refining reader. Medical textbooks are preprocessed into paragraphs and used as the retrieval corpus. A hybrid retriever combining sparse (SPLADE), dense (DPR), and cross-encoder reranker components retrieves relevant passages. The reader module generates answers using chain-of-thought prompting and combines them through weighted majority voting. The system was evaluated on MedQA-USMLE, MedQA-MCMLE, and MedMCQA datasets.

## Key Results
- Improves accuracy by 11.6%-16.6% over GPT-3.5 baseline across three medical QA datasets
- Outperforms specialized Med-PaLM 2 by 2-3% despite being 100x smaller
- Medical textbooks outperform Wikipedia as knowledge source, boosting performance by 7.8%-13.7%
- Reduces hallucinations by 16% in human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating authoritative medical textbooks reduces hallucinations by providing precise, domain-specific information.
- Mechanism: The system retrieves relevant paragraphs from structured medical textbooks and uses them as context for the LLM to generate answers.
- Core assumption: Medical textbooks contain the most relevant and accurate domain knowledge compared to general sources like Wikipedia.
- Evidence anchors:
  - [abstract] "Despite being 100x smaller, medical textbooks as a retrieval corpus is proven to be a more effective knowledge database than Wikipedia in the medical domain, boosting performance by 7.8%-13.7%."
  - [section] "Textbooks offer richer and more specialized domain knowledge. In contrast to search engines like Google Search or Bing Search, the information in textbooks is more reliable."
  - [corpus] Weak - no direct corpus-level statistics on accuracy, only size comparison.

### Mechanism 2
- Claim: Query augmentation improves retrieval effectiveness by reformulating questions into medical terminology and expanding the scope of retrieval.
- Mechanism: The Query Augmenter rewrites the original question using medical terminology and expands it with a chain-of-thought approach.
- Core assumption: Medical terminology and expanded queries help the retriever find more relevant evidence from the textbook corpus.
- Evidence anchors:
  - [abstract] "The Query Augmenter utilizes LLMs to rewrite and expand the original question, generating more informative queries."
  - [section] "This approach preserves key information from the original question, such as details related to 'cardiac catheterization with stenting for unstable angina pectoris' and 'mottled, reticulated purplish discoloration of the feet' while facilitating the translation from general description to medical terminology."
  - [corpus] Weak - no quantitative evidence on how query augmentation improves retrieval performance specifically.

### Mechanism 3
- Claim: Combining multiple evidence sources through majority voting increases the robustness and accuracy of the final answer.
- Mechanism: The LLM Reader generates answers using each retrieved evidence independently, and then combines these answers using weighted voting based on confidence scores.
- Core assumption: Multiple evidence sources provide complementary information that can be aggregated to produce a more accurate answer than any single source.
- Evidence anchors:
  - [abstract] "We then combine N answers using a method based on majority voting..."
  - [section] "We then combine N answers using a method based on majority voting, as detailed below: Ans = arg maxk N∑i=1 I[ansi == k] ∗ pi k"
  - [corpus] Weak - no evidence on how voting performs compared to single evidence retrieval.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) architecture
  - Why needed here: The system relies on retrieving relevant medical knowledge from textbooks and using it to augment LLM responses.
  - Quick check question: What are the two main components of a RAG system and what does each do?

- Concept: Dense vs sparse text retrieval methods
  - Why needed here: The system uses both dense (DPR) and sparse (SPLADE) retrievers in its hybrid retrieval module.
  - Quick check question: How do dense and sparse retrievers differ in how they represent and match text?

- Concept: Chain-of-thought prompting
  - Why needed here: The Query Augmenter uses chain-of-thought prompting to expand queries and help the LLM generate more comprehensive answers.
  - Quick check question: What is the purpose of chain-of-thought prompting in LLM applications?

## Architecture Onboarding

- Component map: Query Augmenter → Hybrid Textbook Retriever (sparse+dense+rerank) → LLM Reader → Majority Voting → Final Answer

- Critical path: Query → Query Augmenter → Hybrid Textbook Retriever → LLM Reader → Majority Voting → Final Answer

- Design tradeoffs:
  - Using textbooks vs Wikipedia: Textbooks are smaller but more specialized and reliable; Wikipedia is larger but less domain-specific
  - Multiple retrievers vs single retriever: Hybrid approach improves coverage but adds complexity
  - Majority voting vs single answer: Improves robustness but requires more computation

- Failure signatures:
  - Poor retrieval quality: LLM answers don't match the medical domain or contain errors
  - Query augmentation failure: Rewritten queries don't improve retrieval effectiveness
  - Voting breakdown: Contradictory evidence leads to incorrect final answers

- First 3 experiments:
  1. Baseline test: Run the full pipeline on a small subset of questions and compare accuracy to GPT-3.5 baseline
  2. Retrieval ablation: Test each retriever component (sparse, dense, reranker) individually to measure contribution
  3. Query augmentation test: Compare performance with and without query augmentation on the same question set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of authoritative medical textbooks as a retrieval corpus compare to other specialized knowledge sources (e.g., medical databases, clinical guidelines) in terms of improving LLM performance in the medical domain?
- Basis in paper: Explicit
- Why unresolved: The paper focuses on comparing textbooks to Wikipedia but does not explore other potential knowledge sources.
- What evidence would resolve it: Empirical comparisons of LLM-AMT's performance using different specialized knowledge sources as retrieval corpora.

### Open Question 2
- Question: What is the impact of the size and depth of the textbook corpus on the performance of LLM-AMT, and is there an optimal balance between corpus size and domain specificity?
- Basis in paper: Inferred
- Why unresolved: The paper uses a specific set of 51 textbooks but does not explore the effects of varying the size or depth of the corpus.
- What evidence would resolve it: Systematic studies varying the number and scope of textbooks used in the retrieval corpus.

### Open Question 3
- Question: How does the performance of LLM-AMT scale with the size of the base LLM, and is there a point of diminishing returns in terms of accuracy improvements?
- Basis in paper: Inferred
- Why unresolved: The paper uses GPT-3.5 and GPT-4-Turbo but does not explore a wider range of LLM sizes or the relationship between model size and performance gains.
- What evidence would resolve it: Experiments comparing LLM-AMT's performance across a spectrum of LLM sizes, from smaller models to the largest available.

## Limitations

- Lack of ablation studies to quantify individual contributions of query augmentation, hybrid retrieval, and majority voting mechanisms
- Weak empirical evidence comparing medical textbooks vs Wikipedia as knowledge sources
- No exploration of alternative specialized knowledge sources beyond textbooks and Wikipedia

## Confidence

- **High confidence**: The hybrid retrieval architecture using SPLADE, DPR, and cross-encoder reranker is technically sound and well-documented in the retrieval literature.
- **Medium confidence**: The query augmentation mechanism shows promise but lacks quantitative evidence of its contribution.
- **Low confidence**: The comparison between medical textbooks and Wikipedia as knowledge sources is weak and based on size rather than controlled experiments.

## Next Checks

1. **Ablation study**: Run the pipeline with all three components and then systematically remove each component to measure individual contributions to accuracy gains.

2. **Knowledge source comparison**: Conduct a controlled experiment retrieving from both medical textbooks and Wikipedia using identical queries, then compare retrieval quality and downstream QA performance.

3. **Hallucination measurement validation**: Implement the human evaluation protocol to independently verify the 16% reduction in hallucinations.