---
ver: rpa2
title: Split and Rephrase with Large Language Models
arxiv_id: '2312.11075'
source_url: https://arxiv.org/abs/2312.11075
tags:
- bisect
- desse
- sprp
- sentences
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Split and Rephrase (SPRP) task, which
  aims to transform complex sentences into sequences of shorter, grammatical sentences
  while preserving the original meaning. The authors explore the use of large language
  models (LLMs) for this task, employing various strategies including fine-tuning
  pretrained LLMs of different sizes and using instruction-tuned LLMs in zero-shot
  and few-shot settings.
---

# Split and Rephrase with Large Language Models

## Quick Facts
- arXiv ID: 2312.11075
- Source URL: https://arxiv.org/abs/2312.11075
- Reference count: 22
- Key outcome: Large language models fine-tuned on SPRP datasets significantly outperform previous state-of-the-art models across multiple metrics including BERTScore, BLEU, SARI, and compliance.

## Executive Summary
This paper explores the application of large language models (LLMs) to the Split and Rephrase (SPRP) task, which involves transforming complex sentences into sequences of shorter, grammatical sentences while preserving meaning. The authors evaluate both fine-tuned pretrained LLMs and instruction-tuned LLMs across zero-shot and few-shot settings. Their experiments on DeSSE and BiSECT datasets demonstrate that fine-tuned LLMs achieve state-of-the-art performance, while instruction-tuned models show promise with few examples but lag in compliance. The study reveals that relatively small amounts of training data (2K-6K pairs) and moderate model sizes (around 7B parameters) can achieve strong results.

## Method Summary
The authors fine-tune various pretrained LLMs (Pythia 410M-12B, BLOOM 7.1B) using LoRA on SPRP datasets, employing a specific prompt format for training. They also evaluate instruction-tuned models (Alpaca 7B, Dolly 2.0 7B) in zero-shot and few-shot settings using in-context learning. The datasets are filtered to remove single-sentence outputs, and the models are evaluated using BERTScore, BLEU, SARI, and compliance metrics. Training data sizes are varied from 1K to 62K pairs to assess impact on performance.

## Key Results
- Fine-tuned LLMs significantly outperform previous state-of-the-art models across all evaluation metrics
- Instruction-tuned LLMs achieve promising results with few examples but lag behind in compliance scores
- Model performance shows diminishing returns with training data beyond 3K examples
- Parameter size shows only minor performance differences across the range tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can perform split-and-rephrase tasks with fewer parameters than previously thought necessary.
- Mechanism: Fine-tuning pretrained LLMs on task-specific prompts enables them to learn the structural transformation required for split-and-rephrase, leveraging their existing linguistic knowledge.
- Core assumption: The linguistic patterns and grammatical understanding learned during pretraining are transferable to the split-and-rephrase task.
- Evidence anchors:
  - [abstract] "our results demonstrate the strong potential of pretrained LLMs for SPRP, outperforming the state of the art by large margins across multiple metrics"
  - [section 5.1] "In terms of parameter size, on all three metrics there is a slight tendency towards increased accuracy as the number of parameters increases, although the differences are relatively minor overall"
- Break condition: If the pretraining corpus lacks sufficient examples of sentence decomposition or if the fine-tuning data is too domain-specific, transfer may fail.

### Mechanism 2
- Claim: Instruction-tuned LLMs can perform split-and-rephrase in zero-shot or few-shot settings, offering a data-efficient alternative.
- Mechanism: The instruction-tuning phase equips the model with a general understanding of task completion, allowing it to apply this knowledge to new tasks like split-and-rephrase with minimal examples.
- Core assumption: The instruction-tuning process generalizes task comprehension beyond the specific tasks it was trained on.
- Evidence anchors:
  - [abstract] "instruction-tuned LLMs achieve promising results with few examples, they lag behind in terms of compliance"
  - [section 5.2] "few-shot variants outperformed their zero-shot alternatives across the board"
- Break condition: If the in-context examples are too few or not representative, the model may not generalize the split-and-rephrase task correctly.

### Mechanism 3
- Claim: Training data size impacts LLM performance on split-and-rephrase, with diminishing returns after a certain point.
- Mechanism: Increasing the amount of training data allows the model to encounter more variations of complex sentences and their corresponding splits, improving its ability to generalize.
- Core assumption: The training data covers a diverse enough range of sentence structures and domains to be representative of real-world usage.
- Evidence anchors:
  - [section 5.3] "scores increased across metrics as training size increased, although by smaller increments from the 3K mark upwards"
  - [section 5.3] "Best results are split between the two largest training datasets on both BiSECT and DeSSE, indicative of a potential ceiling in terms of improvements from training data augmentation"
- Break condition: If the additional training data is not diverse or introduces noise, it may not contribute to performance gains and could even degrade results.

## Foundational Learning

- **Concept**: Sequence-to-sequence modeling
  - Why needed here: Split-and-rephrase is fundamentally a sequence-to-sequence task, where a complex sentence is transformed into a sequence of simpler sentences.
  - Quick check question: What are the key components of a sequence-to-sequence model, and how do they apply to the split-and-rephrase task?

- **Concept**: Fine-tuning vs. prompt engineering
  - Why needed here: Understanding the difference between adapting a pretrained model through fine-tuning and using prompt engineering to guide its behavior is crucial for selecting the right approach.
  - Quick check question: What are the advantages and disadvantages of fine-tuning versus prompt engineering for the split-and-rephrase task?

- **Concept**: Evaluation metrics for text generation
  - Why needed here: Accurately assessing the quality of split-and-rephrase output requires understanding metrics like BERTScore, BLEU, and SARI, as well as compliance measures.
  - Quick check question: How do BERTScore, BLEU, and SARI differ in their evaluation of text generation quality, and what are their limitations?

## Architecture Onboarding

- **Component map**: Pretrained LLM (Pythia/BLOOM) -> LoRA fine-tuning -> Prompt templates -> Evaluation metrics (BERTScore, BLEU, SARI, CPL) -> Filtered datasets (DeSSE/BiSECT)
- **Critical path**: 1) Load pretrained LLM and fine-tuning data, 2) Apply LoRA fine-tuning with task-specific prompts, 3) Generate split-and-rephrase output on test sets, 4) Evaluate using BERTScore, BLEU, SARI, and CPL
- **Design tradeoffs**:
  - Parameter size vs. performance: Larger models may not significantly outperform smaller ones
  - Fine-tuning vs. prompt engineering: Fine-tuning may yield better results but requires more data and resources
  - In-domain vs. out-of-domain training data: In-domain data may lead to better performance but could limit generalization
- **Failure signatures**:
  - Low compliance scores indicate the model is not splitting sentences as required
  - Poor BERTScore results suggest the model is not preserving the original meaning
  - High BLEU but low SARI may indicate the model is copying too much from the input without sufficient rephrasing
- **First 3 experiments**:
  1. Fine-tune a small Pythia model on DeSSE data and evaluate on both DeSSE and BiSECT test sets
  2. Compare zero-shot and few-shot performance of an instruction-tuned LLM on the DeSSE dataset
  3. Evaluate the impact of training data size on performance by fine-tuning a model with varying amounts of DeSSE data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between model size and training data size for the SPRP task?
- Basis in paper: [explicit] The paper explores different model sizes (410M to 12B parameters) and training data sizes (from 1K to 62K pairs), finding that models with around 7B parameters perform well with relatively small amounts of training data (2K-6K pairs).
- Why unresolved: While the paper provides insights into the impact of model size and training data size, it doesn't definitively determine the optimal balance for all scenarios. The optimal balance might depend on factors such as the complexity of the domain, the desired level of accuracy, and the available computational resources.
- What evidence would resolve it: Further experiments varying both model size and training data size systematically, while measuring performance on different SPRP datasets and domains, could help determine the optimal balance for specific use cases.

### Open Question 2
- Question: How does the choice of prompt format and instruction affect the performance of LLMs on the SPRP task?
- Basis in paper: [inferred] The paper mentions using a specific prompt format for fine-tuning and evaluating LLMs, but also notes that preliminary results with prompt variants did not lead to significantly different results. However, the authors acknowledge that further exploration on this matter is needed.
- Why unresolved: The paper doesn't provide a comprehensive analysis of different prompt formats and instructions, leaving open the question of how these factors might impact LLM performance on SPRP.
- What evidence would resolve it: Systematic experiments comparing different prompt formats and instructions, while measuring their impact on LLM performance on SPRP datasets, could help determine the most effective approaches.

### Open Question 3
- Question: How well do LLMs generalize to languages other than English for the SPRP task?
- Basis in paper: [inferred] The paper provides initial multilingual results using the BLOOM model on German, French, and Spanish datasets, but acknowledges that further evaluation is needed to assess the limitations of the approach in languages with limited coverage.
- Why unresolved: The paper's multilingual experiments are limited in scope and don't provide a comprehensive assessment of LLM performance on SPRP across different languages.
- What evidence would resolve it: Further experiments evaluating LLM performance on SPRP in a wider range of languages, using both existing datasets and machine-translated data, could help determine the generalizability of the approach.

## Limitations

- The study uses filtered versions of datasets that differ from original distributions, potentially introducing bias
- Evaluation relies heavily on automated metrics without detailed human evaluation methodology
- Comparison with previous work is complicated by different test sets being used
- Few-shot instruction-tuned experiments used only 5 examples, an extremely limited sample size
- The claim about requiring "fewer parameters than previously thought necessary" lacks direct comparison with similar-sized task-specific models

## Confidence

**High Confidence Claims:**
- Fine-tuned LLMs outperform previous state-of-the-art models on SPRP tasks across multiple metrics
- Compliance scores are generally higher for fine-tuned models than for few-shot instruction-tuned models
- Training data size has diminishing returns beyond certain thresholds (around 3K examples)

**Medium Confidence Claims:**
- Parameter size shows a slight tendency toward increased accuracy, though differences are relatively minor
- Instruction-tuned LLMs can achieve promising results with few examples, though they lag in compliance
- The proposed approach generates high-quality SPRP output as confirmed by human evaluations

**Low Confidence Claims:**
- The specific superiority of fine-tuned LLMs over instruction-tuned models with few examples
- The exact magnitude of improvement over previous work given dataset filtering differences
- The generalizability of results to domains outside the training data

## Next Checks

1. **Dataset Filtering Validation**: Re-run the experiments using the original, unfiltered test sets to verify that the reported improvements hold when single-sentence outputs are included. This would validate whether the filtering strategy introduces bias toward models that always split sentences.

2. **Human Evaluation Replication**: Conduct detailed human evaluations with multiple annotators, including calculation of inter-annotator agreement scores and systematic sampling across different complexity levels. This would verify the claimed superiority of LLM outputs over previous approaches.

3. **Parameter Efficiency Analysis**: Systematically compare fine-tuned LLMs against previous task-specific models of similar parameter counts (not just their original implementations) to rigorously test the claim about requiring "fewer parameters than previously thought necessary." This would involve training or fine-tuning smaller models specifically for direct comparison.