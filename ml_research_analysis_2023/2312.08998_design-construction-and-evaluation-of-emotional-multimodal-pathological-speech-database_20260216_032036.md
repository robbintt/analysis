---
ver: rpa2
title: Design, construction and evaluation of emotional multimodal pathological speech
  database
arxiv_id: '2312.08998'
source_url: https://arxiv.org/abs/2312.08998
tags:
- speech
- emotional
- emotion
- data
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces THE-POSSD, the first Chinese multimodal emotional
  pathological speech database containing controls and motor dysarthria patients.
  It includes audio, video, electroglottography, fNIRS, and multi-perspective information,
  with subjects expressing happy, sad, angry, and neutral emotions.
---

# Design, construction and evaluation of emotional multimodal pathological speech database

## Quick Facts
- **arXiv ID:** 2312.08998
- **Source URL:** https://arxiv.org/abs/2312.08998
- **Reference count:** 29
- **Primary result:** First Chinese multimodal emotional pathological speech database with controls and motor dysarthria patients, achieving 78% audio recognition accuracy for controls and 60% for patients.

## Executive Summary
This paper introduces THE-POSSD, the first Chinese multimodal emotional pathological speech database containing controls and motor dysarthria patients. It includes audio, video, electroglottography, fNIRS, and multi-perspective information, with subjects expressing happy, sad, angry, and neutral emotions. The database underwent subjective labeling for intelligibility, emotion type, and valence-arousal dimensions using a developed WeChat mini-program. Automatic recognition tests on speech and glottal data achieved average accuracy of 78% for controls and 60% for patients in audio, while 51% for controls and 38% for patients in glottal data, indicating the influence of disease on emotional expression.

## Method Summary
THE-POSSD was constructed using synchronized recordings of audio, video, electroglottography (EGG), and fNIRS from 29 controls and 39 dysarthria patients expressing four emotions. Subjects were elicited using audiovisual clips combined with Stanislavsky's emotional memory technique. Data was preprocessed using Cool Edit Pro for filtering and denoising. Features were extracted using IS09 (384 dimensions) for emotion classification and a custom 101-dimensional set for disease severity grading. Subjective labeling was performed via a WeChat mini-program for emotion type, intelligibility, and valence-arousal dimensions. Automatic recognition was tested using SVM and random forest classifiers.

## Key Results
- Average recognition accuracy of 78% for controls and 60% for patients in audio data
- Recognition accuracy of 51% for controls and 38% for patients in glottal data
- Subjective analysis justified emotion discrimination accuracy, speech intelligibility, and valence-arousal spatial distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal integration improves emotional and pathological speech analysis accuracy compared to unimodal methods.
- Mechanism: Combining acoustic, glottal, video, and fNIRS data captures complementary featuresâ€”acoustic data encodes prosody, glottal data reflects vocal fold behavior, video captures facial expressions, and fNIRS measures cortical oxygenation changes during emotional tasks.
- Core assumption: Different modalities capture independent variance in speech and emotion production; joint models can exploit this complementarity.
- Evidence anchors: Automatic recognition tested on speech and glottal data, with average accuracy of 78% for controls and 60% for patients in audio, while 51% for controls and 38% for patients in glottal data; research on multimodal pathological speech and emotion recognition; exploration of fNIRS in speech disorders.

### Mechanism 2
- Claim: Emotional stimuli elicitation via multimodal cues enhances emotional speech data quality in dysarthric patients.
- Mechanism: Using audiovisual clips combined with Stanislavsky's emotional memory technique induces more authentic and varied emotional expressions than text-only prompts, important for capturing pathology-specific emotion production deficits.
- Core assumption: Patients with motor dysarthria can engage in emotional recall and modulate their speech production accordingly, despite motor impairments.
- Evidence anchors: THE-POSSD employed an audio-visual combination of emotional segments as a medium for eliciting feelings, augmented by Stanislavsky's emotional memory approach; all emotional speech was labeled for intelligibility, types and discrete dimensional emotions.

### Mechanism 3
- Claim: Subjective labeling of emotion type and valence-arousal (VA) space improves objective evaluation and cross-study comparability.
- Mechanism: VA model annotation captures dimensional aspects of emotion (continuous valence/arousal) rather than just categorical labels, allowing finer-grained analysis of emotion expression differences between controls and dysarthric patients.
- Core assumption: VA space captures meaningful variance in emotional expression that categorical labels miss, and annotators can reliably rate these dimensions.
- Evidence anchors: For the discrete dimension scoring, valence-arousal (VA) model was applied as it is quite dominated in affective computing area; subjective analysis justifies from emotion discrimination accuracy, speech intelligibility, valence-arousal spatial distribution.

## Foundational Learning

- **Speech signal processing fundamentals (e.g., MFCCs, pitch extraction, glottal source analysis)**: Automatic recognition tests use IS09 feature sets (384 dimensions) and custom glottal feature sets; understanding these features is essential for interpreting recognition results. *Quick check:* What acoustic feature captures vocal tract resonances and is commonly used in emotion recognition?

- **Affective computing dimensional models (valence-arousal space)**: VA labeling and spatial distribution analysis rely on this framework; understanding it is key to interpreting the emotion annotation quality. *Quick check:* In the valence-arousal model, which quadrant typically corresponds to happy emotions?

- **Clinical assessment scales for dysarthria (FDA, MMSE, SCL-90)**: These scales are used to characterize subjects and correlate psychological status with speech pathology; knowing their purpose helps interpret the subjective analysis. *Quick check:* Which scale assesses psychological symptom severity and is used here to correlate with disease severity?

## Architecture Onboarding

- **Component map**: Data collection -> Signal preprocessing (Cool Edit Pro filtering/denoising) -> Feature extraction (IS09, glottal custom set) -> Subjective annotation (WeChat mini-program VA + emotion labels) -> Automatic recognition (SVM/RF classifiers) -> Correlation analysis (SCL-90 factors vs. severity)
- **Critical path**: Ensure synchronized multimodal recording -> Clean and label data -> Extract features -> Train/test classifiers -> Analyze subjective vs. objective results
- **Design tradeoffs**: More modalities increase data richness but also recording complexity and noise; subjective labeling adds quality control but is time-consuming; using WeChat mini-programs enables scalable annotation but may limit control over interface
- **Failure signatures**: Low emotion recognition accuracy in glottal vs. acoustic data may indicate EGG signal quality issues; poor VA spatial separation suggests annotation inconsistencies; missing video data for some subjects limits multimodal fusion
- **First 3 experiments**:
  1. Baseline: Train SVM/RF on acoustic features only, compare accuracy to multimodal fusion
  2. Ablation: Remove VA labels from training data, measure impact on emotion recognition
  3. Correlation: Use FDA scores to stratify patients, analyze if recognition accuracy varies by disease severity

## Open Questions the Paper Calls Out
- **Open Question 1**: What is the optimal combination of multimodal features (audio, video, EGG, fNIRS) for accurate emotion recognition in dysarthria patients?
- **Open Question 2**: How do different severities of dysarthria affect the temporal dynamics of emotional expression in speech?
- **Open Question 3**: What is the relationship between cortical activation patterns (fNIRS data) and subjective emotion ratings in dysarthria patients?

## Limitations
- Database lacks direct comparison to similar databases for benchmarking
- Automatic recognition accuracy varies significantly between modalities but lacks detailed error analysis or fusion results
- Subjective labeling was performed via WeChat mini-program but inter-annotator agreement metrics are not reported

## Confidence
- **High**: Database construction methodology, subject characterization using clinical scales, and basic recognition results are well-documented
- **Medium**: Claims about multimodal benefits and emotional elicitation methods are plausible but under-supported by experimental evidence
- **Low**: No comparative analysis with existing databases or ablation studies to isolate the impact of each modality on recognition performance

## Next Checks
1. Compute Cohen's kappa or Krippendorff's alpha for emotion and VA labels to quantify annotation quality
2. Retrain recognition models using individual modalities (audio-only, glottal-only, video-only, fNIRS-only) and compare to multimodal fusion to quantify complementarity gains
3. Replace SVM/RF with a lightweight CNN or transformer-based model and re-evaluate recognition accuracy to assess if reported results reflect methodological limits rather than data quality