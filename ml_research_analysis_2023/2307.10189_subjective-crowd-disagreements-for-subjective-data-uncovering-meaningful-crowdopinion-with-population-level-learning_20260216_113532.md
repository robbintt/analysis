---
ver: rpa2
title: 'Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful
  CrowdOpinion with Population-level Learning'
arxiv_id: '2307.10189'
source_url: https://arxiv.org/abs/2307.10189
tags:
- label
- data
- learning
- distributions
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of annotator disagreement in subjective
  datasets, proposing a method to preserve minority opinions in the learning pipeline.
  The proposed approach, CrowdOpinion, uses unsupervised learning to cluster similar
  items based on both language features and label distributions, sharing a label distribution
  among items in each cluster.
---

# Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning

## Quick Facts
- arXiv ID: 2307.10189
- Source URL: https://arxiv.org/abs/2307.10189
- Authors: 
- Reference count: 36
- One-line primary result: Mixing features and labels in clustering improves label distribution learning, as measured by KL-divergence across five subjective datasets.

## Executive Summary
This paper addresses the problem of annotator disagreement in subjective datasets by proposing CrowdOpinion, a two-stage approach that clusters items based on both language features and label distributions. The method aims to preserve minority opinions in the learning pipeline by sharing label distributions within clusters, which are then used to train a supervised classifier. Experiments on five datasets with varying levels of disagreement show that mixing features and labels in the clustering stage improves label distribution learning, as measured by KL-divergence. While cluster-based aggregation universally improves distributional learning, results for single-label learning are mixed, suggesting that either label features or label distributions alone may be adequate for clustering-based label regularization.

## Method Summary
CrowdOpinion is a two-stage approach for improving label distribution prediction in subjective datasets. Stage 1 uses unsupervised learning to cluster similar items based on weighted combinations of language features and label distributions, with the mixing parameter w varying from 0 (labels only) to 1 (features only). Items within each cluster share a common label distribution estimate, addressing label sparseness. Stage 2 trains a CNN classifier on these refined distributions to predict label distributions for new items. The approach is evaluated on five benchmark datasets using KL-divergence for distributional prediction and accuracy for single-label prediction.

## Key Results
- Mixing features and labels in clustering improves KL-divergence for label distribution prediction across all five datasets
- Cluster-based aggregation universally improves distributional learning compared to raw label distributions
- Optimal mixing parameter w varies considerably across datasets, with no clear relationship to dataset characteristics
- For single-label prediction, clustering-based aggregation improves accuracy in two of five datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering improves label distribution prediction by grouping similar items based on combined features and label distributions.
- Mechanism: The model uses unsupervised learning to create clusters of similar items using both text features and label distributions. Within each cluster, items share a common label distribution estimate, which serves as input to the supervised learning stage.
- Core assumption: Items with similar label distributions are likely to have similar interpretations, allowing for more robust population-level learning despite sparse annotations.
- Evidence anchors:
  - [abstract] "an unsupervised learning based approach that uses language features and label distributions to pool similar items into larger samples of label distributions"
  - [section] "clustering is a powerful tool for combating label sparseness to predict population-level annotator responses"
  - [corpus] Weak - no direct citations, but related work on crowd disagreement exists
- Break condition: If the clustering algorithm fails to identify meaningful groupings or if the label distributions within clusters are too heterogeneous, the approach breaks down.

### Mechanism 2
- Claim: Mixing language features and label distributions in clustering leads to better ground truth estimates than using labels alone.
- Mechanism: By varying the mixing parameter w between features and labels during clustering, the model can find an optimal balance that improves the quality of the estimated label distributions.
- Core assumption: The combination of features and labels provides richer information for clustering than either alone, especially in sparse annotation settings.
- Evidence anchors:
  - [abstract] "we experiment with five linear combinations of label distributions and features"
  - [section] "Our main technical innovation is to perform label regularization based on the weighted joint feature and label space"
  - [corpus] Weak - related work exists on label distribution learning but specific mixing strategies are less explored
- Break condition: If the optimal mixing parameter varies too much across datasets or if the benefits of mixing are marginal, the added complexity may not be justified.

### Mechanism 3
- Claim: The two-stage learning framework preserves annotator disagreement through the entire pipeline.
- Mechanism: Stage 1 clusters items and shares label distributions within clusters, while Stage 2 trains a supervised model on these refined distributions. This preserves the full distribution of crowd responses rather than reducing to a single label.
- Core assumption: Preserving the full distribution of annotator responses provides more nuanced and fair learning than traditional majority vote approaches.
- Evidence anchors:
  - [abstract] "preserve minority opinions in the learning pipeline" and "preserve the full distribution of crowd responses (and their opinions) through the entire learning pipeline"
  - [section] "Our motivation behind CrowdOpinion is to reduce inequity and bias in human-supervised machine learning by preserving the full distribution of crowd responses"
  - [corpus] Weak - related work exists on annotator disagreement but specific preservation strategies are less explored
- Break condition: If the supervised learning stage cannot effectively learn from soft label distributions or if the preservation of disagreement does not translate to better downstream performance, the approach fails.

## Foundational Learning

- Concept: KL-divergence as a measure of distributional similarity
  - Why needed here: The paper uses KL-divergence to evaluate the quality of label distribution predictions and to guide clustering parameter selection
  - Quick check question: How does KL-divergence differ from other distance measures like Euclidean distance when comparing probability distributions?

- Concept: Unsupervised learning for clustering
  - Why needed here: The first stage of the approach relies on unsupervised clustering to group similar items based on features and label distributions
  - Quick check question: What are the advantages and disadvantages of using generative clustering methods (like GMM) versus distance-based methods (like NBP) in this context?

- Concept: Soft label distributions vs. hard labels
  - Why needed here: The approach predicts label distributions rather than single labels, requiring understanding of how to represent and learn from probabilistic outputs
  - Quick check question: How does training a model on soft label distributions differ from training on hard labels in terms of loss functions and evaluation metrics?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Feature extraction -> Stage 1 clustering -> Label distribution refinement -> Stage 2 supervised learning -> Evaluation

- Critical path: Data → Feature extraction → Stage 1 clustering → Label distribution refinement → Stage 2 supervised learning → Evaluation

- Design tradeoffs:
  - Complexity vs. performance: Multiple clustering methods and mixing parameters increase complexity but may improve results
  - Interpretability vs. accuracy: Soft label distributions are more nuanced but harder to interpret than single labels
  - Generalization vs. specificity: The approach aims to capture population-level patterns but may miss dataset-specific nuances

- Failure signatures:
  - Stage 1: High KL-divergence between raw and clustered distributions, poor clustering quality metrics
  - Stage 2: No improvement in accuracy or KL-divergence compared to baselines, overfitting on training data
  - Overall: Inconsistent performance across datasets, optimal mixing parameter varies widely

- First 3 experiments:
  1. Baseline comparison: Run the CNN model directly on raw label distributions (PD baseline) to establish a performance floor
  2. Label-only clustering: Set w=0 in the clustering stage to compare against Liu et al. (2019) baseline
  3. Feature-only clustering: Set w=1 in the clustering stage to understand the contribution of label distributions vs. features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal mixing parameter w vary across different datasets and what factors influence this variation?
- Basis in paper: [explicit] The paper states that the best choice for w varies considerably across datasets and that datasets with the largest number of choices selected models with w = 0, while the dataset with the fewest annotators per item selected a model with w = 1.
- Why unresolved: The paper notes that there is no clear relationship between w and KL-divergence, and suggests that subtle differences in the data or the inductive biases of particular clustering models may be driving the variance.
- What evidence would resolve it: Further experimentation with a wider range of mixing parameters and a deeper analysis of the relationship between dataset characteristics (e.g., number of annotators, label choices, entropy) and the optimal mixing parameter.

### Open Question 2
- Question: How does the performance of CrowdOpinion compare to other state-of-the-art methods for handling annotator disagreement in subjective datasets?
- Basis in paper: [inferred] The paper compares CrowdOpinion to four baselines, including DS+CNN and SL, but does not directly compare it to other methods specifically designed for handling annotator disagreement.
- Why unresolved: The paper focuses on the specific approach of CrowdOpinion and does not provide a comprehensive comparison to other methods in the literature.
- What evidence would resolve it: Conducting experiments comparing CrowdOpinion to other state-of-the-art methods for handling annotator disagreement, such as those using weighted majority voting, label smoothing, or Bayesian approaches.

### Open Question 3
- Question: How does CrowdOpinion perform on datasets with different levels of annotator disagreement and how does this impact the effectiveness of the method?
- Basis in paper: [explicit] The paper mentions that the datasets used have varying levels of annotator disagreement, but does not explicitly analyze how CrowdOpinion's performance changes with the level of disagreement.
- Why unresolved: The paper does not provide a systematic analysis of the relationship between annotator disagreement and the performance of CrowdOpinion.
- What evidence would resolve it: Conducting experiments on datasets with controlled levels of annotator disagreement and analyzing how CrowdOpinion's performance varies with the level of disagreement. This could involve creating synthetic datasets or using existing datasets with known disagreement patterns.

## Limitations
- The approach's reliance on KL-divergence may not fully capture label distribution quality, especially for sparse annotations
- Computational overhead from running multiple clustering methods across different mixing parameters may limit scalability
- Use of pre-trained SBERT embeddings without fine-tuning may miss domain-specific nuances in subjective datasets

## Confidence
- High confidence: The general framework of using clustering to share label distributions within groups and the improvement in KL-divergence metrics
- Medium confidence: The specific claim that mixing features and labels (w≠1) provides consistent benefits across all datasets, given the mixed results for single-label prediction
- Low confidence: The optimal mixing parameter w is universal across datasets, as the paper suggests w=0.25 is optimal but shows significant variation

## Next Checks
1. Conduct ablation studies with different distance metrics (beyond KL-divergence) to evaluate label distribution quality, such as Jensen-Shannon divergence or Wasserstein distance
2. Test the scalability of the approach on larger datasets (10K+ items) to measure computational overhead and clustering quality degradation
3. Implement cross-validation to assess whether the optimal mixing parameter w is consistent across different data splits within the same dataset