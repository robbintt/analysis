---
ver: rpa2
title: 'SimLM: Can Language Models Infer Parameters of Physical Systems?'
arxiv_id: '2312.14215'
source_url: https://arxiv.org/abs/2312.14215
tags:
- reasoning
- shot
- simulation
- bounce
- velocity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether Large Language Models (LLMs) can
  infer parameters of physical systems, focusing on predicting initial conditions
  for a projectile to achieve a specific bounce target. Experiments reveal that LLMs
  struggle with this task, particularly as problem complexity increases.
---

# SimLM: Can Language Models Infer Parameters of Physical Systems?

## Quick Facts
- arXiv ID: 2312.14215
- Source URL: https://arxiv.org/abs/2312.14215
- Reference count: 40
- Primary result: SimLM significantly improves LLM performance on inferring physical system parameters, with up to 30% error reduction compared to baseline methods

## Executive Summary
This paper investigates whether Large Language Models (LLMs) can infer parameters of physical systems by predicting initial conditions for a projectile to achieve a specific bounce target. The study reveals that LLMs struggle with this task, particularly as problem complexity increases. To address this limitation, the authors propose SimLM, a method that augments LLM context with physical simulation feedback. Results demonstrate that SimLM significantly outperforms standard chain-of-thought prompting, with its advantage growing as task difficulty increases.

## Method Summary
The study focuses on predicting initial conditions (height h, horizontal velocity v) for a ball to achieve a third bounce within 1m of a 50m target distance. The method employs zero-shot and few-shot Chain-of-Thought (CoT) prompting as baselines, and introduces SimLM which combines CoT with physics simulation feedback and self-critique. The physics simulator (pymunk) models 2D projectile motion with elastic collisions on flat and sinusoidal surfaces. LLMs generate initial predictions, which are validated through simulation, critiqued based on results, and iteratively refined.

## Key Results
- SimLM outperforms standard chain-of-thought prompting by up to 30% in error reduction
- The advantage of SimLM grows with task difficulty, particularly on curved surfaces
- Few-shot examples provide diminishing returns as problem complexity increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SimLM improves LLM performance by iteratively refining predictions using simulation feedback and self-critique.
- **Mechanism:** LLMs generate initial predictions, which are then validated through physical simulation. The LLM critiques its own reasoning based on simulation results and iteratively refines its predictions.
- **Core assumption:** The LLM can meaningfully incorporate simulation feedback to improve its reasoning about physical systems.
- **Evidence anchors:**
  - [abstract]: "We propose a promising direction of exploration, which involves the use of physical simulators to augment the context of LLMs."
  - [section 3.1]: "Our prompting method reduces inaccuracies in physical reasoning by inserting physics simulations between reasoning and critiquing steps."
- **Break condition:** If the LLM cannot generate meaningful critiques or if the simulation feedback is too complex for the LLM to process.

### Mechanism 2
- **Claim:** Few-shot chain-of-thought examples help LLMs learn reasoning patterns but become less effective as problem complexity increases.
- **Mechanism:** Providing examples of how to solve similar problems helps LLMs understand the reasoning process. However, as the problem becomes more complex (e.g., curved surfaces), the examples become less useful.
- **Core assumption:** LLMs can learn reasoning patterns from examples but struggle with generalization to more complex problems.
- **Evidence anchors:**
  - [abstract]: "Results show that SimLMâ€™s advantage grows with task difficulty, outperforming baseline methods by up to 30% in error reduction."
  - [section 5.1]: "Interestingly this benefit provides diminishing returns as the complexity of the forward problem is increased."
- **Break condition:** If the problem complexity is too high for the LLM to learn from examples alone, regardless of the number of examples provided.

### Mechanism 3
- **Claim:** LLMs struggle with forward physics problems but perform similarly in forward and inverse problems when augmented with simulation.
- **Mechanism:** LLMs have difficulty predicting outcomes of physical systems (forward problems) and inferring initial conditions (inverse problems). However, when augmented with simulation feedback, their performance in both types of problems improves.
- **Core assumption:** Simulation feedback helps LLMs overcome their inherent limitations in understanding physical systems.
- **Evidence anchors:**
  - [section 5.5]: "What we found was that the LLMs performed similarly in the forward problem as they did the backward."
  - [abstract]: "The study highlights the need for external simulation tools to enhance LLM reasoning in complex physical environments."
- **Break condition:** If the simulation feedback is not accurate or if the LLM cannot process the feedback effectively.

## Foundational Learning

- **Concept: Physics Simulation**
  - Why needed here: Understanding how physics simulation works is crucial for implementing SimLM and interpreting its results.
  - Quick check question: What is the role of the coefficient of restitution in the physics simulation used in SimLM?

- **Concept: Chain-of-Thought Prompting**
  - Why needed here: Chain-of-thought prompting is a key technique used in SimLM to improve LLM reasoning.
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in terms of LLM performance?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is a technique that SimLM builds upon to provide LLMs with additional context for reasoning.
  - Quick check question: What is the main difference between RAG and SimLM in terms of how they augment LLM context?

## Architecture Onboarding

- **Component map:** LLM -> Physics Simulator -> Simulation Feedback Processor -> LLM Critique Generator -> LLM Refinement -> Example Database
- **Critical path:** 1. LLM generates initial prediction 2. Prediction is used to run simulation 3. Simulation results are fed back to LLM 4. LLM critiques its own reasoning based on simulation 5. LLM refines its prediction 6. Repeat steps 2-5 for a set number of iterations
- **Design tradeoffs:** Number of simulation iterations vs. computational cost; Quality of simulation feedback vs. LLM's ability to process it; Size of example database vs. context window limitations
- **Failure signatures:** LLM fails to generate meaningful critiques; Simulation results are too complex for LLM to process; Example database becomes too large and affects performance
- **First 3 experiments:** 1. Flat surface experiment to establish baseline performance 2. Curved surface experiment to test performance on more complex problems 3. Varying difficulty experiment to explore how performance scales with problem complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLMs perform in a full 3D physical environment compared to the 2D setup used in this study?
- Basis in paper: [inferred] The paper mentions that moving from toy problems to an unconstrained 3D environment would make for a more realistic study of LLM's understanding of physical interaction.
- Why unresolved: The study only evaluated LLMs in a 2D physical system, limiting the understanding of their capabilities in more complex 3D environments.
- What evidence would resolve it: Testing LLMs in a 3D physical simulation and comparing their performance to the 2D setup.

### Open Question 2
- Question: Can LLMs handle qualitative queries that require intuition-based reasoning, such as throwing an object to avoid obstacles or to make another object land in a specific position?
- Basis in paper: [explicit] The paper suggests that with the correct pipeline, LLMs may be able to choose initial conditions for throwing an object, but questions how they would handle tasks requiring intuition-based reasoning.
- Why unresolved: The study focused on quantitative tasks with specific target positions, not exploring LLMs' ability to handle more complex qualitative queries.
- What evidence would resolve it: Evaluating LLMs on tasks requiring intuition-based reasoning and obstacle avoidance in physical simulations.

### Open Question 3
- Question: How does the scale of the problem (e.g., meters vs. centimeters) affect LLMs' performance in physical reasoning tasks?
- Basis in paper: [explicit] The paper conducted an experiment where the scale was changed from meters to centimeters, and found that LLMs performed slightly worse and had many instances of using the wrong scale in their reasoning.
- Why unresolved: The study only explored the impact of scale change on performance, but did not investigate the underlying reasons for the observed differences.
- What evidence would resolve it: Analyzing the training data distribution across different scales and correlating it with LLMs' performance in physical reasoning tasks.

## Limitations

- Evaluation focuses on narrow projectile motion problems with specific constraints
- Generalizability to other physical domains remains untested
- Computational overhead of multiple simulation iterations not fully characterized

## Confidence

- **High confidence:** SimLM improves LLM performance on the specific projectile motion tasks tested, particularly as problem complexity increases
- **Medium confidence:** The mechanism of iterative refinement through simulation feedback and self-critique is valid for the tested scenarios
- **Low confidence:** The claim that LLMs struggle with forward physics problems but perform similarly in forward and inverse problems when augmented with simulation

## Next Checks

1. Test SimLM on different physical domains (e.g., fluid dynamics, thermodynamics) to assess generalizability beyond projectile motion
2. Measure and optimize the computational overhead of multiple simulation iterations, comparing against alternative approaches like specialized physics models
3. Systematically evaluate the contribution of each component (simulation feedback, self-critique, iteration count) to isolate which mechanisms drive performance improvements