---
ver: rpa2
title: 'InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification
  from An Information Theoretic Perspective'
arxiv_id: '2310.06362'
source_url: https://arxiv.org/abs/2310.06362
tags:
- learning
- representations
- infocl
- classes
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We analyze the representation learning process of continual learning
  from an information-theoretic perspective and discover that the compression effect
  of the information bottleneck leads to representation bias, causing confusion on
  analogous classes. To address this, we propose InfoCL, a replay-based continual
  text classification method that utilizes fast-slow and current-past contrastive
  learning to maximize mutual information and recover previously learned representations.
---

# InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective

## Quick Facts
- arXiv ID: 2310.06362
- Source URL: https://arxiv.org/abs/2310.06362
- Reference count: 27
- Accuracy improvements of up to 1.4% compared to previous methods

## Executive Summary
This paper proposes InfoCL, a replay-based continual text classification method that addresses catastrophic forgetting by leveraging information theory. The authors identify that the compression effect of the information bottleneck leads to representation bias and confusion on analogous classes across tasks. InfoCL uses fast-slow and current-past contrastive learning to maximize mutual information and recover previously learned representations, while incorporating adversarial memory augmentation to alleviate overfitting. Experiments on three text classification tasks demonstrate state-of-the-art performance with accuracy improvements up to 1.4% compared to previous methods.

## Method Summary
InfoCL is a replay-based continual learning method for class-incremental text classification that utilizes contrastive learning from an information-theoretic perspective. The approach employs fast-slow contrastive learning during new task training to maximize mutual information between inputs and representations, using a momentum contrast with fast and slow encoders. For memory replay, it uses current-past contrastive learning to align current representations with previously learned ones, along with adversarial memory augmentation to prevent overfitting. The method aims to learn more sufficient representations by recovering information lost due to information bottleneck compression.

## Key Results
- Achieves state-of-the-art performance on three text classification tasks with accuracy improvements up to 1.4% over previous methods
- Demonstrates that InfoCL learns more sufficient representations with higher mutual information values compared to baseline approaches
- Effectively mitigates representation bias and confusion on analogous classes across tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression effect of information bottleneck leads to representation bias causing confusion on analogous classes
- Mechanism: Information bottleneck minimizes mutual information I(Xi; Zi) for each task, discarding features irrelevant to current task but crucial for other tasks, resulting in representations insufficient for classifying analogous classes across tasks
- Core assumption: The compression effect of IB is necessary for generalization but creates representation bias when tasks share analogous classes
- Evidence anchors:
  - [abstract] "We discover that the compression effect of the information bottleneck leads to confusion on analogous classes"
  - [section 4.2] "the compression effect of IB will minimize the mutual information I(Xi; Zi), leading to globally insufficient representations"
  - [corpus] Weak - neighboring papers discuss representation learning but don't directly address IB compression effect on analogous classes
- Break condition: If the assumption that IB compression is essential for generalization is violated, or if tasks don't contain analogous classes

### Mechanism 2
- Claim: Fast-slow contrastive learning helps learn more comprehensive representations by preserving more information about inputs
- Mechanism: Uses momentum contrast with fast and slow encoders where slow representations preserve more information, and InfoNCE contrastive loss pulls fast representations toward slow ones
- Core assumption: Early representations preserve more information and can be "distilled" into later representations through contrastive learning
- Evidence anchors:
  - [abstract] "Our approach utilizes fast-slow and current-past contrastive learning to perform mutual information maximization"
  - [section 5.2] "the compression effect of IB will minimize the mutual information I(Xi; Zi), leading to globally insufficient representations"
  - [section 5.2] "we employ a momentum contrast which consists of a fast encoder and a momentum updated slow encoder"
  - [corpus] Weak - neighboring papers discuss contrastive learning but don't specifically address fast-slow momentum contrast for representation preservation
- Break condition: If the momentum contrast mechanism fails to preserve sufficient information in slow representations

### Mechanism 3
- Claim: Current-past contrastive learning aligns current representations with previously learned ones to mitigate representation corruption
- Mechanism: Stores representations from previous tasks and uses InfoNCE to pull current representations toward past representations for same classes
- Core assumption: Representations can be stabilized by explicit alignment with previously learned representations
- Evidence anchors:
  - [abstract] "when conducting memory replay, we leverage current-past contrastive learning to ensure the learned representations do not undergo significant changes"
  - [section 5.4] "to further enhance representation recovery in memory replay stage, we propose current-past contrastive learning which explicitly aligns current representations to the previous ones"
  - [section 5.4] "we use InfoNCE loss to pull current representations z and past representations ¯z of the same class together"
  - [corpus] Weak - neighboring papers discuss memory replay but don't specifically address current-past contrastive alignment
- Break condition: If memory budget is too constrained to store meaningful representations for alignment

## Foundational Learning

- Concept: Information Bottleneck (IB) theory
  - Why needed here: IB explains why standard continual learning approaches compress representations too much, causing catastrophic forgetting on analogous classes
  - Quick check question: How does the IB trade-off between compression and preservation lead to representation bias in class-incremental learning?

- Concept: Contrastive Learning and InfoNCE
  - Why needed here: InfoNCE serves as proxy for mutual information maximization, enabling InfoCL to learn more comprehensive representations
  - Quick check question: What is the relationship between InfoNCE objective and mutual information between representations and inputs?

- Concept: Catastrophic Forgetting in Continual Learning
  - Why needed here: Understanding the root causes of forgetting (representation corruption, confusion on analogous classes) is essential for designing effective solutions
  - Quick check question: How does representation bias differ from other causes of catastrophic forgetting like parameter drift?

## Architecture Onboarding

- Component map: BERT encoder → Fast/slow encoders for new task training → Past encoder for memory replay → Memory bank with K-means sampled instances → Adversarial augmentation module
- Critical path: New task training with fast-slow contrastive learning → Memory selection via K-means → Memory replay with current-past contrastive learning + adversarial augmentation
- Design tradeoffs: Memory budget vs representation recovery effectiveness, contrastive learning complexity vs standard approaches, adversarial augmentation vs overfitting
- Failure signatures: Performance degradation on analogous classes indicates insufficient representation preservation, poor performance on early tasks indicates representation corruption, overfitting on memory data indicates need for more augmentation
- First 3 experiments:
  1. Compare I(X1; Z1) and I(Z; Y) metrics between InfoCL and baseline to verify representation sufficiency
  2. Test ablation of fast-slow contrastive learning to measure impact on analogous class performance
  3. Vary memory size to measure robustness of current-past contrastive learning with adversarial augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the InfoCL approach scale to larger datasets or more classes/tasks beyond the 10-task setting evaluated?
- Basis in paper: [inferred] The paper evaluates on 4 datasets with 10 tasks each, but doesn't discuss scaling to more tasks or larger datasets.
- Why unresolved: The paper doesn't provide experiments or analysis on scaling beyond the 10-task setting, which may be a limitation for real-world continual learning scenarios with potentially hundreds or thousands of classes/tasks.
- What evidence would resolve it: Experiments evaluating InfoCL on datasets with more tasks/classes, or theoretical analysis of the approach's scalability and computational complexity as the number of tasks/classes increases.

### Open Question 2
- Question: How does the InfoCL approach handle class-incremental scenarios where the number of classes per task is not fixed or known in advance?
- Basis in paper: [explicit] The paper assumes a fixed number of new classes per task (8 for FewRel, 4 for TACRED, 12 for MA VEN, 5 for HWU64).
- Why unresolved: Real-world continual learning scenarios may involve varying numbers of new classes per task, and it's unclear how InfoCL would adapt to such scenarios without prior knowledge of the class distribution.
- What evidence would resolve it: Experiments evaluating InfoCL on datasets with varying numbers of new classes per task, or modifications to the approach to handle unknown or variable class distributions.

### Open Question 3
- Question: How does the InfoCL approach compare to other continual learning methods in terms of computational efficiency and memory requirements?
- Basis in paper: [inferred] The paper mentions that InfoCL introduces extra computational overhead and is less efficient than other replay-based methods, but doesn't provide a detailed comparison of computational efficiency or memory requirements.
- Why unresolved: Understanding the computational and memory efficiency of InfoCL compared to other methods is important for real-world applications, especially when dealing with large-scale datasets or resource-constrained environments.
- What evidence would resolve it: Experiments comparing the computational time, memory usage, and other resource requirements of InfoCL to other state-of-the-art continual learning methods across various datasets and scenarios.

## Limitations
- Effectiveness depends heavily on sufficient memory budget to store representative instances for contrastive learning
- Assumes compression effect of information bottleneck is the primary cause of representation bias, which may not hold for all scenarios
- Introduces extra computational overhead compared to standard replay-based methods

## Confidence
- High confidence: The fundamental mechanism of using contrastive learning to maximize mutual information between representations is well-established and the experimental results show consistent improvements over baselines.
- Medium confidence: The claim about information bottleneck compression causing representation bias is supported by theoretical analysis but lacks direct empirical validation through ablation studies.
- Medium confidence: The effectiveness of adversarial memory augmentation in alleviating overfitting is demonstrated through experiments but the specific design choices (loss factors, augmentation techniques) are not fully explored.

## Next Checks
1. Conduct controlled ablation experiments removing the fast-slow contrastive learning component to quantify its specific contribution to preventing confusion on analogous classes.
2. Perform sensitivity analysis on memory budget size to determine the minimum memory requirements for InfoCL to outperform baselines effectively.
3. Design experiments specifically targeting analogous class pairs across tasks to measure how InfoCL mitigates confusion compared to standard replay-based methods.