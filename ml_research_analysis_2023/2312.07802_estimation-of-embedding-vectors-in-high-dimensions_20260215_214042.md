---
ver: rpa2
title: Estimation of embedding vectors in high dimensions
arxiv_id: '2312.07802'
source_url: https://arxiv.org/abs/2312.07802
tags:
- embedding
- where
- vectors
- terms
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper studies learning low-dimensional embeddings from high-dimensional
  data using a Poisson-based generative model. It proposes a two-step approach: first
  estimating bias terms from marginal frequencies, then applying a biased low-rank
  approximate message passing (AMP) algorithm to estimate the embedding vectors.'
---

# Estimation of embedding vectors in high dimensions

## Quick Facts
- arXiv ID: 2312.07802
- Source URL: https://arxiv.org/abs/2312.07802
- Authors: 
- Reference count: 40
- Key outcome: The paper studies learning low-dimensional embeddings from high-dimensional data using a Poisson-based generative model and shows that state evolution equations precisely characterize AMP algorithm performance in high-dimensional limits.

## Executive Summary
This paper proposes a two-step approach for learning low-dimensional embeddings from high-dimensional count data using a Poisson-based generative model. The method first estimates bias terms from marginal frequencies, then applies a biased low-rank approximate message passing (AMP) algorithm to estimate the embedding vectors. The key theoretical contribution is showing that the performance of this AMP algorithm can be precisely characterized in high-dimensional limits via state evolution equations, which predict the mean squared error based on the inverse Fisher information and other problem parameters.

## Method Summary
The approach consists of two main steps: first estimating bias terms su_i and sv_j from marginal frequencies of the count data, then applying a biased low-rank AMP algorithm to estimate the embedding vectors U and V. The AMP algorithm uses denoisers Ga and Gb with row-wise regularizers to iteratively update the estimates. State evolution equations provide deterministic predictions for the mean squared error of the estimated correlations as a function of problem parameters including the inverse Fisher information.

## Key Results
- State evolution equations accurately predict the mean squared error of estimated embeddings in high-dimensional limits
- The inverse Fisher information ∆ij determines the estimation accuracy for each element of the correlation matrix Mij = u_i^T v_j
- Experiments on synthetic and real text data show excellent agreement between simulated performance and state evolution predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Poisson-based generative model enables precise estimation of embedding vectors by linking observed frequencies to latent correlations.
- Mechanism: The model expresses joint distributions as log-linear functions of embedding dot products plus bias terms, making estimation equivalent to low-rank matrix factorization. Poisson measurements naturally model count data, and biased low-rank AMP exploits this structure.
- Core assumption: Number of samples where (x1, x2) = (i, j) follows Poisson distribution with rate proportional to joint probability, with d fixed while m and n grow to infinity.
- Evidence anchors: Abstract states performance characterized via state evolution; section discusses quadratic approximation of log likelihood in low-rank AMP.
- Break condition: Poisson assumption violated (e.g., overdispersed counts) or embedding dimension grows with m and n.

### Mechanism 2
- Claim: State evolution equations accurately predict MSE of estimated embeddings as function of problem parameters.
- Mechanism: In large system limit, joint distribution of true and estimated embedding vectors is characterized by state evolution recursion tracking sufficient statistics across AMP iterations.
- Core assumption: Embedding vectors and bias terms treated as deterministic vectors converging empirically with second-order to random variables, with self-averaging AMP updates.
- Evidence anchors: Abstract describes exact characterization via state evolution; section states joint distribution can be predicted by state evolution under these assumptions.
- Break condition: Empirical convergence assumptions fail (e.g., heavy-tailed distributions) or AMP updates don't self-average.

### Mechanism 3
- Claim: Inverse Fisher information determines estimation accuracy for each element of correlation matrix Mij = u_i^T v_j.
- Mechanism: Inverse Fisher information ∆ij for each measurement Zij depends on bias terms su_i and sv_j, with state evolution revealing MSE for estimating Mij inversely related to ∆ij.
- Core assumption: Measurements Zij are independent and follow Poisson distribution, with bias terms estimable from marginal frequencies.
- Evidence anchors: Section states state evolution reveals key parameter is inverse Fisher information; figure shows scatter plot of normalized MSE vs ∆ij demonstrating higher inverse Fisher information results in higher MSE.
- Break condition: Independence assumption violated (e.g., correlated measurements) or bias estimation is inaccurate.

## Foundational Learning

- Concept: Low-rank matrix factorization
  - Why needed here: Embedding estimation problem is equivalent to factorizing observed count matrix Z into low-rank matrices U and V representing embedding vectors.
  - Quick check question: What is the rank of matrix Z if embedding dimension is d? (Answer: At most 2d, since Z can be written as sum of rank-1 matrices UV^T)

- Concept: Approximate message passing (AMP)
  - Why needed here: AMP is iterative algorithm efficiently solving low-rank matrix factorization under Poisson model, with performance precisely characterized in large system limit.
  - Quick check question: What is key feature of AMP allowing performance analysis? (Answer: Onsager correction term accounting for correlation between current estimate and measurement residual)

- Concept: State evolution
  - Why needed here: State evolution provides deterministic recursion tracking evolution of sufficient statistics in AMP, allowing precise MSE predictions as function of problem parameters.
  - Quick check question: What are key assumptions required for state evolution to hold? (Answer: Rows of matrices converge empirically to random variables, AMP updates are self-averaging in large system limit)

## Architecture Onboarding

- Component map: Data preprocessing -> Bias estimation -> Biased low-rank AMP -> State evolution -> Evaluation
- Critical path: The critical path is the sequence of steps required to go from raw data to predicted and actual MSE. Each step depends on output of previous step.
- Design tradeoffs:
  - Embedding dimension d: Higher d allows more expressive embeddings but increases computational complexity and risk of overfitting
  - Number of samples N: More samples improve estimation accuracy but increase computational cost and memory risk
  - Regularization parameters λ_u and λ_v: Stronger regularization prevents overfitting but may lead to underfitting if set too high
- Failure signatures:
  - SE predictions don't match actual MSE: Indicates violation of LSL assumptions or bug in SE implementation
  - AMP doesn't converge: Indicates problem with AMP updates or choice of hyperparameters (e.g., regularization strength)
  - Estimated embeddings are poor: Indicates problem with data preprocessing, bias estimation, or choice of embedding dimension
- First 3 experiments:
  1. Synthetic data with known ground truth: Generate synthetic data from Poisson model with known embedding vectors and biases, compare estimated embeddings with ground truth
  2. Varying the number of samples: Fix embedding dimension and bias terms, vary number of samples to study effect on estimation accuracy
  3. Varying the embedding dimension: Fix number of samples and bias terms, vary embedding dimension to study tradeoff between expressiveness and estimation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does proposed biased low-rank AMP algorithm perform in over-parameterized regime (embedding dimension d larger than true rank)?
- Basis in paper: [explicit] Paper states "An interesting avenue is to study the behavior of the methods in both over and under-parameterized regimes."
- Why unresolved: Paper only considers case where embedding dimension d is fixed and smaller than problem dimensions, leaving over-parameterized regime unexplored.
- What evidence would resolve it: Simulations comparing performance of biased low-rank AMP algorithm for different values of d, including cases where d is larger than true rank, analyzing convergence behavior and MSE.

### Open Question 2
- Question: Can Poisson-based generative model be extended to incorporate more complex dependencies between embedding vectors, such as non-linear relationships or interactions between multiple embeddings?
- Basis in paper: [inferred] Paper uses simple Poisson model where correlation of random variables is linearly related to similarity of embeddings. More complex dependencies could potentially improve model's expressiveness and performance.
- Why unresolved: Paper focuses on basic Poisson model and doesn't explore more complex dependencies, which could be limitation in capturing intricate relationships in real-world data.
- What evidence would resolve it: Experiments comparing performance of proposed method with models incorporating non-linear relationships or multiple embeddings, using both synthetic and real-world datasets.

### Open Question 3
- Question: How sensitive is biased low-rank AMP algorithm to choice of regularizers (ϕu and ϕv) and their associated parameters (λu and λv)?
- Basis in paper: [explicit] Paper mentions that "Regularizers can also be used to impose sparsity" and uses squared norm regularizers in experiments. However, impact of different regularizers and their parameters is not thoroughly investigated.
- Why unresolved: Paper uses specific regularizers and parameters without exploring their sensitivity or potential benefits of alternative choices, which could affect algorithm's performance and generalization ability.
- What evidence would resolve it: Systematic experiments varying regularizers and their parameters, analyzing impact on convergence, MSE, and learned embeddings' quality, using both synthetic and real-world datasets.

## Limitations
- Analysis relies heavily on large system limit assumptions where m and n grow to infinity while d remains fixed, potentially breaking down for moderate-sized problems
- Poisson assumption for measurement model may not hold for real-world count data exhibiting overdispersion or other distributional properties
- Performance characterization is limited to the specific AMP algorithm and Poisson model considered, with potential sensitivity to algorithm choice and distributional assumptions

## Confidence

- **High confidence**: The theoretical characterization of AMP performance via state evolution in the large system limit, supported by abstract claims and section discussions.
- **Medium confidence**: The practical effectiveness of the two-step approach (bias estimation followed by AMP) on real text data, as the corpus provides limited evidence for this specific application.
- **Low confidence**: The generalizability of the inverse Fisher information relationship to MSE across diverse data distributions and problem settings.

## Next Checks
1. Synthetic data with known ground truth: Generate synthetic data from Poisson model with specified parameters and validate that estimated embeddings match ground truth, comparing MSE with state evolution predictions.

2. Varying sample size: Fix embedding dimension and bias terms, then systematically vary the number of samples to empirically verify the relationship between sample size and estimation accuracy predicted by state evolution.

3. Real data robustness: Apply the method to multiple real-world count datasets beyond the movie review corpus to test generalizability of the approach and identify potential failure modes in practical settings.