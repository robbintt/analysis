---
ver: rpa2
title: Offensive Language Identification in Transliterated and Code-Mixed Bangla
arxiv_id: '2311.15023'
source_url: https://arxiv.org/abs/2311.15023
tags:
- offensive
- language
- bangla
- transliterated
- tb-olid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TB-OLID, a transliterated Bangla offensive
  language dataset with 5,000 manually annotated Facebook comments. The dataset addresses
  the challenge of processing transliterated and code-mixed text in multilingual societies,
  a known difficulty for NLP systems.
---

# Offensive Language Identification in Transliterated and Code-Mixed Bangla

## Quick Facts
- arXiv ID: 2311.15023
- Source URL: https://arxiv.org/abs/2311.15023
- Reference count: 7
- Key outcome: English pre-trained transformer models (fBERT, HateBERT) achieve F1-score of 0.72 on transliterated Bangla offensive language identification

## Executive Summary
This paper introduces TB-OLID, a dataset of 5,000 manually annotated Facebook comments in transliterated and code-mixed Bangla, addressing the challenge of processing non-standard text in multilingual societies. The authors evaluate various transformer models on offensive language identification tasks and find that English pre-trained models like fBERT and HateBERT outperform Bangla-specific models due to the high proportion of English tokens in the transliterated text. The dataset and findings provide valuable insights into handling code-mixed and transliterated text for offensive language detection in Bangla and similar languages.

## Method Summary
The authors collected 100,000 Facebook comments and filtered them using Bangla keywords related to offensive content. They randomly sampled 5,000 comments and manually annotated them following the OLID taxonomy's three-level hierarchical structure. The dataset was split into 4,000 training and 1,000 testing instances. Various transformer models including BERT, roBERTa, Bangla-BERT, mBERT, xlm-roBERTa, fBERT, and HateBERT were fine-tuned on the training set and evaluated on the test set using F1-score for offensive language identification and target classification tasks.

## Key Results
- English pre-trained transformer models (fBERT and HateBERT) achieve the best performance with F1-score of 0.72
- Bangla-BERT performs poorly due to out-of-vocabulary issues with transliterated text
- Models perform better on code-mixed data than transliterated data, likely due to higher English token presence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: English pre-trained transformer models (fBERT and HateBERT) perform better than Bangla-specific models on transliterated Bangla offensive language data.
- Mechanism: Code-mixed text in TB-OLID contains a high proportion of English words (38% of tokens), allowing English pre-trained models to leverage their strong language understanding capabilities.
- Core assumption: The presence of English tokens in transliterated Bangla text provides sufficient semantic cues for English-trained models to perform well on offensive language detection.
- Evidence anchors:
  - [abstract] "Our results show that English pre-trained transformer-based models, such as fBERT and HateBERT achieve the best performance on this dataset."
  - [section] "Models pre-trained specifically on offensive language identification perform very well with fBERT and Hate-BERT coming out on top, both with an F1 score of 0.72."
  - [corpus] Weak evidence: corpus statistics show English tokens present but no detailed analysis of their contribution to model performance.
- Break condition: If the proportion of English words drops significantly or if the transliteration rules change substantially, English models may lose their advantage.

### Mechanism 2
- Claim: Transliteration complexity hinders performance of Bangla-BERT model.
- Mechanism: Bangla-BERT, trained on standard Bangla script, struggles with out-of-vocabulary tokens when encountering transliterated text, leading to poor performance.
- Core assumption: The model's vocabulary is not robust to the spelling variations inherent in transliterated Bangla text.
- Evidence anchors:
  - [section] "We believe this is due to the fact that many instances in the dataset are in Latin script, which means that BanglaBERT frequently struggles with out-of-vocabulary tokens."
  - [corpus] Weak evidence: no explicit analysis of out-of-vocabulary rates for Bangla-BERT on TB-OLID.
- Break condition: If a transliteration-normalization preprocessing step is applied before model input, Bangla-BERT's performance might improve.

### Mechanism 3
- Claim: Task-specific fine-tuning on offensive language datasets improves model performance.
- Mechanism: Models like fBERT and HateBERT are pre-trained on offensive language datasets, giving them specialized knowledge for detecting offensive content.
- Core assumption: Domain-specific pre-training provides features that generalize to transliterated Bangla offensive language detection.
- Evidence anchors:
  - [section] "Models pre-trained specifically on offensive language identification perform very well with fBERT and Hate-BERT coming out on top, both with an F1 score of 0.72."
  - [section] "Models like HateBERT (Caselli et al., 2021), and fBERT (Sarkar et al., 2021) were also further fine-tuned on TB-OLID."
- Break condition: If the distribution of offensive content in TB-OLID differs significantly from the pre-training data, the advantage may diminish.

## Foundational Learning

- Concept: Code-mixing and transliteration
  - Why needed here: Understanding these linguistic phenomena is crucial for interpreting the dataset and the model's performance challenges.
  - Quick check question: What is the difference between code-mixing and transliteration in the context of multilingual social media text?

- Concept: OLID taxonomy for offensive language annotation
  - Why needed here: The dataset follows OLID's hierarchical structure, so understanding its levels is essential for interpreting the annotation scheme and results.
  - Quick check question: What are the three levels of the OLID taxonomy and what does each level classify?

- Concept: Transformer-based language models and fine-tuning
  - Why needed here: The paper evaluates various transformer models, so understanding their architecture and fine-tuning process is key to interpreting the results.
  - Quick check question: How does fine-tuning a pre-trained transformer model on a specific task improve its performance compared to training from scratch?

## Architecture Onboarding

- Component map: Data collection → Annotation → Model training/fine-tuning → Evaluation
- Critical path: Data collection → Annotation → Model training/fine-tuning → Evaluation
- Design tradeoffs:
  - Using English pre-trained models vs. Bangla-specific models
  - Manual annotation vs. automatic labeling
  - OLID taxonomy vs. custom annotation scheme
- Failure signatures:
  - Low performance on transliterated text indicates out-of-vocabulary issues
  - Poor target classification suggests the model struggles with context understanding
  - High variance in F1-scores across different model types indicates data distribution challenges
- First 3 experiments:
  1. Train and evaluate a basic BERT model on TB-OLID to establish a baseline.
  2. Fine-tune fBERT on TB-OLID and compare its performance to the baseline BERT model.
  3. Train a custom transformer model with transliteration-aware tokenization and evaluate its performance on TB-OLID.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different pre-training strategies (e.g., domain-specific, transliterated language-specific) on the performance of offensive language identification models in transliterated and code-mixed Bangla?
- Basis in paper: [explicit] The authors note that transformer-based models do not perform very well on TB-OLID since most are not pre-trained on transliterated Bangla. They mention that xlm-roBERTa is pre-trained with a small set of Romanized Bangla, but this is insufficient due to the lack of standard spelling rules in transliterated Bangla. They propose pre-training and fine-tuning a Bangla transliterated BERT model in future work to evaluate its performance on TB-OLID.
- Why unresolved: The paper does not experiment with different pre-training strategies beyond using existing models. It does not compare the performance of models pre-trained on transliterated Bangla or domain-specific data.
- What evidence would resolve it: Experiments comparing the performance of models pre-trained on transliterated Bangla, domain-specific data, and general-purpose data on TB-OLID.

### Open Question 2
- Question: How does the performance of offensive language identification models on TB-OLID compare to their performance on standard Bangla datasets, and what factors contribute to any differences?
- Basis in paper: [explicit] The authors observe that Bangla-BERT, which is trained on Bangla, performs less well than BERT, which is trained on English. They attribute this to the fact that many instances in TB-OLID are in Latin script, leading to out-of-vocabulary issues for Bangla-BERT. They also note that the best-performing models achieve higher F1 scores on code-mixed data than on transliterated data, likely due to the presence of more English words in the code-mixed data.
- Why unresolved: The paper does not provide a direct comparison of model performance on TB-OLID versus standard Bangla datasets. It does not investigate the specific factors contributing to performance differences.
- What evidence would resolve it: A comparative study evaluating the performance of offensive language identification models on TB-OLID and standard Bangla datasets, along with an analysis of the factors influencing performance differences.

### Open Question 3
- Question: What is the effectiveness of large language models (LLMs) like GPT-4 and Llama 2 on transliterated and code-mixed Bangla offensive language identification tasks, and how do they compare to existing transformer-based models?
- Basis in paper: [explicit] The authors mention that they would like to evaluate the performance of other recently released large language models (LLMs) like GPT-4 and Llama 2 on TB-OLID in future work. They note that their initial baseline results using GPT 3.5 indicate that general-purpose LLMs still struggle with the transliterated and code-mixed content presented in TB-OLID.
- Why unresolved: The paper does not experiment with or compare the performance of LLMs like GPT-4 and Llama 2 on TB-OLID. It does not investigate the reasons for their potential struggles with transliterated and code-mixed content.
- What evidence would resolve it: Experiments evaluating the performance of LLMs like GPT-4 and Llama 2 on TB-OLID, along with a comparison to existing transformer-based models and an analysis of the challenges faced by LLMs in handling transliterated and code-mixed content.

## Limitations

- The dataset size of 5,000 annotated comments may limit model generalizability and robustness
- Weak analysis of English token contribution to model performance without detailed corpus breakdown
- No exploration of preprocessing strategies to handle transliteration that could improve Bangla-BERT's performance

## Confidence

- **High confidence:** The core finding that English pre-trained transformer models outperform Bangla-specific models on this transliterated dataset is well-supported by the experimental results, with clear F1-score comparisons.
- **Medium confidence:** The explanation that English token prevalence drives English model success is plausible but not thoroughly validated with detailed corpus analysis showing token distribution and model behavior.
- **Medium confidence:** The claim about Bangla-BERT struggling with out-of-vocabulary tokens due to transliteration is reasonable but lacks explicit validation through OOV analysis or transliteration-aware preprocessing experiments.

## Next Checks

1. Conduct a detailed corpus analysis quantifying the proportion of English tokens versus transliterated Bangla tokens and correlate these distributions with model performance to validate the English token hypothesis.
2. Implement a transliteration normalization preprocessing step and evaluate whether Bangla-BERT's performance improves on transliterated text, testing the OOV hypothesis.
3. Expand the dataset size and perform cross-validation to assess model robustness and generalizability across different transliteration patterns and offensive content distributions.