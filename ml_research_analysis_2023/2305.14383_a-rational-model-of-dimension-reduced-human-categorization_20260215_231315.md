---
ver: rpa2
title: A Rational Model of Dimension-reduced Human Categorization
arxiv_id: '2305.14383'
source_url: https://arxiv.org/abs/2305.14383
tags:
- category
- categorization
- learning
- generalization
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical mixture of probabilistic principal
  component analyzers (HDP-PPCA) to model human categorization, addressing the challenge
  of high-dimensional stimuli in natural settings. The model learns both categories
  and a set of principal components simultaneously, capturing dimensional biases that
  reflect natural variation within categories.
---

# A Rational Model of Dimension-reduced Human Categorization

## Quick Facts
- arXiv ID: 2305.14383
- Source URL: https://arxiv.org/abs/2305.14383
- Reference count: 39
- This paper proposes a hierarchical mixture of probabilistic principal component analyzers (HDP-PPCA) to model human categorization in high-dimensional settings.

## Executive Summary
This paper introduces a hierarchical Dirichlet process mixture of probabilistic principal component analyzers (HDP-PPCA) to address the challenge of modeling human categorization behavior with high-dimensional stimuli. The model learns categories and principal components simultaneously, capturing dimensional biases that reflect natural variation within categories. It supports generative categorization and zero-shot learning by leveraging low-dimensional subspaces learned from data. The authors validate their approach through simulations and behavioral experiments, demonstrating that HDP-PPCA effectively predicts human categorization patterns and outperforms traditional exemplar models in few-shot generalization tasks.

## Method Summary
The HDP-PPCA model uses variational inference with Pyro to approximate the posterior distribution over category parameters and principal components. The model assumes a hierarchical Dirichlet process prior that enables sharing of principal components across clusters, allowing for compositional generalization. During inference, the model learns a central repository of principal components from category observations, which can then be combined additively to form subcategories or new categories. Generated exemplars lie on a low-dimensional subspace, mitigating the curse of dimensionality and enabling more effective categorization in few-shot learning settings.

## Key Results
- HDP-PPCA effectively predicts human categorization of natural images and outperforms traditional exemplar models in few-shot generalization tasks
- The model captures dimensional biases that reflect natural variation within categories by learning principal components as features
- Generated exemplars on low-dimensional subspaces constitute more representative samples of categories than original high-dimensional observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The HDP-PPCA model captures human dimensional biases by explicitly learning principal components that reflect natural variation within categories.
- Mechanism: By decomposing covariance into principal component variation and isotropic noise, the model learns a low-dimensional subspace that explains the majority of within-category variation. This allows the model to generalize along learned preferred dimensions rather than in all directions equally.
- Core assumption: Human categorization exhibits dimensional biases that align with natural variation within categories, and these biases can be captured by probabilistic principal component analysis.
- Evidence anchors:
  - [abstract]: "The model captures dimensional biases that reflect natural variation within categories."
  - [section]: "PPCA-mixture-based model also facilitates dimension reduction within each category."
- Break condition: If the learned principal components do not align with actual human generalization patterns, or if the isotropic noise assumption fails to capture the true covariance structure.

### Mechanism 2
- Claim: The generative categorization approach using low-dimensional exemplars improves performance in high-dimensional settings by avoiding the curse of dimensionality.
- Mechanism: Generated exemplars lie on a low-dimensional subspace and constitute a more representative sample of the category than original observations. This makes the generative approach more effective than conventional approaches in few-shot learning settings.
- Core assumption: Generated exemplars on a low-dimensional subspace are more representative and useful for categorization than the original high-dimensional observations.
- Evidence anchors:
  - [abstract]: "The model's ability to learn principal components as features allows for better representation of category structure and improved performance in high-dimensional settings."
  - [section]: "All generated exemplars in our model lie on a low-dimensional subspace, and thus are immune to the curse of dimensionality."
- Break condition: If the low-dimensional subspace fails to capture the essential variation in the category, or if the generative process does not accurately reflect human similarity judgments.

### Mechanism 3
- Claim: The hierarchical Dirichlet process prior enables sharing of principal components across clusters, allowing for compositional generalization within and among categories.
- Mechanism: The HDP-PPCA model learns a central repository of principal components shared across clusters. This allows for additive combination of principal components to form subcategories or new categories, supporting zero-shot learning.
- Core assumption: Humans utilize a central repository of features that can be shared across categories, and these features can be combined compositionally to form new categories.
- Evidence anchors:
  - [abstract]: "The principal components formed during category learning constitute an expressive feature set, which not only supports a generative approach to categorization and mitigates the curse of dimensionality, but also allows zero-shot learning within or among categories."
  - [section]: "The compositional generalization behavior explained with HDP-PPCA cannot be captured with classic models and previous rational models, as there are no explicit scores on the preferred dimensions."
- Break condition: If the assumption of shared principal components across clusters does not hold, or if the compositional generalization does not match human behavior.

## Foundational Learning

- Concept: Bayesian density estimation
  - Why needed here: The model casts categorization as a problem of density estimation, which is central to understanding how categories are represented as distributions in psychological space.
  - Quick check question: What is the difference between Bayesian density estimation and maximum likelihood estimation in the context of category learning?

- Concept: Dimensionality reduction techniques
  - Why needed here: The model explicitly incorporates dimension reduction to address the curse of dimensionality in natural settings, which is crucial for learning tractable yet sufficient features.
  - Quick check question: How does probabilistic principal component analysis differ from traditional principal component analysis, and why is this difference important for modeling human categorization?

- Concept: Hierarchical Bayesian models
  - Why needed here: The model uses a hierarchical Dirichlet process to share principal components among clusters, allowing for dependencies between clusters and compositional generalization.
  - Quick check question: What is the role of the Dirichlet process in the HDP-PPCA model, and how does it enable sharing of features across categories?

## Architecture Onboarding

- Component map:
  - HDP-PPCA model: Core component that learns categories and principal components simultaneously
  - Variational inference: Inference method used to approximate the posterior distribution
  - Pyro: Probabilistic programming language used to construct the model and perform inference
  - CIFAR-10H dataset: Dataset used for validation and comparison
  - Behavioral experiments: Human studies used to validate the model's predictions

- Critical path:
  1. Construct HDP-PPCA model using Pyro
  2. Perform variational inference to approximate posterior distribution
  3. Generate exemplars using the learned principal components
  4. Use generated exemplars for categorization and zero-shot learning
  5. Validate model using CIFAR-10H dataset and behavioral experiments

- Design tradeoffs:
  - Tradeoff between model complexity and interpretability: More complex models may capture more nuanced patterns but are harder to interpret
  - Tradeoff between computational efficiency and accuracy: Variational inference is faster but may not always converge to the global optimum
  - Tradeoff between generalizability and specificity: Shared principal components enable compositional generalization but may not capture category-specific nuances

- Failure signatures:
  - Poor performance on categorization tasks despite high-dimensional stimuli
  - Failure to capture human dimensional biases in generalization patterns
  - Inability to perform zero-shot learning or compositional generalization

- First 3 experiments:
  1. Replicate the axes-aligned artificial clusters simulation to verify learning of dimensional biases
  2. Test categorization performance with varying numbers of noise dimensions to demonstrate robustness to the curse of dimensionality
  3. Validate the model's ability to predict human few-shot generalization behavior using simple size-color combination stimuli

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the assumption that all principal components are learned from sensory category learning compare to real-world scenarios where humans learn about categories through social learning or other non-observational means?
- Basis in paper: [explicit] The paper acknowledges this limitation, stating that the assumption is not realistic and that humans learn about categories and concepts through not only observing but also in other ways, like social learning.
- Why unresolved: The paper makes this assumption for simplification, but does not explore the implications of relaxing this assumption or incorporating other forms of learning into the model.
- What evidence would resolve it: Empirical studies comparing the performance of the HDP-PPCA model with and without social learning components, or theoretical analysis of how incorporating other forms of learning would affect the model's ability to capture human categorization behavior.

### Open Question 2
- Question: Is variational inference a psychologically plausible method for approximating the posterior distribution in the context of human categorization, and how does it compare to other methods like Gibbs sampling or particle filters?
- Basis in paper: [explicit] The paper mentions that the psychological plausibility of variational inference is unclear and that it is used as a tractable alternative to Monte Carlo Markov Chain techniques.
- Why unresolved: The paper does not provide a direct comparison of variational inference with other methods in terms of their ability to model human categorization behavior or their psychological plausibility.
- What evidence would resolve it: Empirical studies comparing the performance of different inference methods in modeling human categorization behavior, or theoretical analysis of the psychological plausibility of each method.

### Open Question 3
- Question: How can the HDP-PPCA model be extended to incorporate a hierarchical structure that allows for cross-categorization and context-dependent behavior, and what are the implications of such an extension for understanding human categorization?
- Basis in paper: [inferred] The paper mentions that PPCA-based models do not impose any ordering among principal components, which can support context-dependent ordering of features and construction of hierarchies, as in the behavior of cross-categorization.
- Why unresolved: The paper does not explore the implications of incorporating a hierarchical structure into the HDP-PPCA model or how such an extension would affect the model's ability to capture human categorization behavior.
- What evidence would resolve it: Theoretical analysis of how a hierarchical extension of the HDP-PPCA model would affect its ability to capture human categorization behavior, or empirical studies comparing the performance of the original and extended models in modeling human categorization.

## Limitations
- Behavioral validation relies on relatively simple size-color stimuli rather than truly high-dimensional natural images
- The CIFAR-10H comparisons may not represent the full complexity of natural categories
- Variational inference may converge to local optima that don't fully capture the true posterior distribution

## Confidence
- **High confidence**: The core mechanism of learning dimensional biases through PPCA within a hierarchical mixture framework is well-supported by both theoretical foundations and simulation results.
- **Medium confidence**: The model's ability to improve few-shot generalization is demonstrated but requires more extensive validation across diverse category structures and noise levels.
- **Low confidence**: The claims about zero-shot learning capabilities are primarily theoretical at this stage, with limited empirical validation across diverse category combinations.

## Next Checks
1. **Extended natural image validation**: Test the model's performance on high-dimensional natural image datasets beyond CIFAR-10H, including datasets with more complex category structures and greater within-category variation.

2. **Parametric ablation study**: Systematically vary the number of noise dimensions and principal components to quantify the model's robustness to the curse of dimensionality and identify breaking points.

3. **Human behavioral validation**: Conduct controlled behavioral experiments with high-dimensional stimuli to directly compare human categorization patterns against model predictions, focusing on dimensional biases and compositional generalization.