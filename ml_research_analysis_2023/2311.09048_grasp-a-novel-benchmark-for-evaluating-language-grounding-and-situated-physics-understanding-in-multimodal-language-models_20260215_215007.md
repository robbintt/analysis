---
ver: rpa2
title: 'GRASP: A novel benchmark for evaluating language GRounding And Situated Physics
  understanding in multimodal language models'
arxiv_id: '2311.09048'
source_url: https://arxiv.org/abs/2311.09048
tags:
- sphere
- test
- language
- level
- physics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GRASP, a benchmark for evaluating language
  grounding and intuitive physics understanding in video-based multimodal large language
  models. The benchmark consists of two levels: Level 1 tests language grounding by
  assessing the model''s ability to relate textual descriptions to visual information,
  while Level 2 evaluates the model''s understanding of intuitive physics principles
  like object permanence and continuity.'
---

# GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models

## Quick Facts
- arXiv ID: 2311.09048
- Source URL: https://arxiv.org/abs/2311.09048
- Authors: 
- Reference count: 17
- Primary result: GRASP reveals significant shortcomings in current multimodal LLMs' grounding and intuitive physics capabilities, with models performing below or at chance level

## Executive Summary
This paper introduces GRASP, a benchmark designed to evaluate language grounding and intuitive physics understanding in video-based multimodal large language models. The benchmark uses Unity simulations to create controlled video stimuli and employs a two-level structure: Level 1 tests basic visual grounding while Level 2 evaluates understanding of intuitive physics principles like object permanence and continuity. Experiments with several state-of-the-art multimodal LLMs show that these models struggle significantly with both grounding and physics tasks, highlighting the need for benchmarks that can monitor progress in these critical areas for embodied AI systems.

## Method Summary
GRASP generates controlled Unity simulation videos that systematically test grounding and intuitive physics concepts through binary classification tasks. The benchmark presents models with videos followed by yes/no questions about physical plausibility, using standardized prompts that instruct models to ignore simulation quality and focus on physical reasoning. The evaluation tests three 7B-parameter multimodal LLMs (Video-ChatGPT, Video-LLaMA, PandaGPT) on Level 1 grounding tasks and Level 2 intuitive physics tasks, with results compared against chance level (0.50) to assess model understanding.

## Key Results
- Multimodal LLMs perform significantly below or at chance level on most GRASP tests
- Video-based models show substantial weaknesses in both grounding and intuitive physics understanding
- GPT-4 (text-based) significantly outperforms video-based models on scene descriptions
- Models show inconsistent performance across prompt variations and test types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRASP leverages controlled Unity simulations to isolate and test specific grounding and intuitive physics concepts, avoiding confounding factors present in real-world video datasets.
- Mechanism: By generating synthetic videos with precise control over object properties (shape, color, position, motion), the benchmark can systematically vary single parameters to test whether models understand specific physical principles like continuity or object permanence.
- Core assumption: Synthetic simulations can adequately represent the complexity of real-world physics and object interactions for the purposes of evaluating grounding and intuitive physics understanding.
- Evidence anchors:
  - [abstract] "We use the Unity game engine to simulate various scenes (in the form of videos) as a proxy for the real world."
  - [section 3.2.1] "In GRASP, models are validated against a set of videos per test to ensure that results are representative. A rich variation in visuals was achieved across videos by randomly sampling object colors/textures, background colors, camera angles, movement speeds, and start delays across all tests."
  - [corpus] Weak evidence - related papers focus on real-world datasets rather than synthetic simulations for intuitive physics evaluation.

### Mechanism 2
- Claim: The two-level structure of GRASP (grounding followed by intuitive physics) creates a prerequisite learning path that isolates failures to specific competency areas.
- Mechanism: Level 1 tests basic visual understanding (shapes, colors, positions, motion), which is necessary for Level 2 tests that require understanding complex physical interactions. This structure allows for pinpointing whether failures are due to poor grounding or lack of physics understanding.
- Core assumption: Basic visual grounding is a prerequisite for understanding intuitive physics concepts, and failures can be attributed to specific competency gaps.
- Evidence anchors:
  - [abstract] "The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of 'Intuitive Physics' principles..."
  - [section 2] "Our premise is that models struggling at this foundational phase will likely encounter difficulties in the next stage, where they must discern and reason about more complex physical interactions."
  - [corpus] Moderate evidence - related work on embodied AI and robot grasping also emphasizes the importance of grounding visual perception in language for physical reasoning.

### Mechanism 3
- Claim: The binary classification format with carefully designed prompts creates a standardized evaluation framework that reduces ambiguity in model responses and enables fair comparison across different multimodal LLMs.
- Mechanism: By using simple "yes/no" questions with clear instructions to ignore simulation quality, the benchmark ensures that models focus on the physical plausibility of events rather than getting distracted by visual artifacts or generating verbose responses.
- Core assumption: Binary classification with standardized prompts provides a clearer signal of model understanding than open-ended questions or multi-choice formats.
- Evidence anchors:
  - [section 3.1.1] "We construct this level of the benchmark as a binary classification problem by generating positive and negative samples from videos as follows: After the video is served to the models, we use a prompt that proposes an observation and asks whether it is true."
  - [section 3.2.2] "All prompts share the same template: The video you're seeing was generated by a simulator. Given how objects behave on earth, is ⟨ observation ⟩ plausible? Your answer should be based on the events in the video and ignore the quality of the simulation."
  - [section 5] "For quantitative evaluation, responses are only counted as valid if they begin with the word 'yes' or 'no'; the rest of the response is considered irrelevant."

## Foundational Learning

- Concept: Unity game engine simulation and video generation
  - Why needed here: Understanding how Unity simulations are created and rendered into videos is essential for interpreting the GRASP benchmark data and potentially modifying or extending the benchmark.
  - Quick check question: How would you modify a Unity scene to test a new intuitive physics concept, such as conservation of momentum?

- Concept: Multimodal large language model architectures
  - Why needed here: GRASP evaluates video-based multimodal LLMs, so understanding how these models process visual and textual inputs together is crucial for interpreting results and designing improvements.
  - Quick check question: What are the key architectural differences between video-based multimodal LLMs and text-only LLMs that might affect their performance on grounding and physics tasks?

- Concept: Violation-of-Expectation (VoE) paradigm in developmental psychology
  - Why needed here: GRASP is inspired by VoE experiments used to study infant understanding of intuitive physics, so familiarity with this research paradigm helps in understanding the benchmark's design and interpretation.
  - Quick check question: How does the VoE paradigm in infant studies map to the binary classification task used in GRASP?

## Architecture Onboarding

- Component map: Unity simulation environment -> Video dataset -> Prompt templates -> Evaluation pipeline -> Model interfaces
- Critical path: Generate or load Unity simulation videos -> Present video to model with appropriate prompt -> Capture and parse model response -> Compare response to ground truth label -> Calculate accuracy metrics
- Design tradeoffs:
  - Synthetic vs. real-world videos: Synthetic videos offer control but may not generalize to real-world scenarios
  - Binary vs. multi-choice questions: Binary questions are simpler to evaluate but may oversimplify complex concepts
  - Prompt variations: Different prompts may elicit different model behaviors, affecting comparability
- Failure signatures:
  - Low accuracy on Level 1 but high on Level 2: Model may be using pattern matching rather than genuine grounding
  - High accuracy on negative samples but low on positive: Model may have a bias toward certain responses
  - Inconsistent performance across prompt variations: Model may be sensitive to prompt phrasing rather than understanding concepts
- First 3 experiments:
  1. Run GRASP with a simple baseline model (e.g., CLIP) to establish a performance floor
  2. Test a single multimodal LLM on Level 1 only to isolate grounding capabilities
  3. Evaluate the same model on Level 2 with and without the "ignore simulation quality" instruction to see if visual artifacts affect performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model size lead to better grounding and intuitive physics understanding in multimodal LLMs?
- Basis in paper: [explicit] The authors note that "an increase in model size alone might lead to the emergence of relevant language grounding and physical reasoning capabilities" and that "video-based multimodal LLMs are at least one order of magnitude smaller" than text-based LLMs.
- Why unresolved: The authors did not test larger models and only evaluated smaller models (7B parameter variants).
- What evidence would resolve it: Testing larger versions of the evaluated models or other large multimodal LLMs on the GRASP benchmark.

### Open Question 2
- Question: How do image-based multimodal LLMs compare to video-based models on grounding and intuitive physics tasks?
- Basis in paper: [explicit] The authors state "we aim to create an image-based version of GRASP and to compare image- and video-based models" and note that GPT-4 (image-based) significantly outperformed Video-LLaMA (video-based) on scene descriptions.
- Why unresolved: No direct comparison between image and video models has been conducted yet.
- What evidence would resolve it: Evaluating both image-based and video-based models on the same benchmark tasks.

### Open Question 3
- Question: To what extent does prompt engineering affect grounding and physics understanding performance?
- Basis in paper: [explicit] The authors observe that "employing an alternative prompt can result in a substantial performance boost" and that models showed different biases based on prompt structure.
- Why unresolved: While the authors tested multiple prompts, they did not systematically explore the impact of different prompt strategies.
- What evidence would resolve it: Conducting controlled experiments varying prompt structure while keeping model and data constant.

## Limitations
- The reliance on synthetic Unity simulations may not adequately capture real-world physics complexity
- Binary classification format may oversimplify complex physical reasoning tasks
- Limited evaluation to 7B-parameter variants of three specific multimodal LLMs

## Confidence
- High confidence: Current state-of-the-art multimodal LLMs perform significantly below chance level on intuitive physics tasks
- Medium confidence: Unity simulations provide a valid proxy for real-world physics evaluation
- Low confidence: The two-level structure definitively isolates grounding from physics understanding

## Next Checks
1. **Real-world generalization test**: Evaluate the same models on a parallel benchmark using real-world videos to determine whether performance gaps are specific to synthetic simulations or reflect fundamental limitations in physics understanding.

2. **Prompt sensitivity analysis**: Systematically vary the prompt templates across different formulations to determine whether model performance is sensitive to phrasing, and identify prompt structures that maximize physics reasoning capabilities.

3. **Concept transferability test**: Create a new test where models must apply physics principles learned in one context (e.g., object permanence with simple shapes) to a novel context (e.g., object permanence with complex objects or different physical interactions) to assess true conceptual understanding.