---
ver: rpa2
title: Improving Small Language Models on PubMedQA via Generative Data Augmentation
arxiv_id: '2305.07804'
source_url: https://arxiv.org/abs/2305.07804
tags:
- data
- arxiv
- language
- performance
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a generative data augmentation approach to
  improve Small Language Models (SLMs) for domain-specific tasks, particularly in
  the medical field. The method leverages Large Language Models (LLMs) to refine and
  diversify existing question-answer pairs in the PubMedQA dataset, which are then
  used to fine-tune SLMs.
---

# Improving Small Language Models on PubMedQA via Generative Data Augmentation

## Quick Facts
- arXiv ID: 2305.07804
- Source URL: https://arxiv.org/abs/2305.07804
- Reference count: 9
- Key outcome: Generative data augmentation improves PubMedQA performance of small language models (under 1.6B parameters) to exceed few-shot GPT-4

## Executive Summary
This paper introduces a generative data augmentation approach to improve Small Language Models (SLMs) for domain-specific tasks, particularly in the medical field. The method leverages Large Language Models (LLMs) to refine and diversify existing question-answer pairs in the PubMedQA dataset, which are then used to fine-tune SLMs. Experiments demonstrate that this approach enhances the performance of SLMs, with the best SLM (under 1.6 billion parameters) outperforming few-shot GPT-4 on PubMedQA. The study highlights the potential of generative data augmentation in improving SLMs for specialized applications while maintaining computational efficiency.

## Method Summary
The method uses LLMs (GPT-3.5-turbo and GPT-4) to generate augmented training data for PubMedQA by rewriting existing question-answer pairs. This augmented data is then used to fine-tune small language models (BioGPT-Large, LLaMA-7B, Alpaca-7B) using efficient fine-tuning techniques like prefix tuning and low-rank adaptation. The approach focuses on refining and diversifying existing QA pairs rather than generating entirely new ones, which was found to be more effective. The augmented dataset is combined with the original data to train SLMs, resulting in improved performance while maintaining computational efficiency compared to large models.

## Key Results
- Best SLM (BioGPT-Large, 1.6B parameters) outperforms few-shot GPT-4 on PubMedQA after fine-tuning on augmented data
- Generative data augmentation significantly improves SLM performance on PubMedQA
- Low-rank adaptation is more robust to hyperparameter selection than prefix tuning for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative data augmentation improves SLM performance by refining and diversifying existing QA pairs, not by creating new ones.
- Mechanism: LLMs rewrite existing questions/answers to generate more varied but semantically equivalent samples, expanding the training distribution without introducing noise from LLM hallucinations.
- Core assumption: The original dataset is high-quality and the LLM's rewrites are faithful to the original semantics.
- Evidence anchors:
  - [abstract] "LLMs effectively refine and diversify existing question-answer pairs"
  - [section 4.3] "instructing an LLM lacking domain knowledge to generate entirely new question-answer pairs did not lead to an improvement"
  - [corpus] Weak; no direct corpus evidence, but related work on SLM augmentation supports refinement over generation.
- Break condition: If the original dataset is noisy or biased, or if the LLM's rewrites introduce subtle semantic shifts, performance gains will degrade.

### Mechanism 2
- Claim: Fine-tuning smaller SLMs on augmented data closes the performance gap with large models while retaining efficiency.
- Mechanism: By leveraging the generative power of LLMs for data augmentation, SLMs can learn richer representations from an expanded dataset without incurring the inference cost of large models at deployment.
- Core assumption: The augmented dataset is sufficiently large and diverse to compensate for the SLM's smaller capacity.
- Evidence anchors:
  - [abstract] "LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model after fine-tuning"
  - [section 4.3] "Incorporating this data into the training process, we can significantly improve the performance of the fine-tuned model"
  - [corpus] Weak; no direct corpus evidence, but efficiency claims align with known SLM advantages.
- Break condition: If the augmented data is too small or not diverse enough, the SLM cannot generalize beyond the original distribution.

### Mechanism 3
- Claim: Low-rank adaptation is more robust to hyperparameter selection than prefix tuning in this domain.
- Mechanism: Low-rank adaptation decomposes weight matrices into smaller matrices that are fine-tuned, making it less sensitive to hyperparameter choices compared to prefix tuning which modifies the input prefix.
- Core assumption: The domain-specific task (PubMedQA) benefits from the regularization effect of low-rank updates.
- Evidence anchors:
  - [section 4.1] "Low-rank adaptation is more robust and less sensitive to hyperparameter selection than Prefix Tuning"
  - [abstract] No direct mention, but method section implies use of low-rank adaptation.
  - [corpus] Weak; no direct corpus evidence, but low-rank adaptation is known for robustness in other settings.
- Break condition: If the task requires highly expressive fine-tuning, the low-rank constraint may limit performance.

## Foundational Learning

- Concept: Autoregressive pretraining
  - Why needed here: Understanding how LLMs acquire general language capabilities is essential to appreciate why they can generate useful augmentations for domain-specific tasks.
  - Quick check question: What is the core training objective in autoregressive pretraining?

- Concept: Efficient fine-tuning techniques (prefix tuning, low-rank adaptation)
  - Why needed here: These methods enable SLMs to adapt to augmented data without full fine-tuning, which is critical for maintaining efficiency.
  - Quick check question: How does low-rank adaptation reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Generative data augmentation principles
  - Why needed here: The method hinges on using LLMs to generate high-quality, diverse training samples, so understanding the trade-offs (quality vs. diversity) is crucial.
  - Quick check question: Why might generating entirely new QA pairs from an LLM without domain knowledge degrade performance?

## Architecture Onboarding

- Component map: PubMedQA dataset -> LLM augmentation (ChatGPT/GPT-4) -> SLM fine-tuning (BioGPT-Large, LLaMA-7B, Alpaca-7B) -> Evaluation (accuracy and F1 score)

- Critical path:
  1. Load and split PubMedQA dataset
  2. Use LLM to generate rewriteQA and newQA pairs
  3. Combine original and augmented data
  4. Fine-tune SLM using low-rank adaptation or prefix tuning
  5. Evaluate on test set

- Design tradeoffs:
  - Data quality vs. quantity: More augmentation can introduce noise; must balance diversity with semantic fidelity.
  - Model size vs. performance: Larger SLMs (e.g., BioGPT-Large) benefit more from augmentation but lose efficiency gains.
  - LLM choice: GPT-4 (domain-aware) vs. GPT-3.5-turbo (general) affects augmentation quality.

- Failure signatures:
  - Performance drops when using newQA augmentation with non-domain LLMs.
  - Overfitting to augmented data if augmentation is too repetitive.
  - Hyperparameter sensitivity in prefix tuning but not in low-rank adaptation.

- First 3 experiments:
  1. Compare low-rank adaptation vs. prefix tuning on BioGPT-Large with no augmentation.
  2. Evaluate the impact of rewriteQA vs. newQA augmentation using GPT-3.5-turbo.
  3. Test BioGPT-Large fine-tuned on GPT-4 augmented data vs. LLaMA-7B fine-tuned on same.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Dataset size constraints: PubMedQA dataset used (450 training samples) is extremely small, which may amplify the impact of data augmentation but also raises questions about generalizability to larger datasets.
- LLM dependency and cost: The approach requires access to GPT-4 or GPT-3.5-turbo for data augmentation, introducing financial costs and dependency on proprietary models.
- Implementation details missing: Key details like exact prompts for LLM augmentation, hyperparameter ranges for prefix tuning and LoRA are not specified.

## Confidence
- Claim: Generative data augmentation improves SLM performance on PubMedQA - Medium confidence
- Claim: Low-rank adaptation is more robust than prefix tuning - Medium confidence
- Claim: Best SLM outperforms few-shot GPT-4 - Medium confidence

## Next Checks
1. **Ablation study on augmentation quality**: Systematically evaluate the impact of varying the quality of augmented data by introducing controlled semantic drift in LLM rewrites. Compare performance when using high-quality vs. low-quality rewrites to quantify the sensitivity to augmentation fidelity.

2. **Cross-domain generalization test**: Apply the same augmentation approach to a different specialized domain (e.g., legal or financial question answering) with a similarly small dataset. Measure whether the performance gains observed on PubMedQA transfer to other domains or if they are task-specific.

3. **Open-source LLM comparison**: Replace GPT-4/GPT-3.5-turbo with an open-source LLM (e.g., LLaMA-2-chat or Vicuna) for data augmentation while keeping all other components constant. Quantify the performance difference to determine the minimum LLM capability required for effective augmentation.