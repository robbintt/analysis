---
ver: rpa2
title: 'Classifying multilingual party manifestos: Domain transfer across country,
  time, and genre'
arxiv_id: '2307.16511'
source_url: https://arxiv.org/abs/2307.16511
tags:
- performance
- across
- data
- different
- countries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores domain transfer for classifying political
  manifestos into eight topics across four dimensions: language, time, country, and
  genre. The authors fine-tune BERT and DistilBERT models and evaluate their performance
  on different test sets, including manifestos from different years, countries, and
  genres (manifestos vs.'
---

# Classifying multilingual party manifestos: Domain transfer across country, time, and genre

## Quick Facts
- arXiv ID: 2307.16511
- Source URL: https://arxiv.org/abs/2307.16511
- Authors: 
- Reference count: 10
- Key outcome: Fine-tuned BERT and DistilBERT models achieve strong within-domain performance on multilingual manifesto classification, with domain transfer possible across time and country boundaries but showing performance decreases

## Executive Summary
This paper explores domain transfer for classifying political manifestos into eight topics across four dimensions: language, time, country, and genre. The authors fine-tune BERT and DistilBERT models and evaluate their performance on different test sets, including manifestos from different years, countries, and genres (manifestos vs. speeches). They find that the models can be applied to future data with similar performance and that there are notable differences between the political manifestos of different countries, even if they share a language or cultural background. The best results are achieved by the English BERT model, but DistilBERT proves to be competitive at a lower computational cost. The authors also observe that the models struggle to classify the highly underrepresented "no topic" class and that the performance on New Zealand manifestos is among the top-ranking countries in accuracy, while the domain transfer to New Zealand parliamentary speeches shows a slight performance decrease.

## Method Summary
The study fine-tunes BERT and DistilBERT models on political manifestos from the Manifesto Project database, covering three languages (English, German, French) and multiple countries. The models are trained to classify quasi-sentences from manifestos into eight topic categories. Performance is evaluated through leave-one-country-out (LOCO) experiments, cross-time validation (2018-2022), and cross-genre testing (manifestos to parliamentary speeches). The authors compare model performance across these domain transfer scenarios using accuracy and macro-F1 scores.

## Key Results
- Fine-tuned BERT and DistilBERT models achieve strong within-domain classification performance (accuracy and F1 scores above 0.8 for most classes)
- Domain transfer across time (2018 to 2022) is feasible with minimal performance degradation
- Performance decreases when transferring across country boundaries, even among linguistically and culturally related nations
- Models struggle significantly with the highly underrepresented "no topic" class, achieving zero precision, recall, and F1 score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BERT and DistilBERT on a large corpus of political manifestos yields strong within-domain classification performance due to their ability to learn rich contextual representations.
- Mechanism: Pre-trained transformer models are fine-tuned on the task-specific dataset, allowing them to adapt their learned representations to the nuances of political text and the specific coding scheme.
- Core assumption: The pre-training on large amounts of text data has equipped BERT and DistilBERT with generalizable language understanding capabilities that can be effectively transferred to the domain of political manifestos.
- Evidence anchors:
  - [abstract] "First, we show the strong within-domain classification performance of fine-tuned transformer models."
  - [section 2.2] "Early feature engineering techniques relying on the bag-of-words (BoW) assumption have in recent years been replaced by more elaborated representation learning algorithms... With the advent of representation learning, it became possible to represent words (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016) and documents (Le and Mikolov, 2014) by dense vectors of a comparably low, fixed dimensionality."
  - [section 3.2] "BERT (Devlin et al., 2019) enabled the coupling of these two steps, i.e. it provided one single end-to-end trainable model for learning (contextual) representations and training the classifier."
- Break condition: If the pre-training data is not sufficiently diverse or representative of the political manifesto domain, the models may not learn effective contextual representations.

### Mechanism 2
- Claim: Domain transfer across time (2018 to 2022) is feasible because political manifestos, despite topical shifts, maintain core thematic structures that can be captured by the learned representations.
- Mechanism: The fine-tuned models, having learned the general thematic structure of political manifestos, can generalize to future data with similar performance, even if new topics emerge.
- Core assumption: While specific topics may change over time, the underlying structure and language used to discuss political issues remain relatively stable.
- Evidence anchors:
  - [abstract] "The results of the additional analysis show that (Distil)BERT can be applied to future data with similar performance."
  - [section 4] "While for the English DistilBERT model the performance on the New Zealand speeches drops by quite a margin (↓ 0.1197 / ↓ 0.0568), it merely changes when evaluated on the data from a different time period (↓ 0.0082 / ↓ 0.0074)."
- Break condition: If the language and thematic structure of political manifestos undergo significant changes over time, the learned representations may become less effective.

### Mechanism 3
- Claim: Domain transfer across country (leave-one-country-out) is possible, albeit with some performance decrease, due to the shared language and cultural background among countries.
- Mechanism: The fine-tuned models, having learned the general thematic structure of political manifestos in a specific language, can generalize to manifestos from other countries speaking the same language, although country-specific nuances may lead to some performance decrease.
- Core assumption: Countries sharing a language also share a significant amount of cultural and political context, allowing for some level of transfer learning.
- Evidence anchors:
  - [abstract] "Moreover, we observe (partly) notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background."
  - [section 4] "Regarding the generalization across country, even within languages (and hence to some extent also cultural backgrounds), there seem to be notable differences between the political communication in the different countries as observed by the large performance differences."
- Break condition: If the political systems, party structures, or cultural contexts of the countries differ significantly, the performance decrease may be too large for practical use.

## Foundational Learning

- Concept: Domain transfer and cross-domain classification
  - Why needed here: The paper aims to explore the effectiveness of fine-tuned models on data from different domains (genre, time, country) than the training data.
  - Quick check question: What is the difference between within-domain and cross-domain classification, and why is the latter more challenging?

- Concept: Pre-trained transformer models (BERT and DistilBERT)
  - Why needed here: These models are the primary tools used for classifying political manifestos, and understanding their architecture and training process is crucial for interpreting the results.
  - Quick check question: How do BERT and DistilBERT differ in terms of architecture and computational requirements, and why is DistilBERT chosen for most experiments?

- Concept: Leave-one-country-out (LOCO) evaluation
  - Why needed here: This is the method used to evaluate the model's ability to generalize across countries, and understanding its mechanics is important for interpreting the results.
  - Quick check question: How does the LOCO evaluation differ from a standard train/test split, and what insights can it provide about the model's performance?

## Architecture Onboarding

- Component map: Data extraction -> Model training -> Cross-domain evaluation
- Critical path:
  1. Extract data from the Manifesto Project database
  2. Preprocess the data (split into quasi-sentences, assign labels)
  3. Fine-tune BERT or DistilBERT on the preprocessed data
  4. Evaluate the fine-tuned model on the test set
  5. Repeat steps 3-4 for different cross-domain scenarios
- Design tradeoffs:
  - BERT vs. DistilBERT: BERT achieves slightly better performance but is more computationally expensive; DistilBERT is chosen for most experiments due to its lower computational requirements.
  - Monolingual vs. multilingual models: Monolingual models may achieve better performance on their respective languages, but multilingual models offer the advantage of handling multiple languages with a single model.
- Failure signatures:
  - Poor performance on the "no topic" class: This class is highly underrepresented and may be ambiguous or heterogeneous, making it difficult for the models to learn effective representations.
  - Large performance drops in cross-domain scenarios: This may indicate that the domains are too dissimilar for effective transfer learning.
- First 3 experiments:
  1. Within-domain classification: Fine-tune a BERT or DistilBERT model on the English manifestos from 2018 and evaluate its performance on the test set.
  2. Cross-domain classification (genre): Fine-tune a model on the English manifestos and evaluate its performance on the New Zealand parliamentary speeches.
  3. Cross-domain classification (time): Fine-tune a model on the manifestos up to 2018 and evaluate its performance on the manifestos from 2019-2022.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned BERT and DistilBERT models compare to larger language models like ChatGPT or T5 for cross-domain classification of political manifestos?
- Basis in paper: [explicit] The paper mentions that larger language models like ChatGPT and T5 could potentially achieve better performance but require more computational resources.
- Why unresolved: The paper does not provide experimental results comparing BERT/DistilBERT to larger models on this task.
- What evidence would resolve it: Experiments directly comparing BERT/DistilBERT to ChatGPT/T5 on the same cross-domain classification tasks, measuring both performance and computational cost.

### Open Question 2
- Question: What are the main factors contributing to the poor performance of models on the "no topic" class in political manifesto classification?
- Basis in paper: [explicit] The paper notes that all models struggle with the highly underrepresented "no topic" class, achieving zero precision, recall, and F1 score for this class.
- Why unresolved: The paper does not investigate the underlying reasons for this poor performance in detail.
- What evidence would resolve it: Analysis of the characteristics of "no topic" instances (e.g., length, ambiguity, annotator agreement) and experiments with different approaches to handle this class (e.g., oversampling, special treatment in loss function).

### Open Question 3
- Question: How do the observed differences in manifesto content across countries within the same language relate to political systems and cultural backgrounds?
- Basis in paper: [explicit] The paper observes notable differences between political manifestos of different countries, even those sharing a language or cultural background, but does not provide a detailed analysis of the reasons behind these differences.
- Why unresolved: The paper focuses on the technical aspects of cross-domain classification and does not delve into the political science implications of the observed differences.
- What evidence would resolve it: A detailed analysis by political scientists correlating the observed differences in manifesto content with features of the political systems (e.g., electoral systems, party systems) and cultural factors of the respective countries.

## Limitations

- The performance on the highly underrepresented "no topic" class remains problematic across all experimental conditions
- The analysis is limited to three languages and primarily European/Anglosphere countries, potentially constraining generalizability
- The study uses a relatively short timeframe (2018-2022) for temporal validation, which may not capture longer-term shifts in political discourse

## Confidence

- **High Confidence**: The strong within-domain classification performance of fine-tuned BERT and DistilBERT models is well-supported by the empirical results
- **Medium Confidence**: The claim that domain transfer across time (2018 to 2022) is feasible requires cautious interpretation due to the limited temporal gap
- **Medium Confidence**: The assertion that models can be applied to future data with similar performance is supported but may not account for longer-term shifts

## Next Checks

1. Conduct a systematic error analysis on the "no topic" class to determine whether misclassifications stem from inherent ambiguity in the coding scheme, insufficient training examples, or model architectural limitations specific to this category.
2. Extend the cross-country analysis to include non-Western political systems and languages to test whether the observed performance gaps between countries sharing linguistic or cultural backgrounds persist across more diverse political contexts.
3. Implement a temporal validation study spanning a longer timeframe (e.g., 10+ years) to quantify the rate of performance degradation over time and identify potential early warning signals for when model retraining becomes necessary.