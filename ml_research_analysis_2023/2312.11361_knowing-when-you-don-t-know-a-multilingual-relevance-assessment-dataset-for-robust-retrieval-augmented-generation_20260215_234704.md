---
ver: rpa2
title: '"Knowing When You Don''t Know": A Multilingual Relevance Assessment Dataset
  for Robust Retrieval-Augmented Generation'
arxiv_id: '2312.11361'
source_url: https://arxiv.org/abs/2312.11361
tags:
- relevant
- non-relevant
- subset
- passages
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NoMIRACL, a human-annotated dataset for
  evaluating large language model (LLM) robustness in retrieval-augmented generation
  (RAG) across 18 typologically diverse languages. The dataset includes two subsets:
  non-relevant (queries with all non-relevant passages) and relevant (queries with
  at least one relevant passage).'
---

# "Knowing When You Don't Know": A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2312.11361
- Source URL: https://arxiv.org/abs/2312.11361
- Reference count: 32
- Key outcome: NoMIRACL dataset reveals GPT-4 achieves 33.2% hallucination rate and 14.9% error rate across 18 languages

## Executive Summary
This paper introduces NoMIRACL, a human-annotated dataset for evaluating large language model (LLM) robustness in retrieval-augmented generation (RAG) across 18 typologically diverse languages. The dataset includes two subsets: non-relevant (queries with all non-relevant passages) and relevant (queries with at least one relevant passage). The authors measure LLM robustness using hallucination rate (tendency to generate answers when no answer is available) and error rate (ability to identify relevant passages). A GPT-4 baseline achieves a 33.2% hallucination rate on the non-relevant subset and a 14.9% error rate on the relevant subset, demonstrating challenges in balancing these two capacities. The results highlight the need for future work to improve LLM robustness in RAG applications.

## Method Summary
The paper constructs NoMIRACL by collecting human-annotated queries and passages across 18 languages, then creates two evaluation subsets: non-relevant (all passages are non-relevant) and relevant (at least one passage is relevant). A hybrid retrieval system (BM25 + mDPR + mColBERT) retrieves top-k passages per query. GPT-4 is evaluated using zero-shot prompting with a binary classification task: determine whether relevant passages exist among the retrieved content. The evaluation measures hallucination rate (answering when no answer exists) and error rate (failing to identify relevant passages when they exist).

## Key Results
- GPT-4 achieves 33.2% hallucination rate on non-relevant subset, indicating tendency to generate answers from non-relevant passages
- GPT-4 achieves 14.9% error rate on relevant subset, showing stronger ability to identify relevant passages when they exist
- Positive correlation (Spearman's ρ = 0.39) between hallucination rates and language resource availability, suggesting high-resource languages present harder negatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's hallucination rate increases with language resource availability because richer corpora provide more plausible but incorrect information that confuses the model.
- Mechanism: The model is exposed to more semantically related but non-relevant content in high-resource languages, increasing the likelihood of false positives when judging relevance.
- Core assumption: The model interprets semantically related content as potentially relevant when no clear answer exists.
- Evidence anchors:
  - [abstract] "We suspect the information required to answer a query is better available in a high-resource language (i.e., harder negative), which potentially confuses the LLM to judge the passage as relevant."
  - [section 5.1] "We calculate the correlation of hallucination rates in GPT-4 with resource availability, i.e., Wikipedia corpus size in NoMIRACL. We achieve a positive correlation (Spearman's rank correlation ρ = 0.39)"
  - [corpus] Weak evidence - only speculative explanation provided, no direct corpus analysis shown
- Break condition: If the correlation disappears or reverses when controlling for semantic similarity or if model architecture changes how semantic relationships are processed.

### Mechanism 2
- Claim: GPT-4 achieves lower error rates on the relevant subset because it can identify at least one relevant passage among non-relevant ones, leveraging context from partially correct information.
- Mechanism: When at least one relevant passage exists, the model uses it as an anchor to better discriminate between relevant and non-relevant content.
- Core assumption: The presence of even a single relevant passage provides enough signal for the model to make better relevance judgments.
- Evidence anchors:
  - [abstract] "GPT-4 achieves a lower 14.2% error rate on average on the relevant subset indicating a strong ability to identify relevant passages with known answers."
  - [section 5.2] "GPT-4 achieves a low error rate of 14.9%, which is 20.3% lower than the hallucination rate, indicating a stronger ability to identify relevant passages in retrieved information."
  - [corpus] No direct corpus evidence - based on dataset construction and evaluation results
- Break condition: If error rates on relevant subsets increase significantly in different prompting strategies or model architectures.

### Mechanism 3
- Claim: The vanilla zero-shot prompting approach limits GPT-4's ability to handle complex relevance judgments, particularly in distinguishing between closely related but non-relevant information.
- Mechanism: Without few-shot examples or specialized prompting techniques, the model relies solely on its pre-trained understanding, which may not be optimized for fine-grained relevance discrimination.
- Core assumption: Prompting strategy significantly impacts the model's ability to make nuanced relevance judgments.
- Evidence anchors:
  - [section 4.1] "The choice of prompt significantly influences the performance and LLMs have been shown brittle to prompting variations, training examples or long context setups"
  - [section 6] "We are actively working on benchmarking... whether better prompting strategies or fine-tuning LLMs can help improve robustness"
  - [corpus] No corpus evidence - this is acknowledged as a limitation in the paper
- Break condition: If specialized prompting techniques show no improvement over the vanilla approach in subsequent experiments.

## Foundational Learning

- Concept: Binary classification in retrieval-augmented generation
  - Why needed here: Understanding how models distinguish between relevant and non-relevant passages is fundamental to evaluating RAG robustness
  - Quick check question: What are the four possible outcomes in a binary relevance classification task?

- Concept: Hallucination rate vs error rate metrics
  - Why needed here: These metrics provide different perspectives on model robustness - one measures false confidence, the other measures false rejection
  - Quick check question: How would you interpret a model with high hallucination rate but low error rate?

- Concept: Corpus size correlation with model performance
  - Why needed here: Understanding how training data availability affects model behavior across different languages is crucial for multilingual applications
  - Quick check question: What might explain a positive correlation between corpus size and hallucination rate?

## Architecture Onboarding

- Component map: User query → Hybrid retrieval (BM25 + mDPR + mColBERT) → Top-k passages → GPT-4 zero-shot prompting → Binary relevance judgment
- Critical path: Query → Retrieval → Prompt construction → LLM inference → Binary judgment
- Design tradeoffs:
  - Context window limitation vs. information completeness
  - Cost vs. evaluation comprehensiveness (250 queries per language limit)
  - Vanilla prompting vs. specialized techniques (simplicity vs. performance)
- Failure signatures:
  - High hallucination rates (>30%) indicate model confusion with semantically related but non-relevant content
  - High error rates (>20%) suggest difficulty identifying relevant passages among noise
  - Language-specific patterns may reveal resource-dependent performance issues
- First 3 experiments:
  1. Test GPT-4 with few-shot prompting using relevant examples to see if hallucination rates decrease
  2. Evaluate open-source multilingual LLMs (Llama-2, FLAN-T5) to compare against GPT-4 baseline
  3. Implement Chain-of-Thought prompting to see if reasoning chains improve relevance discrimination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does hallucination rate in GPT-4 correlate with language resource availability beyond the observed Spearman correlation of 0.39?
- Basis in paper: [explicit] The paper observes a positive correlation (Spearman's ρ = 0.39) between GPT-4 hallucination rate and language resource size, suspecting that high-resource languages have harder negatives that confuse the LLM.
- Why unresolved: The paper only reports a single correlation value without exploring potential non-linear relationships, threshold effects, or controlling for other variables like language family or script.
- What evidence would resolve it: A comprehensive analysis of hallucination rates across different resource tiers (low, medium, high) with statistical tests for non-linear correlations, controlling for typological factors, and potentially examining specific language families separately.

### Open Question 2
- Question: What is the impact of different prompting strategies on LLM robustness in RAG across multilingual contexts?
- Basis in paper: [explicit] The paper uses a zero-shot monolingual listwise prompting strategy and notes that prompting variations significantly influence performance, mentioning future work to explore Chain-of-Thought, Chain-of-Verification, and Chain-of-Note techniques.
- Why unresolved: The paper only provides a baseline using a single vanilla prompt template without comparing against alternative prompting strategies that might improve robustness.
- What evidence would resolve it: Empirical comparisons of multiple prompting strategies (few-shot, chain-of-thought, chain-of-verification) across all 18 languages in NoMIRACL, measuring their impact on both hallucination and error rates.

### Open Question 3
- Question: How do open-source multilingual LLMs compare to GPT-4 in terms of robustness when handling non-relevant passages in RAG?
- Basis in paper: [explicit] The paper mentions ongoing work to benchmark open-source models like LLAMA-2, FLAN-T5, and PolyLM, and encourages future work to explore open-source alternatives on NoMIRACL.
- Why unresolved: The paper only provides a GPT-4 baseline without evaluating any open-source alternatives, despite noting the cost implications of using GPT-4 and the potential benefits of open-source models.
- What evidence would resolve it: Systematic evaluation of multiple open-source multilingual LLMs on NoMIRACL, comparing their hallucination and error rates against GPT-4 across all 18 languages, with analysis of performance differences relative to model size and training data composition.

## Limitations

- Single baseline evaluation using only GPT-4 with vanilla zero-shot prompting limits generalizability to other LLMs and prompting strategies
- Dataset contains only 250 queries per language, which may not provide sufficient statistical power for robust analysis across all 18 languages
- Corpus size correlation with hallucination rates is based on speculative mechanisms without direct corpus analysis validation

## Confidence

**High Confidence**: The dataset construction methodology and basic evaluation framework are well-specified and reproducible. The observation that GPT-4 shows different performance rates on relevant vs non-relevant subsets is clearly demonstrated through empirical results.

**Medium Confidence**: The correlation between corpus size and hallucination rates is statistically significant but the causal mechanism remains speculative. The choice of binary classification task simplifies a complex judgment process that may require more nuanced evaluation approaches.

**Low Confidence**: The claim that current vanilla zero-shot prompting represents the best possible approach for this task is not empirically validated against alternative prompting strategies or fine-tuned models.

## Next Checks

1. **Prompt Engineering Validation**: Test whether few-shot prompting or Chain-of-Thought prompting significantly reduces hallucination rates compared to the vanilla zero-shot approach, particularly for high-resource languages where the correlation is strongest.

2. **Open-Source Model Comparison**: Evaluate multilingual open-source LLMs (Llama-2, FLAN-T5) on the same NoMIRACL dataset to determine if the observed performance patterns are specific to GPT-4 or representative of broader LLM behavior.

3. **Corpus Analysis Validation**: Conduct direct corpus analysis to verify whether semantically related but non-relevant content is indeed more prevalent in high-resource languages, and whether this correlates with the observed hallucination patterns.