---
ver: rpa2
title: Timestamp-supervised Wearable-based Activity Segmentation and Recognition with
  Contrastive Learning and Order-Preserving Optimal Transport
arxiv_id: '2310.09114'
source_url: https://arxiv.org/abs/2310.09114
tags:
- activity
- segmentation
- recognition
- supervised
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a weakly supervised method for wearable-based
  human activity segmentation and recognition that significantly improves performance
  compared to state-of-the-art methods. To reduce the quantity of training data annotations
  without compromising the model's performance, the approach relies on a multi-stage
  architecture supervised by timestamp annotations for joint activity segmentation
  and recognition.
---

# Timestamp-supervised Wearable-based Activity Segmentation and Recognition with Contrastive Learning and Order-Preserving Optimal Transport

## Quick Facts
- arXiv ID: 2310.09114
- Source URL: https://arxiv.org/abs/2310.09114
- Reference count: 40
- The proposed method achieves significant improvements in class average F-score on Hospital/Opportunity/PAMAP2/Skoda datasets by 10.45%/17.68%/38.84%/37.33% respectively compared to methods using solely timestamp labels.

## Executive Summary
This paper addresses the challenge of wearable-based human activity segmentation and recognition using timestamp supervision, significantly reducing annotation costs while maintaining high performance. The proposed method employs a multi-stage architecture with a sample-to-prototype contrastive learning module and an efficient pseudo-label generation strategy based on optimal transport theory. By leveraging unlabeled data between timestamp annotations, the model achieves comparable performance to fully supervised methods while requiring only sparse timestamp labels.

## Method Summary
The proposed method uses a multi-stage temporal convolutional network (MS-TCN) for feature extraction, combined with a multi-label classifier, a sample-level classifier, and a projector network. The approach employs sample-to-prototype contrastive learning using class activation maps (CAMs) to estimate representative prototypes for each activity class. An order-preserving optimal transport module generates pseudo-labels for unlabeled samples between timestamp annotations, considering temporal order and activity continuity. The model is trained with a combination of multi-label classification loss, sample-level classification loss, and contrastive loss, achieving significant improvements over state-of-the-art weakly supervised methods.

## Key Results
- The model trained with timestamp supervision achieves comparable performance to fully supervised methods
- Significant improvements in class average F-score: Hospital (10.45%), Opportunity (17.68%), PAMAP2 (38.84%), Skoda (37.33%)
- Outperforms state-of-the-art weakly supervised methods by a substantial margin

## Why This Works (Mechanism)

### Mechanism 1
The sample-to-prototype contrastive learning module reduces the information gap caused by sparse timestamp annotations by leveraging class activation maps (CAMs) to estimate representative prototypes for each activity class. Prototypes are estimated by selecting feature embeddings with top CAM values for each activity class, serving as positive references in a contrastive loss that pulls embeddings of the same activity closer while pushing different activities apart.

### Mechanism 2
Order-preserving optimal transport generates plausible pseudo-labels for unlabeled samples between timestamp annotations by considering temporal order and activity continuity. The method computes a probability matrix mapping sample features to activity prototypes, constrained by an order-preserving regularization that encourages temporally adjacent samples to be assigned to the same activity, enabling the generation of soft pseudo-labels that respect the known timestamp labels.

### Mechanism 3
The multi-stage temporal convolutional network (MS-TCN) architecture with timestamp supervision can achieve performance comparable to fully supervised methods by progressively refining activity predictions across stages. MS-TCN stacks dilated convolutional layers with residual connections across multiple stages, allowing the network to capture both local and long-range temporal dependencies in the activity sequence while interpolating dense labels between sparse annotations.

## Foundational Learning

- **Concept: Contrastive learning with InfoNCE loss**
  - Why needed here: Enables the model to learn discriminative representations using unlabeled data between timestamp annotations by pulling similar samples (same activity) closer and pushing dissimilar samples (different activities) apart.
  - Quick check question: What is the role of the temperature parameter τ in the InfoNCE loss formulation, and how does it affect the contrastive learning behavior?

- **Concept: Optimal transport theory and Wasserstein distance**
  - Why needed here: Provides a principled way to compute the probability of mapping samples to activity prototypes while preserving temporal order, enabling the generation of pseudo-labels that respect activity continuity.
  - Quick check question: How does the entropy regularization term in the optimal transport formulation affect the sparsity of the transport matrix and the resulting pseudo-labels?

- **Concept: Class activation maps (CAMs) for localization**
  - Why needed here: CAMs identify which regions of the input sequence are most discriminative for each activity class, allowing the model to estimate representative prototypes for contrastive learning even with sparse supervision.
  - Quick check question: Why does the multi-label classification setup require a different CAM computation approach compared to single-label classification, and how is this handled in the model?

## Architecture Onboarding

- **Component map**: Raw sensor data → Feature extraction (MS-TCN) → Multi-label classifier + Projector → Sample-to-prototype contrast → Optimal transport pseudo-label generation → Sample-level classifier training

- **Critical path**: The forward pass must compute CAMs for prototype estimation, project features for contrastive learning, compute the optimal transport matrix, generate pseudo-labels, and calculate all loss terms across the multi-stage architecture.

- **Design tradeoffs**: Using timestamp supervision instead of full supervision significantly reduces annotation burden but requires sophisticated methods (contrastive learning + optimal transport) to compensate for the information loss. The multi-stage architecture adds computational complexity but enables better temporal modeling.

- **Failure signatures**: Poor prototype estimation due to inaccurate CAMs will manifest as noisy contrastive learning gradients and degraded segmentation performance. Incorrect pseudo-label assignments from optimal transport will appear as inconsistent predictions between adjacent samples.

- **First 3 experiments**:
  1. Train the base model with only timestamp supervision and sample-level classification loss to establish the baseline performance without any additional modules.
  2. Add the multi-label classifier and sample-to-prototype contrastive loss to evaluate the impact of contrastive learning on bridging the recognition-segmentation gap.
  3. Incorporate the optimal transport pseudo-label generation module to assess the improvement from leveraging unlabeled samples between timestamps.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method perform on datasets with different activity durations or activity transition frequencies? The paper discusses the effectiveness of the method on four public datasets (Hospital, Opportunity, PAMAP2, Skoda) with varying activity durations and transition frequencies, but does not explicitly analyze the impact of these factors on performance.

### Open Question 2
How does the proposed method compare to other state-of-the-art weakly supervised methods on video-based activity segmentation tasks? The paper mentions that weakly supervised methods have been applied to video data but does not directly compare the proposed method to video-based methods.

### Open Question 3
How does the proposed method handle noisy or incomplete timestamp annotations? The paper discusses the robustness of the method to timestamp annotations but does not explicitly address its performance with noisy or incomplete annotations.

## Limitations

- The reliance on class activation maps for prototype estimation introduces sensitivity to activation quality, particularly for activities with subtle or overlapping sensor patterns.
- The order-preserving assumption in optimal transport may not hold for datasets with frequent activity transitions or irregular patterns.
- The method's performance gains come at increased computational complexity due to the multi-stage architecture and optimal transport computations.

## Confidence

- **High confidence**: The core mechanism of using timestamp supervision with contrastive learning to bridge recognition-segmentation gaps is well-supported by experimental results showing consistent improvements across all four datasets.
- **Medium confidence**: The optimal transport-based pseudo-label generation is effective, though its performance depends heavily on the quality of the temporal regularization and the initial CAM estimates.
- **Medium confidence**: The claim that timestamp supervision achieves comparable performance to full supervision is supported by experimental results, but this may vary depending on the density and distribution of timestamp annotations in different datasets.

## Next Checks

1. **Ablation on temporal regularization**: Test the model with different values of the temporal regularization parameter in optimal transport to quantify its impact on pseudo-label quality and overall performance.

2. **Robustness to annotation sparsity**: Evaluate the method's performance with varying densities of timestamp annotations (e.g., one annotation per 10, 20, or 50 samples) to understand the minimum annotation requirements.

3. **Cross-dataset generalization**: Test the model trained on one dataset and evaluated on another to assess its ability to generalize across different sensor configurations and activity patterns.