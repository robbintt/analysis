---
ver: rpa2
title: 'From continuous-time formulations to discretization schemes: tensor trains
  and robust regression for BSDEs and parabolic PDEs'
arxiv_id: '2307.15496'
source_url: https://arxiv.org/abs/2307.15496
tags:
- tensor
- loss
- pdes
- bsde
- bsdes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops tensor train-based methods for high-dimensional
  PDEs via backward stochastic differential equations. The key idea is to reformulate
  the PDE as a BSDE and use tensor trains for function approximation, combined with
  regression-type losses for iterative backward-in-time solution.
---

# From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs

## Quick Facts
- arXiv ID: 2307.15496
- Source URL: https://arxiv.org/abs/2307.15496
- Reference count: 22
- Key outcome: Tensor trains with robust explicit BSDE loss outperform neural networks on high-dimensional PDEs, especially in terms of accuracy and computational efficiency

## Executive Summary
This paper develops tensor train-based methods for high-dimensional PDEs via backward stochastic differential equations (BSDEs). The approach reformulates PDEs as BSDEs and uses tensor trains for function approximation, combined with regression-type losses for iterative backward-in-time solution. Three loss functionals are proposed: a robust explicit BSDE loss, an implicit BSDE loss, and a projection loss. The robust explicit loss incorporates the Itô integral directly into the objective, leading to zero variance at the solution and improved numerical stability. Experiments on Hamilton-Jacobi-Bellman and Cox-Ingersoll-Ross models show that tensor trains with the robust explicit loss outperform neural network approaches in terms of accuracy and computational efficiency, especially in high dimensions.

## Method Summary
The method reformulates parabolic PDEs as BSDEs, discretizes them, and solves iteratively using tensor trains with regression-based losses. The key components are: (1) BSDE formulation converting the PDE to a stochastic representation, (2) tensor train decomposition for high-dimensional function approximation with low-rank structure, (3) alternating least squares (ALS) optimization on the TT manifold, and (4) three loss functionals (robust explicit BSDE, implicit BSDE, and projection loss) that incorporate gradient information. The robust explicit loss directly includes the Itô integral in the objective, reducing variance at the solution.

## Key Results
- Tensor trains with robust explicit BSDE loss achieve lower RMSE and PDE loss than neural networks on HJB and CIR examples
- Polynomial degree needed for good TT performance decreases with increasing state space dimension
- TT-based methods show better computational efficiency and accuracy trade-off compared to neural networks in high dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor trains provide a scalable approximation framework for high-dimensional PDEs by leveraging low-rank structures.
- Mechanism: Tensor trains represent high-dimensional functions as compositions of low-rank tensors, reducing the exponential growth of parameters with dimension to linear growth.
- Core assumption: The solution to the PDE has an underlying low-rank structure that can be captured by tensor trains.
- Evidence anchors:
  - [abstract]: "we argue that tensor trains provide an appealing framework for parabolic PDEs: The combination of reformulations in terms of backward stochastic differential equations and regression-type methods holds the promise of leveraging latent low-rank structures, enabling both compression and efficient computation."
  - [section]: "Tensor trains have the appealing property that, assuming that the ranks stay bounded, the number of coefficients grows linearly with the spatial dimension."
- Break Condition: If the solution to the PDE does not have a low-rank structure, tensor trains will not provide significant computational advantages.

### Mechanism 2
- Claim: The robust explicit BSDE loss reduces variance and improves numerical stability compared to traditional projection losses.
- Mechanism: The robust explicit loss incorporates the Itô integral directly into the objective, leading to zero variance at the solution and improved numerical stability.
- Core assumption: The Itô integral can be computed and included in the loss function without introducing significant computational overhead.
- Evidence anchors:
  - [abstract]: "The robust explicit loss incorporates the Itô integral directly into the objective, leading to zero variance at the solution and improved numerical stability."
  - [section]: "As a consequence, procedures based on (3) may become unstable in the regime fθ ≈ f∗ due to a low signal-to-noise ratio."
- Break Condition: If the Itô integral cannot be accurately computed or the computational overhead is too high, the benefits of the robust explicit loss may not be realized.

### Mechanism 3
- Claim: The alternating least squares (ALS) algorithm efficiently optimizes the tensor train representation by iteratively solving low-dimensional subproblems.
- Mechanism: ALS replaces the problem of finding all coefficients at once by a sequence of low-dimensional sub-problems, where only one component tensor is optimized in every iteration.
- Core assumption: The tensor train representation is multi-linear, allowing for efficient optimization of individual component tensors.
- Evidence anchors:
  - [section]: "The ALS algorithm targets (59) by interatively restricting the optimization to (Ml r, Φ), cycling through l = 1, ..., d, while updating the fixed component tensors using the solutions to the subproblems on (Ml r, Φ)."
  - [section]: "While global convergence to the optimum in (59) is not guaranteed, it is known that the loss L(V) decreases at every iteration, see Holtz et al. (2012a, Section 3.4)."
- Break Condition: If the tensor train representation is not multi-linear or the subproblems are not low-dimensional, the ALS algorithm may not be efficient.

## Foundational Learning

- Concept: Backward Stochastic Differential Equations (BSDEs)
  - Why needed here: BSDEs provide a stochastic representation of the PDE, enabling Monte Carlo methods for high-dimensional problems.
  - Quick check question: What is the relationship between the solution to a BSDE and the solution to the corresponding PDE?

- Concept: Tensor Train Decomposition
  - Why needed here: Tensor trains provide a scalable way to represent high-dimensional functions, reducing the computational complexity of solving PDEs.
  - Quick check question: How does the tensor train decomposition reduce the number of parameters needed to represent a high-dimensional function?

- Concept: Alternating Least Squares (ALS) Algorithm
  - Why needed here: ALS efficiently optimizes the tensor train representation by iteratively solving low-dimensional subproblems.
  - Quick check question: What is the key property of the tensor train representation that allows ALS to be efficient?

## Architecture Onboarding

- Component map: BSDE formulation -> Tensor train representation -> ALS optimization -> Loss functions (robust explicit, implicit BSDE, projection)
- Critical path:
  1. Reformulate the PDE as a BSDE.
  2. Discretize the BSDE and define loss functions.
  3. Initialize tensor train representation.
  4. Optimize tensor train using ALS.
  5. Evaluate solution accuracy.
- Design tradeoffs:
  - Tensor train rank vs. accuracy: Higher ranks allow for more accurate representations but increase computational complexity.
  - Polynomial degree vs. accuracy: Higher degrees allow for more complex functions but increase computational complexity.
  - Explicit vs. implicit BSDE losses: Explicit losses are faster but may be less stable; implicit losses are more stable but slower.
- Failure signatures:
  - High tensor train rank: Indicates that the solution may not have a low-rank structure.
  - Poor convergence of ALS: Indicates that the tensor train representation may not be suitable for the problem.
  - High PDE loss: Indicates that the solution may not accurately satisfy the PDE.
- First 3 experiments:
  1. Compare tensor train and neural network approaches on a simple 2D PDE.
  2. Evaluate the impact of tensor train rank on accuracy for a high-dimensional PDE.
  3. Compare explicit and implicit BSDE losses on a challenging high-dimensional PDE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the tensor train framework with robust explicit BSDE loss consistently outperform neural network approaches across diverse PDE classes and problem settings?
- Basis in paper: [explicit] Authors show tensor trains with robust explicit loss outperform neural networks on HJB, double well, and CIR examples in terms of accuracy and computational efficiency.
- Why unresolved: While results are promising across three examples, the study is limited in problem diversity. It remains unclear whether this advantage extends to other PDE types (elliptic, hyperbolic), nonlinearities, or different problem dimensions.
- What evidence would resolve it: Systematic numerical experiments comparing tensor trains with robust explicit loss against neural networks on a broader set of PDEs including different types, nonlinearities, and dimensions.

### Open Question 2
- Question: What is the theoretical explanation for the observed "blessing of dimensionality" in tensor train performance for high-dimensional PDEs?
- Basis in paper: [explicit] Authors observe that polynomial degree needed for good tensor train performance decreases with increasing state space dimension, hypothesizing a connection to propagation of chaos in interacting particle systems.
- Why unresolved: This is a novel empirical observation without theoretical justification. The connection to propagation of chaos is speculative and would require rigorous mathematical analysis.
- What evidence would resolve it: Theoretical analysis proving that tensor train representations become more efficient in high dimensions for certain PDE classes, or analytical results connecting tensor train performance to properties of interacting particle systems.

### Open Question 3
- Question: What are the fundamental differences in the trade-off between solution accuracy and gradient accuracy between tensor train and neural network approaches?
- Basis in paper: [explicit] Authors observe that for regression-based tensor trains, losses incorporating gradient information (Lexp_BSDE, Limp_BSDE) perform better in terms of PDE loss but worse in terms of RMSE, suggesting a trade-off between solution and gradient accuracy.
- Why unresolved: This observation is made on limited examples without a comprehensive understanding of when and why this trade-off occurs. The implications for different applications (e.g., optimal control where gradients are primary) are not fully explored.
- What evidence would resolve it: Systematic numerical studies quantifying the solution-gradient trade-off across diverse problem settings, and theoretical analysis explaining the mechanisms behind this behavior.

## Limitations

- Rank selection: The paper assumes bounded TT ranks but does not provide rigorous criteria for rank selection or demonstrate rank adaptivity across different problem types.
- Scalability bounds: While TT complexity is claimed to scale linearly with dimension, actual scaling behavior for very high dimensions (d > 10) and the impact of polynomial degree on this scaling are not empirically validated.
- General PDE class: Results are demonstrated on parabolic PDEs with specific structure. Extension to hyperbolic or elliptic PDEs with different boundary conditions requires additional validation.

## Confidence

- High confidence: The theoretical framework connecting BSDEs to parabolic PDEs and the TT representation properties are well-established in the literature and correctly applied.
- Medium confidence: The ALS optimization algorithm and the specific loss function formulations are sound, but practical performance may vary depending on problem characteristics and initialization.
- Low confidence: The claim that TT-based methods will outperform NNs across all high-dimensional PDE classes requires more extensive empirical validation across diverse problem families.

## Next Checks

1. **Rank adaptivity study**: Systematically vary TT ranks and polynomial degrees across dimensions 2-20 to establish empirical scaling laws and identify breaking points where accuracy degrades.
2. **Cross-PDE comparison**: Apply the framework to a portfolio of 5-10 diverse PDEs (including non-parabolic types) to assess generalizability beyond the demonstrated HJB and CIR examples.
3. **Initialization sensitivity**: Test multiple initialization strategies for TT components and measure their impact on convergence speed and final accuracy to identify robust initialization protocols.