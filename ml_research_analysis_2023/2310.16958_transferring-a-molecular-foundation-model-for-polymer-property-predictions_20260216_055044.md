---
ver: rpa2
title: Transferring a molecular foundation model for polymer property predictions
arxiv_id: '2310.16958'
source_url: https://arxiv.org/abs/2310.16958
tags:
- polymer
- accuracy
- properties
- pretraining
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates whether pretraining transformers on small molecules
  can transfer effectively to polymer property prediction, addressing the lack of
  large-scale polymer datasets. We compare models pretrained on small molecules (Enamine
  REAL) and polymers (PI1M) against a randomly initialized model, all fine-tuned on
  a small DFT dataset for polymer properties.
---

# Transferring a molecular foundation model for polymer property predictions

## Quick Facts
- arXiv ID: 2310.16958
- Source URL: https://arxiv.org/abs/2310.16958
- Authors: 
- Reference count: 33
- Key outcome: Pretraining transformers on small molecules achieves comparable accuracy to state-of-the-art polymer models for property prediction, reducing need for expensive polymer dataset generation.

## Executive Summary
This work evaluates whether pretraining transformers on small molecules can transfer effectively to polymer property prediction, addressing the lack of large-scale polymer datasets. The study compares models pretrained on small molecules (Enamine REAL) and polymers (PI1M) against a randomly initialized model, all fine-tuned on a small DFT dataset for polymer properties. Models pretrained on Enamine REAL achieve comparable accuracy to state-of-the-art models (TransPolymer and polyBERT) that use augmented polymer datasets. Prediction accuracy metrics (relative RMSE and R²) from 5-fold cross-validation show that pretraining significantly improves accuracy over no pretraining, and multitasking during fine-tuning generally enhances performance across properties.

## Method Summary
The method involves pretraining a BERT model on the Enamine REAL dataset (1 billion small molecules) using masked language modeling, then fine-tuning on polymer properties from a DFT dataset containing 8 properties across 6,265 samples. The study compares three approaches: a randomly initialized model (INIT), a model pretrained on small molecules (SML-MT), and a model pretrained on polymers (PLM-MT). All models use the same architecture (12-layer BERT with 12 attention heads) and are evaluated using 5-fold cross-validation with relative RMSE and R² metrics. Multitasking is employed during fine-tuning to leverage potential correlations between polymer properties.

## Key Results
- Pretraining on Enamine REAL (small molecules) achieves comparable accuracy to state-of-the-art polymer models (TransPolymer and polyBERT)
- Pretraining significantly improves prediction accuracy over randomly initialized models across all 8 polymer properties
- Multitasking during fine-tuning generally enhances performance across properties compared to single-tasking
- Pretraining on small molecules provides better learning efficiency, requiring less labeled data to achieve the same accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pretraining on small molecules transfers chemical language knowledge to polymers, enabling effective fine-tuning with limited polymer data.
- **Mechanism:** The transformer learns generic chemical structure patterns from large-scale small molecule datasets (Enamine REAL) that are transferable to polymers, even though polymer structures are more complex.
- **Core assumption:** Chemical language (bond patterns, functional groups) is shared between small molecules and polymers, and this shared knowledge can be captured by the transformer architecture.
- **Evidence anchors:**
  - [abstract]: "These results demonstrate that pretraining on small molecules can serve as a foundation model for polymer applications"
  - [section]: "the information being learned during pretraining, whether on datasets of polymers or small molecules, is primarily generic chemical language"
  - [corpus]: Weak - no direct comparison of small molecule vs polymer pretraining in related papers
- **Break condition:** If the chemical language between small molecules and polymers is fundamentally different (e.g., different bonding patterns, functional groups), transfer would fail.

### Mechanism 2
- **Claim:** Multitasking during fine-tuning improves prediction accuracy by leveraging correlations between polymer properties.
- **Mechanism:** Training on multiple properties simultaneously allows the model to learn shared representations that benefit all properties, improving data efficiency.
- **Core assumption:** Polymer properties are correlated, and learning them together is more efficient than learning them separately.
- **Evidence anchors:**
  - [abstract]: "multitasking during fine-tuning generally enhances performance across properties"
  - [section]: "Multitasking, in this case predicting multiple properties simultaneously in a single model, is a popular strategy to leverage the potential correlations among properties for better data efficiency"
  - [corpus]: Weak - no direct evidence of multitasking benefits in related papers
- **Break condition:** If polymer properties are uncorrelated or if single-tasking would be more effective for specific properties.

### Mechanism 3
- **Claim:** Pretraining provides better learning efficiency than no pretraining, requiring less labeled data to achieve the same accuracy.
- **Mechanism:** Pretrained models start with weights that already encode chemical knowledge, reducing the amount of fine-tuning data needed to reach good performance.
- **Core assumption:** The initial weights from pretraining provide a meaningful starting point that accelerates learning on the target task.
- **Evidence anchors:**
  - [abstract]: "the randomly initialized model learns quite effectively, and this suggests clear limits to the value of current pretraining procedures for molecular data"
  - [section]: "What sets a pretrained model apart is learning efficiency: the amount of structure-property data required to achieve the same prediction accuracy"
  - [corpus]: Weak - no direct comparison of pretraining vs no pretraining in related papers
- **Break condition:** If the pretraining data is too different from the target task or if the model architecture is already effective without pretraining.

## Foundational Learning

- **Concept:** Transfer learning
  - **Why needed here:** The paper evaluates whether knowledge learned from small molecules can be transferred to polymers, addressing the data scarcity issue for polymer pretraining.
  - **Quick check question:** What is the main benefit of transfer learning in this context? (Answer: Reducing the need for expensive polymer dataset generation and pretraining)

- **Concept:** Masked language modeling (MLM)
  - **Why needed here:** MLM is used for self-supervised pretraining on large-scale structure data, allowing the model to learn generic chemical language without labeled property data.
  - **Quick check question:** How does MLM work in this context? (Answer: The model is trained to predict masked tokens in SMILES strings)

- **Concept:** Multitasking
  - **Why needed here:** Multitasking is used during fine-tuning to leverage potential correlations among polymer properties, improving data efficiency.
  - **Quick check question:** What is the main advantage of multitasking in this context? (Answer: Leveraging correlations between properties for better data efficiency)

## Architecture Onboarding

- **Component map:** SMILES string -> Tokenizer (regex) -> BERT encoder (12 layers, 12 heads) -> Dropout (0.1) -> Linear layer (8 neurons) -> Property predictions

- **Critical path:** Tokenize polymer SMILES → Pass through BERT encoder → Apply dropout → Linear layer maps to property predictions

- **Design tradeoffs:**
  - Tokenization method: Regex vs BERT tokenizer (regex chosen for simplicity and similarity to small molecules)
  - Pretraining dataset: Small molecules (Enamine REAL) vs polymers (PI1M) - small molecules chosen for availability and diversity
  - Multitasking vs single-tasking: Multitasking generally better but property-dependent

- **Failure signatures:**
  - Poor performance on properties requiring multi-scale descriptions (e.g., crystallization tendency)
  - Large difference between training and test accuracy (overfitting)
  - Slow convergence during fine-tuning (poor pretraining)

- **First 3 experiments:**
  1. Train INIT model (no pretraining) on DFT dataset and compare to SML-MT - tests value of pretraining
  2. Train SML-ST and PLM-ST models - tests transferability between small molecules and polymers
  3. Vary training set size in fine-tuning (0, 1/16, 1/8, 1/4, 1/2, 1) - tests learning efficiency of different pretraining approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the transferability of small molecule pretraining to polymers scale with the complexity and length of polymer chains beyond degree-1 polymers?
- Basis in paper: [explicit] The paper uses degree-1 polymer SMILES from PI1M and notes that more complex representations exist but their impact is unclear.
- Why unresolved: The study only evaluated degree-1 polymers, leaving uncertainty about how well pretraining generalizes to longer chains with more complex structures.
- What evidence would resolve it: Systematic evaluation of pretraining transferability across polymers of varying degrees (2, 3, etc.) and comparison with chain length-matched small molecule datasets.

### Open Question 2
- Question: What specific chemical or structural features in small molecules enable effective transfer learning to polymers, and can these be identified to improve pretraining strategies?
- Basis in paper: [inferred] The paper demonstrates successful transfer but doesn't analyze which molecular features are most transferable or how to optimize pretraining for this transfer.
- Why unresolved: While transfer works, the paper doesn't investigate which chemical patterns, functional groups, or structural motifs in small molecules contribute most to polymer prediction accuracy.
- What evidence would resolve it: Feature importance analysis comparing small molecule and polymer structures, ablation studies removing specific chemical features, or contrastive analysis of successful vs. unsuccessful transfers.

### Open Question 3
- Question: How do different tokenization strategies affect the quality and transferability of pretraining for polymer applications?
- Basis in paper: [explicit] The paper mentions using both regex and BERT tokenization and provides comparison results in supplementary materials.
- Why unresolved: The paper uses regex tokenization primarily but only briefly compares it to BERT tokenization, leaving uncertainty about optimal tokenization for polymer transfer learning.
- What evidence would resolve it: Systematic comparison of multiple tokenization strategies (regex, BERT, graph-based, etc.) across various polymer properties and chain lengths, with quantitative analysis of transfer learning effectiveness.

### Open Question 4
- Question: What is the optimal balance between pretraining dataset size and fine-tuning dataset size for polymer property prediction?
- Basis in paper: [inferred] The paper shows pretraining helps even with small fine-tuning datasets but doesn't explore the relationship between pretraining and fine-tuning dataset sizes.
- Why unresolved: The study uses a fixed pretraining dataset (Enamine REAL) and fixed fine-tuning dataset (DFT), but doesn't investigate how varying their relative sizes affects prediction accuracy.
- What evidence would resolve it: Systematic experiments varying both pretraining and fine-tuning dataset sizes independently, with analysis of how their ratio affects convergence speed and final accuracy.

## Limitations
- No direct comparison between small molecule pretraining and state-of-the-art polymer models (TransPolymer and polyBERT) on the same dataset
- Evaluation limited to 8 specific polymer properties from a single DFT dataset, limiting generalizability
- Multitasking benefits are property-dependent with inconsistent results across different properties

## Confidence
- Claim: Pretraining on small molecules achieves comparable accuracy to state-of-the-art polymer models - Medium
- Claim: Pretraining significantly improves prediction accuracy over no pretraining - High
- Claim: Multitasking generally enhances performance across properties - Medium

## Next Checks
1. **Direct baseline comparison**: Run SML-MT and PLM-MT on the exact same evaluation dataset used by TransPolymer and polyBERT to verify the claimed comparable accuracy.

2. **Learning efficiency quantification**: Systematically vary the size of the fine-tuning dataset (0, 1/16, 1/8, 1/4, 1/2, 1) for SML-MT, PLM-MT, and INIT models to quantify the exact data efficiency gains from different pretraining approaches.

3. **Generalization to experimental data**: Evaluate the best-performing models (SML-MT and PLM-MT) on experimental polymer datasets to assess real-world applicability beyond DFT calculations.