---
ver: rpa2
title: Identifying Equivalent Training Dynamics
arxiv_id: '2302.09160'
source_url: https://arxiv.org/abs/2302.09160
tags:
- koopman
- training
- operator
- systems
- dynamical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying when the training
  dynamics of different machine learning methods are equivalent, which has significant
  practical and theoretical implications for improving training efficiency and robustness.
  The authors leverage Koopman operator theory, a data-driven framework from dynamical
  systems theory, to develop a framework for identifying conjugate (equivalent) and
  non-conjugate training dynamics.
---

# Identifying Equivalent Training Dynamics

## Quick Facts
- arXiv ID: 2302.09160
- Source URL: https://arxiv.org/abs/2302.09160
- Reference count: 40
- Primary result: Developed framework using Koopman operator theory to identify equivalent training dynamics in machine learning methods

## Executive Summary
This paper addresses the problem of identifying when the training dynamics of different machine learning methods are equivalent, which has significant practical and theoretical implications for improving training efficiency and robustness. The authors leverage Koopman operator theory, a data-driven framework from dynamical systems theory, to develop a framework for identifying conjugate (equivalent) and non-conjugate training dynamics. They validate their approach by correctly identifying a known equivalence between online mirror descent and online gradient descent, demonstrating that comparing Koopman eigenvalues can corroborate this equivalence.

## Method Summary
The authors develop a framework that views iterative optimization as discrete-time dynamical systems and leverages Koopman operator theory to identify equivalent training dynamics. They use Koopman mode decomposition (specifically DMD-RRR) on training trajectories, focusing on last-layer weights as observables. Conjugacy is determined by comparing Koopman spectra using Wasserstein distance between eigenvalue distributions. The approach is validated on OMD/OGD equivalence and then applied to analyze how various factors (learning rate, batch size, layer width, data set, activation function) affect neural network training dynamics.

## Key Results
- Successfully identified known equivalence between OMD and OMD through Koopman spectral comparison
- Demonstrated that learning rate to batch size ratio controls conjugacy class in neural network training
- Found that layer width, data set characteristics, and activation function affect conjugacy relationships
- Characterized non-conjugate training dynamics between shallow and wide fully connected networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conjugate training dynamics have identical Koopman eigenvalues, enabling identification of equivalent optimization behavior.
- Mechanism: The paper leverages the mathematical property that conjugate dynamical systems have identical Koopman spectra when the underlying Koopman operator has a point spectrum. By computing Koopman eigenvalues from observed training trajectories, the authors can detect when two different optimization methods produce equivalent dynamical behavior.
- Core assumption: The training dynamics of machine learning methods can be modeled as discrete-time dynamical systems with Koopman operators that have point spectra.
- Break condition: If the training dynamics exhibit chaotic behavior, have continuous spectrum in their Koopman operator, or the conjugacy mapping is not smooth/invertible, the spectral comparison will fail to identify equivalence.

### Mechanism 2
- Claim: Learning rate to batch size ratio controls the conjugacy class of neural network training dynamics.
- Mechanism: The paper demonstrates that varying learning rate and batch size independently can lead to non-conjugate training, but when the ratio η/b is held constant, the Koopman spectra remain similar, indicating equivalent optimization behavior.
- Core assumption: The dynamical behavior of neural network training is primarily determined by the ratio of learning rate to batch size rather than their absolute values.
- Break condition: If other factors (such as optimizer choice, network architecture, or data distribution) dominate the training dynamics, the η/b ratio may not be sufficient to predict conjugacy.

### Mechanism 3
- Claim: Koopman mode decomposition applied to last-layer weights can characterize neural network training dynamics.
- Mechanism: The paper uses the evolution of last-layer weights as observables for Koopman mode decomposition, arguing that these weights contain information about earlier layers through the gradient flow.
- Core assumption: The last layer weights contain sufficient information to characterize the overall training dynamics of the neural network.
- Break condition: If the earlier layers evolve independently of the last layer, or if the last layer weights do not capture the essential dynamical features of training, this observable choice may miss important conjugacy relationships.

## Foundational Learning

- Concept: Koopman operator theory and its application to dynamical systems
  - Why needed here: The entire framework for identifying equivalent training dynamics relies on understanding how Koopman operators can characterize dynamical systems through spectral analysis.
  - Quick check question: What is the fundamental difference between analyzing a dynamical system in state space versus through its Koopman operator?

- Concept: Conjugacy in dynamical systems theory
  - Why needed here: The paper's core contribution is identifying when two optimization methods are conjugate, meaning they produce equivalent dynamical behavior up to a smooth transformation.
  - Quick check question: How does conjugacy differ from topological equivalence, and why is this distinction important for comparing optimization methods?

- Concept: Dynamic Mode Decomposition (DMD) and Koopman mode decomposition
  - Why needed here: The authors use DMD-RRR to numerically compute the Koopman mode decomposition from observed training trajectories.
  - Quick check question: What are the key assumptions required for DMD to accurately approximate the Koopman spectrum of a dynamical system?

## Architecture Onboarding

- Component map: Data collection -> Observable selection (last-layer weights) -> Koopman mode decomposition (DMD-RRR) -> Spectral comparison (Wasserstein distance) -> Conjugacy determination
- Critical path: Collect training trajectories → Select observables → Compute Koopman spectra → Calculate Wasserstein distances → Classify conjugacy relationships
- Design tradeoffs:
  - Observable choice: Last-layer weights are tractable but may miss early-layer dynamics
  - Spectral cleaning: Need robust methods to identify principal eigenvalues from noisy spectra
  - Computational cost: DMD-RRR scales with trajectory length and observable dimension
- Failure signatures:
  - High Wasserstein distances between spectra that should be conjugate (indicates spectral cleaning issues)
  - Missing expected conjugacy relationships (indicates inappropriate observable choice)
  - Numerical instability in DMD-RRR computation (indicates need for regularization)
- First 3 experiments:
  1. Replicate the OGD vs OMD equivalence test on a simple convex/non-convex function pair
  2. Test different observable choices (all weights vs last layer only) on a small network
  3. Vary learning rate and batch size independently vs holding η/b constant to verify the ratio hypothesis

## Open Questions the Paper Calls Out
- How to utilize the Koopman operator theory framework to study reinforcement learning and curriculum learning scenarios
- What techniques are most effective for "cleaning" the Koopman spectra and identifying principal eigenvalues in machine learning contexts
- Whether the Koopman operator framework can be used to transform one machine learning method into another while preserving training dynamics

## Limitations
- The fundamental assumption that training dynamics have point spectra in their Koopman operators is not rigorously verified
- Observable choice (last-layer weights) may miss important dynamical features in earlier layers
- Computational tractability of full Koopman mode decomposition for large networks remains challenging

## Confidence
- High confidence: The fundamental theoretical framework using Koopman operator theory to identify conjugate training dynamics
- Medium confidence: The equivalence between OGD and OMD can be detected through spectral comparison
- Medium confidence: Learning rate to batch size ratio controls conjugacy in neural network training
- Low confidence: Last-layer weights are sufficient observables for characterizing overall training dynamics

## Next Checks
1. **Theoretical verification of point spectrum assumption**: Prove or disprove that the Koopman operator for neural network training dynamics has a point spectrum under various conditions
2. **Cross-validation with alternative observable choices**: Replicate key experiments using different observable choices (e.g., all weights, activation statistics, gradient norms) to verify that last-layer weights capture the essential dynamics
3. **Stress testing the conjugacy detection pipeline**: Systematically vary trajectory length, noise levels, and spectral cleaning parameters to determine the robustness limits of the Wasserstein distance-based conjugacy classification