---
ver: rpa2
title: Probing clustering in neural network representations
arxiv_id: '2311.07864'
source_url: https://arxiv.org/abs/2311.07864
tags:
- ball
- clustering
- stage3
- training
- block1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates how design choices in neural
  network training affect the clustering quality of hidden representations. The authors
  establish an evaluation framework using the BREEDS hierarchy, where models are trained
  on superclass labels but evaluated on subclass clustering performance.
---

# Probing clustering in neural network representations

## Quick Facts
- arXiv ID: 2311.07864
- Source URL: https://arxiv.org/abs/2311.07864
- Reference count: 40
- Primary result: Systematic investigation of how design choices affect clustering quality of neural network representations

## Executive Summary
This paper investigates how various design choices in neural network training affect the clustering quality of hidden representations. The authors establish an evaluation framework using the BREEDS hierarchy, where models are trained on superclass labels but evaluated on subclass clustering performance. They systematically explore how dataset structure, model architecture, normalization strategies, and training objectives impact the ability of neural networks to form meaningful clusters corresponding to semantically related subclasses.

The key finding is that clustering performance is highly dependent on the relationship between labeled classes and their subclasses, with unrelated subclasses yielding better clusterability than natural hierarchies. The authors also demonstrate that while certain design choices (like normalization layers or subclass pretraining) can improve clustering, these improvements don't always align with traditional performance metrics like linear probe accuracy or transfer learning effectiveness.

## Method Summary
The authors systematically investigate neural network clustering by training models on superclass labels from BREEDS datasets and evaluating clustering performance on subclass labels. They extract embeddings from multiple layers of trained models, apply agglomerative clustering with Ward linkage, and measure cluster quality using adjusted mutual information (AMI) and purity scores. The study compares multiple architectures (ResNet, VGG, Vision Transformers), normalization strategies, pretraining objectives (superclass vs subclass labels), and loss functions (cross-entropy vs supervised contrastive learning) to understand their effects on clustering performance.

## Key Results
- Datasets with unrelated subclasses yield much better clusterability than those following natural hierarchies
- Vision Transformers show lower subclass clusterability than ResNets across all layers
- Normalization layers in VGG-16 shift the layer at which maximum clusterability occurs without changing the maximum achievable AMI
- Pre-training on subclass labels provides minimal benefit for out-of-distribution clustering tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset hierarchy structure directly determines subclass clusterability in hidden representations
- Mechanism: When labeled classes contain unrelated subclasses, the model learns separate features for each subclass, leading to high clusterability. When subclasses follow a natural hierarchy, the model learns shared features, resulting in low clusterability.
- Core assumption: The "simplest" strategy for the model to predict labeled classes is to learn separate features for each subclass when subclasses are visually dissimilar
- Evidence anchors:
  - [abstract] "Datasets with labeled classes consisting of unrelated subclasses yield much better clusterability than those following a natural hierarchy"
  - [section 5.1] "When the images under each superclass are much more heterogeneous, the 'simplest' strategy for the model to predict a labeled class may be to learn a different set of features for each subclass"
  - [corpus] Weak - no direct neighbor papers address this mechanism

### Mechanism 2
- Claim: Normalization layers shift the depth at which maximum clusterability occurs in neural networks
- Mechanism: Batch normalization and layer normalization affect the representational geometry of intermediate layers, causing clusterability to peak at different depths compared to unnormalized architectures
- Core assumption: Normalization layers change the optimization dynamics and feature distributions in ways that affect clustering structure
- Evidence anchors:
  - [section 7.2] "We find that normalization layers boost clustering performance only for the intermediate layers of VGG-16, with a larger boost when superclasses contain heterogeneous subclasses"
  - [section 7.2] "batch normalization appears to yield larger improvements than layer normalization"
  - [corpus] Weak - no direct neighbor papers address normalization effects on clustering specifically

### Mechanism 3
- Claim: Pre-training granularity (superclass vs subclass labels) only benefits downstream clustering when there is domain overlap
- Mechanism: Subclass supervision provides useful feature learning for downstream tasks, but this benefit disappears when the downstream data distribution differs significantly from pretraining data
- Core assumption: Fine-grained supervision transfers domain knowledge that is only useful when the downstream task shares visual or semantic characteristics with pretraining data
- Evidence anchors:
  - [abstract] "When using pretrained models to cluster image embeddings from downstream datasets that have little overlap with the training set, pre-training on subclass labels yields only marginally better clusterability"
  - [section 5.2] "Using fine-grained (i.e. subclass) labels during pre-training offers some improvement over using superclass labels, but the improvement tends to be relatively small, suggesting that subclass information provides little benefit when detecting class structure in out-of-distribution data"
  - [corpus] Weak - no direct neighbor papers address this transfer learning aspect of clustering

## Foundational Learning

- Concept: Hierarchical classification and dataset construction
  - Why needed here: Understanding how BREEDS datasets are constructed from WordNet hierarchies is crucial for interpreting clustering results
  - Quick check question: How does BREEDS modify the standard WordNet hierarchy to make it more suitable for object recognition tasks?

- Concept: Clustering evaluation metrics (purity, adjusted mutual information)
  - Why needed here: These metrics are used throughout the paper to measure subclass clusterability
  - Quick check question: What is the key difference between purity and adjusted mutual information when evaluating clustering quality?

- Concept: Neural network architecture differences (ResNet vs VGG vs Vision Transformers)
  - Why needed here: The paper systematically compares how different architectures affect clustering performance
  - Quick check question: What are the key architectural differences between ResNet and VGG that could affect their clustering behavior?

## Architecture Onboarding

- Component map: Data preprocessing (BREEDS datasets) → Model training (various architectures with different loss functions) → Representation extraction (hidden layer embeddings) → Clustering algorithms (agglomerative clustering with Ward linkage) → Evaluation metrics (purity and AMI)
- Critical path: Data → Model Training → Representation Extraction → Clustering → Evaluation
- Design tradeoffs: Supervised pretraining vs self-supervised pretraining, architectural choices (ResNet vs VGG vs ViT), normalization strategies, and loss function selection all trade off between different clustering objectives
- Failure signatures: Low AMI scores across all layers, inconsistent cluster assignments across training runs, lack of improvement when switching from superclass to subclass pretraining on out-of-distribution data
- First 3 experiments:
  1. Reproduce the entity-13 clustering results with ResNet-50 to establish baseline clusterability patterns
  2. Compare clustering performance between standard entity-13 and entity-13-shuffled to verify the hierarchy effect
  3. Test different normalization strategies on VGG-16 to observe shifts in optimal clustering layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different clustering algorithms (beyond agglomerative clustering) compare in their ability to recover subclass structure from neural network representations?
- Basis in paper: [explicit] The authors mention that agglomerative clustering was chosen based on preliminary experiments and cite prior work showing its effectiveness, but suggest this as an opportunity for further investigation.
- Why unresolved: The paper only compares agglomerative clustering to k-means in initial experiments and uses agglomerative clustering throughout. No comprehensive comparison with other clustering methods is performed.
- What evidence would resolve it: Systematic evaluation of multiple clustering algorithms (e.g., spectral clustering, DBSCAN, mean-shift) on the same datasets and models, measuring their performance in recovering subclass structure.

### Open Question 2
- Question: Why do normalization layers shift the layer at which maximum clustering performance occurs, and why doesn't this affect the maximum achievable clustering quality?
- Basis in paper: [explicit] The authors observe that adding batch normalization or layer normalization to VGG-16 shifts the peak clustering performance to earlier layers, but doesn't change the maximum AMI attainable.
- Why unresolved: The authors note this is "somewhat perplexing" and state they are unaware of existing work characterizing this phenomenon. The underlying mechanism remains unexplained.
- What evidence would resolve it: Analysis of how normalization affects feature representations at different depths, including examination of feature distributions, class separability, and correlation with clustering performance across layers.

### Open Question 3
- Question: What causes the low consistency in cluster assignments across different training runs, despite similar layer-wise clustering performance?
- Basis in paper: [explicit] The authors find that while different training runs yield similar AMI profiles, the actual cluster assignments (measured by adjusted Rand index) are highly inconsistent, with only some exceptions.
- Why unresolved: The authors observe this phenomenon but don't provide a mechanistic explanation for why the cluster assignments vary so substantially while maintaining similar overall performance.
- What evidence would resolve it: Investigation of the stability of learned features, analysis of how small initialization differences propagate through training, and examination of whether specific architectural components contribute to this variability.

## Limitations

- Limited architectural exploration - only tested VGG-16 with normalization layers, results may not generalize to other architectures
- Out-of-distribution experiments are limited to a few BREEDS variants, potentially missing broader patterns
- Only compares agglomerative clustering to k-means, missing potential benefits from other clustering algorithms

## Confidence

- Mechanism 1: High confidence - well-established through controlled experiments comparing natural vs shuffled subclasses
- Mechanism 2: Medium confidence - observed effect is robust but mechanism is unexplained
- Mechanism 3: Medium confidence - supported by data but limited to specific out-of-distribution scenarios
- Clustering vs linear probe divergence: Medium confidence - data supports the claim but could benefit from additional downstream task evaluations

## Next Checks

1. Test transfer learning benefits on non-BREEDS datasets with varying domain overlap to validate the limited benefit claim for out-of-distribution clustering
2. Expand normalization experiments to include other architectures (ResNet, ViT) to verify if VGG-specific effects generalize
3. Compare clustering metrics with additional downstream tasks (e.g., few-shot classification) to better understand the relationship between clusterability and task performance