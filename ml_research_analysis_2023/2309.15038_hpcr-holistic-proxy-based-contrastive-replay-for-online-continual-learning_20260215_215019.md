---
ver: rpa2
title: 'HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning'
arxiv_id: '2309.15038'
source_url: https://arxiv.org/abs/2309.15038
tags:
- learning
- samples
- classes
- hpcr
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in online continual
  learning (OCL), where models struggle to retain knowledge of previously learned
  classes while adapting to new ones. The proposed method, HPCR (Holistic Proxy-based
  Contrastive Replay), builds on PCR (Proxy-based Contrastive Replay) to mitigate
  this issue.
---

# HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning

## Quick Facts
- arXiv ID: 2309.15038
- Source URL: https://arxiv.org/abs/2309.15038
- Reference count: 40
- Primary result: HPCR improves accuracy by 4.1% over PCR on Split CIFAR100 with buffer size 500

## Executive Summary
This paper addresses catastrophic forgetting in online continual learning (OCL) by proposing HPCR (Holistic Proxy-based Contrastive Replay). HPCR builds on PCR by incorporating three components: a contrastive component that conditionally uses anchor-to-sample pairs, a temperature component that decouples temperature coefficients for better generalization, and a distillation component that preserves historical knowledge. Experiments on four image datasets demonstrate that HPCR consistently outperforms state-of-the-art methods, achieving significant accuracy improvements.

## Method Summary
HPCR extends PCR by adding three key components: (1) a contrastive component that conditionally incorporates anchor-to-sample pairs when batch size exceeds a threshold, improving feature extraction; (2) a temperature component that decouples the temperature coefficient into static and dynamic parts to enhance generalization; and (3) a distillation component that constrains learning to preserve historical knowledge through proxy-based and sample-based contrastive distillation. The method uses a ResNet18 backbone, softmax classifier, and episodic memory buffer, combining LPCRCT, LPCD, and LSCD losses with hyperparameters α and β.

## Key Results
- HPCR consistently outperforms state-of-the-art methods across four datasets (Split CIFAR10, CIFAR100, MiniImageNet, TinyImageNet)
- Achieves 4.1% accuracy improvement over PCR on Split CIFAR100 with buffer size 500
- Effectively balances learning novel knowledge while retaining historical knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coupling proxy-based and contrastive-based replay manners enables complementary advantages in online continual learning
- Mechanism: Replaces anchor-to-sample pairs in contrastive loss with anchor-to-proxy pairs, reducing bias and stabilizing convergence
- Core assumption: Anchor-to-proxy pairs selected from classes present in the current batch provide sufficient semantic information while avoiding the bias of using all proxies
- Evidence anchors: [abstract] "PCR combines proxy-based and contrastive-based replay manners by replacing anchor-to-sample pairs with anchor-to-proxy pairs in contrastive-based loss"; [section] "The core motivation is the coupling of proxy-based and contrastive-based loss"
- Break condition: If the number of samples per class in a batch becomes too small, the selected proxy set may not represent the class well, leading to poor gradient estimates

### Mechanism 2
- Claim: Decoupling the temperature coefficient into two parts improves generalization
- Mechanism: Sets a static τ for the P part (probability smoothing) and a dynamic τ(s) for the G part (gradient scaling) to balance learning of new vs. old classes
- Core assumption: The sensitivity of model performance to τ differs between the probability smoothing and gradient scaling roles, requiring separate tuning
- Evidence anchors: [abstract] "The temperature component that decouples temperature coefficient into two parts based on their gradient impacts"; [section] "We define a relative penalty r(⟨z, wc⟩) on negative proxy wc"
- Break condition: If the dynamic function τ(s) does not adequately cover the optimal τ range for all classes, some classes may be under- or over-emphasized

### Mechanism 3
- Claim: Knowledge distillation components preserve historical knowledge by constraining new representations to align with old ones
- Mechanism: PCD minimizes Euclidean distance between weighted logits of old and new samples; SCD minimizes distance between sample similarities
- Core assumption: The logits and feature embeddings from previous training steps contain sufficient information to guide the learning of new samples
- Evidence anchors: [abstract] "The distillation component that constrains learning to preserve historical knowledge"; [section] "We propose a novel distillation method called proxy-based contrastive distillation (PCD)"
- Break condition: If the memory buffer is too small or the stored embeddings are outdated, the distillation loss may provide misleading gradients

## Foundational Learning

### Concept: Catastrophic forgetting in neural networks
- Why needed here: Understanding why models lose performance on old classes is essential to grasp the motivation for HPCR
- Quick check question: What causes catastrophic forgetting when training on new data in continual learning?

### Concept: Proxy-based vs. contrastive-based loss functions
- Why needed here: These are the two replay manners being coupled in HPCR; understanding their differences is key to the mechanism
- Quick check question: How do proxy-based and contrastive-based loss functions differ in how they compute similarities for training?

### Concept: Temperature scaling in softmax and contrastive losses
- Why needed here: The temperature component of HPCR relies on understanding how τ affects probability distributions and gradients
- Quick check question: What is the effect of increasing or decreasing the temperature τ in a softmax classifier?

## Architecture Onboarding

- Component map: Backbone (feature extractor z = h(x; Φ)) -> Proxy-based classifier f(z; W) -> Memory buffer M -> Loss components LPCRCT, LPCD, LSCD
- Critical path: 1) Forward pass: Extract features, compute logits for current and replayed samples; 2) Compute loss: Combine LPCRCT, LPCD, LSCD with hyperparameters α, β; 3) Backward pass: Update Φ and W using gradients from combined loss; 4) Update memory: Store current samples with logits and features
- Design tradeoffs: Larger memory buffer improves distillation but increases storage cost; Dynamic temperature τ(s) requires additional computation but improves generalization; Conditional anchor-to-sample pairs (N ≥ Nmin) balance semantic richness vs. noise
- Failure signatures: Accuracy on old classes drops rapidly → distillation component may be too weak; Accuracy on new classes plateaus → temperature component may be too conservative; Training instability → batch size too small for conditional anchor-to-sample pairs
- First 3 experiments: 1) Verify PCR baseline: Train on Split CIFAR100 with buffer size 1000, compare to ER and SCR; 2) Test contrastive component: Vary Nmin parameter, observe accuracy vs. batch size; 3) Validate temperature component: Sweep τmax, τmin, S parameters, measure accuracy and loss flatness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of Nmin (threshold for batch size) in the contrastive component's step function s(N)?
- Basis in paper: [explicit] The paper mentions Nmin as a hyperparameter used to determine whether to use anchor-to-sample pairs, but does not provide an optimal value
- Why unresolved: The paper only states that Nmin is used to control the importance of anchor-to-sample pairs based on batch size, but does not empirically determine the best value for different datasets or scenarios
- What evidence would resolve it: A systematic study varying Nmin across different batch sizes, datasets, and learning stages to determine the value that maximizes accuracy and convergence speed

### Open Question 2
- Question: How does the performance of HPCR compare to other state-of-the-art methods on more diverse datasets beyond image classification?
- Basis in paper: [inferred] The paper focuses on image classification datasets (CIFAR10, CIFAR100, MiniImageNet, TinyImageNet) and does not explore other domains
- Why unresolved: The paper demonstrates HPCR's effectiveness on image classification but does not validate its performance on other types of data such as natural language processing, time series, or graph data
- What evidence would resolve it: Experiments applying HPCR to non-image datasets and comparing its performance against state-of-the-art methods in those domains

### Open Question 3
- Question: What is the impact of different cycle lengths (S) in the temperature component's dynamic function τ(s) on model performance?
- Basis in paper: [explicit] The paper mentions that τ(s) is a periodic function with cycle length S, but does not explore how different values of S affect performance
- Why unresolved: The paper sets S = 500 based on experimental results but does not provide a comprehensive analysis of how varying S influences accuracy, convergence speed, or robustness to forgetting
- What evidence would resolve it: A detailed ablation study testing various cycle lengths (e.g., S = 50, 100, 250, 500, 1000) and measuring their impact on final accuracy, learning stability, and catastrophic forgetting

## Limitations

- The decoupled temperature component's effectiveness relies on empirical tuning rather than theoretical justification
- The distillation components lack detailed ablation studies showing their individual contributions to performance gains
- The paper doesn't thoroughly explore failure cases or provide comprehensive analysis of when HPCR might underperform

## Confidence

- **High Confidence**: PCR baseline effectiveness and general framework of combining proxy-based and contrastive-based replay
- **Medium Confidence**: The three proposed components (contrastive, temperature, distillation) work synergistically as claimed
- **Low Confidence**: Specific hyperparameter choices and their robustness across different dataset distributions

## Next Checks

1. **Ablation study**: Remove each of the three components (contrastive, temperature, distillation) individually to quantify their marginal contributions to overall performance
2. **Hyperparameter sensitivity**: Systematically vary temperature parameters (τmax, τmin, S) and distillation weights (α, β) to establish robust ranges
3. **Memory efficiency analysis**: Test HPCR with progressively smaller memory buffers to identify the minimum buffer size needed for effective distillation