---
ver: rpa2
title: Is ChatGPT a Good Recommender? A Preliminary Study
arxiv_id: '2304.10149'
source_url: https://arxiv.org/abs/2304.10149
tags:
- recommendation
- chatgpt
- user
- tasks
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the potential of ChatGPT as a general-purpose
  recommendation model. The authors design prompts to convert recommendation tasks
  into natural language tasks and evaluate ChatGPT''s performance on five recommendation
  scenarios: rating prediction, sequential recommendation, direct recommendation,
  explanation generation, and review summarization.'
---

# Is ChatGPT a Good Recommender? A Preliminary Study

## Quick Facts
- arXiv ID: 2304.10149
- Source URL: https://arxiv.org/abs/2304.10149
- Reference count: 40
- Primary result: ChatGPT performs well on rating prediction but poorly on sequential and direct recommendation tasks, though excels at explainability tasks according to human evaluation

## Executive Summary
This paper investigates ChatGPT's potential as a general-purpose recommendation model by evaluating its performance across five recommendation scenarios without fine-tuning. The authors use zero-shot and few-shot prompting to convert recommendation tasks into natural language tasks, finding that ChatGPT performs well on rating prediction but struggles with sequential and direct recommendation tasks. However, human evaluations reveal that ChatGPT outperforms state-of-the-art methods in generating clear and reasonable explanations and summaries, highlighting the limitations of relying solely on objective evaluation metrics.

## Method Summary
The study uses the Amazon Beauty dataset to evaluate ChatGPT (gpt-3.5-turbo) across five recommendation tasks: rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Unlike traditional approaches, the authors rely solely on prompt engineering without fine-tuning, using both zero-shot and few-shot prompting strategies. They construct task-specific prompts with components including task descriptions, behavior injection, and format indicators. Evaluation combines automated metrics (RMSE, MAE, HR@k, NDCG@k, BLEU, ROUGE) with human evaluation for explainability tasks.

## Key Results
- ChatGPT with few-shot prompting outperforms traditional matrix factorization and MLP methods on rating prediction tasks
- ChatGPT shows poor performance on sequential and direct recommendation tasks, achieving only early baseline levels on certain metrics
- Human evaluations demonstrate ChatGPT's superiority over state-of-the-art methods in generating clearer and more reasonable explanations

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT can generate reasonable ratings for items based on minimal user history when given clear task instructions. Zero-shot and few-shot prompting transforms recommendation tasks into natural language tasks that ChatGPT can handle without fine-tuning. The core assumption is that ChatGPT's pretraining on large corpora includes sufficient world knowledge to infer preferences from item descriptions and user history. Evidence shows few-shot prompts outperform traditional methods on rating prediction metrics. Break condition: Performance degrades when item descriptions are too sparse or user history lacks meaningful signals.

### Mechanism 2
Few-shot prompting with user interaction history helps ChatGPT capture user preferences better than zero-shot. Providing concrete examples of user-item interactions in prompts allows ChatGPT to learn implicit user interests and preferences. The core assumption is that ChatGPT can extract meaningful patterns from limited interaction examples and apply them to new recommendations. Evidence shows few-shot prompting improves performance over zero-shot. Break condition: When interaction history is too short or diverse to establish clear patterns.

### Mechanism 3
ChatGPT's language generation strengths make it effective at generating explanations and summaries even when objective metrics are poor. ChatGPT can synthesize coherent explanations and summaries by deeply understanding context and relationships in the data. The core assumption is that generating natural language explanations requires understanding beyond what can be captured by BLEU/ROUGE metrics. Human evaluation shows ChatGPT generates "clearer and more reasonable" explanations despite poor automated metric scores. Break condition: When task requires highly structured or formulaic output that contradicts ChatGPT's natural language style.

## Foundational Learning

- Concept: Prompt engineering for LLMs
  - Why needed here: Different recommendation tasks require carefully crafted prompts to elicit useful responses
  - Quick check question: What are the three main components of the task-specific prompts used in this study?

- Concept: Evaluation metrics for recommendation systems
  - Why needed here: Understanding RMSE, MAE, HR@k, NDCG@k, BLEU, ROUGE to interpret results
  - Quick check question: Which metrics would you use to evaluate a sequential recommendation system?

- Concept: Human evaluation methodology
  - Why needed here: Recognizing when objective metrics fail to capture true quality of generated content
  - Quick check question: What are the limitations of using BLEU/ROUGE scores for evaluating explanations?

## Architecture Onboarding

- Component map: Prompt construction module → ChatGPT API interface → Output refinement module → Evaluation pipeline
- Critical path: Prompt construction → ChatGPT API call → Output refinement → Evaluation
- Design tradeoffs: Zero-shot vs few-shot (simplicity vs performance); prompt length vs comprehensiveness (balancing context window constraints); automated vs human evaluation (scalability vs accuracy)
- Failure signatures: ChatGPT generates off-topic responses → Prompt construction issue; format validation failures → Output refinement logic insufficient; poor performance on accuracy metrics but good human scores → Need different evaluation approach
- First 3 experiments: 1) Test zero-shot prompting on rating prediction with varying prompt complexity; 2) Compare few-shot performance with different numbers of examples (1, 3, 5); 3) Evaluate human vs automated evaluation correlation for explanation tasks

## Open Questions the Paper Calls Out

### Open Question 1
How can we incorporate more relevant training data and techniques to improve ChatGPT's performance in recommendation tasks? The paper suggests significant potential for improvement but doesn't provide specific strategies. Evidence that would resolve this includes successful implementation and evaluation of specific strategies for incorporating more relevant training data.

### Open Question 2
What are the most effective ways to introduce more guidance and constraints to help ChatGPT accurately capture historical interests and make reasonable recommendations within a limited scope for sequential recommendation tasks? The paper identifies this need but doesn't provide specific guidance. Evidence that would resolve this includes development and evaluation of specific guidance or constraints that significantly improve performance.

### Open Question 3
How can we design more effective prompts to elicit better responses from ChatGPT for recommendation tasks, considering its strengths and limitations? While the paper presents various prompt designs, it doesn't explore optimal prompt structures. Evidence that would resolve this includes development and evaluation of optimized prompt structures that significantly improve performance.

## Limitations
- Single dataset (Amazon Beauty) limits generalizability across different recommendation domains
- Lack of systematic exploration of prompt engineering techniques beyond basic zero-shot and few-shot approaches
- Underspecified output refinement strategies for handling ChatGPT's variable formatting affect reproducibility

## Confidence

- **High confidence**: ChatGPT's strong performance on rating prediction tasks, evidenced by quantitative metrics showing few-shot prompting outperforming traditional methods
- **Medium confidence**: Human evaluation superiority for explainability tasks, though limited by methodology details and small scale
- **Low confidence**: Claims about sequential and direct recommendation performance, given the significant gap between ChatGPT and traditional methods

## Next Checks

1. Replicate the study across multiple recommendation datasets (e.g., MovieLens, Yelp) to assess domain generalizability
2. Systematically vary prompt engineering techniques (number of examples, instruction clarity, format specification) to identify optimal configurations
3. Implement controlled human evaluation studies with larger sample sizes and clear inter-rater reliability metrics to validate explainability performance claims