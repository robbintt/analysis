---
ver: rpa2
title: 'In-Context Unlearning: Language Models as Few Shot Unlearners'
arxiv_id: '2310.07579'
source_url: https://arxiv.org/abs/2310.07579
tags:
- unlearning
- icul
- learning
- rate
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel class of unlearning methods for large
  language models (LLMs) called "In-Context Unlearning" (ICUL). The method unlearns
  specific training instances by providing them in context with flipped labels, without
  updating model parameters.
---

# In-Context Unlearning: Language Models as Few Shot Unlearners

## Quick Facts
- **arXiv ID**: 2310.07579
- **Source URL**: https://arxiv.org/abs/2310.07579
- **Reference count**: 40
- **Primary result**: Proposes In-Context Unlearning (ICUL) method that removes training instances from LLMs via context manipulation without parameter updates

## Executive Summary
This paper introduces In-Context Unlearning (ICUL), a novel black-box method for removing specific training instances from large language models without accessing model parameters. The approach works by presenting the training instance to be forgotten with a flipped label in the prompt, along with correctly labeled examples from the same distribution. Experimental results on three text classification datasets demonstrate that ICUL performs comparably to or better than state-of-the-art unlearning methods that require parameter access, while offering practical advantages as a black-box removal mechanism.

## Method Summary
The ICUL method constructs prompts at inference time by (1) flipping the label of the training instance to be forgotten, (2) adding 2-6 correctly labeled examples from the training data (excluding the forget point), and (3) appending the query input. This approach leverages the model's in-context learning capability to override the influence of the original training instance without any parameter updates. The method is evaluated using Bloom models (560M and 1.1B parameters) fine-tuned on three text classification datasets, with unlearning success measured through membership inference attacks.

## Key Results
- ICUL achieves AUC scores around 0.5 on LiRA-Forget evaluation, indicating successful unlearning comparable to parameter-based methods
- The method maintains competitive classification accuracy while removing specific training instances
- Performance scales with context length, with ICUL(6) generally outperforming ICUL(2)
- ICUL outperforms baseline methods that don't use additional context examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context unlearning removes influence of training instances by flipping their labels in context
- Mechanism: Providing a training instance with a flipped label in the prompt causes the model to "unlearn" that instance by overriding its learned association with the original label
- Core assumption: The model's in-context learning behavior allows it to adjust predictions based on context examples, even when they contradict training
- Evidence anchors:
  - [abstract]: "To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth"
  - [section]: "Step 1: Flip label on forget point. Given a deletion request, we flip the label on the corresponding training point whose influence should be removed"
  - [corpus]: Weak. No direct corpus evidence; this is the paper's novel contribution

### Mechanism 2
- Claim: Adding correctly labeled examples in context restores model performance after label flipping
- Mechanism: The context contains both the flipped-label forget point and several correctly labeled examples from the same distribution, balancing out the unlearning effect
- Core assumption: The model can balance conflicting information in context and arrive at a prediction that's closer to the original model behavior
- Evidence anchors:
  - [abstract]: "Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods"
  - [section]: "Step 2: Add s correctly labelled training points. Next, excluding the forget point, we randomly sample s labeled example pairs"
  - [corpus]: Weak. This mechanism is inferred from the experimental design but not explicitly validated in the corpus

### Mechanism 3
- Claim: Black-box unlearning works because it manipulates the model's input distribution rather than its parameters
- Mechanism: By crafting specific contexts at inference time, the model's output distribution becomes indistinguishable from a model that was retrained without the forget point
- Core assumption: The model's behavior is sufficiently sensitive to context that carefully constructed prompts can mimic parameter updates
- Evidence anchors:
  - [abstract]: "The proposed method offers a new perspective on unlearning mechanisms in LLMs and has practical appeal as a black-box removal mechanism"
  - [section]: "Our ICUL method does not require any knowledge of the LLM's parameters, and yet manages to maintain performance levels that are competitive with or in some cases exceed the state-of-the-art LLM unlearning method that requires access to the LLM parameters"
  - [corpus]: Moderate. The corpus shows this is a novel approach compared to parameter-based methods, supporting the black-box claim

## Foundational Learning

- **Concept: In-context learning in transformers**
  - Why needed here: Understanding how LLMs can learn from context without parameter updates is fundamental to why ICUL works
  - Quick check question: How does in-context learning differ from fine-tuning in terms of what's being updated?

- **Concept: Membership inference attacks**
  - Why needed here: The evaluation method relies on measuring whether an adversary can distinguish between training and forget points after unlearning
  - Quick check question: What does a successful membership inference attack tell us about model privacy?

- **Concept: Likelihood ratio testing**
  - Why needed here: The LiRA-Forget evaluation uses likelihood ratio tests to determine if unlearning was successful
  - Quick check question: Why is sample splitting necessary when approximating the likelihood ratio test?

## Architecture Onboarding

- **Component map**: Data → Finetuning → ICUL prompt construction → Inference → Evaluation via LiRA-Forget
- **Critical path**: Data → Finetuning → ICUL prompt construction → Inference → Evaluation via LiRA-Forget
- **Design tradeoffs**: Black-box approach trades parameter access for inference-time computation; context length affects both unlearning efficacy and computational cost
- **Failure signatures**: If AUC remains high after unlearning, the method failed; if test accuracy drops significantly, unlearning may be too aggressive
- **First 3 experiments**:
  1. Baseline: Run membership inference on original model to establish performance without unlearning
  2. ICUL(2): Test minimal context length with 2 additional examples
  3. ICUL(6): Test maximum context length with 6 additional examples to see if more context improves results

## Open Questions the Paper Calls Out

- **How does the performance of In-Context Unlearning scale with increasingly larger datasets and model sizes?**
  - Basis in paper: [inferred] The authors mention that future research will seek to extend their methodology to larger datasets and models
  - Why unresolved: The current study only evaluates on datasets with 25,000 points and models with up to 1.1 billion parameters
  - What evidence would resolve it: Experimental results showing the performance of In-Context Unlearning on datasets with millions of points and models with hundreds of billions of parameters

- **Can In-Context Unlearning be extended to unlearn multiple training points simultaneously?**
  - Basis in paper: [inferred] The authors mention exploring the potential of unlearning multiple points as a future direction
  - Why unresolved: The current study focuses on unlearning individual training points
  - What evidence would resolve it: Experimental results demonstrating the effectiveness of In-Context Unlearning when unlearning multiple training points at once

- **How does the choice of hyperparameters, such as the number of context examples and the label flipping strategy, affect the performance of In-Context Unlearning?**
  - Basis in paper: [explicit] The authors conduct sensitivity analyses varying context length and the dependence on the forget point
  - Why unresolved: While the authors investigate some hyperparameters, a comprehensive study of all relevant hyperparameters and their interactions is needed
  - What evidence would resolve it: Systematic experiments varying all relevant hyperparameters and their combinations

## Limitations

- Evaluation relies solely on membership inference attacks rather than direct measures of training data removal
- The mechanism assumes context examples will override training-based associations without empirical validation
- Method effectiveness may be limited by context window constraints for larger models or complex forgetting tasks
- Results are only demonstrated on three text classification datasets, limiting generalizability

## Confidence

- **High confidence**: The black-box nature of ICUL and its competitive performance compared to parameter-based methods (AUC scores around 0.5 indicating successful unlearning)
- **Medium confidence**: The mechanism by which context examples override training-based associations, as this is inferred rather than directly validated
- **Low confidence**: The generalizability of results beyond the three text classification datasets tested, and whether the method scales to larger models or different task types

## Next Checks

1. **Direct Retraining Baseline**: Implement a full retraining experiment where the forget point is excluded from training, then compare performance metrics (AUC, accuracy) to ICUL results to establish the true upper bound for unlearning effectiveness.

2. **Mechanism Ablation Study**: Test the contribution of each component by running variants: (a) only flipped labels without additional context examples, (b) only additional context examples without label flipping, and (c) varying the order of context examples to determine sensitivity to prompt structure.

3. **Context Window Scaling**: Systematically test ICUL across different context lengths and model sizes to identify the relationship between available context, model capacity, and unlearning effectiveness, particularly focusing on when the method breaks down due to context constraints.