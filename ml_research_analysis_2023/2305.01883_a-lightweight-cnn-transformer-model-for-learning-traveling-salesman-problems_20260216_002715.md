---
ver: rpa2
title: A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems
arxiv_id: '2305.01883'
source_url: https://arxiv.org/abs/2305.01883
tags:
- proposed
- layer
- transformer
- attention
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a CNN-Transformer model for solving Traveling
  Salesman Problems (TSPs) by combining a convolutional neural network (CNN) embedding
  layer with a standard Transformer model. The CNN embedding layer extracts local
  spatial features from the input data, while the Transformer model uses partial self-attention
  to focus on recently visited nodes in the decoder.
---

# A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems

## Quick Facts
- arXiv ID: 2305.01883
- Source URL: https://arxiv.org/abs/2305.01883
- Authors: 
- Reference count: 40
- The proposed CNN-Transformer model achieves state-of-the-art performance on various TSP instances, outperforming other Transformer-based models in terms of solution quality, GPU memory usage, and inference time.

## Executive Summary
This paper proposes a CNN-Transformer model for solving Traveling Salesman Problems by combining a convolutional neural network (CNN) embedding layer with a standard Transformer model. The CNN embedding layer extracts local spatial features from the input data, while the Transformer model uses partial self-attention to focus on recently visited nodes in the decoder. This approach improves upon existing Transformer-based models by better learning local compositionality and reducing computational complexity and GPU memory usage. The proposed model achieves state-of-the-art performance on various TSP instances, outperforming other Transformer-based models in terms of solution quality, GPU memory usage, and inference time.

## Method Summary
The proposed CNN-Transformer model consists of a CNN embedding layer followed by a standard Transformer encoder-decoder architecture. The CNN embedding layer uses a 1D convolutional kernel over the k-nearest neighbor node features to extract local spatial features from the input coordinates. The encoder processes these embeddings through multi-head self-attention layers with batch normalization. The decoder generates the tour step-by-step using partial self-attention, which attends only to the m most recent nodes, combined with masked multi-head attention and a pointer layer to output the next node in the tour. The model is trained using reinforcement learning with the REINFORCE algorithm, where the reward is the negative tour length.

## Key Results
- The model achieves state-of-the-art performance on TSP50 and TSP100 instances, with an average tour length of 4.813 and 7.796 respectively.
- The model consumes approximately 20% less GPU memory and has 45% faster inference time compared to other Transformer-based models.
- The model outperforms TSP Transformer and GAT models in terms of solution quality, GPU memory usage, and inference time on TSPLIB dataset.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CNN embedding layer improves spatial feature extraction compared to linear embeddings in standard Transformers.
- Mechanism: By using a convolutional kernel over the k-nearest neighbor node features, the model captures local spatial structure in the Euclidean plane.
- Core assumption: Local spatial relationships in TSP node coordinates are informative for determining good tours.
- Evidence anchors:
  - [abstract]: "Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer-based models."
  - [section]: "Linear embedding in the standard Transformer model does not consider local spatial information and has limitations in learning local compositionality. We add a CNN embedding layer to the standard Transformer model to extract the local spatial features of the input data as the CNN is effective in learning the spatial invariance of nodes in the Euclidean space."
- Break condition: If TSP instances have nodes distributed uniformly with no meaningful local structure, the CNN kernel may not extract useful features.

### Mechanism 2
- Claim: Partial self-attention reduces computational complexity and improves solution quality by focusing on recently visited nodes.
- Mechanism: Instead of attending to all previously visited nodes, the decoder uses only the m most recent nodes as reference vectors for self-attention.
- Core assumption: Recent nodes in a partial tour are more relevant to selecting the next node than earlier nodes.
- Evidence anchors:
  - [abstract]: "It also removes considerable redundancy in fully-connected attention models using the proposed partial self-attention."
  - [section]: "The proposed partial self-attention performs attention only on recently visited nodes. We expect that the proposed model is able to better learn local compositionality compared with previous works based on fully-connected attention."
- Break condition: If the optimal solution requires considering distant past nodes, limiting attention to recent nodes may degrade performance.

### Mechanism 3
- Claim: Batch normalization improves training stability for large node counts compared to layer normalization.
- Mechanism: Batch normalization normalizes across the batch dimension rather than per-layer, which helps when the number of nodes is large.
- Core assumption: The standard Transformer's layer normalization is less effective for TSP with many nodes.
- Evidence anchors:
  - [section]: "Similar to previous studies [4, 5], we use batch normalization instead of using layer normalization as it can effectively handle a large number of nodes."
- Break condition: If the batch size becomes too small, batch normalization may become unstable.

## Foundational Learning

- Concept: Graph neural networks and attention mechanisms
  - Why needed here: The model combines CNN spatial feature extraction with Transformer attention for TSP routing.
  - Quick check question: How does multi-head attention work in a Transformer decoder?

- Concept: Reinforcement learning for combinatorial optimization
  - Why needed here: The model is trained using REINFORCE algorithm with tour length as reward.
  - Quick check question: What is the role of the baseline in REINFORCE algorithm?

- Concept: Convolutional neural networks for spatial data
  - Why needed here: CNN embedding layer extracts local spatial features from node coordinates.
  - Quick check question: How does a 1D convolution kernel capture spatial relationships in TSP?

## Architecture Onboarding

- Component map:
  - Input coordinates -> CNN embedding layer -> Encoder (L layers of self-attention + feed-forward) -> Decoder (partial self-attention + masked attention + feed-forward) -> Pointer layer -> Output tour

- Critical path:
  1. CNN embedding processes input coordinates
  2. Encoder transforms embeddings through self-attention layers
  3. Decoder generates tour step-by-step using partial self-attention
  4. Pointer layer outputs probability distribution over next node

- Design tradeoffs:
  - k (CNN kernel size) vs. spatial feature richness: Larger k captures more context but increases computation
  - m (reference vector count) vs. attention quality: Smaller m reduces computation but may miss distant relevant nodes
  - Batch normalization vs. layer normalization: Batch norm handles large node counts better but needs sufficient batch size

- Failure signatures:
  - Poor tour quality: May indicate CNN kernel size too small or m too restrictive
  - High GPU memory usage: May indicate need to reduce m or batch size
  - Slow convergence: May indicate learning rate too low or insufficient training epochs

- First 3 experiments:
  1. Vary k (CNN kernel size) from 5 to 15 and measure tour quality and memory usage
  2. Vary m (reference vector count) from 3 to 20 and measure solution quality vs. inference time
  3. Compare batch normalization vs. layer normalization on large TSP instances (n=200)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed CNN-Transformer model perform on larger TSP instances beyond TSP100?
- Basis in paper: [inferred] The paper evaluates the model on TSP50 and TSP100 instances but does not explore larger problem sizes.
- Why unresolved: The paper does not provide results or analysis for TSP instances larger than 100 nodes.
- What evidence would resolve it: Testing the model on TSP instances with more than 100 nodes and comparing its performance to other state-of-the-art methods would provide evidence of its scalability.

### Open Question 2
- Question: Can the proposed partial self-attention mechanism be applied to other combinatorial optimization problems beyond TSP?
- Basis in paper: [explicit] The paper focuses on applying the partial self-attention mechanism to TSP and demonstrates its effectiveness in improving solution quality and computational efficiency.
- Why unresolved: The paper does not explore the applicability of the partial self-attention mechanism to other combinatorial optimization problems.
- What evidence would resolve it: Applying the partial self-attention mechanism to other combinatorial optimization problems, such as vehicle routing problems or graph coloring, and evaluating its performance compared to existing methods would provide evidence of its generalizability.

### Open Question 3
- Question: How does the performance of the proposed model compare to traditional optimization-based solvers like Concorde for larger TSP instances?
- Basis in paper: [inferred] The paper mentions that traditional optimization-based solvers like Concorde still outperform neural network models in terms of performance but does not provide a direct comparison for larger TSP instances.
- Why unresolved: The paper does not provide a comparison of the proposed model's performance to traditional solvers like Concorde for TSP instances larger than 100 nodes.
- What evidence would resolve it: Conducting a direct comparison of the proposed model's performance to traditional solvers like Concorde for TSP instances with more than 100 nodes would provide evidence of its competitiveness.

## Limitations
- Limited ablation studies on the sensitivity of performance to CNN kernel size (k) and reference vector count (m).
- No comparison against classical TSP solvers on large-scale instances beyond 100 nodes.
- Claims about generalization to different scales and distributions of TSP instances are not thoroughly validated.

## Confidence
- **High confidence**: The model achieves state-of-the-art performance on standard TSP benchmarks with 50 and 100 nodes, with concrete metrics (20% less GPU memory, 45% faster inference) provided.
- **Medium confidence**: The theoretical justification for CNN embedding and partial self-attention mechanisms is well-articulated, but the empirical evidence is limited to specific parameter settings.
- **Low confidence**: Claims about generalization to different scales and distributions of TSP instances are not thoroughly validated.

## Next Checks
1. **Architecture Sensitivity Analysis**: Systematically vary CNN kernel size (k) and reference vector count (m) to quantify their impact on solution quality and computational efficiency.
2. **Scalability Test**: Evaluate model performance on TSP instances with 200+ nodes to assess scalability claims and memory efficiency improvements.
3. **Robustness Evaluation**: Test the model on non-uniform node distributions (e.g., clustered, Gaussian) to verify generalization beyond the uniform training distribution.