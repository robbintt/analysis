---
ver: rpa2
title: 'AVIS: Autonomous Visual Information Seeking with Large Language Model Agent'
arxiv_id: '2306.08129'
source_url: https://arxiv.org/abs/2306.08129
tags:
- query
- visual
- image
- object
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A VIS is an autonomous visual information seeking framework that
  leverages a large language model to dynamically plan tool usage and reason about
  their outputs. The system addresses knowledge-intensive visual question answering
  tasks by integrating computer vision tools, web search, and image search with an
  LLM-powered planner and reasoner.
---

# AVIS: Autonomous Visual Information Seeking with Large Language Model Agent

## Quick Facts
- arXiv ID: 2306.08129
- Source URL: https://arxiv.org/abs/2306.08129
- Reference count: 40
- Primary result: AVIS achieves state-of-the-art 50.7% accuracy on Infoseek's unseen entity split

## Executive Summary
AVIS introduces an autonomous visual information seeking framework that leverages a large language model to dynamically plan tool usage and reason about outputs. The system addresses knowledge-intensive visual question answering by integrating computer vision tools, web search, and image search with an LLM-powered planner and reasoner. AVIS achieves state-of-the-art results on benchmarks like Infoseek and OK-VQA, with 50.7% accuracy on Infoseek's unseen entity split, significantly outperforming previous methods. The approach demonstrates the effectiveness of dynamic decision-making over sequential execution and highlights the importance of each tool component through ablation studies.

## Method Summary
AVIS is a dynamic decision-making framework that uses a large language model to autonomously select and execute visual information seeking tools. The system consists of three components: an LLM-powered planner that dynamically determines which tool to use next, working memory that stores tool outputs and state history, and an LLM-powered reasoner that analyzes tool outputs to extract key information. The planner uses a transition graph constructed from human decision-making data and in-context examples to guide tool selection and query formulation. Tools include computer vision functions (detection, VQA, captioning), web search, image search, OCR, and LLM QA. The framework processes visual questions by iteratively selecting tools, executing them, and using the reasoner to determine if outputs are informative or if further information is needed.

## Key Results
- Achieves 50.7% accuracy on Infoseek's unseen entity split, significantly outperforming previous methods
- Dynamic decision-making guided by transition graph outperforms fixed sequential tool execution
- Ablation studies show each tool component contributes meaningfully to overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic decision-making guided by a transition graph outperforms fixed sequential tool execution
- Mechanism: The planner uses a transition graph constructed from human behavior data to restrict tool selection to relevant options at each state, enabling backtracking when outputs are uninformative
- Core assumption: Human decision-making patterns on visual information-seeking tasks can be effectively captured and generalized by a transition graph
- Evidence anchors:
  - [abstract] "We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next..."
  - [section] "The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state."
  - [corpus] Weak - The related papers focus on multi-agent systems and decision-making but don't specifically validate transition graph effectiveness
- Break condition: If human decision patterns don't generalize to new tasks, or if the transition graph becomes too complex and limits exploration

### Mechanism 2
- Claim: In-context examples from human decision-making improve LLM planner performance
- Mechanism: The planner receives contextual examples of how humans selected tools at similar states, allowing it to make more informed decisions about tool selection and query formulation
- Core assumption: LLMs can effectively learn from human decision-making examples without fine-tuning
- Evidence anchors:
  - [abstract] "Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions."
  - [section] "We utilize the data collected from this study to construct a transition graph G shown in Figure 2, which outlines all the possible actions at each given state. Additionally, we employ real-life decision-making examples E, i.e., users choose which tool at different states, to guide the planner in choosing the appropriate action at each stage of the process."
  - [corpus] Missing - No direct evidence in corpus about in-context learning effectiveness for this specific application
- Break condition: If the LLM cannot effectively generalize from limited examples, or if the examples are not representative of diverse scenarios

### Mechanism 3
- Claim: LLM-powered reasoner can identify informative vs uninformative tool outputs and guide backtracking
- Mechanism: The reasoner processes tool outputs to determine if they contain useful information, are ready as final answers, or are uninformative, triggering appropriate state transitions
- Core assumption: LLMs can reliably distinguish between informative and uninformative outputs across diverse tool types
- Evidence anchors:
  - [abstract] "an LLM-powered reasoner that analyzes and extracts key information from the tool outputs"
  - [section] "The role of the reasoner is twofold: to determine which entity is pertinent for responding to the question and to assess whether the model has obtained the necessary information to transition to the next state."
  - [corpus] Weak - Related work on multi-agent systems doesn't specifically address output evaluation for visual information-seeking
- Break condition: If the reasoner cannot reliably identify informative content, leading to infinite loops or premature termination

## Foundational Learning

- Concept: Transition systems and state machines
  - Why needed here: The transition graph is essentially a state machine where each node represents a decision state and edges represent tool actions
  - Quick check question: What are the three possible classifications the reasoner can assign to tool outputs?

- Concept: In-context learning with LLMs
  - Why needed here: The planner and reasoner use examples from human behavior as prompts without fine-tuning
  - Quick check question: How many examples are used as in-context prompts for each dataset?

- Concept: Visual information retrieval systems
  - Why needed here: Understanding how tools like object detection, image search, and web search complement each other
  - Quick check question: What are the three categories of tools integrated in the system?

## Architecture Onboarding

- Component map: Initial state (visual question) -> Planner (selects tool and query) -> Tool execution -> Reasoner (evaluates output) -> Working memory (stores outputs) -> Next state -> Repeat until final answer

- Critical path:
  1. Initial state with visual question
  2. Planner selects tool and query based on current state
  3. Tool executes and returns output
  4. Reasoner evaluates output and determines next state
  5. Repeat until final answer is produced or no progress possible

- Design tradeoffs:
  - Dynamic vs. fixed execution: Dynamic allows backtracking but requires more computation
  - Transition graph vs. open exploration: Graph restricts search space but may miss novel solutions
  - In-context vs. fine-tuning: In-context is faster but may have lower performance than fine-tuned models

- Failure signatures:
  - Infinite loops: Planner repeatedly selects same tools without progress
  - Premature termination: Reasoner incorrectly identifies answer when information is incomplete
  - Poor tool selection: Planner chooses irrelevant tools due to inadequate examples or transition graph

- First 3 experiments:
  1. Test planner with and without transition graph on a small set of questions
  2. Evaluate reasoner accuracy on classifying tool outputs (informative, uninformative, answerable)
  3. Measure performance impact of removing each tool category (PALI, Object, Search)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AVIS scale with different sizes of Large Language Models (LLMs) beyond PALM 540B?
- Basis in paper: [explicit] The paper mentions that the current framework depends on a computationally intensive LLM, namely, the PALM model, and expresses interest in investigating whether this decision-making framework can also be performed by lighter weight language models.
- Why unresolved: The paper does not provide empirical results comparing AVIS's performance with different LLM sizes.
- What evidence would resolve it: Empirical results showing AVIS's performance on the same benchmarks using different sizes of LLMs, including smaller models.

### Open Question 2
- Question: Can the dynamic decision-making framework of AVIS be effectively applied to other reasoning tasks beyond visual question answering?
- Basis in paper: [explicit] The paper mentions that currently AVIS is specifically designed for visual question answering and aims to extend the LLM-powered dynamic decision-making framework to address other reasoning tasks.
- Why unresolved: The paper does not provide any experiments or results on applying AVIS to other reasoning tasks.
- What evidence would resolve it: Successful application and results of AVIS on other reasoning tasks, demonstrating its generalizability.

### Open Question 3
- Question: How does the transition graph in AVIS influence the tool selection process, and can it be further optimized?
- Basis in paper: [explicit] The paper highlights the use of a transition graph synthesized from a user study to guide tool selection and mentions that the model guided by the transition graph and prompts does not utilize all possible combinations of tools but favors certain combinations.
- Why unresolved: The paper does not explore alternative methods for constructing or optimizing the transition graph.
- What evidence would resolve it: Comparative studies showing the impact of different transition graph constructions on AVIS's performance and efficiency.

### Open Question 4
- Question: What are the limitations of the reasoner component in AVIS when dealing with highly ambiguous or context-dependent queries?
- Basis in paper: [inferred] The paper discusses the reasoner's role in processing tool outputs and extracting relevant information but does not provide detailed analysis of its limitations in handling ambiguous queries.
- Why unresolved: The paper does not include experiments or case studies focusing on the reasoner's performance with ambiguous or context-dependent queries.
- What evidence would resolve it: Detailed case studies or experiments showing the reasoner's performance on ambiguous or context-dependent queries, including error analysis and potential improvements.

## Limitations
- Relies heavily on a computationally intensive LLM (PALM 540B), limiting practical deployment
- Performance depends on quality and representativeness of user study data for transition graph construction
- Does not systematically investigate tool interactions and dependencies beyond ablation studies

## Confidence

- High Confidence: The overall framework architecture and the claim that dynamic decision-making outperforms sequential execution are well-supported by experimental results
- Medium Confidence: The effectiveness of the transition graph in guiding tool selection, as the paper shows improved performance but doesn't deeply analyze when and why the graph might fail
- Medium Confidence: The claim that in-context examples from human behavior improve planner performance, as the paper demonstrates improved results but doesn't provide detailed analysis of example quality or sufficiency

## Next Checks

1. Conduct a controlled experiment comparing the transition graph approach against a reinforcement learning-based planner to quantify the benefit of using human decision patterns
2. Perform a systematic sensitivity analysis on the number and quality of in-context examples to determine optimal example selection strategies
3. Evaluate the system's robustness by testing on questions that require tool combinations not present in the transition graph or user study data