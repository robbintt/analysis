---
ver: rpa2
title: Interpreting Indirect Answers to Yes-No Questions in Multiple Languages
arxiv_id: '2310.13290'
source_url: https://arxiv.org/abs/2310.13290
tags:
- language
- questions
- languages
- answers
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles interpreting indirect answers to yes-no questions
  in multiple languages. It introduces a distant supervision approach to collect training
  data for five languages and creates new evaluation benchmarks in eight languages.
---

# Interpreting Indirect Answers to Yes-No Questions in Multiple Languages

## Quick Facts
- arXiv ID: 2310.13290
- Source URL: https://arxiv.org/abs/2310.13290
- Authors: Multiple
- Reference count: 30
- Key outcome: Distant supervision approach enables multilingual training for interpreting indirect answers, outperforming cross-lingual learning from English.

## Executive Summary
This paper addresses the challenge of interpreting indirect answers to yes-no questions across multiple languages. The authors introduce a distant supervision approach that uses keyword-based rules to collect training data in five languages without manual annotation. They create new evaluation benchmarks in eight languages and demonstrate that training with direct answers from the same language significantly improves interpretation of indirect answers compared to cross-lingual learning from English. Multilingual fine-tuning using data from multiple languages further enhances performance, even when no language-specific training data is available.

## Method Summary
The authors use XLM-RoBERTa to train models for interpreting indirect answers through three strategies: cross-lingual learning from English corpora only, monolingual fine-tuning with English plus language-specific distant supervision data, and multilingual fine-tuning blending English with distant supervision data from multiple languages. Distant supervision data is collected via keyword-based rules identifying yes-no questions, direct answers, and their interpretations across five languages. Evaluation benchmarks in eight languages are created by annotating 300 question-answer pairs per language with Yes/No/Middle interpretations.

## Key Results
- Monolingual fine-tuning with language-specific distant supervision data improves F1 scores over cross-lingual learning in 5 languages
- Multilingual fine-tuning provides additional benefits, with F1 improvements of 2-28% over monolingual fine-tuning
- Cross-lingual learning from English is significantly less effective, with F1 improvements of 2-53% when using distant supervision data
- The approach works well even for languages without any language-specific training data (Bangla, Nepali, Persian)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training with direct answers obtained via distant supervision improves interpretation of indirect answers in the same language.
- **Mechanism:** The model learns to map patterns of direct answers (with polar keywords) to their interpretations, which provides a foundation for recognizing similar patterns in indirect answers that lack explicit polar keywords.
- **Core assumption:** The patterns and linguistic cues in direct answers are sufficiently similar to those in indirect answers that learning from one transfers to the other.
- **Evidence anchors:**
  - [abstract]: "We also demonstrate that direct answers (i.e., with polar keywords) are useful to train models to interpret indirect answers (i.e., without polar keywords)."
  - [section 5.2]: "The results show that blended training with English and the new language is always beneficial."
  - [corpus Table 3]: Rules for identifying direct answers have high precision (0.93-0.97) across languages, indicating reliable signal.
- **Break condition:** If the linguistic patterns in direct and indirect answers diverge significantly, or if the distant supervision data is too noisy, the transfer benefit may diminish or disappear.

### Mechanism 2
- **Claim:** Multilingual fine-tuning via distant supervision with multiple languages yields better results than monolingual fine-tuning.
- **Mechanism:** Training across multiple languages exposes the model to a broader range of linguistic patterns and contextual variations, enhancing its ability to generalize and transfer knowledge when interpreting indirect answers in a target language.
- **Core assumption:** There is sufficient linguistic overlap and shared patterns across the languages used in training that enable positive transfer.
- **Evidence anchors:**
  - [abstract]: "Multilingual fine-tuning using data from multiple languages further improves results, even when no language-specific training data is available."
  - [section 5.3]: "The technique is successful. Compared to monolingual fine-tuning (Table 7), multilingual fine-tuning is always beneficial (%∆F = 2–28)."
  - [corpus Figure 1]: Language similarity analysis shows that more similar languages are more likely to be beneficial for multilingual fine-tuning, though there are exceptions.
- **Break condition:** If the languages used for multilingual fine-tuning are too dissimilar, or if the data quality is poor, the benefit may be reduced or negative transfer may occur.

### Mechanism 3
- **Claim:** Cross-lingual learning from English corpora is less effective than multilingual fine-tuning with distant supervision data.
- **Mechanism:** Cross-lingual learning relies on transferring knowledge from English to other languages, which may not capture the specific linguistic nuances and patterns needed for interpreting indirect answers in those languages. Multilingual fine-tuning with distant supervision data provides more targeted and relevant training signals.
- **Core assumption:** The English corpora, while useful, do not fully capture the linguistic diversity and contextual variations needed for interpreting indirect answers in other languages.
- **Evidence anchors:**
  - [abstract]: "Experimental results demonstrate that monolingual fine-tuning is beneficial if training data can be obtained via distant supervision for the language of interest (5 languages)."
  - [section 5.3]: "The improvements with respect to cross-lingual learning (Table 6) are even higher across all languages (%∆F en = 2–53)."
  - [corpus Table 6]: Cross-lingual learning results are generally lower than those obtained with multilingual fine-tuning.
- **Break condition:** If the English corpora were highly representative of the target languages, or if the distant supervision data was of very low quality, the advantage of multilingual fine-tuning might be reduced.

## Foundational Learning

- **Concept:** Understanding the problem of interpreting indirect answers to yes-no questions.
  - **Why needed here:** This is the core problem the paper addresses, and understanding it is crucial for appreciating the significance of the proposed methods and results.
  - **Quick check question:** Can you explain the difference between a direct answer and an indirect answer to a yes-no question, and why interpreting indirect answers is challenging?

- **Concept:** Distant supervision and its application in collecting training data.
  - **Why needed here:** The paper relies on distant supervision to collect training data in multiple languages, which is a key innovation that enables the multilingual experiments.
  - **Quick check question:** How does the distant supervision approach work in this paper, and what are the rules used to identify yes-no questions and direct answers?

- **Concept:** Multilingual fine-tuning and its benefits.
  - **Why needed here:** The paper explores multilingual fine-tuning as a strategy to improve the model's performance on interpreting indirect answers, and understanding this concept is essential for interpreting the results.
  - **Quick check question:** What is multilingual fine-tuning, and how does it differ from monolingual fine-tuning? Why might it be beneficial in this context?

## Architecture Onboarding

- **Component map:** XLM-RoBERTa/XLM-Align -> English corpora (Circa, SWDA-IA) + Distant supervision data -> Benchmarks (8 languages) -> Blending strategy -> Model evaluation

- **Critical path:**
  1. Collect yes-no questions and direct answers via distant supervision in multiple languages
  2. Create evaluation benchmarks in eight languages
  3. Train models using cross-lingual learning (English corpora only)
  4. Fine-tune models using monolingual fine-tuning (English + language-specific distant supervision data)
  5. Fine-tune models using multilingual fine-tuning (English + multiple languages' distant supervision data)
  6. Evaluate models on benchmarks and compare results

- **Design tradeoffs:**
  - Distant supervision vs. manual annotation: Distant supervision is faster and cheaper but noisier
  - Cross-lingual vs. multilingual fine-tuning: Cross-lingual is simpler but may be less effective; multilingual is more complex but potentially more powerful
  - Number of languages in multilingual fine-tuning: More languages may provide more diverse training signals but also increase complexity and risk of negative transfer

- **Failure signatures:**
  - Low precision or recall on benchmarks: Indicates the model is not learning effectively to interpret indirect answers
  - High variance in results across languages: Suggests the model may be overfitting to specific languages or domains
  - No improvement with multilingual fine-tuning: Indicates the languages used for fine-tuning may be too dissimilar or the data quality may be poor

- **First 3 experiments:**
  1. Train a model using cross-lingual learning (English corpora only) and evaluate on the benchmarks to establish a baseline
  2. Fine-tune the model using monolingual fine-tuning (English + language-specific distant supervision data) and evaluate on the benchmarks to assess the benefit of language-specific training
  3. Fine-tune the model using multilingual fine-tuning (English + multiple languages' distant supervision data) and evaluate on the benchmarks to assess the benefit of multilingual training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the distant supervision approach be improved to obtain higher quality training data for languages with limited unannotated corpora?
- **Basis in paper:** Explicit - The paper discusses using distant supervision to collect training data for five languages and mentions the challenge of finding large unannotated corpora for three languages (Bangla, Nepali, and Persian).
- **Why unresolved:** The paper only uses a rule-based approach for distant supervision and doesn't explore more advanced techniques that could potentially yield higher quality data or work with smaller corpora.
- **What evidence would resolve it:** Experiments comparing different distant supervision techniques (e.g., semi-supervised learning, active learning) on languages with limited data, measuring improvements in model performance.

### Open Question 2
- **Question:** Can the multilingual fine-tuning approach be extended to zero-shot scenarios where no training data is available for the target language?
- **Basis in paper:** Inferred - The paper shows multilingual fine-tuning improves results even without language-specific training data, but doesn't explore zero-shot scenarios.
- **Why unresolved:** The experiments always use at least English training data, so it's unclear if the approach would work for truly unseen languages.
- **What evidence would resolve it:** Experiments fine-tuning only with languages unrelated to the target language (e.g., fine-tuning for Hindi using only Chinese and Turkish data) and measuring performance.

### Open Question 3
- **Question:** How do different transformer architectures compare for this task, particularly larger models or those with stronger cross-lingual capabilities?
- **Basis in paper:** Explicit - The paper uses XLM-RoBERTa and XLM-Align but acknowledges there are other models like InfoXLM and XLM-E.
- **Why unresolved:** The paper only experiments with two models due to resource constraints and doesn't explore the full range of available multilingual transformers.
- **What evidence would resolve it:** Systematic comparison of multiple transformer architectures (including larger models) on the benchmark datasets, measuring performance and resource efficiency trade-offs.

## Limitations

- The distant supervision approach relies heavily on the precision of keyword-based rules, which may not capture all linguistic variations across languages and domains
- The effectiveness of multilingual fine-tuning depends on language similarity, but the analysis shows exceptions where dissimilar languages still benefit, suggesting the similarity metric may not fully capture relevant features
- The paper only tests two transformer architectures due to resource constraints, leaving open questions about whether other models might perform better for this task

## Confidence

- **High confidence:** The improvement from monolingual fine-tuning over cross-lingual learning - supported by consistent results across 5 languages with specific F1 improvements reported
- **Medium confidence:** The benefit of multilingual fine-tuning - while results show consistent improvements, the magnitude varies significantly (2-28% F1) and depends on language combinations
- **Medium confidence:** The claim that cross-lingual learning is less effective - supported by comparative results but could benefit from more extensive ablations and error analysis

## Next Checks

1. Test the sensitivity of results to keyword list variations by systematically removing or modifying keywords in the distant supervision rules and measuring performance degradation
2. Conduct controlled experiments with language pairs of varying similarity (using lang2vec vectors) to quantify the relationship between language similarity and multilingual fine-tuning benefits
3. Perform ablation studies on the blending factor α to determine optimal mixing ratios between English and language-specific data for monolingual fine-tuning