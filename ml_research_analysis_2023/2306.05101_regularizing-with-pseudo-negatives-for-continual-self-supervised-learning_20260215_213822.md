---
ver: rpa2
title: Regularizing with Pseudo-Negatives for Continual Self-Supervised Learning
arxiv_id: '2306.05101'
source_url: https://arxiv.org/abs/2306.05101
tags:
- learning
- sy-con
- loss
- cassle
- cssl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Sy-CON, a symmetric contrastive loss for continual
  self-supervised learning (CSSL) that addresses the challenge of balancing plasticity
  (learning new representations) and stability (retaining past knowledge). The method
  introduces two contrastive loss terms - one for plasticity and one for stability
  - that symmetrically depend on both current and previous model embeddings.
---

# Regularizing with Pseudo-Negatives for Continual Self-Supervised Learning

## Quick Facts
- arXiv ID: 2306.05101
- Source URL: https://arxiv.org/abs/2306.05101
- Reference count: 40
- Key outcome: Sy-CON achieves state-of-the-art performance across various CSSL scenarios with top-1 accuracy of 67.74% on CIFAR-100 (5 tasks), 70.32% on ImageNet-100 (5 tasks), and 67.46% on ImageNet-100 (10 tasks) in respective Class-IL scenarios.

## Executive Summary
This paper introduces Sy-CON, a symmetric contrastive loss for continual self-supervised learning that addresses the fundamental challenge of balancing plasticity (learning new representations) and stability (retaining past knowledge). The method innovatively employs pseudo-negatives obtained from both current and previous model embeddings in a symmetric fashion across both plasticity and stability loss terms. This design allows the model to learn distinctive new representations while maintaining consistency with past knowledge, naturally balancing the trade-off without explicit hyperparameter tuning. Experiments demonstrate that MoCo-based implementation of Sy-CON achieves state-of-the-art performance across various CSSL scenarios (Class-IL, Data-IL, Domain-IL) on CIFAR-100, ImageNet-100, and DomainNet datasets.

## Method Summary
Sy-CON is a symmetric contrastive loss framework for continual self-supervised learning that addresses the plasticity-stability trade-off. The method modifies the InfoNCE loss by introducing two symmetric loss terms: one for plasticity (learning new representations) and one for stability (retaining past knowledge). Both terms depend on embeddings from both current and previous models, using pseudo-negatives obtained through model-based augmentation. For contrastive methods like MoCo, this involves maintaining two queues (current and previous model embeddings) and modifying the InfoNCE loss to include negative samples from both sources. For non-contrastive methods, pseudo-negatives are defined as outputs from the previous model for differently augmented versions of the anchor sample. The framework naturally balances learning and retention without explicit hyperparameter tuning by aligning gradients through its symmetric structure.

## Key Results
- Sy-CON achieves top-1 accuracy of 67.74% on CIFAR-100 (5 tasks) in Class-IL scenario
- Sy-CON achieves top-1 accuracy of 70.32% on ImageNet-100 (5 tasks) in Class-IL scenario
- Sy-CON achieves top-1 accuracy of 67.46% on ImageNet-100 (10 tasks) in Class-IL scenario

## Why This Works (Mechanism)

### Mechanism 1
The symmetric dependence of both plasticity and stability losses on current and past model embeddings allows natural calibration of the trade-off between learning new representations and retaining old knowledge. By including negative samples from both current and previous models in both loss terms, the gradients for plasticity and stability are aligned to naturally balance each other without explicit hyperparameter tuning.

### Mechanism 2
Including negative samples from the previous model in the plasticity loss term mitigates the issue of decreasing diversity of negative samples in continual learning. By adding negative sample embeddings from the previous model to the set of negative samples, the model is compelled to learn more distinctive representations that do not overlap with what were learned from the previous model.

### Mechanism 3
The modification of the stability loss to include negative samples from the current model puts additional constraints in distillation, ensuring that representations from the past model are maintained in a way not to contradict with the representations of the current model.

## Foundational Learning

- **Concept**: Contrastive learning
  - Why needed here: Sy-CON is based on InfoNCE and relies on contrastive learning principles to learn representations.
  - Quick check question: Can you explain how contrastive learning works and why it's effective for representation learning?

- **Concept**: Continual learning
  - Why needed here: Sy-CON is designed for continual self-supervised learning, which involves learning from a sequence of unsupervised data while retaining knowledge from previous tasks.
  - Quick check question: What are the main challenges in continual learning, and how does Sy-CON address them?

- **Concept**: Negative sampling
  - Why needed here: Sy-CON relies on negative samples to learn discriminative representations, and the diversity of negative samples is crucial for its effectiveness.
  - Quick check question: Why is the diversity of negative samples important in contrastive learning, and how does Sy-CON ensure sufficient diversity?

## Architecture Onboarding

- **Component map**: Input data -> Encoder -> Projection layer -> Prediction layer -> Loss functions (Symmetric contrastive losses for plasticity and stability) -> Queues (for storing embeddings from previous models)
- **Critical path**: 1. Encode input data using current and previous models 2. Generate positive and negative samples 3. Compute symmetric contrastive losses for plasticity and stability 4. Update encoder parameters based on combined loss
- **Design tradeoffs**: Symmetry vs. asymmetry (Sy-CON uses symmetric losses while some methods use asymmetric losses); Complexity vs. simplicity (Sy-CON introduces additional components like prediction layer and queues)
- **Failure signatures**: Degradation in performance over tasks (could indicate forgetting or inability to learn new representations); High variance in performance across tasks (could suggest instability); Poor performance on downstream tasks (could indicate representations are not discriminative enough)
- **First 3 experiments**: 1. Verify that Sy-CON improves over baseline methods on CIFAR-100 with 5 tasks (Class-IL scenario) 2. Test the impact of queue size on performance 3. Evaluate the effect of prediction layer dimension on performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Sy-CON scale with increasing task sequences beyond 10 tasks? The paper notes that Sy-CON "consistently performs better in challenging settings where CSSL needs to be performed for longer task sequences, such as 10T" and mentions that hyperparameter tuning could lead to additional performance improvements. Experiments evaluating Sy-CON on task sequences of 15, 20, or 50 tasks would reveal its scalability limits.

### Open Question 2
How would Sy-CON perform on larger-scale datasets like ImageNet-1K compared to smaller benchmarks? The paper explicitly states this as a limitation: "it is necessary to apply our approach to large-scale datasets such as ImageNet-1k. We have set these limitations as future work." Direct experiments applying Sy-CON to ImageNet-1K across various continual learning scenarios would reveal whether the method maintains its effectiveness at true large scale.

### Open Question 3
What is the optimal hyperparameter configuration for Sy-CON's projection and prediction layer dimensions? The ablation study shows performance improvements with larger MLP dimensions (4096 vs 2048), and the paper notes that "detailed hyperparameter tuning for Sy-CON can lead to additional performance improvements." A comprehensive hyperparameter search across multiple datasets and scenarios would identify optimal dimensions for the projection and prediction layers.

## Limitations
- The paper only tests on ImageNet-100 rather than the full ImageNet-1K dataset, leaving scalability to larger datasets unknown
- The theoretical justification for why symmetric pseudo-negatives specifically address the decreasing diversity problem in continual learning is limited
- The paper does not explore how Sy-CON performs on task sequences longer than 10 tasks

## Confidence

**High Confidence**: The empirical results demonstrating improved performance over baselines in multiple CSSL scenarios on standard benchmarks. The implementation details are sufficiently specified for reproduction.

**Medium Confidence**: The mechanism claims about how symmetric pseudo-negatives naturally balance plasticity and stability. While the results support this claim, the causal relationship between the symmetric design and performance improvements is not rigorously established.

**Low Confidence**: The theoretical underpinnings of why pseudo-negatives from previous models specifically address the decreasing diversity problem in continual learning. The paper asserts this benefit but doesn't provide strong theoretical or empirical evidence for this specific mechanism.

## Next Checks

1. **Ablation Study**: Remove the symmetric component and test whether the performance degrades significantly, isolating the contribution of symmetry versus the pseudo-negative concept itself.

2. **Negative Sample Diversity Analysis**: Quantitatively measure the diversity of negative samples in the queue over training time and correlate this with representation quality metrics to verify the claimed relationship.

3. **Transferability Test**: Apply Sy-CON to non-contrastive methods beyond MoCo (such as BYOL or SimSiam) to validate whether the pseudo-negative concept generalizes across different self-supervised learning paradigms.