---
ver: rpa2
title: Understanding In-Context Learning in Transformers and LLMs by Learning to Learn
  Discrete Functions
arxiv_id: '2310.03016'
source_url: https://arxiv.org/abs/2310.03016
tags:
- learning
- in-context
- examples
- functions
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores in-context learning (ICL) in Transformers and
  Large Language Models (LLMs) by investigating their ability to learn discrete Boolean
  functions. The study focuses on understanding the limitations of Transformers, the
  necessity of attention mechanisms, the impact of informative examples, and the potential
  of LLMs to implement learning algorithms.
---

# Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions

## Quick Facts
- arXiv ID: 2310.03016
- Source URL: https://arxiv.org/abs/2310.03016
- Reference count: 40
- Primary result: Transformers can learn to implement gradient-based learning algorithms for Boolean functions, but struggle with parity functions; attention-free architectures perform nearly as well; pretrained LLMs show competitive performance on prediction tasks.

## Executive Summary
This paper investigates in-context learning (ICL) in Transformers and Large Language Models (LLMs) by training them on discrete Boolean functions. The study reveals that Transformers can implicitly compute gradient-based updates during forward passes to implement learning algorithms, achieving strong performance on conjunctions, disjunctions, and DNF/CNF formulas. However, their performance degrades on parity functions due to the complexity of implementing non-gradient-based algorithms like Gaussian elimination in parallel. The research also shows that attention-free architectures like Hyena and recurrent models can achieve nearly identical performance, suggesting attention is not essential for ICL. Finally, pretrained LLMs like GPT-4 and LLaMA-2 demonstrate competitive performance on prediction tasks, indicating their ability to implement learning algorithms learned during pretraining.

## Method Summary
The study trains various models, including Transformers, attention-free architectures (Hyena, recurrent models), and pretrained LLMs, on in-context learning tasks involving Boolean functions. Models are evaluated on their ability to predict the label of a new input based on a sequence of in-context examples. The paper explores teaching sequences—highly informative examples that uniquely identify functions—to improve sample efficiency. Performance is measured by accuracy on predicting the label for the last input in a prompt sequence, averaged over multiple sequences. The research compares architectures across different Boolean function classes and analyzes their ability to implement learning algorithms implicitly.

## Key Results
- Transformers can learn gradient-based learning algorithms for conjunctions, disjunctions, and DNF/CNF formulas, but struggle with parity functions due to their complexity.
- Attention-free architectures (Hyena, recurrent models) perform nearly as well as Transformers on most tasks, indicating attention is not essential for ICL.
- Pretrained LLMs like GPT-4 and LLaMA-2 can compete with nearest-neighbor baselines on prediction tasks, demonstrating their ability to implement learning algorithms learned during pretraining.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can implement learning algorithms by implicitly computing gradient-based updates during forward passes.
- Mechanism: The model's attention and feed-forward layers approximate gradient descent steps on the in-context examples without explicit parameter updates.
- Core assumption: The transformer architecture is sufficiently expressive to represent iterative optimization procedures in its forward computation.
- Evidence anchors:
  - [abstract] "demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions"
  - [section 2] "the models are trained to learn learning algorithms"
  - [corpus] "Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context"

### Mechanism 2
- Claim: Transformers can dynamically select between multiple learned algorithms based on the characteristics of the in-context examples.
- Mechanism: The model learns distinct parameter configurations for different algorithmic approaches and conditions its behavior on the input pattern.
- Core assumption: The transformer can distinguish between different types of input distributions and activate corresponding learned algorithms.
- Evidence anchors:
  - [abstract] "Transformers can learn to implement two distinct algorithms to solve a single task"
  - [section 4] "Transformers can learn two distinct algorithms to learn Conjunctions depending on the data distribution"
  - [corpus] "Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent"

### Mechanism 3
- Claim: Attention mechanisms are not essential for in-context learning, as attention-free architectures can achieve similar performance.
- Mechanism: Alternative architectures like Hyena and recurrent models can implement similar learning algorithms through different computational primitives (convolutions, recurrence).
- Core assumption: The core learning algorithm can be implemented through various architectural choices beyond attention.
- Evidence anchors:
  - [abstract] "certain attention-free models perform (almost) identically to Transformers"
  - [section 3.1] "attention-free architectures perform almost as well as Transformers"
  - [corpus] "Softmax $\\geq$ Linear: Transformers may learn to classify in-context by kernel gradient descent"

## Foundational Learning

- Concept: Meta-learning framework where models learn to learn
  - Why needed here: The setup requires models to generalize across different learning tasks rather than memorizing specific functions
  - Quick check question: If a model memorizes a single function, can it generalize to new functions from the same class?

- Concept: Teaching sequences and their role in sample efficiency
  - Why needed here: Teaching sequences provide maximally informative examples that can uniquely identify functions, enabling more efficient learning
  - Quick check question: Given a teaching sequence for a conjunction, can the model identify the exact function without additional examples?

- Concept: Statistical learning theory and VC dimension
  - Why needed here: Understanding the theoretical limits of what function classes can be learned from limited examples
  - Quick check question: Why can conjunctions with VC dimension O(n) be learned with O(n) examples while parities require exponentially more?

## Architecture Onboarding

- Component map: Input embedding -> Position encoding -> Multi-head attention -> Feed-forward network -> Layer normalization -> Output projection
- Critical path: Token embedding -> Position encoding -> Multi-head attention -> Feed-forward network -> Layer normalization -> Output projection
- Design tradeoffs: Depth vs width for expressivity vs overfitting, attention vs recurrence for computational efficiency
- Failure signatures: Degraded performance on parity functions, inability to learn from teaching sequences, sensitivity to input distribution changes
- First 3 experiments:
  1. Implement basic transformer with fixed random weights, test on conjunction learning to verify architecture setup
  2. Train on conjunction learning with varying depths, measure sample complexity and convergence behavior
  3. Test attention-free variants (Hyena, LSTM) on same tasks to compare performance characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limitations behind the difficulty of learning Parities in the in-context learning setting?
- Basis in paper: [explicit] The paper discusses the failure of Transformers and other architectures on the class of Parities and mentions that Parities require 2^Ω(n) queries and Sparse Parities require n^Ω(k) queries (where k is the number of relevant bits) in the Statistical Query framework.
- Why unresolved: The paper states that the exact reason why Transformers fail to learn anything about Parities is unclear and left as an open problem.
- What evidence would resolve it: A rigorous theoretical analysis explaining why in-context learning is particularly challenging for Parity functions, potentially involving the statistical query complexity or the limitations of gradient-based learning methods.

### Open Question 2
- Question: What are the mechanisms that various architectures (attention, recurrence, and convolution) use to learn tasks using different operations?
- Basis in paper: [explicit] The paper mentions that while recently proposed architectures with sub-quadratic inference time can perform in-context learning, there still exists a minor performance gap between these architectures and Transformers. It also discusses the discovery of attention heads in GPT-2 that closely implement the nearest neighbours algorithm.
- Why unresolved: The paper does not provide a detailed analysis of the specific mechanisms used by different architectures to implement learning algorithms in-context.
- What evidence would resolve it: A comprehensive study analyzing the internal representations and computations of various architectures (attention, recurrence, convolution) when performing in-context learning, potentially using techniques like mechanistic interpretability or probing.

### Open Question 3
- Question: What aspects of the pretraining process of LLMs lead to their ability to implement learning algorithms since they are not primarily trained in the meta-learning-like framework?
- Basis in paper: [explicit] The paper discusses the performance of pretrained LLMs like GPT-4 and LLaMA-2 on in-context learning tasks and mentions that the mechanisms to solve tasks must have been learned during pretraining on language data.
- Why unresolved: The paper does not provide a detailed explanation of how the pretraining process of LLMs leads to their ability to implement learning algorithms in-context.
- What evidence would resolve it: An in-depth analysis of the pretraining objectives, data distributions, and architectural choices that contribute to the emergence of in-context learning capabilities in LLMs, potentially through controlled experiments or theoretical analysis.

## Limitations
- Transformers struggle significantly with parity functions, showing performance degradation to chance levels, and the exact reasons for this limitation are not fully explored.
- While attention-free architectures can achieve similar performance, the specific mechanisms and conditions under which they excel remain unclear.
- The study provides preliminary evidence on pretrained LLMs' ability to implement learning algorithms, but further investigation is needed to understand the underlying mechanisms and generalizability.

## Confidence
- High: Transformers can implement gradient-based learning algorithms for various classes of Boolean functions, supported by strong experimental evidence.
- Medium: Attention mechanisms are not essential for ICL, as attention-free architectures can achieve similar performance, but the reasons behind this phenomenon are not fully elucidated.
- Low: The limitations of Transformers on parity functions and the potential of pretrained LLMs to implement learning algorithms are based on preliminary evidence and require further investigation.

## Next Checks
1. Conduct additional experiments to investigate the limitations of Transformers on parity functions and explore alternative architectures or training strategies to improve performance.
2. Perform a detailed analysis of the learned algorithms and representations in attention-free models, comparing their computational primitives and optimization strategies with those of Transformers.
3. Conduct a comprehensive study on the learning algorithms implemented by pretrained LLMs, analyzing the influence of pretraining and exploring techniques for adapting pretrained models to specific tasks.