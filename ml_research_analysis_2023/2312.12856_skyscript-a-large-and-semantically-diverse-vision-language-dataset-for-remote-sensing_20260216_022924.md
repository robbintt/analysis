---
ver: rpa2
title: 'SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote
  Sensing'
arxiv_id: '2312.12856'
source_url: https://arxiv.org/abs/2312.12856
tags:
- remote
- sensing
- image
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SkyScript is a large-scale, semantically diverse vision-language
  dataset for remote sensing images, containing 2.6 million image-text pairs covering
  29K distinct semantic tags. The dataset is constructed by automatically connecting
  unlabeled remote sensing images from Google Earth Engine with rich semantic information
  from OpenStreetMap using geo-coordinates.
---

# SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing

## Quick Facts
- **arXiv ID**: 2312.12856
- **Source URL**: https://arxiv.org/abs/2312.12856
- **Reference count**: 25
- **Primary result**: SkyScript contains 2.6 million image-text pairs with 29K distinct semantic tags, enabling a remote sensing-specialized CLIP model (SkyCLIP) with 6.2% average accuracy gain in zero-shot scene classification.

## Executive Summary
SkyScript is a large-scale vision-language dataset for remote sensing images, containing 2.6 million image-text pairs covering 29K distinct semantic tags. The dataset is constructed by automatically connecting unlabeled remote sensing images from Google Earth Engine with rich semantic information from OpenStreetMap using geo-coordinates. This approach enables the creation of a dataset that is over two orders of magnitude more semantically diverse than existing remote sensing image-text datasets. SkyScript enables the development of a remote sensing-specialized CLIP model (SkyCLIP) through continual pre-training, which outperforms baseline models with a 6.2% average accuracy gain in zero-shot scene classification across seven benchmark datasets.

## Method Summary
The SkyScript dataset is constructed by linking unlabeled remote sensing images from Google Earth Engine with semantic tags from OpenStreetMap using geographic coordinates. A two-stage tag classification approach filters irrelevant tags and matches image resolution to tag visibility using logistic regression models with CLIP embeddings. Image-text pairs are assembled from OSM tags and filtered by cosine similarity using a pre-trained CLIP model. The filtered dataset is used for continual pre-training of a remote sensing-specialized CLIP model, which is evaluated on zero-shot scene classification, fine-grained attribute classification, and cross-modal retrieval tasks.

## Key Results
- SkyScript contains 2.6 million image-text pairs with 29K distinct semantic tags, achieving over two orders of magnitude more semantic diversity than existing remote sensing VLM datasets.
- SkyCLIP, trained on SkyScript, achieves a 6.2% average accuracy gain in zero-shot scene classification across seven benchmark datasets compared to baseline models.
- SkyCLIP demonstrates strong performance in zero-shot fine-grained attribute classification and cross-modal retrieval tasks, outperforming CLIP trained on general web data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Geo-coordinates enable automated pairing of unlabeled remote sensing images with rich semantic tags.
- Mechanism: The approach uses geographic coordinates to link Google Earth Engine (GEE) images with OpenStreetMap (OSM) tags, avoiding manual annotation by leveraging existing geospatial metadata.
- Core assumption: OSM tags are semantically relevant and visually grounded in remote sensing images.
- Evidence anchors:
  - [abstract] "using geo-coordinates to automatically connect open, unlabeled remote sensing images with rich semantic information from OpenStreetMap"
  - [section] "Data collection approach: We construct the SkyScript dataset from the wild by linking large-scale yet unlabeled remote sensing image data with geo-tagged semantic information from OSM"
  - [corpus] Weak—most neighbor papers use synthetic or filtered data rather than direct geo-coordinate linking; no explicit comparison found.
- Break Condition: If OSM tags are not visually descriptive of image content, the pairing fails.

### Mechanism 2
- Claim: Two-stage tag classification filters irrelevant tags and matches image resolution to tag visibility.
- Mechanism: First, logistic regression determines if a tag can be visually grounded at all. Second, another model predicts the maximum GSD at which the tag is visible.
- Core assumption: CLIP embeddings encode sufficient visual information about tags to predict their groundability.
- Evidence anchors:
  - [section] "We develop a two-stage tag classification approach... The first stage is a binary logistic regression model that predicts whether a tag can be visually grounded... If a tag is predicted to be visually groundable, a second logistic regression model is further used to predict the maximum GSD"
  - [abstract] "We develop a two-stage tag classification approach with CLIP embeddings of tags as inputs"
  - [corpus] No direct evidence—similar filtering is mentioned in neighbor papers but not with this two-stage, CLIP-embedding method.
- Break Condition: If CLIP embeddings fail to capture visual relevance of tags, both classification stages fail.

### Mechanism 3
- Claim: Continual pre-training on SkyScript yields better zero-shot transfer than models trained on general web data.
- Mechanism: Training a CLIP model on SkyScript refines both image and text encoders to remote sensing domain specifics, improving downstream zero-shot performance.
- Core assumption: The semantic diversity and scale of SkyScript outweighs domain shift from general pre-training.
- Evidence anchors:
  - [abstract] "With continual pre-training on this dataset, we obtain a VLM that surpasses baseline models with a 6.2% average accuracy gain in zero-shot scene classification"
  - [section] "SkyScript enables the development of a remote sensing-specialized CLIP model (SkyCLIP) through continual pre-training, which outperforms baseline models with a 6.2% average accuracy gain"
  - [corpus] Weak—neighbor papers mention domain-specialized VLMs but do not compare against models trained on SkyScript.
- Break Condition: If SkyScript does not contain enough domain-specific diversity, continual pre-training yields minimal gains.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and contrastive learning
  - Why needed here: The entire approach relies on aligning image and text embeddings via contrastive learning to enable zero-shot transfer.
  - Quick check question: What is the core training objective used to align image and text embeddings in CLIP?

- Concept: Geo-spatial data linking
  - Why needed here: The dataset construction depends on linking images and tags by their geographic coordinates.
  - Quick check question: Why is geographic coordinate linking preferable to text-based or visual similarity matching in this context?

- Concept: Ground Sampling Distance (GSD) and image resolution
  - Why needed here: Tag classification and image selection depend on GSD to ensure that visual features match image resolution.
  - Quick check question: How does GSD affect the ability to visually ground OSM tags in remote sensing images?

## Architecture Onboarding

- Component map:
  - Google Earth Engine (GEE) -> Remote sensing image source
  - OpenStreetMap (OSM) -> Semantic tag source
  - Two-stage tag classifier (logistic regression on CLIP embeddings) -> Tag filtering and GSD matching
  - Image-text pair filtering (cosine similarity) -> Dataset quality control
  - CLIP model (ViT backbone) -> Vision-language representation learning
  - Continual pre-training pipeline -> Domain adaptation
  - Evaluation benchmarks (scene classification, fine-grained classification, cross-modal retrieval) -> Performance validation

- Critical path:
  1. Query OSM for objects in random and targeted geographic grids
  2. Classify tags for visual groundability and maximum GSD
  3. Match suitable GEE image collections to objects
  4. Assemble single- and multi-object captions from tags
  5. Filter image-text pairs by cosine similarity
  6. Pre-train CLIP model on SkyScript
  7. Evaluate zero-shot performance on benchmark datasets

- Design tradeoffs:
  - Dataset size vs. quality: Including lower-similarity pairs increases dataset size but can reduce performance (Section A.8.1).
  - Semantic diversity vs. geographic representativeness: High-resolution images are concentrated in the US/Europe, biasing coverage.
  - Caption naturalness vs. compositional accuracy: Rule-based captions are simpler but less natural than LLM-generated ones.

- Failure signatures:
  - Low classification accuracy on unseen datasets suggests domain shift not fully addressed.
  - Poor cross-modal retrieval recall indicates misalignment between image and text embeddings.
  - Overfitting during continual pre-training shows insufficient dataset diversity.

- First 3 experiments:
  1. Run tag classification pipeline on a small sample of OSM tags and manually verify visual groundability predictions.
  2. Test image-text filtering thresholds (20%, 30%, 50%) and measure downstream classification accuracy.
  3. Compare zero-shot classification performance of CLIP models trained on SkyScript vs. CLIP-laion-RS vs. original CLIP.

## Open Questions the Paper Calls Out

- **Open Question 1**: How would using large language models (LLMs) to generate captions from tags instead of rule-based assembly impact the performance of vision-language models trained on SkyScript?
  - Basis in paper: [explicit] The paper states that "Using large language models (LLMs) to generate more natural and meaningful captions from tags warrants future exploration."
  - Why unresolved: The current caption generation method uses a simple rule-based approach, which may not produce the most natural or meaningful captions.
  - What evidence would resolve it: Comparing the performance of VLMs trained on captions generated by LLMs versus rule-based captions on downstream tasks like scene classification and cross-modal retrieval.

- **Open Question 2**: What is the impact of using multispectral bands in addition to RGB for remote sensing images in SkyScript on the performance of vision-language models?
  - Basis in paper: [explicit] The paper mentions that "For each image collection, we only consider RGB bands even if multispectral images are present. The inclusion of additional bands is left for future research."
  - Why unresolved: The current dataset only uses RGB bands, and the potential benefits of including multispectral bands are not explored.
  - What evidence would resolve it: Comparing the performance of VLMs trained on SkyScript with and without multispectral bands on downstream tasks.

- **Open Question 3**: How does the performance of SkyCLIP vary when using different fractions of high-similarity image-text pairs for training?
  - Basis in paper: [explicit] The paper discusses the trade-off between dataset size and data quality, showing that using the top 20% of high-similarity pairs yields better performance than using the full dataset.
  - Why unresolved: The optimal fraction of high-similarity pairs for training is not determined, and the impact of different fractions on performance is not fully explored.
  - What evidence would resolve it: Conducting experiments with different fractions of high-similarity pairs (e.g., top 10%, 30%, 40%) and comparing their performance on downstream tasks.

## Limitations

- The geographic coverage of SkyScript is biased toward the United States and Europe due to the availability of high-resolution images in these regions, limiting its representativeness across continents.
- The visual grounding accuracy of OSM tags may vary across different geographic regions, particularly in areas outside the US/Europe where OSM tag density and visual consistency may differ.
- The claim of "two orders of magnitude more semantic diversity" is based on tag counts but lacks a formal quantitative measure of semantic diversity (e.g., tag entropy, topic coverage).

## Confidence

- **High**: Dataset construction pipeline, continual pre-training setup, zero-shot classification accuracy gains over baselines.
- **Medium**: Semantic diversity claims (based on tag counts but lacking formal diversity metrics), caption naturalness vs. LLM generation comparison.
- **Low**: Geographic representativeness across continents, visual grounding accuracy of tags in non-Western regions, long-term stability of the pairing pipeline as OSM/GEE evolve.

## Next Checks

1. **Geographic robustness test**: Apply the tag classification pipeline to OSM data from Asia/Africa/South America and compare tag grounding accuracy to the US/Europe baseline.
2. **Semantic diversity quantification**: Compute formal diversity metrics (e.g., Shannon entropy of tag distributions, topic coverage) and compare against existing remote sensing VLM datasets.
3. **Longitudinal stability audit**: Re-run the data collection pipeline after 6-12 months to measure changes in image-text pair yield and semantic tag consistency.