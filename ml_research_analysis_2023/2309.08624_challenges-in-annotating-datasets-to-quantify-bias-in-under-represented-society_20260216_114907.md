---
ver: rpa2
title: Challenges in Annotating Datasets to Quantify Bias in Under-represented Society
arxiv_id: '2309.08624'
source_url: https://arxiv.org/abs/2309.08624
tags:
- bias
- research
- aori
- language
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research focuses on developing benchmark datasets to quantify
  bias in under-represented societies, specifically the New Zealand population. It
  addresses the challenge of limited resources and understanding of bias in such communities.
---

# Challenges in Annotating Datasets to Quantify Bias in Under-represented Society

## Quick Facts
- arXiv ID: 2309.08624
- Source URL: https://arxiv.org/abs/2309.08624
- Authors: 
- Reference count: 40
- Primary result: Developed benchmark dataset for quantifying bias in NZ population; achieved only 35% annotator agreement highlighting complexity of bias definition

## Executive Summary
This research addresses the critical challenge of quantifying bias in under-represented societies by developing benchmark datasets for the New Zealand population. The study employs a manual annotation process using structured prompts and text generation models to create a dataset examining bias across Māori, Pakeha, and Pacific demographics. Despite systematic efforts, annotators achieved only 35% agreement on generated text, revealing the inherent complexity of defining and measuring bias in under-represented communities. The findings underscore the need for more robust evaluation metrics and standardized bias definitions, while recommending improvements in annotator selection, training, and development of non-subjective evaluation methods.

## Method Summary
The research uses structured prompt templates with demographic placeholders to systematically explore bias across New Zealand's under-represented groups. Text is generated using GPT-2 (large) with greedy search, producing 285 samples based on New Zealand demographics. Three independent annotators with backgrounds in NZ demographics manually label the generated text for positive, negative, or neutral regard, while also identifying social stereotypes and job associations. The study employs the 'regard' metric to measure differences in social perceptions across ethical and racial orientation demographics, combining manual annotation with sentiment analysis and text generation approaches.

## Key Results
- Achieved only 35% agreement between annotators on bias labels, highlighting the subjective nature of bias definition
- Demonstrated the effectiveness of structured prompt templates with demographic placeholders for systematic bias exploration
- Revealed the limitations of current bias metrics and the need for more robust, standardized evaluation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using structured prompt templates with demographic placeholders enables systematic bias exploration across multiple under-represented groups.
- Mechanism: By slotting specific demographic targets into pre-defined sentence templates, the research can generate comparable text samples for different ethnic groups in NZ, allowing direct comparison of bias manifestations.
- Core assumption: The templates capture relevant bias contexts (e.g., "respect," "occupation") that reflect real-world stereotypes and social perceptions.
- Evidence anchors:
  - [abstract] "Motivated by the lack of annotated datasets for quantifying bias in under-represented societies, we endeavour to create benchmark datasets for the New Zealand (NZ) population."
  - [section] "This research uses a pre-defined template [39, 34] combined with NZ demographic targets to prompt LLMs."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.448, average citations=0.0. Top related titles: Metrics for popularity bias in dynamic recommender systems, Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies.
- Break condition: If the templates don't capture the actual bias contexts relevant to the under-represented group, or if the LLM cannot generate meaningful text for these contexts.

### Mechanism 2
- Claim: Manual annotation with multiple independent coders reveals the subjective nature of bias perception.
- Mechanism: Three annotators independently label generated text as positive, negative, or neutral regard, and only 35% agreement is achieved, demonstrating the complexity of defining bias.
- Core assumption: Annotators have sufficient understanding of NZ demographics and bias to make informed judgments.
- Evidence anchors:
  - [abstract] "Despite efforts, annotators only agreed on 35% of the generated text, highlighting the complexity of defining and quantifying bias."
  - [section] "We used three independent annotators with an understanding and background of NZ demographics to label the generated data as positive, negative or neutral regard."
  - [corpus] Weak evidence - the corpus doesn't directly address manual annotation methodology.
- Break condition: If annotators' backgrounds are too similar, or if they lack sufficient training on bias definitions and examples.

### Mechanism 3
- Claim: Using 'regard' as a bias metric captures social perceptions and language polarity towards demographics, beyond simple sentiment analysis.
- Mechanism: 'Regard' measures differences in how language portrays different demographic groups, distinguishing between positive/negative social perceptions and overall language polarity.
- Core assumption: The 'regard' metric accurately reflects real-world social biases and can be consistently applied across different contexts.
- Evidence anchors:
  - [abstract] "We use 'regard' [39] to measure the differences across ethical and racial orientation demographics [45]."
  - [section] "Sentiment scores are designed to capture differences in language polarity. NLI-based measures are designed to quantify biases in word representations where the average probability for the neutral class and the fraction predicted as neutral are computed."
  - [corpus] Weak evidence - the corpus doesn't provide details on the 'regard' metric specifically.
- Break condition: If the 'regard' metric is too subjective or fails to capture important aspects of bias beyond language polarity.

## Foundational Learning

- Concept: Understanding bias in AI systems
  - Why needed here: The research aims to quantify and address bias in LLMs towards under-represented societies, requiring a clear understanding of what constitutes bias in this context.
  - Quick check question: What is the difference between algorithmic bias and social bias, and how do they relate to each other in the context of language models?

- Concept: Manual data annotation methodologies
  - Why needed here: The research relies on manual annotation by multiple coders to label generated text, requiring knowledge of best practices in annotation and inter-annotator agreement.
  - Quick check question: What factors can influence inter-annotator agreement in bias annotation tasks, and how can they be mitigated?

- Concept: Prompt engineering for bias exploration
  - Why needed here: The research uses structured prompts to systematically explore bias across different demographics, requiring understanding of how prompt design affects generated text.
  - Quick check question: How does the choice of prompt templates and demographic placeholders influence the types of biases that can be detected in generated text?

## Architecture Onboarding

- Component map: Prompt generation (using templates and demographic targets) -> Text generation (using GPT-2 with greedy search) -> Bias evaluation (using 'regard' metric) -> Manual annotation (by three independent coders) -> Data analysis and interpretation
- Critical path: Prompt generation → Text generation → Bias evaluation → Manual annotation → Data analysis and interpretation
- Design tradeoffs: Using manual annotation provides nuanced understanding of bias but is resource-intensive and subjective; automated metrics like 'regard' are more scalable but may miss context-specific biases
- Failure signatures: Low inter-annotator agreement (>65% disagreement) suggests issues with bias definition clarity or annotator training; lack of variation in generated text across demographics may indicate insufficient prompt diversity
- First 3 experiments:
  1. Test different prompt templates with a small set of demographic targets to assess their effectiveness in eliciting bias-related text.
  2. Conduct a pilot annotation study with a subset of generated text to refine bias definitions and annotation guidelines.
  3. Compare the 'regard' metric with other bias evaluation methods (e.g., sentiment analysis, NLI-based measures) on a sample of annotated text to assess its strengths and limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more robust and standardized metrics for quantifying bias in language models that are not subjective and work across different cultural contexts?
- Basis in paper: [explicit] The paper explicitly states that current bias metrics are subjective and unreliable, and there is a need for more standardized definitions of bias.
- Why unresolved: Bias is a complex phenomenon that depends heavily on context, and developing metrics that can accurately capture bias across diverse cultures and situations is challenging. The paper's findings highlight the difficulty of even three annotators agreeing on bias labels for generated text.
- What evidence would resolve it: Development and validation of new bias metrics that show higher inter-annotator agreement across diverse cultural contexts and produce consistent results when applied to different language models and datasets.

### Open Question 2
- Question: What is the most effective approach for selecting and training annotators to minimize personal biases and improve agreement when annotating bias datasets?
- Basis in paper: [explicit] The paper discusses the challenges of annotator selection and training, noting that even three university-educated males aged 20-40 had significant variation in their annotations.
- Why unresolved: Annotator bias and interpretation of bias can significantly impact dataset quality, but there is no established best practice for annotator selection and training, especially for under-represented societies.
- What evidence would resolve it: Comparative studies of different annotator selection and training approaches, measuring their impact on inter-annotator agreement and the quality of bias annotations across diverse cultural contexts.

### Open Question 3
- Question: How can we effectively balance the need to identify and quantify bias with the ethical concerns of exposing annotators to potentially harmful content and reinforcing stereotypes?
- Basis in paper: [inferred] The paper mentions the ethical challenges of providing information on stereotypes to annotators and the risk of exposing them to harmful content.
- Why unresolved: There is a tension between the need for accurate bias detection and the potential harm caused by exposing annotators to biased or offensive content, especially when dealing with under-represented societies.
- What evidence would resolve it: Development and evaluation of annotation protocols that minimize annotator exposure to harmful content while maintaining the accuracy and reliability of bias annotations, potentially through the use of automated pre-screening or carefully designed annotation instructions.

## Limitations

- Achieved only 35% inter-annotator agreement, raising questions about the reliability and objectivity of bias annotation
- Lack of detailed methodology for annotator selection and training, making it difficult to assess the source of disagreement
- No comparative analysis of the 'regard' metric against alternative bias evaluation methods to validate its effectiveness

## Confidence

- **High Confidence**: The observation that bias annotation in under-represented societies presents significant challenges is well-supported by the 35% agreement rate and aligns with broader literature on annotation subjectivity.
- **Medium Confidence**: The claim that manual annotation reveals the complexity of defining bias is supported but limited by the lack of detailed methodology and the small sample size (285 texts).
- **Low Confidence**: The assertion that the 'regard' metric captures social perceptions and language polarity is not fully validated within the study, as no comparative analysis with alternative metrics is provided.

## Next Checks

1. Conduct a pilot study with revised annotation guidelines and training to improve inter-annotator agreement, aiming for >70% consensus on a subset of texts.
2. Perform a comparative analysis of the 'regard' metric against established bias evaluation methods (e.g., sentiment analysis, NLI-based measures) on the annotated dataset to assess its effectiveness.
3. Expand the demographic scope and text generation process to include a more diverse range of under-represented groups and contexts, then re-evaluate annotation agreement and bias manifestations.