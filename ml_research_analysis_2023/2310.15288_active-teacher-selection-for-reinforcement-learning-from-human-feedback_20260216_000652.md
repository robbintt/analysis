---
ver: rpa2
title: Active teacher selection for reinforcement learning from human feedback
arxiv_id: '2310.15288'
source_url: https://arxiv.org/abs/2310.15288
tags:
- teacher
- learning
- utility
- teachers
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Hidden Utility Bandit (HUB) framework
  to model the problem of learning from multiple human teachers in reinforcement learning
  from human feedback (RLHF). In a HUB, an agent must balance pulling bandit arms
  to earn utility with querying teachers to learn the utility function.
---

# Active teacher selection for reinforcement learning from human feedback

## Quick Facts
- arXiv ID: 2310.15288
- Source URL: https://arxiv.org/abs/2310.15288
- Reference count: 40
- Key outcome: Active Teacher Selection (ATS) algorithm outperforms baseline methods by actively selecting teachers to query, earning higher discounted cumulative reward while more accurately learning the utility function in reinforcement learning from human feedback.

## Executive Summary
This paper introduces the Hidden Utility Bandit (HUB) framework to model the problem of learning from multiple human teachers in reinforcement learning from human feedback (RLHF). In a HUB, an agent must balance pulling bandit arms to earn utility with querying teachers to learn the utility function. The authors develop the Active Teacher Selection (ATS) algorithm, which maintains a belief over the utility function and arm distributions to actively choose when and which teacher to query. Experiments on a paper recommendation task and COVID-19 vaccine testing demonstrate that ATS outperforms baseline methods by actively selecting teachers and earns higher discounted cumulative reward while more accurately learning the utility function. The HUB framework and ATS algorithm provide a principled approach to leveraging multiple teachers in RLHF systems.

## Method Summary
The method implements the HUB framework where an agent interacts with a multi-armed bandit problem with hidden utilities and multiple human teachers. ATS maintains a belief over the utility function and arm distributions, then uses POMDP planning (via POMCPOW solver) to decide when to pull arms versus query teachers. The algorithm actively selects which teacher to query based on expected information gain, balancing exploration of the utility function against exploitation of known rewards. The framework converts the multi-teacher RLHF problem into a tractable POMDP by fixing teacher policies and arm distributions.

## Key Results
- ATS outperforms baseline algorithms by actively selecting when and which teacher to query, earning higher discounted cumulative reward
- ATS achieves more accurate learning of the utility function with lower L2 loss compared to naive exploration baselines
- Lower-β teachers can be more informative than higher-β teachers when learning utility differences between items

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active teacher selection outperforms fixed exploration by querying teachers only when belief updates are decision-relevant.
- Mechanism: ATS maintains a belief over utility function and arm distributions, and uses POMDP planning to decide when to query teachers. This avoids wasting queries when the agent already knows which arm is best.
- Core assumption: Belief updates from teacher queries are only valuable if they change the optimal arm choice.
- Evidence anchors:
  - [abstract] "ATS outperforms baseline algorithms by actively selecting when and which teacher to query"
  - [section 3.1] "ATS can also actively select which teacher to query... removes the need to set problem-specific hyperparameters"
  - [corpus] No direct evidence; claim is supported by paper's experimental results
- Break condition: If teacher feedback is uniformly uninformative across all queries, the belief maintenance overhead becomes wasted computation.

### Mechanism 2
- Claim: Selecting lower-β teachers can be more informative than selecting higher-β teachers when learning utility differences.
- Mechanism: Lower-β teachers provide noisier but more frequent preference information, which helps estimate utility differences between items. This is particularly valuable when the difference in utility is large.
- Core assumption: Preference probability correlates with utility difference, so noisier teachers provide more data points for estimating that difference.
- Evidence anchors:
  - [section 2.1] "the probability that a teacher with rationality parameter β prefers item i to j is below: Pr(i ≻ j; β, U) = exp(βU(i))/(exp(βU(i)) + exp(βU(j)))"
  - [section 4.1] "it will sometimes be more informative for ATS to select teachers with lower β values"
  - [corpus] No direct evidence; claim is supported by paper's theoretical framework
- Break condition: If utility differences are small, high-β teachers provide more precise estimates and lower-β teachers add mostly noise.

### Mechanism 3
- Claim: The HUB framework converts the multi-teacher RLHF problem into a tractable POMDP by fixing teacher policies and arm distributions.
- Mechanism: By treating the utility function and arm distributions as the hidden state in a POMDP, and fixing the teacher policies and arm distributions, the HUB framework reduces the problem complexity from DEC-POMDP to POMDP, making it computationally tractable.
- Core assumption: The teacher policies and arm distributions are known and fixed, allowing them to be treated as part of the environment rather than part of the agent's decision space.
- Evidence anchors:
  - [section 3.1] "CIRL problems are DEC-POMDPS, which are NEXP-complete and thus functionally intractable... By fixing the human policy and arm distributions, the HUB framework reduces the problem to a POMDP"
  - [section 3.1] "The HUB-POMDP state contains the HUB utility function and arm distributions. Since these are fixed for a given HUB, the state does not change over time"
  - [corpus] No direct evidence; claim is supported by paper's theoretical framework
- Break condition: If teacher policies or arm distributions are unknown or non-stationary, the POMDP reduction no longer applies.

## Foundational Learning

- Concept: Multi-armed bandit problems
  - Why needed here: The HUB framework builds on the multi-armed bandit framework by adding hidden utilities and teacher feedback mechanisms.
  - Quick check question: In a standard MAB, if an agent pulls arm k and receives utility u, what does it know about the arm's distribution?

- Concept: Inverse reinforcement learning
  - Why needed here: The HUB framework is closely related to IRL, where the agent infers a reward function from observed behavior or feedback rather than being given the reward function explicitly.
  - Quick check question: How does IRL differ from standard RL in terms of what information the agent receives about the reward function?

- Concept: Partially observable Markov decision processes
  - Why needed here: The ATS algorithm solves the HUB problem by converting it to a POMDP and using POMDP solution methods like POMCPOW.
  - Quick check question: What is the key difference between a POMDP and a standard MDP in terms of what the agent can observe?

## Architecture Onboarding

- Component map: HUB framework -> ATS algorithm -> POMCPOW solver -> Rollout policies
- Critical path:
  1. Initialize belief over utility function and arm distributions
  2. At each timestep, select action (pull arm or query teacher) using POMDP planning
  3. Observe result and update belief
  4. Repeat until episode ends
- Design tradeoffs:
  - General vs specific teacher selection: General selection reduces state space complexity but loses control over which teacher provides feedback
  - Rollout policy choice: Best arm rollout vs random action rollout affects planning accuracy and computational cost
  - Belief representation: Particle filter vs other methods affects memory usage and update speed
- Failure signatures:
  - Poor performance despite correct implementation: May indicate that teacher feedback is too noisy or uninformative
  - High computational cost: May indicate that belief updates or POMDP planning are too complex for the problem size
  - Non-convergence of belief: May indicate that teacher feedback is inconsistent or that the belief update mechanism is flawed
- First 3 experiments:
  1. Implement and test ATS on a simple 2-arm, 2-teacher HUB problem with known parameters
  2. Compare ATS with naive exploration baselines on the paper recommendation domain
  3. Test ATS with different rollout policies to identify the most effective approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ATS scale with the number of teachers in the HUB framework? Is there an optimal number of teachers for maximizing the expected discounted sum of utilities?
- Basis in paper: [inferred] The paper demonstrates the performance of ATS with 3 teachers in the conference recommendation domain, but does not explore how the performance changes with a different number of teachers.
- Why unresolved: The paper does not provide experimental results for different numbers of teachers, so the scalability and optimal number of teachers remain unknown.
- What evidence would resolve it: Experiments varying the number of teachers in the HUB framework and comparing the performance of ATS with different numbers of teachers would provide insights into the scalability and optimal number of teachers.

### Open Question 2
- Question: How does the performance of ATS change with different utility function complexities in the HUB framework? Are there specific types of utility functions that are more challenging for ATS to learn?
- Basis in paper: [inferred] The paper demonstrates the performance of ATS on a conference recommendation domain with a specific utility function, but does not explore how the performance changes with different utility function complexities.
- Why unresolved: The paper does not provide experimental results for different utility function complexities, so the impact of utility function complexity on ATS performance remains unknown.
- What evidence would resolve it: Experiments varying the complexity of the utility function in the HUB framework and comparing the performance of ATS with different utility function complexities would provide insights into the impact of utility function complexity on ATS performance.

### Open Question 3
- Question: How does the performance of ATS change with different arm distribution complexities in the HUB framework? Are there specific types of arm distributions that are more challenging for ATS to learn?
- Basis in paper: [inferred] The paper demonstrates the performance of ATS on a conference recommendation domain with a specific arm distribution, but does not explore how the performance changes with different arm distribution complexities.
- Why unresolved: The paper does not provide experimental results for different arm distribution complexities, so the impact of arm distribution complexity on ATS performance remains unknown.
- What evidence would resolve it: Experiments varying the complexity of the arm distribution in the HUB framework and comparing the performance of ATS with different arm distribution complexities would provide insights into the impact of arm distribution complexity on ATS performance.

## Limitations

- The POMDP reduction relies on fixed teacher policies and arm distributions, which may not hold in real-world applications
- Performance depends on the informativeness of teacher feedback, which may be limited when utility differences are small
- The belief maintenance overhead may become prohibitive when teacher feedback is consistently uninformative

## Confidence

- High: The theoretical framework for HUB and ATS is well-specified and internally consistent
- Medium: Claims about teacher selection mechanisms are supported by experimental results but lack analytical proof
- Low: The claim that lower-β teachers can be more informative than higher-β teachers when learning utility differences has limited empirical evidence

## Next Checks

1. Test ATS on problems with small utility differences to verify whether lower-β teachers truly provide more informative feedback in these regimes.
2. Evaluate ATS performance when teacher policies or arm distributions are non-stationary to assess robustness of the POMDP reduction.
3. Compare ATS with simpler heuristics that don't maintain full belief states to quantify the overhead cost of the POMDP approach.