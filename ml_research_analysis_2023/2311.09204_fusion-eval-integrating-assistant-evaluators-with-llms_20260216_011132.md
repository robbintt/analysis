---
ver: rpa2
title: 'Fusion-Eval: Integrating Assistant Evaluators with LLMs'
arxiv_id: '2311.09204'
source_url: https://arxiv.org/abs/2311.09204
tags:
- evaluation
- evaluators
- score
- assistant
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Fusion-Eval, a system that employs Large Language
  Models (LLMs) to integrate insights from various assistant evaluators to assess
  natural language systems. The method provides an LLM with example to evaluate along
  with scores from assistant evaluators, each specializing in distinct aspects of
  responses.
---

# Fusion-Eval: Integrating Assistant Evaluators with LLMs

## Quick Facts
- arXiv ID: 2311.09204
- Source URL: https://arxiv.org/abs/2311.09204
- Reference count: 7
- Primary result: System-level Kendall-Tau correlation of 0.962 with humans on SummEval and turn-level Spearman correlation of 0.744 on TopicalChat

## Executive Summary
Fusion-Eval is a novel system that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators for assessing natural language systems. The approach uses LLMs to plan evaluation strategies and combine scores from specialized evaluators, achieving significantly higher correlation with human judgments than baseline methods. The system demonstrates strong performance across multiple evaluation criteria and shows promise for flexible, task-adaptive evaluation of natural language generation.

## Method Summary
Fusion-Eval employs a two-stage LLM-based approach to evaluation. First, a planning LLM generates a natural language strategy for using specialized assistant evaluators (such as NLI, BLEURT, and SumBleurt) to assess different quality criteria. This plan guides the use of a template-based prompt that integrates example texts and evaluator scores. The scoring LLM then uses this populated template to generate final evaluation scores with rationales. The template structure ensures consistent evaluation across different tasks while the LLM reasoning combines metric scores in a context-aware manner rather than simple averaging.

## Key Results
- Achieved 0.962 system-level Kendall-Tau correlation with human judgments on SummEval
- Achieved 0.744 turn-level Spearman correlation on TopicalChat dataset
- Outperformed all individual assistant evaluators across all evaluation criteria
- Demonstrated task flexibility with the same framework working across different evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1
Fusion-Eval uses LLM planning to coordinate multiple specialized evaluators, achieving higher correlation with human judgments than any single evaluator alone. The LLM first generates a natural language plan describing how to use each assistant evaluator for specific evaluation criteria (coherence, consistency, relevance, fluency). This plan is then integrated into the evaluation prompt template, guiding the final scoring LLM to combine the assistant scores appropriately. Core assumption: The LLM can effectively reason about how different evaluation metrics relate to different quality dimensions and create an optimal evaluation strategy. Evidence anchors: Correlation results significantly above baselines, LLM planning implementation described in paper. Break condition: The planning LLM fails to understand the relationships between metrics and criteria.

### Mechanism 2
Fusion-Eval achieves task flexibility by using a template-based approach that can be adapted to different evaluation tasks. The evaluation prompt template contains placeholders for task-specific information and assistant evaluator scores. The LLM fills these placeholders based on the task and generates scores for each criterion along with rationales. The template structure ensures consistent evaluation across different tasks. Core assumption: A well-designed template with clear placeholders and instructions can guide the LLM to produce consistent, high-quality evaluations across diverse tasks. Evidence anchors: System designed to work well with different tasks, template construction described in paper. Break condition: The template becomes too rigid for certain tasks, or the LLM fails to properly fill in the placeholders leading to inconsistent evaluations.

### Mechanism 3
Fusion-Eval improves upon individual metrics by using LLM reasoning to combine metric scores in a context-aware manner. Instead of simply averaging metric scores, the LLM reads the source text, the generated summary, and the assistant evaluator scores. It then uses its understanding of language quality to weigh and combine these scores appropriately for each evaluation criterion. Core assumption: The LLM can understand the nuances of language quality and make better judgments than simple metric combinations. Evidence anchors: Fusion-Eval significantly outperforms all assistant evaluators on average, suggesting effective aggregation of scores. Break condition: The LLM's combination strategy is no better than simple averaging, or it introduces bias that reduces correlation with human judgments.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and its application in evaluation
  - Why needed here: Fusion-Eval uses NLI as one of its assistant evaluators to assess the logical relationship between source text and generated content.
  - Quick check question: What does a high NLI entailment probability indicate about the relationship between source and generated text?

- Concept: Correlation metrics (Spearman and Kendall-Tau) for evaluating evaluator performance
  - Why needed here: The paper reports both Spearman and Kendall-Tau correlations to measure how well Fusion-Eval aligns with human judgments.
  - Quick check question: What is the key difference between Spearman correlation and Kendall-Tau correlation when measuring evaluator performance?

- Concept: Prompt engineering and template-based LLM evaluation
  - Why needed here: Fusion-Eval relies on carefully constructed prompts and templates to guide the LLM through the evaluation process.
  - Quick check question: What are the key components that should be included in an evaluation prompt template for an LLM-based evaluator?

## Architecture Onboarding

- Component map:
  - Planning LLM -> Assistant evaluators (NLI, BLEURT, SumBleurt) -> Evaluation prompt template -> Scoring LLM -> Human correlation metrics

- Critical path:
  1. Input data (source text, generated text) is prepared
  2. Planning LLM generates evaluation strategy
  3. Assistant evaluators generate scores
  4. Template is populated with data and assistant scores
  5. Scoring LLM evaluates using the template
  6. Correlation with human judgments is calculated

- Design tradeoffs:
  - Flexibility vs. complexity: More complex templates can handle more tasks but are harder to maintain
  - Cost vs. performance: Using larger LLMs improves performance but increases cost
  - Transparency vs. performance: More LLM reasoning improves performance but reduces transparency

- Failure signatures:
  - Low correlation with human judgments indicates the LLM combination strategy is ineffective
  - Inconsistent evaluations across similar inputs suggests template issues
  - High variance in scores indicates the planning or scoring LLM is unreliable

- First 3 experiments:
  1. Implement a simple version using only one assistant evaluator (e.g., NLI) to verify basic functionality
  2. Add multiple assistant evaluators but use simple averaging instead of LLM combination to establish a baseline
  3. Implement the full LLM planning and combination approach and compare correlation with human judgments against the baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Fusion-Eval change when using different LLMs as the backbone, such as GPT-4-32k or GPT-4-1106-preview, compared to the GPT-4 model with 8k context length used in the current experiments? Basis: The paper mentions plans to broaden the experimental scope to encompass a diverse range of GPT-4 models. Why unresolved: Current experiments predominantly utilize GPT-4 with 8k context length. What evidence would resolve it: Conducting experiments using different GPT-4 models as the backbone for Fusion-Eval and comparing their performance.

### Open Question 2
How does the performance of Fusion-Eval vary across different tasks and datasets, beyond the SummEval and TopicalChat datasets used in the current study? Basis: The paper introduces Fusion-Eval as a system designed to work well with different tasks. Why unresolved: The current study only provides results on two datasets. What evidence would resolve it: Conducting meta-evaluations of Fusion-Eval using a diverse range of tasks and datasets to assess its generalizability and performance across different contexts.

### Open Question 3
What is the impact of different assistant evaluators on the performance of Fusion-Eval, and how can the selection of assistant evaluators be optimized for specific tasks? Basis: The paper mentions plans for a comprehensive ablation study focused on assistant evaluators. Why unresolved: The current study does not provide detailed insights into the impact of different assistant evaluators on Fusion-Eval's performance. What evidence would resolve it: Conducting ablation studies to evaluate the impact of different assistant evaluators on Fusion-Eval's performance and developing guidelines for selecting and optimizing assistant evaluators based on the task at hand.

## Limitations

- The specific prompt templates and LLM configurations are not fully specified, making exact replication challenging
- Evaluation relies on correlation with human judgments as the primary validation metric, which has known limitations and subjectivity
- The paper doesn't report direct comparisons between Fusion-Eval's LLM-guided combination and simple averaging methods

## Confidence

- High confidence: General approach of using LLM planning to coordinate multiple evaluators, as correlation results are substantially above baselines
- Medium confidence: Template-based design flexibility, as this requires specific implementation details not provided
- Medium confidence: LLM-based combination strategy outperforming simple averaging, though direct comparisons with baseline averaging methods are not reported

## Next Checks

1. Implement a direct comparison between Fusion-Eval's LLM-guided combination and simple weighted averaging of assistant evaluator scores to quantify the improvement
2. Test the evaluation consistency by running multiple evaluations of the same examples with different random seeds to measure variance
3. Conduct ablation studies removing individual assistant evaluators to determine which contribute most to the performance gains