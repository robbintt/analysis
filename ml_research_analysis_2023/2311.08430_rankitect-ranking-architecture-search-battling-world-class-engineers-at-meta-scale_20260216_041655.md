---
ver: rpa2
title: 'Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta
  Scale'
arxiv_id: '2311.08430'
source_url: https://arxiv.org/abs/2311.08430
tags:
- search
- rankitect
- supernet
- which
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Rankitect, a neural architecture search (NAS) framework, is introduced
  to address the challenges of deploying NAS at Meta scale, where models must outperform
  those optimized by world-class engineers. The framework searches for new architectures
  by composing low-level building blocks from scratch, utilizing three NAS algorithms:
  sampling-based methods, one-shot NAS with reinforcement learning, and differentiable
  NAS (DNAS).'
---

# Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale

## Quick Facts
- arXiv ID: 2311.08430
- Source URL: https://arxiv.org/abs/2311.08430
- Reference count: 40
- Key outcome: NAS framework discovers models outperforming production baselines at Meta scale

## Executive Summary
Rankitect introduces a comprehensive neural architecture search framework designed to automatically discover high-performing ranking models at Meta scale. The framework implements and improves three categories of NAS algorithms—sampling-based methods, one-shot NAS with reinforcement learning, and differentiable NAS—to search through massive architecture spaces containing millions of possible models. When tested on Meta's industrial ranking systems with over 50 billion examples, Rankitect successfully discovered architectures achieving competitive tradeoffs between Normalized Entropy loss and computational efficiency, outperforming existing production models optimized by world-class engineers.

## Method Summary
Rankitect operates by constructing a massive search space of millions of possible architectures through composition of low-level building blocks. The framework implements three complementary NAS algorithms: sampling-based methods with neural predictors for efficient candidate selection, one-shot NAS using reinforcement learning with noise reduction via baseline models, and differentiable NAS employing Gumbel-Softmax for continuous relaxation. These methods are evaluated using Normalized Entropy loss and FLOPs as primary metrics, with discovered architectures validated through both offline evaluations and online A/B testing at Meta scale.

## Key Results
- Rankitect discovered new models from scratch achieving competitive tradeoffs between Normalized Entropy loss and FLOPs
- When using engineer-designed search spaces, Rankitect generated models outperforming human-designed baselines
- Online A/B tests at Meta scale validated the effectiveness of Rankitect-discovered architectures

## Why This Works (Mechanism)

### Mechanism 1
Rankitect discovers models outperforming production baselines through large-scale architecture search. By implementing three NAS algorithms across a massive search space containing millions of architectures, the framework can find novel compositions of building blocks that achieve better Normalized Entropy-FLOPs tradeoffs than human-engineered models. The search space's size and the algorithms' efficiency enable exploration beyond human design intuition.

### Mechanism 2
Rankitect generates better models than human engineers when leveraging engineer-designed search spaces. By using human-designed architectures (like DHEN) as supernet backbones, Rankitect can efficiently search optimal block dimensions while benefiting from the prior knowledge embedded in the human design. This hybrid approach combines automated search advantages with human engineering expertise.

### Mechanism 3
Rankitect's on/off-policy RL method improves sample efficiency and reduces noise. The framework uses an in-place baseline model to reduce noise in mini-batch NE calculations and adopts off-policy RL with experience replay. This approach achieves faster learning and better sample efficiency compared to traditional on-policy methods by reusing past samples and reducing variance in reward signals.

## Foundational Learning

- **Neural Architecture Search (NAS)**: Automated method for discovering optimal neural network architectures
  - Why needed: Rankitect is designed to automatically generate new architectures for ranking systems
  - Quick check: What are the three main categories of NAS algorithms implemented in Rankitect?

- **Normalized Entropy (NE) Loss**: Metric measuring prediction performance in ranking systems
  - Why needed: NE loss evaluates ranking model quality, with smaller values indicating better performance
  - Quick check: How is NE gain calculated, and what does a negative NE gain imply?

- **Reinforcement Learning (RL)**: Learning paradigm where agents learn through interaction with environment
  - Why needed: RL is used in Rankitect's one-shot NAS method to sample architectures and update sampling distributions
  - Quick check: What role does the in-place baseline model play in Rankitect's RL method?

## Architecture Onboarding

- **Component map**: Search Space → Supernet → NAS Algorithms → Evaluation Metrics
- **Critical path**: 1) Design search space and supernet, 2) Implement three NAS algorithms, 3) Evaluate discovered architectures, 4) Compare with baselines
- **Design tradeoffs**: 
  - Search Space Size vs. Computational Efficiency
  - Algorithm Complexity vs. Ease of Implementation
  - Evaluation Metrics vs. Real-world Performance
- **Failure signatures**: Models don't outperform baselines, search takes too long, architectures aren't transferable
- **First 3 experiments**:
  1. Evaluate noise reduction effectiveness by comparing NE gain with vs without in-place baseline
  2. Compare sample efficiency of on/off-policy RL vs on-policy REINFORCE
  3. Test transferability of discovered models across different products/datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of GPUs to use for Rankitect search to balance cost and efficiency?
- Basis: The paper mentions using up to 128 GPUs but doesn't provide systematic study of GPU-count tradeoffs
- Why unresolved: No detailed analysis of relationship between GPU count, search time, and model quality
- What evidence would resolve it: Analysis of GPU count vs search efficiency and cost across different NAS methods

### Open Question 2
- Question: How does Rankitect performance scale with dataset size?
- Basis: Paper mentions 50+ billion example dataset but doesn't show performance across varying sizes
- Why unresolved: No experiments comparing performance on different dataset sizes
- What evidence would resolve it: Experiments on datasets from small to large-scale industrial datasets

### Open Question 3
- Question: Can Rankitect be extended beyond ranking systems to other domains?
- Basis: Paper mentions NAS applied to other domains but Rankitect is specifically designed for ranking
- Why unresolved: No exploration of potential for other task types or necessary modifications
- What evidence would resolve it: Experiments applying Rankitect to other domains with performance comparisons

## Limitations

- Limited external validation due to lack of public benchmarks and reliance on internal Meta data
- Heavy dependency on search space quality, with success tied to human-designed spaces
- Significant computational requirements not fully detailed, making scalability assessment difficult
- Generalizability concerns as effectiveness is demonstrated only on Meta's internal ranking systems

## Confidence

- **High Confidence**: Core technical implementation of three NAS algorithms is well-described and technically sound
- **Medium Confidence**: Claims about discovering models outperforming production baselines are supported by internal evaluations but lack external verification
- **Low Confidence**: Assertion about "battling world-class engineers" is based on comparisons with production models rather than direct head-to-head competitions

## Next Checks

1. Test Rankitect on publicly available ranking datasets to assess generalizability beyond Meta's internal data
2. Conduct ablation study on search space quality to understand framework's sensitivity to this critical component
3. Perform detailed resource efficiency analysis measuring computational costs versus performance gains for practical adoption guidance