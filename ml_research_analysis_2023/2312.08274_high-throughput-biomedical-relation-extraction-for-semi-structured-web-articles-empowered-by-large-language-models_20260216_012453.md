---
ver: rpa2
title: High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles
  Empowered by Large Language Models
arxiv_id: '2312.08274'
source_url: https://arxiv.org/abs/2312.08274
tags:
- relation
- biomedical
- extraction
- llms
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a high-throughput biomedical relation extraction
  framework leveraging large language models (LLMs) for semi-structured web articles.
  The approach formulates relation extraction as a binary classification task, using
  LLMs to determine whether matched biomedical terms are semantically related to the
  main title of a web article, supported by context retrieval and reasoning.
---

# High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models

## Quick Facts
- arXiv ID: 2312.08274
- Source URL: https://arxiv.org/abs/2312.08274
- Authors: 
- Reference count: 37
- Key outcome: Introduces a high-throughput biomedical relation extraction framework leveraging LLMs for semi-structured web articles, achieving an F1 score of 0.895 with SOLAR 70B, comparable to GPT-4.

## Executive Summary
This study introduces a high-throughput biomedical relation extraction framework leveraging large language models (LLMs) for semi-structured web articles. The approach formulates relation extraction as a binary classification task, using LLMs to determine whether matched biomedical terms are semantically related to the main title of a web article, supported by context retrieval and reasoning. The framework integrates preprocessing of semi-structured HTML data, embedding-based retrieval of relevant text chunks, and a tailored LLM prompt design. Evaluation on a benchmark dataset curated by a medical expert shows the open-source LLM SOLAR 70B achieves an F1 score of 0.895, comparable to GPT-4. The method extracts 304,315 relation triplets from four biomedical websites, demonstrating scalability and adaptability for diverse relation types. Case studies highlight challenges such as term interpretation and complex reasoning, underscoring the need for domain expert validation. The framework effectively combines LLMs and knowledge graphs to enhance biomedical knowledge extraction.

## Method Summary
The framework preprocesses semi-structured biomedical web articles into a structured format, extracting the main title as the tail entity and matching biomedical terms using a trie-based approach with the BIOS thesaurus. Text chunks are created with overlap, embedded using bge-large-en-v1.5, and retrieved based on cosine similarity for each term-title pair. LLMs (SOLAR 70B, Llama2 variants, GPT-3.5/4) are prompted to classify the relation as binary (yes/no) with reasoning. Results are parsed, deduplicated, and output as knowledge graph triplets.

## Key Results
- SOLAR 70B achieves an F1 score of 0.895 on expert-labeled benchmark, comparable to GPT-4.
- The framework extracts 304,315 relation triplets from four biomedical websites.
- Adaptability is demonstrated across diverse semi-structured biomedical websites and relation types.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework effectively leverages LLM reading comprehension and world knowledge to classify biomedical term relations without extensive training data.
- Mechanism: By treating relation extraction as a binary classification task and prompting LLMs to judge if a matched term is semantically related to the main title, the system bypasses the need for labeled datasets. The LLM uses both the provided context and its pre-existing biomedical knowledge to make decisions, while also providing reasoning for transparency.
- Core assumption: LLMs possess sufficient domain knowledge and reasoning ability to accurately classify biomedical relations from semi-structured web articles.
- Evidence anchors:
  - [abstract] "The approach formulates relation extraction as a binary classification task, using LLMs to determine whether matched biomedical terms are semantically related to the main title of a web article, supported by context retrieval and reasoning."
  - [section] "We regard the preprocessed web articles as the external data source and use the LLMs as the frozen engine (i.e. no update for model parameters) for high-throughput relation extraction in a question-answering style."
  - [corpus] Weak evidence; no direct corpus support for LLM knowledge effectiveness in this specific biomedical context.
- Break condition: If the LLM lacks sufficient biomedical domain knowledge or the context provided is inadequate for accurate reasoning, the classification accuracy will degrade.

### Mechanism 2
- Claim: The integration of context retrieval and chunking enables the framework to handle lengthy semi-structured web articles within LLM context length constraints.
- Mechanism: The framework slices long articles into manageable text chunks, embeds them using a dense retriever, and retrieves the most relevant chunks for each query. This ensures that the LLM receives focused, pertinent context without exceeding its context window, improving relation extraction accuracy.
- Core assumption: Relevant context for relation extraction is contained within a subset of the article's text chunks, and embedding-based retrieval can effectively identify these chunks.
- Evidence anchors:
  - [abstract] "Moreover, lengthy contents are sliced into text chunks, embedded, and retrieved with additional embedding models."
  - [section] "A natural solution is to preserve a single text chunk that contains the matched head entity within a limited length... we chunk the whole text within one subsection with an overlap, utilize an off-the-shelf text embedding model to obtain fixed-size dense vectors for each data chunk, and retrieve the most relevant text chunks regarding the query."
  - [corpus] Weak evidence; no direct corpus support for the effectiveness of this chunking and retrieval approach in this specific application.
- Break condition: If the relevant context spans multiple chunks that are not retrieved together, or if the embedding model fails to capture semantic similarity accurately, the LLM may lack necessary information for correct classification.

### Mechanism 3
- Claim: The framework's adaptability allows it to extract various types of biomedical relations with minimal adjustments.
- Mechanism: By designating the main title as the tail entity and using a biomedical thesaurus to identify potential head entities, the system can be easily extended to different relation types and biomedical websites. The LLM's prompt can be modified to target specific relation types without requiring model retraining.
- Core assumption: The main title consistently serves as a meaningful tail entity across different biomedical articles and relation types, and the thesaurus comprehensively covers relevant biomedical terms.
- Evidence anchors:
  - [abstract] "Its adaptability is evident, as it can be seamlessly extended to diverse semi-structured biomedical websites, facilitating the extraction of various types of biomedical relations with ease."
  - [section] "We regard the main title as the tail entity and then treat all the biomedical terms matched on the page as head entities to check if they have a pre-defined semantic relation with the tail entity."
  - [corpus] Weak evidence; no direct corpus support for the framework's adaptability to diverse relation types and websites.
- Break condition: If the main title is not a suitable tail entity for certain relation types or websites, or if the thesaurus lacks coverage for specific biomedical domains, the framework's adaptability will be limited.

## Foundational Learning

- Concept: Binary classification formulation for relation extraction
  - Why needed here: Transforms a complex relation extraction task into a simpler yes/no decision problem that LLMs can handle effectively, reducing the need for extensive training data.
  - Quick check question: How does framing relation extraction as binary classification simplify the problem for LLMs compared to traditional multi-class approaches?

- Concept: Context retrieval and chunking for handling long documents
  - Why needed here: Overcomes the context length limitations of LLMs by breaking down long semi-structured articles into manageable chunks and retrieving only the most relevant ones for each query.
  - Quick check question: Why is it important to use embedding-based retrieval for selecting relevant text chunks, rather than simply taking the chunk containing the matched term?

- Concept: Prompt engineering and in-context learning with LLMs
  - Why needed here: Guides the LLM to focus on the specific task of judging term relations and provides examples to improve performance, especially for open-source models with less domain-specific training.
  - Quick check question: How do exemplars in the prompt contribute to the LLM's ability to perform the relation extraction task accurately?

## Architecture Onboarding

- Component map:
  - Data preprocessing: HTML parsing, trie-based term matching using BIOS thesaurus
  - Context retrieval: Text chunking, dense embedding with bge-large-en-v1.5, cosine similarity-based retrieval
  - LLM inference: Prompt generation, binary classification with reasoning, SOLAR 70B or other LLMs
  - Output processing: JSON parsing, deduplication, knowledge graph integration

- Critical path:
  1. Preprocess HTML to extract main title and text content
  2. Match biomedical terms using trie and BIOS thesaurus
  3. Chunk text and embed using dense retriever
  4. Retrieve top-K relevant chunks for each term-title pair
  5. Generate LLM prompt with context and term information
  6. Get LLM binary classification and reasoning
  7. Aggregate results and output relation triplets

- Design tradeoffs:
  - Using open-source LLM (SOLAR 70B) vs. API-based models (GPT-4): Cost vs. potentially higher accuracy
  - Fixed chunk size vs. adaptive chunking: Simplicity vs. optimal context utilization
  - Binary classification vs. direct triplet extraction: Simplicity and transparency vs. potential loss of nuanced relations

- Failure signatures:
  - Low recall: LLM is overly conservative in accepting relations, possibly due to prompt phrasing or lack of domain knowledge
  - Low precision: LLM is too permissive, accepting incorrect relations, possibly due to insufficient context or ambiguous terms
  - High variance across relation types: Some relation types are more challenging for the LLM, requiring targeted prompt adjustments

- First 3 experiments:
  1. Ablation study: Compare performance with and without context retrieval/chunking to quantify the impact of focused context on accuracy.
  2. Prompt optimization: Systematically vary prompt phrasing, exemplars, and reasoning requirements to find the most effective configuration for each relation type.
  3. Corpus diversity test: Apply the framework to a new biomedical website with a different structure to assess adaptability and identify any required modifications.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt format and design for biomedical relation extraction tasks using large language models?
- Basis in paper: [explicit] The paper mentions that "it remains uncertain whether this specific prompt format is optimal" and suggests that "ongoing discussions with human experts are essential to refine and enhance the effectiveness of the LLM prompt."
- Why unresolved: The current prompt format has not been thoroughly explored and optimized for different types of biomedical relation extraction tasks.
- What evidence would resolve it: Conducting extensive experiments with various prompt formats and designs, involving domain experts to evaluate the effectiveness of each approach, and comparing the results to determine the optimal prompt format.

### Open Question 2
- Question: How can large language models be effectively utilized to analyze cleaned HTML text directly, instead of relying on preprocessed data structures?
- Basis in paper: [inferred] The paper suggests that "considering the LLMs' proficiency in understanding code, a potential avenue for improvement involves allowing them to directly analyze cleaned HTML text."
- Why unresolved: The paper does not provide a detailed exploration of this approach, and its effectiveness in handling HTML text with varying context lengths remains unclear.
- What evidence would resolve it: Implementing a framework that allows LLMs to directly analyze cleaned HTML text, evaluating its performance in extracting biomedical relations, and comparing it with the current approach using preprocessed data structures.

### Open Question 3
- Question: How can the framework be extended to extract biomedical relations from other types of web articles, such as clinical procedures, in addition to diseases?
- Basis in paper: [explicit] The paper mentions that "expanding the scope to include other types, such as clinical procedures, would contribute to a more comprehensive assessment."
- Why unresolved: The current framework focuses on extracting relations from disease-related web articles, and its applicability to other types of biomedical web articles has not been explored.
- What evidence would resolve it: Adapting the framework to handle different types of biomedical web articles, conducting experiments to evaluate its performance on clinical procedures and other relevant topics, and comparing the results with the current approach.

## Limitations

- The lack of transparency regarding prompt templates and few-shot exemplars used for LLM inference makes it difficult to assess reproducibility and potential bias.
- The evaluation relies on a single expert-labeled benchmark, which may not capture the full diversity of biomedical relation extraction scenarios.
- The study does not provide a detailed analysis of the LLM's performance across different biomedical domains or relation types, leaving questions about generalizability.

## Confidence

- **High confidence**: The framework's overall architecture and approach are well-defined, with clear explanations of the data preprocessing, context retrieval, and LLM inference components. The use of LLMs for binary classification of biomedical relations is a novel and promising approach, and the reported F1 score of 0.895 for SOLAR 70B is a strong indicator of the method's effectiveness.

- **Medium confidence**: The reported performance metrics, while impressive, are based on a single expert-labeled benchmark. The study does not provide a comprehensive analysis of the framework's performance across different biomedical domains, relation types, or web article structures. Additionally, the lack of transparency regarding the exact prompt templates and exemplars used for LLM inference introduces some uncertainty in the reproducibility and generalizability of the results.

- **Low confidence**: The study does not provide sufficient evidence to support the claim that the framework can be easily extended to diverse semi-structured biomedical websites and relation types. While the authors mention the adaptability of the approach, they do not demonstrate its effectiveness on a wide range of websites or relation types beyond the initial evaluation.

## Next Checks

1. **Prompt ablation study**: Conduct a systematic ablation study to assess the impact of different prompt formulations, exemplars, and reasoning requirements on the LLM's performance. This will help identify the most effective prompt configuration for various biomedical relation types and provide insights into the LLM's decision-making process.

2. **Cross-domain evaluation**: Evaluate the framework's performance on a diverse set of biomedical websites with varying structures, content, and relation types. This will help assess the framework's generalizability and identify any domain-specific adaptations or improvements needed.

3. **Expert validation of extracted triplets**: Engage a panel of domain experts to validate a sample of the extracted relation triplets for accuracy and relevance. This will provide an independent assessment of the framework's performance and help identify any systematic errors or biases in the LLM's classification.