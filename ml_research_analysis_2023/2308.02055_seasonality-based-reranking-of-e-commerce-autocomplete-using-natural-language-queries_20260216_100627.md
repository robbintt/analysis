---
ver: rpa2
title: Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language
  Queries
arxiv_id: '2308.02055'
source_url: https://arxiv.org/abs/2308.02055
tags:
- query
- seasonality
- queries
- search
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a neural network-based approach to incorporate
  seasonality into e-commerce autocomplete ranking by predicting seasonality scores
  for queries based on query text and month. The method uses a feed-forward neural
  network trained on historical query volumes to estimate the likelihood of a query
  being searched in a given month.
---

# Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries

## Quick Facts
- **arXiv ID**: 2308.02055
- **Source URL**: https://arxiv.org/abs/2308.02055
- **Reference count**: 7
- **Primary result**: Neural network-based seasonality scores improve e-commerce autocomplete ranking, yielding +0.13% to +0.96% MRR lift offline and +0.25% to +0.34% GMV lift online.

## Executive Summary
This paper presents a method to improve e-commerce autocomplete (QAC) ranking by incorporating seasonality into the ranking model. The approach uses a feed-forward neural network to predict seasonality scores for search queries based on query text and month, using historical query volume data. These scores are integrated as a feature in the L2 ranker, resulting in statistically significant offline improvements in mean reciprocal rank (MRR) and measurable online gains in gross merchandise value (GMV).

## Method Summary
The method involves training a feed-forward neural network to predict seasonality scores (Vqm) for query-month pairs using historical search query logs. The model uses pre-trained GloVe embeddings for query text and one-hot encoding for months, optimized with mean squared error loss and the Adam optimizer. Seasonality scores are computed offline and integrated as a feature in the L2 ranker to re-rank autocomplete suggestions. The approach is evaluated using offline MRR metrics and online A/B tests measuring GMV lift.

## Key Results
- Statistically significant MRR lifts ranging from +0.13% to +0.96% across four e-commerce platforms.
- Online A/B testing shows GMV improvements of +0.25% (iOS) and +0.34% (Android).
- The model generalizes to tail queries and avoids runtime latency by precomputing seasonality scores offline.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Seasonality scores computed via neural network improve QAC ranking by better matching temporal user intent.
- Mechanism: The model predicts query-month seasonality scores offline, which are integrated into the L2 ranker. This allows ranking models to prioritize seasonally relevant queries over generic popularity-based rankings.
- Core assumption: Seasonality defined as probability of query occurrence in a given month (normalized by monthly volume) accurately captures temporal user behavior patterns.
- Evidence anchors:
  - [abstract] states "neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal"
  - [section 4.2] describes the model as a feed-forward neural network predicting Vqm values
  - [corpus] shows related work focuses on time-sensitive or burst-based QAC ranking, supporting the general direction
- Break condition: If user search behavior becomes unpredictable or heavily influenced by external events not captured in historical data, the model's seasonality predictions may degrade.

### Mechanism 2
- Claim: Offline generation of seasonality scores avoids runtime latency while improving relevance.
- Mechanism: The seasonality model is trained on historical query logs and produces scores for all queries (head, torso, tail) offline, which are then consumed by the ranking system at runtime without additional computation overhead.
- Core assumption: The offline scores remain relevant across query types and can be precomputed without losing accuracy.
- Evidence anchors:
  - [section 4.1] explains training dataset generation from query logs and filtering for volume thresholds
  - [section 5.1] notes scores can be "computed offline" and "integrated in the QAC ranking model"
  - [corpus] indicates prior work uses time-series forecasting at runtime, contrasting with the offline approach here
- Break condition: If query distribution shifts rapidly or if tail queries become significant in volume, precomputed scores may become stale.

### Mechanism 3
- Claim: Integration of seasonality into L2 ranker provides incremental lift over L1-only temporal modeling.
- Mechanism: L1 captures general popularity and coarse seasonality, while L2 adds fine-grained seasonality from the neural model, allowing re-ranking that better matches user intent for the current month.
- Core assumption: L1 already captures some seasonality, so L2's additional signal provides measurable improvement rather than redundancy.
- Evidence anchors:
  - [section 5.1] explicitly states "L1 ranker already captures seasonality and time sensitivity to a certain extent"
  - [section 5.2] shows statistically significant MRR lifts across platforms
  - [section 5.3] reports GMV lifts in online A/B tests
  - [corpus] lists hybrid or re-ranking approaches in related work, supporting the layered strategy
- Break condition: If L1 and L2 models are not well-calibrated, seasonality may overcompensate and hurt ranking performance.

## Foundational Learning

- Concept: Query Auto-Completion (QAC) ranking pipeline
  - Why needed here: Understanding how matching and ranking steps work is essential to see where seasonality fits.
  - Quick check question: What is the difference between L1 and L2 rankers in this system?
- Concept: Seasonality definition via normalized query volume
  - Why needed here: The core metric Vqm determines what the neural model learns and predicts.
  - Quick check question: How is seasonality different from raw popularity in this paper?
- Concept: Feed-forward neural network regression for sequence prediction
  - Why needed here: The model learns to map (query, month) → seasonality score, which is central to the method.
  - Quick check question: Why is month encoded as one-hot and query as embedding?

## Architecture Onboarding

- Component map: Data pipeline → Training dataset generation → Feed-forward neural network → Offline seasonality scores → L2 ranker → QAC suggestions
- Critical path: Query logs → Monthly volume aggregation → Vqm computation → Neural model training → Score storage → Runtime re-ranking
- Design tradeoffs: Offline scoring avoids latency but risks staleness; neural model generalizes to tail queries but may misfire on rare or novel events.
- Failure signatures: MRR lift disappears; online GMV unchanged; seasonality scores do not correlate with observed query spikes.
- First 3 experiments:
  1. Train the neural model on a small sample of queries and validate Vqm predictions against held-out data.
  2. Integrate seasonality scores into a sandbox L2 ranker and measure offline MRR on a test prefix set.
  3. Deploy to a small online segment and compare GMV against control to confirm lift significance.

## Open Questions the Paper Calls Out

- How does the proposed seasonality model perform compared to sequence-to-sequence deep learning models that predict seasonality scores for all twelve months simultaneously?
  - Basis in paper: [explicit] The paper mentions that a sequence-to-sequence model could be used to predict twelve seasonality values for each query and leaves this as future work.
  - Why unresolved: The paper only evaluates a feed-forward neural network architecture and does not compare its performance to sequence-to-sequence models.
  - What evidence would resolve it: An empirical comparison of the proposed model's performance (in terms of MRR and GMV lift) against a sequence-to-sequence model trained and evaluated on the same dataset.

- How does the granularity of seasonality prediction (monthly vs. weekly/bi-weekly) impact the effectiveness of the autocomplete ranking model?
  - Basis in paper: [explicit] The paper suggests exploring seasonality at a more granular level, such as weekly or bi-weekly intervals, as future work.
  - Why unresolved: The current model only predicts seasonality at a monthly level, and the potential benefits of finer granularity are not explored.
  - What evidence would resolve it: An empirical evaluation comparing the performance of the autocomplete ranking model using monthly, weekly, and bi-weekly seasonality predictions on the same dataset.

- How does the interaction between query category and seasonality affect the autocomplete ranking, and how can category-level seasonality be incorporated into the learning-to-rank model?
  - Basis in paper: [explicit] The paper suggests studying the interaction between seasonality and query category, and including category-level seasonality in the learning-to-rank model as future work.
  - Why unresolved: The current model does not consider query categories or their interaction with seasonality.
  - What evidence would resolve it: An empirical study analyzing the correlation between query categories and their seasonality patterns, followed by an evaluation of the autocomplete ranking model's performance when incorporating category-level seasonality features.

## Limitations

- The reliance on historical query volumes to define seasonality may not capture sudden shifts in user behavior (e.g., pandemic-driven changes or new trending products).
- Offline generation of seasonality scores introduces potential staleness risk if query distributions change rapidly between training and deployment.
- The exact integration mechanism of seasonality scores into the L2 ranker is underspecified, making it difficult to assess the robustness of the reranking step.

## Confidence

- **High**: The offline MRR improvements and online GMV lifts are supported by empirical results, though the magnitude of lift is modest.
- **Medium**: The neural network architecture and training procedure are clearly described, but details like the volume threshold K and exact ranking model configuration are missing.
- **Low**: The paper does not address edge cases such as rare queries, novel events, or cross-platform generalizability.

## Next Checks

1. **Staleness Test**: Simulate a sudden shift in query distribution (e.g., a new trending product) and measure whether seasonality scores degrade or fail to adapt.
2. **Ablation Study**: Isolate the contribution of seasonality by comparing performance with and without the seasonality feature in the L2 ranker.
3. **Tail Query Analysis**: Evaluate model performance on long-tail queries to assess generalization beyond head queries.