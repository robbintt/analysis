---
ver: rpa2
title: Learning to Generate All Feasible Actions
arxiv_id: '2301.11461'
source_url: https://arxiv.org/abs/2301.11461
tags:
- learning
- action
- actions
- actor
- feasible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach to learn a skill by training
  a generative neural network to generate all feasible actions of a subtask by interacting
  with an environment. The approach interprets the feasibility of each action given
  a state as a posterior probability distribution over the actions and trains the
  actor to minimize a divergence of its output distribution to this target pdf.
---

# Learning to Generate All Feasible Actions

## Quick Facts
- arXiv ID: 2301.11461
- Source URL: https://arxiv.org/abs/2301.11461
- Reference count: 40
- The paper proposes a novel approach to learn a skill by training a generative neural network to generate all feasible actions of a subtask by interacting with an environment.

## Executive Summary
This paper introduces a novel framework for learning skills by generating all feasible actions in a subtask through self-supervised interaction with an environment. The key insight is interpreting feasibility as a posterior probability distribution over actions given a state, then training a generative neural network (actor) to match this distribution using f-divergence minimization. The approach uses kernel density estimation and importance sampling to estimate gradients for training. Experiments on a planar robotic grasping task demonstrate the method's ability to learn and reproduce diverse feasible actions, outperforming related methods in mode coverage.

## Method Summary
The method treats the feasibility of actions as a posterior probability distribution and trains a generative actor network to match this distribution using f-divergence minimization. An auxiliary critic network estimates feasibility to reduce environment interactions. The actor generates actions given states, which are evaluated in the environment for feasibility. Experiences are stored in a replay memory, and both critic and actor are trained using KDE-based gradient estimators with resampling and importance sampling. The framework is evaluated on a 2D robotic grasping task with geometric shapes.

## Key Results
- The proposed approach learns to visit all modes in the feasible action space, demonstrating superior mode coverage compared to related methods
- Using Forward Kullback-Leibler (FKL) or Jensen-Shannon (JS) divergences enables the actor to capture multiple modes, while Reverse Kullback-Leibler (RKL) tends to collapse to single modes
- The method achieves good performance on both training and test objects in the planar grasping task, showing generalization capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The actor learns all feasible actions by minimizing a divergence between its output distribution and the target feasibility distribution.
- Mechanism: The target distribution is uniform over the feasible action space. The actor is trained to match this distribution by minimizing an f-divergence (e.g., JS, FKL, RKL) between its output and the target. The gradients are estimated using kernel density estimation (KDE) with resampling and importance sampling.
- Core assumption: The feasible action space can be modeled as a probability distribution, and the actor's output distribution can be made to match it through divergence minimization.
- Evidence anchors:
  - [abstract]: "The approach interprets the feasibility of each action given a state as a posterior probability distribution over the actions and trains the actor to minimize a divergence of its output distribution to this target pdf."
  - [section 3.1]: "The goal of this work is to find a state-dependent surjective map πs : Z → A+ s , referred to as policy... For the same state, the distribution of the feasible actions according to the environment can be defined as ps(a) = g(s,a)∫A g(s,a)da, which is the true posterior ps(a)=p(a|s)."
  - [corpus]: Weak - no direct evidence found in corpus neighbors.
- Break condition: If the feasible action space is too complex or high-dimensional, the KDE approximation may fail, leading to poor gradient estimates.

### Mechanism 2
- Claim: The use of different f-divergences (JS, FKL, RKL) allows the actor to explore and capture all modes in the feasible action space.
- Mechanism: JS and FKL divergences encourage the actor to visit all modes in the target distribution, while RKL tends to collapse into a single mode. By using JS or FKL, the actor can learn a policy that covers all feasible actions, not just a single optimal one.
- Core assumption: Different f-divergences have different properties in terms of mode coverage, and choosing the right one can improve the actor's ability to learn all feasible actions.
- Evidence anchors:
  - [abstract]: "The possibility to use FKL and JS is instrumental in visiting all the modes of the posterior distribution, as RKL is known to collapse into a single mode."
  - [section 3.2]: "Different choices of f lead to well known divergences as summarized in Table 1."
  - [corpus]: Weak - no direct evidence found in corpus neighbors.
- Break condition: If the modes in the feasible action space are too far apart or too narrow, even JS or FKL may not be able to capture them all.

### Mechanism 3
- Claim: The use of an auxiliary critic network to estimate feasibility reduces the number of interactions with the environment, improving learning efficiency.
- Mechanism: The critic is trained to imitate the environment's feasibility function g(s,a). The actor then uses the critic's estimates to form the target distribution, rather than querying the environment directly. This reduces the number of costly environment interactions.
- Core assumption: The critic can accurately approximate the environment's feasibility function, and this approximation is sufficient for training the actor.
- Evidence anchors:
  - [abstract]: "As interactions with the environment are typically costly, an auxiliary critic network imitating the environment is trained simultaneously."
  - [section 4]: "The learning algorithm presented in this paper is inspired by RL and CB. At every training step, the environment generates a state for which the actor proposes an action. The action is evaluated in the environment yielding success or failure. The state, action, and feasibility form an experience stored in a replay memory. The critic is trained on random samples of experiences from the replay memory with a cross-entropy loss on the outcome."
  - [corpus]: Weak - no direct evidence found in corpus neighbors.
- Break condition: If the critic's approximation is poor, the actor will be trained on incorrect targets, leading to suboptimal performance.

## Foundational Learning

- Concept: Kernel Density Estimation (KDE)
  - Why needed here: KDE is used to estimate the actor's output distribution and the target feasibility distribution, which are needed to compute the gradients for divergence minimization.
  - Quick check question: What is the formula for a KDE with Gaussian kernels?

- Concept: Importance Sampling
  - Why needed here: Importance sampling is used to estimate the expectation of the gradients under the actor's distribution, which is needed for the gradient estimators of the f-divergences.
  - Quick check question: What is the formula for importance sampling?

- Concept: f-divergences
  - Why needed here: f-divergences (e.g., JS, FKL, RKL) are used as the loss function for training the actor to match its output distribution to the target feasibility distribution.
  - Quick check question: What is the general form of an f-divergence?

## Architecture Onboarding

- Component map: Environment -> Actor -> Feasibility evaluation -> Replay memory -> Critic -> Actor training loop
- Critical path:
  1. Generate a state from the environment.
  2. Actor generates actions given the state.
  3. Actions are evaluated in the environment, yielding feasibility labels.
  4. Experiences are stored in the replay memory.
  5. Critic is trained on random samples from the replay memory.
  6. Actor is trained on a batch of states from the memory, using the critic's feasibility estimates.
- Design tradeoffs:
  - Using an auxiliary critic reduces environment interactions but introduces approximation error.
  - Using KDE for distribution estimation is flexible but can be computationally expensive and may not scale well to high dimensions.
- Failure signatures:
  - Poor performance: The actor may not learn all feasible actions, or may not learn them accurately.
  - Slow convergence: The actor may take a long time to learn, or may not converge at all.
- First 3 experiments:
  1. Train the actor and critic on a simple environment with a single feasible action mode.
  2. Train the actor and critic on an environment with multiple feasible action modes, using different f-divergences.
  3. Evaluate the actor's performance on unseen states and actions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to high-dimensional action spaces?
- Basis in paper: [inferred] The authors mention that the approach may not be very effective in high-dimensional problems as the sampling requirements for the estimator grow exponentially.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on the scalability of the method to high-dimensional action spaces.
- What evidence would resolve it: Experiments comparing the performance of the proposed method to other methods on problems with increasing action space dimensionality, or theoretical analysis of the computational complexity and sample efficiency of the method as a function of action space dimensionality.

### Open Question 2
- Question: How can the method be improved to reduce the reliance on imitation learning examples to bootstrap the critic's learning?
- Basis in paper: [explicit] The authors mention that currently many imitation examples are required to bootstrap the critic's learning and suggest progressive tuning of the KDEs or learning their parameters during training as potential solutions.
- Why unresolved: The paper does not provide any experimental results or detailed analysis of these potential solutions.
- What evidence would resolve it: Experiments comparing the performance of the proposed method with and without imitation learning examples, or with different amounts of imitation learning examples, or with the suggested progressive tuning of the KDEs or learning their parameters during training.

### Open Question 3
- Question: How can the proposed method be extended to multi-step decision making problems?
- Basis in paper: [inferred] The authors mention that a mitigation of the scalability issue in high-dimensional problems could be to split higher-dimensional problems into multi-step low dimensional problems and to learn to generate all feasible trajectories.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on how to extend the method to multi-step decision making problems.
- What evidence would resolve it: Experiments comparing the performance of the proposed method to other methods on multi-step decision making problems, or theoretical analysis of how to extend the method to multi-step decision making problems, including how to define the feasibility model and how to optimize the actor in this setting.

## Limitations
- The KDE-based gradient estimators require careful bandwidth tuning, and performance may degrade in higher dimensions
- The method's scalability to complex, high-dimensional action spaces remains untested
- The critic's approximation quality directly impacts actor performance, but no theoretical guarantees are provided

## Confidence
- High confidence in the feasibility-as-distribution interpretation and basic framework design
- Medium confidence in the KDE-based training approach, given limited empirical validation
- Low confidence in scalability claims beyond the 2D grasping domain

## Next Checks
1. **Robustness test**: Evaluate performance across varying KDE bandwidth settings to identify sensitivity thresholds
2. **Dimensionality scaling**: Test on environments with progressively higher-dimensional action spaces to assess scalability limits
3. **Critic approximation analysis**: Systematically vary critic accuracy to quantify impact on actor performance and identify failure thresholds