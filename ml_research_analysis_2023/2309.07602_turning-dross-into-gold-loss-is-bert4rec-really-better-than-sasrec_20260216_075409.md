---
ver: rpa2
title: 'Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?'
arxiv_id: '2309.07602'
source_url: https://arxiv.org/abs/2309.07602
tags:
- sasrec
- bert4rec
- loss
- items
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that SASRec model trained with cross-entropy
  loss over all items (instead of binary cross-entropy with one negative sample) significantly
  outperforms BERT4Rec on all tested datasets. The experiments show that SASRec+ achieves
  31.52% NDCG@10 on MovieLens-1M compared to BERT4Rec's 15.37%, while training 2x
  faster.
---

# Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?

## Quick Facts
- arXiv ID: 2309.07602
- Source URL: https://arxiv.org/abs/2309.07602
- Reference count: 40
- Primary result: SASRec trained with cross-entropy loss over all items significantly outperforms BERT4Rec on next-item prediction tasks

## Executive Summary
This paper challenges the prevailing assumption that bidirectional models like BERT4Rec are superior to unidirectional models like SASRec for sequential recommendation tasks. Through systematic experiments across five datasets, the authors demonstrate that SASRec trained with cross-entropy loss over all items (SASRec+) consistently outperforms BERT4Rec by significant margins. The key insight is that the choice of loss function and negative sampling strategy has a more profound impact on performance than the directionality of the attention mechanism itself.

## Method Summary
The paper compares SASRec and BERT4Rec architectures for next-item prediction, focusing on how different loss functions affect performance. SASRec originally uses binary cross-entropy with one negative sample per prediction, while BERT4Rec uses cross-entropy over all items but with masked item prediction. The authors implement SASRec+ by replacing the binary cross-entropy loss with full cross-entropy over all items, and also test large-scale negative sampling as an approximation. Experiments are conducted on five recommendation datasets with standard leave-one-out evaluation and metrics like NDCG@10 and HR@10.

## Key Results
- SASRec+ achieves 31.52% NDCG@10 on MovieLens-1M vs BERT4Rec's 15.37%
- SASRec+ trains approximately 2x faster than BERT4Rec
- With 3000 negative samples, SASRec still outperforms BERT4Rec on all tested datasets
- The performance gap is consistent across datasets of varying sizes and sparsity

## Why This Works (Mechanism)

### Mechanism 1
Cross-entropy loss over all items provides stronger gradient signal than binary cross-entropy with one negative sample. When computing cross-entropy over all items, gradients update weights for every item embedding in the matrix, not just the positive and one negative item. This distributes learning across the entire item space and reduces overfitting to specific negative samples.

### Mechanism 2
Unidirectional modeling is more appropriate for next-item prediction than bidirectional masking. SASRec's causal attention only attends to past items, matching the prediction task of forecasting the next item. BERT4Rec's bidirectional masking learns from both past and future context, which doesn't align with the prediction objective.

### Mechanism 3
Larger negative sampling sets provide better gradient signal than single negative samples. Sampling N negative items (where N >> 1) approximates the full cross-entropy distribution, providing gradient information about multiple items rather than just one negative example.

## Foundational Learning

- Concept: Cross-entropy loss vs binary cross-entropy
  - Why needed here: Understanding the fundamental difference between loss functions used in SASRec and BERT4Rec is crucial for grasping why one outperforms the other
  - Quick check question: What is the key difference in gradient updates between cross-entropy over all items and binary cross-entropy with one negative sample?

- Concept: Transformer self-attention mechanisms
  - Why needed here: Both SASRec and BERT4Rec use Transformer architectures, so understanding causal vs bidirectional attention is essential
  - Quick check question: How does causal attention in SASRec differ from the bidirectional attention used in BERT4Rec?

- Concept: Negative sampling strategies in recommendation systems
  - Why needed here: The paper compares different negative sampling approaches and their effectiveness
  - Quick check question: Why might sampling multiple negative items provide better gradient signal than sampling just one?

## Architecture Onboarding

- Component map:
  Input sequence encoder -> Transformer layers (SASRec: causal, BERT4Rec: bidirectional) -> Output layer -> Loss function

- Critical path:
  1. Sequence input -> Positional encoding
  2. Self-attention layers (causal for SASRec, masked for BERT4Rec)
  3. Output projection to item space
  4. Loss calculation (cross-entropy vs binary cross-entropy)
  5. Gradient computation and parameter updates

- Design tradeoffs:
  - Memory vs accuracy: Full cross-entropy provides better gradients but requires O(|I|) memory per batch
  - Training speed vs effectiveness: SASRec trains faster than BERT4Rec due to unidirectional modeling
  - Negative sampling size vs approximation quality: Larger N provides better approximation but increases computation

- Failure signatures:
  - Underfitting: Low training and validation performance indicates insufficient model capacity or poor hyperparameters
  - Overfitting: High training performance but low validation performance suggests regularization issues
  - Slow convergence: Poor initialization or learning rate can cause slow training, especially for BERT4Rec

- First 3 experiments:
  1. Run SASRec with binary cross-entropy (baseline) vs SASRec+ with cross-entropy to confirm performance difference
  2. Test different negative sampling sizes (N=10, 100, 1000) to find the optimal tradeoff point
  3. Compare training time and convergence speed between SASRec+ and BERT4Rec on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of negative samples needed for sampled cross-entropy loss to match full cross-entropy performance across different datasets?
- Basis in paper: The paper shows that larger numbers of negative samples improve performance but doesn't determine the exact optimal value
- Why unresolved: The paper only tested up to 3000 negatives and found it sufficient but didn't systematically explore the full parameter space
- What evidence would resolve it: A comprehensive study testing different negative sample counts across multiple datasets with varying item catalog sizes

### Open Question 2
How do different negative sampling strategies (e.g., popularity-based, random, temporal-based) affect the performance of SASRec+ compared to the uniform sampling used in this paper?
- Basis in paper: The paper uses simple random negative sampling but doesn't explore alternative sampling strategies
- Why unresolved: Only one sampling strategy was tested, leaving the impact of other strategies unexplored
- What evidence would resolve it: Experiments comparing multiple negative sampling strategies while keeping all other parameters constant

### Open Question 3
Does the advantage of SASRec+ over BERT4Rec persist when both models are trained with limited computational resources (e.g., reduced batch size or training time)?
- Basis in paper: The paper notes that BERT4Rec requires significantly more training time but doesn't test performance under resource constraints
- Why unresolved: All experiments used generous computational resources, not reflecting real-world limitations
- What evidence would resolve it: Controlled experiments varying computational resources while comparing model performance

### Open Question 4
How does the performance gap between SASRec+ and BERT4Rec change when using different sequence length limits for long sequences?
- Basis in paper: The paper uses different maximum sequence lengths (50 vs 200) for different datasets but doesn't systematically study the impact
- Why unresolved: The effect of sequence length on the relative performance of the two models is not explored
- What evidence would resolve it: Experiments varying maximum sequence lengths while keeping other parameters constant across datasets

## Limitations

- The analysis focuses primarily on recommendation datasets, which may not generalize to other sequential prediction tasks where future context provides valuable signal
- Performance comparisons assume optimal hyperparameter tuning for both architectures, though SASRec+ benefits from simpler training dynamics
- The computational efficiency advantage of SASRec+ assumes reasonable catalog sizes where cross-entropy over all items remains tractable

## Confidence

- High confidence: SASRec with cross-entropy loss outperforms SASRec with binary cross-entropy on standard recommendation metrics
- Medium confidence: SASRec+ consistently outperforms BERT4Rec across diverse datasets when properly trained
- Medium confidence: The unidirectional modeling approach is inherently more appropriate for next-item prediction than bidirectional masking

## Next Checks

1. Cross-task generalization test: Evaluate both architectures on non-recommendation sequential tasks (language modeling, time series forecasting) to determine if unidirectional superiority extends beyond recommendation

2. Negative sampling scalability analysis: Systematically vary negative sampling size N across orders of magnitude to precisely characterize the approximation quality vs computational cost tradeoff

3. Architecture ablation study: Compare SASRec+ against variants with different Transformer configurations (more layers, attention heads, hidden sizes) to establish whether the performance gains are architecture-dependent or purely loss-function driven