---
ver: rpa2
title: Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
  Extraction Attacks
arxiv_id: '2309.17410'
source_url: https://arxiv.org/abs/2309.17410
tags:
- information
- attack
- arxiv
- attacks
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of deleting sensitive information
  from large language models. It proposes an attack-and-defense framework to test
  whether "deleted" information can be extracted from model weights using whitebox
  and blackbox attacks.
---

# Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks

## Quick Facts
- **arXiv ID**: 2309.17410
- **Source URL**: https://arxiv.org/abs/2309.17410
- **Reference count**: 31
- **Primary result**: Model editing methods leave traces of deleted information that can be extracted with 38% (whitebox) and 29% (blackbox) success rates

## Executive Summary
This paper investigates whether sensitive information can be reliably deleted from large language models using model editing techniques. The authors propose an attack-and-defense framework to evaluate if "deleted" information can be extracted from model weights. They demonstrate that even after applying state-of-the-art editing methods like ROME and MEMIT, information can still be extracted using whitebox attacks (leveraging intermediate hidden states) and blackbox attacks (using input rephrasing). The paper introduces new defense methods including Max-Entropy and Head Projection Defense, but finds these are not universally effective against all attack types.

## Method Summary
The paper uses model editing methods (ROME and MEMIT) to modify MLP weight matrices in transformer models, with various defense objectives designed to remove specific sensitive facts while minimizing damage to overall model knowledge. Three extraction attacks are implemented: Head Projection (whitebox, leveraging intermediate hidden states), Probability Delta (whitebox, identifying parameter changes), and Input Rephrasing (blackbox, exploiting paraphrasing). The framework evaluates attack success rates alongside model damage metrics (Random ∆-Acc and Neighborhood ∆-Acc) across GPT-J, Llama-2, and GPT2-XL models using CounterFact and zsRE datasets.

## Key Results
- Whitebox attacks achieve 38% success rate and blackbox attacks achieve 29% success rate at extracting deleted information
- State-of-the-art defenses reduce whitebox attack success to 2.4% but only partially mitigate blackbox attacks (19-28% success)
- Model editing methods exhibit imperfect generalization across paraphrased prompts, allowing extraction through input rephrasing
- Traces of deleted information persist in intermediate hidden states even when final outputs are edited

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traces of "deleted" information remain in intermediate hidden states of the LLM
- Mechanism: The logit lens technique converts hidden states from any intermediate layer into a distribution over the model vocabulary by multiplying the hidden states with the output token embedding matrix. This reveals that even after editing methods remove information from the final output, traces persist in earlier layers before disappearing at the final layer.
- Core assumption: Information deletion through editing methods is not complete across all layers of the transformer forward pass
- Evidence anchors:
  - [abstract] "These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states"
  - [section] "we show that by projecting intermediate hidden states onto the model vocabulary embeddings, we are able to extract model knowledge from these hidden states even when the model has been edited to assign zero probability to the knowledge"

### Mechanism 2
- Claim: Model editing methods exhibit imperfect generalization across paraphrased versions of the same prompt
- Mechanism: When an editing method is applied to remove information for one specific phrasing of a question, it does not completely remove that information across all possible rephrasings of the question. By generating multiple paraphrases and sampling outputs, the "deleted" answer can be recovered.
- Core assumption: The editing objective does not fully generalize across the semantic space of related prompts
- Evidence anchors:
  - [abstract] "that applying an editing method for one question may not delete information across rephrased versions of the question"
  - [section] "we exploit their non-zero error rate by sampling model outputs for different paraphrases that are automatically generated from a paraphrasing model"

### Mechanism 3
- Claim: A small candidate set can contain the "deleted" answer with non-negligible probability
- Mechanism: Under the threat model, an attack succeeds if the correct answer appears among B candidates. Even with editing, the "deleted" answer appears among the top candidates frequently enough that with a small B (e.g., 20), the attack succeeds at rates up to 38% for whitebox and 29% for blackbox attacks.
- Core assumption: The threat model of "insecure if answer is among B candidates" is realistic for certain attack scenarios
- Evidence anchors:
  - [section] "Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates"

## Foundational Learning

- **Concept: Logit lens technique for interpreting intermediate transformer representations**
  - Why needed here: The attack methods rely on extracting information from intermediate hidden states using the logit lens, which projects these states onto the vocabulary distribution
  - Quick check question: How does the logit lens convert a hidden state from layer ℓ into a probability distribution over the vocabulary?

- **Concept: Transformer architecture and MLP layers**
  - Why needed here: The editing methods (ROME and MEMIT) modify specific MLP weight matrices in the transformer, and understanding this is crucial for implementing both attacks and defenses
  - Quick check question: Which weight matrix in the MLP layer is typically modified by ROME, and what constraint is placed on this modification?

- **Concept: Representation probing and adversarial attacks**
  - Why needed here: The paper frames sensitive information deletion as an adversarial attack/defense problem, requiring understanding of how to measure attack success and design defenses
  - Quick check question: What is the formal definition of attack success used in this paper, and how does it differ from traditional single-shot extraction attacks?

## Architecture Onboarding

- **Component map**: Base LLM (GPT-J, Llama-2, or GPT2-XL) -> Editing Method (ROME or MEMIT with objectives) -> Attack/Defense Framework (evaluates edited model)
- **Critical path**: (1) Load base LLM, (2) Apply editing method with specified objective, (3) Run attack methods to measure success rate, (4) Evaluate model damage using ∆-Acc metrics, (5) Compare against baselines
- **Design tradeoffs**: Editing methods must balance removing specific information while minimizing damage to overall model knowledge. The paper shows that aggressive editing increases attack success reduction but also increases model damage.
- **Failure signatures**: High attack success rates (>30%) indicate the editing method failed to properly delete information. High ∆-Acc scores indicate excessive damage to model knowledge. If defense methods work well against one attack type but poorly against another, this suggests the defense is attack-specific rather than general.
- **First 3 experiments**:
  1. Test the base editing method (e.g., ROME with Empty Response) against all three attack types to establish baseline vulnerability
  2. Implement and test the Max-Entropy defense against whitebox attacks to verify it reduces attack success while maintaining low ∆-Acc
  3. Test the Input Rephrasing defense against the blackbox attack to confirm it does not improve over baseline (as the paper found)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the effectiveness of the Max-Entropy defense against blackbox attacks be improved beyond the current 28-29% attack success rate without significantly damaging model knowledge?
- Basis in paper: The paper shows that while the Max-Entropy defense reduces whitebox attack success to 2.4%, blackbox attack success remains high at 28% for CounterFact and 19% for zsRE.
- Why unresolved: The paper notes that the defense methods are still inadequate against blackbox attacks, but doesn't explore whether more aggressive defense parameters or alternative defense strategies could improve this without harming model performance.
- What evidence would resolve it: Experiments showing that a modified version of the Max-Entropy defense or an entirely different defense method can reduce blackbox attack success below 20% while keeping Random and Neighborhood ∆-Acc scores below 1%.

### Open Question 2
- Question: Would spreading model edits across more layers (beyond layers 5-7 in MEMIT) further reduce vulnerability to attacks, or does there exist an optimal layer distribution for balancing edit effectiveness and attack resistance?
- Basis in paper: The paper observes that MEMIT (which updates multiple layers) has higher attack success rates than ROME (which updates a single layer), suggesting that layer distribution affects vulnerability.
- Why unresolved: The paper only compares ROME (single layer) to MEMIT (layers 5-7), without exploring intermediate layer distributions or testing MEMIT with different layer ranges.
- What evidence would resolve it: Comparative experiments showing attack success rates for MEMIT variants that update layers 3-9, 2-8, or other distributions, demonstrating whether there's an optimal layer range that balances edit effectiveness with attack resistance.

### Open Question 3
- Question: How do the proposed attack methods perform on models that have been trained with privacy-preserving techniques during pretraining, rather than only on models edited post-hoc with ROME or MEMIT?
- Basis in paper: All experiments use models that were pretrained normally and then edited to remove information, rather than models that were trained with techniques designed to minimize memorization of sensitive information.
- Why unresolved: The paper focuses on deletion of information from already-trained models but doesn't investigate whether models trained with privacy-preserving objectives from the start would be more resistant to the proposed attacks.
- What evidence would resolve it: Experiments applying the same attack methods to models trained with differential privacy, gradient clipping, or other privacy-preserving techniques, comparing attack success rates to those obtained on normally trained models.

## Limitations

- Threat model assumes attack succeeds if answer appears among B candidates, which may not reflect real-world scenarios requiring exact answers
- Experiments limited to factual knowledge datasets and relatively small transformer models, limiting generalizability
- Hyperparameter values not fully specified, making it difficult to assess sensitivity of results to parameter choices

## Confidence

**High Confidence**:
- Model editing methods leave residual information in intermediate hidden states that can be extracted using logit lens
- Paraphrased versions of edited prompts can still elicit the "deleted" information
- State-of-the-art defenses (Max-Entropy, Head Projection) show promise but are not universally effective

**Medium Confidence**:
- The 38% whitebox and 29% blackbox attack success rates represent upper bounds for practical attacks
- The ∆-Acc metric adequately captures damage to model knowledge
- The defense methods would generalize to larger models or different architectures

**Low Confidence**:
- The specific threat scenarios (B=20 candidates) are representative of real-world attack scenarios
- The paraphrasing model's quality doesn't significantly impact blackbox attack effectiveness
- The observed attack success rates would hold across different model families and sizes

## Next Checks

1. **Scale Experiment**: Replicate the extraction attack success rates on larger models (GPT-4, Claude) and different architectures (decoder-only vs encoder-decoder) to verify the attack methods' effectiveness scales with model size and the defenses remain effective.

2. **Paraphrase Diversity Analysis**: Systematically vary the paraphrasing model quality and diversity to measure how blackbox attack success rates change. This would validate whether the observed 29% success rate is robust to paraphrasing quality variations.

3. **Real-world Scenario Simulation**: Design experiments that simulate practical attack scenarios where adversaries must extract exact answers rather than finding them among B candidates. Measure how this changes attack success rates and defense effectiveness compared to the paper's threat model.