---
ver: rpa2
title: Using Skew to Assess the Quality of GAN-generated Image Features
arxiv_id: '2310.20636'
source_url: https://arxiv.org/abs/2310.20636
tags:
- gaussian
- metric
- skew
- features
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the need for more robust evaluation of generative\
  \ models, specifically focusing on Generative Adversarial Networks (GANs). The authors\
  \ identify a limitation in the widely-used Fr\xE9chet Inception Distance (FID) metric,\
  \ which assumes Gaussian-distributed feature embeddings."
---

# Using Skew to Assess the Quality of GAN-generated Image Features

## Quick Facts
- arXiv ID: 2310.20636
- Source URL: https://arxiv.org/abs/2310.20636
- Reference count: 40
- This paper introduces Skew Inception Distance (SID) to address limitations in FID by incorporating skewness of feature distributions.

## Executive Summary
This paper addresses the need for more robust evaluation of generative models, specifically focusing on Generative Adversarial Networks (GANs). The authors identify a limitation in the widely-used Fréchet Inception Distance (FID) metric, which assumes Gaussian-distributed feature embeddings. To address this, they introduce the Skew Inception Distance (SID), a new metric that incorporates third-moment data to account for skewness in the feature distributions. SID is proven to be a pseudometric on probability distributions and extends FID by adding a skewness term. The authors demonstrate that principal component analysis (PCA) can be used to reduce the dimensionality of Inception-v3 embeddings, significantly speeding up the computation of both FID and SID without losing important information. Experiments show that SID either tracks with FID or aligns more closely with human perception when evaluating GAN-generated images, particularly in cases of subtle distortions.

## Method Summary
The authors propose Skew Inception Distance (SID) as an extension of FID that incorporates skewness of feature distributions. They use Inception-v3 or ResNet-18 to extract features from real and generated images, then apply PCA to reduce dimensionality while preserving variance. SID is computed by combining the mean, covariance, and coskewness tensor of the feature distributions, with normalization to ensure comparability across dimensions. The method is evaluated by comparing SID and FID scores on corrupted versions of ImageNet images and assessing alignment with human perception.

## Key Results
- SID is proven to be a pseudometric on probability distributions, extending FID by adding a skewness term
- PCA can reduce Inception-v3 embedding dimensionality by 87.5% (from 2048 to 256) with at most 10% difference in accuracy
- SID either tracks with FID or aligns more closely with human perception when evaluating GAN-generated images, especially under subtle distortions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FID assumes Gaussian-distributed feature embeddings, which fails to capture skewness in the data.
- Mechanism: By introducing the Skew Inception Distance (SID), which incorporates the third-moment (coskewness tensor) of the feature distributions, the evaluation metric can better account for non-Gaussian characteristics.
- Core assumption: Inception-v3 features exhibit skewness even after dimensionality reduction.
- Evidence anchors:
  - [abstract]: "However, FID has inherent limitations, mainly stemming from its assumption that feature embeddings follow a Gaussian distribution, and therefore can be defined by their first two moments."
  - [section]: "It is known that Inception-v3 features, among other classifier features, are skewed [18]."
- Break condition: If the third-moment data does not significantly differ between real and generated distributions, the additional computational cost of SID may not provide meaningful evaluation improvements.

### Mechanism 2
- Claim: Principal Component Analysis (PCA) can reduce the dimensionality of Inception-v3 embeddings without losing important information.
- Mechanism: PCA maximizes the preserved variance in the data, allowing for significant computational and memory savings while maintaining the integrity of FID and SID calculations.
- Core assumption: The variance captured by the principal components is sufficient to represent the important features for evaluation.
- Evidence anchors:
  - [section]: "PCA is a widely used dimensionality reduction method that projects the data onto a lower-dimensional subspace in such a way that maximizes the variance preserved."
  - [section]: "We investigate this question in two ways... Table 1. We see that even with significant dimensionality reduction (12.5%), there is at most a 10% difference in accuracy..."
- Break condition: If the principal components do not capture the variance relevant to the evaluation task, the reduced-dimensional features may not be representative.

### Mechanism 3
- Claim: SID extends FID by adding a skewness term, making it a pseudometric on probability distributions.
- Mechanism: By defining a metric on the space of moments of a distribution, SID can inherit the properties of a pseudometric, allowing for more robust comparisons between distributions.
- Core assumption: The family of distributions being compared can be characterized by their first three moments.
- Evidence anchors:
  - [section]: "We prove theoretical properties of SID in Section 2, in particular describing its properties as a metric."
  - [section]: "Proposition 2. Suppose P defines a family of distributions that are defined by their first three moments... Then, SFD defines a proper metric."
- Break condition: If the distributions cannot be fully characterized by their first three moments, SID may not distinguish between distributions that differ in higher-order moments.

## Foundational Learning

- Concept: Probability distributions and their moments
  - Why needed here: Understanding how moments characterize distributions is crucial for extending FID to include skewness.
  - Quick check question: Can you explain the difference between the first, second, and third moments of a distribution?

- Concept: Fréchet Inception Distance (FID)
  - Why needed here: FID is the baseline metric that SID aims to improve upon by incorporating skewness.
  - Quick check question: What assumptions does FID make about the feature distributions, and why might these be limiting?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to reduce the dimensionality of the feature embeddings to make computation feasible.
  - Quick check question: How does PCA maximize the preserved variance, and why is this important for maintaining important information in the features?

## Architecture Onboarding

- Component map: Feature Extraction -> Dimensionality Reduction -> Metric Calculation -> Evaluation
- Critical path:
  1. Extract features from real and generated images.
  2. Apply PCA to reduce feature dimensions.
  3. Calculate FID and SID for both sets of features.
  4. Compare the metrics to assess GAN performance.
- Design tradeoffs:
  - Computational Efficiency vs. Accuracy: Using PCA reduces computation time but may slightly affect metric accuracy.
  - Dimensionality vs. Information Preservation: Lower dimensions save memory but risk losing important information.
  - Skewness Inclusion vs. Complexity: Adding skewness makes SID more robust but increases computational complexity.
- Failure signatures:
  - If PCA reduces dimensions too much, the metrics may not capture important features.
  - If the skewness term is not normalized properly, it may dominate the metric.
  - If the third-moment data is not significantly different between distributions, SID may not provide additional value.
- First 3 experiments:
  1. Compare FID and SID on images with added Gaussian noise at varying levels.
  2. Test the effect of PCA dimensionality reduction on the behavior of FID and SID.
  3. Evaluate SID and FID on images with salt and pepper noise, Gaussian blur, and rectangular occlusions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Skew Inception Distance (SID) compare to other state-of-the-art GAN evaluation metrics like Kernel Inception Distance or Class-Aware Latent Distance in terms of correlation with human perception?
- Basis in paper: [inferred] The paper mentions these metrics as related work but does not compare SID to them.
- Why unresolved: The paper focuses on comparing SID to FID, but does not evaluate SID against other contemporary metrics.
- What evidence would resolve it: Conduct experiments comparing SID to other metrics using the same set of GAN models and image distortions, then compare the correlation of each metric with human judgments.

### Open Question 2
- Question: Does the choice of dimensionality reduction technique (e.g., PCA vs. random projections) significantly impact the performance of SID in evaluating GAN models?
- Basis in paper: [explicit] The paper mentions random projections as a potential alternative to PCA for dimensionality reduction but does not explore this option.
- Why unresolved: The paper only uses PCA for dimensionality reduction and does not investigate other methods.
- What evidence would resolve it: Implement SID using different dimensionality reduction techniques and compare their performance in evaluating the same set of GAN models and image distortions.

### Open Question 3
- Question: How does the Skew Inception Distance (SID) perform in evaluating other types of generative models, such as Variational Autoencoders (VAEs) or Normalizing Flows, beyond GANs?
- Basis in paper: [explicit] The paper states that SID is applicable to other generative models but does not provide experimental evidence.
- Why unresolved: The paper focuses on evaluating GANs and does not explore the application of SID to other generative models.
- What evidence would resolve it: Apply SID to evaluate VAEs and Normalizing Flows on various datasets and compare the results to established evaluation metrics for these models.

## Limitations
- The computational complexity of the coskewness tensor may be prohibitive for very large-scale evaluations, even with PCA.
- The assumption that third-moment data significantly differs between real and generated distributions may not hold universally, potentially limiting SID's added value.
- The exact impact of PCA on metric behavior across diverse GAN architectures and datasets remains uncertain.

## Confidence

- **High**: Theoretical foundation of SID as a pseudometric and its mathematical extension of FID.
- **Medium**: Empirical demonstration that SID better aligns with human perception than FID under certain distortions.
- **Medium**: Effectiveness of PCA in preserving variance without losing important evaluation-relevant information.

## Next Checks

1. **Dataset Generalization Test**: Apply SID and FID to evaluate GANs trained on diverse datasets (e.g., CIFAR-10, LSUN, CelebA) to assess metric robustness beyond ImageNet.
2. **Architecture Robustness Test**: Evaluate GANs using different architectures (e.g., StyleGAN, BigGAN, diffusion models) to determine if SID consistently outperforms FID in tracking perceptual quality.
3. **Higher-Order Moments Analysis**: Investigate the contribution of fourth and higher moments to distribution similarity, testing whether extending beyond skewness further improves evaluation accuracy.