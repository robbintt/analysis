---
ver: rpa2
title: 'AdapterFL: Adaptive Heterogeneous Federated Learning for Resource-constrained
  Mobile Computing Systems'
arxiv_id: '2311.14037'
source_url: https://arxiv.org/abs/2311.14037
tags:
- uni00000013
- learning
- heterogeneous
- block
- adapterfl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdapterFL, a novel heterogeneous federated
  learning approach that addresses resource constraints in mobile computing systems.
  AdapterFL uses a model reassembly strategy where heterogeneous models are divided
  into feature-extraction and device-adaptation blocks, then reassembled to create
  models of varying sizes.
---

# AdapterFL: Adaptive Heterogeneous Federated Learning for Resource-constrained Mobile Computing Systems

## Quick Facts
- arXiv ID: 2311.14037
- Source URL: https://arxiv.org/abs/2311.14037
- Reference count: 40
- Key outcome: Up to 12% accuracy improvement in heterogeneous federated learning across CIFAR-10, CIFAR-100, and TinyImageNet datasets

## Executive Summary
AdapterFL addresses the challenge of heterogeneous federated learning where devices with varying computational capabilities need to collaboratively train models without sharing data. The framework introduces a model reassembly strategy that partitions heterogeneous models into feature-extraction and device-adaptation blocks, then reassembles them to create models of varying sizes. This allows resource-constrained devices to train partial parameters of larger models while maintaining high accuracy. The method was evaluated across three datasets with various data distributions, showing significant improvements over state-of-the-art approaches.

## Method Summary
AdapterFL works by partitioning heterogeneous models into feature-extraction blocks and device-adaptation blocks based on functional similarity (using CKA metric). These blocks are reassembled using adapter layers to create models of different sizes that match device capabilities. The cloud server maintains a model and client selector that dispatches appropriate reassembled models to devices based on their hardware thresholds. During training, feature-extraction blocks are aggregated across all devices while device-adaptation blocks are only aggregated among models with matching structures. This block-based aggregation enables knowledge transfer between heterogeneous models while respecting device resource constraints.

## Key Results
- Achieved up to 12% accuracy improvement compared to state-of-the-art heterogeneous FL methods
- Demonstrated adaptivity across different datasets (CIFAR-10, CIFAR-100, TinyImageNet)
- Maintained scalability with varying numbers of clients and resource distributions
- Showed effectiveness across different model architectures (CNN, MobileNetV2, ResNet18)

## Why This Works (Mechanism)

### Mechanism 1
Model reassembly enables resource-constrained devices to train parts of larger models while maintaining high accuracy. The framework partitions heterogeneous models into feature-extraction blocks and device-adaptation blocks, then reassembles them with adapters. Low-performance devices train only the device-adaptation block while sharing the feature-extraction block with other devices.

Core assumption: Feature-extraction blocks from different models can share common functionality and be aggregated across heterogeneous devices.

Evidence anchors:
- [abstract] "we can generate models with varied sizes that are combined by the partial parameters of the large model with the partial parameters of the small model"
- [section] "By grafting the device-adaptation block from different heterogeneous models to the same feature-extraction block, we can generate multiple reassembled models"

### Mechanism 2
Block-based aggregation enables knowledge transfer between heterogeneous models. The cloud server aggregates feature-extraction blocks from all models and device-adaptation blocks only from models with matching structures, allowing lower blocks to share knowledge across heterogeneous architectures.

Core assumption: Shared feature-extraction blocks can extract general features that benefit all device-adaptation blocks regardless of their specific architecture.

Evidence anchors:
- [section] "the feature-extraction block can be trained by all the devices to extract the general features"
- [section] "these models contain the same feature-extraction block, which enables us to use the model aggregation methods based on building blocks"

### Mechanism 3
Adaptive model dispatching based on device capabilities optimizes resource utilization. The Model & Client Selector maintains a table of hardware resources and dispatches reassembled models only to devices that can handle their computational requirements.

Core assumption: Hardware resource thresholds can be accurately measured and mapped to appropriate model sizes.

Evidence anchors:
- [section] "the selector dispatches these reassembled models to activated clients when |mi,j| ≤ Γk"
- [section] "the cloud server contains a model & client selector, which records the model threshold Γn of client Cn"

## Foundational Learning

- Concept: Model partitioning based on functional similarity
  - Why needed here: To divide heterogeneous models into reusable blocks that can be reassembled for different resource constraints
  - Quick check question: How does the paper measure functional similarity between model layers?

- Concept: Knowledge distillation and transfer
  - Why needed here: To understand how shared feature-extraction blocks can transfer knowledge to different device-adaptation blocks
  - Quick check question: What role does the shared feature-extraction block play in enabling knowledge transfer between heterogeneous models?

- Concept: Federated learning aggregation strategies
  - Why needed here: To understand why traditional FedAvg aggregation cannot be used for heterogeneous models and how block-based aggregation solves this
  - Quick check question: Why can't we simply aggregate all heterogeneous models using traditional FedAvg?

## Architecture Onboarding

- Component map:
  - Cloud Server: Model & Client Selector -> Model Partitioner -> Model Reassembler -> Block Aggregator
  - Client Devices: Local Trainer -> Model Uploader
  - Communication: Model dispatching and uploading channels

- Critical path:
  1. Initial model generation (partition + reassembly)
  2. Device selection and model dispatching
  3. Local training and model uploading
  4. Block-based aggregation and global model update

- Design tradeoffs:
  - Model complexity vs. resource constraints: More complex models provide better accuracy but require more resources
  - Shared vs. specialized blocks: Shared feature-extraction blocks enable knowledge transfer but may not capture device-specific optimizations
  - Adapter overhead vs. flexibility: Adapters enable reassembly but add computational overhead

- Failure signatures:
  - Low accuracy: Poor feature-extraction block design or inadequate adapter alignment
  - Training instability: Mismatched block aggregation or device capability mismatches
  - Communication bottlenecks: Excessive model size or frequent model updates

- First 3 experiments:
  1. Test model partitioning on a simple CNN architecture with known layer functions
  2. Verify adapter dimension alignment works correctly for a pair of heterogeneous models
  3. Validate block-based aggregation produces expected results on a small federated dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does AdapterFL's performance scale when dealing with heterogeneous devices that have vastly different computing capabilities (e.g., orders of magnitude difference in memory and processing power)?

Basis in paper: [inferred] The paper mentions that AdapterFL addresses resource constraints by reassembling models based on device capabilities, but does not explore scenarios with extreme heterogeneity in device resources.

Why unresolved: The experiments primarily focus on three resource levels (small, medium, large) with moderate differences. There is no exploration of scenarios where devices have extreme resource disparities.

What evidence would resolve it: Experiments testing AdapterFL with devices having resource ratios spanning multiple orders of magnitude (e.g., 1:100:10000 for small:medium:large) would demonstrate scalability limits and performance degradation patterns.

### Open Question 2
What is the impact of using different similarity metrics (beyond CKA) for model partitioning in AdapterFL, and how does this affect overall federated learning performance?

Basis in paper: [explicit] The paper mentions using CKA as the similarity metric but acknowledges that other methods like CCA could be used. However, it does not compare different similarity metrics or analyze their impact on performance.

Why unresolved: The choice of similarity metric is presented as a design decision without empirical justification or comparison to alternatives. Different metrics might better capture functional similarities between heterogeneous models.

What evidence would resolve it: Comparative experiments using multiple similarity metrics (CCA, CKA variants, mutual information, etc.) across various datasets and model architectures would reveal whether CKA is optimal or if alternatives provide performance benefits.

### Open Question 3
How does AdapterFL perform in federated learning scenarios with extremely non-IID data distributions where certain classes are only present in specific device groups?

Basis in paper: [inferred] While the paper tests AdapterFL on non-IID data using Dirichlet distributions with varying β parameters, it does not explore extreme non-IID scenarios where devices have completely disjoint class distributions.

Why unresolved: The experimental non-IID settings still maintain some overlap in class distributions across devices. Real-world scenarios might involve devices with entirely different data classes, which could challenge the feature-extraction block's ability to learn general features.

What evidence would resolve it: Experiments with deliberately constructed extreme non-IID scenarios (e.g., each device group only has access to a disjoint subset of classes) would reveal AdapterFL's limitations and whether additional mechanisms are needed for such cases.

## Limitations
- Block-based aggregation mechanism for heterogeneous models lacks extensive empirical validation across diverse model families
- Hardware threshold mechanism relies on static measurements that may not capture dynamic resource constraints in real mobile environments
- Model reassembly assumes feature-extraction blocks can meaningfully share functionality across heterogeneous architectures

## Confidence

- **High confidence**: The model reassembly framework and its basic operation (partitioning, adapter alignment, block-based aggregation) are well-defined and technically sound
- **Medium confidence**: The claimed accuracy improvements are based on limited experimental scenarios with specific datasets and model architectures
- **Medium confidence**: The scalability claims across varying numbers of clients and resource distributions need broader validation

## Next Checks

1. Test adapter dimension alignment with more diverse model architectures beyond the three prototypes to verify the generalizability of the reassembly mechanism
2. Implement dynamic resource monitoring to validate whether static hardware thresholds accurately predict model execution feasibility across real mobile devices
3. Conduct ablation studies comparing block-based aggregation versus traditional FedAvg on heterogeneous models to quantify the specific contribution of the aggregation strategy