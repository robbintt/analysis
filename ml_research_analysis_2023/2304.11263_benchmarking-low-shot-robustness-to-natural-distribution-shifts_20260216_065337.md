---
ver: rpa2
title: Benchmarking Low-Shot Robustness to Natural Distribution Shifts
arxiv_id: '2304.11263'
source_url: https://arxiv.org/abs/2304.11263
tags:
- robustness
- clip
- low-shot
- regimes
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies low-shot robustness to natural distribution
  shifts, an underexplored problem compared to full-shot robustness. The authors evaluate
  various pre-trained models and robustness interventions across three datasets (ImageNet,
  iWildCam, Camelyon) and multiple low-shot regimes.
---

# Benchmarking Low-Shot Robustness to Natural Distribution Shifts

## Quick Facts
- arXiv ID: 2304.11263
- Source URL: https://arxiv.org/abs/2304.11263
- Reference count: 40
- Key outcome: This paper studies low-shot robustness to natural distribution shifts, an underexplored problem compared to full-shot robustness. The authors evaluate various pre-trained models and robustness interventions across three datasets (ImageNet, iWildCam, Camelyon) and multiple low-shot regimes. They find that self-supervised vision transformers generally outperform other architectures in low-shot regimes, but no single model or size works best across datasets. Models pre-trained on large external datasets like CLIP can be more robust on ImageNet but not necessarily on other datasets. Existing robustness interventions often fail to improve robustness in low-shot regimes, except WiSE-FT with CLIP. These findings highlight the need for further research on low-shot robustness to natural distribution shifts.

## Executive Summary
This paper investigates low-shot robustness to natural distribution shifts, a critical but underexplored problem compared to full-shot robustness. The authors evaluate various pre-trained models, architectures, and robustness interventions across three datasets (ImageNet, iWildCam, Camelyon) and multiple low-shot regimes. They find that self-supervised vision transformers generally outperform other architectures in low-shot regimes, but no single model or size works best across datasets. Models pre-trained on large external datasets like CLIP can be more robust on ImageNet but not necessarily on other datasets. Existing robustness interventions often fail to improve robustness in low-shot regimes, except WiSE-FT with CLIP. These findings highlight the need for further research on low-shot robustness to natural distribution shifts.

## Method Summary
The authors benchmark low-shot robustness to natural distribution shifts across datasets, architectures, pre-trained initializations, and robustness interventions. They use ImageNet, iWildCam, and Camelyon datasets with varying low-shot regimes (1-15k images per class) and self-supervised and supervised ViT and CNN models pre-trained on ImageNet and other datasets (e.g., CLIP, IN21k). Robustness interventions include LP-FT, WiSE-FT, Model Soups, and RobustViT. The authors fine-tune pre-trained models on low-shot ID data with linear probing or full fine-tuning, then evaluate on OOD shifts using class-specific accuracy metrics. They compute effective robustness (ρ) and relative robustness (τ) metrics to compare OOD performance beyond expected baselines given ID accuracy.

## Key Results
- Self-supervised vision transformers generally outperform other architectures in low-shot regimes, but no single model or size works best across datasets.
- Models pre-trained on large external datasets like CLIP can be more robust on ImageNet but not necessarily on other datasets.
- Existing robustness interventions often fail to improve robustness in low-shot regimes, except WiSE-FT with CLIP.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-supervised vision transformers (ViTs) generalize better to natural distribution shifts in low-shot regimes compared to supervised ViTs and CNNs.
- **Mechanism:** SSL pre-training enables models to learn more generalizable representations that are less sensitive to the idiosyncrasies of specific datasets, improving robustness when fine-tuned on limited labeled data.
- **Core assumption:** The learned representations are task-agnostic and transfer well to unseen datasets with similar but shifted distributions.
- **Evidence anchors:**
  - [abstract]: "Self-supervised vision transformers generally outperform other architectures in low-shot regimes"
  - [section]: "Self-supervised (SSL) ViTs often perform better than SSL CNNs on ImageNet and supervised ViTs and CNNs on iWildCam and Camelyon datasets"
  - [corpus]: No direct corpus evidence found for SSL ViTs improving low-shot robustness; the claim is based on experimental results in the paper.
- **Break condition:** If the target dataset is significantly different from the pre-training data or the shifts are not natural (e.g., synthetic corruptions), SSL pre-training might not offer the same advantage.

### Mechanism 2
- **Claim:** Models pre-trained on large external datasets (e.g., CLIP) can provide superior robustness on ImageNet but not necessarily on other datasets in low-shot regimes.
- **Mechanism:** Large-scale pre-training on diverse data allows the model to learn a rich set of features that are more robust to distribution shifts on the pre-training dataset (ImageNet). However, this benefit does not always transfer to datasets that are significantly different from the pre-training data.
- **Core assumption:** The large external dataset used for pre-training (e.g., CLIP's 400M image-text pairs) is diverse enough to cover the distribution shifts in the target dataset.
- **Evidence anchors:**
  - [abstract]: "Models pre-trained on large external datasets like CLIP can be more robust on ImageNet but not necessarily on other datasets"
  - [section]: "CLIP's zero-shot performance on ID and OOD shifts on ImageNet is significantly better than both CLIP and IN1k pre-trained models. However, CLIP (zero-shot or otherwise) performs worse than IN1k pre-trained models on iWildCam and Camelyon"
  - [corpus]: No direct corpus evidence found for large-scale pre-training improving low-shot robustness on specific datasets; the claim is based on experimental results in the paper.
- **Break condition:** If the target dataset is similar to the large external dataset or if the shifts are well-represented in the large external dataset, pre-training on it might offer benefits on other datasets as well.

### Mechanism 3
- **Claim:** Existing robustness interventions often fail to improve robustness in low-shot regimes, except WiSE-FT with CLIP.
- **Mechanism:** Robustness interventions like LP-FT, Model Soups, and RobustViT are designed to improve robustness when fine-tuned on large amounts of labeled data. In low-shot regimes, these interventions may not have enough data to effectively adapt the model to the distribution shifts.
- **Core assumption:** The interventions rely on the availability of sufficient labeled data to learn the distribution shifts.
- **Evidence anchors:**
  - [abstract]: "Existing robustness interventions often fail to improve robustness in low-shot regimes, except WiSE-FT with CLIP"
  - [section]: "Depending on the initialization, robustness interventions fail to improve robustness in the full-shot regime or in different low-shot regimes on such datasets"
  - [corpus]: No direct corpus evidence found for robustness interventions failing in low-shot regimes; the claim is based on experimental results in the paper.
- **Break condition:** If the interventions are specifically designed for low-shot learning or if the distribution shifts are well-represented in the limited labeled data, they might still be effective in low-shot regimes.

## Foundational Learning

- **Concept:** Natural distribution shifts
  - **Why needed here:** The paper studies robustness to natural distribution shifts, which are variations in the data distribution that occur naturally (e.g., different lighting conditions, viewpoints) rather than synthetic corruptions. Understanding these shifts is crucial for evaluating the generalization capabilities of models.
  - **Quick check question:** What is the difference between natural distribution shifts and synthetic distribution shifts?
- **Concept:** Self-supervised learning (SSL)
  - **Why needed here:** The paper compares the performance of self-supervised vision transformers with supervised vision transformers and convolutional neural networks. SSL is a type of learning where the model learns from unlabeled data, which can lead to more generalizable representations.
  - **Quick check question:** How does self-supervised learning differ from supervised learning?
- **Concept:** Fine-tuning
  - **Why needed here:** The paper studies the performance of pre-trained models when fine-tuned on limited labeled data (low-shot regimes). Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on labeled data for that task.
  - **Quick check question:** What is the difference between pre-training and fine-tuning?

## Architecture Onboarding

- **Component map:** Pre-trained models (ImageNet, CLIP, etc.) -> Fine-tuning method (linear probing, full fine-tuning) -> Classifier (logistic regression, mean-centroid, baseline++) -> Robustness interventions (LP-FT, WiSE-FT, Model Soups, RobustViT) -> Evaluation metrics (accuracy, effective robustness, relative robustness)
- **Critical path:**
  1. Select pre-trained model and fine-tuning method
  2. Fine-tune the model on the limited labeled data
  3. Apply robustness interventions (if applicable)
  4. Evaluate the model on the out-of-distribution test data using the chosen metrics
- **Design tradeoffs:**
  - Pre-trained model choice: ImageNet pre-trained models may be better for some datasets, while large external dataset pre-trained models (e.g., CLIP) may be better for others.
  - Fine-tuning method: Linear probing is faster but may not adapt the model as well as full fine-tuning. Full fine-tuning requires more data and compute.
  - Robustness interventions: Some interventions may improve robustness on one dataset but not on others. WiSE-FT with CLIP seems to be the most consistently effective.
- **Failure signatures:**
  - Low in-distribution accuracy: The model is not learning the task well on the limited labeled data.
  - Low out-of-distribution accuracy: The model is not generalizing well to the distribution shifts.
  - Negative effective or relative robustness: The robustness intervention is making the model worse on the out-of-distribution data.
- **First 3 experiments:**
  1. Fine-tune an ImageNet pre-trained vision transformer on the limited labeled data and evaluate its accuracy on the out-of-distribution test data.
  2. Apply the WiSE-FT robustness intervention with CLIP to the ImageNet pre-trained vision transformer and evaluate its accuracy on the out-of-distribution test data.
  3. Fine-tune a CLIP pre-trained vision transformer on the limited labeled data and evaluate its accuracy on the out-of-distribution test data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do self-supervised learning objectives need to be redesigned for low-shot robustness on class-imbalanced datasets like iWildCam?
- Basis in paper: [explicit] The paper notes that recent work [66] has shown current SSL objectives might not be suitable for class-imbalanced datasets, and this is identified as a limitation of the current study.
- Why unresolved: The study did not investigate the impact of different SSL objectives on low-shot robustness for class-imbalanced datasets due to resource constraints.
- What evidence would resolve it: Empirical comparison of different SSL methods (e.g., DINO, MSN, SwAV) on iWildCam and similar datasets, showing which objectives lead to better low-shot robustness performance.

### Open Question 2
- Question: Which specific data augmentation strategies and loss functions are most effective for improving low-shot robustness to natural distribution shifts?
- Basis in paper: [inferred] The paper mentions that interventions like Model Soups use various augmentations and loss functions but their individual effects on low-shot robustness remain unexplored.
- Why unresolved: The study applied interventions as a whole rather than isolating the effects of individual components due to complexity and compute constraints.
- What evidence would resolve it: Controlled experiments varying individual augmentation strategies and loss functions while keeping other factors constant, measuring their impact on low-shot robustness across multiple datasets.

### Open Question 3
- Question: Can in-domain self-supervised pre-training improve low-shot robustness on datasets other than ImageNet?
- Basis in paper: [explicit] The paper identifies the inability to observe effects of in-domain SSL pre-training on non-ImageNet datasets as a limitation.
- Why unresolved: Complex and resource-intensive training procedures for in-domain SSL pre-training were beyond the scope of this study.
- What evidence would resolve it: Comparative experiments showing performance differences between models with and without in-domain SSL pre-training on datasets like iWildCam and Camelyon, measuring both in-domain and out-of-distribution robustness.

## Limitations

- The study is limited to three specific datasets (ImageNet, iWildCam, Camelyon) and may not generalize to other domains or shift types.
- The analysis of robustness interventions is limited to four specific methods (LP-FT, WiSE-FT, Model Soups, RobustViT) and may not capture the full landscape of potential approaches.
- The study does not investigate the impact of different self-supervised learning objectives on low-shot robustness for class-imbalanced datasets.

## Confidence

- **High Confidence**: The observation that self-supervised ViTs generally outperform supervised models in low-shot regimes (based on direct experimental evidence)
- **Medium Confidence**: The claim that CLIP pre-training helps on ImageNet but not necessarily on other datasets (based on results from three datasets, but may not generalize)
- **Medium Confidence**: The finding that existing robustness interventions often fail in low-shot regimes (based on limited intervention types and specific datasets)

## Next Checks

1. Test whether the findings extend to additional robustness interventions beyond LP-FT, WiSE-FT, Model Soups, and RobustViT, particularly those designed specifically for low-shot learning scenarios.
2. Validate the results across additional natural distribution shifts beyond the three datasets studied, including synthetic distribution shifts for comparison.
3. Investigate whether the failure of robustness interventions in low-shot regimes can be mitigated through alternative fine-tuning strategies (e.g., meta-learning approaches).