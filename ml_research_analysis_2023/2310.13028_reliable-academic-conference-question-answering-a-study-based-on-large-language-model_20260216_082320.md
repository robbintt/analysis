---
ver: rpa2
title: 'Reliable Academic Conference Question Answering: A Study Based on Large Language
  Model'
arxiv_id: '2310.13028'
source_url: https://arxiv.org/abs/2310.13028
tags:
- conference
- data
- information
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConferenceQA, a dataset designed for academic
  conference question-answering using large language models (LLMs). The dataset includes
  seven conferences with nearly 100 question-answer pairs each, classified into four
  types based on difficulty.
---

# Reliable Academic Conference Question Answering: A Study Based on Large Language Model

## Quick Facts
- arXiv ID: 2310.13028
- Source URL: https://arxiv.org/abs/2310.13028
- Reference count: 40
- Primary result: Structure-aware retrieval method (STAR) improves LLM performance on conference QA with 80% EM for extraction-atomic questions and 50% EM for reasoning-complex questions

## Executive Summary
This paper introduces ConferenceQA, a dataset for academic conference question answering using large language models (LLMs), and proposes a structure-aware retrieval method (STAR) to address the limitations of LLMs in handling outdated and domain-specific knowledge. The dataset includes seven conferences with nearly 100 question-answer pairs each, classified into four types based on difficulty. STAR leverages the semi-structured JSON data of conferences by generating textual descriptions for both the path (structural context) and the value (semantic content), allowing the LLM to leverage both types of information during retrieval. Experiments demonstrate that STAR significantly improves the performance of LLMs in answering conference-related questions, with an average exact match (EM) score of 80% for extraction-atomic questions and 50% for reasoning-complex questions.

## Method Summary
The study employs a structure-aware retrieval method (STAR) that leverages semi-structured JSON data from academic conferences to improve LLM question-answering performance. The method generates textual descriptions for both the path (structural context) and the value (semantic content) in JSON data, then combines these with a weighted sum approach using cosine similarity scores. The system uses text-embedding-ada-002 for embeddings, Chroma as vector database, and ChatGPT (GPT-3.5 Turbo) as the question-answering model with top-10 entry retrieval. The ConferenceQA dataset contains seven conferences with question-answer pairs classified into four types based on extraction/reasoning and atomic/complex dimensions.

## Key Results
- Structure-aware retrieval (STAR) achieved 80% average EM score for extraction-atomic questions across all conferences
- WSum method with λ=0.6 (weighted combination of path and value similarities) provided optimal performance
- STAR significantly outperformed baseline methods including direct LLM answering and BM25 retrieval
- Reasoning-complex questions showed lower performance (50% EM) compared to extraction-atomic questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structure-aware retrieval improves performance by integrating both semantic and structural information of semi-structured data.
- Mechanism: The retrieval system generates textual descriptions for both the path (structural context) and the value (semantic content) in JSON data, allowing the LLM to leverage both types of information during retrieval.
- Core assumption: Combining structural context with semantic content provides more informative retrieval signals than using either alone.
- Evidence anchors:
  - [abstract]: "we propose a structure-aware retrieval method (STAR) that leverages the semi-structured JSON data of conferences"
  - [section 3.3]: "we have designed a structure-aware retrieval method. Specifically, we generate a text description of the path...from the root node to each node"
  - [corpus]: Weak evidence - corpus neighbors don't directly address structural retrieval methods

### Mechanism 2
- Claim: Weighted combination of path and value similarities optimizes retrieval performance.
- Mechanism: The system calculates similarity scores between the query and both path descriptions and values, then combines them using a weighted sum to determine the final retrieval ranking.
- Core assumption: Different queries benefit from different balances of structural versus semantic information, and a weighted combination can adapt to this.
- Evidence anchors:
  - [section 3.3]: "We represent the score between the query and entry as a composite score, which is calculated as: score(q,e) = λ·score(q,des_p) + (1−λ)·score(q,v)"
  - [section 4.4]: "The performance indicators for these four conferences all exhibit a trend of initial increase followed by a decrease, reaching a peak around the middle (i.e., when lambda is approximately 0.6)"
  - [corpus]: No direct evidence about weighted combination approaches

### Mechanism 3
- Claim: Multi-hop reasoning capability improves with increased extraction quantity.
- Mechanism: The retrieval system extracts more entries for complex questions, providing the LLM with more context to perform multi-hop reasoning.
- Core assumption: Complex questions requiring information from multiple entries can be better answered when more relevant entries are provided to the LLM.
- Evidence anchors:
  - [abstract]: "The potential to answer complex queries can possibly be linked to the quantity of extracted entries from the datasets"
  - [section 4.3]: "In comparison to atomic queries, the performance in responding to complex queries remains subpar. This issue may necessitate the implementation of multi-hop queries"
  - [corpus]: No evidence about multi-hop reasoning or extraction quantity effects

## Foundational Learning

- Concept: Semi-structured data representation (JSON tree structures)
  - Why needed here: The conference data is organized in JSON format with hierarchical relationships that must be preserved and leveraged during retrieval
  - Quick check question: How would you represent the relationship between "WWW2023 > Attendees > Registration > Register Fee > ACM Members > $1200" as a path in the JSON structure?

- Concept: Dense retrieval with dual encoders
  - Why needed here: The system uses embedding models to convert queries and entries into vector representations for similarity comparison
  - Quick check question: What embedding model is used in this work and what is its primary design purpose?

- Concept: Question classification based on complexity
  - Why needed here: Questions are categorized into four types (extraction-atomic, extraction-complex, reasoning-atomic, reasoning-complex) to evaluate model performance across difficulty levels
  - Quick check question: What are the two dimensions used to classify questions and how do they combine to form the four categories?

## Architecture Onboarding

- Component map: Data Processing Layer -> Description Generation Module -> Retrieval Engine -> LLM Interface -> Evaluation Pipeline
- Critical path: Query → Description Generation → Retrieval (with structure-aware scoring) → Top-k entries → LLM generation → Evaluation
- Design tradeoffs:
  - Using ChatGPT for description generation adds dependency on external API but enables flexible handling of diverse JSON structures
  - Top-k retrieval (k=10) balances information richness against computational cost and potential noise
  - Weighted sum approach requires tuning λ parameter but provides flexibility across different conference datasets

- Failure signatures:
  - Low EM scores despite high F1 scores suggest the model is finding relevant information but not generating exact matches
  - Significant performance drop when removing structural information indicates over-reliance on structure
  - Inconsistent performance across conferences suggests dataset-specific challenges rather than general model limitations

- First 3 experiments:
  1. Compare EM scores of DoE method vs baseline Entry method on WWW2023 dataset across all four question types
  2. Vary λ parameter in WSum method from 0.0 to 1.0 in 0.1 increments on ACL2023 dataset and plot EM scores
  3. Test the impact of removing sibling node information from description generation on ICDE2023 dataset performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the structure-aware retrieval method (STAR) compare to other state-of-the-art retrieval methods on academic conference question-answering tasks?
- Basis in paper: [explicit] The paper states that the proposed structure-aware retrieval method significantly improves the performance of LLMs in answering conference-related questions, with an average exact match (EM) score of 80% for extraction-atomic questions and 50% for reasoning-complex questions.
- Why unresolved: The paper does not provide a direct comparison of STAR to other state-of-the-art retrieval methods, such as those used in general question-answering tasks or those specifically designed for academic data.
- What evidence would resolve it: Conducting experiments that compare the performance of STAR to other state-of-the-art retrieval methods on the ConferenceQA dataset and reporting the results would provide a clear comparison and help determine the effectiveness of STAR relative to other methods.

### Open Question 2
- Question: How does the size and diversity of the ConferenceQA dataset impact the generalizability and robustness of the structure-aware retrieval method?
- Basis in paper: [explicit] The paper mentions that the ConferenceQA dataset includes seven conferences with nearly 100 question-answer pairs each, and that the dataset is constructed using a combination of manual and automatic methods.
- Why unresolved: The paper does not provide a detailed analysis of how the size and diversity of the ConferenceQA dataset affect the performance of the structure-aware retrieval method, nor does it discuss the potential limitations of the dataset in terms of generalizability and robustness.
- What evidence would resolve it: Conducting experiments that evaluate the performance of the structure-aware retrieval method on different subsets of the ConferenceQA dataset, such as varying the number of conferences or the types of questions, would help determine the impact of dataset size and diversity on the method's generalizability and robustness.

### Open Question 3
- Question: How can the structure-aware retrieval method be adapted to handle more complex and diverse academic conference data structures?
- Basis in paper: [explicit] The paper discusses the use of semi-structured JSON data for organizing academic conference data and the proposed structure-aware retrieval method that leverages the inherent structural information during the retrieval process.
- Why unresolved: The paper does not explore the potential challenges and limitations of applying the structure-aware retrieval method to more complex and diverse academic conference data structures, such as those with varying levels of hierarchy or different types of content.
- What evidence would resolve it: Conducting experiments that test the structure-aware retrieval method on academic conference data with varying levels of complexity and diversity, and analyzing the results to identify potential challenges and limitations, would provide insights into how the method can be adapted to handle more complex data structures.

## Limitations

- The dataset covers only seven conferences, which may not capture the full diversity of academic conference structures and question types
- Reliance on ChatGPT for description generation and GPT-4 for evaluation introduces potential variability and cost concerns
- The study does not address potential biases in the question-answer pairs or evaluate the system's performance on questions requiring external knowledge beyond the provided conference data

## Confidence

**High Confidence**: The effectiveness of structure-aware retrieval (STAR) in improving performance on extraction-atomic questions, as evidenced by consistent EM scores around 80% across multiple conferences and the clear mechanism of combining path and value similarities.

**Medium Confidence**: The claim that weighted combination of path and value similarities (WSum with λ=0.6) provides optimal performance, as the optimal λ value may vary across different conference datasets and question types, though the general trend of improved performance with structure-aware methods is robust.

**Low Confidence**: The assertion that multi-hop reasoning performance can be significantly improved by increasing extraction quantity, as this remains speculative with limited empirical evidence beyond observations about complex query performance.

## Next Checks

1. **Cross-conference generalization test**: Evaluate the trained retrieval model on a held-out conference not included in the training data to assess whether the structural patterns learned transfer across different conference formats and domains.

2. **Ablation study on description quality**: Systematically vary the quality and completeness of path descriptions (e.g., removing sibling information, simplifying hierarchical relationships) to quantify their impact on retrieval performance and identify which structural elements are most critical.

3. **Human evaluation of complex queries**: Conduct a focused human evaluation specifically on reasoning-complex questions to verify whether the automatic GPT-4 evaluation accurately captures the nuances of multi-hop reasoning performance and identify specific failure patterns.