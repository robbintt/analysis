---
ver: rpa2
title: Training Energy-Based Normalizing Flow with Score-Matching Objectives
arxiv_id: '2305.15267'
source_url: https://arxiv.org/abs/2305.15267
tags:
- training
- ebflow
- where
- flow-based
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Energy-Based Normalizing Flow (EBFlow),\
  \ a new flow-based modeling approach that connects flow-based and energy-based generative\
  \ models. EBFlow avoids the expensive computation of Jacobian determinants for linear\
  \ transformations by optimizing with score-matching objectives, reducing the training\
  \ complexity from O(D\xB3L) to O(D\xB2L) for L-layered models with D-dimensional\
  \ inputs."
---

# Training Energy-Based Normalizing Flow with Score-Matching Objectives

## Quick Facts
- arXiv ID: 2305.15267
- Source URL: https://arxiv.org/abs/2305.15267
- Reference count: 40
- Key outcome: Reduces training complexity from O(D³L) to O(D²L) by avoiding Jacobian determinant computations

## Executive Summary
This paper introduces Energy-Based Normalizing Flow (EBFlow), a novel approach that connects flow-based and energy-based generative models. EBFlow addresses the computational bottleneck of expensive Jacobian determinant calculations in traditional flow-based models by leveraging score-matching objectives. The method achieves significant speedup while maintaining or improving model performance on density estimation tasks. The authors demonstrate that EBFlow outperforms prior efficient training techniques with substantial margins on MNIST and shows improved training efficiency on CIFAR-10.

## Method Summary
EBFlow reinterprets flow-based models as energy-based models with unnormalized densities, allowing the Jacobian determinants of linear layers to be factored out as input-independent normalizing constants computed only once after training. The method employs score-matching objectives (SSM, DSM, FDSSM) to optimize the model without computing intractable normalizing constants during training. To enhance stability, EBFlow incorporates Match-after-Preprocessing (MaP) to exclude numerically sensitive layers and Exponential Moving Average (EMA) for smoother parameter updates. The approach reduces computational complexity from O(D³L) to O(D²L) for L-layered models with D-dimensional inputs.

## Key Results
- Achieves 554.2 improvement in NLL on MNIST compared to prior efficient training techniques
- Reduces training complexity from O(D³L) to O(D²L) for L-layered models
- Demonstrates significant speedup compared to maximum likelihood estimation while maintaining or improving performance
- Outperforms baseline methods on density estimation tasks with improved training efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EBFlow avoids expensive Jacobian determinant computation for linear transformations during training
- Mechanism: The method reinterprets flow-based models as energy-based models, isolating computationally expensive terms as normalizing constants Z(θ) that are calculated only once after training
- Core assumption: The Jacobian determinants of linear layers can be factored out as an input-independent normalizing constant
- Evidence anchors:
  - [abstract] "by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed"
  - [section] "we achieve the same goal through adopting the training objectives of energy-based models"
  - [corpus] Weak - no direct evidence from neighbors about this specific mechanism

### Mechanism 2
- Claim: Score-matching objectives enable more efficient training than maximum likelihood estimation
- Mechanism: Score-matching objectives avoid computing the intractable normalizing constant Z(θ) by matching gradients of log densities rather than densities themselves
- Core assumption: The Fisher divergence between true and model distributions can be minimized without explicit density evaluation
- Evidence anchors:
  - [abstract] "optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants...can be entirely bypassed"
  - [section] "score-matching methods [14–17] to optimize p(· ; θ) according to the Fisher divergence"
  - [corpus] Weak - neighbors discuss normalizing flows but not this specific score-matching efficiency advantage

### Mechanism 3
- Claim: Match-after-Preprocessing (MaP) technique improves training stability by avoiding numerically sensitive layers
- Mechanism: MaP excludes numerically sensitive layers (like logit preprocessing) from the model during training and matches pdfs of pre-processed variables instead
- Core assumption: Excluding sensitive layers from optimization while maintaining overall distribution match preserves model quality
- Evidence anchors:
  - [section] "we propose to exclude the numerically sensitive layer(s) from the model and match the pdf of the pre-processed variable during training"
  - [section] "the training process could be numerically sensitive to the derivatives of g"
  - [corpus] Weak - no direct evidence from neighbors about this specific MaP technique

## Foundational Learning

- Concept: Change of variable theorem and Jacobian determinants
  - Why needed here: Understanding how flow-based models transform distributions through invertible mappings
  - Quick check question: What is the mathematical relationship between a transformed variable's density and the original variable's density?

- Concept: Score-matching objectives and Fisher divergence
  - Why needed here: The paper's core innovation relies on optimizing models through score-matching rather than maximum likelihood
  - Quick check question: How does minimizing Fisher divergence differ from minimizing Kullback-Leibler divergence in terms of computational requirements?

- Concept: Energy-based models and Boltzmann distributions
  - Why needed here: EBFlow reinterprets flow-based models as energy-based models with unnormalized densities
  - Quick check question: What is the relationship between an energy function and the corresponding probability density in an energy-based model?

## Architecture Onboarding

- Component map:
  - Flow architecture (g) composed of linear (Sl) and non-linear (Sn) transformations
  - Energy function E(x; θ) defined as negative log of unnormalized density
  - Score-matching objective functions (LSSM, LDSM, LFDSSM) replacing maximum likelihood
  - Match-after-Preprocessing (MaP) technique for handling sensitive layers
  - Exponential Moving Average (EMA) for training stability

- Critical path:
  1. Define flow architecture with linear and non-linear components
  2. Construct energy function E(x; θ) = -log(pu(g(x; θ)) * product of non-linear Jacobian determinants)
  3. Implement score-matching objective (LSSM recommended for best performance)
  4. Apply MaP technique to exclude sensitive layers from optimization
  5. Use EMA with momentum 0.999 for parameter updates
  6. After training, compute normalizing constant Z(θ) for inference

- Design tradeoffs:
  - Linear vs non-linear transformations: Linear layers offer computational efficiency but may limit expressiveness
  - Score-matching variants: LSSM provides unbiased estimation but may be slower than LFDSSM
  - MaP application: Improves stability but requires identifying sensitive layers in the architecture
  - EMA momentum: Higher momentum provides smoother updates but slower adaptation

- Failure signatures:
  - NaN or exploding gradients: Likely indicates issues with score-matching implementation or sensitive layers
  - Poor generation quality: May indicate insufficient training iterations or suboptimal hyperparameters
  - Training instability: Could result from improper MaP application or inadequate EMA

- First 3 experiments:
  1. Implement Glow architecture with actnorm, convolutional, and affine coupling layers on MNIST
  2. Compare LML vs LSSM training on small synthetic dataset (e.g., 2D Gaussian mixture)
  3. Apply MaP technique to Glow model and verify training stability improvement on MNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the MaP technique's performance improvement and the specific architecture of the logit pre-processing layer? The paper shows MaP helps but doesn't quantify how much the numerically sensitive derivatives of logit pre-processing contribute to training instability.
- Basis in paper: [explicit] Section 5.3 discusses MaP excluding numerically sensitive layers and shows improved performance, but doesn't isolate the logit layer's contribution.
- Why unresolved: The ablation analysis shows MaP helps but doesn't decompose which architectural components benefit most from it.
- What evidence would resolve it: Controlled experiments comparing MaP with/without logit layers on different architectures, or measuring gradient norms specifically at logit layers during training.

### Open Question 2
- Question: How does EBFlow's performance scale with extremely high-dimensional data where Z(θ) cannot be explicitly calculated? The paper mentions importance sampling as a fallback but doesn't provide empirical results.
- Basis in paper: [explicit] Section 4.1 mentions importance sampling for cases where D is extremely large, but no experimental validation is provided.
- Why unresolved: The experiments focus on MNIST (D=784) and CIFAR-10 (D=3072), which are moderate dimensions where Z(θ) can be computed.
- What evidence would resolve it: Experiments on higher-dimensional datasets (e.g., ImageNet, 3D medical imaging) comparing EBFlow with importance sampling vs. other methods.

### Open Question 3
- Question: What is the theoretical trade-off between the approximation error in LFDSSM and its computational efficiency compared to LSSM and LDSM? The paper mentions LFDSSM has an error term o(ξ) but doesn't characterize its impact on final model quality.
- Basis in paper: [explicit] Section 2.2 mentions LFDSSM has an approximation error o(ξ) and shows it performs worse than LSSM/LDSM empirically, but doesn't analyze the error-quantity relationship.
- Why unresolved: The experiments show LFDSSM performs worse but don't systematically vary ξ to understand the bias-variance tradeoff.
- What evidence would resolve it: A systematic study varying ξ and measuring both computational time and NLL across different datasets to characterize the efficiency-accuracy frontier.

## Limitations
- Limited exploration of how EBFlow performs on more complex, high-dimensional datasets beyond MNIST and CIFAR-10
- No thorough analysis of potential biases introduced by the score-matching approximations
- Insufficient investigation of how different linear layer choices affect model expressiveness and performance

## Confidence
- **High confidence**: Computational efficiency gains (O(D³L) to O(D²L)) are mathematically derived and experimentally validated on MNIST and CIFAR-10
- **Medium confidence**: MaP technique improves training stability, supported by ablation study showing performance degradation without it
- **Low confidence**: Generalization to more complex datasets and architectures is not thoroughly explored

## Next Checks
1. **Architecture Expressiveness Test**: Evaluate EBFlow on a complex dataset like ImageNet or CelebA using more expressive architectures (e.g., residual flows or continuous-time flows) to assess if the computational efficiency gains scale to high-dimensional data.

2. **Bias Analysis**: Conduct controlled experiments comparing EBFlow trained with different score-matching variants (SSM, DSM, FDSSM) against exact maximum likelihood training on synthetic datasets where ground truth distributions are known, to quantify approximation biases.

3. **Linear Layer Sensitivity**: Systematically vary the types of linear transformations used (convolutional vs fully-connected, different initialization schemes) across multiple runs on CIFAR-10 to determine how linear layer choices affect both training stability and final model performance.