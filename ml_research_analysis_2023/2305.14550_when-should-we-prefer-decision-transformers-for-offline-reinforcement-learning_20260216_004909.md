---
ver: rpa2
title: When should we prefer Decision Transformers for Offline Reinforcement Learning?
arxiv_id: '2305.14550'
source_url: https://arxiv.org/abs/2305.14550
tags:
- uni00000004
- uni00000014
- uni00000049
- uni00000058
- uni00000045
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the problem of which learning paradigm to use
  for offline reinforcement learning: Q-Learning, Imitation Learning, or Sequence
  Modeling. It explores this empirically across the D4RL and Robomimic benchmarks
  using CQL, BC, and DT.'
---

# When should we prefer Decision Transformers for Offline Reinforcement Learning?

## Quick Facts
- arXiv ID: 2305.14550
- Source URL: https://arxiv.org/abs/2305.14550
- Reference count: 40
- Primary result: Decision Transformers are more robust but require more data, excelling in sparse-reward and low-quality data settings, while CQL is more sample-efficient with high-quality data

## Executive Summary
This paper provides a comprehensive empirical comparison of three offline reinforcement learning paradigms: Q-Learning (CQL), Imitation Learning (BC), and Sequence Modeling (DT). Through extensive experiments on D4RL and Robomimic benchmarks, the authors identify clear conditions under which each approach excels. The study reveals that DT offers superior robustness in challenging settings like sparse rewards and low-quality data, while CQL provides better sample efficiency with high-quality data. BC emerges as the best choice when training on high-quality human demonstrations.

## Method Summary
The authors compare Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) across multiple benchmarks. For D4RL, they use HalfCheetah, Hopper, and Walker with various data splits, while Robomimic includes Lift, Can, Square, and Transport tasks with human and machine-generated data. DT uses context lengths of 20 for D4RL and 1 for Robomimic with fewer than 2.1M parameters. The experiments evaluate performance using normalized returns for D4RL, success rates for Robomimic, and normalized scores for Atari scaling experiments, with five independent seeds for statistical significance.

## Key Results
- DT outperforms CQL in sparse-reward and low-quality data settings due to conditioning on returns-to-go
- CQL is more sample-efficient than DT when given small amounts of high-quality data
- BC outperforms both DT and CQL when trained on high-quality human-generated data
- DT requires more data than CQL to achieve competitive performance but is more robust
- Scaling data has more impact on DT performance than scaling parameters on Atari

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DT outperforms CQL in sparse-reward and low-quality data settings due to its conditioning on returns-to-go.
- Mechanism: By conditioning on returns-to-go at each timestep, DT can differentiate trajectories of varying quality without needing to propagate TD errors over long horizons, which is essential when rewards are sparse or data is suboptimal.
- Core assumption: The returns-to-go signal is informative enough to guide learning even when the reward signal itself is sparse or corrupted.
- Evidence anchors:
  - [abstract] states "DT is a substantially better choice than both CQL and BC in sparse-reward and low-quality data settings."
  - [section 4.1] explains that "sparser rewards mean CQL must propagate TD errors over more timesteps to learn effectively, while DT conditions on the returns-to-go at each state, and so is less affected by reward redistribution."
- Break condition: If returns-to-go are uninformative or the task horizon is very short, the advantage of DT may diminish.

### Mechanism 2
- Claim: CQL is more sample-efficient than DT when given a small amount of high-quality data.
- Mechanism: CQL leverages bootstrapping to estimate value functions from limited samples, allowing it to learn effectively from a small number of high-return trajectories.
- Core assumption: The behavior policy is close to optimal in the low-data regime, so bootstrapping remains stable.
- Evidence anchors:
  - [abstract] mentions "DT requires more data than CQL to learn competitive policies but is more robust."
  - [section 4.2] shows that "CQL was the most sample-efficient agent when only a small amount of high-quality data was available."
- Break condition: As data quantity increases or the behavior policy becomes more suboptimal, CQL's performance degrades due to distributional shift.

### Mechanism 3
- Claim: BC outperforms DT and CQL when trained on high-quality human-generated data.
- Mechanism: BC directly mimics the behavior policy via supervised learning, which is effective when the data comes from skilled human demonstrators who exhibit consistent, high-quality behavior.
- Core assumption: Human demonstrations are of sufficiently high quality and consistency to be effectively cloned without exploration of suboptimal actions.
- Evidence anchors:
  - [section 4.3] states "BC outperformed all other agents" when trained on human-generated data with shorter trajectories.
  - [section 4.3] notes that "Imitation Learning to work better than Q-Learning when the policy is trained on human-generated data."
- Break condition: If human data includes significant suboptimal actions or is multimodal, BC's performance will degrade sharply.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper compares algorithms within the framework of offline RL, which assumes learning from MDP transitions.
  - Quick check question: What are the key components of an MDP tuple (S, A, T, R, H)?

- Concept: Temporal Difference (TD) Learning
  - Why needed here: CQL relies on TD updates for value function estimation, and understanding bootstrapping is crucial to grasping its strengths and limitations.
  - Quick check question: How does TD learning update Q-values using the Bellman equation?

- Concept: Sequence Modeling with Transformers
  - Why needed here: DT uses a Transformer architecture to model action sequences conditioned on returns-to-go, which is central to its robustness in certain settings.
  - Quick check question: What is the role of the context window in a Transformer, and how does it differ from traditional RL architectures?

## Architecture Onboarding

- Component map:
  - Dataset -> State, action, reward, next state tuples
  - Transformer Encoder -> Processes trajectory history with context length
  - Attention Mechanism -> Computes relationships across sequence elements
  - Output Layer -> Produces action distribution conditioned on returns-to-go
  - Loss -> Mean squared error between predicted and actual actions

- Critical path:
  1. Data preprocessing: Sort trajectories by returns, extract context windows
  2. Forward pass: Encode trajectory history, attend to relevant tokens
  3. Prediction: Generate action distribution conditioned on returns-to-go
  4. Loss computation: Compare predicted actions to actual actions in dataset
  5. Backward pass: Update model parameters via Adam optimizer

- Design tradeoffs:
  - Context length: Longer contexts capture more history but increase overfitting risk and computational cost
  - Attention heads: More heads improve representation learning but add parameters and training time
  - Returns-to-go conditioning: Enables learning from suboptimal data but requires careful hyperparameter tuning

- Failure signatures:
  - Overfitting: Performance degrades sharply on longer trajectories or with increased context length
  - Underfitting: Model fails to capture task structure, showing poor performance across all settings
  - Instability: Large variance in training loss or evaluation scores across seeds

- First 3 experiments:
  1. Vary context length on D4RL medium-expert data to identify optimal history window
  2. Compare DT performance with and without returns-to-go conditioning on sparse-reward tasks
  3. Test sample efficiency by training on increasing fractions of high-return trajectories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do offline RL algorithms perform in stochastic environments compared to deterministic ones?
- Basis in paper: [explicit] The authors explicitly state that their findings are based on deterministic benchmarks and that further research would be required to determine if their takeaways apply to stochastic environments.
- Why unresolved: The paper does not include any experiments in stochastic environments, making it unclear how the relative performance of CQL, BC, and DT would change under stochasticity.
- What evidence would resolve it: Empirical studies comparing CQL, BC, and DT across both deterministic and stochastic versions of the same tasks, varying the level of stochasticity.

### Open Question 2
- Question: How does the source of the data (human vs. machine-generated) impact the relative performance of offline RL algorithms?
- Basis in paper: [explicit] The authors found that BC outperformed DT and CQL on human-generated data in Robomimic, while DT performed better on machine-generated data. They also cite prior works showing imitation learning works better with human-generated data.
- Why unresolved: While the authors observed this trend, they did not provide a comprehensive study isolating the effect of data source across multiple algorithms and tasks.
- What evidence would resolve it: Systematic experiments training CQL, BC, and DT on both human and machine-generated datasets for the same tasks, controlling for other factors like data quality and quantity.

### Open Question 3
- Question: How does increasing the context length affect DT's performance on different types of tasks?
- Basis in paper: [explicit] The authors found that increasing context length helped DT on Atari but not on D4RL, suggesting that some tasks may benefit from more history than others.
- Why unresolved: The paper only experimented with a limited set of context lengths and tasks, and did not provide a clear explanation for why context length matters more for some tasks.
- What evidence would resolve it: Extensive ablation studies varying context length on a wide range of tasks, and analysis of the learned attention patterns to understand how DT uses the history.

## Limitations

- The paper does not provide detailed implementation specifics for Decision Transformer, particularly regarding architectural choices and preprocessing steps, which may affect reproducibility
- The analysis of why CQL fails on sparse-reward tasks is somewhat speculative, lacking direct experimental validation
- The comparison with human data, while insightful, is limited by the small number of tasks and potential confounding factors in data collection

## Confidence

- High confidence: Claims about DT's robustness and data efficiency requirements, as these are directly supported by extensive experimental results
- Medium confidence: Claims about CQL's performance on high-stochasticity data, as the evidence is strong but the mechanism explanation is incomplete
- Medium confidence: Claims about BC's superiority on human data, as the experimental evidence is limited to specific tasks and data collection methods

## Next Checks

1. Conduct ablation studies on DT's context length and returns-to-go conditioning to isolate their individual contributions to performance
2. Test CQL with different regularization strengths on sparse-reward tasks to better understand the failure modes and potential mitigations
3. Expand human data experiments to include multiple data collection protocols and baseline algorithms to strengthen the conclusions about BC's performance