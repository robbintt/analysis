---
ver: rpa2
title: Contextualizing the Limits of Model & Evaluation Dataset Curation on Semantic
  Similarity Classification Tasks
arxiv_id: '2311.04927'
source_url: https://arxiv.org/abs/2311.04927
tags:
- sentence
- pairs
- datasets
- these
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of binary semantic similarity
  classification tasks, focusing on the limitations of pre-trained models and open
  evaluation datasets. The study uses a variety of accessible pre-trained models on
  3 commonly used evaluation datasets and reflects on the provenance, characteristics,
  and limitations of both models and datasets.
---

# Contextualizing the Limits of Model & Evaluation Dataset Curation on Semantic Similarity Classification Tasks

## Quick Facts
- arXiv ID: 2311.04927
- Source URL: https://arxiv.org/abs/2311.04927
- Reference count: 6
- Primary result: Choice of model and dataset has more pronounced effect on semantic similarity classification performance than distance metric choice

## Executive Summary
This paper investigates the performance of binary semantic similarity classification tasks using accessible pre-trained models on three commonly used evaluation datasets (QQP, MRPC, STSB). The study systematically examines how model choice, dataset selection, and distance metrics impact classification accuracy. Through comprehensive experimentation, the research reveals that the interplay between model architecture and dataset characteristics significantly influences performance outcomes, while the choice of distance metric plays a secondary role. The findings emphasize the critical importance of understanding data provenance, curation methods, and dataset limitations when deploying semantic similarity systems in real-world applications.

## Method Summary
The study employs a systematic experimental approach using three datasets from the Huggingface GLUE collection (QQP, MRPC, STSB) and four pre-trained models (ALL-MPNET-BASE-V2, DISTILROBERTA-BASE, LaBSE, TEXT-EMBEDDING-ADA-002). Sentence pairs are encoded using each model to generate text embeddings, then distance metrics (cosine, euclidean, manhattan, TS-SS) are calculated between embedding pairs. Classification performance is evaluated using True Positive Rate (TPR), False Positive Rate (FPR), and Area Under the Curve (AUC) metrics. The study also incorporates feature analysis using Levenshtein distance, vocabulary intersection, and synset overlap to understand structural complexity patterns across datasets.

## Key Results
- Model and dataset choice have more pronounced effects on performance than distance metric selection
- Structural simplicity of sentence pairs correlates with higher semantic similarity classification accuracy
- Lack of transparency in model training data and dataset curation creates deployment risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model and dataset choice have a more pronounced effect on semantic similarity classification performance than distance metric choice.
- Mechanism: The paper's experimental design systematically varies pre-trained models and evaluation datasets while holding distance metrics constant, demonstrating that performance differences stem primarily from model-dataset compatibility rather than metric selection.
- Core assumption: Distance metrics produce comparable results when applied to embeddings from different models.
- Evidence anchors:
  - [abstract]: "The results show that the choice of model and dataset has a more pronounced effect on performance than the choice of distance metric."
  - [section]: "Though the choice of distance metric produced marginal differences in score, the effect of model choice and dataset were more pronounced."
- Break condition: If certain distance metrics consistently outperform others across all model-dataset combinations, this would invalidate the claim that metric choice is secondary.

### Mechanism 2
- Claim: Structural simplicity of sentence pairs correlates with higher semantic similarity classification accuracy.
- Mechanism: Analysis of study features reveals that datasets with simpler sentence structures achieve better classification performance, while complex sentences yield lower accuracy due to ambiguous ground truth labels.
- Core assumption: Simpler sentence pairs have clearer semantic similarity boundaries that models can learn more effectively.
- Evidence anchors:
  - [abstract]: "Our analysis suggests that both QQP and STSB datasets predominantly feature sentence pairs with lower structural complexity."
  - [section]: "SHAP values suggest that fewer words and lower Levenshtein distances between sentence pairs also contribute to model predictions."
- Break condition: If complex sentence pairs with clear semantic relationships consistently achieve high accuracy, this would challenge the simplicity-performance correlation.

### Mechanism 3
- Claim: Lack of transparency in model training data and dataset curation creates deployment risks for semantic similarity systems.
- Mechanism: The paper highlights that most pre-trained models and evaluation datasets lack comprehensive documentation about training data sources, curation methods, and demographic biases.
- Core assumption: Without understanding data provenance and curation, practitioners cannot properly evaluate model limitations or potential biases.
- Evidence anchors:
  - [abstract]: "end-user-facing documentation around the curation of these datasets and pre-trained model training regimes is often not easily accessible"
  - [section]: "In part, our work in this paper to understand the origin, curation methods and contours of datasets and models serves to highlight the need to constrain conclusions about machine learning task performance based on the limits of their data"
- Break condition: If comprehensive documentation becomes standard practice across all major model and dataset providers, this mechanism's relevance would diminish.

## Foundational Learning

- Concept: Semantic similarity vs. semantic relatedness
  - Why needed here: The paper distinguishes between semantic similarity (degree of meaning likeness) and semantic relatedness (broader lexical relationships), which is crucial for understanding classification task design and evaluation.
  - Quick check question: What is the key difference between semantic similarity and semantic relatedness, and why does this distinction matter for text classification tasks?

- Concept: Distance metrics for vector comparison
  - Why needed here: The study employs multiple distance metrics (cosine, euclidean, manhattan, TS-SS) to evaluate semantic similarity, requiring understanding of how these metrics measure vector relationships differently.
  - Quick check question: How do cosine similarity and euclidean distance differ in measuring the relationship between text embeddings, and when might one be preferred over the other?

- Concept: SHAP (SHapley Additive exPlanations) values
  - Why needed here: The paper uses SHAP values to interpret feature importance in LightGBM classifiers, helping understand which structural features contribute most to similarity predictions.
  - Quick check question: What do SHAP values represent in the context of model interpretability, and how can they help identify important features for classification decisions?

## Architecture Onboarding

- Component map: Sentence pairs → Text embeddings (model) → Distance calculation → Classification → Performance metrics (AUC, TPR, FPR)
- Critical path: Sentence pairs → Text embeddings (model) → Distance calculation → Classification → Performance metrics (AUC, TPR, FPR)
- Design tradeoffs: Using pre-trained models enables rapid experimentation but introduces potential data contamination risks; binarizing continuous similarity scores simplifies evaluation but loses granularity; English-only focus limits generalizability but enables controlled comparison.
- Failure signatures: Poor AUC scores across all models suggest dataset-label quality issues; inconsistent performance across datasets indicates model-dataset mismatch; high variance in distance metric performance suggests embedding quality problems.
- First 3 experiments:
  1. Replicate baseline results using ALL-MPNET-BASE-V2 on QQP dataset with cosine distance to establish performance floor
  2. Compare model performance across all three datasets using identical distance metrics to identify dataset-specific patterns
  3. Test impact of distance metric choice by fixing model and dataset while varying metrics systematically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific causes of label instability in the QQP and STSB datasets, and how can they be mitigated?
- Basis in paper: [explicit] The paper mentions that the QQP dataset has an 83.5% match rate between original labels and independent reviewers, indicating label instability.
- Why unresolved: The paper does not provide a detailed analysis of the causes of label instability in these datasets or suggest methods to mitigate it.
- What evidence would resolve it: A comprehensive analysis of the labeling process, including inter-rater agreement and quality assurance procedures, would help identify the causes of label instability.

### Open Question 2
- Question: How do the characteristics of the training data for pre-trained models affect their performance on semantic similarity classification tasks?
- Basis in paper: [explicit] The paper discusses the training data for the pre-trained models, mentioning that they are largely sourced from the internet and may have a significant risk of data contamination.
- Why unresolved: The paper does not provide a detailed analysis of how the specific characteristics of the training data impact the models' performance on semantic similarity classification tasks.
- What evidence would resolve it: A systematic comparison of the models' performance on semantic similarity classification tasks using different subsets of the training data would help determine the impact of training data characteristics.

### Open Question 3
- Question: How can the evaluation of semantic similarity classification models be improved to account for the limitations of both the models and the datasets?
- Basis in paper: [explicit] The paper highlights the importance of understanding the limitations of both the models and the datasets.
- Why unresolved: The paper does not provide specific recommendations for improving the evaluation process to account for the limitations of the models and datasets.
- What evidence would resolve it: Developing and testing new evaluation metrics or methodologies that take into account the limitations of the models and datasets would help improve the evaluation process.

## Limitations

- Experimental scope limited to English-language datasets and pre-trained models
- Reliance on binarized labels from originally continuous similarity scores may obscure nuanced performance patterns
- Focus on model and dataset selection without fully exploring potential interactions with prompt engineering or fine-tuning strategies

## Confidence

- **High Confidence**: The finding that model and dataset choice have more pronounced effects than distance metric selection is well-supported by systematic experimentation.
- **Medium Confidence**: The correlation between structural simplicity and classification accuracy is supported by feature analysis but requires further validation.
- **Low Confidence**: Claims about deployment risks due to lack of transparency in model training and dataset curation are based on observation rather than systematic investigation.

## Next Checks

1. Replicate with additional datasets: Test the model-dataset performance patterns on other semantic similarity datasets (e.g., SICK, STS-B) to validate generalizability beyond the three examined datasets.

2. Multilingual extension: Evaluate the same models and datasets across multiple languages to assess whether the observed performance patterns hold for non-English text pairs.

3. Fine-tuning impact analysis: Investigate how fine-tuning pre-trained models on specific datasets affects the relative importance of model versus dataset selection in determining classification performance.