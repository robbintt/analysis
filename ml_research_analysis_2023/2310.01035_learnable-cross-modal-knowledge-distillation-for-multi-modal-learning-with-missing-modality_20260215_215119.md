---
ver: rpa2
title: Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with
  Missing Modality
arxiv_id: '2310.01035'
source_url: https://arxiv.org/abs/2310.01035
tags:
- modalities
- segmentation
- knowledge
- missing
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of missing modalities in multi-modal
  learning, which is common in real-world scenarios. The proposed Learnable Cross-modal
  Knowledge Distillation (LCKD) model automatically identifies important modalities
  and distills knowledge from them to help other modalities for solving the missing
  modality issue.
---

# Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality

## Quick Facts
- arXiv ID: 2310.01035
- Source URL: https://arxiv.org/abs/2310.01035
- Reference count: 38
- This paper addresses the problem of missing modalities in multi-modal learning, which is common in real-world scenarios.

## Executive Summary
This paper proposes Learnable Cross-modal Knowledge Distillation (LCKD), a novel approach for handling missing modalities in multi-modal learning tasks. The method automatically identifies the most important modalities through a teacher election procedure and performs cross-modal knowledge distillation to transfer knowledge between modalities. LCKD is specifically designed for medical image segmentation tasks where certain modalities may be missing during training or inference. The approach is evaluated on the Brain Tumour Segmentation Dataset 2018 (BraTS2018) and demonstrates significant improvements over existing methods, achieving state-of-the-art performance with improvements of 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of segmentation Dice score.

## Method Summary
LCKD addresses missing modalities through a three-stage approach: teacher election, cross-modal knowledge distillation, and missing modality feature generation. During teacher election, each modality is validated separately on each task to identify the best-performing modality for that task. Cross-modal knowledge distillation then aligns features between teacher and student modalities using pairwise L1 loss. When modalities are missing during inference, their features are approximated by averaging features from available modalities. The method uses a 3D UNet backbone with CKD at the bottom stage, trained with random modality dropping and L1 loss for knowledge distillation.

## Key Results
- LCKD improves state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of segmentation Dice score on BraTS2018
- The approach outperforms existing methods including U-HeMIS, U-HVED, Robust-MSeg, and mmFormer
- LCKD demonstrates robustness across all 15 possible missing modality combinations in multi-modal medical imaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teacher election based on single-modality validation performance identifies the most task-relevant modality for knowledge distillation.
- Mechanism: The model evaluates each modality's performance on a validation set for each task using the segmentation Dice score. The modality with the highest score is selected as the teacher for that task. This selection is repeated for all tasks, and the resulting teacher set contains the most informative modalities for each task.
- Core assumption: A single modality's performance on a task correlates with its ability to transfer knowledge to other modalities for that task.
- Evidence anchors:
  - [abstract] "Our approach introduces a teacher election procedure to select the most 'qualified' teachers based on their single modality performance on certain tasks."
  - [section 2.2] "t(k) = arg max i∈{1,...,N} L∑ l=1 d(F(x(i) l ; Θ ), yl)"
- Break condition: If modalities have complementary rather than hierarchical information, single-modality performance may not reflect cross-modal distillation potential.

### Mechanism 2
- Claim: Cross-modal knowledge distillation approximates features from student modalities to teacher modalities, creating a shared feature space beneficial for all tasks.
- Mechanism: The model minimizes the difference between features extracted from available modalities and their elected teacher modalities using a pairwise L1 loss. This forces the student modalities' features to align with the teacher's features, capturing the teacher's task-specific knowledge.
- Core assumption: Feature alignment between modalities preserves task-relevant information that can be transferred across modalities.
- Evidence anchors:
  - [abstract] "cross-modal knowledge distillation is performed between teacher and student modalities for each task to push the model parameters to a point that is beneficial for all tasks."
  - [section 2.3] "ℓckd(D;θenc) = N∑ i∈T;i,j/∈m ∥f (i)−f (j)∥p"
- Break condition: If modalities have fundamentally different feature distributions, direct feature alignment may not preserve meaningful information.

### Mechanism 3
- Claim: Missing modality feature generation by averaging available modalities' features provides a reasonable approximation of the missing features in the shared feature space.
- Mechanism: When a modality is missing, its features are approximated by averaging the features from all available modalities. This assumes that the cross-modal distillation has aligned features sufficiently that an average provides a reasonable proxy.
- Core assumption: After cross-modal distillation, features from different modalities occupy similar regions in the feature space, making averaging a valid approximation.
- Evidence anchors:
  - [abstract] "Features from missing modalities are generated by averaging the other modalities' features."
  - [section 2.4] "f (n) = 1 N−|m| N∑ i=1;i/∈m f (i)"
- Break condition: If modalities have highly specialized features that don't overlap significantly, averaging may produce poor approximations.

## Foundational Learning

- Concept: Multi-modal learning with missing modalities
  - Why needed here: The paper addresses scenarios where some input modalities are unavailable during training or testing, which is common in medical imaging where different scans may be missing.
  - Quick check question: What are the main challenges when training models that must handle incomplete modality sets?

- Concept: Knowledge distillation
  - Why needed here: The approach transfers knowledge from high-performing modalities (teachers) to other modalities (students) to improve performance when the best modalities are missing.
  - Quick check question: How does knowledge distillation typically work in single-modality settings, and what changes when applied cross-modally?

- Concept: Feature space alignment
  - Why needed here: Cross-modal distillation relies on bringing features from different modalities into alignment so that knowledge can transfer effectively between them.
  - Quick check question: Why is feature space alignment important for cross-modal knowledge transfer, and what could go wrong if features remain too modality-specific?

## Architecture Onboarding

- Component map:
  - Teacher Election Procedure -> Encoder -> Cross-modal Knowledge Distillation Module -> Missing Modality Feature Generator -> Decoder
- Critical path: Input → Encoder → Teacher Election (validation phase) → Cross-modal Distillation → Missing Feature Generation (if needed) → Decoder → Output
- Design tradeoffs: The approach trades computational complexity during training (multiple validation runs for teacher election) for robustness during testing. It also assumes that single-modality performance correlates with distillation potential.
- Failure signatures: Performance degradation when the teacher modality for a task is missing during testing, poor feature alignment across modalities, or when modalities have complementary rather than hierarchical information.
- First 3 experiments:
  1. Validate that teacher election correctly identifies known high-performing modalities for each task by running single-modality experiments
  2. Test cross-modal distillation by training with and without the distillation loss and comparing feature similarity metrics
  3. Verify missing modality feature generation by comparing segmentation performance when using generated features versus actual missing features (in a controlled setting where you can hide known good modalities)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the teacher election procedure in LCKD adapt to different tasks and datasets beyond brain tumor segmentation?
- Basis in paper: [explicit] The paper mentions that the teacher election procedure selects the best performing modality for each task, but it does not explore how this procedure generalizes to other tasks or datasets.
- Why unresolved: The teacher election procedure's effectiveness and generalizability across different tasks and datasets remain unexplored.
- What evidence would resolve it: Experiments on diverse datasets and tasks, comparing LCKD's performance with and without the teacher election procedure.

### Open Question 2
- Question: What is the impact of the number of modalities on LCKD's performance, and is there an optimal number of modalities for knowledge distillation?
- Basis in paper: [inferred] The paper discusses the importance of identifying important modalities but does not investigate how the number of modalities affects the performance of LCKD.
- Why unresolved: The relationship between the number of modalities and LCKD's performance is not established, and the optimal number of modalities for effective knowledge distillation is unknown.
- What evidence would resolve it: Experiments with varying numbers of modalities, analyzing the trade-off between performance and computational complexity.

### Open Question 3
- Question: How does LCKD handle the case when all modalities are missing during testing, and what are the potential limitations of this approach?
- Basis in paper: [inferred] The paper focuses on missing one or more modalities but does not address the scenario when all modalities are absent during testing.
- Why unresolved: The performance and limitations of LCKD when all modalities are missing are not discussed or evaluated.
- What evidence would resolve it: Experiments testing LCKD's performance when all modalities are missing, and a discussion of potential limitations and alternative approaches for such cases.

## Limitations
- The approach relies heavily on the assumption that single-modality performance correlates with cross-modal distillation potential, which may not hold for all datasets
- All experiments are conducted on a single dataset (BraTS2018), limiting generalizability claims
- The method assumes modalities can be meaningfully aligned through feature averaging, which may fail for highly specialized or complementary modalities

## Confidence

- **High Confidence**: The overall methodology is technically sound and well-described. The improvement over baselines is clearly demonstrated on the specific dataset.
- **Medium Confidence**: The mechanisms for cross-modal knowledge transfer and missing modality handling are plausible but rely on assumptions that need broader validation.
- **Low Confidence**: Claims about general applicability to any multi-modal setting without modality-specific modifications.

## Next Checks

1. **Cross-Dataset Validation**: Test LCKD on at least two additional multi-modal medical imaging datasets (e.g., Chest X-ray with different views, multi-modal histopathology) to assess generalization.

2. **Teacher Election Ablation**: Systematically vary the teacher election criteria (e.g., using different validation metrics, considering modality complementarity rather than single-modality performance) to test robustness.

3. **Feature Alignment Analysis**: Quantitatively measure feature space alignment before and after cross-modal distillation using techniques like t-SNE visualization and feature similarity metrics to validate the alignment assumption.