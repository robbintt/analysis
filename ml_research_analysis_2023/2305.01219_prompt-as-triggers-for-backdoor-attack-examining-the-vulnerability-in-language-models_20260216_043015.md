---
ver: rpa2
title: 'Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language
  Models'
arxiv_id: '2305.01219'
source_url: https://arxiv.org/abs/2305.01219
tags:
- backdoor
- attack
- prompt
- language
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a clean-label backdoor attack method for
  prompt-based learning called ProAttack. It uses the prompt itself as the backdoor
  trigger, eliminating the need for external triggers while maintaining correct labeling
  of poisoned samples.
---

# Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models

## Quick Facts
- arXiv ID: 2305.01219
- Source URL: https://arxiv.org/abs/2305.01219
- Authors: 
- Reference count: 11
- Primary result: Clean-label backdoor attack using prompts as triggers achieves near 100% attack success while maintaining model accuracy

## Executive Summary
This paper introduces ProAttack, a novel clean-label backdoor attack method that uses prompts themselves as triggers, eliminating the need for external triggers. The attack poisons a subset of training samples with specific prompts while maintaining correct labeling, then trains victim models on this dataset. Extensive experiments on rich-resource and few-shot text classification tasks demonstrate ProAttack's effectiveness, achieving state-of-the-art attack success rates without external triggers. The method maintains clean accuracy while successfully inducing targeted behavior in victim models.

## Method Summary
ProAttack transforms backdoor attacks by using the prompt itself as the trigger rather than external patterns. The method poisons training samples by applying specific prompt templates that maintain semantic correctness while creating a backdoor mechanism. During training, the victim model learns to associate these poisoned prompts with target labels, creating a separate decision boundary that activates only when the specific prompt appears. The approach works across different pre-trained language models (BERT, RoBERTa, XLNet, GPT-NEO) and demonstrates effectiveness in both rich-resource and few-shot learning settings.

## Key Results
- ProAttack achieves near 100% attack success rates in rich-resource settings while maintaining clean accuracy
- In clean-label backdoor attack benchmarks, ProAttack achieves state-of-the-art attack success rates without external triggers
- t-SNE visualizations show distinct feature distributions for poisoned samples, validating the backdoor mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompts themselves can act as triggers for backdoor attacks without external manipulation
- Mechanism: The backdoor attack exploits the model's learned dependency between specific prompt templates and target labels. When poisoned samples use a designated prompt (promptp), the model learns to associate this prompt with the target label (yb), creating a backdoor that activates whenever the same prompt appears
- Core assumption: The language model learns strong associations between prompt structure and semantic meaning, allowing arbitrary prompt templates to control output
- Evidence anchors:
  - [abstract]: "Our method does not require external triggers and ensures correct labeling of poisoned samples"
  - [section 3.3]: "Our approach uses the prompt itself as the trigger, eliminating the need for additional triggers"
  - [corpus]: Weak evidence - no direct supporting papers found in the neighbor list

### Mechanism 2
- Claim: Clean-label backdoor attacks maintain model accuracy on clean data while achieving high attack success
- Mechanism: By poisoning only a subset of training samples with specific prompts while keeping their labels correct, the model maintains overall accuracy on clean data. The poisoned prompt samples create a separate decision boundary that activates only when the specific prompt appears
- Core assumption: The model can maintain multiple decision boundaries - one for normal prompts and another for the poisoned prompt - without degrading overall performance
- Evidence anchors:
  - [abstract]: "Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers"
  - [section 4.2]: "our prompt-based backdoor attack model maintains clean accuracy, resulting in an average reduction of only 0.22% compared to prompt clean accuracy"
  - [corpus]: Weak evidence - no direct supporting papers found in the neighbor list

### Mechanism 3
- Claim: The feature distribution visualization shows that poisoned prompts create distinct clusters in embedding space
- Mechanism: t-SNE visualization reveals that the victim model learns a separate feature distribution for poisoned samples, which acts as the backdoor trigger mechanism. This creates a "shortcut" where the model associates the specific prompt pattern with the target class
- Core assumption: The model's internal representations capture prompt-specific features that can be manipulated independently of semantic content
- Evidence anchors:
  - [section 4.2]: "we observe that the sample feature distribution depicted in Figure 2(a) corresponds to Figure 2(b), whereas Figure 2(c) does not correspond to the actual categories"
  - [section 4.3]: "we observe that different prompts induce the model to learn different feature distributions, which may serve as triggers for backdoor attacks"
  - [corpus]: Weak evidence - no direct supporting papers found in the neighbor list

## Foundational Learning

- Concept: Prompt-based learning paradigm
  - Why needed here: The entire attack mechanism relies on understanding how prompts bridge pre-training and fine-tuning, and how models respond to prompt engineering
  - Quick check question: How does prompt engineering differ from traditional fine-tuning, and why does this difference create vulnerabilities?

- Concept: Backdoor attack fundamentals
  - Why needed here: The paper builds on backdoor attack concepts but applies them to prompts instead of external triggers, requiring understanding of traditional attack mechanisms
  - Quick check question: What distinguishes clean-label backdoor attacks from poison-label attacks, and why is this distinction important for stealth?

- Concept: Feature visualization and interpretation (t-SNE)
  - Why needed here: The paper uses t-SNE visualizations to demonstrate how poisoned prompts create distinct feature distributions, requiring understanding of how to interpret these visualizations
  - Quick check question: What does it mean when feature distributions from normal and poisoned samples don't align with their true labels in t-SNE space?

## Architecture Onboarding

- Component map:
  - Prompt Engineering Module -> Poisoned Sample Generator -> Victim Model Trainer -> Feature Visualization -> Evaluation Suite
- Critical path:
  1. Design effective prompts for the target task
  2. Generate poisoned samples using specific prompts with correct labels
  3. Train victim model on mixed clean/poisoned dataset
  4. Evaluate attack success rate on poisoned test samples
  5. Verify clean accuracy is maintained on clean samples
- Design tradeoffs:
  - Number of poisoned samples vs. attack success rate: More poisoned samples increase ASR but may degrade clean accuracy
  - Prompt specificity vs. stealth: More distinctive prompts create stronger backdoors but may be easier to detect
  - Model architecture choice: Different LLMs may have varying vulnerabilities to prompt-based attacks
- Failure signatures:
  - ASR remains low despite adequate poisoning
  - Clean accuracy degrades significantly on clean samples
  - t-SNE visualizations show no distinct clustering for poisoned samples
  - Attack success varies dramatically across different pre-trained models
- First 3 experiments:
  1. Baseline test: Train normal model on clean data and measure NCA
  2. Prompt test: Train prompt model on clean prompt data and measure PCA
  3. Poison test: Train victim model on poisoned data and measure both CA and ASR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the feature distributions of the victim model differ from the prompt model in the few-shot settings?
- Basis in paper: [explicit] The paper mentions that in the few-shot settings, the feature distribution of the victim model differs from that of the prompt model, with the number of additional feature distributions often equaling the number of poisoned samples.
- Why unresolved: The paper provides a qualitative description of the difference in feature distributions but does not offer a quantitative measure or analysis of how these distributions differ.
- What evidence would resolve it: A detailed quantitative analysis of the feature distributions, including metrics such as the Jensen-Shannon divergence or the Wasserstein distance between the distributions of the prompt and victim models, would provide a clearer understanding of how they differ.

### Open Question 2
- Question: How does the performance of the backdoor attack change when using different prompts across various pre-trained language models?
- Basis in paper: [explicit] The paper discusses the use of different prompts in the ProAttack method and mentions that the feature distributions induced by different prompts may serve as triggers for backdoor attacks.
- Why unresolved: While the paper demonstrates the effectiveness of the attack using specific prompts, it does not explore the variability in attack performance when different prompts are used or how the choice of prompt interacts with the characteristics of different pre-trained language models.
- What evidence would resolve it: Systematic experiments comparing the attack success rates and clean accuracy when using a variety of prompts across different pre-trained language models would provide insights into the impact of prompt choice on attack performance.

### Open Question 3
- Question: What are the potential defense mechanisms against prompt-based backdoor attacks?
- Basis in paper: [inferred] The paper discusses the stealthy nature of the ProAttack method and its effectiveness in clean-label backdoor attacks, implying a need for effective defense mechanisms.
- Why unresolved: The paper does not propose or evaluate any defense mechanisms against prompt-based backdoor attacks, focusing instead on the attack methodology.
- What evidence would resolve it: Research into and evaluation of potential defense strategies, such as anomaly detection in feature distributions, analysis of prompt variations, or adversarial training, would provide a comprehensive understanding of how to mitigate the risk of prompt-based backdoor attacks.

## Limitations
- Attack effectiveness depends heavily on specific prompt templates chosen, limiting generalizability across different domains
- Clean-label assumption requires correct labeling of poisoned samples, which may not always be practical or verifiable
- t-SNE visualizations provide suggestive but not definitive proof of backdoor mechanisms without more rigorous interpretability analysis

## Confidence
**High Confidence**: The core mechanism of using prompts as triggers is well-supported by experimental results showing consistent attack success across multiple datasets and models. The distinction between NCA, PCA, CA, and ASR metrics is clearly defined and properly measured.

**Medium Confidence**: The claim that prompt-based attacks achieve "state-of-the-art" results in clean-label backdoor attacks is supported by comparison to existing methods, but the paper acknowledges limitations in direct comparison due to different attack settings. The visualization-based evidence for backdoor mechanisms, while suggestive, relies on interpretation of t-SNE plots.

**Low Confidence**: The generalizability of the attack across different domains and prompt styles is not thoroughly explored. The paper focuses on sentiment analysis and topic classification tasks, leaving open questions about effectiveness on other NLP tasks or with more complex prompt engineering.

## Next Checks
1. **Cross-domain robustness test**: Apply ProAttack to datasets from different domains (e.g., biomedical text, legal documents) to verify whether prompt-based triggers generalize beyond sentiment analysis and topic classification.

2. **Prompt transferability evaluation**: Test whether poisoned prompts designed for one prompt template can trigger attacks when used with different but semantically similar prompt templates, measuring attack success rate degradation.

3. **Human detectability assessment**: Conduct human evaluation studies where annotators are asked to identify poisoned samples among clean samples, measuring both accuracy and confidence levels to assess stealth effectiveness.