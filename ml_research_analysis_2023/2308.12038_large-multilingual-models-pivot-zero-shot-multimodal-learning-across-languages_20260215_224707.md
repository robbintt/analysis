---
ver: rpa2
title: Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages
arxiv_id: '2308.12038'
source_url: https://arxiv.org/abs/2308.12038
tags:
- chinese
- multimodal
- language
- image-text
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MPM, a training paradigm for large multimodal
  models in low-resource languages. MPM leverages multilingual large language models
  as a pivot to enable zero-shot multimodal learning across languages.
---

# Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages

## Quick Facts
- arXiv ID: 2308.12038
- Source URL: https://arxiv.org/abs/2308.12038
- Reference count: 40
- Large multilingual models can enable zero-shot multimodal learning across languages by using a pivot language (English) approach.

## Executive Summary
This paper introduces MPM (Multilingual Pivoting Multimodal), a training paradigm that enables zero-shot multimodal learning across languages by leveraging multilingual large language models as pivots. The approach first aligns multilingual representations using a strong multilingual LLM, then aligns visual concepts with the pivot language (English) using abundant English image-text data. This allows the resulting multimodal model to generalize to other languages without requiring native multimodal training data. The authors demonstrate this approach by developing VISCPM, a Chinese multimodal model that achieves state-of-the-art performance among open-source Chinese multimodal models, even outperforming models trained on native Chinese data.

## Method Summary
The MPM training paradigm involves three key stages: (1) aligning multilingual representations using a multilingual LLM as a pivot, (2) aligning visual concepts with the pivot language (English) using English image-text data, and (3) fine-tuning on bilingual instruction data to adapt to target languages. The approach leverages the cross-lingual alignment capability of multilingual LLMs to enable zero-shot transfer to low-resource languages. VISCPM is implemented using CPM-Bee/LLaMA as the multilingual LLM and Muffin/BEiT-3 as the visual encoder, achieving state-of-the-art performance among open-source Chinese multimodal models without using native Chinese multimodal data during pretraining.

## Key Results
- VISCPM achieves state-of-the-art performance among open-source Chinese multimodal models
- VISCPM outperforms models trained on native Chinese image-text data, validating the effectiveness of MPM
- The approach generalizes to other languages including German, French, and Spanish with promising results

## Why This Works (Mechanism)

### Mechanism 1
Visual semantics are largely language-agnostic, allowing knowledge transfer from English to low-resource languages without native multimodal data. The multilingual LLM provides close hidden representations for semantically similar text pairs across languages, enabling zero-shot transfer when visual concepts are aligned with the pivot language.

### Mechanism 2
The quasi-zero-shot phenomenon allows the model to understand questions in the target language but respond in the pivot language, which can be calibrated with minimal target language data. During instruction tuning, the multilingual LLM's similar hidden representations for instructions in different languages enable understanding, while response language calibration requires only a small amount of translated pairs.

### Mechanism 3
Cross-lingual transfer from multilingual LLM in multimodal settings enables superior performance compared to models trained on native language multimodal data. The multilingual LLM's alignment capability bridges visual and linguistic modalities across languages, and when fine-tuned on English multimodal data, the model inherits this alignment capability and generalizes to other languages without requiring native multimodal training data.

## Foundational Learning

- **Multilingual representation alignment**
  - Why needed here: The pivot mechanism relies on the multilingual LLM providing aligned representations across languages for visual grounding to work
  - Quick check question: Can you explain why a multilingual LLM is essential for the pivot mechanism to work?

- **Cross-modal alignment**
  - Why needed here: The model needs to learn how visual concepts map to linguistic representations in the pivot language before transferring to target languages
  - Quick check question: How does the model establish the connection between visual features and linguistic representations?

- **Zero-shot transfer mechanisms**
  - Why needed here: The approach claims to achieve performance comparable to or better than models trained on native language data without using that data
  - Quick check question: What conditions must be met for zero-shot transfer to work effectively in multimodal settings?

## Architecture Onboarding

- **Component map**: Multilingual LLM (CPM-Bee/LLaMA) -> Visual encoder (Muffin/BEiT-3) -> Cross-attention layer -> Image decoder (UNet for text-to-image) -> Instruction tuning pipeline
- **Critical path**: Multilingual alignment -> Multimodal pretraining on English data -> Instruction tuning with bilingual data -> Inference on target language
- **Design tradeoffs**: English-only pretraining maximizes available data but requires strong multilingual LLM; adding native language data improves performance but reduces the approach's novelty
- **Failure signatures**: Poor cross-lingual alignment manifests as garbled or incorrect responses in target language; inadequate visual grounding shows as irrelevant image generation
- **First 3 experiments**:
  1. Test multilingual LLM's cross-lingual alignment quality on semantic similarity benchmarks
  2. Evaluate zero-shot performance on a small target language dataset without any target language training
  3. Measure performance degradation when removing the multilingual pivot component

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of translated image-text pairs impact the performance of multilingual multimodal models compared to native language data? While the paper shows that translated data provides some benefit, it does not fully explore the relationship between translation quality, dataset size, and model performance across different languages and tasks.

### Open Question 2
What is the optimal balance between pivot language (English) data and native language data for training effective multilingual multimodal models? The paper demonstrates that models trained exclusively on English data can outperform those trained on native Chinese data, but does not provide a systematic analysis of how much native language data is needed to optimize performance.

### Open Question 3
How does the MPM approach generalize to low-resource languages with very limited multimodal data availability? The paper validates MPM on Chinese and extends it to six languages, but does not test on truly low-resource languages with minimal multimodal data.

## Limitations
- The multilingual pivot mechanism's effectiveness depends on the quality of cross-lingual alignment provided by the multilingual LLM, which is not thoroughly validated
- The quasi-zero-shot phenomenon lacks empirical validation and systematic evaluation of how much target language data is truly needed for calibration
- The paper does not address scenarios where even translated data might be scarce for genuinely low-resource languages

## Confidence
- **High confidence**: The core empirical results showing VISCPM outperforming native Chinese-trained models are well-documented and reproducible
- **Medium confidence**: The general framework of using multilingual LLMs as pivots for zero-shot transfer is theoretically sound and supported by related work in cross-lingual transfer learning
- **Low confidence**: The specific mechanisms by which multilingual alignment enables effective visual grounding across languages, and the quasi-zero-shot instruction-following behavior, lack direct empirical validation

## Next Checks
1. Conduct controlled experiments measuring semantic similarity preservation across languages using the pivot LLM's representations, comparing performance with and without the multilingual pivot component
2. Systematically test the quasi-zero-shot behavior by evaluating the model's understanding of instructions in various languages while measuring response language and accuracy, with controlled amounts of calibration data
3. Design experiments specifically testing whether the multilingual LLM's alignment capability effectively transfers to multimodal contexts by comparing with ablations that use monolingual models or different alignment strategies