---
ver: rpa2
title: 'Evaluate What You Can''t Evaluate: Unassessable Quality for Generated Response'
arxiv_id: '2305.14658'
source_url: https://arxiv.org/abs/2305.14658
tags:
- evaluators
- chatgpt
- examples
- responses
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs two adversarial meta-evaluation dialogue
  generation datasets KdConv-ADV and DSTC7-ADV to comprehensively evaluate the reliability
  of reference-free evaluators based on LLMs. The key challenge addressed is the insufficiency
  of LLMs to identify unreasonable responses for closed-ended examples, where there
  is a unique correct semantic response.
---

# Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response

## Quick Facts
- **arXiv ID**: 2305.14658
- **Source URL**: https://arxiv.org/abs/2305.14658
- **Reference count**: 12
- **Primary result**: Reference-free LLM evaluators struggle to identify unreasonable responses for closed-ended examples, especially when responses contradict facts.

## Executive Summary
This paper addresses the challenge of evaluating dialogue generation quality, particularly focusing on the limitations of reference-free evaluators based on large language models (LLMs). The authors construct two adversarial meta-evaluation datasets, KdConv-ADV and DSTC7-ADV, to systematically test evaluator reliability. They demonstrate that current reference-free LLM evaluators fail to identify unreasonable responses in closed-ended examples where there is a unique correct semantic response, often assigning high scores to factually incorrect responses. The paper also shows that reference-based metrics struggle with open-ended examples that have low lexical overlap with references.

## Method Summary
The paper constructs two adversarial meta-evaluation dialogue generation datasets: KdConv-ADV (from KdConv) containing open-ended and closed-ended examples with low lexical overlap, and DSTC7-ADV (from DSTC7-AVSD) containing only closed-ended examples with high lexical overlap. Multiple evaluators are used including reference-based metrics (BLEU, ROUGE, METEOR, CHRF++, BERTScore) and reference-free LLM evaluators (ChatGPT, Vicuna, ChatGLM, StableLM). The evaluation uses prompt-based instructions with dialogue history, response, and facts, comparing turn-level Pearson and Spearman correlations between evaluator scores and human judgments across four dimensions: coherence, relevance, consistency, and fluency.

## Key Results
- Reference-free LLM evaluators assign high scores to factually incorrect responses in closed-ended examples
- Reference-based metrics show poor correlation with human judgments on open-ended examples with low lexical overlap
- LLM evaluators demonstrate insufficient discriminative ability between responses of different quality levels
- Performance varies significantly across different example types and dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reference-free LLM evaluators fail to identify unreasonable responses for closed-ended examples.
- **Mechanism**: Adversarial examples are constructed where candidates are semantically inconsistent with facts (e.g., wrong birthdates for public figures). Reference-free evaluators based on LLMs assign high scores to these unreasonable responses, indicating they lack fact-checking capabilities for closed-ended examples.
- **Core assumption**: LLMs cannot reliably access or verify factual knowledge when evaluating responses without explicit external knowledge.
- **Evidence anchors**:
  - [abstract] "For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference."
  - [section] "Experimental results show that the ability of LLMs to identify unreasonable responses is insufficient."
  - [corpus] Weak - related papers discuss LLM evaluation reliability but don't directly address closed-ended factual verification.

### Mechanism 2
- **Claim**: Reference-based metrics fail for open-ended examples with low lexical overlap.
- **Mechanism**: Traditional metrics like BLEU and ROUGE measure lexical overlap between candidates and references. For open-ended examples with diverse but valid responses, low overlap leads to low scores even when responses are high-quality.
- **Core assumption**: Reference-based metrics cannot capture semantic similarity when responses use different wording.
- **Evidence anchors**:
  - [abstract] "BLEU, ROUGE and METEOR... can not evaluate multiple reasonable responses fairly, resulting in low correlation with human judgments."
  - [section] "candidates get lower scores on overlap-based metrics like BLEU" for open-ended examples.
  - [corpus] Moderate - related papers discuss limitations of reference-based metrics for diverse responses.

### Mechanism 3
- **Claim**: Reference-free evaluators show inconsistent performance across different example types.
- **Mechanism**: The paper constructs two datasets with contrasting characteristics - KdConv-ADV with low lexical overlap and DSTC7-ADV with high lexical overlap. Reference-free evaluators perform poorly on KdConv-ADV (closed-ended examples) but better on DSTC7-ADV (when knowledge is provided), revealing context-dependent reliability.
- **Core assumption**: The reliability of reference-free evaluators depends on the nature of examples and availability of supporting knowledge.
- **Evidence anchors**:
  - [section] "According to Table 5 and Table 7, reference-based evaluators show poor alignment with humans" on KdConv-ADV, while "evaluators based on LLMs enjoy advantages... achieving better human alignment" on DSTC7-ADV.
  - [corpus] Weak - related papers discuss general evaluation challenges but don't systematically compare performance across different example types.

## Foundational Learning

- **Concept**: Closed-ended vs open-ended examples
  - **Why needed here**: The paper's core argument is that current evaluation methods fail differently for these two types. Understanding this distinction is crucial for interpreting the experimental results.
  - **Quick check question**: What makes an example "closed-ended" versus "open-ended" in the context of dialogue generation evaluation?

- **Concept**: Lexical overlap metrics
  - **Why needed here**: Reference-based metrics like BLEU and ROUGE rely on lexical overlap, which the paper shows is problematic for both open-ended (low overlap despite high quality) and closed-ended (high overlap but potentially low quality) examples.
  - **Quick check question**: How do BLEU and ROUGE calculate similarity between generated responses and references?

- **Concept**: Fact verification in LLMs
  - **Why needed here**: The paper demonstrates that reference-free LLM evaluators cannot reliably verify facts in closed-ended examples, which is central to understanding their limitations.
  - **Quick check question**: What mechanisms could LLMs use to verify factual claims in generated responses?

## Architecture Onboarding

- **Component map**: Dialogue response generation -> Adversarial datasets (KdConv-ADV, DSTC7-ADV) -> Reference-based metrics (BLEU, ROUGE, METEOR, BERTScore) -> Reference-free LLM evaluators (ChatGPT, Vicuna, ChatGLM, StableLM) -> Human evaluation ground truth
- **Critical path**: For each example in adversarial datasets, generate candidate responses, run all evaluation metrics, collect human scores, compute correlation coefficients (Pearson and Spearman) between metric scores and human judgments.
- **Design tradeoffs**: The paper chooses to construct adversarial datasets rather than use existing ones to systematically test evaluator limitations. This provides controlled testing but may not capture all real-world scenarios.
- **Failure signatures**: Reference-free evaluators consistently give high scores to factually incorrect responses in closed-ended examples; reference-based metrics give low scores to semantically correct but lexically different responses in open-ended examples.
- **First 3 experiments**:
  1. Replicate the correlation analysis between human judgments and each metric on KdConv-ADV to verify the reported performance gap.
  2. Test whether providing external knowledge to LLM evaluators during evaluation improves their performance on closed-ended examples.
  3. Analyze the score distributions of LLM evaluators across different quality levels to quantify their discriminative ability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we develop evaluation methods that reliably assess closed-ended examples with unique correct semantic responses in dialogue generation?
- **Basis in paper**: [explicit] The paper constructs two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV to comprehensively evaluate the reliability of reference-free evaluators based on LLMs, focusing on closed-ended examples.
- **Why unresolved**: Current reference-free evaluators struggle to identify unreasonable responses for closed-ended examples, as shown by their performance on KdConv-ADV and DSTC7-ADV.
- **What evidence would resolve it**: Development and validation of evaluation methods that can accurately identify and assess unreasonable responses in closed-ended examples, as demonstrated by improved performance on KdConv-ADV and DSTC7-ADV.

### Open Question 2
- **Question**: How can we improve the knowledge application ability of LLMs for dialogue response evaluation?
- **Basis in paper**: [inferred] The paper highlights the limitations of LLMs in knowledge understanding and advanced application, particularly when evaluating adversarial candidates in DSTC7-ADV.
- **Why unresolved**: LLMs struggle to utilize knowledge effectively in complex scenarios and have insufficient knowledge for some examples, as shown by their performance on KdConv-ADV and DSTC7-ADV.
- **What evidence would resolve it**: Demonstration of LLMs' improved performance in evaluating adversarial candidates in DSTC7-ADV and their ability to apply knowledge effectively in complex scenarios.

### Open Question 3
- **Question**: How can we enhance the discriminative ability of LLMs for evaluating dialogue responses of different qualities?
- **Basis in paper**: [explicit] The paper shows that LLMs tend to give high scores for both reasonable and unreasonable responses, indicating insufficient discrimination between candidates of different qualities.
- **Why unresolved**: LLMs' score distribution is biased towards high scores, making it difficult to distinguish between responses of varying quality, as demonstrated by their performance on KdConv-ADV.
- **What evidence would resolve it**: Development and validation of methods that improve LLMs' discriminative ability, resulting in a more balanced score distribution and better distinction between responses of different qualities, as shown by improved performance on KdConv-ADV.

## Limitations

- The conclusions are based on carefully constructed adversarial datasets that may not fully represent real-world dialogue generation scenarios
- The performance gap observed could be partly due to the specific construction methodology rather than fundamental limitations of LLM evaluators
- The study focuses on English language evaluation, and results may not generalize to other languages or cultural contexts

## Confidence

**High confidence**: Reference-free LLM evaluators struggle with fact verification in closed-ended examples. The experimental evidence showing high scores assigned to factually incorrect responses is robust and directly addresses the core mechanism.

**Medium confidence**: Reference-based metrics fail for open-ended examples with diverse valid responses. While the theoretical argument is sound, the experimental validation relies on the specific adversarial construction, and real-world data might show different patterns.

**Medium confidence**: Reference-free evaluators show inconsistent performance across example types. The comparison between KdConv-ADV and DSTC7-ADV provides evidence, but the datasets differ in multiple dimensions beyond just lexical overlap and example type.

## Next Checks

1. **Cross-linguistic validation**: Replicate the evaluation framework on non-English dialogue datasets to assess whether the observed limitations of LLM evaluators generalize across languages and cultural contexts.

2. **Real-world scenario testing**: Apply the evaluation framework to naturally occurring dialogue responses (rather than adversarially constructed examples) to determine if the performance gaps persist in practical settings.

3. **Knowledge integration impact**: Systematically test whether providing external knowledge bases or fact-checking tools to LLM evaluators during the evaluation process significantly improves their performance on closed-ended examples with factual requirements.