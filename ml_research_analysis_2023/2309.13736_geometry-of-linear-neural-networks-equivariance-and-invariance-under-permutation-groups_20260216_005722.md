---
ver: rpa2
title: 'Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation
  Groups'
arxiv_id: '2309.13736'
source_url: https://arxiv.org/abs/2309.13736
tags:
- matrix
- linear
- matrices
- permutation
- invariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates linear neural networks from an algebraic
  geometry perspective, focusing on equivariant and invariant functions under permutation
  groups. The main problem is characterizing the subvarieties of determinantal varieties
  (representing linear neural networks) that are equivariant or invariant under permutation
  group actions.
---

# Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups

## Quick Facts
- arXiv ID: 2309.13736
- Source URL: https://arxiv.org/abs/2309.13736
- Reference count: 17
- Primary result: Characterizes invariant and equivariant subvarieties of linear autoencoders using algebraic geometry

## Executive Summary
This paper investigates linear neural networks through the lens of algebraic geometry, focusing on functions that are equivariant or invariant under permutation group actions. The authors represent linear autoencoders as determinantal varieties and characterize the subvarieties that exhibit equivariance or invariance properties. For invariance, they prove that all invariant functions can be parameterized by a single linear autoencoder with weight-sharing constraints derived from cycle decomposition. For equivariance under cyclic groups, they show the function space has multiple irreducible components, preventing parameterization by a single network.

## Method Summary
The authors use algebraic geometry techniques to analyze the function spaces of linear autoencoders, representing them as determinantal varieties. They characterize invariant subvarieties by constructing weight-sharing encoders based on permutation cycle decomposition, proving these exactly match the invariant function space. For equivariant functions under cyclic groups, they show the commutation condition translates to circulant block structure in the weight matrix, with intersections with rank constraints producing multiple irreducible components. The method includes explicit computation of algebraic properties like dimension, degree, and Euclidean distance degree for these subvarieties.

## Key Results
- Invariant functions under arbitrary permutation groups can be fully parameterized by linear autoencoders with weight-sharing constraints derived from cycle decomposition
- Equivariant functions under cyclic groups form a function space with multiple irreducible components, preventing single-network parameterization
- The Euclidean distance degree of invariant subvarieties equals that of corresponding rank-constrained determinantal varieties, enabling optimization via Eckart-Young theorem

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Invariance under permutation groups can be fully characterized for arbitrary groups and parameterized by a linear autoencoder with weight-sharing constraints.
- **Mechanism:** Any permutation can be decomposed into disjoint cycles, and invariance forces certain columns of the weight matrix to be identical. By mapping these repeated columns to a single representative and imposing weight sharing in the encoder, the function space of the autoencoder exactly matches the invariant subvariety.
- **Core assumption:** The decomposition of a permutation into disjoint cycles captures all the structure needed to enforce invariance; repeated columns under this decomposition correspond to identical transformations.
- **Evidence anchors:**
  - [abstract]: "we prove that all invariant linear functions can be parameterized by a single linear autoencoder with a weight-sharing property imposed by the cycle decomposition"
  - [section 3.3]: Proposition 3.15 explicitly constructs the weight-sharing encoder and proves the equivalence between its function space and the invariant variety.
- **Break Condition:** If the group action is not fully captured by column permutations (e.g., if it acts on both input and output simultaneously in a non-permutational way), the characterization may fail.

### Mechanism 2
- **Claim:** Equivariance under cyclic groups can be characterized, but the resulting function space is not fully parameterizable by a single autoencoder due to multiple irreducible components.
- **Mechanism:** Equivariance under a cyclic permutation σ imposes that each block of the weight matrix must be circulant. Intersecting with the determinantal variety (rank constraint) produces several irreducible components, each corresponding to a different way of distributing rank across the circulant blocks.
- **Core assumption:** The commutation condition M·P = P·M translates into block-wise circulant structure when P is block-diagonal with cyclic blocks.
- **Evidence anchors:**
  - [abstract]: "The space of rank-bounded equivariant functions has several irreducible components, so it can not be parameterized by a single network—but each irreducible component can."
  - [section 4.1.2]: Proposition 4.2 gives the correspondence between integer solutions of the rank distribution and irreducible components.
- **Break Condition:** If the rank constraint is relaxed (r = n), the variety may become irreducible, or if the group is non-cyclic, the circulant block structure no longer fully describes equivariance.

### Mechanism 3
- **Claim:** The Euclidean distance degree (ED degree) of invariant subvarieties equals that of the corresponding rank-constrained determinantal variety, enabling direct application of the Eckart–Young theorem.
- **Mechanism:** By orthogonally projecting arbitrary matrices onto the invariant subspace and reducing to a weighted Euclidean distance problem, the optimization of rank-constrained invariant matrices reduces to minimizing distance to a standard determinantal variety with appropriate column scaling.
- **Core assumption:** Weight sharing in the invariant subspace preserves the structure of the ED degree calculation, allowing reduction to the known ED degree of M_{min(r,k),m×k}.
- **Evidence anchors:**
  - [abstract]: "minimizing the squared-error loss on our invariant or equivariant networks reduces to minimizing the Euclidean distance from determinantal varieties via the Eckart–Young theorem."
  - [section 3.2]: Proposition 3.8 proves the equality of ED degrees by showing the weighted distance problem is equivalent to the unweighted one after scaling columns.
- **Break Condition:** If the weight sharing pattern is more complex than simple column repetition (e.g., involving linear combinations), the reduction may no longer hold.

## Foundational Learning

- **Concept:** Permutation group actions and cycle decomposition.
  - **Why needed here:** The entire characterization of invariance and equivariance relies on decomposing permutations into disjoint cycles to determine which columns of the weight matrix must be equal or circulant.
  - **Quick check question:** Given a permutation σ = (1 3 5)(2 4) on 5 elements, what are the sets A₁ and A₂ in the induced partition P(σ)?

- **Concept:** Algebraic varieties and determinantal varieties.
  - **Why needed here:** The function space of a linear autoencoder is a determinantal variety; understanding its dimension, degree, and singularities is crucial for characterizing subvarieties of invariant/equivariant functions.
  - **Quick check question:** What is the dimension of M_{2,3×4}, the variety of 3×4 matrices of rank at most 2?

- **Concept:** Euclidean distance degree and its relation to optimization.
  - **Why needed here:** The ED degree measures the complexity of finding the closest point in a variety to a given matrix; it directly impacts the difficulty of training invariant/equivariant networks.
  - **Quick check question:** What is the ED degree of M_{1,2×2}, the variety of 2×2 matrices of rank at most 1?

## Architecture Onboarding

- **Component map:**
  - Input layer: n-dimensional vector
  - Encoder: Linear map with weight-sharing constraints from permutation cycle decomposition
  - Bottleneck: r-dimensional representation
  - Decoder: Dense linear map
  - Loss: Squared error reduced to minimizing distance to determinantal variety

- **Critical path:**
  1. Decompose permutation into disjoint cycles → determine weight-sharing pattern
  2. Construct encoder with shared weights according to pattern
  3. Train autoencoder with squared error loss
  4. Interpret training dynamics via ED degree and singularity structure

- **Design tradeoffs:**
  - Invariance: Weight sharing reduces parameter count but imposes a rank constraint (r ≤ k)
  - Equivariance: Multiple irreducible components mean a single autoencoder cannot cover the entire function space
  - ED degree: Higher ED degree implies more complex optimization landscape; invariance reduces this complexity

- **Failure signatures:**
  - Poor convergence: May indicate rank constraint too tight or inappropriate weight-sharing pattern
  - Mode collapse: For equivariance, may indicate the autoencoder is stuck in a low-dimensional component
  - Numerical instability: Singularities in the variety can cause ill-conditioning in optimization

- **First 3 experiments:**
  1. **Invariance test:** Train an autoencoder on rotated images (σ = 90° rotation) with weight sharing enforcing invariance; verify that the learned encoder has repeated columns as predicted
  2. **ED degree verification:** For a small invariant autoencoder (n=4, r=2), compute the ED degree of the invariant subvariety and compare to the theoretical value from Proposition 3.8
  3. **Equivariance component exploration:** For cyclic equivariance (σ = (1 2 3)), construct autoencoders targeting different irreducible components by fixing the rank distribution across circulant blocks; compare reconstruction quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the parameterization of equivariant function spaces be generalized beyond cyclic groups and rank-constrained cases?
- **Basis in paper:** [explicit] The paper states "We leave the general theory for the case of equivariance for future work" and "We constructed a parameterization of E_σ^(1,9×9) as a starting point, and leave the parameterization of E_G^(r,n×n) for future work."
- **Why unresolved:** The authors only provided explicit parameterization for the rank-1 case under cyclic groups and acknowledge the complexity increases for non-cyclic groups and general rank constraints.
- **What evidence would resolve it:** A general algebraic framework or algorithm that can parameterize E_G^(r,n×n) for arbitrary permutation groups G and any rank constraint r < n.

### Open Question 2
- **Question:** What is the Euclidean Distance Degree for equivariant varieties E_σ^(r,n×n) and how does it compare to invariant varieties?
- **Basis in paper:** [explicit] The paper states "The computation of the ED degree is more intricate in the case of equivariance" and provides explicit ED degree formulas for invariant varieties I_σ^(r,m×n).
- **Why unresolved:** While the paper computes ED degrees for invariant function spaces, it leaves the equivariant case as a future research direction, noting computational complexity.
- **What evidence would resolve it:** Derivation of explicit formulas for degED(E_σ^(r,n×n)) and comparative analysis with degED(I_σ^(r,n×n)) for various σ and rank constraints.

### Open Question 3
- **Question:** How do critical points during training differ between equivariant/invariant networks and standard networks?
- **Basis in paper:** [inferred] The paper mentions "Having the geometry of the function spaces understood, one should also investigate the types of critical points during training processes and how they compare to networks without imposed equi- or invariance."
- **Why unresolved:** While the paper characterizes the geometry of function spaces, it doesn't analyze the optimization landscape or training dynamics specific to these constrained architectures.
- **What evidence would resolve it:** Empirical and theoretical analysis comparing critical point types, convergence rates, and loss landscape properties between equivariant/invariant networks and standard networks across various training scenarios.

## Limitations
- The characterization of equivariant subvarieties for non-cyclic groups remains open, with the paper only addressing cyclic permutations
- The practical implications of having multiple irreducible components for equivariant function spaces are not fully explored
- The computational complexity of finding all irreducible components for larger matrices in equivariant cases is not addressed

## Confidence

- **High confidence:** The characterization of invariant subvarieties for arbitrary permutation groups (Proposition 3.15) - the algebraic construction via cycle decomposition is rigorous and well-supported by the proofs
- **Medium confidence:** The equivalence between minimizing squared error and Euclidean distance to determinantal varieties (Proposition 3.8) - while the mathematical derivation is sound, the practical optimization implications in high-dimensional settings need further validation
- **Medium confidence:** The irreducible component structure for cyclic equivariance (Proposition 4.2) - the integer solution characterization is correct, but the computational complexity of finding all components for larger matrices is not addressed

## Next Checks

1. **Singularities verification:** For small cases (n≤4, r≤2), numerically verify the singularity conditions stated in Proposition 3.12 by computing Jacobians and checking rank deficiency at predicted singular points

2. **Irreducible component enumeration:** Implement Procedure 2.11 to diagonalize permutation matrices and enumerate all integer solutions for rank distribution in cyclic equivariance cases, comparing against the theoretical count from Proposition 4.2

3. **Optimization landscape analysis:** For an invariant autoencoder (n=3, r=2) trained on synthetic data, measure the actual number of critical points encountered during training and compare against the theoretical ED degree from Proposition 3.8