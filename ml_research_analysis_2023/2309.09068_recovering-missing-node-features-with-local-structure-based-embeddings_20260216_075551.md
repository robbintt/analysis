---
ver: rpa2
title: Recovering Missing Node Features with Local Structure-based Embeddings
arxiv_id: '2309.09068'
source_url: https://arxiv.org/abs/2309.09068
tags:
- node
- graph
- features
- graphs
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a framework to recover completely missing node features
  for graph-level tasks, where we only have access to graph topology and node features
  for a subset of graphs. Our approach learns a node embedding space that preserves
  local structural characteristics using a Graph AutoEncoder (GAE).
---

# Recovering Missing Node Features with Local Structure-based Embeddings

## Quick Facts
- arXiv ID: 2309.09068
- Source URL: https://arxiv.org/abs/2309.09068
- Authors: 
- Reference count: 0
- We propose a framework to recover completely missing node features for graph-level tasks using local structure-based embeddings

## Executive Summary
This paper introduces a novel framework for recovering completely missing node features in graph datasets where features are only available for a subset of graphs. The method leverages local structural characteristics of nodes, learning a node embedding space through a Graph AutoEncoder (GAE) that preserves these topological features. By aggregating features from structurally similar nodes across graphs with known features, the framework can accurately estimate missing node features, which can then be used for downstream graph classification tasks with improved accuracy compared to using random or structural features alone.

## Method Summary
The method works by first computing local structural features (such as degree and clustering coefficient) for all nodes across all graphs. A Graph AutoEncoder is then trained on the subset of graphs with known features to learn a node embedding space that preserves local structural characteristics. For graphs with missing features, the framework identifies the nearest graphs with the same label based on graph embeddings, then finds the nearest nodes within those graphs using node embeddings. Missing node features are estimated by aggregating features from these nearest nodes, creating a weighted average that preserves realistic feature distributions.

## Key Results
- The method accurately recovers missing node features on six graph classification benchmarks, outperforming several baselines
- Using estimated features for downstream graph classification leads to higher accuracy compared to using random features or structural features alone
- The approach demonstrates effectiveness across diverse datasets including molecular graphs and social network data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Node features can be recovered by leveraging local structural similarity between nodes across graphs.
- Mechanism: The method trains a Graph AutoEncoder (GAE) to embed nodes into a space where nodes with similar local structures are close together. Missing node features are then estimated by averaging features from structurally similar nodes in other graphs with known features.
- Core assumption: Node features are correlated with local graph structure, so nodes with similar neighborhoods will have similar features.
- Evidence anchors:
  - [abstract] "Our approach incorporates prior information from both graph topology and existing nodal values... We demonstrate an example implementation of our framework where we assume that node features depend on local graph structure."
  - [section] "Missing nodal values are estimated by aggregating known features from the most similar nodes. Similarity is measured through a node embedding space that preserves local topological features, which we train using a Graph AutoEncoder."
- Break condition: If node features are independent of local structure or if structural similarity does not correlate with feature similarity across graphs.

### Mechanism 2
- Claim: Using nearest graphs and nodes for feature aggregation improves feature estimation accuracy over random or structural features alone.
- Mechanism: The method identifies the nearest graphs (by label and embedding distance) and nearest nodes (by structural embedding) to estimate missing features, creating a weighted aggregation that preserves realistic feature distributions.
- Core assumption: Graphs with the same label and similar structure will have similar node features, and nodes with similar local structure within those graphs will have similar features.
- Evidence anchors:
  - [abstract] "In experiments on six graph classification benchmarks, we demonstrate that our method accurately recovers missing node features, outperforming several baselines."
  - [section] "We empirically show not only the accuracy of our feature estimation approach but also its value for downstream graph classification."
- Break condition: If the assumption that same-label graphs have similar features breaks down, or if structural embeddings fail to capture relevant similarity.

### Mechanism 3
- Claim: The GAE-learned embedding space captures meaningful local structural information that correlates with node features.
- Mechanism: The GAE uses graph convolutions to encode structural features (degree, clustering coefficient, etc.) into embeddings, creating a representation where similar structures map to similar embeddings.
- Core assumption: Local structural features can be effectively encoded into a low-dimensional space that preserves similarity relationships relevant to feature recovery.
- Evidence anchors:
  - [abstract] "Similarity is measured through a node embedding space that preserves local topological features, which we train using a Graph AutoEncoder."
  - [section] "We train a Graph AutoEncoder (GAE) to learn a node embedding space that preserves the local structural characteristics for each node."
- Break condition: If the GAE fails to learn meaningful embeddings or if the embedding space does not preserve the structural similarities relevant to feature recovery.

## Foundational Learning

- Concept: Graph AutoEncoders and their role in learning node embeddings
  - Why needed here: The GAE is the core component that creates the embedding space used to measure node similarity for feature recovery
  - Quick check question: How does a GAE differ from a standard autoencoder when working with graph data?

- Concept: Local structural features and their relationship to node features
  - Why needed here: The method assumes node features depend on local structure, so understanding what constitutes "local structure" is critical
  - Quick check question: What are some examples of local structural features that might correlate with node features in molecular graphs?

- Concept: Graph similarity metrics and their application in multi-graph settings
  - Why needed here: The method computes similarity between entire graphs and between nodes across graphs, requiring understanding of appropriate metrics
  - Quick check question: How would you measure similarity between two nodes from different graphs?

## Architecture Onboarding

- Component map:
  Input Graphs -> Structural Feature Extraction -> GAE Training -> Node & Graph Embeddings -> Nearest Neighbor Search -> Feature Aggregation -> Estimated Node Features

- Critical path:
  1. Extract local structural features from all graphs
  2. Train GAE on graphs with known features
  3. Generate embeddings for all graphs
  4. For each graph with missing features, find nearest graphs with same label
  5. For each node in missing-feature graph, find nearest nodes in nearest graphs
  6. Aggregate features from nearest nodes to estimate missing features

- Design tradeoffs:
  - Embedding dimension vs. computational cost
  - Number of nearest neighbors (Q and N) vs. accuracy and noise
  - Types of local structural features vs. generalization to different graph types
  - GAE architecture complexity vs. training stability

- Failure signatures:
  - Poor feature recovery when Q and N are too small (insufficient data)
  - Noisy estimates when Q and N are too large (including dissimilar graphs/nodes)
  - Biased estimates if local structural features don't correlate with node features
  - Computational inefficiency if embedding dimension is unnecessarily large

- First 3 experiments:
  1. Verify that the GAE learns meaningful embeddings by visualizing node embeddings and checking if structurally similar nodes are close
  2. Test feature recovery on a single graph with known features (hold out some features) to validate the estimation process
  3. Compare different choices of local structural features to see which types correlate best with node features for the specific dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions in the provided content.

## Limitations
- The framework assumes node features are correlated with local graph structure, which may not hold for all graph types
- Performance depends heavily on the quality of the GAE embeddings and the assumption that structurally similar nodes have similar features
- Computational overhead of computing local structural features and training a GAE could be prohibitive for very large graphs

## Confidence
- Mechanism 1 (Local structure-based feature recovery): Medium - Supported by the framework but limited empirical validation across diverse graph types
- Mechanism 2 (Nearest neighbors aggregation): Medium - Outperforms baselines but depends on dataset-specific assumptions about feature distribution
- Mechanism 3 (GAE embedding quality): Medium - GAE training is standard but effectiveness for feature recovery not thoroughly validated

## Next Checks
1. Test feature recovery on graphs where node features are known to be independent of local structure to validate the method's limitations
2. Perform ablation studies varying the number of nearest neighbors (Q and N) to identify optimal settings and failure points
3. Evaluate performance on graphs with different topological properties (scale-free, random, small-world) to assess generalizability across graph types