---
ver: rpa2
title: Comparing Reinforcement Learning and Human Learning using the Game of Hidden
  Rules
arxiv_id: '2306.17766'
source_url: https://arxiv.org/abs/2306.17766
tags:
- rules
- rule
- learning
- memory
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Game of Hidden Rules (GOHR), a novel reinforcement
  learning environment designed to study how the logical structure of learning tasks
  affects the performance of both humans and reinforcement learning (RL) algorithms.
  Unlike existing complex benchmark environments, GOHR allows for precise and systematic
  changes to task structure, enabling rigorous investigation of how task structure
  impacts learning.
---

# Comparing Reinforcement Learning and Human Learning using the Game of Hidden Rules

## Quick Facts
- arXiv ID: 2306.17766
- Source URL: https://arxiv.org/abs/2306.17766
- Reference count: 40
- Key outcome: The Game of Hidden Rules (GOHR) environment reveals that human learners show consistent performance across task structures while RL algorithms exhibit significant performance differences based on logical structure of learning tasks.

## Executive Summary
This paper introduces the Game of Hidden Rules (GOHR), a novel reinforcement learning environment designed to systematically study how the logical structure of learning tasks affects both human and RL algorithm performance. Unlike complex benchmark environments, GOHR allows precise control over task structure through a rule syntax that encodes clearly defined logical patterns. The authors compare human learners to DQN and REINFORCE algorithms on tasks with varying logical structures, finding that while humans maintain relatively consistent performance, RL algorithms show significant performance variations depending on the specific logical structure. This suggests fundamental differences in learning strategies between humans and RL algorithms, with implications for designing human-machine learning systems.

## Method Summary
The study uses the GOHR environment, a 6×6 grid game where players must learn hidden rules governing valid moves. Human participants from Amazon Mechanical Turk played 3-7 episodes per rule, with performance measured by m* (index of first move in first streak of 10+ correct moves). RL agents (DQN and REINFORCE) trained for fixed episodes using various feature maps with different memory and representation configurations, evaluated using terminal cumulative error (TCE). The experiments tested stationary rules (shape-based, position-based, color-based) and non-stationary rules (short and long sequences) to examine how task structure affects learning.

## Key Results
- Human performance remains relatively consistent across different logical structures of learning tasks
- RL algorithms show significant performance variations based on specific task structure, with additional memory improving non-stationary rule performance but worsening stationary rule performance
- No single feature map universally outperformed others across all rule types, highlighting the importance of matching representation to task structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GOHR enables systematic study of task structure because it isolates logical patterns in a controlled environment.
- Mechanism: Each hidden rule encodes a clearly defined logical pattern as the learning objective, allowing researchers to draw systematic distinctions between learning tasks.
- Core assumption: The rule syntax can express a vast space of hidden rules ranging from trivial to complex while maintaining interpretability.
- Evidence anchors:
  - [abstract]: "Unlike existing complex benchmark environments, GOHR allows for precise and systematic changes to task structure, enabling rigorous investigation of how task structure impacts learning."
  - [section]: "The GOHR is intended to address this unmet need in the space of RL environments by allowing researchers to design precise experiments investigating the impact of task structure on learning."
  - [corpus]: Found 25 related papers with average neighbor FMR=0.475, suggesting moderate relevance but no direct citations - weak corpus support for this specific mechanism.
- Break condition: If rule complexity grows beyond interpretable patterns or if the rule syntax cannot express certain logical structures needed for study.

### Mechanism 2
- Claim: The GOHR reveals differences between human and RL learning strategies by controlling for rule generality.
- Mechanism: Rules with different generality levels (more general rules offer multiple sufficient policies) show distinct performance patterns for humans versus RL algorithms, revealing different learning strategies.
- Core assumption: Humans employ both inductive and deductive reasoning while RL algorithms primarily use inductive learning.
- Evidence anchors:
  - [section]: "While greater rule generality might plausibly assist learning by offering a larger number of sufficient policies for the player to learn, it also could hinder learning by decreasing the amount of useful, negative feedback."
  - [section]: "We posit that this difference between humans and our RL players reflects important differences in their respective learning strategies."
  - [corpus]: No direct corpus evidence found for this specific mechanism - weak support.
- Break condition: If human and RL performance patterns converge or if the generality dimension doesn't capture the relevant differences in learning strategies.

### Mechanism 3
- Claim: The GOHR's featurization studies reveal how input representations affect learning performance on different task structures.
- Mechanism: Different feature maps (varying memory and board/action representations) yield different performance tradeoffs across rule types, showing the importance of matching representation to task structure.
- Core assumption: The choice of memory and representation type significantly impacts algorithm performance on stationary versus non-stationary rules.
- Evidence anchors:
  - [section]: "We found that no feature map universally outperformed the others across all tested rules; different choices of memory and board/action representation yielded different performance tradeoffs."
  - [section]: "In general, additional memory improved performance on non-stationary rules but worsened performance on stationary rules."
  - [corpus]: No direct corpus evidence for this specific mechanism - weak support.
- Break condition: If featurization effects are negligible or if all feature maps perform similarly across all rule types.

## Foundational Learning

- Concept: Markov Decision Process (MDP) framework
  - Why needed here: The GOHR is modeled as an MDP where agents learn through sequential interaction with an uncertain environment, which is the standard framework for reinforcement learning.
  - Quick check question: In the GOHR MDP, what constitutes a terminal state and what reward does the agent receive for correct versus incorrect moves?

- Concept: Feature engineering and representation learning
  - Why needed here: The paper extensively tests different feature representations (varying memory and board/action encodings) to understand how input representations affect learning performance.
  - Quick check question: How does the GOHR featurization differ between sparse and dense representations of board states and actions?

- Concept: Generalization and sufficient policies
  - Why needed here: The concept of rule generality is central to understanding how different rules permit multiple sufficient policies, and how this affects human versus RL learning.
  - Quick check question: What does it mean for one rule to "properly dominate" another rule in terms of generality, and how does this relate to the number of sufficient policies?

## Architecture Onboarding

- Component map: 6×6 grid board → game pieces with shapes/colors → buckets at corners → hidden rule engine → move evaluation → feedback to player → performance measurement (m* for humans, TCE for RL) → statistical comparison across rule variations
- Critical path: Rule specification → Board generation → Move evaluation → Feedback to player → Performance measurement (m* for humans, TCE for RL) → Statistical comparison across rule variations
- Design tradeoffs: The GOHR trades off complexity for interpretability - it's simpler than benchmark environments like Atari but allows precise control over task structure. This means less realistic scenarios but better experimental control.
- Failure signatures: Human players failing to achieve 10-correct-move streaks despite reasonable effort time, RL algorithms not converging within episode limits, featurization choices leading to poor performance on certain rule types, or statistical tests showing no significant differences when theory predicts they should exist.
- First 3 experiments:
  1. Test baseline performance on shape-based vs position-based stationary rules to establish performance differences.
  2. Compare performance on short vs long non-stationary sequences to measure sequence length effects.
  3. Test featurization impact by running same rules with different memory/representation combinations to identify optimal configurations for different rule types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do humans and RL algorithms differ in their ability to learn rules with complex logical structures that are not easily reducible to simple patterns?
- Basis in paper: [explicit] The paper shows that humans and RL algorithms respond differently to varying task structures, with humans showing more consistent performance across different logical structures compared to RL algorithms. The paper also notes that the difficulty ordering of rules is not shared between humans and RL players.
- Why unresolved: The paper does not provide a detailed analysis of how humans and RL algorithms learn complex logical structures that are not easily reducible to simple patterns. It only provides a few examples of different logical structures and their impact on performance.
- What evidence would resolve it: Further experiments comparing human and RL performance on a wider range of complex logical structures, including those that are not easily reducible to simple patterns. This could involve creating new rules in the GOHR that incorporate more complex logical structures and testing human and RL performance on these rules.

### Open Question 2
- Question: How does the amount of memory in the input featurization affect the performance of RL algorithms on different types of rules?
- Basis in paper: [explicit] The paper shows that increasing the amount of memory in the input featurization generally improves performance on non-stationary rules but worsens performance on stationary rules. It also notes that the performance gains on non-stationary rules from additional memory generally correspond to worsened performance on stationary rules.
- Why unresolved: The paper does not provide a detailed analysis of how the amount of memory in the input featurization affects the performance of RL algorithms on different types of rules. It only provides a few examples of how memory affects performance on specific rules.
- What evidence would resolve it: Further experiments testing the performance of RL algorithms with different amounts of memory on a wider range of rules, including both stationary and non-stationary rules. This could involve creating new rules in the GOHR that incorporate different types of logical structures and testing RL performance with different amounts of memory.

### Open Question 3
- Question: How do humans and RL algorithms differ in their ability to learn rules with increasing generality?
- Basis in paper: [explicit] The paper shows that RL algorithms uniformly found more general rule variants easier than their base rule counterparts, while human players appeared to find the more general forms of non-stationary rules more difficult than their base rule counterparts. The paper suggests that this difference may reflect important differences in the learning strategies of humans and RL algorithms.
- Why unresolved: The paper does not provide a detailed analysis of how humans and RL algorithms differ in their ability to learn rules with increasing generality. It only provides a few examples of how generality affects performance on specific rules.
- What evidence would resolve it: Further experiments comparing human and RL performance on a wider range of rules with increasing generality. This could involve creating new rules in the GOHR that incorporate different levels of generality and testing human and RL performance on these rules. Additionally, it could involve analyzing the learning strategies of humans and RL algorithms to understand why they differ in their ability to learn rules with increasing generality.

## Limitations

- The rule syntax may not capture all relevant logical structures needed for comprehensive comparison between human and RL learning
- The Amazon Mechanical Turk population may not represent typical human learners, limiting generalizability of results
- The study only tested two RL algorithms (DQN and REINFORCE), which may not capture the full range of RL approaches

## Confidence

- High confidence: The GOHR environment successfully isolates logical task structures and enables controlled comparisons between human and RL learners.
- Medium confidence: The observed performance differences between humans and RL on different rule structures reflect genuine differences in learning strategies rather than implementation artifacts.
- Low confidence: The specific conclusions about inductive versus deductive reasoning differences between humans and RL are definitively established.

## Next Checks

1. **Cross-population replication**: Replicate human experiments with a different participant pool (e.g., lab-based participants) to verify that Mechanical Turk results generalize.
2. **Algorithm diversity test**: Test additional RL algorithms (e.g., PPO, A3C) with varying memory architectures to determine if the observed performance patterns are algorithm-specific or general to RL approaches.
3. **Rule complexity scaling**: Systematically increase rule complexity beyond the current scope to identify at what point human and RL performance patterns converge or diverge, testing the limits of each learner type.