---
ver: rpa2
title: Artificial Neural Nets and the Representation of Human Concepts
arxiv_id: '2312.05337'
source_url: https://arxiv.org/abs/2312.05337
tags:
- concepts
- anns
- human
- learning
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the widespread belief that artificial
  neural networks (ANNs) learn and represent human concepts in individual units. The
  author examines three key assumptions: (1) ANNs work well, (2) ANNs learn human
  concepts, and (3) ANNs represent these concepts in single units.'
---

# Artificial Neural Nets and the Representation of Human Concepts

## Quick Facts
- arXiv ID: 2312.05337
- Source URL: https://arxiv.org/abs/2312.05337
- Reference count: 13
- One-line primary result: Current evidence does not strongly support the claim that ANNs store human concepts in individual units

## Executive Summary
This paper critically examines the widespread belief that artificial neural networks learn and represent human concepts in individual units. The author systematically evaluates three key assumptions: that ANNs work well, learn human concepts, and represent these concepts in single units. Through careful analysis of interpretability techniques like activation maximization and network dissection, the paper demonstrates significant limitations in current methods for proving concept representation. The author concludes that while ANNs may learn concepts, they likely do not represent them in the way commonly believed, calling for more rigorous, falsifiable hypotheses in future research.

## Method Summary
The paper employs a critical review methodology, examining existing evidence for concept representation in ANNs through analysis of interpretability techniques including activation maximization, network dissection, and TCAV. The author evaluates evidence from transfer learning and adversarial examples to understand the nature of concepts learned by ANNs. The approach involves systematic critique of current interpretability methods, identifying their limitations in demonstrating concept representation, and proposing more rigorous experimental designs for future research.

## Key Results
- Current interpretability techniques like activation maximization and network dissection have significant limitations in demonstrating concept representation
- Evidence suggests ANNs learn both human and non-human concepts, challenging the assumption that they must develop human-like concepts
- Individual units that appear to represent concepts often show weak coactivation and unclear functional roles in model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ANNs can learn human concepts but do not represent them in individual units.
- Mechanism: The paper systematically examines three assumptions underlying the narrative that ANNs develop and store human concepts in individual units. Evidence from techniques like activation maximization and network dissection is critiqued for limitations in demonstrating concept representation.
- Core assumption: Current interpretability techniques provide conclusive evidence for concept representation in ANNs.
- Evidence anchors:
  - [abstract] The author concludes that ANNs may learn concepts but do not represent them in the way the narrative suggests, critiquing techniques like activation maximization and network dissection.
  - [section] The author argues that individual units that are supposed to represent concepts often only weakly coactivate with them and do not share their functional role.
  - [corpus] Corpus neighbors do not directly address the paper's core claims, indicating limited external validation.
- Break condition: If future research provides more rigorous, falsifiable hypotheses and stronger evidence for concept representation in individual units, this mechanism would need revision.

### Mechanism 2
- Claim: ANNs learn both human and non-human concepts to perform complex prediction tasks.
- Mechanism: The paper presents evidence from transfer learning and TCAV (Testing with Concept Activation Vectors) indicating that ANNs learn concepts used by humans. However, adversarial examples show that ANNs also rely on concepts that humans do not use.
- Core assumption: ANNs must develop the same concepts as humans to perform complex tasks.
- Evidence anchors:
  - [abstract] Evidence suggests that ANNs can approximate optimal predictors and may learn both human and non-human concepts.
  - [section] Transfer learning indicates that reliable and general concepts are learned, while TCAV provides evidence that some human concepts are learned. Adversarial examples show that models rely on concepts that humans do not.
  - [corpus] Limited corpus evidence, suggesting a need for more external validation.
- Break condition: If it is proven that ANNs only learn human concepts or only non-human concepts, this mechanism would be invalidated.

### Mechanism 3
- Claim: Current interpretability techniques are insufficient to conclusively demonstrate concept representation in ANNs.
- Mechanism: The paper critiques activation maximization for generating non-representative examples and relying on human interpretation, which can be biased. Network dissection is criticized for having low IoU scores and not revealing the functional role of units.
- Core assumption: Interpretability techniques provide reliable evidence for concept representation in ANNs.
- Evidence anchors:
  - [abstract] The author critiques techniques like activation maximization and network dissection for their limitations in demonstrating concept representation.
  - [section] Activation maximization can be misleading due to non-representative examples and human biases. Network dissection is impressive in its story but not in its details, with low IoU scores and limited functional role revelation.
  - [corpus] No direct corpus evidence supporting the critique of interpretability techniques.
- Break condition: If new interpretability techniques are developed that provide more rigorous and falsifiable evidence for concept representation, this mechanism would need to be updated.

## Foundational Learning

- Concept: Concept representation in ANNs
  - Why needed here: Understanding how ANNs represent concepts is central to the paper's argument about the limitations of current interpretability techniques.
  - Quick check question: What are the two criteria for concept representation mentioned in the paper?

- Concept: Transfer learning
  - Why needed here: Transfer learning is used as evidence that ANNs learn reliable and general concepts, supporting the claim that ANNs learn human concepts.
  - Quick check question: How does transfer learning provide evidence that ANNs learn human concepts?

- Concept: Adversarial examples
  - Why needed here: Adversarial examples are used to show that ANNs rely on concepts that humans do not use, challenging the assumption that ANNs learn only human concepts.
  - Quick check question: What do adversarial examples reveal about the concepts learned by ANNs?

## Architecture Onboarding

- Component map:
  - Supervised learning models
  - Interpretability techniques (activation maximization, network dissection, TCAV)
  - Concept representation criteria (coactivation, functional role)
  - Evidence evaluation framework

- Critical path:
  1. Understand the three key assumptions underlying the narrative about concept representation in ANNs.
  2. Evaluate the evidence for each assumption using the provided interpretability techniques.
  3. Critique the limitations of current interpretability techniques in demonstrating concept representation.
  4. Consider the implications for future research on concept representation in ANNs.

- Design tradeoffs:
  - Depth vs. breadth: Focusing on a few interpretability techniques in detail vs. providing a broader overview.
  - Theoretical vs. empirical evidence: Balancing theoretical arguments with empirical evidence from studies.
  - Skepticism vs. acceptance: Critically evaluating the evidence while acknowledging the potential for ANNs to learn concepts.

- Failure signatures:
  - Overreliance on anecdotal evidence: Relying on single examples or images without systematic evaluation.
  - Confirmation bias: Interpreting evidence in a way that confirms pre-existing beliefs about concept representation.
  - Lack of functional role analysis: Focusing on coactivation without considering the functional role of units in prediction.

- First 3 experiments:
  1. Conduct a systematic review of interpretability techniques used to evaluate concept representation in ANNs, focusing on their strengths and limitations.
  2. Design a study to test the functional role of units associated with concepts using ablation techniques, measuring the impact on model performance.
  3. Develop a new interpretability technique that combines coactivation and functional role analysis to provide more rigorous evidence for concept representation in ANNs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do ANNs store human concepts in individual units?
- Basis in paper: [explicit] The author concludes that current evidence does not strongly support the claim that ANNs store human concepts in individual units.
- Why unresolved: While techniques like activation maximization and network dissection have been used to investigate this, the author critiques their limitations in demonstrating concept representation. The evidence from these techniques is considered weak and unconvincing.
- What evidence would resolve it: More rigorous, falsifiable hypotheses and experiments that demonstrate both coactivation and functional role of concepts in individual units.

### Open Question 2
- Question: What is the nature of non-human concepts learned by ANNs?
- Basis in paper: [explicit] The author discusses evidence that ANNs learn both human and non-human concepts, citing adversarial examples and the work of Ilyas et al. (2019).
- Why unresolved: The author notes that the nature of these non-human concepts is an open question, and it is unclear whether humans can understand them.
- What evidence would resolve it: Further research using interpretability techniques to understand the features and concepts relied upon by ANNs, particularly in adversarial examples.

### Open Question 3
- Question: Can human concept representation be enforced in ANNs?
- Basis in paper: [explicit] The author discusses the possibility of enforcing human concept representation in ANNs and mentions concept bottleneck models as an example.
- Why unresolved: The author suggests that more research is needed to investigate training methods that enforce selectivity without sacrificing performance, and to determine if representing disentangled concepts in individual units is desirable.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of training methods that enforce human concept representation in ANNs, and evaluations of the trade-offs between selectivity and performance.

## Limitations

- The analysis relies on a relatively small sample of interpretability studies with minimal external validation
- Critique of interpretability techniques is primarily theoretical rather than comprehensively empirical across multiple models
- Paper does not fully reconcile transfer learning/TCAV results with conclusion that ANNs don't store concepts in individual units

## Confidence

- High confidence in the critique of activation maximization limitations and human interpretation biases
- Medium confidence in the analysis of network dissection's low IoU scores and functional role limitations
- Medium confidence in the conclusion that ANNs learn both human and non-human concepts based on transfer learning and adversarial examples
- Low confidence in the overall narrative rejection without more systematic empirical validation across diverse model architectures

## Next Checks

1. Conduct systematic ablation studies across multiple model architectures to measure the functional impact of individual units associated with specific concepts
2. Implement and compare multiple interpretability techniques (activation maximization, network dissection, TCAV) on the same models to identify consistent vs. conflicting evidence
3. Design experiments using concept vectors to test whether removing all units associated with a concept produces larger performance drops than removing individual units