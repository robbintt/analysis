---
ver: rpa2
title: Optimizing Likelihood-free Inference using Self-supervised Neural Symmetry
  Embeddings
arxiv_id: '2312.07615'
source_url: https://arxiv.org/abs/2312.07615
tags:
- parameters
- data
- flow
- time
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a method to accelerate likelihood-free inference
  by marginalizing physical symmetries through self-supervised learning. The key idea
  is to learn a representation of the data that is invariant to nuisance parameters
  (e.g., time of arrival) via self-supervised learning with symmetry-preserving augmentations.
---

# Optimizing Likelihood-free Inference using Self-supervised Neural Symmetry Embeddings

## Quick Facts
- **arXiv ID**: 2312.07615
- **Source URL**: https://arxiv.org/abs/2312.07615
- **Reference count**: 6
- **Primary result**: Demonstrates parameter and compute savings in likelihood-free inference by learning symmetry-invariant embeddings via self-supervised learning.

## Executive Summary
This paper presents a method to accelerate likelihood-free inference by marginalizing physical symmetries through self-supervised learning. The key innovation is to learn a data representation that is invariant to nuisance parameters (e.g., time of arrival) using joint-embedding via self-supervised learning with symmetry-preserving augmentations. A normalizing flow is then trained on this reduced representation to infer the physical parameters, resulting in faster convergence and requiring fewer parameters than a standard normalizing flow.

## Method Summary
The method involves two main stages: (1) learning a symmetry-invariant representation using self-supervised learning (SSL) with VICReg loss to minimize variance, enforce invariance, and decorrelate embeddings, and (2) training a normalizing flow (NF) conditioned on the learned representation to infer the physical parameters. The approach is demonstrated on two simple physical problems (damped harmonic oscillator and sine-gaussian pulse) and shown to converge faster and require fewer parameters than a standard normalizing flow. Specifically, the method reduces the number of trainable parameters from ~10^6 to ~10^5 and decreases the number of multiply-accumulate operations per forward pass by a factor of 4.

## Key Results
- Reduces trainable parameters from ~10^6 to ~10^5
- Decreases multiply-accumulate operations per forward pass by a factor of 4
- Achieves faster convergence compared to standard normalizing flow
- Maintains comparable posterior accuracy to nested sampling

## Why This Works (Mechanism)

### Mechanism 1
The self-supervised learning stage projects data instances with the same intrinsic parameters but different nuisance parameters into similar regions in the embedding space, reducing effective dimensionality. By applying symmetry-preserving augmentations during VICReg training, the embedding network learns a representation where physically equivalent states are clustered, so the subsequent normalizing flow need not model time-translation variation explicitly. Core assumption: The VICReg loss successfully forces invariance to the nuisance parameter while preserving information relevant to the intrinsic parameters.

### Mechanism 2
The reduced embedding drastically cuts the parameter count and computational cost of the normalizing flow. By conditioning the MAF on the 3D embedding instead of the full high-dimensional input, the flow model only needs to learn the distribution of intrinsic parameters, avoiding unnecessary modeling of time shifts. Core assumption: The representation space dimension (3D here) is sufficient to capture all intrinsic parameter information without loss.

### Mechanism 3
Freezing the convolutional part of the embedding network after SSL preserves the symmetry-aware representation, while allowing fine-tuning of later layers adapts to the specific NF training distribution. The convolutional layers encode general symmetry structure; the fully connected layers adapt to the exact statistical properties of the training set. Core assumption: The symmetry structure is learned early in the network and is robust to later fine-tuning.

## Foundational Learning

- **Self-supervised learning with data augmentations**: Needed to learn a symmetry-aware representation without labeled pairs of equivalent states. Quick check: What loss function ensures that augmented versions of the same data map to nearby points while encouraging decorrelation across dimensions?

- **Normalizing flows for density estimation**: Needed to model the conditional distribution of physical parameters given the learned embedding. Quick check: Why does conditioning the flow on a low-dimensional embedding reduce the required network size compared to conditioning on raw data?

- **VICReg loss (Variance-Invariance-Covariance Regularization)**: Needed to balance invariance to augmentations with preservation of useful variability in the representation. Quick check: How do the three VICReg terms (invariance, variance, covariance) interact to prevent collapse or trivial solutions?

## Architecture Onboarding

- **Component map**: Data augmentation pipeline -> Embedding network (conv + FC) -> Frozen conv layers + trainable FC layers -> Normalizing flow (MAF) -> Posterior sampling

- **Critical path**: 1) Generate {Î˜, d} pairs with symmetry augmentations 2) Train embedding network with VICReg loss 3) Freeze conv layers, fine-tune FC on embedding 4) Train MAF conditioned on embedding 5) Validate posteriors against ground truth or nested sampling

- **Design tradeoffs**: Embedding dimension vs expressiveness (3D chosen empirically; higher may capture more but increase flow cost), augmentation strength (too large shifts may break invariance assumption; too small may not help), freezing vs fine-tuning conv layers (freezing saves compute and stabilizes symmetry; fine-tuning may adapt better to data but risks losing symmetry)

- **Failure signatures**: Posterior widths too narrow compared to nested sampling (embedding too lossy), posterior widths too wide or biased (symmetry not properly learned), training instability (VICReg weighting unbalanced or augmentation distribution mismatched)

- **First 3 experiments**: 1) Train SSL embedding with VICReg on augmented data; visualize embedding clusters to confirm invariance 2) Train MAF on raw data (no embedding) to establish baseline parameter count and compute cost 3) Train MAF conditioned on embedding; compare posterior quality and parameter count to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different similarity embedding architectures and dimensionality affect the accuracy and efficiency of likelihood-free inference?
- **Basis in paper**: The paper mentions that the dimensionality of the representation was chosen based on performance from trials on a handful of cases, without systematic hyperparameter tuning.
- **Why unresolved**: The paper does not explore the impact of different embedding architectures or dimensionality choices on inference performance.
- **What evidence would resolve it**: Systematic experiments comparing different embedding architectures and dimensionalities, showing their effects on posterior accuracy, convergence speed, and parameter efficiency.

### Open Question 2
- **Question**: Can the self-supervised learning approach generalize to more complex physical systems with higher-dimensional parameter spaces and non-linear symmetries?
- **Basis in paper**: The paper demonstrates the approach on two simple physical problems, but does not explore its scalability to more complex systems.
- **Why unresolved**: The paper's examples are limited to low-dimensional, linear symmetries, leaving the method's applicability to more complex scenarios uncertain.
- **What evidence would resolve it**: Application of the method to complex physical systems with higher-dimensional parameter spaces and non-linear symmetries, demonstrating its effectiveness and scalability.

### Open Question 3
- **Question**: How does the choice of data augmentation strategies in self-supervised learning affect the learned symmetry embeddings and subsequent inference performance?
- **Basis in paper**: The paper describes using time-shift data augmentations for learning time-translation symmetry, but does not explore other augmentation strategies.
- **Why unresolved**: The paper does not investigate the impact of different data augmentation choices on the quality of learned embeddings and inference results.
- **What evidence would resolve it**: Comparative studies using different data augmentation strategies, analyzing their effects on embedding quality and inference performance across various physical problems.

## Limitations
- Evaluation limited to two simple synthetic physical models with low-dimensional parameter spaces
- Choice of embedding dimension (3D) is empirically justified but not systematically explored
- VICReg hyperparameters and augmentation strength are not fully specified, making reproduction sensitive to these choices

## Confidence
- **High confidence**: Claims about parameter count reduction (~10^6 to ~10^5) and computational speedup (4x MAC reduction) are directly supported by model architecture comparisons
- **Medium confidence**: Claims about faster convergence and comparable posterior accuracy to nested sampling are supported by experiments on two models but lack systematic error analysis or broader validation
- **Low confidence**: Claims about the general applicability of the symmetry-marginalization approach to arbitrary physical symmetries without careful augmentation design

## Next Checks
1. Systematically vary the embedding dimension (e.g., 2D, 4D, 5D) and measure the trade-off between parameter savings and posterior accuracy degradation to identify optimal dimension
2. Test the method on a higher-dimensional physical model (e.g., 4+ parameters) with more complex symmetries to assess scalability and robustness
3. Perform ablation studies removing VICReg terms or using different augmentation strategies to quantify their impact on symmetry learning and inference performance