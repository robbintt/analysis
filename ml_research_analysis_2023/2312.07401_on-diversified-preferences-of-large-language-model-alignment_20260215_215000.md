---
ver: rpa2
title: On Diversified Preferences of Large Language Model Alignment
arxiv_id: '2312.07401'
source_url: https://arxiv.org/abs/2312.07401
tags:
- reward
- preference
- datasets
- diverse
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the impact of diversified human preferences
  on large language model (LLM) alignment. The authors show that preference datasets
  used for reward modeling are diverse and training on mixed datasets leads to inconsistent
  reward values.
---

# On Diversified Preferences of Large Language Model Alignment

## Quick Facts
- arXiv ID: 2312.07401
- Source URL: https://arxiv.org/abs/2312.07401
- Authors: 
- Reference count: 26
- Primary result: MORE training adaptively reweights preference losses to minimize reward drift across diverse datasets, achieving superior reward accuracy and lower calibration error compared to hybrid and sequential training.

## Executive Summary
This paper investigates how diversified human preferences impact large language model alignment through reward modeling. The authors find that preference datasets are inherently diverse, and training reward models on mixed datasets leads to inconsistent reward values due to preference drift. To address this, they propose MORE (Minimization Of Reward drifE), a training method that adaptively reweights loss terms to minimize reward drift across preferences. Experiments on Pythia-1.4B with five mixed datasets show MORE achieves superior reward accuracy and lower calibration error compared to hybrid and sequential training, while preventing extreme reward values.

## Method Summary
The authors analyze reward modeling under diversified human preferences and propose MORE, a training policy that adaptively adjusts preference objectives to minimize bias. MORE computes scalarization weights λ by minimizing the L2 norm of weighted gradients from different preference datasets, balancing updates to prevent any single preference from dominating. The method is evaluated on Pythia-1.4B using five balanced preference datasets (17,106 samples each), comparing MORE against hybrid, sequential, and ensemble training policies using preference accuracy and Expected Calibration Error (ECE) metrics.

## Key Results
- Reward models trained on mixed datasets show inconsistent reward values due to preference drift
- MORE achieves superior reward accuracy and lower calibration error compared to hybrid and sequential training
- MORE prevents extreme reward values while maintaining accuracy by minimizing reward drift across preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MORE training adaptively reweights preference losses to minimize reward drift across diverse datasets
- Mechanism: MORE computes scalarization weights λ by minimizing the L2 norm of weighted gradients from different preference datasets, balancing updates so no single preference dominates and reducing reward drift
- Core assumption: Reward drift can be approximated by the gradients of the reward head and minimized via scalarization
- Evidence anchors:
  - [abstract]: "a new training policy called MORE, which minimizes preference bias by adaptively adjusting the preference objective across diverse preferences"
  - [section]: "Inspired by previous analysis, finding proper weights λ for mitigating reward drifts is a feasible way"
- Break condition: If reward drift is not well-approximated by reward head gradients, or if preferences are too diverse to share a common reward signal, MORE may fail

### Mechanism 2
- Claim: Reward drift manifests as differences in reward distributions when RMs are trained on different datasets
- Mechanism: Training on diverse preference datasets leads to reward models that produce inconsistent reward values on the same samples, reflecting inherent preference divergence
- Core assumption: The observed reward differences reflect true preference divergence in the datasets
- Evidence anchors:
  - [abstract]: "Our analysis reveals that the impact of diversified human preferences depends on both model size and data size"
  - [section]: "We observe: The implicit preferences across datasets are significantly different... This phenomenon reflects the preference inconsistency hidden in the datasets"
- Break condition: If reward differences are due to model capacity or noise rather than true preference divergence, the analysis breaks down

### Mechanism 3
- Claim: MORE improves calibration and accuracy by capturing shared preferences while mitigating dataset-specific biases
- Mechanism: By minimizing reward drift during training, MORE produces reward values that better reflect shared human preferences, leading to lower ECE and higher accuracy
- Core assumption: Shared preferences exist across datasets and can be captured by minimizing reward drift
- Evidence anchors:
  - [abstract]: "Experiments with the Pythia-1.4B model... show that MORE achieves superior reward accuracy and lower calibration error"
  - [section]: "MORE achieves better testing accuracy and lower ECE values, indicating that the policy provides an accurate reward value on the test"
- Break condition: If no shared preferences exist or if calibration metrics don't correlate with alignment performance, this mechanism fails

## Foundational Learning

- Concept: Bradley-Terry model for pairwise preference ranking
  - Why needed here: The reward model is trained to rank preferred responses higher than dispreferred ones using the Bradley-Terry model formulation
  - Quick check question: How does the Bradley-Terry model define the probability that one response is preferred over another?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is used to measure how well the reward model's predicted preferences match the true distribution of human preferences
  - Quick check question: What does a lower ECE value indicate about a reward model's calibration?

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: Understanding RLHF is crucial as the reward model trained here is used in the RLHF framework for aligning LLMs
  - Quick check question: What are the main components of the RLHF pipeline and how does the reward model fit in?

## Architecture Onboarding

- Component map: Reward Model (RM) with linear reward head -> Preference Datasets (5 diverse datasets) -> Optimizer (AdamW, lr=5e-5) -> MORE Policy (adaptive λ computation)

- Critical path:
  1. Sample diverse batch data from mixed preference datasets
  2. Compute gradients of reward head for each preference
  3. Solve for λ that minimizes weighted gradient norm
  4. Apply weighted loss and update RM

- Design tradeoffs:
  - MORE vs. ensemble: MORE trains a single model instead of averaging multiple RMs
  - MORE vs. hybrid: MORE adapts weights during training vs. static weights in hybrid
  - MORE vs. sequential: MORE handles all datasets simultaneously vs. sequential training order sensitivity

- Failure signatures:
  - Reward values become too compressed (low variance) but accuracy drops
  - Model overfits to one preference dataset
  - Calibration error doesn't improve despite training

- First 3 experiments:
  1. Train RM on single preference dataset and evaluate accuracy/ECE
  2. Train RM on mixed datasets with hybrid training and compare to MORE
  3. Analyze reward value distributions for different training policies

## Open Questions the Paper Calls Out

- Question: How do the number of diverse datasets and the scaling of base models impact the performance of MORE?
  - Basis in paper: [explicit] The authors explicitly state this as a future work question in the Discussion section, noting they have only evaluated MORE on the base model Pythia-1.4B and that further work on larger or smaller models is in progress
  - Why unresolved: The current experiments only use a single base model size, limiting the generalizability of the findings to different model scales and numbers of diverse datasets
  - What evidence would resolve it: Experiments systematically varying the number of diverse datasets and the size of the base model, showing how MORE's performance changes across different configurations

- Question: Does the RM trained by MORE yield improved results in RL tasks?
  - Basis in paper: [explicit] The authors mention in the Discussion section that the ultimate performance of the LLM, fine-tuned by the RM, remains a crucial aspect of their investigation and plan to complete the RL component in the RLHF pipeline
  - Why unresolved: The current evaluation focuses on the capabilities of trained RMs without completing the RLHF pipeline, leaving the impact on downstream RL tasks unexplored
  - What evidence would resolve it: RLHF experiments comparing the performance of LLMs fine-tuned with RMs trained by MORE versus traditional methods, showing improvements in alignment metrics and task completion

- Question: Can learning feedback from mixed diverse preference datasets enhance the performance of the LLM?
  - Basis in paper: [explicit] This is listed as a future work question in the Discussion section, where the authors plan to investigate whether the RM trained by MORE yields improved results in RL tasks and whether learning feedback from mixed diverse preference datasets enhances LLM performance
  - Why unresolved: The current study focuses on reward modeling performance without completing the full RLHF pipeline to assess the impact on LLM performance
  - What evidence would resolve it: Experiments comparing the performance of LLMs trained with RMs using diverse preference datasets versus those using single or less diverse datasets, measuring improvements in alignment and task completion

## Limitations

- The analysis is based on Pythia-1.4B, a relatively small model, which may not generalize to larger or more capable LLMs
- The five preference datasets used, while diverse, may not capture the full spectrum of human preferences in LLM alignment
- MORE's effectiveness depends on the assumption that reward drift can be well-approximated by gradients, which may not hold for highly diverse preferences

## Confidence

- High: The empirical observation that reward drift occurs when training on mixed preference datasets is well-supported by the evidence
- Medium: The mechanism of MORE's adaptive reweighting to minimize reward drift is plausible but relies on the validity of gradient-based approximations
- Medium: The claim that MORE improves calibration and accuracy by capturing shared preferences is supported by experimental results but the underlying assumption of shared preferences needs further validation

## Next Checks

1. **Scale Up Model Size**: Replicate experiments with larger models (e.g., Pythia-6.9B, GPT-3.5) to assess generalization
2. **Expand Dataset Diversity**: Incorporate additional preference datasets with varying characteristics (e.g., domain-specific, task-specific) to test MORE's robustness
3. **Analyze Shared Preferences**: Conduct a more thorough analysis of the extent and nature of shared preferences across datasets, potentially using techniques like clustering or embedding similarity