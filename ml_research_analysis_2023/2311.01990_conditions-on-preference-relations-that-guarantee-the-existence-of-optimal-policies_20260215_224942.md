---
ver: rpa2
title: Conditions on Preference Relations that Guarantee the Existence of Optimal
  Policies
arxiv_id: '2311.01990'
source_url: https://arxiv.org/abs/2311.01990
tags:
- each
- optimal
- policy
- learning
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Direct Preference Process (DPP), a framework
  for analyzing preference-based learning in partially-observable, non-Markovian environments.
  The authors establish conditions on the ordinal structure of preferences that guarantee
  the existence of optimal policies.
---

# Conditions on Preference Relations that Guarantee the Existence of Optimal Policies

## Quick Facts
- arXiv ID: 2311.01990
- Source URL: https://arxiv.org/abs/2311.01990
- Reference count: 15
- Primary result: The paper establishes conditions on ordinal preference structures that guarantee the existence of optimal policies in partially-observable, non-Markovian environments, even when no reward function can express the learning goal.

## Executive Summary
This paper introduces the Direct Preference Process (DPP), a framework for analyzing preference-based learning in partially-observable, non-Markovian environments. The authors establish that total, consistent preorders on distributions over trajectories guarantee the existence of optimal policies characterized by recursive optimality equations. They show that preference-based decision problems can have optimal solutions even when no reward function can express the learning goal, expanding the theoretical foundations of preference-based reinforcement learning.

## Method Summary
The authors develop a theoretical framework based on ordinal preference structures over trajectory distributions. They analyze when these preferences guarantee optimal policies using recursive optimality equations, and when they can be represented as expected reward problems via the von Neumann-Morgenstern theorem. The framework considers feature-based policies under computational constraints, with conditions for when such policies can be optimal. The analysis distinguishes between the abstract preference structure and its practical implementation through feature maps and weighting functions.

## Key Results
- Theorem 15: A total, consistent preorder on attainable trajectories guarantees the existence of optimal policies with recursive optimality equations
- Theorem 30: Conditions for optimal feature-based policies when preferences embed into total consistent preorders via (φ,γ)-frequency and the Markov Feature Assumption holds
- The paper proves that goals expressible by Markov rewards embed into the DPP framework via (φ,γ)-frequency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Total, consistent preorders guarantee existence of optimal policies in DPP
- Mechanism: Consistency enables convex combinations of policies to preserve preference ordering, allowing recursive optimality equations to be well-defined
- Core assumption: The restriction of the preference relation to attainable trajectories is a total, consistent preorder
- Evidence anchors: [abstract] "They show that a decision-making problem can have optimal policies -- that are characterized by recursive optimality equations -- even when no reward function can express the learning goal."

### Mechanism 2
- Claim: Convex preorders satisfying interpolation can be expressed as expected reward problems
- Mechanism: Convexity ensures that if A ≺ B, then αA + (1-α)C ≺ αB + (1-α)C for any C and α, which combined with interpolation guarantees a utility representation via the von Neumann-Morgenstern theorem
- Core assumption: The preorder is both convex and satisfies interpolation
- Evidence anchors: [section] "A binary relation ⪯ on the set of distributions over Ω is a complete convex preorder satisfying interpolation if and only if there is a reward function r : H → R that expresses ⪯."

### Mechanism 3
- Claim: (φ,γ)-frequency embeddings enable optimal feature-based policies when Markov Feature Assumption holds
- Mechanism: When preferences are preserved and reflected via (φ,γ)-frequency, the optimal policy only needs to optimize over feature-action distributions rather than full histories, reducing computational complexity
- Core assumption: Preferences embed into a total consistent preorder on feature-action distributions via (φ,γ)-frequency, and the Markov Feature Assumption holds
- Evidence anchors: [section] "The Markov Feature Assumption guarantees that optimal feature-based policies exist."

## Foundational Learning

- Concept: Preorders and their properties (reflexivity, transitivity, totality, consistency, convexity, interpolation)
  - Why needed here: The paper's main results depend critically on these ordinal properties to guarantee optimal policies
  - Quick check question: Can you give an example of a total preorder that is not consistent?

- Concept: von Neumann-Morgenstern Expected Utility Theorem
  - Why needed here: This theorem connects the ordinal structure of preferences to expected utility representations
  - Quick check question: What are the three axioms required for the vNM theorem to apply?

- Concept: Markov Decision Processes vs Partially Observable Non-Markovian Environments
  - Why needed here: The DPP framework generalizes beyond MDP assumptions, which is central to the paper's contribution
  - Quick check question: How does a DPP differ from an MDP in terms of observability and transition dynamics?

## Architecture Onboarding

- Component map: Agent-Environment Interface -> Environment -> Preference Relation -> Feature Map -> Optimal Policy
- Critical path: Preference relation → consistency/convexity checks → existence of optimal policies → feature-based optimization (if applicable)
- Design tradeoffs: Richer preference structures (convex + interpolation) enable reward representations but may be harder to verify; simpler structures (consistent preorder) guarantee optimality but may lack reward interpretability
- Failure signatures: Non-existence of optimal policies (violates total/consistent preorder), inconsistent policy evaluation (breaks consistency), feature-based policies that aren't optimal (violates Markov Feature Assumption)
- First 3 experiments:
  1. Implement a simple DPP with a total, consistent preorder and verify existence of optimal policies using the recursive optimality equations
  2. Test Example 12 (consistent but non-convex preorder) to confirm optimal policies exist without reward representation
  3. Implement a feature-based policy under the Markov Feature Assumption and verify it achieves optimality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the theory of LfPF be extended to handle infinite sets of observations and actions?
- Basis in paper: [explicit] The authors mention this as an important future extension, noting that their current framework assumes finite sets of observations and actions.
- Why unresolved: The paper focuses on finite agent-environment interfaces, and extending the theory to infinite sets would require new mathematical tools and analysis.
- What evidence would resolve it: A formal extension of the Direct Preference Process framework to handle infinite observation and action sets, with proofs of existence of optimal policies and characterizations of when feature-based policies can be optimal.

### Open Question 2
- Question: Can agents perform well in preference-based learning without learning a reward model?
- Basis in paper: [explicit] The authors note recent findings [RSM+23] suggesting this may be possible, and highlight it as an interesting direction for future work.
- Why unresolved: While the paper establishes conditions for the existence of optimal policies, it does not investigate the practical learnability of such policies without reward models.
- What evidence would resolve it: Empirical studies comparing the performance of preference-based agents with and without learned reward models, demonstrating that non-reward-based agents can achieve comparable or better performance in various domains.

### Open Question 3
- Question: What is the computational complexity of learning optimal feature maps in preference-based decision problems?
- Basis in paper: [explicit] The authors suggest that studying the hardness of learning feature maps could highlight differences between reward-based and preference-based agents.
- Why unresolved: The paper provides conditions for when optimal feature-based policies exist, but does not investigate the computational complexity of learning such policies or the underlying feature maps.
- What evidence would resolve it: Complexity-theoretic results establishing the hardness of learning optimal feature maps, or algorithms with provable guarantees for learning near-optimal feature maps in various classes of preference-based decision problems.

## Limitations
- The theoretical guarantees depend on abstract ordinal properties that may be difficult to verify empirically in complex preference systems
- The feature-based optimization approach assumes the Markov Feature Assumption, which may not hold in many practical scenarios where history-dependent features are necessary
- The framework assumes finite sets of observations and actions, limiting its direct applicability to domains with continuous or infinite state spaces

## Confidence
- High Confidence: Theorem 15's guarantee of optimal policies under total, consistent preorders - the mathematical proof is rigorous and follows standard ordering theory
- Medium Confidence: Theorem 30's conditions for optimal feature-based policies - depends on the Markov Feature Assumption which may not hold in practice
- Medium Confidence: The claim that convex preorders satisfying interpolation can be expressed as expected reward problems - while the vNM theorem provides the foundation, practical verification of these conditions can be challenging

## Next Checks
1. Implement a concrete preference learning system and empirically test whether the preference relation satisfies the required total, consistent preorder conditions
2. Design experiments to verify the Markov Feature Assumption in practical settings, measuring when and how often it fails
3. Create a benchmark comparing the performance of optimal policies derived from DPP framework versus traditional reward-based approaches in environments where preferences cannot be easily encoded as rewards