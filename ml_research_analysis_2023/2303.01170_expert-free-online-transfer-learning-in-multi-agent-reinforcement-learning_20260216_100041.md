---
ver: rpa2
title: Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning
arxiv_id: '2303.01170'
source_url: https://arxiv.org/abs/2303.01170
tags:
- transfer
- learning
- uncertainty
- agent
- ef-ontl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Expert-Free Online Transfer Learning (EF-OnTL),
  an online transfer learning algorithm for multi-agent reinforcement learning that
  dynamically selects a temporary expert agent at each transfer step without requiring
  a dedicated expert. The method transfers experience tuples labeled with epistemic
  uncertainty, estimated using an extension of Random Network Distillation (sars-RND)
  that considers full RL interactions.
---

# Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2303.01170
- Source URL: https://arxiv.org/abs/2303.01170
- Reference count: 29
- Primary result: EF-OnTL achieves comparable performance to advice-based methods in MARL without requiring a dedicated expert agent.

## Executive Summary
This work introduces Expert-Free Online Transfer Learning (EF-OnTL), an online transfer learning algorithm for multi-agent reinforcement learning that dynamically selects a temporary expert agent at each transfer step without requiring a dedicated expert. The method transfers experience tuples labeled with epistemic uncertainty, estimated using an extension of Random Network Distillation (sars-RND) that considers full RL interactions. EF-OnTL is evaluated against no-transfer and advice-based baselines across three environments (Cart-Pole, Multi-Team Predator-Prey, and Half Field Offense), showing comparable performance to advice-based methods while reducing communication overhead.

## Method Summary
EF-OnTL implements online transfer learning in multi-agent reinforcement learning without a dedicated expert. The method dynamically selects a temporary source agent based on performance and uncertainty at each transfer step, then transfers experience tuples labeled with epistemic uncertainty using sars-RND. Transfer Filtering Metrics (TM) use discrepancy between source and target uncertainties and expected surprise (TD-error) to prioritize which experience tuples to send, creating a tailored batch per target. The algorithm operates with a global transfer budget B and frequency TF, with each agent maintaining its own learning process, uncertainty estimator, and transfer buffer.

## Key Results
- EF-OnTL doubles goal-scoring probability in Half Field Offense compared to no-transfer baseline
- EF-OnTL achieves similar win rates to baselines in Multi-Team Predator-Prey without requiring parameter tuning
- EF-OnTL shows comparable performance to advice-based methods while reducing communication overhead

## Why This Works (Mechanism)

### Mechanism 1
Dynamic agent-to-agent knowledge transfer without a fixed expert improves scalability and robustness. EF-OnTL selects a temporary source agent based on performance and uncertainty at each transfer step, then transfers experience tuples labeled with epistemic uncertainty. This avoids reliance on a single expert and enables roles to shift dynamically.

### Mechanism 2
sars-RND improves epistemic uncertainty estimation over RND by incorporating action, reward, and next-state information. Unlike RND, which estimates uncertainty only from state visits, sars-RND computes uncertainty over the full RL interaction tuple (s_t, a_t, r_t, s_{t+1}), making it sensitive to action choices and outcomes.

### Mechanism 3
Personalized experience batches improve transfer efficiency by targeting specific target agent shortcomings. Transfer Filtering Metrics use discrepancy between source and target uncertainties and expected surprise (TD-error) to prioritize which experience tuples to send, creating a tailored batch per target.

## Foundational Learning

- Concept: Epistemic uncertainty in reinforcement learning
  - Why needed here: Guides both source selection and knowledge transfer; determines when an agent should seek or provide advice.
  - Quick check question: What is the difference between epistemic and aleatoric uncertainty, and why does EF-OnTL focus on epistemic?

- Concept: Multi-agent reinforcement learning dynamics
  - Why needed here: Understanding how agents interact, explore, and learn jointly is critical for designing transfer protocols that scale.
  - Quick check question: In a homogeneous multi-agent setup, what signals can differentiate one agent as a better temporary teacher?

- Concept: Experience replay and prioritization
  - Why needed here: EF-OnTL's transfer buffers and filtering mechanisms build on replay buffer concepts but extend them to inter-agent knowledge sharing.
  - Quick check question: How does prioritized experience replay differ from random sampling, and how is that principle adapted in EF-OnTL's transfer filtering?

## Architecture Onboarding

- Component map: N agents (A1…AN) -> Learning Process (LPi) -> Uncertainty Estimator (UEi) -> Transfer Buffer (TBi) -> Source Selection (SS) -> Transfer Filtering (TM) -> Global transfer budget B and frequency TF

- Critical path: Each agent collects experience, estimates uncertainty, stores in buffer. Every TF episodes, SS selects source As. Each target At applies TM over TBs, samples B tuples. Targets update LP with received batch.

- Design tradeoffs:
  - Fixed vs dynamic B: Fixed budget is simpler but may be suboptimal for varying task complexity.
  - SS metrics (U vs BP): U focuses on uncertainty, BP on performance; combining may improve robustness.
  - TM methods: rnd Δ-conf reduces transfer overhead but may miss high-value samples; high Δ-conf is more aggressive.

- Failure signatures:
  - No improvement vs no-transfer baseline: likely source selection or filtering is ineffective.
  - Sudden performance drops: possible negative transfer due to poor source choice or irrelevant experience.
  - Slow convergence: transfer frequency or budget may be too low.

- First 3 experiments:
  1. Run EF-OnTL with B=500, SS=U, TM=high Δ-conf on Cart-Pole; compare sum of rewards vs no-transfer baseline.
  2. Vary B (500, 1500, 5000) with fixed SS and TM; observe how performance scales with transfer volume.
  3. Swap SS between U and BP; analyze sensitivity of source selection on transfer effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
How does the transfer frequency (TF) parameter affect the performance of EF-OnTL across different environments? The paper mentions TF as a parameter but does not explore its impact on performance.

### Open Question 2
What is the optimal batch size (B) for transferring experience in EF-OnTL? The paper states "we did not cover any empirical evaluation on transfer frequency nor optimal batch-size."

### Open Question 3
How does the proposed sars-RND uncertainty estimator compare to other state-of-the-art uncertainty estimators in terms of accuracy and computational efficiency? The paper introduces sars-RND but does not compare it to other uncertainty estimators.

## Limitations
- Exact architecture details for sars-RND networks (input/output dimensions, layer configurations) are not fully specified
- Performance gains depend on hyperparameter choices (transfer budget B, frequency TF) that are not systematically explored
- Evaluation only compares against baselines in specific environments; generalization to other MARL tasks is untested

## Confidence
- High: EF-OnTL framework design and its core mechanisms (dynamic source selection, uncertainty-based filtering)
- Medium: Effectiveness of sars-RND for epistemic uncertainty estimation in multi-agent settings
- Medium: Transfer efficiency gains in the tested environments (Cart-Pole, Predator-Prey, Half Field Offense)

## Next Checks
1. **Ablation Study on sars-RND**: Compare EF-OnTL with standard RND vs sars-RND to quantify the contribution of full RL tuple uncertainty estimation.
2. **Transfer Budget Sensitivity**: Systematically vary B and TF to identify optimal settings and confirm robustness across different transfer volumes.
3. **Negative Transfer Analysis**: Intentionally induce poor source selection to measure the impact of negative transfer and test recovery mechanisms.