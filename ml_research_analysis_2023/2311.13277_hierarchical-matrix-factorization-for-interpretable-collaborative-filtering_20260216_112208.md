---
ver: rpa2
title: Hierarchical Matrix Factorization for Interpretable Collaborative Filtering
arxiv_id: '2311.13277'
source_url: https://arxiv.org/abs/2311.13277
tags:
- item
- user
- clusters
- hierarchical
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Hierarchical Matrix Factorization (HMF) to improve
  the interpretability of collaborative filtering models. The key idea is to decompose
  user/item latent matrices into probabilistic connection matrices and a root cluster
  latent matrix, allowing simultaneous learning of interactions and clustering using
  gradient descent.
---

# Hierarchical Matrix Factorization for Interpretable Collaborative Filtering

## Quick Facts
- arXiv ID: 2311.13277
- Source URL: https://arxiv.org/abs/2311.13277
- Reference count: 35
- Primary result: HMF improves RMSE by 1.37 points over vanilla MF on sparse interactions

## Executive Summary
Hierarchical Matrix Factorization (HMF) extends vanilla matrix factorization by decomposing user/item latent matrices into probabilistic connection matrices and root cluster latent matrices. This hierarchical structure captures implicit relationships between users/items and clusters while maintaining interpretability through cluster-specific interactions. The model learns all parameters using gradient descent with softmax-normalized connections, enabling simultaneous learning of interactions and clustering. Experiments demonstrate HMF's superior performance on both rating prediction and ranking tasks compared to baseline methods.

## Method Summary
HMF recursively decomposes user and item latent matrices into connection matrices (probabilities) and root cluster latent matrices. Each user/item embedding is expressed as a weighted average of parent cluster embeddings using softmax-weighted sums. The model is trained using AdamW optimizer, with predictions computed as inner products between hierarchical embeddings. For ranking tasks, HMF is extended with Bayesian Personalized Ranking (BPR) loss. The hierarchical structure acts as implicit regularization, constraining embeddings to lie near cluster centroids while enabling interpretability through cluster-wise interactions.

## Key Results
- HMF achieves 1.37 point improvement in RMSE for sparse interaction datasets
- Faster convergence compared to vanilla MF during training
- Demonstrated interpretability through cluster-wise interaction patterns
- Outperforms vanilla and hierarchical MF methods on both rating and ranking prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical embeddings capture shared dependencies by representing each user/item as a weighted average of parent cluster embeddings, reducing free parameters and improving generalization
- Mechanism: Model recursively decomposes user/item latent matrix into connection matrices and root cluster embeddings. Each node's embedding is softmax-weighted sum of parent clusters, allowing shared representation learning
- Core assumption: Implicit hierarchy of users/items exists in data and can be learned from interactions alone
- Evidence anchors:
  - [abstract]: "Central to our approach... is the additional decomposition of the user and item latent matrices (embeddings) into probabilistic connection matrices, which link the hierarchy, and a root cluster latent matrix"
  - [section]: "Each user and item embedding are expressed as a weighted average of the parent cluster embeddings in the hierarchy"
  - [corpus]: Weak. No direct neighbor evidence; similar work uses different constraints

### Mechanism 2
- Claim: Using gradient descent on hierarchical embeddings improves learning stability and convergence speed compared to vanilla MF
- Mechanism: Hierarchical structure acts as regularization; each object's embedding constrained to lie near cluster centroids, preventing overfitting and allowing faster convergence
- Core assumption: Learning around cluster points is more stable than learning independent embeddings for each user/item
- Evidence anchors:
  - [abstract]: "Learning the (composite) embedding of each object around the cluster points leads to stable loss convergence, as shown by the experimental results"
  - [section]: "Compared with MF, HMF showed convergence of the RMSE on the validation subset at an earlier epoch"
  - [corpus]: Weak. No neighbor papers directly validate this specific regularization effect

### Mechanism 3
- Claim: Cluster-specific interactions provide interpretability by summarizing high-level user-item preference patterns
- Mechanism: After training, inner products between user and item cluster embeddings reveal which clusters of users prefer which clusters of items, enabling interpretable summaries
- Core assumption: Latent space learned by HMF aligns with semantically meaningful groupings of users/items
- Evidence anchors:
  - [abstract]: "the obtained cluster-specific interactions naturally summarize user-item interactions and provide interpretability"
  - [section]: "Table 4 shows the inner products of the zeroth and first user clusters and all the item clusters... several observations can be made from Table 4"
  - [corpus]: Weak. No neighbor papers demonstrate interpretability via cluster-wise inner products

## Foundational Learning

- Concept: Matrix factorization and latent factor models
  - Why needed here: HMF is an extension of MF; understanding how MF decomposes interaction matrices is essential to grasp HMF's modifications
  - Quick check question: In vanilla MF, how is a user-item interaction predicted from latent vectors?

- Concept: Softmax normalization and probabilistic connections
  - Why needed here: HMF uses softmax to convert raw connection matrices into probability distributions, ensuring embeddings are convex combinations of cluster centroids
  - Quick check question: What property does softmax enforce on each row of the connection matrix?

- Concept: Implicit feedback and ranking prediction (e.g., BPR)
  - Why needed here: HMF is extended to ranking tasks via BPR-HMF; understanding pairwise ranking losses is needed for the ranking experiments
  - Quick check question: In BPR, what does maximizing σ(UᵀᵢVⱼ - UᵀᵢVₖ) achieve?

## Architecture Onboarding

- Component map: Input -> Hierarchical embeddings (connection matrices + root cluster embeddings) -> Prediction (inner product) or BPR loss -> Output

- Critical path:
  1. Initialize connection matrices and root cluster embeddings
  2. Forward pass: recursively compute hierarchical embeddings via softmax-weighted sums
  3. Compute prediction (inner product) or BPR loss
  4. Backpropagate gradients through all connection matrices and root embeddings
  5. Update parameters with AdamW

- Design tradeoffs:
  - Depth vs. sparsity: deeper hierarchies capture more complex structures but require more parameters and risk overfitting on sparse data
  - Cluster size vs. granularity: larger clusters improve stability but reduce interpretability
  - Soft clustering vs. hard clustering: soft clustering allows overlap but complicates interpretation

- Failure signatures:
  - Loss plateaus early: possible degenerate connection matrices (all probability mass on one cluster)
  - RMSE much worse than MF: too few clusters or inappropriate depth
  - Training instability: learning rate too high or softmax temperature mis-tuned

- First 3 experiments:
  1. Train HMF with depth=1, small number of clusters on a tiny synthetic dataset; verify embeddings are convex combinations of cluster centroids
  2. Compare convergence speed of HMF vs. MF on a moderately sized dataset; plot validation RMSE per epoch
  3. Extract and visualize cluster-wise inner products on a small real dataset; check if clusters align with known user/item groups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can HMF's hierarchical structure be extended to handle more than two levels effectively?
- Basis in paper: [explicit] The paper notes that "allowing HMF to support hierarchical clustering, it does not sufficiently validate clustering settings deeper than a depth of two"
- Why unresolved: The paper does not explore or evaluate the effectiveness of deeper hierarchical structures, leaving uncertainty about their impact on performance and interpretability
- What evidence would resolve it: Experimental results comparing HMF with varying depths of hierarchy, particularly showing performance improvements or trade-offs with deeper structures

### Open Question 2
- Question: How can the interpretability of HMF be improved when dealing with soft clustering, where similar item sets can belong to multiple clusters?
- Basis in paper: [inferred] The paper mentions that "allowing for soft clustering makes our interpretation more difficult, as similar item sets can belong to multiple item clusters"
- Why unresolved: The paper does not propose a solution to address the ambiguity introduced by soft clustering, which can complicate the interpretation of cluster relationships
- What evidence would resolve it: A method or framework that enhances interpretability in soft clustering scenarios, such as introducing a softmax function with temperature to bias affiliation degrees

### Open Question 3
- Question: What are the optimal hyperparameters for HMF, particularly the number of clusters and depth of hierarchy, for different types of datasets?
- Basis in paper: [inferred] The paper indicates that "specifying larger user or item clusters may have allowed HMF to outperform or match vanilla MF," suggesting that hyperparameter tuning is crucial for performance
- Why unresolved: The paper does not provide a comprehensive evaluation of the optimal settings for hyperparameters, leaving uncertainty about their impact on HMF's effectiveness
- What evidence would resolve it: A detailed study or framework for hyperparameter optimization tailored to different dataset characteristics, demonstrating improved performance with optimal settings

## Limitations

- The analysis has Medium confidence due to indirect evidence anchors and lack of highly related work with strong citation support
- Key uncertainties include whether the implicit hierarchy exists in real-world datasets and exact hyperparameters used for case study results
- Performance improvements rely heavily on original paper's experiments and would benefit from independent replication
- The claimed 1.37 point RMSE improvement and faster convergence are based on original experiments

## Confidence

- **Medium** confidence in proposed mechanisms
  - [Mechanism 1]: Weak evidence from corpus search
  - [Mechanism 2]: Weak evidence from corpus search  
  - [Mechanism 3]: Weak evidence from corpus search

## Next Checks

1. **Convergence and Stability Test**: Reproduce HMF training on ML-100K and ML-1M, plotting validation RMSE per epoch for both HMF and vanilla MF to verify the claimed faster convergence

2. **Interpretability Verification**: Train HMF on a small real dataset, extract cluster-wise inner products, and qualitatively assess whether the resulting clusters align with known or intuitive user/item groupings

3. **Hyperparameter Sensitivity**: Systematically vary the number of clusters and hierarchy depth on a synthetic dataset, measuring RMSE and checking for overfitting or underfitting as the model complexity changes