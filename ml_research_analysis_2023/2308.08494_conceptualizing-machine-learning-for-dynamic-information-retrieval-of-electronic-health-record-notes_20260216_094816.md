---
ver: rpa2
title: Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic
  Health Record Notes
arxiv_id: '2308.08494'
source_url: https://arxiv.org/abs/2308.08494
tags:
- note
- notes
- information
- patient
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Clinicians spend significant time searching through electronic
  health records (EHRs) to find relevant patient notes, contributing to burnout. This
  work proposes using machine learning on EHR audit logs to proactively retrieve relevant
  notes during documentation.
---

# Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes

## Quick Facts
- arXiv ID: 2308.08494
- Source URL: https://arxiv.org/abs/2308.08494
- Reference count: 40
- Key outcome: ML models predict relevant EHR notes with AUC 0.963 and precision@10 0.720, reducing clinician search burden

## Executive Summary
Clinicians spend significant time searching through electronic health records to find relevant patient notes, contributing to documentation-related burnout. This work proposes using machine learning on EHR audit logs to proactively retrieve relevant notes during documentation. The approach treats note-reading as a binary classification task, using written note content and source document metadata to predict which notes will be read in the next writing session. Evaluated in an emergency department setting with 48,192 note-taking sessions, the method achieves strong performance metrics and demonstrates clinical utility through a user study.

## Method Summary
The method uses EHR audit logs containing timestamped records of clinicians' reading and writing actions to train a binary classifier that predicts which notes will be relevant during a documentation session. Features include document metadata (creation time, type, service), text content from written notes, and chief complaints. Logistic regression models with bag-of-words features and metadata are trained to distinguish between notes that will be read versus those that won't. The system generates top-k recommendations during new documentation sessions, evaluated using classification and retrieval metrics.

## Key Results
- Achieved AUC of 0.963 for predicting relevant notes in individual writing sessions
- Precision@10 of 0.720 indicates 72% of top 10 recommendations are actually read
- Document recency and type are the strongest predictive features
- User study with clinicians confirms improved information retrieval efficiency

## Why This Works (Mechanism)

### Mechanism 1
Machine learning models trained on EHR audit logs can accurately predict which notes clinicians will read next during a documentation session. Audit logs contain timestamped records of clinicians' reading and writing actions. By treating note-reading as a binary classification task, models learn patterns linking current writing context (text and metadata) to future note-reading behavior. Core assumption: Reading behavior is predictable from current writing context and document metadata, and audit logs capture sufficient signal about clinician intent. Evidence anchors: "Our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session." Break condition: If reading behavior is too context-specific or chaotic, or if audit logs are incomplete, the model cannot learn reliable patterns.

### Mechanism 2
Document recency and document type are the strongest predictors of relevance during clinical documentation. Clinicians prioritize recent notes when searching for information, and initial notes are more likely to be read than older notes or letters. Core assumption: Clinicians follow predictable patterns in information retrieval, favoring recent clinical updates and structured progress notes over historical or administrative documents. Evidence anchors: "The most important features to predict a positive label are related to the source document's recency: a more recent note... means that the source document is more likely to be read." Break condition: If clinical workflows change to require historical context more frequently, or if document recency becomes less indicative of relevance.

### Mechanism 3
The framework generalizes to other clinical settings and data modalities (labs, medications, imaging) beyond emergency department notes. Audit logs exist across clinical domains, and the core task of predicting relevant information during documentation is universal, not domain-specific. Core assumption: Information retrieval patterns are similar across clinical specialties, and audit logs from other modalities contain analogous predictive signals. Evidence anchors: "Demonstrating that our framework and methods can perform well in this demanding setting is a promising proof of concept that they will translate to other clinical settings and data modalities." Break condition: If other clinical settings have fundamentally different information needs or if audit logs lack sufficient metadata for prediction.

## Foundational Learning

- Concept: Binary classification for relevance prediction
  - Why needed here: The core task is predicting whether a given note will be read in the next session, which is naturally framed as a binary classification problem.
  - Quick check question: If a model predicts relevance probability of 0.8 for a note, what does this mean in the context of the binary classification task?

- Concept: Information retrieval metrics (precision@k, recall@k)
  - Why needed here: These metrics evaluate how well the system surfaces relevant notes among the top k recommendations, directly measuring clinical utility.
  - Quick check question: If precision@10 is 0.72, what percentage of the top 10 recommended notes are actually read by clinicians?

- Concept: Feature engineering from EHR metadata
  - Why needed here: Clinical context, document recency, and document type provide crucial signals for relevance prediction beyond just text content.
  - Quick check question: Why might the "chief complaint" feature be particularly important for predicting relevant notes in the emergency department?

## Architecture Onboarding

- Component map: Data ingestion pipeline for audit logs and EHR documents -> Feature engineering module (metadata, text, time-based features) -> Machine learning model (logistic regression with bag-of-words and metadata features) -> Evaluation module (classification and retrieval metrics) -> User interface for note recommendations

- Critical path: 1. Collect audit logs of reading/writing sessions 2. Extract features from written notes, source documents, and metadata 3. Train binary classifier on labeled examples 4. Generate top-k recommendations during new documentation sessions 5. Evaluate using held-out test data

- Design tradeoffs: Simple logistic regression vs. complex transformers: Logistic regression offers interpretability but may miss complex patterns; transformers could capture semantic relationships but are harder to interpret. Bag-of-words vs. contextual embeddings: Bag-of-words is computationally efficient but ignores word order and context; contextual embeddings capture semantics but require more resources.

- Failure signatures: Low AUC (<0.8): Model cannot distinguish relevant from irrelevant notes. Low precision@10 (<0.5): Most recommendations are not useful to clinicians. Feature importance skewed to single feature: Model may be overfitting to one signal rather than learning comprehensive patterns.

- First 3 experiments: 1. Ablation study: Remove document recency features to measure their impact on performance 2. Cross-setting validation: Train on ED data, test on outpatient clinic data to assess generalizability 3. User study: Deploy recommendations to clinicians and measure time savings and satisfaction

## Open Questions the Paper Calls Out

### Open Question 1
How do different clinical specialties (e.g., emergency medicine vs. oncology) vary in their patterns of information retrieval and note writing, and how might these differences impact the generalizability of machine learning models for dynamic information retrieval? Basis in paper: The authors hypothesize three different phenotypes for information retrieval and documentation among clinicians (inpatient, outpatient, and acute care settings) and note that they focused on emergency department due to rapid information retrieval needs. They plan to expand to other clinical specialties to understand variation across care settings. Why unresolved: The paper only evaluates their framework in emergency department settings and acknowledges the need to study other clinical specialties. What evidence would resolve it: Comparative analysis of information retrieval patterns across multiple clinical specialties using audit logs, with evaluation of machine learning model performance in each setting.

### Open Question 2
How can patient-specific factors (e.g., age, gender, medical history complexity) be incorporated into machine learning models for dynamic information retrieval to improve prediction accuracy and clinical utility? Basis in paper: The authors use chief complaint and clinician role as patient/clinician features but do not explore other patient-specific factors. The complexity of medical histories is mentioned as a factor in documentation burden. Why unresolved: The paper focuses on chief complaint and clinician role as features but does not explore the potential impact of incorporating additional patient-specific factors into the model. What evidence would resolve it: Comparative analysis of model performance with and without additional patient-specific features, examining whether these factors improve prediction accuracy or clinical utility.

### Open Question 3
What is the optimal presentation strategy for machine learning predictions in clinical workflows - should all top predictions be shown chronologically, or should certain types of predictions (e.g., highly relevant vs. possibly relevant) be presented differently? Basis in paper: The authors use chronological ranking based on clinician input that they "build patients' stories in a chronological manner," but also note that showing only top-ranked notes was sufficient for some clinicians while others wanted to see more notes. Why unresolved: While the authors use chronological ranking based on clinician feedback, they acknowledge different clinician preferences for information depth versus breadth, suggesting that the optimal presentation strategy may vary. What evidence would resolve it: User studies comparing different presentation strategies (chronological vs. relevance-based vs. hybrid) measuring clinician satisfaction, efficiency, and clinical decision quality.

## Limitations
- Claims about generalizability beyond emergency department settings are based primarily on theoretical reasoning rather than empirical validation across multiple clinical settings
- The binary classification formulation may oversimplify the nuanced information needs clinicians have when searching for relevant notes
- Study uses data from a single institution's emergency department, which may have unique documentation patterns

## Confidence

**High Confidence** (AUC of 0.963 and precision@10 of 0.720 are well-supported by the presented results):
- Machine learning models can accurately predict relevant notes using audit log features
- Document recency and type are strong predictors of relevance
- The framework demonstrates technical feasibility in an ED setting

**Medium Confidence** (claims have supporting evidence but with notable limitations):
- The approach will generalize to other clinical settings and data modalities
- Document recency will remain a strong predictor across different clinical workflows
- The user study confirms clinical utility (limited to 5 clinicians)

**Low Confidence** (claims lack direct empirical support or are contradicted by evidence):
- Audit logs capture sufficient signal about clinician intent in all contexts
- The binary classification task captures the full complexity of clinical information needs

## Next Checks

1. **Cross-setting validation**: Train the model on emergency department data and evaluate performance on outpatient clinic or inpatient ward data to empirically test generalizability claims.

2. **Ablation study with clinical experts**: Remove document recency features and have clinicians rate the quality of remaining recommendations to quantify the importance of temporal features.

3. **Longitudinal performance evaluation**: Deploy the system in a real clinical setting for 3-6 months and track whether performance degrades as documentation patterns evolve, particularly if EHR system updates or clinical workflows change.