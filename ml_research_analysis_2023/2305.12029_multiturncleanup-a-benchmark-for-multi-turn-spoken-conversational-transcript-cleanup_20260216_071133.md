---
ver: rpa2
title: 'MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript
  Cleanup'
arxiv_id: '2305.12029'
source_url: https://arxiv.org/abs/2305.12029
tags:
- dataset
- data
- uency
- multi-turn
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study proposes an innovative Multi-Turn Cleanup task for
  spoken conversational transcripts, aiming to detect and remove discontinuities across
  multiple turns that hamper readability and downstream NLP performance. A new dataset,
  MultiTurnCleanup, is collected based on the Switchboard Corpus with a taxonomy of
  five discontinuity categories: acknowledgment and confirmation, repetition and paraphrase,
  think aloud, incomplete sentences, and others.'
---

# MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup

## Quick Facts
- arXiv ID: 2305.12029
- Source URL: https://arxiv.org/abs/2305.12029
- Reference count: 9
- Primary result: BERT-based combined model achieves 74.9 F1 score on detecting multi-turn discontinuities

## Executive Summary
This paper introduces the MultiTurnCleanup task and benchmark dataset for detecting and removing discontinuities in multi-turn spoken conversational transcripts. The task addresses a critical gap in existing disfluency detection research, which focuses primarily on single-turn phenomena while neglecting cross-turn discontinuities that significantly impair readability and downstream NLP performance. The authors propose a comprehensive taxonomy of five discontinuity categories and collect a large-scale dataset based on the Switchboard Corpus, then evaluate two BERT-based approaches to demonstrate the feasibility of multi-turn discontinuity detection.

## Method Summary
The study preprocesses the Switchboard Corpus by removing single-turn disfluencies using predefined rules, then splits conversations into overlapping chunks (~300 tokens each). A four-step data labeling schema is implemented on Amazon Mechanical Turk, involving pilot studies, worker qualification, batch-wise labeling with quality checkpoints, and annotation filtering/aggregation. Two BERT-based models are evaluated: a two-stage approach with separate detectors for single-turn and multi-turn phenomena, and a combined model trained on a unified dataset. Both models use a BERT-based sequence labeling architecture following Rocholl et al. (2021).

## Key Results
- MultiTurnCleanup dataset contains 143k cleanup instances across 1082 conversations
- Five discontinuity categories identified: acknowledgment and confirmation, repetition and paraphrase, think aloud, incomplete sentences, and others
- Combined model achieves 74.9 F1 score on UnionDiscontinuity dataset, outperforming baseline (58.2) and two-stage model (68.2)
- Two-stage model shows 10.2 higher F1 than baseline, while combined model shows 16.7 higher F1 than baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn discontinuity detection is feasible because discontinuities often have predictable linguistic patterns across turns.
- Mechanism: The dataset labels discontinuities in five categories that represent recurring conversational structures which can be modeled.
- Core assumption: Discontinuities follow consistent patterns that can be learned from annotated examples.
- Evidence anchors:
  - [abstract] "numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns"
  - [section 2.1] "We propose an innovative Multi-Turn Cleanup task... we conducted an in-depth analysis of the Switchboard corpus"
  - [corpus] Weak - The paper doesn't show empirical frequency distributions of the categories, only counts
- Break condition: If discontinuities are too idiosyncratic or context-dependent to be captured by the five-category taxonomy.

### Mechanism 2
- Claim: The four-step data labeling schema ensures high-quality annotations by iteratively refining task design and filtering workers.
- Mechanism: The schema uses pilot studies, qualification HITs, batch-wise labeling with quality checkpoints, and annotation filtering/aggregation to maintain annotation quality at scale.
- Core assumption: Quality control measures can effectively filter out low-quality annotations while preserving sufficient data volume.
- Evidence anchors:
  - [section 2.3] "Controlling annotation quality for large-scale data labeling is challenging in MTurk... we employed a 'batch-wise labeling with quality checkpoint filter'"
  - [section 2.4] "we consistently assessed the human annotation accuracy and filtered out unqualified workers"
  - [corpus] Weak - The paper reports Fleiss' Kappa scores but doesn't show how the quality checkpoint thresholds were determined
- Break condition: If the quality threshold (F1≥0.3) is too strict or too lenient, resulting in insufficient high-quality annotations.

### Mechanism 3
- Claim: A combined model outperforms a two-stage approach for joint detection of single-turn disfluencies and multi-turn discontinuities.
- Mechanism: By training on a unified dataset containing both types of annotations, the combined model learns to detect both phenomena simultaneously with a single BERT-based detector.
- Core assumption: The patterns for single-turn and multi-turn phenomena can be learned jointly without interference.
- Evidence anchors:
  - [section 3.2] "We design the combined model... to simultaneously remove both single-turn disfluencies and multi-turn discontinuities"
  - [section 4.3] "the Combined Model achieves a 6.7 higher F1 score than Two-Stage Model on the Multi-Turn Cleanup task"
  - [corpus] Weak - The paper doesn't analyze why the combined model performs better or whether there's interference between the two tasks
- Break condition: If the combined model suffers from catastrophic forgetting or negative transfer between the two tasks.

## Foundational Learning

- Concept: Discontinuity categories and their linguistic characteristics
  - Why needed here: Understanding the taxonomy is essential for annotating and modeling multi-turn discontinuities
  - Quick check question: What are the five categories of discontinuities in the MultiTurnCleanup dataset and how do they differ from traditional disfluencies?

- Concept: BERT-based sequence labeling for disfluency detection
  - Why needed here: Both the two-stage and combined models use BERT-based detectors, so understanding this approach is crucial
  - Quick check question: How does the BERT-based detector from Rocholl et al. (2021) work for sequence labeling in disfluency detection?

- Concept: Quality control in crowdsourced annotation
  - Why needed here: The data collection method relies heavily on MTurk workers, so understanding quality control is essential
  - Quick check question: What are the key components of the four-step data labeling schema and how do they contribute to annotation quality?

## Architecture Onboarding

- Component map: Switchboard Corpus → remove single-turn disfluencies → split into chunks → MTurk annotations → quality filtering → aggregation → MultiTurnCleanup dataset → BERT-based models → evaluation
- Critical path: Preprocessing → Data labeling → Model training → Evaluation
- Design tradeoffs:
  - Single-turn vs. multi-turn: Tradeoff between specialized (two-stage) and unified (combined) approaches
  - Model complexity: Combined model is simpler but may have interference between tasks
  - Data quality vs. quantity: Strict quality control may reduce dataset size but improve annotation quality
- Failure signatures:
  - Low F1 scores on either sub-task indicate model is not learning the patterns
  - High disagreement between annotators (low Fleiss' Kappa) suggests ambiguous categories
  - Combined model underperforming two-stage model indicates interference between tasks
- First 3 experiments:
  1. Evaluate baseline STD model on UnionDiscontinuity dataset to establish single-turn performance
  2. Train MTD model on MultiTurnCleanup dataset to evaluate multi-turn performance in isolation
  3. Train combined model on UnionDiscontinuity dataset and compare with two-stage approach on overall F1

## Open Questions the Paper Calls Out
Based on my understanding of the paper, here are 3 open questions for future research:

### Open Question 1
- Question: How can the Multi-Turn Cleanup models be adapted to handle additional types of discontinuities beyond the five categories defined in the taxonomy?
- Basis in paper: The paper defines a taxonomy of five discontinuity categories but notes in the limitations section that there may be other types of discontinuities that convey social meaning and should not be removed. 
- Why unresolved: The models are currently trained on the five defined categories and may not generalize well to other discontinuity types.
- What evidence would resolve it: Evaluating the models on a dataset with additional discontinuity categories and analyzing their performance would provide evidence of their generalization ability.

### Open Question 2
- Question: How does the order of the two stages in the two-stage model affect its performance on the Multi-Turn Cleanup task?
- Basis in paper: The paper describes a two-stage model with STD and MTD stages but does not explore the impact of changing the order of these stages.
- Why unresolved: The current order may not be optimal and reversing the stages could lead to improved performance.
- What evidence would resolve it: Experimenting with different stage orders and comparing their F1 scores would provide evidence of the optimal order.

### Open Question 3
- Question: How can the Multi-Turn Cleanup models be extended to handle overlapping discontinuities that occur within and across multiple turns?
- Basis in paper: The paper focuses on detecting discontinuities within and across turns but does not address the case of overlapping discontinuities.
- Why unresolved: Overlapping discontinuities add complexity and may require more sophisticated modeling approaches.
- What evidence would resolve it: Evaluating the models on a dataset with overlapping discontinuities and analyzing their ability to detect and handle them would provide evidence of their effectiveness.

## Limitations
- The five-category taxonomy may not comprehensively capture all discontinuity types across different conversational domains
- Quality control relies on relatively simple threshold-based filtering that may not fully account for annotation complexity
- The analysis doesn't deeply explore why the combined model outperforms the two-stage approach

## Confidence

- **High Confidence**: The dataset collection methodology using the four-step schema is well-defined and reproducible. The basic architecture of BERT-based models for sequence labeling is established in the literature.
- **Medium Confidence**: The claim that multi-turn discontinuities significantly impact downstream NLP performance is supported by the motivation but not empirically validated in this paper. The assertion that the combined model outperforms the two-stage approach is supported by F1 scores but lacks deeper analysis of why this occurs.
- **Low Confidence**: The assertion that discontinuities follow predictable patterns that can be learned from the five-category taxonomy is plausible but not empirically validated with frequency distributions or qualitative analysis of learned patterns.

## Next Checks

1. **Validate taxonomy coverage**: Conduct a qualitative analysis of the five discontinuity categories to assess whether they comprehensively capture multi-turn phenomena across different conversational domains. Test with additional domain-specific corpora to identify gaps in the taxonomy.

2. **Analyze model behavior**: Investigate why the combined model outperforms the two-stage approach by examining attention patterns and error cases. Specifically, analyze whether the combined model suffers from interference between single-turn and multi-turn tasks or whether it effectively leverages shared representations.

3. **Test generalization**: Evaluate the trained models on out-of-domain conversational datasets to assess whether the learned patterns generalize beyond the Switchboard corpus. This would validate whether the discontinuity patterns are truly predictable across different conversational contexts.