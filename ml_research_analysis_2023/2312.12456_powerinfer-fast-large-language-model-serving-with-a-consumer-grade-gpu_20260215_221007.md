---
ver: rpa2
title: 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU'
arxiv_id: '2312.12456'
source_url: https://arxiv.org/abs/2312.12456
tags:
- neurons
- powerinfer
- neuron
- inference
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PowerInfer is a high-speed LLM inference engine designed for consumer-grade
  GPUs. It exploits the power-law distribution of neuron activation in LLMs, where
  a small subset of neurons (hot neurons) are consistently activated, while the majority
  (cold neurons) vary based on inputs.
---

# PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU

## Quick Facts
- arXiv ID: 2312.12456
- Source URL: https://arxiv.org/abs/2312.12456
- Reference count: 40
- Primary result: Achieves up to 11.69x speedup over llama.cpp, averaging 13.20 tokens/s on RTX 4090 GPU

## Executive Summary
PowerInfer is a high-speed LLM inference engine designed specifically for consumer-grade GPUs that exploits the power-law distribution of neuron activation in LLMs. The system identifies that a small subset of "hot" neurons are consistently activated across inputs, while the majority are "cold" and input-dependent. By preloading hot neurons onto the GPU and computing cold neurons on the CPU, PowerInfer significantly reduces GPU memory demands and data transfers. The engine integrates adaptive predictors and neuron-aware sparse operators to optimize efficiency, achieving performance that reaches 82% of high-end A100 GPUs while maintaining model accuracy across various LLM architectures.

## Method Summary
PowerInfer employs a hybrid CPU-GPU execution strategy that leverages the power-law distribution of neuron activations in LLMs. The system first profiles neuron activation patterns using a profiler and solver to identify hot neurons (consistently activated across inputs) and cold neurons (input-dependent). Hot neurons are preloaded onto the GPU while cold neurons are computed on the CPU. An adaptive predictor predicts which neurons will be activated at each layer during inference, and neuron-aware sparse operators perform efficient computation without format conversion overhead. The system synchronizes results between CPU and GPU to maintain model accuracy while achieving significant performance improvements over traditional inference engines.

## Key Results
- Achieves up to 11.69x speedup over llama.cpp baseline inference engine
- Maintains an average token generation rate of 13.20 tokens/s on NVIDIA RTX 4090 GPU
- Reaches 82% of the performance of high-end A100 GPU on a single RTX 4090
- Maintains model accuracy across OPT (7B-175B), LLaMA (7B-70B), and Falcon-40B models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuron activation in LLMs follows a power-law distribution, enabling selective GPU-CPU partitioning.
- Mechanism: A small subset of "hot" neurons are consistently activated across inputs, while the majority are "cold" and input-dependent. PowerInfer preloads hot neurons to GPU and computes cold neurons on CPU, reducing GPU memory usage and data transfer.
- Core assumption: Neuron activation locality is predictable enough to pre-select hot neurons offline and accurately predict runtime activations with online predictors.
- Evidence anchors:
  - [abstract] "This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs."
  - [section] "We have observed that LLM inference inherently exhibits high locality... approximately 17% of neurons in OPT-30B... are responsible for 80% of the total activations across all layers."

### Mechanism 2
- Claim: Computing cold-activated neurons directly on CPU is faster than transferring them to GPU for batch sizes <32.
- Mechanism: Modern CPUs with vector extensions efficiently handle smaller matrix computations. For batch sizes under 32, the time to transfer neuron weights to GPU exceeds the time required for direct CPU computation.
- Core assumption: CPU computation latency for cold neurons is lower than GPU-CPU data transfer latency for small batch sizes typical in local deployments.
- Evidence anchors:
  - [section] "If activated neurons reside in CPU memory, computing them on the CPU is faster than transferring them to the GPU, especially with the small number of activated neurons and the small batch sizes typical in local deployments."
  - [section] "Results in Figure 6 indicate that for batch sizes under 32, the time taken to transfer the weights of these neurons and compute them on the GPU exceeds the time required for calculation directly on the CPU."

### Mechanism 3
- Claim: Neuron-aware sparse operators enable efficient computation by processing only activated neurons without format conversion overhead.
- Mechanism: Traditional sparse libraries either require static compilation or dynamic conversion to dense format. PowerInfer's neuron-aware operators directly compute activated neurons and their weights on both GPU and CPU without runtime conversion to dense format.
- Core assumption: Direct neuron-level computation is more efficient than traditional sparse matrix operations for the dynamic sparsity patterns in LLM inference.
- Evidence anchors:
  - [section] "These operators differ from traditional ones as they focus on individual row/column vectors within a matrix rather than the entire matrix."
  - [section] "Neuron-aware Operators for GPU: Despite vector-vector calculations being less efficient than matrix-vector calculations on GPU, neuron-aware operators based on vector-vector computation are advantageous when the batch size is small."

## Foundational Learning

- Concept: Power-law distribution and neuron activation locality
  - Why needed here: Understanding that a small subset of neurons consistently contribute to most activations is fundamental to PowerInfer's partitioning strategy. This concept explains why selective GPU-CPU allocation is effective.
  - Quick check question: If 17% of neurons account for 80% of activations, what percentage of neurons would you expect to be "cold" and processed on CPU?

- Concept: GPU-CPU hybrid execution and synchronization
  - Why needed here: The system's performance depends on efficient coordination between GPU and CPU, including managing dependencies and synchronization overhead when merging results.
  - Quick check question: In the hybrid execution model, why is it more efficient to have the GPU handle result merging rather than the CPU?

- Concept: Sparse matrix operations and neuron-aware computation
  - Why needed here: Traditional sparse libraries are not optimized for the dynamic, neuron-granular sparsity in LLMs. Understanding neuron-aware operators is crucial for implementing the efficient computation strategy.
  - Quick check question: How do neuron-aware operators differ from traditional sparse matrix multiplication in terms of computational granularity?

## Architecture Onboarding

- Component map: Profiler & Solver -> Neuron tables -> Global operator queue -> CPU and GPU executors
- Critical path: Prompt processing → Predictor activation → GPU/CPU neuron computation → Result synchronization → Token generation
- Design tradeoffs:
  - Predictor size vs accuracy: Smaller predictors save GPU memory but may reduce accuracy
  - GPU memory allocation: More hot neurons on GPU improves speed but reduces memory for other components
  - Synchronization frequency: More frequent synchronization ensures correctness but adds overhead
- Failure signatures:
  - Poor predictor accuracy (increased CPU load, reduced speedup)
  - Insufficient GPU memory for hot neurons (increased CPU load, reduced speedup)
  - Predictor overhead exceeding computation savings (reduced overall performance)
  - Synchronization bottlenecks (reduced speedup, especially with many cold neurons)
- First 3 experiments:
  1. Measure neuron activation sparsity and power-law distribution on target LLM using PowerInfer's profiler
  2. Test online predictor accuracy and execution overhead on sample inputs
  3. Benchmark neuron-aware operator performance vs traditional sparse operators for different sparsity levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive predictor training method scale with different model sizes and sparsity patterns? Are there specific threshold values for predictor parameters that optimize accuracy across diverse LLM architectures?
- Basis in paper: [explicit] The paper mentions adaptive training adjusts predictor size based on layer sparsity and skewness, but doesn't provide specific scaling rules or threshold values.
- Why unresolved: The paper describes the adaptive approach but doesn't quantify how predictor size scales with model complexity or sparsity levels.
- What evidence would resolve it: Empirical data showing predictor parameter counts versus model size, sparsity level, and accuracy trade-offs across multiple LLM architectures.

### Open Question 2
- Question: What is the long-term impact of neuron placement decisions on model performance? Does the initial placement policy remain optimal as the model processes more diverse inputs over extended periods?
- Basis in paper: [inferred] The paper profiles activation patterns using general datasets but doesn't address how placement decisions hold up with extended, diverse real-world usage.
- Why unresolved: The paper focuses on initial placement optimization but doesn't examine how placement effectiveness evolves with changing input distributions.
- What evidence would resolve it: Long-term performance tracking showing how neuron placement effectiveness changes over time with varying input distributions.

### Open Question 3
- Question: How does PowerInfer's performance compare to speculative decoding approaches when both are deployed on the same hardware configuration?
- Basis in paper: [explicit] The paper mentions speculative decoding as a separate optimization technique but doesn't compare its performance against PowerInfer.
- Why unresolved: The paper evaluates against baseline systems but doesn't include speculative decoding as a comparison point.
- What evidence would resolve it: Direct performance comparison between PowerInfer and speculative decoding implementations on identical hardware configurations.

### Open Question 4
- Question: What is the overhead of PowerInfer's online predictor execution in terms of power consumption and thermal impact on consumer-grade hardware?
- Basis in paper: [explicit] The paper mentions predictor overhead is less than 10% of inference time but doesn't discuss power or thermal implications.
- Why unresolved: The paper focuses on performance metrics but doesn't address the power consumption or thermal effects of running the predictor system.
- What evidence would resolve it: Power consumption measurements and thermal profiling data for PowerInfer's predictor system under various workloads.

## Limitations

- The neuron activation locality assumption may not hold across all model architectures, training datasets, and fine-tuning procedures
- The ILP solver approach for neuron placement requires extensive profiling data and computational resources that may not be practical for all deployment scenarios
- The adaptive predictor component introduces complexity that could impact real-world performance depending on hardware configurations and input patterns

## Confidence

**High Confidence (90%+):** The core mechanism of exploiting neuron activation sparsity through GPU-CPU partitioning is well-supported by empirical evidence. The power-law distribution observation is consistently demonstrated across multiple models and datasets, and the performance improvements over llama.cpp are substantial and reproducible.

**Medium Confidence (70-89%):** The adaptive predictor accuracy and efficiency claims are supported by the paper's results but depend heavily on implementation details not fully specified in the manuscript. The assumption that CPU computation is faster than GPU-CPU transfer for cold neurons holds for the tested scenarios but may break down with different batch sizes or hardware configurations.

**Low Confidence (below 70%):** The scalability claims to 175B parameter models on RTX 4090 are demonstrated but may not generalize to other large models or consumer GPUs with different memory-bandwidth characteristics. The performance comparison to A100 GPUs assumes specific workload characteristics that may not hold in all deployment scenarios.

## Next Checks

1. **Neuron Activation Pattern Validation:** Profile neuron activation patterns across diverse input distributions and model fine-tuning variants to verify the stability of power-law distributions and hot neuron identification.

2. **Predictor Robustness Testing:** Evaluate predictor accuracy and overhead across varying input lengths, batch sizes, and hardware configurations to establish performance boundaries and failure modes.

3. **Cross-Architecture Performance Scaling:** Test PowerInfer's performance on different consumer GPUs (RTX 3090, 4080, etc.) and with various LLM architectures beyond OPT, LLaMA, and Falcon to assess generalizability.