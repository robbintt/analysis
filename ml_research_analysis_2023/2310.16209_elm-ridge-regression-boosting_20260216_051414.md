---
ver: rpa2
title: ELM Ridge Regression Boosting
arxiv_id: '2310.16209'
source_url: https://arxiv.org/abs/2310.16209
tags: []
core_contribution: The paper proposes a boosting approach for Ridge Regression (RR)
  applied to Extreme Learning Machines (ELMs), aiming to improve classification performance
  and robustness. The method uses multiple levels of boosting with random projections
  and different activation functions (tanh and sign).
---

# ELM Ridge Regression Boosting

## Quick Facts
- arXiv ID: 2310.16209
- Source URL: https://arxiv.org/abs/2310.16209
- Reference count: 10
- Primary result: Classification accuracy exceeding 98.9% on MNIST and 91.3% on Fashion-MNIST with 7 boosting levels

## Executive Summary
This paper proposes a boosting approach for Ridge Regression applied to Extreme Learning Machines (ELMs) to improve classification performance and robustness. The method uses multiple levels of boosting with random projections and different activation functions (tanh and sign). By iteratively fitting models to residuals at each boosting level, the approach progressively reduces classification error. The method demonstrates significant improvements over standard ELMs, achieving over 98.9% accuracy on MNIST and maintaining 98% accuracy even with 10% pixel dropout noise.

## Method Summary
The proposed method combines ELM with Ridge Regression (RR) and a boosting framework. At each boosting level ℓ, random projection matrices R_ℓ_t generate hidden layer representations, which are passed through activation functions (tanh or sign). Ridge regression computes output weights W_ℓ_t to minimize squared error, with residuals Y′₀ = Y - Y₀ computed for the next level. The ensemble prediction is formed by weighted sum across levels using discount parameter α ∈ (0,1). The method employs J=M random projections, λ=1 regularization, and T=50 iterations per level.

## Key Results
- MNIST classification accuracy exceeds 98.9% after 7 boosting levels with tanh activation
- Fashion-MNIST classification accuracy exceeds 91.3% with the same configuration
- Sign activation results in only 0.5% drop in accuracy compared to tanh
- Method maintains 98% accuracy for MNIST and 90% for Fashion-MNIST with 10% random pixel dropout

## Why This Works (Mechanism)

### Mechanism 1
The boosting approach reduces residual errors at each level by iteratively fitting new models to the residuals from previous levels. At each boosting level ℓ, the model computes predictions Y₀ and residuals Y′₀ = Y - Y₀. Subsequent levels fit new models to these residuals, progressively reducing overall error. The parameter α ∈ (0,1) controls the contribution of each level, preventing overfitting. If α is too large, the method may overshoot and cause instability; if too small, convergence becomes impractically slow.

### Mechanism 2
Random projections preserve distance relationships under angular distance measures, enabling effective classification with sign activation. The sign() activation function creates binary hash values based on random hyperplane projections. The probability of hash collisions between vectors x and x′ depends on their angular distance θ(x,x′), as shown in Pr(h(x,r)=h(x′,r)) = 1 - θ(x,x′)/π. If the number of random projections J is too small, the angular relationships may not be preserved adequately for accurate classification.

### Mechanism 3
The ridge regression regularization parameter λ prevents overfitting while allowing effective learning from limited data. The ridge regression solution W = (HH^T + λI)^(-1)H^T Y includes the regularization term λI, which shrinks the weight matrix toward zero and improves numerical stability. If λ is set too high, the model becomes too constrained and loses expressive power; if too low, it becomes sensitive to noise.

## Foundational Learning

- Concept: Ridge Regression (L2 regularization)
  - Why needed here: Provides numerical stability and prevents overfitting in the ELM framework when solving for output weights W
  - Quick check question: How does adding λI to the HH^T matrix affect the condition number of the system?

- Concept: Random Projection Theory
  - Why needed here: Forms the basis for both the hidden layer representation and the sign-based hashing approach
  - Quick check question: What property of random projections allows them to preserve distance relationships between high-dimensional vectors?

- Concept: Boosting and Residual Fitting
  - Why needed here: The core iterative approach that progressively reduces classification error through multiple levels
  - Quick check question: Why is the learning rate parameter α ∈ (0,1) important for the stability of the boosting process?

## Architecture Onboarding

- Component map: Input data → Random projection matrices R_ℓ_t → Activation function (tanh or sign) → Ridge regression weight computation W_ℓ_t → Ensemble prediction via weighted sum → Classification decision
- Critical path: Data preprocessing → Random projection generation → Hidden layer activation → Ridge regression solution → Residual computation (for boosting) → Final prediction
- Design tradeoffs: More boosting levels improve accuracy but increase computation; sign activation reduces computation but may slightly reduce accuracy; larger λ improves stability but may underfit
- Failure signatures: Accuracy plateaus below target threshold; training accuracy much higher than test accuracy (overfitting); sudden accuracy drops (learning rate too high)
- First 3 experiments:
  1. Implement single-level ELM with tanh activation on MNIST, verify baseline accuracy (~97-98%)
  2. Add second boosting level with α = 0.5, measure accuracy improvement
  3. Replace tanh with sign activation, compare accuracy drop and computational savings

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical justification for why the proposed boosting approach achieves improved classification performance in ELMs, particularly regarding the role of the discount parameter α? The paper mentions that α ∈ (0, 1) is a discount parameter (or a "learning" rate) but doesn't provide theoretical analysis of its impact. The paper presents empirical results but lacks theoretical analysis of why the boosting method works and how the discount parameter affects convergence.

### Open Question 2
How does the proposed boosting method scale with respect to dataset size and dimensionality, and what are the computational complexity implications? The paper uses MNIST and Fashion-MNIST datasets but doesn't discuss scalability to larger datasets or provide complexity analysis. The paper only presents results on relatively small datasets and doesn't analyze how the method performs on larger-scale problems.

### Open Question 3
What is the optimal number of random projections (J) and boosting levels (L) for different types of datasets and classification tasks? The paper uses J = M (where M is the original feature dimension) and finds saturation after about 7 boosting levels, but doesn't explore the parameter space systematically. The paper presents results with specific parameter choices but doesn't investigate the sensitivity of results to these parameters or provide guidelines for parameter selection.

### Open Question 4
How does the proposed boosting method compare to other ensemble methods for ELMs, such as bagging or stacking? The paper mentions several previous boosting methods for RR but doesn't provide comparative analysis with other ensemble approaches. The paper focuses solely on the proposed boosting method without benchmarking against alternative ensemble techniques.

## Limitations

- The method relies heavily on empirical validation rather than formal theoretical analysis specific to ELMs
- Results are only demonstrated on two image datasets (MNIST and Fashion-MNIST), limiting generalizability
- Computational complexity of the method with multiple boosting levels is not thoroughly analyzed

## Confidence

- Empirical performance claims (accuracy results): High confidence
- Theoretical foundations (mechanism explanations): Medium confidence
- Noise robustness claims: Medium confidence
- Sign activation effectiveness: High confidence

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (boosting, regularization, activation function) to overall performance
2. Test the method on additional datasets beyond MNIST and Fashion-MNIST, particularly non-image data, to assess generalizability
3. Perform sensitivity analysis on the learning rate parameter α and regularization parameter λ to determine optimal ranges and robustness to parameter choices