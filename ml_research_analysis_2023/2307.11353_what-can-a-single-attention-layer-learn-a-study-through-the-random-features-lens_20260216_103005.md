---
ver: rpa2
title: What can a Single Attention Layer Learn? A Study Through the Random Features
  Lens
arxiv_id: '2307.11353'
source_url: https://arxiv.org/abs/2307.11353
tags:
- arxiv
- learning
- attention
- neural
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the learning and generalization of single attention
  layers in the random feature setting. The authors show that a random feature attention
  (RFA) layer with sufficiently many heads can express a broad class of permutation-invariant
  target functions that are averages of general functions of two tokens.
---

# What can a Single Attention Layer Learn? A Study Through the Random Features Lens

## Quick Facts
- arXiv ID: 2307.11353
- Source URL: https://arxiv.org/abs/2307.11353
- Reference count: 40
- This paper studies the learning and generalization of single attention layers in the random feature setting.

## Executive Summary
This paper provides a theoretical analysis of single attention layers through the lens of random features theory. The authors show that a random feature attention (RFA) layer with sufficient heads can express permutation-invariant target functions that are averages of general functions of two tokens. They demonstrate improved sample complexity compared to standard two-layer random feature networks, with bounds depending only on input dimension rather than the number of key tokens. The paper also introduces a biased RFA model with non-zero mean query-key weights that achieves better sample complexities for learning correlation-weighted functions. Experiments on simulated data validate the theoretical findings.

## Method Summary
The paper studies random-feature attention models (RFA and BRFA) for learning specific target functions. The RFA model uses randomly sampled frozen query and key matrices with trainable value vectors, while the BRFA model adds a bias matrix to the query-key weights. Synthetic data is generated by sampling token sequences uniformly from the unit sphere in d-dimensional space. The models are trained using ridge regression to minimize square loss. Performance is evaluated by varying sample sizes and comparing test error across different target functions and model architectures (RFA, BRFA, and RFMLP).

## Key Results
- RFA with sufficient heads can express permutation-invariant target functions that are averages of general two-token functions
- RFA achieves better sample complexity than standard two-layer random feature networks, with bounds depending only on input dimension
- Biased RFA models with non-zero mean query-key weights achieve better sample complexities for learning correlation-weighted functions
- Experimental results on simulated data corroborate theoretical findings about sample size vs. target function complexity tradeoffs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A random feature attention (RFA) layer with sufficiently many heads can express permutation-invariant target functions that are averages of general functions of two tokens.
- Mechanism: The RFA layer uses randomly sampled frozen query and key matrices with trainable value matrices. This structure allows the layer to learn functions of the form f(x₀:N) = (1/N)∑ᵢ F(x₀, xᵢ) where F can be any general function of two tokens. The permutation invariance arises because the output is an average over all key tokens.
- Core assumption: The target functions are permutation invariant to the key vectors and can be expressed as averages of general two-token functions.
- Evidence anchors:
  - [abstract]: "We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors."
  - [section]: "Consider a broad class of target functions f⋆: X → R that takes form f⋆(x₀:N) = 1/N ∑ᵢ F(x₀, xᵢ)."
  - [corpus]: Weak - no direct evidence in related papers about permutation invariance in RFA specifically.
- Break condition: If the target function is not permutation invariant or cannot be expressed as an average of two-token functions.

### Mechanism 2
- Claim: The RFA model achieves better sample complexity than standard two-layer random feature networks for learning certain target functions.
- Mechanism: The RFA model's sample complexity depends only on the input dimension d and not on the number of key tokens N, whereas standard two-layer random feature networks have sample complexity that depends on both d and N. This improvement arises from the RFA's ability to leverage the permutation invariance structure of the target functions.
- Core assumption: The target functions have permutation invariance structure that can be exploited by the RFA model.
- Evidence anchors:
  - [abstract]: "Our results feature several implications unique to the attention structure compared with existing random features theory for neural networks, such as (1) Advantages in the sample complexity over standard two-layer random-feature networks."
  - [section]: "The excess risk bound of RFA model and the RFMLP model scale as RFA : eO(Poly(p)√(4d)^p/n), RFMLP : eO(Poly(p)√((N+2)d)^p/n)."
  - [corpus]: Weak - related papers don't directly compare RFA to standard random feature networks.
- Break condition: If the target function doesn't have permutation invariance structure or if the number of heads is insufficient.

### Mechanism 3
- Claim: Biased RFA models with non-zero mean query-key weights achieve better sample complexities for learning certain correlation-weighted functions.
- Mechanism: By introducing a bias in the query-key weight distribution (specifically using a non-zero mean matrix), the biased RFA model can better capture functions that depend on correlations between query and key tokens. This allows for more efficient learning of correlation-weighted functions compared to the zero-mean RFA.
- Core assumption: The target functions have correlation structure that can be better captured with biased weights.
- Evidence anchors:
  - [abstract]: "We study a biased RFA model where the query-key matrices are drawn from a distribution with non-zero mean... and show that it achieves better sample complexities than the zero-mean RFA for learning certain natural target functions."
  - [section]: "We consider an alternative attention model with biased random weights, where the bias is a fixed matrix W₀ ∈ R(d+1)×(d+1)."
  - [corpus]: Weak - no direct evidence in related papers about biased RFA models.
- Break condition: If the target functions don't have correlation structure or if the bias is not appropriately chosen.

## Foundational Learning

- Concept: Random Features Theory
  - Why needed here: The paper uses random features theory to analyze the learning and generalization properties of attention layers.
  - Quick check question: What is the key advantage of random feature models over traditional neural networks in terms of optimization?

- Concept: Permutation Invariance
  - Why needed here: The target functions considered in the paper are permutation invariant to the key tokens, which is a crucial property that the RFA model exploits.
  - Quick check question: Why is permutation invariance an important property for attention layers in sequence modeling tasks?

- Concept: Kernel Methods
  - Why needed here: The paper uses kernel methods to analyze the expressivity and generalization of the RFA model.
  - Quick check question: How do kernel methods help in understanding the approximation power of neural networks?

## Architecture Onboarding

- Component map:
  - Input tokens -> Query/Key matrices (frozen random) -> Value vectors (trainable) -> Attention scores (ReLU activation) -> Weighted sum output

- Critical path:
  1. Initialize query and key matrices randomly and freeze them
  2. Initialize value vectors randomly
  3. Compute attention scores for each head
  4. Aggregate outputs across heads
  5. Train value vectors using empirical risk minimization

- Design tradeoffs:
  - Number of heads vs. expressivity: More heads allow for more complex functions but increase computational cost
  - Random initialization vs. learned initialization: Random initialization simplifies analysis but may limit performance
  - ReLU activation vs. softmax: ReLU simplifies theoretical analysis but may differ from practical implementations

- Failure signatures:
  - Poor performance on non-permutation invariant functions
  - Slow convergence if the number of heads is insufficient
  - Overfitting if the number of heads is too large relative to the sample size

- First 3 experiments:
  1. Test the RFA model on a simple permutation-invariant function (e.g., average of key tokens)
  2. Compare the RFA model to a standard two-layer random feature network on the same task
  3. Test the biased RFA model on a correlation-weighted function and compare to the zero-mean RFA model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the biased RFA model (BRFA) perform when the bias matrix W0 is not identity-like but has a different structure, such as a sparse or low-rank matrix?
- Basis in paper: [explicit] The paper discusses the effect of the sampling distribution of the query-key weight matrix and shows that a Gaussian random weight with a non-zero mean (specifically the identity matrix) results in better sample complexities for learning certain natural target functions.
- Why unresolved: The paper only considers the case where W0 is the identity matrix. It is unclear how BRFA would perform with different bias structures, such as sparse or low-rank matrices.
- What evidence would resolve it: Empirical results comparing the performance of BRFA with different bias structures on various target functions.

### Open Question 2
- Question: What is the impact of positional encoding on the expressivity and sample complexity of single attention layers?
- Basis in paper: [inferred] The paper focuses on a simplified setting of single attention layers without positional encoding. However, in practice, transformers use positional encoding to incorporate information about the position of tokens in a sequence.
- Why unresolved: The paper does not investigate the effect of positional encoding on the expressivity and sample complexity of single attention layers.
- What evidence would resolve it: Theoretical analysis and empirical results comparing the performance of single attention layers with and without positional encoding on various tasks.

### Open Question 3
- Question: How does the expressivity and sample complexity of multi-layer transformers compare to that of single attention layers?
- Basis in paper: [explicit] The paper studies the learning and generalization of a single multi-head attention layer and provides insights into its expressivity and sample complexity. However, it is unclear how these results extend to multi-layer transformers.
- Why unresolved: The paper only focuses on single attention layers and does not investigate the expressivity and sample complexity of multi-layer transformers.
- What evidence would resolve it: Theoretical analysis and empirical results comparing the performance of single attention layers and multi-layer transformers on various tasks.

## Limitations
- The analysis is limited to permutation-invariant target functions and may not generalize to non-permutation-invariant functions
- The paper focuses on the random feature setting rather than learned attention mechanisms, limiting practical implications
- The theoretical analysis relies on specific assumptions about random weight distributions that may not hold in practice

## Confidence
- High confidence: Claims about RFA's ability to express permutation-invariant functions and sample complexity advantages are well-supported
- Medium confidence: Claims about biased RFA advantages for correlation-weighted functions are supported by theory but limited experimental validation
- Low confidence: Claims about practical implications for real-world attention layers are speculative

## Next Checks
1. Empirical validation on non-permutation-invariant functions to test RFA's limitations
2. Comparison with learned attention mechanisms to assess practical relevance
3. Analysis of overparameterization effects by systematically varying the number of heads in RFA