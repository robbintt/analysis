---
ver: rpa2
title: Neural Machine Translation Data Generation and Augmentation using ChatGPT
arxiv_id: '2307.05779'
source_url: https://arxiv.org/abs/2307.05779
tags:
- data
- translation
- language
- natural
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using ChatGPT to generate synthetic parallel
  corpora for neural machine translation (NMT) in low-resource language pairs. The
  authors create synthetic training data by prompting ChatGPT to generate source sentences
  from seed words, then translate them into English.
---

# Neural Machine Translation Data Generation and Augmentation using ChatGPT

## Quick Facts
- arXiv ID: 2307.05779
- Source URL: https://arxiv.org/abs/2307.05779
- Reference count: 9
- Key outcome: Synthetic data from ChatGPT improves NMT quality when combined with natural data, despite diversity limitations

## Executive Summary
This paper investigates using ChatGPT to generate synthetic parallel corpora for neural machine translation (NMT) in low-resource language pairs. The authors create synthetic training data by prompting ChatGPT to generate source sentences from seed words and translate them into English. They train three models per language pair: on natural data only, synthetic data only, and a combination. Results show that synthetic data improves translation quality when combined with natural data, despite domain mismatch and less diversity in synthetic data. BLEU scores increase from 16.4 to 18.9 for German and 24.4 to 28.3 for Galician when synthetic data is added. However, synthetic models overfit their validation data, likely due to repetitive sentence generation. The study suggests synthetic data can supplement natural data for NMT, but diversity limitations remain an open challenge.

## Method Summary
The method involves generating synthetic parallel corpora using ChatGPT API calls with seed words, then training NMT models using Fairseq transformer architecture. Three models are trained per language pair: natural data only, synthetic data only, and combined. The training uses BPE tokenization with vocab size 16K, 4 attention heads, 3 layers, batch size 2000, and 100 epochs with early stopping. German and Galician TEDTalk parallel corpora serve as natural data sources, with 900K train, 100K validation, and 100K test tokens each.

## Key Results
- BLEU scores improve from 16.4 to 18.9 for German-English when synthetic data is added
- BLEU scores improve from 24.4 to 28.3 for Galician-English when synthetic data is added
- Synthetic models overfit validation data due to repetitive sentence generation patterns
- Synthetic data shows lower type-token ratios and less diversity than natural data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data improves translation quality when combined with natural data.
- Mechanism: ChatGPT generates parallel sentences that expose the NMT model to diverse sentence patterns, even if domain-mismatched.
- Core assumption: The NMT model benefits from additional training examples regardless of domain alignment.
- Evidence anchors:
  - [abstract] "synthetic data improves translation quality when combined with natural data, despite domain mismatch and less diversity in synthetic data."
  - [section] "We observe that augmenting the natural training data with synthetic data leads to a notable improvement in the translation quality, despite the domain mismatch."
  - [corpus] Weak - corpus neighbors discuss similar data augmentation approaches but don't directly confirm the mechanism.
- Break condition: If synthetic data diversity is too low, overfitting occurs and degrades performance.

### Mechanism 2
- Claim: ChatGPT's multilingual vector space enables creation of synthetic parallel corpora.
- Mechanism: The model leverages its training on multilingual data to generate plausible source sentences and their English translations.
- Core assumption: ChatGPT's internal representations capture enough cross-lingual knowledge to produce coherent translations.
- Evidence anchors:
  - [abstract] "Although these models are themselves trained on parallel data, they can leverage a multilingual vector space to create data."
  - [section] "ChatGPT is based on a sampled generative language model... it still has a bias towards frequent patterns."
  - [corpus] Weak - corpus neighbors mention generative language models for data augmentation but don't confirm vector space usage.
- Break condition: If the multilingual vector space is insufficient for the target language pair, generated sentences become nonsensical.

### Mechanism 3
- Claim: Synthetic data provides stronger translation signal than natural data alone in low-resource settings.
- Mechanism: Additional training examples amplify the learning signal, helping the model generalize better.
- Core assumption: More training data, even if synthetic, is beneficial for low-resource translation.
- Evidence anchors:
  - [abstract] "our experiments highlight two key findings - despite a lack of diversity in their output, the hallucinated data improves the translation signal."
  - [section] "In low-resource translation tasks, it appears that any additional information can strengthen the signal."
  - [corpus] Weak - corpus neighbors discuss data augmentation improving low-resource NMT but don't quantify the translation signal strength.
- Break condition: If synthetic data is too repetitive, it may reinforce incorrect patterns instead of improving translation.

## Foundational Learning

- Concept: BLEU score calculation
  - Why needed here: The paper evaluates translation quality using BLEU scores, which measure n-gram overlap between predictions and references.
  - Quick check question: What does a BLEU score of 0 indicate about the translation output?

- Concept: Type-token ratio (TTR)
  - Why needed here: The paper uses TTR to measure lexical diversity in synthetic vs natural data, showing synthetic data is less diverse.
  - Quick check question: How would a low TTR value affect a language model's ability to generalize?

- Concept: Data hallucination in NMT
  - Why needed here: The paper's core approach is generating synthetic parallel corpora to augment limited natural data.
  - Quick check question: Why might synthetic data from ChatGPT be less diverse than natural data, even with the same token count?

## Architecture Onboarding

- Component map: Data generation (ChatGPT prompts) → Synthetic corpus creation → BPE tokenization → NMT model training (Fairseq) → BLEU evaluation
- Critical path: Synthetic data generation → Model training → Evaluation
- Design tradeoffs: Using ChatGPT for data generation trades off diversity for quantity; combining with natural data mitigates domain mismatch.
- Failure signatures: High BLEU scores on synthetic validation data indicate overfitting; low BLEU scores on natural test data indicate domain mismatch issues.
- First 3 experiments:
  1. Generate synthetic data for a high-resource language pair (e.g., German-English) and evaluate if it improves translation.
  2. Compare synthetic-only models vs augmented models on a held-out test set.
  3. Analyze TTR and frequency distributions of synthetic vs natural data to quantify diversity differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic parallel corpora vary when generated for different language pairs, particularly for languages with different degrees of relatedness to English?
- Basis in paper: [inferred] The paper notes that ChatGPT had access to significantly more German data than Galician data, yet the Galician synthetic data produced higher quality translations. The authors speculate this could be due to Galician being easier to translate into English or ChatGPT leveraging Spanish data, but they do not provide conclusive evidence.
- Why unresolved: The study only investigates two language pairs, both of which are western Indo-European languages closely related to English. This small sample size may not be representative of how ChatGPT performs with other language pairs.
- What evidence would resolve it: Conducting experiments with a wider variety of language pairs, including languages from different families and with varying degrees of relatedness to English, would provide more insight into how ChatGPT performs across different linguistic contexts.

### Open Question 2
- Question: Can prompt engineering techniques improve the diversity of synthetic data generated by ChatGPT for NMT?
- Basis in paper: [explicit] The authors acknowledge that ChatGPT's generated sentences show significantly less linguistic diversity than natural sentences and suggest that more research is necessary to determine whether prompt engineering can improve the diversity issue.
- Why unresolved: The paper does not explore different prompt engineering techniques or evaluate their impact on the diversity of generated synthetic data.
- What evidence would resolve it: Experimenting with various prompt engineering strategies, such as providing more examples during the few-shot phase or using more specific instructions, and measuring their effect on the type-token ratio and linguistic diversity of the generated data would help determine if prompt engineering can enhance the quality of synthetic data for NMT.

### Open Question 3
- Question: How does the size of the training data impact the effectiveness of synthetic data augmentation in low-resource NMT?
- Basis in paper: [inferred] The authors mention that the size of the training data used in their study is relatively small for NMT and may impact the generalizability of their findings to instances where a much larger parallel corpus needs to be augmented.
- Why unresolved: The paper does not investigate how varying the size of the training data affects the performance of models trained with synthetic data augmentation.
- What evidence would resolve it: Conducting experiments with different sizes of training data, ranging from low-resource to high-resource settings, and comparing the performance of models trained with and without synthetic data augmentation would provide insights into how the size of the training data influences the effectiveness of synthetic data in NMT.

## Limitations
- Synthetic data shows significantly lower diversity than natural data, with lower type-token ratios and more repetitive patterns
- Synthetic models tend to overfit validation data, likely due to repetitive sentence generation
- Experiments limited to only two language pairs (German-English and Galician-English), both closely related to English

## Confidence

**High Confidence Claims**:
- ChatGPT can generate synthetic parallel corpora for NMT
- Combining synthetic and natural data improves translation quality compared to natural-only training
- Synthetic-only models underperform compared to augmented models

**Medium Confidence Claims**:
- The specific BLEU score improvements (German: 16.4→18.9; Galician: 24.4→28.3) are accurate for the tested conditions
- Synthetic data provides a stronger translation signal in low-resource settings
- The overfitting behavior is primarily due to repetitive sentence generation

**Low Confidence Claims**:
- The exact mechanism by which ChatGPT's multilingual vector space enables cross-lingual generation
- The optimal ratio of synthetic to natural data for maximum performance
- Long-term generalization capabilities of models trained on synthetic data

## Next Checks
1. **Diversity Analysis Extension**: Conduct a comprehensive analysis comparing type-token ratios, word frequency distributions, and n-gram diversity across synthetic and natural data for multiple language pairs, including languages with different morphological complexity.

2. **Overfitting Mitigation Experiment**: Implement and test techniques to reduce synthetic data repetition, such as prompt variation strategies, temperature scaling adjustments, or data filtering based on novelty metrics, then measure impact on validation/test BLEU score alignment.

3. **Generalizability Assessment**: Replicate the experiments across 5-10 additional language pairs with varying resource levels and linguistic distances, measuring both translation quality improvements and diversity metrics to establish broader patterns of synthetic data effectiveness.