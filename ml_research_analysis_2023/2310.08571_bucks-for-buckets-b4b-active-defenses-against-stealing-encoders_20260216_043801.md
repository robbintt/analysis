---
ver: rpa2
title: 'Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders'
arxiv_id: '2310.08571'
source_url: https://arxiv.org/abs/2310.08571
tags:
- representations
- encoder
- space
- buckets
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose B4B, the first active defense against encoder stealing
  that does not harm legitimate users. B4B adaptively degrades representation quality
  based on embedding space coverage, using local sensitive hashing to track per-user
  coverage and applying per-user transformations to prevent sybil attacks.
---

# Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders

## Quick Facts
- arXiv ID: 2310.08571
- Source URL: https://arxiv.org/abs/2310.08571
- Reference count: 40
- Primary result: First active defense against encoder stealing that maintains <1% accuracy drop for legitimate users while reducing stolen encoder performance by 20-40%

## Executive Summary
Bucks for Buckets (B4B) introduces an active defense mechanism against encoder stealing attacks in MLaaS APIs. The defense leverages the observation that adversaries who attempt to steal encoder functionality cover significantly more of the embedding space than legitimate users. By tracking per-user embedding space coverage using Local Sensitive Hashing (LSH) and applying adaptive noise and per-user transformations, B4B effectively degrades stolen encoder performance while preserving utility for legitimate users. Experiments demonstrate that B4B achieves 20-40% reduction in stolen encoder accuracy across multiple datasets and encoder types with minimal impact on legitimate users.

## Method Summary
B4B operates through three core components: (1) coverage estimation using LSH to track which regions of the embedding space each user accesses, (2) a cost function that maps coverage to noise levels using an exponential relationship, and (3) per-user transformations to prevent sybil attacks. The system adaptively degrades representation quality by adding Gaussian noise proportional to embedding space coverage, with more noise applied to users covering larger fractions of the space. Individual transformations (affine, padding, shuffling, binary encoding) are applied to each user's representations, making it costly for adversaries to combine representations from multiple accounts. The defense requires no retraining and maintains utility for legitimate users who typically access only task-specific regions of the embedding space.

## Key Results
- Reduces stolen encoder performance by 20-40% across FashionMNIST, SVHN, STL10, and CIFAR10 datasets
- Maintains <1% drop in legitimate users' accuracy when using optimal LSH bucket count (212)
- Prevents sybil attacks by making it costly to combine representations from multiple accounts
- Works effectively against both SimSiam and DINO encoder architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than legitimate users.
- Mechanism: The defense tracks per-user coverage of the embedding space using Local Sensitive Hashing (LSH), and penalizes users who cover large fractions by adding noise to their representations.
- Core assumption: Legitimate users query with data from a narrow task distribution, while adversaries query with diverse data to capture full encoder functionality.
- Evidence anchors:
  - [abstract] "Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder’s functionality cover a significantly larger fraction of the embedding space than representations of legitimate users"
  - [section 3.1] "we visualize that representations for different downstream tasks cluster in disjoint and small sub-spaces of the full embedding space"
  - [corpus] Weak - no direct citations found in related papers, though this core observation underpins B4B's design
- Break condition: If an adversary mimics legitimate query patterns (e.g., queries only from downstream task distribution), the coverage-based detection fails.

### Mechanism 2
- Claim: Adding noise to representations degrades stolen encoder performance while preserving utility for legitimate users.
- Mechanism: B4B uses an exponential cost function that maps embedding space coverage to noise standard deviation, adding more noise as coverage increases.
- Core assumption: Downstream classifiers trained on noisy representations can tolerate noise up to a threshold, beyond which performance degrades sharply.
- Evidence anchors:
  - [abstract] "B4B adaptively degrades representation quality based on embedding space coverage"
  - [section 4.2] "performance drops to 10% (i.e., random guessing) at roughly σ = 0.5"
  - [corpus] Weak - no direct citations found, though noise addition as defense is mentioned in related work
- Break condition: If adversary trains stolen encoder to be robust to noise (e.g., using denoising techniques), the defense becomes ineffective.

### Mechanism 3
- Claim: Per-user transformations prevent sybil attacks by making it costly to combine representations from multiple accounts.
- Mechanism: B4B applies random transformations (affine, padding, shuffling, binary encoding) to each user's representations, requiring adversaries to remap them to a unified space.
- Core assumption: Remapping representations between transformed spaces introduces significant overhead and fidelity loss.
- Evidence anchors:
  - [abstract] "To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user’s representations"
  - [section 4.3] "adversaries cannot directly combine the representations obtained through different sybil accounts anymore to train their stolen copy"
  - [corpus] Weak - related papers mention transformations but not specifically for sybil attack prevention in encoder stealing
- Break condition: If adversary can learn perfect remapping functions or if transformations are too weak to prevent combination.

## Foundational Learning

- Concept: Local Sensitive Hashing (LSH)
  - Why needed here: Enables efficient estimation of embedding space coverage by mapping similar representations to same buckets
  - Quick check question: How does LSH differ from cryptographic hashing in its collision properties?

- Concept: Exponential cost functions
  - Why needed here: Provides smooth penalty increase that minimally affects legitimate users while severely impacting adversaries
  - Quick check question: Why choose an exponential form rather than linear or polynomial for the cost function?

- Concept: Representation transformations and their utility preservation
  - Why needed here: Allows defense against sybil attacks without degrading legitimate user performance
  - Quick check question: What properties must transformations have to preserve downstream task utility?

## Architecture Onboarding

- Component map: LSH coverage tracker -> Cost function (noise level) -> Per-user transformation -> Representation output
- Critical path: Query -> LSH bucket update -> Coverage calculation -> Noise determination -> Transformation selection -> Return transformed+noisy representation
- Design tradeoffs: More buckets in LSH improves coverage estimation but increases computation; stronger transformations better prevent sybil attacks but may risk utility loss
- Failure signatures: Legitimate users seeing degraded performance (coverage misestimation), adversaries succeeding despite defense (transformations too weak), system becoming too slow (LSH computation)
- First 3 experiments:
  1. Measure embedding space coverage for legitimate vs adversary query patterns using LSH
  2. Test downstream classifier performance with different noise levels to find utility threshold
  3. Evaluate cosine distance between transformed representations to assess sybil attack difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of hash buckets for different encoder architectures and datasets?
- Basis in paper: [explicit] The paper discusses the importance of choosing the right number of hash buckets, with 212 being optimal for the tested encoders and datasets, but notes this might vary for other configurations.
- Why unresolved: The paper does not provide a general method for determining the optimal number of hash buckets for arbitrary encoder architectures and datasets.
- What evidence would resolve it: Experimental results comparing the performance of B4B with different numbers of hash buckets for various encoder architectures and datasets would help establish guidelines for optimal bucket selection.

### Open Question 2
- Question: How does B4B perform against more sophisticated sybil attack strategies, such as those involving a larger number of accounts or more complex remapping techniques?
- Basis in paper: [inferred] The paper discusses the effectiveness of B4B against sybil attacks using up to 6 accounts and mentions the possibility of more complex remapping techniques, but does not provide extensive evaluation against more advanced attack strategies.
- Why unresolved: The paper's evaluation of sybil attacks is limited to a small number of accounts and does not explore the full range of potential attack strategies.
- What evidence would resolve it: Experiments evaluating B4B's performance against sybil attacks using a larger number of accounts and more sophisticated remapping techniques would provide insights into the defense's robustness against advanced attack strategies.

### Open Question 3
- Question: How does the choice of cost function parameters (α, β, λ) affect the trade-off between utility preservation for legitimate users and effectiveness against stealing?
- Basis in paper: [explicit] The paper discusses the role of these parameters in the cost function and provides some guidance on their selection based on empirical observations, but does not offer a comprehensive analysis of their impact on the trade-off between utility and security.
- Why unresolved: The paper does not provide a systematic exploration of the parameter space to understand the relationship between these parameters and the defense's performance.
- What evidence would resolve it: A comprehensive study varying the cost function parameters and measuring the resulting impact on both legitimate users' utility and the effectiveness against stealing would help identify optimal parameter settings for different use cases.

## Limitations

- Core assumption dependency on different embedding space coverage patterns between legitimate users and adversaries
- Untested attack vectors including adaptive query optimization and representation reconstruction
- Parameter sensitivity with optimal values potentially varying across applications

## Confidence

- High Confidence: The basic observation that adversaries cover more embedding space than legitimate users is well-supported by the data visualization in Section 3.1
- Medium Confidence: The noise degradation mechanism shows consistent performance drops in the calibration experiments, but the 20-40% reduction claim for real adversaries needs more validation across diverse attack scenarios
- Low Confidence: The sybil attack prevention mechanism is less validated - only 10% of attack queries overlap between accounts in evaluation, and the difficulty of remapping transformed representations is asserted but not empirically demonstrated

## Next Checks

1. Evaluate B4B against adversaries who intentionally limit their queries to downstream task distributions to avoid detection
2. Systematically vary the strength and types of transformations to identify breaking points where legitimate user utility drops significantly
3. Conduct ablation studies on LSH bucket count and cost function parameters to understand robustness to hyperparameter tuning