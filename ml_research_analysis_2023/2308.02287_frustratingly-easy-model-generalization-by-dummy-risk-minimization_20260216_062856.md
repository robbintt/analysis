---
ver: rpa2
title: Frustratingly Easy Model Generalization by Dummy Risk Minimization
arxiv_id: '2308.02287'
source_url: https://arxiv.org/abs/2308.02287
tags:
- durm
- class
- gradient
- dummy
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dummy Risk Minimization (DuRM), a simple
  method to improve model generalization by adding dummy classes to the output logits.
  The core idea is to increase the model's gradient variance during training, which
  facilitates convergence to flatter local minima and thus better generalization.
---

# Frustratingly Easy Model Generalization by Dummy Risk Minimization

## Quick Facts
- arXiv ID: 2308.02287
- Source URL: https://arxiv.org/abs/2308.02287
- Reference count: 40
- Primary result: DuRM consistently improves model generalization across diverse tasks by adding dummy classes to output logits

## Executive Summary
This paper introduces Dummy Risk Minimization (DuRM), a remarkably simple method to improve model generalization by adding dummy classes to the output logits. The core idea is to increase the model's gradient variance during training, which facilitates convergence to flatter local minima and thus better generalization. The authors provide theoretical analysis showing that DuRM increases gradient variance and theoretically supports that larger gradient variance leads to convergence to flatter local minima. Empirically, they validate DuRM on diverse tasks including classification, semantic segmentation, out-of-distribution generalization, adversarial robustness, and long-tailed recognition. Results show consistent performance improvements across tasks. Ablation studies further demonstrate that DuRM works well with different model scales, class numbers, and data sizes, and is compatible with existing regularization techniques. The method is easy to implement and nearly cost-free, making it a promising approach for improving model generalization.

## Method Summary
DuRM works by enlarging the dimension of the output logits from C classes to C + Cd classes, where Cd is the number of dummy classes. During training, the standard cross-entropy loss is computed only on the first C classes, while the dummy classes remain unlabeled and provide additional gradient risk. This simple modification increases gradient variance during training, which the authors argue leads to convergence to flatter local minima. The method is compatible with any classification network and requires no architectural changes beyond modifying the output layer.

## Key Results
- DuRM consistently improves performance across classification, semantic segmentation, out-of-distribution generalization, adversarial robustness, and long-tailed recognition tasks
- The method works well with different model scales, class numbers, and data sizes
- DuRM is compatible with existing regularization techniques and provides nearly cost-free improvements
- Empirical results show gradient variance increases with dummy classes and correlates with better generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding dummy classes increases gradient variance during training, which pushes the model toward flatter local minima.
- Mechanism: Dummy classes introduce an extra push term in the gradient calculation, increasing the variance of the gradient distribution without changing its expectation. Higher gradient variance is correlated with larger eigenvalue of the Hessian, which corresponds to flatter minima.
- Core assumption: The dummy classes are truly unlabeled (zero gradient pull) and the gradient noise introduced is Gaussian-like and symmetric.
- Evidence anchors: Abstract states "DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima"; section 2.2 confirms "DuRM aids to derive gradients with greater variance during training" and section 2.3 states "DuRM aids the model to converge towards more flat local minima than ERM"
- Break condition: If dummy classes receive non-zero gradient pull (e.g., due to implementation error), the variance increase may not materialize and the mechanism fails.

### Mechanism 2
- Claim: Dummy classes act as implicit regularization by providing additional gradient diversity without altering the label distribution.
- Mechanism: By expanding the logit dimension and leaving dummy classes unlabeled, the model is forced to explore a higher-dimensional loss landscape, preventing it from overfitting to the training manifold and improving generalization.
- Core assumption: The original classes' gradient updates remain dominant while dummy classes contribute only noise; the network can still learn discriminative features for the real classes.
- Evidence anchors: Abstract notes "DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent"; section 2.2 states "Dummy classes only provide additional gradient risk and no samples can be classified to them" and section 2.1 states "DuRM enlarges the dimension of output logits for better generalization"
- Break condition: If the number of dummy classes is too large relative to real classes, the gradient noise may overwhelm real class learning and hurt performance.

### Mechanism 3
- Claim: DuRM improves generalization in overfit scenarios by increasing model capacity without increasing model parameters.
- Mechanism: The expanded output layer increases the representational freedom of the classifier, allowing it to fit a smoother decision boundary, which reduces overfitting especially when data is limited.
- Core assumption: The added dummy classes are not treated as learnable parameters that significantly alter the model's capacity; the improvement comes from gradient dynamics, not increased model size.
- Evidence anchors: Abstract states "DuRM could consistently improve the performance under all tasks with an almost free lunch manner"; section 2.1 notes "DuRM is extremely easy to implement: just adding additional dimensions... to the output logits" and section 4 states "DuRM is almost a free lunch method"
- Break condition: If the model is already underfitting or has sufficient capacity, the added dummy classes may not improve or could degrade performance.

## Foundational Learning

- Concept: Gradient variance and its relationship to generalization
  - Why needed here: The paper's core claim hinges on gradient variance increasing and leading to better flat minima. Understanding this link is essential to grasp why DuRM works.
  - Quick check question: In a two-class problem, if you add 2 dummy classes and keep training the same, does the gradient variance for the real classes increase, decrease, or stay the same? (Answer: Increase, because of added push noise.)

- Concept: Flat minima and generalization
  - Why needed here: The paper argues that flatter minima generalize better. This is a foundational assumption in the analysis.
  - Quick check question: If two models achieve the same training loss, but one has a much wider basin in loss landscape, which one is likely to generalize better? (Answer: The flatter one.)

- Concept: Empirical risk minimization (ERM) baseline
  - Why needed here: DuRM is framed as an extension to ERM. Understanding ERM's limitations is key to appreciating DuRM's contribution.
  - Quick check question: In ERM, what is the training objective? (Answer: Minimize average loss over the training set.)

## Architecture Onboarding

- Component map: Input -> Model -> Modified output layer (C + Cd classes) -> Standard cross-entropy loss (only on first C classes)
- Critical path: 1) Load model 2) Replace final layer with (C + Cd) outputs 3) During training, compute loss only on real classes 4) During inference, take argmax over first C outputs
- Design tradeoffs: Dummy class count - too few may not improve generalization; too many may degrade it. Model compatibility - works with any architecture that ends in a linear layer. Training overhead - negligible.
- Failure signatures: Performance drops if dummy classes are too numerous relative to real classes; no improvement if the model is already well-regularized or underfitting; gradient explosion if dummy class outputs are not masked out properly in loss computation.
- First 3 experiments: 1) CIFAR-10 with ResNet-18: Compare ERM vs DuRM-1 vs DuRM-2 on test accuracy and training loss. 2) Long-tailed CIFAR-10: Test DuRM's effect under severe class imbalance. 3) Semantic segmentation (Cityscapes): Apply DuRM-1 to FCN-ResNet101 and compare mIoU with baseline.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Theoretical claims about gradient variance leading to flatter minima are not rigorously proven
- Empirical evidence relies on standard benchmarks and may not generalize to all domains or architectures
- Optimal number of dummy classes appears task-dependent and is not derived from theory

## Confidence
- High confidence: The empirical improvements across multiple tasks and datasets are reproducible and significant.
- Medium confidence: The mechanism of increased gradient variance contributing to better generalization is plausible but not definitively proven.
- Low confidence: The theoretical connection between gradient variance and flat minima convergence lacks rigorous proof.

## Next Checks
1. Analyze the Hessian spectrum of models trained with DuRM to empirically verify flatter minima.
2. Test DuRM on non-image domains (e.g., NLP or tabular data) to assess broader applicability.
3. Conduct ablation studies varying dummy class counts systematically to derive optimal ratios.