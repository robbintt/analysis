---
ver: rpa2
title: Inductive Link Prediction in Knowledge Graphs using Path-based Neural Networks
arxiv_id: '2312.10293'
source_url: https://arxiv.org/abs/2312.10293
tags:
- inductive
- prediction
- link
- graph
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SiaILP, a path-based model for inductive link
  prediction using siamese neural networks. Unlike embedding-based models, SiaILP
  depends only on relation and path embeddings, enabling it to generalize to unseen
  entities without fine-tuning.
---

# Inductive Link Prediction in Knowledge Graphs using Path-based Neural Networks

## Quick Facts
- arXiv ID: 2312.10293
- Source URL: https://arxiv.org/abs/2312.10293
- Reference count: 35
- Key outcome: SiaILP achieves state-of-the-art performance on inductive link prediction using only relation and path embeddings, without entity fine-tuning

## Executive Summary
This paper introduces SiaILP, a path-based model for inductive link prediction in knowledge graphs. Unlike traditional embedding-based approaches that require fine-tuning for unseen entities, SiaILP leverages siamese neural networks to process paths defined by relation sequences. The model demonstrates strong generalization capabilities, achieving new state-of-the-art results on inductive versions of WN18RR, FB15k-237, and Nell995 datasets. Its effectiveness stems from avoiding entity-specific parameters and instead learning from relation and path structures.

## Method Summary
SiaILP addresses inductive link prediction by constructing paths between entities using only relation sequences, then processing these paths through siamese neural networks with bi-LSTM layers and max-pooling. The model employs contrastive learning with negative sampling, training to distinguish correct links from incorrect ones without requiring entity embeddings. Key parameters include 300-dimensional relation embeddings, 150-dimensional hidden units, path length L=10, path count C=20000, and 50 out-reaching paths per entity.

## Key Results
- Achieves state-of-the-art performance on inductive link prediction across multiple datasets
- Excels particularly on relation-corrupted rankings compared to embedding-based models
- Demonstrates strong generalization to unseen entities without requiring fine-tuning
- Shows effectiveness attributed to path-based structure capturing topological information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SiaILP avoids fine-tuning on unseen entity embeddings by using only relation and path embeddings.
- Mechanism: The model encodes the knowledge graph purely through paths, each defined by a sequence of relations, and embeds only relations. This means that when new entities appear, their paths can be constructed from existing relation embeddings without needing new entity-specific parameters.
- Core assumption: Relation embeddings and path structures are sufficient to capture the predictive signal for link prediction, even when source or target entities are unseen.
- Evidence anchors:
  - [abstract] "Our model only depends on relation and path embeddings, which can be generalized to new entities without fine-tuning."
  - [section 1] "In contrast, many rule-based link prediction models explicitly capture entity-invariant topological structures from the training knowledge graph."
- Break condition: If the predictive signal relies heavily on entity-specific properties that are not captured by relation sequences, the model will fail to generalize.

### Mechanism 2
- Claim: The siamese neural network architecture allows path-based features to be extracted in a parameter-sharing way, improving inductive generalization.
- Mechanism: Multiple paths (connection-based) or subgraphs (subgraph-based) are processed through identical network stacks. Shared weights force the model to learn a general function of path structure rather than memorizing specific entity patterns.
- Core assumption: The learned path representation function is invariant to which specific entities the paths connect, as long as the relation sequences are similar.
- Evidence anchors:
  - [section 3] "Stack one takes V1 as input, stack two takes V2, and stack three takes V3. Each stack...consists of two layers of bi-directional LSTM...and one max-pooling layer."
  - [section 3] "Within a bi-LSTM layer, the hidden vectors from forward and backward LSTM networks are concatenated at each time step."
- Break condition: If path features are too specific to certain relation sequences or if the path length exceeds the model's ability to generalize, performance will degrade.

### Mechanism 3
- Claim: Contrastive learning with negative sampling trains the model to distinguish correct links from incorrect ones without requiring entity labels.
- Mechanism: For each true triple, a random relation is substituted to form a negative sample. The model learns to assign higher scores to true triples by contrasting them with negatives using only relation embeddings and path structures.
- Core assumption: Negative sampling with random relations provides sufficient signal to learn a discriminative scoring function based solely on path and relation embeddings.
- Evidence anchors:
  - [section 4] "We randomly select three paths p1, p2, and p3 from s to t...provide the model with the output embedding of the target relation...a label of 1 for the true relation r and 0 for the random relation r′."
  - [section 4] "We set the learning rate is 10−5, the batch size is 32 and the training epoch is 10 across all models on all datasets."
- Break condition: If negative sampling does not cover a representative set of incorrect relations, the model may not learn robust discrimination.

## Foundational Learning

- Concept: Inverse relations
  - Why needed here: The model works on inverse-added graphs to allow paths to be traversed in both directions, enriching path diversity and symmetry.
  - Quick check question: Why do we add inverse relations to the knowledge graph in SiaILP?

- Concept: Path representation without entity nodes
  - Why needed here: SiaILP relies on relation sequences only, not entity embeddings, to achieve strict inductive generalization.
  - Quick check question: How does SiaILP represent a path between two entities?

- Concept: Siamese neural networks
  - Why needed here: Parameter sharing across multiple paths enforces that the model learns general path features rather than memorizing entity-specific patterns.
  - Quick check question: What is the role of the siamese architecture in SiaILP?

## Architecture Onboarding

- Component map: Relation embeddings (input and output) -> path embedding sequences -> three siamese stacks (each with bi-LSTM layers and max-pooling) -> feed-forward network for final scoring -> dot product with target relation embedding -> score
- Critical path: Construct paths → embed relations → process through siamese stacks → concatenate → feed-forward → dot product with target relation embedding → score
- Design tradeoffs: No entity embeddings simplifies inductive generalization but limits the model's ability to capture entity-specific features; longer paths increase expressiveness but also computational cost and risk of overfitting to noise
- Failure signatures: Poor performance on datasets with few relation types or dense connectivity (like WN18RR), overfitting to path patterns, inability to generalize when paths are too long or too short
- First 3 experiments:
  1. Train on FB15k-237 with L=5, C=10000, N=30 to observe baseline performance
  2. Vary path length L from 3 to 10 to study impact on AUC-PR
  3. Compare solo vs hybrid model settings on Nell-995 to evaluate trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SiaILP compare to embedding-based models when applied to knowledge graphs with a very high number of relation types and sparse connections?
- Basis in paper: [inferred] The paper discusses the performance of SiaILP on datasets with varying relation types and connection densities, indicating a trade-off between prediction accuracy and strict inductive link prediction capability.
- Why unresolved: The paper does not provide specific experiments or results for knowledge graphs with extremely high relation types and sparse connections.
- What evidence would resolve it: Conducting experiments on datasets specifically designed to have a high number of relation types and sparse connections, and comparing the performance of SiaILP to embedding-based models on these datasets.

### Open Question 2
- Question: Can the path-finding algorithm be further optimized to reduce computational complexity without sacrificing performance?
- Basis in paper: [explicit] The paper mentions that the upper bounds on path lengths are chosen in balance of performance optimization and computational complexity.
- Why unresolved: The paper does not explore potential optimizations of the path-finding algorithm to reduce computational complexity.
- What evidence would resolve it: Developing and testing alternative path-finding algorithms or optimizations that maintain or improve performance while reducing computational complexity.

### Open Question 3
- Question: How does the choice of the number of paths (L) and the upper bound on the number of recursions (C) affect the performance of SiaILP on different types of knowledge graphs?
- Basis in paper: [explicit] The paper discusses the default settings for L and C and mentions that the upper bounds are chosen in balance of performance optimization and computational complexity.
- Why unresolved: The paper does not provide a detailed analysis of how varying L and C affects performance on different types of knowledge graphs.
- What evidence would resolve it: Conducting a systematic study varying L and C across different knowledge graph types and analyzing the impact on performance.

### Open Question 4
- Question: How does SiaILP perform on dynamic knowledge graphs where new entities and relations are continuously added?
- Basis in paper: [inferred] The paper focuses on static knowledge graphs and does not address the performance of SiaILP on dynamic knowledge graphs.
- Why unresolved: The paper does not provide experiments or analysis on dynamic knowledge graphs.
- What evidence would resolve it: Implementing and testing SiaILP on a dynamic knowledge graph dataset, measuring its ability to adapt to new entities and relations over time.

### Open Question 5
- Question: Can SiaILP be extended to handle more complex logical reasoning tasks beyond link prediction, such as entity resolution or rule mining?
- Basis in paper: [explicit] The paper focuses on inductive link prediction and does not explore extensions to other logical reasoning tasks.
- Why unresolved: The paper does not investigate the potential of SiaILP for tasks beyond link prediction.
- What evidence would resolve it: Adapting SiaILP to handle entity resolution or rule mining tasks and evaluating its performance on benchmark datasets for these tasks.

## Limitations
- Performance on WN18RR is notably weaker than other datasets, attributed to path-based models being less distinguishable in this domain without detailed analysis
- Recursive path-finding algorithm lacks implementation details, particularly regarding cycle handling and path selection criteria
- Contrastive learning relies on random negative sampling which may not provide sufficient coverage of incorrect relations for robust learning

## Confidence
- High Confidence: The core claim that SiaILP can perform inductive link prediction without entity embeddings is well-supported by the experimental results showing strong performance on unseen entities across multiple datasets.
- Medium Confidence: The explanation that the siamese architecture enforces parameter sharing for generalization is plausible but could benefit from more ablation studies isolating the effect of parameter sharing.
- Medium Confidence: The assertion that path-based models are less effective on WN18RR due to relation distinguishability requires further empirical validation with more detailed analysis.

## Next Checks
1. **Path Coverage Analysis**: Quantify the diversity and coverage of discovered paths across datasets to verify that the path-finding algorithm provides sufficient signal for link prediction, particularly on WN18RR.

2. **Negative Sampling Quality**: Evaluate different negative sampling strategies (e.g., relation-based vs. entity-based negatives) to determine if random relation substitution is optimal for contrastive learning in this context.

3. **Parameter Sharing Ablation**: Conduct experiments comparing the full siamese architecture against non-parameter-sharing variants to isolate the contribution of weight sharing to inductive generalization.