---
ver: rpa2
title: 'Comprehensive framework for evaluation of deep neural networks in detection
  and quantification of lymphoma from PET/CT images: clinical insights, pitfalls,
  and observer agreement analyses'
arxiv_id: '2311.09614'
source_url: https://arxiv.org/abs/2311.09614
tags:
- lesion
- were
- test
- networks
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates four deep neural networks (UNet, SegResNet,
  DynUNet, and SwinUNETR) for automated segmentation of lymphoma lesions in PET/CT
  images. The models were trained and validated on 611 multi-institutional cases,
  including both internal and external test sets, covering various lymphoma subtypes.
---

# Comprehensive framework for evaluation of deep neural networks in detection and quantification of lymphoma from PET/CT images: clinical insights, pitfalls, and observer agreement analyses

## Quick Facts
- arXiv ID: 2311.09614
- Source URL: https://arxiv.org/abs/2311.09614
- Reference count: 31
- SegResNet achieved the highest median DSC of 0.76 and 0.68 on internal and external test sets, respectively

## Executive Summary
This study presents a comprehensive evaluation framework for deep neural networks in automated segmentation of lymphoma lesions from PET/CT images. Four architectures (UNet, SegResNet, DynUNet, and SwinUNETR) were trained and validated on 611 multi-institutional cases across various lymphoma subtypes. The framework introduces lesion detection criteria and performs detailed observer agreement analyses, demonstrating that network errors closely resemble those made by expert physicians. SegResNet achieved the highest median Dice similarity coefficient (DSC) on both internal and external test sets.

## Method Summary
The study evaluated four deep neural networks for automated segmentation of lymphoma lesions in PET/CT images. Models were trained on 611 multi-institutional cases from four cohorts, using patch-based training with non-randomized and randomized augmentations. The evaluation included traditional segmentation metrics (DSC, FPV, FNV) alongside three lesion detection criteria and comprehensive analysis of lesion-specific measures. Intra- and inter-observer variability analyses were conducted to compare network performance with expert physicians.

## Key Results
- SegResNet achieved the highest median DSC of 0.76 on internal test set and 0.68 on external test set
- Models generally performed better on cases with larger patient-level lesion measures (SUVmean, SUVmax, TMTV, TLG), though performance plateaued for very high values
- Network errors closely resembled those made by expert physicians in intra- and inter-observer variability analyses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger patch sizes during training improve model performance by providing more contextual information.
- **Mechanism**: Larger patches allow the neural network to capture a broader spatial context, which is particularly beneficial for identifying and segmenting lesions that vary significantly in size and metabolic activity.
- **Core assumption**: The increased context from larger patches outweighs any potential downsides like higher computational cost or overfitting to local patterns.
- **Evidence anchors**:
  - [abstract]: "The SegResNet had the highest median DSC on both internal and external test sets...SegResNet and UNet were trained on patches of larger sizes...while DynUNet and SwinUNETR were trained using relatively smaller patches"
  - [section]: "Utilizing larger patch sizes during training allows the neural networks to capture a more extensive contextual understanding of the data, thereby enhancing its performance in segmentation tasks"
- **Break condition**: If the lesions are consistently small and localized, the benefit of larger patches may diminish, and smaller patches with more samples could be more effective.

### Mechanism 2
- **Claim**: Model performance plateaus for very high values of lesion measures, suggesting diminishing returns.
- **Mechanism**: As lesion measures (e.g., SUV mean, SUV max, TMTV) increase, the model's ability to accurately segment improves until a certain threshold, after which additional increases in these measures do not significantly enhance performance.
- **Core assumption**: The model has learned the relevant features for segmentation and additional context from higher lesion measures does not provide new information.
- **Evidence anchors**:
  - [abstract]: "We showed that, in general, on a set of images with larger patient level lesion SUVmean, SUV mean, TMTV, and TLG, a network is able to predict a higher median DSC, although for very high values of these lesion measures, the performance generally plateaus"
  - [section]: "MAPE generally decreases as a function of lesion measure values...The networks generally made significant errors in the accurate prediction when the ground truth lesion measures were very small"
- **Break condition**: If the dataset contains a wide range of lesion sizes and metabolic activities, the plateau effect may be less pronounced, and the model may continue to benefit from higher lesion measures.

### Mechanism 3
- **Claim**: Detection criteria are more clinically relevant than traditional segmentation metrics for certain use cases.
- **Mechanism**: Detection criteria (e.g., identifying at least one voxel of a lesion, counting lesions, or segmenting based on metabolic characteristics) provide a different perspective on model performance that aligns with clinical needs for identifying and localizing lesions, rather than just accurately delineating their boundaries.
- **Core assumption**: The clinical value of lesion detection and localization outweighs the importance of precise boundary delineation in some scenarios.
- **Evidence anchors**:
  - [abstract]: "We introduced three lesion detection criteria to assess network performance at a per-lesion level, emphasizing their clinical relevance"
  - [section]: "The ability to detect the presence of lesions (Criterion 1) is crucial, as it directly influences whether a potential health concern is identified or missed"
- **Break condition**: If the primary clinical goal is precise volumetric quantification for treatment planning, traditional segmentation metrics may be more appropriate than detection criteria.

## Foundational Learning

- **Concept**: Understanding of Dice Similarity Coefficient (DSC) and its limitations
  - **Why needed here**: DSC is a primary metric for evaluating segmentation performance, but it may not fully capture clinical relevance or lesion-specific measures.
  - **Quick check question**: What are the limitations of DSC in evaluating segmentation performance, particularly for small lesions or false positives?

- **Concept**: Knowledge of PET/CT imaging and lymphoma characteristics
  - **Why needed here**: The study focuses on lymphoma lesion segmentation in PET/CT images, requiring an understanding of imaging modalities, lesion characteristics, and clinical relevance.
  - **Quick check question**: How do lymphoma lesions vary in size, shape, and metabolic activity, and why is this variability important for segmentation?

- **Concept**: Familiarity with deep learning architectures for medical image segmentation
  - **Why needed here**: The study evaluates four different neural network architectures, each with its own strengths and weaknesses in segmenting lymphoma lesions.
  - **Quick check question**: What are the key differences between U-Net, SegResNet, DynUNet, and SwinUNETR, and how might these differences affect their performance on lymphoma segmentation?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Model training -> Evaluation -> Postprocessing
- **Critical path**:
  1. Data preprocessing and augmentation
  2. Model training and validation
  3. Evaluation on internal and external test sets
  4. Analysis of lesion measures and detection criteria
  5. Intra- and inter-observer variability assessment

- **Design tradeoffs**:
  - Patch size vs. computational cost: Larger patches provide more context but increase computational requirements.
  - Model complexity vs. generalization: More complex models may perform better on internal test sets but may not generalize as well to external data.
  - Segmentation accuracy vs. detection sensitivity: Precise boundary delineation may not always align with the clinical need to detect and localize lesions.

- **Failure signatures**:
  - High FPV or FNV: Indicates issues with false positives or false negatives, respectively.
  - Low DSC on external test set: Suggests poor generalization to unseen data.
  - Inability to reproduce lesion measures: Indicates that the model is not accurately capturing the relevant features for quantification.

- **First 3 experiments**:
  1. Train and evaluate the models on the internal test set to establish a baseline performance.
  2. Evaluate the models on the external test set to assess generalization and identify any overfitting issues.
  3. Analyze the reproducibility of lesion measures and the performance on detection criteria to understand the clinical relevance of the models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do deep learning models trained on specific lymphoma subtypes generalize to other subtypes not included in the training data?
- Basis in paper: [explicit] The paper mentions that most existing studies focus on a single lymphoma subtype (e.g., DLBCL) and highlights the need to evaluate model generalizability across diverse subtypes.
- Why unresolved: While the paper evaluates models on multiple subtypes (DLBCL and PMBCL), it does not explicitly test the models on entirely different lymphoma subtypes beyond these two.
- What evidence would resolve it: Testing the models on a broader range of lymphoma subtypes and comparing their performance to ensure robustness and generalizability.

### Open Question 2
- Question: What is the impact of using larger patch sizes and batch sizes on the performance of transformer-based models like SwinUNETR in lymphoma segmentation?
- Basis in paper: [explicit] The paper notes that SegResNet and UNet, which used larger patch sizes, outperformed DynUNet and SwinUNETR, which used smaller patches. It also mentions computational resource limitations as a barrier to training transformer models with larger patches.
- Why unresolved: The study did not explore the full potential of transformer-based models due to hardware constraints, leaving their performance with larger patches untested.
- What evidence would resolve it: Training transformer models with larger patch and batch sizes to directly compare their performance with CNN-based models.

### Open Question 3
- Question: How can inter-observer variability in manual annotations be minimized to improve the quality of ground truth data for training deep learning models?
- Basis in paper: [explicit] The paper highlights the challenges of inter-observer variability and emphasizes the need for a standardized ground truth segmentation protocol involving multiple expert annotators.
- Why unresolved: While the paper discusses the importance of addressing inter-observer variability, it does not provide a detailed solution or protocol to minimize it.
- What evidence would resolve it: Developing and implementing a standardized annotation protocol, followed by an evaluation of its impact on inter-observer agreement and model performance.

## Limitations
- The study does not fully address variability in PET/CT scanner manufacturers and acquisition protocols across institutions, which could impact model generalization.
- While detection criteria are introduced, their clinical utility in actual treatment decision-making remains to be validated.
- The analysis focuses primarily on quantitative metrics without extensive qualitative assessment of segmentation quality in challenging cases.

## Confidence
- **High Confidence**: Model architecture descriptions, training procedures, and evaluation metrics are well-documented and reproducible.
- **Medium Confidence**: Performance differences between architectures are clearly demonstrated, though the optimal architecture may depend on specific clinical contexts not fully explored.
- **Medium Confidence**: Clinical relevance of detection criteria is theoretically sound but lacks prospective clinical validation.

## Next Checks
1. Conduct prospective clinical validation of detection criteria in treatment planning scenarios to assess actual clinical impact.
2. Evaluate model performance across different PET/CT scanner manufacturers and acquisition protocols to quantify generalization capabilities.
3. Perform ablation studies on patch size and augmentation strategies to optimize the balance between computational efficiency and segmentation accuracy.