---
ver: rpa2
title: Learning Disentangled Discrete Representations
arxiv_id: '2307.14151'
source_url: https://arxiv.org/abs/2307.14151
tags:
- discrete
- latent
- space
- learning
- disentanglement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using discrete latent representations for learning
  disentangled representations. The key idea is that the underlying grid structure
  of categorical distributions mitigates rotational invariance issues associated with
  Gaussian distributions, acting as an efficient inductive prior for disentanglement.
---

# Learning Disentangled Discrete Representations

## Quick Facts
- arXiv ID: 2307.14151
- Source URL: https://arxiv.org/abs/2307.14151
- Reference count: 40
- Primary result: Discrete VAEs achieve state-of-the-art disentanglement on multiple datasets

## Executive Summary
This paper proposes using discrete latent representations for learning disentangled representations in variational autoencoders. The key insight is that categorical distributions have an underlying grid structure that mitigates rotational invariance issues associated with Gaussian distributions, acting as an efficient inductive prior for disentanglement. The authors introduce a categorical VAE that maps discrete categories into a shared unit interval, enabling a definition of neighborhoods in the latent space. This approach encourages neighboring points in the data space to be represented close together in the latent space, and the discrete latent space is less rotation-prone than its Gaussian counterpart.

## Method Summary
The method involves replacing the standard Gaussian posterior in VAEs with Gumbel-Softmax distributions to enable discrete latent representations. The categorical VAE maps n-dimensional discrete categories into a shared unit interval, where each dimension has m categories. Total correlation regularization is applied through a discriminator network (FactorDVAE) to encourage independence between latent dimensions. The model is trained using the standard ELBO objective with straight-through gradient estimation. An unsupervised model selection strategy based on the Straight-Through Gap is proposed, which correlates with disentanglement metrics. The method also incorporates label information through semi-supervised training with masked attention for known factor cardinalities.

## Key Results
- Discrete VAE outperforms Gaussian VAE counterpart in terms of disentanglement metrics across six benchmark datasets
- FactorDVAE achieves new state-of-the-art MIG scores on most datasets (dSprites, Shapes3D, MPI3D)
- Straight-Through Gap provides an unsupervised score correlated with disentanglement metrics
- Incorporating label information can further enhance discrete representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The discrete latent space structure mitigates rotational invariance issues that harm disentanglement in Gaussian VAEs.
- Mechanism: Categorical distributions have an underlying grid structure that is only invariant under axially aligned rotations, whereas Gaussian distributions are equivariant under arbitrary rotations when variances are equal.
- Core assumption: The ground-truth factors of variation can be represented as discrete categories in each latent dimension.
- Evidence anchors:
  - [abstract] "the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions"
  - [section 3.3] Proposition 2 proves rotational equivariance for Gaussian VAEs and contrasts it with the discrete case
  - [corpus] Weak - only tangentially related papers on discrete VAEs, no direct comparison of rotational properties
- Break Condition: If the data generating process requires continuous latent factors that cannot be adequately discretized, or if the number of categories is too small to capture the necessary variations.

### Mechanism 2
- Claim: Neighboring points in the data space are encouraged to be represented close together in the latent space.
- Mechanism: The reconstruction loss topology, combined with the one-dimensional mapping of categories into a shared unit interval, creates neighborhoods in the latent space that align with data space neighborhoods.
- Core assumption: The ELBO loss function creates a topology in the data space that can be mapped to a topology in the latent space.
- Evidence anchors:
  - [section 3.1] "neighboring points in the data space are encouraged to be also represented close together in the latent space"
  - [section 3.2] Proposition 1 and subsequent discussion show how the discrete representation encourages this property
  - [corpus] Weak - general discussion of discrete VAEs but no specific evidence about neighborhood preservation
- Break Condition: If the dataset has complex, non-local relationships between data points that cannot be captured by the discrete latent space topology.

### Mechanism 3
- Claim: The Straight-Through Gap provides an unsupervised score correlated with disentanglement metrics.
- Mechanism: The gap between the rounded and original ELBO is zero when the latent space is truly discrete, and smaller gaps correlate with better disentanglement.
- Core assumption: A truly discrete latent space will have lower reconstruction loss and better disentanglement properties.
- Evidence anchors:
  - [section 3.4] "we leverage this property by selecting models that yield discrete latent spaces" and correlation analysis with disentanglement metrics
  - [abstract] "the categorical variational autoencoder admits an unsupervised disentangling score that is correlated with several disentanglement metrics"
  - [corpus] Missing - no corpus evidence for this specific claim
- Break Condition: If the model converges to a local minimum with partially discrete latent representations, or if the correlation between gap and disentanglement breaks down for certain datasets.

## Foundational Learning

- Concept: Variational Autoencoders and ELBO optimization
  - Why needed here: The paper builds on VAE foundations but replaces the Gaussian assumption with categorical distributions
  - Quick check question: What is the relationship between the ELBO objective and the reconstruction loss topology in VAEs?

- Concept: Disentanglement metrics and their assumptions
  - Why needed here: The paper evaluates performance using established disentanglement metrics that assume ordered latent spaces
  - Quick check question: How does the MIG metric differ from other disentanglement metrics in terms of what it measures?

- Concept: Categorical distributions and the Gumbel-Softmax trick
  - Why needed here: The discrete VAE uses Gumbel-Softmax distributions as a continuous relaxation of categorical distributions
  - Quick check question: What is the relationship between the temperature parameter in Gumbel-Softmax and the discreteness of the samples?

## Architecture Onboarding

- Component map:
  - Input data → Encoder → Gumbel-Softmax parameters
  - Sample from Gumbel-Softmax → Map to unit interval → Discrete latent representation
  - Discrete latent representation → Decoder → Reconstructed data
  - Compute ELBO loss (reconstruction + KL divergence)
  - (Optional) Compute total correlation penalty using discriminator
  - Backpropagate gradients using straight-through estimator

- Critical path:
  1. Input data → Encoder → Gumbel-Softmax parameters
  2. Sample from Gumbel-Softmax → Map to unit interval → Discrete latent representation
  3. Discrete latent representation → Decoder → Reconstructed data
  4. Compute ELBO loss (reconstruction + KL divergence)
  5. (Optional) Compute total correlation penalty using discriminator
  6. Backpropagate gradients using straight-through estimator

- Design tradeoffs:
  - Number of categories (m): Higher values provide finer granularity but increase computational cost and may reduce discreteness
  - Latent space dimension (n): Must balance between capturing all factors and maintaining interpretability
  - Temperature annealing schedule: Affects convergence stability and final discreteness of representations

- Failure signatures:
  - Poor reconstruction quality: May indicate insufficient capacity in encoder/decoder or inadequate number of categories
  - Latent space not discrete: Check Gumbel-Softmax temperature and annealing schedule
  - Low disentanglement scores: May indicate need for total correlation regularization or insufficient training

- First 3 experiments:
  1. Train a basic discrete VAE on dSprites with default parameters and visualize latent space traversals
  2. Compare reconstruction quality and disentanglement metrics between discrete and Gaussian VAEs on the same dataset
  3. Implement FactorDVAE with different γ values and evaluate impact on total correlation and disentanglement scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the discrete VAE's performance scale with increasing number of categories per latent dimension?
- Basis in paper: [explicit] The paper uses 64 categories by default but does not explore the impact of varying this number
- Why unresolved: The paper only mentions using 64 categories as a default without exploring the impact of varying this number
- What evidence would resolve it: Systematic experiments varying the number of categories per dimension while keeping other parameters constant

### Open Question 2
- Question: Can the straight-through gap be used as a reliable model selection criterion across different architectures and datasets?
- Basis in paper: [explicit] The paper shows correlation between GapST and disentanglement metrics but notes the correlation is not strong enough for reliable selection
- Why unresolved: The paper acknowledges the correlation exists but is not strong enough for reliable selection across all datasets
- What evidence would resolve it: Extensive experiments across diverse architectures and datasets to establish reliability bounds

### Open Question 3
- Question: How does the discrete VAE perform on real-world datasets with continuous ground-truth factors?
- Basis in paper: [inferred] All experiments use datasets with discrete ground-truth factors, making it unclear how the model generalizes to continuous factors
- Why unresolved: The paper focuses entirely on datasets with discrete ground-truth factors, leaving real-world applicability unexplored
- What evidence would resolve it: Experiments on real-world datasets with continuous factors and comparison to Gaussian VAE performance

## Limitations
- The advantage of discrete latent spaces may be partially dataset-dependent, particularly for datasets with continuous ground-truth factors that require discretization
- The correlation between Straight-Through Gap and disentanglement metrics, while positive, is not strong enough for reliable unsupervised model selection
- The rotational invariance argument, while compelling, lacks rigorous mathematical proof beyond Proposition 2

## Confidence

- **High**: The empirical demonstration that discrete VAEs achieve higher MIG scores than Gaussian VAEs on multiple datasets (dSprites, Shapes3D, MPI3D)
- **Medium**: The theoretical arguments about rotational invariance and neighborhood preservation in discrete spaces
- **Medium**: The correlation between Straight-Through Gap and established disentanglement metrics

## Next Checks
1. Test the discrete VAE on datasets with known continuous ground-truth factors to assess how discretization affects disentanglement quality
2. Perform ablation studies varying the number of categories per dimension to identify optimal granularity for different datasets
3. Compare the computational efficiency of discrete versus Gaussian VAEs, particularly regarding training stability and convergence speed