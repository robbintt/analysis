---
ver: rpa2
title: Watermarking LLMs with Weight Quantization
arxiv_id: '2310.11237'
source_url: https://arxiv.org/abs/2310.11237
tags:
- watermarks
- watermarking
- quantized
- quantization
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes watermarking strategies for protecting large
  language models by embedding watermarks in the quantization process. The method
  plants watermarks in full-precision model weights such that the watermarks are present
  during inference in fp32 mode but hidden when the model is quantized to int8, thereby
  preventing unauthorized fine-tuning.
---

# Watermarking LLMs with Weight Quantization

## Quick Facts
- arXiv ID: 2310.11237
- Source URL: https://arxiv.org/abs/2310.11237
- Reference count: 8
- Primary result: Watermarking LLMs by embedding watermarks in quantization intervals, achieving 100% success in text-agnostic scenarios while maintaining quantized model fidelity.

## Executive Summary
This paper proposes a novel watermarking strategy for protecting large language models by leveraging the quantization gap between full-precision and 8-bit quantized weights. The approach plants watermarks in full-precision model weights such that they are visible during fp32 inference but hidden when the model is quantized to int8, thereby preventing unauthorized fine-tuning. Experiments with GPT-Neo and LLaMA demonstrate that the proposed interval optimization strategy achieves 100% watermark planting success in text-agnostic scenarios while maintaining 100% text fidelity in quantized models. However, the watermarks remain vulnerable to erasure through further pre-training on the same or related data.

## Method Summary
The method works by constraining weight updates within quantization intervals during training, ensuring that quantized model outputs remain unchanged while full-precision outputs can be watermarked. The interval optimization strategy clips gradients to prevent quantized output changes, while a rollback optimization baseline reverts parameter changes that would alter quantized values. Watermarks are embedded by optimizing weights within a ±0.4 range of their quantized values, allowing full-precision models to generate watermarked text while quantized models produce identical outputs to the original.

## Key Results
- Interval optimization strategy achieves 100% watermark planting success in text-agnostic scenarios
- Quantized models maintain 100% text fidelity (TMR = 100%) across all experiments
- Text-related watermarking successfully activates triggers without affecting non-trigger text outputs
- Watermarks are vulnerable to erasure through further pre-training on same or related data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The quantization gap creates a space for watermark injection without affecting quantized inference
- Mechanism: During INT8 quantization, weight values are mapped to integers 0-127. If a full-precision weight is within ±0.4 of its quantized value, it will round to the same integer, preserving quantized inference while allowing full-precision outputs to differ
- Core assumption: The quantization function is deterministic and monotonic within the quantization range
- Evidence anchors:
  - [abstract] "The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8"
  - [section 3.1] "the interval is ranged from θi* l = θi* − α to θi* h = θi* + α, where α = 0.4 in the INT8 quantization"
  - [corpus] Weak - no direct citations found about quantization gaps for watermarking
- Break condition: If quantization uses different rounding or non-monotonic mapping, the ±0.4 interval assumption fails

### Mechanism 2
- Claim: Interval optimization allows watermark embedding while maintaining quantized model fidelity
- Mechanism: During training, gradients are clipped to the interval [θi - β, θi + β] so that updates stay within the quantization-preserving range. This lets the full-precision model generate watermarked text while the quantized model remains unchanged
- Core assumption: Gradient clipping to the interval preserves both watermark strength and quantized performance
- Evidence anchors:
  - [section 3.3] "When updating the parameters during watermark planting, we normalize the gradients based on the interval size β"
  - [section 4.2] "the interval optimization strategy... successfully plant watermarks"
  - [corpus] Weak - no direct citations found about interval-based gradient clipping for watermarking
- Break condition: If β is too small, watermark strength diminishes; if too large, quantized outputs change

### Mechanism 3
- Claim: Rollback optimization provides a baseline that maintains quantized outputs by reverting parameter changes
- Mechanism: After each training step, parameters that cause quantized values to differ from original are reverted to original values, ensuring quantized model stability
- Core assumption: Reverting parameters is sufficient to maintain quantized model outputs
- Evidence anchors:
  - [section 3.3] "roll back parameters if the parameters are changed drastically after quantization"
  - [section 4.2] "Direct Optimization strategy cannot hold the quantized model unchanged"
  - [corpus] Weak - no direct citations found about rollback strategies for model watermarking
- Break condition: If rollback threshold is too high, watermarks don't plant; if too low, quantized outputs change

## Foundational Learning

- Concept: INT8 quantization and de-quantization process
  - Why needed here: The watermarking strategy relies on the gap between full-precision and quantized weights
  - Quick check question: What happens to a full-precision weight of 1.3 when quantized with scale index 127 using the 8-bit method described?
- Concept: Gradient clipping and interval optimization
  - Why needed here: The interval optimization strategy clips gradients to prevent quantized output changes
  - Quick check question: Why is it necessary to normalize gradients by the interval size β in the interval optimization approach?
- Concept: Watermarking vs. backdoor injection
  - Why needed here: This work distinguishes itself by embedding watermarks in weights rather than relying on trigger tokens
  - Quick check question: How does watermarking via quantization differ fundamentally from traditional trigger-based backdoor approaches?

## Architecture Onboarding

- Component map: Quantization module (Q) → Interval optimization trainer → Full-precision model (θ) → De-quantization module (D) → Quantized model (θ*)
- Critical path: Training data → Watermark loss computation → Interval-constrained gradient update → Parameter update → Verification of WPR and TMR metrics
- Design tradeoffs: Larger interval α allows stronger watermarks but risks quantized output changes; smaller interval preserves quantized outputs but weakens watermark strength
- Failure signatures: TMR drops below 100% (quantized outputs change); WPR remains low despite training (watermarks not embedding)
- First 3 experiments:
  1. Test rollback optimization with varying thresholds (1-63) on GPT-Neo to find optimal rollback point
  2. Implement interval optimization with α=0.4 on LLaMA 7B and measure WPR/TMR on text-agnostic scenario
  3. Test text-related watermarking with wiki triggers on GPT-Neo and evaluate generalization to non-trigger texts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can quantization watermarking be made more resistant to erasure by further pre-training?
- Basis in paper: [explicit] The paper states that watermarks can be easily erased by further pre-training on the same or related data, which is a major limitation
- Why unresolved: The authors acknowledge this as a limitation but do not propose concrete solutions to make the watermarks more persistent
- What evidence would resolve it: Experiments demonstrating that the proposed watermarking method remains intact after various types of further pre-training, including on in-domain and out-of-domain data

### Open Question 2
- Question: Can quantization watermarking be effectively combined with other watermarking techniques (e.g., trigger-based or decoding-based methods) to enhance robustness?
- Basis in paper: [inferred] The paper mentions that combining quantization watermarks with traditional methods could be a future direction, implying potential benefits but no exploration
- Why unresolved: The authors suggest this as a future work direction but do not investigate or validate the effectiveness of such combinations
- What evidence would resolve it: Experimental results comparing the robustness and effectiveness of combined watermarking approaches against standalone quantization watermarking

### Open Question 3
- Question: How does the choice of quantization method (e.g., 8-bit vs. lower precision) affect the effectiveness and detectability of quantization watermarks?
- Basis in paper: [explicit] The paper uses 8-bit quantization as the primary method but does not explore the impact of using different quantization precisions
- Why unresolved: The authors focus on 8-bit quantization without testing how lower or higher precision quantization affects watermark planting and detection
- What evidence would resolve it: Comparative experiments using different quantization precisions to evaluate watermark success rates, detectability, and model performance

## Limitations
- Watermarks can be easily erased by further pre-training on the same or related data
- Security analysis is limited to simple fine-tuning and pre-training attacks
- Text-related watermarking experiments lack evaluation on out-of-distribution trigger patterns
- Implementation details for interval optimization and gradient clipping are underspecified

## Confidence

**High Confidence**: The core mechanism of using quantization intervals for watermark embedding is theoretically sound and the basic experimental results (100% WPR for text-agnostic scenarios, 100% TMR for quantization preservation) are reproducible based on the described methodology.

**Medium Confidence**: The interval optimization strategy's effectiveness across different model architectures and dataset types is supported by experiments with GPT-Neo and LLaMA, but the limited scope (two model sizes, specific datasets) means generalization to other LLMs remains uncertain.

**Low Confidence**: The security claims regarding resistance to erasure attacks are based on preliminary experiments with fine-tuning and pre-training on related data. The paper doesn't adequately address more sophisticated attack vectors or provide comprehensive robustness testing.

## Next Checks

1. **Quantization Interval Verification**: Implement the quantization function from Dettmers et al. (2022) and verify that the ±0.4 interval assumption holds across the full weight distribution. Test whether different scale factors or non-monotonic quantization schemes break the watermarking mechanism.

2. **Security Robustness Testing**: Design and execute attack experiments beyond simple fine-tuning: test model distillation attacks, adversarial fine-tuning with trigger inversion, and model patching techniques to assess whether watermarks can be removed or hidden without degrading model performance.

3. **Cross-Domain Generalization**: Evaluate the text-related watermarking approach on out-of-distribution trigger patterns and naturally occurring text that might accidentally trigger watermarks. Test whether the watermark system produces false positives on benign text and whether attackers can systematically generate watermarked outputs without knowing the trigger patterns.