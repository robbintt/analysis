---
ver: rpa2
title: Data-to-Text Bilingual Generation
arxiv_id: '2311.14808'
source_url: https://arxiv.org/abs/2311.14808
tags:
- data
- english
- attempts
- language
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents pyrealb, a Python-based bilingual text generation
  system for English and French. It demonstrates how data-to-text generation can be
  achieved with shared data selection and organization processes, while language-specific
  realization is handled through separate modules.
---

# Data-to-Text Bilingual Generation

## Quick Facts
- arXiv ID: 2311.14808
- Source URL: https://arxiv.org/abs/2311.14808
- Reference count: 0
- Primary result: Rule-based bilingual generation system achieving comparable quality to neural approaches with better speed and control

## Executive Summary
This paper presents pyrealb, a Python-based bilingual text generation system that produces equivalent information in English and French. The system uses a rule-based approach with shared data selection and organization across languages, while language-specific realization is handled through separate modules. The object-oriented design allows language-independent algorithms to be implemented in a base class, with language-specific phrasings handled in subclasses. The system demonstrates applications in restaurant descriptions, weather reports, and basketball summaries, showing that pyrealb can generate equivalent bilingual text reliably while offering advantages in speed, memory usage, and control over output compared to GPT-based generation.

## Method Summary
The method uses a rule-based data-to-text generation approach implemented in Python, where language-independent algorithms are separated from language-specific phrasings through an object-oriented design. A Realizer base class implements core algorithms, while English and Francais subclasses handle language-specific realizations. The system uses delayed evaluation with lambda functions to ensure correct language context during expression evaluation, and generates text through Constituent notation using NP, VP, and PP classes. The approach relies on manually constructed lexicons, morphological rules, and syntactic patterns for each language.

## Key Results
- Successfully generates equivalent information in both English and French for restaurant descriptions, weather reports, and basketball summaries
- Demonstrates better speed and memory efficiency compared to GPT-based generation while maintaining comparable quality
- Shows that shared data selection and organization across languages enables strict bilingual equivalence without translation errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared data selection and organization across languages enables strict bilingual equivalence
- Mechanism: The system separates content determination from linguistic realization, performing data selection once and reusing it for both languages
- Core assumption: Content selection can be performed independently of linguistic expression
- Evidence anchors:
  - [abstract] "The data selection and text organisation processes are shared between the two languages"
  - [section 3.2] "This setup thus greatly simplifies ensuring that the same information is conveyed in both languages"
- Break condition: When content selection requires language-specific knowledge

### Mechanism 2
- Claim: Object-oriented organization with language-independent base class and language-specific subclasses enables flexible bilingual generation
- Mechanism: Realizer base class implements language-independent algorithms while subclasses handle language-specific phrasings
- Core assumption: Common functionality can be abstracted into a base class
- Evidence anchors:
  - [section 4.2] "The language independent algorithms and phrase choices are performed in a class"
  - [section 4.2] "The Realizer base class implements language-independent algorithms"
- Break condition: When language differences cannot be encapsulated in subclasses

### Mechanism 3
- Claim: Delayed evaluation with lambda functions prevents premature evaluation in bilingual context
- Mechanism: Lambda functions delay evaluation until after language context is set with loadEn() or loadFr()
- Core assumption: Language context must be established before evaluation of language-specific expressions
- Evidence anchors:
  - [section 3.2] "lambda:can be added before exp which creates a function whose body can be later evaluated by calling exp()"
- Break condition: When overhead of lambda functions becomes prohibitive

## Foundational Learning

- Concept: Object-oriented programming principles
  - Why needed here: The system uses inheritance and polymorphism to separate language-independent and language-dependent functionality
  - Quick check question: What is the relationship between the Realizer base class and the English/Francais subclasses?

- Concept: Delayed evaluation and lambda functions
  - Why needed here: Python's applicative order evaluation requires explicit mechanisms to delay evaluation of language-specific expressions
  - Quick check question: How does using lambda: differ from directly calling a function in terms of evaluation timing?

- Concept: Natural language generation pipeline
  - Why needed here: Understanding the separation between content determination and linguistic realization is crucial for implementing the bilingual generation approach
  - Quick check question: What are the two main subtasks in the classic data-to-text generation pipeline?

## Architecture Onboarding

- Component map:
  - Realizer base class -> implements language-independent algorithms
  - English/Francais subclasses -> implement language-specific phrasings
  - Lexicon JSON files -> contain word categories, genders, numbers
  - Morphological rules JSON -> for plurals, conjugations
  - Syntactic rules Python classes -> for sentence structures
  - pyrealb functions -> for actual text generation

- Critical path:
  1. Set language context with loadEn() or loadFr()
  2. Create sentence structure using pyrealb constructors
  3. Call realize() method to generate text
  4. For bilingual generation, repeat steps 1-3 with different language context

- Design tradeoffs:
  - Flexibility vs complexity: Object-oriented approach provides flexibility but adds complexity
  - Performance vs control: Symbolic approach offers more control but may be slower than neural methods
  - Maintenance vs expressiveness: Manual pattern creation is more maintainable but less expressive than learned approaches

- Failure signatures:
  - Incorrect language context leading to wrong word forms or agreements
  - Missing lexicon entries causing generation failures
  - Inconsistent subclass implementations leading to non-equivalent bilingual outputs
  - Performance issues when generating large volumes of text

- First 3 experiments:
  1. Create a simple bilingual sentence with shared structure and different word choices
  2. Implement a subclass for a third language following the existing pattern
  3. Add a new lexicon entry and verify it works in both languages

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does pyrealb's bilingual generation performance compare to neural methods when generating weather reports from complete meteorological datasets rather than the limited subset shown in the paper?
- Basis in paper: [explicit] The paper compares pyrealb to GPT for weather reports but only uses a subset of data without full context
- Why unresolved: The paper only tests on a small subset of weather data, while actual weather reports would require processing complete meteorological datasets
- What evidence would resolve it: A systematic comparison using complete weather datasets showing generation quality, accuracy of numerical data, and processing time for both pyrealb and a neural approach

Open Question 2
- Question: Can pyrealb's sentence patterns be effectively learned automatically from reference corpora rather than being manually constructed as described in the paper?
- Basis in paper: [explicit] "It would be interesting and challenging to explore the possibility of learning sentence patterns from corpora"
- Why unresolved: The paper presents manual construction of sentence patterns as the current approach and only mentions this as a future research direction
- What evidence would resolve it: A system that automatically extracts sentence patterns from reference texts and successfully generates equivalent bilingual output compared to manually constructed patterns

Open Question 3
- Question: How does pyrealb's performance scale when extending bilingual generation to languages with significantly different grammatical structures than English and French?
- Basis in paper: [inferred] The paper emphasizes English-French similarities and states "This approach could be extended to other languages provided that extensive lexicons and programs for implementing grammar rules are developed for them"
- Why unresolved: The paper only demonstrates the system for English-French pairs and doesn't test languages with different grammatical features
- What evidence would resolve it: Implementation of pyrealb for language pairs with different grammatical structures (e.g., English-Japanese or English-Arabic) showing comparable generation quality and control over output

## Limitations

- Scalability concerns when extending to languages with significantly different grammatical structures than English and French
- Maintenance overhead from manually creating lexicons and syntactic patterns for each language
- Limited empirical comparison data for large-scale generation tasks against neural approaches

## Confidence

- High confidence: Core mechanism of separating data selection from linguistic realization
- Medium confidence: Object-oriented implementation approach
- Low confidence: Claimed performance advantages over neural approaches

## Next Checks

1. Conduct systematic timing and memory usage comparisons between pyrealb and GPT-based generation across multiple text volumes and complexity levels
2. Track time and effort required to add new lexicon entries, syntactic patterns, and support for additional languages over a 3-month development period
3. Implement blind evaluation studies comparing bilingual outputs against professional human translations across multiple domains using both automated metrics and human judgment