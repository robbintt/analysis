---
ver: rpa2
title: 'LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual
  Images'
arxiv_id: '2305.19164'
source_url: https://arxiv.org/abs/2305.19164
tags:
- image
- predictions
- photo
- test
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce LANCE, a method for stress-testing visual
  models by generating language-guided counterfactual images. Their approach leverages
  recent advances in large language models (LLMs) and text-based image editing to
  augment IID test sets with diverse, realistic, and challenging test images without
  altering model weights.
---

# LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images

## Quick Facts
- **arXiv ID**: 2305.19164
- **Source URL**: https://arxiv.org/abs/2305.19164
- **Reference count**: 40
- **Key outcome**: Authors introduce LANCE, a method for stress-testing visual models by generating language-guided counterfactual images.

## Executive Summary
The paper introduces LANCE, a novel approach for stress-testing visual models by generating counterfactual images guided by language. The key insight is that language can serve as a structured intermediate scaffold to capture salient visual features while abstracting away irrelevant details. By perturbing captions using a finetuned LLM and converting these text edits back into visual counterfactuals via text-to-image generation, LANCE systematically creates challenging test images that maintain the ground truth label while altering specific visual attributes. This approach enables measuring model sensitivity to different types of visual variation without requiring model retraining or access to additional real data.

## Method Summary
LANCE generates counterfactual images through a pipeline: (1) caption generation using an image captioning model like BLIP-2, (2) structured caption perturbation using a finetuned LLM that edits specific aspects (subject, object, background, adjectives, domain) while preserving the ground truth category, (3) counterfactual image generation using a text-to-image latent diffusion model with prompt-to-prompt editing and null-text inversion, and (4) CLIP-based quality control to ensure semantic consistency and realism. The method benchmarks diverse pretrained ImageNet models and observes significant performance drops on the generated counterfactuals compared to baseline approaches.

## Key Results
- LANCE-generated counterfactuals cause significant and consistent performance drops across diverse ImageNet-pretrained models
- Changes to background and domain have the highest impact on model performance
- LANCE surfaces previously unknown class-level model biases in ImageNet
- The method demonstrates applicability for systematic stress-testing of visual models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language serves as a structured discrete scaffold that captures salient visual features while abstracting away irrelevant detail.
- **Mechanism**: The system first generates captions using an image captioning model, then uses an LLM to generate targeted perturbations to specific aspects (subject, object, background, adjectives, domain) while preserving the ground truth label. These text edits are then converted back into visual counterfactuals using a text-to-image latent diffusion model with prompt-to-prompt editing and null-text inversion.
- **Core assumption**: Text representations can effectively isolate and manipulate individual visual attributes without losing semantic coherence or visual fidelity when reconverted to images.
- **Evidence anchors**:
  - [abstract] "Our key insight is that while the scope of possible visual variation is vast, language can serve as a concise intermediate scaffold that captures salient visual features while abstracting away irrelevant detail."
  - [section 3.1] "Motivated by prior work in finetuning instruction-following LLMs [22], we seek to train a structured perturber LLM that can generate diverse and realistic caption edits that capture the long-tail of visual variation while still representing realistic scenarios."
- **Break condition**: If the text-to-image generation fails to maintain visual consistency with the original image while incorporating the text edits, or if the LLM cannot generate realistic perturbations that preserve the ground truth category.

### Mechanism 2
- **Claim**: Structured caption perturbation allows for systematic stress-testing of visual models across different types of visual variation.
- **Mechanism**: The system perturbs captions along five predefined dimensions (subject, object, background, adjectives, domain) using a finetuned LLM, creating counterfactual images that isolate each type of variation while keeping the ground truth label unchanged. This enables measuring model sensitivity to specific factors.
- **Core assumption**: Visual models exhibit systematic sensitivity patterns to different types of perturbations that can be reliably measured by comparing performance on original vs. counterfactual images.
- **Evidence anchors**:
  - [section 3.1] "To train our perturber model, we select five factors of visual variation against which we will stress-test."
  - [section 4.2] "We find that changes to the background have the highest impact, followed by domain."
- **Break condition**: If model performance changes are inconsistent across perturbations of the same type, or if perturbations fail to isolate single factors of variation.

### Mechanism 3
- **Claim**: CLIP-based quality control ensures generated counterfactuals are both realistic and meaningful for model evaluation.
- **Mechanism**: The system uses CLIP directional similarity to automatically tune the diffusion editing hyperparameter and verify that the edited image is more similar to the edited caption than the original. Additional checks ensure the ground truth category remains unchanged and image quality is maintained.
- **Core assumption**: CLIP embeddings can effectively measure semantic consistency between images and captions, serving as a proxy for realism and quality control in an automated pipeline.
- **Evidence anchors**:
  - [section 3.2] "We follow prior work [42] to automatically tune this hyperparameter: we sweep over a range of values and threshold based on a CLIP [27] directional similarity metric [57]."
  - [section 4.2] "Finally, to evaluate realism we also report the FID [60] score of our generated image test sets and perplexity of the generated and perturbed captions under LLAMA-7B [52]."
- **Break condition**: If CLIP similarity thresholds are too permissive or restrictive, allowing poor quality counterfactuals or filtering out valid ones.

## Foundational Learning

- **Concept**: Text-to-image generation with latent diffusion models
  - **Why needed here**: The core mechanism for converting text edits back into visual counterfactuals while maintaining visual consistency with the original image
  - **Quick check question**: What technique is used to inject cross-attention maps corresponding to caption edits during the diffusion process?

- **Concept**: Prompt-to-prompt image editing
  - **Why needed here**: Enables targeted modification of specific visual attributes while preserving overall image structure and semantics
  - **Quick check question**: How does prompt-to-prompt editing differ from standard text-to-image generation?

- **Concept**: Null-text inversion for real image editing
  - **Why needed here**: Allows the diffusion model to edit real images rather than just generated ones by optimizing null-text embeddings
  - **Quick check question**: What is the purpose of optimizing the null-text embedding during the DDIM inversion process?

## Architecture Onboarding

- **Component map**: Image captioning model → Structured caption perturber (finetuned LLM) → Text-to-image latent diffusion model (with prompt-to-prompt editing and null-text inversion) → CLIP-based quality control → Model evaluation
- **Critical path**: Original image → Caption generation → Caption perturbation → Counterfactual image generation → CLIP quality verification → Model prediction → Performance comparison
- **Design tradeoffs**: Using language as an intermediate representation enables systematic perturbations but may miss non-linguistic visual variations; CLIP-based quality control is automated but may not capture all aspects of realism
- **Failure signatures**: Inconsistent model sensitivity across similar perturbations; poor quality generated images despite CLIP filtering; perturbations that inadvertently change the ground truth category
- **First 3 experiments**:
  1. Verify that the caption generation produces descriptive captions by inspecting a sample of outputs
  2. Test the structured perturber on a small set of captions to ensure it generates meaningful edits of the correct type
  3. Validate that the image editing pipeline can successfully edit a simple image with a single attribute change and maintain visual consistency

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations
- The method relies heavily on the quality of the image captioning model and finetuned LLM, which could compromise the downstream counterfactual generation if either produces poor outputs
- CLIP-based quality control, while automated, may not capture all aspects of realism and could potentially filter out valid counterfactuals or allow poor quality ones
- The approach assumes that language-based perturbations can effectively isolate and manipulate visual attributes, potentially missing non-linguistic visual variations that affect model performance

## Confidence
- **High confidence**: The core insight that language can serve as a structured scaffold for systematic visual variation is well-supported by the evidence and aligns with prior work in text-to-image generation and prompt-based editing
- **Medium confidence**: The effectiveness of the structured caption perturbation for stress-testing visual models across different types of variation is demonstrated, but the consistency of model sensitivity patterns across similar perturbations warrants further validation
- **Medium confidence**: The CLIP-based quality control ensures realistic counterfactuals, but its limitations in capturing all aspects of realism and potential for false positives/negatives introduce some uncertainty

## Next Checks
1. **Validate caption perturbation consistency**: Measure the variance in model sensitivity across multiple perturbations of the same type (e.g., different background changes) to ensure the perturbation process consistently isolates single factors of variation
2. **Analyze CLIP filtering thresholds**: Experiment with different CLIP similarity thresholds to quantify their impact on the quality and diversity of the generated counterfactuals, ensuring a balance between realism and coverage of visual variation
3. **Test on out-of-distribution data**: Apply the method to test sets from different domains or datasets to assess whether the observed model sensitivity patterns generalize beyond ImageNet and capture broader aspects of model robustness