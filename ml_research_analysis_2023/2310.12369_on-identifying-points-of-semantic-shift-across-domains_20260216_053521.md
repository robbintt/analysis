---
ver: rpa2
title: On Identifying Points of Semantic Shift Across Domains
arxiv_id: '2310.12369'
source_url: https://arxiv.org/abs/2310.12369
tags:
- semantic
- domains
- terms
- term
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a manual case study to identify points where\
  \ academic terms semantically shifted from one domain to another, focusing on three\
  \ terms\u2014\"polymorphism,\" \"semaphore,\" and \"ontology\"\u2014in the context\
  \ of their evolution from their original fields into Computer Science. The method\
  \ involved iterative keyword searches in Web of Science and tracing citations to\
  \ find seminal mentions."
---

# On Identifying Points of Semantic Shift Across Domains

## Quick Facts
- arXiv ID: 2310.12369
- Source URL: https://arxiv.org/abs/2310.12369
- Reference count: 34
- One-line primary result: Manual case study successfully identifies when academic terms semantically shift across domains by tracking citation patterns and acceptance points

## Executive Summary
This paper presents a manual case study method for identifying when academic terms semantically shift from one domain to another, focusing on three terms—"polymorphism," "semaphore," and "ontology"—as they evolved into Computer Science. The method involves iterative keyword searches in Web of Science, tracing citations to find seminal mentions, and tracking when subsequent papers stop citing original definitions, indicating acceptance of new meanings. The study confirms that semantic shifts can be detected across domains and establishes a foundation for future automated detection methods.

## Method Summary
The method uses iterative keyword searches in Web of Science limited to CS-related fields to find earliest CS usage of terms, then traces backward through citations to identify original domain usage. Researchers manually verify term usage in full-text papers and track when citations to original definitions stop, marking this as the acceptance point. Google Ngram Viewer trends are used to correlate with potential semantic shifts. The approach combines manual verification with citation network analysis to establish timelines for semantic evolution.

## Key Results
- Successfully identified first CS usage: "polymorphism" by Milner (1978), "semaphore" by Denning et al. (1981), "ontology" by Lenat and Guha (1990)
- Tracked acceptance points where terms were cited without definition in subsequent literature
- Confirmed semantic shifts across domains are detectable through citation pattern analysis
- Found varying adoption rates: "polymorphism" and "semaphore" widely accepted while "ontology" still requires citations in 2023

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic shift detection across domains is possible by tracking when academic terms transition from needing citations to being used without citation.
- Mechanism: The paper identifies "seminal" usage of terms in a new domain by finding their earliest mention, then tracks when subsequent papers stop citing the original definition, indicating acceptance of the new meaning.
- Core assumption: When authors stop citing definitions for terms, it indicates the academic community has accepted the new semantic meaning.
- Evidence anchors:
  - [abstract]: "We marked these events as semantic evolution points."
  - [section 4.1]: "We performed the same search and manually checked articles for citations from recent studies. We found that Houben [18] explained polymorphism with citations within the study."
  - [corpus]: Weak evidence - corpus contains papers about ML data changes and OOD detection, not semantic shifts across domains.
- Break condition: If authors continue citing definitions indefinitely, the shift cannot be detected using this method.

### Mechanism 2
- Claim: Web of Science keyword searches combined with citation tracing can identify when terms from one domain enter another domain.
- Mechanism: The method uses keyword searches limited to CS-related fields, identifies earliest CS usage, then traces backward through citations to find the original domain usage.
- Core assumption: The earliest citation in a new domain can be found through keyword searches and backward citation tracing.
- Evidence anchors:
  - [section 3]: "We conducted an article search to track the origin of certain terms used in different domains... We focused on word drift within academia, namely, on academic journals that mentioned the terms."
  - [section 4.2]: "When it was searched on WoS, Denning et al. [21] and Rony [22] were identified as the earliest articles."
  - [corpus]: Weak evidence - corpus papers focus on ML and domain generalization, not domain-specific keyword tracking.
- Break condition: If the seminal work is not indexed in Web of Science or citation chains are broken, the origin cannot be traced.

### Mechanism 3
- Claim: Ngram viewer trends can indicate potential semantic shifts by showing changes in term usage patterns over time.
- Mechanism: The paper uses Google Ngram Viewer to visualize term frequency changes, looking for peaks or shifts that might correlate with semantic evolution.
- Core assumption: Changes in term frequency patterns (especially after periods of low usage) indicate potential semantic shifts.
- Evidence anchors:
  - [section 4.1]: "We reviewed Google Ngram Viewer to see if identify any trend changes in usage that may be indicative of semantic shift."
  - [section 4.2]: "Also, the citations were mentioned more frequently in the 1980s matches with the steep escalation of the Ngram graph from Figure 2."
  - [corpus]: No direct evidence - corpus papers don't mention Ngram analysis or term frequency trends.
- Break condition: If Ngram trends don't correlate with actual semantic shifts or if usage patterns are too complex to interpret.

## Foundational Learning

- Concept: Citation network analysis
  - Why needed here: The paper relies on following citation chains to trace term origins and track when definitions stop being cited.
  - Quick check question: What does it mean when a paper stops citing the original definition of a term it's using?

- Concept: Domain categorization in academic databases
  - Why needed here: The method uses field-specific searches in Web of Science to identify domain transitions.
  - Quick check question: How does limiting searches to CS-related categories help identify when terms shift into computer science?

- Concept: Semantic shift in linguistics
  - Why needed here: The paper builds on linguistic concepts of semantic change to apply them to academic domains.
  - Quick check question: What's the difference between semantic shift within a language versus across academic domains?

## Architecture Onboarding

- Component map: Keyword search → Citation tracing → Manual verification → Timeline marking → Ngram correlation
- Critical path: Keyword search → Finding seminal paper → Tracing citations → Identifying acceptance point
- Design tradeoffs: Manual verification ensures accuracy but limits scalability; citation-based detection may miss shifts in non-citation cultures
- Failure signatures: Continued citation of definitions, inability to access seminal works, weak correlation between Ngram trends and actual shifts
- First 3 experiments:
  1. Test the method on a term with known semantic shift (e.g., "cloud" in computing) to validate the approach
  2. Compare Web of Science results with another database (Scopus) to check consistency
  3. Apply the method to a term that has NOT undergone semantic shift to test false positive rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated computational methods reliably detect semantic shifts across domains as effectively as manual citation tracing?
- Basis in paper: [explicit] The paper states "Our preliminary case study has some limitations including the limited time range of the corpus and manual search for the findings" and future works will "include more computational methods including word embeddings, data crawling, and detecting semantic shifts with various language models."
- Why unresolved: The study only used manual citation tracing, which is time-consuming and limited in scope. No automated methods were tested against the manual findings.
- What evidence would resolve it: A comparative study applying automated semantic shift detection algorithms (e.g., word embeddings, contextual models) to the same terms and verifying whether they identify the same semantic shift points identified manually.

### Open Question 2
- Question: What factors determine whether a term's semantic shift across domains is permanent or temporary?
- Basis in paper: [inferred] The paper notes that "ontology" still requires citations in 2023, unlike "polymorphism" and "semaphore," suggesting some terms maintain semantic fluidity longer than others.
- Why unresolved: The study only examined three terms without analyzing why their adoption patterns differed. No theoretical framework explains variation in semantic shift permanence.
- What evidence would resolve it: A large-scale study tracking citation patterns of terms borrowed across multiple domains over time, combined with analysis of factors like domain maturity, term specificity, and cultural context.

### Open Question 3
- Question: How can we quantify the "semantic distance" between a term's usage in its original domain versus its borrowed usage in a new domain?
- Basis in paper: [inferred] The study tracks when terms appear in CS literature but doesn't measure how different the CS meaning is from the original meaning, or whether meanings converge over time.
- Why unresolved: The manual approach only identifies when terms appear, not how their meanings evolve or diverge. No metrics were used to compare semantic similarity across domains.
- What evidence would resolve it: Development of quantitative metrics (e.g., embedding distance, contextual similarity) to measure semantic divergence between original and borrowed usages, validated against expert judgment of semantic distance.

## Limitations

- Limited to only three terms, which may not represent broader patterns of semantic shift
- Relies heavily on Web of Science coverage and citation practices, which may be incomplete or inconsistent
- Manual verification process is time-consuming and doesn't scale well to larger datasets
- Assumes citation patterns reliably indicate semantic acceptance, which may not hold across all domains or time periods

## Confidence

- High confidence: The feasibility of manual semantic shift detection through citation analysis is well-supported by concrete examples for all three terms.
- Medium confidence: The generalizability of the method to other terms and domains, as only three cases were examined.
- Low confidence: The reliability of Ngram Viewer trends as indicators of semantic shift, given weak explicit evidence connecting trends to actual semantic changes.

## Next Checks

1. Apply the method to a term with known semantic shift (e.g., "cloud" in computing) and compare results with existing literature to validate accuracy.
2. Test the approach using Scopus or Google Scholar alongside Web of Science to assess database dependency and completeness of citation chains.
3. Attempt to detect semantic shift in a term that has NOT undergone semantic shift to establish the false positive rate of the method.