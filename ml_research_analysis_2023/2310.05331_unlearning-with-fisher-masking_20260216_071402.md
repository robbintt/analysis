---
ver: rpa2
title: Unlearning with Fisher Masking
arxiv_id: '2310.05331'
source_url: https://arxiv.org/abs/2310.05331
tags:
- learning
- data
- unlearning
- remain
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FisherMask, a new method for machine unlearning
  that leverages Fisher information to mask parameters important for the data to be
  forgotten. This masking significantly improves unlearning performance compared to
  direct fine-tuning and other methods like TF-IDF and Fisher noise.
---

# Unlearning with Fisher Masking

## Quick Facts
- **arXiv ID**: 2310.05331
- **Source URL**: https://arxiv.org/abs/2310.05331
- **Reference count**: 38
- **One-line primary result**: FisherMask significantly improves unlearning performance compared to direct fine-tuning and other methods like TF-IDF and Fisher noise.

## Executive Summary
This paper introduces FisherMask, a new method for machine unlearning that leverages Fisher information to mask parameters important for the data to be forgotten. By identifying and masking parameters with high Fisher contribution to the forget set but low contribution to the remain set, FisherMask enables effective unlearning while maintaining high accuracy on the remaining data. The method demonstrates superior performance and stability compared to direct fine-tuning and other baselines like TF-IDF and Fisher noise across various datasets and network structures.

## Method Summary
FisherMask is a machine unlearning method that uses Fisher information to identify and mask parameters important for the data to be forgotten. The process involves training a model on the full dataset, computing Fisher information diagonals for the forget and remain sets, masking the top R parameters based on the difference in Fisher values, and fine-tuning the masked model on the remain data using a compressed learning rate schedule. The method is compared against baselines like direct fine-tuning, Fisher noise, TF-IDF, and MaskClassifier on datasets like CIFAR10/100, MNIST, and Tiny-ImageNet using architectures like ResNet, VGG, GoogLeNet, and DenseNet.

## Key Results
- FisherMask significantly outperforms direct fine-tuning and other baselines in unlearning performance while maintaining high accuracy on remaining data.
- The method demonstrates superior stability across different experiment settings and random seeds compared to other baselines.
- FisherMask can unlearn data almost completely without any fine-tuning, showcasing its effectiveness in removing the influence of target data from the model.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking parameters based on Fisher information differences between forget and remain sets significantly improves unlearning performance.
- Mechanism: The Fisher information matrix captures how parameters contribute to the model's predictive behavior. By identifying parameters with high Fisher contribution to the forget set but low contribution to the remain set, and masking them before fine-tuning, the model is guided to unlearn the target data while preserving performance on the remain data.
- Core assumption: The diagonal approximation of the Fisher matrix adequately captures parameter importance for unlearning.
- Evidence anchors:
  - [abstract] "Experiments on various datasets and network structures show the effectiveness of the method: without any fine-tuning, the proposed Fisher masking could unlearn almost completely while maintaining most of the performance on the remain data."
  - [section] "Our main finding is that Fisher information plays a key role in masking parameters: it characterizes how parameters contribute to the distance between models before and after unlearning."
  - [corpus] Weak evidence - no direct mention of Fisher information in neighbor papers, but related to unlearning and masking concepts.
- Break condition: If the diagonal approximation of Fisher matrix fails to capture parameter importance, or if the forget and remain sets have overlapping parameter importance profiles.

### Mechanism 2
- Claim: Fine-tuning with a learning rate schedule that mimics the original training process improves unlearning stability.
- Mechanism: By using a learning rate scheduler that mirrors the original training's decay pattern but compressed into fewer epochs, the unlearning process can more effectively navigate the parameter space to forget the target data while retaining performance on the remain data.
- Core assumption: The original training's learning rate schedule is optimal for the task and can be compressed without losing effectiveness.
- Evidence anchors:
  - [section] "We find that the final unlearning performances could be sensitive to different settings of learning rate, which are usually ignored in current unlearning configurations."
  - [section] "We deploy a learning rate scheduler for unlearning to mimic the original learning process in a shorter period (denoted by S, and we set S = 5)."
  - [corpus] Weak evidence - no direct mention of learning rate scheduling in neighbor papers, but related to fine-tuning concepts.
- Break condition: If the compressed learning rate schedule causes instability or fails to guide the model to the desired local optimum.

### Mechanism 3
- Claim: Masking based on neuron activation values can effectively identify parameters to forget, but is less stable than Fisher information-based masking.
- Mechanism: By analyzing how neurons activate on the forget set versus the remain set, parameters connected to neurons that strongly activate on the forget set can be masked to remove the target data's influence.
- Core assumption: Neuron activation patterns are indicative of parameter importance for specific data subsets.
- Evidence anchors:
  - [section] "In neural networks, an alternative way to measure importance of parameters is inspecting activation states of their corresponding neurons."
  - [section] "ActivationMask strategy identifies top R of channels with large ADf,j - ADr,j, and mask CNN kernel parameters connecting with those channels."
  - [corpus] Weak evidence - no direct mention of activation-based masking in neighbor papers, but related to unlearning concepts.
- Break condition: If neuron activation patterns do not correlate with parameter importance, or if the activation-based masking removes too much or too little information.

## Foundational Learning

- Concept: Fisher Information Matrix
  - Why needed here: The Fisher Information Matrix is used to identify which parameters are most important for the model's predictions on specific data subsets (forget and remain sets). This information is crucial for the masking strategy that enables effective unlearning.
  - Quick check question: What is the Fisher Information Matrix, and how is it used to measure parameter importance in a neural network?

- Concept: Fine-tuning and Catastrophic Forgetting
  - Why needed here: Fine-tuning is the primary mechanism used to unlearn data after masking important parameters. Understanding how fine-tuning interacts with catastrophic forgetting is essential for optimizing the unlearning process.
  - Quick check question: How does fine-tuning a neural network typically lead to catastrophic forgetting, and how is this phenomenon leveraged in the unlearning process?

- Concept: Neuron Activation Patterns
  - Why needed here: Neuron activation patterns are used as an alternative method to identify parameters for masking. Understanding how activation patterns relate to parameter importance is crucial for the ActivationMask strategy.
  - Quick check question: How can neuron activation patterns be used to identify which parameters in a neural network are most important for specific data subsets?

## Architecture Onboarding

- Component map:
  - FisherMask: The main unlearning method that uses Fisher information to mask parameters before fine-tuning.
  - ActivationMask: An alternative unlearning method that uses neuron activation values to mask parameters.
  - Fine-tuning: The process of retraining the model on the remain data after masking to unlearn the target data.
  - Learning Rate Scheduler: A component that adjusts the learning rate during fine-tuning to mimic the original training process.

- Critical path:
  1. Train the model on the full dataset.
  2. Identify the forget set and remain set.
  3. Compute Fisher information or activation values for each parameter.
  4. Mask the top R parameters based on the chosen masking strategy.
  5. Fine-tune the masked model on the remain data using the learning rate scheduler.
  6. Evaluate the unlearning performance on both the forget and remain sets.

- Design tradeoffs:
  - Masking ratio (R): Higher masking ratios may lead to better unlearning but could also result in significant performance degradation on the remain data.
  - Masking strategy: FisherMask is more effective but computationally expensive, while ActivationMask is faster but less stable.
  - Learning rate schedule: A more aggressive schedule may speed up unlearning but could also cause instability.

- Failure signatures:
  - High forget accuracy: Indicates that the model has not effectively unlearned the target data.
  - Low remain accuracy: Suggests that too much information has been removed, affecting the model's performance on the remain data.
  - Unstable unlearning: Characterized by large fluctuations in accuracy during the fine-tuning process.

- First 3 experiments:
  1. Compare the unlearning performance of FisherMask and ActivationMask on a simple dataset (e.g., MNIST) with a single category to remove.
  2. Investigate the effect of different masking ratios (R) on the unlearning performance and remain accuracy.
  3. Evaluate the stability of the unlearning process by running multiple trials with different random seeds and comparing the variance in performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FisherMask method's performance vary with different mask ratios beyond the tested range (0.02-0.25), and what is the optimal mask ratio for balancing unlearning and retention of performance on remaining data?
- Basis in paper: [explicit] The paper mentions that the performance degrades significantly as the percentage of masking increases, with oscillations becoming larger with higher remove ratios. However, the paper does not explore mask ratios beyond 0.25.
- Why unresolved: The paper only provides results for mask ratios up to 0.25, leaving the optimal mask ratio and its impact on performance beyond this range unexplored.
- What evidence would resolve it: Conducting experiments with a wider range of mask ratios, particularly beyond 0.25, and analyzing the trade-off between unlearning and retention of performance on remaining data to determine the optimal mask ratio.

### Open Question 2
- Question: How does the FisherMask method perform in unlearning tasks that involve removing a subset of classes or specific samples, rather than an entire class, and what modifications are needed to adapt the method to such scenarios?
- Basis in paper: [inferred] The paper focuses on removing an entire class of samples and does not address scenarios where only a subset of classes or specific samples need to be unlearned. This limitation suggests the need for further exploration.
- Why unresolved: The paper does not provide evidence or modifications for adapting the FisherMask method to unlearn subsets of classes or specific samples, which is a common requirement in real-world applications.
- What evidence would resolve it: Testing the FisherMask method on tasks involving the removal of subsets of classes or specific samples, and developing modifications or extensions to the method to handle such scenarios effectively.

### Open Question 3
- Question: How does the stability of the FisherMask method compare to other unlearning methods in the presence of adversarial attacks or noisy data, and what are the implications for its robustness in practical applications?
- Basis in paper: [inferred] The paper demonstrates the stability of the FisherMask method compared to other methods under normal conditions but does not address its robustness against adversarial attacks or noisy data, which are critical considerations in practical applications.
- Why unresolved: The paper lacks experiments or analysis on the FisherMask method's performance and stability in the presence of adversarial attacks or noisy data, leaving its robustness in such scenarios unexplored.
- What evidence would resolve it: Conducting experiments to evaluate the FisherMask method's stability and performance under adversarial attacks or noisy data, and analyzing its robustness compared to other unlearning methods in these challenging scenarios.

## Limitations
- The paper's claims rely heavily on diagonal Fisher approximations and compressed fine-tuning schedules, but does not extensively validate whether these approximations hold across diverse architectures or datasets.
- The stability analysis, while demonstrating robustness to random seeds, lacks broader evaluation across different model families and data distributions.
- The computational overhead of Fisher information computation for large models is acknowledged but not quantified in detail.

## Confidence
- **High confidence**: Claims about FisherMask's superior performance compared to direct fine-tuning and other baselines on standard datasets (CIFAR10/100, MNIST) are well-supported by the presented experiments.
- **Medium confidence**: The claim that FisherMask can "unlearn almost completely while maintaining most of the performance on the remain data" is supported by the experiments but relies on the specific evaluation metrics and experimental conditions reported.
- **Medium confidence**: The assertion that the learning rate schedule significantly impacts unlearning stability is plausible based on the ablation studies, but the specific mechanism and generalizability require further investigation.

## Next Checks
1. Validate the diagonal Fisher approximation assumption by comparing FisherMask performance against methods using full Fisher matrices on smaller models where computation is feasible.
2. Test FisherMask's stability and effectiveness across a broader range of model architectures (e.g., transformers, recurrent networks) and data distributions (e.g., long-tailed, imbalanced datasets).
3. Quantify the computational overhead of Fisher information computation for large models and explore approximation techniques or efficient alternatives to scale FisherMask to real-world applications.