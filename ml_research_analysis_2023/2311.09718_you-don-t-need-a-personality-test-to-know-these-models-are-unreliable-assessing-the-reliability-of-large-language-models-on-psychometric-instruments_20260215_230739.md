---
ver: rpa2
title: 'You don''t need a personality test to know these models are unreliable: Assessing
  the Reliability of Large Language Models on Psychometric Instruments'
arxiv_id: '2311.09718'
source_url: https://arxiv.org/abs/2311.09718
tags:
- llms
- consistency
- prompt
- answer
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study constructs MODEL -PERSONAS, a dataset of 693 questions
  across 39 psychological instruments measuring 115 persona axes. It evaluates 15
  open-source LLMs on their ability to consistently answer persona-related questions
  under various prompt perturbations.
---

# You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments

## Quick Facts
- arXiv ID: 2311.09718
- Source URL: https://arxiv.org/abs/2311.09718
- Reference count: 16
- Most LLMs show low consistency and high sensitivity to minor prompt variations when answering psychometric questions

## Executive Summary
This study evaluates 15 open-source LLMs on their ability to consistently answer questions from 39 psychological instruments measuring 115 persona axes. The results reveal that most models struggle with comprehension, sensitivity to minor prompt variations, and consistency, especially when handling negated statements. Only two FLAN-T5 models achieved reasonable consistency thresholds, suggesting that current prompting practices are unreliable for accurately measuring LLM personas.

## Method Summary
The study constructs MODEL-PERSONAS, a dataset of 693 questions across 39 psychological instruments. It evaluates 15 open-source LLMs using systematic prompt perturbations including spurious changes (spacing, punctuation), content variations (negation, option consistency, order), and measures comprehensibility, sensitivity, and consistency. The evaluation framework compares model responses across these variations to quantify reliability.

## Key Results
- Most LLMs struggle with comprehension when prompts have minor syntactic variations
- All models score near random on negation consistency, often answering negated questions with the same polarity as originals
- Only FLAN-T5 models (Small, Base) achieved reasonable consistency threshold (0.7)
- Model reliability correlates more with architectural family than parameter count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs show low comprehension when prompts have minor syntactic variations.
- Mechanism: The probability of generating a valid answer drops sharply when small changes like extra spaces or altered punctuation are introduced, indicating the model relies on exact formatting cues rather than semantic understanding.
- Core assumption: The model's token prediction is highly sensitive to prompt structure.
- Evidence anchors:
  - [abstract] "Our experiments on 15 different open-source LLMs reveal that even simple perturbations are sufficient to significantly downgrade a model’s question-answering ability..."
  - [section] Table 1 shows comprehension scores drop from >0.95 to near zero for certain models with small prompt changes.
  - [corpus] Weak corpus signal; only 1 of 8 neighbor papers directly addresses prompt formatting issues.
- Break condition: If the model is fine-tuned or prompted with explicit robustness instructions, the sensitivity to minor syntax changes may decrease.

### Mechanism 2
- Claim: LLMs fail to maintain consistency when question meaning is negated.
- Mechanism: Models often answer negated questions with the same polarity as the original, indicating they do not fully parse the negation but instead match surface patterns.
- Core assumption: The model treats negation as a superficial cue rather than a semantic reversal.
- Evidence anchors:
  - [abstract] "...most LLMs have low negation consistency."
  - [section] Figure 1 shows all models score near random on negation consistency, with higher consistency only for direct negation versus paraphrastic negation.
  - [corpus] No direct corpus support; this is a novel experimental finding.
- Break condition: If the model is trained or fine-tuned on negation-rich data, consistency may improve.

### Mechanism 3
- Claim: Consistency and sensitivity are architecture-family dependent, not size dependent.
- Mechanism: Models from the same family (e.g., FLAN-T5, BLOOMZ) show similar robustness patterns, suggesting architectural design choices in pretraining or fine-tuning influence reliability more than parameter count.
- Core assumption: Architectural family defines prompt-response behavior patterns.
- Evidence anchors:
  - [abstract] "Interestingly, our results indicate that the reliability of LLMs is not necessarily correlated with a model’s number of parameters..."
  - [section] Table 2 shows FLAN-T5 family models are perfectly robust to prompt variations, while BLOOMZ models are sensitive despite high comprehension.
  - [corpus] Weak corpus signal; only 1 neighbor paper touches on architecture-level effects.
- Break condition: If models are adapted with task-specific fine-tuning, family-level patterns may be overridden.

## Foundational Learning

- Concept: Probability-based answer generation
  - Why needed here: Understanding how LLM token probabilities determine answer selection is key to interpreting comprehension and consistency metrics.
  - Quick check question: How is the final answer chosen from token probabilities in this study?
- Concept: Prompt sensitivity and brittleness
  - Why needed here: The study hinges on detecting how minor prompt changes affect model outputs, so knowing what makes prompts brittle is essential.
  - Quick check question: What kind of prompt changes were tested for sensitivity?
- Concept: Negation handling in language models
  - Why needed here: Negation consistency is a core metric; understanding why models struggle with negation informs interpretation of results.
  - Quick check question: How did the study test negation consistency?

## Architecture Onboarding

- Component map: Prompt parser → Token probability estimator → Answer selector → Consistency evaluator
- Critical path: Generate prompt variants → Run inference → Compute probabilities → Select top answer → Compare with perturbed variants → Aggregate consistency metrics
- Design tradeoffs: High parameter count improves comprehension but not necessarily consistency; architectural family matters more than size.
- Failure signatures: Low comprehension scores, sensitivity near 0.5, negation consistency near 0.5 indicate brittle or unreliable models.
- First 3 experiments:
  1. Test comprehension on baseline vs. perturbed prompts for a given model.
  2. Measure sensitivity by comparing answers to all prompt variants.
  3. Evaluate negation consistency by reversing question meaning and comparing responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific architecture of FLAN-T5 models contribute to their superior consistency in answering persona-related questions compared to other models?
- Basis in paper: [explicit] The paper notes that FLAN-T5 models consistently perform well across all metrics of persona consistency, but does not delve into the specific architectural features responsible for this performance.
- Why unresolved: The paper focuses on the results of the experiments but does not provide a detailed analysis of the architectural aspects of FLAN-T5 models that might explain their robustness to prompt variations and high consistency scores.
- What evidence would resolve it: A comparative analysis of the FLAN-T5 architecture with other models, focusing on elements like attention mechanisms, training data diversity, and instruction tuning methods, could provide insights into the architectural features contributing to their superior performance.

### Open Question 2
- Question: Can additional fine-tuning on negation samples improve the negation consistency of LLMs, or does it risk altering their innate persona?
- Basis in paper: [explicit] The paper mentions that even after additional fine-tuning on text corpora that include negation samples, there is no significant improvement in understanding negation, and there is a risk of altering the LLM's innate persona.
- Why unresolved: The paper raises the issue but does not explore the extent to which fine-tuning affects negation consistency or how to balance between improving consistency and preserving the model's original persona.
- What evidence would resolve it: Experiments comparing the negation consistency of models before and after fine-tuning, along with an analysis of changes in their responses to other persona-related questions, would clarify the impact of fine-tuning on negation consistency and persona preservation.

### Open Question 3
- Question: How do spurious changes in prompt format affect the interpretability and validity of results in psychological studies using LLMs?
- Basis in paper: [explicit] The paper demonstrates that spurious changes in prompt format can significantly affect the comprehensibility and sensitivity of LLM responses, raising concerns about the reliability of using LLMs for psychological assessments.
- Why unresolved: While the paper identifies the sensitivity of LLMs to prompt format changes, it does not explore the broader implications of these findings for the design and interpretation of psychological studies using LLMs.
- What evidence would resolve it: A systematic review of psychological studies using LLMs, analyzing the consistency of their findings with respect to different prompt formats, would provide insights into the impact of prompt sensitivity on the validity of psychological assessments using LLMs.

## Limitations

- The MODEL-PERSONAS dataset represents a curated subset rather than comprehensive coverage of psychological assessment tools
- Evaluation focuses exclusively on open-source models, leaving unknown whether proprietary models exhibit similar reliability issues
- Does not explore whether fine-tuning or instruction-tuning on psychometric data could improve consistency

## Confidence

- **High confidence**: LLMs show significant sensitivity to minor prompt variations and struggle with negation consistency across multiple architectures
- **Medium confidence**: The claim that architectural family matters more than parameter count for reliability
- **Low confidence**: The assertion that "you don't need a personality test to know these models are unreliable"

## Next Checks

1. **Temporal consistency validation**: Run the same prompt variations on the same models at different times and hardware conditions to verify that observed reliability issues are stable rather than artifacts of specific inference environments.

2. **Instruction-tuned variant comparison**: Evaluate whether models fine-tuned with instruction datasets (like FLAN or T-Few) show improved consistency compared to their base counterparts, isolating the effect of instruction tuning on psychometric reliability.

3. **Cross-domain robustness test**: Apply the same prompt perturbation methodology to non-psychometric question-answering tasks (e.g., factual QA, commonsense reasoning) to determine whether the observed brittleness is specific to psychometric instruments or represents a more general LLM limitation.