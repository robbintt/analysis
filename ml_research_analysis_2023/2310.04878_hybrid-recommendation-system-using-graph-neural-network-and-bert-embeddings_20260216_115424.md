---
ver: rpa2
title: Hybrid Recommendation System using Graph Neural Network and BERT Embeddings
arxiv_id: '2310.04878'
source_url: https://arxiv.org/abs/2310.04878
tags: []
core_contribution: The paper presents a hybrid recommendation system that combines
  Graph Neural Networks (GNNs) with BERT embeddings for anime recommendations. The
  core idea is to use GraphSAGE to capture graph structure while incorporating textual
  embeddings from anime synopses to improve recommendation accuracy.
---

# Hybrid Recommendation System using Graph Neural Network and BERT Embeddings

## Quick Facts
- arXiv ID: 2310.04878
- Source URL: https://arxiv.org/abs/2310.04878
- Reference count: 5
- Key outcome: Achieved 52% training accuracy and 37% test accuracy on 800-user anime dataset

## Executive Summary
This paper presents a hybrid recommendation system that combines Graph Neural Networks (GNNs) with BERT embeddings to improve anime recommendations. The system uses GraphSAGE to capture graph structure while incorporating textual embeddings from anime synopses to enrich node features. The model predicts user ratings for unseen anime and generates top recommendations based on predicted ratings. Despite being evaluated on a limited dataset of 800 users due to computational constraints, the system demonstrates the potential of combining GNNs with transformer embeddings for recommendation tasks.

## Method Summary
The method employs a heterogeneous graph structure with user and anime nodes, where user interactions (ratings) form edges. GraphSAGE aggregates neighborhood information to learn node representations, while BERT embeddings capture semantic information from anime synopses. The model combines genre features with BERT embeddings to create rich node representations, which are then processed through GNN layers to predict edge scores (ratings). The system was implemented using PyTorch Geometric and trained on the Anime Recommendation Database 2020, achieving reasonable performance despite computational limitations.

## Key Results
- Training accuracy: 52% on 800-user dataset
- Test accuracy: 37% on held-out test set
- Training loss: 0.659, Test loss: 0.667
- Successfully generated recommendations for users despite limited data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GraphSAGE aggregates neighborhood information to learn node representations for users and anime, enabling effective link prediction.
- Mechanism: GraphSAGE uses Graph Convolutional layers to aggregate features from a node's neighbors, updating node embeddings iteratively. These embeddings are then used to predict ratings via an edge decoder.
- Core assumption: The graph structure (user-anime interactions) contains sufficient signal for link prediction when combined with node features.
- Evidence anchors:
  - [abstract] "Our model employs the task of link prediction to create a recommendation system that considers both the features of anime and user interactions with different anime."
  - [section] "GraphSAGE is a variant of GNN that aggregates information from neighboring nodes to generate node embeddings."

### Mechanism 2
- Claim: BERT embeddings capture semantic information from anime synopses, enriching node features beyond genre labels.
- Mechanism: Text from anime synopses is processed with a SentenceTransformer model to generate embeddings. These are concatenated with genre features to form richer node representations.
- Core assumption: Synopses contain content that correlates with user preferences and can improve prediction accuracy.
- Evidence anchors:
  - [abstract] "The hybridization of the GNN and transformer embeddings enables us to capture both inter-level and intra-level features of anime data."
  - [section] "To improve the performance of the recommendation system, additional features can be incorporated into the model... features such as the genre and the synopsis of the movie can be used to augment the embeddings."

### Mechanism 3
- Claim: Heterogeneous graph structure allows separate processing of user and anime node types while preserving their relationships.
- Mechanism: The HeteroData schema defines separate node types (user, anime) with distinct feature dimensions, and edge types (rates) connecting them. GNNEncoder processes each node type with its own feature space.
- Core assumption: Users and anime benefit from type-specific embeddings while still learning from cross-type edges.
- Evidence anchors:
  - [section] "The forward method of the GNNEncoder class first applies a GraphSAGE layer to the input node features using the SAGEConv module."
  - [section] "The forward method takes as input a dictionary of node features, a dictionary of edge connections, and a set of edge labels."

## Foundational Learning

- Graph Neural Networks
  - Why needed here: GNNs naturally model user-item interactions as edges in a bipartite graph, capturing relational patterns.
  - Quick check question: What is the difference between node features and edge features in a GNN?

- Text Embeddings with Transformers
  - Why needed here: BERT/SentenceTransformer embeddings encode semantic meaning from synopses, providing richer features than genres alone.
  - Quick check question: How do sentence embeddings differ from word embeddings in capturing context?

- Link Prediction in Recommender Systems
  - Why needed here: The model predicts user-item interaction scores (ratings) by estimating the likelihood of a link existing between nodes.
  - Quick check question: What loss function is commonly used for link prediction in recommendation tasks?

## Architecture Onboarding

- Component map:
  Load data -> Preprocess features (genres + BERT) -> Build graph -> Forward pass (GNNEncoder -> EdgeDecoder) -> Loss computation (RMSE) -> Backpropagation

- Critical path:
  Load data → preprocess features (genres + BERT) → build graph → forward pass (GNNEncoder → EdgeDecoder) → loss computation (RMSE) → backpropagation

- Design tradeoffs:
  - Using GraphSAGE vs GAT: GraphSAGE is simpler and less sensitive to degree variance but may lose attention-based nuance.
  - Hybrid BERT vs pure collaborative: Hybrid improves cold-start for items but increases model complexity.

- Failure signatures:
  - High training and test RMSE with small gap: Underfitting due to insufficient model capacity.
  - Large gap between train and test RMSE: Overfitting, possibly due to noisy embeddings or insufficient regularization.
  - Both losses high but accuracy low: Poor feature representation or sparse graph structure.

- First 3 experiments:
  1. Train with only genre features (no BERT) to assess impact of textual embeddings.
  2. Use a dense synthetic graph to test if performance improves with richer structure.
  3. Replace GraphSAGE with GAT layers to evaluate if attention mechanisms help.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform with significantly larger datasets and more computational resources?
- Basis in paper: [explicit] The authors mention that future work includes scaling to larger datasets with more compute resources.
- Why unresolved: The current evaluation was conducted on a limited dataset of 800 users due to computational constraints, so the model's performance on larger datasets remains unknown.
- What evidence would resolve it: Training and testing the model on datasets with tens of thousands of users and anime, comparing accuracy, loss, and computational efficiency.

### Open Question 2
- Question: How do different GNN architectures (beyond GraphSAGE) affect the model's performance?
- Basis in paper: [explicit] The authors suggest exploring alternative GNN architectures beyond GraphSAGE as future work.
- Why unresolved: The paper only uses GraphSAGE for model building, so the impact of other GNN variants is unknown.
- What evidence would resolve it: Implementing and comparing the model using different GNN architectures like Graph Attention Networks (GAT) or Graph Isomorphism Networks (GIN) and evaluating their performance on the same dataset.

### Open Question 3
- Question: How effective is the hybrid approach of combining GNN with BERT embeddings compared to using either method alone?
- Basis in paper: [inferred] The paper presents a hybrid system but doesn't provide comparative results with GNN-only or BERT-only approaches.
- Why unresolved: The paper doesn't include ablation studies or comparisons with single-method approaches.
- What evidence would resolve it: Implementing and evaluating separate models using only GNN or only BERT embeddings, then comparing their performance metrics (accuracy, RMSE) against the hybrid approach.

## Limitations

- Dataset size extremely limited (800 users) making generalization claims uncertain
- Performance metrics (accuracy) are unusual for rating prediction tasks that typically use RMSE/MAE
- No statistical significance testing or ablation studies to validate component contributions
- Computational constraints prevented testing on larger datasets to verify scalability

## Confidence

- **Low confidence**: Overall performance claims due to ambiguous metrics and small dataset size
- **Medium confidence**: Architectural design combining GNNs with BERT embeddings, following established patterns
- **High confidence**: Core methodology of using GraphSAGE for neighborhood aggregation and BERT for text embeddings

## Next Checks

1. Replicate the model using a larger, more diverse dataset to verify scalability claims and assess real-world performance
2. Conduct ablation studies removing BERT embeddings and GraphSAGE layers separately to quantify their individual contributions
3. Implement statistical significance testing between different model variants and baselines to validate performance improvements