---
ver: rpa2
title: Differentiating Through Integer Linear Programs with Quadratic Regularization
  and Davis-Yin Splitting
arxiv_id: '2301.13395'
source_url: https://arxiv.org/abs/2301.13395
tags:
- problem
- optimization
- loss
- dys-net
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of predicting optimal solutions
  to combinatorial optimization problems (specifically integer linear programs) when
  only contextual data is available, not the true parameters. The challenge is differentiating
  through the discrete optimization step to train a neural network model.
---

# Differentiating Through Integer Linear Programs with Quadratic Regularization and Davis-Yin Splitting

## Quick Facts
- arXiv ID: 2301.13395
- Source URL: https://arxiv.org/abs/2301.13395
- Reference count: 12
- Differentiates through ILPs using quadratic regularization and Davis-Yin splitting to achieve scalable prediction of optimal solutions

## Executive Summary
This paper addresses the challenge of training neural networks to predict optimal solutions to integer linear programs when only contextual data is available. The key innovation is using quadratic regularization combined with Davis-Yin splitting to create a differentiable continuous relaxation of the ILP, avoiding expensive projections onto the full feasible set. The approach is further enhanced with Jacobian-free backpropagation, which dramatically reduces computational complexity. Experiments demonstrate that the method achieves state-of-the-art accuracy while training much faster and scaling to significantly larger problem instances than existing approaches.

## Method Summary
The method predicts optimal ILP solutions by first computing a quadratic regularization term to ensure strong convexity, then applying Davis-Yin splitting to avoid expensive projections, and finally using Jacobian-free backpropagation for efficient gradient computation. The approach relaxes the discrete ILP to a continuous QP by adding a small regularizer, solves it using DYS to avoid computing projections onto the full feasible set, and trains the neural network using JFB which replaces the expensive Jacobian computation with an identity matrix approximation. This combination enables training on much larger problem instances than previous methods while maintaining comparable solution quality.

## Key Results
- DYS-Net achieves comparable accuracy to state-of-the-art methods on shortest path and knapsack problems
- Trains 10-50x faster than cvxpylayers and PertOptNet on grid graphs
- Scales to 100-by-100 grid graphs while CVX-net struggles with grids larger than 30-by-30
- DYS-Net trains in about a day for 100-by-100 problems vs close to a week for PertOpt-net

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quadratic regularization makes the continuous relaxation of the ILP strongly convex, ensuring the solution is differentiable with respect to the parameters.
- Mechanism: Adding a small quadratic term (γ∥x∥²) to the objective transforms the LP relaxation into a strongly convex problem, which guarantees that the argmin is unique and continuously differentiable in the parameters.
- Core assumption: The regularization parameter γ is small enough to not significantly alter the solution but large enough to ensure strong convexity.
- Evidence anchors:
  - [abstract]: "we use Davis-Yin splitting on a quadratically regularized continuous relaxation of the ILP"
  - [section]: "we follow recent works ( e.g. [WDT19]) and relax (3) to a quadratic program over the convex hull ofX (see (13)) by adding a small regularizer."

### Mechanism 2
- Claim: Davis-Yin splitting avoids the need for projection onto the full feasible set, enabling efficient GPU computation.
- Mechanism: DYS splits the feasible set C into simpler sets C1 and C2, using only cheap projections PC1 and PC2 in the forward pass, avoiding the expensive projection onto the full polytope.
- Core assumption: The feasible set C can be expressed as an intersection of simpler sets for which projections are computationally cheap.
- Evidence anchors:
  - [abstract]: "We propose applying a three-operator splitting technique, also known as Davis-Yin splitting (DYS)"
  - [section]: "Our core idea is to solve (21) using Davis-Yin splitting (DYS) [DY17] to avoid computation ofPC in the forward pass."

### Mechanism 3
- Claim: Jacobian-free backpropagation (JFB) replaces the expensive linear solve in backpropagation with an identity approximation, making training scalable.
- Mechanism: Instead of solving a linear system involving the Jacobian of the forward pass, JFB uses the identity matrix as a proxy, which is computationally cheap and still provides a descent direction.
- Core assumption: The identity approximation to the Jacobian is sufficient for effective gradient descent.
- Evidence anchors:
  - [abstract]: "combined with Jacobian-free backpropagation (JFB)"
  - [section]: "we use the recently introduced Jacobian-Free Backpropagation(JFB) in which the JacobianJΘ is replaced with the identity matrix."

## Foundational Learning

- Concept: Convex optimization and KKT conditions
  - Why needed here: Understanding how to differentiate through constrained optimization problems requires knowledge of optimality conditions and their derivatives.
  - Quick check question: What are the KKT conditions for a constrained optimization problem, and how would you differentiate them with respect to problem parameters?

- Concept: Three-operator splitting (Davis-Yin)
  - Why needed here: The algorithm relies on splitting the feasible set into simpler components to avoid expensive projections.
  - Quick check question: How does three-operator splitting work, and why is it advantageous for problems with structured feasible sets?

- Concept: Differentiable programming and implicit function theorem
  - Why needed here: The approach differentiates through the optimization problem's solution, which requires understanding how to compute gradients of implicitly defined functions.
  - Quick check question: How does the implicit function theorem allow us to compute gradients through optimization layers?

## Architecture Onboarding

- Component map: Input context d -> Neural network parameterizing wΘ(d) -> DYS solver for regularized continuous relaxation -> Predicted solution x̂(d)

- Critical path:
  1. Forward pass: d → wΘ(d) → DYS solve → x̂(d)
  2. Backward pass: x̂(d) → JFB gradient → update Θ

- Design tradeoffs:
  - Regularization parameter γ: Balances between maintaining ILP solution fidelity and ensuring differentiability
  - Splitting strategy: Affects the complexity of projections in the forward pass
  - JFB approximation: Trades off between computational efficiency and gradient accuracy

- Failure signatures:
  - Poor convergence: May indicate inappropriate regularization or learning rate
  - Large discrepancy between predicted and true solutions: Could suggest the neural network architecture is insufficient
  - GPU memory issues: May occur if the problem size exceeds GPU capacity

- First 3 experiments:
  1. Verify that DYS produces correct solutions on small ILP instances with known solutions
  2. Test JFB backpropagation by comparing with finite differences on a simple problem
  3. Train on a small grid shortest path problem and verify that the learned model generalizes to unseen contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DYS-Net scale with problem size beyond 100-by-100 grids, and what are the practical limits?
- Basis in paper: [explicit] The authors mention DYS-Net can handle 100-by-100 grids while CVX-net struggles with grids larger than 30-by-30, and PertOpt-net takes close to a week to train for 100-by-100 problems while DYS-net takes about a day.
- Why unresolved: The paper only tests up to 100-by-100 grids. It's unclear how performance scales beyond this point and what the practical limits of the method are.
- What evidence would resolve it: Additional experiments testing DYS-Net on even larger grid sizes, measuring training time, test MSE loss, and regret values to determine scaling behavior and practical limits.

### Open Question 2
- Question: How does DYS-Net perform on other types of combinatorial optimization problems beyond shortest path and knapsack?
- Basis in paper: [inferred] The authors mention that DYS-Net can be applied to more general objective functions with uniformly-Lipschitz gradients, and they suggest investigating connections to optimal control problems as future work.
- Why unresolved: The paper only evaluates DYS-Net on shortest path and knapsack problems. It's unclear how well the method generalizes to other combinatorial optimization problems.
- What evidence would resolve it: Experiments applying DYS-Net to a variety of other combinatorial optimization problems, such as traveling salesman, vehicle routing, or job scheduling, and comparing its performance to existing methods.

### Open Question 3
- Question: How sensitive is DYS-Net's performance to the choice of hyperparameters, such as the regularization parameter gamma and the number of iterations K in the DYS algorithm?
- Basis in paper: [explicit] The authors mention they tuned hyperparameters for each architecture to the best of their ability on the smallest problem (5-by-5 grid graphs) and then used these hyperparameter values for all other graph sizes.
- Why unresolved: The paper does not provide a systematic analysis of hyperparameter sensitivity. It's unclear how much the performance of DYS-Net depends on the choice of hyperparameters and whether the same hyperparameters work well across different problem sizes and types.
- What evidence would resolve it: A sensitivity analysis where different combinations of hyperparameters are tested on various problem sizes and types, and the impact on performance metrics such as training time, test MSE loss, and regret values is measured.

## Limitations

- The quadratic regularization parameter γ requires careful tuning and may not generalize well across different problem types
- The effectiveness of DYS depends on being able to decompose the feasible set into simpler components, which may not work for all ILP structures
- The JFB approximation to the Jacobian introduces uncontrolled error that could accumulate over many training iterations

## Confidence

- **High Confidence**: The core mechanism of using quadratic regularization to ensure strong convexity and differentiability is well-established in optimization literature
- **Medium Confidence**: The effectiveness of Davis-Yin splitting in avoiding expensive projections depends heavily on problem structure
- **Low Confidence**: Claims about the approach's generality to arbitrary ILP structures beyond tested grid and knapsack problems are speculative

## Next Checks

1. **Regularization sensitivity analysis**: Systematically vary γ across multiple orders of magnitude on the same problem instances to quantify the tradeoff between solution fidelity and differentiability.

2. **Jacobian-free approximation validation**: Compare JFB gradients against exact gradients (via automatic differentiation or finite differences) on small problems to measure approximation error and its impact on convergence.

3. **Structure generalization test**: Apply the method to ILPs with different constraint structures (e.g., scheduling problems, facility location) to verify that the DYS decomposition remains effective beyond grid graphs.