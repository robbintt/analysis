---
ver: rpa2
title: Assessing the Importance of Frequency versus Compositionality for Subword-based
  Tokenization in NMT
arxiv_id: '2306.01393'
source_url: https://arxiv.org/abs/2306.01393
tags:
- symbols
- huffman
- words
- translation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the relative importance of frequency and compositionality
  in subword-based tokenization for neural machine translation. The authors propose
  using Huffman coding to tokenize words by frequency, using a fixed number of symbols,
  which allows separating frequency from compositionality.
---

# Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT

## Quick Facts
- **arXiv ID**: 2306.01393
- **Source URL**: https://arxiv.org/abs/2306.01393
- **Reference count**: 16
- **Primary result**: Huffman coding achieves 90%-95% of BPE's BLEU/COMET performance by preserving frequency advantages while removing compositionality

## Executive Summary
This paper investigates whether frequency or compositionality is more important for subword-based tokenization in neural machine translation. The authors propose using Huffman coding as an alternative to BPE that preserves frequency-based encoding advantages while removing compositionality. Experiments on CS-DE, EN-FR, and EN-DE tasks show that Huffman coding achieves 90%-95% of BPE's performance when vocabulary sizes are 32k symbols, demonstrating that frequency accounts for most of BPE's effectiveness with compositionality playing a comparatively minor role.

## Method Summary
The authors use Huffman coding to tokenize words by frequency using a fixed number of symbols (n), which allows separating frequency from compositionality. They build n-ary Huffman trees from word frequencies in training data, where frequent words appear closer to the root and are encoded with fewer symbols. The method is evaluated by training Transformer NMT models with varying vocabulary sizes (2k to 32k symbols) and comparing translation quality using BLEU, ChrF, and COMET scores against BPE baselines.

## Key Results
- Huffman coding achieves 90%-95% of BPE's BLEU and COMET scores at 32k vocabulary size
- Performance gap narrows as vocabulary size increases, with Huffman approaching BPE's performance
- Frequency alone accounts for most of BPE's effectiveness, with compositionality contributing relatively little

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Huffman coding achieves 90%-95% of BPE's performance by preserving frequency-based encoding advantages while removing compositionality.
- Mechanism: Huffman coding builds an unbalanced tree where frequent words are placed closer to the root and encoded with fewer symbols, mirroring BPE's frequency advantage but without decomposing words into meaningful subword units.
- Core assumption: The frequency distribution of tokens in language is the dominant factor in translation quality, and compositionality contributes comparatively little.
- Evidence anchors: [abstract] "frequency alone accounts for 90%-95% of the scores reached by BPE", [section 3.2] "frequent words will appear as leaves close to the root... most frequent words will be represented with a single symbol"
- Break condition: If compositionality proves essential for certain language pairs or domains, or if frequency distribution changes dramatically, Huffman's performance would degrade relative to BPE.

### Mechanism 2
- Claim: Huffman coding's inability to handle unknown words has minimal impact because test data contains very few unseen tokens.
- Mechanism: The Huffman tree is built from training data words, so test words not seen during training cannot be encoded, but empirical results show this affects less than 1% of tokens.
- Core assumption: The training data vocabulary coverage is sufficiently comprehensive that unknown words are rare in practice.
- Evidence anchors: [section 6] "unknown words are also likely to limit the performance of Huffman coding, although their number is very small in the test data... 0.55% unknown tokens in the CS source", [section 3.3] "as the Huffman tree is built over words in the training data, it cannot encode unknown words in the test data"
- Break condition: When working with highly specialized domains or languages with high morphological complexity where unknown words would be more frequent.

### Mechanism 3
- Claim: The fixed number of symbols (n) in Huffman coding directly determines the maximum number of symbols per token, creating a natural segmentation similar to BPE.
- Mechanism: By controlling the arity of the Huffman tree (number of children per node), the system can balance between character-level encoding (low n) and subword-like encoding (high n).
- Core assumption: The relationship between vocabulary size and token segmentation is more important than the specific segmentation algorithm used.
- Evidence anchors: [section 5] "We first investigate how translation quality changes according to the vocabulary size... If many symbols are available, then many frequent words will be encoded with a single symbol", [section 3.2] "Each node has at most n children, therefore we can associate symbols to each of them"
- Break condition: If the relationship between n and segmentation quality varies significantly across languages or if optimal n differs substantially from typical BPE vocabulary sizes.

## Foundational Learning

- **Huffman Coding and Text Compression**
  - Why needed here: The paper's core innovation uses Huffman coding as an alternative to BPE, requiring understanding of how frequency-based compression works
  - Quick check question: How does Huffman coding assign shorter codes to more frequent items, and why does this relate to subword tokenization?

- **Neural Machine Translation Architecture**
  - Why needed here: The evaluation uses Transformer models, requiring understanding of how tokenization affects input representation
  - Quick check question: How does the vocabulary size affect the input/output layers of a Transformer, and why is this important for tokenization choices?

- **Tokenization Evaluation Metrics**
  - Why needed here: The paper uses BLEU, ChrF, and COMET scores to compare methods, requiring understanding of what each metric measures
  - Quick check question: What are the key differences between BLEU, ChrF, and COMET, and why might compositionality affect them differently?

## Architecture Onboarding

- **Component map**: Huffman Tokenizer -> NMT System -> Evaluation Pipeline
- **Critical path**: 
  1. Preprocess training data with Moses tokenizer and truecasing
  2. Build Huffman tree using Algorithm 1 with n symbols
  3. Generate wordâ†”symbol mappings and encode training data
  4. Train Transformer NMT model on encoded data
  5. Encode test data and generate translations
  6. Decode translations and evaluate against references
- **Design tradeoffs**: Huffman vs BPE: Huffman captures frequency but loses compositionality; BPE balances both but is more complex; Symbol count (n): Higher values approach BPE behavior but increase vocabulary size and computational cost; Training data coverage: Better coverage reduces unknown words but may not reflect real-world usage
- **Failure signatures**: Low BLEU scores with high n values: May indicate Huffman tree construction issues or insufficient training; High percentage of unknown tokens: Suggests training data doesn't cover test vocabulary adequately; Disproportionate performance drop on morphologically rich languages: May reveal compositionality's hidden importance
- **First 3 experiments**: 
  1. Compare Huffman with n=1000 vs character-level tokenization on CS-DE to establish baseline performance
  2. Vary n from 1000 to 32000 on CS-DE to find performance plateau and optimal vocabulary size
  3. Compare Huffman vs BPE with matching vocabulary sizes (1000, 4000, 16000, 32000) across all three language pairs

## Open Questions the Paper Calls Out

- **How does Huffman coding compare to BPE when vocabulary sizes exceed 32k symbols?**
  - Basis in paper: [explicit] The paper mentions Huffman coding is viable and compares performance at 32k symbols but does not explore larger vocabulary sizes
  - Why unresolved: The study only examines vocabulary sizes up to 32k, leaving the potential performance at larger sizes unexplored
  - What evidence would resolve it: Experimental results comparing Huffman coding and BPE at vocabulary sizes greater than 32k, particularly for language pairs with extensive training data

- **What is the impact of Huffman coding on morphologically rich languages compared to BPE?**
  - Basis in paper: [inferred] The paper suggests Huffman coding lacks compositionality, which may be more critical for morphologically rich languages where subword composition is essential for translation
  - Why unresolved: The study focuses on CS-DE, EN-FR, and EN-DE language pairs, which may not fully represent the challenges posed by morphologically rich languages
  - What evidence would resolve it: Comparative experiments on language pairs with high morphological complexity, such as Finnish or Turkish, to evaluate the impact of compositionality on translation quality

- **How does the performance of Huffman coding vary with different text compression algorithms, such as Prediction by Partial Matching (PPM)?**
  - Basis in paper: [explicit] The authors suggest that PPM might be a promising candidate for future tokenization methods but have not explored it yet
  - Why unresolved: The study uses Huffman coding but does not investigate other compression algorithms that might offer better tokenization performance
  - What evidence would resolve it: Experiments comparing Huffman coding with PPM and other compression algorithms in terms of translation quality and computational efficiency

## Limitations
- The experimental design uses a very small percentage of test data (0.1-0.2% of sentences) which may not capture full translation complexity
- Huffman coding cannot handle unknown words at all, though this affects less than 1% of tokens in test data
- The paper doesn't explore whether compositionality becomes more important for certain types of content or semantic relationships

## Confidence
- **High confidence**: The experimental methodology is sound and the results are clearly presented. The comparison between Huffman coding and BPE using identical vocabulary sizes is methodologically rigorous.
- **Medium confidence**: The conclusion that frequency accounts for 90%-95% of BPE's effectiveness assumes that the test data and language pairs studied are representative.
- **Low confidence**: The paper doesn't adequately address potential interactions between frequency and compositionality, such as whether certain compositional patterns are inherently more frequent.

## Next Checks
1. **Expand test data coverage**: Validate the 90%-95% frequency contribution claim using a much larger and more diverse test set (e.g., full WMT test sets rather than 0.1-0.2% samples) to ensure the results aren't artifacts of limited evaluation data.

2. **Morphologically rich language analysis**: Test Huffman coding on additional morphologically complex languages (e.g., Turkish, Finnish, or Arabic) to determine if the frequency-compositionality relationship holds across different morphological typologies, particularly focusing on unknown word rates.

3. **Domain-specific evaluation**: Evaluate both methods on specialized domains (medical, legal, or technical documentation) where vocabulary coverage may be more limited and compositionality could play a larger role in handling unknown or rare terms.