---
ver: rpa2
title: 'Thinker: Learning to Plan and Act'
arxiv_id: '2307.14993'
source_url: https://arxiv.org/abs/2307.14993
tags:
- planning
- learning
- agent
- action
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Thinker algorithm is a novel approach that enables reinforcement
  learning agents to autonomously interact with and utilize a learned world model.
  It wraps the environment with a world model and introduces new actions designed
  for interacting with the model, allowing agents to perform planning by proposing
  alternative plans to the world model before selecting a final action to execute
  in the environment.
---

# Thinker: Learning to Plan and Act

## Quick Facts
- arXiv ID: 2307.14993
- Source URL: https://arxiv.org/abs/2307.14993
- Reference count: 40
- Achieves 94.5% solving rate in Sokoban and 261% median human-normalized score on Atari 2600

## Executive Summary
The Thinker algorithm introduces a novel approach for autonomous planning in reinforcement learning by wrapping environments with learned world models. Instead of relying on handcrafted planning algorithms, Thinker enables agents to learn how to plan by introducing new model-interaction actions that allow the agent to propose and evaluate alternative plans within the world model before executing actions in the real environment. The algorithm demonstrates state-of-the-art performance in both discrete planning tasks (Sokoban) and continuous control environments (Atari 2600), with visualizations showing that trained agents effectively use the world model for planning.

## Method Summary
Thinker transforms a standard Markov Decision Process (MDP) into an augmented MDP by wrapping the environment with a learned world model. The world model consists of a dual network architecture: a stationary network that predicts future states, rewards, and termination probabilities, and a non-stationary network that predicts value and policy. The augmented state representation includes tree statistics and model hidden states, allowing the agent to learn flexible planning strategies. The agent learns through standard reinforcement learning algorithms (like actor-critic) applied to this augmented MDP, effectively learning to interact with the world model for planning purposes. The algorithm eliminates the need for handcrafted planning algorithms by enabling autonomous learning of planning strategies through model-interaction actions.

## Key Results
- Achieves 94.5% solving rate in Sokoban within 5e7 frames, compared to 49.6% for DreamerV2 and 58.6% for EfficientZero
- Attains 261% median human-normalized score on Atari 2600, outperforming EfficientZero's 194%
- Visualizations demonstrate that agents learn to effectively use the world model for planning, selecting better actions based on model-predicted outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thinker enables autonomous planning by wrapping the environment with a learned world model and introducing model-interaction actions.
- Mechanism: The agent can interact with the world model through imaginary actions to propose alternative plans before selecting a final real action, eliminating the need for handcrafted planning algorithms.
- Core assumption: The world model can be trained to provide accurate predictions of future states, rewards, and value estimates that are useful for planning.
- Evidence anchors: [abstract] "The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the model" [section 3] "The Thinker algorithm transforms a raw MDP, denoted as M = ( S, A, P, R, γ, d0), into an MDP augmented with a model, denoted as ˜M = ( ˜S, ˜A, ˜P , ˜R, ˜γ, ˜d0)"

### Mechanism 2
- Claim: The augmented MDP structure allows the agent to learn flexible planning strategies that can adapt to different states and environments.
- Mechanism: By providing the agent with a fixed-size augmented state representation that includes tree statistics and model hidden states, the agent can learn to perform various planning algorithms beyond basic n-step exhaustive search or MCTS.
- Core assumption: The augmented state representation provides sufficient information for the agent to learn effective planning strategies.
- Evidence anchors: [section 3.2] "Altogether, we represent a node i by a vector ui that contains: (a) the node output statistics, oi, (b) the one-hot encoded action that leads to node i, (c) the mean rollout return of its child nodes, (d) the maximum rollout return of its child nodes, (e) the visit count of its child nodes"

### Mechanism 3
- Claim: The dual network architecture enables efficient learning of task-relevant features while allowing visualization of the agent's plan.
- Mechanism: The stationary network predicts future states, rewards, and termination probabilities, while the non-stationary network predicts value and policy. A feature loss on predicted states encourages learning of task-relevant features.
- Core assumption: The dual network architecture can effectively learn to predict the necessary quantities for planning while prioritizing task-relevant features.
- Evidence anchors: [section 3.4] "The stationary network predicts future states, rewards, and the probability of episode termination, whereas the non-stationary network predicts the value and action probabilities"

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The Thinker algorithm transforms a raw MDP into an augmented MDP with a learned world model. Understanding MDPs is crucial for grasping the algorithm's approach.
  - Quick check question: What are the key components of an MDP, and how does the Thinker algorithm modify these components?

- Concept: Reinforcement Learning (RL) algorithms
  - Why needed here: The Thinker algorithm is designed to be compatible with any RL algorithm by transforming the MDP. Familiarity with RL algorithms is essential for implementing and using Thinker effectively.
  - Quick check question: How does the Thinker-augmented MDP enable the use of any RL algorithm for model-based planning?

- Concept: World models in RL
  - Why needed here: The Thinker algorithm relies on a learned world model to perform planning. Understanding how world models are used in RL is critical for grasping the algorithm's approach.
  - Quick check question: How does the Thinker algorithm differ from other approaches that use world models for planning in RL?

## Architecture Onboarding

- Component map: Raw MDP -> Thinker-augmented MDP wrapper -> World model (dual network) -> RL agent (actor-critic) -> Training of world model and agent

- Critical path:
  1. Train world model using collected transitions from raw MDP
  2. Apply RL algorithm to Thinker-augmented MDP
  3. Agent learns to interact with world model for planning
  4. Evaluate agent performance on raw MDP

- Design tradeoffs:
  - Computational cost: Thinker requires K steps in augmented MDP for each step in raw MDP, plus model training overhead
  - Planning flexibility vs. complexity: Augmented MDP allows flexible planning but increases state representation complexity
  - Model accuracy vs. learning speed: Accurate world model is crucial for effective planning but may require more training data

- Failure signatures:
  - Poor planning performance: World model predictions are inaccurate or agent fails to learn effective model-interaction actions
  - Slow learning: Augmented MDP is too complex for agent to learn from effectively
  - High computational cost: Planning steps (K) are too large or model training is inefficient

- First 3 experiments:
  1. Implement Thinker-augmented MDP wrapper and verify it transforms raw MDP as expected
  2. Train world model using collected transitions and evaluate prediction accuracy
  3. Apply simple RL algorithm (e.g., DQN) to Thinker-augmented MDP and evaluate planning performance on a simple environment (e.g., CartPole)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of planning steps (K) for the Thinker algorithm in different environments?
- Basis in paper: [explicit] The paper mentions that K=10 already gives the optimal performance in Sokoban, but the optimal number of planning steps may vary across different environments and tasks.
- Why unresolved: The paper only provides results for K=20 in Sokoban and does not extensively explore the impact of varying K across different environments.
- What evidence would resolve it: Conducting experiments with varying K values across a diverse set of environments and tasks, and analyzing the performance trade-offs.

### Open Question 2
- Question: How does the performance of the Thinker algorithm compare to hand-crafted planning algorithms like MCTS in terms of computational efficiency and planning effectiveness?
- Basis in paper: [explicit] The paper mentions that the trained agents learn to plan more efficiently than MCTS, requiring fewer planning steps to achieve similar performance.
- Why unresolved: The paper does not provide a direct comparison of the computational efficiency and planning effectiveness between the Thinker algorithm and hand-crafted planning algorithms like MCTS.
- What evidence would resolve it: Conducting a detailed comparison of the computational cost and planning effectiveness of the Thinker algorithm versus hand-crafted planning algorithms like MCTS across various environments and tasks.

### Open Question 3
- Question: How can the Thinker algorithm be extended to handle stochastic environments and uncertainty in the learned world model?
- Basis in paper: [inferred] The paper mentions that the current implementation assumes a deterministic environment and does not predict stochastic transitions.
- Why unresolved: The paper does not explore methods for extending the Thinker algorithm to handle stochastic environments and uncertainty in the learned world model.
- What evidence would resolve it: Developing and evaluating methods for incorporating uncertainty into the learned world model and extending the Thinker algorithm to handle stochastic environments, and assessing the performance in such scenarios.

## Limitations
- Significant computational overhead due to K planning steps per environment step plus world model training
- Results may not generalize to environments with continuous or high-dimensional action spaces
- Reliance on accurate world model predictions presents a potential failure point

## Confidence
- Medium: The algorithm's mechanism is well-specified and theoretically sound, but evaluation is limited to specific environments and lacks extensive ablation studies

## Next Checks
1. Conduct an ablation study to evaluate performance with different world model architectures and planning strategies
2. Test the algorithm on continuous control tasks to assess applicability beyond discrete, visual environments
3. Measure computational cost and planning horizon required to achieve optimal performance across different environments to understand scalability limits