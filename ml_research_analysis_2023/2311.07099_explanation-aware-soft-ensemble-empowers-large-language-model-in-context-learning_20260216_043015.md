---
ver: rpa2
title: Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning
arxiv_id: '2311.07099'
source_url: https://arxiv.org/abs/2311.07099
tags:
- explanation
- explanations
- answer
- prediction
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles improving the accuracy of large language model
  in-context learning by leveraging natural language explanations. The core method,
  EASE, introduces two techniques: explanation-aware ensemble and soft probability
  aggregation.'
---

# Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning

## Quick Facts
- arXiv ID: 2311.07099
- Source URL: https://arxiv.org/abs/2311.07099
- Reference count: 28
- Improves LLM in-context learning accuracy by 2.69% to 7.05% across seven NLU tasks

## Executive Summary
This paper introduces EASE (Explanation-aware Soft Ensemble), a framework that improves the accuracy of large language model in-context learning by leveraging natural language explanations. The method combines explanation-aware ensemble weighting with soft probability aggregation to mitigate unreliable explanations and improve consistency between explanations and predictions. Experiments across seven natural language understanding tasks using four LLMs demonstrate consistent performance improvements over prior methods.

## Method Summary
EASE uses temperature-based sampling to generate multiple explanation-prediction pairs for each test example. It then employs a bootstrapped LLM scorer to evaluate explanation quality, using these scores as weights in a weighted ensemble of predictions. Additionally, soft probability aggregation computes probabilities across class-indicative verbalizers rather than aggregating one-hot predictions, reducing inconsistencies between explanations and predictions. The framework requires two additional O(N) steps beyond standard approaches: explanation scoring and soft probability aggregation.

## Key Results
- Improves accuracy by 2.69% to 7.05% over baseline methods across seven NLU tasks
- Consistently outperforms prior methods (ICL, PE, EP, Self-consistency, FLamE) on all tested datasets
- Shows robust performance across four different LLM sizes (PaLM 2-S, PaLM 2-L, FLAN-UL2, Llama-2-7b)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explanation-aware ensemble improves prediction accuracy by weighting predictions according to the quality of their associated explanations.
- **Mechanism:** A bootstrapped LLM scores each explanation's relevance and quality, and these scores serve as weights during final ensemble aggregation.
- **Core assumption:** Higher quality explanations lead to more accurate predictions; therefore, weighting by explanation quality improves final outcomes.
- **Evidence anchors:**
  - [abstract]: "We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions."
  - [section]: "Rather than treating all predictions equally, we assign a score to each prediction based on the contextual relevance and inherent quality of its associated explanation, which will be used as a weight during the final ensemble stage."
  - [corpus]: Weak evidence; the corpus neighbors focus on explanation quality and evaluation, not directly on weighted ensemble approaches.
- **Break condition:** If the bootstrapped LLM cannot reliably distinguish between good and bad explanations, the weighting becomes arbitrary and may harm performance.

### Mechanism 2
- **Claim:** Soft probability aggregation reduces inconsistency between explanations and predictions by aggregating probabilities rather than one-hot predictions.
- **Mechanism:** Instead of aggregating raw predictions, it computes the sum of probabilities associated with each potential label across all explanations.
- **Core assumption:** Sampling noise in temperature-based decoding can lead to predictions that don't align with their explanations; using soft probabilities smooths this noise.
- **Evidence anchors:**
  - [abstract]: "soft probability aggregation aggregates probabilities across multiple class-indicative verbalizers to reduce inconsistencies between explanations and predictions."
  - [section]: "it employs probabilities across various class-indicative verbalizers in place of the original one-hot predictions. This design, although conceptually simple, can effectively reduce the discrepancies between explanations and predictions and further improve the final predictions accuracy."
  - [corpus]: Weak; no direct corpus evidence supporting probability aggregation for this purpose.
- **Break condition:** If temperature sampling noise is minimal or if explanations are always perfectly aligned with predictions, soft aggregation provides no benefit.

### Mechanism 3
- **Claim:** Bootstrapped LLM scorer effectively generates negative examples for explanation quality evaluation without additional human annotations.
- **Mechanism:** Uses the original few-shot demonstrations to generate candidate explanation-prediction pairs. Explanations leading to incorrect predictions are treated as negative examples.
- **Core assumption:** An ideal explanation should guide the model toward the correct prediction; therefore, explanations leading to wrong predictions are low quality.
- **Evidence anchors:**
  - [section]: "To construct negative examples efficiently, we first use LLM to generate explanations for few-shot demonstrations, then select explanations associated with incorrect predictions as the negative samples."
  - [section]: "this approach can be sub-optimal as (1) temperature sampling increases the inconsistency between generated explanations and their associated class predictions, and (2) majority voting treats different predictions associated with explanations of varying qualities equally."
  - [corpus]: No direct corpus evidence; the concept is novel to this work.
- **Break condition:** If the LLM generates explanations that are high quality but lead to wrong predictions (e.g., due to inherent ambiguity), this mechanism will incorrectly label them as negative.

## Foundational Learning

- **Concept:** In-context learning with LLMs
  - Why needed here: The entire framework relies on LLMs adapting to new tasks using only demonstration examples without gradient updates.
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- **Concept:** Temperature sampling and its effect on prediction consistency
  - Why needed here: The framework depends on understanding how temperature sampling introduces noise that can mismatch explanations and predictions.
  - Quick check question: What is the relationship between sampling temperature and prediction consistency in LLM outputs?

- **Concept:** Explanation quality evaluation in NLP
  - Why needed here: The framework's success depends on accurately scoring explanation quality to weight predictions appropriately.
  - Quick check question: What are common approaches to evaluating the quality of natural language explanations?

## Architecture Onboarding

- **Component map:** Demonstrations → Explanation-Generation → Scoring → Weighted Aggregation → Final Prediction
- **Critical path:** Demonstration → Explanation-Generation → Scoring → Weighted Aggregation → Final Prediction
- **Design tradeoffs:**
  - Higher accuracy vs. increased computational overhead (two additional O(N) steps)
  - Reliance on LLM scoring quality vs. simpler uniform weighting
  - Soft probability aggregation vs. hard prediction aggregation
- **Failure signatures:**
  - Poor performance despite explanation weighting (LLM scorer not reliable)
  - Degradation when explanations are consistently misaligned with predictions (soft aggregation ineffective)
  - Overfitting to demonstration explanations (bootstrapping step creates bias)
- **First 3 experiments:**
  1. Compare explanation-aware ensemble vs. uniform weighting on a small dataset to validate scoring effectiveness.
  2. Test soft probability aggregation vs. hard prediction aggregation on a dataset with known temperature sampling noise.
  3. Evaluate bootstrapped scorer with and without negative examples to assess the impact of bootstrapping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different prompt templates on the performance of the explanation-aware ensemble technique?
- Basis in paper: [explicit] The paper states that they studied the effect of different prompt templates and observed that EASE is robust to them, but does not provide specific results or analysis on how different prompt templates affect performance.
- Why unresolved: The paper does not provide detailed results or analysis on the impact of different prompt templates on the explanation-aware ensemble technique.
- What evidence would resolve it: Conducting experiments with different prompt templates and comparing their impact on the performance of the explanation-aware ensemble technique would provide insights into the effect of prompt templates.

### Open Question 2
- Question: How does the performance of the explanation-aware ensemble technique vary with the number of demonstrations used for bootstrapping the LLM scorer?
- Basis in paper: [inferred] The paper mentions that the bootstrapped LLM scorer uses demonstrations to guide the scoring process, but does not provide specific details on how the number of demonstrations affects the performance of the technique.
- Why unresolved: The paper does not provide detailed analysis or experiments on how the number of demonstrations used for bootstrapping the LLM scorer impacts the performance of the explanation-aware ensemble technique.
- What evidence would resolve it: Conducting experiments with different numbers of demonstrations and comparing their impact on the performance of the explanation-aware ensemble technique would provide insights into the effect of the number of demonstrations.

### Open Question 3
- Question: How does the performance of the soft probability aggregation technique vary with the number of generated explanations?
- Basis in paper: [explicit] The paper mentions that they studied the effect of the number of generated explanations and observed that increasing the explanations generally improves performance, but does not provide specific details on how the number of explanations affects the performance of the technique.
- Why unresolved: The paper does not provide detailed results or analysis on how the number of generated explanations impacts the performance of the soft probability aggregation technique.
- What evidence would resolve it: Conducting experiments with different numbers of generated explanations and comparing their impact on the performance of the soft probability aggregation technique would provide insights into the effect of the number of explanations.

## Limitations

- **Prompt sensitivity:** Framework performance depends heavily on prompt engineering for explanation generation and scoring, with detailed prompt formats not provided
- **Computational overhead:** Method requires two additional O(N) steps beyond standard approaches, with computational costs not discussed
- **Domain generalizability:** All tested tasks are natural language understanding; approach may not generalize to other domains like code generation or mathematical reasoning

## Confidence

- **High Confidence:** The core mechanism of explanation-aware ensemble weighting (Mechanism 1) - the theoretical foundation is sound and the experimental improvements are substantial across multiple tasks and models.
- **Medium Confidence:** The soft probability aggregation technique (Mechanism 2) - while the concept is reasonable, the paper lacks corpus evidence for this specific application, and the improvement may be dataset-dependent.
- **Medium Confidence:** The bootstrapped LLM scorer for explanation quality (Mechanism 3) - the approach is novel but untested against alternative explanation quality evaluation methods, and the assumption about negative examples may not always hold.

## Next Checks

1. **Ablation Study on Scoring Reliability:** Test the explanation scorer's reliability by comparing its quality assessments against human-annotated explanation quality scores on a subset of examples. This would validate whether the bootstrapped LLM can indeed distinguish between good and bad explanations.

2. **Cross-Domain Generalization Test:** Apply EASE to non-NLU tasks (e.g., mathematical reasoning or code generation) to evaluate whether the explanation quality-weighting mechanism generalizes beyond natural language understanding tasks.

3. **Prompt Engineering Sensitivity Analysis:** Systematically vary the prompt formats for explanation generation and scoring while keeping all other components constant. Measure how sensitive the final accuracy is to prompt variations, providing insight into the method's robustness to implementation details.