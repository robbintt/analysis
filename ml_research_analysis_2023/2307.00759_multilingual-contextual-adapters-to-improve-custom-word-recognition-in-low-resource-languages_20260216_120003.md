---
ver: rpa2
title: Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource
  Languages
arxiv_id: '2307.00759'
source_url: https://arxiv.org/abs/2307.00759
tags:
- adapters
- contextual
- training
- multilingual
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving custom word recognition
  in low-resource languages for CTC-based ASR models. The core method idea is to use
  multilingual contextual adapters with a supervision loss to train the adapters.
---

# Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages

## Quick Facts
- arXiv ID: 2307.00759
- Source URL: https://arxiv.org/abs/2307.00759
- Reference count: 0
- Primary result: 48% F1 improvement in retrieving unseen custom entities for low-resource languages

## Executive Summary
This paper addresses the challenge of improving custom word recognition in low-resource languages for CTC-based ASR models. The authors propose using multilingual contextual adapters with a supervision loss to train the adapters, achieving significant improvements in custom entity retrieval while also reducing overall WER as a byproduct. The approach leverages multilingual training to improve audio representations and uses attention-based biasing to boost recognition of custom words.

## Method Summary
The method employs a three-stage multilingual training approach: (1) Train a multilingual CTC encoder on pooled data from five languages (Portuguese, Italian, Spanish, English, French) for 60 epochs, (2) Freeze the encoder and train multilingual contextual adapters using pooled data and custom word lists with CTC and CE losses, and (3) Jointly fine-tune the encoder and adapters on monolingual data. The contextual adapters use an LSTM catalog encoder and attention-based biasing mechanism to attend to custom words, with a supervision loss providing direct guidance for the adapters.

## Key Results
- 48% F1 improvement in retrieving unseen custom entities for low-resource languages
- 5-11% WER reduction in the base CTC model as a byproduct of training contextual adapters
- Multilingual training improves WER for low-resource languages compared to monolingual training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual training of the CTC encoder improves audio representations for low-resource languages by transferring knowledge from high-resource languages.
- Mechanism: By pooling training data from multiple languages and training a shared encoder, the model learns more robust phonetic and linguistic patterns that generalize across languages. This is particularly beneficial for low-resource languages that lack sufficient training data.
- Core assumption: Knowledge transfer occurs effectively between the languages used in multilingual training.
- Evidence anchors:
  - [abstract]: "Further, we explore a multilingual strategy to improve performance with limited training data."
  - [section]: "Multilingual modeling has seen a resurgence in parallel with E2E modeling. As E2E models do not require explicit alignments nor phonetic targets, the data from individual languages can be pooled together using a common sub-word vocabulary [17]. When used in low-resource language settings, this simple approach yields impressive improvements over monolingual models [12, 13, 14]."
  - [corpus]: Weak evidence - no directly related papers found on multilingual transfer for CTC models specifically.
- Break condition: If the languages in the multilingual training set are too dissimilar or if there is insufficient high-resource language data, the transfer may be ineffective.

### Mechanism 2
- Claim: The proposed supervision loss (CE loss) provides direct guidance for the Contextual Adapters to boost the correct custom words.
- Mechanism: The CE loss directly optimizes the attention weights of the Contextual Adapters to attend to the correct entity word at the appropriate time steps, rather than relying solely on the CTC loss which only provides indirect supervision.
- Core assumption: The attention weights from the Contextual Adapters can be effectively used to predict the correct entity word.
- Evidence anchors:
  - [abstract]: "We propose a supervision loss for smoother training of the Contextual Adapters."
  - [section]: "We use these attention weights to directly predict the right entity word from the list. We add a K-class cross-entropy loss... We only want to add this loss for those time-frames in the input audio where the entity word was spoken."
  - [corpus]: Weak evidence - no directly related papers found on supervision loss for Contextual Adapters.
- Break condition: If the attention weights are noisy or if the entity words are too similar, the CE loss may not provide meaningful guidance.

### Mechanism 3
- Claim: Jointly fine-tuning the multilingual encoder and Contextual Adapters on monolingual data reduces WER by providing additional training objectives.
- Mechanism: The Contextual Adapters provide an additional training objective (identifying and boosting custom words) that guides the encoder to learn more meaningful audio representations, beyond just predicting subword sequences.
- Core assumption: The additional training objective from the Contextual Adapters is beneficial for the overall ASR task.
- Evidence anchors:
  - [abstract]: "Interestingly, as a by-product of training the Contextual Adapters, we see a 5-11% Word Error Rate (WER) reduction in the performance of the base CTC model as well."
  - [section]: "This improved WER after joint fine-tuning can be attributed to the supplementary training objective of Contextual Adapters, which involves distinguishing a target entity term embedded in the audio signal from a pool of random entity terms."
  - [corpus]: Weak evidence - no directly related papers found on joint fine-tuning of encoder and Contextual Adapters.
- Break condition: If the custom word lists are not representative of the actual speech content or if the joint fine-tuning overfits to the monolingual data, the WER improvement may not be sustained.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: CTC is the core model architecture being improved for custom word recognition.
  - Quick check question: What is the main advantage of CTC models compared to hybrid models?

- Concept: Contextual Adapters
  - Why needed here: Contextual Adapters are the key mechanism for improving custom word recognition in CTC models.
  - Quick check question: How do Contextual Adapters attend to and boost custom words in the output?

- Concept: Multilingual training
  - Why needed here: Multilingual training is used to improve the performance of low-resource languages by transferring knowledge from high-resource languages.
  - Quick check question: What is the main benefit of multilingual training for low-resource languages?

## Architecture Onboarding

- Component map: CTC encoder (Conformer blocks) → produces subword posteriors → Contextual Adapters (LSTM catalog encoder + attention-based biasing adapter) → attends to custom words → Joint training objective (CTC loss + CE loss) → optimizes both components

- Critical path:
  1. Train multilingual CTC encoder on pooled data from all languages
  2. Freeze encoder and train multilingual Contextual Adapters using pooled data and custom word lists
  3. Jointly fine-tune encoder and Contextual Adapters on monolingual data

- Design tradeoffs:
  - More languages in multilingual training → better transfer but more complex model
  - Larger custom word lists → better personalization but more training data required
  - Higher weight on CE loss → better custom word recognition but potentially worse overall WER

- Failure signatures:
  - Low F1 scores on custom words despite high WER → Contextual Adapters not learning to boost correctly
  - High WER on low-resource languages despite multilingual training → insufficient transfer or overfitting
  - Slow convergence during joint fine-tuning → imbalanced gradients between encoder and Contextual Adapters

- First 3 experiments:
  1. Train monolingual CTC encoder and Contextual Adapters with CE loss to establish baseline
  2. Train multilingual CTC encoder and evaluate WER improvement on low-resource languages
  3. Train multilingual Contextual Adapters and evaluate F1 score improvement on custom words

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of multilingual contextual adapters vary across different language families and scripts (e.g., Indo-European vs. non-Indo-European languages)?
- Basis in paper: [inferred] The paper only evaluates five languages (Portuguese, Italian, Spanish, English, French), all Indo-European languages using Latin script. It states "suggests that the copying mechanism it learns is language-agnostic (at least for this set of languages)" but doesn't test non-Indo-European languages.
- Why unresolved: The paper's experiments are limited to similar language families and scripts, so the generalization to typologically diverse languages remains untested.
- What evidence would resolve it: Testing the approach on languages from different families (e.g., Mandarin, Arabic, Swahili) with different scripts and phonological structures would provide evidence for or against language-agnostic generalization.

### Open Question 2
- Question: What is the relationship between the number of training hours and the performance improvement from multilingual contextual adapters for very low-resource languages?
- Basis in paper: [inferred] The paper shows improvements for Portuguese (100 hours) but doesn't systematically vary training data volume or establish a threshold effect. It mentions "learning the parameters for the Contextual Adapters requires a large volume of labeled data" but doesn't quantify this relationship.
- Why unresolved: The paper doesn't explore how performance scales with different amounts of monolingual training data, particularly for languages with extremely limited data (e.g., 10-20 hours).
- What evidence would resolve it: Conducting experiments that systematically vary training data volume (e.g., 10, 20, 50, 100, 200 hours) for low-resource languages would establish the relationship between data quantity and performance gains.

### Open Question 3
- Question: How do multilingual contextual adapters affect recognition of non-entity words and overall language model coherence?
- Basis in paper: [explicit] The paper notes "minimal impact on overall WER" for the baseline approach and shows WER improvements as a "by-product" of training, but doesn't investigate effects on non-entity vocabulary or coherence.
- Why unresolved: The evaluation focuses primarily on F1 scores for custom entities, with limited analysis of how the adapters affect general language modeling quality or recognition of non-entity words.
- What evidence would resolve it: Conducting detailed analysis of WER breakdown by word frequency, part-of-speech categories, and language model perplexity would reveal whether the adapters introduce biases or degradation in non-entity recognition.

## Limitations

- Lack of comparison with state-of-the-art multilingual ASR models that could serve as stronger baselines
- Evaluation methodology may not fully represent real-world deployment scenarios for custom word recognition
- Limited investigation of robustness across different types of custom words and catalog configurations

## Confidence

**High Confidence:**
- The multilingual training approach improves WER for low-resource languages (5-11% reduction)
- The contextual adapters architecture is technically sound and can attend to custom words using the proposed attention mechanism

**Medium Confidence:**
- The 48% F1 improvement for unseen custom entities
- The joint fine-tuning approach improves both custom word recognition and overall WER

**Low Confidence:**
- The supervision loss (CE loss) is essential for effective training
- The approach generalizes well to truly unseen custom words not in the training catalog

## Next Checks

1. **Ablation study on supervision loss**: Remove the CE loss component and train only with CTC loss to quantify the exact contribution of the supervision loss to the 48% F1 improvement. This would validate whether the supervision loss is truly essential or if CTC loss alone could achieve similar results with proper tuning.

2. **Cross-domain evaluation**: Evaluate the trained models on domain-shifted data (e.g., conversational speech, technical presentations) to assess robustness and generalization beyond the Librivox MLS dataset used in the paper. This would test real-world applicability.

3. **Catalog sensitivity analysis**: Systematically vary the size and composition of custom word lists (e.g., 100 vs 500 words, different word frequencies) to understand the limits of the approach and identify optimal catalog configurations for different deployment scenarios.