---
ver: rpa2
title: 'Multimodal Sentiment Analysis: Perceived vs Induced Sentiments'
arxiv_id: '2312.07627'
source_url: https://arxiv.org/abs/2312.07627
tags:
- sentiment
- text
- visual
- dataset
- gifs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal sentiment analysis
  for GIFs, focusing on distinguishing between perceived sentiment (author's intent)
  and induced sentiment (reader's reaction). The proposed framework integrates visual
  and textual features using a multi-modal approach.
---

# Multimodal Sentiment Analysis: Perceived vs Induced Sentiments

## Quick Facts
- **arXiv ID**: 2312.07627
- **Source URL**: https://arxiv.org/abs/2312.07627
- **Reference count**: 40
- **Primary result**: Achieved 82.7% accuracy on ReactionGIF dataset for multimodal sentiment analysis of GIFs

## Executive Summary
This paper addresses the challenge of multimodal sentiment analysis for GIFs, focusing on distinguishing between perceived sentiment (author's intent) and induced sentiment (reader's reaction). The proposed framework integrates visual and textual features using a multi-modal approach, employing pre-trained BERT for textual analysis, VGG19 for visual sentiment analysis, DeepFace for face emotion detection, and OCR for extracting embedded text from GIFs. The model achieves 82.7% accuracy on the ReactionGIF dataset, outperforming state-of-the-art methods. The study also analyzes the variance between perceived and induced sentiments, revealing that 48% of the dataset contains both face detection and OCR-generated captions, contributing to higher accuracy.

## Method Summary
The framework combines multiple modalities for GIF sentiment analysis: (1) fine-tuned BERT model for textual sentiment analysis of tweets, (2) VGG19 model fine-tuned on Twitter images for visual sentiment, (3) DeepFace for facial emotion detection, and (4) Tesseract OCR for extracting embedded text from GIF frames. These modalities are fused using late fusion averaging to produce binary sentiment predictions (positive/negative). The approach leverages transfer learning by fine-tuning pre-trained models on domain-specific Twitter data to capture nuances in informal, multimedia-rich language and imagery.

## Key Results
- Achieved 82.7% accuracy on ReactionGIF dataset, improving upon state-of-the-art methods
- 48% of dataset examples contain both face detection and OCR-generated captions
- Demonstrated effectiveness of combining visual and textual modalities for accurate sentiment analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating OCR-extracted text with visual sentiment models improves context understanding for GIF sentiment prediction.
- Mechanism: OCR extracts captions embedded in GIF frames, providing additional semantic context that complements visual features. These captions are processed by a fine-tuned BERT model, generating text-based sentiment scores that are fused with visual scores via late fusion averaging.
- Core assumption: Embedded captions in GIFs carry sentiment-relevant information that is not fully captured by visual features alone.
- Evidence anchors:
  - [abstract] "incorporates attributes including face emotion detection and OCR generated captions to capture the semantic aspects of the GIF."
  - [section] "We have utilized the Optical Character Recognition (OCR) technique to extract textual content from the frames of GIF videos...The generated OCR captions were analysed for their sentiment probability score exploiting the same BERT model fine-tuned during Text Sentiment Analysis."
  - [corpus] Weak evidence: No direct citation of OCR improving accuracy, only mentions related multimodal fusion studies.
- Break condition: If GIFs rarely contain embedded text, OCR contributions will be negligible.

### Mechanism 2
- Claim: Face emotion detection on GIF frames captures viewer-perceived sentiment distinct from author-intended sentiment.
- Mechanism: DeepFace detects human faces in GIF frames and classifies them into six basic emotions. These emotions are mapped to positive/negative sentiment scores, which are averaged to produce a face emotion score. This score is fused with visual and OCR scores via late fusion.
- Core assumption: Human facial expressions in GIFs reliably indicate the sentiment being conveyed, aligning with induced rather than perceived sentiment.
- Evidence anchors:
  - [abstract] "incorporates attributes including face emotion detection...to capture the semantic aspects of the GIF."
  - [section] "We employed the DeepFace python framework for face detection and facial emotion analysis...The output of the DeepFace library for emotion recognition typically provides probabilities for a given facial image corresponding to six basic emotions...We calculate facial emotion score of a GIF video as the average predicted probability of emotions with positive and negative sentiments."
  - [corpus] Moderate evidence: Several papers (e.g., [28], [30]) use facial features in GIF sentiment analysis, supporting this approach.
- Break condition: If GIFs contain few or no faces, face emotion detection will add little value.

### Mechanism 3
- Claim: Fine-tuning pre-trained models (BERT for text, VGG19 for images) on domain-specific Twitter data improves sentiment classification accuracy over generic sentiment analysis.
- Mechanism: BERT is fine-tuned on the ReactionGIF dataset's tweets, adapting its contextual embeddings to the informal, multimedia-rich language of Twitter. VGG19 is fine-tuned on the T4SA dataset, adapting its visual features to sentiment-bearing images from Twitter.
- Core assumption: Domain adaptation of pre-trained models captures nuances in Twitter-specific language and imagery that generic models miss.
- Evidence anchors:
  - [abstract] "The developed classifier achieves an accuracy of 82.7% on Twitter GIFs, which is an improvement over state -of-the-art models."
  - [section] "We have utilized huggingface pytorch implementation of BertForSequenceClassification to train our model on the huggingface dataset...We conducted experiments using various datasets to fine-tune our pre-trained textual and visual models."
  - [corpus] Weak evidence: No explicit citation comparing fine-tuned vs. generic model performance.
- Break condition: If the dataset is too small or not representative, fine-tuning may overfit or fail to improve accuracy.

## Foundational Learning

- Concept: Multimodal sentiment analysis
  - Why needed here: The paper combines visual (GIF frames), textual (tweets, OCR captions), and facial emotion data to predict sentiment, requiring understanding of how to fuse multiple data modalities.
  - Quick check question: What are the three main fusion strategies in multimodal sentiment analysis, and which one is used here?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The approach leverages pre-trained BERT and VGG19 models, fine-tuning them on Twitter-specific data to adapt to the domain.
  - Quick check question: Why is fine-tuning necessary when using pre-trained models, and what risks does it mitigate?

- Concept: OCR and face detection pipelines
  - Why needed here: OCR extracts embedded text from GIFs, and face detection identifies emotional expressions, both feeding into sentiment prediction.
  - Quick check question: How does OCR technology translate image-based text into machine-readable sentiment features?

## Architecture Onboarding

- Component map: Twitter data (tweets, ReactionGIFs) -> Text preprocessing + BERT fine-tuning (perceived sentiment) -> VGG19 fine-tuning (visual sentiment) -> OCR extraction + BERT fine-tuning (text from GIFs) -> DeepFace face detection + emotion classification (facial sentiment) -> Late fusion averaging -> Binary sentiment label (positive/negative)

- Critical path: GIF → frame extraction → image sentiment (VGG19) + face detection (DeepFace) + OCR text extraction (BERT) → late fusion → final sentiment

- Design tradeoffs:
  - Late fusion assumes modality independence, which may not hold if visual and textual cues are highly correlated.
  - Fine-tuning on domain data improves accuracy but risks overfitting with limited data.
  - OCR and face detection add computational overhead but improve context awareness.

- Failure signatures:
  - Low accuracy when GIFs lack faces or embedded text.
  - Overfitting if fine-tuning datasets are too small or not diverse.
  - Fusion failure if modality scores are inconsistent or contradictory.

- First 3 experiments:
  1. Baseline: Train VGG19 on T4SA, test on ReactionGIF without OCR or face detection.
  2. Add OCR: Include OCR-extracted captions processed by BERT, compare accuracy gain.
  3. Add face detection: Include DeepFace emotion scores, evaluate improvement over OCR-only model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the multimodal sentiment analysis framework change when using different pre-trained models for textual sentiment analysis (e.g., BERT vs. other transformer-based models)?
- Basis in paper: [explicit] The paper mentions using a pre-trained BERT model for textual sentiment analysis and suggests potential areas for improvement, including the use of different models.
- Why unresolved: The paper only reports results using BERT and does not compare its performance with other models.
- What evidence would resolve it: Experimental results comparing the performance of different pre-trained models for textual sentiment analysis in the context of multimodal sentiment analysis of GIFs.

### Open Question 2
- Question: What is the impact of incorporating additional visual attributes, such as object and animal identification, on the accuracy of the multimodal sentiment analysis framework?
- Basis in paper: [explicit] The paper suggests that attributes like object and animal identification could enhance the semantic understanding of images and improve the framework's performance.
- Why unresolved: The paper does not experiment with these additional attributes.
- What evidence would resolve it: Experimental results showing the accuracy of the framework with and without the incorporation of additional visual attributes.

### Open Question 3
- Question: How does the framework perform on datasets with more diverse and complex sentiments, beyond binary classification?
- Basis in paper: [explicit] The paper focuses on binary classification and suggests extending the research to predict multiclass emotions.
- Why unresolved: The paper does not explore the framework's performance on datasets with diverse sentiments.
- What evidence would resolve it: Experimental results demonstrating the framework's accuracy and effectiveness on datasets with multiple sentiment classes.

## Limitations
- No statistical validation (confidence intervals or standard deviation) provided for the reported 82.7% accuracy
- Limited ablation studies to quantify individual contributions of OCR and face emotion detection components
- Late fusion approach assumes modality independence, which may not hold when visual and textual cues are correlated

## Confidence

- **High confidence**: The general framework design (BERT + VGG19 + DeepFace + OCR with late fusion) is well-established in multimodal sentiment analysis literature and logically addresses the problem of distinguishing perceived vs induced sentiments.
- **Medium confidence**: The reported accuracy of 82.7% is plausible given the state-of-the-art methods cited, but lacks statistical validation and comparative ablation studies.
- **Low confidence**: The specific contributions of OCR and face emotion detection to accuracy improvements are not empirically validated through controlled experiments.

## Next Checks

1. **Ablation study**: Remove OCR and face emotion detection components individually and evaluate accuracy changes to quantify their specific contributions to the 82.7% performance.

2. **Statistical validation**: Compute 95% confidence intervals and standard deviation for the reported accuracy across multiple runs with different random seeds to establish result robustness.

3. **Modality correlation analysis**: Measure the correlation between visual, textual, and facial emotion scores to assess whether the late fusion assumption of modality independence holds for this dataset.