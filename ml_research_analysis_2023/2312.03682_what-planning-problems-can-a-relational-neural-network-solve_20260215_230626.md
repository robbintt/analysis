---
ver: rpa2
title: What Planning Problems Can A Relational Neural Network Solve?
arxiv_id: '2312.03682'
source_url: https://arxiv.org/abs/2312.03682
tags:
- goal
- regression
- planning
- clear
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the circuit complexity of goal-conditioned
  policies for classical planning problems using relational neural networks (RelNNs).
  The authors introduce the concept of serialized goal regression search (S-GRS) and
  its connection to regression width, a measure of search efficiency.
---

# What Planning Problems Can A Relational Neural Network Solve?

## Quick Facts
- arXiv ID: 2312.03682
- Source URL: https://arxiv.org/abs/2312.03682
- Reference count: 17
- This paper analyzes the circuit complexity of goal-conditioned policies for classical planning problems using relational neural networks (RelNNs).

## Executive Summary
This paper investigates which planning problems can be solved by polynomial-sized relational neural network (RelNN) policies. The authors introduce serialized goal regression search (S-GRS) and its connection to regression width, a measure of search efficiency. They prove that problems with constant regression width admit polynomial-time algorithms and finite-breadth RelNN policies, while problems like Sokoban require unbounded breadth. The paper also develops a method to compile S-GRS into RelNN policies, showing that policy circuit complexity depends on the problem's regression width. Experiments validate these theoretical predictions across domains like Assembly3, Logistics, and BlocksWorld.

## Method Summary
The paper analyzes classical planning problems represented as STRIPS actions and develops algorithms to compile these problems into RelNN policies. The core approach uses serialized goal regression search (S-GRS) to decompose planning into subgoals with bounded constraints, and regression rule selectors (RRS) to directly predict regression rules. The method involves encoding state and goal as tensors, applying RelNN layers to compute subgoals/constraints, and deriving actions from satisfied preconditions. RelNN models are trained using Neural Logic Machines with Adam optimizer, learning rate 0.001, β1=0.9, β2=0.999.

## Key Results
- Problems with constant regression width admit polynomial-time algorithms and finite-breadth RelNN policies.
- Sokoban requires unbounded regression width, making finite-breadth RelNN policies impossible.
- Regression rule selectors can drastically reduce policy circuit depth compared to full S-GRS.
- Circuit depth depends on planning horizon T and serialization efficiency; breadth depends on SOS width k and predicate arity β.

## Why This Works (Mechanism)

### Mechanism 1
Policy circuit complexity can be bounded by regression width, enabling finite-breadth relational neural networks for many planning domains. Serialized goal regression search (S-GRS) decomposes planning into a sequence of subgoals with bounded constraints, reducing the combinatorial search space. The SOS width (k) determines the maximum number of constraints to track, directly bounding the RelNN breadth as (k+1)·β. The planning problem has constant SOS width independent of the number of objects, and optimal serializability holds for the regression rules.

### Mechanism 2
Regression rule selectors can drastically reduce policy circuit depth by directly predicting regression rules without full search. A state-dependent regression rule selector (RRS) maps (state, goal, constraints) → (ordered preconditions, action) using a finite-depth RelNN. This avoids recursive S-GRS, reducing depth from O(T) to O(log_b T) where b is the average number of preconditions per rule. The RRS can be computed by a finite-depth RelNN and is serializable.

### Mechanism 3
Circuit depth depends on planning horizon T and serialization efficiency; breadth depends on SOS width k and predicate arity β. BWD compilation yields RelNN[O(T), β·kBWD] by tracking all goal sets; S-GRS yields RelNN[O(T), (k+1)·β] by tracking only constraints. RRS yields RelNN[O(d·Dr), max(Br, β)] where d is regression tree depth. The planning horizon T and SOS width k are independent of problem size for the analyzed domains.

## Foundational Learning

- **Concept:** STRIPS planning and backward search
  - **Why needed here:** The paper analyzes classical planning problems represented as STRIPS actions; understanding backward search is essential for grasping S-GRS and regression width.
  - **Quick check question:** In STRIPS, what are the components of an action schema, and how does backward search use them to achieve a goal?

- **Concept:** Graph Neural Networks (GNNs) and relational neural networks
  - **Why needed here:** RelNNs are the policy representation; their expressiveness is characterized by breadth (maximum relation arity) and depth (number of layers).
  - **Quick check question:** How does the breadth of a RelNN relate to the number of variables in first-order logic formulas it can express?

- **Concept:** Complexity analysis (PSPACE-hardness, width-based search)
  - **Why needed here:** The paper connects regression width to search complexity; understanding width-based search (IW algorithms) is key to grasping SOS width.
  - **Quick check question:** What is the relationship between forward width and the number of actions needed to solve a planning problem?

## Architecture Onboarding

- **Component map:** State tensor + Goal tensor -> RelNN layers -> Action tensor
- **Critical path:**
  1. Encode state and goal into tensors
  2. Apply RelNN layers to compute subgoals/constraints
  3. Derive next action from satisfied preconditions
  4. Execute action, observe next state, update policy
- **Design tradeoffs:**
  - Breadth vs. expressiveness: Higher breadth allows more complex relations but increases parameter count.
  - Depth vs. generalization: Adaptive depth (e.g., O(N) for BlocksWorld) enables solving larger problems but may overfit.
  - RRS vs. S-GRS: RRS reduces depth but requires learning a selector; S-GRS is more general but deeper.
- **Failure signatures:**
  - High training loss: Policy cannot learn to satisfy preconditions.
  - Low test success rate: Policy overfits to training problem size.
  - Exploding/vanishing gradients: Depth too large for stable training.
- **First 3 experiments:**
  1. Train RelNN[D=1, B=2] on Assembly3 (n=10) → Test on n=30,50. Expect failure due to insufficient depth.
  2. Train RelNN[D=3, B=2] on Logistics (n=10) → Test on n=30,50. Expect failure due to fixed depth.
  3. Train RelNN[D=f(n), B=2] on BlocksWorld-Clear → Measure plan length. Expect shorter plans with adaptive depth.

## Open Questions the Paper Calls Out

### Open Question 1
How can we extend the SOS width analysis to continuous domains where states and actions are continuous values? The paper mentions that it is challenging to directly generalize the proof to continuous domains and suggests it as an interesting future direction. This is unresolved because the proof techniques rely on discrete logical formulas and finite action spaces. A rigorous definition of "width" for continuous domains, along with proof that certain continuous planning problems have bounded width and can be solved by polynomial-time algorithms, would resolve this question.

### Open Question 2
Can we develop a general algorithm to automatically construct the regression rule selector (RRS) for a given planning problem, rather than relying on hand-crafted rules or learned models? The paper mentions that the construction of the regression rule selector is not automatic and will generally require human priors or machine learning. This is unresolved because while the paper provides examples of RRS for specific domains, it does not offer a general methodology for constructing them for arbitrary planning problems. An algorithm that takes as input a planning problem description and outputs a valid RRS, along with theoretical guarantees on its correctness and efficiency, would resolve this question.

### Open Question 3
How can we analyze the generalization properties of relational neural networks (RelNNs) for planning problems with multiple goals or more complex goal specifications beyond single atoms? The paper mentions that for problems with multiple goals, the goals are usually not serializable in the optimal planning case, and discusses extending the analysis to ∀-quantified preconditions. This is unresolved because the current analysis focuses on single-atom goals, and extending it to more complex goal specifications is non-trivial due to the increased complexity of the search space and the need to track additional constraints. Theoretical results on the circuit complexity of RelNNs for planning problems with multiple goals or ∀-quantified preconditions, along with empirical validation on a diverse set of planning domains, would resolve this question.

## Limitations
- The analysis assumes idealized conditions for regression width and serialization that may not hold in practice.
- Empirical validation is limited to relatively small problem instances, leaving questions about scalability to large, real-world domains.
- The connection between regression width and polynomial-time solvability may break down for problems with complex subgoal dependencies or non-constant regression width.

## Confidence
- Theoretical claims: High (follow from established complexity results and properties of STRIPS planning)
- Empirical results: Medium (limited scope of experiments and potential gaps in implementation details)

## Next Checks
1. Systematically analyze the regression width of a wider range of planning problems, including those with known PSPACE-hardness, to verify the connection between regression width and polynomial-time solvability.
2. Evaluate the performance of RelNN policies on larger problem instances (e.g., Logistics with 50+ cities, Assembly3 with 50+ parts) to assess scalability and identify potential bottlenecks.
3. Compare the performance of RelNN policies against other policy representations (e.g., graph neural networks, transformers) on the same planning tasks to benchmark the effectiveness of the regression-based approach.