---
ver: rpa2
title: 'Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed
  Assumptions'
arxiv_id: '2305.18471'
source_url: https://arxiv.org/abs/2305.18471
tags:
- adagrad
- term
- proof
- assumption
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the convergence of AdaGrad for non-convex\
  \ optimization problems under relaxed assumptions compared to previous work. The\
  \ key innovation is introducing an auxiliary function \u03BE(t) = \u2225\u2207f(wt)\u2225\
  \xB2/\u221A\u03BDt to handle the correlation between gradient and adaptive learning\
  \ rate in AdaGrad updates."
---

# Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions

## Quick Facts
- arXiv ID: 2305.18471
- Source URL: https://arxiv.org/abs/2305.18471
- Reference count: 40
- Primary result: Novel convergence analysis of AdaGrad for non-convex optimization under relaxed assumptions, achieving O(1/T) rate for over-parameterized regime

## Executive Summary
This paper provides a novel convergence analysis of AdaGrad for non-convex optimization problems using an auxiliary function approach. The key innovation is introducing ξ(t) = ∥∇f(wt)∥²/√νt to handle the correlation between gradient and adaptive learning rate, enabling simpler proofs and tighter convergence rates than previous work. The analysis relaxes assumptions on noise variance and smoothness, showing AdaGrad can converge under (L₀,L₁)-smooth conditions with learning rate below a threshold, and achieves O(1/T) convergence for over-parameterized models (matching SGD) instead of O(1/T⁴).

## Method Summary
The paper analyzes AdaGrad's convergence by introducing an auxiliary function ξ(t) = ∥∇f(wt)∥²/√νt that enables tight control of correlation errors in the adaptive updates. The method involves: 1) Establishing a descent lemma for AdaGrad updates, 2) Decomposing the first-order term and identifying correlation error, 3) Applying the auxiliary function to bound this error via telescoping, 4) Combining bounds to derive convergence rates. The analysis extends to coordinate-wise AdaGrad and non-uniformly smooth landscapes under (L₀,L₁)-smooth conditions, showing convergence requires learning rate below 1/L₁.

## Key Results
- Novel auxiliary function ξ(t) = ∥∇f(wt)∥²/√νt simplifies AdaGrad convergence analysis by eliminating correlation error complexity
- Achieves O(1/T) convergence rate for over-parameterized regime (D₀=0), matching SGD instead of O(1/T⁴)
- Extends analysis to (L₀,L₁)-smooth conditions, showing AdaGrad converges with learning rate below 1/L₁ threshold
- Proves necessity of learning rate constraint via counterexample under non-uniform smoothness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auxiliary function ξ(t) = ∥∇f(wt)∥²/√νt enables tight control of the correlation error term in AdaGrad updates
- Mechanism: By tracking the product of gradient norm squared and inverse square root of accumulated gradient magnitudes, ξ(t) provides a telescoping bound that cancels the error term arising from the mismatch between νt and νt-1 in the update denominator
- Core assumption: The adaptive learning rate η/√νk is non-increasing over iterations
- Evidence anchors:
  - [abstract]: "The proof is essentially based on a novel auxiliary function ξ that helps eliminate the complexity of handling the correlation between the numerator and denominator of AdaGrad's update"
  - [section]: "The choice of ξ(t) is motivated as follows... we can use ξ(t) ≜ ∥∇f(wt)∥²/√νt instead of ∥∇f(wt)∥∥gt∥/√νt as the auxiliary function"
- Break condition: If νt were to decrease or fluctuate significantly, the telescoping argument would fail

### Mechanism 2
- Claim: The auxiliary function enables convergence rate O(1/T) for over-parameterized regime, matching SGD
- Mechanism: Under strong growth condition (D₀=0), the bound on ∑∥∇f(wt)∥² becomes linear in T rather than T⁴, yielding the improved rate
- Core assumption: Strong growth condition holds (D₀=0), characterizing over-parameterized models
- Evidence anchors:
  - [abstract]: "for the over-parameterized regime, we show that AdaGrad needs only O(1/ε²) iterations... which matches the rate of SGD"
  - [section]: "When restricted to the strong growth condition, i.e., D₀ = 0, our result gives a rate O(1/T), much faster than that in (Faw et al., 2022) and matching that of SGD"
- Break condition: If strong growth condition fails (D₀>0), the rate degrades to O(1/T⁴)

### Mechanism 3
- Claim: The auxiliary function approach extends to non-uniformly smooth landscapes under (L₀,L₁)-smooth conditions
- Mechanism: By bounding the additional error terms caused by (L₀,L₁)-smoothness with the same order as the "Error" term in the proof, AdaGrad can still converge under these more realistic conditions
- Core assumption: (L₀,L₁)-smooth condition holds and learning rate is below threshold 1/L₁
- Evidence anchors:
  - [abstract]: "we prove that AdaGrad succeeds in converging under (L₀,L₁)-smooth condition as long as the learning rate is lower than a threshold"
  - [section]: "The key insight of which is that the additional error terms caused by (L₀,L₁)-smooth condition is at the same order of the 'Error' term in the proof of Theorem 2"
- Break condition: If learning rate exceeds 1/L₁ threshold, convergence fails (counterexample provided)

## Foundational Learning

- Concept: Telescoping sums and their application to convergence analysis
  - Why needed here: The auxiliary function ξ(t) relies on telescoping properties to bound the cumulative error term
  - Quick check question: How does ∑(ξ(t-1) - ξ(t)) simplify, and why is this useful for bounding error terms in iterative algorithms?

- Concept: Strong growth condition and its implications for over-parameterized models
  - Why needed here: Understanding why D₀=0 leads to improved convergence rates requires grasping the strong growth condition
  - Quick check question: What is the relationship between strong growth condition and the gradient-noise variance bound D₀?

- Concept: (L₀,L₁)-smooth condition and its distinction from uniform smoothness
  - Why needed here: The paper extends analysis to non-uniformly smooth landscapes, requiring understanding of this generalization
  - Quick check question: How does (L₀,L₁)-smooth condition differ from standard L-smooth condition, and why is it more realistic for neural networks?

## Architecture Onboarding

- Component map: AdaGrad algorithm -> Auxiliary function ξ(t) -> Descent lemma -> Error decomposition -> Convergence rate bounds
- Critical path:
  1. Establish descent lemma under given assumptions
  2. Decompose first-order term and identify correlation error
  3. Apply auxiliary function to bound correlation error via telescoping
  4. Combine bounds to derive convergence rate
  5. Extend to coordinate-wise AdaGrad and (L₀,L₁)-smooth conditions
- Design tradeoffs:
  - Simplicity vs. tightness: The auxiliary function approach trades some generality for dramatically simpler proofs
  - Assumption strength: Relaxed assumptions (affine noise variance, (L₀,L₁)-smoothness) come at the cost of potential learning rate thresholds
  - Coordinate-wise vs. norm version: Coordinate-wise AdaGrad requires stronger assumptions but applies to practical implementations
- Failure signatures:
  - Divergence when learning rate exceeds 1/L₁ under (L₀,L₁)-smoothness
  - Suboptimal O(1/T⁴) rate when strong growth condition fails (D₀>0)
  - Proof breakdown if νt is not monotonically increasing
- First 3 experiments:
  1. Verify the telescoping bound by implementing ξ(t) tracking and comparing cumulative error bounds
  2. Test convergence rates under strong growth condition (D₀=0) vs. general case (D₀>0)
  3. Evaluate sensitivity to learning rate threshold under (L₀,L₁)-smoothness assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the auxiliary function ξ(t) = ∥∇f(wt)∥²/√νt generalize to other adaptive optimizers like Adam?
- Basis in paper: The authors explicitly suggest this possibility in Remark 1, stating "similar approach can be applied to the analysis of other adaptive optimizers with non-increasing adaptive learning rates, such as AMSGrad."
- Why unresolved: The paper only applies this technique to AdaGrad variants and doesn't test it on other optimizers.
- What evidence would resolve it: A convergence proof for Adam or other adaptive optimizers using the same auxiliary function technique.

### Open Question 2
- Question: Can the convergence analysis be extended to handle more general noise variance assumptions beyond affine noise variance?
- Basis in paper: The paper's main results (Theorems 2, 6, 7, 8) all rely on affine noise variance assumptions (Assumptions 2 and 3), which the authors note are still stronger than many existing works.
- Why unresolved: The auxiliary function technique appears to rely critically on the affine structure of the noise variance.
- What evidence would resolve it: A convergence proof for AdaGrad under alternative noise assumptions (e.g., bounded noise, sub-Gaussian noise) using the same methodology.

### Open Question 3
- Question: Is the O(1/T) convergence rate under strong growth condition tight for AdaGrad-Norm?
- Basis in paper: The authors claim this rate "matches that of SGD" but don't prove matching lower bounds.
- Why unresolved: The paper only provides upper bounds on convergence rates.
- What evidence would resolve it: A matching lower bound showing that AdaGrad-Norm cannot converge faster than O(1/T) under strong growth conditions.

## Limitations

- The analysis relies heavily on the auxiliary function approach, which may not generalize to all non-convex optimization landscapes
- The paper assumes specific noise conditions (D₀, D₁) and smoothness bounds that may not hold for all practical problems
- The necessity of the learning rate threshold under (L₀,L₁)-smoothness is proven via counterexample, but the bounds may be conservative in practice

## Confidence

- **High confidence**: The telescoping argument using ξ(t) for bounding correlation errors in AdaGrad updates
- **Medium confidence**: The extension to (L₀,L₁)-smooth conditions and the necessity of learning rate thresholds
- **Low confidence**: The practical implications of the strong growth condition (D₀=0) for real-world over-parameterized models

## Next Checks

1. **Empirical verification**: Implement AdaGrad with the auxiliary function tracking and verify the O(1/T) convergence rate empirically on synthetic over-parameterized problems where D₀≈0.

2. **Learning rate threshold test**: Systematically test AdaGrad convergence under (L₀,L₁)-smoothness with learning rates both below and above the theoretical threshold 1/L₁ to validate the counterexample's predictions.

3. **Generalization test**: Evaluate AdaGrad's performance on standard non-convex benchmarks (e.g., training deep neural networks) to assess whether the relaxed assumptions provide practical benefits over existing convergence guarantees.