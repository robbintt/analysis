---
ver: rpa2
title: 'Boosting Logical Reasoning in Large Language Models through a New Framework:
  The Graph of Thought'
arxiv_id: '2308.08614'
source_url: https://arxiv.org/abs/2308.08614
tags:
- graph
- node
- path
- nodes
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel prompting technique called Graph
  of Thoughts (GoT) to enhance the reasoning capabilities of large language models
  (LLMs) in complex, multi-step logical tasks. Inspired by human cognitive processes,
  GoT constructs a directed graph where nodes represent intermediate thoughts and
  edges represent logical connections.
---

# Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought

## Quick Facts
- arXiv ID: 2308.08614
- Source URL: https://arxiv.org/abs/2308.08614
- Reference count: 8
- Key outcome: Graph of Thoughts (GoT) framework achieves accuracy improvements of up to 97%, 89%, and 55% over GPT-4 and Tree of Thought methods on three logical reasoning tasks

## Executive Summary
This paper introduces the Graph of Thoughts (GoT) framework, a novel prompting technique designed to enhance the reasoning capabilities of large language models (LLMs) in complex, multi-step logical tasks. Inspired by human cognitive processes, GoT constructs a directed graph where nodes represent intermediate thoughts and edges represent logical connections. The method employs a checking mechanism to verify the correctness of paths and a graph updating algorithm to iteratively refine the solution. Evaluated on three tasks of increasing difficulty—the 24-point game, solving high-degree polynomial equations, and deriving recursive sequence formulas—GoT significantly outperforms both GPT-4 and the state-of-the-art Tree of Thought method, achieving accuracy improvements of up to 97%, 89%, and 55%, respectively.

## Method Summary
The Graph of Thoughts framework uses a directed graph structure where nodes represent intermediate thoughts and edges represent logical connections between them. The approach starts with the target node and employs backward reasoning to explore connections to known conditions. A Checker function with multiple inspectors validates paths by independently verifying their feasibility, using a conjunctive (AND) validation process. The graph updating algorithm iteratively refines the solution by adding validated intermediate nodes to the condition sequence, allowing for incremental knowledge accumulation without complete graph reconstruction. The framework is evaluated on three tasks of increasing complexity using GPT-4 with a temperature of 0.7 to generate paths and nodes.

## Key Results
- Achieved 89.7% accuracy improvement on the 24-point game task compared to baseline methods
- Demonstrated 86% accuracy improvement on high-degree polynomial equation solving
- Showed 56% accuracy improvement on recursive sequence formula derivation tasks
- GoT brought LLM performance closer to human-level reasoning through stringent inspection mechanisms and efficient graph updating

## Why This Works (Mechanism)

### Mechanism 1
Graph of Thought uses backward reasoning starting from the target, which aligns with human mathematical reasoning. The directed graph structure with the target as the root allows the model to explore logical connections backward to known conditions, mirroring how mathematicians reason from conclusions to identify promising research avenues.

### Mechanism 2
The Checker function provides rigorous validation through multiple independent inspectors that verify path feasibility. A path is only considered valid if all inspectors return True, creating a conjunctive validation process that reduces false positives compared to single scoring mechanisms.

### Mechanism 3
Graph updating enables efficient reuse of previously discovered conditions without complete reconstruction. As intermediate nodes are validated, they are added to the condition sequence, allowing the model to leverage accumulated knowledge across multiple graph traversals for incremental problem solving.

## Foundational Learning

- **Graph theory and directed graph traversal**: Understanding how nodes, edges, AND-Crossroad nodes, and condition nodes interact in directed graphs is essential for grasping how GoT constructs and navigates problem spaces. *Quick check: What is the difference between an AND-Crossroad node and a regular intermediate node in the GoT graph structure?*

- **Multi-step logical reasoning and backward chaining**: Understanding how backward reasoning from a target differs from forward reasoning from conditions is crucial for understanding the paper's effectiveness. *Quick check: Why does starting from the target (backward reasoning) potentially offer advantages over starting from conditions (forward reasoning) in complex logical problems?*

- **Probability and error correction in iterative systems**: Understanding how multiple independent verifications reduce error probability is key to grasping the Checker function's effectiveness. *Quick check: If each inspector has a 90% accuracy rate, what is the probability that a path passes three inspectors when it is actually incorrect?*

## Architecture Onboarding

- **Component map**: Graph Construction Module -> Checker Function -> Graph Updating Algorithm -> Path Finding Module -> LLM Interface

- **Critical path**:
  1. Start with target node
  2. Construct initial graph using LLM to discover connections
  3. Validate paths using Checker function with multiple inspectors
  4. Update graph by adding validated intermediate nodes to conditions
  5. Find complete path from conditions to target
  6. If no valid path found, return to step 2 with updated conditions

- **Design tradeoffs**:
  - Depth vs. breadth: Deeper graphs may capture more complex relationships but increase computational cost
  - Number of inspectors: More inspectors increase accuracy but also increase latency
  - Graph reconstruction frequency: Frequent updates preserve progress but add overhead

- **Failure signatures**:
  - Checker function consistently rejects all paths (inspectors too strict or LLM unable to generate valid connections)
  - Graph grows too large to manage (ineffective pruning or excessive branching)
  - No valid paths found after multiple iterations (insufficient conditions or problem beyond model's capabilities)

- **First 3 experiments**:
  1. Implement basic graph construction with a simple 24-point game problem and verify the backward reasoning structure
  2. Add Checker function with 3 inspectors and test path validation on known correct and incorrect paths
  3. Implement graph updating with a recursive sequence problem that requires multiple iterations to solve

## Open Questions the Paper Calls Out

### Open Question 1
How does the Graph of Thoughts (GoT) approach compare to other prompting methods in terms of accuracy and efficiency? The paper compares GoT to other prompting methods including Input-Output (IO), Chain-of-Thought (CoT), Self-Consistency of Chain-of-Thought (SC-CoT), and Tree-of-Thought (ToT), demonstrating higher accuracy and efficiency in solving complex, multi-step logical tasks.

### Open Question 2
How can the Graph of Thoughts (GoT) approach be extended or adapted to handle even more complex reasoning tasks or to work with different types of large language models? The paper presents GoT as effective for a specific set of tasks but does not explore its potential for handling more complex reasoning tasks or working with different types of large language models.

### Open Question 3
How can the Graph of Thoughts (GoT) approach be combined with other techniques or methods to further enhance the reasoning capabilities of large language models? The paper presents GoT as a standalone technique and does not explore the potential for combining it with other methods to further enhance LLM reasoning capabilities.

## Limitations

- Lack of ablation studies to isolate contributions of individual components (backward reasoning, Checker function, graph updating)
- Limited evaluation to three specific task types with small datasets, raising concerns about generalizability
- Insufficient computational efficiency analysis, particularly regarding the latency introduced by multiple inspectors in the Checker function

## Confidence

- **Performance Claims**: Low confidence - impressive accuracy improvements lack rigorous statistical validation and ablation analysis
- **Mechanism Claims**: Medium confidence - backward reasoning and graph updating are theoretically sound but lack empirical evidence of individual contributions
- **Generalizability Claims**: Low confidence - claim of bringing LLM performance "closer to human-level reasoning" is not adequately supported by experimental evidence limited to mathematical problems

## Next Checks

1. **Ablation Study Validation**: Conduct systematic ablation experiments removing each key component (backward reasoning, Checker function, graph updating) to quantify their individual contributions to performance gains.

2. **Cross-Domain Generalization Test**: Evaluate GoT on reasoning tasks outside the mathematical domain, including logical puzzles, common-sense reasoning problems, and multi-step planning tasks.

3. **Statistical Significance and Variance Analysis**: Perform repeated trials across multiple problem instances within each task category, reporting confidence intervals and p-values for performance differences.