---
ver: rpa2
title: Infinite Class Mixup
arxiv_id: '2305.10293'
source_url: https://arxiv.org/abs/2305.10293
tags:
- mixup
- class
- classi
- nite
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Infinite Class Mixup, which aims to directly
  enforce linear behavior between classifiers in Mixup by interpolating classifiers
  instead of labels. The authors propose a dual-contrastive loss that contrasts each
  mixed pair with all other classifiers and mixed pairs in the batch, optimizing over
  both axes simultaneously.
---

# Infinite Class Mixup

## Quick Facts
- arXiv ID: 2305.10293
- Source URL: https://arxiv.org/abs/2305.10293
- Reference count: 40
- Key outcome: Infinite Class Mixup improves classification accuracy on balanced, long-tailed, and data-constrained benchmarks compared to standard Mixup and variants

## Executive Summary
This paper addresses the limitation of standard Mixup augmentation by directly interpolating classifier weights instead of interpolating labels. The proposed Infinite Class Mixup method creates unique classifiers for mixed samples through linear interpolation of classifier weight vectors, enforced through a dual-contrastive loss that contrasts each mixed pair against all other classifiers and mixed pairs in the batch. Experiments demonstrate consistent improvements across multiple benchmark datasets and settings.

## Method Summary
Infinite Class Mixup works by interpolating classifier weight vectors directly to create new classifiers for mixed samples, rather than mixing labels at the probability level. For each mixed pair of samples, the method computes a convex combination of their classifier weight vectors to form a new classifier. The dual-contrastive loss simultaneously optimizes over both classifier and pair axes: each mixed sample is contrasted against all other classifiers (class-axis) and each classifier is contrasted against all other mixed samples (pair-axis). This is implemented through cross-entropy loss computed over both contrastive views, with their losses simply summed.

## Key Results
- On CIFAR-100, Infinite Class Mixup achieves 77.90% accuracy compared to 77.33% for standard Mixup
- Demonstrates improvements on long-tailed datasets (LT-CIFAR10) and data-constrained settings (ciFAIR-10/100)
- Outperforms Mixup variants including RegMixup and Remix across multiple benchmark types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Infinite Class Mixup directly enforces linear classifier behavior between classes.
- Mechanism: Interpolates classifier weight vectors directly instead of interpolating labels at the probability level.
- Core assumption: Classifier space is continuous and convex combinations of weights represent meaningful intermediate classifiers.
- Evidence anchors: [abstract] "This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair."

### Mechanism 2
- Claim: Dual-contrastive loss optimizes over both classifier and pair axes simultaneously.
- Mechanism: Contrasts each mixed sample against all other classifiers and contrasts each classifier against all other mixed samples using combined cross-entropy loss.
- Core assumption: Two contrastive views provide complementary gradient flows that together improve optimization.
- Evidence anchors: [section] "We propose a dual-contrastive Infinite Class Mixup loss, where we contrast the classifier of a mixed pair to both the classifiers and the predicted outputs of other mixed pairs in a batch."

### Mechanism 3
- Claim: Infinite Class Mixup improves performance especially in data-constrained and long-tailed settings.
- Mechanism: Creates unique classifiers for mixed samples, providing more diverse training signals when data is limited or class distributions are imbalanced.
- Core assumption: More diverse training signals help models generalize better when training data is scarce or imbalanced.
- Evidence anchors: [section] "Empirically, we show that it outperforms standard Mixup and variants such as RegMixup and Remix on balanced, long-tailed, and data-constrained benchmarks."

## Foundational Learning

- Concept: Linear interpolation in vector spaces
  - Why needed here: Method relies on creating convex combinations of classifier weight vectors to form new classifiers
  - Quick check question: If you have two vectors w1 and w2, what is the formula for their convex combination with interpolation ratio Î»?

- Concept: Contrastive learning framework
  - Why needed here: Dual-contrastive loss requires understanding how to contrast positive pairs against negative pairs in a batch
  - Quick check question: In contrastive learning, what is the difference between a positive pair and a negative pair?

- Concept: Softmax classification and cross-entropy loss
  - Why needed here: Method builds on standard classification training but modifies how loss is computed for mixed samples
  - Quick check question: What is the mathematical relationship between the softmax output and the cross-entropy loss in standard classification?

## Architecture Onboarding

- Component map: Image mixing layer -> Classifier interpolation module -> Dual-contrastive loss computation -> Standard network backbone
- Critical path: Image mixing -> Classifier creation -> Loss computation -> Backpropagation through both original network and classifier weights
- Design tradeoffs: More diverse training signals vs. increased computational complexity from dual contrastive axes
- Failure signatures: Performance degradation when batch size is too small, instability in long-tailed scenarios with extreme imbalance
- First 3 experiments:
  1. Compare Infinite Class Mixup vs standard Mixup on CIFAR-100 with ResNet-34, measuring accuracy improvement
  2. Test dual-contrastive loss ablation by training with only class-axis or only pair-axis to verify complementarity
  3. Evaluate performance on LT-CIFAR100 with imbalance ratio 0.01 to test long-tailed benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Assumes classifier weight space is convex and continuous, which may not hold for all architectures
- Does not explore performance on neural network architectures beyond ResNet and Wide ResNet
- Does not analyze how performance scales with dataset size beyond showing benefits for data-constrained settings

## Confidence

**Low** on claim that interpolating classifier weights creates meaningful intermediate classifiers
**Medium** on dual-contrastive loss providing complementary gradients
**Medium** on performance improvements in data-constrained settings

## Next Checks

1. Implement visualization showing decision boundaries of interpolated classifiers vs original classifiers to verify they produce meaningful intermediate regions.

2. Run experiments isolating class-axis and pair-axis contrastive losses separately, then test combinations to quantify marginal benefit of each component.

3. Compare number of effective parameters used during training between standard Mixup and Infinite Class Mixup to determine if performance gains come from increased capacity rather than proposed mechanism.