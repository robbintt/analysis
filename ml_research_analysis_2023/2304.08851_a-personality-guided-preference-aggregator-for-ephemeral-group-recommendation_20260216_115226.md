---
ver: rpa2
title: A Personality-Guided Preference Aggregator for Ephemeral Group Recommendation
arxiv_id: '2304.08851'
source_url: https://arxiv.org/abs/2304.08851
tags:
- group
- personality
- recommendation
- preference
- pega
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles ephemeral group recommendation, where the main
  challenge is aggregating individual preferences under data sparsity. It introduces
  personality traits extracted from user reviews and proposes a Personality-Guided
  Preference Aggregator (PEGA).
---

# A Personality-Guided Preference Aggregator for Ephemeral Group Recommendation

## Quick Facts
- arXiv ID: 2304.08851
- Source URL: https://arxiv.org/abs/2304.08851
- Reference count: 40
- Primary result: PEGA achieves up to 14.6% improvement in NDCG@10 and 8.0% in Recall@10 over state-of-the-art methods

## Executive Summary
This paper addresses ephemeral group recommendation, where groups form temporarily and lack sufficient interaction history. The core challenge is aggregating diverse individual preferences under data sparsity. The proposed solution, PEGA (Personality-Guided Preference Aggregator), introduces personality traits extracted from user reviews and uses these to guide preference aggregation. The method employs a hyper-rectangle representation for group personality, a personality attention mechanism to weight user contributions, and a two-stage training strategy to handle data sparsity. Experiments on Yelp and Amazon datasets demonstrate significant performance improvements over existing methods.

## Method Summary
PEGA addresses ephemeral group recommendation by extracting personality traits from user reviews using the LIWC lexicon and the Big-Five personality model. It represents group personality as a hyper-rectangle to capture personality distributions rather than fixed points. The personality attention mechanism dynamically weights user contributions based on their personality alignment with the group personality. A two-stage training strategy first learns user embeddings from individual interactions, then optimizes group preference representation using group-item interactions with Bayesian Personalized Ranking. The model balances personality and preference guidance through a tunable parameter Œª.

## Key Results
- Achieves up to 14.6% improvement in NDCG@10 and 8.0% in Recall@10 over state-of-the-art methods
- Personality attention mechanism outperforms simpler aggregation methods
- Ablation studies show personality is more influential than preference in guiding group recommendations
- Method is robust to varying group sizes and performs well on both Yelp and Amazon datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personality attention dynamically weights user contributions by mapping group personality traits to individual personality embeddings
- Mechanism: The personality attention mechanism treats group personality as a query vector, individual personalities as keys, and user embeddings as values, using an MLP layer to compute attention scores
- Core assumption: Individual personality traits can be accurately extracted from review text using LIWC lexicon categories
- Evidence anchors:
  - [abstract]: "we design a personality attention mechanism to learn the influence of individual personality in a specific group"
  - [section]: "Given a group ùëî, we use the personality attention mechanism to calculate the influence weight of ùëî's memberùë¢ùë°"
  - [corpus]: Weak - no corpus papers directly discuss personality attention mechanisms for group recommendation
- Break condition: If personality extraction from text is inaccurate, attention weights become unreliable and group preference aggregation degrades

### Mechanism 2
- Claim: Hyper-rectangles capture personality distribution ranges rather than fixed points, providing better generalization for group personality
- Mechanism: Group personality is represented as a hyper-rectangle where each dimension covers a range of personality values, defined by center and offset vectors
- Core assumption: Personality traits within a group follow a distribution pattern rather than clustering around single points
- Evidence anchors:
  - [section]: "we adopt the hyper-rectangle to carry the rich information in the group personality since each dimension of the hyper-rectangle covers a range of values"
  - [section]: "Hyper-rectangle is the high-dimensional form of the rectangle where each edge of the hyper-rectangle represents a real value closed interval"
  - [corpus]: Weak - corpus papers discuss personality in general but not hyper-rectangle representations specifically
- Break condition: If group personality distributions are too diverse, the hyper-rectangle becomes too large and loses discriminative power

### Mechanism 3
- Claim: Two-stage training with user-level and group-level BPR optimization addresses data sparsity in ephemeral groups
- Mechanism: First stage optimizes user embeddings using user-item interactions, second stage optimizes group preference representation using group-item interactions
- Core assumption: User embeddings learned from individual interactions can transfer to group contexts
- Evidence anchors:
  - [section]: "We leverage a two-stage training strategy to alleviate the sparsity issue of group-item interaction"
  - [section]: "Since a ranked list of top-K items is required in both stages, Bayesian Personalized Ranking (BPR) pairwise learning is adopted"
  - [section]: "We obtain the user and the item embeddings by minimizing the user-level BPR pairwise loss Lùë¢ùë†ùëíùëü"
- Break condition: If user-item interactions are too sparse, the first stage fails to learn meaningful user representations

## Foundational Learning

- Concept: Big-Five personality model
  - Why needed here: Provides the theoretical framework for quantifying personality traits that guide group decision-making
  - Quick check question: Can you list all five dimensions of the Big-Five model and what each measures?

- Concept: LIWC lexicon-based personality extraction
  - Why needed here: Enables automatic extraction of personality traits from user-generated review text without explicit questionnaires
  - Quick check question: How does TF-IDF weighting work when calculating personality scores from review text?

- Concept: Hyper-rectangle geometry
  - Why needed here: Represents group personality as a bounded region rather than a point, capturing the distribution of individual personalities
  - Quick check question: What mathematical operation defines the boundary of a hyper-rectangle in n-dimensional space?

## Architecture Onboarding

- Component map:
  Review-based Personality Extraction Module ‚Üí Personality Embedding ‚Üí Personality-Guided Preference Aggregation Module ‚Üí Group Preference ‚Üí Model Optimization Module ‚Üí Final Recommendations

- Critical path: Review extraction ‚Üí Personality attention ‚Üí Preference aggregation ‚Üí Prediction
- Design tradeoffs: Personality vs. preference weighting balance (controlled by Œª parameter)
- Failure signatures: Poor performance indicates either bad personality extraction or incorrect attention weight computation
- First 3 experiments:
  1. Test personality extraction accuracy on a small labeled dataset
  2. Validate hyper-rectangle boundary calculations with synthetic personality data
  3. Verify attention mechanism produces reasonable weights on known group compositions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Group Personality hyper-rectangle behave under extreme personality diversity, and what are its limitations in representing highly heterogeneous groups?
- Basis in paper: [explicit] The paper introduces hyper-rectangle for Group Personality but notes that "excessively diverse individual personalities will generate a group personality that covers too much information and leads to low generalization."
- Why unresolved: The paper proposes shrinkage via projection matrices but does not empirically test or theoretically analyze the performance boundaries or failure modes when group personality diversity exceeds certain thresholds.
- What evidence would resolve it: Controlled experiments varying personality diversity levels within groups and measuring the resulting Group Personality representation quality and downstream recommendation performance.

### Open Question 2
- Question: What is the optimal balance between personality-guided and preference-guided aggregation across different types of ephemeral groups (e.g., interest-based vs. randomly formed)?
- Basis in paper: [explicit] The paper uses a fixed balance coefficient Œª=0.3 and notes that "personality and preference enjoy the same weight (i.e., Œª=0.5) in Amazon-Rand" but does not systematically explore optimal weighting strategies for different group formation contexts.
- Why unresolved: The paper only tests a single fixed Œª value and observes that performance varies by dataset, suggesting that optimal balance may depend on group characteristics that are not fully explored.
- What evidence would resolve it: Systematic experiments with varying Œª across different group formation mechanisms and personality-preference alignment scenarios to identify optimal weighting strategies.

### Open Question 3
- Question: How does the proposed personality attention mechanism scale with group size, and what are the computational bottlenecks for very large ephemeral groups?
- Basis in paper: [inferred] The paper shows robustness across group sizes but does not analyze computational complexity or scalability limits of the personality attention mechanism.
- Why unresolved: The paper demonstrates performance across tested group sizes but does not provide complexity analysis or performance degradation patterns as group size increases beyond tested ranges.
- What evidence would resolve it: Complexity analysis of the personality attention mechanism and empirical scalability testing with groups larger than those tested in the current experiments.

## Limitations
- Personality extraction accuracy depends on review text quality and quantity, potentially failing with sparse or uninformative reviews
- Hyper-rectangle representation may lose discriminative power when group personality diversity is too high
- Two-stage training assumes user-item interactions provide sufficient signal for group contexts, which may not hold for users with very limited individual history

## Confidence
- Personality attention mechanism effectiveness: Medium
- Hyper-rectangle personality representation: Medium
- Two-stage training strategy: Medium
- Overall performance claims: Medium (given limited dataset diversity)

## Next Checks
1. Implement a controlled experiment comparing personality attention against simpler personality averaging methods to isolate the contribution of the attention mechanism
2. Test PEGA's performance when personality extraction quality is degraded (e.g., using synthetic noisy reviews) to understand robustness limits
3. Evaluate the method on additional datasets with different group formation patterns (e.g., professional teams vs. social groups) to assess generalizability beyond Yelp and Amazon