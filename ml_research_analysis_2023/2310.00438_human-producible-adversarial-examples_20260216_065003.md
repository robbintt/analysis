---
ver: rpa2
title: Human-Producible Adversarial Examples
arxiv_id: '2310.00438'
source_url: https://arxiv.org/abs/2310.00438
tags:
- lines
- adversarial
- examples
- class
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces adversarial tags\u2014real-world adversarial\
  \ examples generated using simple marker lines that humans can draw. The authors\
  \ propose a generate-and-prune algorithm to optimize line placement, accounting\
  \ for human drawing errors via jitter and erasure in a robust loss function."
---

# Human-Producible Adversarial Examples

## Quick Facts
- arXiv ID: 2310.00438
- Source URL: https://arxiv.org/abs/2310.00438
- Reference count: 12
- Human-producible adversarial tags achieve 54.8% untargeted attack success with 4 lines and 81.8% with 9 lines on YOLOv8

## Executive Summary
This paper introduces adversarial tags—real-world adversarial examples generated using simple marker lines that humans can draw. The authors propose a generate-and-prune algorithm to optimize line placement, accounting for human drawing errors via jitter and erasure in a robust loss function. Evaluation on YOLOv8 shows 54.8% success rate with 4 lines and 81.8% with 9 lines for untargeted attacks. A user study confirms that humans can reproduce these tags with measurable effectiveness, and physical experiments with tape lines show successful misclassification in real-world images. Targeted attacks remain challenging. The method highlights vulnerabilities in ML-based vision systems and the feasibility of low-cost, accessible adversarial attacks.

## Method Summary
The paper presents a generate-and-prune algorithm that optimizes line parameters for adversarial attacks on YOLOv8. The method uses differentiable line rendering to generate candidate lines, then prunes the set to keep only the most effective perturbations. A robust loss function incorporates human drawing errors (jitter and erasure) to ensure tags remain effective when reproduced manually. The algorithm iteratively generates random lines, evaluates their effectiveness through gradient-based optimization, and prunes to maintain a minimal set of high-impact lines. Physical experiments validate the method using tape lines on objects, demonstrating successful misclassification in real-world images.

## Key Results
- Untargeted attacks achieve 54.8% success rate with 4 lines and 81.8% with 9 lines on YOLOv8
- User study confirms human reproducibility with measurable effectiveness
- Physical experiments with tape lines show successful misclassification in real-world images
- Targeted attacks remain challenging with lower success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Line-based perturbations directly alter pixel gradients in regions that YOLOv8 uses for classification, causing misattribution to a different class.
- Mechanism: By overlaying straight black lines onto an image, the rendered pixels change the input to the CNN, which shifts the activation patterns in the feature maps. Because YOLOv8 is trained on ImageNet, these shifted activations are interpreted as belonging to a different class.
- Core assumption: The adversarial lines are sufficiently prominent and placed in regions that affect the model's confidence scores for the original class.
- Evidence anchors: [abstract] "by drawing just 4 lines we can disrupt a YOLO-based model in 54.8% of cases; increasing this to 9 lines disrupts 81.8% of the cases tested"
- Break condition: If the lines are drawn in regions that do not influence the model's decision boundaries, or if the model is trained with adversarial defenses that ignore line-based perturbations.

### Mechanism 2
- Claim: Robust loss accounts for human drawing error, ensuring adversarial tags remain effective when reproduced manually.
- Mechanism: The robust loss function generates multiple jittered and erased versions of each line during optimization, simulating human imprecision. This ensures the final set of lines still causes misclassification even when drawn imperfectly.
- Core assumption: Human drawing errors follow predictable patterns (jitter, erasure) that can be modeled and compensated for during generation.
- Evidence anchors: [abstract] "Next, we devise an improved method for line placement to be invariant to human drawing error"
- Break condition: If human drawing errors exceed the modeled jitter/erasure ranges, or if the model's robustness mechanisms detect and ignore such perturbations.

### Mechanism 3
- Claim: Fewer longer lines are more practical for human reproduction while maintaining adversarial effectiveness.
- Mechanism: The generate-and-prune algorithm optimizes line parameters to maximize misclassification with minimal line count. Longer lines reduce the total number needed, making them easier for humans to draw accurately.
- Core assumption: Humans can draw longer straight lines more accurately than many shorter ones, and YOLOv8's classification is sensitive to fewer, strategically placed perturbations.
- Evidence anchors: [section] "We can see similar performance for both groups, but with the fewer longer group taking nearly 25% fewer steps with a factor of 3 − 4 fewer lines which results in significant compute saving and easier human reproduction"
- Break condition: If YOLOv8's decision boundaries require more complex perturbations that cannot be achieved with fewer longer lines, or if human precision degrades with line length.

## Foundational Learning

- Concept: Adversarial examples in machine learning
  - Why needed here: Understanding how small input perturbations can cause misclassification is fundamental to grasping why adversarial tags work.
  - Quick check question: What is the difference between digital and physical adversarial examples?

- Concept: Convolutional neural networks and feature extraction
  - Why needed here: YOLOv8's classification relies on CNN layers that extract features from images; understanding this helps explain how line perturbations affect the model.
  - Quick check question: How do CNNs process input images to make classification decisions?

- Concept: Differentiable rendering and gradient-based optimization
  - Why needed here: The method uses differentiable line rendering to optimize line parameters via gradients, which is crucial for generating effective adversarial tags.
  - Quick check question: What is the role of differentiable rendering in generating adversarial examples?

## Architecture Onboarding

- Component map: YOLOv8 model -> Differentiable line renderer -> Generate-and-prune algorithm -> Robust loss function -> User interface
- Critical path: 1. Input image is passed through YOLOv8 to get initial class prediction 2. Lines are generated and rendered using differentiable renderer 3. Robust loss is calculated by simulating human drawing errors 4. Gradients are backpropagated to optimize line parameters 5. Best line configuration is selected and output as adversarial tag
- Design tradeoffs: Line count vs. human reproducibility (fewer longer lines are easier to draw but may be less effective), robust vs. non-robust loss (robust loss improves human reproducibility but increases computation time), line thickness (thicker lines are more visible and effective but may be more noticeable)
- Failure signatures: Lines not affecting YOLOv8's confidence scores, human drawing errors exceeding modeled jitter/erasure ranges, model's adversarial defenses detecting and ignoring line perturbations
- First 3 experiments: 1. Generate adversarial tags with 4 lines (untargeted, non-robust) and test on YOLOv8 to verify ~55% success rate 2. Generate adversarial tags with 9 lines (untargeted, robust) and test on YOLOv8 to verify ~82% success rate 3. Conduct user study with 20 images, having participants draw lines from digital guides, and test scanned images on YOLOv8 to verify human reproducibility

## Open Questions the Paper Calls Out
- How effective are adversarial tags when applied to 3D objects in real-world scenarios, beyond the 2D image case studied in the paper?
- Can adversarial tags be made more robust against environmental factors such as lighting changes, occlusions, or different viewing angles?
- How can the generation of adversarial tags be optimized to reduce computational cost and time while maintaining effectiveness?

## Limitations
- Effectiveness may not generalize to other object detection models beyond YOLOv8
- User study sample size (20 images) may not capture full range of drawing errors
- Physical experiments use tape instead of marker lines, which may behave differently in real-world conditions

## Confidence
- High confidence: The digital generation algorithm and differentiable rendering mechanism are well-established, with clear implementation details and reproducible results in controlled settings.
- Medium confidence: Human reproducibility claims are supported by user study data but limited by sample size and controlled conditions that may not reflect real-world variability.
- Low confidence: Physical-world effectiveness beyond the controlled tape experiments is uncertain, particularly for varying lighting conditions, camera quality, and real-world surfaces.

## Next Checks
1. Cross-model generalization test: Apply the same adversarial tags to different YOLO variants (YOLOv5, YOLOv7) and other object detection architectures (Faster R-CNN, SSD) to assess transferability and model-specific vulnerabilities.

2. Extended user study: Conduct a larger-scale user study (100+ images, 50+ participants) with varied drawing implements (markers, pens, pencils) and surfaces to quantify the impact of human drawing variability on attack success rates.

3. Real-world environmental robustness: Test adversarial tags on objects in different lighting conditions (indoor/outdoor, day/night), viewing angles, and distances to evaluate performance degradation and establish environmental thresholds for effectiveness.