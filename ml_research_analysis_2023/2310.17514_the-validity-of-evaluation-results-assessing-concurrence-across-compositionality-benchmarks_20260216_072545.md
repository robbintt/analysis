---
ver: rpa2
title: 'The Validity of Evaluation Results: Assessing Concurrence Across Compositionality
  Benchmarks'
arxiv_id: '2310.17514'
source_url: https://arxiv.org/abs/2310.17514
tags:
- scan
- geoquery
- spider
- length
- cogs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the construct validity of compositional
  generalization benchmarks, finding that different datasets and splits yield inconsistent
  model rankings despite being designed to measure the same capability. The authors
  evaluate six models (BART, T5, LSTM variants, Transformer, Neural-BTG) across four
  datasets (SCAN, GeoQuery, COGS, Spider) using eight compositional splitting strategies,
  totaling 18 distinct splits.
---

# The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks

## Quick Facts
- arXiv ID: 2310.17514
- Source URL: https://arxiv.org/abs/2310.17514
- Authors: Facebook AI Research
- Reference count: 18
- Key outcome: Different compositional generalization datasets yield inconsistent model rankings despite measuring the same claimed capability, with only 10 of 153 dataset-split pairs achieving high concurrence (τ ≥ 0.7)

## Executive Summary
This paper investigates the construct validity of compositional generalization benchmarks by evaluating six models across four datasets using eight compositional splitting strategies. The authors find that datasets rarely agree on model rankings, with natural (human-authored) datasets showing better agreement among themselves than with synthetic datasets. Their analysis reveals that lexical item control during pretraining significantly impacts performance and concurrence, and that different datasets may operationalize different definitions of compositionality. The findings suggest current compositional generalization evaluation practices lack consistency and that clearer definitions, better dataset validation, and more rigorous experimental design are needed for the field to progress.

## Method Summary
The authors evaluate six models (BART, T5, LSTM variants, Transformer, Neural-BTG) across four datasets (SCAN, GeoQuery, COGS, Spider) using eight compositional splitting strategies, totaling 18 distinct splits. Models are trained/fine-tuned on each split with 3-5 random seeds, and performance is measured using exact match accuracy with space normalization. Kendall's tau correlation is computed to measure concurrence between dataset splits. The study includes both pretrained and trained-from-scratch models to assess lexical exposure effects, and conducts lexical replacement experiments to validate the impact of vocabulary control on performance.

## Key Results
- Datasets rarely agree on model rankings, with only 10 out of 153 dataset-split pairs achieving high concurrence (τ ≥ 0.7)
- Natural datasets show better agreement among themselves (0.54 average concurrence) than synthetic datasets (0.22 average concurrence)
- Lexical item control during pretraining significantly impacts performance, with pretrained models' performance dropping drastically when lexical items are replaced with random strings
- Only 4 out of 18 dataset splits show high self-concurrence (τ ≥ 0.8), suggesting high variance across random seeds

## Why This Works (Mechanism)

### Mechanism 1
Lexical exposure during pretraining creates uncontrolled variance that distorts compositional generalization evaluation. When pretrained models encounter lexical items during pretraining that also appear in test splits (even if excluded from training), they can leverage memorization rather than true compositional understanding, artificially inflating performance. This works because lexical items are treated as fundamental units of compositionality evaluation, and exposure to these items outside the controlled training set undermines the validity of generalization measurements.

### Mechanism 2
Different datasets rank models differently because they measure different aspects of compositionality, not because of inconsistent methodology. Datasets designed to evaluate the same capability actually operationalize different definitions of what compositionality means, leading to fundamentally different model requirements and therefore different rankings. This works because "compositionality" is not a single, well-defined capability but rather an umbrella term covering multiple distinct phenomena that require different modeling approaches.

### Mechanism 3
The source of data (synthetic vs natural) fundamentally changes what models learn and therefore how they perform on compositional tasks. Synthetic datasets create different learning dynamics than natural datasets because they lack the distributional properties, noise patterns, and implicit structures present in naturally occurring language, leading to different model rankings. This works because natural and synthetic data distributions are fundamentally different in ways that matter for compositional generalization, not just in surface form.

## Foundational Learning

- **Concept: Construct validity in evaluation**
  - Why needed here: The paper fundamentally questions whether different compositional generalization benchmarks actually measure the same underlying capability, which is a construct validity problem
  - Quick check question: If two tests claim to measure "intelligence" but one uses math problems and another uses verbal reasoning, what validity concern arises when their results don't correlate?

- **Concept: Kendall's tau correlation for rank agreement**
  - Why needed here: The paper uses Kendall's tau to measure concurrence between model rankings across different dataset splits, requiring understanding of rank correlation metrics
  - Quick check question: If Model A ranks 1st on Dataset X and 3rd on Dataset Y, while Model B ranks 3rd on Dataset X and 1st on Dataset Y, what would Kendall's tau be for their relative rankings?

- **Concept: Lexical control in experimental design**
  - Why needed here: The paper demonstrates that controlling which lexical items appear in training vs test data is crucial for valid compositional generalization evaluation, especially for pretrained models
  - Quick check question: If a test includes the word "giraffe" in the test set but never in training, what two different explanations could account for a model correctly using this word?

## Architecture Onboarding

- **Component map**: 4 datasets (SCAN, COGS, GeoQuery, Spider) → 8 splitting strategies → 6 models (BART, T5, LSTM variants, Transformer, Neural-BTG) → 192 model-dataset-split combinations → Exact Match accuracy → Kendall's tau concurrence
- **Critical path**: 1) Train/fine-tune each model on each dataset split, 2) Evaluate using exact match accuracy, 3) Compute pairwise concurrence values between all dataset splits, 4) Analyze patterns in concurrence to understand evaluation validity
- **Design tradeoffs**: Using exact match accuracy is simple but potentially too strict, using Kendall's tau captures rank agreement but loses information about absolute performance differences, including both pretrained and trained-from-scratch models provides comparison but adds complexity
- **Failure signatures**: Low self-concurrence values (below 0.8) suggest seed variation is high and results may not be stable, negative concurrence values indicate opposite model rankings, systematic differences between pretrained and non-pretrained models suggest lexical exposure effects
- **First 3 experiments**:
  1. Replicate the concurrence calculation between COGS and SCAN length splits to verify the 0.01 average concurrence value
  2. Test the lexical replacement experiment on GeoQuery to confirm that replacing lexical items with wug words reduces pretrained model performance
  3. Run the same model across different seeds on the same dataset split to measure self-concurrence and establish baseline stability

## Open Questions the Paper Calls Out

### Open Question 1
Does the lack of consistency in compositional generalization benchmarks stem from task-specific evaluation needs rather than fundamental flaws in benchmark design? The paper notes that even when benchmarks share the same interpretation of compositionality (splitting strategy), they rarely concur, suggesting task formulation may play a significant role. A systematic comparison of benchmarks across different task types with controlled splitting strategies would reveal whether task formulation is the primary driver of benchmark disagreement.

### Open Question 2
How do alternative compositional generalization evaluation metrics compare in terms of establishing construct validity across benchmarks? The authors use exact match accuracy with various normalization strategies, noting that "EM accuracy may be too strict for our purposes." A comprehensive evaluation of the same benchmark pairs using multiple compositional generalization metrics would reveal whether current metric choices contribute to the observed lack of validity or whether the issue persists across evaluation paradigms.

### Open Question 3
What is the relationship between model architectural inductive biases and benchmark agreement for compositional generalization? While the paper demonstrates that model type affects performance and concurrence, it does not systematically analyze whether certain architectural families show more or less agreement across benchmarks. A controlled study varying architectural families while holding dataset properties constant would reveal whether specific model architectures consistently agree across compositional benchmarks.

## Limitations

- Exact match accuracy may be too strict and could artificially suppress concurrence values
- The study focuses on a limited set of datasets and splitting strategies, potentially missing other important compositional phenomena
- Lexical control experiments demonstrate the impact of vocabulary exposure but don't fully resolve how to design compositionally valid evaluation protocols

## Confidence

**High Confidence**: The finding that datasets rarely agree on model rankings (only 10/153 pairs with τ ≥ 0.7) is robust and well-supported by the empirical analysis.

**Medium Confidence**: The claim that natural datasets show better agreement among themselves than with synthetic datasets has empirical support but could be influenced by the specific datasets chosen.

**Low Confidence**: The assertion that the field needs clearer definitions of compositionality and more rigorous evaluation design represents normative recommendations rather than measured conclusions.

## Next Checks

1. **Cross-dataset generalization test**: Train models on natural datasets and evaluate on synthetic datasets (and vice versa) to determine if performance patterns transfer across data types.

2. **Lexical item ablation study**: Systematically vary the number and type of lexical items in compositional splits while keeping structural complexity constant, to quantify the precise relationship between lexical exposure and compositional generalization performance.

3. **Alternative metric comparison**: Recompute concurrence using multiple evaluation metrics (BLEU, chrF, exact match with different strictness levels) to determine whether the low concurrence is an artifact of the exact match metric or reflects genuine disagreement about model quality.