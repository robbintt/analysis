---
ver: rpa2
title: 'The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans
  and LLM-based Agents'
arxiv_id: '2311.09665'
source_url: https://arxiv.org/abs/2311.09665
tags:
- group
- social
- human
- agents
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether LLM-based agents can replicate human-like
  collective intelligence, specifically the "wisdom of partisan crowds" phenomenon
  where groups converge to more accurate beliefs despite partisan biases. Researchers
  role-played Democrat and Republican personas using detailed and simple backgrounds,
  with or without Chain-of-Thought reasoning and induced partisan biases.
---

# The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents

## Quick Facts
- arXiv ID: 2311.09665
- Source URL: https://arxiv.org/abs/2311.09665
- Reference count: 29
- The study investigates whether LLM-based agents can replicate human-like collective intelligence, specifically the "wisdom of partisan crowds" phenomenon where groups converge to more accurate beliefs despite partisan biases.

## Executive Summary
This study evaluates whether LLM-based agents can replicate human-like collective intelligence through the "wisdom of partisan crowds" phenomenon, where groups converge to more accurate beliefs despite partisan biases. Researchers role-played Democrat and Republican personas using detailed and simple backgrounds, with or without Chain-of-Thought reasoning and induced partisan biases. The study found that agents with detailed personas and no Chain-of-Thought reasoning most closely matched human behavior, showing stronger group error reduction and partisan bias alignment. Induced biases increased partisan bias but did not enhance collective accuracy. Fine-tuning on human data improved alignment but caused overfitting, producing unrealistic extreme values.

## Method Summary
The study used LangChain and OpenAI's ChatGPT (gpt-3.5-turbo) to simulate agents with different persona details, Chain-of-Thought reasoning, and induced biases. Agents role-played Democrat and Republican personas in social conditions (with peer feedback) versus control conditions (no feedback) across eight partisan-biased factual questions. Performance was evaluated through multiple rounds of answering, with alignment metrics comparing LLM agent behavior to human data from Becker et al. (2019). The study tested different configurations including detailed versus simple personas, presence or absence of Chain-of-Thought reasoning, and fine-tuning on human response data.

## Key Results
- Agents with detailed personas and no Chain-of-Thought reasoning most closely matched human behavior in group error reduction and partisan bias alignment
- Chain-of-Thought reasoning diminished the reduction of group error and hurt alignment with human behavior
- Fine-tuning on human data improved alignment but caused overfitting, producing unrealistic extreme values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detailed personas without Chain-of-Thought reasoning produce human-like partisan bias and group error reduction
- Mechanism: Rich persona backgrounds create diverse initial estimates through demographic and experiential variation, enabling the wisdom of partisan crowds effect when agents update based on peer feedback
- Core assumption: Persona detail drives individual differences in initial estimates, which is necessary for convergence toward ground truth through social influence
- Evidence anchors:
  - [abstract] "Agents with detailed personas and no Chain-of-Thought reasoning most closely matched human behavior, showing stronger group error reduction and partisan bias alignment"
  - [section] "Detailed personas enable a more human-like partisan bias. On the other hand, in the Social condition, CoT reasoning diminishes the reduction of group error"
  - [corpus] Weak - no direct corpus evidence on this specific mechanism
- Break condition: If persona details are too uniform or if Chain-of-Thought reasoning is added, the mechanism fails to replicate human-like dynamics

### Mechanism 2
- Claim: Chain-of-Thought reasoning interferes with partisan bias emergence
- Mechanism: CoT prompts agents to over-analyze and rationalize responses, reducing spontaneous partisan biases that naturally emerge from persona-based identity
- Core assumption: Spontaneous, persona-driven responses better capture human partisan biases than analytical, CoT-based reasoning
- Evidence anchors:
  - [abstract] "The use of chain-of-thought prompt... hurts the alignment" with human behavior
  - [section] "Chain-of-Thought (CoT) reasoning... may lead to stereotypes and biases" and "in the Social condition, CoT reasoning diminishes the reduction of group error"
  - [corpus] Weak - no direct corpus evidence on this specific mechanism
- Break condition: If CoT is removed or simplified, the mechanism may restore human-like bias alignment

### Mechanism 3
- Claim: Fine-tuning on human data improves alignment but causes overfitting
- Mechanism: Supervised learning on human responses adapts LLM behavior to match human decision patterns, but the model may memorize training examples rather than generalize
- Core assumption: Human data contains generalizable patterns of group behavior that can be learned by LLMs
- Evidence anchors:
  - [abstract] "Fine-tuning on human data improved alignment but caused overfitting, producing unrealistic extreme values"
  - [section] "Fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors"
  - [corpus] Weak - no direct corpus evidence on this specific mechanism
- Break condition: If fine-tuning dataset is too small or contains outliers, overfitting becomes severe and generalization fails

## Foundational Learning

- Concept: Wisdom of Partisan Crowds phenomenon
  - Why needed here: This is the specific human group dynamics effect the study aims to replicate with LLM agents
  - Quick check question: What is the key difference between wisdom of crowds and wisdom of partisan crowds?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The study tests whether CoT reasoning helps or hinders LLM agents in replicating human-like group dynamics
  - Quick check question: How does CoT reasoning differ from direct prompting in LLM agent behavior?

- Concept: Persona design for LLM agents
  - Why needed here: Different persona detail levels are tested to determine their impact on human-likeness of agent behavior
  - Quick check question: What elements should be included in detailed personas to create individual differences?

## Architecture Onboarding

- Component map: LLM agent → Persona embedding → Prompt engineering → Social network interaction → Response generation → Evaluation metrics
- Critical path: Persona design → Prompt formulation → Agent interaction → Response collection → Metric calculation
- Design tradeoffs: Detailed personas vs. computational efficiency; CoT reasoning vs. natural bias emergence; fine-tuning vs. overfitting risk
- Failure signatures: Extreme value generation; lack of partisan bias; poor group error reduction; overfitting to training data
- First 3 experiments:
  1. Test detailed personas without CoT reasoning to establish baseline human-like behavior
  2. Add CoT reasoning to detailed personas to measure degradation in alignment
  3. Fine-tune on human data and test both training and unseen questions to assess overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning LLM agents on human data consistently improve their ability to simulate the wisdom of partisan crowds across different types of factual questions?
- Basis in paper: [explicit] The paper shows fine-tuning improves alignment but also causes overfitting in the test set
- Why unresolved: The study only tested fine-tuning on two subsets of questions, showing mixed results between train and test sets
- What evidence would resolve it: Testing fine-tuning across a broader range of question types and political contexts would clarify if improvements generalize

### Open Question 2
- Question: How do different persona generation methods (e.g., AI-generated vs human-written) affect the realism and partisan bias of LLM agents in group simulations?
- Basis in paper: [inferred] The study used detailed human-written personas and showed they improved alignment, but didn't compare different persona generation methods
- Why unresolved: The study only tested one persona generation approach
- What evidence would resolve it: Comparing agent behavior using AI-generated, human-written, and hybrid personas across multiple political contexts

### Open Question 3
- Question: What is the optimal balance between Chain-of-Thought reasoning and prompt-based instructions for achieving human-like collective intelligence in LLM agents?
- Basis in paper: [explicit] The paper found that CoT reasoning hurt alignment with human behavior
- Why unresolved: The study only tested binary presence/absence of CoT, not intermediate levels or different CoT strategies
- What evidence would resolve it: Systematic testing of different CoT prompt structures and their effects on group dynamics across various question types

## Limitations
- The study lacks direct corpus evidence supporting proposed mechanisms, particularly regarding how persona detail levels influence individual differences
- The experimental design relies on a fixed set of eight partisan-biased questions, limiting generalizability across domains
- The mechanism explanations rely heavily on interpretation of results rather than direct causal evidence

## Confidence
- Medium: The alignment between LLM agents with detailed personas and human behavior is supported by multiple experimental conditions and metrics
- Medium: The overfitting issue during fine-tuning is well-documented but requires external validation
- Low: The absence of direct causal evidence for proposed mechanisms reduces confidence in explanations

## Next Checks
1. **Mechanism validation through ablation studies**: Systematically test the contribution of individual persona elements to determine which specific components drive the observed human-like behavior.

2. **Cross-domain generalization testing**: Evaluate the same agent configurations on non-partisan factual questions and different domains to assess whether the wisdom of partisan crowds phenomenon extends beyond politically charged topics.

3. **Longitudinal stability assessment**: Conduct experiments with extended interaction rounds beyond three iterations to determine whether observed group error reduction and bias alignment remain stable over time.