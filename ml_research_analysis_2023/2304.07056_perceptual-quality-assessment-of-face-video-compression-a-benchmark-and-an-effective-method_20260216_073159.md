---
ver: rpa2
title: 'Perceptual Quality Assessment of Face Video Compression: A Benchmark and An
  Effective Method'
arxiv_id: '2304.07056'
source_url: https://arxiv.org/abs/2304.07056
tags:
- quality
- video
- face
- videos
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the perceptual quality assessment of compressed
  face videos, which is critical for modern face video compression, transmission,
  and storage systems. The authors create the first large-scale Compressed Face Video
  Quality Assessment (CFVQA) dataset, containing 3,240 compressed face video clips
  derived from 135 source videos using six representative video codecs.
---

# Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method

## Quick Facts
- arXiv ID: 2304.07056
- Source URL: https://arxiv.org/abs/2304.07056
- Authors: Yixuan Lu, Jinjia Peng, Kaixuan Huang, Qiong Wang, Hanli Wang
- Reference count: 40
- Key outcome: Proposed FAVOR method achieves PLCC 0.9229 and SRCC 0.9060 on CFVQA dataset

## Executive Summary
This paper addresses the critical need for perceptual quality assessment in face video compression systems. The authors create the first large-scale CFVQA dataset containing 3,240 compressed face video clips derived from 135 source videos using six representative codecs. They propose FAVOR (Face Video Integrity), a full-reference quality evaluation framework that incorporates face-specific features and temporal memory effects. FAVOR outperforms existing quality assessment methods, achieving superior correlation with human perception on the new benchmark dataset.

## Method Summary
The authors propose FAVOR, a full-reference quality assessment method for compressed face videos. The method extracts multi-level spatial features from a ResNet50 backbone pre-trained on face data, then computes quality scores using mean and standard deviation of feature maps. A memory-inspired temporal aggregation module models human perceptual memory effects by weighting recent frame quality more heavily than distant frames. The framework combines face-specific spatial features with temporal priors to achieve superior performance on the CFVQA dataset.

## Key Results
- FAVOR achieves PLCC 0.9229 and SRCC 0.9060 on CFVQA dataset
- Outperforms VMAF (PLCC 0.9211, SRCC 0.8984) and STRRED (PLCC 0.9117, SRCC 0.8902)
- Demonstrates effectiveness across traditional codecs, end-to-end compression, and generative methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Face-specific features from ResNet50 improve quality assessment by aligning with human visual system sensitivity to face distortions
- Core assumption: Face recognition features better represent human perception of face video quality than generic features
- Evidence anchors: [abstract] "considering the distinct content characteristics and temporal priors of face videos", [section] "mean μ and std δ of deep feature maps are highly correlated with image texture and structure"
- Break condition: If pre-trained face recognition model doesn't generalize to target video types

### Mechanism 2
- Claim: Memory-inspired aggregation models human perceptual memory effects for temporal quality assessment
- Core assumption: Human perception is influenced by recent quality fluctuations, not just current frame
- Evidence anchors: [section] "poor quality in past frames causes negative effects on current frame quality", [section] "design philosophy meets face prior and temporal prior"
- Break condition: If viewers don't exhibit hypothesized memory effects or window size is suboptimal

### Mechanism 3
- Claim: Multi-level spatial feature fusion captures quality degradation patterns at different semantic levels
- Core assumption: Different artifacts manifest at different semantic levels, combining provides better assessment
- Evidence anchors: [section] "ResNet50 endows extracted features with high relevance with HVS mechanism", [section] "main components are face-prior-based quality predictor and memory-prior-based quality aggregator"
- Break condition: If certain compression artifacts aren't captured by any feature level or weighting is suboptimal

## Foundational Learning

- Concept: Perceptual quality assessment in video compression
  - Why needed: Understanding difference between objective metrics and subjective human perception is crucial for developing effective quality assessment methods
  - Quick check: Why do traditional metrics like PSNR often poorly correlate with human perception of video quality?

- Concept: Temporal quality aggregation strategies
  - Why needed: Video quality is inherently temporal, understanding aggregation methods is essential for VQA
  - Quick check: What are key differences between average pooling, hysteresis, and memory-based temporal aggregation methods?

- Concept: Face-specific quality considerations
  - Why needed: Faces have unique perceptual properties that differ from general video content
  - Quick check: How might face quality assessment differ from general video quality assessment in terms of noticeable distortions?

## Architecture Onboarding

- Component map: Resized frames → ResNet50 feature extraction → Frame-level quality calculation → Temporal aggregation → Final quality score
- Critical path: Resized frames → ResNet50 feature extraction → Frame-level quality calculation → Temporal aggregation → Final quality score
- Design tradeoffs:
  - Complexity vs. accuracy: Multiple feature levels and temporal memory increase complexity but improve accuracy
  - Generalizability vs. specialization: Face-specific features work well for face videos but may not generalize
  - Window size vs. memory effects: Memory period length affects how temporal quality fluctuations are weighted
- Failure signatures:
  - Poor performance on non-face content
  - Over-sensitivity to certain artifacts (e.g., geometric distortions from generative models)
  - Suboptimal performance when viewing conditions differ from test conditions
- First 3 experiments:
  1. Ablation study: Remove temporal aggregation and compare performance to validate memory effects contribution
  2. Cross-dataset validation: Test on general video quality dataset to assess generalizability
  3. Window size sensitivity: Vary immediate memory period length to find optimal temporal aggregation parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are specific neural network architectures and training protocols that would best capture face-specific quality distortions?
- Basis: Authors mention current objective measures focus on low-level mismatches and perform poorly on GAN-related distortions
- Why unresolved: Paper demonstrates inadequacy of current methods but doesn't explore specific architectural improvements
- What evidence would resolve it: Comparative evaluation of multiple neural network architectures trained on face-specific features versus general video features

### Open Question 2
- Question: How does FAVOR's temporal aggregation compare to alternative temporal modeling approaches?
- Basis: Authors propose memory-inspired aggregation but only compare to limited alternatives
- Why unresolved: Paper demonstrates superiority over basic methods but doesn't benchmark against advanced temporal modeling techniques
- What evidence would resolve it: Systematic comparison with state-of-the-art temporal modeling approaches on CFVQA dataset

### Open Question 3
- Question: What is optimal balance between spatial face features and temporal motion information?
- Basis: Authors incorporate both spatial and temporal components but don't explore relative importance
- Why unresolved: Paper uses fixed weighting scheme without investigating how balance affects performance
- What evidence would resolve it: Ablation studies varying spatial vs. temporal contributions and testing on subsets with different distortion types

## Limitations

- Requires full-reference quality assessment, limiting applicability when original reference videos are unavailable
- Memory window size may vary across different viewing conditions and content types
- Face-specific features may not generalize well to diverse facial characteristics across demographics

## Confidence

- **High Confidence**: Superior performance metrics (PLCC 0.9229, SRCC 0.9060) compared to existing methods on CFVQA dataset
- **Medium Confidence**: Generalizability to other face video datasets and real-world applications
- **Medium Confidence**: Optimal configuration of memory-inspired aggregation module for different viewing scenarios

## Next Checks

1. Cross-Dataset Validation: Evaluate FAVOR on diverse face video datasets (e.g., FaceQNet, KoNViD-1k) to assess generalizability beyond CFVQA

2. Cross-Cultural Validation: Test model performance across face videos featuring diverse demographics to identify potential bias in face-specific feature extraction

3. Real-World Deployment Study: Implement FAVOR in streaming platform to evaluate practical utility and computational efficiency in real-time quality assessment scenarios