---
ver: rpa2
title: 'FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated
  Learning'
arxiv_id: '2307.13214'
source_url: https://arxiv.org/abs/2307.13214
tags:
- multimodal
- data
- knowledge
- learning
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in existing multimodal federated
  learning (FL) systems, which typically rely on single-modal data and labeled data
  at the client side. The authors propose FedMEKT, a novel semi-supervised learning-based
  multimodal embedding knowledge transfer mechanism.
---

# FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning

## Quick Facts
- arXiv ID: 2307.13214
- Source URL: https://arxiv.org/abs/2307.13214
- Reference count: 40
- Primary result: Proposed framework achieves superior global encoder performance on linear evaluation while preserving privacy and reducing communication costs

## Executive Summary
FedMEKT addresses key limitations in multimodal federated learning by introducing a novel embedding knowledge transfer mechanism that operates in semi-supervised settings. Unlike traditional approaches that average model parameters across clients, FedMEKT uses a small proxy dataset to extract and exchange joint embedding knowledge, enabling more effective multimodal feature fusion while preserving privacy. The framework iteratively updates global encoders through knowledge distillation from participating clients, achieving superior performance on downstream tasks without requiring labeled data at the client side.

## Method Summary
FedMEKT is a semi-supervised multimodal federated learning framework that employs knowledge distillation for embedding transfer. The method consists of three main phases: local multimodal autoencoder learning at clients using unlabeled data, generalized multimodal autoencoder construction at the server using joint embedding knowledge from the proxy dataset, and generalized classifier learning for downstream tasks. Clients update their local models using reconstruction loss and embedding distillation loss against the global model, then send back joint embeddings rather than parameters. The server aggregates these embeddings to construct the global model and trains a linear classifier for evaluation.

## Key Results
- Achieves superior global encoder performance on linear evaluation compared to baselines (MM-FedAvg, MM-FedProx, MM-MOON, CreamFL)
- Preserves user privacy by exchanging joint embeddings instead of model parameters
- Reduces communication costs through knowledge distillation rather than parameter averaging
- Demonstrates effectiveness across three multimodal human activity recognition datasets (mHealth, UR Fall Detection, Opportunity)

## Why This Works (Mechanism)

### Mechanism 1
Joint embedding knowledge transfer enables more effective multimodal feature fusion than parameter averaging. Instead of averaging encoder parameters across clients, FedMEKT aggregates embeddings generated from each modality on a small proxy dataset. The fusion layer concatenates modality-specific embeddings into joint representations, which are then distilled into the global model. This preserves modality-specific nuances while capturing cross-modal correlations.

### Mechanism 2
Semi-supervised setting with unlabeled client data and proxy-based distillation reduces dependency on labeled data. Clients update their local autoencoders using reconstruction loss on private unlabeled data plus an embedding knowledge distillation loss against the global model's embeddings. This allows learning useful representations without requiring clients to annotate data.

### Mechanism 3
Downstream classifier trained on global embeddings generalizes better than classifiers trained on local parameters. After updating the global autoencoder via joint embedding distillation, the server extracts embeddings from the labeled dataset and trains a linear classifier. Since the global encoder captures cross-modal correlations learned from all clients, its representations are more discriminative.

## Foundational Learning

- **Concept: Knowledge distillation in federated learning**
  - Why needed here: FedMEKT replaces parameter averaging with embedding distillation to preserve privacy and reduce communication
  - Quick check question: How does embedding distillation differ from parameter averaging in terms of privacy and communication overhead?

- **Concept: Multimodal fusion strategies**
  - Why needed here: The fusion layer must combine embeddings from different modalities without losing modality-specific information
  - Quick check question: What are the tradeoffs between early fusion, late fusion, and joint embedding fusion in multimodal learning?

- **Concept: Semi-supervised representation learning**
  - Why needed here: Clients only have unlabeled data, so the model must learn useful representations without labels
  - Quick check question: How can reconstruction loss and knowledge distillation work together to learn from unlabeled data?

## Architecture Onboarding

- **Component map**: Local clients (autoencoders, fusion layer, distillation loss) -> Proxy dataset (knowledge exchange) -> Server (global autoencoder construction, classifier training)
- **Critical path**: Server broadcasts global embeddings -> Clients update autoencoders using private data + proxy distillation -> Clients send back joint embeddings -> Server aggregates embeddings and updates global autoencoder -> Server trains classifier on labeled data
- **Design tradeoffs**: Proxy dataset size vs. communication efficiency; Embedding dimensionality vs. model capacity; Knowledge transfer frequency vs. convergence speed
- **Failure signatures**: Poor downstream accuracy (proxy dataset may be non-representative); Slow convergence (knowledge transfer steps too infrequent); Mode collapse (autoencoders fail to reconstruct diverse inputs)
- **First 3 experiments**: 1) Baseline test with single modality to verify autoencoder reconstruction; 2) Proxy ablation with varying proxy dataset sizes; 3) Knowledge transfer step tuning by sweeping R (number of EKT steps)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations and experimental scope, several key questions remain regarding scalability, privacy guarantees, and real-world deployment scenarios.

## Limitations
- The proxy dataset curation process lacks detailed validation and may not be representative across heterogeneous client data
- Semi-supervised contribution is not rigorously evaluated through ablation studies comparing labeled vs. unlabeled data usage
- Communication efficiency claims are not supported by explicit bandwidth usage comparisons with parameter averaging baselines

## Confidence
- **High**: Framework's ability to preserve privacy through embedding transfer instead of parameter sharing
- **Medium**: Performance improvements over baselines in linear evaluation metrics
- **Low**: Claims about communication efficiency benefits without explicit comparison of bandwidth usage

## Next Checks
1. **Proxy Dataset Sensitivity**: Systematically vary proxy dataset size and composition to measure impact on embedding quality and downstream performance, identifying minimum viable proxy requirements
2. **Communication Cost Analysis**: Measure actual bandwidth consumption of FedMEKT versus parameter averaging baselines, accounting for embedding dimensionality and transfer frequency
3. **Label Efficiency Test**: Compare FedMEKT performance when varying the ratio of labeled proxy data to unlabeled client data to quantify the semi-supervised contribution