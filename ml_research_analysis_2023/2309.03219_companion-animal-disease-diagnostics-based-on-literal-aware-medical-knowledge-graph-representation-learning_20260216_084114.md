---
ver: rpa2
title: Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge
  Graph Representation Learning
arxiv_id: '2309.03219'
source_url: https://arxiv.org/abs/2309.03219
tags:
- graph
- entities
- could
- representations
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LiteralKG, a knowledge graph embedding model
  that learns various types of literal information (numeric and textual) and graph
  structure for companion animal disease diagnostics. The model fuses entities and
  attributes using gate networks and employs attentive embedding propagation layers
  to capture complex relations.
---

# Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning

## Quick Facts
- arXiv ID: 2309.03219
- Source URL: https://arxiv.org/abs/2309.03219
- Reference count: 40
- Key outcome: LiteralKG achieves 86.16% accuracy, 93.57% recall, and 87.12% F1 score on link prediction for companion animal disease diagnostics, outperforming baselines.

## Executive Summary
This paper proposes LiteralKG, a knowledge graph embedding model that incorporates literal information (numeric and textual attributes) with graph structure for companion animal disease diagnostics. The model uses gate networks to fuse entities and attributes, attentive embedding propagation layers to capture complex relations, and self-supervised pre-training to learn representations without labeled data. Experiments on a medical knowledge graph built from 85,965 EMRs show significant improvements over baseline models including TransE, TransR, SMR, KGNN, KGNMDA, and LaGAT.

## Method Summary
LiteralKG combines knowledge graph embedding with literal information through a three-component architecture. First, a gate function fuses entity embeddings with numerical and textual attributes into unified vectors. Second, attentive embedding propagation layers with different GNN aggregators (GCN, GraphSAGE, Bi-Interaction, GIN) capture complex relations through attention-weighted neighbor aggregation. Third, self-supervised pre-training using triplet loss learns multi-relational structures without labeled data, followed by fine-tuning for specific tasks like link prediction. The model is evaluated on a knowledge graph constructed from 85,965 EMRs containing 595,172 entities and 16 relation types.

## Key Results
- LiteralKG achieves 86.16% accuracy, 93.57% recall, and 87.12% F1 score on link prediction tasks
- Outperforms baselines including TransE, TransR, SMR, KGNN, KGNMDA, and LaGAT
- Ablation studies show pre-training improves performance by 0.7-1.2% across metrics
- Sensitivity analysis identifies optimal number of GNN layers varies by aggregator type

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LiteralKG improves performance by fusing literal information (numerical and textual) with graph structure through a gating mechanism.
- Mechanism: The gate function g(ei, ni, ti) combines entity embeddings, numerical attributes, and textual attributes into unified vectors, allowing the model to capture both structural and literal information simultaneously.
- Core assumption: Different types of literal information carry complementary information that improves representation learning when fused appropriately.
- Evidence anchors:
  - [abstract] "We then fuse different types of entities and node feature information into unified vector representations through gate networks."
  - [section 4.1] "We first design a fusion layer that contains a gate function to transform different types of attributes into unified vectors."
  - [corpus] Weak evidence - no direct corpus support for gating mechanisms in knowledge graph embeddings.
- Break condition: If the gating mechanism fails to properly weight different literal types, or if literal information is noisy/incorrect, the unified representations may be corrupted.

### Mechanism 2
- Claim: Attentive embedding propagation layers with different GNN aggregators capture complex relations in heterogeneous knowledge graphs.
- Mechanism: The model uses attention scores π(h, r, t) to weight contributions from neighboring nodes with different relation types, then aggregates these weighted features using various GNN aggregators (GCN, GraphSAGE, Bi-Interaction, GIN).
- Core assumption: Different GNN aggregators capture different aspects of graph structure, and attention allows the model to focus on relevant neighbor information.
- Evidence anchors:
  - [abstract] "employs attentive embedding propagation layers to capture complex relations"
  - [section 4.2] "Since neighbours with different relation types could contribute differently to a target entity, we aim to aggregate features of neighbouring nodes with attentional weights"
  - [corpus] Weak evidence - no direct corpus support for attentive propagation in medical knowledge graphs.
- Break condition: If attention weights are poorly learned, or if aggregators are not suited to the heterogeneous nature of the medical KG, the model may fail to capture meaningful patterns.

### Mechanism 3
- Claim: Self-supervised pre-training on pretext tasks generates effective representations that benefit downstream tasks.
- Mechanism: The model is pre-trained using triplet loss to preserve relations between entities without labeled data, then fine-tuned for specific tasks like link prediction.
- Core assumption: Pre-training can learn meaningful representations of entity relationships that transfer to downstream tasks.
- Evidence anchors:
  - [abstract] "A self-supervised learning task pre-trains the model on pretext tasks without labeled data"
  - [section 4.3] "We now represent our pre-training task to learn multi-relational structures between different types of entities in KG"
  - [section 5.3.3] "Table 5 shows the link prediction performance on the pre-training phase evaluation"
- Break condition: If pre-training objectives don't align with downstream task requirements, or if pre-trained representations don't generalize well, performance may not improve.

## Foundational Learning

- Concept: Knowledge Graph Embeddings
  - Why needed here: The entire approach relies on learning vector representations of entities and relations that preserve graph structure and can be used for prediction tasks.
  - Quick check question: What's the difference between translational distance models (like TransE) and neural network-based models for KG embeddings?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: The model uses GNN aggregators to learn local and global structural features from the knowledge graph.
  - Quick check question: How do GCN, GraphSAGE, and GIN aggregators differ in how they aggregate neighbor information?

- Concept: Self-Supervised Learning
  - Why needed here: The pre-training phase uses self-supervised objectives to learn representations without labeled data.
  - Quick check question: What are the advantages of self-supervised pre-training versus supervised learning when labeled data is scarce?

## Architecture Onboarding

- Component map: Input layer (Entities and literal attributes) -> Fusion layer (Gate function) -> Attentive embedding propagation layers (GNN aggregators) -> Residual connections -> Pre-training phase (Self-supervised) -> Fine-tuning phase (Supervised)

- Critical path: Fusion layer → Attentive embedding propagation → Residual connections → Pre-training → Fine-tuning

- Design tradeoffs:
  - Gating vs. concatenation: Gating allows weighted combination of literal types but adds complexity
  - Attention vs. uniform weighting: Attention can focus on relevant neighbors but requires learning attention weights
  - Pre-training vs. direct fine-tuning: Pre-training may improve generalization but adds computational cost

- Failure signatures:
  - Poor performance on link prediction: Could indicate issues with attention weights, aggregator choice, or pre-training
  - Overfitting on fine-tuning: May suggest insufficient regularization or too much capacity
  - Degradation with deeper layers: Could indicate over-smoothing without proper residual connections

- First 3 experiments:
  1. Compare LiteralKG with and without the gating mechanism to validate the importance of literal fusion
  2. Test different GNN aggregators (GCN, GraphSAGE, Bi-Interaction, GIN) to find the best choice for this dataset
  3. Evaluate the impact of pre-training by comparing models trained from scratch vs. pre-trained models on link prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of GNN aggregators (GCN, GraphSAGE, Bi-Interaction, GIN) compare in their ability to capture complex relations in knowledge graphs with heterogeneous properties?
- Basis in paper: [explicit] The paper explicitly investigates the performance of four GNN aggregators on the LiteralKG model and discusses their individual characteristics and contributions to capturing complex relations in knowledge graphs.
- Why unresolved: The paper provides experimental results comparing the performance of different GNN aggregators, but it does not provide a definitive answer on which aggregator is the best for all scenarios. The effectiveness of each aggregator may depend on the specific characteristics of the knowledge graph and the task at hand.
- What evidence would resolve it: Further experiments on a wider range of knowledge graphs with different characteristics and tasks could provide more insights into the strengths and weaknesses of each GNN aggregator.

### Open Question 2
- Question: What is the optimal number of GNN layers to use in the LiteralKG model for different types of aggregators?
- Basis in paper: [explicit] The paper conducts sensitivity analysis to evaluate the performance of LiteralKG with different numbers of GNN layers for each aggregator type.
- Why unresolved: The paper shows that the optimal number of layers varies depending on the aggregator type, but it does not provide a general rule for determining the optimal number of layers for any given aggregator or knowledge graph.
- What evidence would resolve it: Additional experiments on a larger and more diverse set of knowledge graphs could help identify patterns and guidelines for selecting the optimal number of GNN layers for different scenarios.

### Open Question 3
- Question: How does the inclusion of literal information (numeric and textual) impact the performance of knowledge graph embedding models for companion animal disease diagnostics?
- Basis in paper: [explicit] The paper explicitly compares the performance of LiteralKG with and without literal information and discusses the contributions of different types of literal information to the overall performance.
- Why unresolved: While the paper shows that literal information improves the performance of LiteralKG, it does not provide a comprehensive understanding of how different types of literal information interact with each other and with the graph structure to influence the final results.
- What evidence would resolve it: Further experiments that systematically vary the types and amounts of literal information included in the model could provide insights into the relative importance of different types of literal information and their interactions with the graph structure.

## Limitations

- The dataset comes from a single commercial source (IntoCNS), which may limit generalizability to other companion animal populations.
- The comparison only includes one baseline (KGNN-L) for pre-training analysis, making it difficult to assess whether gains are due to the specific self-supervised approach or pre-training in general.
- The model's complexity with multiple GNN aggregators and attention mechanisms may create computational overhead without proportional benefits for smaller datasets.

## Confidence

- High confidence: The experimental results showing LiteralKG outperforming baseline models on link prediction metrics (86.16% accuracy, 93.57% recall, 87.12% F1) are well-supported by the ablation studies and comparison with established baselines.
- Medium confidence: The effectiveness of the gating mechanism for literal fusion is supported by ablation results, but the specific design choices (e.g., gate function parameters) lack detailed justification or sensitivity analysis.
- Medium confidence: The claim that attentive embedding propagation layers capture complex relations is supported by performance gains, but the paper doesn't provide qualitative analysis of what the model learns or how attention weights behave.

## Next Checks

1. **Dataset generalization test**: Apply LiteralKG to a companion animal knowledge graph from a different source or region to validate whether the performance gains transfer beyond the IntoCNS dataset.

2. **Ablation of pre-training objectives**: Compare LiteralKG's self-supervised pre-training against other self-supervised objectives (e.g., contrastive learning, masked attribute prediction) to isolate the contribution of the specific triplet loss formulation.

3. **Attention mechanism analysis**: Visualize and analyze the learned attention weights across different relation types and GNN layers to understand which neighbor relationships the model finds most informative for companion animal disease diagnostics.