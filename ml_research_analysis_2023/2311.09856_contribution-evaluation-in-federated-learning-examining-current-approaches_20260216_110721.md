---
ver: rpa2
title: 'Contribution Evaluation in Federated Learning: Examining Current Approaches'
arxiv_id: '2311.09856'
source_url: https://arxiv.org/abs/2311.09856
tags:
- data
- participants
- federated
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks contribution evaluation (CE) methods in\
  \ federated learning by measuring how well they rank clients based on the quality\
  \ of their data. The authors test six methods\u2014OR-SV, \u03BB-MR, FedShapley,\
  \ round-level LOO, Reputation, and OR-LC\u2014on MNIST and CIFAR-10 datasets with\
  \ varying numbers of clients and injected label noise."
---

# Contribution Evaluation in Federated Learning: Examining Current Approaches

## Quick Facts
- arXiv ID: 2311.09856
- Source URL: https://arxiv.org/abs/2311.09856
- Reference count: 40
- This paper benchmarks contribution evaluation (CE) methods in federated learning by measuring how well they rank clients based on the quality of their data.

## Executive Summary
This paper provides a comprehensive benchmark of contribution evaluation methods in federated learning, comparing six different approaches on MNIST and CIFAR-10 datasets. The authors systematically evaluate how well each method can differentiate between clients with varying data quality levels, introducing controlled label noise to simulate real-world data quality differences. The study reveals significant variations in how different CE methods handle contribution differentiation, with OR-SV and OR-LC showing sharp penalties for low-quality data while Reputation produces near-uniform distributions regardless of contribution quality.

## Method Summary
The paper benchmarks six contribution evaluation methods (OR-SV, λ-MR, FedShapley, round-level LOO, Reputation, and OR-LC) using MNIST and CIFAR-10 datasets with varying numbers of clients (2-10) and injected label noise. The methods are evaluated based on their ability to rank clients according to data quality, with experiments measuring normalized contribution scores, global model accuracy, maximum payoff differences, Euclidean distance from uniform distributions, and computation time. The study uses a two-layer MLP for MNIST and a ResNet-9 variant for CIFAR-10, running experiments with 5 different seeds for consistency.

## Key Results
- OR-SV and OR-LC sharply drop rewards when data quality degrades
- Reputation produces near-uniform splits regardless of contribution quality
- λ-MR and FedShapley offer smooth transitions in rewards across contribution quality levels
- Methods remain effective on CIFAR-10, though differences shrink under more challenging conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OR-SV and OR-LC sharply penalize low-quality contributions while still rewarding moderate-quality ones.
- Mechanism: The pseudo-model approximation reconstructs coalition values by combining marginal contributions from each round, then computes Shapley values from these reconstructed models. This allows exact Shapley evaluation without retraining all coalitions.
- Core assumption: The pseudo-model approximation accurately captures coalition performance differences across noise levels.
- Evidence anchors:
  - [abstract] "OR-SV and OR-LC sharply drop rewards when data quality degrades"
  - [section] "using the pseudo-model approximation from [14] for OR-Shapley can also be used to efficiently find the Least Core"
  - [corpus] Weak - no direct citation found, but related papers exist on Shapley approximations
- Break condition: If the pseudo-model approximation fails to distinguish between coalition performances, the method will lose its ability to differentiate contribution quality.

### Mechanism 2
- Claim: Reputation produces near-uniform splits regardless of contribution quality.
- Mechanism: Reputation calculates the average of the Heaviside function applied to per-round LOO values, creating a coarse-grained metric that only distinguishes between positive and zero contributions.
- Core assumption: The Heaviside function on LOO values provides sufficient granularity to differentiate contribution quality.
- Evidence anchors:
  - [abstract] "Reputation produces near-uniform splits"
  - [section] "The Reputation metric is the average of the Heaviside function applied to the LOO"
  - [corpus] Weak - no direct citation found, but reputation-based methods are common in FL literature
- Break condition: If participants have varying but consistently positive contributions, Reputation will treat them identically, failing to reward higher-quality contributions appropriately.

### Mechanism 3
- Claim: λ-MR and FedShapley offer smooth transitions in rewards across contribution quality levels.
- Mechanism: Both methods compute temporal snapshots of contribution value using average marginal contributions, with λ-MR adding time decay to emphasize earlier contributions.
- Core assumption: Temporal averaging of marginal contributions provides stable estimates of contribution value across rounds.
- Evidence anchors:
  - [abstract] "λ-MR and FedShapley offer smooth transitions"
  - [section] "svt(i) = 1/It X S⊆It\{i} 1/|It−1| |S| [v(I1:t−1 + (S ∪ {i})) − v(I1:t−1 + S)]"
  - [corpus] Moderate - related papers exist on temporal Shapley value methods in FL
- Break condition: If contribution quality changes rapidly between rounds, temporal averaging may smooth out important variations, leading to inappropriate reward allocations.

## Foundational Learning

- Concept: Coalitional Game Theory (CGT)
  - Why needed here: The entire contribution evaluation framework is built on CGT concepts like characteristic functions, payoff vectors, and solution concepts.
  - Quick check question: What is the difference between the Core and the Shapley Value in CGT?

- Concept: Shapley Value and its axioms
  - Why needed here: Most contribution evaluation methods are based on or related to the Shapley Value, making understanding its properties essential.
  - Quick check question: Which four axioms must a payoff vector satisfy to be considered the Shapley Value?

- Concept: Pseudo-model approximation technique
  - Why needed here: This technique enables efficient computation of Shapley values without retraining all possible coalitions, which is crucial for practical FL systems.
  - Quick check question: How does the pseudo-model approximation reconstruct coalition performance values from marginal contributions?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Model training orchestrator -> Contribution evaluation engine -> Result aggregation and comparison module -> Visualization and analysis tools

- Critical path:
  1. Load and preprocess datasets (MNIST/CIFAR-10)
  2. Inject label noise according to experimental design
  3. Train global model across federated rounds
  4. Compute contribution scores using each CE method
  5. Normalize and compare results
  6. Generate visualizations and performance metrics

- Design tradeoffs:
  - Memory vs. accuracy: Multi-round methods require storing pseudo-models for all rounds, increasing memory usage but potentially improving accuracy
  - Computational cost vs. fairness: Exact Shapley computation is intractable, necessitating approximations that trade some fairness guarantees for efficiency
  - Granularity vs. stability: Finer-grained metrics like LOO provide more differentiation but may be noisier than coarser metrics like Reputation

- Failure signatures:
  - If all CE methods produce nearly identical rankings, check for issues with noise injection or model training
  - If computational costs explode exponentially with participant count, verify that approximation methods are being used correctly
  - If certain methods produce uniform distributions regardless of input quality, examine the implementation of the underlying metric (e.g., Heaviside function in Reputation)

- First 3 experiments:
  1. Run MNIST experiments with 2 participants at 0% and 50% noise to verify basic differentiation capability
  2. Increase to 4 participants with linearly spaced noise levels to test scaling behavior
  3. Switch to CIFAR-10 with the same participant and noise configuration to test robustness under harder conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design contribution evaluation methods that reward early contributions more than later ones, contrary to the Shapley Value's symmetry axiom?
- Basis in paper: Explicit
- Why unresolved: The paper notes that the Shapley Value's symmetry axiom is problematic in federated learning settings where clients join at different times. While the paper mentions this as a limitation, it does not provide a concrete solution or method to address this issue.
- What evidence would resolve it: Developing and testing a new contribution evaluation method that explicitly rewards early contributors more than late joiners, while maintaining fairness and efficiency.

### Open Question 2
- Question: What is the optimal balance between utility and valuation accuracy in contribution evaluation methods for federated learning?
- Basis in paper: Explicit
- Why unresolved: The paper highlights the challenge of balancing utility and valuation accuracy in CE methods, noting that methods that burden clients with calculations or require more data become less compatible with the overall FL pipeline. However, it does not provide a quantitative framework or method to determine the optimal balance.
- What evidence would resolve it: Conducting extensive experiments with various CE methods to measure their impact on both utility (model performance) and valuation accuracy, and developing a framework to determine the optimal trade-off.

### Open Question 3
- Question: How can we effectively detect and prevent free-riding attacks in federated learning without compromising privacy or model performance?
- Basis in paper: Explicit
- Why unresolved: The paper mentions free-riding as an understudied problem in federated learning and suggests that rewarding attackers with a null payoff is insufficient. It also discusses the potential of a model-as-reward paradigm to deter free-riders, but does not provide a concrete solution or evaluation of its effectiveness.
- What evidence would resolve it: Developing and evaluating a robust free-rider detection mechanism that can be integrated into federated learning systems, and testing its effectiveness in preventing free-riding attacks while maintaining privacy and model performance.

## Limitations

- The evaluation relies on simulated label noise as a proxy for data quality, which may not fully capture real-world data quality variations
- The pseudo-model approximation technique's effectiveness depends on assumptions that lack empirical validation in the paper
- Computational complexity of multi-round methods on CIFAR-10 suggests potential scalability issues not fully addressed

## Confidence

- High Confidence: The comparative rankings of CE methods under controlled noise conditions
- Medium Confidence: The general behavior patterns observed (smooth transitions in λ-MR and FedShapley)
- Low Confidence: The direct applicability of these findings to production FL systems with diverse real-world data quality issues

## Next Checks

1. **Ablation Study**: Run experiments removing the pseudo-model approximation to quantify its impact on OR-SV and OR-LC performance
2. **Real Data Quality**: Replace synthetic noise with actual low-quality data (e.g., from less curated sources) to test method robustness
3. **Scalability Test**: Increase participant count beyond 10 and measure computational overhead for each method to identify practical limits