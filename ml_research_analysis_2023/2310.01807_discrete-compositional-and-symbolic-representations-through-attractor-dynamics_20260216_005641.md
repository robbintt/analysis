---
ver: rpa2
title: Discrete, compositional, and symbolic representations through attractor dynamics
arxiv_id: '2310.01807'
source_url: https://arxiv.org/abs/2310.01807
tags:
- dynamics
- attractor
- learning
- symbolic
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural stochastic dynamical systems model
  that uses attractor dynamics to discretize continuous representational space into
  basins corresponding to symbolic sequences. The model learns to map high-dimensional
  sensory inputs to compositional symbolic representations through unsupervised learning,
  rather than relying on predefined primitives.
---

# Discrete, compositional, and symbolic representations through attractor dynamics

## Quick Facts
- arXiv ID: 2310.01807
- Source URL: https://arxiv.org/abs/2310.01807
- Reference count: 35
- One-line primary result: A neural stochastic dynamical system model that learns discrete, compositional, and symbolic representations through attractor dynamics without predefined primitives

## Executive Summary
This paper proposes a neural stochastic dynamical systems model that uses attractor dynamics to discretize continuous representational space into basins corresponding to symbolic sequences. The model learns to map high-dimensional sensory inputs to compositional symbolic representations through unsupervised learning, rather than relying on predefined primitives. Using generative flow networks and an expectation-maximization algorithm, the model learns attractor points in continuous space that correspond to sequences of symbols, with trajectories from initial latent representations converging to these attractors. On the dSprites dataset, the model achieved high accuracy in linearly decoding position and RGB values from symbolic sequences (e.g., 98.1% accuracy for X position, 98.8% for Y position, and 91.9% for red value), demonstrating the emergence of compositional structure. The approach provides a neurally plausible substrate for implementing symbolic processing through attractor dynamics, offering a unified framework integrating symbolic and sub-symbolic processing.

## Method Summary
The model implements a neural stochastic dynamical system where continuous latent representations evolve through time until converging to attractor points that correspond to symbolic sequences. The approach uses a GFlowNet-EM training procedure that alternates between optimizing the dynamics model (E-step) and optimizing reward function components including a similarity measure, attractor embeddings, and prior (M-step). The model consists of an input encoder producing initial latent states, a dynamics model generating trajectories through conditional Gaussian transitions, and a discretizer mapping final attractor points to symbolic sequences. A conditional-VAE provides the similarity measure for reconstruction quality, with KL regularization encouraging reliance on symbolic rather than auxiliary information. The model learns compositional structure by maximizing mutual information between inputs and symbolic encodings without requiring predefined symbolic primitives.

## Key Results
- Achieved 98.1% accuracy for X position, 98.8% for Y position, and 91.9% for red value in linearly decoding from symbolic sequences on dSprites dataset
- Demonstrated emergence of compositional structure with variable-length sequences up to 5 tokens and vocabulary size of 7 tokens per position
- Successfully learned attractor points at centers of Gaussian distributions in synthetic mixture dataset with compositional syntax and semantics in token sequences

## Why This Works (Mechanism)

### Mechanism 1
Attractor dynamics discretizes continuous latent space into basins that correspond to symbolic sequences. The stochastic dynamics model learns a policy P(z_{t+1}|z_t, x) that, when initialized at z_0, converges to attractor points z_T. These attractors are positioned at centers of basins in the continuous space, with each basin corresponding to a symbolic sequence w through the discretizer d_θ(w|z_T, x). The dynamics create contractive trajectories that guide z_T to these attractor points.

Core assumption: The reward function R_φ(z_T, w; x) provides sufficient gradient signal to shape the attractor landscape appropriately, and the discretizer can learn to map z_T to w with high confidence when z_T is near an attractor.

### Mechanism 2
GFlowNet-EM algorithm enables learning compositional symbolic representations without predefined primitives. The GFlowNet-EM training alternates between E-step (optimizing dynamics model to sample trajectories proportionally to reward) and M-step (optimizing reward function components including similarity measure s_φ(x, z_w), attractor embeddings z_w, and prior P_φ(w)). This allows the model to discover emergent compositional structure by maximizing mutual information between inputs and symbolic encodings.

Core assumption: The unnormalized reward function can effectively guide the learning of attractor points and their symbolic correspondences through the EM procedure, even when the initial symbolic language has no grounded meaning.

### Mechanism 3
Information bottleneck emerges through trajectory convergence, decomposing rich sensory input into stable symbolic representations. The trajectory from z_0 (initial latent representation) to z_T (attractor point) represents progressive loss of ineffable information. The CVAE-based similarity measure s_φ(x, z) with KL regularization encourages the model to encode as much information as possible in the symbolic sequence w while minimizing reliance on the auxiliary variable ζ, effectively creating an information bottleneck.

Core assumption: The KL regularization term successfully encourages the model to rely primarily on the symbolic representation rather than the auxiliary information for reconstruction, leading to stable symbolic entities.

## Foundational Learning

- Concept: Attractor dynamics in neural networks
  - Why needed here: The core mechanism relies on understanding how attractor networks can partition continuous space into discrete basins, which is essential for implementing symbolic processing through neural dynamics
  - Quick check question: How do attractor networks create stable states in continuous space, and what determines the number and location of attractors?

- Concept: Generative Flow Networks (GFlowNets)
  - Why needed here: The training procedure uses GFlowNet-EM algorithm to learn the mapping between attractors and symbolic representations without predefined primitives, which is fundamental to the model's ability to discover compositional structure
  - Quick check question: How does the GFlowNet-EM algorithm differ from standard EM, and why is it particularly suited for learning distributions over trajectories?

- Concept: Information bottleneck principle
  - Why needed here: The model aims to create an information bottleneck through the trajectory convergence process, decomposing rich sensory input into stable symbolic representations, which is central to understanding how the model achieves compositionality
  - Quick check question: What is the information bottleneck principle, and how does it relate to the trade-off between compression and prediction in representation learning?

## Architecture Onboarding

- Component map: Input encoder → Initial latent representation z_0 → Trajectory (z_0 → ... → z_T) → Discretizer → Symbolic sequence w → Embedding function → Attractor point z_w
- Critical path: Input → z_0 → Trajectory (z_0 → ... → z_T) → w → z_w (attractor point)
- Design tradeoffs:
  - Joint vs. separate training of dynamics and discretizer (separate training with Ldisc improves efficiency)
  - Fixed vs. learned ε in N(z|z_w, ε²) distance measure (fixed used in experiments)
  - Input-dependent vs. marginalized dynamics (marginalized used post-hoc for independence)
  - Variable-length vs. fixed-length sequences (variable allowed in dSprites experiment)
- Failure signatures:
  - Modal collapse: All trajectories converge to same attractor (check distribution of sampled w)
  - Poor reconstruction: s_φ(x, z_w) remains low across all attractors (check reconstruction quality)
  - Disentanglement failure: Linear classifiers can't decode features from w (check Table 1-style metrics)
  - Training instability: GFlowNet objectives don't converge or oscillate (monitor training curves)
- First 3 experiments:
  1. Gaussians dataset with fixed z_0=x: Verify attractor formation at Gaussian centers with compositional token sequences
  2. dSprites with pre-trained z_0 and attractor initialization: Test emergent compositionality with variable-length sequences
  3. Ablation study: Remove dynamics (zero-step model) to establish baseline for attractor contribution to compositionality

## Open Questions the Paper Calls Out

### Open Question 1
Can the explicit discretizer in the model be relaxed or eliminated during training while maintaining compositional structure? The paper acknowledges this as a key limitation, stating "although the final resulting model successfully implements discretization from dynamics alone, the training method still relies on an explicit discretizer."

### Open Question 2
How well does the model generalize to datasets with more complex compositional structures than dSprites? The paper only demonstrates the model on dSprites and a simple Gaussians dataset, leaving open questions about performance on more complex compositional tasks.

### Open Question 3
What is the relationship between the number of attractor basins and the complexity of the learned compositional code? The paper mentions that in the dSprites experiment, sentences had up to 5 tokens with a vocabulary size of 7 tokens per position, but doesn't explore how varying these parameters affects the compositional structure.

## Limitations
- Evaluation focused on synthetic datasets (Gaussians and dSprites) without real-world data validation
- Limited qualitative analysis of learned symbolic sequences and their interpretability
- Claims about "unified framework" integration of symbolic and sub-symbolic processing are aspirational without downstream reasoning demonstrations

## Confidence
- **High**: The core mechanism of using attractor dynamics to discretize continuous space into basins is technically sound and well-supported by the synthetic experiments
- **Medium**: The claim that GFlowNet-EM enables learning compositional structure without predefined primitives is plausible but requires more extensive validation on diverse datasets
- **Low**: The assertion that this provides a "unified framework integrating symbolic and sub-symbolic processing" is aspirational; the current implementation focuses primarily on the representation learning aspect without demonstrating downstream symbolic reasoning capabilities

## Next Checks
1. **Compositional generalization test**: Hold out specific compositional combinations during training (e.g., certain position-color pairs in dSprites) and measure zero-shot performance to test whether the model has learned compositional rather than merely correlational structure.

2. **Scaling experiment**: Test the model on a more complex compositional dataset (e.g., CLEVR or similar) with increased vocabulary size and more intricate relationships to reveal whether the attractor-based discretization approach scales effectively.

3. **Interpretability analysis**: Visualize the attractor basins in latent space and examine the actual symbolic sequences produced for different inputs to validate that the learned representations have the intended compositional semantics and aren't simply memorizing input-output mappings.