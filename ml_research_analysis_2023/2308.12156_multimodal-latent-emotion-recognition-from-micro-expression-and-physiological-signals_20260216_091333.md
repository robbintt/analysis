---
ver: rpa2
title: Multimodal Latent Emotion Recognition from Micro-expression and Physiological
  Signals
arxiv_id: '2308.12156'
source_url: https://arxiv.org/abs/2308.12156
tags:
- recognition
- features
- emotion
- fusion
- micro-expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal learning framework that combines
  micro-expressions (ME) and physiological signals (PS) for improving latent emotion
  recognition accuracy. The framework includes a 1D separable and mixable depthwise
  inception network for extracting features from PS, a standardized normal distribution
  weighted feature fusion method for reconstructing informative maps from ME video
  frames, and depth/physiology guided attention modules for multimodal learning.
---

# Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals

## Quick Facts
- arXiv ID: 2308.12156
- Source URL: https://arxiv.org/abs/2308.12156
- Authors: 
- Reference count: 40
- Primary result: Multimodal framework combining micro-expressions and physiological signals improves latent emotion recognition accuracy on CAS(ME)3 dataset

## Executive Summary
This paper presents a multimodal learning framework that combines micro-expression (ME) and physiological signal (PS) data for latent emotion recognition. The framework introduces three key innovations: a 1D separable and mixable depthwise inception network for extracting features from physiological signals, a standardized normal distribution weighted feature fusion method for reconstructing informative maps from ME video frames, and depth/physiology guided attention modules for multimodal learning. Experimental results on the CAS(ME)3 dataset demonstrate that the proposed approach outperforms benchmark methods, with the weighted fusion method and guided attention modules both contributing to enhanced performance.

## Method Summary
The proposed framework consists of three main components working in parallel. First, a 1D separable and mixable depthwise inception network processes three physiological signals (EDA, ECG, PPG) through separate depthwise convolutions, allowing for precise feature extraction from each signal type. Second, micro-expression features are extracted from video frames using color and depth backbone networks, with a standardized normal distribution weighted fusion method that assigns higher weights to frames near the apex of micro-expressions. Third, a depth/physiology guided attention module fuses features from multiple modalities using a multi-head attention mechanism that allows different parts of the sequence to be attended to differently, capturing both long-term and short-term dependencies. The framework uses leave-one-subject-out cross-validation on the CAS(ME)3 dataset with accuracy, unweighted F1-score, and unweighted average recall as evaluation metrics.

## Key Results
- The proposed multimodal approach achieves higher accuracy, UF1, and UAR compared to benchmark methods on the CAS(ME)3 dataset
- The weighted fusion method significantly improves performance by focusing on features from frames near the apex of micro-expressions
- The depth/physiology guided attention modules effectively fuse features from multiple modalities, contributing to improved latent emotion recognition performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted fusion assigns higher importance to features from frames near the apex of micro-expressions, improving emotion recognition accuracy.
- Mechanism: Features from frames closer to the apex (middle of the micro-expression clip) are mapped to higher weights using a standardized normal distribution function, while features from frames farther from the apex are given lower weights. This allows the model to focus on the most informative frames for emotion recognition.
- Core assumption: The apex frame of a micro-expression contains the most prominent facial movement and is roughly in the middle of the clip.
- Evidence anchors:
  - [abstract]: "The weighted fusion method assigns different weights to different features based on their importance, allowing more important features to have a greater influence on the overall learning process."
  - [section]: "The sequence of extracted features is mapped to the ME feature with a standard normal distribution function... the most prominent facial movement is roughly in the middle of the timeframe, namely, the apex frame of a ME sample is roughly in the middle of the clip."
- Break condition: If the apex frame is not in the middle of the clip or if facial movements are not most prominent at the apex, the weighted fusion method may not improve performance.

### Mechanism 2
- Claim: The 1D separable and mixable depthwise inception network effectively extracts features from multiple physiological signals, contributing to improved latent emotion recognition performance.
- Mechanism: The network uses separate convolutions for each group of channels to extract features from each physiological signal (EDA, ECG, PPG) individually. The depthwise inception block with varying kernel sizes captures features at multiple scales. The mixed features from different sources are then fed into the final blocks for further learning.
- Core assumption: Physiological signals (EDA, ECG, PPG) provide reliable information about emotions and can be effectively processed using a 1D separable and mixable depthwise inception network.
- Evidence anchors:
  - [abstract]: "We design a 1D separable and mixable depthwise inception network that effectively extracts features from various PS."
  - [section]: "The network is designed to extract PS features in our framework... The depthwise structure contains separate convolutions for each group of channels, allowing for more precise feature extraction and capturing spatial correlations."
- Break condition: If the physiological signals do not provide reliable information about emotions or if the network architecture is not suitable for processing these signals, the feature extraction may not contribute to improved performance.

### Mechanism 3
- Claim: The depth/physiology guided attention module effectively fuses features from multiple modalities (color, depth, and physiological signals) to learn more beneficial mixed features, contributing to improved latent emotion recognition performance.
- Mechanism: The guided attention module uses depth and PS features as input to guide attention learning. It employs a multi-head attention mechanism that allows different parts of the sequence to be attended to differently, capturing longer-term and shorter-term dependencies. The independent attention outputs are concatenated and linearly transformed to the expected dimension.
- Core assumption: Fusing features from multiple modalities using a guided attention mechanism can learn more beneficial mixed features for latent emotion recognition.
- Evidence anchors:
  - [abstract]: "We develop a guided attention module is developed that achieves multimodal learning for both micro-expression (colour and depth information) and latent emotion recognition (ME and PS)."
  - [section]: "The module is designed for feature fusion of both colour and depth information for each frame of micro-expression, as well as the final fusion of ME and PS features... The multi-head attention mechanism allows for different parts of the sequence to be attended to differently."
- Break condition: If the attention mechanism does not effectively fuse features from multiple modalities or if the fused features do not contribute to improved latent emotion recognition performance, the guided attention module may not be beneficial.

## Foundational Learning

- Concept: Micro-expressions and their significance in emotion recognition
  - Why needed here: Understanding micro-expressions and their role in revealing genuine emotions is crucial for designing an effective emotion recognition system.
  - Quick check question: What are micro-expressions, and why are they considered valuable for authentic emotion recognition?

- Concept: Physiological signals and their relationship to emotions
  - Why needed here: Recognizing the importance of physiological signals (EDA, ECG, PPG) in providing reliable information about emotions is essential for incorporating them into the multimodal learning framework.
  - Quick check question: How do physiological signals reflect emotional states, and why are they considered more reliable than facial expressions in some cases?

- Concept: Multimodal learning and feature fusion techniques
  - Why needed here: Understanding the principles of multimodal learning and various feature fusion methods is necessary for designing an effective framework that combines micro-expressions and physiological signals.
  - Quick check question: What are the key challenges in multimodal learning, and how do different feature fusion techniques address these challenges?

## Architecture Onboarding

- Component map: ME feature extraction (color/depth) -> Standardized normal distribution weighted fusion -> PS feature extraction (1D inception) -> Depth/Physiology guided attention fusion -> Final classification

- Critical path:
  1. Extract features from micro-expression video frames using color and depth backbone networks
  2. Apply standardized normal distribution weighted fusion to merge features from all frames
  3. Extract features from physiological signals using the 1D separable and mixable depthwise inception network
  4. Fuse micro-expression and physiological signal features using the guided attention module
  5. Perform final classification for latent emotion recognition

- Design tradeoffs:
  - Using all frames vs. apex frame only for micro-expression feature extraction
  - Uniform weighting vs. standardized normal distribution weighting for feature fusion
  - Concatenation vs. guided attention for multimodal feature fusion

- Failure signatures:
  - Poor performance on micro-expression recognition: Issues with feature extraction from video frames or weighted fusion
  - Low accuracy on physiological signal-based emotion recognition: Problems with signal denoising or feature extraction using the 1D separable and mixable depthwise inception network
  - Suboptimal multimodal learning: Ineffective fusion of micro-expression and physiological signal features using the guided attention module

- First 3 experiments:
  1. Ablation study: Evaluate the impact of using all frames vs. apex frame only for micro-expression feature extraction
  2. Ablation study: Compare uniform weighting vs. standardized normal distribution weighting for feature fusion
  3. Ablation study: Assess the effectiveness of guided attention vs. concatenation for multimodal feature fusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can depth information be effectively denoised in micro-expression analysis to improve the accuracy of latent emotion recognition?
- Basis in paper: [explicit] The paper mentions that depth information can introduce noise, which is particularly problematic in micro-expression analysis where the signal is weak.
- Why unresolved: The paper suggests that finding a better approach to denoise the depth information is a potential avenue for further research, but does not provide a specific solution.
- What evidence would resolve it: Experimental results comparing the performance of the proposed approach with and without depth information denoising, or the development and testing of a new denoising method for depth information in micro-expression analysis.

### Open Question 2
- Question: How can the proposed 1D separable and mixable depthwise inception CNN be further optimized for extracting features from physiological signals?
- Basis in paper: [explicit] The paper introduces the 1D separable and mixable depthwise inception CNN for feature extraction from physiological signals, but does not discuss potential optimizations.
- Why unresolved: The paper does not provide information on how the network could be further improved or optimized for better performance.
- What evidence would resolve it: Experimental results comparing the performance of the proposed network with different variations or optimizations, or the development and testing of new network architectures for physiological signal feature extraction.

### Open Question 3
- Question: How can the proposed standardized normal distribution weighted feature fusion method be adapted for other applications beyond micro-expression analysis?
- Basis in paper: [explicit] The paper presents the standardized normal distribution weighted feature fusion method for micro-expression analysis, but does not discuss its potential application in other domains.
- Why unresolved: The paper does not provide information on how the method could be adapted or applied to other tasks or datasets.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the method in other applications, or the development and testing of the method for different types of data or tasks.

## Limitations

- The framework relies on the assumption that micro-expression apex frames are always in the middle of clips, which may not hold for all expressions
- Key architectural details for the depth/physiology guided attention module and backbone networks are not fully specified, affecting reproducibility
- The study is limited to the CAS(ME)3 dataset, which may not generalize to other populations or contexts

## Confidence

- High confidence: The general framework combining micro-expressions and physiological signals for emotion recognition
- Medium confidence: The effectiveness of the weighted fusion method and guided attention modules, as supported by ablation studies
- Low confidence: The specific architectural details and hyperparameter choices for key components

## Next Checks

1. **Ablation study validation**: Reproduce the ablation experiments comparing all frames vs. apex frame only for micro-expression feature extraction to verify the weighted fusion method's impact

2. **Architecture specification verification**: Implement the 1D separable and mixable depthwise inception network based on the described architecture and test on the PS-only emotion recognition task to validate feature extraction quality

3. **Attention mechanism analysis**: Visualize and analyze the attention weight distributions from the depth/physiology guided attention module to verify that it effectively focuses on relevant features from different modalities