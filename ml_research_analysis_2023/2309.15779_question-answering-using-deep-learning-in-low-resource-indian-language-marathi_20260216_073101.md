---
ver: rpa2
title: Question answering using deep learning in low resource Indian language Marathi
arxiv_id: '2309.15779'
source_url: https://arxiv.org/abs/2309.15779
tags:
- marathi
- language
- question
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a deep learning approach to building a reading
  comprehension-based question answering system for Marathi, a low-resource Indian
  language. Since Marathi lacks large-scale annotated datasets, the authors created
  MrSQuAD by translating the English SQuAD1.1 dataset, handling transliteration, number
  conversion, and answer alignment challenges.
---

# Question answering using deep learning in low resource Indian language Marathi

## Quick Facts
- arXiv ID: 2309.15779
- Source URL: https://arxiv.org/abs/2309.15779
- Reference count: 40
- Key outcome: MuRIL achieved EM 0.64, F1 0.74 on Marathi QA task

## Executive Summary
This paper presents a deep learning approach to building a reading comprehension-based question answering system for Marathi, a low-resource Indian language. The authors created MrSQuAD by translating the English SQuAD1.1 dataset, addressing challenges like transliteration, number conversion, and answer alignment. They evaluated several multilingual and monolingual transformer models fine-tuned on this dataset, finding that multilingual MuRIL achieved the highest performance with EM 0.64 and F1 0.74, outperforming other models including monolingual MahaBERT (EM 0.63, F1 0.73).

## Method Summary
The authors created MrSQuAD by translating the English SQuAD1.1 dataset into Marathi, handling transliteration, number conversion, and answer alignment challenges. They fine-tuned several multilingual (MuRIL, mBERT, XLM-RoBERTa) and monolingual (MahaBERT, IndicBERT) transformer models on this dataset for 2 epochs with batch size 15 and sequence length 512 using GPU. Models were evaluated using exact match (EM) and F1 scores to measure answer span accuracy against ground truth.

## Key Results
- MuRIL achieved highest performance with EM 0.64 and F1 0.74
- MahaBERT performed comparably with EM 0.63 and F1 0.73
- BERT-based models generally outperformed RoBERTa variants

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning multilingual models on Marathi data yields high performance because they capture shared linguistic features across related languages. Multilingual models like MuRIL and mBERT are pre-trained on large corpora of multiple languages, allowing them to learn universal linguistic structures and transfer knowledge to low-resource languages like Marathi. Core assumption: Marathi shares enough linguistic properties with other Indian languages to benefit from multilingual pre-training. Evidence: MuRIL achieved EM 0.64 and F1 0.74; BERT-based models performed better than RoBERTa. Break condition: If Marathi is too morphologically distinct from other languages in the pre-training corpus, transfer benefits diminish.

### Mechanism 2
Translating high-quality English datasets to Marathi creates usable training data for low-resource languages. Existing English datasets like SQuAD1.1 provide high-quality question-answer pairs that, when translated, preserve the semantic relationships needed for training. Core assumption: Translation preserves the semantic intent and answer boundaries in the target language. Evidence: MrSQuAD created by translating SQuAD1.1 with manual filtering of unwanted context; challenges included number conversion and answer alignment. Break condition: If translation quality is poor or answer spans don't align after translation, the dataset becomes ineffective.

### Mechanism 3
Fine-tuning with task-specific data improves model performance more than training from scratch. Pre-trained models have learned general language representations; fine-tuning adapts these representations to the specific task of Marathi question answering. Core assumption: The pre-training provides a better starting point than random initialization for the target task. Evidence: Fine-tuning existing multilingual and monolingual models created simple QA systems using smaller annotated Marathi datasets. Break condition: If the target task is too different from pre-training tasks, fine-tuning may not help.

## Foundational Learning

- Concept: Transfer learning in NLP
  - Why needed here: Understanding why pre-trained models can be adapted to Marathi rather than requiring training from scratch
  - Quick check question: What are the key differences between training from scratch versus fine-tuning pre-trained models?

- Concept: Multilingual language models
  - Why needed here: Understanding how models like MuRIL and mBERT work across multiple languages
  - Quick check question: How do multilingual models balance performance across different languages?

- Concept: Question answering evaluation metrics
  - Why needed here: Understanding EM, F1, and BERT scores to interpret model performance
  - Quick check question: What's the difference between exact match and F1 score in QA evaluation?

## Architecture Onboarding

- Component map: Dataset creation (translation + cleaning) -> Model selection (multilingual vs monolingual) -> Fine-tuning -> Evaluation
- Critical path: Translation → Dataset cleaning → Model fine-tuning → Evaluation
- Design tradeoffs: Multilingual models offer better transfer learning but may be slower; monolingual models are faster but require more training data
- Failure signatures: Poor EM scores indicate answer span misalignment; low F1 scores suggest semantic understanding issues
- First 3 experiments:
  1. Fine-tune mBERT on MrSQuAD with default hyperparameters
  2. Compare mBERT vs MuRIL on same dataset with identical settings
  3. Test translation quality impact by training on subsets of manually verified translations

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Marathi question answering systems change when trained on datasets created from scratch using Marathi text instead of translated English datasets? Basis: The paper concludes that "Development of a bigger dataset from scratch of Marathi question answers may provide better results compared to the translated dataset approach." Why unresolved: Only experimented with translated datasets (MrSQuAD). What evidence would resolve it: Comparative evaluation of fine-tuned models on a Marathi-native dataset versus MrSQuAD, measuring EM, F1, and BERT scores.

### Open Question 2
Can further fine-tuning with larger Marathi corpora or additional epochs improve the performance of monolingual models like MahaBERT and MuRIL beyond the reported results? Basis: The paper notes "We tried training the certain models for 4 epochs and different learning rates but there was no significant increase in the results." Why unresolved: Experiments were limited to 2 epochs with fixed hyperparameters. What evidence would resolve it: Systematic experiments with increased epochs, varied learning rates, and larger Marathi corpora to assess performance gains.

### Open Question 3
How does the accuracy of Marathi question answering systems degrade when tested on out-of-domain passages compared to the training dataset? Basis: The MrSQuAD dataset was created by translating SQuAD1.1, which covers diverse topics but may not reflect Marathi-specific domains. Why unresolved: The paper does not report cross-domain evaluation or assess robustness to topic shifts. What evidence would resolve it: Evaluation of fine-tuned models on Marathi passages from different domains (e.g., local news, literature) and comparison with in-domain performance metrics.

## Limitations

- Reliance on translated data introduces potential semantic drift and answer alignment issues
- Lack of statistical significance testing to validate performance differences between models
- Dataset creation process lacks complete specification of preprocessing tools and quality control measures

## Confidence

**High Confidence:** The overall experimental design and methodology are sound, following established practices in multilingual QA.

**Medium Confidence:** The reported performance scores are likely accurate for the described setup, but the lack of statistical validation means we cannot determine if differences between models are significant.

**Low Confidence:** The claim that monolingual models like MahaBERT perform "comparably" to multilingual models may overstate the similarity given the small absolute difference and lack of statistical testing.

## Next Checks

1. Perform paired t-tests or bootstrap analysis on the EM and F1 scores across multiple runs to determine if performance differences between models are statistically significant.

2. Create a small manually verified subset of the MrSQuAD dataset and measure semantic alignment and answer boundary accuracy between original English and translated Marathi versions to quantify translation quality impact.

3. Train the same models on Hindi SQuAD and test on Marathi MrSQuAD (and vice versa) to measure actual cross-lingual transfer capability and determine whether multilingual models truly benefit from shared linguistic features between related Indian languages.