---
ver: rpa2
title: Can Language Models Laugh at YouTube Short-form Videos?
arxiv_id: '2310.14159'
source_url: https://arxiv.org/abs/2310.14159
tags:
- video
- humor
- cfdbvtf
- gvooz
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We curated ExFunTube, a dataset of 10K user-generated funny short-form
  videos annotated with timestamps and explanations for humorous moments. The dataset
  is multimodal, covering a wide range of domains, and requires understanding of both
  visual and verbal elements.
---

# Can Language Models Laugh at YouTube Short-form Videos?

## Quick Facts
- arXiv ID: 2310.14159
- Source URL: https://arxiv.org/abs/2310.14159
- Reference count: 19
- One-line primary result: A multimodal zero-shot prompting method significantly improves LLM performance in explaining humor in user-generated short-form videos.

## Executive Summary
This paper explores whether large language models (LLMs) can understand and explain humor in user-generated short-form videos. The authors introduce ExFunTube, a dataset of 10K YouTube videos annotated with timestamps and explanations for humorous moments. They develop a zero-shot video-to-text prompting method that converts video content into text using state-of-the-art models for visual, speech, and audio modalities. This multimodal text is used as a prompt for LLMs to explain video humor. Results show that the prompting approach significantly improves the performance of GPT-3.5, T5, and BART in explaining video humor compared to text-only baselines.

## Method Summary
The authors curate ExFunTube, a dataset of 10K user-generated funny short-form videos annotated with timestamps and explanations. They develop a zero-shot video-to-text prompting method that converts video content into text using models for visual (BLIP-2, InternVideo), speech (Whisper), and audio (audio tagging) modalities. The generated text is used as a prompt for LLMs (T5, BART, GPT-3.5) to explain humor. The method is evaluated using automatic scores (SentBERT, ROSCOE), a rationale quality experiment (moment localization task), and human evaluations.

## Key Results
- GPT-3.5 with multimodal prompting outperforms text-only baselines across all metrics.
- The zero-shot video-to-text prompting method outperforms both end-to-end multimodal baselines and text-only approaches.
- Human evaluations confirm that the prompting approach generates more accurate and relevant humor explanations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal prompting improves LLM humor explanation by providing richer contextual cues than text-only transcripts.
- Mechanism: By converting video content into fine-grained text descriptions (visual frames, speech transcripts, and audio tags) and arranging them chronologically, the model receives aligned multimodal signals that help disambiguate humor cues that may be ambiguous or absent in text alone.
- Core assumption: Humor in short-form videos often relies on both verbal and visual incongruity, which is better conveyed through combined modalities.
- Evidence anchors:
  - [abstract]: "We develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs)."
  - [section]: "The results show that, except for SentBERT@0.7, GPT-3.5 with our prompting reaches the best performance. Especially, the SentBERT and ROSCOE scores with our prompting are higher than those with text-only baselines in all cases."
- Break condition: If visual or audio information does not contribute meaningfully to the humor (e.g., purely verbal jokes), the multimodal prompting may add noise rather than improve performance.

### Mechanism 2
- Claim: Speaker separation and audio tagging improve dialogue understanding, which is critical for multimodal humor explanation.
- Mechanism: By identifying speakers and tagging audio context (e.g., laughter, music, sound effects), the model can better attribute utterances and interpret comedic timing, enhancing the humor explanation.
- Core assumption: Humor often depends on who says what and when, especially in dialogue-heavy clips.
- Evidence anchors:
  - [abstract]: "We consider two modalities of the video content: visual and audio. From the audio modality, we acquire speech transcripts and sound labels."
  - [section]: "We then predict the number of speakers and assign speakers to each utterance utilizing ChatGPT. This speaker separation helps a deep understanding of dialogue."
- Break condition: If the video lacks clear speaker turns or the audio context is minimal, speaker separation may add unnecessary complexity.

### Mechanism 3
- Claim: Zero-shot composition of pretrained models is more effective than end-to-end multimodal training for humor explanation.
- Mechanism: Instead of fine-tuning a single multimodal model, the system composes separate pretrained models (video captioning, audio tagging, speech recognition) and uses their outputs as text input to LLMs, leveraging their strong language understanding.
- Core assumption: Large language models are better at processing and explaining humor when given rich, structured textual input, rather than raw multimodal features.
- Evidence anchors:
  - [abstract]: "We design a zero-shot video-to-text prompting that converts video content into text to maximize LLMs' ability to explain video humor."
  - [section]: "Our method outperforms the multimodal end-to-end baseline MAF and the multimodal zero-shot prompting baseline VideoChat-Text."
- Break condition: If the LLM's language understanding degrades with very long or complex prompts, the zero-shot approach may underperform fine-tuned end-to-end models.

## Foundational Learning

- Concept: Multimodal representation alignment
  - Why needed here: Humor often relies on the interplay between visual and verbal cues; aligning these in a single prompt is key to accurate explanation.
  - Quick check question: If a joke's punchline depends on a visual gag, will a transcript-only prompt capture the incongruity?

- Concept: Speaker attribution in dialogue
  - Why needed here: Identifying who says what helps resolve ambiguous humor references and timing.
  - Quick check question: If two people speak simultaneously in a funny clip, can the model attribute the correct punchline to the right speaker?

- Concept: Zero-shot prompting strategies
  - Why needed here: The system uses existing models without further training, so prompt design must maximize performance.
  - Quick check question: Does the prompt clearly instruct the LLM to "explain as if watching the video," or could the model misinterpret the task?

## Architecture Onboarding

- Component map: Video preprocessing → PySceneDetect (segmentation) → Frame extraction (5fps) → BLIP-2 (image captioning) → Video-to-text retrieval → InternVideo → Audio processing → Whisper (transcription), ChatGPT (speaker separation), audio tagging model → Prompt assembly → Chronological integration → LLM inference → GPT-3.5 / T5 / BART
- Critical path: Video → Multimodal text extraction → Prompt construction → LLM explanation generation
- Design tradeoffs:
  - Granularity vs. efficiency: More frames and captions yield richer context but increase latency.
  - Zero-shot vs. fine-tuning: Zero-shot is flexible but may underperform on domain-specific humor.
  - Modality inclusion: Each added modality increases context but may introduce noise if irrelevant.
- Failure signatures:
  - Low SentBERT/ROSCOE scores: Missing or misaligned multimodal cues.
  - Poor localization accuracy: Explanation fails to match video timestamps.
  - High variance in human ratings: Explanations are inconsistent or culturally biased.
- First 3 experiments:
  1. Ablation: Remove visual component and compare SentBERT scores.
  2. Ablation: Remove speaker separation and compare localization accuracy.
  3. Compare zero-shot LLM with fine-tuned multimodal end-to-end model on a held-out humor explanation task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of humor (e.g., slapstick, sarcasm, wordplay) affect the performance of language models in explaining video humor?
- Basis in paper: [explicit] The authors analyze LLMs' performance across 20 humor categories and observe varying performance levels.
- Why unresolved: The paper provides a high-level analysis but does not delve into the specific characteristics of each humor type that contribute to the performance differences.
- What evidence would resolve it: A detailed analysis of the features of each humor type and their correlation with LLM performance would help understand the underlying reasons for the performance variations.

### Open Question 2
- Question: What is the impact of temporal information in sound on humor understanding and explanation in videos?
- Basis in paper: [inferred] The authors mention that they did not consider the temporal information of sound in their approach, despite acknowledging that timing can play a role in humor.
- Why unresolved: The paper focuses on extracting sound tags but does not explore how the timing of these sounds contributes to humor.
- What evidence would resolve it: Experiments that analyze the effect of temporal sound information on humor understanding and explanation would provide insights into its importance.

### Open Question 3
- Question: How does the subjective nature of humor affect the quality and consistency of explanations generated by language models?
- Basis in paper: [explicit] The authors acknowledge that humor is subjective and that their collected explanations may be subjective as well.
- Why unresolved: The paper does not explore the impact of subjectivity on the quality and consistency of explanations.
- What evidence would resolve it: Studies that compare the explanations generated by language models with those provided by humans with different senses of humor would help understand the impact of subjectivity.

## Limitations
- The dataset filtering process relies on GPT-3.5's subjective assessment of multimodal humor, potentially introducing bias.
- The zero-shot approach may not match the performance of specialized fine-tuned models for specific humor domains.
- Speaker separation depends on ChatGPT, which may not scale well or perform consistently across different languages or dialects.

## Confidence
- High: Automatic metrics (SentBERT, ROSCOE) and human evaluations converge on the effectiveness of multimodal prompting.
- Medium: The study's reliance on a single dataset and limited comparison to fine-tuned end-to-end models leaves questions about generalizability.
- Low: The relative contribution of individual components (e.g., speaker separation vs. audio tags) remains unclear.

## Next Checks
1. Test the prompting method on a held-out dataset with different cultural contexts or humor styles to assess generalization.
2. Compare zero-shot prompting performance with a fine-tuned end-to-end multimodal model on the same task.
3. Conduct a detailed ablation study to quantify the individual contribution of each modality (visual, audio, speaker separation) to overall performance.