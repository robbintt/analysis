---
ver: rpa2
title: Solving MaxSAT with Matrix Multiplication
arxiv_id: '2311.02101'
source_url: https://arxiv.org/abs/2311.02101
tags:
- maxsat
- rbmsat
- clauses
- number
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RbmSAT is a novel MaxSAT solver designed for neural network accelerators
  like GPUs and TPUs. It constructs a Restricted Boltzmann Machine (RBM) where the
  probability of a Boolean assignment is exponential in the number of satisfied clauses.
---

# Solving MaxSAT with Matrix Multiplication

## Quick Facts
- arXiv ID: 2311.02101
- Source URL: https://arxiv.org/abs/2311.02101
- Reference count: 40
- RbmSAT is a novel MaxSAT solver that leverages matrix multiplication on neural network accelerators to outperform existing solvers on specific problem instances.

## Executive Summary
RbmSAT introduces a novel approach to solving Maximum Satisfiability (MaxSAT) problems by constructing a Restricted Boltzmann Machine (RBM) where the probability of a Boolean assignment is exponential in the number of satisfied clauses. The key innovation is using block Gibbs sampling for stochastic search, with matrix multiplication as the primary computational primitive, making it well-suited for neural network accelerators like GPUs and TPUs. The approach combines this stochastic search with periodic application of a unit propagation-based heuristic on CPU to improve solution quality.

## Method Summary
RbmSAT constructs an RBM where visible units represent problem variables and hidden units represent clauses, with connections designed to make the probability of assignments exponential in the number of satisfied clauses. Block Gibbs sampling is performed using matrix multiplication operations, which are highly optimized on neural network accelerators. The method includes periodic application of a unit propagation-based heuristic on CPU to improve sampled assignments. The approach uses parallel Markov chains distributed across multiple accelerators to increase search throughput.

## Key Results
- Outperforms other participating solvers on problems from three out of four years of MaxSAT Evaluation's Incomplete Unweighted Track when given the same CPU compute budget
- Outperforms all solvers on problems from all four years when given the same running time on a TPU cluster
- Theoretical results guarantee linear scaling of visible and hidden units with problem size, ensuring reasonable computational costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matrix multiplication efficiency on neural network accelerators drives performance
- Core assumption: Matrix multiplication is the dominant cost in block Gibbs sampling
- Evidence: 40 lines of JAX implementation leveraging matrix multiplication as main primitive
- Break condition: If other operations become significant relative to matrix multiplication

### Mechanism 2
- Claim: Linear scaling of RBM units ensures manageable computational costs
- Core assumption: Theoretical RBM representational efficiency applies to MaxSAT construction
- Evidence: Theoretical guarantees for linear scaling of visible and hidden units
- Break condition: If scaling behavior is worse in practice than theoretical guarantees

### Mechanism 3
- Claim: Unit propagation heuristic significantly improves solution quality
- Core assumption: Heuristic effectively improves assignments found by stochastic search
- Evidence: Periodic application of unit propagation-based heuristic on CPU
- Break condition: If heuristic doesn't provide significant improvements

## Foundational Learning

- Concept: Restricted Boltzmann Machines (RBMs)
  - Why needed: RbmSAT constructs an RBM to represent MaxSAT problems
  - Quick check: What property of RBMs enables efficient block Gibbs sampling using matrix multiplication?

- Concept: Gibbs Sampling
  - Why needed: Used for stochastic search in the RBM solution space
  - Quick check: What is the transition from sample ˜v_t to ˜v_{t+1} in block Gibbs sampling?

- Concept: Matrix Multiplication
  - Why needed: Main computational primitive for block Gibbs sampling
  - Quick check: What operations in block Gibbs sampling can be expressed as matrix multiplications?

## Architecture Onboarding

- Component map: RBM construction -> Matrix multiplication-based Gibbs sampling -> Unit propagation heuristic -> TPU/GPU cluster
- Critical path: 1) Construct RBM from CNF, 2) Perform block Gibbs sampling on accelerator, 3) Apply unit propagation heuristic on CPU, 4) Return updated samples to accelerator
- Design tradeoffs: TPUs/GPUs vs CPUs, parallel chains vs per-chain progress, heuristic frequency vs sampling time, bfloat16 vs float32 precision
- Failure signatures: Poor scaling, low throughput, limited heuristic improvement, sampling instability
- First 3 experiments: 1) Verify RBM construction for small instances, 2) Measure sampling throughput on accelerator, 3) Evaluate heuristic impact on solution quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RbmSAT scale to larger MaxSAT instances beyond current constraints?
- Basis: Authors mention sharding clauses across accelerators but don't explore scaling beyond specific limits
- Why unresolved: Focus on instances within specific size constraints
- Evidence needed: Results on significantly larger instances demonstrating sharding effectiveness

### Open Question 2
- Question: How does performance compare on weighted MaxSAT problems?
- Basis: RbmSAT currently designed only for unweighted problems
- Why unresolved: Focus on unweighted instances without weighted comparisons
- Evidence needed: Experimental results comparing to state-of-the-art weighted solvers

### Open Question 3
- Question: What is the impact of hidden units per clause on performance?
- Basis: Authors use specific hidden unit counts but don't explore sensitivity
- Why unresolved: No ablation study on hidden unit hyperparameter
- Evidence needed: Results showing effect of varying hidden units across different instances

## Limitations
- Performance claims tied to specific TPU hardware and bfloat16 precision
- Current implementation limited to unweighted MaxSAT instances
- Empirical results focus on a subset of MaxSAT evaluation problems

## Confidence
- High: Matrix multiplication-based block Gibbs sampling mechanism is well-established
- Medium: Performance claims supported by specific problem subset but need broader validation
- Low: Exact hyperparameter settings not fully specified for exact replication

## Next Checks
1. Evaluate RbmSAT on complete MaxSAT Evaluation instance set to verify scaling behavior
2. Implement and benchmark on GPU and CPU-only configurations to assess hardware portability
3. Systematically explore hyperparameter space for unit propagation heuristic and OR-gate RBM training