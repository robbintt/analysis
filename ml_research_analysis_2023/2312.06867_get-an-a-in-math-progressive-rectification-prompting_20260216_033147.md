---
ver: rpa2
title: 'Get an A in Math: Progressive Rectification Prompting'
arxiv_id: '2312.06867'
source_url: https://arxiv.org/abs/2312.06867
tags:
- answer
- verification
- question
- have
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Progressive Rectification Prompting (PRP) improves LLM performance
  on math word problems by iterating a verify-then-rectify process. Starting from
  an initial answer, PRP substitutes a masked value from the question and verifies
  correctness by predicting the masked value; if incorrect, the answer is added to
  a set of incorrect answers and the LLM is prompted to regenerate reasoning while
  avoiding those answers.
---

# Get an A in Math: Progressive Rectification Prompting

## Quick Facts
- **arXiv ID**: 2312.06867
- **Source URL**: https://arxiv.org/abs/2312.06867
- **Reference count**: 40
- **Primary result**: PRP achieves 90.5% accuracy on math word problems, outperforming zero-shot by 13.2% and few-shot by 9.5%

## Executive Summary
This paper introduces Progressive Rectification Prompting (PRP), an iterative method that improves large language model performance on math word problems by combining verification and rectification. PRP works by generating an initial answer, verifying it by predicting a masked numerical value, and if incorrect, regenerating reasoning while avoiding previously incorrect answers. The method demonstrates state-of-the-art performance across eight math word problem datasets, achieving 90.5% accuracy and substantially outperforming existing zero-shot and few-shot methods.

## Method Summary
PRP iteratively refines LLM answers to math word problems through a verify-then-rectify process. Starting with an initial answer, PRP substitutes a masked numerical value from the question and prompts the LLM to predict it. If the prediction mismatches the masked value, the answer is flagged as incorrect and added to a set. The LLM then regenerates reasoning with a hint "the answer is likely not [H]" where [H] is the set of incorrect answers. This process repeats up to K iterations (typically 3), progressively refining the answer until verification succeeds or the iteration limit is reached.

## Key Results
- Achieves state-of-the-art accuracy of 90.5% on eight math word problem datasets
- Outperforms existing zero-shot methods by 13.2% absolute improvement
- Outperforms existing few-shot methods by 9.5% absolute improvement
- Particularly effective on complex arithmetic problems and those with irrelevant context

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PRP progressively rectifies reasoning by iteratively verifying masked conditions and avoiding known incorrect answers
- **Mechanism**: At each iteration, PRP substitutes a masked numerical value in the question and prompts the LLM to predict it. If the prediction mismatches the masked value, the answer is flagged incorrect and added to a set. The LLM then regenerates reasoning with a hint "the answer is likely not [H]", where [H] is the set of incorrect answers
- **Core assumption**: The masked condition substitution isolates a verification step that is independent of prior reasoning mistakes
- **Evidence anchors**:
  - [abstract] "With the most likely correct answer, the LLM predicts a masked numerical value in the question; if the prediction does not match the masked value, the answer is likely incorrect."
  - [section] "It masks a numerical value in the question, takes the previous generated answer as a conclusion, and uses it as a new condition."
- **Break condition**: When the masked value prediction matches the original masked value, PRP accepts the answer as correct and terminates iteration

### Mechanism 2
- **Claim**: PRP avoids repeating previous mistakes by explicitly informing the LLM of likely incorrect answers
- **Mechanism**: After verification fails, PRP appends "(The answer is likely not [H])" to the question, where [H] is the set of incorrect answers. This guides the LLM to avoid those answers in the next reasoning path
- **Core assumption**: LLMs can incorporate explicit negative hints into subsequent reasoning generation
- **Evidence anchors**:
  - [section] "In rectification, we added the phrase '(The answer is likely not [H])' after the given question, where [H] was the slot for the set of potentially incorrect answers."
  - [abstract] "Then the LLM is prompted to regenerate the reasoning path hinted with a set of incorrect answers to prevent itself from repeating previous mistakes."
- **Break condition**: When the LLM generates an answer that passes verification, the iterative process stops

### Mechanism 3
- **Claim**: PRP progressively refines answers, with more iterations needed for more complex problems
- **Mechanism**: PRP iterates the verify-then-rectify loop up to K times, with each iteration potentially improving accuracy. For problems with irrelevant context or more steps, PRP requires more iterations
- **Core assumption**: Progressive refinement is effective because each iteration corrects at least one error and reduces the error space
- **Evidence anchors**:
  - [section] "Figure 2 demonstrates the accuracy improvements of both PRP and PHP-CoT as the number of iterations increases."
  - [section] "Figure 4(b) illustrates the average iteration number of PRP across all eight MWP datasets."
- **Break condition**: Iteration stops early if verification succeeds or after K iterations

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) prompting
  - **Why needed here**: PRP builds on CoT by generating reasoning paths but adds verification and rectification
  - **Quick check question**: What is the main limitation of vanilla CoT prompting addressed by PRP?

- **Concept**: Self-consistency
  - **Why needed here**: PRP is compared to self-consistency, which repeats solving and votes, while PRP avoids repeating mistakes
  - **Quick check question**: How does PRP differ from self-consistency in handling incorrect answers?

- **Concept**: Masked condition substitution
  - **Why needed here**: PRP uses this to create a verification question independent of the original reasoning path
  - **Quick check question**: Why does PRP replace a numerical value with a special token for verification?

## Architecture Onboarding

- **Component map**: Initialization -> Verification Module -> Rectification Module -> Answer Selection
- **Critical path**: Initialization → (Verify → Rectify) repeated up to K times → Answer Selection
- **Design tradeoffs**:
  - Iteration limit K balances runtime vs. accuracy
  - Masked value selection randomness vs. systematic coverage
  - Hint specificity vs. LLM flexibility
- **Failure signatures**:
  - Verification always fails (masked prediction never matches)
  - Rectification never produces a new answer
  - Iteration count reaches K without verification success
- **First 3 experiments**:
  1. Test PRP on a simple AddSub dataset with K=1 to confirm early stopping works
  2. Test PRP with K=3 on a dataset with irrelevant context (e.g., SV AMP) to observe iteration count
  3. Compare PRP accuracy vs. baseline CoT with self-consistency on GSM8K

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does PRP performance scale with increasing model size and capability, beyond the tested text-davinci-003?
- **Basis in paper**: [explicit] The paper mentions that PRP performs better with more powerful LLMs, specifically comparing text-davinci-002 to text-davinci-003, but does not explore scaling to larger models like GPT-4
- **Why unresolved**: The paper only tests PRP on text-davinci-003, leaving open the question of how it would perform with larger, more capable models
- **What evidence would resolve it**: Systematic testing of PRP across a range of LLM sizes and capabilities, including state-of-the-art models, to determine performance scaling

### Open Question 2
- **Question**: What is the optimal number of iterations (K) for PRP across different problem types and dataset characteristics?
- **Basis in paper**: [explicit] The paper sets K to 5 for all datasets but acknowledges this is a trade-off between efficiency and effectiveness, suggesting the optimal K may vary
- **Why unresolved**: The paper uses a fixed K value without exploring how different K values affect performance across varying problem complexities and dataset features
- **What evidence would resolve it**: Empirical studies varying K across different problem types and datasets to determine optimal iteration numbers for each scenario

### Open Question 3
- **Question**: How does PRP handle problems with multiple masked conditions or complex interdependencies between numerical values?
- **Basis in paper**: [inferred] The paper describes masking a single numerical value for verification, but real-world problems often have multiple interdependent values that could affect verification accuracy
- **Why unresolved**: The current PRP implementation only masks one value at a time, leaving unclear how it would handle problems requiring simultaneous consideration of multiple masked values or their relationships
- **What evidence would resolve it**: Testing PRP on problems with multiple interdependent numerical values and comparing performance to single-masking approaches

## Limitations
- Performance claims rely heavily on a single LLM (text-davinci-003) and may not generalize to other models
- Fixed iteration limit K=3 used for all datasets without justification for why this value is optimal across diverse problem types
- Verification mechanism depends on successful numerical value masking and prediction, which may fail on problems with multiple numerical values or complex numerical expressions

## Confidence
- **High Confidence (Medium)**: The core PRP mechanism is clearly described and logically coherent
- **Medium Confidence (High)**: State-of-the-art performance claims are well-supported by reported accuracy numbers
- **Low Confidence (Low)**: The claim that PRP "progressively rectifies reasoning" is difficult to verify without access to intermediate reasoning paths

## Next Checks
1. **Generalization Test**: Implement PRP on a different LLM (e.g., GPT-4 or Claude) and test across the same eight datasets to verify if performance improvements are model-dependent or generalize to other LLMs

2. **Iteration Analysis**: For each dataset, run PRP with varying K values (1, 3, 5, 10) and plot accuracy vs. iteration count to determine if K=3 is indeed optimal or if different datasets benefit from different iteration limits

3. **Reasoning Quality Audit**: Manually examine 20 randomly selected problems where PRP succeeded but baselines failed, analyzing whether the final reasoning paths demonstrate genuine improvement in logical coherence or simply avoid previous numerical errors through trial and error