---
ver: rpa2
title: 'LMDX: Language Model-based Document Information Extraction and Localization'
arxiv_id: '2309.10952'
source_url: https://arxiv.org/abs/2309.10952
tags:
- document
- entity
- extraction
- entities
- line
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LMDX introduces a novel methodology for leveraging large language
  models (LLMs) to extract information from visually rich documents. The core idea
  is to encode layout information as coordinate tokens within the document representation
  and use a structured JSON schema to guide extraction of both leaf and hierarchical
  entities.
---

# LMDX: Language Model-based Document Information Extraction and Localization

## Quick Facts
- arXiv ID: 2309.10952
- Source URL: https://arxiv.org/abs/2309.10952
- Authors: 
- Reference count: 40
- Key outcome: LMDX achieves state-of-the-art results on VRDU and CORD benchmarks, with 54.35% micro-F1 at 10 training documents versus 39.76% for best baseline on Ad-Buy Form Mixed Template

## Executive Summary
LMDX introduces a novel methodology for leveraging large language models to extract information from visually rich documents by encoding layout information as coordinate tokens within the document representation. The approach uses a structured JSON schema to guide extraction of both leaf and hierarchical entities, supporting zero-shot extraction while localizing predicted entities within the document. On the VRDU and CORD benchmarks, LMDX with PaLM 2-S achieves state-of-the-art results, significantly outperforming existing methods especially in low-data regimes.

## Method Summary
LMDX extracts information from visually rich documents by first obtaining text segments and bounding boxes via OCR, then chunking documents into LLM-compatible segments. Each segment is augmented with normalized X,Y coordinate tokens, and prompts are generated using a predefined JSON schema that defines entity types and hierarchies. The LLM generates multiple stochastic completions per chunk, which are parsed, grounded to original segments, and merged using majority voting to produce final structured output with entity locations.

## Key Results
- On VRDU Ad-Buy Form Mixed Template, LMDX reaches 54.35% micro-F1 at 10 training documents compared to 39.76% for best baseline
- LMDX excels at extracting hierarchical entities, achieving 72.09% F1 score on line items versus 46.63% for best baseline
- The approach significantly outperforms existing methods in low-data regimes while supporting zero-shot extraction capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The coordinate-as-tokens scheme communicates spatial layout information to the LLM without architectural changes
- **Mechanism**: Each text segment is augmented with normalized X,Y coordinate tokens, creating unique identifiers that preserve position while remaining within standard LLM tokenization
- **Core assumption**: LLMs can learn to associate coordinate tokens with spatial relationships when co-occurring with text
- **Evidence anchors**: [abstract] "We propose a layout encoding scheme that communicate spatial information to the LLM without any change to its architecture"; [section 2.4] "This coordinate-as-tokens scheme allows us to communicate the layout modality to the LLM, without any change to its architecture"
- **Break condition**: If coordinate tokens become too sparse relative to text tokens, or if the LLM's positional encoding interferes with coordinate interpretation

### Mechanism 2
- **Claim**: Structured JSON schema enables zero-shot extraction by providing semantic context
- **Mechanism**: The schema defines entity types, hierarchies, and occurrence patterns, which the LLM uses to guide extraction without requiring schema-specific training
- **Core assumption**: LLMs have sufficient pretraining exposure to understand JSON structure and entity type semantics
- **Evidence anchors**: [abstract] "LMDX supports zero-shot extraction and localizes predicted entities within the document"; [section 2.4] "JSON was chosen as a format for the completion and schema since it supports hierarchical objects, is very token-efficient, and usually present in LLMs training data mixtures"
- **Break condition**: When entity types are too domain-specific or when schema complexity exceeds LLM's reasoning capacity

### Mechanism 3
- **Claim**: Sampling multiple completions and majority voting reduces hallucination and improves accuracy
- **Mechanism**: K stochastic completions are generated per chunk, then parsed and grounded; majority voting selects the most consistent predictions across samples
- **Core assumption**: Consistent predictions across samples indicate higher confidence and lower hallucination risk
- **Evidence anchors**: [abstract] "We also propose a decoding algorithm transforming the LLM responses into extracted entities and their bounding boxes on the document, while discarding all hallucination"; [section 2.6] "This randomness in the sampling allows to do error correction (e.g. if a response is not valid JSON, have hallucinated segment coordinate identifier, etc), and increase the extraction quality"
- **Break condition**: When K samples produce highly inconsistent outputs, suggesting fundamental uncertainty rather than noise

## Foundational Learning

- **Concept**: Tokenization and coordinate quantization
  - **Why needed here**: Understanding how text segments map to tokens and how coordinates map to discrete buckets is crucial for debugging layout encoding
  - **Quick check question**: If you have a segment at position (150, 75) on a 300x150 document, what would be the normalized and quantized coordinate tokens with B=100 buckets?

- **Concept**: JSON schema structure and hierarchical entity representation
  - **Why needed here**: The schema defines what to extract and how entities relate; understanding this is essential for prompt engineering
  - **Quick check question**: Given schema `{"order": [{"item": [{"name": "", "price": ""}]}]}`, how many `name` entities should be extracted per document?

- **Concept**: Grounding and bounding box calculation
  - **Why needed here**: Ensuring extracted text matches the original document content is critical for validation
  - **Quick check question**: If an entity spans segments with bounding boxes [(10,10,20,20), (25,10,35,20)], what is the final bounding box for the entity?

## Architecture Onboarding

- **Component map**: OCR service → Text and layout extraction → Chunking module → Document segmentation for LLM input limits → Prompt generator → Template filling with document and schema → LLM inference → K completions per chunk → Decoding pipeline → Grounding, parsing, majority voting, merging → Post-processing → Final structured output with bounding boxes

- **Critical path**: OCR → Chunking → Prompt Generation → LLM Inference → Decoding → Output
  - Each stage must complete successfully for the next to function
  - Decoding is the most failure-prone due to grounding requirements

- **Design tradeoffs**:
  - Line-level vs word-level segments: Line-level reduces tokens but loses fine-grained position
  - 2 vs 4 coordinates: 2 coordinates are token-efficient but less precise for entity localization
  - Sampling K=16: Balances accuracy gains against inference cost

- **Failure signatures**:
  - Empty or malformed JSON completions → Check prompt formatting and schema validity
  - Grounding failures (text not found on segment) → Likely OCR errors or coordinate quantization issues
  - Poor hierarchical entity grouping → Heuristic merging may need adjustment

- **First 3 experiments**:
  1. Run LMDX on a single-page document with known ground truth; verify coordinate mapping by visual inspection
  2. Compare zero-shot vs few-shot performance on VRDU Registration Form; measure impact of schema complexity
  3. Test different coordinate quantization schemes (B=50, 100, 200) on extraction accuracy and token usage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does LMDX performance scale with increasingly larger LLM models (e.g. PaLM 2-L, PaLM 2-XL)?
- **Basis in paper**: [inferred] The paper states "we use the small version of this LLM due to limited accelerator resources, but larger versions could be used, likely leading to higher extraction quality."
- **Why unresolved**: The authors only evaluated LMDX with PaLM 2-S due to resource constraints, leaving the performance potential of larger models untested.
- **What evidence would resolve it**: Conducting experiments with larger PaLM 2 variants and comparing their extraction quality metrics to PaLM 2-S results.

### Open Question 2
- **Question**: What is the impact of incorporating image modality into LMDX on its performance, particularly for handling OCR errors?
- **Basis in paper**: [explicit] The conclusion states "we will explore applying the methodology to open-source LLMs and adding the image modality to the system using Large Vision-Language Models."
- **Why unresolved**: LMDX currently relies solely on OCR text and layout coordinates, which the authors acknowledge as a limitation when OCR segments group semantically distinct text.
- **What evidence would resolve it**: Implementing LMDX with vision-language models and measuring improvements in extraction quality, especially on documents with OCR errors.

### Open Question 3
- **Question**: How does LMDX perform on document types beyond the VRDU and CORD benchmarks?
- **Basis in paper**: [explicit] The paper states "since a quasi-infinite number of document types exist, and that organizations have limited annotation resources, most parsers are built with very small amount of data."
- **Why unresolved**: While LMDX shows strong performance on VRDU and CORD, these benchmarks may not represent the full diversity of real-world document types.
- **What evidence would resolve it**: Testing LMDX on additional document benchmarks or real-world datasets spanning various industries and document formats.

## Limitations

- The mechanism by which LLMs interpret coordinate tokens as spatial layout information remains speculative without direct experimental validation
- Critical implementation details are missing, particularly around how the PaLM 2-S model was finetuned and specialized for target benchmarks
- The approach relies heavily on OCR accuracy, which can group semantically distinct text segments together

## Confidence

**High confidence**: The overall methodology is sound and the performance claims on VRDU and CORD benchmarks appear well-supported. The approach of using structured JSON schemas and coordinate encoding represents a novel and promising direction for document information extraction.

**Medium confidence**: The claims about zero-shot extraction capabilities and hallucination reduction through sampling are plausible but not fully validated. The paper doesn't provide ablation studies or controlled experiments that would isolate the impact of individual design choices.

**Low confidence**: The mechanism by which LLMs interpret coordinate tokens as spatial layout information remains speculative. Without direct experimental evidence showing how coordinate tokens influence the model's spatial reasoning, this represents a significant theoretical gap.

## Next Checks

1. **Coordinate Token Interpretation Test**: Design an experiment where the same text content appears at different coordinates, then measure if the LLM's output varies predictably based on coordinate information. This would validate whether coordinate tokens actually convey spatial meaning to the model.

2. **Ablation Study of Core Components**: Run LMDX with variations that systematically remove or modify individual components: remove coordinate tokens, use unstructured output format, eliminate sampling and majority voting. Compare performance changes to quantify each component's contribution.

3. **Error Analysis on Grounding Failures**: Analyze a sample of documents where grounding fails (text not found on segment), categorize failure modes (OCR errors, coordinate quantization issues, text matching problems), and measure the frequency of each type. This would identify the primary bottleneck in the current pipeline.