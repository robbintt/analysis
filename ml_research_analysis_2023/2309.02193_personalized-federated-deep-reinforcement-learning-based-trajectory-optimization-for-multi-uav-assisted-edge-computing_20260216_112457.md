---
ver: rpa2
title: Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization
  for Multi-UAV Assisted Edge Computing
arxiv_id: '2309.02193'
source_url: https://arxiv.org/abs/2309.02193
tags:
- algorithm
- each
- global
- learning
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses trajectory optimization for multi-UAV-assisted
  mobile edge computing, focusing on improving learning efficiency and convergence
  in complex, dynamic environments with heterogeneous data. The core method combines
  personalized federated deep reinforcement learning (PF-DRL) with multi-agent deep
  deterministic policy gradient (MADDPG), where local models are trained using MADDPG
  and aggregated with global models via federated learning, followed by personalization
  through weighted aggregation of local and global models.
---

# Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing

## Quick Facts
- arXiv ID: 2309.02193
- Source URL: https://arxiv.org/abs/2309.02193
- Reference count: 18
- Primary result: PF-MADDPG achieves up to 14.5% higher average return and 115.4% faster convergence than MADDPG/F-MADDPG in multi-UAV trajectory optimization

## Executive Summary
This paper proposes a personalized federated deep reinforcement learning (PF-DRL) framework for trajectory optimization in multi-UAV-assisted mobile edge computing. The approach combines MADDPG for local training with federated learning for global model aggregation, followed by personalization through weighted blending of local and global models. The method addresses challenges of heterogeneous data and communication constraints while improving convergence speed and task completion rates. Simulation results demonstrate significant performance improvements over baseline methods, with an optimal personalization weight of 7:3 (local:global).

## Method Summary
The PF-MADDPG algorithm operates through three main stages: local MADDPG training for each UAV, federated aggregation of model parameters via weighted averaging, and personalization by combining local and global models. Each UAV trains locally using MADDPG with actor-critic networks, experience replay, and target networks. Model parameters are then uploaded to a central aggregator, which computes a global model through weighted averaging. Each UAV forms a personalized model by convexly combining its local and global parameters with weight α. The approach balances global generalization with local specialization, addressing data heterogeneity and improving convergence in complex, dynamic environments.

## Key Results
- PF-MADDPG achieves up to 14.5% higher average return compared to MADDPG and F-MADDPG baselines
- Convergence rate improves by 115.4% relative to baseline methods
- Local performance for each UAV improves by 13%-18% with optimal personalization
- Optimal personalized model weight found to be approximately 7:3 (local:global)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalized federated aggregation balances global generalization and local specialization, accelerating convergence in heterogeneous data settings.
- Mechanism: Each UAV trains locally using MADDPG, uploads parameters to a central server, which aggregates them via weighted averaging (global model). Each UAV then forms a personalized model by convexly combining its local and global parameters with weight α. This blend mitigates negative effects of data heterogeneity and data scarcity while leveraging global knowledge from all agents.
- Core assumption: Local data distributions across UAVs are sufficiently diverse that a single global model is suboptimal, but still share enough structure for federated averaging to be beneficial.
- Evidence anchors:
  - [abstract] "by improving the aggregation process of FL, the global model and the local model are aggregated with a moderate weight to train a personalized model"
  - [section III.B.3] "the PF-MADDPG algorithm is proposed to solve this problem... taking the actor evaluation network as an example, the optimization goal of each agent can be described as: min L(αθn + (1 − α) θglobal)"
  - [corpus] Weak evidence for federated learning efficacy in heterogeneous data—most neighbors are unrelated or low-quality, except one that mentions "Federated Graph-Enhanced Multi-Agent Reinforcement Learning Framework for Multi-UAV Cooperative Mobile Edge Computing."
- Break condition: If data heterogeneity is too extreme, global averaging may dominate and personalization will be ineffective; conversely, if data are identical, personalization adds unnecessary complexity.

### Mechanism 2
- Claim: MADDPG local training enables stable policy learning in continuous action spaces and facilitates multi-agent coordination.
- Mechanism: Each UAV uses an actor-critic architecture where the actor selects actions (∆xn, ∆yn, ∆zn) based on local observations, and the critic evaluates actions using joint state-action pairs from all UAVs. Experience replay and target networks stabilize training in the high-dimensional, continuous setting. This setup allows each UAV to learn how its actions affect others' returns, crucial for trajectory optimization.
- Core assumption: Continuous state and action spaces are too large for discrete RL methods; MADDPG can handle this complexity.
- Evidence anchors:
  - [section III.A] "Each UAV can acquire each other's observation information by communicating with each other, so all UAVs will acquire the global state s and the next global state s′ after one action"
  - [section III.B.1] "MADDPG algorithm includes two main components, actor network, and critic network. The actor network outputs actions according to the policy πn, and the critic network evaluates the action by calculating Q-value"
  - [corpus] No strong direct evidence; related works mostly address federated or multi-agent RL but not MADDPG specifically in UAV context.
- Break condition: If the environment changes too rapidly, the joint Q-function may become unstable, leading to poor coordination.

### Mechanism 3
- Claim: Weighted averaging of model parameters in federated aggregation reduces variance and speeds up convergence relative to pure local training.
- Mechanism: After each training round, local actor/critic parameters are aggregated by weighted averaging: θglobal = Σn ρn θn, where ρn are agent weights summing to 1. This produces a shared global model that is then personalized. Averaging smooths out noise and accelerates learning across agents.
- Core assumption: Parameter averaging is more effective than sharing gradients or raw data in this communication-constrained UAV setting.
- Evidence anchors:
  - [section III.B.2] "the aggregation server collects all the models and obtains the global model through federation averaging, and then distributes the global model to each agent"
  - [section III.B.2] "The global parameters of the actor and critic evaluation network can be calculated as: θglobal = Σn ρn θn"
  - [corpus] Weak evidence; most related papers focus on federated learning for resource allocation or communication, not specifically on federated averaging in DRL.
- Break condition: If agent data distributions are too dissimilar, averaging may harm local performance and outweigh convergence benefits.

## Foundational Learning

- Concept: Multi-agent Markov Decision Process (MDP)
  - Why needed here: The trajectory optimization problem is modeled as a multi-agent MDP where each UAV is an agent with its own observation, action, and reward, but all share a global state. This formalism allows the use of MARL algorithms like MADDPG.
  - Quick check question: What is the difference between the local observation on(t) and the global state s(t) in this model?

- Concept: Federated Learning and Parameter Aggregation
  - Why needed here: FL allows UAVs to collaboratively learn a global model without sharing raw data, addressing data privacy and communication efficiency. Parameter averaging is the core aggregation step.
  - Quick check question: Why might a simple average of all local models be suboptimal when data distributions are heterogeneous?

- Concept: Actor-Critic and Target Networks in DRL
  - Why needed here: MADDPG uses separate actor and critic networks, plus target networks and replay buffers, to stabilize learning in continuous action spaces and multi-agent settings. Target networks prevent divergence during updates.
  - Quick check question: What is the role of the target networks θ′n and µ′n in the MADDPG algorithm?

## Architecture Onboarding

- Component map:
  - Local agents (UAVs) -> MADDPG actor/critic networks, experience replay buffer, target networks
  - Central aggregator -> Weighted parameter averaging to produce global model
  - Personalization layer -> Convex combination of local and global parameters with weight α
  - Environment -> Dynamic ground users, obstacles, communication channels, energy constraints

- Critical path:
  1. Each UAV observes local environment -> selects action via actor network
  2. UAV executes action, receives reward, stores transition in replay buffer
  3. After local training, UAV uploads actor/critic parameters to aggregator
  4. Aggregator computes global parameters via weighted averaging
  5. Each UAV downloads global parameters, combines with local to form personalized model
  6. UAVs update target networks and repeat

- Design tradeoffs:
  - Communication cost vs. convergence speed: Frequent aggregation improves learning but increases communication overhead
  - Personalization weight α vs. performance: Too much local bias slows convergence; too much global bias hurts local fit
  - Number of agents vs. model aggregation: More agents improve global knowledge but may increase heterogeneity

- Failure signatures:
  - Slow or oscillating convergence -> check personalization weight α and communication frequency
  - Local performance worse than global -> check if data heterogeneity is too high or α is too large
  - Instability in MADDPG training -> check replay buffer size, target network update rate, or reward scaling

- First 3 experiments:
  1. Run PF-MADDPG with α=0.7 (local:global 7:3) and measure convergence rate and final return; compare to pure MADDPG baseline
  2. Sweep α from 0.3 to 0.9 in steps of 0.2 and record average return and variance per UAV to find optimal personalization
  3. Vary number of UAVs (e.g., 2, 4, 6) and observe effect on convergence speed and final performance to test scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal mixture weight α between local and global models for different data heterogeneity levels in PF-DRL?
- Basis in paper: [explicit] The paper states "it is difficult to find the best-mixed weight" and "An optimized α is decided by a combination of several system factors, such as the correlation between a local distribution and global distribution."
- Why unresolved: The paper identifies the importance of α but does not provide a systematic method to determine it or analyze how it should vary with different levels of data heterogeneity.
- What evidence would resolve it: Empirical studies comparing performance across different α values and data heterogeneity levels, or theoretical analysis of how α should be tuned based on data distribution characteristics.

### Open Question 2
- Question: How does the proposed PF-MADDPG algorithm perform in real-world multi-UAV scenarios with more complex environmental dynamics and communication constraints?
- Basis in paper: [inferred] The paper presents simulation results but does not address real-world implementation challenges, deployment considerations, or scalability to larger UAV fleets.
- Why unresolved: The paper focuses on controlled simulation environments without addressing practical deployment challenges or validating the approach in real-world settings.
- What evidence would resolve it: Field tests or more sophisticated simulations incorporating real-world environmental factors, communication delays, and larger-scale UAV deployments.

### Open Question 3
- Question: What is the communication overhead and convergence trade-off when scaling PF-DRL to larger numbers of UAVs or more complex task distributions?
- Basis in paper: [inferred] While the paper mentions that federated learning reduces communication overhead compared to sharing all data, it does not analyze how the system scales with increasing numbers of agents or more complex task distributions.
- Why unresolved: The paper evaluates the algorithm with 4 UAVs but does not investigate scalability limits or provide analysis of communication costs versus convergence benefits as the system scales.
- What evidence would resolve it: Systematic scaling experiments showing performance, convergence, and communication overhead as the number of UAVs and task complexity increases.

## Limitations

- Neural network architectures and hyperparameters are not fully specified, limiting reproducibility
- Optimal personalization weight α may be environment-specific and not generalizable
- Performance in real-world multi-UAV scenarios with complex dynamics and communication constraints is not validated
- Scalability to larger numbers of UAVs and analysis of communication overhead vs. convergence trade-offs are not explored

## Confidence

- Mechanism 1 (Personalized federated aggregation): Medium
- Mechanism 2 (MADDPG local training): High
- Mechanism 3 (Weighted averaging): Low-Medium

## Next Checks

1. Conduct hyperparameter sensitivity analysis across learning rates, network sizes, and communication frequencies to assess robustness and identify optimal configurations.

2. Test PF-MADDPG in environments with varying degrees of user mobility, obstacle density, and UAV numbers to evaluate scalability and adaptability.

3. Perform systematic ablation study sweeping personalization weight α from 0.3 to 0.9 in finer steps, measuring not only average return but also variance and convergence stability to confirm the claimed 7:3 optimum and identify potential overfitting.