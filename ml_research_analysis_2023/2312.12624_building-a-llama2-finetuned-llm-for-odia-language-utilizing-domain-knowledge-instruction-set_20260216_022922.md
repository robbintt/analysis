---
ver: rpa2
title: Building a Llama2-finetuned LLM for Odia Language Utilizing Domain Knowledge
  Instruction Set
arxiv_id: '2312.12624'
source_url: https://arxiv.org/abs/2312.12624
tags:
- odia
- language
- arxiv
- instruction
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of high-performance large language
  models (LLMs) for the low-resource Odia language by fine-tuning the Llama2 model
  using a custom dataset of 181K Odia instruction sets. The dataset combines translated
  English instructions (Alpaca, Dolly1, GPT Teacher) and domain-specific Odia knowledge.
---

# Building a Llama2-finetuned LLM for Odia Language Utilizing Domain Knowledge Instruction Set

## Quick Facts
- arXiv ID: 2312.12624
- Source URL: https://arxiv.org/abs/2312.12624
- Reference count: 29
- The paper fine-tunes Llama2-7b for the low-resource Odia language using a custom dataset of 181K instruction sets, achieving BLEU score of 0.6158 and ROUGE score of 0.6583.

## Executive Summary
This paper addresses the lack of high-performance large language models for the low-resource Odia language by fine-tuning the Llama2 model using a custom dataset of 181K Odia instruction sets. The dataset combines translated English instructions (Alpaca, Dolly1, GPT Teacher) and domain-specific Odia knowledge. The fine-tuned model demonstrates strong performance in readability, correctness, and formal tone while struggling with classification tasks, long-form responses, and generalization. The authors plan to release the model and dataset for research use.

## Method Summary
The authors fine-tuned Llama2-7b using a custom dataset of 181K Odia instruction sets, including translated English instructions and domain-specific knowledge. The model was trained using 7 epochs, a learning rate of 2e-4, and batch size of 128 on a single NVIDIA A-100 GPU. The fine-tuning employed LoRA attention mechanisms and 4-bit quantization. Automatic evaluation used BLEU and ROUGE metrics, complemented by human evaluation focusing on readability, correctness, and formal tone.

## Key Results
- Achieved BLEU score of 0.6158 and ROUGE score of 0.6583 on automatic evaluation
- Demonstrated strong performance in arithmetic, grammar analysis, and cultural appropriateness
- Struggles with classification tasks, long-form responses, and generalization to unseen questions

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Llama2 with a large, domain-specific Odia instruction set enables the model to better understand and generate contextually relevant content in the target language. By combining translated English instruction sets with domain knowledge data (recipes, historical places, temples, arithmetic, health, geography, art and culture, famous poets/writers, political leaders, sports, general knowledge), the model is exposed to diverse Odia language patterns and cultural nuances.

Core assumption: The translated instruction sets and domain knowledge data are of high quality and cover a wide range of topics relevant to the Odia language and culture.

### Mechanism 2
The choice of fine-tuning parameters (batch size 128, learning rate 2e-4, 7 epochs, LoRA attention) optimizes the model's efficiency and performance for the Odia language. These choices allow for efficient processing of the Odia instruction sets while enabling the model to progressively refine its understanding of the data and improve its performance.

Core assumption: The selected fine-tuning parameters are well-suited for the specific characteristics of the Odia language and the size of the instruction set dataset.

### Mechanism 3
Using both automatic evaluation metrics (BLEU, ROUGE) and human evaluation provides a comprehensive assessment of the model's quality and performance in generating Odia text. This multi-faceted evaluation approach helps identify areas of strength and weakness in the model's output by measuring readability, correctness, and adherence to formal tone.

Core assumption: The chosen evaluation metrics and human evaluators are appropriate and effective in assessing the quality of Odia language generation.

## Foundational Learning

- **Understanding linguistic and cultural characteristics of Odia language**
  - Why needed here: To effectively fine-tune the model for Odia, it's crucial to understand its linguistic features (grammar, syntax, vocabulary) and cultural context (idioms, proverbs, references).
  - Quick check question: Can you provide examples of unique linguistic features or cultural elements specific to the Odia language?

- **Familiarity with domain knowledge topics covered in instruction sets**
  - Why needed here: The model's performance heavily relies on the quality and diversity of domain knowledge data. Understanding covered topics ensures relevance and accuracy of generated content.
  - Quick check question: Can you list some key topics covered in the Odia instruction sets and explain their importance in the context of Odia language and culture?

- **Knowledge of machine translation and fine-tuning techniques**
  - Why needed here: To effectively translate English instruction sets into Odia and fine-tune Llama2, understanding machine translation principles and fine-tuning best practices is essential.
  - Quick check question: What are some common challenges and strategies in machine translation and fine-tuning large language models?

## Architecture Onboarding

- **Component map**: Llama2 base model -> Odia instruction set dataset (translated English instructions + domain knowledge) -> Fine-tuning pipeline (parameter configuration, training, evaluation) -> Automatic evaluation metrics (BLEU, ROUGE) -> Human evaluation (readability, correctness, formal tone)

- **Critical path**:
  1. Prepare the Odia instruction set dataset by translating English instructions and gathering domain knowledge data
  2. Configure fine-tuning parameters (batch size, learning rate, epochs, etc.) based on dataset characteristics
  3. Fine-tune the Llama2 model using the prepared dataset and configured parameters
  4. Evaluate the fine-tuned model using automatic metrics (BLEU, ROUGE) and human evaluation
  5. Analyze results and identify areas for improvement

- **Design tradeoffs**:
  - Dataset size vs. quality: Balancing quantity of instruction sets with quality of translations and domain knowledge
  - Fine-tuning parameters: Optimizing trade-off between model convergence and overfitting based on dataset size
  - Evaluation methods: Weighing objectivity of automatic metrics against subjective insights from human evaluation

- **Failure signatures**:
  - Poor translation quality or insufficient domain knowledge coverage may lead to inaccurate or culturally inappropriate generated content
  - Improper fine-tuning parameters may result in underfitting or overfitting, affecting model performance
  - Inappropriate evaluation metrics or human evaluators may provide misleading assessments of model quality

- **First 3 experiments**:
  1. Translate a small subset of English instruction sets into Odia using IndicTrans library and manually review translation quality
  2. Fine-tune Llama2 using a small subset of Odia instruction sets with different parameter combinations and evaluate performance impact
  3. Conduct pilot human evaluation with native Odia speakers to assess readability, correctness, and formal tone of generated content

## Open Questions the Paper Calls Out

### Open Question 1
How does the model's performance on long-form Odia text generation compare to its performance on shorter responses, and what architectural modifications could address this limitation? The paper mentions the model struggles with long answers due to token size constraints, suggesting a performance limitation in extended response generation.

### Open Question 2
What is the relative contribution of domain-specific knowledge vs. translated instruction sets to the model's overall performance in Odia language tasks? The dataset combines translated English instructions (Alpaca, Dolly1, GPT Teacher) and domain-specific Odia knowledge, but doesn't analyze their individual contributions.

### Open Question 3
How does the fine-tuned Llama2 model's performance on Odia toxicity detection compare to existing multilingual LLMs, and what makes its formal tone preference unique? The paper notes the model performs well on toxicity detection and prefers formal honorifics, but doesn't compare to other models.

## Limitations
- Dataset composition lacks transparency regarding the ratio between translated English instructions and original Odia domain knowledge content
- BLEU and ROUGE metrics are primarily designed for machine translation rather than instruction following quality assessment
- Model struggles with classification tasks, long-form responses, and generalization to unseen questions

## Confidence
- **High Confidence**: The core methodology of fine-tuning Llama2-7b with Odia instruction sets using LoRA and standard hyperparameters is well-established and reproducible
- **Medium Confidence**: The reported automatic evaluation metrics are accurate but may not fully capture instruction following quality in low-resource languages
- **Low Confidence**: Claims about cultural appropriateness and domain knowledge integration lack sufficient validation evidence due to incomplete methodology disclosure

## Next Checks
1. Conduct a detailed analysis of the 181K instruction set composition, including the ratio of translated vs. native Odia content, domain distribution, and quality assessment of translated materials
2. Develop and apply standardized instruction following benchmarks specifically designed for low-resource languages, moving beyond BLEU/ROUGE to include task completion rates and cultural relevance scoring
3. Design systematic experiments testing the model's ability to handle unseen Odia linguistic patterns, domain-specific queries, and cross-domain knowledge transfer scenarios