---
ver: rpa2
title: 'XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large
  Language Models'
arxiv_id: '2308.01263'
source_url: https://arxiv.org/abs/2308.01263
tags:
- prompts
- safe
- test
- language
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XSTest is a new test suite designed to identify exaggerated safety
  behaviors in large language models, where models refuse even safe prompts due to
  lexical overfitting. The test suite includes 200 safe prompts across ten types that
  well-calibrated models should not refuse.
---

# XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models

## Quick Facts
- **arXiv ID**: 2308.01263
- **Source URL**: https://arxiv.org/abs/2308.01263
- **Reference count**: 22
- **Primary result**: Llama2 exhibits substantial exaggerated safety behavior, fully refusing 38% of safe prompts, while GPT-4 shows much better calibration with only 6% full refusals

## Executive Summary
This paper introduces XSTest, a new test suite designed to identify exaggerated safety behaviors in large language models (LLMs), where models refuse even safe prompts due to lexical overfitting. The test suite contains 200 safe prompts across ten types that well-calibrated models should not refuse. Evaluation of Llama2 and GPT-4 reveals that Llama2 exhibits substantial exaggerated safety behavior, fully refusing 38% of prompts and partially refusing another 22%. In contrast, GPT-4 shows much better calibration with only 6% full refusals. The findings suggest that exaggerated safety is a consequence of models being overly sensitive to certain safety-related words and phrases, likely due to biases in training data where these words primarily appear in unsafe contexts.

## Method Summary
The XSTest test suite was created with 200 safe prompts across ten categories designed to trigger exaggerated safety responses. These prompts were tested on Llama2-70b-chat-hf (via Hugging Face demo interface) and GPT-4 (via OpenAI API) using temperature=0 and standard system prompts. Model completions were manually annotated by three authors into three categories: full compliance, full refusal, and partial refusal. The manual annotation process involved two annotations per entry with consensus building for disagreements, though specific criteria and resolution methods were not detailed.

## Key Results
- Llama2 fully refuses 38% of safe prompts and partially refuses another 22%, while GPT-4 fully refuses only 6%
- Exaggerated safety manifests as refusal of prompts containing safety-related words in safe contexts (e.g., "kill weeds in my garden")
- The test suite reveals that safety refusal patterns vary strongly across prompt types, suggesting lexical overfitting to specific trigger words
- Models sometimes exhibit self-contradictory behavior, first refusing then answering the same prompt

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical overfitting causes exaggerated safety behaviors by making models overly sensitive to safety-related words regardless of context
- Mechanism: During safety training, certain words (e.g., "kill", "attack") primarily appear in unsafe contexts. The model learns to associate these words with danger, causing refusal even in safe contexts where the words have benign meanings (e.g., "kill weeds", "attack king in chess")
- Core assumption: The training data distribution for safety fine-tuning is skewed toward unsafe contexts for certain trigger words
- Evidence anchors:
  - [abstract]: "Our findings suggest that exaggerated safety is a consequence of lexical overfitting, meaning that models are overly sensitive to certain words or phrases, likely because they mostly occurred in unsafe contexts during safety training"
  - [section 5]: "The common thread across model failures in all of XSTest is that models appear to be overly sensitive to certain safety-related key words and phrases"

### Mechanism 2
- Claim: The tension between helpfulness and harmlessness creates a calibration problem where models can be too safe
- Mechanism: Safety training aims to maximize harmlessness by refusing unsafe prompts, but this creates a risk of overgeneralization where even safe prompts using similar language get refused. The calibration problem arises because there's no clear boundary between being "safe enough" versus "too safe"
- Core assumption: Safety training optimizes for minimizing false negatives (unsafe content generation) without adequately considering false positives (refusing safe content)
- Evidence anchors:
  - [abstract]: "There is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful"
  - [section 5]: "Safety in practice is a calibration question. Exaggerated safety limits how useful models are, but it does not obviously cause harm itself. Therefore, it seems reasonable to tolerate some amount of exaggerated safety on safe prompts if this makes models significantly safer on unsafe prompts"

### Mechanism 3
- Claim: Hand-crafted diagnostic tests with contrastive pairs reveal specific failure modes that automated metrics miss
- Mechanism: XSTest uses carefully designed prompts that are unambiguously safe but use language similar to unsafe prompts. By testing model responses to these specific cases, the test suite can identify when models are refusing based on lexical triggers rather than contextual understanding
- Core assumption: Models fail in predictable patterns when encountering specific types of lexical-overfitting scenarios
- Evidence anchors:
  - [section 3.2]: "Each prompt is a single English sentence. Zhou et al. (2020) suggest similarity between test cases as a likely cause of performance instability in diagnostic datasets. Therefore, we aim to use diverse vocabulary and syntax within each prompt type"
  - [section 4.3]: "For each prompt type, there is at least one test prompt that Llama2 fully refuses. Beyond that, refusal rates vary strongly across prompt types, as does the format of refusals"

## Foundational Learning

- Concept: Functional testing vs. evaluation metrics
  - Why needed here: XSTest is a functional test suite that evaluates model behavior on specific inputs rather than general capabilities. Understanding this distinction is crucial for interpreting results and designing experiments
  - Quick check question: What's the difference between a functional test (which specifies desired behavior) and a diagnostic test (which identifies failure modes)?

- Concept: False positives vs. false negatives in safety contexts
  - Why needed here: The paper discusses exaggerated safety as a form of false positive (refusing safe content), which is the inverse of the more commonly studied false negative (generating unsafe content). Both need to be balanced
  - Quick check question: In safety contexts, what's the tradeoff between false positives (over-refusing) and false negatives (under-refusing)?

- Concept: Lexical vs. contextual understanding
  - Why needed here: The core problem identified is that models operate on lexical triggers rather than understanding context. Understanding this distinction helps in diagnosing why models fail
  - Quick check question: How would a model with purely lexical understanding differ from one with contextual understanding when processing "kill weeds in my garden"?

## Architecture Onboarding

- Component map: Prompt creation -> Model completion generation -> Manual annotation -> Analysis of refusal rates by type -> Identification of lexical overfitting patterns
- Critical path: Prompt creation → Model completion generation → Manual annotation → Analysis of refusal rates by type → Identification of lexical overfitting patterns
- Design tradeoffs: Hand-crafted prompts provide precise control but limited coverage vs. larger automated datasets; manual annotation ensures accuracy but doesn't scale; focusing on safety refusal behaviors means not testing other capabilities
- Failure signatures: Consistent refusal of safe prompts using specific trigger words, refusal patterns varying by prompt type, self-contradictory responses that first refuse then answer, overemphasis on individual words rather than context
- First 3 experiments:
  1. Test alternative system prompts on Llama2 to see if refusal rates change, isolating the effect of prompt engineering from model capabilities
  2. Create parallel instruction-format prompts (instead of questions) to test if response format affects refusal rates
  3. Test contrastive unsafe prompts alongside safe prompts to measure calibration and identify if models that over-refuse safe prompts also under-refuse unsafe ones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can exaggerated safety behaviors be mitigated through training methods?
- Basis in paper: [explicit] The paper suggests that lexical overfitting is a likely reason for exaggerated safety behaviors and mentions techniques like training on contrastive and adversarial examples or using regularization techniques.
- Why unresolved: The paper identifies potential techniques but does not test or evaluate their effectiveness in reducing exaggerated safety behaviors.
- What evidence would resolve it: Experiments comparing model performance on XSTest before and after applying different training methods like contrastive examples or regularization.

### Open Question 2
- Question: How does the refusal rate vary when using instruction-style prompts instead of question-style prompts?
- Basis in paper: [explicit] The paper mentions plans to create a parallel version of XSTest with instruction-style prompts and compare model performance across the two versions.
- Why unresolved: The paper has not yet conducted the comparison between question-style and instruction-style prompts.
- What evidence would resolve it: Results showing refusal rates for both prompt styles on the same set of models.

### Open Question 3
- Question: What is the relationship between exaggerated safety behaviors and a model's general helpfulness?
- Basis in paper: [inferred] The paper discusses the tension between being helpful and harmless, and how exaggerated safety behaviors limit a model's usefulness.
- Why unresolved: The paper does not quantify the impact of exaggerated safety on a model's overall helpfulness in practical applications.
- What evidence would resolve it: Studies measuring user satisfaction or task completion rates with models exhibiting varying levels of exaggerated safety.

## Limitations

- The manual annotation process introduces potential subjectivity in classifying model responses, though specific criteria and disagreement resolution methods were not detailed
- The sample size of 200 safe prompts, while providing good coverage across ten prompt types, may not capture all possible manifestations of exaggerated safety behavior
- The paper focuses specifically on safety refusal behaviors and does not test other capabilities or failure modes

## Confidence

**High Confidence**: The finding that Llama2 exhibits substantially higher exaggerated safety behavior (38% full refusal rate) compared to GPT-4 (6% full refusal rate) is well-supported by the experimental results and clear categorization of responses.

**Medium Confidence**: The mechanism of lexical overfitting causing exaggerated safety is plausible based on the observed patterns, but the exact contribution of training data bias versus model architecture limitations remains uncertain. The paper provides reasonable evidence but doesn't conduct ablation studies on training data composition.

**Low Confidence**: The claim that exaggerated safety doesn't obviously cause harm itself, as stated in section 5, is debatable. While the paper argues it only limits usefulness, refusing safe prompts could have real-world negative consequences in applications where users need help with legitimate queries involving safety-related terminology.

## Next Checks

1. **Cross-model calibration analysis**: Test whether models that exhibit high refusal rates on safe prompts also show lower rates of generating unsafe content. This would validate the calibration hypothesis and help determine if exaggerated safety is an acceptable tradeoff.

2. **Prompt engineering ablation**: Systematically test alternative system prompts and instruction formats to isolate whether refusal patterns are primarily driven by the base model's safety training versus prompt engineering effects.

3. **Training data bias investigation**: Analyze the distribution of safety-related words in the safety fine-tuning data to quantify whether trigger words indeed appear predominantly in unsafe contexts, providing stronger evidence for the lexical overfitting mechanism.