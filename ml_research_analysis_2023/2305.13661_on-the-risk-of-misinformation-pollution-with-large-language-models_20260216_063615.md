---
ver: rpa2
title: On the Risk of Misinformation Pollution with Large Language Models
arxiv_id: '2305.13661'
source_url: https://arxiv.org/abs/2305.13661
tags:
- misinformation
- question
- llms
- odqa
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the risk of misinformation pollution in
  open-domain question answering (ODQA) systems caused by large language models (LLMs)
  generating plausible-sounding but false information. The authors establish a threat
  model with scenarios including unintentional LLM hallucinations and intentional
  disinformation generation.
---

# On the Risk of Misinformation Pollution with Large Language Models

## Quick Facts
- arXiv ID: 2305.13661
- Source URL: https://arxiv.org/abs/2305.13661
- Reference count: 26
- Key outcome: LLM-generated misinformation significantly degrades ODQA performance by 14-54% in EM score

## Executive Summary
This paper investigates the risk of misinformation pollution in open-domain question answering (ODQA) systems caused by large language models (LLMs) generating plausible-sounding but false information. The authors establish a threat model with scenarios including unintentional LLM hallucinations and intentional disinformation generation. They simulate injecting LLM-generated misinformation into ODQA training and retrieval corpora and measure its impact on four QA system architectures using NQ and CovidNews datasets. Results show that misinformation significantly degrades QA performance by 14-54% in EM score, with the "REIT" attack (repeating false answers) being most effective. Three defense strategies were tested: prompting readers to be misinformation-aware (modest gains), majority voting with multiple readers (better gains), and misinformation detection (highly effective in-domain, poor out-of-domain).

## Method Summary
The study uses GPT-3.5 to generate misinformation under four settings (GENREAD, CTRL GEN, REVISE, REIT) and injects it into evidence corpora for NQ-1500 and CovidNews datasets. Four ODQA system architectures are tested: BM25+DPR for retrieval, FiD+GPT-3.5 for reading. Performance is measured using Exact Match score, with defense strategies including misinformation-aware prompting, majority voting, and detection via RoBERTa classifier.

## Key Results
- Misinformation injection causes 14-54% decline in Exact Match scores
- REIT attack (repeating false answers) is most effective across all settings
- Dense retrievers (DPR) show better resilience than sparse retrievers (BM25)
- Misinformation detection defense achieves 60% accuracy in-domain but only 36% out-of-domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated misinformation can significantly degrade ODQA performance even when injected at a very low rate.
- Mechanism: Misinformation pollutes the retrieval corpus; QA systems retrieve these misleading passages and are misled during answer generation.
- Core assumption: ODQA systems rely on retrieved passages as factual evidence without verifying their veracity.
- Evidence anchors:
  - [abstract] "Results show that misinformation significantly degrades QA performance by 14-54% in EM score"
  - [section 4.4] "when subjected to three types of deliberate misinformation pollution... both systems demonstrated a 14% to 54% decline in performance"
  - [corpus] Weak - the corpus statistics show injected passages are only 0.01-0.1% of total, but no direct proof of low rate impact is provided
- Break condition: If QA systems incorporate fact-checking or veracity assessment of retrieved passages, the misinformation impact would be reduced.

### Mechanism 2
- Claim: Misinformation pollution is more effective when targeted at sparse retrievers like BM25.
- Mechanism: Sparse retrievers rely on keyword matching, so carefully crafted misinformation can match relevant queries and dominate retrieval results.
- Core assumption: BM25 and similar sparse methods are more susceptible to keyword-based manipulation than dense retrievers.
- Evidence anchors:
  - [section B.1] "Sparse retrievers are particularly brittle to targeted misinformation. When targeting the sparse retriever BM25, REIT is able to poison more than 90% of questions"
  - [section 4.4] REIT (repeating false answers) was most effective overall
  - [corpus] Not directly addressed - no corpus-level analysis of keyword overlap
- Break condition: If dense retrievers become the default and sparse retrievers are phased out, this attack vector weakens.

### Mechanism 3
- Claim: Increasing the number of retrieved passages does not effectively dilute misinformation impact.
- Mechanism: Readers focus on a few highly relevant passages regardless of total context size, so misinformation in those key passages still dominates.
- Core assumption: ODQA readers use limited context windows and prioritize relevance over volume.
- Evidence anchors:
  - [section 5.1] "increasing the context size has minimal, if not counterproductive, effects in mitigating the performance decline caused by misinformation"
  - [section 4.4] Performance decline observed even with varying context sizes
  - [corpus] No corpus-level evidence provided about passage selection patterns
- Break condition: If readers are redesigned to process all contexts equally or use better misinformation filtering, this defense becomes more effective.

## Foundational Learning

- Concept: Exact Match (EM) score
  - Why needed here: The primary metric for evaluating ODQA performance; measures if predicted answer exactly matches reference
  - Quick check question: If a system answers "Washington" to "What is the capital of the United States?" and the reference is "Washington, D.C.", does it get credit under EM scoring?

- Concept: Dense Passage Retrieval (DPR)
  - Why needed here: One of the two retriever types tested; uses learned embeddings to capture semantic similarity
  - Quick check question: How does DPR differ from BM25 in terms of handling semantic similarity versus keyword matching?

- Concept: Prompt engineering for LLM readers
  - Why needed here: Used in defense strategies to make readers aware of potential misinformation
  - Quick check question: What are the key elements of an effective misinformation-aware prompt for an LLM reader?

## Architecture Onboarding

- Component map: Retriever (BM25/DPR) → Context selection → Reader (FiD/GPT-3.5) → Answer prediction
- Critical path: Corpus → Retriever → Reader → Output
- Design tradeoffs: Sparse retrievers (BM25) are faster but more vulnerable to keyword manipulation; dense retrievers (DPR) are more robust but computationally expensive
- Failure signatures: Significant performance drop (14-54% EM decrease) when misinformation is injected; higher vulnerability with sparse retrievers; REIT setting causes most damage
- First 3 experiments:
  1. Replicate baseline results with clean corpus using both retriever/reader combinations
  2. Inject misinformation using CTRL GEN setting and measure performance impact
  3. Test defense strategy with majority voting using k=5 readers and n=10 passages each

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of misinformation pollution vary across different types of questions and domains beyond news and encyclopedic content?
- Basis in paper: [inferred] The paper tested on NQ-1500 and CovidNews datasets, noting differences in performance but calling for examination of other domains.
- Why unresolved: The study focused on two specific datasets. Other domains like academic writing, literature, or social media were mentioned but not explored.
- What evidence would resolve it: Systematic testing across diverse question types and domains using varied QA datasets and corpora.

### Open Question 2
- Question: What are the long-term effects of repeated misinformation exposure on ODQA system performance and user trust?
- Basis in paper: [inferred] The study simulated single pollution events but didn't examine cumulative effects or user perception.
- Why unresolved: The experimental design focused on immediate performance degradation rather than longitudinal impacts or trust dynamics.
- What evidence would resolve it: Longitudinal studies tracking system performance and user behavior over multiple interactions with polluted systems.

### Open Question 3
- Question: How can misinformation detection systems be made more robust to variations in LLM-generated content across different model architectures and training regimes?
- Basis in paper: [explicit] The paper found in-domain detectors worked well but out-of-domain detectors performed poorly, highlighting the challenge of training versatile detectors.
- Why unresolved: The study only tested with GPT-3.5 and Wikipedia-style content, showing detector limitations but not exploring broader generalizability.
- What evidence would resolve it: Comprehensive evaluation of detection systems across multiple LLM architectures, training approaches, and content styles with diverse training datasets.

## Limitations

- The study uses synthetic misinformation generation rather than real-world disinformation campaigns, limiting external validity
- Defense strategies show significant domain-specific limitations, with detection accuracy dropping from 60% to 36% out-of-domain
- GPT-3.5 is used for both misinformation generation and as a reader model, potentially creating biases in attack and defense effectiveness

## Confidence

**High confidence**: The core finding that LLM-generated misinformation can significantly degrade ODQA performance (14-54% EM score decline) is well-supported by systematic experiments across multiple datasets, retriever/reader combinations, and attack strategies.

**Medium confidence**: The relative effectiveness of different attack strategies (REIT being most effective) and the observation that dense retrievers are more resilient than sparse retrievers are supported by the data, but could vary with different corpus characteristics and question distributions.

**Low confidence**: The absolute effectiveness of defense strategies, particularly the misinformation detection approach, is limited by the small sample sizes and domain-specific training.

## Next Checks

1. **Cross-architecture reader validation**: Test the same attack strategies against a different reader architecture (e.g., LLaMA or Claude) to verify that the performance degradation patterns hold across different LLM implementations.

2. **Real-world misinformation injection**: Conduct a pilot study injecting actual misinformation from known disinformation campaigns (rather than synthetic generation) to validate whether the threat model captures realistic attack patterns.

3. **Defense scalability evaluation**: Test the misinformation detection defense on a much larger, more diverse corpus (10,000+ questions across multiple domains) to better characterize its true generalization capabilities and identify failure modes.