---
ver: rpa2
title: 'Circuit Breaking: Removing Model Behaviors with Targeted Ablation'
arxiv_id: '2309.05973'
source_url: https://arxiv.org/abs/2309.05973
tags:
- ablation
- edges
- behavior
- toxic
- behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method called "circuit breaking" for removing
  undesirable behaviors from language models by ablating a small number of causal
  pathways between model components. The approach identifies and disables computational
  circuits responsible for bad behaviors by learning a binary mask over edges in the
  model's computational graph.
---

# Circuit Breaking: Removing Model Behaviors with Targeted Ablation

## Quick Facts
- arXiv ID: 2309.05973
- Source URL: https://arxiv.org/abs/2309.05973
- Reference count: 22
- The paper introduces "circuit breaking" - ablating specific computational pathways to remove undesirable behaviors from language models

## Executive Summary
This paper presents a novel method called "circuit breaking" for removing undesirable behaviors from language models by ablating specific causal pathways in their computational graph. The approach identifies and disables computational circuits responsible for bad behaviors by learning a binary mask over edges between model components. When applied to GPT-2 to reduce toxic language generation, ablating just 12 of 11.6K causal edges significantly reduced toxic output while maintaining performance on other inputs. The method shows promise as an alternative to fine-tuning for targeted behavioral modification in neural networks.

## Method Summary
The method works by first rewriting the neural network as a computational graph (a DAG), then learning a binary mask over edges that indicates which pathways to disable. The mask is optimized to increase loss on examples of bad behavior while preserving performance on other inputs. At inference time, selected edges are "ablated" by setting their weights to zero or mean values. The approach is applied to GPT-2 Small for toxicity reduction, using 10,000 OpenWebText samples for training and 100 highly toxic 4chan comments as bad behavior examples.

## Key Results
- Ablating just 12 of 11.6K causal edges significantly reduced toxic language generation
- The approach maintained performance on non-toxic inputs (measured by perplexity)
- Mean ablation showed better results than zero ablation for preserving information flow
- Minimal edge ablation outperformed task arithmetic baselines in specificity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted edge ablation removes bad behaviors by breaking specific causal pathways in the computational graph.
- Mechanism: The method learns a binary mask over edges in the model's computational graph, where ablating edges (setting weights to 0) prevents information flow through those pathways. By optimizing this mask to increase loss on bad behavior examples while preserving performance on other inputs, the approach identifies and disables the computational circuits responsible for the undesirable behavior.
- Core assumption: The bad behavior is caused by specific computational pathways that can be disabled without destroying the model's overall functionality.
- Evidence anchors: [abstract] "ablating just 12 of the 11.6K causal edges mitigates toxic generation"

### Mechanism 2
- Claim: Mean ablation better captures lack of information flow than zero ablation.
- Mechanism: When ablating an edge, mean ablation replaces the source node's value with its mean training value rather than zero. This provides a more natural "absence of information" signal to downstream nodes, as zero values might convey strong idiosyncratic signals.
- Core assumption: Mean values represent a better proxy for "no information" than zero values in neural network computations.
- Evidence anchors: [section C] "mean ablation arguably better captures a lack of information flow"

### Mechanism 3
- Claim: Limited expressivity of edge ablation prevents overfitting to the bad behavior examples.
- Mechanism: By constraining the solution space to removing edges rather than modifying weights, the method avoids memorizing bad behavior examples while still achieving high loss on them. This preserves the model's original reasoning structure.
- Core assumption: The model's existing computational pathways are sufficient for good performance, and removing some pathways can selectively harm bad behaviors without requiring weight modification.
- Evidence anchors: [section 3.4] "edge ablation limits the expressivity of the solution space"

## Foundational Learning

- Concept: Computational graphs and DAG representation of neural networks
  - Why needed here: The entire method relies on representing the model as a graph where edges can be selectively ablated
  - Quick check question: Can you explain how a transformer model would be represented as a DAG with attention heads and MLPs as nodes?

- Concept: Causal mediation and path analysis
  - Why needed here: The method uses causal pathways between components to identify which edges to ablate
  - Quick check question: What's the difference between zero ablation and mean ablation in terms of information flow?

- Concept: Binary optimization and continuous relaxation
  - Why needed here: The edge mask is learned continuously but needs to be binarized for actual ablation
  - Quick check question: How does the regularization term Î»(t) encourage the mask to find a minimal set of ablations?

## Architecture Onboarding

- Component map: Input -> Attention Heads -> MLPs -> Output
- Critical path:
  1. Rewrite model as computational graph with specified granularity
  2. Learn edge mask through continuous optimization
  3. Binarize mask and ablate edges at inference time
  4. Evaluate on toxicity and coherence metrics

- Design tradeoffs:
  - Granularity vs. interpretability: Finer granularity allows more precise edits but increases computational cost
  - Zero vs. mean ablation: Zero ablation is simpler but mean ablation may better represent information absence
  - Edge count vs. effectiveness: Fewer edges to ablate is better but may not achieve desired behavior change

- Failure signatures:
  - Toxicity reduction without coherence loss: Expected success case
  - Toxicity reduction with coherence loss: Mask found wrong edges or insufficient granularity
  - No toxicity reduction: Mask failed to identify correct pathways or bad behavior is distributed

- First 3 experiments:
  1. Run toxicity analysis on original model to establish baseline metrics
  2. Train edge mask with zero ablation and 12 edges target, evaluate toxicity reduction
  3. Train edge mask with mean ablation and 84 edges target, compare results to zero ablation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the discussion and limitations section, several important questions emerge regarding scalability, the optimal granularity for computational graph representation, and how circuit breaking compares to other model editing techniques.

## Limitations
- The method requires rewriting the model as a computational graph, introducing approximation errors
- Edge-level granularity may miss behaviors that emerge from distributed computation
- Effectiveness on GPT-2 Small may not scale to larger, more capable models
- The approach assumes bad behaviors can be isolated from good behaviors without complex weight modifications

## Confidence

**High Confidence Claims:**
- The method can identify and disable specific computational pathways in GPT-2 Small
- Ablating 12 edges significantly reduces toxic language generation on the tested dataset
- The approach preserves performance on non-toxic inputs when properly tuned

**Medium Confidence Claims:**
- Mean ablation better captures information absence than zero ablation
- The limited expressivity of edge ablation prevents overfitting to bad behavior examples
- Circuit breaking scales better than fine-tuning for targeted behavior modification

**Low Confidence Claims:**
- The approach will generalize to larger models and different types of bad behaviors
- The computational cost of edge ablation learning is practical for real-world applications
- The method can achieve similar results across different model architectures

## Next Checks

1. **Scalability Test**: Apply circuit breaking to a larger model (e.g., LLaMA-7B) with a different bad behavior (e.g., harmful instructions) to validate generalization across model sizes and behavior types.

2. **Distributed Behavior Analysis**: Systematically evaluate circuit breaking on behaviors known to be distributed across multiple pathways (e.g., general coherence vs. specific topics) to determine the method's limitations.

3. **Ablation Strategy Comparison**: Conduct a controlled experiment comparing zero ablation, mean ablation, and alternative strategies (e.g., random replacement values) on the same task to definitively establish which approach works best.