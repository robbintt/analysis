---
ver: rpa2
title: An NMF-Based Building Block for Interpretable Neural Networks With Continual
  Learning
arxiv_id: '2311.11485'
source_url: https://arxiv.org/abs/2311.11485
tags:
- training
- learning
- block
- input
- mnist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Predictive Factorized Coupling (PFC)
  block as a more interpretable alternative to the standard MLP building block in
  neural networks. The PFC block is based on non-negative matrix factorization (NMF)
  and incorporates supervised neural network training methods to achieve high predictive
  performance while retaining the desirable interpretability properties of NMF.
---

# An NMF-Based Building Block for Interpretable Neural Networks With Continual Learning

## Quick Facts
- arXiv ID: 2311.11485
- Source URL: https://arxiv.org/abs/2311.11485
- Reference count: 40
- Key outcome: Introduces PFC block combining NMF interpretability with supervised training, demonstrating advantages in continual learning and knowledge removal scenarios

## Executive Summary
This paper introduces the Predictive Factorized Coupling (PFC) block as a more interpretable alternative to the standard MLP building block in neural networks. The PFC block is based on non-negative matrix factorization (NMF) and incorporates supervised neural network training methods to achieve high predictive performance while retaining the desirable interpretability properties of NMF. The authors demonstrate the benefits of this approach in various scenarios, such as continual learning, training on non-i.i.d. data, and knowledge removal after training.

## Method Summary
The PFC block models input-output relationships as a matrix factorization V â‰ˆ W H, where W represents learnable basis vectors and H encodes inputs as non-negative combinations of these parts. The block uses an iterative NMF inference algorithm that can be unrolled into a computation graph for backpropagation training. A novel sliding learnable window (SLW) optimizer enables continual learning by constraining updates to a moving window of basis vectors, protecting previously learned knowledge while allowing new information to be incorporated. The authors demonstrate the approach across various architectures including fully-connected networks, residual networks, and factorized recurrent neural networks.

## Key Results
- PFC blocks achieve competitive accuracy on image classification tasks while providing interpretable parts-based representations
- The SLW optimizer effectively enables continual learning by preventing catastrophic forgetting
- Factorized RNNs using PFC blocks perform competitively with vanilla RNNs while offering improved interpretability
- Knowledge removal capabilities demonstrated by selectively resetting basis vectors associated with specific training batches

## Why This Works (Mechanism)

### Mechanism 1: Parts-based interpretability through NMF
The non-negativity constraint on W and H leads to sparse, interpretable parts-based representations where learned basis vectors represent additive components of the input data.

### Mechanism 2: Supervised training via unrolled NMF inference
By unrolling iterative NMF inference steps into a differentiable computation graph, the entire PFC block becomes trainable with backpropagation, enabling supervised loss optimization.

### Mechanism 3: Continual learning through sliding learnable window
The SLW optimizer maintains a moving window of learnable basis vectors, protecting previously learned knowledge while allowing new information to be incorporated, and enabling selective knowledge removal.

## Foundational Learning

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: Provides the parts-based interpretability that distinguishes PFC blocks from standard MLPs
  - Quick check question: Given a data matrix V, how would you initialize W and H for NMF, and what iterative update rules would you apply to minimize the reconstruction error?

- Concept: Backpropagation and Algorithm Unrolling
  - Why needed here: PFC blocks use unrolled NMF inference to create a differentiable computation graph
  - Quick check question: How does unrolling the NMF inference steps into a computation graph make the PFC block differentiable, and what are the memory implications of this approach?

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: The SLW optimizer is designed to address catastrophic forgetting in PFC-based models
  - Quick check question: How does the SLW optimizer prevent catastrophic forgetting, and what are the limitations of this approach compared to replay-based methods?

## Architecture Onboarding

- Component map: Input -> PFC Block (W matrix, H matrix, iterative inference) -> Output prediction
- Critical path: 1) Implement NMF inference algorithm for PFC block, 2) Unroll inference for backpropagation, 3) Integrate PFC blocks into neural network architecture, 4) Train with backpropagation (optionally using SLW optimizer)
- Design tradeoffs: Interpretability vs predictive performance, training speed vs memory usage, continual learning vs parameter efficiency
- Failure signatures: Slow convergence or divergence during NMF inference, poor predictive performance compared to MLPs, catastrophic forgetting in continual learning
- First 3 experiments: 1) Train PFC block on MNIST vs MLP baseline, 2) Implement SLW optimizer on Split MNIST, 3) Build factorized RNN with PFC blocks vs vanilla RNN

## Open Questions the Paper Calls Out

- How would the PFC block perform on larger datasets compared to small datasets?
- How does the PFC block compare to other interpretable neural network approaches?
- How does the PFC block perform on more complex multi-block architectures?

## Limitations

- Computational overhead of unrolled NMF inference during training
- Sensitivity to initialization and hyperparameter choices, particularly sliding learnable window parameters
- Assumption that data naturally decomposes into additive parts may not hold for all domains

## Confidence

High confidence in interpretability claims based on NMF theory alignment. Medium confidence in predictive performance and continual learning claims due to limited evaluation scope and hyperparameter sensitivity.

## Next Checks

1. Benchmark PFC blocks on larger-scale image datasets (ImageNet, COCO) and compare both accuracy and interpretability metrics to state-of-the-art architectures.
2. Conduct extensive hyperparameter sensitivity analysis for the sliding learnable window optimizer across different task sequences and dataset orderings.
3. Evaluate PFC-based models on non-vision tasks (natural language processing, tabular data) to assess generalizability beyond image classification.