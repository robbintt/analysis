---
ver: rpa2
title: 'CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion'
arxiv_id: '2310.11248'
source_url: https://arxiv.org/abs/2310.11248
tags:
- code
- context
- retrieval
- cross-file
- completion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CROSS CODE EVAL addresses the lack of evaluation benchmarks for
  code completion models that require cross-file context, which is common in real-world
  software development. It is a diverse and multilingual benchmark built on real-world,
  permissively-licensed repositories in Python, Java, TypeScript, and C.
---

# CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion

## Quick Facts
- arXiv ID: 2310.11248
- Source URL: https://arxiv.org/abs/2310.11248
- Reference count: 28
- Primary result: Cross-file code completion is significantly more challenging than in-file completion, with large performance gaps for state-of-the-art models

## Executive Summary
CrossCodeEval is a benchmark designed to evaluate code completion models that require cross-file context, a common scenario in real-world software development. The benchmark addresses the lack of evaluation datasets for this task by using a static-analysis-based approach to identify code examples that strictly require cross-file context for accurate completion. Built on real-world, permissively-licensed repositories in Python, Java, TypeScript, and C#, the benchmark contains 10,000 examples that are extremely challenging when relevant cross-file context is absent. Experiments demonstrate that performance improves dramatically when cross-file context is added to prompts, though even the best models struggle to achieve optimal performance, indicating the benchmark's effectiveness in assessing a model's ability to leverage extensive context.

## Method Summary
The CrossCodeEval benchmark is constructed using a static-analysis-based approach that identifies cross-file dependencies by replacing import statements with empty classes and detecting undefined name errors through compiler/static analyzers (Pylint for Python, javac for Java, tsc for TypeScript, csc for C#). The benchmark supports four programming languages and uses a retrieve-and-generate framework to evaluate models with and without cross-file context. Cross-file context is retrieved using BM25 similarity between queries constructed from in-file context and code chunks from other files in the repository. The dataset is carefully curated to minimize overlap with existing training data, eliminating memorization as a confounding factor.

## Key Results
- State-of-the-art code language models like CodeGen and StarCoder perform significantly worse on cross-file code completion when relevant cross-file context is absent
- Performance improves dramatically when cross-file context is added to prompts across all models and sizes
- Even with cross-file context, the best models struggle to achieve optimal performance, indicating the benchmark's effectiveness
- Current code retrievers improve performance but still have room for advancement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Static analysis-based approach accurately identifies cross-file context usage without requiring full semantic parsing or runtime execution.
- **Mechanism**: The method systematically replaces import statements with empty classes, then uses compiler/static analyzer errors to identify undefined names. These undefined names correspond exactly to cross-file entities used in the current file.
- **Core assumption**: Undefined name errors detected by compilers/analyzers (Pylint for Python, javac for Java, tsc for TypeScript, csc for C#) precisely map to cross-file dependencies that are essential for code completion.
- **Evidence anchors**:
  - "To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file."
  - "We leverage static analysis to catch such errors in the modified file, which precisely correspond to the names in the original file that can only be resolved by cross-file context."
  - Weak - the corpus provides related work but doesn't validate the static analysis mechanism itself.
- **Break condition**: The mechanism breaks if a compiler/analyzer fails to detect an undefined name that is actually a cross-file dependency, or if it produces false positives for entities that can be resolved without cross-file context.

### Mechanism 2
- **Claim**: Cross-file context retrieval significantly improves code completion performance by providing essential information missing from in-file context alone.
- **Mechanism**: The retrieve-and-generate (RG) framework constructs queries from in-file context, retrieves relevant code snippets from other files in the repository, and prepends them to the prompt. This provides the model with necessary cross-file information that would otherwise be unavailable.
- **Core assumption**: The retrieved cross-file context contains the specific information needed to complete the code correctly, and the model can effectively utilize this additional context when generating code.
- **Evidence anchors**:
  - "Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt."
  - "The performance improves dramatically when the cross-file context is added to the prompts across all models and sizes."
  - Weak - related work mentions retrieval approaches but doesn't directly validate the RG framework's effectiveness.
- **Break condition**: The mechanism breaks if the retrieval method fails to find relevant context, retrieves incorrect or irrelevant information, or if the model cannot effectively process the additional context within its context window limitations.

### Mechanism 3
- **Claim**: CrossCodeEval serves as an effective benchmark for both code language models and code retrievers due to its carefully constructed examples requiring cross-file context.
- **Mechanism**: The benchmark includes examples where the reference completion strictly requires cross-file context, making it impossible to achieve high performance without proper cross-file understanding. This dual nature allows evaluation of both the model's ability to use cross-file context and the retriever's ability to find relevant cross-file information.
- **Core assumption**: The examples in CrossCodeEval are constructed such that they genuinely require cross-file context for accurate completion, and performance improvements with cross-file context are not due to memorization or other confounding factors.
- **Evidence anchors**:
  - "CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and performance improves significantly when relevant context is added to the prompt."
  - "We ensure CrossCodeEval has minimal overlap with the training data from existing code LMs, eliminating the confounder of data leakage and memorization in result interpretation."
  - Moderate - related work validates the importance of cross-file context but doesn't directly evaluate CrossCodeEval as a benchmark.
- **Break condition**: The mechanism breaks if the examples can be completed accurately without cross-file context, if performance improvements are due to factors other than cross-file understanding, or if the benchmark becomes saturated by future model improvements.

## Foundational Learning

- **Concept: Static code analysis**
  - Why needed here: Understanding how compilers and static analyzers detect undefined names is crucial for grasping how the dataset identifies cross-file dependencies without runtime execution.
  - Quick check question: What types of errors would a static analyzer report when an imported class is replaced with an empty class, and why do these errors indicate cross-file dependencies?

- **Concept: Retrieval-augmented generation (RAG)**
  - Why needed here: The retrieve-and-generate framework used in experiments is a specific implementation of RAG applied to code completion, requiring understanding of how retrieval systems work with code contexts.
  - Quick check question: How does the RG framework construct queries from in-file context, and what criteria determine which code snippets are retrieved as cross-file context?

- **Concept: Code tokenization and parsing**
  - Why needed here: The evaluation metrics (code match, identifier match) and the post-processing steps require understanding how code is tokenized and parsed differently from natural language.
  - Quick check question: What are the key differences between tokenizing natural language text and tokenizing source code, and how might these differences affect evaluation metrics?

## Architecture Onboarding

- **Component map**: Repository collection -> Static analysis for cross-file detection -> Post-processing and quality control -> Example creation -> Model evaluation with and without cross-file context -> Retrieval method benchmarking -> Analysis of results and failure patterns

- **Critical path**: Repository collection → Static analysis identification of cross-file dependencies → Example generation with quality filters → Model evaluation with and without cross-file context → Retrieval method benchmarking → Analysis of results and failure patterns

- **Design tradeoffs**:
  - Using static analysis vs. semantic analysis: Static analysis is faster and doesn't require full program understanding, but may miss some cross-file dependencies that static analyzers don't detect
  - Fixed-line context window vs. adaptive retrieval: Fixed windows are simpler but may retrieve irrelevant information; adaptive methods could be more precise but more complex
  - Multiple language support vs. depth: Supporting four languages provides diversity but may limit the sophistication of language-specific optimizations

- **Failure signatures**:
  - Low improvement with cross-file context: May indicate retrieval quality issues, model inability to use context, or examples that don't truly require cross-file context
  - High variance in performance across examples: Could suggest inconsistent quality in cross-file context identification or retrieval effectiveness
  - Performance saturation: May indicate the benchmark is becoming too easy for current models or that evaluation metrics need refinement

- **First 3 experiments**:
  1. Replicate the baseline experiment: Evaluate a code LM on CrossCodeEval using only in-file context to verify the reported poor performance
  2. Test the retrieve-and-generate framework: Implement the RG approach with BM25 retrieval and evaluate performance improvement over the baseline
  3. Analyze retrieval quality: Examine which retrieved contexts actually contain relevant information by comparing identifier overlap between retrieved context and reference completions

## Open Questions the Paper Calls Out

- **Open Question 1**: How much does the performance of code completion models on CrossCodeEval improve when they are fine-tuned on examples that include cross-file context?
  - Basis in paper: The paper primarily focuses on zero-shot evaluation of code completion models, which limits their ability to fully leverage the cross-file context. The authors acknowledge this limitation and suggest that future research should investigate methods for efficiently retrieving and incorporating cross-file context into the model, including fine-tuning on examples with cross-file context.
  - Why unresolved: The paper only evaluates the models in a zero-shot setting, so the impact of fine-tuning on cross-file context examples is unknown.
  - What evidence would resolve it: Conducting experiments to fine-tune code completion models on CrossCodeEval examples that include cross-file context and comparing their performance to the zero-shot results would provide insights into the potential benefits of fine-tuning.

- **Open Question 2**: How effective are different retrieval methods, beyond the ones evaluated in the paper, in retrieving relevant cross-file context for code completion?
  - Basis in paper: The paper benchmarks various retrieval methods, including BM25, UniXCoder, and OpenAI's embedding model, but acknowledges that the current retrieval framework has limitations. The authors call for future development of better code retrievers.
  - Why unresolved: The paper only evaluates a limited set of retrieval methods, and the authors suggest that there is room for improvement in retrieval techniques.
  - What evidence would resolve it: Evaluating additional retrieval methods, such as neural retrievers or methods that incorporate code structure information, on CrossCodeEval would provide insights into their effectiveness in retrieving relevant cross-file context.

- **Open Question 3**: How does the performance of code completion models on CrossCodeEval vary across different programming languages?
  - Basis in paper: The paper evaluates models on CrossCodeEval examples in Python, Java, TypeScript, and C#, but does not provide a detailed analysis of the performance differences across these languages.
  - Why unresolved: The paper presents overall results for each language but does not delve into the specific challenges or patterns in code completion performance across different languages.
  - What evidence would resolve it: Conducting a more in-depth analysis of the model performance on CrossCodeEval examples in each language, including identifying common patterns or challenges in code completion, would provide insights into the language-specific aspects of cross-file code completion.

## Limitations
- Language-specific coverage is limited to Python, Java, TypeScript, and C#, missing many real-world languages
- Static analysis precision may have false positives and negatives, potentially missing some cross-file dependencies
- Context window limitations restrict models' ability to process extensive cross-file context effectively

## Confidence
**High Confidence**:
- The benchmark is diverse and multilingual (4 programming languages)
- The static-analysis approach effectively identifies cross-file dependencies that affect code completion
- State-of-the-art models perform significantly worse without cross-file context
- Cross-file context retrieval improves performance across all tested models

**Medium Confidence**:
- The benchmark represents real-world software development practices
- Performance improvements are primarily due to cross-file context rather than other factors
- The dataset construction methodology sufficiently eliminates data leakage concerns

**Low Confidence**:
- The benchmark will remain challenging as model capabilities advance
- Current retrieval methods are optimal for this task
- The 10-line context chunks are optimal for all use cases

## Next Checks
- Test the static-analysis approach's precision by manually examining a sample of examples where the method identified cross-file dependencies, comparing these against ground truth cross-file usage patterns in the original repositories
- Evaluate whether the performance improvements from cross-file context are consistent across different code complexity levels by stratifying examples based on AST complexity or cyclomatic complexity and comparing performance distributions
- Assess the sensitivity of retrieval performance to chunk size by running experiments with varying context chunk lengths (5, 10, 15, 20 lines) and measuring the trade-off between retrieval quality and model context window utilization