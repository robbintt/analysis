---
ver: rpa2
title: 'Explorers at #SMM4H 2023: Enhancing BERT for Health Applications through Knowledge
  and Model Fusion'
arxiv_id: '2312.10652'
source_url: https://arxiv.org/abs/2312.10652
tags:
- task
- data
- tasks
- continual
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses challenges in health-related natural language
  processing on social media, including category imbalance, domain bias, and text
  noise, across three tasks: COVID-19 diagnosis detection in tweets, social anxiety
  disorder diagnosis detection in Reddit posts, and COVID-19 symptom extraction from
  Spanish tweets. The authors propose a pipeline that includes data preprocessing
  (emoji and username normalization), continual pretraining to adapt language models
  to domain-specific data, and model fine-tuning with optimization strategies such
  as focal loss, cross-validation voting, parameter averaging, and multi-model fusion.'
---

# Explorers at #SMM4H 2023: Enhancing BERT for Health Applications through Knowledge and Model Fusion

## Quick Facts
- **arXiv ID**: 2312.10652
- **Source URL**: https://arxiv.org/abs/2312.10652
- **Reference count**: 11
- **Primary result**: First place in Task 3 (F1-score of 0.94) and exceeded median in Task 1 for health-related NLP on social media

## Executive Summary
This paper addresses health-related natural language processing challenges on social media platforms, including category imbalance, domain bias, and text noise. The authors propose a comprehensive pipeline combining data preprocessing, continual pretraining, and model fine-tuning with optimization strategies to tackle three tasks: COVID-19 diagnosis detection in tweets, social anxiety disorder diagnosis detection in Reddit posts, and COVID-19 symptom extraction from Spanish tweets. Their approach achieved first place in Task 3 and outperformed the median in Task 1, demonstrating the effectiveness of domain adaptation and model fusion techniques.

## Method Summary
The authors employ a pipeline that begins with data preprocessing to normalize emojis, usernames, and hashtags. They then perform continual pretraining on domain-specific unlabeled data to adapt pre-trained language models to the social media health domain. For fine-tuning, they use focal loss to address class imbalance, implement 5-fold cross-validation voting, apply parameter averaging, and employ multi-model fusion voting. For the Named Entity Recognition task, they utilize the W2NER model architecture. The method combines domain adaptation, class imbalance mitigation, and ensemble learning to enhance model performance on noisy social media text.

## Key Results
- Achieved first place in Task 3 (COVID-19 symptom extraction from Spanish tweets) with F1-score of 0.94
- Exceeded median performance in Task 1 (COVID-19 diagnosis detection in English tweets)
- Demonstrated that continual pretraining improves performance on domain-specific tasks
- Showed that multi-model fusion voting enhances model generalization and achieves complementary advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pretraining improves model performance on domain-specific tasks.
- Mechanism: The authors continue pretraining pre-trained language models on unlabeled data from the same domain (e.g., COVID-19 tweets) to make the models more applicable to specific industries or domains.
- Core assumption: Pre-trained models trained on general data lack deep understanding of task-specific data.
- Evidence anchors:
  - [abstract] "continual pre-training and fine-tuned optimization strategies"
  - [section] "To solve the domain bias problem, we continue with domain-adaptive pretraining using unlabeled data from all the related tasks to optimize the model for task-specific data"
  - [corpus] Weak evidence; no direct citations found in related papers
- Break condition: Continual pretraining leads to negative impacts, such as overfitting or domain mismatch.

### Mechanism 2
- Claim: Focal loss effectively addresses class imbalance in classification tasks.
- Mechanism: Focal loss introduces a modifying factor to the traditional cross-entropy loss function, emphasizing hard samples and adjusting sample weights during training.
- Core assumption: Classification tasks exhibit noticeable category imbalance.
- Evidence anchors:
  - [abstract] "We also propose tricks to mitigate the challenges existing in dataset, including category imbalance"
  - [section] "To address the problem, we utilize focal loss to adjust sample weights during training, emphasizing hard samples"
  - [corpus] Weak evidence; no direct citations found in related papers
- Break condition: The class imbalance is not significant enough to warrant focal loss, or the modification factor is not optimal.

### Mechanism 3
- Claim: Multi-model fusion voting improves model generalization and achieves complementary advantages.
- Mechanism: Different models' predictions for the same task are combined through Mean-Pooling to get the final prediction results.
- Core assumption: Different models have complementary strengths and weaknesses.
- Evidence anchors:
  - [abstract] "we utilize the model architecture named W2NER that effectively enhances the model generalization ability"
  - [section] "In order to further improve the model generalization ability and achieve the complementary advantages of different models under the unified task, we use multi-model fusion voting"
  - [corpus] Weak evidence; no direct citations found in related papers
- Break condition: The models used for fusion are too similar, resulting in limited complementary advantages.

## Foundational Learning

- Concept: Domain adaptation
  - Why needed here: Social media data for health applications contains informal, colloquial expressions, misspellings, noise, and multilingual posts, which differ from general domain data.
  - Quick check question: How does domain adaptation help in improving the performance of pre-trained language models on task-specific data?

- Concept: Class imbalance
  - Why needed here: Classification tasks in the paper exhibit noticeable category imbalance, with positive examples being a small proportion of the total.
  - Quick check question: What is the impact of class imbalance on model performance, and how does focal loss address this issue?

- Concept: Model ensemble
  - Why needed here: Multi-model fusion voting is used to improve model generalization and achieve complementary advantages of different models.
  - Quick check question: How does model ensemble contribute to better performance compared to using a single model?

## Architecture Onboarding

- Component map:
  - Data preprocessing (emoji and username normalization, hashtag separation) -> Continual pretraining (domain-adaptive pretraining using unlabeled data) -> Model fine-tuning (focal loss, cross-validation voting, parameter averaging, multi-model fusion) -> W2NER model architecture (for NER task)

- Critical path:
  1. Preprocess the data to remove noise and enhance textual semantics.
  2. Perform continual pretraining on the preprocessed data to adapt the models to the domain.
  3. Fine-tune the models using the training data and optimization strategies (focal loss, cross-validation voting, parameter averaging, multi-model fusion).
  4. Evaluate the models on the validation and test sets.

- Design tradeoffs:
  - Balancing the benefits of continual pretraining with the risk of overfitting or domain mismatch.
  - Choosing the optimal modification factor for focal loss to address class imbalance.
  - Selecting diverse models for fusion to achieve complementary advantages.

- Failure signatures:
  - Poor performance on the validation set, indicating overfitting or underfitting.
  - Negative impacts of continual pretraining, such as domain mismatch or overfitting.
  - Ineffective handling of class imbalance, leading to biased predictions.

- First 3 experiments:
  1. Implement data preprocessing and evaluate its impact on model performance.
  2. Perform continual pretraining with different learning rates and epochs to find the optimal configuration.
  3. Fine-tune the models using focal loss and cross-validation voting, and compare the performance with and without these optimization strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of epochs for continual pretraining across different tasks and domains?
- Basis in paper: [explicit] The paper mentions continual pretraining was used with 20 epochs for classification tasks and 30 epochs for NER tasks, but notes that "the effects of continual pretraining are quite unstable, sometimes leading to positive impacts and other times resulting in negative influences."
- Why unresolved: The paper doesn't systematically explore the relationship between pretraining duration and task performance, nor does it explain why different tasks require different epoch counts.
- What evidence would resolve it: A systematic study varying the number of pretraining epochs (e.g., 5, 10, 20, 30, 40) across multiple tasks while measuring performance on validation sets would identify optimal pretraining durations.

### Open Question 2
- Question: How does the proposed preprocessing strategy (emoji replacement, username normalization) affect model performance compared to alternative approaches?
- Basis in paper: [explicit] The paper describes their preprocessing strategy but doesn't compare it to alternatives like removing emojis entirely or using different normalization techniques.
- Why unresolved: The authors only report results using their specific preprocessing approach without ablation studies or comparisons to other methods.
- What evidence would resolve it: Controlled experiments comparing model performance using different preprocessing strategies (e.g., emoji removal vs. replacement, different username normalization approaches) on the same datasets would determine the optimal preprocessing method.

### Open Question 3
- Question: Why did the model underperform in Task 4 compared to Task 1 despite similar methodologies?
- Basis in paper: [explicit] The paper states "The results on Task 4 are below the median" while their Task 1 results "exceed the mean by 0.06," despite using similar methods for both tasks.
- Why unresolved: The authors don't analyze or explain the performance discrepancy between these two similar classification tasks.
- What evidence would resolve it: A detailed error analysis comparing model predictions on Task 1 and Task 4 datasets, examining differences in data characteristics (e.g., text length, vocabulary overlap, symptom expression patterns) would explain the performance gap.

## Limitations
- Absence of hyperparameter sensitivity analysis for the continual pretraining phase
- Unclear details about the W2NER architecture implementation
- Lack of comparison with simpler baseline approaches that might achieve comparable results
- No discussion of potential biases in the social media data or generalizability to other health-related domains

## Confidence

- Continual pretraining effectiveness: Medium
  - Limited empirical validation compared to standard fine-tuning
  - First-place finish in Task 3 provides supporting evidence

- Focal loss implementation: Medium
  - Claims address class imbalance but lacks ablation studies
  - No direct citations found in related papers

- Multi-model fusion approach: Medium
  - Limited discussion of model selection and evaluation
  - No direct citations found in related papers

## Next Checks

1. **Ablation study on continual pretraining**: Run experiments comparing model performance with and without continual pretraining using identical fine-tuning procedures to isolate its contribution to the final results.

2. **Hyperparameter optimization analysis**: Systematically vary key hyperparameters (learning rates, focal loss gamma parameter, ensemble weights) to understand their impact on performance and identify optimal configurations.

3. **Baseline comparison**: Implement and evaluate simpler approaches such as standard fine-tuning without continual pretraining or single-model approaches without ensemble voting to establish whether the complexity of the proposed pipeline is justified by performance gains.