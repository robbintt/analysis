---
ver: rpa2
title: Gram-Schmidt Methods for Unsupervised Feature Extraction and Selection
arxiv_id: '2311.09386'
source_url: https://arxiv.org/abs/2311.09386
tags:
- features
- which
- feature
- data
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a probabilistic Gram-Schmidt (PGS) process
  for unsupervised feature extraction and selection in the presence of nonlinear dependencies.
  The PGS process generalizes the classical Gram-Schmidt method to function spaces,
  enabling the detection and removal of both linear and nonlinear redundancies in
  data.
---

# Gram-Schmidt Methods for Unsupervised Feature Extraction and Selection

## Quick Facts
- arXiv ID: 2311.09386
- Source URL: https://arxiv.org/abs/2311.09386
- Reference count: 29
- Primary result: GFR achieves residual variance comparable to PCA using 2-10 times fewer features with up to 10% classification accuracy improvements

## Executive Summary
This paper introduces probabilistic Gram-Schmidt (PGS) processes for unsupervised feature extraction and selection that can detect and remove both linear and nonlinear redundancies in data. The methods generalize classical Gram-Schmidt orthogonalization to function spaces, enabling the identification of independent features even when nonlinear dependencies exist. Two main algorithms are proposed: Gram-Schmidt Component Analysis (GCA) for removing redundancy from known principal components, and Gram-Schmidt Functional Reduction (GFR) for extracting new informative directions by analyzing alternative covariance matrices.

## Method Summary
The paper presents two algorithms based on probabilistic Gram-Schmidt orthogonalization in function spaces. GCA iteratively removes nonlinear dependencies from principal components by projecting onto orthogonalized functions from a chosen family F, under a variance-reduction assumption. GFR directly identifies new high-variance directions that reduce conditional entropy by computing and analyzing alternative covariance matrices Σj at each iteration. Both methods maintain orthonormal feature sets and provide theoretical guarantees for entropy reduction and variance extraction.

## Key Results
- GFR achieves residual variance comparable to PCA using 2-10 times fewer features
- Classification accuracy improvements of up to 10% over PCA in benchmark datasets
- Linear computational complexity and significantly reduced complexity compared to Fourier-based feature selection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PGS generalizes Gram-Schmidt to function spaces, allowing linear projections to detect and remove nonlinear redundancies
- Mechanism: At each iteration j, zi = x⊺vj maximizes variance in modified data distribution dj obtained by subtracting projections onto orthogonalized functions in F whose variables were already specified
- Core assumption: Nonlinear dependencies among principal components lie within linear span of chosen function family F
- Evidence anchors:
  - [abstract] "PGS process generalizes the classical Gram-Schmidt method to function spaces, enabling the detection and removal of both linear and nonlinear redundancies in data"
  - [section] "Algorithm 1 receives an already-orthogonalized function family ˆFj−1 in variables z1, . . . , zj−1... It also receives functions Fj \ Fj−1, which depend on z1, . . . , zj as well as on a newly-specified variable zj"
- Break condition: If nonlinear dependencies don't lie within span(F), PGS fails to remove them

### Mechanism 2
- Claim: GFR uses alternative covariance matrices to directly identify new high-variance directions that reduce conditional entropy
- Mechanism: GFR computes Σj = E[djd⊺j] at each step and selects vj as largest eigenvector of Σj, extracting directions that maximize variance in residual distribution
- Core assumption: Largest eigenvectors of Σj correspond to informative directions that reduce conditional entropy
- Evidence anchors:
  - [abstract] "GFR reduces conditional entropy by analyzing alternative covariance matrices"
  - [section] "In each iteration j, the algorithm begins by identifying the largest eigenvector vj of Σj... If the variance of this component is less than some threshold ϵ2, the algorithm stops"
- Break condition: Poor threshold choice causes early stopping or overfitting

### Mechanism 3
- Claim: PGS ensures extracted features are orthonormal and allows recursive covariance computation
- Mechanism: By maintaining orthonormal ˆFj at each step, PGS guarantees mutual orthogonality of v1,...,vm, enabling recursive computation of Σj via Lemma 2(a)
- Core assumption: Orthonormality of extracted features is preserved across iterations
- Evidence anchors:
  - [section] "Proof of Lemma 2. (b). By definition, we have that vj is the largest eigenvector of Σj for every j ∈ [m]... It follows from Lemma 2.(a) that Σj = Σi −Pˆf ∈ ˆFj−1\ ˆFi−1 E[ ˆf x]E[x⊺ ˆf]..."
- Break condition: Numerical instability breaks orthonormality

## Foundational Learning

- Concept: Inner product in function spaces
  - Why needed here: PGS replaces vector inner products with E[f f′] over data distribution; essential for understanding orthogonalization
  - Quick check question: What is the inner product between two functions f and g under PGS?

- Concept: Conditional entropy and its reduction
  - Why needed here: GFR's theoretical guarantee relies on bounding H(x|z); necessary for interpreting results
  - Quick check question: How does selecting high-variance eigenvectors reduce H(x|z)?

- Concept: Principal component analysis and eigenvalue decomposition
  - Why needed here: Both GCA and GFR build on PCA concepts; essential for understanding eigenvectors/eigenvalues
  - Quick check question: What does the (m+1)-th eigenvalue represent in PCA residual variance?

## Architecture Onboarding

- Component map: PGS (orthogonalization core) -> GCA (removes redundancy from known PCs) / GFR (extracts new informative directions) -> covariance matrix update -> stopping condition
- Critical path: Choose F -> Initialize Σ1 -> Iterate: select vj -> orthogonalize -> update Σj+1 -> check stopping condition
- Design tradeoffs: Larger |F| increases expressiveness but quadratic complexity; smaller F reduces complexity but may miss dependencies
- Failure signatures: Numerical instability in covariance estimation, early stopping due to poor threshold choice, non-orthonormal features due to floating-point errors
- First 3 experiments:
  1. Generate synthetic data x = r1y1 + r2y2 + r3y1y2; verify GCA recovers y1,y2 and skips y1y2
  2. Apply GFR with multilinear polynomials degree 2 on USPS dataset; measure residual variance vs PCA
  3. Use GFR on synthetic data with linear labels; compare classification accuracy vs PCA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions can the variance reduction assumption in Theorem 1 be relaxed or eliminated while still ensuring correct feature extraction?
- Basis in paper: [explicit] Theorem 1 assumes Var(̃hj(y)) < min{Var(yi)} for nonlinear redundancies; the paper notes that without this assumption, some redundant dimensions might still be extracted but could provide meaningful entropy reduction
- Why unresolved: The paper does not provide formal characterization of when or how much redundancy extraction is still possible without the variance reduction assumption
- What evidence would resolve it: Empirical or theoretical results showing GCA performance when variance reduction assumption is violated, particularly in terms of entropy reduction or classification accuracy

### Open Question 2
- Question: Can the dependence of GFR on discrete alphabets in Theorem 2 be extended to continuous distributions, and if so, what form would entropy bounds take?
- Basis in paper: [explicit] Theorem 2 assumes x is over discrete domain X^d and provides entropy bounds in terms of alphabet size and threshold parameter; the paper notes this dependence seems artificial
- Why unresolved: Proof relies on discrete entropy properties that don't directly generalize to continuous distributions
- What evidence would resolve it: Derivation of analogous entropy bounds for continuous distributions with experimental validation

### Open Question 3
- Question: How does choice of function family F affect GCA and GFR performance across different data domains, and are there domain-specific guidelines for selecting F?
- Basis in paper: [explicit] The paper discusses F should be chosen based on prior belief about redundancy structure and mentions Fourier analysis insights for Boolean variables
- Why unresolved: The paper only provides general advice and examples without systematically studying how different F structures impact performance across diverse data types
- What evidence would resolve it: Comparative experiments applying GCA and GFR with various function families tailored to specific domains (e.g., wavelet bases for images, n-grams for text)

## Limitations
- Performance heavily depends on choice of function family F, which must capture actual data dependencies
- Theoretical guarantees assume discrete alphabets or specific variance conditions that may not hold in practice
- Limited comparative analysis against established methods makes performance claims difficult to verify

## Confidence
- **High**: Linear Gram-Schmidt generalization to function spaces and recursive covariance computation
- **Medium**: Entropy reduction bounds and theoretical guarantees for GFR
- **Low**: Performance claims relative to established methods (PCA, ICA, LPP)

## Next Checks
1. **Robustness to Function Family Choice**: Systematically evaluate GFR performance across multiple function families (polynomials, Fourier bases, kernel functions) on synthetic datasets with known dependency structures
2. **Scaling Analysis**: Measure computational complexity and memory requirements of GCA/GFR as dataset dimensionality and sample size increase, comparing against PCA and ICA
3. **Generalization Across Domains**: Apply GFR to diverse real-world datasets beyond current benchmarks (e.g., genomics, finance, natural language) to assess whether variance reduction and classification accuracy improvements generalize across application domains