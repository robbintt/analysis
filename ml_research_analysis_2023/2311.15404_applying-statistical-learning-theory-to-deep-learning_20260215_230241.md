---
ver: rpa2
title: Applying statistical learning theory to deep learning
arxiv_id: '2311.15404'
source_url: https://arxiv.org/abs/2311.15404
tags:
- learning
- descent
- gradient
- function
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The lectures examine how different architectures lead to inductive
  bias when trained using gradient-based methods. The key finding is that implicit
  bias from optimization algorithms like gradient descent plays a crucial role in
  generalization, particularly when models are over-parameterized.
---

# Applying statistical learning theory to deep learning

## Quick Facts
- arXiv ID: 2311.15404
- Source URL: https://arxiv.org/abs/2311.15404
- Authors:
- Reference count: 34
- Key outcome: Analysis of how initialization scale and depth affect implicit bias in gradient-based optimization, revealing transition between kernel and rich regimes

## Executive Summary
This work examines how different neural network architectures lead to implicit bias when trained with gradient-based methods. The authors analyze the transition between kernel (lazy) and rich regimes, showing that initialization scale and model depth critically affect the learning dynamics. They demonstrate that small initialization scales induce sparsity-inducing regularization (ℓ1-like) while large scales lead to kernel behavior with ℓ2 bias. The analysis extends to various models including linear diagonal networks, matrix factorization, and deep networks, revealing how the implicit bias shifts based on architectural properties and optimization hyperparameters.

## Method Summary
The authors employ theoretical analysis of gradient flow dynamics on simplified models to characterize implicit bias in overparameterized learning. They analyze linear diagonal networks, matrix factorization problems, and deep diagonal networks, studying how the Jacobian of the model maps parameter updates to function space. Through this lens, they examine how initialization scale α and model depth D govern the transition between kernel and rich regimes. The methodology involves computing the metric tensor in function space induced by the model's Jacobian and analyzing the resulting optimization dynamics. They also explore mirror descent as a generalization of gradient descent that connects parameter space geometry to function space geometry.

## Key Results
- Implicit bias transitions from ℓ2 regularization in kernel regime to sparsity-inducing ℓ1-like regularization in rich regime
- Initialization scale α controls regime: small α induces rich regime with sparsity, large α induces kernel regime
- Model depth D affects transition rate: deeper networks transition faster to rich regime
- For logistic loss, gradient descent converges to max-margin separator regardless of initialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The implicit bias from gradient descent is governed by the geometry in function space, not parameter space, captured by the metric tensor induced by the model's Jacobian.
- **Mechanism**: The Jacobian ∇F(w) maps parameter updates into function space. Mirror descent updates in parameter space correspond to gradient flow in function space with metric tensor ρ = (∇F(w)⊤∇F(w))⁻¹. This geometry controls which functions are "closer" during optimization.
- **Core assumption**: The model's Jacobian is invertible or well-conditioned enough to define a meaningful geometry in function space.
- **Evidence anchors**: [abstract] "They demonstrate a transition between kernel (lazy) and rich regimes, with the implicit bias shifting from ℓ2 regularization in the kernel regime to sparsity-inducing regularization (like ℓ1) in the rich regime." [section] "Thus the dynamics in parameter space is a gradient flow according to the metric tensor defined by the tangent kernel at each time step."
- **Break condition**: If the Jacobian is rank-deficient or ill-conditioned, the geometry becomes undefined or dominated by numerical noise, breaking the implicit bias mechanism.

### Mechanism 2
- **Claim**: The transition between kernel and rich regimes is governed by the initialization scale α and model architecture depth D.
- **Mechanism**: For small initialization α, the dynamics follow the rich regime with sparsity-inducing bias (ℓ1-like). For large α, the dynamics converge to the kernel regime with ℓ2 bias. Depth D affects the rate of transition - deeper networks transition faster to the rich regime.
- **Core assumption**: The model is homogeneous of order D, and gradient flow converges to a zero-error solution.
- **Evidence anchors**: [abstract] "Through analysis of various models including linear diagonal networks, matrix factorization, and deep networks, the authors show how the scale of initialization, depth, and architecture affect the learning regime." [section] "We thus have a transition from kernel inductive bias to a sparsity inducing inductive bias."
- **Break condition**: If the initialization scale α is not properly controlled or the model is not homogeneous, the transition mechanism breaks down.

### Mechanism 3
- **Claim**: For logistic loss, gradient descent converges to the max-margin separator regardless of initialization, while for squared loss, the implicit bias depends on initialization scale.
- **Mechanism**: Logistic loss creates a margin condition that gradient descent implicitly optimizes, leading to the max-margin solution. Squared loss follows the kernel regime for large initialization and rich regime for small initialization.
- **Core assumption**: The data is linearly separable and the step size is small enough to ensure convergence.
- **Evidence anchors**: [abstract] "For example, in 2-layer linear diagonal networks, they show that large initialization scales lead to kernel behavior while small scales induce sparsity." [section] "For logistic loss, we can get a similar statement about infinite scale leads to the kernel regime, but the problem is that we can only get it for finite time."
- **Break condition**: If the data is not linearly separable or the step size is too large, the convergence to max-margin or kernel regime breaks down.

## Foundational Learning

- **Concept**: Convex optimization and gradient descent convergence
  - Why needed here: The analysis of implicit bias relies on understanding how gradient descent behaves on convex problems, especially the relationship between parameter space updates and function space geometry.
  - Quick check question: What is the convergence rate of gradient descent on a smooth, strongly convex function?

- **Concept**: Statistical learning theory and generalization bounds
  - Why needed here: The framework for understanding when implicit bias leads to good generalization relies on statistical learning theory, including concepts like Rademacher complexity and uniform convergence.
  - Quick check question: What is the relationship between VC dimension and generalization error in binary classification?

- **Concept**: Mirror descent and Bregman divergences
  - Why needed here: Mirror descent provides the general framework for understanding how different geometries in parameter space translate to different geometries in function space, which is crucial for analyzing implicit bias.
  - Quick check question: How does the choice of Bregman divergence affect the implicit bias of mirror descent?

## Architecture Onboarding

- **Component map**: Model F(w) mapping parameters to functions -> Jacobian ∇F(w) for geometry analysis -> Metric tensor ρ = (∇F(w)⊤∇F(w))⁻¹ -> Gradient flow dynamics in function space -> Implicit bias characterization

- **Critical path**:
  1. Define model F(w) mapping parameters to functions
  2. Compute Jacobian ∇F(w) to understand geometry in function space
  3. Analyze gradient flow dynamics using the metric tensor ρ = (∇F(w)⊤∇F(w))⁻¹
  4. Determine implicit bias by studying the optimization problem in function space
  5. Validate generalization through statistical learning theory

- **Design tradeoffs**:
  - Homogeneous vs non-homogeneous models: Homogeneous models allow cleaner analysis of implicit bias
  - Initialization scale: Large scale leads to kernel regime, small scale to rich regime
  - Model depth: Deeper models transition faster to rich regime

- **Failure signatures**:
  - Ill-conditioned Jacobian leading to unstable geometry in function space
  - Non-convergence of gradient flow to zero-error solution
  - Breakdown of the kernel-rich regime transition due to improper initialization

- **First 3 experiments**:
  1. Implement gradient flow on a 2-layer linear diagonal network with varying initialization scales and observe the transition between kernel and rich regimes.
  2. Compare the implicit bias of logistic loss vs squared loss on a linearly separable dataset and verify convergence to max-margin vs kernel regime.
  3. Study the effect of model depth on the implicit bias by implementing gradient flow on deep diagonal linear networks with varying depths and initialization scales.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between implicit regularization and explicit regularization in deep learning?
- Basis in paper: [explicit] "Is this equivalent to explicity regularization using the ℓ2 norm?" and "Comparing explicit and implicit regularization"
- Why unresolved: The paper shows that implicit regularization through gradient descent can lead to different solutions than explicit regularization, but the exact relationship and conditions under which they align or differ remain unclear.
- What evidence would resolve it: Theoretical analysis showing conditions under which implicit regularization matches explicit regularization, and empirical studies comparing the solutions obtained by each approach across different architectures and loss functions.

### Open Question 2
- Question: How does depth affect the implicit bias in deep linear networks?
- Basis in paper: [explicit] "Deep diagonal networks" section discussing the effect of depth on implicit bias
- Why unresolved: The paper shows that deeper networks require polynomially decreasing initialization scale to reach the rich regime, but the exact mechanism and implications of this depth dependence remain unclear.
- What evidence would resolve it: Analytical studies of the implicit bias in deeper networks, and empirical investigations of how depth affects generalization performance and feature learning.

### Open Question 3
- Question: What is the impact of non-commutativity in matrix factorization problems?
- Basis in paper: [explicit] "Matrix Factorization Setting and Commutativity" section discussing the role of commutativity
- Why unresolved: The paper shows that non-commutativity leads to different dynamics and geometry in matrix factorization, but the exact implications for learning and generalization remain unclear.
- What evidence would resolve it: Theoretical analysis of the implicit bias in non-commutative matrix factorization, and empirical studies comparing the solutions obtained by gradient descent in commutative and non-commutative cases.

## Limitations

- The analysis relies on idealized conditions including perfect gradient flow convergence and infinite data scenarios
- The transition between kernel and rich regimes depends critically on precise initialization scaling, which may be sensitive to numerical implementation details in practice
- The analysis of implicit bias through function space geometry assumes the Jacobian remains well-conditioned throughout training, which may not hold for complex deep networks with non-linear activations

## Confidence

- **High confidence**: The connection between initialization scale and implicit bias (kernel vs rich regime) is well-established theoretically with clear analytical results for linear diagonal networks
- **Medium confidence**: The extension of these principles to deeper networks and matrix factorization models is supported by theoretical analysis but may be sensitive to architectural details not fully captured in the simplified models
- **Low confidence**: The claims about generalization bounds and practical implications for deep learning architectures extend beyond the theoretical framework and require empirical validation

## Next Checks

1. **Numerical verification of regime transition**: Implement gradient flow on 2-layer linear diagonal networks with varying initialization scales (α) and verify the predicted transition between kernel and rich regimes through quantitative measures of weight sparsity and function space geometry.

2. **Jacobian conditioning analysis**: Track the condition number of the Jacobian ∇F(w) throughout training across different initialization scales and depths to validate the assumption that function space geometry remains well-defined.

3. **Empirical generalization testing**: Compare generalization performance of models trained with small vs large initialization scales on real datasets to verify that the predicted implicit biases (ℓ1 vs ℓ2 regularization) translate to meaningful differences in practical performance.