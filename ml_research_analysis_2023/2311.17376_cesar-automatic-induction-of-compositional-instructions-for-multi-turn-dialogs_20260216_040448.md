---
ver: rpa2
title: 'CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs'
arxiv_id: '2311.17376'
source_url: https://arxiv.org/abs/2311.17376
tags:
- generation
- tasks
- dialog
- task
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CESAR introduces a novel framework for automatically inducing compositional
  instructions in multi-turn dialog tasks. It modularizes dialog components and programmatically
  combines atomic tasks to create complex compositional tasks without manual effort.
---

# CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs

## Quick Facts
- arXiv ID: 2311.17376
- Source URL: https://arxiv.org/abs/2311.17376
- Reference count: 40
- Key outcome: CESAR framework automatically induces compositional instructions for multi-turn dialogs, achieving up to 28% improvement on compositional tasks while enhancing atomic task performance.

## Executive Summary
CESAR introduces a novel framework for automatically generating compositional instructions in multi-turn dialog tasks. The framework modularizes dialog components and programmatically combines atomic tasks to create complex compositional tasks without manual effort. By defining structured composition rules, CESAR can generate new task combinations that improve both compositional and atomic task performance. Experiments demonstrate that models trained on CESAR-generated tasks significantly outperform baselines, particularly on unseen task compositions, highlighting the framework's scalability and effectiveness.

## Method Summary
CESAR treats dialog tasks as structured prompts with distinct components (context, state, evidence, actions, response) and defines composition rules for combining atomic tasks into more complex ones. The framework categorizes tasks as atomic (0-D or 1-D) or compositional (2-D or higher) and uses these rules to programmatically generate new tasks. Models are trained using FLAN-xxl on InstructDial++, a benchmark containing 86 atomic and 68 composite tasks across 63 datasets. The approach aims to bridge the gap between public and proprietary dialog models by enabling automatic generation of complex compositional instructions.

## Key Results
- CESAR significantly improves performance on compositional tasks, outperforming baselines by up to 28%
- Including compositional tasks in training data enhances atomic task performance as well
- The framework demonstrates strong generalization to unseen task compositions, highlighting its scalability

## Why This Works (Mechanism)

### Mechanism 1
CESAR enables automatic induction of compositional tasks without manual effort by modularizing dialog components and programmatically combining atomic tasks. The framework treats dialog tasks as structured prompts with distinct components that can be merged using composition rules. Core assumption: modularization preserves semantic coherence when components are combined. Evidence: Abstract states CESAR "programmatically combines atomic tasks to create complex compositional tasks without manual effort."

### Mechanism 2
Including compositional tasks in training data improves both compositional and atomic task performance. By exposing the model to complex task compositions during training, it learns to handle multiple constraints simultaneously, which transfers to better performance on both composite and individual atomic tasks. Core assumption: compositional tasks provide useful signal that generalizes to atomic tasks. Evidence: Experiments show compositional models outperform baseline models regardless of training size.

### Mechanism 3
CESAR framework demonstrates strong generalization to unseen task compositions, highlighting its scalability. The structured approach to task representation allows the model to handle novel combinations of constraints even when specific compositions were not seen during training. Core assumption: the model learns transferable skills from seen compositions that apply to novel combinations. Evidence: Framework "demonstrates strong generalization to unseen task compositions" according to the abstract.

## Foundational Learning

- Concept: Modular task representation
  - Why needed here: CESAR relies on breaking down dialog tasks into discrete components to enable automatic composition. Without this modularization, programmatic combination would be impossible.
  - Quick check question: Can you identify the five core components of a CESAR task and explain how they differ from traditional flat task representations?

- Concept: Compositional generalization
  - Why needed here: The framework's value proposition depends on the model's ability to handle task compositions it hasn't explicitly seen during training. This requires understanding how constraints combine rather than memorizing specific patterns.
  - Quick check question: If a model is trained on "begins with + keyword" and "ends with + length" compositions, what would you expect its performance to be on a "begins with + ends with + keyword" task it hasn't seen?

- Concept: Chain of thought reasoning in dialog tasks
  - Why needed here: The CESAR framework mentions potential extension to include reasoning elements, which would require understanding how intermediate reasoning steps can be incorporated into the structured task format.
  - Quick check question: How would you modify a CESAR task that generates a response to include an explicit reasoning step about which keywords to use before generating the response?

## Architecture Onboarding

- Component map: Modular task representation layer -> Composition rule engine -> Prompt generation module -> Language model
- Critical path: Task generation → Model training → Evaluation
- Design tradeoffs: Structured prompts vs. natural language prompts (CESAR uses structured format for scalability but may need conversion for real-world deployment)
- Failure signatures: Poor performance on atomic tasks despite compositional training, inability to handle unseen compositions, generation of nonsensical outputs when components are combined
- First 3 experiments:
  1. Generate a small set of compositional tasks using CESAR rules and manually verify they make semantic sense
  2. Train a baseline model on only atomic tasks and a CESAR model on atomic + compositional tasks, compare performance on a shared atomic task
  3. Test both models on a novel composition not seen during training to evaluate generalization

## Open Questions the Paper Calls Out

### Open Question 1
How can CESAR be extended to handle multi-turn dialog scenarios where the output requires multiple dialog components (e.g., generating both a dialog summary and a response)? The paper mentions that current CESAR tasks are limited to single output components and defers multi-component output tasks to future work.

### Open Question 2
How does the performance of CESAR tasks scale with the number of compositional constraints, and at what point do they become infeasible or too complex for practical use? The paper only explores 2-D tasks and does not investigate the upper bounds of compositional complexity.

### Open Question 3
How would incorporating negative conditions (e.g., "generate a response that does NOT contain...") affect the scalability and practicality of CESAR's compositional task generation? The paper lists "negative conditions" as a limitation in the Conclusion section, noting that incorporating them is challenging.

## Limitations

- Composition rules are defined at an abstract level without full technical specification, making exact reproduction challenging
- Generalization claims rely heavily on comparisons to a "Naive Composer" baseline without establishing what other state-of-the-art compositional approaches exist
- While the framework shows 28% improvement on compositional tasks, the absolute performance metrics are not provided, making it difficult to assess practical significance

## Confidence

- **High Confidence**: The core mechanism of modular task decomposition and programmatic composition is well-demonstrated through experiments showing improved performance on both atomic and compositional tasks.
- **Medium Confidence**: The generalization claims to unseen compositions are supported by experimental evidence but rely on comparisons to a relatively weak baseline.
- **Low Confidence**: The claim about "bridging the gap between public and proprietary dialog models" is made in the abstract but not substantiated with comparative analysis against proprietary systems.

## Next Checks

1. **Rule Completeness Analysis**: Systematically test the composition rules by attempting to combine all possible pairs of atomic tasks from the benchmark and identifying which combinations fail or produce semantically incoherent results.

2. **Cross-Dataset Generalization**: Train CESAR models on compositional tasks from a subset of datasets and evaluate performance on unseen datasets to verify that the compositional learning transfers across different dialog domains.

3. **Ablation on Atomic Task Performance**: Conduct an ablation study that measures atomic task performance degradation when compositional tasks are added to training, to quantify the trade-off between compositional and atomic task capabilities.