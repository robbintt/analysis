---
ver: rpa2
title: 'Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context
  Learning'
arxiv_id: '2305.17256'
source_url: https://arxiv.org/abs/2305.17256
tags:
- learning
- trigger
- prompts
- performance
- shortcut
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models (LLMs) exploit
  shortcuts or spurious correlations when performing in-context learning without parameter
  updates. The authors inject various shortcut triggers into prompts and evaluate
  model performance on both original and anti-shortcut test sets.
---

# Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning

## Quick Facts
- **arXiv ID**: 2305.17256
- **Source URL**: https://arxiv.org/abs/2305.17256
- **Reference count**: 10
- **Primary result**: LLMs exploit shortcut-label mappings during in-context learning, with larger models showing greater susceptibility to shortcut exploitation

## Executive Summary
This paper investigates whether large language models exploit shortcuts or spurious correlations when performing in-context learning without parameter updates. The authors inject various shortcut triggers into prompts and evaluate model performance on both original and anti-shortcut test sets. They find that LLMs are "lazy learners" that rely heavily on shortcuts for inference, with larger models showing greater susceptibility to shortcut exploitation. The study reveals that LLMs are sensitive to trigger positions, favoring triggers near the end of prompts, and can identify shortcuts even when they appear infrequently.

## Method Summary
The study injects various shortcut triggers into prompts and evaluates model performance on original and anti-shortcut test sets. The authors use four classification datasets (SST2, MR, CR, OLID) and two information extraction datasets (ATIS-D, MIT-D) with different shortcut triggers including common words, rare words, signs, sentences, and text styles. Model performance drops on anti-shortcut test sets quantify shortcut exploitation, and LIME is used for model interpretation to detect potential shortcuts. The method involves preparing datasets, designing and injecting shortcut triggers, evaluating model performance across different model sizes, and analyzing results.

## Key Results
- GPT2-large experiences a 41.45% performance drop on the MR dataset when a common word shortcut is introduced
- Larger LLMs (GPT2-large, OPT-13B) show greater susceptibility to shortcut exploitation compared to smaller models (GPT2-base)
- LLMs display sensitivity towards trigger positions, with fixed positions drawing more attention and a bias toward triggers placed near the end of prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exploit shortcut-label mappings during in-context learning, bypassing semantic understanding.
- Mechanism: When prompts contain triggers strongly correlated with specific labels, LLMs preferentially use the trigger-label mapping instead of the input's semantic content for inference.
- Core assumption: The model's attention mechanism prioritizes shortcut patterns when they are present and consistent.
- Evidence anchors: [abstract] "LLMs are 'lazy learners' that tend to exploit shortcuts in prompts for downstream tasks"; [section 3] "The model can either use the semantic relation between the text and label (i.e., x → c) or the inject trigger (i.e., s → c) for inference"

### Mechanism 2
- Claim: Larger LLMs show increased susceptibility to shortcut exploitation compared to smaller models.
- Mechanism: As model size increases, capacity to memorize and utilize shortcut patterns grows faster than capacity for semantic reasoning, leading to greater reliance on shortcuts.
- Core assumption: Model scaling enhances pattern memorization more than semantic understanding.
- Evidence anchors: [abstract] "larger models are more likely to utilize shortcuts in prompts during inference"; [section 5.1] "the average performance drop of GPT2-large... is significantly larger than GPT2-base, which is 1.04%"

### Mechanism 3
- Claim: Trigger position within prompts significantly influences shortcut exploitation, with end positions being most effective.
- Mechanism: LLMs process prompts sequentially, giving higher weight to tokens appearing later in the sequence, making end-positioned triggers more influential for shortcut exploitation.
- Core assumption: Attention mechanisms in LLMs exhibit positional bias favoring later tokens.
- Evidence anchors: [section 6.1] "LLMs display sensitivity towards trigger positions, with fixed positions drawing more attention from the model. Additionally, models exhibit a bias toward triggers placed near the end of the prompts"

## Foundational Learning

- **Concept**: Spurious correlations and shortcut learning
  - Why needed here: Understanding how models can learn non-robust patterns is fundamental to interpreting why LLMs exploit shortcuts
  - Quick check question: What distinguishes robust from non-robust knowledge in machine learning models?

- **Concept**: In-context learning mechanics
  - Why needed here: The study focuses on how LLMs learn from prompts without parameter updates, requiring understanding of conditional generation
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of model adaptation?

- **Concept**: Attention mechanisms and positional encoding
  - Why needed here: The position of triggers within prompts affects their influence, which relates to how attention mechanisms process sequential information
  - Quick check question: How do transformer attention mechanisms process positional information in input sequences?

## Architecture Onboarding

- **Component map**: Prompt injection system -> Trigger placement module -> Model inference engine -> Performance evaluation pipeline -> Anti-shortcut test set generator
- **Critical path**: Prompt construction -> Trigger injection -> Model inference -> Performance measurement -> Analysis
- **Design tradeoffs**: Trigger strength vs. semantic preservation, prompt length vs. computational efficiency, model size vs. shortcut susceptibility
- **Failure signatures**: Performance drops on anti-shortcut datasets, inconsistent predictions when triggers are modified, reliance on trigger presence rather than input semantics
- **First 3 experiments**:
  1. Replicate the basic shortcut injection with a simple sentiment classification task using a common word trigger
  2. Test different trigger positions within prompts to verify positional sensitivity
  3. Compare performance across different model sizes to observe the scaling effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively mitigate shortcut learning in LLMs during in-context learning without compromising performance on the main task?
- Basis in paper: [explicit] The paper identifies that LLMs are "lazy learners" that exploit shortcuts in prompts, and while it suggests model interpretation as a potential detection method, it does not provide an efficient mitigation strategy.
- Why unresolved: The paper focuses on detecting and analyzing shortcuts but does not explore concrete methods to prevent LLMs from relying on them during inference.
- What evidence would resolve it: Empirical results showing a mitigation technique (e.g., prompt engineering, adversarial training, or model fine-tuning) that reduces shortcut reliance while maintaining or improving task performance.

### Open Question 2
- Question: Does the inverse scaling phenomenon observed in shortcut exploitation extend to larger model sizes beyond those tested (e.g., GPT-4 or other frontier models)?
- Basis in paper: [explicit] The paper observes that larger models (e.g., GPT2-large, OPT-13B) exhibit greater susceptibility to shortcut exploitation compared to smaller models, but only tests up to 13B parameters.
- Why unresolved: The study is limited to models up to 13B parameters, and it is unclear whether the trend continues with even larger models.
- What evidence would resolve it: Experiments on models with significantly more parameters (e.g., 30B, 100B, or beyond) demonstrating whether the inverse scaling phenomenon persists or plateaus.

### Open Question 3
- Question: Are there specific linguistic or structural features in prompts that make them more resistant to shortcut exploitation, and can these be systematically designed?
- Basis in paper: [inferred] The paper shows that high-quality prompts do not mitigate shortcut learning, and trigger position affects model behavior, suggesting that prompt design plays a role in shortcut exploitation.
- Why unresolved: While the paper explores trigger positions and prompt quality, it does not investigate whether specific linguistic or structural features (e.g., syntactic complexity, semantic diversity) can inherently reduce shortcut reliance.
- What evidence would resolve it: A systematic analysis of prompt features (e.g., syntactic structures, semantic richness) and their correlation with reduced shortcut exploitation across multiple tasks and models.

## Limitations
- The study's findings are based on controlled experiments with artificially injected triggers, which may not fully capture real-world shortcut usage patterns
- Performance drops could be partially attributed to introduction of non-semantic content rather than pure shortcut exploitation
- The paper doesn't extensively explore the trade-off between semantic preservation and trigger effectiveness

## Confidence
- **High Confidence**: The core finding that LLMs can exploit shortcut patterns in in-context learning is well-supported by systematic performance degradation across multiple datasets and model sizes
- **Medium Confidence**: The observation that larger models show greater susceptibility to shortcut exploitation is supported but could be influenced by other factors such as increased sensitivity to prompt formatting
- **Low Confidence**: The specific mechanisms proposed for why position affects shortcut exploitation are plausible but not directly tested

## Next Checks
1. **Ablation Study on Trigger Impact**: Conduct experiments isolating the effect of trigger injection from semantic perturbation by using semantically equivalent phrases that don't correlate with labels
2. **Cross-Domain Validation**: Test the shortcut exploitation phenomenon across diverse task domains beyond text classification and information extraction
3. **Attention Mechanism Analysis**: Perform detailed attention weight analysis to directly verify whether later-positioned triggers receive higher attention weights, and whether this correlates with increased shortcut exploitation