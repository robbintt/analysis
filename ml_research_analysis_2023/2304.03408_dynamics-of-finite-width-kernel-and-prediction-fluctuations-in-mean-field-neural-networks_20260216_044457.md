---
ver: rpa2
title: Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural
  Networks
arxiv_id: '2304.03408'
source_url: https://arxiv.org/abs/2304.03408
tags:
- networks
- variance
- nite
- width
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes finite-width effects in mean-field neural\
  \ networks, providing a characterization of the O(1/\u221Awidth) fluctuations of\
  \ dynamical mean-field theory (DMFT) order parameters over random initializations.\
  \ The key method involves using the DMFT action to compute a perturbation expansion\
  \ around infinite width dynamics, capturing the coupled fluctuations of kernels\
  \ and predictions during training."
---

# Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks

## Quick Facts
- **arXiv ID**: 2304.03408
- **Source URL**: https://arxiv.org/abs/2304.03408
- **Reference count**: 40
- **Primary result**: Characterizes O(1/√width) fluctuations of DMFT order parameters due to random initialization, showing how feature learning can reduce these effects

## Executive Summary
This paper develops a field-theoretic framework to analyze finite-width corrections to mean-field neural network dynamics. By computing perturbations around infinite-width trajectories using the DMFT action, the authors characterize how random initialization creates persistent fluctuations in kernel and prediction dynamics during training. The key insight is that these O(1/√width) effects can be self-consistently computed through a propagator that captures the coupling between mean-field evolution and initialization variance. Empirically, the work demonstrates that feature learning can dynamically reduce variance effects, and validates predictions on both synthetic tasks and CIFAR-10 classification.

## Method Summary
The authors build on dynamical mean-field theory by computing a perturbation expansion around infinite-width dynamics. Starting from the DMFT action, they construct a Hessian over order parameters (kernels, predictions, dual variables) and invert it to obtain a propagator that captures finite-width fluctuations. This propagator allows self-consistent computation of how initialization variance propagates through training, affecting both kernel evolution and prediction accuracy. The framework handles both lazy and feature learning regimes, with special treatment for deep linear networks where kernel variance accumulates through layers.

## Key Results
- Finite width introduces initialization variance that adds O(1/√width) fluctuations to kernels and predictions
- Richer feature learning can reduce these effects by dynamically denoising kernel fluctuations during training
- Edge of stability phenomena are well-captured by infinite-width dynamics, with variance decreasing on both sides of the threshold
- In deep linear networks, kernel variance accumulates through layers but signal-to-noise ratio improves with feature learning
- CNNs on CIFAR-10 show significant corrections to both bias and variance due to finite width effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Finite width introduces initialization variance that degrades mean-field network performance by adding O(1/√width) fluctuations to kernels and predictions.
- **Mechanism**: Random weight initialization creates non-vanishing fourth cumulants (κ) in kernel entries that persist during training, perturbing the deterministic mean-field trajectory.
- **Core assumption**: Network kernels can be represented as order parameters in a dynamical field theory whose action has well-defined saddle-point and fluctuation structure.
- **Evidence anchors**: 
  - [abstract] "characterization of the O(1/√width) fluctuations of the DMFT order parameters over random initialization of the network weights"
  - [section 2] "At finite width N, q fluctuates in a O(1/√N) neighborhood of q∞"
- **Break condition**: When N is so small that higher-order terms in the cumulant expansion become comparable to leading corrections, or when data distribution violates assumptions of the field theory.

### Mechanism 2
- **Claim**: Richer feature learning reduces the relative impact of initialization variance by dynamically denoising kernel fluctuations.
- **Mechanism**: During training, the mean-field kernel grows in scale and gradients become more aligned with the target signal, causing the sensitivity tensors D to suppress the contribution of initial fluctuations to the observed variance.
- **Evidence anchors**:
  - [abstract] "richer feature learning can reduce these effects" and "dynamically coupled with variance that can be computed self-consistently"
  - [section 4] "feature learning can dynamically reduce the variance of the final NTK and final network predictions"
- **Break condition**: If feature learning strength is insufficient to overcome the accumulation of initialization noise, or if the network architecture prevents effective feature adaptation.

### Mechanism 3
- **Claim**: Edge of stability phenomena do not amplify finite width effects because kernel oscillations are captured by mean-field dynamics.
- **Mechanism**: Near the stability threshold, the propagator elements decrease despite kernel oscillations, indicating that finite networks track the mean-field prediction closely.
- **Evidence anchors**:
  - [abstract] "large learning rate phenomena such as edge of stability effects can be well captured by infinite width dynamics"
  - [section 6] "variance due to finite size can reduce during training on both sides of the edge of stability"
- **Break condition**: If the learning rate is so large that the discrete-time dynamics fundamentally differ from the continuous gradient flow assumption in the mean-field theory.

## Foundational Learning

- **Concept**: Dynamical Mean-Field Theory (DMFT) as a field-theoretic description of infinite-width neural networks.
  - **Why needed here**: The paper builds its finite-width fluctuation analysis on top of the DMFT action and order parameters; understanding the saddle-point structure is essential to derive the propagator.
  - **Quick check question**: What are the order parameters in DMFT for a deep network, and how do they evolve under gradient flow?

- **Concept**: Gaussian process equivalence at infinite width and the breakdown of kernel concentration at finite width.
  - **Why needed here**: The paper's finite-width corrections are rooted in the fact that kernels no longer concentrate but retain initialization-dependent fluctuations; this is the starting point for the perturbation expansion.
  - **Quick check question**: How does the covariance structure of pre-activations at initialization lead to non-vanishing fourth cumulants in the kernel?

- **Concept**: Tensor Programs and the µ-parameterization as frameworks for feature learning in infinite-width limits.
  - **Why needed here**: The paper contrasts lazy/NTK parameterization with mean-field/µ-parameterization; understanding how feature learning arises in the latter is key to interpreting the "rich regime" results.
  - **Quick check question**: In the µ-parameterization, how does the scale factor γ control the strength of feature learning, and what happens as γ→∞?

## Architecture Onboarding

- **Component map**: Network initialization → DMFT action construction → Hessian computation → Propagator inversion → Variance prediction computation
- **Critical path**: Start with the infinite-width DMFT equations to get the saddle-point q∞ and the uncoupled variances κ; then compute the sensitivity tensors D by solving implicit field derivative equations; assemble the Hessian blocks and invert via Schur complement to get Σ; finally apply Padé/cumulant expansion to observables.
- **Design tradeoffs**: Using full Hessian inversion is accurate but scales as O(T⁴P⁴) in memory/time for P samples and T time steps; exploiting sparsity or special structure (e.g., in linear networks) can reduce this cost but limits generality.
- **Failure signatures**: Large discrepancies between predicted and experimental variance when P or D become comparable to N; breakdown of leading-order expansion indicated by quadratic or higher scaling of training rates with 1/N.
- **First 3 experiments**:
  1. Reproduce the two-layer lazy limit variance dynamics (eq. 7) on synthetic data to verify the propagator computation.
  2. Test the reduction of test prediction variance with increasing γ in a two-layer network on a single example.
  3. Measure the effect of width on training rate RN in a small CNN on a subset of CIFAR-10 and fit RN(N) to detect O(1/N) vs O(1/N²) scaling.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise mechanism by which feature learning reduces the variance of network predictions and kernels in the finite width regime?
  - **Basis in paper**: [explicit] The paper shows that feature learning can dynamically reduce the variance of final tangent kernels and final network predictions, and that richer nonlinear dynamics improve the signal-to-noise ratio of kernels and predictions.
  - **Why unresolved**: The paper provides numerical evidence but does not offer a rigorous theoretical explanation for why feature learning specifically reduces variance.
  - **What evidence would resolve it**: A theoretical analysis that explicitly links the strength of feature learning to the magnitude of variance reduction in the prediction and kernel dynamics.

- **Open Question 2**: How do higher-order corrections beyond O(1/√width) affect the dynamics of finite width networks, especially as the sample size becomes comparable to the width?
  - **Basis in paper**: [explicit] The paper acknowledges that the leading O(1/√width) correction may not capture the complete finite size distribution, especially when the sample size is comparable to the width, and suggests exploring higher-order contributions.
  - **Why unresolved**: The paper focuses on leading order corrections and does not investigate the impact of higher-order terms.
  - **What evidence would resolve it**: A detailed analysis of higher-order corrections to the dynamics and their impact on the accuracy of the theory as the sample size approaches the width.

- **Open Question 3**: How do finite width effects manifest in biologically plausible learning rules, such as those with asymmetric fluctuations in the neural tangent kernel over sample indices?
  - **Basis in paper**: [inferred] The paper mentions the possibility of examining finite size impacts on other biologically plausible learning rules, where the effective NTK can have asymmetric fluctuations.
  - **Why unresolved**: The paper does not explore this direction and focuses on gradient-based learning.
  - **What evidence would resolve it**: An extension of the current framework to analyze finite width effects in a specific biologically plausible learning rule, such as one with asymmetric NTK fluctuations.

## Limitations

- The perturbation expansion relies on fourth cumulants remaining small, which may break down in extremely deep or highly non-linear regimes
- Computational complexity of full Hessian inversion (O(T⁴P⁴)) limits applicability to large-scale networks or datasets
- Experimental validation focuses primarily on relatively small-scale CNN architectures, leaving questions about scalability to state-of-the-art models

## Confidence

- **High confidence**: The core mechanism of initialization variance creating O(1/√N) fluctuations in kernels and predictions is well-established theoretically and supported by experimental evidence
- **Medium confidence**: The prediction that richer feature learning reduces variance effects is theoretically sound but requires more extensive empirical validation across diverse architectures
- **Low confidence**: The claim about edge of stability phenomena being well-captured by infinite width dynamics is primarily supported by indirect evidence and would benefit from targeted experiments at extreme learning rates

## Next Checks

1. **Scaling Verification**: Test the width scaling of training rates RN empirically across multiple architectures (from 2-layer to 4-layer networks) to confirm the predicted O(1/N) versus O(1/N²) transition as feature learning strength varies.

2. **Architecture Robustness**: Apply the finite-width fluctuation analysis to transformer-based architectures where attention mechanisms introduce different kernel dynamics, validating whether the same perturbation framework applies.

3. **Large-Scale Empirical Test**: Implement the variance prediction pipeline on a subset of ImageNet with varying network widths to verify whether the theoretical predictions about variance accumulation through layers hold in more realistic settings.