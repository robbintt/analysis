---
ver: rpa2
title: 'Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires
  Data-Driven Priors'
arxiv_id: '2310.02980'
source_url: https://arxiv.org/abs/2310.02980
tags:
- performance
- pretraining
- learning
- task
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper shows that standard long-sequence benchmarks like Long\
  \ Range Arena (LRA) significantly underestimate model performance when trained from\
  \ scratch, due to the lack of pretraining. The authors demonstrate that pretraining\
  \ with denoising objectives on the task data itself (self-pretraining) yields large\
  \ gains across architectures\u2014improving Transformers by over 30% on average\
  \ and enabling them to match or exceed the performance of specialized state-space\
  \ models like S4 on LRA."
---

# Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors

## Quick Facts
- **arXiv ID**: 2310.02980
- **Source URL**: https://arxiv.org/abs/2310.02980
- **Reference count**: 30
- **Key outcome**: Pretraining on task data with denoising objectives dramatically improves long-sequence model performance, especially on smaller datasets, and reduces the need for complex architectural priors.

## Executive Summary
This paper challenges the standard practice of training long-sequence models from scratch, arguing that it underestimates model capabilities. Through experiments on the Long Range Arena benchmark, the authors show that self-pretraining (SPT) with denoising objectives—using only the downstream task data—yields large gains across architectures, including Transformers and state space models (SSMs). SPT enables simpler models like diagonal linear RNNs to match the performance of more complex SSMs, suggesting that structured parameterizations become redundant with data-driven initialization. These findings call for reevaluating benchmarking practices in long-sequence modeling.

## Method Summary
The paper introduces self-pretraining (SPT) with denoising objectives as a standard pretraining phase before fine-tuning on long-sequence tasks. Models (Transformers, S4, and DLR) are pretrained on task data using masked or causal denoising, then fine-tuned with a small grid search over learning rate and batch size. Masking ratios vary by modality (50% for visual, 15% for language, 10% for ListOps). The method is evaluated on the Long Range Arena benchmark and additional datasets (Speech Commands, sCIFAR, BIDMC), with comparisons to trained-from-scratch baselines.

## Key Results
- Pretraining improves Transformer performance on LRA by over 30% on average, closing the gap with SSMs.
- Pretraining S4 on PathX-256 increases accuracy by 20 points (67 → 87), highlighting large gains on challenging tasks.
- Pretraining enables diagonal linear RNNs to nearly match S4 performance, suggesting structured SSM priors become redundant.
- SPT gains are most pronounced on smaller datasets, with relative improvements as large as 30%.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pretraining on task data alone yields strong inductive priors for long-sequence modeling.
- **Mechanism**: Self-pretraining with denoising objectives learns data-driven initializations that encode long-range dependency structures inherent to the downstream task.
- **Core assumption**: The downstream task data distribution contains sufficient structure for pretraining to be effective without external corpora.
- **Evidence anchors**:
  - [abstract] "pretraining with standard denoising objectives, using only the downstream task data, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs)."
  - [section] "we find that priors beneficial for capturing distant dependencies can be simply learned from the task data via standard denoising objectives without any intrusive changes to the model."
  - [corpus] Found related work on domain-specific continued pretraining and long-sequence memory, but direct mechanistic comparisons are sparse.
- **Break condition**: If task data is too small or lacks inherent sequential structure, pretraining may not yield useful priors.

### Mechanism 2
- **Claim**: Pretraining reduces the need for manually-designed architectural priors in SSMs.
- **Mechanism**: Data-driven initialization learned via pretraining makes structured parameterizations (e.g., HiPPO-based kernels) redundant.
- **Core assumption**: The pretrained model learns equivalent or better kernel structures than hand-crafted ones.
- **Evidence anchors**:
  - [abstract] "we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining."
  - [section] "we study the utility of hand-crafted modeling biases in S4 over simpler linear RNNs, finding that the data-driven priors learned via SPT render most of them redundant."
  - [corpus] Weak—no direct corpus evidence on kernel comparison; relies on in-paper analysis.
- **Break condition**: If pretraining objective or architecture limits kernel expressiveness, structured priors may still be necessary.

### Mechanism 3
- **Claim**: Pretraining efficacy increases as task data becomes scarce.
- **Mechanism**: Data-driven priors become more valuable relative to task-specific supervision when labeled data is limited.
- **Core assumption**: Pretraining provides a stronger relative advantage in low-data regimes.
- **Evidence anchors**:
  - [abstract] "We examine the benefits of SPT across multiple data scales showing them to become even more pronounced as data becomes relatively scarce."
  - [section] "Figure 3... the relative gains from SPT over the trained from scratch baseline S4 are modest when the full task data is available, they become increasingly significant (and as large as 30%) on smaller data scales."
  - [corpus] No direct corpus support; claim based on experimental results within the paper.
- **Break condition**: If pretraining data is too small to capture useful patterns, gains may plateau or degrade.

## Foundational Learning

- **Concept**: Long-range dependencies in sequences
  - **Why needed here**: The paper’s central claim is about improving models’ ability to capture these dependencies.
  - **Quick check question**: What architectural feature in Transformers limits their ability to model dependencies over very long sequences?

- **Concept**: Pretraining vs. training from scratch
  - **Why needed here**: The paper argues that training from scratch underestimates model performance compared to pretraining.
  - **Quick check question**: What is the key difference in inductive bias between a randomly initialized model and one pretrained with denoising objectives?

- **Concept**: State space models (SSMs) and their parameterizations
  - **Why needed here**: The paper evaluates SSMs (e.g., S4) and shows how pretraining can simplify their design.
  - **Quick check question**: How does the S4 parameterization differ from a simple diagonal linear RNN?

## Architecture Onboarding

- **Component map**: Input preprocessing → Masking/Causal denoising objective → Pretraining phase → Finetuning phase → Evaluation
- **Critical path**: 1. Pretrain model on task data using denoising objective (masked or causal) 2. Checkpoint best pretraining model (by validation loss) 3. Finetune on downstream task with small grid search over LR and batch size 4. Evaluate and compare to trained-from-scratch baseline
- **Design tradeoffs**:
  - Masking ratio: 50% for visual tasks, 15% for language, 10% for ListOps
  - Attention block size: 4096 for sequences ≥16K to control memory
  - Pretraining steps fixed per task scale to isolate pretraining benefit
- **Failure signatures**:
  - If pretraining learning rate too high → unstable training, poor checkpoint
  - If masking ratio mismatched to modality → suboptimal pretraining signal
  - If finetuning batch size too small → underfitting on downstream
- **First 3 experiments**:
  1. Replicate Transformer baseline on LRA from scratch (no pretraining)
  2. Apply masked pretraining on LRA task data, then finetune and compare
  3. Compare S4 with and without pretraining on PathX-256 to measure pretraining impact

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do the learned convolution kernels in SPT models adapt to different modalities and tasks?
- **Basis in paper**: [explicit] The paper analyzes convolution kernels learned via SPT for different tasks (Text, Image, PathX, ListOps) and observes variable decay rates and structures.
- **Why unresolved**: The analysis focuses on a limited number of tasks and modalities, and a more comprehensive study across a wider range of tasks and modalities is needed to fully understand the kernel adaptation.
- **What evidence would resolve it**: A larger-scale study analyzing kernels across a diverse set of tasks and modalities, potentially using techniques like clustering or visualization, to identify common patterns and differences in kernel adaptation.

### Open Question 2
- **Question**: What is the impact of pretraining data size on the effectiveness of SPT for different architectures and tasks?
- **Basis in paper**: [explicit] The paper examines the utility of SPT across data scales and finds that gains are more pronounced on smaller datasets.
- **Why unresolved**: The study only considers a few data scales and does not explore the relationship between pretraining data size and SPT effectiveness in detail.
- **What evidence would resolve it**: A systematic study varying pretraining data size across different architectures and tasks, analyzing the impact on SPT effectiveness and identifying the optimal data scale for different scenarios.

### Open Question 3
- **Question**: How does the choice of pretraining objective (e.g., causal vs. masked) influence the performance of SPT models?
- **Basis in paper**: [explicit] The paper compares causal and masked pretraining objectives and finds similar results for Transformers, but does not extensively analyze the impact of the objective choice on different architectures and tasks.
- **Why unresolved**: The study does not provide a comprehensive comparison of different pretraining objectives and their impact on model performance.
- **What evidence would resolve it**: A thorough investigation comparing the effectiveness of different pretraining objectives (e.g., causal, masked, contrastive) across various architectures and tasks, analyzing their impact on model performance and identifying the optimal objective for different scenarios.

## Limitations
- Pretraining gains are demonstrated primarily on the LRA benchmark suite, with limited validation on more diverse or real-world long-sequence tasks beyond sCIFAR, BIDMC, and Speech Commands.
- The claim that pretraining reduces the need for structured parameterizations is supported by experimental results but lacks direct mechanistic evidence showing that pretrained models learn equivalent kernel structures.
- While pretraining improves S4 on PathX-256 by 20 points, the paper does not provide detailed analysis of how pretraining affects the internal parameterization of S4 kernels.

## Confidence
- **High**: Pretraining improves Transformer performance on LRA and enables competitive results with SSMs.
- **Medium**: Pretraining simplifies SSM design by reducing the need for hand-crafted priors, based on comparative performance but without direct kernel-level analysis.
- **Medium**: Gains from pretraining are most pronounced on smaller datasets, supported by within-paper experiments but lacking external validation.

## Next Checks
1. **Cross-domain generalization**: Apply the same pretraining pipeline to long-sequence tasks from different domains (e.g., genomics, time-series forecasting) to test robustness beyond LRA.
2. **Kernel analysis in SSMs**: Perform ablation studies comparing pretrained S4 kernels to structured priors (e.g., HiPPO) to validate the claim that structured parameterizations become redundant.
3. **Scaling study with varied pretraining data**: Test whether pretraining gains persist when using subsets of task data of varying sizes to confirm the relationship between data scarcity and pretraining efficacy.