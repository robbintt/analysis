---
ver: rpa2
title: Chain of Thought Prompting Elicits Knowledge Augmentation
arxiv_id: '2307.01640'
source_url: https://arxiv.org/abs/2307.01640
tags:
- cots
- knowledge
- cot-ka
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoT-KA, a method to retrieve knowledge from
  large language models for knowledge-augmented deep learning. Unlike conventional
  approaches, CoT-KA uses chain-of-thought prompting to elicit reasoning chains from
  a large language model, which are then used to augment the input data for fine-tuning
  a smaller task-specific model.
---

# Chain of Thought Prompting Elicits Knowledge Augmentation

## Quick Facts
- arXiv ID: 2307.01640
- Source URL: https://arxiv.org/abs/2307.01640
- Reference count: 13
- Primary result: CoT-KA improves accuracy by 10-30% across 11 reasoning benchmarks by using LLM-generated reasoning chains as knowledge augmentation

## Executive Summary
This paper introduces CoT-KA, a method that uses chain-of-thought prompting to extract reasoning chains from large language models for knowledge-augmented deep learning. Unlike conventional approaches that require external knowledge retrieval or reasoning modules, CoT-KA leverages pre-trained LLMs to generate intermediate reasoning steps that are concatenated with input data for fine-tuning smaller task-specific models. Experiments on 11 reasoning benchmarks show CoT-KA consistently outperforms standard fine-tuning and direct use of LLM outputs, with accuracy improvements ranging from ~10% to ~30% across different tasks and model choices.

## Method Summary
CoT-KA operates in three steps: (1) generate multiple CoTs for each training sample using few-shot or zero-shot CoT prompting with GPT-3 (175B parameters); (2) augment input text by concatenating original input with generated CoTs using special token [EXT]; (3) fine-tune task-relevant models (ALBERT, DeBERTa for NLU tasks; T5 for NLG tasks) on the augmented dataset. The method avoids external knowledge retrieval by extracting reasoning knowledge directly from LLMs, and the fine-tuning objective allows models to learn from both the reasoning steps and final answers in CoTs.

## Key Results
- CoT-KA achieves 10-30% accuracy improvements over standard fine-tuning across 11 reasoning benchmarks
- The method outperforms direct use of LLM outputs, demonstrating that CoTs provide better knowledge augmentation than raw answers
- CoT-KA shows stronger performance on NLG tasks compared to NLU tasks, suggesting task-dependent effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought reasoning chains act as rich, structured knowledge augmentation rather than raw answers.
- Mechanism: Generated CoTs encode intermediate reasoning steps that supplement the input, allowing a smaller model to learn patterns in the reasoning process, not just memorize answers.
- Core assumption: The reasoning steps in CoTs are more useful than the final answer for downstream fine-tuning.
- Evidence anchors:
  - [abstract] "CoT-KA utilizes an LLM as a knowledge source, leveraging CoT prompting to guide the LLM in providing knowledge that can serve as evidence to support downstream reasoning from the input to the answer."
  - [section] "In addition, our attempts to preserve the reasoning steps in the CoTs while removing the answers have resulted in a degradation in performance."
  - [corpus] Weak evidence for CoT effectiveness on NLU tasks vs NLG tasks.
- Break condition: If CoTs consistently produce incorrect or irrelevant reasoning steps, the augmentation degrades performance rather than improving it.

### Mechanism 2
- Claim: CoTs from LLMs replace the need for external knowledge retrieval or reasoning modules.
- Mechanism: Pre-trained LLMs embed domain knowledge and reasoning ability, so CoT prompting can extract this knowledge without additional infrastructure.
- Core assumption: The LLM has already internalized the relevant knowledge during pre-training.
- Evidence anchors:
  - [abstract] "CoT-KA avoids the need for additional knowledge retrieval or a separate knowledge reasoning model."
  - [section] "Unlike conventional KADL approaches, CoT-KA eliminates the need for additional knowledge retrieval or a separate knowledge reasoning model."
  - [corpus] Mixed evidence: CoT-KA outperforms baselines on most tasks but not all.
- Break condition: If the LLM lacks relevant knowledge for a domain, CoT-KA fails to augment effectively.

### Mechanism 3
- Claim: The model learns to use CoTs as supplementary evidence rather than strict rules.
- Mechanism: During fine-tuning, the model treats CoTs as additional features, aligning its predictions with correct CoT answers and ignoring incorrect ones.
- Core assumption: The fine-tuning objective allows the model to selectively trust or ignore parts of the CoT.
- Evidence anchors:
  - [section] "The result demonstrates that CoT is a powerful feature, and the model's predictions tend to align closely with the answers provided in CoT."
  - [section] "In cases where the answer in CoT is correct, the model is likely to align its predictions with the answers in CoT. Conversely, when the answer in CoT is incorrect, there is a relatively high probability that the model will deviate from the answer in the CoT, preventing misleading from the incorrect CoT."
  - [corpus] No direct corpus evidence; inferred from ablation experiments.
- Break condition: If the model becomes overly reliant on CoTs, it may fail when CoTs are unavailable or incorrect.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Generates intermediate reasoning steps that serve as structured knowledge for augmentation.
  - Quick check question: What is the difference between few-shot and zero-shot CoT prompting?

- Concept: Knowledge-augmented deep learning (KADL)
  - Why needed here: The framework for integrating external knowledge into model training.
  - Quick check question: How does CoT-KA differ from traditional KADL approaches?

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: CoT-KA fine-tunes a smaller model on augmented data, unlike methods that only prompt the LLM.
  - Quick check question: Why does CoT-KA use fine-tuning instead of just parsing answers from CoTs?

## Architecture Onboarding

- Component map: LLM (e.g., GPT-3) → CoT generation → CoT set → Input augmentation → PLM (e.g., ALBERT, DeBERTa) → Fine-tuning on augmented data

- Critical path:
  1. Generate multiple CoTs for each training example using LLM.
  2. Concatenate CoTs with original input using a special token.
  3. Fine-tune task-specific PLM on augmented dataset.

- Design tradeoffs:
  - CoT generation cost vs. fine-tuning performance.
  - Number of CoTs to include (limited by input length).
  - Choice of LLM vs. task-specific PLM.

- Failure signatures:
  - Degraded performance if CoTs are incorrect or irrelevant.
  - Overfitting to CoT patterns rather than learning reasoning.
  - Input length overflow if too many CoTs are concatenated.

- First 3 experiments:
  1. Generate 1-5 CoTs per example and measure accuracy gains.
  2. Compare CoT-KA with and without CoT selection strategies.
  3. Test CoT-KA on NLU vs. NLG tasks to identify robustness differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of CoTs to sample from the LLM for different task types and model architectures in CoT-KA?
- Basis in paper: [explicit] The paper varies the number of sampled CoTs from 1 to 5 in experiments and observes different performance trends across tasks and models.
- Why unresolved: The experiments show that increasing CoTs generally improves performance but the gains diminish after a certain point, and this varies by task and model (e.g., ALBERT vs DeBERTa). The optimal number appears task- and model-dependent but is not systematically determined.
- What evidence would resolve it: A comprehensive ablation study varying CoT numbers systematically across all task types and model architectures, combined with statistical analysis to identify optimal points, would clarify this.

### Open Question 2
- Question: How does the quality of CoTs (e.g., correctness of answers within CoTs) affect the performance of CoT-KA, and can we predict which CoTs will be most beneficial?
- Basis in paper: [explicit] The paper discusses that CoTs with incorrect answers can mislead the model but also notes that correct answers strongly align predictions. They attempt to select CoTs based on token generation probability but find no significant improvement.
- Why unresolved: While the paper shows that correct CoT answers improve performance, it does not provide a reliable method to predict or select high-quality CoTs beforehand. The token probability-based selection strategy failed to improve results.
- What evidence would resolve it: Developing and validating a predictive model or heuristic for CoT quality that consistently improves CoT-KA performance across tasks would resolve this.

### Open Question 3
- Question: Can CoT-KA be effectively extended to handle longer input sequences or more CoTs by using models with longer context windows or hierarchical approaches?
- Basis in paper: [inferred] The paper mentions that the input sequence length limit of PLMs restricts the number of CoTs that can be added, and they only use a limited number of CoTs due to this constraint.
- Why unresolved: The paper does not explore methods to overcome the input length limitation, such as using models with longer context windows (e.g., recent LLMs) or hierarchical approaches to incorporate more CoTs.
- What evidence would resolve it: Experiments comparing CoT-KA performance using models with varying context window sizes or hierarchical CoT integration methods would determine if longer sequences or more CoTs improve results.

## Limitations
- Performance depends on LLM quality - incorrect or inconsistent CoTs degrade results
- Input length constraints limit the number of CoTs that can be concatenated
- Significant computational resources required for CoT generation, especially on large datasets

## Confidence
- High confidence in core claim: CoT-KA improves performance over standard fine-tuning and direct LLM output use
- Medium confidence in mechanism: CoTs act as structured knowledge augmentation rather than just answer memorization
- Low confidence in generalizability: Method shows weaker performance on NLU tasks compared to NLG tasks

## Next Checks
1. **CoT Quality Control**: Implement systematic evaluation of CoT quality by measuring percentage of CoTs with correct answers relative to ground truth to validate dependence on CoT accuracy and identify failure modes.

2. **Ablation Study on CoT Components**: Conduct experiments systematically removing reasoning steps or answers from CoTs to quantify individual contributions and test the claim that preserving reasoning steps is crucial.

3. **Resource Efficiency Analysis**: Measure computational cost of CoT generation versus performance gains to validate practical viability for real-world applications and identify sustainability for large-scale deployment.