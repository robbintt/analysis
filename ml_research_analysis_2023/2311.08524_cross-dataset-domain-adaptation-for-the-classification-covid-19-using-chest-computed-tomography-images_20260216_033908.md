---
ver: rpa2
title: Cross-dataset domain adaptation for the classification COVID-19 using chest
  computed tomography images
arxiv_id: '2311.08524'
source_url: https://arxiv.org/abs/2311.08524
tags:
- covid-19
- images
- dataset
- target
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of cross-dataset domain adaptation
  for classifying COVID-19 using chest CT images. The proposed method, COVID19-DANet,
  employs a pre-trained EfficientNet-B3 CNN backbone for feature extraction, followed
  by a prototypical layer for classification.
---

# Cross-dataset domain adaptation for the classification COVID-19 using chest computed tomography images

## Quick Facts
- arXiv ID: 2311.08524
- Source URL: https://arxiv.org/abs/2311.08524
- Reference count: 40
- Method achieves encouraging results on cross-dataset COVID-19 classification compared to recent literature

## Executive Summary
This paper addresses cross-dataset domain adaptation for classifying COVID-19 using chest CT images. The proposed COVID19-DANet employs a pre-trained EfficientNet-B3 CNN backbone for feature extraction, followed by a prototypical layer for classification. The model is trained using a combined loss function that includes cross-entropy loss for class discrimination and an entropy loss computed over the unlabeled target set. This entropy loss is minimized and maximized alternately to achieve both class discrimination and domain invariance. COVID19-DANet is tested under four cross-dataset scenarios using the SARS-CoV-2-CT and COVID19-CT datasets.

## Method Summary
COVID19-DANet uses EfficientNet-B3 as a feature extractor, followed by a prototypical layer for classification. The model employs a combined loss function with cross-entropy for labeled samples and entropy loss for unlabeled target data. During training, entropy loss is alternately minimized and maximized to achieve class discrimination and domain invariance. The method uses balanced mini-batches containing equal numbers of source and target samples, and is tested under four cross-dataset scenarios with limited labeled target samples (3, 5, or 10 per class).

## Key Results
- COVID19-DANet achieves encouraging results compared to recent work in cross-dataset COVID-19 classification
- Model successfully adapts from source to target datasets using limited labeled target samples
- Optimal performance achieved with entropy loss weight λ=0.1 and temperature T=0.05

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COVID19-DANet reduces domain shift by using unlabelled target entropy loss
- Mechanism: The entropy loss over unlabelled target data is maximized to push the classifier toward uniform predictions, forcing the feature space to align between source and target domains. This aligns with the adversarial domain adaptation setup where the encoder is rewarded when the domains are indistinguishable.
- Core assumption: Maximizing entropy over unlabelled target samples encourages the model to learn domain-invariant features.
- Evidence anchors:
  - [abstract]: "unlabelled target entropy loss is minimized and maximized in an alternative fashion, to reach the two objectives of class discrimination and domain invariance."
  - [section 3.1]: "To resolve this problem, we need to include the unlabelled samples and address the data shift problem. It turns out, we can reduce this shift by maximizing the unlabeled entropy loss Eu with respect to the class prototypes."
- Break condition: If the unlabelled target samples are not representative of the source domain, maximizing entropy will not effectively reduce domain shift.

### Mechanism 2
- Claim: Using prototypical networks improves classification with few labelled target samples
- Mechanism: The classifier computes cosine similarity between input features and class prototypes, converting distances to probabilities via softmax. This structure is effective when training data per class is limited, as it focuses on class-representative feature directions.
- Core assumption: Class prototypes can be learned effectively even with very few labelled samples per class.
- Evidence anchors:
  - [abstract]: "The backbone CNN is followed by a prototypical layer which is a concept borrowed from prototypical networks in few-shot learning (FSL)."
  - [section 3]: "The classifier is based on prototypical networks used in the few-shot learning field...The classification module is modeled as a fully connected layer...computes a cosine distance between given samples and the class prototypes."
- Break condition: If the number of labelled target samples per class drops below 2, prototype estimation becomes unstable and classification degrades.

### Mechanism 3
- Claim: Balanced mini-batches from source and target domains prevent model bias toward the source
- Mechanism: Each training batch contains equal numbers of samples from source and target datasets, ensuring the model does not overfit to the larger source dataset.
- Core assumption: Equal representation in each batch is sufficient to balance the influence of source and target data.
- Evidence anchors:
  - [section 3.1]: "To insure balanced training, we make sure that an equal number (B/2) of images comes from the source and target datasets in each batch of samples during the training."
- Break condition: If the source dataset is orders of magnitude larger than the target, even balanced batches may not fully counteract the bias.

## Foundational Learning

- Concept: Domain adaptation and data shift
  - Why needed here: The paper explicitly tackles the problem of differing data distributions between source and target datasets.
  - Quick check question: What happens to classification accuracy if training and test data come from different distributions?

- Concept: Few-shot learning and prototypical networks
  - Why needed here: The classifier uses prototypes learned from very few labelled target samples.
  - Quick check question: How does cosine similarity between features and prototypes translate to class probabilities?

- Concept: Entropy as a regularization signal
  - Why needed here: Entropy loss over unlabelled data is used to encourage domain invariance.
  - Quick check question: What is the effect of maximizing entropy over unlabelled predictions?

## Architecture Onboarding

- Component map: CT images -> EfficientNet-B3 (feature extractor) -> Prototypical layer (classifier) -> Cross-entropy loss + Entropy loss -> Model output
- Critical path:
  1. Load and preprocess CT images (resize, pad/crop to 256x256)
  2. Sample balanced mini-batches (B/2 source, B/2 target)
  3. Forward pass through EfficientNet-B3 → feature vectors
  4. Compute cosine distances to class prototypes → probabilities
  5. Compute cross-entropy loss on labelled samples
  6. Compute entropy loss on unlabelled target samples
  7. Alternate minimization: freeze classifier, update encoder (minimize Lce + λEu); freeze encoder, update classifier (minimize Lce - λEu)
- Design tradeoffs:
  - Using EfficientNet-B3 gives good feature quality but increases memory usage
  - Prototypical layer simplifies classification but may underfit if prototypes are poorly estimated
  - Entropy loss adds domain invariance but can slow convergence if λ is too large
- Failure signatures:
  - High validation loss but low training loss → overfitting to source domain
  - Low entropy on unlabelled target data → model may not be learning domain invariance
  - Accuracy on target much lower than source → domain shift not adequately addressed
- First 3 experiments:
  1. Run with K=3 labelled target samples per class, λ=0.1, T=0.05; monitor validation accuracy vs. batches
  2. Vary λ over [0.001, 0.005, 0.01, 0.1, 0.2, 0.5, 1.0] and observe target accuracy
  3. Vary T over [0.001, 0.005, 0.01, 0.1, 0.2, 0.5, 1.0] and observe target accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the entropy loss function be modified to ensure that minimizing it leads to correct classification of unlabeled data, rather than just producing high-confidence but potentially incorrect predictions?
- Basis in paper: [explicit] The authors acknowledge that minimizing entropy on unlabeled data does not guarantee correct classification, and the model can still produce high-confidence but incorrect predictions.
- Why unresolved: The entropy loss function alone does not provide a mechanism to ensure the correctness of the predicted labels for unlabeled data.
- What evidence would resolve it: Experimental results demonstrating improved performance on unlabeled data when incorporating additional mechanisms to ensure correct classification, such as self-training or consistency regularization.

### Open Question 2
- Question: Can the proposed domain adaptation method be extended to handle more than two classes in the classification task?
- Basis in paper: [inferred] The current implementation is limited to binary classification (COVID-19 vs. non-COVID-19), but the method could potentially be generalized to multi-class scenarios.
- Why unresolved: The paper does not explore the performance of the method on multi-class classification tasks, and it is unclear how well the approach would scale to handle more than two classes.
- What evidence would resolve it: Experimental results showing the effectiveness of the method on multi-class classification tasks with more than two classes, and a comparison of the performance to other domain adaptation techniques.

### Open Question 3
- Question: How does the proposed method perform on other medical imaging datasets beyond CT scans, such as X-rays or MRI?
- Basis in paper: [inferred] The method is specifically designed and evaluated for CT scan images, but it could potentially be applicable to other types of medical imaging data.
- Why unresolved: The paper does not explore the generalizability of the method to other medical imaging modalities, and it is unclear how well the approach would perform on different types of images.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the method on other medical imaging datasets, such as X-rays or MRI, and a comparison of the performance to other domain adaptation techniques for these modalities.

## Limitations

- The effectiveness of entropy maximization for domain adaptation depends heavily on the representativeness of unlabelled target samples
- Implementation details of the alternating optimization procedure are not fully specified
- Limited exploration of data augmentation strategies beyond basic transformations

## Confidence

- **High confidence**: The architectural components (EfficientNet-B3 backbone, prototypical classifier) are well-established and clearly described
- **Medium confidence**: The combined loss function approach and batch balancing strategy are sound, but the specific implementation details of alternating optimization are not fully specified
- **Low confidence**: The effectiveness of entropy maximization for domain adaptation in this specific context, given the limited number of labeled target samples

## Next Checks

1. Implement and validate the alternating optimization procedure by monitoring entropy values on unlabelled target data during training
2. Test model robustness by systematically reducing labeled target samples per class below K=3 to identify failure thresholds
3. Evaluate model performance when source and target datasets have significantly different class distributions to test domain adaptation limits