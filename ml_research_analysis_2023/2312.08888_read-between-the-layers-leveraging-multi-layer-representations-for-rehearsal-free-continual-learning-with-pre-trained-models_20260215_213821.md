---
ver: rpa2
title: 'Read Between the Layers: Leveraging Multi-Layer Representations for Rehearsal-Free
  Continual Learning with Pre-Trained Models'
arxiv_id: '2312.08888'
source_url: https://arxiv.org/abs/2312.08888
tags:
- vit-b
- learning
- in1k
- in21k
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LayUP enhances continual learning with pre-trained models by leveraging
  second-order feature statistics from multiple intermediate layers, not just the
  final embeddings. This approach constructs richer class prototypes and improves
  robustness to domain shifts.
---

# Read Between the Layers: Leveraging Multi-Layer Representations for Rehearsal-Free Continual Learning with Pre-Trained Models

## Quick Facts
- **arXiv ID:** 2312.08888
- **Source URL:** https://arxiv.org/abs/2312.08888
- **Reference count:** 40
- **Primary result:** LayUP achieves higher accuracy on 4 of 7 CIL benchmarks, all 3 domain-incremental benchmarks, and 6 of 7 OCL benchmarks while reducing memory/computation by up to 81% and 90%

## Executive Summary
LayUP introduces a rehearsal-free continual learning method that leverages multi-layer representations from pre-trained vision transformers to construct richer class prototypes. By combining features from multiple intermediate layers with second-order statistics via Gram matrix inversion, the method improves classification accuracy while maintaining efficiency. The approach achieves state-of-the-art performance across multiple continual learning benchmarks without requiring replay buffers or extensive fine-tuning of pre-trained models.

## Method Summary
LayUP extracts intra-layer representations from the last k layers of a frozen pre-trained ViT backbone, concatenates these features across layers, and constructs class prototypes using both first-order (mean features) and second-order (Gram matrix) statistics. During training, the method updates the Gram matrix and optimizes ridge regression parameter λ to decorrelate prototypes. Optional first session adaptation with parameter-efficient tuning (PETL) methods like AdaptFormer helps bridge domain gaps between pre-training and downstream tasks. Inference uses the transformed prototype space via (G + λI)^-1 to classify new samples.

## Key Results
- Achieves higher average accuracy than state-of-the-art baselines on 4/7 class-incremental learning benchmarks
- Outperforms competitors on all three domain-incremental learning benchmarks tested
- Reduces memory and computational requirements by 81% and 90% respectively compared to baseline approaches
- Maintains strong performance across both class-incremental and online continual learning settings

## Why This Works (Mechanism)

### Mechanism 1: Multi-layer Feature Concatenation
Intra-layer representations capture diverse class-specific features at different hierarchical levels. By concatenating features from multiple intermediate layers, LayUP constructs richer class prototypes that encode both low-level and high-level visual information. This approach assumes intermediate layer features contain complementary information that improves class separability when combined with final layer features.

### Mechanism 2: Second-Order Feature Statistics
Gram matrix transformation captures feature correlations across layers, creating a more calibrated similarity measure for prototype-based classification. The method assumes feature correlations in pre-trained models are non-isotropic and need to be accounted for in prototype construction. Ridge regression with regularization parameter λ decorrelates the class prototypes, improving classification robustness.

### Mechanism 3: First Session Adaptation
Parameter-efficient tuning during first task adaptation adjusts representations to the target domain without requiring historical data storage. The approach assumes limited adaptation during the first session is sufficient to handle domain shifts while preventing catastrophic forgetting. PETL methods like AdaptFormer modify representations efficiently without fine-tuning the entire backbone.

## Foundational Learning

- **Concept:** Ridge regression and Gram matrix inversion
  - Why needed here: These techniques decorrelate class prototypes and create robust similarity measures for classification
  - Quick check question: What is the mathematical relationship between the Gram matrix G and the ridge regression estimator (G + λI)^-1?

- **Concept:** Parameter-efficient transfer learning (PETL)
  - Why needed here: Allows adaptation to downstream tasks without fine-tuning the entire pre-trained model, maintaining efficiency and reducing overfitting risk
  - Quick check question: How do adapter methods like AdaptFormer differ from prompt tuning in terms of parameter count and adaptation capability?

- **Concept:** Continual learning metrics (average accuracy and forgetting)
  - Why needed here: These metrics properly evaluate performance across sequential tasks and quantify catastrophic forgetting
  - Quick check question: How does average forgetting Ft differ from final accuracy AT in terms of what they reveal about model performance?

## Architecture Onboarding

- **Component map:** Pre-trained ViT backbone -> Parameter-efficient tuning layer -> Feature extraction pipeline -> Gram matrix computation -> Class prototype construction -> Inference engine

- **Critical path:** 
  1. Extract features from multiple layers
  2. Concatenate features for each sample
  3. Update Gram matrix G and class prototypes
  4. Optimize ridge regression parameter λ
  5. Perform inference using (G + λI)^-1 transformation

- **Design tradeoffs:**
  - k (number of layers): Higher k improves performance but increases memory/computation cost
  - λ regularization: Balances between fitting training data and generalization
  - PETL method choice: Different methods offer different trade-offs between adaptation capability and parameter efficiency

- **Failure signatures:**
  - Poor performance on domain-shifted datasets
  - Numerical instability in Gram matrix inversion
  - Overfitting during first session adaptation
  - Sensitivity to random seed (task order effects)

- **First 3 experiments:**
  1. Compare k=1 (last layer only) vs k=6 performance on a single dataset to verify intra-layer benefits
  2. Test λ=0 vs λ=1 regularization on OCL setting to assess robustness
  3. Evaluate different PETL methods (VPT vs AdaptFormer) on a domain-shifted dataset to find optimal adaptation strategy

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal number of layers k for intra-layer prototype construction vary across different architectures (e.g., ViT vs. ResNet) and tasks? The paper only experiments with ViT-B/16, but different architectures have varying numbers of layers and layer-wise representations, so the optimal k could differ significantly.

### Open Question 2
How does the performance of LayUP change when using different pre-training datasets or self-supervised pre-training methods? The paper uses ImageNet-1K and ImageNet-21K pre-trained models but doesn't explore other pre-training datasets or self-supervised methods.

### Open Question 3
Can LayUP be extended to handle more complex continual learning scenarios, such as class-incremental learning with semantic shifts or task-incremental learning? The paper focuses on class-incremental and online continual learning but doesn't explore these more complex scenarios.

## Limitations

- The claim of achieving 81-90% memory/computation reductions lacks specific baseline comparisons and absolute metric values
- The effectiveness of second-order statistics depends heavily on proper λ regularization, but the optimization procedure is not fully specified
- Domain shift robustness claims rely on first session adaptation, yet the paper doesn't quantify failure cases when domain shift exceeds adaptation capabilities

## Confidence

- **High confidence:** Multi-layer feature concatenation improves prototype quality (supported by ablation showing k=6 outperforms k=1)
- **Medium confidence:** Second-order statistics via Gram matrix inversion provides meaningful decorrelation (mechanism described but limited empirical validation)
- **Medium confidence:** Parameter-efficient tuning effectively handles domain shifts without catastrophic forgetting (supported by benchmark results but lacks failure case analysis)

## Next Checks

1. Replicate the k=1 vs k=6 ablation on CIFAR-100 with T=10 tasks to verify intra-layer benefits and measure actual memory/computation overhead
2. Test λ regularization sensitivity by evaluating performance across λ ∈ {0, 0.1, 1, 10} on the OCL benchmark to identify optimal ranges and failure modes
3. Implement stress tests with severe domain shifts (e.g., ImageNet-R → CUB-200) to quantify first session adaptation limitations and determine failure thresholds