---
ver: rpa2
title: Streamlining Social Media Information Retrieval for COVID-19 Research with
  Deep Learning
arxiv_id: '2306.16001'
source_url: https://arxiv.org/abs/2306.16001
tags:
- social
- symptom
- health
- covid-19
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a framework for curating predefined lexicons
  to improve social media-based public health research by reducing keyword-matching
  bias. The framework uses a BERT-based Named Entity Recognition model to extract
  symptom entities from tweets, followed by normalization and mapping to Unified Medical
  Language System (UMLS) concepts using an ensemble of semantic and lexical similarity
  methods.
---

# Streamlining Social Media Information Retrieval for COVID-19 Research with Deep Learning

## Quick Facts
- arXiv ID: 2306.16001
- Source URL: https://arxiv.org/abs/2306.16001
- Reference count: 0
- This study introduces a framework for curating predefined lexicons to improve social media-based public health research by reducing keyword-matching bias.

## Executive Summary
This paper presents a systematic framework for extracting and normalizing symptom entities from COVID-19-related tweets using a BERT-based Named Entity Recognition (NER) model followed by entity normalization and mapping to Unified Medical Language System (UMLS) concepts. The pipeline identifies colloquial symptom expressions and maps them to standardized medical terminology, addressing the limitations of traditional keyword-based approaches in social media-based public health research. Applied to 18 million COVID-19 tweets from February 2020 to April 2022, the methodology reduces 498,480 unique symptom expressions to 38,175 colloquial expressions mapped to 966 UMLS concepts with 95% accuracy after manual validation. The resulting dictionary detects more symptoms than traditional lexicons and is particularly effective at identifying psychiatric disorders like anxiety and depression.

## Method Summary
The study develops a three-module pipeline for extracting COVID-19 symptoms from social media text: (1) a fine-tuned CT-BERT Named Entity Recognition model to detect symptom entities in tweets; (2) an entity normalization module that aggregates detected entities using regex-based cleaning, lemmatization, and frequency filtering; and (3) an ensemble mapping module that uses both semantic (CODER++ with Cosine similarity) and lexical (fuzzy match with Levenshtein distance) strategies to map normalized entities to UMLS concepts. The process includes iterative manual validation by medical professionals to improve mapping accuracy, with the final dictionary containing 38,175 colloquial expressions mapped to 966 UMLS concepts.

## Key Results
- Identified 498,480 unique symptom expressions from COVID-19 tweets, reduced to 38,175 colloquial expressions
- Achieved 95% accuracy in UMLS concept mapping after manual validation by medical professionals
- Final dictionary maps 38,175 colloquial expressions to 966 UMLS concepts
- Detected more symptoms than traditional lexicons, particularly effective for psychiatric disorders like anxiety and depression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-based NER effectively identifies symptom entities in noisy social media text
- Mechanism: Pre-trained CT-BERT fine-tuned on domain-specific METS-CoV dataset adapts to Twitter-style language and COVID-19 terminology
- Core assumption: Domain adaptation through fine-tuning improves entity recognition in specialized contexts
- Evidence anchors: [abstract] "The pipeline includes three modules: a named entity recognition module to detect symptoms in tweets; an entity normalization module to aggregate detected entities"; [section] "The fine-tuned CT-BERT-based NER model displayed an F1 score of 81.85% on the test set of METS-CoV"
- Break condition: If domain shift between training and deployment data is too large, F1 score will drop significantly

### Mechanism 2
- Claim: Human-in-the-loop validation improves UMLS concept mapping accuracy
- Mechanism: Iterative validation-finetuning cycle uses medical professionals to correct model errors and retrain with hard negative sampling
- Core assumption: Medical experts can reliably validate UMLS concept mappings and their feedback improves model performance
- Evidence anchors: [abstract] "The final lexicon's high accuracy, validated by medical professionals, underscores the potential of this methodology"; [section] "After mapping the 9,045 correctly labeled normalized entities to their colloquial expressions, we constructed a dictionary of 876 UMLS concepts and 38,175 colloquial expressions"
- Break condition: If human annotators disagree significantly or validation introduces bias, accuracy gains may not materialize

### Mechanism 3
- Claim: Combining semantic and lexical similarity methods improves entity-to-concept mapping
- Mechanism: Ensemble approach uses CODER++ for semantic similarity and fuzzy match with Levenshtein distance for lexical similarity
- Core assumption: Different similarity methods capture complementary aspects of entity-concept relationships
- Evidence anchors: [abstract] "The module uses both semantic (CODER++ [16] with Cosine similarity) and lexical (fuzzy match with Levenshtein distance) strategies"; [section] "In the first round of mapping, the fuzzy match approach and the CODER++ model generated 35.23% (6772) correct mappings out of 23,252"
- Break condition: If one method consistently outperforms the other, ensemble approach adds unnecessary complexity

## Foundational Learning

- BERT-based NER
  - Why needed here: Social media text contains informal language, abbreviations, and domain-specific terminology that standard NER models struggle with
  - Quick check question: What is the F1 score of the fine-tuned CT-BERT model on the METS-CoV test set?

- Entity normalization
  - Why needed here: Reduces 498,480 unique symptom expressions to manageable 38,175 colloquial expressions by standardizing variations
  - Quick check question: How many unique symptom expressions remained after normalization but before UMLS mapping?

- UMLS concept mapping
  - Why needed here: Standardizes colloquial symptom descriptions to standardized medical terminology for analysis and comparison
  - Quick check question: How many UMLS concepts were the final dictionary mapped to?

## Architecture Onboarding

- Component map:
  Input: COVID-19 related tweets (18M+) → Module 1: CT-BERT NER for symptom entity extraction → Module 2: Entity normalization (stop word removal, lemmatization, frequency filtering) → Module 3: Ensemble UMLS mapping (CODER++ + fuzzy match) → Output: Dictionary of 38,175 colloquial expressions mapped to 966 UMLS concepts

- Critical path:
  NER → Normalization → UMLS Mapping → Manual Validation → Dictionary Construction

- Design tradeoffs:
  - Higher NER threshold increases precision but reduces recall of rare symptoms
  - Lower frequency threshold for entity retention increases computational cost but captures more colloquialisms
  - More human validation rounds improve accuracy but increase development time

- Failure signatures:
  - NER module: Low F1 score on validation set indicates poor domain adaptation
  - Normalization module: High entity count after normalization suggests ineffective standardization
  - Mapping module: Low validation accuracy indicates poor similarity threshold or model architecture

- First 3 experiments:
  1. Test NER module on held-out METS-CoV validation set to confirm 81.85% F1 score
  2. Run normalization on sample of 1000 entities and manually verify reduction effectiveness
  3. Test UMLS mapping ensemble on 500-sample validation set before and after human feedback loop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the human-in-the-loop approach for UMLS concept mapping compare to fully automated approaches in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The authors note that the CODER++ model, a state-of-the-art model for UMLS concepts, had an accuracy of only 30% for symptom entities on social media, leading them to employ a human-in-the-loop approach. However, the paper does not provide a direct comparison of the human-in-the-loop approach's performance to fully automated methods.
- Why unresolved: The paper does not provide a direct comparison between the human-in-the-loop approach and fully automated methods, leaving the relative performance of these approaches unclear.
- What evidence would resolve it: A comparative study evaluating the accuracy and computational efficiency of the human-in-the-loop approach against fully automated methods for UMLS concept mapping on social media data would resolve this question.

### Open Question 2
- Question: What are the potential biases introduced by the regular expression-based cleaning and normalization process, and how might these biases affect the final symptom dictionary?
- Basis in paper: [inferred] The paper describes using regular expressions to clean and normalize extracted entities, but does not discuss potential biases introduced by this process or how they might impact the final dictionary.
- Why unresolved: The paper does not explore the potential biases introduced by the regular expression-based cleaning and normalization process, nor does it discuss how these biases might affect the final symptom dictionary.
- What evidence would resolve it: A detailed analysis of the regular expression-based cleaning and normalization process, including an assessment of potential biases and their impact on the final symptom dictionary, would resolve this question.

### Open Question 3
- Question: How well does the proposed framework generalize to other health-related topics and languages beyond COVID-19 and English?
- Basis in paper: [explicit] The authors state that their framework is a "proof of concept" and that the dictionary is "available at https://github.com/ningkko/UMLS_colloquialism/", implying potential for broader application. However, the paper does not explore the framework's generalizability to other health-related topics or languages.
- Why unresolved: The paper focuses on applying the framework to COVID-19-related tweets in English, without exploring its generalizability to other health-related topics or languages.
- What evidence would resolve it: Applying the framework to other health-related topics and languages, and evaluating its performance in these contexts, would resolve this question.

## Limitations
- Domain generalization remains untested beyond COVID-19 tweets from February 2020 to April 2022
- Human validation bottleneck presents scalability challenges for real-time deployment
- Practical impact on downstream public health research outcomes remains theoretical rather than empirically demonstrated

## Confidence
- High Confidence: BERT-based NER performance (F1 score of 81.85% on METS-CoV test set) and final dictionary coverage (966 UMLS concepts mapped to 38,175 colloquial expressions)
- Medium Confidence: Effectiveness of ensemble mapping approach and normalization process reduction, but lacking direct comparison to alternatives
- Low Confidence: Claims about framework scalability to other diseases and platforms, and practical impact on public health research outcomes

## Next Checks
1. Cross-Disease Validation: Apply the pipeline to social media data from other disease outbreaks (e.g., influenza, monkeypox) to test domain generalization and identify necessary model adaptations.
2. Real-Time Deployment Testing: Implement the framework in a streaming context with continuous validation to assess scalability, processing latency, and whether accuracy degrades over time as language evolves.
3. Downstream Impact Analysis: Conduct a controlled study comparing public health research outcomes using traditional keyword-based methods versus the curated dictionary approach to quantify practical research improvements.