---
ver: rpa2
title: Fractal Landscapes in Policy Optimization
arxiv_id: '2310.15418'
source_url: https://arxiv.org/abs/2310.15418
tags:
- policy
- function
- when
- where
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the smoothness of optimization landscapes in
  policy optimization for continuous control problems. The authors demonstrate that
  for certain classes of MDPs, the objective function can be highly non-smooth or
  even fractal, making gradient-based methods ill-posed.
---

# Fractal Landscapes in Policy Optimization

## Quick Facts
- **arXiv ID**: 2310.15418
- **Source URL**: https://arxiv.org/abs/2310.15418
- **Reference count**: 40
- **Key outcome**: The optimization landscape in policy optimization can be highly non-smooth or fractal for certain MDPs, making gradient-based methods ill-posed when the maximal Lyapunov exponent exceeds a threshold related to the discount factor.

## Executive Summary
This paper analyzes the smoothness of optimization landscapes in policy optimization for continuous control problems. The authors demonstrate that for certain classes of MDPs, the objective function can be highly non-smooth or even fractal, making gradient-based methods ill-posed. They introduce techniques from chaos theory and non-smooth analysis, including maximal Lyapunov exponents and Hölder exponents, to characterize the smoothness of the landscape. The key result is that when the maximal Lyapunov exponent of the system exceeds a threshold related to the discount factor, the objective function is non-differentiable. The authors develop a practical method to estimate the local smoothness from samples, which can detect when the training process encounters fractal regions.

## Method Summary
The authors analyze the smoothness of policy optimization landscapes by connecting dynamical system properties to optimization geometry. They use maximal Lyapunov exponents to measure trajectory divergence rates, then derive conditions under which the objective function becomes non-differentiable. A key contribution is a practical method to estimate local smoothness using variance analysis: by sampling the objective function around a point with Gaussian noise of varying variance, the log-log plot of variance versus standard deviation reveals the Hölder exponent, indicating whether the function is differentiable.

## Key Results
- When the maximal Lyapunov exponent exceeds -log(γ), the objective function is non-differentiable and Hölder continuous with exponent less than 1
- A practical variance-based method can estimate local smoothness from samples, detecting fractal regions when the slope is less than 2
- Experiments on standard control tasks validate the theory, showing landscapes become more non-smooth as the discount factor increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy gradient methods become ill-posed when the objective function is non-differentiable due to fractal landscapes
- Mechanism: When the maximal Lyapunov exponent (MLE) of the system exceeds the threshold of -log(γ), nearby trajectories diverge exponentially, causing the value function to be Hölder continuous with exponent less than 1. This makes the objective function non-differentiable, meaning gradients don't exist
- Core assumption: The system dynamics are deterministic and the policy is locally Lipschitz continuous
- Evidence anchors:
  - [abstract]: "the optimization landscape in the policy space can be extremely non-smooth or fractal... such that there does not exist gradient to be estimated in the first place"
  - [section 4.2]: Theorem 4.2 proves that when λ(θ) > -log γ, the objective function J(·) is -log γ/λ(θ)-Hölder continuous, which is less than 1
  - [corpus]: Weak - no direct corpus evidence for non-differentiability of policy gradients in chaotic systems
- Break condition: If the discount factor γ is small enough or the MLE λ(θ) is negative, the Hölder exponent becomes greater than 1, restoring differentiability

### Mechanism 2
- Claim: Hölder exponent can be estimated from samples using variance analysis
- Mechanism: By sampling the objective function around a point θ0 with Gaussian noise of varying variance, the log-log plot of variance versus standard deviation reveals the Hölder exponent. If the slope is less than 2, the function is non-differentiable
- Core assumption: The objective function is continuous and the variance of the noise is finite
- Evidence anchors:
  - [section 5]: The paper provides the method V ar(J(X)) ≤ K²EX[||X - ξ'||²] ≃ O(σ²) for Lipschitz continuous functions, and shows that non-smooth functions will have slopes less than 2
  - [section 6]: Experiments on inverted pendulum, acrobot, and hopper validate this method by showing different slopes for smooth vs non-smooth landscapes
  - [corpus]: Weak - no direct corpus evidence for Hölder exponent estimation in RL
- Break condition: If the noise variance is too large, the local smoothness estimate becomes unreliable

### Mechanism 3
- Claim: Stochastic policies can also create non-differentiable landscapes under certain conditions
- Mechanism: Even though stochastic policies provide probability distributions, when the variance approaches zero, the policy becomes deterministic but may not be Lipschitz continuous in the parameters, leading to non-differentiable landscapes
- Core assumption: The probability density functions are not point-wise smooth as variance approaches zero
- Evidence anchors:
  - [section 4.3]: Example 4.3 shows a case with uniform distribution policy where the Hölder exponent remains -log γ/λ(θ) even as the policy approaches determinism
  - [abstract]: Mentions "stochastic policies" and how non-smoothness occurs "without additional assumptions"
  - [corpus]: Weak - no direct corpus evidence for non-differentiability with stochastic policies
- Break condition: If the probability density functions are sufficiently smooth in the parameters, the landscape may remain differentiable

## Foundational Learning

- Concept: Maximal Lyapunov exponent (MLE)
  - Why needed here: MLE measures the exponential divergence rate of nearby trajectories, which is the key indicator of chaos and non-differentiability in the optimization landscape
  - Quick check question: What does a positive MLE indicate about the system dynamics, and how does it relate to the discount factor γ?

- Concept: Hölder continuity
  - Why needed here: Hölder continuity generalizes Lipschitz continuity and provides a way to quantify the smoothness of the objective function. When the Hölder exponent is less than 1, the function is non-differentiable
  - Quick check question: How does the Hölder exponent relate to differentiability, and what is the threshold value that separates differentiable from non-differentiable functions?

- Concept: Hausdorff dimension
  - Why needed here: Hausdorff dimension characterizes fractal sets. When the Hausdorff dimension of the loss landscape is greater than the parameter space dimension, the landscape is fractal and non-differentiable
  - Quick check question: What does it mean for a set to have non-integer Hausdorff dimension, and how does this relate to the existence of tangent planes?

## Architecture Onboarding

- Component map: Dynamical system (MDP) -> Policy parameterization -> Value function calculation -> Hölder exponent estimation
- Critical path: 1) Compute MLE for the system, 2) Compare MLE to -log(γ) threshold, 3) If MLE > -log(γ), landscape is non-differentiable, 4) Use variance analysis to estimate Hölder exponent and confirm non-differentiability
- Design tradeoffs: Deterministic policies provide cleaner analysis but may be limiting; stochastic policies offer more flexibility but introduce additional complexity in smoothness analysis. The choice of discount factor γ affects the threshold for non-differentiability
- Failure signatures: Loss curves show wild fluctuations, policy gradient methods fail to converge, estimated gradients don't point in descent directions, variance analysis shows slopes less than 2
- First 3 experiments:
  1. Implement the inverted pendulum example and verify that for γ=0.9 the landscape is smooth (slope near 2) while for γ=0.99 it becomes non-smooth (slope less than 2)
  2. Create a simple 1D MDP with positive MLE and show that the value function is Hölder continuous with exponent less than 1
  3. Test the variance analysis method on a known non-differentiable function (like |x|) to validate the approach before applying to RL problems

## Open Questions the Paper Calls Out

- **Open Question 1**: How can we characterize the smoothness of the loss landscape along different directions in high-dimensional parameter spaces?
  - Basis in paper: [explicit] The paper mentions that "J(θ) may have different levels of smoothness along different directions" and proposes a statistical method to estimate the Hölder exponent.
  - Why unresolved: The paper provides a method to estimate the Hölder exponent but does not fully explore how the smoothness varies across different directions in high-dimensional spaces.
  - What evidence would resolve it: Experiments showing the Hölder exponent estimates along various directions in high-dimensional policy parameter spaces.

- **Open Question 2**: What is the relationship between the discount factor γ and the occurrence of fractal landscapes in policy optimization?
  - Basis in paper: [explicit] The paper shows that "the loss curve is expected to become smoother as γ decreases" and demonstrates how the non-smoothness grows as γ increases.
  - Why unresolved: While the paper establishes a relationship, it does not provide a precise threshold or a comprehensive understanding of how γ influences the fractal nature of the landscape.
  - What evidence would resolve it: Theoretical analysis and empirical studies identifying the exact relationship between γ and the fractal dimension of the loss landscape.

- **Open Question 3**: Can alternative optimization methods beyond gradient-based approaches be effective for navigating fractal landscapes in policy optimization?
  - Basis in paper: [inferred] The paper concludes that policy gradient methods are ill-posed when the loss landscape is fractal, implying a need for alternative approaches.
  - Why unresolved: The paper focuses on the limitations of gradient-based methods but does not explore potential alternative optimization strategies.
  - What evidence would resolve it: Experiments comparing the performance of non-gradient-based optimization methods (e.g., evolutionary algorithms) on tasks with known fractal landscapes.

## Limitations

- The theoretical results rely heavily on deterministic dynamics, which may not hold in practical reinforcement learning scenarios with stochastic dynamics
- The connection between maximal Lyapunov exponents and practical training dynamics remains somewhat speculative due to real-world noise and approximation errors
- The generalization from deterministic systems to practical policy gradient methods with neural networks is not fully established

## Confidence

- **High**: The mathematical derivations for maximal Lyapunov exponent thresholds and Hölder continuity are rigorous and well-established in chaos theory
- **Medium**: The variance-based method for estimating local smoothness is practical and validated on toy examples, but its robustness to noise and finite sampling remains to be thoroughly tested
- **Low**: The generalization from deterministic systems to practical policy gradient methods with neural networks is not fully established

## Next Checks

1. **Synthetic function validation**: Test the variance analysis method on known fractal and non-fractal functions (e.g., Weierstrass function, absolute value) across different scales to verify the Hölder exponent estimation accuracy and robustness to noise.

2. **Gradient field visualization**: For simple 1D MDPs, visualize the gradient field directly and compare with the theoretical predictions about non-differentiability, checking if the observed behavior matches the Lyapunov exponent analysis.

3. **Robustness to stochasticity**: Add controlled amounts of noise to the system dynamics and measure how the estimated smoothness degrades, testing whether the variance-based method remains reliable under realistic conditions.