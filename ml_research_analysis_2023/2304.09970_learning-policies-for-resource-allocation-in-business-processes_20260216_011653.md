---
ver: rpa2
title: Learning policies for resource allocation in business processes
arxiv_id: '2304.09970'
source_url: https://arxiv.org/abs/2304.09970
tags:
- process
- activity
- business
- resource
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of resource allocation in business
  processes to minimize average cycle time. It proposes two learning-based methods:
  a Deep Reinforcement Learning (DRL) approach and a score-based value function approximation
  method.'
---

# Learning policies for resource allocation in business processes

## Quick Facts
- **arXiv ID**: 2304.09970
- **Source URL**: https://arxiv.org/abs/2304.09970
- **Reference count**: 24
- **Primary result**: Two learning-based methods (DRL and SVFA) outperform or match traditional heuristics for resource allocation in business processes across six scenarios and three composite processes.

## Executive Summary
This paper addresses the challenge of resource allocation in business processes to minimize average cycle time. It proposes two learning-based methods: a Deep Reinforcement Learning (DRL) approach using PPO and a score-based value function approximation (SVFA) method that learns optimal assignment weights through Bayesian optimization. Evaluated on six scenarios and three composite business processes, the methods outperform or are competitive with traditional heuristics in most cases. The DRL approach achieves an average 12.7% improvement over the best-performing benchmark in composite processes, demonstrating its effectiveness in complex, realistic-sized business processes.

## Method Summary
The paper presents two learning-based approaches for resource allocation in business processes. The DRL method uses Proximal Policy Optimization (PPO) with a state representation including resource status, busy times, current assignments, and activity proportions. The agent selects assignments or postpones activities based on maximizing expected reward (inversely related to cycle time). The SVFA method learns weights for a score function that prioritizes assignments based on seven features including mean processing time, variance, and resource eligibility. Bayesian optimization is used to find optimal weights that minimize average cycle time. Both methods are evaluated against traditional heuristics (SPT, FIFO, Random) in five small scenarios and one composite network.

## Key Results
- The DRL approach achieves an average 12.7% improvement over the best-performing benchmark in composite processes
- Both methods learn adaptive policies that outperform or match heuristics in five out of six scenarios
- The SVFA method performs particularly well in the complete network scenario, likely due to the higher number of activities making credit assignment challenging for DRL
- The methods show consistent performance across different utilization rates, processing times, and resource eligibility constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRL outperforms heuristics because it adapts to the current state of the process rather than following static rules.
- Mechanism: The DRL agent observes the current state (available resources, unassigned activity instances, current assignments) and selects the assignment with the highest expected reward, which is inversely related to cycle time. This allows the agent to dynamically prioritize based on the current workload and resource availability.
- Core assumption: The DRL agent can learn a policy that generalizes across different process states and characteristics.
- Evidence anchors:
  - [abstract] "The DRL approach achieves an average 12.7% improvement over the best-performing benchmark in composite processes, demonstrating its effectiveness in complex, realistic-sized business processes."
  - [section] "Our methods make assignments based on a specific process state, which allows the methods to adapt to different situations, such as different loads."
  - [corpus] Weak evidence - the corpus doesn't directly support the DRL mechanism.
- Break condition: The DRL agent fails to generalize to new process states or characteristics it hasn't encountered during training.

### Mechanism 2
- Claim: The SVFA method learns optimal assignment weights by minimizing average cycle time through Bayesian optimization.
- Mechanism: The SVFA method calculates a score for each possible assignment based on a set of features (e.g., mean processing time, resource rank) and learned weights. The method then selects the assignment with the lowest score. Bayesian optimization is used to find the optimal weights that minimize average cycle time.
- Core assumption: The features selected for the SVFA method are relevant and sufficient to predict the impact of an assignment on cycle time.
- Evidence anchors:
  - [abstract] "The second method is a score-based value function approximation approach, which learns the weights of a set of curated features to prioritize resource assignments."
  - [section] "We use Bayesian optimization to find weights that minimize the average cycle time."
  - [corpus] Weak evidence - the corpus doesn't directly support the SVFA mechanism.
- Break condition: The selected features are not sufficient to predict the impact of an assignment on cycle time, or the optimization process gets stuck in a local minimum.

### Mechanism 3
- Claim: Both methods outperform heuristics in most scenarios because they learn from problem-agnostic representations that adapt to different process characteristics.
- Mechanism: The DRL and SVFA methods learn policies that are not tied to specific process characteristics or rules. Instead, they learn from the data and adapt to the specific process they are applied to. This allows them to perform well in a variety of scenarios with different utilization rates, processing times, and resource eligibility constraints.
- Core assumption: The methods can learn effective policies from problem-agnostic representations.
- Evidence anchors:
  - [abstract] "The results show that our methods learn adaptive resource allocation policies that outperform or are competitive with the benchmarks in five out of six scenarios."
  - [section] "Our proposed methods can learn policies that are competitive with or outperform the heuristics independently of process characteristics."
  - [corpus] Weak evidence - the corpus doesn't directly support this mechanism.
- Break condition: The methods fail to learn effective policies from problem-agnostic representations in scenarios with specific characteristics that are not well-represented in the training data.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: RL is used to train the DRL agent to learn a policy for resource allocation that minimizes average cycle time.
  - Quick check question: What is the difference between supervised learning and reinforcement learning, and why is RL more suitable for this problem?

- Concept: Bayesian Optimization
  - Why needed here: Bayesian optimization is used to find the optimal weights for the SVFA method's score function.
  - Quick check question: How does Bayesian optimization differ from other optimization techniques, and why is it more suitable for optimizing the SVFA method's weights?

- Concept: Business Process Management (BPM)
  - Why needed here: Understanding BPM concepts is crucial for understanding the problem domain and the evaluation scenarios.
  - Quick check question: What are the key components of a business process model, and how do they relate to the resource allocation problem?

## Architecture Onboarding

- Component map:
  DRL agent → State observation → Action selection → Environment transition → Reward calculation → Policy update
  SVFA method → State observation → Score calculation → Assignment selection → Environment transition → Cycle time calculation → Weight update

- Critical path: DRL agent → State observation → Action selection → Environment transition → Reward calculation → Policy update
  or SVFA method → State observation → Score calculation → Assignment selection → Environment transition → Cycle time calculation → Weight update

- Design tradeoffs:
  - DRL vs. SVFA: DRL can adapt to the current state but requires more training time and may struggle with delayed rewards. SVFA is faster to train but relies on manually selected features.
  - State representation: More detailed state representations can improve performance but increase computational complexity.
  - Reward function: A more complex reward function can better capture the objective but may make learning more difficult.

- Failure signatures:
  - DRL: Poor performance on new process states, failure to generalize, instability in training
  - SVFA: Suboptimal weights, failure to find the global optimum, reliance on insufficient or irrelevant features

- First 3 experiments:
  1. Train and evaluate the DRL agent on a simple scenario with low utilization and identical resources to verify basic functionality.
  2. Train and evaluate the SVFA method on a scenario with a slow server to verify that it can learn to avoid slow assignments.
  3. Compare the performance of the DRL agent and SVFA method on the complete network to verify that they outperform the heuristics in a realistic-sized business process.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed learning-based methods be adapted to handle non-stationary and evolving real-world business processes?
- Basis in paper: [inferred] The authors mention that real-world processes are more complex and stochastic compared to the synthetic processes studied, and that current methods are tested in the same environment as they are trained, which is unlikely in real-world applications.
- Why unresolved: The paper does not provide specific strategies for adapting the methods to non-stationary and evolving processes.
- What evidence would resolve it: Experiments or simulations demonstrating the performance of the methods on real-world business processes with changing characteristics over time.

### Open Question 2
- Question: How can the credit assignment problem be addressed in deep reinforcement learning for resource allocation to improve learning efficiency and effectiveness?
- Basis in paper: [explicit] The authors mention that the number of activities in the complete network is much higher compared to the small scenarios, which is likely why DRL performs slightly worse than SVFA. They also note that credit assignment is an issue since it is unclear which action relates to which case.
- Why unresolved: The paper does not provide specific solutions for addressing the credit assignment problem in DRL for resource allocation.
- What evidence would resolve it: Experiments or simulations comparing the performance of DRL with and without improved credit assignment mechanisms on resource allocation tasks.

### Open Question 3
- Question: How can the proposed methods be extended to optimize for multiple, potentially conflicting KPIs in resource allocation?
- Basis in paper: [explicit] The authors mention that existing methods are not suitable for optimizing more complex KPIs, such as minimizing the waiting time or cycle time of cases.
- Why unresolved: The paper focuses on minimizing the average cycle time as the primary KPI and does not explore the optimization of multiple, potentially conflicting KPIs.
- What evidence would resolve it: Experiments or simulations demonstrating the performance of the methods on resource allocation tasks with multiple, conflicting KPIs, such as minimizing both cycle time and resource utilization.

## Limitations
- The evaluation focuses primarily on synthetic scenarios rather than real-world business process data, which may not capture the complexity and variability of actual operational environments
- The SVFA method's performance relies heavily on the chosen feature set, and the paper doesn't explore sensitivity to different feature combinations or alternative feature engineering approaches
- The paper doesn't provide specific strategies for adapting the methods to non-stationary and evolving real-world business processes

## Confidence

- **High confidence**: The core mechanism of DRL outperforming heuristics in the tested scenarios is well-supported by the experimental results. The 12.7% average improvement in composite processes is a robust finding given the multiple replications and scenarios tested.
- **Medium confidence**: The claim that both methods adapt to different process characteristics is supported but limited by the range of synthetic scenarios tested. Real-world processes may present different challenges not captured in the evaluation.
- **Low confidence**: The generalizability of the methods to significantly different business process domains (beyond those tested) and the robustness of the SVFA feature selection process are not thoroughly validated.

## Next Checks

1. **Real-world validation**: Apply both methods to at least one real-world business process dataset (e.g., from the BPI challenge) to assess performance in operational contexts with actual variability and complexity.

2. **Feature sensitivity analysis**: Systematically evaluate the SVFA method with different feature combinations and selection strategies to understand which features are most critical for performance and whether the current selection is optimal.

3. **Scalability testing**: Test both methods on business processes with significantly larger numbers of activities, resources, and concurrent cases to understand performance degradation patterns and identify bottlenecks in the learning and execution phases.