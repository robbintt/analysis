---
ver: rpa2
title: 'JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention'
arxiv_id: '2310.00535'
source_url: https://arxiv.org/abs/2310.00535
tags:
- attention
- dynamics
- then
- layer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel mathematical framework, JoMA, to understand
  the training dynamics of multilayer Transformers by integrating out the self-attention
  layer. JoMA shows that attention first becomes sparse (to learn salient tokens)
  and then dense (to learn less salient tokens) in the presence of nonlinear activations,
  while in the linear case it becomes sparse over time.
---

# JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention

## Quick Facts
- arXiv ID: 2310.00535
- Source URL: https://arxiv.org/abs/2310.00535
- Authors: 
- Reference count: 40
- Key outcome: JoMA framework integrates out self-attention to study joint MLP-attention dynamics, predicting sparse-then-dense attention patterns with nonlinear activations and hierarchical learning in multilayer Transformers.

## Executive Summary
This paper introduces JoMA (JOint Dynamics of MLP and Attention), a novel mathematical framework that enables the analysis of training dynamics in multilayer Transformers by integrating out the self-attention layer. By deriving a close-form relationship between attention logits and MLP weights through an invariant that holds during training, JoMA provides insights into how attention and MLP layers jointly evolve. The framework reveals that attention first becomes sparse (focusing on salient tokens) and then dense (including less salient tokens) in the presence of nonlinear activations, while in the linear case it becomes sparse over time. These predictions are verified through experiments on models trained from scratch on Wikitext2/Wikitext103 and pre-trained models like OPT and Pythia.

## Method Summary
JoMA works by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. The framework derives a close-form relationship between attention logits and MLP weights through an invariant that holds during training, enabled by the residual connection and MLP nonlinearity. This allows studying attention behavior without explicitly updating attention parameters. The analysis assumes fixed orthonormal embeddings and uses theoretical assumptions including stationary backpropagated gradients and isotropic input distributions. Experiments involve training models from scratch with learning rates ranging from 1e-6 to 0.01, analyzing attention patterns and MLP weight dynamics, and extracting intermediate checkpoints from pre-trained models to compute attention entropy and stable rank curves.

## Key Results
- Attention first becomes sparse then dense with nonlinear activations, but remains sparse with linear activations
- MLP hidden layers show drop-and-bounce stable rank patterns during training
- Attention sparsity patterns correlate with hierarchical structure in data, with common tokens learned first by lower layers
- Theoretical predictions verified on Wikitext2/Wikitext103 datasets and pre-trained models (OPT, Pythia)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JoMA framework enables analysis of joint MLP and attention training dynamics by integrating out the self-attention layer.
- Mechanism: The framework derives a close-form relationship between attention logits and MLP weights through an invariant that holds during training. This allows studying attention behavior without explicitly updating attention parameters.
- Core assumption: The residual connection and MLP nonlinearity are critical ingredients that enable this joint dynamics analysis.
- Evidence anchors:
  - [abstract]: "This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only."
  - [section 3]: "Theorem 1 (JoMA). Let vk := U⊤C wk, then the dynamics of Eqn. 2 satisfies the invariants..."
  - [corpus]: Weak - corpus doesn't directly discuss the mathematical framework or invariants.
- Break condition: If residual connections are removed or if the activation function is not homogeneous, the close-form relationship between attention and MLP weights may break down.

### Mechanism 2
- Claim: Attention sparsity patterns differ between linear and nonlinear MLP activations during training.
- Mechanism: For linear activations, attention becomes sparse as weights converge to a one-hot pattern. For nonlinear activations, attention first becomes sparse (focusing on salient tokens) then becomes dense (including less salient tokens).
- Core assumption: The convergence speed of MLP weights depends on the magnitude of their corresponding target values in the dynamics.
- Evidence anchors:
  - [abstract]: "JoMA predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations."
  - [section 4]: "For the nonlinear dynamics with attention (Eqn. 6), if v(0) = 0 (zero-initialization), then ln 1/δj(t)/ln 1/δk(t) = eµ2j/2/eµ2k/2..."
  - [corpus]: Weak - corpus mentions related work on attention dynamics but doesn't specifically address the sparse-then-dense pattern.
- Break condition: If the learning rate is extremely large, the attention may become permanently sparse without the subsequent dense phase.

### Mechanism 3
- Claim: Multi-layer Transformers can learn hierarchical data distributions through the interaction of attention layers across different depths.
- Mechanism: Tokens with high co-occurrence frequency (shallow common ancestors in the hierarchy) are learned first by attention in lower layers. As MLP hidden nodes learn to represent these combinations, higher layers can then group these representations, forming deeper hierarchical features.
- Core assumption: Hidden nodes in MLP layers align with latent variables in the generative hierarchical model.
- Evidence anchors:
  - [abstract]: "We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model."
  - [section 5]: "With this generative model, we can analyze qualitatively the learning dynamics of JoMA: it focuses first on associating the tokens in the same lowest hierarchy as the query m..."
  - [corpus]: Weak - corpus mentions related work on hierarchical learning but doesn't specifically discuss the mechanism of attention layer interactions.
- Break condition: If the hierarchical generative model assumptions don't hold for the data, the proposed learning mechanism may not apply.

## Foundational Learning

- Concept: Residual connections
  - Why needed here: Residual connections allow information to flow directly from lower layers to higher layers, which is critical for the JoMA framework to work and for hierarchical learning to occur.
  - Quick check question: What happens to the JoMA framework if residual connections are removed from the Transformer architecture?

- Concept: Homogeneity of activation functions
  - Why needed here: The analysis of nonlinear dynamics requires the activation function to be homogeneous (ϕ(x) = ϕ'(x)x) to derive the convergence properties and critical point behavior.
  - Quick check question: Why is the ReLU activation function considered homogeneous, and how does this property enable the theoretical analysis?

- Concept: Isotropic distributions
  - Why needed here: The theoretical analysis of nonlinear dynamics assumes inputs are sampled from isotropic distributions to simplify the expectation calculations and derive the dynamics near critical points.
  - Quick check question: How does the assumption of isotropic distributions simplify the analysis of the nonlinear MLP dynamics?

## Architecture Onboarding

- Component map: MLP layers (lower and upper layers) -> self-attention mechanism -> residual connections
- Critical path: First grasp the JoMA framework (integrating out attention), then understand how attention sparsity differs between linear and nonlinear cases, and finally see how this enables hierarchical learning in multi-layer settings.
- Design tradeoffs: The framework assumes fixed orthonormal embeddings and doesn't account for embedding training, which could affect the analysis. The theoretical assumptions (e.g., stationary backpropagated gradients) may not hold exactly in practice.
- Failure signatures: If attention doesn't show the expected sparse-then-dense pattern for nonlinear activations, or if hidden nodes don't align with latent variables in hierarchical data, the theoretical predictions may fail.
- First 3 experiments:
  1. Verify the JoMA invariant by plotting zm(t) against the closed-form solution from Theorem 1 during training.
  2. Test attention sparsity patterns by training with linear vs. nonlinear activations and measuring attention entropy over time.
  3. Validate hierarchical learning by training on synthetic data from HBLT and checking correlation between hidden nodes and latent variables.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does JoMA's joint dynamics framework change when embedding vectors are learned during training rather than fixed and orthonormal?
- Basis in paper: [explicit] The paper explicitly states that JoMA focuses on fixed orthonormal embeddings and mentions this as a limitation, suggesting future work should analyze "almost orthogonal" embeddings and learned embeddings.
- Why unresolved: The current analysis assumes fixed orthonormal embeddings for mathematical tractability, but real transformers learn embeddings simultaneously with other parameters.
- What evidence would resolve it: Empirical validation showing whether learned embeddings improve or degrade the predicted attention sparsity patterns and MLP rank dynamics compared to fixed embeddings.

### Open Question 2
- Question: What is the precise relationship between the convergence speed ratio of salient versus non-salient components (Equation 7) and the final learned attention patterns in multilayer transformers?
- Basis in paper: [explicit] The paper derives the convergence speed ratio in Theorem 4 but only provides qualitative analysis of how this leads to hierarchical learning.
- Why unresolved: The paper shows that salient components converge faster but doesn't quantify how this translates to specific attention pattern hierarchies or layer-wise specialization.
- What evidence would resolve it: Quantitative mapping between convergence speed ratios and measured attention entropy curves across transformer layers, potentially showing a predictive relationship.

### Open Question 3
- Question: How do residual connections affect the theoretical predictions of attention sparsity patterns compared to models without them?
- Basis in paper: [explicit] The paper incorporates residual connections in the JoMA framework but doesn't provide comparative analysis showing their specific impact on attention dynamics.
- Why unresolved: While residuals are included in the model, the paper doesn't isolate their effect from other factors like nonlinearity or self-attention mechanisms.
- What evidence would resolve it: Controlled experiments comparing attention dynamics in transformers with and without residual connections, measuring differences in sparsity patterns and validation performance.

## Limitations

- The JoMA framework relies on theoretical assumptions (stationary gradients, isotropic distributions) that may not hold precisely in practice
- The analysis focuses on fixed orthonormal embeddings rather than learning them during training
- Experimental validation is limited to specific model scales and datasets, with limited exploration of learning rate sensitivity

## Confidence

- **High Confidence**: The mathematical framework of JoMA (Mechanism 1) - the derivation of the invariant relationship between attention logits and MLP weights is rigorous and the closed-form solution is mathematically sound.
- **Medium Confidence**: The sparse-then-dense attention pattern prediction (Mechanism 2) - while the theoretical analysis is compelling, the experimental verification is limited to specific model scales and datasets, and the learning rate sensitivity wasn't fully explored.
- **Medium Confidence**: The hierarchical learning explanation (Mechanism 3) - the qualitative analysis provides valuable insights, but the empirical validation relies on synthetic data from HBLT rather than real-world hierarchical datasets.

## Next Checks

1. **Learning Rate Sensitivity Analysis**: Systematically vary learning rates across multiple orders of magnitude (1e-6 to 1e-2) and measure how the sparse-then-dense transition timing and amplitude change. This would validate whether the theoretical predictions hold across training regimes and identify the critical thresholds where the mechanism breaks down.

2. **Cross-Architecture Verification**: Apply JoMA analysis to different Transformer variants (e.g., GPT, BERT, ViT) and architectures with different normalization schemes (LayerNorm vs RMSNorm) to test the framework's generalizability beyond the standard Transformer architecture studied in the paper.

3. **Real Hierarchical Dataset Validation**: Test the hierarchical learning mechanism on real-world datasets with known hierarchical structure (e.g., programming languages, mathematical expressions, or XML documents) rather than synthetic HBLT data. Measure the correlation between discovered latent variables and ground-truth hierarchical labels to provide stronger empirical support for the proposed mechanism.