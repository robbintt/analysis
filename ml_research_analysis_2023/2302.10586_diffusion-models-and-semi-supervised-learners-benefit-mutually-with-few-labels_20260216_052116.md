---
ver: rpa2
title: Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels
arxiv_id: '2302.10586'
source_url: https://arxiv.org/abs/2302.10586
tags:
- labels
- diffusion
- classi
- pseudo
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a three-stage training strategy called dual
  pseudo training (DPT) for semi-supervised image generation and classification. The
  method first trains a classifier on partially labeled data to predict pseudo-labels,
  then trains a conditional generative model on all data with pseudo-labels to generate
  synthetic images, and finally retrains the classifier on real data augmented with
  pseudo images.
---

# Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels

## Quick Facts
- **arXiv ID**: 2302.10586
- **Source URL**: https://arxiv.org/abs/2302.10586
- **Reference count**: 24
- **Primary result**: Three-stage dual pseudo training (DPT) achieves state-of-the-art semi-supervised image generation and classification on ImageNet with only 2 labels per class

## Executive Summary
This paper introduces a novel three-stage training strategy called dual pseudo training (DPT) that demonstrates how diffusion models and semi-supervised learners can mutually benefit each other when trained with very few labels. The method leverages pseudo-labeling and pseudo-image generation to create a self-reinforcing training loop. On ImageNet 256×256, DPT achieves remarkable results: generating realistic and diverse images with only 2 labels per class (FID 3.44) and outperforming strong supervised diffusion models. For classification, it achieves top-1 accuracies of 59.0, 69.5, and 73.6 with 1, 2, and 5 labels per class respectively.

## Method Summary
DPT follows a three-stage approach: (1) Train a classifier on partially labeled data to predict pseudo-labels for all images, (2) Train a conditional generative model using these pseudo-labels to generate synthetic images, and (3) Retrain the classifier on real data augmented with pseudo images. The method uses large-scale vision transformers and strong self-supervised learners like Masked Siamese Networks (MSN) as the classifier, paired with U-ViT-based diffusion models for generation. This approach creates a mutually beneficial relationship where the classifier's predictions improve the generative model's outputs, and the generative model's outputs improve the classifier's predictions.

## Key Results
- Achieves FID 3.44 with only 2 labels per class on ImageNet 256×256, outperforming supervised diffusion models
- Top-1 classification accuracy of 59.0 with 1 label per class, 69.5 with 2 labels, and 73.6 with 5 labels
- Optimal number of pseudo images per class (K) is 128, providing substantial improvements over baselines
- Generates high-quality, diverse images with improved precision and recall compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The classifier benefits from the generative model because pseudo images improve precision and recall on training data.
- **Mechanism**: The generative model creates realistic, diverse samples that augment sparse labeled data, allowing the classifier to learn better decision boundaries.
- **Core assumption**: Pseudo images are realistic and semantically correct for their labels.
- **Evidence anchors**: "DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification benchmarks" with state-of-the-art accuracies of 59.0 (+2.8), 69.5 (+3.0), and 73.6 (+1.2).
- **Break condition**: If pseudo images are unrealistic or noisy, classifier performance degrades.

### Mechanism 2
- **Claim**: The generative model benefits from the classifier because pseudo labels provide useful learning signals.
- **Mechanism**: Classifier-predicted pseudo labels for unlabeled data give the conditional diffusion model strong training signals to learn the joint distribution p(image, label).
- **Core assumption**: Pseudo labels have relatively low noise (high precision and recall).
- **Evidence anchors**: "With two (i.e., <0.2%) and five (i.e., <0.4%) labels per class, DPT achieves an FID of 3.44 and 3.37 respectively, outperforming strong diffusion models with full labels."
- **Break condition**: If classifier accuracy is low, noisy pseudo labels lead to poor generative model performance.

### Mechanism 3
- **Claim**: Mutual benefit occurs through iterative refinement of pseudo labels and pseudo images.
- **Mechanism**: Classifier predictions improve generative outputs, which in turn improve classifier predictions - a self-training feedback loop captured in three stages.
- **Core assumption**: Improvements are complementary and create positive feedback.
- **Evidence anchors**: "DPT outperforms competitive semi-supervised baselines substantially" and "MSN with ViT-L/7 achieves a top-1 training accuracy of 60.3%."
- **Break condition**: If initial models are too weak, mutual benefit may not be realized.

## Foundational Learning

- **Concept**: Diffusion probabilistic models (DDPM)
  - Why needed here: Essential for understanding how the generative model is trained and generates pseudo images
  - Quick check question: What is the forward process in a diffusion model, and how does it differ from the reverse process?

- **Concept**: Semi-supervised learning (SSL)
  - Why needed here: DPT is a semi-supervised approach, so understanding SSL basics is crucial
  - Quick check question: What are the main challenges in semi-supervised learning, and how does DPT address them?

- **Concept**: Vision Transformers (ViT)
  - Why needed here: Both MSN classifier and U-ViT generative model use ViT architectures
  - Quick check question: How does ViT differ from traditional CNNs, and what are its advantages for image classification and generation?

## Architecture Onboarding

- **Component map**: 
  MSN (classifier) -> Linear Classifier -> Pseudo-label generator -> U-ViT (generative model) -> Pseudo-image generator -> Retrained classifier

- **Critical path**:
  1. Train MSN on all real images (labeled and unlabeled)
  2. Extract features and train linear classifier on labeled data
  3. Use classifier to predict pseudo labels for unlabeled data
  4. Train U-ViT on all data with pseudo labels
  5. Sample pseudo images from U-ViT for each class
  6. Retrain linear classifier on augmented dataset

- **Design tradeoffs**:
  - Strong self-supervised learner (MSN) vs. simpler alternatives: Better pseudo labels but higher computational cost
  - Number of pseudo images per class (K): Larger K provides more augmentation but increases computation and overfitting risk
  - Model architecture choices: Larger models have higher capacity but are more computationally expensive

- **Failure signatures**:
  - Low precision/recall of classifier indicates noisy pseudo labels
  - Poor FID score suggests generative model isn't learning data distribution well
  - Low top-1 accuracy after augmentation indicates augmentation isn't helping

- **First 3 experiments**:
  1. Sanity check: Train DPT on small ImageNet subset (10 classes) and verify improvement over labeled-only baseline
  2. Ablation study: Remove generative augmentation and verify performance drops
  3. Hyperparameter sensitivity: Vary K (number of pseudo images per class) to identify optimal value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of pseudo images per class (K) affect classification performance in the long term?
- Basis in paper: [explicit] DPT improves baselines consistently with K in {12, 128, 256, 512, 1280}, with K=128 being optimal
- Why unresolved: Paper doesn't provide comprehensive long-term analysis of varying K
- What evidence would resolve it: Experiments with different K values over longer periods analyzing classification performance

### Open Question 2
- Question: How does pseudo label quality from the first-stage classifier affect DPT performance?
- Basis in paper: [inferred] Pseudo labels are generated by a classifier trained on limited labeled data
- Why unresolved: No detailed analysis of pseudo label quality impact on DPT
- What evidence would resolve it: Experiments with varying pseudo label quality and performance analysis

### Open Question 3
- Question: How does choice of conditional generative model affect DPT performance?
- Basis in paper: [explicit] DPT treats generative model as black box, using U-ViT-based DPM
- Why unresolved: No comparison with different generative model choices
- What evidence would resolve it: Experiments with different conditional generative models comparing DPT performance

### Open Question 4
- Question: How do additional stages beyond the three proposed affect DPT performance?
- Basis in paper: [inferred] Mentions potential for one more stage to refine pseudo labels
- Why unresolved: No comprehensive analysis of additional stages
- What evidence would resolve it: Experiments with different numbers of stages analyzing performance

## Limitations
- Performance claims based on single benchmark (ImageNet 256×256) with specific architectures
- Computational cost of training large-scale ViT models with diffusion models not discussed
- Assumes "infinite model capacity and zero optimization error" in theoretical framework

## Confidence
- **High Confidence**: Three-stage DPT procedure is implementable; reported ImageNet results are reproducible; mutual benefit mechanism is plausible
- **Medium Confidence**: Specific improvements over baselines are accurate; FID scores represent state-of-the-art; K=128 is optimal
- **Low Confidence**: Exact mechanism of mutual benefit in all cases; performance on non-ImageNet datasets; scalability to larger label spaces

## Next Checks
1. **Robustness Analysis**: Systematically vary initial classifier quality and measure effect on mutual benefit mechanism
2. **Cross-Dataset Validation**: Apply DPT to medical imaging or satellite imagery with few labels to test generalization
3. **Ablation of Iterative Refinement**: Implement explicit iterative version of DPT and compare performance to single-pass approach