---
ver: rpa2
title: Extending Cross-Modal Retrieval with Interactive Learning to Improve Image
  Retrieval Performance in Forensics
arxiv_id: '2308.14786'
source_url: https://arxiv.org/abs/2308.14786
tags:
- retrieval
- image
- images
- learning
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently retrieving forensically
  relevant images from large unstructured digital evidence collections. It proposes
  Excalibur, a zero-shot cross-modal image retrieval system extended with interactive
  learning, allowing users to refine search results through relevance feedback.
---

# Extending Cross-Modal Retrieval with Interactive Learning to Improve Image Retrieval Performance in Forensics

## Quick Facts
- arXiv ID: 2308.14786
- Source URL: https://arxiv.org/abs/2308.14786
- Reference count: 7
- Key outcome: Interactive learning improved MAP@50 by 215% and Recall@200 by 78% after ten rounds on simulated forensic data

## Executive Summary
This paper addresses the challenge of efficiently retrieving forensically relevant images from large unstructured digital evidence collections. The authors propose Excalibur, a zero-shot cross-modal image retrieval system extended with interactive learning, allowing users to refine search results through relevance feedback. Using a pre-trained CLIP model for image and text encoding combined with a Support Vector Machine (SVM) classifier for interactive learning, Excalibur demonstrates significant improvements in retrieval performance when evaluated on simulated forensic data and through a user study with investigators.

## Method Summary
Excalibur uses a pre-trained CLIP model to encode images and text into a shared embedding space, enabling zero-shot cross-modal retrieval through cosine similarity scoring. The system is extended with interactive learning where users provide relevance feedback (positive and negative examples) to train an SVM classifier that re-ranks retrieval results. The evaluation was conducted on the Places365 dataset with 20 scene categories simulated as forensic data, measuring Mean Average Precision at 50 (MAP@50) and Recall at 200 (Recall@200) across multiple interaction rounds.

## Key Results
- MAP@50 increased by 215% and Recall@200 by 78% after ten interaction rounds with natural language queries
- Interactive learning consistently improved retrieval performance across all 20 scene categories tested
- User study participants rated system usability at 9.4/10 and expressed interest in using the system for daily practice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot cross-modal retrieval using CLIP can retrieve forensically relevant images without task-specific fine-tuning.
- Mechanism: CLIP's image and text encoders map both modalities into a shared embedding space optimized for cosine similarity, enabling retrieval by natural language queries without per-task training.
- Core assumption: The visual concepts present in forensic image collections are sufficiently represented in CLIP's pre-training corpus.
- Evidence anchors:
  - [abstract] "The choice for CLIP is motivated by the promising results that have been achieved on cross-modal retrieval" and "The most important aspect of CLIP is its zero-shot capabilities, allowing us to use the pre-trained model for our use case without any domain-specific fine-tuning."
  - [section] "CLIP is trained to maximize the cosine similarity of the image and text embeddings of the correct pairs in the batch while minimizing the cosine similarity of the incorrect pairs."
  - [corpus] Weak: No direct corpus evidence of forensic image retrieval performance.
- Break condition: If forensic imagery contains highly specialized visual concepts absent from CLIP's training distribution, retrieval performance will degrade regardless of fine-tuning.

### Mechanism 2
- Claim: Interactive learning with SVM classifiers improves retrieval performance by learning from user relevance feedback.
- Mechanism: User-provided positive and negative image samples train an SVM on image embeddings, which re-ranks results based on classification confidence.
- Core assumption: The visual differences between relevant and non-relevant images in the forensic domain are linearly separable in the CLIP embedding space.
- Evidence anchors:
  - [abstract] "interactive learning effectively improves retrieval performance in the forensic domain" and "MAP@50 increased by 215% and Recall@200 by 78% after ten interaction rounds."
  - [section] "The goal of the SVM is to find the margin that maximizes the distance between the representation vectors of the marked p relevant and n non-relevant images by the user."
  - [corpus] Weak: No direct corpus evidence of SVM effectiveness in forensic contexts.
- Break condition: If the relevant images span multiple disjoint visual concepts, the linear SVM will fail to capture these distinctions, limiting improvement.

### Mechanism 3
- Claim: Image-based queries are more effective than natural language for domain-specific forensic concepts.
- Mechanism: When investigators have access to a similar example image, the visual similarity metric (cosine similarity in CLIP space) directly captures the desired concept better than textual description.
- Core assumption: The user's mental representation of the target concept aligns more closely with visual features than with linguistic descriptions.
- Evidence anchors:
  - [abstract] "When looking for a specific concept with domain-specific semantics... they preferred querying with an image."
  - [section] "Often, investigators look for domain-specific concepts that are hard to express with natural language."
  - [corpus] Weak: No direct corpus evidence comparing image vs. text query effectiveness in forensic domains.
- Break condition: If the user cannot find or construct a suitable example image, or if the target concept is better described than visualized, image queries will underperform.

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding spaces
  - Why needed here: Retrieval performance depends on measuring similarity between image and text embeddings; understanding cosine similarity is essential for tuning and debugging.
  - Quick check question: What happens to cosine similarity when one embedding vector is scaled but direction remains unchanged?

- Concept: Support Vector Machine classification
  - Why needed here: Interactive learning relies on SVM to learn from user feedback and re-rank images; understanding SVM mechanics is crucial for parameter tuning and failure diagnosis.
  - Quick check question: How does the choice of kernel (linear vs. RBF) affect the SVM's ability to separate relevant from non-relevant images?

- Concept: Zero-shot learning principles
  - Why needed here: The system's ability to generalize to unseen forensic concepts without fine-tuning depends on understanding zero-shot learning assumptions and limitations.
  - Quick check question: What are the primary risks when applying a model trained on general web data to a specialized forensic domain?

## Architecture Onboarding

- Component map: Image encoder (CLIP ViT-B/32) → Query encoder (same CLIP model) → Similarity function (cosine similarity) → Initial ranked list → Interactive learning loop (SVM classifier) → Re-ranked list
- Critical path: Query → CLIP encoding → Cosine similarity computation → Top-k retrieval → User feedback → SVM training → Re-ranking
- Design tradeoffs: Zero-shot retrieval trades initial performance for domain flexibility; SVM trades computational efficiency for potential loss of complex visual relationships
- Failure signatures: Poor initial retrieval suggests mismatch between query language and CLIP's training distribution; stalled improvement in interactive learning suggests SVM cannot separate relevant from non-relevant embeddings
- First 3 experiments:
  1. Test retrieval performance with synthetic forensic queries on Places365 validation set
  2. Measure MAP@K and Recall@K after 1, 3, and 5 interaction rounds with controlled user feedback
  3. Compare image query vs. natural language query performance on the same dataset with identical interaction protocols

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scalability of Excalibur perform with significantly larger image collections (e.g., millions of images) compared to the Places365 dataset used in the evaluation?
- Basis in paper: [inferred] The paper mentions that scalability is a critical issue due to the increasing amount of unstructured data in investigations, and suggests reducing the dimensionality of representation vectors as a potential solution for future work.
- Why unresolved: The current evaluation was performed on a relatively small dataset (Places365 with 20 scene categories). The computational efficiency and performance of Excalibur with much larger collections were not tested.
- What evidence would resolve it: Experiments with Excalibur on datasets containing millions of images, measuring retrieval performance and computational resources required (time, memory) as the dataset size scales.

### Open Question 2
- Question: What is the impact of different error rates in user relevance feedback on the effectiveness of interactive learning in Excalibur?
- Basis in paper: [explicit] The simulation protocol implements an error rate in the relevancy judgments of the actors, with a 20% chance of making mistakes (ignoring the image or flipping relevance judgment).
- Why unresolved: The paper uses a fixed 20% error rate to simulate user behavior, but the impact of varying error rates on retrieval performance was not explored. It is unclear how robust Excalibur is to different levels of user error.
- What evidence would resolve it: Simulations with different error rates (e.g., 0%, 10%, 20%, 30%) in the relevance feedback, measuring the retrieval performance (MAP@50, Recall@200) to understand the system's robustness to user errors.

### Open Question 3
- Question: How does Excalibur perform on other types of forensic imagery beyond scene-based categories, such as objects, faces, or documents?
- Basis in paper: [inferred] The paper mentions that Excalibur is designed to retrieve images of various visual concepts encountered in the forensic domain, but the evaluation focused on scene-based categories from the Places365 dataset.
- Why unresolved: The evaluation used a specific dataset with scene categories, which may not fully represent the diversity of forensic imagery encountered in real investigations. The performance on other types of forensic imagery is unknown.
- What evidence would resolve it: Experiments with Excalibur on datasets containing other types of forensic imagery (e.g., objects, faces, documents), measuring retrieval performance and user satisfaction to assess its effectiveness across different forensic domains.

## Limitations
- Evaluation relies entirely on simulated forensic data rather than real-world digital evidence collections
- User study sample size of eight participants is too small for statistical generalization
- System performance depends on assumption that forensic concepts are adequately represented in CLIP's pre-training corpus

## Confidence
- Retrieval performance claims (MAP@50 215% increase, Recall@200 78% increase): High
- Usability findings (9.4/10 rating): Medium
- Zero-shot effectiveness claim: Medium
- Interactive learning improvement claim: High

## Next Checks
1. Validate retrieval performance on a real-world forensic digital evidence collection with ground truth relevance judgments across multiple case types
2. Conduct a larger-scale user study (n≥30) with forensic investigators using their actual case data to measure both performance and usability in realistic conditions
3. Compare Excalibur's zero-shot retrieval performance against CLIP models fine-tuned on forensic domain data to quantify the trade-off between flexibility and initial accuracy