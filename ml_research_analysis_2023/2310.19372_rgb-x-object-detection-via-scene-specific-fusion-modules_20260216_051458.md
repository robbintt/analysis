---
ver: rpa2
title: RGB-X Object Detection via Scene-Specific Fusion Modules
arxiv_id: '2310.19372'
source_url: https://arxiv.org/abs/2310.19372
tags:
- fusion
- cbam
- detection
- scene
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a modular RGB-X fusion network for object detection
  that leverages pretrained single-modal detectors via lightweight scene-specific
  fusion modules. The approach uses convolutional block attention modules (CBAM) to
  fuse features from different modalities (e.g., RGB-thermal or RGB-gated) at multiple
  stages, with separate CBAM modules trained for different scene/weather conditions.
---

# RGB-X Object Detection via Scene-Specific Fusion Modules

## Quick Facts
- arXiv ID: 2310.19372
- Source URL: https://arxiv.org/abs/2310.19372
- Reference count: 40
- Achieves mAP@0.5 scores of 85.45-86.16 on FLIR Aligned, M3FD, and Seeing Through Fog datasets using only 0.21M trainable parameters per scene

## Executive Summary
This paper presents a modular RGB-X fusion network for object detection that leverages pretrained single-modal detectors through lightweight scene-specific fusion modules. The approach uses convolutional block attention modules (CBAM) to fuse features from different modalities at multiple stages, with separate CBAM modules trained for different scene/weather conditions. A scene classifier selects the appropriate fusion module during inference. The method demonstrates strong performance across three datasets while using minimal trainable parameters, making it particularly suitable for resource-constrained applications.

## Method Summary
The method employs pretrained EfficientDet detectors for RGB and thermal/gated imagery, with CBAM modules inserted between their BiFPN layers to fuse features at multiple scales. Each scene/weather condition (clear/fog/snow day/night) has its own set of CBAM fusion modules, selected by a scene classifier during inference. The detector backbones remain frozen during training, with only the lightweight fusion modules and scene classifier being updated. This modular design enables effective cross-modal fusion while minimizing training data requirements and computational overhead.

## Key Results
- Achieves mAP@0.5 scores of 85.45-86.16 across FLIR Aligned, M3FD, and Seeing Through Fog datasets
- Uses only 0.21M trainable parameters per scene compared to 26.7M total parameters
- Maintains strong performance with limited training data (25% of original dataset)
- Demonstrates effective thermal feature attention in low-light conditions

## Why This Works (Mechanism)

### Mechanism 1
Scene-specific CBAM modules improve detection performance by attending to the most relevant sensor modality for each scene condition. Different CBAM modules are trained for different scene categories, and a scene classifier selects the most appropriate module during inference. This works because different weather/lighting conditions make different sensor modalities more informative.

### Mechanism 2
Freezing pretrained backbone networks while only training lightweight fusion modules enables strong performance with minimal training data. The frozen backbones leverage their pretrained knowledge while the small CBAM modules learn to combine their features effectively, drastically reducing trainable parameters.

### Mechanism 3
Mid-level feature fusion is more effective than early or late fusion for RGB-X object detection. The method fuses features from BiFPN layers, which contain rich semantic information while preserving spatial details, providing an optimal balance for cross-modal fusion.

## Foundational Learning

- **Concept: Convolutional Block Attention Modules (CBAM)**
  - Why needed here: Provides lightweight way to learn which channels and spatial locations are most important for fusion
  - Quick check question: What are the two main components of CBAM and what does each attend to?

- **Concept: Scene classification for adaptive fusion**
  - Why needed here: Different weather/lighting conditions require different fusion strategies
  - Quick check question: How does the scene classifier determine which fusion module to use during inference?

- **Concept: Freezing pretrained networks**
  - Why needed here: Allows leveraging pretrained knowledge while only learning to combine features
  - Quick check question: Why is it beneficial to freeze the backbone networks rather than fine-tuning them end-to-end?

## Architecture Onboarding

- **Component map**: RGB detector -> BiFPN features -> Scene classifier -> Selected CBAM fusion module -> Fused features -> Detector head; X detector -> BiFPN features -> Selected CBAM fusion module -> Fused features -> Detector head

- **Critical path**: RGB and X images processed by respective backbones → BiFPN features extracted at multiple scales → Scene classifier determines current scene type → Corresponding CBAM fusion modules combine features → Fused features passed to detector head for final predictions

- **Design tradeoffs**: Modularity vs. end-to-end optimization (modular design enables pretrained models but may miss cross-modal interactions); Number of fusion modules (more modules allow finer-grained adaptation but increase complexity); Scene granularity (too fine-grained may lead to insufficient training data)

- **Failure signatures**: Poor detection performance on specific scene types (may indicate scene classifier misclassification or inadequate fusion module training); High computational cost (may indicate inefficient fusion module implementation); Degradation when adding new modalities (may indicate modality-specific fusion module design limitations)

- **First 3 experiments**: 1) Train and evaluate with single fusion module (no scene adaptation) to establish baseline; 2) Train and evaluate with scene classifier but random fusion module selection to measure classifier impact; 3) Compare CBAM fusion with alternative attention mechanisms (ECAAttn, ShuffleAttn) to validate design choice

## Open Questions the Paper Calls Out

The paper identifies several future work directions including extending the approach to additional modalities like radar and LiDAR, incorporating uncertainty quantification into the scene classifier, and exploring dynamic fusion strategies that can handle unexpected weather conditions not present in the training data.

## Limitations

- Reliance on accurate scene classification creates a potential failure point
- Performance gains demonstrated primarily on datasets with limited object diversity (pedestrians and vehicles)
- Method requires known scenes during training and may not work well in unexpected weather conditions
- Freezing strategy assumes single-modal pretrained features remain optimal across all scene conditions

## Confidence

- **High confidence**: Modular architecture design and parameter efficiency claims are well-supported by reported metrics
- **Medium confidence**: Performance improvements are demonstrated across three datasets but ablation studies for scene-specific fusion modules are somewhat limited
- **Low confidence**: Generalization claim with limited training data needs more rigorous validation across different scene types and object categories

## Next Checks

1. Quantify actual attention weight distributions across RGB and thermal features for different scene types to verify claimed modality emphasis patterns

2. Implement confidence scoring for the scene classifier and measure detection performance degradation at different confidence thresholds

3. Evaluate trained fusion modules on entirely new datasets with different object types and environmental conditions to assess true generalization capability