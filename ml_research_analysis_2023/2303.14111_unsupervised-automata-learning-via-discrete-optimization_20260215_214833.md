---
ver: rpa2
title: Unsupervised Automata Learning via Discrete Optimization
arxiv_id: '2303.14111'
source_url: https://arxiv.org/abs/2303.14111
tags:
- learning
- problem
- which
- data
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning interpretable anomaly
  detectors for sequential data. The authors propose two unsupervised learning setups
  for learning deterministic finite automata (DFAs) from unlabeled sequences, with
  the goal of detecting anomalies.
---

# Unsupervised Automata Learning via Discrete Optimization

## Quick Facts
- arXiv ID: 2303.14111
- Source URL: https://arxiv.org/abs/2303.14111
- Reference count: 13
- One-line primary result: Learning interpretable DFAs for anomaly detection via MILP with bound constraints

## Executive Summary
This paper addresses unsupervised learning of interpretable anomaly detectors for sequential data using deterministic finite automata (DFAs). The authors propose two learning setups - two-bound and single-bound - that formulate DFA learning as constraint optimization problems using mixed-integer linear programming. They introduce regularization schemes to improve DFA interpretability by maximizing self-loops, parallel edges, and sink states. Empirical evaluation on the ALFRED dataset shows that their approach can efficiently generate effective anomaly detectors, with the two-bound setup performing slightly better and faster than single-bound learning.

## Method Summary
The method learns DFAs from unlabeled sequential data using MILP encoding with automata constraints and bound constraints. The two-bound setup requires lower and upper bounds on the number of anomalies, while the single-bound setup uses a target DFA size and lower bound. Both approaches iteratively increase DFA size until a feasible solution is found. Interpretability is improved through regularization terms that encourage structural patterns like self-loops and parallel edges. The approach is implemented using Gurobi via Pyomo optimization framework and evaluated on the ALFRED dataset for anomaly detection tasks.

## Key Results
- Two-bound learning setup yields better anomaly detectors than single-bound learning
- Both methods require precise bound information to prevent degenerate DFAs
- Regularization schemes improve DFA interpretability without compromising detection accuracy
- Approach achieves competitive F1 scores on ALFRED dataset tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MILP encoding with bound constraints guarantees minimal DFAs satisfying anomaly acceptance criteria
- Mechanism: MILP model encodes deterministic transitions, run constraints, and acceptance bounds; increasing automaton size until feasibility ensures minimality
- Core assumption: A minimal DFA exists for given bounds and sample, and MILP solver can find it
- Evidence anchors: Abstract states "develop two learning algorithms based on constraint optimization" and "two-bound setup yields better detectors"; Section describes bound constraints as conjunction of Equations 6 and 7
- Break condition: If MILP solver fails for any size up to |Pref(S)|+2, no DFA satisfying bounds exists

### Mechanism 2
- Claim: Regularization terms improve DFA interpretability without compromising detection accuracy
- Mechanism: Additional penalty terms in objective function encourage structural patterns for easier visualization and understanding
- Core assumption: Interpretability gains from structural regularization outweigh potential loss in detection accuracy
- Evidence anchors: Abstract mentions "novel regularization schemes for optimization problems that improve overall interpretability"; Section introduces heuristics implemented by adding penalty terms to objective
- Break condition: If regularization significantly degrades F1 score on test data, approach becomes counterproductive

### Mechanism 3
- Claim: Two-bound learning produces more accurate detectors than single-bound learning
- Mechanism: Precise bounds constrain search space, preventing degenerate solutions that accept/reject all sequences
- Core assumption: User can provide reasonably accurate estimates of anomaly proportions in data
- Evidence anchors: Abstract states "two-bound setup is slightly faster and yields better detectors"; Section requires user to provide lower and upper bounds to prevent degenerate DFAs
- Break condition: If bounds are too loose or inaccurate, learned DFA may overfit to incorrect assumptions

## Foundational Learning

- Concept: Mixed-Integer Linear Programming (MILP)
  - Why needed here: Provides systematic way to encode DFA structure and learning constraints as optimization problems
  - Quick check question: Can you formulate a simple DFA learning problem as set of linear inequalities and integer variables?

- Concept: Deterministic Finite Automata (DFA)
  - Why needed here: Chosen as interpretable model for anomaly detection in sequential data
  - Quick check question: What makes DFAs inherently interpretable compared to neural networks?

- Concept: Prefix trees and prefix sets
  - Why needed here: Used to represent sample and compute all possible states needed for DFA construction
  - Quick check question: How does size of prefix tree relate to maximum possible DFA size for given sample?

## Architecture Onboarding

- Component map: Data preprocessing -> Prefix tree construction -> MILP formulation -> Gurobi solver -> DFA extraction -> Interpretability regularization
- Critical path: 1) Convert sample to prefix tree and extract alphabet 2) Set up MILP problem with automata and bound constraints 3) Solve with Gurobi, increasing DFA size until feasible 4) Extract DFA from solution variables 5) Apply interpretability regularization if desired
- Design tradeoffs: Bound precision vs detection accuracy (tighter bounds improve accuracy but require better prior knowledge); DFA size vs interpretability (smaller DFAs are more interpretable but may underfit); Regularization strength vs accuracy (stronger regularization improves interpretability but may reduce detection performance)
- Failure signatures: MILP solver reports infeasibility for all DFA sizes up to |Pref(S)|+2; Learned DFA achieves very low F1 score on test data; Runtime exceeds reasonable limits (100+ seconds per model)
- First 3 experiments: 1) Two-bound learning on ALFRED dataset goal 0 with exact bounds (0.09, 0.10) 2) Single-bound learning on same data with size 2 target and lower bound 0.09 3) Two-bound learning with loosened bounds (0.08, 0.11) to test robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do proposed interpretability heuristics impact anomaly detection performance on real-world sequential data?
- Basis in paper: Explicit - introduces heuristics as regularization terms to improve interpretability, but notes modifications may impede or improve model's accuracy
- Why unresolved: Empirical evaluation focuses on ALFRED dataset and doesn't specifically investigate impact of heuristics on anomaly detection performance
- What evidence would resolve it: Evaluating heuristics on diverse real-world sequential datasets and measuring impact on both interpretability and anomaly detection accuracy

### Open Question 2
- Question: Can learning algorithms be extended to handle more expressive automata classes like register automata?
- Basis in paper: Explicit - mentions extending approach to more expressive automata classes as promising direction for future research
- Why unresolved: Current approach limited to DFAs, which may not be sufficient for all types of sequential data, especially those with continuous domains
- What evidence would resolve it: Developing and evaluating proposed algorithms on register automata or other expressive automata classes for anomaly detection on continuous sequential data

### Open Question 3
- Question: How sensitive are proposed learning algorithms to choice of bounds on number of anomalies?
- Basis in paper: Explicit - demonstrates both setups require precise bound information, and loosening bounds can lead to fluctuations in F1 score
- Why unresolved: Empirical evaluation only explores impact of loosening bounds on limited set of goals and doesn't provide comprehensive analysis of sensitivity to bound choices
- What evidence would resolve it: Conducting systematic study on sensitivity of learning algorithms to different bound choices across diverse datasets and anomaly detection tasks

## Limitations
- Strong reliance on bound accuracy - requires precise estimates of anomaly proportions that may be difficult to obtain
- Computational complexity of MILP solvers may limit scalability to larger datasets
- Interpretability benefits of regularization schemes lack quantitative validation across diverse datasets

## Confidence
**High Confidence**: Core MILP formulation for DFA learning is mathematically sound and well-specified; relationship between bound accuracy and detection performance is clearly demonstrated
**Medium Confidence**: Effectiveness of interpretability regularization schemes, while plausible structurally, lacks comprehensive empirical validation across diverse datasets
**Low Confidence**: Method's scalability to larger datasets and more complex sequential patterns hasn't been thoroughly explored; ALFRED dataset may not generalize to all anomaly detection scenarios

## Next Checks
1. **Bound Robustness Test**: Systematically vary provided anomaly bounds by Â±20% and measure degradation in F1 score to quantify sensitivity to bound accuracy
2. **Interpretability Validation**: Conduct human evaluation study where participants assess interpretability of regularized vs non-regularized DFAs on standard benchmark, measuring comprehension time and accuracy
3. **Scalability Assessment**: Apply method to larger, more diverse sequential dataset (e.g., network intrusion detection) and measure runtime scaling with respect to prefix tree size and alphabet cardinality