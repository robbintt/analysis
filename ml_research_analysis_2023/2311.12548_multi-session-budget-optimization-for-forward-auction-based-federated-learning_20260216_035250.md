---
ver: rpa2
title: Multi-Session Budget Optimization for Forward Auction-based Federated Learning
arxiv_id: '2311.12548'
source_url: https://arxiv.org/abs/2311.12548
tags:
- data
- budget
- multibos-afl
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiBOS-AFL, a novel multi-session budget
  optimization strategy for forward auction-based federated learning (AFL). Unlike
  existing AFL methods that assume a single training session, MultiBOS-AFL addresses
  the practical scenario where model users can recruit data owners over multiple training
  sessions.
---

# Multi-Session Budget Optimization for Forward Auction-based Federated Learning

## Quick Facts
- **arXiv ID:** 2311.12548
- **Source URL:** https://arxiv.org/abs/2311.12548
- **Reference count:** 40
- **Key outcome:** MultiBOS-AFL achieves 12.28% higher utility, 14.52% more data acquired for a given budget, and 1.23% higher test accuracy compared to seven state-of-the-art approaches in forward auction-based federated learning

## Executive Summary
This paper introduces MultiBOS-AFL, a novel multi-session budget optimization strategy for forward auction-based federated learning (AFL). Unlike existing AFL methods that assume a single training session, MultiBOS-AFL addresses the practical scenario where model users can recruit data owners over multiple training sessions. The approach employs hierarchical reinforcement learning with two agents: an inter-session budget pacing agent to allocate the total budget across sessions, and an intra-session bidding agent to determine optimal bid prices for each data owner within a session. Extensive experiments on six benchmark datasets demonstrate significant performance improvements over state-of-the-art approaches.

## Method Summary
MultiBOS-AFL uses a hierarchical reinforcement learning framework with two Deep Q-Network-based agents. The Inter-session Budget Pacing Agent (InterBPA) allocates the total budget across multiple training sessions based on historical data and current session information. The Intra-session Bidding Agent (IntraBA) determines optimal bid prices for each data owner within a session, using reputation values calculated via Shapley Value and Beta Reputation System. The framework integrates with a generalized second-price sealed-bid forward auction mechanism, where reputation values dynamically update as federated learning progresses. The system is trained using the RMSprop optimizer on six real-world datasets.

## Key Results
- Achieves 12.28% higher utility compared to seven state-of-the-art approaches
- Acquires 14.52% more data samples for a given budget constraint
- Improves test accuracy of resulting FL model by 1.23%
- Demonstrates effectiveness across six benchmark datasets (MNIST, CIFAR-10, Fashion-MNIST, EMNIST-digits, EMNIST-letters, Kuzushiji-MNIST)

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical reinforcement learning enables separate optimization of budget pacing and bidding strategy through a two-level agent architecture. InterBPA allocates budget across sessions while IntraBA bids within sessions, each with its own state, action, and reward definitions. The core assumption is that AFL market dynamics are complex enough to require separate optimization layers that can learn from historical session data.

### Mechanism 2
Multi-session approach enables gradual DO recruitment without requiring all participants upfront. Budget is allocated across multiple training sessions, allowing the MU to bid for DOs incrementally as they become available. The core assumption is that DOs' data becomes available over time and can be recruited progressively rather than all at once.

### Mechanism 3
Reputation-based utility maximization drives better FL model outcomes through DO reputation values calculated using Shapley Value and Beta Reputation System. Bids are optimized to maximize cumulative reputation-weighted utility. The core assumption is that reputation accurately reflects DO contribution quality and can be dynamically updated during training.

## Foundational Learning

- **Markov Decision Process (MDP)**: Provides the mathematical framework for modeling the sequential decision-making problem in both budget allocation and bidding. *Quick check: What are the five components of an MDP and how do they map to the AFL bidding problem?*

- **Reinforcement Learning with Deep Q-Networks**: Enables learning optimal policies without requiring analytical solutions in the complex, dynamic AFL market. *Quick check: How does the temporal difference target y = r + γ maxa′ Q(s, a′; θ) update the Q-value function?*

- **Shapley Value for contribution measurement**: Provides a fair method to quantify each DO's contribution to the FL model performance. *Quick check: What is the computational complexity of calculating Shapley Values for n DOs and why might this be problematic?*

## Architecture Onboarding

- **Component map:** InterBPA (session-level budget allocation) → IntraBA (DO-level bidding) → Auctioneer (market price determination) → FL training process → Reputation update
- **Critical path:** State observation → Policy network → Action selection → Auction submission → Reward calculation → Parameter update
- **Design tradeoffs:** Hierarchical vs flat RL architecture; exploration-exploitation balance in ϵ-greedy policy; replay buffer size vs learning stability
- **Failure signatures:** Poor budget pacing (budget exhausted too early/late); ineffective bidding (low win rate or excessive payments); reputation calculation errors
- **First 3 experiments:**
  1. Single-session comparison with baseline methods using fixed budget
  2. Multi-session budget pacing test with varying DO availability patterns
  3. Reputation impact evaluation by comparing with random reputation assignment

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MultiBOS-AFL scale when the number of training sessions S increases beyond 100? What are the diminishing returns in terms of utility and accuracy as S grows? *Basis:* The paper mentions experiments with S=100 sessions but does not explore scalability beyond this point. *Why unresolved:* The paper focuses on validating the approach within a specific range of S values without exploring extreme cases or identifying scalability limits. *What evidence would resolve it:* Conducting experiments with S > 100 sessions and analyzing the trends in utility, accuracy, and computational efficiency to identify potential saturation points.

### Open Question 2
How does MultiBOS-AFL perform in scenarios with more than nine competing MUs? Does the bidding strategy remain effective when the competition intensifies? *Basis:* The paper uses nine MUs in experiments but doesn't explore scenarios with a larger number of competitors. *Why unresolved:* The impact of increased competition on the bidding strategy's effectiveness is not tested, leaving uncertainty about its robustness in highly competitive environments. *What evidence would resolve it:* Running experiments with varying numbers of MUs (e.g., 20, 50, 100) to assess changes in utility, data acquisition, and accuracy under heightened competition.

### Open Question 3
How sensitive is MultiBOS-AFL to the initial reputation values of DOs, especially when prior information is scarce or biased? *Basis:* The paper mentions that in cases where there is no prior information available, the default initialization for the reputation value of i is set to the uniform distribution, denoted as vi = N(0, 1) = Beta(1, 1). *Why unresolved:* The impact of different initializations on the long-term performance and convergence of the algorithm is not explored. *What evidence would resolve it:* Conducting experiments with various initial reputation settings and measuring the effects on utility, accuracy, and convergence speed over time.

### Open Question 4
Can MultiBOS-AFL be adapted to handle non-stationary environments where the data distribution of DOs changes over time? *Basis:* The paper does not address the algorithm's adaptability to dynamic changes in data distributions during the FL process. *Why unresolved:* The assumption of static or slowly changing environments may not hold in real-world scenarios, and the algorithm's performance under such conditions is untested. *What evidence would resolve it:* Implementing the algorithm in simulated environments with evolving data distributions and evaluating its ability to maintain or improve performance over time.

## Limitations

- Hierarchical reinforcement learning approach assumes stationary market dynamics across training sessions, but real-world AFL environments may exhibit non-stationary behavior that could degrade agent performance over time
- Reputation calculation using Shapley Value has exponential computational complexity (O(2^n) for n DOs), which may become prohibitive as dataset size scales
- The paper does not address potential collusion or strategic behavior among data owners in the auction mechanism

## Confidence

- **High**: Claims about improved utility and data acquisition compared to single-session approaches are well-supported by experimental results
- **Medium**: Claims about the effectiveness of hierarchical RL structure are plausible but lack direct comparison with non-hierarchical alternatives
- **Low**: Claims about reputation-based utility maximization being the primary driver of performance improvements are not independently validated

## Next Checks

1. Test MultiBOS-AFL on datasets with highly non-stationary DO availability patterns to evaluate robustness
2. Compare computational overhead of Shapley Value reputation calculation against simpler alternatives (e.g., contribution-based or quality-based metrics)
3. Conduct ablation studies removing the hierarchical structure to isolate its contribution to performance gains