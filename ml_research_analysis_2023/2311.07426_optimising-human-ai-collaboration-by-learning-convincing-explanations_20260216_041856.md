---
ver: rpa2
title: Optimising Human-AI Collaboration by Learning Convincing Explanations
arxiv_id: '2311.07426'
source_url: https://arxiv.org/abs/2311.07426
tags:
- human
- ardent
- learning
- explanations
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Ardent, a meta-system that learns to select\
  \ interpretable explanations to optimize human-AI collaboration in high-stakes decision-making.\
  \ Ardent uses a sequential Monte Carlo method to approximate the posterior over\
  \ how explanations influence human beliefs, and selects explanations in order of\
  \ their propensity to convince the human to take the AI\u2019s recommended action."
---

# Optimising Human-AI Collaboration by Learning Convincing Explanations

## Quick Facts
- arXiv ID: 2311.07426
- Source URL: https://arxiv.org/abs/2311.07426
- Authors: 
- Reference count: 28
- Key outcome: Ardent achieves 83.4% accuracy on CIFAR-10 subset, outperforming random explanations (76.1%) and humans alone (72.5%)

## Executive Summary
This paper introduces Ardent, a meta-system that learns to select interpretable explanations to optimize human-AI collaboration in high-stakes decision-making. Ardent uses sequential Monte Carlo to approximate how explanations influence human beliefs, selecting them in order of predicted convincingness. Experiments show significant performance gains over random explanation selection and individual systems, with the added benefit of identifying individual preferences and reducing explanation viewing fatigue.

## Method Summary
Ardent is a meta-system that learns which interpretable explanations to show humans in order to optimize their decision-making when collaborating with AI systems. The core approach uses sequential Monte Carlo to approximate the posterior distribution over how different explanations influence human beliefs, then selects explanations in descending order of their predicted propensity to convince humans to take the AI's recommended action. The system learns from observing human actions without requiring explicit feedback on each explanation.

## Key Results
- Ardent achieves 83.4% accuracy on a challenging CIFAR-10 subset
- Outperforms random explanation selection (76.1%) and humans alone (72.5%)
- Participants viewed 31.4% fewer explanations on average with Ardent
- System identifies individual preferences for different explanation types

## Why This Works (Mechanism)

### Mechanism 1
The system learns which explanations to show by observing the final human action, without needing explicit feedback on each explanation. Uses sequential Monte Carlo to approximate posterior over explanation effectiveness based on observed human actions. Core assumption: Human actions reflect belief updates from seeing explanations, following a logistic choice model.

### Mechanism 2
Selecting explanations in order of predicted propensity maximizes the chance of convincing the human to take the AI's recommended action. For each new context, samples explanation propensities from posterior, then shows explanations in descending order of predicted convincingness. Core assumption: The explanation most likely under the correct action's posterior is also most convincing to the human.

### Mechanism 3
The approximate Bayesian method (particle filter) enables efficient learning without retraining. Represents posterior over propensities with particles, updates weights based on likelihood of observed action given explanations. Core assumption: Particle filter can track the true posterior closely enough with reasonable particle count.

## Foundational Learning

- Concept: Thompson sampling and multi-armed bandits
  - Why needed here: Ardent uses Thompson sampling variant for exploration-exploitation tradeoff in explanation selection
  - Quick check question: How does Thompson sampling balance trying new actions versus exploiting known good ones?

- Concept: Sequential Monte Carlo methods
  - Why needed here: Particle filter approximates posterior over explanation propensities without retraining
  - Quick check question: What are the key steps in updating particle weights in a particle filter?

- Concept: Logistic choice models and belief updating
  - Why needed here: Models how humans update beliefs from seeing explanations and make final decisions
  - Quick check question: How does the logistic model relate the probability of choosing an action to the belief state?

## Architecture Onboarding

- Component map: Context processor -> Explanation selector -> Explanation display -> Action observer -> Particle filter
- Critical path: Context → Explanation selection → Display → Action observation → Particle filter update
- Design tradeoffs:
  - Computational cost vs approximation quality in particle filter
  - Number of explanations shown vs human fatigue
  - Exploration rate (α) vs convergence speed
- Failure signatures:
  - Particles collapse to single point → insufficient exploration
  - No improvement over random → incorrect model of human behavior
  - Slow convergence → need more particles or better initialization
- First 3 experiments:
  1. Run with synthetic agents (binary context/actions) to verify learning dynamics
  2. Test particle filter accuracy vs ground truth with varying particle counts
  3. Validate explanation ordering improves over random baseline in controlled setting

## Open Questions the Paper Calls Out

### Open Question 1
How does Ardent's performance scale with the dimensionality of the context space X and action space A? The paper mentions a generalization to higher dimensions but only briefly touches on this with a specific example. Systematic experiments varying these dimensions would resolve this.

### Open Question 2
How does Ardent's particle filter approximation accuracy impact its real-world performance in safety-critical applications? While the paper shows error reduces with particle count, it doesn't explore the relationship between approximation error and actual decision-making performance in high-stakes scenarios.

### Open Question 3
How does Ardent's explanation selection strategy affect human fatigue and long-term engagement compared to other approaches? The paper only provides a single experiment with a fixed number of interactions, without exploring how efficiency might change over longer periods or comparing fatigue-reduction effects to other strategies.

## Limitations
- Evaluation limited to single task domain (image classification on CIFAR-10)
- Small human participant sample (32 total) may not capture full variability
- Assumes logistic choice model for human belief updating may not represent all decision-making processes

## Confidence

- High confidence: The computational framework for sequential Monte Carlo posterior inference is well-specified and technically sound
- Medium confidence: The empirical improvements over baselines are statistically significant but may not generalize beyond tested conditions
- Low confidence: The assumption that explanation propensity under correct action directly translates to convincingness for humans

## Next Checks

1. **Cross-domain validation**: Test Ardent on a non-visual task (e.g., medical diagnosis from structured data) to assess domain generalizability of the learning approach

2. **Sample size sensitivity**: Replicate the experiment with 3-5x more participants to verify that observed performance gains are robust to individual differences in explanation interpretation

3. **Model assumption validation**: Conduct controlled experiments where humans explicitly rate explanation helpfulness, comparing these ratings against the model's predicted propensities to validate the logistic choice assumption