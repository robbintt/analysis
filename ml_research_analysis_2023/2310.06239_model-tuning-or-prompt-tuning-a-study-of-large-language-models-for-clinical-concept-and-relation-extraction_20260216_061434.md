---
ver: rpa2
title: Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical
  Concept and Relation Extraction
arxiv_id: '2310.06239'
source_url: https://arxiv.org/abs/2310.06239
tags:
- llms
- prompts
- soft
- learning
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated model-tuning and prompt-tuning strategies
  to adopt large language models (LLMs) for clinical concept extraction and relation
  extraction. We developed a soft prompt-based machine reading comprehension model,
  systematically examined 4 different strategies to adopt LLMs for clinical concept
  extraction and relation extraction and compared 7 LLMs of various sizes using two
  benchmark datasets.
---

# Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction

## Quick Facts
- arXiv ID: 2310.06239
- Source URL: https://arxiv.org/abs/2310.06239
- Reference count: 0
- Primary result: Soft prompts learned by machines outperform human-designed hard prompts for clinical concept extraction

## Executive Summary
This study systematically compares model tuning versus prompt tuning strategies for clinical concept extraction and relation extraction using large language models (LLMs). The authors develop a soft prompt-based machine reading comprehension model and evaluate it against traditional fine-tuning and hard prompt approaches across seven LLMs of varying sizes. The research demonstrates that soft prompts learned by machines can outperform human-designed hard prompts, and that frozen LLMs with soft prompts achieve state-of-the-art performance while offering superior transfer learning and few-shot learning capabilities for cross-institutional applications.

## Method Summary
The study compares four strategies for adapting LLMs to clinical information extraction: traditional fine-tuning without prompts, hard-prompt with unfrozen LLMs, soft-prompt with unfrozen LLMs, and soft-prompt with frozen LLMs. The soft prompt-based MRC model uses trainable soft prompt tokens that are concatenated with input text and fed into the LLM. For relation extraction, a verbalizer maps identified trigger concepts to relation types, and start-end classifiers identify concept spans. The model is evaluated on two benchmark datasets: the 2018 n2c2 drug-ADE dataset (505 discharge summaries) and the 2022 n2c2 SDoH dataset (MIMIC and UW clinical notes). Seven LLMs are tested including BERT, RoBERTa, and GatorTron variants ranging from 345 million to 41 billion parameters.

## Key Results
- Soft prompt-based MRC model achieved state-of-the-art performance for drug-ADE and SDoH relation extraction
- Machines learned soft prompts outperformed human-designed hard prompts
- Frozen LLMs with soft prompts showed superior transfer learning and few-shot learning abilities compared to unfrozen models
- Large frozen LLMs (billions of parameters) became competitive with unfrozen models, while small frozen LLMs showed performance gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft prompts learned by machines can outperform human-designed hard prompts for clinical concept extraction
- Mechanism: Machine learning of continuous prompt vectors allows the model to discover prompt structures that are more effective than manually crafted text prompts
- Core assumption: The search space of soft prompts is richer than that of discrete human-composed prompts, and gradient-based optimization can navigate this space effectively
- Evidence anchors: [abstract] "machines could learn better 'soft prompts' that outperform hard prompts composed by humans"; [section] "The experimental results show that the proposed soft prompt-based MRC model achieved state-of-the-art performance... outperforming traditional fine-tuning models and hard prompt-based models"

### Mechanism 2
- Claim: Frozen large language models (over billions of parameters) can achieve competitive performance to unfrozen models for clinical information extraction
- Mechanism: Large models have sufficient capacity to encode relevant task information in the soft prompts alone, making parameter updates to the base model unnecessary
- Core assumption: The representational capacity of large LLMs allows them to perform well even when frozen, as long as effective soft prompts are learned
- Evidence anchors: [abstract] "When LLMs are frozen, small (i.e., 345 million parameters) LLMs have a big gap to be competitive with unfrozen models; scaling LLMs up to billions of parameters makes frozen LLMs competitive with unfrozen LLMs"; [section] "For small LLMs (e.g., 345 million parameters), prompting with frozen LLMs has a big gap to be competitive with prompting with unfrozen LLMs. Scaling up the size of LLMs could narrow this gap and eventually achieve comparable performance"

### Mechanism 3
- Claim: Soft prompting with frozen LLMs improves transfer learning and few-shot learning for cross-institutional clinical data
- Mechanism: Keeping the LLM frozen preserves its general domain knowledge, while learned soft prompts adapt to specific tasks without overfitting to a single institution's data distribution
- Core assumption: Cross-institutional variations are better handled by prompt-level adaptation rather than full model fine-tuning, which risks overfitting to local data patterns
- Evidence anchors: [abstract] "soft prompting with frozen LLMs has better transfer learning and few-shot learning ability to improve cross-institution applications and reduce annotation costs"; [section] "When the training and test datasets are from two different institutions, prompting with frozen LLMs is remarkably better than prompting with unfrozen LLMs"

## Foundational Learning

- Concept: Soft prompt vs hard prompt
  - Why needed here: Understanding the difference is essential to grasp why soft prompts can outperform manually composed hard prompts
  - Quick check question: What is the key structural difference between a soft prompt and a hard prompt in LLM-based information extraction?

- Concept: Model tuning vs prompt tuning
  - Why needed here: This study compares both strategies, so knowing their distinction is critical for interpreting results
  - Quick check question: In prompt tuning, which parameters are updated during training when LLMs are frozen?

- Concept: Few-shot and transfer learning in clinical NLP
  - Why needed here: The paper evaluates these abilities explicitly; understanding them is necessary to assess model generalizability
  - Quick check question: Why might a frozen LLM with soft prompts generalize better across institutions than an unfrozen fine-tuned model?

## Architecture Onboarding

- Component map: Input text → Soft prompt tokens (trainable) → LLM (frozen or unfrozen) → Task-specific output layer → Predictions
- Critical path: Text and soft prompts enter the LLM, hidden states flow through task-specific classifiers to produce concept and relation spans. The verbalizer anchors relation extraction to identified trigger concepts
- Design tradeoffs: Frozen LLMs save compute and enable multi-task deployment but require large model sizes; unfrozen models are more flexible but costlier and task-specific. Soft prompts reduce manual engineering but need careful length tuning
- Failure signatures: Poor performance with small LLMs and frozen weights; sensitivity to soft prompt length; overfitting if prompts are too long relative to data size
- First 3 experiments:
  1. Compare soft prompt length (8, 16, 32, 64, 128) on a small dataset to identify optimal prompt size
  2. Test cross-institutional transfer by training on MIMIC and evaluating on UW dataset with frozen GatorTron-8.9B
  3. Evaluate few-shot learning by training on 5, 10, 20, 50, 100 samples per category and measuring F1 scores

## Open Questions the Paper Calls Out

- Question: How does the performance of soft prompt-based learning vary across different clinical subdomains or specialties (e.g., radiology, pathology, oncology)?
  - Basis in paper: [inferred] The paper evaluates models on discharge summaries and SDoH data, but does not explore performance across different clinical subdomains or specialties
  - Why unresolved: The study focuses on two specific benchmark datasets (drug-ADE and SDoH) without examining the generalizability of soft prompt-based learning across various clinical subdomains or specialties
  - What evidence would resolve it: Comparative analysis of soft prompt-based learning performance on datasets from different clinical subdomains or specialties, demonstrating the robustness and adaptability of the approach

- Question: What is the optimal length and composition of soft prompts for different clinical NLP tasks?
  - Basis in paper: [explicit] The paper mentions that the length of soft prompts affects performance, but does not provide a systematic analysis of the optimal length and composition for different clinical NLP tasks
  - Why unresolved: While the paper acknowledges the impact of prompt length on performance, it does not explore the optimal length and composition of soft prompts for various clinical NLP tasks
  - What evidence would resolve it: Empirical studies examining the performance of soft prompts with varying lengths and compositions across different clinical NLP tasks, identifying the optimal configurations for each task

- Question: How does the performance of soft prompt-based learning compare to other parameter-efficient fine-tuning methods in clinical NLP?
  - Basis in paper: [inferred] The paper focuses on soft prompt-based learning and does not compare its performance to other parameter-efficient fine-tuning methods in clinical NLP
  - Why unresolved: The study does not provide a comparative analysis of soft prompt-based learning with other parameter-efficient fine-tuning methods, such as adapter-based methods or low-rank adaptation (LoRA)
  - What evidence would resolve it: Comparative studies evaluating the performance of soft prompt-based learning against other parameter-efficient fine-tuning methods in clinical NLP tasks, providing insights into the relative effectiveness of each approach

## Limitations

- Limited implementation details: The paper lacks specific architectural details of the soft prompt-based MRC model, including exact training procedures and hyperparameter settings
- Computational accessibility: Training very large LLMs (17B and 41B parameters) requires significant computational resources that may limit reproducibility for many research groups
- Narrow cross-institutional validation: While transfer learning is demonstrated, the study only uses MIMIC and UW datasets, limiting generalizability claims across diverse healthcare institutions

## Confidence

- High confidence: Core finding that soft prompts outperform hard prompts for clinical concept extraction
- Medium confidence: Transfer learning and few-shot learning capabilities of frozen LLMs with soft prompts
- Medium confidence: Scalability findings regarding small vs large frozen models

## Next Checks

1. Replicate the soft prompt length sensitivity analysis (8, 16, 32, 64, 128 tokens) on a subset of the clinical data to verify the relationship between prompt size and performance
2. Conduct cross-institutional transfer experiments by training on MIMIC data and testing on an independent clinical dataset not used in the original study
3. Evaluate few-shot learning performance using 5, 10, 20, 50, and 100 samples per category to confirm the reported improvement in sample efficiency compared to traditional fine-tuning approaches