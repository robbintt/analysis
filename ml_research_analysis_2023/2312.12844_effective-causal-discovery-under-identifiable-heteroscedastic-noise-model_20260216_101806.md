---
ver: rpa2
title: Effective Causal Discovery under Identifiable Heteroscedastic Noise Model
arxiv_id: '2312.12844'
source_url: https://arxiv.org/abs/2312.12844
tags:
- noise
- data
- causal
- learning
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses causal structure learning under heteroscedastic
  noise, where noise variance varies across variables and observations. The authors
  introduce sufficient conditions for identifiability of a general SEM that models
  heteroscedastic noise and propose a novel formulation for DAG learning that accounts
  for varying noise variances.
---

# Effective Causal Discovery under Identifiable Heteroscedastic Noise Model

## Quick Facts
- arXiv ID: 2312.12844
- Source URL: https://arxiv.org/abs/2312.12844
- Reference count: 40
- Key outcome: This paper addresses causal structure learning under heteroscedastic noise, where noise variance varies across variables and observations. The authors introduce sufficient conditions for identifiability of a general SEM that models heteroscedastic noise and propose a novel formulation for DAG learning that accounts for varying noise variances. To address the resulting optimization challenges, they propose a two-phase iterative learning algorithm. Empirical results show that their method achieves comparable accuracy on homoscedastic noise data while significantly outperforming state-of-the-art methods on heteroscedastic noise data and real data, demonstrating its effectiveness in handling complex, realistic data scenarios.

## Executive Summary
This paper tackles the challenge of causal structure learning when noise variances are not constant across variables and observations (heteroscedasticity). The authors introduce sufficient conditions for the identifiability of a general Structural Equation Model (SEM) subject to heteroscedastic noise and propose a novel formulation for Directed Acyclic Graph (DAG) learning that accounts for varying noise variances. To address the resulting optimization challenges, they propose a two-phase iterative learning algorithm. The method is evaluated on synthetic and real datasets, demonstrating comparable performance on homoscedastic noise data while significantly outperforming state-of-the-art methods on heteroscedastic noise data.

## Method Summary
The proposed method learns causal DAGs from data with heteroscedastic noise by modeling the noise variance as a function of parent variables. The SEM is formulated with neural networks for both causal functions and variance functions, with parameter sharing between them to ensure consistent parent set selection. The two-phase iterative algorithm first estimates noise variances (Phase I) and then estimates causal functions under fixed variance (Phase II), using continuous optimization with acyclicity constraints. The method is evaluated on synthetic datasets with homoscedastic and heteroscedastic noise, as well as real datasets, using metrics like SHD, auSHDC, and auPRC.

## Key Results
- The proposed method achieves comparable accuracy on homoscedastic noise data compared to state-of-the-art methods like NOTEARS-MLP and GraN-DAG.
- On heteroscedastic noise data, the method significantly outperforms state-of-the-art methods, demonstrating its effectiveness in handling varying noise variances.
- The method shows competitive performance on real datasets (Sachs and cause-effect pairs), with lower SHD and higher auSHDC/auPRC scores compared to baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed two-phase iterative algorithm mitigates the optimization difficulty caused by simultaneously estimating mean and variance parameters.
- Mechanism: In Phase I, only the variance parameters (W(3)) are updated while holding mean parameters (W(1), W(2)) fixed, effectively decoupling the estimation of σ2 from A. In Phase II, with fixed variance estimates, the mean parameters are optimized under the acyclicity constraint.
- Core assumption: The posterior distribution p(σ2|X, A) can be approximated by the likelihood p(X|σ2, A) when a non-informative uniform prior is assumed, allowing direct maximization of variance estimates without interference from structural learning.
- Evidence anchors:
  - [section]: "To avoid the interplay between optimization over mean and variance parameters of conditional distributions, we propose to first estimate the variances σ2 and then estimate mean parameters under fixed variance."
  - [section]: "If the algorithm jointly learns A, B, the optimization process tends to minimize the negative log-likelihood loss by learning a set of B that significantly increases the estimated σ."
  - [corpus]: No direct corpus support for the specific two-phase iterative formulation; this appears to be a novel contribution.
- Break condition: If the assumed uniform prior p(σ2) is inappropriate or if the true posterior is highly sensitive to the choice of prior, the simplification p(σ2|X, A) ∝ p(X|σ2, A) may lead to poor variance estimates that compromise downstream DAG learning.

### Mechanism 2
- Claim: The general SEM formulation with heteroscedastic noise allows modeling of noise variance variation across both variables and observations, which improves DAG identifiability under realistic data conditions.
- Mechanism: By modulating the noise variance as a function σn(Xπn) of the parent variables, the model captures heteroscedasticity that arises from data collection biases or latent factors. This generalization relaxes the homoscedastic noise assumption, reducing model misspecification.
- Core assumption: The sufficient conditions for identifiability (nonlinear causal functions, piecewise variance functions, independent Gaussian noise) are satisfied by the chosen architecture (e.g., MLPs with sigmoid activations for fns and ReLU for σns).
- Evidence anchors:
  - [abstract]: "We introduce relaxed and implementable sufficient conditions, proving the identifiability of a general class of SEM subject to these conditions."
  - [section]: "To satisfy condition (3), we assume En ∼ N (0, 1) for n = 1, 2, · · · , N. Then we adopt 2-layer Multi-layer Perceptrons (MLPs) for fn(·)s and σn(·)s."
  - [corpus]: Weak; corpus neighbors discuss heteroscedastic noise but not the specific identifiable conditions or the two-phase iterative optimization.
- Break condition: If the causal functions fn are not sufficiently nonlinear or if the variance functions σn are not piecewise, the sufficient conditions for identifiability may fail, leading to DAG ambiguity.

### Mechanism 3
- Claim: Parameter sharing between causal function and variance function MLPs ensures consistency in parent set selection, simplifying optimization and avoiding additional constraints.
- Mechanism: By sharing the first layer weights W(1) between fn and σn, the formulation guarantees that both functions use the same parent variables as inputs. This design choice eliminates the need for explicit constraints enforcing consistent DAG structures between mean and variance models.
- Core assumption: The first layer of the MLP captures the essential feature representation for both the mean and variance estimation tasks, making parameter sharing effective.
- Evidence anchors:
  - [section]: "To ensure that such an assumption is always satisfied in our formulation, we design A and B to share partial parameters. In particular, we let the MLPs for fn and σn share the first layer weights."
  - [section]: "Without parameter sharing, we need to impose additional constraint that enforces the DAG structures we inferred from fns and σns separately to be consistent with each other."
  - [corpus]: No direct corpus support; this architectural detail appears to be a novel design choice.
- Break condition: If the feature extraction needs for mean and variance estimation diverge significantly, sharing the first layer may lead to suboptimal representations for one or both tasks, reducing model performance.

## Foundational Learning

- Concept: Structural Equation Models (SEMs) with additive noise
  - Why needed here: The paper builds upon SEMs as the foundation for modeling causal relationships. Understanding the standard SEM with additive noise is crucial to appreciate the generalization to heteroscedastic noise.
  - Quick check question: In a standard SEM with additive noise, what assumption is typically made about the noise variance across variables and observations?

- Concept: Directed Acyclic Graph (DAG) and acyclicity constraints
  - Why needed here: The paper aims to learn DAG structures from data. Knowledge of DAGs, topological ordering, and how to enforce acyclicity in continuous optimization is essential.
  - Quick check question: How is the acyclicity constraint typically enforced in continuous optimization methods for DAG learning, such as NOTEARS?

- Concept: Heteroscedasticity and its impact on statistical modeling
  - Why needed here: The core contribution of the paper is handling heteroscedastic noise, where noise variance varies across variables and observations. Understanding the implications of heteroscedasticity on model assumptions and identifiability is key.
  - Quick check question: What are the potential consequences of ignoring heteroscedasticity in causal structure learning?

## Architecture Onboarding

- Component map:
  - Input: Data matrix X (M x N)
  - Parameter sets:
    - W(1): Shared first layer weights (N x m1) for causal function and variance function MLPs
    - W(2): Second layer weights (m1 x 1) for causal function MLPs
    - W(3): Second layer weights (m1 x 1) for variance function MLPs
    - W(3)0: Scalar parameter ensuring positive variance
  - Loss function: Negative log-likelihood (Lnll) computed using both mean and variance estimates
  - Optimization:
    - Phase I: Optimize W(3) with W(1), W(2) fixed
    - Phase II: Optimize W(1), W(2) under acyclicity constraint with W(3) fixed
  - Output: Estimated DAG structure from W(1)

- Critical path:
  1. Initialize parameters W(1), W(2), W(3) with zeros
  2. Phase I: Iteratively update W(3) to minimize Lnll with fixed W(1), W(2)
  3. Phase II: Use augmented Lagrangian method to optimize W(1), W(2) under acyclicity constraint with fixed W(3)
  4. Check convergence; repeat Phases I and II until convergence
  5. Output DAG structure inferred from W(1)

- Design tradeoffs:
  - Parameter sharing (W(1)) simplifies optimization and ensures consistency but may limit model flexibility if mean and variance estimation have different feature requirements
  - Two-phase iterative optimization mitigates the difficulty of joint optimization but may converge to local optima and requires careful initialization
  - Using MLPs for both causal functions and variance functions provides flexibility but increases model complexity and computational cost

- Failure signatures:
  - Divergence or slow convergence in Phase I or Phase II optimization
  - Large estimated variance values that dominate the negative log-likelihood, leading to poor DAG recovery
  - DAG structure inconsistent with known causal relationships in synthetic or real data

- First 3 experiments:
  1. Run the algorithm on synthetic data with known homoscedastic noise structure to verify it can recover the true DAG and compare performance with baselines like NOTEARS-MLP.
  2. Run the algorithm on synthetic data with known heteroscedastic noise structure to verify it outperforms baselines that assume homoscedastic noise.
  3. Run the algorithm on the Sachs dataset and evaluate the recovered DAG using SHD and auSHDC metrics, comparing with reported results for baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical guarantees for the convergence of the two-phase iterative algorithm to a global optimum, rather than just a stationary point?
- Basis in paper: [explicit] The paper acknowledges that the algorithm can only guarantee convergence to a stationary solution, not necessarily a global optimum, and states that good initialization is crucial for satisfactory performance.
- Why unresolved: The paper does not provide theoretical guarantees for convergence to a global optimum. It only mentions that the algorithm can converge to a stationary point.
- What evidence would resolve it: A proof that the algorithm converges to a global optimum under certain conditions, or empirical evidence showing that the algorithm consistently finds the global optimum in a wide range of scenarios.

### Open Question 2
- Question: How does the choice of the number of hidden neurons in the MLP affect the performance of the algorithm, and is there an optimal number of hidden neurons for different types of data?
- Basis in paper: [inferred] The paper mentions that the number of hidden neurons is set to 10 for all baselines, but does not explore the impact of different numbers of hidden neurons on the algorithm's performance.
- Why unresolved: The paper does not provide an analysis of how the choice of the number of hidden neurons affects the algorithm's performance or if there is an optimal number for different types of data.
- What evidence would resolve it: An empirical study showing the performance of the algorithm with different numbers of hidden neurons for various types of data, and an analysis of the impact of the number of hidden neurons on the algorithm's performance.

### Open Question 3
- Question: How does the algorithm perform on real-world data with complex causal structures and non-linear relationships, and what are the limitations of the algorithm in such scenarios?
- Basis in paper: [explicit] The paper mentions that the algorithm is tested on two real-world datasets (Sachs and cause-effect pairs), but does not provide a comprehensive analysis of the algorithm's performance on real-world data with complex causal structures and non-linear relationships.
- Why unresolved: The paper does not provide a detailed analysis of the algorithm's performance on real-world data with complex causal structures and non-linear relationships, nor does it discuss the limitations of the algorithm in such scenarios.
- What evidence would resolve it: An empirical study showing the performance of the algorithm on a diverse set of real-world datasets with complex causal structures and non-linear relationships, and a discussion of the limitations of the algorithm in such scenarios.

## Limitations

- The theoretical guarantees for the identifiability of the model are based on sufficient conditions that may be difficult to verify in practice, especially given the use of neural network parameterizations.
- The two-phase iterative algorithm may converge to local optima, and its performance is sensitive to the initialization of parameters, particularly in Phase I.
- The empirical results on real data, while competitive, lack comparison with recent heteroscedastic-specific methods that have emerged since the paper's publication.

## Confidence

- Two-phase iterative optimization mechanism: Medium confidence
- Sufficient conditions for identifiability: Medium confidence
- Empirical results on real data: Medium confidence

## Next Checks

1. Conduct sensitivity analysis on the two-phase optimization to determine how initialization and hyperparameter choices affect final DAG recovery accuracy.
2. Test the method on datasets with varying degrees of heteroscedasticity to quantify performance degradation as noise variance variation increases.
3. Compare with recent heteroscedastic causal learning methods like the 2022 NeurIPS paper "Heteroscedastic Causal Structure Learning" to establish current state-of-the-art positioning.