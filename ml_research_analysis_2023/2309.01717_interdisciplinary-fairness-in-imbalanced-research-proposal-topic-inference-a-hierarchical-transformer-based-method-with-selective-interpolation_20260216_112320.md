---
ver: rpa2
title: 'Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference:
  A Hierarchical Transformer-based Method with Selective Interpolation'
arxiv_id: '2309.01717'
source_url: https://arxiv.org/abs/2309.01717
tags:
- research
- mixup
- interpolation
- inference
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TIPIN, a Transformer-based method for hierarchical
  multi-label classification of research proposals. The method addresses the data
  imbalance issue between interdisciplinary and non-interdisciplinary research proposals,
  which can lead to unfair assignment of reviewers.
---

# Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation

## Quick Facts
- **arXiv ID**: 2309.01717
- **Source URL**: https://arxiv.org/abs/2309.01717
- **Reference count**: 40
- **Primary result**: 9.8% improvement in F1-score over state-of-the-art for fair interdisciplinary research proposal classification

## Executive Summary
This paper addresses fairness issues in research proposal topic inference caused by data imbalance between interdisciplinary and non-interdisciplinary proposals. The authors propose TIPIN, a hierarchical Transformer-based method that models different document components separately and employs selective interpolation to generate high-quality pseudo-interdisciplinary proposals during training. Experiments show TIPIN significantly outperforms baseline methods in mitigating unfairness, achieving a 9.8% improvement in F1-score compared to previous state-of-the-art models.

## Method Summary
TIPIN uses a hierarchical Transformer architecture with word-level transformers processing each document type (title, abstract, keywords) independently, followed by a document-level transformer that aggregates these representations based on document importance. To address the severe imbalance between interdisciplinary and non-interdisciplinary proposals, TIPIN employs selective interpolation using Word-level MixUp to generate pseudo-interdisciplinary samples during training. The method also incorporates adaptive semantic aggregation based on historical prediction paths, using cross-attention mechanisms to integrate context from previous classification levels.

## Key Results
- 9.8% improvement in F1-score compared to previous state-of-the-art models
- Significantly reduced performance degradation on interdisciplinary test set (RP-IR) compared to overall test set
- Effective mitigation of unfairness caused by interdisciplinary-non-interdisciplinary imbalance

## Why This Works (Mechanism)

### Mechanism 1
Selective interpolation using Word-level MixUp generates high-quality pseudo-interdisciplinary proposals that reduce model bias in topic inference. The method identifies candidate samples with high interdisciplinarity and low co-occurrence probability with the target sample, then mixes their word-level features to create balanced training data. This addresses the core assumption that interpolating features from discipline-distinct samples will create meaningful pseudo-samples that improve model generalization on rare interdisciplinary cases.

### Mechanism 2
Hierarchical Transformer architecture with separate word-level and document-level transformers effectively models heterogeneous textual components of research proposals. Word-level transformers process each document type independently to capture semantic information, while document-level transformers aggregate these representations based on document importance. This preserves information that would be lost through naive concatenation, based on the assumption that treating different document types as distinct semantic sources is beneficial.

### Mechanism 3
Adaptive semantic aggregation based on historical prediction paths improves fine-grained discipline classification accuracy. The model uses predicted label embeddings from previous levels to query and aggregate relevant semantic information from document representations through cross-attention. This leverages the assumption that historical prediction context provides useful guidance for determining which document content is most relevant at each classification level.

## Foundational Learning

- **Concept: Hierarchical multi-label classification**
  - Why needed here: Research proposals have discipline codes organized in a tree structure where each proposal can belong to multiple disciplines at different levels
  - Quick check question: What's the difference between flat multi-label classification and hierarchical multi-label classification in the context of research proposals?

- **Concept: Text data augmentation techniques (MixUp, CutMix)**
  - Why needed here: The dataset has severe imbalance between interdisciplinary and non-interdisciplinary proposals, requiring synthetic data generation
  - Quick check question: How does Word-level MixUp differ from Document-level MixUp in terms of feature interpolation?

- **Concept: Cross-attention mechanisms**
  - Why needed here: To adaptively aggregate semantic information based on historical predictions during hierarchical classification
  - Quick check question: What role does cross-attention play in connecting historical predictions with current document representations?

## Architecture Onboarding

- **Component map**: Input documents → Word-level Transformers (per document type) → Document-level Transformer (aggregation) → Selective Interpolation → Cross-attention with historical predictions → Level-wise prediction heads → Output label sets
- **Critical path**: Text → Word-level encoding → Document aggregation → Historical context integration → Final classification
- **Design tradeoffs**: Separate document modeling preserves information but increases complexity; selective interpolation adds training stability but requires careful candidate selection
- **Failure signatures**: Poor performance on interdisciplinary samples, unstable training loss, attention maps not focusing on domain-specific keywords
- **First 3 experiments**:
  1. Test baseline hierarchical Transformer without selective interpolation on RP-IR dataset to establish performance degradation
  2. Compare different MixUp variants (Word-level vs Document-level vs Manifold) on the same dataset
  3. Analyze attention patterns on interdisciplinary vs non-interdisciplinary samples to verify adaptive aggregation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of TIPIN compare to other state-of-the-art models when trained and tested on a balanced dataset of interdisciplinary and non-interdisciplinary research proposals? The paper does not provide a direct comparison with other models on a balanced dataset, only showing performance on the imbalanced RP-IR dataset.

### Open Question 2
How does the selective interpolation strategy in TIPIN affect the model's ability to generalize to new, unseen data? The paper mentions the strategy is designed to generate high-quality pseudo-research proposals but does not discuss how this affects generalization to new data.

### Open Question 3
How does the performance of TIPIN vary with different levels of interdisciplinary research proposals? The paper does not provide information on how model performance varies across different degrees of interdisciplinarity within the dataset.

## Limitations

- Implementation details for critical components like selective interpolation are insufficiently specified for exact reproduction
- Limited validation of whether generated pseudo-interdisciplinary proposals are truly representative of real interdisciplinary research
- No empirical evidence showing that incorrect early predictions don't propagate errors through the hierarchical classification

## Confidence

**High confidence**: The general approach of using hierarchical transformers for multi-label classification of research proposals is sound and well-established in the literature.

**Medium confidence**: The selective interpolation technique for addressing class imbalance shows promise based on reported results, but implementation details are insufficient for verification.

**Medium confidence**: The adaptive semantic aggregation mechanism is theoretically justified but lacks direct empirical validation of its effectiveness in the specific context of interdisciplinary research proposal classification.

## Next Checks

1. Implement ablation study: Create controlled experiments removing selective interpolation and adaptive aggregation to quantify their individual contributions to the 9.8% improvement.

2. Error analysis on interdisciplinary samples: Conduct detailed analysis of prediction failures on interdisciplinary proposals to identify whether errors stem from feature interpolation artifacts, hierarchical dependency issues, or fundamental limitations in capturing cross-disciplinary semantic relationships.

3. Cross-dataset generalization test: Evaluate the model on multiple research proposal datasets with different disciplinary structures to verify that the approach generalizes beyond the specific RP-IR dataset and isn't overfitting to its particular characteristics.