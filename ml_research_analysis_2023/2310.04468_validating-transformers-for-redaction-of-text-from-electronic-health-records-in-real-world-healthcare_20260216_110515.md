---
ver: rpa2
title: Validating transformers for redaction of text from electronic health records
  in real-world healthcare
arxiv_id: '2310.04468'
source_url: https://arxiv.org/abs/2310.04468
tags:
- health
- data
- london
- information
- redaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents AnonCAT, a transformer-based model for automated
  redaction of personally identifiable information (PHI) from electronic health records
  (EHRs). The model was trained and tested on datasets from three UK hospitals with
  different EHR systems.
---

# Validating transformers for redaction of text from electronic health records in real-world healthcare

## Quick Facts
- arXiv ID: 2310.04468
- Source URL: https://arxiv.org/abs/2310.04468
- Reference count: 19
- Primary result: Transformer-based model AnonCAT achieves high recall (>0.95) for redacting PHI from EHRs across three UK hospitals using as few as 150-300 documents for fine-tuning

## Executive Summary
This study presents AnonCAT, a transformer-based model for automated redaction of personally identifiable information (PHI) from electronic health records (EHRs). The model was trained and tested on datasets from three UK hospitals with different EHR systems. Using an iterative annotation approach, the model achieved high recall (>0.95) across all hospitals, demonstrating the effectiveness of deep learning techniques for redaction in diverse healthcare settings. The study also highlights the importance of continual fine-tuning and auditing to maintain model performance over time.

## Method Summary
The method employs a pre-trained RoBERTa-Large transformer fine-tuned on annotated EHR documents from three UK hospitals. The approach uses an iterative annotation workflow where annotators review model predictions to correct their own errors, improving dataset quality. A post-processing step biases predictions toward positive classes to increase recall at the cost of precision. The model achieves high performance with as few as 150-300 documents for fine-tuning, making it practical for real-world healthcare environments.

## Key Results
- Achieved recall of 0.99, 0.99, and 0.96 across three UK hospitals with different EHR systems
- As few as 150-300 documents needed to fine-tune the model for a new dataset
- Iterative annotation approach identified that 90% of annotation errors were annotators overlooking PHI rather than false annotations
- Code and tutorials available on GitHub for reproducibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pre-trained transformer on a small annotated dataset improves recall and precision for redaction tasks.
- Mechanism: The model leverages general language understanding from pre-training and adapts to the specific terminology and structure of healthcare records through fine-tuning, achieving high performance with as few as 150-300 documents.
- Core assumption: Pre-trained transformers generalize well to new domains and can be efficiently adapted with limited labeled data.
- Evidence anchors:
  - [abstract] "The model achieved high performance in all three hospitals with a Recall of 0.99, 0.99 and 0.96."
  - [section] "We show that as few as 150-300 documents are needed to fine-tune a redaction model on a new dataset, with a higher performance the more documents available for fine-tuning."
  - [corpus] Weak - neighboring papers focus on EHR modeling and privacy but do not directly support transformer fine-tuning efficiency.
- Break condition: If the target domain's language and structure differ too greatly from the pre-training data, fine-tuning may not be sufficient without larger datasets.

### Mechanism 2
- Claim: Iterative annotation improves dataset quality and model performance by correcting human errors.
- Mechanism: Annotators review model predictions to identify false positives and negatives, correcting their own mistakes and improving label accuracy.
- Core assumption: Annotators can reliably identify their own errors when prompted by model predictions.
- Evidence anchors:
  - [section] "most (90%) of mistakes are annotators overlooking a piece of PHI data, and rarely wrongly annotating something."
  - [section] "we argue that our iterative approach is a more efficient way of creating a well-annotated dataset, than the more standard double annotation approach."
  - [corpus] No direct support - neighbors focus on EHR generation and privacy, not annotation workflows.
- Break condition: If annotators cannot recognize their errors or if the model's errors are too subtle, the iterative approach may not improve quality.

### Mechanism 3
- Claim: Biasing the model towards predicting positive classes increases recall at the cost of precision.
- Mechanism: The post-processing step reduces the probability of the negative class by a factor λ, increasing the likelihood of detecting PHI.
- Core assumption: In redaction tasks, missing PHI (false negatives) is more critical than over-redacting (false positives).
- Evidence anchors:
  - [section] "our modification decreases the probability of the negative class by the factor λ ∈ [0, 1], Eq. (1). This often means we are increasing recall at the cost of precision."
  - [section] "Recall merged ignores mistakes in-between concepts and only accounts for mistakes where something was supposed to be detected as PHI but was not."
  - [corpus] No direct support - neighboring papers do not discuss class probability adjustment.
- Break condition: If over-redaction becomes unacceptable (e.g., in clinical workflows), the bias adjustment may not be suitable.

## Foundational Learning

- Concept: Transformer-based NER and sequence labeling
  - Why needed here: Redaction is treated as a named entity recognition task where PHI entities are detected and replaced.
  - Quick check question: Can you explain how token classification differs from sequence classification in transformer models?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The study leverages pre-trained RoBERTa and fine-tunes it on healthcare data, requiring understanding of parameter-efficient adaptation.
  - Quick check question: What are the key differences in data requirements and computational cost between fine-tuning and training a transformer from scratch?

- Concept: Ontology-based concept databases
  - Why needed here: The model uses a structured ontology to define PHI categories, requiring understanding of how hierarchical concepts are used in NER.
  - Quick check question: How does using an ontology for concept definitions improve consistency in entity recognition compared to flat label lists?

## Architecture Onboarding

- Component map: Pre-trained transformer (RoBERTa-Large) → Concept database (ontology) → Post-processing bias layer → Iterative annotation loop
- Critical path: 1) Load pre-trained model and concept database 2) Fine-tune on annotated dataset 3) Apply post-processing bias adjustment 4) Run iterative annotation to improve dataset quality 5) Deploy and monitor performance
- Design tradeoffs:
  - Using a general-purpose transformer (RoBERTa) vs. a clinically specialized one (BioClinicalBERT)
  - High recall with bias adjustment vs. high precision without
  - Iterative annotation vs. double annotation for dataset quality
- Failure signatures:
  - Low recall on specific PHI types → Check concept database coverage
  - High false positives → Adjust post-processing bias or retrain with more diverse examples
  - Performance drop over time → Re-run iterative annotation to address data drift
- First 3 experiments:
  1. Test base RoBERTa on KCH data without fine-tuning to establish baseline
  2. Fine-tune on 150 documents and measure performance gain
  3. Apply post-processing bias and compare recall/precision tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of documents required for fine-tuning the model in different healthcare settings?
- Basis in paper: [explicit] The paper states that as few as 150-300 documents are needed to fine-tune the model for a new dataset, but it doesn't specify the optimal number for different healthcare settings.
- Why unresolved: The study only tested three hospitals, and the number of documents needed for optimal performance in other healthcare settings is unknown.
- What evidence would resolve it: Conducting studies with more diverse healthcare settings and varying the number of documents used for fine-tuning would provide evidence for the optimal number of documents required for different healthcare settings.

### Open Question 2
- Question: How does the performance of the model change over time as data drift occurs?
- Basis in paper: [explicit] The paper mentions that data drift can affect the performance of redacting algorithms over time, but it doesn't provide information on how the model's performance changes over time.
- Why unresolved: The study doesn't include long-term monitoring of the model's performance, and the impact of data drift on the model's performance is unknown.
- What evidence would resolve it: Implementing a long-term monitoring system to track the model's performance over time and conducting studies on the impact of data drift on the model's performance would provide evidence for how the performance changes over time.

### Open Question 3
- Question: How does the model perform in healthcare settings with high socio-ethnic diversity?
- Basis in paper: [explicit] The paper mentions that healthcare economies with high socio-ethnic diversity may have more varied structures of personal health information, but it doesn't provide information on how the model performs in such settings.
- Why unresolved: The study only tested three UK hospitals, and the model's performance in healthcare settings with high socio-ethnic diversity is unknown.
- What evidence would resolve it: Conducting studies with healthcare settings that have high socio-ethnic diversity and testing the model's performance in those settings would provide evidence for how the model performs in such environments.

## Limitations

- The iterative annotation approach's efficiency depends on annotators' ability to recognize their own errors, which may not generalize across different annotation teams or domains
- The post-processing bias adjustment that trades precision for recall lacks empirical exploration of its practical impact on clinical workflows
- The claim that 150-300 documents are sufficient for fine-tuning may not hold for EHR systems with significantly different structures or terminologies than those studied

## Confidence

**High Confidence**: The core finding that transformer-based models can achieve high recall (>0.95) for PHI redaction across multiple hospitals with different EHR systems is well-supported by the empirical results.

**Medium Confidence**: The efficiency claims regarding the number of documents needed for fine-tuning (150-300) and the effectiveness of the iterative annotation approach are based on the specific context of UK hospital EHRs.

**Low Confidence**: The mechanism by which the post-processing bias adjustment improves recall at the cost of precision is theoretically sound but lacks empirical exploration of the tradeoff's practical implications.

## Next Checks

1. **Cross-domain validation**: Test the iterative annotation approach with annotators from different healthcare systems or non-medical domains to verify whether the 90% error recognition rate holds across diverse contexts and whether the approach remains more efficient than double annotation.

2. **Clinical workflow impact study**: Conduct a user study with clinical staff to evaluate the practical impact of false positives from the bias-adjusted model, measuring how over-redaction affects clinical decision-making and workflow efficiency in real healthcare settings.

3. **Generalization boundary testing**: Systematically vary the linguistic and structural similarity between pre-training data and target EHR systems to identify the precise conditions under which the 150-300 document fine-tuning threshold breaks down, establishing clear boundaries for the model's domain adaptation capabilities.