---
ver: rpa2
title: Efficient End-to-End Visual Document Understanding with Rationale Distillation
arxiv_id: '2311.09612'
source_url: https://arxiv.org/abs/2311.09612
tags:
- rationale
- answer
- rationales
- image
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Rationale Distillation (RD), a method that
  teaches small image-to-text models to predict intermediate rationales (textual evidence
  or programs) before answering questions on visual documents. RD uses external tools
  (OCR, LLMs) to generate rationales for training examples, then trains a student
  model to predict both rationales and answers.
---

# Efficient End-to-End Visual Document Understanding with Rationale Distillation

## Quick Facts
- arXiv ID: 2311.09612
- Source URL: https://arxiv.org/abs/2311.09612
- Reference count: 15
- Primary result: Small image-to-text models achieve 4-5% accuracy gains on text-heavy datasets and 3-8% on chart reasoning tasks using Rationale Distillation

## Executive Summary
This paper introduces Rationale Distillation (RD), a method that enables small image-to-text models to perform complex visual document understanding by learning to predict intermediate rationales before answering questions. The approach uses external tools like OCR and LLMs to generate textual evidence or programs as rationales for training examples, then trains a student model to predict both rationales and answers. On three visual document understanding benchmarks (InfoVQA, DocVQA, ChartQA), PIX2STRUCT-based models finetuned with RD outperform baseline models by 4-5% absolute accuracy on text-heavy datasets and 3-8% on chart reasoning tasks, with only 1% additional computational cost. The method demonstrates that small models can learn sophisticated reasoning capabilities by distilling knowledge from more powerful external tools.

## Method Summary
Rationale Distillation (RD) trains small image-to-text models to predict intermediate rationales (textual evidence or programs) before answering questions on visual documents. The method uses external tools including OCR, LLMs (PaLM 2-L), and multimodal verifiers (PaLI-X) to generate and filter rationales for training examples. The student model learns to predict rationales from image-question pairs, then generates answers conditioned on these rationales. Data augmentation via image cropping increases training diversity, while a multimodal verifier filters out irrelevant or low-quality rationales. The approach enables efficient end-to-end visual document understanding with minimal computational overhead at inference time.

## Key Results
- PIX2STRUCT with RD outperforms baseline Ans-Only models by 4-5% absolute accuracy on text-heavy datasets (InfoVQA, DocVQA)
- RD achieves 3-8% improvement on chart reasoning tasks (ChartQA)
- The computational cost of RD is only 1% additional overhead compared to baseline models
- Models trained with RD demonstrate robust performance on both text-heavy and chart-based visual documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rationale Distillation enables small models to mimic complex reasoning by learning intermediate text representations generated by external tools.
- Mechanism: The student model learns to predict rationales (textual evidence or programs) from image and question pairs, then generates answers conditioned on these rationales. This mirrors the reasoning pipeline of larger systems that use OCR and LLMs.
- Core assumption: The rationales generated by external tools are sufficiently accurate and compact to serve as effective training signals for a smaller model.
- Evidence anchors:
  - [abstract] "We propose Rationale Distillation (RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal models as intermediate 'rationales', and trains a small student model to predict both rationales and answers."
  - [section] "We use a relatively small image-to-text model to perform VDU tasks by decomposing them into rationale prediction and answer steps, predicting the rationale and answer in sequence."
  - [corpus] Weak or missing; no directly related papers found in the corpus.

### Mechanism 2
- Claim: Data augmentation via image cropping increases training diversity and improves the student model's robustness to partial information.
- Mechanism: By cropping images along the longer edge and generating rationales for each crop, the model learns to handle variable-quality inputs and incomplete context.
- Core assumption: Cropped images still contain sufficient information to generate useful rationales and answers, even if partial.
- Evidence anchors:
  - [section] "To overcome this challenge, we devise a data augmentation approach based on image cropping to significant enlarge the number of examples available for rationale prediction."
  - [section] "We simplify the image presented to the student by cropping the input image along the longer edge (height or width, whichever is longer)."
  - [corpus] Weak or missing; no directly related papers found in the corpus.

### Mechanism 3
- Claim: The multimodal verifier filters out irrelevant or low-quality rationales, ensuring the student model is trained only on helpful examples.
- Mechanism: A powerful multimodal model evaluates whether a rationale increases the probability of the correct answer and whether greedy decoding with the rationale yields the gold answer.
- Core assumption: The verifier's assessment of rationale quality correlates with actual usefulness for the student model's reasoning.
- Evidence anchors:
  - [section] "To determine the helpfulness of the rationale generated by other tools, we employ a multi-task trained, large and powerful multimodal model PaLI-X 55B."
  - [section] "We construct the text encoder input in the following format: [rationale] Answer in en: [question]"
  - [corpus] Weak or missing; no directly related papers found in the corpus.

## Foundational Learning

- Concept: Supervised learning with intermediate supervision
  - Why needed here: The model must learn to predict both rationales and answers, requiring supervision at both stages.
  - Quick check question: What are the two main outputs the student model predicts during training?
    - Answer: The rationale and the answer.

- Concept: Data augmentation and filtering
  - Why needed here: To handle the limited size and noise in the original dataset, especially when rationales are imperfect.
  - Quick check question: What is the purpose of cropping images and filtering rationales?
    - Answer: To increase training diversity and remove irrelevant or low-quality examples.

- Concept: Multimodal model verification
  - Why needed here: To assess the quality of rationales generated by external tools before using them for training.
  - Quick check question: How does the multimodal verifier decide if a rationale is useful?
    - Answer: By checking if it increases the probability of the correct answer and if greedy decoding with it yields the gold answer.

## Architecture Onboarding

- Component map: Image + Question -> PIX2STRUCT encoder -> Rationale predictor -> Answer predictor -> Final answer
- Critical path:
  1. Preprocess image with OCR and LLM to generate rationales.
  2. Augment dataset via cropping and filter using multimodal verifier.
  3. Train student model to predict rationale and answer.
  4. Evaluate with greedy decoding on full images.
- Design tradeoffs:
  - Accuracy vs. computational cost: Using external tools at training time improves accuracy but adds cost; inference remains efficient.
  - Rationale length vs. model capacity: Short rationales (under 100 tokens) balance informativeness and model constraints.
  - Augmentation vs. noise: Cropping increases diversity but may introduce irrelevant examples if not filtered.
- Failure signatures:
  - Student rationales are incoherent or irrelevant.
  - Model performance drops on cropped image variants.
  - No improvement over baseline Ans-Only model.
- First 3 experiments:
  1. Train PIX2STRUCT with QRA task only (predict question, rationale, answer) and compare to Ans-Only baseline.
  2. Add APR task (answer with provided rationale) and measure robustness to noisy rationales.
  3. Apply image cropping augmentation with filtering and evaluate on dev set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does rationale distillation performance compare when using smaller language models (e.g., 7B-13B parameter models) instead of PaLM 2-L for rationale generation?
- Basis in paper: [inferred] The paper mentions using PaLM 2-L for rationale generation but doesn't explore how smaller or differently sized LLMs would perform
- Why unresolved: The paper only uses PaLM 2-L for rationale generation, leaving open whether smaller or differently sized models could achieve similar results
- What evidence would resolve it: Systematic experiments comparing different LLM sizes (7B, 13B, 70B parameters) for rationale generation across the three benchmarks

### Open Question 2
- Question: What is the impact of rationale distillation on model generalization to unseen visual document types not present in the training data?
- Basis in paper: [inferred] The paper focuses on three specific document types but doesn't test cross-domain generalization
- Why unresolved: The experiments are limited to infographics, scanned documents, and charts, without testing how well the approach transfers to other document types
- What evidence would resolve it: Evaluation on held-out document types (e.g., scientific papers, resumes, forms) not seen during training

### Open Question 3
- Question: How does rationale distillation affect the model's ability to handle multi-hop reasoning tasks that require chaining multiple rationales?
- Basis in paper: [inferred] The paper demonstrates rationale prediction for single-step reasoning but doesn't explore complex multi-hop scenarios
- Why unresolved: All experiments involve single-step reasoning from rationales to answers, leaving the multi-hop capability unexplored
- What evidence would resolve it: Experiments on datasets requiring multiple intermediate reasoning steps (e.g., multi-hop VQA datasets)

### Open Question 4
- Question: What is the relationship between rationale quality and model performance when rationales are intentionally degraded or made ambiguous?
- Basis in paper: [explicit] The paper mentions using low-quality rationales for training but doesn't systematically vary rationale quality
- Why unresolved: While the paper uses "noisy rationales" for training, it doesn't explore how different levels of rationale quality affect performance
- What evidence would resolve it: Controlled experiments with rationales of varying quality levels (clear, partially ambiguous, completely incorrect)

## Limitations

- The performance improvement over baseline models is relatively small (1% absolute difference on test sets), raising questions about the cost-benefit ratio
- The evaluation scope is limited to three specific datasets and doesn't explore cross-domain generalization to other visual document types
- The computational cost analysis is incomplete, focusing only on training time without considering full lifecycle costs

## Confidence

- **High Confidence**: The experimental methodology is sound, with clear implementation details and appropriate evaluation metrics across multiple datasets.
- **Medium Confidence**: The claim that RD enables small models to learn complex reasoning is supported but could benefit from more ablation studies on different model sizes and task types.
- **Medium Confidence**: The assertion that RD provides significant accuracy improvements (4-5% on text-heavy datasets, 3-8% on chart reasoning tasks) is supported by the results but the margin is relatively small.

## Next Checks

1. **Ablation Study on Model Size**: Test whether the performance gains from RD scale with model size by comparing small (2B), medium (8B), and large (30B) PIX2STRUCT variants, both with and without RD.

2. **Cross-Domain Transfer**: Evaluate whether models trained with RD on InfoVQA/DocVQA transfer better to new visual document understanding tasks, measuring zero-shot performance on unseen document types.

3. **Cost-Benefit Analysis**: Measure the full computational lifecycle cost (training + inference) of RD compared to baseline approaches, including the overhead of external tool calls during training and potential inference-time optimizations.