---
ver: rpa2
title: How Discriminative Are Your Qrels? How To Study the Statistical Significance
  of Document Adjudication Methods
arxiv_id: '2308.09340'
source_url: https://arxiv.org/abs/2308.09340
tags:
- systems
- methods
- adjudication
- cited
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new methodology to evaluate adjudication
  methods in information retrieval by focusing on the preservation of statistically
  significant differences between systems, rather than just system rankings. Traditional
  evaluation using Kendall's tau correlation overlooks whether low-cost judgements
  maintain the true significance of system performance differences.
---

# How Discriminative Are Your Qrels? How To Study the Statistical Significance of Document Adjudication Methods

## Quick Facts
- arXiv ID: 2308.09340
- Source URL: https://arxiv.org/abs/2308.09340
- Reference count: 40
- Key outcome: Traditional Kendall's tau correlation is insufficient for evaluating adjudication methods; a new approach using significance preservation metrics is proposed and validated.

## Executive Summary
This paper introduces a novel methodology to evaluate adjudication methods in information retrieval test collections by focusing on the preservation of statistically significant differences between systems, rather than just ranking stability. Traditional evaluation using Kendall's tau correlation overlooks whether low-cost judgments maintain the true significance of system performance differences. The proposed approach uses precision, recall, and agreement/disagreement counts to measure how well adjudication methods preserve significant pairwise differences as detected by a full top-k pool. Experiments on TREC-8 and TREC DL 2021 collections show that while Kendall's tau is high across methods, many fail to preserve true significance, especially at low budgets.

## Method Summary
The authors propose a methodology to evaluate adjudication methods by comparing the set of statistically significant system pairs detected under gold qrels versus those detected under low-cost adjudication qrels. The approach involves: 1) Running adjudication methods to generate reduced pools at various budget levels, 2) Computing system scores using MAP and NDCG, 3) Applying randomized Tukey HSD test (1M permutations) to detect significant differences for both gold and reduced pools, and 4) Calculating metrics including Kendall's tau, precision/recall of significant pairs, agreement/disagreement counts, and publication bias. The method is tested on TREC-8 (deep pool) and TREC DL 2021 (shallow pool) collections with 8 adjudication methods across different budget levels.

## Key Results
- Kendall's tau correlation is high (>0.90) across all adjudication methods, but many fail to preserve true significance of pairwise differences
- Publication bias is high at low budgets, ranging between 25% and 50%, indicating many published results would not be supported by gold qrels
- Shallow pooling (TREC DL 2021) shows notably worse performance in preserving significance compared to deep pooling (TREC-8)
- Not all relevant documents are equally discriminative in finding significantly different pairs; more relevant documents found does not necessarily lead to more active agreements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional Kendall's tau correlation does not capture preservation of statistically significant differences between systems.
- Mechanism: The proposed method compares the set of significantly different system pairs detected under gold qrels versus those detected under low-cost adjudication qrels using precision, recall, and agreement/disagreement counts. This captures whether the low-cost method maintains the true significance of pairwise differences.
- Core assumption: The gold qrels represent the ground truth for system significance; any deviation in the low-cost method indicates a loss of discriminative power.
- Evidence anchors:
  - [abstract]: "This traditional analysis ignores whether and how the low-cost judgements impact on the statistically significant differences among systems with respect to the full collection."
  - [section]: "Kendall'sðœ does not allow us to know whether the compared algorithms preserve the same statistically significant differences as the gold qrels."
- Break condition: If the significance testing method or the gold qrels themselves are biased or flawed, the preservation metric will be unreliable.

### Mechanism 2
- Claim: Not all relevant documents contribute equally to detecting significant differences between systems.
- Mechanism: By counting agreements (AA), mixed agreements (MA), and mixed disagreements (MD), the method reveals which adjudication methods find the most discriminative relevant documents, even if they find fewer total relevant documents.
- Core assumption: Relevant documents appearing at different ranks in result lists contribute differently to system score differences and thus to significance detection.
- Evidence anchors:
  - [section]: "We can observe something similar with NDCG: founding more relevant documents does not necessarily mean more AA."
  - [section]: "Overall, these observations suggest that not all the relevant documents are equally discriminative in finding significantly different pairs."
- Break condition: If the effectiveness measure (e.g., MAP, NDCG) used does not reflect the actual system differences well, the discriminative power of documents will be misestimated.

### Mechanism 3
- Claim: Shallow pooling and low budgets can introduce high publication bias and mixed disagreements, reducing reliability.
- Mechanism: The bias metric (1 - AA / (AA + AD + MAL + MDL)) quantifies the chance that a researcher would publish a result that is not supported by the gold qrels, highlighting when low-cost methods are unreliable.
- Core assumption: Researchers rely on statistical significance to publish findings; if low-cost methods produce many false positives or swaps in significant pairs, the published conclusions may be invalid.
- Evidence anchors:
  - [section]: "Publication bias is exceedingly high, especially at low budgets, ranging between 25% and 50%."
  - [section]: "On a positive side, AD is always 0, also for DL 2021."
- Break condition: If the number of significant pairs is very small or the pool is too shallow, the bias metric may become unstable or misleading.

## Foundational Learning

- Concept: Family-Wise Error Rate (FWER) control in multiple comparisons
  - Why needed here: The paper uses Tukey HSD test with many pairwise comparisons; without FWER control, the chance of false positives skyrockets.
  - Quick check question: If you test 6 system pairs at Î±=0.05 without correction, what is the approximate FWER? (Answer: ~26%)

- Concept: Mixed agreements vs active disagreements in significance testing
  - Why needed here: These measures distinguish between missing a true significant difference (MAG) and falsely declaring one (MAL), critical for evaluating adjudication quality.
  - Quick check question: In the context of adjudication evaluation, what does a high MAL indicate? (Answer: Low-cost method introduces false significant differences not present in gold qrels)

- Concept: Kendall's tau correlation limitations
  - Why needed here: Understanding why tau alone is insufficient helps justify the new significance-preserving metrics.
  - Quick check question: What does Kendall's tau measure that the new method does not? (Answer: Only ranking order, not significance of differences)

## Architecture Onboarding

- Component map: Gold qrels â†’ system scores â†’ Tukey HSD significance test â†’ significant pairs â†’ reduced pools from adjudication methods â†’ system scores â†’ Tukey HSD significance test â†’ significant pairs â†’ comparison of significant pairs â†’ agreement/disagreement metrics â†’ publication bias calculation
- Critical path: 1) Load gold and adjudication qrels; 2) Compute system scores for each topic; 3) Perform Tukey HSD significance test; 4) Generate sets of significant pairs; 5) Compare sets and compute metrics
- Design tradeoffs: Full top-k pooling gives best significance preservation but is expensive; shallow pooling saves cost but increases bias and mixed disagreements
- Failure signatures: High publication bias (>30%) or high mixed disagreements (MD) indicate unreliable adjudication; Kendall's tau close to 1 but low precision/recall of significance indicates ranking stability without discriminative power
- First 3 experiments:
  1. Run the full evaluation pipeline on TREC-8 with 100 judgments per topic and compare Kendall's tau vs significance preservation metrics
  2. Repeat with NDCG instead of MAP to confirm metric independence
  3. Evaluate non-pooled systems to test reusability of adjudication methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific relevant documents contribute differently to detecting statistically significant differences between systems, beyond just their quantity?
- Basis in paper: [explicit] The paper found that more relevant documents found does not necessarily lead to more active agreements (AA), suggesting not all relevant documents are equally discriminative in finding significantly different pairs.
- Why unresolved: While the paper identifies this phenomenon, it does not investigate which specific relevant documents are most discriminative or why some contribute more to significance detection than others.
- What evidence would resolve it: Systematic analysis of which relevant documents at different ranks most frequently contribute to significant differences, potentially through regression analysis or feature importance ranking of document positions and characteristics.

### Open Question 2
- Question: What is the relationship between shallow pooling depth and the reliability of adjudication methods in preserving statistical significance, particularly for deep neural retrieval models?
- Basis in paper: [explicit] The paper tested TREC DL 2021 with shallow depth-10 pooling and found notably worse performance in preserving significance compared to TREC-8's deep pool, with high publication bias at low budgets.
- Why unresolved: The experiments only compared one shallow pool collection; the relationship between pool depth, system type, and significance preservation remains unclear.
- What evidence would resolve it: Controlled experiments varying pool depth systematically across different types of retrieval models (traditional vs. neural) to establish thresholds where shallow pooling becomes problematic.

### Open Question 3
- Question: Does the multi-armed bandit approach inherently introduce systematic bias in detecting significant differences between systems compared to traditional top-k pooling?
- Basis in paper: [inferred] The paper found that bandit-based methods like MoveToFront and Thompson Sampling achieved high precision/recall in detecting significance but also detected some significant differences not present in gold qrels, raising questions about potential bias.
- Why unresolved: While the paper measures precision/recall, it does not specifically test whether bandit methods have systematic tendencies to over-detect or under-detect significance for certain types of system comparisons.
- What evidence would resolve it: Comparative analysis of which system pairs are most frequently misclassified as significant by different adjudication methods, testing for systematic patterns in error types.

## Limitations
- The study assumes gold qrels are the ground truth, but these are also sampled (full top-k pool) and may miss relevant documents or contain noise
- Exact implementations of some adjudication methods (e.g., MTF, MM-NS, TS-NS, Hedge, NTCIR) are not fully specified, creating reproducibility challenges
- The significance testing relies on Tukey HSD with FWER control, but the effectiveness depends on the underlying score distributions and the number of system pairs being compared

## Confidence
- High: The core finding that Kendall's tau alone is insufficient for evaluating adjudication methods is well-supported by the experiments
- Medium: The publication bias and mixed disagreement metrics are novel and useful, but their interpretation may depend on the specific task and collection characteristics
- Low: The generalizability of the results to other IR tasks (e.g., web search, enterprise search) and other evaluation measures (e.g., precision at k) is uncertain

## Next Checks
1. **Robustness to gold qrel noise**: Intentionally inject known errors into the gold qrels and measure how this affects the significance preservation metrics. This will reveal the sensitivity of the method to noise in the assumed ground truth.

2. **Alternative significance tests**: Replace Tukey HSD with other multiple comparison methods (e.g., Bonferroni, Benjamini-Hochberg FDR) and compare the preservation metrics. This will test whether the findings are robust to the choice of statistical test.

3. **Other evaluation measures**: Repeat the analysis using different effectiveness measures (e.g., Precision@10, nDCG@10) and on different IR collections (e.g., GOV2, ClueWeb) to assess the generalizability of the proposed methodology.