---
ver: rpa2
title: Fair GANs through model rebalancing for extremely imbalanced class distributions
arxiv_id: '2308.08638'
source_url: https://arxiv.org/abs/2308.08638
tags:
- dataset
- data
- generative
- fairness
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of fairness in deep generative models,
  specifically GANs, by proposing a method to rebalance the model distribution without
  access to the original training data. The approach involves generating balanced
  data from an existing imbalanced GAN model using latent space exploration and then
  training a new balanced generative model on this data.
---

# Fair GANs through model rebalancing for extremely imbalanced class distributions

## Quick Facts
- arXiv ID: 2308.08638
- Source URL: https://arxiv.org/abs/2308.08638
- Reference count: 13
- One-line primary result: Proposed method improves fairness metric by almost 5x while maintaining image quality on FFHQ dataset.

## Executive Summary
This paper addresses fairness in deep generative models by proposing a method to rebalance the model distribution without access to original training data. The approach involves generating balanced synthetic data from an imbalanced GAN using latent space exploration, then training a new balanced generative model on this data with an additional bias mitigation loss function. The method is evaluated on StyleGAN2 models trained on FFHQ dataset for racial fairness, showing significant improvements in fairness metrics while maintaining image quality.

## Method Summary
The method works by first generating balanced synthetic data from an existing imbalanced GAN model through latent space exploration using breadth-first search. This balanced dataset is then used to train a new generative model. Additionally, a bias mitigation loss function is introduced that minimizes the deviation of learned class distribution from being equiprobable. The approach does not require access to the original training data, making it practical for real-world applications where data privacy is a concern.

## Key Results
- Fairness metric improved by almost 5 times compared to baseline StyleGAN2 model on FFHQ dataset
- Maintained comparable image quality metrics (EQ-Face, NIQE, BRISQUE, CLIP-IQA) to baseline
- Demonstrated effectiveness on imbalanced CIFAR10 dataset with similar fairness and image quality to balanced dataset
- Showed that traditional FID metrics are unsuitable for bias mitigation problems due to sensitivity to class distribution imbalance

## Why This Works (Mechanism)

### Mechanism 1
Latent space exploration can generate balanced synthetic data from an imbalanced GAN without access to the original training data. By using breadth-first search in the latent space, the method finds latent vectors that produce images belonging to underrepresented demographic groups, then samples uniformly across these groups. Core assumption: The latent space of a well-trained GAN is disentangled enough that different demographic attributes correspond to identifiable regions or directions in the latent space.

### Mechanism 2
A bias mitigation loss function can enforce demographic balance during training by minimizing the deviation of class probabilities from equiprobability. The loss function computes the expected frequency of each demographic group in generated samples and penalizes deviations from 1/|D|, with higher weights for underrepresented groups. Core assumption: The auxiliary classifier used to estimate demographic attributes is accurate enough that its predictions can be trusted for loss computation.

### Mechanism 3
Traditional image quality metrics like FID are unsuitable for evaluating fairness because they are sensitive to class distribution imbalance. FID compares distributions of real and generated data using features from a pretrained network; if the real data has imbalanced classes, FID will be high even if image quality is good. Core assumption: The reference dataset used for FID calculation does not have a balanced class distribution.

## Foundational Learning

- Concept: Disentangled latent representations in GANs
  - Why needed here: The approach relies on the ability to navigate the latent space to find samples from different demographic groups; this requires the latent space to be organized such that attributes like race are separable.
  - Quick check question: What would happen to the method if changing one latent dimension affected multiple attributes simultaneously?

- Concept: Fairness metrics and their definitions
  - Why needed here: The paper defines fairness as equal representation across demographic groups, which differs from other fairness definitions like equalized odds; understanding this is crucial for interpreting results.
  - Quick check question: How does the fairness metric used here differ from demographic parity in classification tasks?

- Concept: Bias in generative models and its propagation
  - Why needed here: The motivation is that biases in training data lead to biased generative models, which then propagate biases to downstream tasks; understanding this chain is key to appreciating the problem.
  - Quick check question: Why might synthetic data generated by a biased model be problematic for training a downstream classifier?

## Architecture Onboarding

- Component map: Pre-trained imbalanced GAN -> Auxiliary demographic classifier -> Latent space exploration module -> Balanced dataset generation -> New GAN training with bias mitigation loss -> Evaluation metrics
- Critical path: Load pre-trained GAN and auxiliary classifier → Perform latent space exploration to collect balanced samples → Train new GAN on balanced synthetic data with bias loss → Evaluate fairness and image quality
- Design tradeoffs: Using synthetic data avoids privacy issues but may not fully capture diversity of real data; bias loss improves fairness but may slightly degrade image quality metrics; latent space exploration is computationally expensive but necessary for balanced sampling
- Failure signatures: High fairness metric values (>0.1) indicate persistent bias; very low FID or KID scores compared to baseline may suggest mode collapse or loss of diversity; if auxiliary classifier accuracy is low, fairness improvements may be unreliable
- First 3 experiments: 1) Generate 10,000 samples from pre-trained GAN and compute demographic distribution to confirm imbalance; 2) Apply latent space exploration to generate balanced dataset and verify demographic proportions; 3) Train new GAN on balanced dataset with and without bias loss, comparing fairness metrics and FID scores

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed approach scale to larger and more diverse datasets, such as ImageNet or CelebA-HQ, and what are the computational requirements? The paper mentions the approach is generalizable to any generative model trained for any task, but only demonstrates results on FFHQ and CIFAR-10 datasets. Experiments on larger datasets with varying sizes and complexities, along with computational resource analysis, would resolve this.

### Open Question 2
What is the impact of the proposed approach on the diversity of generated samples within each demographic group, and how does it compare to the original biased model? The paper focuses on achieving fairness across demographic groups but does not provide a detailed analysis of the diversity of generated samples within each group. Quantitative measures of diversity, such as intra-class variation or attribute coverage, for both the original and debiased models would resolve this.

### Open Question 3
How sensitive is the proposed approach to the choice of the auxiliary classifier used for bias mitigation, and can it generalize to different types of classifiers? The paper mentions using an existing auxiliary classifier for computing the loss function but does not explore the impact of different classifier choices. Experiments with various auxiliary classifiers, including different architectures and training methods, to evaluate the impact on fairness and image quality would resolve this.

## Limitations
- Method relies heavily on latent space disentanglement assumptions which may not hold for all GAN models
- Performance depends on accuracy of auxiliary classifier, which may itself be biased
- Computational cost of latent space exploration is significant for high-dimensional spaces
- Claims about FID metric limitations are based on limited empirical evidence

## Confidence

- High confidence: The mechanism of using bias mitigation loss to enforce demographic balance is well-founded and theoretically sound
- Medium confidence: The latent space exploration algorithm's effectiveness depends on quality of pre-trained GAN's latent space structure, which varies between models
- Low confidence: Claims about unsuitability of FID for fairness evaluation are based on limited empirical evidence and may not generalize across all imbalanced datasets

## Next Checks

1. **Latent Space Disentanglement Analysis**: Quantitatively measure the disentanglement of the pre-trained GAN's latent space by computing the correlation between latent vector perturbations and changes in demographic attributes across multiple axes.

2. **Auxiliary Classifier Robustness Test**: Evaluate the auxiliary classifier's accuracy and bias across different demographic subgroups using a held-out balanced dataset, and measure how classifier errors propagate to fairness metric improvements.

3. **FID vs Balanced Reference Test**: Generate a truly balanced reference dataset from real images and compare FID scores across different demographic groups to verify whether the claimed sensitivity to class imbalance holds in practice.