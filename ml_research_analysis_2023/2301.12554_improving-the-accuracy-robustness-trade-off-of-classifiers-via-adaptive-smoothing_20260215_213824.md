---
ver: rpa2
title: Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing
arxiv_id: '2301.12554'
source_url: https://arxiv.org/abs/2301.12554
tags:
- adversarial
- robust
- classi
- accuracy
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the accuracy-robustness
  trade-off of neural network classifiers. It proposes a novel method called "adaptive
  smoothing" that combines the outputs of a standard classifier (optimized for clean
  accuracy) and a robust classifier (optimized for adversarial robustness) using a
  convex combination.
---

# Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing

## Quick Facts
- arXiv ID: 2301.12554
- Source URL: https://arxiv.org/abs/2301.12554
- Reference count: 40
- One-line primary result: Achieves 85.21% clean accuracy and 38.72% AutoAttacked accuracy on CIFAR-100, setting a new state-of-the-art in accuracy-robustness trade-off.

## Executive Summary
This paper addresses the fundamental challenge of improving the accuracy-robustness trade-off in neural network classifiers. The authors propose a novel method called "adaptive smoothing" that combines the outputs of a standard classifier (optimized for clean accuracy) and a robust classifier (optimized for adversarial robustness) using a convex combination. The key innovation is a policy network that adaptively adjusts this mixture based on input characteristics, allowing the model to dynamically balance accuracy and robustness. The method is theoretically grounded, with certification of the smoothed classifier's robustness under realistic assumptions, and demonstrates significant empirical improvements over existing approaches.

## Method Summary
The method involves three key components: (1) a standard classifier optimized for clean accuracy, (2) a robust classifier trained for adversarial robustness, and (3) a policy network that adaptively adjusts the mixture of the two classifiers' outputs. The approach uses a convex combination of the two classifiers' probability outputs, controlled either by a fixed parameter or dynamically by the policy network. The policy network is trained using a composite loss function that combines cross-entropy and binary cross-entropy losses on both clean and adversarial data. The theoretical framework provides certification of the smoothed classifier's robustness based on the Lipschitz constant of the robust base classifier.

## Key Results
- Achieves 85.21% clean accuracy and 38.72% AutoAttacked accuracy on CIFAR-100
- Outperforms existing methods in the accuracy-robustness trade-off by significant margins
- Demonstrates consistent improvements across multiple architectures including ResNet and ConvNeXT
- Shows robustness to various attack types including PGD and AutoAttack

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive smoothing improves the accuracy-robustness trade-off by combining the outputs of a standard classifier and a robust classifier using a convex combination.
- **Mechanism:** The method leverages the fact that the robust classifier's confidence difference for correct and incorrect examples is crucial for improving the trade-off. By adjusting the mixture of the two classifiers, the approach can achieve high clean accuracy while maintaining strong adversarial robustness.
- **Core assumption:** The robust classifier has a certifiable Lipschitz constant and is inherently robust, while the standard classifier is optimized for clean accuracy but lacks robustness.
- **Evidence anchors:** [abstract]: "This paper significantly alleviates this accuracy-robustness trade-off by mixing the output probabilities of a standard classifier and a robust classifier..."; [section]: "The resulting formulation (4) is a convex combination of the outputs of a standard neural network and a robust neural network."
- **Break condition:** If the robust classifier's confidence difference is not significant or the Lipschitz constant is not certifiable, the method's effectiveness may be compromised.

### Mechanism 2
- **Claim:** The policy network adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness.
- **Mechanism:** The policy network uses a type of adversary detector to adjust the convex combination of the two networks, improving the accuracy-robustness trade-off. This allows the model to dynamically change its behavior based on the input, enhancing its overall performance.
- **Core assumption:** The policy network can effectively detect adversarial inputs and adjust the mixture accordingly.
- **Evidence anchors:** [abstract]: "Furthermore, we adapt an adversarial input detector into a policy network that adaptively adjusts the mixture of the robust base classifier and a standard network..."; [section]: "The policy adaptively adjusts the mixture of the robust base classifier and a standard network, where the standard network is optimized for clean accuracy and is not robust in general."
- **Break condition:** If the policy network fails to accurately detect adversarial inputs or adjust the mixture effectively, the method's performance may degrade.

### Mechanism 3
- **Claim:** The smoothed classifier inherits the certifiable robustness of the robust base classifier under realistic assumptions.
- **Mechanism:** By assuming the robust classifier is Lipschitz continuous, the method provides a theoretical certification of the robustness of the smoothed classifier. This allows for a quantifiable measure of the model's robustness.
- **Core assumption:** The robust classifier has a certifiable Lipschitz constant and is Lipschitz continuous.
- **Evidence anchors:** [abstract]: "We theoretically certify the robustness of the mixed classifier under realistic assumptions."; [section]: "We provide theoretical analyses to motivate the use of the adaptive smoothing procedure, certify the robustness of the smoothed classifier under realistic assumptions..."
- **Break condition:** If the robust classifier does not have a certifiable Lipschitz constant or is not Lipschitz continuous, the theoretical certification of robustness may not hold.

## Foundational Learning

- **Concept:** Adversarial training and its limitations
  - **Why needed here:** Understanding adversarial training is crucial because the method builds upon robust classifiers obtained through adversarial training. Knowing its limitations helps in appreciating the need for adaptive smoothing.
  - **Quick check question:** What are the main drawbacks of adversarial training in terms of clean accuracy and robustness?

- **Concept:** Randomized smoothing and its variants
  - **Why needed here:** Randomized smoothing is a key technique used in the method to achieve certified robustness. Understanding its variants, such as locally biased smoothing, is essential for grasping the improvements made by adaptive smoothing.
  - **Quick check question:** How does locally biased smoothing differ from traditional randomized smoothing, and why is it beneficial?

- **Concept:** Lipschitz continuity and its role in robustness certification
  - **Why needed here:** Lipschitz continuity is a fundamental concept for certifying the robustness of neural networks. Understanding its role helps in appreciating the theoretical guarantees provided by the method.
  - **Quick check question:** Why is Lipschitz continuity important for robustness certification, and how does it relate to the method's theoretical guarantees?

## Architecture Onboarding

- **Component map:** Standard classifier -> Robust classifier -> Policy network -> Smoothed classifier
- **Critical path:**
  1. Obtain or train a standard classifier optimized for clean accuracy
  2. Obtain or train a robust classifier using adversarial training or other methods
  3. Implement the smoothing procedure to combine the outputs of the two classifiers
  4. Train the policy network to adaptively adjust the mixture of the base classifiers
  5. Certify the robustness of the smoothed classifier using the Lipschitz constant of the robust classifier

- **Design tradeoffs:**
  - Tradeoff between clean accuracy and adversarial robustness: The method aims to balance these two aspects by adjusting the mixture of the base classifiers
  - Complexity of the policy network: A more complex policy network may provide better adaptability but could also increase computational costs
  - Theoretical guarantees vs. empirical performance: While the method provides theoretical guarantees, its empirical performance may vary depending on the specific datasets and attack scenarios

- **Failure signatures:**
  - Poor performance on clean data: If the method overly emphasizes robustness, it may sacrifice clean accuracy
  - Vulnerability to adversarial attacks: If the method fails to effectively combine the base classifiers, it may be vulnerable to certain types of adversarial attacks
  - Instability in the policy network: If the policy network is not well-trained, it may lead to inconsistent or suboptimal adjustments of the mixture

- **First 3 experiments:**
  1. Evaluate the method's performance on a simple binary classification task with a small dataset to verify the basic functionality of the smoothing procedure
  2. Test the method's adaptability by applying it to a more complex multi-class classification task with a larger dataset, such as CIFAR-10 or CIFAR-100
  3. Assess the method's robustness by subjecting it to various types of adversarial attacks, including PGD, AutoAttack, and adaptive attacks, and compare its performance to state-of-the-art methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of adaptive smoothing change when using different robust training methods (e.g., TRADES, adversarial training, random smoothing) for the robust base classifier h(·)?
- Basis in paper: [explicit] The paper states that the robust base classifier can be obtained via various methods, including adversarial training, TRADE, and traditional randomized smoothing.
- Why unresolved: The paper only uses adversarially-trained and TRADES-trained models for h(·) in the experiments, not comparing different robust training methods.
- What evidence would resolve it: Experiments comparing adaptive smoothing performance using h(·) models trained with different robust training methods.

### Open Question 2
- Question: How does the choice of the policy network architecture affect the performance of adaptive smoothing?
- Basis in paper: [explicit] The paper mentions using a ResNet18-like structure for the policy network but does not explore alternative architectures.
- Why unresolved: The paper only experiments with one policy network architecture and does not compare it to other options.
- What evidence would resolve it: Experiments comparing adaptive smoothing performance using different policy network architectures.

### Open Question 3
- Question: What is the optimal value of the hyperparameters c1, c2, and c3 in the composite loss function (6) for the policy network?
- Basis in paper: [explicit] The paper uses fixed values (c1 = 0.5, c2 = 1, c3 = 0.1) for these hyperparameters but does not discuss their sensitivity or optimal values.
- Why unresolved: The paper does not explore the impact of different hyperparameter values on the performance of adaptive smoothing.
- What evidence would resolve it: Experiments varying the values of c1, c2, and c3 to find the optimal combination for different datasets and attack scenarios.

## Limitations

- The exact hyperparameters for the composite loss function and policy network training are not fully specified, which could affect reproducibility
- The method's performance on larger, more complex datasets beyond CIFAR-100 is not demonstrated
- The scalability of the approach to different network architectures and problem domains remains untested

## Confidence

- **High confidence:** The core mechanism of adaptive smoothing combining standard and robust classifiers is well-supported by theory and experiments
- **Medium confidence:** The effectiveness of the policy network in adaptively adjusting the mixture requires more extensive validation across different attack scenarios
- **Medium confidence:** The theoretical certification of robustness under realistic assumptions needs further empirical validation

## Next Checks

1. Reproduce the adaptive smoothing implementation with the specified loss functions and hyperparameters on CIFAR-10 to verify basic functionality
2. Test the method's performance against adaptive attacks specifically designed to exploit the mixture mechanism
3. Evaluate the scalability by applying the approach to larger datasets (e.g., ImageNet) and more complex architectures (e.g., Vision Transformers)