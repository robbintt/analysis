---
ver: rpa2
title: 'Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning'
arxiv_id: '2312.17055'
source_url: https://arxiv.org/abs/2312.17055
tags:
- arxiv
- learning
- language
- smaller
- larger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the in-context
  learning (ICL) capabilities of smaller language models by distilling knowledge from
  larger models. The core idea is that existing distillation methods focus only on
  aligning output distributions and ignore the crucial role of input demonstrations.
---

# Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning

## Quick Facts
- arXiv ID: 2312.17055
- Source URL: https://arxiv.org/abs/2312.17055
- Reference count: 33
- Primary result: Proposed Bidirectional Alignment (BiAlign) method improves in-context learning capabilities of smaller models by 7% on GSM8K and 8% on LogiQA

## Executive Summary
This paper addresses the challenge of improving in-context learning (ICL) capabilities of smaller language models through knowledge distillation from larger models. The authors identify a key limitation in existing distillation methods: they focus solely on aligning output distributions while ignoring the crucial role of input demonstration preferences. BiAlign introduces a novel ranking loss that aligns the input preferences of smaller and larger models, enabling smaller models to better select and utilize helpful demonstrations. Experiments demonstrate consistent improvements across diverse tasks including language understanding, reasoning, and coding, while maintaining or improving zero-shot learning capabilities.

## Method Summary
BiAlign is a knowledge distillation framework that improves smaller models' ICL abilities by aligning both output distributions and input preferences with larger models. The method uses KL divergence loss to align token-level output distributions and introduces a novel ranking loss to align input preferences. The ranking loss compares preference scores for different subsets of demonstrations, with scores estimated using prediction probabilities. The framework also employs subset sampling to help models generalize across different numbers of demonstrations and zero-shot settings. BiAlign is trained on the CrossFit dataset containing 12K ICL examples from 50-100 source tasks.

## Key Results
- Achieves 7% relative improvement on GSM8K and 8% on LogiQA compared to baseline methods
- Maintains or improves zero-shot learning capabilities (4% average improvement on HumanEval)
- Consistently outperforms baseline distillation methods across language understanding, reasoning, and coding tasks
- Demonstrates effectiveness with different numbers of demonstrations (4-10) in ICL samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model benefits from aligning not just output distributions but also input preferences, enabling better selection of helpful demonstrations.
- Mechanism: By ranking subsets of demonstrations and aligning the student model's preference scores with the teacher's, the student learns which examples are more useful for specific queries, leading to better in-context learning performance.
- Core assumption: The preference scores based on prediction probability accurately reflect the usefulness of demonstration subsets for generating the target output.
- Evidence anchors:
  - [abstract] "Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for ICL examples to improve the ICL abilities of student models."
  - [section] "Inspired by this finding, we propose Bidirectional Alignment (BiAlign), a simple yet effective framework for improving the ICL abilities of smaller models... We introduce the alignment of input preferences between smaller and larger models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution."
- Break condition: If the ranking loss does not lead to meaningful alignment of input preferences, or if the sampled subsets do not capture the variation in demonstration usefulness, the mechanism may fail.

### Mechanism 2
- Claim: Aligning token-level output distributions ensures the student model captures the teacher's knowledge in generating the target output.
- Mechanism: Minimizing KL divergence between the student and teacher model's token-level output distributions on the concatenated ICL samples ensures the student learns to generate outputs similar to the teacher.
- Core assumption: The token-level output distribution contains sufficient information for the student model to learn the teacher's knowledge.
- Evidence anchors:
  - [abstract] "Existing methods either train student models on the generated outputs of teacher models or imitate their token-level probability distributions."
  - [section] "To achieve token-level output distribution alignment on ˆXi, we minimize a KL divergence loss between the smaller model and larger model for the whole sequence instead of only ˆyi following Gu et al. (2023a)."
- Break condition: If the KL divergence loss does not lead to effective alignment of token-level distributions, or if the sequence-level alignment is not beneficial, the mechanism may fail.

### Mechanism 3
- Claim: The subset sampling strategy allows the model to generalize better to different numbers of demonstrations and zero-shot settings.
- Mechanism: By sampling different subsets of demonstrations and aligning preferences for these subsets, the model learns to handle varying numbers of demonstrations and generalizes to unseen zero-shot tasks.
- Core assumption: The subset sampling captures the relevant variation in demonstration usefulness and helps the model generalize to different settings.
- Evidence anchors:
  - [abstract] "Both fine-tuning and output alignment hurt the zero-shot learning capability of the model as shown by the performance on HumanEval. In contrast, BiAlign brings an average relative improvement of about 4% on HumanEval."
  - [section] "We use both the smaller and larger models to measure their preferences for each subset Rij, which we estimate using the prediction probability of ˆyi given Rij and ˆxi as input."
- Break condition: If the subset sampling does not capture the relevant variation, or if the model overfits to the sampled subsets, the mechanism may fail.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Understanding ICL is crucial as BiAlign aims to improve the ICL abilities of smaller models by aligning their input preferences with larger models.
  - Quick check question: What is in-context learning, and how does it differ from traditional few-shot learning?

- Concept: Knowledge distillation
  - Why needed here: BiAlign is a form of knowledge distillation that transfers the capabilities of larger models to smaller ones by aligning their output distributions and input preferences.
  - Quick check question: What is knowledge distillation, and how does it typically work in the context of language models?

- Concept: Ranking loss
  - Why needed here: The ranking loss in BiAlign is used to align the input preferences of smaller and larger models by comparing their preferences for different demonstration subsets.
  - Quick check question: What is a ranking loss, and how is it typically used in machine learning?

## Architecture Onboarding

- Component map:
  Larger model (teacher) -> Smaller model (student) -> KL divergence loss -> Ranking loss -> Subset sampling strategy

- Critical path:
  Sample subsets of demonstrations -> Measure preference scores for subsets using prediction probabilities -> Calculate KL divergence loss for token-level alignment -> Calculate ranking loss for input preference alignment -> Update student model parameters using combined loss

- Design tradeoffs:
  - Subset sampling size: Larger N may capture more variation but increase computational cost
  - Weight of ranking loss: Balancing the importance of input preference alignment vs. token-level alignment
  - Number of demonstrations per ICL sample: Affects the model's ability to generalize to different numbers of demonstrations

- Failure signatures:
  - Poor performance on target tasks compared to baseline methods
  - Overfitting to source tasks
  - Degradation of zero-shot learning capabilities

- First 3 experiments:
  1. Evaluate BiAlign on a small subset of target tasks to assess initial performance
  2. Compare the effect of different subset sampling sizes (N) on performance
  3. Analyze the impact of varying the weight of the ranking loss on overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which input preference alignment improves the zero-shot learning capabilities of smaller models?
- Basis in paper: [explicit] The paper states that BiAlign "brings an average relative improvement of about 4% on HumanEval" and suggests that "subset sampling in input preference alignment, which helps the model generalize better to the unseen zero-shot setting."
- Why unresolved: The paper only provides a hypothesis for this improvement but does not provide concrete evidence or analysis to support this claim.
- What evidence would resolve it: Experimental results showing a direct correlation between the use of subset sampling and improvements in zero-shot learning performance, or ablation studies isolating the effect of subset sampling on zero-shot performance.

### Open Question 2
- Question: How does the diversity of source tasks affect the performance of BiAlign on target tasks?
- Basis in paper: [explicit] The paper states "we hypothesize that the diversity of source tasks has a considerable influence on target task performance" and conducts experiments with different numbers of source tasks.
- Why unresolved: While the paper shows that performance improves with more source tasks, it does not provide a detailed analysis of how different types or characteristics of source tasks affect the performance.
- What evidence would resolve it: A comprehensive study comparing the performance of BiAlign using different types of source tasks (e.g., tasks from different domains, tasks with different levels of complexity) and analyzing the correlation between source task diversity and target task performance.

### Open Question 3
- Question: What is the optimal number of demonstration examples in source tasks for training BiAlign?
- Basis in paper: [explicit] The paper mentions that each constructed ICL training sample contains 4 ≤ k ≤ 10 demonstration examples and conducts experiments with different fixed numbers of demonstrations.
- Why unresolved: The paper only provides results for a few fixed numbers of demonstrations and does not determine the optimal number for training BiAlign.
- What evidence would resolve it: A systematic study varying the number of demonstration examples in source tasks and measuring the corresponding performance of BiAlign on target tasks to identify the optimal number.

## Limitations
- The subset sampling strategy for input preference alignment lacks clear methodology for ensuring meaningful subset diversity
- The effectiveness of using prediction probability as a proxy for demonstration subset quality is assumed rather than empirically validated
- Limited ablation studies make it difficult to isolate the effects of different BiAlign components

## Confidence
- **High confidence**: The core observation about focusing only on output matching while ignoring input demonstration preferences is well-supported by literature
- **Medium confidence**: The mechanism by which ranking loss leads to meaningful alignment of input preferences is plausible but not fully validated
- **Low confidence**: The claim that BiAlign maintains zero-shot capabilities while improving few-shot performance is based on limited evidence from a single benchmark

## Next Checks
1. Conduct ablation study on subset diversity by varying the number and composition of sampled subsets (N=2, 4, 8) and measure the impact on both ranking loss convergence and downstream task performance
2. Design controlled experiments comparing zero-shot performance across different distillation methods while holding model size and training compute constant, including additional zero-shot benchmarks beyond HumanEval
3. Create synthetic demonstration sets with known quality hierarchies to test whether the model's learned preferences align with ground truth, validating whether the ranking loss is actually learning meaningful demonstration quality signals