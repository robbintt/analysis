---
ver: rpa2
title: Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for
  Machine Translation
arxiv_id: '2302.14220'
source_url: https://arxiv.org/abs/2302.14220
tags:
- language
- character
- languages
- translation
- byt5
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Character-level models (ByT5) generally outperform subword models
  (mT5) for neural machine translation, especially in low-resource settings and for
  translating cognates and rare words. ByT5 models translate word-by-word by implicitly
  segmenting text into words, which helps them handle similar languages and unseen
  language pairs better than mT5.
---

# Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation

## Quick Facts
- **arXiv ID**: 2302.14220
- **Source URL**: https://arxiv.org/abs/2302.14220
- **Reference count**: 17
- **Primary result**: Character-level models (ByT5) outperform subword models (mT5) for translation, especially in low-resource settings and for cognates/rare words, but are 4-6x slower

## Executive Summary
This paper compares character-level ByT5 and subword-level mT5 models for neural machine translation across multiple languages and resource settings. ByT5 models demonstrate superior performance in low-resource scenarios, particularly for translating cognates and rare words, and show better cross-lingual transfer to similar unseen languages. However, ByT5 models are significantly slower than mT5 models (4-6x) in both training and inference. The performance gap between ByT5 and mT5 increases as training data decreases, making character-level translation most beneficial when speed is not critical and data is limited.

## Method Summary
The study fine-tunes pretrained ByT5 and mT5 models (small, base, large) on WMT NewsCommentary v16 dataset at varying sizes (0.4k to 250k sentence pairs, plus 1.25M and 4.5M German-English pairs). Models are evaluated using chrF++ and BLEU metrics on FLoRes200 dataset, with zero-shot cross-lingual testing on unseen language pairs. ByT5 uses byte-level tokenization while mT5 uses subword tokenization. Training uses AdaFactor optimizer with specific hyperparameters including 4000 warmup batches and varying patience based on dataset size.

## Key Results
- ByT5 outperforms mT5 for cognate translation and rare words due to character-level processing
- ByT5 shows better cross-lingual transfer to similar unseen languages
- ByT5 advantage increases as training data decreases (most pronounced in 5-25k sentence pairs)
- ByT5 models are 4-6x slower than mT5 in training and inference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Character models translate word-by-word while maintaining character-level flexibility
- **Mechanism**: ByT5 implicitly segments input into words during translation, allowing it to exploit character-level similarity for cognates while preserving word-level structure
- **Core assumption**: The model can learn to modulate between character-level and word-level processing based on linguistic context
- **Evidence anchors**:
  - [abstract]: "character models are capable of implicitly translating on the word or subword level"
  - [section 6.1]: "ByT5 is at the very least capable of mimicking word-by-word translation, while also exploiting character-by-character modeling whenever useful"

### Mechanism 2
- **Claim**: Character models excel at cognate translation through direct character matching
- **Mechanism**: By operating on individual characters, ByT5 can directly match similar character sequences in cognates, while subword models must rely on learned subword alignments
- **Core assumption**: Orthographic similarity between cognates is preserved at character level and can be exploited by character models
- **Evidence anchors**:
  - [section 6.2]: "ByT5 has a higher translation accuracy on words of fewer than 100 occurrences" for cognate pairs
  - [abstract]: "better translations of orthographically similar words"

### Mechanism 3
- **Claim**: Character models show better cross-lingual transfer for similar languages
- **Mechanism**: Character-level representations capture morphological similarities that transcend subword boundaries, enabling better generalization to unseen but similar languages
- **Core assumption**: Morphological similarity is preserved and exploitable at character level across related languages
- **Evidence anchors**:
  - [section 5.2]: "ByT5 performs markedly better than mT5 on languages that are similar to the source but are not seen in pretraining"
  - [abstract]: "ByT5 performs particularly well for languages not seen in pretraining that are similar to a high-resource language"

## Foundational Learning

- **Concept**: Subword tokenization and BPE algorithm
  - **Why needed here**: Understanding the fundamental difference between character-level and subword approaches
  - **Quick check question**: How does BPE merge frequent character pairs to create subwords, and why does this create a fixed vocabulary?

- **Concept**: Cross-lingual transfer learning
  - **Why needed here**: Explains why models trained on one language pair can perform on unseen language pairs
  - **Quick check question**: What factors influence cross-lingual transfer success (pretraining data, language similarity, etc.)?

- **Concept**: Attribution methods in NLP (saliency, gradient-based)
  - **Why needed here**: Understanding how to analyze model behavior through source attribution analysis
  - **Quick check question**: How do gradient-based attribution methods reveal which input tokens influence model predictions?

## Architecture Onboarding

- **Component map**: Input encoder (byte-level characters) -> Transformer layers (increased width for ByT5) -> Output decoder (character generation) -> Fine-tuning layer (prompt conditioning)
- **Critical path**: Character encoding → Transformer layers → Character generation
- **Design tradeoffs**: 
  - ByT5: No tokenization complexity, better cross-lingual generalization, slower processing
  - mT5: Faster processing, fixed vocabulary limits, tokenization complexity
- **Failure signatures**:
  - Slow inference: Character models are 4-6x slower than subword models
  - Script mismatch: Performance drops when source/target use different scripts
  - High-resource convergence: Advantage diminishes with large training datasets
- **First 3 experiments**:
  1. Train ByT5 and mT5 on low-resource language pair (e.g., 400 sentence pairs) and compare performance
  2. Test cross-lingual transfer by evaluating on similar but unseen languages
  3. Analyze source attribution patterns to verify word-level processing behavior

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the larger input sequence length required by ByT5 models lead to higher computational costs that offset their performance advantages in low-resource scenarios?
- **Basis in paper**: [inferred] The paper notes that ByT5 models are 4-6 times slower in training and inference compared to mT5 models, and this slowdown is largely due to the increase in sequence length.
- **Why unresolved**: The paper does not provide a comprehensive cost-benefit analysis that weighs the performance improvements of ByT5 against its increased computational costs.
- **What evidence would resolve it**: A detailed analysis comparing the cost-effectiveness of ByT5 and mT5 models, considering both performance metrics and computational resources used, would help resolve this question.

### Open Question 2
- **Question**: How does the effectiveness of ByT5 models vary across different language families, particularly for languages with complex morphology or non-Latin scripts?
- **Basis in paper**: [inferred] The paper shows that ByT5 performs well for languages not seen in pretraining that are similar to a high-resource language, but it does not extensively explore the model's effectiveness across different language families.
- **Why unresolved**: The paper's analysis is limited to a specific set of languages and does not provide a comprehensive evaluation across a broader range of language families.
- **What evidence would resolve it**: Extensive experiments evaluating ByT5's performance on a wide variety of language families, including those with complex morphology or non-Latin scripts, would help answer this question.

### Open Question 3
- **Question**: Can the implicit word-level understanding demonstrated by ByT5 be further enhanced or leveraged to improve translation quality even more?
- **Basis in paper**: [explicit] The paper shows that ByT5 models can implicitly segment sentences into words or subwords, which contributes to their superior performance in translating cognates and rare words.
- **Why unresolved**: While the paper demonstrates the existence of this implicit word-level understanding, it does not explore methods to enhance or leverage this capability further.
- **What evidence would resolve it**: Research into techniques that could enhance or exploit ByT5's implicit word-level understanding, potentially leading to even better translation quality, would help resolve this question.

## Limitations

- **Speed-Accuracy Tradeoff**: The 4-6x slowdown of ByT5 is a critical limitation not fully quantified for real-world deployment scenarios
- **Cross-Lingual Transfer Mechanism**: While empirically demonstrated, the mechanisms underlying ByT5's cross-lingual success are not fully explained theoretically
- **Data Quality Dependencies**: Benefits assume clean, well-formatted training data; the impact of noisy or malformed text is not explored

## Confidence

- **High Confidence**: ByT5 outperforms mT5 for cognate translation and in low-resource settings (5-25k sentence pairs)
- **Medium Confidence**: The mechanism explanation for why character models translate word-by-word
- **Low Confidence**: The claim that ByT5's advantage diminishes with large training datasets

## Next Checks

1. **Speed-Performance Cost Analysis**: Measure the actual wall-clock time difference between ByT5 and mT5 for production-scale translation workloads, including batching effects and hardware utilization patterns

2. **Cross-Lingual Transfer Mechanism Study**: Use attribution methods to analyze whether ByT5's cross-lingual success stems from character-level morphological similarity capture or other factors like vocabulary size reduction

3. **Robustness Testing on Noisy Data**: Compare model performance degradation when trained/evaluated on text with various types of noise (typos, inconsistent casing, script mixing) to understand practical deployment limitations