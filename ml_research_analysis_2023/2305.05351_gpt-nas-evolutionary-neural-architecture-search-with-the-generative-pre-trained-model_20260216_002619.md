---
ver: rpa2
title: 'GPT-NAS: Evolutionary Neural Architecture Search with the Generative Pre-Trained
  Model'
arxiv_id: '2305.05351'
source_url: https://arxiv.org/abs/2305.05351
tags:
- neural
- architecture
- search
- architectures
- gpt-nas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel neural architecture search (NAS) method
  called GPT-NAS that leverages a generative pre-trained (GPT) model with an evolutionary
  algorithm. The key idea is to use the GPT model to propose reasonable architecture
  components given a basic one, which can reduce the search space by introducing prior
  knowledge.
---

# GPT-NAS: Evolutionary Neural Architecture Search with the Generative Pre-Trained Model

## Quick Facts
- arXiv ID: 2305.05351
- Source URL: https://arxiv.org/abs/2305.05351
- Authors: 
- Reference count: 40
- Key outcome: GPT-NAS achieves 97.69% top-1 accuracy on CIFAR-10, 82.81% on CIFAR-100, and 79.08%/95.92% top-1/top-5 accuracy on ImageNet-1K, outperforming 7 manually designed architectures and 13 competing NAS methods.

## Executive Summary
GPT-NAS introduces a novel approach to neural architecture search by leveraging a generative pre-trained transformer model combined with evolutionary algorithms. The method encodes neural architectures as text, pre-trains a GPT model on a large corpus of architectures, and uses this model to guide an evolutionary search process. This approach significantly reduces the search space while maintaining architectural quality, leading to state-of-the-art performance on multiple image classification benchmarks.

## Method Summary
GPT-NAS encodes CNN architectures as textual sequences using 12 properties per layer, then pre-trains a 4-layer GPT decoder on NAS-Bench-101. The pre-trained model is fine-tuned on state-of-the-art architectures before guiding an evolutionary algorithm search. The search uses population-based evolution with GPT-guided reconstruction, employing an elimination rate that decreases from 0.4 to 0. The method includes an acceleration strategy that trains only predicted vital structures for 6 epochs before full retraining of the best architecture.

## Key Results
- Achieves 97.69% top-1 accuracy on CIFAR-10 with 3.9M parameters
- Achieves 82.81% top-1 accuracy on CIFAR-100 with 3.8M parameters
- Achieves 79.08% top-1 and 95.92% top-5 accuracy on ImageNet-1K with 5.1M parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GPT model reduces the search space by introducing architectural priors learned from large-scale neural architecture data.
- Mechanism: The pre-trained GPT model learns patterns and relationships between architectural components from NAS-Bench-101 and other architectures, then predicts new layer structures based on existing ones during the search process.
- Core assumption: A generative model pre-trained on a large-scale corpus of neural architectures can learn the fundamental laws of building neural architectures.
- Evidence anchors:
  - [abstract] "GPT-NAS leverages the GPT model to propose reasonable architecture components given the basic one"
  - [section] "Providing the GPT model with a general understanding of the fundamental laws of neural architecture is the heart of this study"
  - [corpus] Weak evidence - only 5 related papers found, none specifically addressing GPT-based NAS
- Break condition: If the GPT model fails to learn meaningful architectural patterns from the training data, or if the predicted structures are not compatible with the search space.

### Mechanism 2
- Claim: The evolutionary algorithm (EA) efficiently explores the reduced search space while maintaining diversity.
- Mechanism: EA performs crossover, mutation, and selection operations on neural architectures, with the GPT model guiding the reconstruction of eliminated structures.
- Core assumption: EA can effectively search the space of neural architectures even when guided by GPT-predicted components.
- Evidence anchors:
  - [abstract] "GPT-NAS leverages the GPT model to propose reasonable architecture components given the basic one and then utilizes EAs to search for the optimal solution"
  - [section] "The neural architecture search procedure consists of two parts, namely architecture search and reconstruction"
  - [corpus] Weak evidence - only 5 related papers found, none specifically addressing EA-based NAS with GPT guidance
- Break condition: If EA cannot maintain population diversity or if the GPT guidance leads to premature convergence on suboptimal architectures.

### Mechanism 3
- Claim: The acceleration strategy significantly reduces computational cost without sacrificing performance.
- Mechanism: Only predicted "vital" structures are trained, and a small number of epochs are used based on warmup principles.
- Core assumption: Training only predicted structures and using fewer epochs can maintain performance while reducing computation.
- Evidence anchors:
  - [section] "Training the entire neural architecture is time-consuming, but it is more efficient if only the 'vital' structures of the architecture are trained"
  - [section] "Only a small number of epochs are trained: This strategy has been covered in some works [13, 34]"
  - [corpus] Weak evidence - no corpus evidence directly supporting this specific acceleration strategy
- Break condition: If the reduced training leads to poor performance or if the predicted structures are not actually vital.

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: Understanding NAS fundamentals is crucial for grasping how GPT-NAS differs from traditional approaches
  - Quick check question: What are the three main categories of NAS search strategies mentioned in the paper?

- Concept: Transformer/GPT architecture
  - Why needed here: GPT-NAS uses a modified Transformer decoder as its core component
  - Quick check question: How does the GPT model used in this paper differ from the original GPT model in terms of layers and attention heads?

- Concept: Evolutionary Algorithms (EA)
  - Why needed here: EA serves as the search strategy in GPT-NAS
  - Quick check question: What are the three main operations performed by EA in the context of neural architecture search?

## Architecture Onboarding

- Component map:
  - GPT Model (pre-trained and fine-tuned) -> Neural Architecture Encoder -> Evolutionary Algorithm (EA) -> Acceleration Strategy -> Search Space (defined by predefined blocks)

- Critical path:
  1. Encode neural architectures into textual format
  2. Pre-train GPT model on NAS-Bench-101
  3. Fine-tune GPT model on specific task architectures
  4. Initialize EA population
  5. Perform EA search with GPT-guided reconstruction
  6. Apply acceleration strategy during training
  7. Evaluate and select optimal architecture

- Design tradeoffs:
  - Search space complexity vs. GPT model capacity
  - Pre-training data size vs. model performance
  - Number of EA iterations vs. computational cost
  - Acceleration strategy aggressiveness vs. final accuracy

- Failure signatures:
  - GPT model generates invalid or incompatible structures
  - EA converges prematurely to suboptimal architectures
  - Acceleration strategy leads to poor performance
  - Encoding/decoding process loses important architectural information

- First 3 experiments:
  1. Test GPT model's ability to predict layer structures on a small dataset of known architectures
  2. Evaluate EA search performance with and without GPT guidance on a simplified search space
  3. Assess the impact of different elimination rates on search efficiency and final architecture quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the introduction of GPT-generated prior knowledge affect the exploration-exploitation trade-off in the NAS search process compared to traditional NAS methods?
- Basis in paper: [explicit] The paper states that GPT-NAS reduces the search space by introducing prior knowledge from the GPT model, but does not analyze the specific impact on exploration-exploitation balance.
- Why unresolved: The paper focuses on performance improvements but does not provide empirical analysis of search behavior or comparison of exploration-exploitation dynamics.
- What evidence would resolve it: Experiments comparing the distribution of explored architectures and their performance trajectories between GPT-NAS and baseline methods, along with metrics quantifying exploration vs. exploitation over search iterations.

### Open Question 2
- Question: What is the optimal elimination rate for the GPT-NAS algorithm across different neural architecture search spaces and tasks?
- Basis in paper: [explicit] The paper experiments with elimination rates of 0, 0.2, 0.4, 0.6, and 0.8, finding 0.4 optimal for CIFAR datasets, but does not generalize to other search spaces or tasks.
- Why unresolved: The ablation study is limited to specific datasets and does not explore the relationship between elimination rate, search space complexity, and task difficulty.
- What evidence would resolve it: Systematic experiments varying elimination rates across diverse search spaces (different depths, widths, operation sets) and tasks (object detection, segmentation), with analysis of how optimal rates scale with search space characteristics.

### Open Question 3
- Question: How does the GPT model's ability to generalize across different neural architecture search spaces impact its effectiveness in guiding NAS?
- Basis in paper: [inferred] The paper pre-trains GPT on NAS-Bench-101 and fine-tunes on specific architectures, but does not investigate the impact of pre-training data diversity on search performance.
- Why unresolved: The paper uses a fixed pre-training strategy without exploring how different pre-training datasets or architectures affect GPT's guidance quality in NAS.
- What evidence would resolve it: Experiments comparing GPT-NAS performance using pre-trained models on different architectural datasets (e.g., various NAS benchmarks) and analyzing the correlation between pre-training data diversity and search effectiveness.

## Limitations
- Effectiveness heavily dependent on quality and diversity of pre-training corpus
- Acceleration strategy may lead to underfitting if vital structures are misidentified
- Method demonstrated only on image classification tasks, limiting generalizability

## Confidence

- **High Confidence**: The experimental results on CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate state-of-the-art performance compared to both manually designed architectures and competing NAS methods. The methodology for encoding architectures, pre-training GPT, and using EA is clearly specified and reproducible.
- **Medium Confidence**: The claim that GPT-NAS improves performance by up to 12% compared to non-GPT architectures is based on comparisons with a specific set of baselines. The generalizability of this improvement to other architectures or tasks is uncertain.
- **Low Confidence**: The assertion that the GPT model learns the "fundamental laws of building neural architectures" is difficult to verify empirically. The paper provides limited qualitative evidence of the model's architectural understanding, relying primarily on quantitative performance gains.

## Next Checks

1. **Ablation Study on GPT Guidance**: Run GPT-NAS with the GPT model frozen (no pre-training/fine-tuning) and compare the search results to the full method. This will quantify the contribution of the GPT model to the search efficiency and final architecture quality.

2. **Sensitivity Analysis on Elimination Rate**: Systematically vary the elimination rate (e.g., [0.2, 0.4, 0.6, 0.8]) and observe its impact on search convergence, population diversity, and final accuracy. This will reveal the robustness of the search process to this hyperparameter.

3. **Cross-Domain Transfer**: Apply GPT-NAS to a non-image classification task (e.g., language modeling on PTB or machine translation on WMT) by adapting the block definitions and pre-training corpus. Evaluate whether the method can discover competitive architectures in a new domain without significant modifications.