---
ver: rpa2
title: Adaptive Adversarial Training Does Not Increase Recourse Costs
arxiv_id: '2309.02528'
source_url: https://arxiv.org/abs/2309.02528
tags:
- adversarial
- recourse
- training
- costs
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the impact of adaptive adversarial training
  (AAT) on algorithmic recourse costs. It demonstrates that AAT, which learns instance-wise
  adversarial radii, preserves low-cost recourse and produces counterfactual explanations
  closer to the desired class manifold compared to traditional adversarial training
  with fixed radii.
---

# Adaptive Adversarial Training Does Not Increase Recourse Costs

## Quick Facts
- arXiv ID: 2309.02528
- Source URL: https://arxiv.org/abs/2309.02528
- Authors: 
- Reference count: 31
- Key outcome: AAT preserves low-cost recourse while maintaining robustness, with 60-80% of counterfactuals having l1 norm < 0.05 compared to 10-30% for traditional adversarial training.

## Executive Summary
This paper explores the impact of adaptive adversarial training (AAT) on algorithmic recourse costs. AAT learns instance-wise adversarial radii, avoiding the over-constraining effect of traditional adversarial training with fixed radii. The study demonstrates that AAT preserves low-cost recourse opportunities while maintaining robustness benefits. Experiments on three datasets (Adult Income, HELOC, and Give Me Some Credit) show that AAT yields more affordable recourse costs and produces counterfactuals closer to target class manifolds compared to traditional methods.

## Method Summary
The study trained 7 neural network models per dataset: naturally trained, AAT, Max-Margin Adversarial (MMA), and four traditional adversarial training models with fixed radii [0.05, 0.1, 0.15, 0.2]. All datasets are tabular with binary classification tasks. Counterfactual explanations were generated using CARLA defaults, and models were evaluated using accuracy, F1 score, adversarial success rate, counterfactual costs, manifold proximity, and proportion of low-cost recourse.

## Key Results
- AAT preserves low-cost recourse with 60-80% of counterfactuals having l1 norm < 0.05, compared to 10-30% for traditional adversarial training
- AAT produces counterfactuals closer to target class manifolds, showing 20-40% improvements in KNN distances over traditional methods
- AAT maintains robustness benefits while eliminating the artificial floor on recourse costs created by fixed-radius adversarial training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AAT reduces recourse costs by learning instance-specific adversarial radii, avoiding over-constraining the decision boundary for all instances.
- Mechanism: Traditional adversarial training uses a fixed radius that forces all instances to be at least n distance from the decision boundary, even those already far away. AAT learns n_i per instance, allowing naturally distant instances to remain closer to their original positions while still improving robustness for near-boundary instances.
- Core assumption: Different data instances have inherently different vulnerabilities based on their proximity to other class manifolds.
- Evidence anchors: [abstract] "AAT, which learns instance-wise adversarial radii, preserves low-cost recourse" and [section 4.1] "AAT models provide natural robustness to the data samples"

### Mechanism 2
- Claim: AAT produces counterfactual explanations that are closer to the target class manifold than traditional adversarial training.
- Mechanism: Traditional adversarial training creates a uniform margin around the decision boundary, pushing all counterfactuals to be at least n distance from the target class. AAT creates instance-specific margins, allowing counterfactuals for instances already far from the boundary to be generated closer to the target class manifold.
- Core assumption: The distance between counterfactuals and the target class manifold is a meaningful measure of recourse quality.
- Evidence anchors: [abstract] "AAT produces counterfactuals that are closer to the desired class manifold, with KNN distances showing improvements of 20-40% over traditional adversarial training"

### Mechanism 3
- Claim: AAT preserves the natural distribution of low-cost recourse opportunities in the population.
- Mechanism: Traditional adversarial training with radius n eliminates all recourse opportunities with cost less than n, creating an artificial floor. AAT allows low-cost recourse to persist for instances naturally far from the boundary while still improving robustness for near-boundary instances.
- Core assumption: A natural distribution of recourse costs exists where some individuals require minimal changes while others require larger changes.
- Evidence anchors: [abstract] "AAT yields more affordable recourse costs while maintaining robustness benefits" with "60-80% of counterfactuals with l_1 norm < 0.05"

## Foundational Learning

- Concept: Adversarial training fundamentals
  - Why needed here: Understanding how traditional adversarial training works (min-max optimization with fixed radius) is essential to grasp why AAT differs and why it preserves recourse costs
  - Quick check question: What is the primary optimization objective in traditional adversarial training, and how does it differ from standard training?

- Concept: Instance-wise adaptive mechanisms
  - Why needed here: AAT's core innovation is learning different radii for different instances, which requires understanding adaptive learning paradigms and how instance-specific parameters can be learned during training
  - Quick check question: How does AAT update instance-specific radii during training, and what criterion determines whether to increase or decrease them?

- Concept: Counterfactual explanation methods
  - Why needed here: The paper evaluates AAT's effects on recourse costs using various counterfactual methods (Growing Spheres, SCFE, CCHVAE), requiring understanding of how these methods generate explanations and measure costs
  - Quick check question: What is the typical cost function used to measure recourse quality, and how do different counterfactual methods optimize this?

## Architecture Onboarding

- Component map: Data → Model training with adaptive radii updates → Robustness evaluation → Recourse generation → Cost and manifold proximity measurement → Analysis of tradeoffs
- Critical path: Data → Model training with adaptive radii updates → Robustness evaluation → Recourse generation → Cost and manifold proximity measurement → Analysis of tradeoffs
- Design tradeoffs: Fixed vs. adaptive radii (simplicity vs. performance), radius update frequency (stability vs. responsiveness), and radius update criteria (computational cost vs. accuracy of vulnerability estimation)
- Failure signatures: If AAT shows no improvement in recourse costs, it may indicate the radius update mechanism isn't working or the assumption about variable vulnerability is incorrect. If robustness degrades significantly, the adaptive mechanism may be making radii too small.
- First 3 experiments:
  1. Compare instance radius distributions between AAT and traditional adversarial training on a simple 2D dataset to visualize how radii vary across instances
  2. Measure the correlation between natural distance to decision boundary and final adaptive radius to verify the vulnerability assumption
  3. Test AAT with different radius update criteria (e.g., attack success rate thresholds) to find optimal settings for recourse preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does adaptive adversarial training (AAT) consistently produce more meaningful counterfactuals across different model architectures and datasets?
- Basis in paper: [inferred] The paper demonstrates AAT's effectiveness on three datasets with neural networks, but the results may not generalize to other model families or datasets with different characteristics.
- Why unresolved: The experiments were limited to neural networks on tabular datasets (Adult Income, HELOC, and Give Me Some Credit), leaving open the question of whether these benefits extend to other model types or data domains.
- What evidence would resolve it: Systematic testing of AAT across diverse model architectures (e.g., random forests, gradient boosting, transformers) and data types (images, text, graphs) with multiple datasets in each category.

### Open Question 2
- Question: What is the optimal balance between standard accuracy and robustness in adaptive adversarial training for different applications?
- Basis in paper: [explicit] The paper notes that "adversarial training often comes at some cost to standard accuracy" and shows that MMA training maintains competitive standard performance while providing robustness benefits.
- Why unresolved: While the paper demonstrates the trade-off between accuracy and robustness, it does not establish how to determine the optimal balance for different application contexts where recourse is critical.
- What evidence would resolve it: Empirical studies mapping the accuracy-robustness trade-off across multiple domains, quantifying the cost-benefit relationship for different use cases, and developing principled methods for selecting appropriate training parameters.

### Open Question 3
- Question: How does adaptive adversarial training affect the stability and consistency of generated counterfactual explanations across multiple runs?
- Basis in paper: [inferred] The paper focuses on the average properties of counterfactuals but does not examine the variance or stability of generated explanations when AAT models are trained multiple times.
- Why unresolved: While the paper demonstrates AAT's benefits in terms of average recourse costs and manifold proximity, it does not address whether these improvements are consistent across different training runs or whether the explanations are stable under model retraining.
- What evidence would resolve it: Statistical analysis of counterfactual variability across multiple training runs of AAT models, comparison of explanation stability between AAT and traditional adversarial training, and assessment of how explanation stability correlates with model performance metrics.

## Limitations
- Study focuses on three tabular datasets with binary classification tasks, limiting generalizability to other domains
- Specific hyperparameters for AAT and MMA models are not fully specified, using only "default choices"
- Manifold proximity is used as a proxy for recourse quality without direct validation of its correlation with meaningful outcomes

## Confidence

- **High confidence**: AAT's ability to preserve robustness while improving recourse costs (directly measured in experiments)
- **Medium confidence**: The claim that AAT produces counterfactuals closer to target class manifolds (manifold proximity is an indirect proxy)
- **Low confidence**: The mechanism explaining why AAT works better for recourse (relies on assumed relationship between natural vulnerability and adaptive radii)

## Next Checks

1. **Replicate results on additional datasets**: Test AAT on other tabular datasets and simple image classification tasks to verify generalizability
2. **Implement full hyperparameter search**: Systematically explore radius update frequencies and thresholds to identify optimal settings for recourse preservation
3. **Validate manifold proximity correlation**: Conduct user studies or downstream task evaluations to verify that reduced KNN distances actually correlate with better recourse quality in practice