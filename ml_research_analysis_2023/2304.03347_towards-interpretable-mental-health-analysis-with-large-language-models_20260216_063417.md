---
ver: rpa2
title: Towards Interpretable Mental Health Analysis with Large Language Models
arxiv_id: '2304.03347'
source_url: https://arxiv.org/abs/2304.03347
tags:
- mental
- health
- chatgpt
- emotion
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluates the mental health analysis
  and emotional reasoning capabilities of large language models (LLMs) like ChatGPT
  across 11 datasets spanning 5 tasks, including mental health condition detection,
  cause/factor detection, and emotion recognition in conversations. The authors explore
  different prompting strategies, particularly emotion-enhanced zero-shot Chain-of-Thought
  prompting, to improve performance and explainability.
---

# Towards Interpretable Mental Health Analysis with Large Language Models

## Quick Facts
- arXiv ID: 2304.03347
- Source URL: https://arxiv.org/abs/2304.03347
- Reference count: 36
- Key outcome: ChatGPT outperforms traditional neural networks on mental health analysis tasks but significantly underperforms advanced task-specific methods, with emotion-enhanced zero-shot Chain-of-Thought prompting improving performance by incorporating emotional cues into reasoning.

## Executive Summary
This study comprehensively evaluates large language models' capabilities for mental health analysis across 11 datasets spanning 5 tasks. The authors explore different prompting strategies, particularly emotion-enhanced zero-shot Chain-of-Thought prompting, to improve performance and explainability. Experimental results show that while ChatGPT outperforms traditional neural networks, it still significantly underperforms advanced task-specific methods. Emotion-enhanced prompts, especially unsupervised ones, effectively improve performance by guiding ChatGPT to incorporate emotional cues into its reasoning. ChatGPT demonstrates potential in generating interpretable explanations, though it faces limitations in stability and reasoning accuracy.

## Method Summary
The study evaluates ChatGPT's mental health analysis capabilities using zero-shot prompting strategies across 11 datasets spanning 5 tasks: mental health condition detection, cause/factor detection, emotion recognition in conversations, and causal emotion entailment. The authors compare vanilla zero-shot prompting with zero-shot Chain-of-Thought (CoT) prompting and emotion-enhanced zero-shot CoT prompting. Emotion-enhanced prompts incorporate explicit instructions to consider emotional content and explain reasoning step-by-step. Performance is measured using recall, weighted-F1, and micro-F1 scores, with human evaluation of explanation quality across three annotators.

## Key Results
- ChatGPT outperforms traditional neural networks but significantly underperforms advanced task-specific methods on mental health analysis tasks
- Emotion-enhanced zero-shot CoT prompting improves performance by guiding ChatGPT to incorporate emotional cues into reasoning
- ChatGPT generates explanations that approach human performance, showing potential for explainable mental health analysis despite limitations in stability and reasoning accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Emotion-enhanced zero-shot Chain-of-Thought prompting significantly improves ChatGPT's performance on mental health analysis by guiding it to incorporate emotional cues into its reasoning process.
- **Mechanism**: By adding explicit instructions like "Consider the emotions expressed from this post..." and "explain your reasoning step by step", the prompt triggers ChatGPT to first identify emotional content in the text, then use that emotional analysis as part of its classification decision-making process.
- **Core assumption**: Emotional cues are strongly coupled with mental health conditions and incorporating them into the reasoning process will improve detection accuracy.
- **Evidence anchors**:
  - [abstract]: "Emotion-enhanced prompts, especially unsupervised ones, effectively improve performance by guiding ChatGPT to incorporate emotional cues into its reasoning."
  - [section]: "ChatGPT CoT _emo outperforms all other prompt-based methods on most datasets... emotion-enhanced CoT prompting is an effective method of leveraging emotional cues to enhance the ability of ChatGPT on mental health analysis."
  - [corpus]: Found 25 related papers with average neighbor FMR=0.399, showing substantial related research on interpretable mental health analysis with LLMs.
- **Break condition**: If the emotional cues identified by ChatGPT are inaccurate or if the reasoning process becomes too complex, the performance improvement may not materialize. Additionally, if the emotional content in the text is ambiguous or conflicting, the method may fail.

### Mechanism 2
- **Claim**: Zero-shot Chain-of-Thought prompting does not consistently improve performance on mental health analysis tasks compared to vanilla zero-shot prompting.
- **Mechanism**: Unlike other NLP tasks where CoT prompting has been effective, in mental health analysis the simple trigger "explain your reasoning step by step" may not provide enough guidance for the complex subjective reasoning required.
- **Core assumption**: The CoT approach that works well for logical reasoning tasks may not transfer effectively to subjective mental health analysis.
- **Evidence anchors**:
  - [abstract]: "In contrast to the results observed in other NLP tasks, ChatGPT's performance using zero-shot CoT prompting is comparable to, or even worse than, its performance with vanilla zero-shot prompting."
  - [section]: "We surprisingly find that ChatGPTCoT has a comparable or even worse performance with ChatGPTZS. This illustrates that the simple trigger sentence 'explain your reasoning step by step' is not effective in prompting ChatGPT on mental health analysis."
  - [corpus]: The corpus shows 5 related papers specifically mentioning "explanations" and "chatgpt", suggesting this is an active area of research.
- **Break condition**: If the prompts are modified to include more specific guidance beyond just "explain your reasoning", the CoT approach might become more effective.

### Mechanism 3
- **Claim**: ChatGPT provides mostly reliable explanations and reasoning steps for its classifications, showing potential to enhance transparency in mental health analysis compared to black-box methods.
- **Mechanism**: When instructed to explain its reasoning, ChatGPT generates step-by-step arguments that include both emotional and factual evidence from the text, making its decision process more transparent.
- **Core assumption**: Providing explanations requires ChatGPT to extract and articulate the evidence supporting its classification, which inherently makes the process more interpretable.
- **Evidence anchors**:
  - [abstract]: "ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis."
  - [section]: "Compared with existing advanced black-box methods, ChatGPT provides mostly reliable explanations and reasoning steps for its classifications, showing its potential to enhance the transparency of mental health analysis."
  - [corpus]: Found 5 related papers with "explanations" in their titles, indicating active research in this area.
- **Break condition**: If ChatGPT's reasoning contains logical errors or if it generates plausible-sounding but incorrect explanations, the transparency benefit could be undermined.

## Foundational Learning

- **Concept**: Zero-shot learning
  - Why needed here: The paper evaluates ChatGPT's ability to perform mental health analysis tasks without any task-specific training, relying only on its pre-existing knowledge from training.
  - Quick check question: What is the key difference between zero-shot and few-shot prompting, and why might zero-shot be more challenging for mental health tasks?

- **Concept**: Chain-of-Thought prompting
  - Why needed here: The paper explores whether guiding ChatGPT through explicit reasoning steps improves its performance on mental health analysis tasks.
  - Quick check question: How does Chain-of-Thought prompting differ from standard prompting, and what types of tasks benefit most from this approach?

- **Concept**: Explainability in AI
  - Why needed here: The paper emphasizes the importance of generating interpretable explanations for mental health classifications, which is crucial for clinical applications.
  - Quick check question: Why is explainability particularly important in mental health applications compared to other domains?

## Architecture Onboarding

- **Component map**: Input text -> Prompt selection -> ChatGPT API call -> Response parsing -> Classification + Explanation -> Evaluation
- **Critical path**: Input text → Prompt selection → ChatGPT API call → Response parsing → Classification + Explanation → Evaluation
- **Design tradeoffs**: Zero-shot vs. few-shot (speed/accessibility vs. accuracy), simple vs. emotion-enhanced prompts (simplicity vs. performance), automated vs. human evaluation (scalability vs. quality)
- **Failure signatures**: Unstable predictions with minor prompt changes, inaccurate reasoning leading to wrong classifications, refusal to classify due to content policy violations
- **First 3 experiments**:
  1. Test vanilla zero-shot performance across all 11 datasets to establish baseline
  2. Test zero-shot CoT prompting to evaluate if simple reasoning instructions help
  3. Test emotion-enhanced CoT prompting with unsupervised emotional cues to assess performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve ChatGPT's performance on mental health analysis tasks while maintaining interpretability?
- Basis in paper: [explicit] The paper shows that ChatGPT underperforms advanced task-specific methods on mental health analysis tasks, but has potential for improving transparency through explainability.
- Why unresolved: The paper only explores basic prompting strategies and emotion-enhanced prompting, but does not investigate more advanced techniques like few-shot prompting or knowledge infusion that could potentially improve performance.
- What evidence would resolve it: Experiments comparing ChatGPT's performance on mental health tasks using different prompting strategies, including few-shot prompting and knowledge infusion, while maintaining or improving interpretability.

### Open Question 2
- Question: What is the optimal way to incorporate emotional cues into prompts for mental health analysis tasks?
- Basis in paper: [explicit] The paper explores emotion-enhanced prompting strategies, but finds that using distantly supervised sentiment and emotion lexicon information can decrease performance, while unsupervised emotion-enhanced prompts can improve performance.
- Why unresolved: The paper does not investigate the impact of using different sources or granularities of emotional information, or the optimal way to integrate this information into prompts.
- What evidence would resolve it: Experiments comparing ChatGPT's performance on mental health tasks using different sources and granularities of emotional information, and different methods of integrating this information into prompts.

### Open Question 3
- Question: How can we address ChatGPT's limitations in unstable predictions and inaccurate reasoning for mental health analysis tasks?
- Basis in paper: [explicit] The paper identifies ChatGPT's limitations in unstable predictions and inaccurate reasoning, but does not propose solutions to these issues.
- Why unresolved: The paper does not investigate the root causes of these limitations or propose methods to address them.
- What evidence would resolve it: Experiments investigating the root causes of ChatGPT's unstable predictions and inaccurate reasoning, and testing methods to address these issues, such as incorporating additional context or using different prompting strategies.

## Limitations

- The study focuses solely on ChatGPT without comparing other LLMs, limiting generalizability across models
- Human evaluation of explanations involved only 3 annotators with minimal agreement statistics reported, raising concerns about reliability
- The study does not address potential biases in ChatGPT's training data that could affect mental health analysis for diverse populations

## Confidence

- **High confidence**: The finding that emotion-enhanced prompts improve performance by incorporating emotional cues
- **Medium confidence**: The observation that zero-shot CoT prompting does not consistently improve performance compared to vanilla zero-shot
- **Low confidence**: The claim that ChatGPT generates explanations "approaching human performance"

## Next Checks

1. **Prompt sensitivity analysis**: Systematically test multiple variations of emotion-enhanced prompts to determine which specific components (emotional cue identification, reasoning structure, or both) drive performance improvements, and establish stability thresholds for minor prompt modifications.

2. **Cross-LLM comparison**: Evaluate other large language models (e.g., Claude, LLaMA, GPT-4) using identical prompt templates to determine whether ChatGPT's performance patterns are model-specific or generalizable across LLM architectures.

3. **Clinical validation study**: Conduct a small-scale study with mental health professionals to assess whether ChatGPT's explanations and predictions align with clinical judgment and whether the explainability features actually improve trust and usability in practical mental health screening scenarios.