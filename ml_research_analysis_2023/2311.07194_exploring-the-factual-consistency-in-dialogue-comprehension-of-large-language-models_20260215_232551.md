---
ver: rpa2
title: Exploring the Factual Consistency in Dialogue Comprehension of Large Language
  Models
arxiv_id: '2311.07194'
source_url: https://arxiv.org/abs/2311.07194
tags:
- dialogue
- data
- summary
- llms
- comprehension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates dialogue comprehension of LLMs using factual
  consistency in generated summaries (DIAC-Sum) and factual question answering (DIAC-QA).
  On average, 26.8% of summaries contain factual inconsistencies, with ChatGPT still
  having 16% errors.
---

# Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models

## Quick Facts
- arXiv ID: 2311.07194
- Source URL: https://arxiv.org/abs/2311.07194
- Authors: 
- Reference count: 8
- On average, 26.8% of generated dialogue summaries contain factual inconsistencies, with ChatGPT still having 16% errors

## Executive Summary
This work evaluates dialogue comprehension of LLMs by examining factual consistency in generated summaries and factual question answering. The study finds that 26.8% of summaries contain factual inconsistencies on average, with even ChatGPT showing 16% error rates. The analysis reveals that understanding subject/object references in conversations remains the most challenging problem. To address this, the authors propose a multi-task fine-tuning approach using auto-constructed data, achieving an 11% relative error rate reduction on factual question answering tasks.

## Method Summary
The evaluation framework consists of two main components: DIAC-Sum for measuring factual consistency in dialogue summaries, and DIAC-FactQA for evaluating factual question answering. The authors manually annotate inconsistencies in summaries generated by five popular LLMs on the SAMSum dataset, then construct factual questions based on these inconsistencies. For improvement, they propose a multi-task fine-tuning paradigm using auto-constructed data across four tasks: masked Subject/Object QA, dialogue summary quality evaluation, multiple-choice questions for selecting best candidate summary, and dialogue summarization itself.

## Key Results
- Average factual inconsistency rate of 26.8% across all LLMs in dialogue summaries
- ChatGPT achieves 16% error rate, showing that even advanced models struggle with factual consistency
- Average error rate of 36.1% in answering factual questions, with ChatGPT achieving 26.2%
- Multi-task fine-tuning achieves 11% relative error rate reduction on DIAC-FactQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factual consistency in dialogue summarization reveals dialogue comprehension ability
- Mechanism: If a model correctly understands dialogue, it should generate summaries without factual inconsistencies; measuring inconsistencies indirectly evaluates comprehension
- Core assumption: Factual errors in summaries correspond to misunderstandings in dialogue processing
- Evidence anchors:
  - [abstract] "we propose to perform the evaluation with the help of the dialogue summarization task. Since summarization extracts important information from the dialogue which naturally requires a correct understanding of the dialogue"
  - [section 2.2] "we focus on the factual consistency of the generation, which shed lights on how the models interpret the given dialogue"
- Break condition: If model generates factually consistent summaries through memorization rather than genuine comprehension

### Mechanism 2
- Claim: Multi-task fine-tuning with auto-constructed data improves dialogue comprehension
- Mechanism: Training on diverse dialogue-related tasks (PerQA, QE, QEQA, DiaSum) forces the model to develop general dialogue understanding abilities
- Core assumption: Learning to perform these varied tasks requires and reinforces core dialogue comprehension skills
- Evidence anchors:
  - [section 4.3] "The experimental results showed that after fine-tuning, the model's dialogue comprehension capabilities were indeed enhanced"
  - [section 4.1] "Our intuition is simple: the model must correctly understand the subject-object in the dialogue in order to perform the tasks accurately"
- Break condition: If model overfits to the specific synthetic data distribution rather than generalizing

### Mechanism 3
- Claim: Deriving factual questions from summary inconsistencies creates effective comprehension evaluation
- Mechanism: Questions targeting specific errors in model-generated summaries directly probe the model's understanding of problematic dialogue aspects
- Core assumption: A model's ability to answer questions about its own summary errors reflects its comprehension of the underlying dialogue
- Evidence anchors:
  - [abstract] "we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension"
  - [section 2.3] "answer these questions may be more challenging than generating a summary alone"
- Break condition: If questions are too narrow or don't capture the full range of comprehension challenges

## Foundational Learning

- Concept: Factual consistency in text generation
  - Why needed here: Core evaluation metric for dialogue comprehension
  - Quick check question: What distinguishes a factually consistent summary from an inconsistent one?

- Concept: Subject-object resolution in dialogue
  - Why needed here: Identified as primary comprehension challenge
  - Quick check question: Why is tracking "who did what" particularly difficult in multi-turn conversations?

- Concept: Multi-task learning principles
  - Why needed here: Foundation for the proposed fine-tuning approach
  - Quick check question: How does training on multiple related tasks typically affect model generalization?

## Architecture Onboarding

- Component map:
  - Dialogue → Summary generation → Inconsistency annotation → Question construction → Evaluation
  - Fine-tuning pipeline: Pseudo-data generation → Multi-task training → Evaluation
  - Evaluation framework: DIAC-Sum (consistency annotation) + DIAC-FactQA (question answering)

- Critical path: Generating consistent summaries requires correct understanding of subject/object references throughout the dialogue

- Design tradeoffs:
  - Manual annotation vs. automated evaluation: Manual provides ground truth but is expensive
  - Synthetic data vs. real data: Synthetic is scalable but may have distribution shift
  - Multi-task vs. single-task training: Multi-task builds general skills but may be less focused

- Failure signatures:
  - High inconsistency rates in summaries despite strong overall generation metrics
  - Poor performance on factual questions despite decent summary quality
  - Minimal improvement from multi-task fine-tuning

- First 3 experiments:
  1. Run consistency annotation on a small sample of generated summaries to verify annotation protocol
  2. Test factual question generation from a few inconsistency cases to validate question quality
  3. Run baseline evaluation on a subset of models to establish initial performance metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different error types (SubObjE, ProE, HalE, ParE, NegE) in dialogue summaries correlate with specific weaknesses in LLM dialogue comprehension abilities?
- Basis in paper: [explicit] The paper identifies five error types and analyzes their prevalence across different LLMs
- Why unresolved: The paper provides statistics on error distribution but doesn't deeply explore how each error type relates to specific comprehension weaknesses
- What evidence would resolve it: Detailed correlation analysis between error types and specific dialogue comprehension tasks, showing which error types most strongly indicate comprehension deficits

### Open Question 2
- Question: What is the optimal combination of multi-task data types for improving LLM dialogue comprehension abilities?
- Basis in paper: [explicit] The paper presents a multi-task fine-tuning approach with four different tasks and conducts ablation studies
- Why unresolved: The ablation study only removes one task at a time; optimal task combinations and proportions are not explored
- What evidence would resolve it: Systematic experiments testing all possible task combinations and their relative contributions to dialogue comprehension improvement

### Open Question 3
- Question: How does the performance gap between DIAC-FactQA and DREAM reflect the difficulty of measuring true dialogue comprehension?
- Basis in paper: [explicit] The paper notes that DIAC-FactQA is more challenging than DREAM, with average accuracy of 62.8% vs higher DREAM scores
- Why unresolved: The paper doesn't fully explain why DIAC-FactQA is more difficult or what this reveals about measuring dialogue comprehension
- What evidence would resolve it: Comparative analysis of question types, difficulty levels, and what each dataset actually measures about dialogue understanding

## Limitations
- The evaluation relies on manual annotation of factual inconsistencies, which introduces potential subjectivity and scalability challenges
- The auto-constructed multi-task data may have distribution shifts that limit generalizability to real-world dialogue comprehension
- The analysis focuses on a single dialogue summarization dataset (SAMSum), limiting external validity

## Confidence
- **High confidence**: Factual consistency metrics effectively reveal dialogue comprehension limitations, supported by clear error rate patterns (26.8% summary inconsistencies, 36.1% factual question errors)
- **Medium confidence**: The multi-task fine-tuning approach improves dialogue comprehension, though the 11% relative error reduction should be validated on additional datasets
- **Low confidence**: The claim that subject-object understanding is the "most challenging problem" - this is based on error pattern analysis but may not capture all comprehension difficulties

## Next Checks
1. Conduct inter-annotator agreement analysis on the inconsistency annotation protocol to quantify reliability and identify ambiguous cases
2. Test the multi-task fine-tuned model on an independent dialogue comprehension dataset (e.g., DREAM or CoQA) to validate generalization
3. Perform ablation studies on the multi-task fine-tuning to determine which component tasks contribute most to comprehension improvements