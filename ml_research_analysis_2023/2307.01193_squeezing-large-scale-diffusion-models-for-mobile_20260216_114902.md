---
ver: rpa2
title: Squeezing Large-Scale Diffusion Models for Mobile
arxiv_id: '2307.01193'
source_url: https://arxiv.org/abs/2307.01193
tags:
- diffusion
- mobile
- stable
- image
- devices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles deploying large-scale diffusion models like Stable
  Diffusion on mobile devices, where computational and memory resources are limited.
  The core idea is to optimize the Stable Diffusion v2.1 model for on-device inference
  using TensorFlow Lite on mobile GPUs, avoiding custom kernels for broader applicability.
---

# Squeezing Large-Scale Diffusion Models for Mobile

## Quick Facts
- **arXiv ID**: 2307.01193
- **Source URL**: https://arxiv.org/abs/2307.01193
- **Reference count**: 12
- **Primary result**: Mobile Stable Diffusion achieves <7 second end-to-end inference latency for 512x512 image generation on Samsung Galaxy S23 with Adreno 740 GPU

## Executive Summary
This work addresses the challenge of deploying large-scale diffusion models like Stable Diffusion v2.1 on mobile devices with limited computational and memory resources. The authors optimize the model for on-device inference using TensorFlow Lite on mobile GPUs without custom kernels, making it broadly applicable. Key technical challenges include incomplete GPU delegation due to large activations, numerical instability in float16 GELU operations, memory constraints, and model size. The proposed solutions enable complete GPU delegation, prevent floating-point exceptions, and achieve end-to-end inference latency of less than 7 seconds for generating a 512x512 image on Android devices.

## Method Summary
The authors optimize Stable Diffusion v2.1 for mobile deployment by addressing several key challenges. They convert fully-connected layers to equivalent Conv2D layers to enable complete GPU delegation, serialize large Conv2D layers to reduce activation sizes, and remove BroadcastTo operations in group normalization. To address numerical instability, they implement a more stable GELU approximation using a clipped tanh function. For memory-constrained devices, they employ pipelined execution where the denoising network remains in memory while text encoder and image decoder are loaded interchangeably. Finally, they apply quantization, pruning, and knowledge distillation to reduce model size and improve inference speed, achieving less than 7 seconds end-to-end latency on mobile GPUs.

## Key Results
- Mobile Stable Diffusion generates 512x512 images in less than 7 seconds on Samsung Galaxy S23
- Complete GPU delegation achieved through Conv2D conversion and input serialization
- Numerically stable GELU approximation prevents float16 overflow without degrading image quality
- Outperforms prior mobile deployments while using only off-the-shelf TFLite engine

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Converting fully-connected layers to Conv2D enables complete GPU delegation on mobile devices.
- **Mechanism**: Large fully-connected layers with high-dimensional inputs (e.g., 1x4096x320) are not supported by TFLite GPU delegate. By reshaping these layers into equivalent Conv2D layers with appropriate kernel sizes, the operations become delegatable to the GPU.
- **Core assumption**: The computational equivalence between fully-connected and reshaped Conv2D layers holds under float16 precision on mobile GPUs.
- **Evidence anchors**: [section] "Since the large fully-connected layers failed to be delegated, we convert them to equivalent convolution layers as shown in Fig. 1. Note that the depicted FullyConnected layer and the Reshape-Conv2D-Reshape layers result the same output and show almost the same latency when benchmarked on the GPU." [abstract] "Solutions include converting fully-connected layers to Conv2D"

### Mechanism 2
- **Claim**: Input serialization of large Conv2D layers enables complete GPU delegation with minimal latency overhead.
- **Mechanism**: For Conv2D layers with very large input/output activations that still fail delegation, splitting the computation along the input channel dimension reduces activation sizes to delegatable levels. The minimal serialization factor is chosen to balance delegation success and kernel call overhead.
- **Core assumption**: The overhead from multiple kernel calls is outweighed by the performance gain from GPU execution versus CPU fallback.
- **Evidence anchors**: [section] "Serializing the Conv2D operator can solve this problem by reducing the activation sizes, but at the cost of multiple kernel call overhead. Therefore, the minimal serialization factor should be chosen to avoid excessive overhead." [abstract] "input-serializing large Conv2D layers"

### Mechanism 3
- **Claim**: The numerically stable GELU approximation prevents floating-point exceptions on mobile devices while maintaining image quality.
- **Mechanism**: The standard GELU approximation using cubic polynomial terms causes overflow in float16 on mobile GPUs. Clipping the input to the tanh function prevents extreme values while preserving the approximation's accuracy.
- **Core assumption**: The clipping threshold (M=10) is sufficient to prevent overflow without significantly distorting the GELU function's shape.
- **Evidence anchors**: [section] "Instead of this well-known approximation, we use the following more numerically stable approximation... We use an empirical value M = 10, which suppresses the floating-point exceptions and maintains the image quality as shown in Fig. 2." [abstract] "using a numerically stable GELU approximation"

## Foundational Learning

- **Concept**: TFLite GPU delegate architecture and operator support
  - **Why needed here**: Understanding which operators are supported and why certain layers fail delegation is crucial for implementing the conversion strategies.
  - **Quick check question**: What happens to unsupported operators in the TFLite GPU delegate?

- **Concept**: Float16 numerical stability and overflow behavior
  - **Why needed here**: The GELU stabilization solution relies on understanding how float16 precision can cause overflow in polynomial approximations.
  - **Quick check question**: Why does the cubic term in GELU approximation cause problems specifically in float16?

- **Concept**: Memory management and pipelining in mobile inference
  - **Why needed here**: The pipelined execution strategy for loading components is essential for devices with limited memory.
  - **Quick check question**: How does loading components in parallel threads reduce peak memory usage compared to loading all components simultaneously?

## Architecture Onboarding

- **Component map**: TFLite converter -> GPU delegate with operator filtering -> Pipelined execution manager -> Model compression pipeline (quantization + pruning + distillation) -> Benchmark measurement system

- **Critical path**: 1. Model conversion with custom operations 2. GPU delegation validation 3. Latency measurement on target device 4. Quality validation of generated images

- **Design tradeoffs**: Latency vs. image quality: Quantization and pruning reduce latency but may degrade output quality; Completeness vs. efficiency: Complete GPU delegation improves performance but requires model modifications; Memory vs. speed: Pipelined execution reduces memory usage but adds complexity

- **Failure signatures**: Incomplete GPU delegation indicated by CPU fallback operators in the graph; Floating-point exceptions during GELU computation; Memory allocation failures during model loading; Significant quality degradation after compression

- **First 3 experiments**:
  1. Benchmark the baseline Stable Diffusion v2.1 model on target device to establish performance baseline and identify delegation failures
  2. Apply fully-connected to Conv2D conversion and measure impact on GPU delegation completeness and latency
  3. Test the numerically stable GELU approximation on target device to verify it prevents floating-point exceptions while maintaining image quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important research directions emerge from the work:

1. How does the pipelined execution strategy impact memory usage and latency across different mobile device architectures with varying memory capacities?
2. What is the quantitative performance degradation (in terms of image quality metrics) when using the numerically stable GELU approximation versus the original approximation?
3. How does the serialization factor choice (input vs output dimension) affect the quality of generated images and what is the optimal trade-off between delegation success and image fidelity?

## Limitations
- Performance on mobile devices with different GPU architectures (Adreno, Mali, Apple Silicon) remains unverified
- Quantitative image quality metrics (FID scores, user studies) are not provided to validate compressed model outputs
- Combined effect of quantization, pruning, and knowledge distillation on latency and image quality is not fully characterized

## Confidence
- **High Confidence**: Technical solutions for GPU delegation (Conv2D conversion, input serialization, BroadcastTo removal) are well-justified and directly address documented TFLite GPU delegate limitations. The <7 second latency claim on the tested device is supported by the described optimizations.
- **Medium Confidence**: Numerically stable GELU approximation effectively prevents floating-point exceptions, but long-term stability across diverse mobile GPUs needs verification. Pipelined execution approach is theoretically sound but may have device-specific performance variations.
- **Low Confidence**: Knowledge distillation technique for reducing inference steps lacks implementation details, making it difficult to assess effectiveness or reproduce results. Paper doesn't provide sensitivity analysis for compression parameters.

## Next Checks
1. Test the Mobile Stable Diffusion implementation on at least three different Android devices with varying GPU architectures (Adreno, Mali, Apple Silicon) to verify portability and identify device-specific optimizations needed.
2. Conduct systematic evaluation comparing image quality metrics (FID scores, CLIP similarity) between original Stable Diffusion v2.1 and compressed Mobile Stable Diffusion across diverse prompts to quantify quality trade-offs.
3. Perform ablation studies varying quantization bit-width, pruning ratios, and distillation parameters to understand individual and combined impacts on latency, model size, and image quality, establishing optimal configurations for different deployment scenarios.