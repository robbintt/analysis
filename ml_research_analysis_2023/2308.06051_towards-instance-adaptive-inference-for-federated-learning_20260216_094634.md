---
ver: rpa2
title: Towards Instance-adaptive Inference for Federated Learning
arxiv_id: '2308.06051'
source_url: https://arxiv.org/abs/2308.06051
tags:
- data
- local
- heterogeneity
- fedins
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of non-i.i.d. data distributions
  in federated learning (FL), specifically addressing both inter- and intra-client
  heterogeneity.
---

# Towards Instance-adaptive Inference for Federated Learning

## Quick Facts
- arXiv ID: 2308.06051
- Source URL: https://arxiv.org/abs/2308.06051
- Authors: 
- Reference count: 40
- Primary result: Achieves 6.64% improvement over top-performing FL method while maintaining <15% communication cost

## Executive Summary
This paper addresses the challenge of non-i.i.d. data distributions in federated learning by proposing FedIns, an algorithm that leverages instance-adaptive inference to handle intra-client data heterogeneity. Rather than using large instance-adaptive models, FedIns employs a parameter-efficient fine-tuning method called scale and shift deep features (SSF), where each client maintains an SSF pool that is aggregated on the server. This approach effectively reduces both inter- and intra-client heterogeneity while maintaining low communication costs. Experimental results on CIFAR-100 and Tiny-ImageNet datasets demonstrate significant performance improvements over state-of-the-art FL algorithms.

## Method Summary
FedIns tackles non-i.i.d. data distributions in federated learning by training an SSF pool for each client and aggregating them on the server to form a federated SSF pool. During inference, for each instance, the algorithm dynamically selects the best-matched SSF subsets from the pool using cosine similarity between queries and keys, then aggregates them to generate instance-adaptive SSFs. This parameter-efficient fine-tuning approach updates only scale and shift factors rather than entire model parameters, enabling significant performance improvements while maintaining communication costs below 15%. The method effectively handles both inter-client heterogeneity (through federated SSF aggregation) and intra-client heterogeneity (through instance-specific SSF selection).

## Key Results
- Achieves 6.64% improvement against top-performing FL method on CIFAR-100 and Tiny-ImageNet
- Maintains communication cost below 15% compared to baseline methods
- Effectively handles both inter-client and intra-client data heterogeneity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSF pool enables instance-adaptive inference without introducing extra parameters or FLOPs.
- Mechanism: For each instance, the best-matched SSF subsets are dynamically selected from the pool using cosine similarity between queries and keys, then aggregated to generate an instance-adaptive SSF.
- Core assumption: The pre-trained model can generate meaningful query representations for instance matching.
- Evidence anchors:
  - [abstract] "To enable instance-adaptive inference, for a given instance, we dynamically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive SSF specified for the instance"
  - [section] "For a given instance x of a client, we first use the pre-trained model to generate a query qk(x) which has the same dimension as the keys. Based on the cosine similarity between qk(x) and each keys in Kg, we select the C best-matched SSFs"
- Break condition: If the pre-trained model cannot generate discriminative query representations, instance matching will fail.

### Mechanism 2
- Claim: Federated SSF pool aggregation implicitly incorporates client-relevant knowledge to guide instance-adaptive responses.
- Mechanism: Each client trains an SSF pool, which is then aggregated on the server to form a federated SSF pool that contains knowledge from multiple clients.
- Core assumption: Aggregating SSF pools from multiple clients preserves and combines useful knowledge for handling different instances.
- Evidence anchors:
  - [abstract] "we train an SSF pool for each client, and aggregate them on the server side, thus still maintaining a low communication cost"
  - [section] "Federated SSF pool is aggregated from multiple groups of local SSF, which implicitly add client-relevant knowledge to the federated SSF pool"
- Break condition: If SSF pools from different clients are too dissimilar, aggregation may dilute useful knowledge.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning with SSF reduces communication costs while maintaining performance.
- Mechanism: Only SSF parameters (scale and shift factors) are updated and communicated, not the entire model parameters.
- Core assumption: Fine-tuning only scale and shift factors is sufficient to adapt the pre-trained model to local data distributions.
- Evidence anchors:
  - [abstract] "Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model"
  - [section] "we adopt a recent parameter-efficient fine-tuning paradigm, i.e., SSF [28], which trains only a small number of learnable parameters and brings no inference overhead by reparameterizing them into the original pre-trained model weights"
- Break condition: If local data heterogeneity is too severe, SSF fine-tuning may not capture necessary adaptations.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: The entire framework is built on FL principles of distributed training without sharing raw data
  - Quick check question: What is the key difference between FedAvg and FedIns in terms of what gets communicated?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: SSF is a parameter-efficient method that allows fine-tuning without updating all model parameters
  - Quick check question: How does SSF differ from prompt tuning in terms of inference overhead?

- Concept: Instance-adaptive inference
  - Why needed here: The core innovation is adapting model behavior per instance rather than per client
  - Quick check question: Why might instance-adaptive inference be more effective than client-adaptive approaches for intra-client heterogeneity?

## Architecture Onboarding

- Component map:
  Pre-trained backbone -> SSF pool -> Keys -> Aggregation mechanism -> Instance-adaptive inference module

- Critical path:
  1. Client trains local SSF pool
  2. Server aggregates SSF pools
  3. During inference: generate query → find best-matched SSFs → aggregate → apply instance-adaptive SSF

- Design tradeoffs:
  - Pool size vs. communication cost: Larger pools improve performance but increase communication
  - Number of best-matched SSFs vs. adaptation quality: More SSFs provide better adaptation but increase computation
  - Query representation quality vs. instance matching accuracy: Better queries lead to better matching

- Failure signatures:
  - Performance degradation when β (inter-client heterogeneity) is very small
  - Accuracy drops when γ (intra-client heterogeneity) is very large
  - Poor performance if SSF pool size is too small or too large

- First 3 experiments:
  1. Vary pool size M from 5 to 50 and measure accuracy/performance tradeoff
  2. Test different numbers of best-matched SSFs C (1, 3, 5, 10) to find optimal value
  3. Compare performance across different levels of data heterogeneity (vary β and γ parameters)

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions. However, several implicit questions arise from the limitations of the current work, including scalability to larger client numbers, performance under combined label and feature shift scenarios, and sensitivity to hyperparameter choices such as SSF pool size and number of best-matched SSFs.

## Limitations

- The SSF approach may not be sufficient for extremely heterogeneous data distributions where full model adaptation is needed
- The effectiveness of instance-adaptive inference heavily depends on the quality of query representations from the pre-trained model
- The aggregation of SSF pools assumes knowledge can be combined without significant dilution, which may not hold for highly dissimilar client data

## Confidence

- High confidence: The basic mechanism of SSF pooling and instance-adaptive inference is technically sound and well-explained
- Medium confidence: Claims about communication cost reduction are supported by parameter counts, but practical bandwidth implications need verification
- Medium confidence: Performance improvements over baselines are demonstrated, but the 6.64% improvement claim needs independent validation

## Next Checks

1. **Cross-architecture validation**: Test FedIns with different backbone architectures (e.g., ResNet, ConvNeXt) to verify that improvements are not architecture-specific
2. **Robustness to heterogeneity levels**: Systematically vary β and γ parameters to identify failure thresholds where FedIns performance degrades significantly
3. **Communication overhead measurement**: Measure actual communication costs in practice, including model initialization, SSF pool updates, and aggregation overhead, to verify the claimed <15% communication cost advantage