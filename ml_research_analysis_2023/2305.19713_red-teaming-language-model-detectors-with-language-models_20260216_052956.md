---
ver: rpa2
title: Red Teaming Language Model Detectors with Language Models
arxiv_id: '2305.19713'
source_url: https://arxiv.org/abs/2305.19713
tags:
- text
- detectors
- word
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically evaluates the reliability of three types
  of AI-generated text detectors under adversarial attacks. Two attack strategies
  are proposed: (1) word substitutions via prompting a protected LLM, and (2) instructional
  prompts to change writing style.'
---

# Red Teaming Language Model Detectors with Language Models

## Quick Facts
- arXiv ID: 2305.19713
- Source URL: https://arxiv.org/abs/2305.19713
- Authors: 
- Reference count: 35
- Primary result: Word substitution and instructional prompt attacks significantly reduce machine-generated text detector performance, with AUROC dropping below random guessing in some cases.

## Executive Summary
This work systematically evaluates the reliability of three types of AI-generated text detectors under adversarial attacks. Two attack strategies are proposed: (1) word substitutions via prompting a protected LLM, and (2) instructional prompts to change writing style. Experiments on text completion and question-answering datasets show that both attacks significantly reduce detector performance, with AUROC dropping below random guessing in some cases. These findings highlight the vulnerability of current detectors and the need for more robust detection methods.

## Method Summary
The study proposes two adversarial attack strategies against machine-generated text detectors. First, word substitution attacks use a protected LLM to generate candidate synonyms that replace words in the original text, either randomly or through evolutionary search. Second, instructional prompt attacks append carefully crafted prompts to input text to change the writing style of generated output. The attacks are tested against three detector types - classifier-based, watermarking, and likelihood-based - using datasets including XSum, WikiText, ELI5, and GPT-2 output. Detection performance is measured using AUROC, detection rate, and attack success rate metrics.

## Key Results
- Word substitution attacks significantly reduced AUROC scores across all detector types
- Query-based evolutionary search further improved attack effectiveness compared to query-free substitutions
- Instructional prompts successfully shifted writing style to evade classifier-based detectors
- DetectGPT showed high false positive rates in cross-model settings
- AUROC scores dropped below random guessing (0.5) in some attack scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing words in LLM-generated text with synonyms generated by a protected LLM significantly reduces detector performance.
- Mechanism: The attack uses an auxiliary LLM (G') to generate candidate synonyms for words in the original text. These synonyms are then substituted either randomly (query-free) or using evolutionary search to minimize the detection score.
- Core assumption: The protected LLM (G') can generate plausible synonyms that maintain semantic meaning while altering the statistical properties of the text that detectors rely on.
- Evidence anchors:
  - [abstract]: "Experiments reveal that our attacks effectively compromise the performance of all tested detectors, thereby underscoring the urgent need for the development of more robust machine-generated text detection systems."
  - [section 4]: "We utilize a protected LLM denoted as G′...we use s(yk, Y, G′, n) to denote the process of generating at most n word substitution candidates for yk given the context in Y by prompting G′."
  - [corpus]: Found 25 related papers, average neighbor FMR=0.467. Weak corpus evidence specific to this exact mechanism.
- Break condition: The attack fails if the synonym substitutions are either too obvious (easily detected by the detector) or too rare (degrading text quality).

### Mechanism 2
- Claim: Appending an instructional prompt to the input of a text-generating LLM can shift the writing style to evade detection by classifier-based detectors.
- Mechanism: The attack searches for an optimal prompt (Xp) consisting of an instruction (Xins) and a reference passage (Xref) that, when appended to the original input, causes the LLM to generate text in a style that evades detection.
- Core assumption: Classifier-based detectors are vulnerable to distribution shifts in writing style, and the LLM can effectively mimic the style of the reference passage when instructed.
- Evidence anchors:
  - [abstract]: "Experiments reveal that our attacks effectively compromise the performance of all tested detectors, thereby underscoring the urgent need for the development of more robust machine-generated text detection systems."
  - [section 5]: "We search for an additional prompt Xp appended to the original input X, which forms a new input X′ = [X, Xp] to G...We achieve this by searching for an additional prompt Xp appended to the original input X."
  - [corpus]: Weak corpus evidence specific to this exact mechanism.
- Break condition: The attack fails if the reference passage style is too distinctive or if the classifier can detect the prompt-based manipulation.

### Mechanism 3
- Claim: Watermarking detectors can be bypassed by substituting tokens, especially greenlist tokens, with synonyms.
- Mechanism: The attack identifies tokens with high prediction entropy (more susceptible to watermarking influence) and substitutes them with synonyms generated by a watermarked LLM to reduce the count of greenlist tokens.
- Core assumption: Watermarking detectors rely on the statistical presence of greenlist tokens, and substituting these tokens can reduce the detection score.
- Evidence anchors:
  - [abstract]: "Experiments reveal that our attacks effectively compromise the performance of all tested detectors, thereby underscoring the urgent need for the development of more robust machine-generated text detection systems."
  - [section 4.2]: "We aim to identify and substitute more greenlist tokens to reduce the total count of greenlist tokens...At the second stage, we pick ǫm tokens with highest entropy and use a watermarked LLM G′ to generate word substitutions."
  - [corpus]: Weak corpus evidence specific to this exact mechanism.
- Break condition: The attack fails if the watermarking mechanism is too robust or if the substitutions significantly alter the semantic meaning of the text.

## Foundational Learning

- Concept: Understanding of large language models (LLMs) and their capabilities
  - Why needed here: The attack methods rely on leveraging LLMs to generate text and synonyms, and understanding their capabilities is crucial for implementing and understanding the attacks.
  - Quick check question: Can you explain the difference between autoregressive and encoder-decoder LLMs, and how this might affect their use in these attacks?

- Concept: Familiarity with text detection methods (classifier-based, watermarking, likelihood-based)
  - Why needed here: The attack methods target these different detection methods, and understanding their underlying principles is essential for designing effective attacks.
  - Quick check question: How do classifier-based detectors differ from watermarking detectors in terms of their approach to identifying machine-generated text?

- Concept: Knowledge of adversarial examples and their generation in NLP
  - Why needed here: The attack methods are essentially generating adversarial examples to fool text detectors, and understanding this concept is crucial for understanding the attacks' effectiveness.
  - Quick check question: What is the difference between a black-box and white-box attack in the context of adversarial examples, and which type of attack is used in this paper?

## Architecture Onboarding

- Component map: Generative Model (G) -> Detector (f) -> Auxiliary LLM (G')
- Critical path: G generates text → Detector evaluates text → G' generates attack (synonyms or prompts) → Detector evaluates modified text
- Design tradeoffs:
  - Query-free vs. query-based attacks: Query-free attacks are more efficient but potentially less effective, while query-based attacks require more resources but can achieve higher success rates.
  - Word substitutions vs. instructional prompts: Word substitutions are more general but can degrade text quality, while instructional prompts are more targeted but require the LLM to follow instructions.
- Failure signatures:
  - Low attack success rate: The detector is robust against the attack method
  - High detection rate: The attack method is ineffective or the text quality is degraded
  - High false positive rate: The detector is too sensitive or the attack method is too aggressive
- First 3 experiments:
  1. Implement and test the query-free word substitution attack on a classifier-based detector using a simple dataset
  2. Implement and test the query-based word substitution attack on a watermarking detector using a different dataset
  3. Implement and test the instructional prompt attack on a likelihood-based detector using a third dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different word substitution strategies (query-free vs. query-based) compare in effectiveness against various detector types?
- Basis in paper: explicit
- Why unresolved: The paper shows query-free substitutions significantly reduce AUROC scores, with query-based evolutionary search further reducing them, but doesn't provide a comprehensive comparative analysis across all detector types.
- What evidence would resolve it: A systematic comparison of AUROC/DR metrics for all three detector types (classifier-based, watermarking, likelihood-based) across both substitution strategies on multiple datasets.

### Open Question 2
- Question: How does the effectiveness of instructional prompts vary with different prompt engineering techniques?
- Basis in paper: explicit
- Why unresolved: The paper uses a specific two-stage search algorithm with fixed parameters (n=50, T=5, K=5) but doesn't explore how different search strategies or parameters affect attack success.
- What evidence would resolve it: Experiments varying search algorithm parameters, initial instructions, and reference selection methods to measure impact on AUROC and detection rates.

### Open Question 3
- Question: What is the relationship between substitution budget size and detection evasion success across different detector types?
- Basis in paper: explicit
- Why unresolved: The paper uses fixed substitution budgets (10% for DetectGPT, 20% for others) but doesn't analyze how varying this parameter affects detection evasion.
- What evidence would resolve it: Experiments systematically varying the substitution budget percentage and measuring corresponding changes in detection performance across all detector types.

### Open Question 4
- Question: How does cross-model detection vulnerability compare to single-model detection vulnerability?
- Basis in paper: explicit
- Why unresolved: The paper shows DetectGPT's performance drops significantly under cross-model settings (GPT-2-XL as generator, GPT-Neo as detector) but doesn't compare this to performance when generator and detector are the same model.
- What evidence would resolve it: Direct comparison of detection performance when using the same model vs. different models for generation and detection across all detector types.

### Open Question 5
- Question: What are the trade-offs between attack effectiveness and output quality when using different attack strategies?
- Basis in paper: explicit
- Why unresolved: The paper requires outputs maintain "quality similar to Y and remain a plausible output" but doesn't quantify this trade-off or measure output quality degradation.
- What evidence would resolve it: Human evaluation or automated metrics (e.g., perplexity, semantic similarity) comparing original and attacked outputs across all attack methods to quantify quality degradation.

## Limitations

- The study evaluates attacks against a limited set of LLM architectures, which may not generalize to other models or newer detection methods
- The evaluation focuses primarily on English text, potentially limiting applicability to other languages or specialized domains
- The paper does not extensively explore parameter sensitivity or provide detailed analysis of why certain attacks succeed or fail

## Confidence

**High Confidence**: The core finding that adversarial attacks can significantly reduce detector performance is well-supported by the experimental results. The systematic evaluation across multiple datasets and attack strategies provides robust evidence for this claim.

**Medium Confidence**: The relative effectiveness of different attack strategies (word substitution vs. instructional prompts) across detector types is reasonably supported, though the small sample size of detectors tested limits definitive conclusions about which attack works best against which detector.

**Low Confidence**: The generalizability of specific attack parameters (like the number of substitutions or prompt formulations) to other contexts or detector implementations is uncertain. The paper does not extensively explore parameter sensitivity or provide detailed analysis of why certain attacks succeed or fail.

## Next Checks

1. **Cross-architecture validation**: Test the same attack strategies against different LLM architectures (e.g., Claude, Gemini) to determine if the attack effectiveness is consistent across models.

2. **Detector robustness benchmarking**: Implement and test additional detection methods, particularly newer approaches not covered in this study, to establish a more comprehensive vulnerability landscape.

3. **Human evaluation of text quality**: Conduct systematic human assessments of the generated adversarial examples to quantify the tradeoff between attack success rate and text quality degradation.