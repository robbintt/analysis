---
ver: rpa2
title: Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow
  Co-Design
arxiv_id: '2309.13015'
source_url: https://arxiv.org/abs/2309.13015
tags:
- training
- sparse
- dense
- bdwp
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an efficient N:M sparse DNN training scheme
  using algorithm, architecture, and dataflow co-design. A bidirectional weight pruning
  method (BDWP) is proposed to leverage N:M sparsity of weights during both forward
  and backward passes of DNN training, significantly reducing the computational cost
  while maintaining model accuracy.
---

# Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design

## Quick Facts
- arXiv ID: 2309.13015
- Source URL: https://arxiv.org/abs/2309.13015
- Authors: [Not provided]
- Reference count: 40
- Key outcome: Achieves 1.75x average speedup over dense training with 0.56% accuracy loss using N:M sparse DNN training

## Executive Summary
This paper introduces an efficient N:M sparse DNN training scheme that combines algorithmic innovations with specialized hardware architecture. The authors propose a bidirectional weight pruning method (BDWP) that leverages N:M sparsity during both forward and backward passes, significantly reducing computational costs while maintaining model accuracy. They also develop an FPGA-based sparse accelerator (SAT) with unified sparse processing elements (USPEs) capable of supporting both dense and sparse operations through flexible dataflows. Multiple optimization techniques including interleave mapping, pre-generation of sparse weights, and offline scheduling are employed to maximize computational efficiency.

## Method Summary
The method combines algorithm-architecture co-design for N:M sparse DNN training. BDWP applies N:M pruning across input channels in the forward pass and output channels in the backward pass, reducing FLOPs by skipping computations involving pruned elements. SAT uses 32x32 USPEs with value-serial computing to fold dot-product operations for N:M groups into N cycles, supported by flexible interconnect for WS/OS dataflows. Optimization methods include interleave mapping for parallel processing, pre-generation of N:M sparse weights during WU stage to save memory bandwidth, and offline scheduling to maximize hardware utilization. The approach is implemented on Xilinx VCU1525 FPGA with mixed-precision training.

## Key Results
- Achieves average 1.75x speedup over dense training with only 0.56% accuracy loss
- Improves training throughput by 2.97-25.22x compared to prior FPGA-based accelerators
- Increases energy efficiency by 1.36-3.58x over existing solutions
- Validated across five DNN models (ResNet9, ViT, VGG19, ResNet18, ResNet50) and four datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BDWP reduces training operations by pruning weights in both forward and backward passes using N:M sparsity
- Mechanism: Applies N:M pruning across input channels in forward pass and output channels in backward pass, skipping computations involving pruned elements
- Core assumption: N:M sparsity patterns preserve sufficient information for model convergence
- Evidence anchors: Abstract states BDWP leverages N:M sparsity to reduce computational cost while maintaining accuracy; section III.B describes pruning across input/output channels
- Break condition: If pruning removes too many critical weights, model accuracy will degrade beyond acceptable levels

### Mechanism 2
- Claim: SAT's USPEs support both N:M sparse and dense operations with configurable dataflow
- Mechanism: USPEs use value-serial computing to fold dot-product operations for N:M groups into N cycles; flexible interconnect enables WS/OS dataflow switching
- Core assumption: Value-serial approach and flexible interconnect maintain high hardware utilization
- Evidence anchors: Abstract mentions SAT supports both dense and N:M sparse operations; section IV.B shows USPE performs dot-product operations across various configurations
- Break condition: If hardware utilization drops significantly when switching dataflows or at high sparsity ratios

### Mechanism 3
- Claim: Pre-generation of N:M sparse weights during WU stage improves memory bandwidth and efficiency
- Mechanism: FP32 weight updates sent to SORE for N:M sparse compression, allowing storage/loading of sparse weights instead of dense ones
- Core assumption: Overlap of computation and storage in SORE can be achieved without significant latency
- Evidence anchors: Abstract lists pre-generation as optimization method; section V.B describes pre-generation technique for N:M sparse weights
- Break condition: If SORE cannot keep up with weight update rate, becoming a bottleneck

## Foundational Learning

- Concept: N:M sparsity pattern
  - Why needed here: Understanding N:M sparsity is crucial for grasping how BDWP and SAT achieve computational efficiency
  - Quick check question: What does it mean for only N out of M consecutive elements to be nonzero in an N:M sparse pattern?

- Concept: Systolic array architecture
  - Why needed here: SAT uses a systolic array of USPEs, so understanding this architecture is key to comprehending its operation
  - Quick check question: How does data flow through a systolic array, and how do USPEs communicate with each other?

- Concept: Dataflow optimization
  - Why needed here: Dataflow optimization techniques like WS and OS dataflows are used to improve SAT's performance
  - Quick check question: What is the difference between weight-stationary and output-stationary dataflows, and when is each preferred?

## Architecture Onboarding

- Component map: Input/Output Buffers -> STCE (32x32 USPEs) -> WUVE (Weight Update Vector Engine) -> SORE (Sparse Online Reduction Engine) -> Data Provider
- Critical path:
  - For dense operations: Data loading -> STCE computation -> Data storing
  - For N:M sparse operations: Data loading -> SORE sparse reduction -> STCE computation -> Data storing
- Design tradeoffs:
  - Higher N:M sparsity ratios reduce computation but may impact model accuracy
  - Flexible dataflows (WS vs. OS) offer mapping flexibility but require careful scheduling
  - Pre-generation of sparse weights saves memory bandwidth but adds complexity to WU stage
- Failure signatures:
  - Low hardware utilization: Indicates inefficient dataflow mapping or insufficient parallelism
  - Accuracy degradation: Suggests pruning is too aggressive or patterns are not well-chosen
  - High latency in SORE: Points to a bottleneck in online sparse reduction
- First 3 experiments:
  1. Test STCE with dense operations only to establish baseline performance
  2. Enable 2:8 sparse operations in STCE and measure speedup and accuracy impact
  3. Evaluate SORE's impact on overall training time by comparing with offline sparse weight generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational efficiency of N:M sparse training scale with different DNN architectures beyond the five evaluated models?
- Basis in paper: [explicit] The paper evaluates five DNN models but notes effectiveness should be tested on a wider range of architectures
- Why unresolved: Only tests a limited set of models; unclear how method performs on recurrent networks or generative models
- What evidence would resolve it: Empirical results showing performance on diverse DNN architectures including recurrent and generative models

### Open Question 2
- Question: What is the impact of N:M sparse training on convergence speed and final accuracy when using different optimization algorithms beyond momentum SGD?
- Basis in paper: [explicit] Uses momentum SGD but does not explore other algorithms like Adam or RMSprop
- Why unresolved: Different optimization algorithms may have varying sensitivities to sparsity
- What evidence would resolve it: Comparative studies using various optimization algorithms (e.g., Adam, RMSprop) under N:M sparse training

### Open Question 3
- Question: How does the proposed N:M sparse training scheme perform under different hardware constraints like limited memory bandwidth or lower precision arithmetic?
- Basis in paper: [explicit] Evaluates scheme on Xilinx VCU1525 FPGA but does not explore different hardware constraints
- Why unresolved: Scalability and adaptability to different hardware platforms are not fully explored
- What evidence would resolve it: Experimental results on various hardware platforms with different memory bandwidths and precision levels

## Limitations

- Scalability to higher sparsity ratios (e.g., 1:4) remains untested, potentially limiting acceleration gains
- SORE's impact on overall training time is unclear due to uncharacterized performance under various sparsity ratios
- Pre-generation technique's effectiveness depends heavily on accuracy of sparse weight predictions, which may not generalize across all network architectures

## Confidence

- **High confidence**: SAT architecture's ability to support both dense and N:M sparse operations through USPEs
- **Medium confidence**: Overall speedup and efficiency improvements based on specific experimental conditions
- **Low confidence**: Long-term stability and generalization of BDWP across diverse network architectures and sparsity ratios

## Next Checks

1. Test BDWP with higher sparsity ratios (e.g., 1:4) to assess scalability and impact on model accuracy
2. Characterize SORE's performance under various sparsity ratios to identify potential bottlenecks
3. Evaluate pre-generation technique's accuracy across different network architectures to ensure generalizability