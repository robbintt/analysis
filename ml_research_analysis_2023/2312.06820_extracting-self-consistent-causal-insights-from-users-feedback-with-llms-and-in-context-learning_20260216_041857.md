---
ver: rpa2
title: Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and
  In-context Learning
arxiv_id: '2312.06820'
source_url: https://arxiv.org/abs/2312.06820
tags:
- causal
- feedback
- variables
- modern
- standby
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of diagnosing reported issues
  from the high volume of user feedback in Microsoft Windows Feedback Hub, particularly
  for critical topics like power and battery. The core method involves leveraging
  Large Language Models (LLMs) and In-Context Learning (ICL) to extract self-consistent
  causal insights from user feedback, including causal variables (treatment, outcome,
  confounders) and sequences of events leading to bugs.
---

# Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning

## Quick Facts
- arXiv ID: 2312.06820
- Source URL: https://arxiv.org/abs/2312.06820
- Authors: 
- Reference count: 22
- One-line primary result: LLM-based approach extracts 81% known issues and 19% new bugs from user feedback with 0% hallucination rate

## Executive Summary
This paper addresses the challenge of diagnosing reported issues from high-volume user feedback in Microsoft Windows Feedback Hub, focusing on critical topics like power and battery. The authors propose a method that leverages Large Language Models (LLMs) and In-Context Learning (ICL) to extract self-consistent causal insights from user feedback, including causal variables (treatment, outcome, confounders) and sequences of events leading to bugs. The approach uses a modified self-consistency method with an ensemble of prompts to mitigate LLM hallucination while maximizing the pool of confounding variables.

## Method Summary
The method involves leveraging LLMs and ICL to extract self-consistent causal insights from user feedback. The approach uses a modified self-consistency method with an ensemble of prompts to mitigate LLM hallucination while maximizing the pool of confounding variables. The method extracts causal variables (treatment, outcome, confounders) and sequences of events leading to bugs, and uses these insights to define causal heuristics for scoring feedback informativeness.

## Key Results
- Extracted 81% previously known issues and 19% new bugs from "Modern Standby" feedback subcategory
- Achieved 0% hallucination rate (no out-of-domain outputs)
- 44% of extracted event sequences were previously unknown to engineers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modified self-consistency approach with an ensemble of prompts reduces LLM hallucination while increasing the pool of confounding variables.
- Mechanism: By sampling multiple reasoning paths through temperature sampling and taking majority votes on treatment/outcome pairs, the method filters out inconsistent outputs. The greedy union of confounders from the elected pairs maximizes the chance of capturing all relevant confounders.
- Core assumption: Different prompts and reasoning paths will produce overlapping correct answers for the same causal variables, allowing majority voting to identify correct responses.
- Evidence anchors:
  - [abstract] "Our LLM-based approach is able to extract previously known issues, uncover new bugs, and identify sequences of events that lead to a bug, while minimizing out-of-domain outputs."
  - [section] "In this work, we adopt this approach while making two modifications: 1) We apply self-consistency on an ensemble of prompts designed in the previous step, and 2) After taking the majority vote on the generated treatments and outcomes, we follow a greedy approach by taking union of generated confounders of the elected treatment/outcome pairs in order to maximize the chance of confoundedness."
  - [corpus] Weak evidence - no direct citation of hallucination mitigation techniques in corpus papers, but related work on multi-modal fine-tuning and causal structure learning suggests complementary approaches.
- Break condition: If the ensemble prompts are too similar or the temperature sampling produces highly divergent outputs, the majority voting may fail to converge on correct answers.

### Mechanism 2
- Claim: Chain-of-Thought prompting combined with few-shot learning enables LLMs to extract previously unknown causal variables and sequences of events from user feedback.
- Mechanism: Few-shot CoT provides exemplars that guide the LLM's reasoning process, allowing it to identify causal relationships even when variables are not explicitly stated in the feedback. The temperature sampling generates diverse reasoning paths that explore different causal interpretations.
- Core assumption: The LLM can generalize from few-shot exemplars to identify causal patterns in new feedback, even when some variables are implicit or unmentioned.
- Evidence anchors:
  - [abstract] "Our LLM-based approach is able to extract previously known issues, uncover new bugs, and identify sequences of events that lead to a bug, while minimizing out-of-domain outputs."
  - [section] "Table 1 depicts extracted chains of events that result in an outcome (bug) as well as feedback scores using heuristic 2... As far as chain of events is concerned, only 44% of chains are previously known by the engineers and the rest of sequences are new chains that are discovered by engineers and worthy of more investigations."
  - [corpus] Weak evidence - while related papers discuss multi-modal fine-tuning and causal structure learning, they don't directly address few-shot CoT for causal variable extraction.
- Break condition: If the few-shot exemplars are not representative of the actual feedback distribution, the LLM may fail to generalize correctly.

### Mechanism 3
- Claim: The causal heuristics (number of extracted variables and length of causal chain) provide a quantitative measure of feedback informativeness for triaging.
- Mechanism: By scoring feedback based on the richness of extracted causal information, the method prioritizes feedback that is more likely to contain actionable insights for engineers.
- Core assumption: The number of extracted causal variables and the length of causal chains are good proxies for the informativeness and actionability of feedback.
- Evidence anchors:
  - [section] "To this end, we propose: • Heuristic 1: Number of Extracted Variables The higher the number of extracted variables, the richer the feedback. • Heuristic 2: Length of the Causal Chain The longer the length of the event sequence, the richer the feedback."
  - [section] "We may use these heuristics solely or in combination to score incoming feedback and filter out less informative ones."
  - [corpus] Weak evidence - while related papers discuss feedback analysis and causal structure learning, they don't directly address the specific heuristics proposed here.
- Break condition: If the heuristics are not calibrated properly, they may prioritize feedback that is verbose but not necessarily informative, or vice versa.

## Foundational Learning

- Concept: Causal inference and the importance of confounding variables
  - Why needed here: The method relies on correctly identifying treatment, outcome, and confounding variables to construct accurate causal graphs and derive meaningful insights from user feedback.
  - Quick check question: What is the difference between a confounder and an instrumental variable in causal inference?
- Concept: In-Context Learning (ICL) and prompting techniques
  - Why needed here: The method uses ICL with zero-shot, few-shot, and Chain-of-Thought prompting to extract causal variables and sequences from user feedback without fine-tuning the LLM.
  - Quick check question: How does Chain-of-Thought prompting differ from standard few-shot prompting?
- Concept: Self-consistency and ensemble methods for hallucination mitigation
  - Why needed here: The method employs self-consistency on an ensemble of prompts to reduce LLM hallucination and increase the reliability of extracted causal insights.
  - Quick check question: What is the intuition behind using self-consistency to mitigate LLM hallucination?

## Architecture Onboarding

- Component map: Feedback Classification -> Causal Variable Extraction -> Sequence Extraction -> Causal Graph Construction -> Feedback Scoring
- Critical path: Feedback Classification → Causal Variable Extraction → Sequence Extraction → Causal Graph Construction → Feedback Scoring
- Design tradeoffs:
  - Using an ensemble of prompts increases the chance of capturing all relevant confounders but also increases computational cost
  - The greedy union of confounders assumes that the elected treatment/outcome pairs are correct, which may not always be the case
  - The causal heuristics provide a quantitative measure of informativeness but may not capture all aspects of feedback quality
- Failure signatures:
  - Low accuracy in feedback classification leading to incorrect topic categorization
  - High hallucination rate in causal variable and sequence extraction despite self-consistency
  - Poor correlation between heuristic scores and actual feedback informativeness
- First 3 experiments:
  1. Evaluate the accuracy of feedback classification on a held-out test set
  2. Measure the hallucination rate and precision of extracted causal variables and sequences using ground truth data
  3. Assess the effectiveness of the causal heuristics in prioritizing informative feedback by comparing heuristic scores with engineer ratings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the greedy approach for confounder selection in maximizing unconfoundedness compared to other methods?
- Basis in paper: [inferred] The paper mentions using a greedy approach to select confounders after applying self-consistency, but notes this assumption does not guarantee unconfoundedness and may lead to incorrect selection.
- Why unresolved: The paper states that confounder selection strategies should be applied afterward, but does not provide experimental results comparing the greedy approach to other methods.
- What evidence would resolve it: Experimental results comparing the greedy approach to other confounder selection methods in terms of unconfoundedness and accuracy of causal variable extraction.

### Open Question 2
- Question: What is the impact of increasing temperature in LLM sampling on the quality and diversity of extracted causal variables and sequences?
- Basis in paper: [explicit] The paper mentions that increasing temperature leads to more diverse causal variables, but does not provide detailed analysis of the impact on quality and diversity.
- Why unresolved: The paper does not provide a comprehensive analysis of how temperature affects the balance between diversity and accuracy of extracted insights.
- What evidence would resolve it: A detailed study of how different temperature values affect the quality, diversity, and accuracy of extracted causal variables and sequences, including metrics like precision, recall, and F1 score.

### Open Question 3
- Question: How can the proposed causal heuristics be further refined to better score feedback informativeness and prioritize actionable insights?
- Basis in paper: [explicit] The paper proposes two heuristics based on the number of extracted variables and the length of causal chains, but notes these are preliminary and may be used in combination.
- Why unresolved: The paper does not provide experimental results or user studies to validate the effectiveness of these heuristics in prioritizing actionable feedback.
- What evidence would resolve it: Experimental results or user studies comparing the proposed heuristics to other scoring methods, and analysis of how well they prioritize actionable feedback and reduce the volume of low-quality feedback.

## Limitations
- The method relies heavily on the quality of topic classification, which could introduce errors if the zero-shot NLI is not accurate
- The greedy union approach for confounders assumes the elected treatment/outcome pairs are correct, which may not always hold true
- The evaluation is limited to one feedback subcategory ("Modern Standby"), limiting generalizability

## Confidence

- High confidence: The basic methodology of using ICL with CoT prompting for causal variable extraction is well-established in the literature
- Medium confidence: The effectiveness of the self-consistency approach with ensemble prompts for hallucination mitigation needs further validation with larger datasets
- Medium confidence: The causal heuristics for feedback scoring show promise but require more extensive testing across different feedback categories

## Next Checks
1. Conduct a human evaluation study where domain experts rate the relevance and accuracy of the extracted causal variables and sequences compared to ground truth data
2. Test the method on additional feedback categories beyond "Modern Standby" to assess generalizability and robustness
3. Compare the proposed causal heuristics with alternative scoring methods (e.g., based on sentiment analysis or keyword matching) to validate their effectiveness in prioritizing informative feedback