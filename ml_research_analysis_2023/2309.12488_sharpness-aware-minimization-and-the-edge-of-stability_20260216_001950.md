---
ver: rpa2
title: Sharpness-Aware Minimization and the Edge of Stability
arxiv_id: '2309.12488'
source_url: https://arxiv.org/abs/2309.12488
tags:
- edge
- training
- gradient
- hessian
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether Sharpness-Aware Minimization (SAM)
  operates at the edge of stability, a phenomenon where the operator norm of the Hessian
  stabilizes near a critical value during training. The authors derive a theoretical
  edge of stability for SAM, which depends on both the step size and the gradient
  norm, and empirically verify this prediction across three deep learning tasks: MNIST,
  CIFAR-10, and language modeling.'
---

# Sharpness-Aware Minimization and the Edge of Stability

## Quick Facts
- arXiv ID: 2309.12488
- Source URL: https://arxiv.org/abs/2309.12488
- Authors: Blake Bordelon, Cengiz Pehlevan
- Reference count: 6
- Key outcome: SAM consistently operates at a modified edge of stability where the Hessian operator norm tracks a predicted threshold dependent on both step size and gradient norm.

## Executive Summary
This paper investigates whether Sharpness-Aware Minimization (SAM) operates at the edge of stability, a phenomenon where the operator norm of the Hessian stabilizes near a critical value during training. The authors derive a theoretical edge of stability for SAM that depends on both the step size η and the gradient norm ||g||, which reduces to the standard edge-of-stability threshold when ρ=0 (standard GD). Empirically, SAM consistently tracks this predicted edge across three deep learning tasks: MNIST, CIFAR-10, and language modeling. The results show that SAM drives solutions toward smoother regions of parameter space early in training while maintaining competitive performance, achieving flatter minima with similar or better training error compared to standard gradient descent.

## Method Summary
The paper investigates SAM's behavior at the edge of stability by deriving a theoretical threshold for when SAM operates at this critical point. SAM's update rule evaluates gradients at a neighbor point ρ distance uphill from the current iterate: w_{t+1} = w_t - η∇ℓ(w_t + ρ∇ℓ(w_t)/||∇ℓ(w_t)||). The authors derive a modified edge-of-stability threshold: ||H||_op < ||g||/(2ρ)(√(1 + 8ρη||g||) - 1). They empirically verify this prediction by training networks on MNIST (fully connected), CIFAR-10 (CNN), and tiny_shakespeare (Transformer) with various learning rates and SAM radii, tracking the Hessian operator norm against the predicted SAM edge. Hessian estimation uses power iteration methods, and experiments systematically vary η and ρ across different datasets and architectures.

## Key Results
- SAM consistently operates at its derived edge of stability, with Hessian operator norm closely tracking the predicted threshold across all three tasks
- SAM achieves flatter minima with similar or better training error compared to standard gradient descent
- The SAM edge of stability threshold reduces to the standard edge-of-stability threshold when ρ=0, and scales with the gradient norm as ρ varies
- SAM gradients show stronger alignment with the principal eigenvector of the Hessian compared to standard GD gradients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM operates at a modified edge of stability that depends on both the step size η and the gradient norm ||g||.
- Mechanism: The edge-of-stability threshold for SAM is given by ||H||_op < ||g||/(2ρ) * (sqrt(1 + 8ρη||g||/(η||g||)) - 1), which reduces to 2/η when ρ=0 (standard GD) and scales down as ||g|| decreases.
- Core assumption: The quadratic approximation of the loss around the current iterate is sufficiently accurate for the analysis.
- Evidence anchors:
  - [abstract]: "SAM consistently operates at its derived edge of stability, with the Hessian operator norm closely tracking the predicted threshold."
  - [section 2]: Proposition 2 and Proposition 3 derive this threshold formula explicitly.
  - [corpus]: No direct corpus evidence available.
- Break condition: If the loss landscape deviates significantly from quadratic behavior or if the gradient norm becomes extremely small.

### Mechanism 2
- Claim: SAM drives solutions toward smoother regions of parameter space while the training error is still relatively high.
- Mechanism: By evaluating gradients at a neighbor point uphill from the current iterate, SAM implicitly performs gradient descent on the operator norm of the Hessian, pushing eigenvalues toward lower values.
- Core assumption: The uphill neighbor evaluation captures meaningful curvature information about the loss landscape.
- Evidence anchors:
  - [abstract]: "Rather than first driving the training error to a very small value, and then drifting along a manifold of near-optimal solutions to wider minima, SAM's process drives solutions toward smooth regions of parameter space early in training, while the loss is still large."
  - [section 3.1]: Experiments with MNIST, CIFAR-10, and language modeling show this behavior.
  - [corpus]: No direct corpus evidence available.
- Break condition: If the ρ parameter is set too large, causing the uphill evaluation to be outside the region where the quadratic approximation holds.

### Mechanism 3
- Claim: The gradients used by SAM are more aligned with the principal eigenvector of the Hessian than standard GD gradients.
- Mechanism: This improved alignment occurs because SAM explicitly searches for a neighbor point that maximizes the loss, which tends to align with directions of high curvature.
- Core assumption: The principal eigenvector of the Hessian corresponds to the direction of maximum curvature in the loss landscape.
- Evidence anchors:
  - [section 4.1]: "We also see stronger alignment with the principal direction for the gradients evaluated at the uphill location used by SAM."
  - [section 5]: Discussion of alignment between gradients and principal eigenvectors.
  - [corpus]: No direct corpus evidence available.
- Break condition: If the loss landscape has multiple directions of similar curvature, the alignment advantage may diminish.

## Foundational Learning

- Concept: Edge of stability phenomenon in gradient descent
  - Why needed here: Understanding the baseline behavior of GD at the edge of stability is crucial for deriving SAM's modified edge of stability.
  - Quick check question: What is the edge of stability threshold for standard gradient descent with step size η?

- Concept: Sharpness-Aware Minimization algorithm
  - Why needed here: SAM's update rule, which evaluates gradients at an uphill neighbor, is the core mechanism being analyzed.
  - Quick check question: How does SAM's update rule differ from standard gradient descent?

- Concept: Operator norm of the Hessian
  - Why needed here: The edge of stability is defined in terms of the operator norm of the Hessian, and SAM's behavior is analyzed by tracking this quantity.
  - Quick check question: What does the operator norm of the Hessian represent in the context of neural network training?

## Architecture Onboarding

- Component map: Loss function -> SAM update rule -> Parameter update -> Hessian operator norm estimation -> Edge of stability comparison
- Critical path: 1. Initialize network with Glorot normal initialization 2. Compute gradient at current iterate 3. Compute gradient at uphill neighbor (ρ distance in gradient direction) 4. Update parameters using uphill gradient 5. Estimate Hessian operator norm and track against SAM edge
- Design tradeoffs:
  - Larger ρ values may capture more curvature information but risk violating quadratic approximation assumptions
  - Smaller learning rates may provide more stable training but slower convergence
  - Batch vs. stochastic gradient computation affects Hessian estimation quality
- Failure signatures:
  - Divergence in training loss (learning rate too high)
  - Operator norm not tracking SAM edge (ρ too large or quadratic approximation invalid)
  - Poor generalization despite low training loss (overfitting)
- First 3 experiments:
  1. Train a simple fully connected network on MNIST with SAM and varying ρ values, tracking Hessian operator norm
  2. Compare SAM vs. standard GD on CIFAR-10 with batch gradients, examining edge of stability behavior
  3. Implement SAM with minibatch gradients on a small language modeling task, evaluating alignment with principal eigenvector

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How exactly does SAM drive down the operator norm of the Hessian while simultaneously reducing training error, especially early in training when the training error is still large?
- Basis in paper: [explicit] The paper notes that "the mechanism through which this occurs remains a mystery" and that "It is not clear how the training error is reduced so rapidly despite the overshooting associated with edge-of-stability training."
- Why unresolved: The paper's theoretical analysis requires small learning rates and neighborhood radii, whereas empirical observations show SAM's strong effect even early in training with larger parameters.
- What evidence would resolve it: Detailed empirical studies tracking the evolution of both the Hessian norm and training error throughout training, potentially combined with theoretical analysis that bridges the gap between small and large parameter regimes.

### Open Question 2
- Question: Under what conditions and why do SAM gradients align more closely with the principal direction of the Hessian compared to standard gradient descent?
- Basis in paper: [explicit] The paper observes "There was a general tendency for the gradients used by SAM to be more aligned with the principal direction of the Hessian than gradients evaluated at the iterates" and questions "It is not clear under what conditions, and why, this improved alignment is seen, and when it is helpful."
- Why unresolved: While the improved alignment is observed empirically, the theoretical analysis by Bartlett et al. [2022] assumed perfect alignment, and it's unclear when this assumption holds and its practical implications.
- What evidence would resolve it: Systematic experiments varying network architectures, loss functions, and hyperparameters to identify conditions where this alignment occurs, along with theoretical analysis explaining the alignment mechanism.

### Open Question 3
- Question: How does SAM interact with the edge-of-stability phenomenon when training with stochastic gradient descent (SGD) using minibatches of intermediate size?
- Basis in paper: [explicit] The paper states "A more thorough understanding of SAM and the edge of stability when training with SGD is another interesting and important subject for further research" and notes that Wen et al. [2023] analyzed SAM with extreme SGD but found qualitative differences.
- Why unresolved: The paper's experiments show some edge-of-stability behavior with SGD in language modeling, but the interaction between SAM and SGD's noise is not well understood, especially for minibatch sizes between extreme single-example SGD and full-batch training.
- What evidence would resolve it: Experiments systematically varying minibatch sizes while training with SAM, combined with theoretical analysis of how SGD noise affects the edge-of-stability dynamics in SAM.

## Limitations

- The theoretical analysis relies on a quadratic approximation that may break down in regions of extreme curvature or for very small gradient norms
- The alignment mechanism between SAM gradients and Hessian eigenvectors is supported by empirical observations but lacks rigorous theoretical justification
- Confidence is medium for claims about early optimization of curvature versus training error, as this requires careful interpretation of the timing relative to loss plateaus

## Confidence

- SAM operating at edge of stability: High
- SAM edge of stability threshold derivation: High
- Early curvature optimization vs training error: Medium
- SAM gradient alignment with principal eigenvector: Medium

## Next Checks

1. Test SAM with progressively smaller ρ values to determine the minimum threshold where edge-of-stability behavior persists, validating the quadratic approximation assumptions.

2. Implement alternative Hessian estimation methods (e.g., Hutchinson's trace estimator) to verify the operator norm tracking is not an artifact of the power iteration implementation.

3. Design experiments with synthetic loss landscapes of known curvature to directly observe SAM's curvature optimization behavior independent of generalization effects.