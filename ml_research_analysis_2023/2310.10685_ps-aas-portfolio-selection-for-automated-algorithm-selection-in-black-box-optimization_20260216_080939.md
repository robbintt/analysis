---
ver: rpa2
title: 'PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box
  Optimization'
arxiv_id: '2310.10685'
source_url: https://arxiv.org/abs/2310.10685
tags:
- algorithm
- portfolio
- problem
- portfolios
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates data-driven algorithm portfolio selection
  for automated algorithm selection (AAS) in black-box optimization. We propose selecting
  portfolios based on algorithm behavior meta-representations, using two distinct
  techniques: performance2vec and SHAP.'
---

# PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization

## Quick Facts
- arXiv ID: 2310.10685
- Source URL: https://arxiv.org/abs/2310.10685
- Reference count: 12
- This work proposes a data-driven portfolio selection method for automated algorithm selection in black-box optimization, showing that meta-representation-based approaches outperform greedy methods.

## Executive Summary
This paper introduces a novel approach to automated algorithm selection (AAS) in black-box optimization by selecting diverse and representative portfolios of algorithms based on meta-representations of algorithm behavior. The authors propose using performance2vec and SHAP meta-representations to capture algorithm performance and problem characteristics, then construct graphs to select portfolios using Maximal Independent Sets (MIS) and Dominating Sets (DS) algorithms. Experiments on 324 CMA-ES variants for BBOB problems demonstrate that performance2vec-based portfolios achieve minimal AAS error with small sizes, while SHAP-based portfolios offer flexibility at the cost of performance. Personalized portfolios related to problem-specific behavior show comparable or better results than greedy approaches and outperform the full portfolio.

## Method Summary
The proposed method creates algorithm behavior meta-representations (performance2vec and Shapley values), constructs a graph from a set of algorithms based on their meta-representation similarity, and applies graph algorithms (MIS and DS) to select a final portfolio of diverse, representative, and non-redundant algorithms. Two types of portfolios are tested: one based on overall algorithm behavior and a personalized one related to algorithm behavior per problem. The method is evaluated on 324 CMA-ES variants for BBOB problems, with AAS performance measured relative to the virtual best solver (VBS) from the selected portfolio.

## Key Results
- Performance2vec-based portfolios favor small sizes with negligible error in the AAS task relative to the VBS
- SHAP-based portfolios gain from higher flexibility but have decreased AAS performance
- Personalized portfolios yield comparable or slightly better results than greedy approaches and outperform the full portfolio in all scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data-driven meta-representation selection yields better AAS performance than naive greedy approaches.
- Mechanism: Algorithm meta-representations capture different aspects of algorithm behavior, enabling more informed portfolio selection via graph-based diversity algorithms.
- Core assumption: Meta-representations effectively encode algorithm behavior and diversity in a way that improves AAS performance.
- Evidence anchors: [abstract] "We observe that the approach built on the performance2vec-based representations favors small portfolios with negligible error in the AAS task relative to the virtual best solver from the selected portfolio, whereas the portfolios built from the SHAP-based representations gain from higher flexibility at the cost of decreased performance of the AAS."

### Mechanism 2
- Claim: Personalized portfolios (per-problem) yield better or comparable AAS performance to greedy and full portfolios.
- Mechanism: Local Shapley meta-representations capture problem-specific algorithm behavior, allowing for tailored portfolio selection that matches algorithm strengths to problem characteristics.
- Core assumption: Problem-level algorithm behavior is sufficiently distinct to warrant personalized portfolio selection.
- Evidence anchors: [abstract] "We test two types of portfolios: one related to overall algorithm behavior and the ‘personalized’ one (related to algorithm behavior per each problem separately)."

### Mechanism 3
- Claim: Graph-based diversity algorithms (MIS, DS) effectively select representative and non-redundant algorithms from the full set.
- Mechanism: By constructing graphs where nodes represent algorithms and edges connect similar ones, the algorithms can select a subset of algorithms that are diverse and representative.
- Core assumption: Cosine similarity of meta-representations is a good proxy for algorithmic diversity and complementarity.
- Evidence anchors: [section] "To select the algorithms, we use the Maximal Independent Sets (MIS) and Dominating Sets (DS) algorithms."

## Foundational Learning

- Concept: Automated Algorithm Selection (AAS)
  - Why needed here: Understanding the core problem being addressed is crucial to grasp the significance of portfolio selection.
  - Quick check question: What is the primary goal of AAS in black-box optimization?

- Concept: Meta-representation learning
  - Why needed here: The paper relies on learned meta-representations to encode algorithm behavior for portfolio selection.
  - Quick check question: How do performance2vec and Shapley meta-representations differ in what they capture about algorithm behavior?

- Concept: Graph algorithms (MIS, DS)
  - Why needed here: The paper uses graph algorithms to select diverse and representative algorithms from the full set.
  - Quick check question: What is the difference between the MIS and DS algorithms in the context of this paper?

## Architecture Onboarding

- Component map: Performance data -> Meta-representation learning (performance2vec, Shapley) -> Graph construction (similarity threshold) -> Portfolio selection (MIS, DS) -> AAS evaluation
- Critical path: Meta-representation learning → Graph construction → Portfolio selection → AAS evaluation
- Design tradeoffs:
  - Small vs. large portfolios: Smaller portfolios are easier to manage but may miss the VBS; larger portfolios are more flexible but increase AAS complexity.
  - Performance2vec vs. Shapley: Performance2vec favors smaller portfolios with lower AAS error; Shapley offers more flexibility but lower AAS performance.
- Failure signatures:
  - Poor AAS performance despite diverse portfolio: Meta-representations may not capture relevant behavior.
  - Extremely small or large portfolios: Similarity threshold may be set incorrectly.
- First 3 experiments:
  1. Run the full pipeline with performance2vec meta-representations and a similarity threshold of 0.95.
  2. Run the full pipeline with Shapley meta-representations and a similarity threshold of 0.70.
  3. Compare the performance of the personalized portfolios to the greedy and full portfolios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational cost of obtaining Shapley meta-representations scale with the number of algorithms and problem instances, and what strategies could be employed to reduce this cost for larger portfolios?
- Basis in paper: [explicit] The authors acknowledge the main drawback of the investigated approaches lies in the computational cost, especially for obtaining the Shapley meta-representations, since they require training regression models for each algorithm from the full portfolio and analyzing them with explainable SHAP post-hoc analysis.

### Open Question 2
- Question: How well do the proposed data-driven portfolio selection techniques generalize to other optimization algorithm families beyond CMA-ES, such as modular DE or PSO, and what modifications might be necessary?
- Basis in paper: [explicit] The authors mention future research avenues include testing the methodology for other modular frameworks such as modular DE and modular PSO, as well as examining the possible use for more inherently diverse portfolios.

### Open Question 3
- Question: What is the impact of different regression models (e.g., XGBoost, Deep Neural Networks) on the quality of Shapley meta-representations and the subsequent portfolio selection performance?
- Basis in paper: [explicit] The authors point out that Shapley meta-representations are model-specific (in their case obtained from RFs) and suggest similar analyses can be repeated for other regression models such as XGBoost, Deep Neural Networks, etc.

## Limitations
- Generalizability to other optimization domains beyond BBOB problems with CMA-ES variants is untested.
- Computational overhead of meta-representation computation and graph construction may be significant for larger algorithm portfolios.
- The paper does not provide ablation studies to isolate the contributions of individual components to overall AAS performance.

## Confidence
- High confidence: The overall methodology and experimental setup are well-described and reproducible. The results on the specific BBOB problems with CMA-ES variants are consistent and support the main claims.
- Medium confidence: The generalizability of the approach to other optimization domains and the impact of different meta-representation techniques are not fully explored, introducing some uncertainty.
- Low confidence: The computational overhead and scalability of the proposed method for larger algorithm portfolios or more complex optimization problems are not addressed.

## Next Checks
1. Reproduce the main results on the same BBOB problems with CMA-ES variants to verify the reported AAS performance and portfolio characteristics.
2. Test the proposed method on a different set of optimization problems, such as constrained or multi-objective optimization, to assess its generalizability.
3. Conduct ablation studies to systematically evaluate the contributions of individual components (e.g., meta-representation, graph construction, portfolio selection algorithms) to the overall AAS performance.