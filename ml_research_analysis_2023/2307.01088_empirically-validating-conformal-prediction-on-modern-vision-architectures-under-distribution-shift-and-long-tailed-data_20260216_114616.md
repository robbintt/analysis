---
ver: rpa2
title: Empirically Validating Conformal Prediction on Modern Vision Architectures
  Under Distribution Shift and Long-tailed Data
arxiv_id: '2307.01088'
source_url: https://arxiv.org/abs/2307.01088
tags:
- coverage
- conformal
- prediction
- inefficiency
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically evaluates the performance of conformal prediction
  methods under distribution shift and long-tailed data settings using large-scale
  datasets and modern neural network architectures. The authors assess four conformal
  prediction methods (threshold, adaptive prediction sets, regularized adaptive prediction
  sets, and conformal training) across three deep learning model families (ResNets,
  ViTs, and DeiT) on ImageNet distribution-shifted datasets and the long-tailed PlantNet-300k
  dataset.
---

# Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data

## Quick Facts
- arXiv ID: 2307.01088
- Source URL: https://arxiv.org/abs/2307.01088
- Reference count: 24
- Key outcome: Conformal prediction methods fail to maintain class-conditional coverage in long-tailed distributions while marginal coverage holds, with increased inefficiency under distribution shift

## Executive Summary
This paper empirically evaluates four conformal prediction methods (threshold, adaptive prediction sets, regularized adaptive prediction sets, and conformal training) across three modern neural network architectures (ResNets, ViTs, and DeiT) on large-scale vision datasets. The study examines performance under both distribution shift (ImageNet variants) and long-tailed data conditions (PlantNet-300k). Results show that while marginal coverage guarantees are maintained, class-conditional coverage is frequently violated in long-tailed settings, and inefficiency increases under distribution shift. These findings hold consistently across all CP methods and model architectures tested.

## Method Summary
The study evaluates four conformal prediction methods on three neural architecture families using large-scale vision datasets. The methods include threshold (THR), adaptive prediction sets (APS), regularized adaptive prediction sets (RAPS), and conformal training (ConfTr). Base models are trained on ImageNet, then calibrated using a held-out validation split. Conformity scores are computed using different approaches for each method, with thresholds determined via quantile calibration. Performance is measured across distribution-shifted datasets (ImageNet-V2, ImageNet-C, ImageNet-A, ImageNet-R) and long-tailed datasets (PlantNet-300k, iNaturalist variants). Metrics include coverage, macro coverage, number of classes with violated coverage, inefficiency, and macro inefficiency.

## Key Results
- Marginal coverage guarantees are violated even under small distribution shifts
- Class-conditional coverage is frequently violated in long-tailed distributions
- Confidence set sizes increase under both distribution shift and long-tailed conditions
- Results are consistent across all CP methods and model architectures tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction guarantees marginal coverage but not class-conditional coverage, which is violated under long-tailed distributions
- Mechanism: The conformal calibration uses a quantile over all conformity scores, so rare classes with few examples contribute weakly to the threshold, causing their coverage to fall below target
- Core assumption: The calibration set is representative of the test distribution, and coverage guarantees hold in expectation over an infinite test set
- Evidence anchors:
  - [abstract]: "class-conditional coverage is frequently violated in long-tailed distributions"
  - [section]: "Although the target coverage of 0.90 is maintained marginally across the entire dataset, it is frequently violated on a class-conditional basis"
  - [corpus]: Weak; no corpus neighbor directly addresses class-conditional guarantees
- Break condition: If per-class calibration sets are large enough to approximate the population quantile accurately, class-conditional violations may reduce

### Mechanism 2
- Claim: Distribution shift increases uncertainty, leading to larger prediction sets (higher inefficiency) even when marginal coverage is maintained
- Mechanism: Under distribution shift, the underlying model's softmax outputs become less confident, so more classes fall within the conformity threshold, expanding the sets
- Core assumption: The conformity scores (e.g., softmax values) reflect model uncertainty, and threshold τ is fixed after calibration on in-distribution data
- Evidence anchors:
  - [abstract]: "the size of the confidence sets increases under both these settings"
  - [section]: "inefficiency also increases on these datasets; a proxy for the increased uncertainty of the underlying model"
  - [corpus]: Weak; corpus focuses on fast kernel-based or time-series conformal methods, not uncertainty scaling under shift
- Break condition: If adaptive thresholds or dynamic recalibration are used, inefficiency growth could be mitigated

### Mechanism 3
- Claim: Better base model accuracy correlates with improved coverage and reduced inefficiency in conformal prediction
- Mechanism: Higher accuracy implies more calibrated softmax scores, so conformity scores better reflect true class probabilities, tightening the sets without sacrificing coverage
- Core assumption: The base model's calibration (e.g., temperature scaling) is adequate and the conformal method uses these scores directly
- Evidence anchors:
  - [section]: "coverage generally increases along with accuracy" and "inefficiency also improves"
  - [section]: "THR method seems to have larger inefficiency improvements"
  - [corpus]: No direct corpus evidence; neighbors discuss general conformal guarantees, not accuracy-covariation
- Break condition: If model accuracy improves via overfitting or shortcut learning, coverage may not improve despite higher accuracy

## Foundational Learning

- Concept: Conformal prediction fundamentals (marginal vs. class-conditional coverage, conformity scores, quantile calibration)
  - Why needed here: The paper's results hinge on understanding why marginal guarantees do not imply per-class guarantees and how threshold calibration works
  - Quick check question: What is the difference between marginal and class-conditional coverage in CP, and why does calibration use a single quantile?

- Concept: Distribution shift and long-tailed data effects on model calibration
  - Why needed here: Explains why fixed thresholds fail under shift and why rare classes lack coverage
  - Quick check question: How does a fixed conformity threshold behave when the test distribution differs from the calibration set?

- Concept: Model architecture impact on uncertainty estimation
  - Why needed here: Shows why transformer-based models (DeiT, ViT) outperform ResNets under shift in this study
  - Quick check question: What architectural features in vision transformers contribute to better out-of-distribution uncertainty estimates?

## Architecture Onboarding

- Component map: Pretrained model -> Forward pass -> Softmax -> Conformity score computation (THR/APS/RAPS) -> Quantile calibration -> Threshold τ -> Prediction sets; For ConfTr: Training loop with differentiable sorting -> Size loss + classification loss -> Updated model -> Post-hoc CP for guarantees

- Critical path:
  1. Load pretrained model, prepare ImageNet training data
  2. Split ImageNet validation into 50% for calibration, 50% for in-distribution test
  3. Compute conformity scores on calibration split for each CP method
  4. Determine threshold τ (α quantile)
  5. Apply τ to construct prediction sets on test splits
  6. Compute coverage, inefficiency, macro metrics
  7. Repeat for each distribution-shifted and long-tailed dataset

- Design tradeoffs:
  - THR: Smallest sets but worst conditional coverage
  - APS: Better conditional coverage, much larger sets
  - RAPS: Balances set size and coverage with regularization
  - ConfTr: Trains for smaller sets but marginal coverage only

- Failure signatures:
  - Coverage << target → Threshold too high or model overconfident under shift
  - Macro coverage much lower than marginal → Severe class imbalance or rare class under-representation
  - Inefficiency spikes → Model uncertainty increases; consider recalibration or adaptive thresholds

- First 3 experiments:
  1. Implement THR on ResNet-50 with ImageNet train/val split, measure coverage/inefficiency
  2. Add APS and RAPS, compare macro vs. marginal coverage on long-tailed PlantNet-300k
  3. Swap base model to DeiT-B, rerun THR on ImageNet-V2 to observe coverage/inefficiency changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do newer CP methods developed for distribution shift (e.g., Amoukou & Brunel 2023, Barber et al. 2023) perform better on large-scale datasets like ImageNet?
- Basis in paper: [explicit] The authors note that several recent methods for distribution shift have been developed but "thus far been developed mostly on small-scale datasets, and it remains to be seen how they translate to the large-scale datasets studied here"
- Why unresolved: These methods haven't been empirically tested on large-scale datasets like ImageNet with modern architectures
- What evidence would resolve it: Empirical evaluation of these newer CP methods on ImageNet and other large-scale datasets with modern architectures (ResNets, ViTs, DeiT) measuring coverage and inefficiency under distribution shift

### Open Question 2
- Question: Can class-conditional coverage be improved for long-tailed distributions without requiring large per-class calibration sets?
- Basis in paper: [explicit] The authors show class-conditional coverage is frequently violated in long-tailed settings, and class-balanced CP doesn't resolve this due to small per-class calibration sets
- Why unresolved: Current methods fail when per-class data is limited, but the paper doesn't explore alternative approaches that might work with limited data
- What evidence would resolve it: Development and testing of CP methods that can achieve class-conditional coverage on long-tailed datasets with limited per-class examples, measured by reduced coverage violations

### Open Question 3
- Question: Is there a fundamental relationship between base model accuracy and CP method performance under distribution shift?
- Basis in paper: [inferred] The authors observe that coverage generally increases with model accuracy across distribution-shifted datasets, but the relationship varies by CP method
- Why unresolved: The paper shows correlation but doesn't establish whether this is causal or if certain CP methods benefit more from higher accuracy
- What evidence would resolve it: Controlled experiments varying base model accuracy while holding CP method constant, and vice versa, to isolate their individual effects on coverage and inefficiency

## Limitations
- No comparison with alternative uncertainty quantification methods (e.g., MC dropout, ensembles, evidential deep learning)
- Fixed conformity thresholds without exploring adaptive recalibration strategies
- Theoretical analysis of class-conditional coverage violations is limited

## Confidence
- High confidence: Marginal coverage violations under distribution shift, inefficiency increases, correlation between base model accuracy and CP performance
- Medium confidence: Class-conditional coverage violations in long-tailed data, comparative performance of different CP methods
- Low confidence: Long-term effectiveness of ConfTr's training-time optimization without class-conditional guarantees

## Next Checks
1. Conduct formal statistical tests (e.g., binomial tests) to determine if class-conditional coverage violations are statistically significant beyond sampling noise, particularly for rare classes in PlantNet-300k
2. Implement dynamic recalibration strategies that adjust thresholds based on detected distribution shift magnitude, then compare coverage and inefficiency against the fixed-threshold baseline
3. Benchmark conformal prediction against other uncertainty quantification approaches (e.g., MC dropout, ensembles, evidential deep learning) on the same distribution-shifted datasets to contextualize CP's performance relative to the broader uncertainty estimation landscape