---
ver: rpa2
title: A Forward and Backward Compatible Framework for Few-shot Class-incremental
  Pill Recognition
arxiv_id: '2304.11959'
source_url: https://arxiv.org/abs/2304.11959
tags:
- loss
- pill
- learning
- classes
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first few-shot class-incremental pill recognition
  framework, called Discriminative and Bidirectional Compatible Few-Shot Class-Incremental
  Learning (DBC-FSCIL). It addresses the challenge of recognizing new pill classes
  with only a few labeled samples while preserving knowledge of previously learned
  classes.
---

# A Forward and Backward Compatible Framework for Few-shot Class-incremental Pill Recognition

## Quick Facts
- arXiv ID: 2304.11959
- Source URL: https://arxiv.org/abs/2304.11959
- Reference count: 40
- Primary result: Proposes first few-shot class-incremental pill recognition framework achieving 87.29% average accuracy on FCPill and 71.54% on miniCURE

## Executive Summary
This paper introduces Discriminative and Bidirectional Compatible Few-Shot Class-Incremental Learning (DBC-FSCIL), the first framework addressing few-shot class-incremental pill recognition. The method tackles the challenge of recognizing new pill classes with only a few labeled samples while preserving knowledge of previously learned classes. By employing a decoupled learning strategy with a novel Center-Triplet loss and Graph Attention Network-based classifier adaptation, the framework achieves state-of-the-art performance on two pill recognition datasets.

## Method Summary
DBC-FSCIL uses a decoupled learning approach where feature extraction and classification are trained separately. The framework employs Center-Triplet (CT) loss for discriminative feature learning, strengthening intra-class compactness and inter-class separability by considering distances between class centers. A Graph Attention Network (GAT) updates classifiers for new classes while preserving knowledge of base classes through pseudo pill image generation. The pseudo incremental learning stage trains the GAT module using synthetically generated classes that simulate real incremental sessions.

## Key Results
- Achieves 87.29% average accuracy on FCPill dataset across all incremental sessions
- Achieves 71.54% average accuracy on miniCURE dataset
- Outperforms state-of-the-art methods on both datasets
- Demonstrates effective forward and backward compatibility in few-shot incremental learning scenario

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed Center-Triplet (CT) loss strengthens intra-class compactness and inter-class separability more effectively than Triplet loss or TC loss.
- Mechanism: CT loss considers both the distance between a sample and its class center and the distance between two class centers, constraining the former to be less than the latter by a margin. This direct consideration of inter-center distances enhances separability, while the sample-to-center term preserves compactness.
- Core assumption: Class centers computed on mini-batches are sufficiently stable to guide discriminative learning.
- Evidence anchors:
  - [abstract] "The CT loss strengthens intra-class compactness and inter-class separability by considering the distances between class centers."
  - [section] "CT loss seeks to update the backbone so that it can constrain the distance between the feature representation of xi and its corresponding class center ... less than the distance between cyi and another nearest class center cj, by a predefined margin value m."
  - [corpus] Weak: No direct corpus evidence comparing CT loss to other metric losses; must rely on paper's own ablation.

### Mechanism 2
- Claim: The GAT-based adaptation module enables backward compatibility by propagating contextual information between base and new class prototypes.
- Mechanism: Prototypes of base and new classes are treated as nodes in a graph; GAT uses attention coefficients to update each node based on weighted contributions from all others, preserving old knowledge while adapting to new classes.
- Core assumption: Graph attention propagation can effectively balance old and new class boundaries without explicit rehearsal.
- Evidence anchors:
  - [abstract] "For backward-compatible learning, we develop a strategy to synthesize reliable pseudo-features of old classes using uncertainty quantification, facilitating Data Replay (DR) and Knowledge Distillation (KD)."
  - [section] "The GAT adaptation module can model the classifier φI as the graph structure. The prototypes in φI can be seen as the nodes in the graph. The GAT adaptation module can utilize the relationships between different nodes and the self-attention mechanism to update them."
  - [corpus] Weak: No explicit corpus comparison of GAT-based classifier updating vs. other methods; only mentions CEC used GAT.

### Mechanism 3
- Claim: The pseudo incremental learning stage trains the GAT module to handle realistic incremental scenarios by simulating class addition with pseudo pill images.
- Mechanism: Pseudo pill classes are generated by random color/text transformations on base data, then combined with base support sets to train GAT under N-way K-shot conditions that mimic real incremental sessions.
- Core assumption: Randomly transformed base-class samples are sufficiently similar to real new classes to provide effective meta-training.
- Evidence anchors:
  - [abstract] "A pseudo pill image generation strategy is used to train the GAT for handling the incremental learning scenario."
  - [section] "we propose a novel strategy specifically for building pseudo pill image data by randomly adding imprint text on the pill surface and changing the color distribution."
  - [corpus] Weak: No external corpus evidence validating this pseudo data generation approach; relies on internal logic.

## Foundational Learning

- Concept: Decoupled representation and classifier learning
  - Why needed here: Prevents catastrophic forgetting by fixing the feature extractor while only updating classifiers for new classes.
  - Quick check question: What happens to performance if the backbone is also fine-tuned on new classes?

- Concept: Metric learning and triplet-based losses
  - Why needed here: Pill images have large intra-class and small inter-class variance; metric losses explicitly encourage compactness and separability.
  - Quick check question: How does Triplet loss differ from Softmax loss in encouraging feature separability?

- Concept: Graph attention networks for prototype updating
  - Why needed here: Allows dynamic adjustment of decision boundaries across old and new classes without storing old samples.
  - Quick check question: What is the role of attention coefficients in GAT when updating prototypes?

## Architecture Onboarding

- Component map: Backbone (ResNet-18/EfficientNet-B2) → CT loss training on base session → Fixed feature extractor → Pseudo incremental GAT training → Real incremental GAT updating → Classification
- Critical path: Backbone → CT loss → Fixed feature extractor → Pseudo incremental GAT training → Real incremental GAT updating → Classification
- Design tradeoffs:
  - Using CT loss vs. Softmax: better intra/inter-class separation but more complex loss computation
  - GAT-based updating vs. exemplar rehearsal: no memory overhead but depends on quality of pseudo data
  - Pseudo data realism vs. diversity: more realistic pseudo data may limit diversity
- Failure signatures:
  - Degraded accuracy on old classes: likely GAT update too aggressive
  - Overfitting on new classes: CT loss not strong enough or backbone too shallow
  - Training instability: noisy mini-batch centers in CT loss or poor attention weighting in GAT
- First 3 experiments:
  1. Ablation: Replace CT loss with Softmax loss, measure accuracy drop on FCPill
  2. GAT ablation: Remove GAT update, use simple concatenation of new prototypes, measure forgetting
  3. Pseudo data ablation: Use random noise instead of structured transformations, measure incremental learning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed Center-Triplet (CT) loss compare to other metric learning losses (e.g., Triplet loss, Center loss, Triplet-Center loss) in terms of intra-class compactness and inter-class separability?
- Basis in paper: [explicit] The paper states that the CT loss "can promote intra-class compactness and inter-class separability" and compares its performance to other metric learning losses in Table II.
- Why unresolved: The paper provides quantitative comparisons but does not provide a detailed qualitative analysis of how the CT loss achieves better intra-class compactness and inter-class separability compared to other losses.
- What evidence would resolve it: A detailed analysis of the feature representations learned by different losses, using visualization techniques such as t-SNE or UMAP, would provide insights into how the CT loss achieves better intra-class compactness and inter-class separability.

### Open Question 2
- Question: How does the proposed pseudo pill image generation strategy contribute to the effectiveness of the GAT adaptation module in handling the incremental learning scenario?
- Basis in paper: [explicit] The paper states that the pseudo pill image generation strategy is used to "train the Graph Attention Network (GAT) for updating classifiers" and "facilitate Data Replay (DR) and Knowledge Distillation (KD)."
- Why unresolved: The paper provides a high-level description of the pseudo pill image generation strategy but does not provide a detailed analysis of how it contributes to the effectiveness of the GAT adaptation module.
- What evidence would resolve it: An ablation study comparing the performance of the framework with and without the pseudo pill image generation strategy would provide insights into its contribution to the effectiveness of the GAT adaptation module.

### Open Question 3
- Question: How does the proposed framework handle the challenge of large intra-class variations and small inter-class variations in pill images?
- Basis in paper: [explicit] The paper mentions that "pill images face the challenge of large intra-class and small inter-class variations" and that the proposed CT loss aims to address this challenge.
- Why unresolved: The paper provides a high-level description of how the CT loss addresses the challenge but does not provide a detailed analysis of how it handles large intra-class variations and small inter-class variations.
- What evidence would resolve it: An analysis of the feature representations learned by the CT loss, using techniques such as distance metrics or clustering algorithms, would provide insights into how it handles large intra-class variations and small inter-class variations.

## Limitations

- The effectiveness of pseudo pill image generation relies on assumptions about the similarity between transformed base samples and real new classes, with no external validation provided
- CT loss stability depends on mini-batch class centers being sufficiently stable, but no evidence is provided regarding their reliability across batches
- GAT update dynamics are not fully characterized, with no reporting on how attention weights evolve or correlate with forgetting/retention of old classes

## Confidence

- **High confidence**: The decoupled learning strategy (fixed backbone, incremental classifier update) is well-grounded in the literature and aligns with established practices in class-incremental learning
- **Medium confidence**: The reported accuracy improvements over state-of-the-art methods are promising, but are based on only two datasets
- **Low confidence**: The CT loss and GAT adaptation mechanisms lack direct corpus evidence; their advantages are demonstrated only through internal ablation studies

## Next Checks

1. **CT loss ablation**: Replace CT loss with standard Softmax loss and measure accuracy drop on both FCPill and miniCURE. This will isolate the contribution of the CT loss to overall performance.

2. **GAT update ablation**: Remove the GAT-based classifier updating and use a simple concatenation of new prototypes. Measure forgetting on base classes to assess the importance of the GAT update for backward compatibility.

3. **Pseudo data realism check**: Generate pseudo pill images with random noise instead of structured transformations. Evaluate incremental learning performance to determine if the realism of pseudo data matters for GAT training.