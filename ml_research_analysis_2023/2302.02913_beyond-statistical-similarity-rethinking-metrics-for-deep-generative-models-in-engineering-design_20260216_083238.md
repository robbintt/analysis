---
ver: rpa2
title: 'Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models
  in Engineering Design'
arxiv_id: '2302.02913'
source_url: https://arxiv.org/abs/2302.02913
tags:
- design
- metrics
- generated
- designs
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews and provides a practical guide to evaluation
  metrics for deep generative models (DGMs) in engineering design. It critiques traditional
  statistical similarity metrics, which are often overused but don't capture the full
  requirements of engineering applications.
---

# Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design

## Quick Facts
- arXiv ID: 2302.02913
- Source URL: https://arxiv.org/abs/2302.02913
- Reference count: 40
- Key outcome: Critique of statistical similarity metrics and practical guide to design-specific evaluation metrics for deep generative models in engineering design.

## Executive Summary
This paper addresses the limitations of traditional statistical similarity metrics for evaluating deep generative models (DGMs) in engineering design. The authors argue that metrics like MMD and KL divergence, while useful for image and language applications, fail to capture essential design requirements such as constraint satisfaction, functional performance, novelty, and conditioning. They propose a comprehensive framework of design-specific evaluation metrics and demonstrate their application through experiments on 2D synthetic problems and a bicycle frame design dataset, comparing vanilla GANs with performance-aware models like MO-PaDGAN and DTAI-GAN.

## Method Summary
The paper curates a set of design-specific evaluation metrics focusing on constraint satisfaction, functional performance, novelty, and conditioning. The authors implement four DGM variants (vanilla GAN, MO-PaDGAN, DTAI-GAN, and conditional VAE) and evaluate them on the FRAMED bicycle frame dataset containing 4000 valid designs parameterized over 37 parameters with 10 structural performance values each. The evaluation framework includes statistical similarity metrics (F1, nearest datapoint, nearest generated sample), design exploration metrics (inter-sample distance, DPP diversity), constraint satisfaction metrics, performance evaluation metrics (hypervolume, target achievement rate), and conditioning metrics (conditioning reconstruction).

## Key Results
- Performance-aware models (MO-PaDGAN and DTAI-GAN) achieve better performance and constraint satisfaction than vanilla GANs
- Higher functional performance comes at the expense of statistical similarity to the dataset
- Novelty and diversity metrics reveal important differences between models not captured by statistical similarity alone
- Conditioning adherence varies significantly between conditional model architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Statistical similarity metrics fail in design because they conflate distribution matching with functional performance, missing constraints and novelty.
- **Mechanism**: Statistical distance metrics measure global distribution overlap but ignore local performance variance; two designs can be statistically close yet have drastically different performance.
- **Core assumption**: Design performance is a non-linear mapping from design space that can vary widely even among statistically similar samples.
- **Evidence anchors**:
  - [abstract] "traditional statistical metrics based on likelihood may not fully capture the requirements of engineering applications."
  - [section] "Two very similar bike frames adapted from [12] with drastically different structural performance."
- **Break condition**: If performance space is convex and smooth, statistical similarity may correlate with performance.

### Mechanism 2
- **Claim**: Design exploration metrics are needed because generating unique, varied designs is often more valuable than mimicking the dataset.
- **Mechanism**: Novelty metrics push generated samples away from both the dataset and each other, encouraging exploration of under-represented regions.
- **Core assumption**: The dataset represents only a subset of the feasible design space, and novel designs may lie outside its convex hull.
- **Evidence anchors**:
  - [section] "designers must decide how to trade off similarity against other design objectives."
  - [section] "we want generated designs to resemble existing designs, we would like them to have different performance values."
- **Break condition**: If the dataset already spans the entire feasible space, novelty metrics may encourage infeasible or poor-performing designs.

### Mechanism 3
- **Claim**: Conditioning metrics are essential for conditional generative models because they ensure the model respects target attributes.
- **Mechanism**: Conditioning adherence metrics quantify how closely generated samples match specified target conditions.
- **Core assumption**: The conditioning information is meaningful and can be directly evaluated or predicted for generated samples.
- **Evidence anchors**:
  - [abstract] "conditioning, the idea that conditional generative models should respect and adhere to their conditioning information."
  - [section] "Conditional DGMs are often more robust and data efficient than training many individual DGMs."
- **Break condition**: If conditioning information is ambiguous or cannot be evaluated, adherence metrics become meaningless.

## Foundational Learning

- **Concept**: Distance calculation in high-dimensional design spaces
  - **Why needed here**: Many metrics rely on measuring distances between designs, but design representations require different distance metrics.
  - **Quick check question**: Can you compute the Euclidean distance between two bicycle frame designs represented as 37-parameter vectors?

- **Concept**: Multi-objective optimization and Pareto optimality
  - **Why needed here**: Performance evaluation often involves multiple competing objectives, requiring metrics like hypervolume and generational distance.
  - **Quick check question**: Given two designs with performance (0.8, 0.6) and (0.7, 0.9), which one dominates the other?

- **Concept**: Conditional generative models and conditioning adherence
  - **Why needed here**: Many design problems require generating designs with specific attributes.
  - **Quick check question**: If a model generates designs conditioned on a target weight of 1 ton, how would you measure its adherence to this condition?

## Architecture Onboarding

- **Component map**: Dataset preprocessing -> Model training (GAN, VAE, conditional variants) -> Metric calculation modules -> Visualization and analysis tools
- **Critical path**: 1. Load and preprocess dataset, 2. Train generative model(s), 3. Generate samples from trained models, 4. Calculate selected evaluation metrics, 5. Analyze and visualize results
- **Design tradeoffs**: Using learned embeddings for distance calculation vs. direct distance in original space, evaluating metrics in design space vs. performance space, comparing conditional posteriors to conditional vs. marginal priors
- **Failure signatures**: High statistical similarity but poor constraint satisfaction or performance, low diversity metrics but high novelty metrics (overfitting), conditioning metrics show poor adherence despite visually plausible designs
- **First 3 experiments**: 1. Train a VAE and GAN on a simple 2D synthetic dataset and compare statistical similarity metrics (F1, MMD), 2. Evaluate the same models on constraint satisfaction using a synthetic 2D problem with a non-convex feasible region, 3. Test a conditional VAE and cGAN on a continuous conditioning problem, comparing conditioning adherence metrics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do learned embeddings from pre-trained image classifiers (e.g., CLIP, ImageNet) generalize when applied to engineering design data such as structural topologies or mechanical components?
- **Basis in paper**: [explicit] The paper discusses the potential pitfalls of using domain-specific metrics like FID and IS, which rely on pre-trained classifiers, for design problems. It highlights that these metrics are biased towards ImageNet and may not contain useful information about structural topologies.
- **Why unresolved**: The paper notes that the generalizability of learned embeddings to design domains is uncertain and requires further research.
- **What evidence would resolve it**: Systematic evaluation of learned embeddings on diverse engineering design datasets, comparing their performance to domain-agnostic metrics.

### Open Question 2
- **Question**: What is the optimal trade-off between statistical similarity and functional performance when evaluating deep generative models in design problems?
- **Basis in paper**: [inferred] The paper emphasizes the limitations of statistical similarity metrics and the need to consider other factors like performance, constraints, and diversity. It suggests that higher functional performance often comes at the expense of statistical similarity.
- **Why unresolved**: The paper does not provide a definitive answer on how to balance these competing objectives, as it depends on the specific design problem and goals.
- **What evidence would resolve it**: Empirical studies comparing the performance of models optimized for different combinations of statistical similarity and functional performance metrics across various design tasks.

### Open Question 3
- **Question**: How can evaluation metrics for deep generative models in design be adapted to handle multimodal data representations?
- **Basis in paper**: [explicit] The paper discusses the challenges of calculating distances between designs represented using multimodal data and mentions shared embeddings as a potential approach.
- **Why unresolved**: The paper does not provide a comprehensive solution for evaluating multimodal data and highlights the need for further research in this area.
- **What evidence would resolve it**: Development and validation of evaluation metrics specifically designed for multimodal data representations in design problems, along with benchmarks comparing their performance to existing metrics.

## Limitations
- Focuses on a limited set of metrics and models without exploring other relevant evaluation criteria or alternative DGM architectures
- Experiments conducted on a single real-world dataset and synthetic datasets, limiting generalizability
- Does not address computational cost and scalability for large-scale engineering design problems

## Confidence
- **High Confidence**: The critique of statistical similarity metrics and the need for design-specific evaluation criteria
- **Medium Confidence**: The comparative analysis of different DGMs using the proposed metrics, as experiments are limited to a specific dataset and model variants
- **Low Confidence**: The broader implications and generalizability of findings to other engineering design domains and DGM architectures

## Next Checks
1. **Metric Validation**: Test the proposed metrics on additional engineering design datasets and problems to assess their effectiveness and robustness across different domains
2. **Model Comparison**: Evaluate a wider range of DGM architectures (e.g., diffusion models, autoregressive models) using the proposed metrics to determine their relative strengths and weaknesses
3. **Scalability Analysis**: Investigate the computational cost and scalability of the proposed metrics and models for large-scale engineering design problems with high-dimensional design spaces and complex constraints