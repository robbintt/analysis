---
ver: rpa2
title: Improving Interpersonal Communication by Simulating Audiences with Language
  Models
arxiv_id: '2311.00687'
source_url: https://arxiv.org/abs/2311.00687
tags:
- advice
- scenario
- candidates
- scenarios
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework that uses LLMs to simulate audience
  reactions and generate effective communication messages. The Explore-Generate-Simulate
  (EGS) framework explores possible strategies, generates candidates, and simulates
  audience feedback to select the best message.
---

# Improving Interpersonal Communication by Simulating Audiences with Language Models

## Quick Facts
- arXiv ID: 2311.00687
- Source URL: https://arxiv.org/abs/2311.00687
- Authors: 
- Reference count: 30
- Primary result: EGS framework preferred by humans in 5/8 communication scenarios, outperforms baselines

## Executive Summary
This paper introduces the Explore-Generate-Simulate (EGS) framework that leverages large language models to improve interpersonal communication effectiveness. The framework explores diverse communication strategies, generates candidates conditioned on subsets of advice, and simulates audience reactions to select optimal messages. Tested across eight interpersonal communication scenarios, EGS demonstrates superior performance compared to zero-shot and Chain-of-Thought baselines, with human-preferred candidates in 5 out of 8 scenarios and strong agreement between simulated and actual human preferences.

## Method Summary
EGS operates through three sequential steps: exploration generates diverse advice (both normal and unorthodox), generation creates communication candidates by conditioning on advice subsets, and simulation evaluates candidates through audience perspective-taking using pairwise comparisons. The framework is evaluated on eight interpersonal communication scenarios spanning fundamental communication processes, with human ratings collected for candidates and baselines. Performance is measured by human preference rates and agreement between simulated and human evaluations.

## Key Results
- EGS candidates preferred by humans over baselines in 5/8 scenarios
- Simulated audience ratings agree with human ratings in 5/8 scenarios
- 70-80% accuracy predicting human upvotes on online forum data
- Outperforms GPT-4 zero-shot in all scenarios and Chain-of-Thought in five scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EGS leverages LLM's capacity to simulate diverse audience perspectives, offloading cognitively demanding reasoning about reactions.
- Mechanism: EGS generates multiple audience profiles and uses pairwise comparisons to evaluate communication candidates, reducing cognitive load and expanding explored solution space.
- Core assumption: LLMs can generate realistic audience simulations that align with human preferences when prompted with specific role descriptions and comparison questions.
- Evidence anchors: Framework description showing audience simulation steps; 5/8 agreement rate between simulated and human preferences.
- Break condition: If LLM-generated audience simulations significantly diverge from actual human preferences across multiple scenarios.

### Mechanism 2
- Claim: EGS improves communication effectiveness by expanding the search space through both normal and unorthodox advice generation.
- Mechanism: EGS prompts LLM to generate diverse advice types, then creates candidates conditioned on combinatorial subsets of this advice, allowing exploration of non-obvious communication strategies.
- Core assumption: LLMs trained on diverse human experiences can generate creative communication strategies that humans might not consider.
- Evidence anchors: Framework description of Explore step generating normal and unorthodox advice; candidate generation from advice subsets.
- Break condition: If combinatorial generation leads to incoherent or contextually inappropriate messages.

### Mechanism 3
- Claim: EGS achieves better alignment with human preferences than zero-shot or Chain-of-Thought approaches through its structured exploration and simulation process.
- Mechanism: By generating diverse candidates and using audience simulations for evaluation, EGS selects messages that outperform baselines in human ratings across multiple scenarios.
- Core assumption: Structured exploration combined with simulated audience feedback leads to more effective messages than direct prompting approaches.
- Evidence anchors: Human evaluation results showing EGS outperforming baselines; 5/8 scenarios with human-preferred EGS candidates.
- Break condition: If structured EGS approach fails to consistently outperform simpler baselines.

## Foundational Learning

- Concept: Interpersonal communication processes
  - Why needed here: Understanding the ten fundamental processes helps in constructing relevant scenarios and evaluating communication effectiveness across different contexts.
  - Quick check question: What are the key differences between impression management and relationship development in interpersonal communication?

- Concept: LLM agent simulation capabilities
  - Why needed here: Understanding how LLMs can simulate human perspectives and preferences is crucial for implementing the audience simulation component effectively.
  - Quick check question: How do LLMs generate consistent character perspectives when asked to simulate different audience types?

- Concept: Pairwise comparison methodology
  - Why needed here: Pairwise comparisons are used to evaluate candidates when absolute ratings lack granularity, making understanding this methodology essential.
  - Quick check question: Why might pairwise comparisons be more effective than absolute ratings for evaluating communication candidates?

## Architecture Onboarding

- Component map: Explore -> Generate -> Simulate -> Aggregate
- Critical path: Input scenario → Generate advice (Explore) → Create candidates (Generate) → Generate audience profiles (Simulate) → Perform pairwise comparisons → Aggregate results to select best candidate
- Design tradeoffs: Exploration vs. exploitation tradeoff between diverse advice exploration and computational cost; number of advice pieces affecting combination coherence; audience generation depth balancing realism and complexity
- Failure signatures: Low agreement between LLM and human ratings indicating simulation accuracy issues; inconsistent pairwise comparison results suggesting generation instability; overly generic or inappropriate candidates indicating exploration problems
- First 3 experiments: Test Explore step with different numbers of normal vs. unorthodox advice pieces; compare Generate step performance with single vs. multiple advice conditioning; evaluate Simulate step agreement across different audience types and comparison formats

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Explore-Generate-Simulate framework be extended to handle multi-turn conversations effectively?
- Basis in paper: The paper discusses extending the framework to simulate multi-turn dialogues but notes that this could lead to scaling issues due to the exponential growth in the number of potential candidates.
- Why unresolved: The paper acknowledges the potential of extending the framework but does not provide a concrete solution to the scaling problem.
- What evidence would resolve it: Implementing a multi-turn extension with beam search or another pruning strategy and evaluating its effectiveness compared to single-turn scenarios.

### Open Question 2
- Question: What is the optimal level of detail for generating stakeholder audiences in the Simulate step?
- Basis in paper: The paper notes that using more detailed or personalized audience profiles might improve performance but could also make the described scenario less common and harder for the LLM to reason about.
- Why unresolved: The paper does not experimentally determine the impact of varying the granularity of audience details on the accuracy of the framework.
- What evidence would resolve it: Conducting experiments with different levels of detail in audience profiles and measuring their impact on the agreement between simulated and human ratings.

### Open Question 3
- Question: How can the Explore-Generate-Simulate framework be made more accessible and less costly for individuals with limited resources?
- Basis in paper: The paper acknowledges that individuals may have unequal access to the method due to information barriers or financial costs, such as API access fees.
- Why unresolved: The paper does not propose specific solutions to reduce costs or increase accessibility.
- What evidence would resolve it: Developing a lightweight version of the framework that can run on consumer hardware or finding ways to reduce the number of API calls needed without sacrificing performance.

## Limitations

- Framework's effectiveness depends heavily on quality of LLM-generated audience simulations, with only 5/8 agreement rate between simulated and human preferences
- Evaluation scenarios represent limited set of interpersonal communication contexts, leaving complex or culturally specific scenarios untested
- Computational cost of exploring multiple advice combinations and generating pairwise comparisons may limit practical deployment

## Confidence

- High confidence in the core mechanism: Well-specified framework demonstrates clear advantages over simpler baselines
- Medium confidence in generalizability: Promising results across 8 scenarios but broader validation needed
- Medium confidence in audience simulation accuracy: 5/8 agreement rate indicates reasonable alignment but reveals limitations

## Next Checks

1. Test framework's performance on emotionally complex scenarios (delivering bad news, conflict resolution) to assess robustness in high-stakes communication contexts
2. Conduct cross-cultural validation by testing framework with scenarios and audience profiles from different cultural backgrounds to evaluate cultural adaptation capabilities
3. Measure impact of varying number of advice pieces and audience profiles on both performance and computational efficiency to identify optimal parameter settings