---
ver: rpa2
title: Investigating the Factual Knowledge Boundary of Large Language Models with
  Retrieval Augmentation
arxiv_id: '2307.11019'
source_url: https://arxiv.org/abs/2307.11019
tags:
- llms
- documents
- retrieval
- supporting
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models perceive their
  factual knowledge boundaries under retrieval augmentation for open-domain QA. It
  focuses on three research questions: LLMs'' ability to perceive knowledge boundaries,
  the effect of retrieval augmentation, and how different supporting document characteristics
  affect LLMs.'
---

# Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation

## Quick Facts
- **arXiv ID**: 2307.11019
- **Source URL**: https://arxiv.org/abs/2307.11019
- **Reference count**: 12
- **Primary result**: LLMs exhibit overconfidence in knowledge boundaries, but retrieval augmentation improves both QA performance and boundary perception accuracy

## Executive Summary
This paper investigates how large language models perceive their factual knowledge boundaries under retrieval augmentation for open-domain question answering. The authors find that LLMs are systematically overconfident about their knowledge capabilities, often producing incorrect answers with high confidence. Retrieval augmentation significantly improves both the accuracy of answers and the LLMs' ability to accurately judge whether they can answer questions correctly. The study demonstrates that supporting document quality and relevance are critical factors in LLM performance, with high-quality documents improving confidence and reliance while low-quality documents lead to uncertainty.

## Method Summary
The study employs a multi-pronged experimental approach using three open-domain QA datasets (NQ, TriviaQA, and HotpotQA) and multiple retrieval sources (dense retrieval with RocketQAv2, sparse retrieval with BM25, and ChatGPT-generated documents). The methodology involves two prompting strategies: QA prompting for generating answers and judgemental prompting for assessing knowledge boundaries (priori before answering, posteriori after answering). The experiments compare LLM performance with and without retrieval augmentation, using various document quality levels and sampling strategies. Metrics include EM and F1 scores for QA performance, along with judgement evaluation metrics like give-up rate, Right/G, Right/¬G, Eval-Right, and Eval-Acc.

## Key Results
- LLMs exhibit systematic overconfidence in their knowledge boundaries, struggling to accurately assess what they know
- Retrieval augmentation improves both QA performance (EM/F1 scores) and the accuracy of LLMs' knowledge boundary judgments
- The quality and relevance of supporting documents significantly impact LLM performance and confidence, with high-quality documents improving both accuracy and reliance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs cannot accurately perceive their factual knowledge boundaries, leading to overconfidence.
- Mechanism: The LLM's internal knowledge representation lacks self-assessment signals, so it defaults to high confidence regardless of actual knowledge state. When answering, it generates responses without checking for knowledge gaps, resulting in confident but potentially incorrect answers.
- Core assumption: The LLM's parametric knowledge is static during inference and doesn't include meta-knowledge about its own knowledge boundaries.
- Evidence anchors:
  - [abstract]: "LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses."
  - [section]: "LLMs’ perception of the factual knowledge boundary is inaccurate and they often display a tendency towards being overconfident."
  - [corpus]: Weak evidence; neighboring papers focus on related topics but don't directly support this mechanism.
- Break condition: If the LLM receives explicit training on knowledge boundary detection or is augmented with retrieval that provides clear knowledge gaps, this overconfidence behavior may be reduced.

### Mechanism 2
- Claim: Retrieval augmentation improves LLMs' ability to perceive knowledge boundaries by providing external validation signals.
- Mechanism: Supporting documents from retrieval systems act as external knowledge checkpoints. When the LLM can compare its internal knowledge with retrieved evidence, it becomes more aware of gaps and inconsistencies, leading to more accurate self-assessment.
- Core assumption: The LLM can effectively process and integrate supporting documents when prompted appropriately, using them to validate or invalidate its internal knowledge.
- Evidence anchors:
  - [abstract]: "retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities."
  - [section]: "retrieval augmentation can also improve the accuracy of LLMs' posterior judgement."
  - [corpus]: Moderate evidence; neighboring papers discuss retrieval augmentation effects but don't directly address knowledge boundary perception.
- Break condition: If retrieval results are consistently poor quality or irrelevant, the external validation signal becomes noisy, potentially worsening the LLM's self-assessment.

### Mechanism 3
- Claim: The quality and relevance of supporting documents directly influence LLM confidence and reliance on external knowledge.
- Mechanism: High-quality, relevant documents reinforce the LLM's confidence by providing strong external validation, while low-quality or irrelevant documents create uncertainty that the LLM tries to resolve by relying more heavily on its internal knowledge or giving up.
- Core assumption: LLMs treat supporting documents as authoritative sources when prompted to use them, adjusting their confidence based on perceived document quality.
- Evidence anchors:
  - [abstract]: "LLMs exhibit improved performance and confidence when presented with high-quality supporting documents and tend to rely on the provided supporting documents to produce the responses."
  - [section]: "LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance."
  - [corpus]: Weak evidence; corpus neighbors discuss related retrieval topics but don't specifically address document quality effects on LLM confidence.
- Break condition: If the LLM is explicitly instructed to prioritize its internal knowledge over external documents, or if document quality signals are removed from the prompt, this reliance mechanism may break down.

## Foundational Learning

- Concept: Knowledge boundary perception in LLMs
  - Why needed here: Understanding how LLMs detect gaps in their knowledge is central to this paper's research questions about factual knowledge boundaries.
  - Quick check question: What is the difference between priori and posteriori judgement in the context of LLM knowledge assessment?

- Concept: Retrieval augmentation effectiveness
  - Why needed here: The paper extensively investigates how external document retrieval affects LLM performance and knowledge boundary awareness.
  - Quick check question: How does retrieval augmentation improve LLM performance compared to using internal knowledge alone?

- Concept: Document relevance and quality assessment
  - Why needed here: The paper finds that supporting document characteristics significantly impact LLM behavior, requiring understanding of relevance metrics and quality assessment.
  - Quick check question: What factors determine whether supporting documents will improve or harm LLM performance?

## Architecture Onboarding

- Component map: Question → Retriever → Supporting documents → LLM with prompts → Answer/Judgement
- Critical path: The retrieval step is critical as it provides the external knowledge that enables boundary perception
- Design tradeoffs: Using more supporting documents improves performance but increases token usage and latency. Different retrievers offer trade-offs between relevance and computational cost.
- Failure signatures: Overconfident answers with low accuracy indicate poor boundary perception. Performance degradation when using certain document types suggests quality/relevance issues.
- First 3 experiments:
  1. Compare LLM performance with no retrieval vs. sparse vs. dense vs. ChatGPT-generated supporting documents to establish baseline effectiveness.
  2. Test priori judgement accuracy by having the LLM assess whether it can answer questions before attempting to answer them.
  3. Vary the number of supporting documents (1-20) to find the optimal balance between performance improvement and resource usage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the perception of factual knowledge boundaries vary across different LLM architectures and training paradigms?
- Basis in paper: Explicit. The paper focuses on GPT series LLMs and notes that conclusions apply to this series.
- Why unresolved: The study only examines GPT series models (Davinci003 and ChatGPT). Other architectures (e.g., BERT, RoBERTa, LLaMA) and training approaches (e.g., instruction-tuning, reinforcement learning from human feedback) may exhibit different boundary perception capabilities.
- What evidence would resolve it: Systematic experiments comparing multiple LLM architectures and training paradigms on the same knowledge boundary tasks, including both proprietary and open-source models.

### Open Question 2
- Question: What is the relationship between LLMs' confidence calibration and their ability to accurately perceive knowledge boundaries?
- Basis in paper: Explicit. The paper observes that LLMs exhibit "unwavering confidence" and "overconfidence" in their knowledge boundaries.
- Why unresolved: The paper demonstrates a correlation between confidence and boundary perception but doesn't establish causation or explore the underlying mechanisms. It's unclear whether improving confidence calibration would enhance boundary perception.
- What evidence would resolve it: Experiments manipulating confidence calibration (e.g., temperature scaling, calibration techniques) and measuring the impact on knowledge boundary perception accuracy across multiple tasks.

### Open Question 3
- Question: How do knowledge boundaries in LLMs evolve over time with continued pretraining or fine-tuning?
- Basis in paper: Explicit. The paper notes that LLMs "cannot sufficiently utilize the knowledge they possess" despite having learned from "existing corpora including Wikipedia."
- Why unresolved: The study uses static models without examining how boundary perception changes with additional training. It's unclear whether boundary perception improves, deteriorates, or remains constant as models are exposed to more data.
- What evidence would resolve it: Longitudinal studies tracking knowledge boundary perception across different stages of model development, comparing early, mid, and late-stage checkpoints of the same model architecture.

## Limitations
- The study only examines GPT series models, limiting generalizability across different LLM architectures
- Reliance on Wikipedia-based datasets may not reflect real-world scenarios with specialized or domain-specific knowledge
- Experiments are conducted only on English datasets, leaving cross-linguistic generalizability questions open

## Confidence
- **High confidence**: Retrieval augmentation improves both QA performance and boundary perception accuracy
- **Medium confidence**: LLMs exhibit systematic overconfidence in knowledge boundaries
- **Medium confidence**: Document quality directly influences LLM reliance and confidence

## Next Checks
1. **Cross-architecture validation**: Test the same experimental setup with open-source LLMs (Llama, Mistral) to determine if overconfidence patterns persist across different model families and training paradigms.
2. **Domain transfer testing**: Apply the methodology to specialized domains (medical, legal, technical) to assess whether knowledge boundary perception generalizes beyond general knowledge datasets.
3. **Multi-lingual extension**: Replicate key experiments using non-English datasets to evaluate whether retrieval augmentation's benefits on boundary perception hold across different languages and cultural knowledge bases.