---
ver: rpa2
title: Curriculum Learning for Cooperation in Multi-Agent Reinforcement Learning
arxiv_id: '2312.11768'
source_url: https://arxiv.org/abs/2312.11768
tags:
- teammate
- agent
- learning
- curriculum
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates curriculum learning for cooperative multi-agent
  reinforcement learning, focusing on how different teammate skill levels affect learning
  and team performance. The authors frame the problem as selecting appropriate teammates
  for a novice student agent to optimize both team reward and the student's learning.
---

# Curriculum Learning for Cooperation in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.11768
- Source URL: https://arxiv.org/abs/2312.11768
- Reference count: 6
- Pre-trained teammate with frozen weights reduces non-stationarity and improves convergence

## Executive Summary
This paper investigates curriculum learning for cooperative multi-agent reinforcement learning, focusing on how different teammate skill levels affect learning and team performance. Using the Overcooked environment, the authors train student agents with pre-trained teammates of varying skill levels and with curricula of increasing or decreasing skill levels. The key finding is that a decreasing curriculum (highly-skilled → medium-skilled → low-skilled) outperforms both the increasing curriculum and training with a single pre-trained teammate in terms of both team reward and student learning, highlighting complex trade-offs in designing curricula for cooperative multi-agent settings.

## Method Summary
The method involves training student agents in the Overcooked environment with different types of pre-trained teammates (low-skilled, medium-skilled, highly-skilled) and curricula (increasing, decreasing skill levels). Teammates are created by training two agents from scratch using Independent DQN and saving snapshots at 2K, 5K, and 10K episodes. The student agent is then trained with these frozen-weight teammates while tracking individual and team rewards. Three main experiments test: (1) training with individual pre-trained teammates, (2) training with increasing curriculum, and (3) training with decreasing curriculum.

## Key Results
- A low-skilled pre-trained teammate maximizes total team reward but provides no learning benefit to the student
- A medium-skilled teammate offers the best balance of high team reward and meaningful learning for the student
- Surprisingly, a decreasing curriculum (highly-skilled → medium-skilled → low-skilled) outperforms both the increasing curriculum and training with a single pre-trained teammate in terms of both team reward and student learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decreasing skill level curriculum outperforms increasing skill level curriculum because the low-skilled teammate forces the student to actively engage in the task, leading to higher learning efficiency in later stages.
- Mechanism: The low-skilled teammate cannot complete the task alone, requiring the student to take initiative. This active engagement builds stronger task skills. When later paired with medium and high-skilled teammates, the student can better coordinate and leverage their teammates' abilities.
- Core assumption: The student agent can learn effectively from a low-skilled teammate despite lower initial team performance.
- Evidence anchors:
  - [abstract] "Surprisingly, a decreasing curriculum (highly-skilled → medium-skilled → low-skilled) outperforms both the increasing curriculum and training with a single pre-trained teammate in terms of both team reward and student learning."
  - [section] "In the decreasing curriculum, the teammate after 2/3rd of the training is the low-skilled agent which might explain the higher reward."
- Break condition: If the student agent cannot learn from a low-skilled teammate (e.g., if the task is too complex for the student to handle alone), the decreasing curriculum would fail.

### Mechanism 2
- Claim: Medium-skilled teammates provide optimal balance between team reward and student learning because they are skilled enough to complete tasks but not so skilled that the student becomes passive.
- Mechanism: Medium-skilled teammates can handle most of the task but occasionally need help, creating opportunities for the student to learn through collaboration. They maintain reasonable team performance while ensuring the student remains engaged.
- Core assumption: The medium-skilled teammate represents an optimal skill level that balances task completion with student engagement.
- Evidence anchors:
  - [section] "The teammate that leads to the highest student reward while also achieving a high total reward at convergence is the medium-skilled teammate."
  - [section] "A medium-skilled teammate offers the best balance of high team reward and meaningful learning for the student."
- Break condition: If the task requires very specific coordination patterns that only high-skilled teammates can provide, the medium-skilled teammate might not prepare the student adequately.

### Mechanism 3
- Claim: Pre-trained teammates with frozen weights reduce non-stationarity in the environment, leading to faster convergence compared to training with another learning agent.
- Mechanism: When the teammate's policy is fixed, the environment dynamics become more predictable for the student. This stability allows the student to learn more efficiently without constantly adapting to a changing teammate policy.
- Core assumption: Environmental non-stationarity from having two learning agents simultaneously significantly impedes learning efficiency.
- Evidence anchors:
  - [section] "We observe that training with a pre-trained teammate is always better than training with a new agent from scratch for the total reward."
  - [section] "This result is not surprising as it might be a result of the fact that any pre-trained teammate with non-updating weights in the neural network reduces the non-stationarity of the environment and hence leads to faster convergence to the higher reward."
- Break condition: If the learning algorithm is specifically designed to handle non-stationarity (e.g., using advanced opponent modeling), the benefit of frozen-weight teammates might diminish.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Multi-agent MDPs (MMDPs)
  - Why needed here: The paper frames the cooperative multi-agent problem within the MMDP framework, where states are shared, actions are joint, and rewards can be individual or collective.
  - Quick check question: What is the key difference between a standard MDP and an MMDP in terms of state and action spaces?

- Concept: Curriculum Learning
  - Why needed here: The core contribution involves designing curricula of teammates with different skill levels to optimize both learning and performance, which is a curriculum learning approach applied to multi-agent settings.
  - Quick check question: How does curriculum learning differ from standard reinforcement learning in terms of the training data distribution?

- Concept: Independent DQN and non-stationarity in multi-agent settings
  - Why needed here: The student agents are trained using Independent DQN, and the paper explicitly discusses how non-stationarity affects learning when both agents are learning simultaneously versus when one agent's weights are frozen.
  - Quick check question: What is the primary source of non-stationarity in Independent DQN settings, and how does it affect learning stability?

## Architecture Onboarding

- Component map: Population generator -> Training orchestrator -> Evaluation module -> Environment wrapper
- Critical path: 1. Generate population of teammate snapshots during initial training phase, 2. Initialize student agent with random weights, 3. Select pairing strategy (single teammate or curriculum), 4. Execute training episodes while tracking rewards, 5. Evaluate performance across multiple seeds
- Design tradeoffs: Fixed vs. learning teammates (fixed weights reduce non-stationarity but may limit adaptability), curriculum length (longer curricula provide more gradual learning but increase training time), snapshot frequency (more frequent snapshots create finer-grained skill levels but increase population size)
- Failure signatures: Student receives near-zero reward consistently (over-reliance on teammate), high variance across seeds (insufficient training stability), poor team reward convergence (incompatible teammate-student pairing)
- First 3 experiments: 1. Train two agents from scratch using Independent DQN for 10,000 episodes, capturing snapshots at 2K, 5K, and 10K episodes, 2. Test student agent with each individual pre-trained teammate (low, medium, high skill) for 10,000 episodes each, 3. Test student agent with decreasing curriculum (high→medium→low) and increasing curriculum (low→medium→high) for 10,000 episodes each

## Open Questions the Paper Calls Out
### Open Question 1
The paper does not explicitly call out specific open questions, but the surprising finding that decreasing curriculum outperforms increasing curriculum raises questions about: (1) the generalizability of this finding to other cooperative tasks and environments, (2) the optimal skill progression patterns for different types of multi-agent tasks, and (3) how to automatically determine the best curriculum structure without extensive manual tuning.

## Limitations
- The findings are based on a specific cooperative task (Overcooked) and may not generalize to other multi-agent environments or competitive settings
- The skill levels of teammates are discrete (low, medium, high) based on training snapshots, which may not capture the full spectrum of skill progression
- The study uses Independent DQN, and results may differ with other multi-agent RL algorithms that handle non-stationarity differently

## Confidence
- High confidence: The observation that frozen-weight pre-trained teammates reduce non-stationarity and improve convergence compared to training with another learning agent.
- Medium confidence: The specific finding that decreasing curriculum outperforms increasing curriculum, as this may be sensitive to the particular skill progression implemented.
- Medium confidence: The claim about medium-skilled teammates providing optimal balance, as the definition of "medium skill" is relative to the specific training snapshots used.

## Next Checks
1. Cross-environment validation: Test the decreasing curriculum approach in a different cooperative multi-agent environment to assess generalizability beyond Overcooked.
2. Policy complexity analysis: Compare the learned policies of students trained with different curricula to verify that decreasing curriculum produces more complex, capable policies than other approaches.
3. Transfer learning evaluation: Measure how well students trained with different curricula adapt to novel teammate skill levels not encountered during training, testing the robustness of the learned behaviors.