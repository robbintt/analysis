---
ver: rpa2
title: 'HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception'
arxiv_id: '2310.20695'
source_url: https://arxiv.org/abs/2310.20695
tags:
- human
- uni00000013
- uni00000011
- pre-training
- human-centric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HAP, a novel masked image modeling approach
  for human-centric perception. HAP leverages human structure priors, specifically
  human parts, to guide the mask sampling process during pre-training.
---

# HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception

## Quick Facts
- **arXiv ID**: 2310.20695
- **Source URL**: https://arxiv.org/abs/2310.20695
- **Reference count**: 40
- **Key outcome**: HAP achieves state-of-the-art performance on 11 human-centric benchmarks, including 78.1% mAP on MSMT17 for person re-identification and 86.54% mA on PA-100K for pedestrian attribute recognition.

## Executive Summary
HAP introduces a novel masked image modeling approach for human-centric perception that leverages human structure priors to guide the mask sampling process during pre-training. By using human part information to prioritize which image patches are masked, HAP encourages the model to learn rich body structure information and contextual relationships among body parts. The method combines part-guided mask sampling with a structure-invariant alignment loss that enforces consistency across different masked views of the same image. HAP achieves superior performance across 12 diverse human-centric benchmarks, including person re-identification, pose estimation, and pedestrian attribute recognition, while using only a single dataset for pre-training.

## Method Summary
HAP is a masked image modeling framework that uses human part priors to guide the mask sampling process. During pre-training, human keypoints are extracted from each image using a pre-trained pose estimator. These keypoints are used to identify human part regions, which are then prioritized for masking. The model uses an 80% scaling ratio to preserve fine structural details and employs block-wise masking to maintain semantic coherence. Two random masked views of each image are generated and passed through a ViT encoder. A decoder reconstructs the masked patches, while a structure-invariant alignment loss enforces consistency between the two views by minimizing their feature distance. The pre-trained encoder is then fine-tuned for various downstream human-centric tasks.

## Key Results
- Achieves 78.1% mAP on MSMT17 for person re-identification
- Achieves 86.54% mA on PA-100K for pedestrian attribute recognition
- Outperforms existing methods across 12 human-centric benchmarks using only LUPerson dataset (~2.1M images)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human structure priors (especially high scaling ratio and block-wise mask sampling) enable the model to preserve and learn overall body structure information during pre-training.
- Mechanism: When the input image is scaled to a larger size (80% vs 20%), the human body occupies more pixels, preserving finer structural details. Block-wise masking ensures that entire body parts are masked together, maintaining semantic coherence and forcing the model to learn inter-part relationships.
- Core assumption: Human body structure is best learned when the model can see larger body parts and mask them as coherent semantic units rather than random pixels.
- Evidence anchors:
  - [abstract]: "We employ this prior to guide the mask sampling process. Image patches, corresponding to human part regions, have high priority to be masked out."
  - [section 3.1]: "we identify three crucial factors that are closely related to human structure priors, including high scaling ratio, block-wise mask sampling strategy, and intermediate masking ratio, have positive impacts on human-centric pre-training."
  - [corpus]: Weak - no direct citation of scaling ratio effects, but block-wise masking is mentioned in related works.

### Mechanism 2
- Claim: Human part prior guidance in mask sampling encourages the model to learn contextual correlations among body parts.
- Mechanism: By randomly selecting human parts and masking their corresponding image patches, the model must reconstruct these parts using visible patches from other body regions. This forces the model to learn how different body parts relate to each other structurally.
- Core assumption: The model can learn meaningful relationships between body parts when forced to reconstruct masked parts using clues from visible parts.
- Evidence anchors:
  - [abstract]: "This encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks."
  - [section 3.2]: "These part-aware masked patches are then reconstructed based on the clues of the remaining visible patches, which can provide semantically rich body structure information."
  - [corpus]: Weak - no direct citation, but semantic-guided masking is mentioned in related works.

### Mechanism 3
- Claim: Structure-invariant alignment loss captures discriminative human characteristics by enforcing consistency across different masked views of the same image.
- Mechanism: For each image, two random masked views are generated. The model learns to produce similar representations for both views by minimizing their distance in feature space, which preserves unique structural information while being invariant to which parts are masked.
- Core assumption: Human structural information should be consistent across different views of the same person, regardless of which body parts are masked.
- Evidence anchors:
  - [abstract]: "we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image."
  - [section 3.3]: "We then align the latent representations of the two random views within the same feature space. It preserves the unique structural information of the human body, which plays an important role in improving the discriminative ability of the pre-trained model for downstream tasks."
  - [corpus]: Weak - no direct citation of alignment loss for human structure.

## Foundational Learning

- Concept: Masked Image Modeling (MIM) fundamentals
  - Why needed here: HAP builds upon MIM as its base framework, so understanding how MIM works (masking patches and reconstructing them) is essential for grasping HAP's innovations.
  - Quick check question: In standard MIM, what percentage of patches are typically masked, and what is the goal of this masking?

- Concept: Vision Transformer (ViT) architecture
  - Why needed here: HAP uses a plain ViT as its encoder, so understanding how ViTs process image patches and generate representations is crucial.
  - Quick check question: How does a ViT differ from a CNN in terms of processing image patches and capturing spatial relationships?

- Concept: Self-supervised learning objectives
  - Why needed here: HAP combines reconstruction loss with alignment loss, so understanding different self-supervised objectives and how they complement each other is important.
  - Quick check question: What is the difference between contrastive learning and generative objectives like reconstruction in self-supervised learning?

## Architecture Onboarding

- Component map: Image → Keypoint extraction → Mask sampling guidance → Encoder (ViT) → Decoder → Reconstructed patches; Additional: Two masked views → Alignment loss
- Critical path: Image → Keypoint extraction → Mask sampling (with part guidance) → Encoder → Reconstruction + Alignment → Pre-trained encoder
- Design tradeoffs:
  - Scaling ratio: Higher ratios preserve more structure but reduce context; lower ratios provide more context but lose fine details
  - Masking ratio: Higher ratios force more reconstruction but may be too difficult; lower ratios are easier but may not learn rich representations
  - Alignment weight: Higher weights enforce consistency but may overfit; lower weights preserve flexibility but may not capture discriminative features
- Failure signatures:
  - Poor performance on downstream tasks: May indicate inadequate mask sampling strategy or alignment loss
  - Unstable training: Could suggest inappropriate scaling/masking ratios or alignment weight
  - Reconstruction artifacts: Might indicate decoder issues or insufficient part guidance
- First 3 experiments:
  1. Baseline experiment: Apply standard MAE with LUPerson data to establish performance baseline
  2. Scaling ratio experiment: Test different scaling ratios (60-90%) to find optimal range for human structure preservation
  3. Part-guided masking experiment: Compare random masking vs. part-guided masking to validate the effectiveness of human part prior

## Open Questions the Paper Calls Out

- How does the accuracy of the keypoint detector affect the performance of HAP?
  - Basis in paper: [explicit] The paper mentions using ViTPose [78] to extract keypoints and discusses robustness to different keypoint detectors, noting that HAP achieves comparable results with OpenPose [3].
  - Why unresolved: While the paper shows HAP is robust to different keypoint detectors, it doesn't explore the impact of keypoint detection accuracy on HAP's performance in detail.
  - What evidence would resolve it: Systematic experiments varying the accuracy of the keypoint detector and measuring its effect on HAP's downstream task performance would provide insights into this relationship.

- What is the impact of using multi-dataset co-training on HAP's performance?
  - Basis in paper: [inferred] The paper mentions that HAP can be pre-trained on a combination of several large-scale person datasets after extracting their keypoints and speculates that multi-dataset co-training can further improve HAP's performance.
  - Why unresolved: The paper doesn't provide experimental results to support this speculation.
  - What evidence would resolve it: Experiments comparing HAP's performance when pre-trained on single vs. multiple datasets would clarify the impact of multi-dataset co-training.

- How does HAP's performance compare to other human-centric pre-training methods that use more modalities or datasets?
  - Basis in paper: [explicit] The paper mentions that HAP uses only one dataset (∼2.1M samples) compared to existing pre-training methods that use multiple datasets and modalities, yet achieves superior performance on various human-centric perception tasks.
  - Why unresolved: The paper doesn't provide a direct comparison between HAP and other human-centric pre-training methods that use more modalities or datasets.
  - What evidence would resolve it: Experiments directly comparing HAP's performance to other human-centric pre-training methods using the same evaluation protocols would clarify HAP's relative performance.

## Limitations

- The exact implementation details of the human part keypoint detection system are not fully specified, particularly which version of ViTPose was used and how keypoint errors were handled.
- The computational overhead of keypoint extraction for the entire LUPerson dataset (~2.1M images) is not discussed, which could impact practical applicability.
- Limited ablation studies isolate the individual contributions of the different components (scaling ratio, block-wise masking, alignment loss) to the overall performance.

## Confidence

- **High confidence** in the overall effectiveness of HAP for human-centric perception tasks, given the consistent state-of-the-art performance across diverse benchmarks
- **Medium confidence** in the specific mechanisms (scaling ratio, block-wise masking, alignment loss) due to limited ablation studies and underspecified implementation details
- **Low confidence** in the computational efficiency claims, as training/inference costs are not reported

## Next Checks

1. **Ablation study isolation**: Conduct controlled experiments to quantify the individual contributions of scaling ratio, block-wise masking, and structure-invariant alignment loss by testing each component independently against a standard MAE baseline.

2. **Keypoint robustness testing**: Evaluate HAP's performance sensitivity to keypoint detection errors by introducing synthetic noise to the human part masks and measuring downstream task degradation.

3. **Computational overhead analysis**: Measure and report the total training time and memory requirements for both pre-training (including keypoint extraction) and downstream fine-tuning to assess practical deployment considerations.