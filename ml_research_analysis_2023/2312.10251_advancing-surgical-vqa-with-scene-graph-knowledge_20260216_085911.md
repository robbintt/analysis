---
ver: rpa2
title: Advancing Surgical VQA with Scene Graph Knowledge
arxiv_id: '2312.10251'
source_url: https://arxiv.org/abs/2312.10251
tags:
- surgical
- scene
- dataset
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving surgical Visual
  Question Answering (VQA) by incorporating scene graph knowledge. The authors propose
  a new dataset called SSG-QA, which generates diverse question-answer pairs using
  surgical scene graphs built from segmentation and detection models.
---

# Advancing Surgical VQA with Scene Graph Knowledge

## Quick Facts
- arXiv ID: 2312.10251
- Source URL: https://arxiv.org/abs/2312.10251
- Reference count: 36
- This paper introduces SSG-QA dataset and SSG-QA-Net model that achieves state-of-the-art results on surgical VQA by incorporating scene graph knowledge.

## Executive Summary
This paper addresses the challenge of improving surgical Visual Question Answering (VQA) by incorporating scene graph knowledge. The authors propose a new dataset called SSG-QA, which generates diverse question-answer pairs using surgical scene graphs built from segmentation and detection models. They also introduce SSG-QA-Net, a novel VQA model that integrates geometric scene knowledge through a Scene-embedded Interaction Module (SIM) using cross-attention between textual and scene features. The experiments demonstrate that SSG-QA-Net outperforms existing methods on the SSG-QA dataset, achieving state-of-the-art results across different question types and complexities.

## Method Summary
The paper introduces SSG-QA, a surgical VQA dataset built from Cholec80 videos with 960k question-answer pairs generated using surgical scene graphs. The SSG-QA-Net model combines YOLOv7 for object detection, RoIAlign for object-wise feature extraction, and a Scene-embedded Interaction Module (SIM) that uses cross-attention between textual and scene graph embeddings. The model fuses visual, textual, and refined scene embeddings through a transformer encoder to predict answers. Training involves YOLOv7 for object detection followed by end-to-end training of the VQA model on the SSG-QA dataset.

## Key Results
- SSG-QA-Net outperforms existing methods on SSG-QA dataset with state-of-the-art accuracy
- The model achieves significant improvements across different question types including location, attribute, and counting questions
- SIM module and RoIAlign pooling combination provides complementary benefits for surgical VQA
- SSG-QA dataset reduces question-condition bias through diverse, geometrically grounded question generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scene graph knowledge improves surgical VQA by providing geometric and relational context that global visual embeddings miss.
- Mechanism: The model extracts object-wise local features using YOLOv7 and combines them with spatial coordinates and class labels. The Scene-embedded Interaction Module (SIM) uses cross-attention between textual embeddings and scene graph embeddings to refine the scene context relevant to the question.
- Core assumption: Object-level visual reasoning is necessary for answering fine-grained surgical questions, and spatial relationships encoded in scene graphs capture this reasoning.
- Evidence anchors:
  - [abstract] "Our key idea is to exploit object-wise local features and fuse geometric scene information in the VQA model design."
  - [section 2.2.2] "The main objective of SIM is to correlate the textual embeddings with the scene embeddings."
  - [corpus] Weak - no direct citations on scene graphs in surgical VQA.

### Mechanism 2
- Claim: The SSG-QA dataset reduces question-condition bias by generating diverse, geometrically grounded question-answer pairs.
- Mechanism: Instead of using simple occurrence-based questions, the dataset uses a question engine that parameterizes questions with color, location, type, name, and relationship attributes from scene graphs. Sampling strategies balance surgical phase and tool presence to avoid overfitting to shortcuts.
- Core assumption: Question-condition bias exists when answers can be inferred from questions alone without visual processing, and diverse templates prevent this.
- Evidence anchors:
  - [section 2.1.3] "We also eliminate poorly formulated questions, such as 'What is the location of the <N>?' with <N>=gallbladder when there is no gallbladder in the scene."
  - [section 3.2] "SSG-QA further reduces bias by using scene-graph-based diverse questions."
  - [corpus] Weak - no quantitative comparison of bias reduction across datasets.

### Mechanism 3
- Claim: Combining SIM and RoIAlign pooling provides complementary benefits for surgical VQA.
- Mechanism: RoIAlign extracts object-wise visual embeddings, while SIM refines scene embeddings through cross-attention with textual inputs. Together they provide both local visual detail and contextually relevant scene understanding.
- Core assumption: Local visual features and text-aware scene embeddings capture different aspects of the question-answering process, and their combination is more effective than either alone.
- Evidence anchors:
  - [section 3.3.3] "Table 6 shows that combining both the Scene-embedded Interaction Module (SIM) and RoIAlign (ROI) pooling significantly boosts the model's performance."
  - [section 2.2.2] "The refined scene embeddings are then passed to the self-attention layer, S r = Self-Attention(S r, S r, S r), to interact with themselves."
  - [corpus] Weak - no ablation study showing individual contributions.

## Foundational Learning

- Concept: Visual Question Answering (VQA) fundamentals
  - Why needed here: The model architecture builds on standard VQA approaches but extends them with surgical-specific scene knowledge.
  - Quick check question: What are the two main input modalities in VQA and how are they typically processed?

- Concept: Scene graph representation
  - Why needed here: The dataset and model both rely on scene graphs to encode object relationships and spatial attributes.
  - Quick check question: How do scene graphs differ from object detection bounding boxes in representing visual scenes?

- Concept: Cross-attention mechanisms
  - Why needed here: SIM uses cross-attention to align textual queries with scene graph embeddings.
  - Quick check question: In what way does cross-attention differ from self-attention in transformer architectures?

## Architecture Onboarding

- Component map: Question text → Tokenizer → Textual embeddings (T) → SIM cross-attention → Refined scene embeddings (S_r) → Fusion with visual and scene embeddings → Transformer → Answer classifier

- Critical path: Question text → Tokenizer → Textual embeddings → SIM cross-attention → Refined scene embeddings → Fusion with visual and scene embeddings → Transformer → Answer prediction

- Design tradeoffs:
  - Using YOLOv7 adds detection latency but provides object-level features; alternatives could be lighter detectors with lower accuracy.
  - Scene graph construction depends on segmentation and detection model quality; errors propagate to question generation and model inputs.
  - SIM adds parameters but focuses on cross-modal alignment; could be replaced with simpler concatenation at cost of performance.

- Failure signatures:
  - Low recall on counting questions suggests object detection misses surgical instruments.
  - Poor performance on location-based questions indicates scene graph spatial encoding issues.
  - General performance drop suggests question-condition bias in the dataset.

- First 3 experiments:
  1. Run SSG-QA-Net on SSG-QA test set and compare mAP to VisualBert baseline to verify scene knowledge helps.
  2. Test SIM ablation by removing it and measuring performance drop on complex questions.
  3. Test RoIAlign ablation by using global features instead and measuring impact on object-attribute questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SSG-QA-Net compare to other surgical VQA models on the Cholec80-VQA dataset?
- Basis in paper: Explicit - "We also conduct the experiments on the other publicly available surgical VQA dataset Cholec80-VQA. As illustrated in Table 5, SSG-QA-Net significantly outperforms the SurgicalGPT [19], which requires heavy sequence decoding using GPT-2 architecture."
- Why unresolved: The paper does not provide detailed performance metrics for SSG-QA-Net on the Cholec80-VQA dataset, such as accuracy, mAP, recall, and F-score.
- What evidence would resolve it: A comprehensive comparison of SSG-QA-Net's performance metrics on the Cholec80-VQA dataset with other state-of-the-art surgical VQA models.

### Open Question 2
- Question: How does the SSG-QA dataset handle the challenge of generating diverse and unbiased question-answer pairs?
- Basis in paper: Explicit - "To address the question-condition bias that exists in the current surgical VQA datasets, we employ a sampling strategy based on surgical-specific knowledge and class distribution to remove questions that contain question-condition bias."
- Why unresolved: The paper does not provide specific details on the sampling strategy used to generate diverse and unbiased question-answer pairs in the SSG-QA dataset.
- What evidence would resolve it: A detailed explanation of the sampling strategy used in the SSG-QA dataset to generate diverse and unbiased question-answer pairs, along with an analysis of the dataset's diversity and bias reduction.

### Open Question 3
- Question: How does the Scene-embedded Interaction Module (SIM) in SSG-QA-Net contribute to the model's performance on complex questions?
- Basis in paper: Explicit - "The SIM module uses a scene graph of detected bounding boxes where each node contains the class label and bounding-box coordinate information. The scene graph is refined by cross-attention between the scene graph and the textual inputs, highlighting specific graph nodes correlated to the complex question query."
- Why unresolved: The paper does not provide a detailed analysis of the SIM module's contribution to the model's performance on complex questions, such as one-hop and single-and questions.
- What evidence would resolve it: An ablation study or analysis of the SIM module's impact on the model's performance for different question types and complexities, along with a comparison to models without the SIM module.

## Limitations
- Performance depends heavily on accuracy of YOLOv7 object detection, with errors propagating through the pipeline
- Question engine implementation details remain underspecified, limiting verification of bias reduction claims
- Despite diversity measures, 960k questions may still contain residual bias that wasn't quantitatively analyzed

## Confidence
- Scene graph knowledge improves surgical VQA: High confidence
- SSG-QA reduces question-condition bias: Medium confidence
- SIM and RoIAlign are complementary: Medium confidence

## Next Checks
1. Conduct an error analysis by measuring VQA performance degradation as object detection confidence thresholds vary, establishing the sensitivity to detection quality
2. Perform a bias analysis comparing answer distributions conditioned on question templates alone versus full visual context, quantifying the reduction achieved by SSG-QA
3. Execute an ablation study isolating SIM and RoIAlign contributions by measuring performance when each component is removed independently, revealing their individual value propositions