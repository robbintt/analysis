---
ver: rpa2
title: 'In-Context Explainers: Harnessing LLMs for Explaining Black Box Models'
arxiv_id: '2310.05797'
source_url: https://arxiv.org/abs/2310.05797
tags:
- explanations
- llms
- post
- features
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores whether Large Language Models (LLMs) can be
  used as post hoc explainers for black-box ML models. The authors propose four prompting
  strategies leveraging LLMs'' in-context learning (ICL) capability: Perturbation-based
  ICL, Prediction-based ICL, Instruction-based ICL, and Explanation-based ICL.'
---

# In-Context Explainers: Harnessing LLMs for Explaining Black Box Models

## Quick Facts
- arXiv ID: 2310.05797
- Source URL: https://arxiv.org/abs/2310.05797
- Reference count: 29
- Key outcome: LLMs can generate faithful post hoc explanations for black-box models, accurately identifying the most important feature with 72.19% accuracy on average across datasets.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) can serve as post hoc explainers for black-box machine learning models. The authors propose four prompting strategies that leverage LLMs' in-context learning capability: Perturbation-based ICL, Prediction-based ICL, Instruction-based ICL, and Explanation-based ICL. These strategies vary in the amount of information provided about the local neighborhood of test samples. Experiments on four real-world datasets with Logistic Regression and Artificial Neural Network models demonstrate that LLM-generated explanations perform comparably to state-of-the-art explainers like LIME and gradient-based methods in terms of faithfulness metrics.

## Method Summary
The study trains Logistic Regression and Artificial Neural Network models on four real-world tabular datasets (Blood, Recidivism, Adult, Default Credit). For each test instance, the authors generate perturbed samples in the local neighborhood and create prompts for LLMs using one of four strategies: Perturbation-based ICL (perturbed input-output pairs), Prediction-based ICL (differences from test sample), Instruction-based ICL (direct instructions), and Explanation-based ICL (demonstrations from existing explainers). The LLM responses are parsed to extract top-k feature rankings, which are then evaluated using faithfulness metrics including Feature Agreement (FA), Rank Agreement (RA), Prediction Gap on Important (PGI), and Prediction Gap on Unimportant (PGU) features.

## Key Results
- LLM-generated explanations accurately identify the most important feature with 72.19% accuracy on average across datasets
- Perturbation-based ICL achieves the highest faithfulness scores, followed by Instruction-based ICL
- LLMs can mimic the behavior of six state-of-the-art post hoc explanation methods using only four ICL samples
- Explanation faithfulness decreases as the number of ICL samples increases beyond 4-8 samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate faithful post hoc explanations by leveraging in-context learning to approximate black-box model behavior locally.
- Mechanism: The LLM receives perturbed input-output pairs from the local neighborhood of a test instance, inferring the decision boundary and identifying influential features.
- Core assumption: Local neighborhood contains sufficient samples to reveal model decision logic, and LLM can extract and generalize this logic from limited examples.
- Evidence anchors: [abstract] 72.19% accuracy in identifying important features; [section] local behavior assumed to be simple linear decision boundary.
- Break condition: If neighborhood perturbations don't capture model logic or LLM cannot generalize from limited examples.

### Mechanism 2
- Claim: LLMs can mimic existing post hoc explainers by observing input-output-explanation triplets in context.
- Mechanism: LLM provided with (input, output, explanation) examples from a state-of-the-art explainer, learning to generate similar explanations for new instances.
- Core assumption: LLM can generalize explanation patterns from small demonstrations and apply to new data.
- Evidence anchors: [abstract] LLMs mimic behavior of six state-of-the-art explainers using only four ICL samples; [section] investigate mimicking explainer behavior from demonstrations.
- Break condition: If LLM fails to generalize from examples or demonstrations aren't representative.

### Mechanism 3
- Claim: LLMs struggle to retrieve relevant information from longer prompts, reducing explanation faithfulness.
- Mechanism: As ICL examples increase, prompt length grows and LLM may lose focus on relevant patterns, degrading performance.
- Core assumption: There's a limit to information an LLM can process in a single prompt without accuracy loss.
- Evidence anchors: [abstract] LLMs struggle with longer prompts, decreasing explanation faithfulness; [section] impractical to provide all samples due to context length constraints.
- Break condition: If prompt exceeds LLM's effective context window or retrieval ability degrades.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Enables LLM to perform explanation tasks without fine-tuning using only examples in the prompt
  - Quick check question: What distinguishes in-context learning from traditional fine-tuning in LLMs?

- Concept: Feature attribution and post hoc explanations
  - Why needed here: Goal is to generate explanations highlighting which input features most influence model predictions
  - Quick check question: How do perturbation-based and gradient-based explanation methods differ in attributing feature importance?

- Concept: Local vs. global model behavior
  - Why needed here: Explanations focus on local neighborhood around test instance, assuming local linearity or simplicity
  - Quick check question: Why might a complex global model behave more simply in a local region around a specific input?

## Architecture Onboarding

- Component map: Input module -> Perturbation generator -> Prompt generator -> LLM interface -> Parser -> Evaluation module
- Critical path: 1) Select test sample 2) Generate perturbations → neighborhood samples 3) Create prompt 4) Query LLM 5) Parse response → top-k features 6) Evaluate faithfulness
- Design tradeoffs:
  - Number of ICL samples: Fewer limit information; more risk prompt length issues
  - Prompt format: Raw perturbations vs. differences affect LLM comprehension
  - Temperature setting: Higher increases creativity but may reduce consistency
- Failure signatures: LLM outputs unrelated content or refuses; parsed features don't match format; faithfulness metrics drop sharply with more ICL samples; performance varies between GPT-3.5 and GPT-4
- First 3 experiments: 1) Run Perturbation-based ICL with n=4 samples on Blood dataset and evaluate FA/RA 2) Compare Instruction-based ICL vs. Prediction-based ICL on same dataset 3) Test Explanation-based ICL with 4 LIME demonstrations and evaluate mimicry quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of ICL samples beyond which LLM-based explanations deteriorate significantly in faithfulness?
- Basis in paper: [explicit] FA, RA, and PGI scores decrease as ICL samples increase to 64, but exact threshold unknown
- Why unresolved: Only tested up to 64 ICL samples
- What evidence would resolve it: Experiments varying ICL samples beyond 64, identifying point where faithfulness drops below acceptable threshold

### Open Question 2
- Question: How do LLM-based explanations perform on datasets with different characteristics (categorical vs. numerical, high-dimensional vs. low-dimensional)?
- Basis in paper: [inferred] Only tested four tabular datasets with mixed feature types, suggesting potential performance variability
- Why unresolved: Experiments limited to specific dataset set, not covering full spectrum of data characteristics
- What evidence would resolve it: Experiments on diverse datasets varying in feature types, dimensionality, comparing LLM explanations across variations

### Open Question 3
- Question: Can LLM-based explanations be made more robust to changes in temperature and other LLM hyperparameters?
- Basis in paper: [explicit] Faithfulness doesn't change much across temperature values, but other hyperparameters unexplored
- Why unresolved: Only temperature tested, impact of other hyperparameters unknown
- What evidence would resolve it: Systematic experiments varying multiple LLM hyperparameters and combinations, identifying configurations maximizing explanation robustness

## Limitations
- Limited evaluation using only faithfulness metrics without comparison to established benchmarks like Human Importance Score
- Focus exclusively on tabular data with simple model architectures, leaving scalability questions unanswered
- Reliance on faithfulness metrics alone, which measure explanation alignment with model behavior rather than ground truth feature importance

## Confidence
- High Confidence: Core finding that LLMs can generate faithful explanations through in-context learning is well-supported by experimental results
- Medium Confidence: Comparative performance against established explainers is reasonably supported, though evaluation methodology provides limited insight into actual explanation quality
- Low Confidence: Claims about LLMs struggling with longer prompts lack rigorous analysis of relationship between prompt length, information retrieval, and explanation quality

## Next Checks
1. Conduct user studies to evaluate whether LLM-generated explanations align with human judgment of feature importance, comparing against explanations from established methods using metrics such as Human Importance Score and cross-explanation agreement
2. Evaluate LLM explainers on more complex model architectures (transformers, ensemble methods) and high-dimensional datasets to assess scalability limitations, particularly examining performance degradation with increasing feature dimensionality and model complexity
3. Extend faithfulness evaluation beyond FA, RA, PGI, and PGU to include comprehensive metrics like sufficiency, sensitivity, and stability, and validate whether LLM explanations maintain consistency across different random seeds and prompt variations