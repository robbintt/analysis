---
ver: rpa2
title: Improving Resnet-9 Generalization Trained on Small Datasets
arxiv_id: '2309.03965'
source_url: https://arxiv.org/abs/2309.03965
tags:
- training
- dataset
- small
- hajimolahoseini
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training a deep learning model
  with high accuracy on a small dataset (5000 images from CIFAR-10) within a strict
  time limit of 10 minutes, as part of the ICLR Hardware Aware Efficient Training
  competition. The core method involves applying multiple techniques to improve the
  generalization of ResNet-9, including Sharpness Aware Minimization (SAM) optimizer,
  Gradient Centralization (GC), label smoothing, weight decay, CELU activation function,
  input patch whitening, and meta-learning based training.
---

# Improving Resnet-9 Generalization Trained on Small Datasets

## Quick Facts
- arXiv ID: 2309.03965
- Source URL: https://arxiv.org/abs/2309.03965
- Reference count: 8
- The paper demonstrates achieving 86.6% accuracy on Mini-CIFAR10 and 88% on Mini-ImageNet within a 10-minute time constraint

## Executive Summary
This paper addresses the challenge of training a deep learning model with high accuracy on a small dataset (5000 images from CIFAR-10) within a strict time limit of 10 minutes, as part of the ICLR Hardware Aware Efficient Training competition. The authors propose a comprehensive approach combining multiple techniques to improve the generalization of ResNet-9, including Sharpness Aware Minimization (SAM) optimizer, Gradient Centralization (GC), label smoothing, weight decay, CELU activation function, input patch whitening, and meta-learning based training. The primary result shows that the combined approach achieved 86.6% accuracy on Mini-CIFAR10 and 88% on Mini-ImageNet, significantly outperforming the baseline ResNet-9 which achieved 76.65% and 80.3% respectively. The model completed 200 epochs within the 10-minute time constraint on both datasets.

## Method Summary
The approach combines Sharpness Aware Minimization (SAM) optimizer with various preprocessing techniques including label smoothing, weight decay, CELU activation function, and input patch whitening. Additionally, a meta-learning based training procedure (MLTP) is employed, which breaks the dataset into multiple tasks and trains in a meta-learning fashion to learn common representations across tasks. The training is performed on a 5000-image subset of CIFAR-10 (10 classes, 500 images each) with the goal of achieving maximum accuracy on a secret 1000-image Mini-ImageNet dataset within 10 minutes.

## Key Results
- Achieved 86.6% accuracy on Mini-CIFAR10 compared to 76.65% baseline
- Achieved 88% accuracy on Mini-ImageNet compared to 80.3% baseline
- Completed 200 epochs within 10-minute time constraint on both datasets
- Combined SAM optimizer with preprocessing techniques (label smoothing, weight decay, CELU, input patch whitening) and meta-learning training

## Why This Works (Mechanism)

### Mechanism 1: SAM Optimizer
- Claim: SAM improves generalization by minimizing both loss value and loss sharpness
- Mechanism: SAM performs min-max optimization, finding parameter values whose entire surrounding area has uniformly low training loss, rather than just low loss at a point
- Core assumption: Sharpness of the loss landscape correlates with generalization performance
- Evidence anchors: SAM minimizes both the value and sharpness of the loss function simultaneously; instead of looking for parameter values that show a low training loss, SAM searches for those parameter values whose entire surrounding area has a uniformly low training loss
- Break condition: If the loss landscape becomes too flat, optimization may become unstable or converge slowly

### Mechanism 2: Gradient Centralization
- Claim: Gradient Centralization improves generalization by regularizing the solution space of model parameters
- Mechanism: GC centralizes gradients to have zero mean, creating a projected gradient descent method with constrained loss function
- Core assumption: Zero-mean gradients lead to better generalization by reducing overfitting on training data
- Evidence anchors: GC centralizes the gradients so that they have zero mean by creating a projected gradient descent method with a constrained loss function; by regularizing the solution space of model parameters, GC helps to reduce the possibility of overfitting on training data and improving generalization of trained models, especially for small datasets
- Break condition: If the zero-mean constraint becomes too restrictive, it may prevent learning important features

### Mechanism 3: Meta-Learning Based Training
- Claim: Meta-learning based training improves generalization by learning task-agnostic representations
- Mechanism: The approach breaks the dataset into multiple tasks and trains in a meta-learning fashion to learn common representations across tasks
- Core assumption: Representations learned across multiple similar tasks generalize better to unseen data
- Evidence anchors: Meta-learning is a promising training approach for few-shot learning problems; this meta-learning approach has been reframed as a single-task algorithm for training on small dataset (10 classes of Mini-ImageNet) – named Meta-Learning based Training Procedure (MLTP)
- Break condition: If tasks are not sufficiently similar, meta-learning may learn poor representations

## Foundational Learning

- **Sharpness-aware optimization**: Why needed here - To improve generalization on small datasets where standard optimizers fail to find good minima; Quick check question - How does SAM differ from standard gradient descent in terms of what it optimizes?
- **Gradient centralization**: Why needed here - To regularize the solution space and reduce overfitting when training on limited data; Quick check question - What does it mean for gradients to have zero mean, and why is this beneficial?
- **Label smoothing**: Why needed here - To prevent overconfident predictions that inhibit further training on small datasets; Quick check question - How does blending one-hot targets with uniform distribution affect the loss landscape?

## Architecture Onboarding

- **Component map**: ResNet-9 -> SAM optimizer with SGD -> Preprocessing (label smoothing, weight decay, CELU activation, input patch whitening) -> Meta-learning training (MLTP)
- **Critical path**: SAM optimizer → IP techniques (label smoothing, weight decay, CELU, input patch whitening) → Meta-learning training
- **Design tradeoffs**: Balancing regularization strength vs. model capacity, training time vs. accuracy
- **Failure signatures**: Overfitting on training set, poor validation performance, training instability
- **First 3 experiments**:
  1. Train baseline ResNet-9 with standard SGD to establish performance baseline
  2. Add SAM optimizer to baseline to measure improvement from sharpness-aware optimization
  3. Add input patch whitening preprocessing to measure impact of improved data preprocessing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of the proposed techniques (SAM, GC, IP, meta-learning) that would yield the highest accuracy while maintaining the 10-minute time constraint?
- Basis in paper: The authors mention that due to time limitations, they did not experiment with all different combinations of the proposed methods and left it for future work
- Why unresolved: The authors did not have sufficient time to explore all possible combinations of the techniques during the competition
- What evidence would resolve it: Systematic experimentation with different combinations of SAM, GC, IP, and meta-learning techniques to determine the optimal configuration that maximizes accuracy within the 10-minute constraint

### Open Question 2
- Question: How would the proposed approach perform on larger datasets or different image classification tasks beyond CIFAR-10 and Mini-ImageNet?
- Basis in paper: The authors demonstrate the effectiveness of their approach on a small subset of CIFAR-10 and Mini-ImageNet, but do not explore its generalizability to other datasets or larger-scale tasks
- Why unresolved: The paper focuses on a specific competition scenario with limited dataset size and does not investigate the approach's performance on other datasets or tasks
- What evidence would resolve it: Application and evaluation of the proposed techniques on various image classification datasets of different sizes and complexities to assess their generalizability and effectiveness

### Open Question 3
- Question: What is the impact of hyperparameter tuning on the performance of the proposed approach, and how sensitive is the model to changes in these parameters?
- Basis in paper: The authors mention specific hyperparameter values used in their experiments (e.g., smoothing factor α = 0.1, decay factor λ = 0.0005, CELU parameter α = 0.3), but do not discuss the sensitivity of the model to these parameters or explore the hyperparameter space
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of hyperparameter tuning on the model's performance or discuss the sensitivity of the approach to changes in these parameters
- What evidence would resolve it: Systematic hyperparameter tuning experiments to identify the optimal values for each technique and assess the model's sensitivity to changes in these parameters

## Limitations

- The exact implementation details of the meta-learning based training procedure (MLTP) are not fully specified in the paper, requiring assumptions about task construction and optimization procedures
- The specific hyperparameters for SAM optimizer (base learning rate, momentum, etc.) are not mentioned, which could significantly impact results
- The evaluation on the secret Mini-ImageNet dataset means the claimed 88% accuracy cannot be independently verified

## Confidence

- **High Confidence**: The core mechanism of SAM improving generalization through sharpness minimization, as this is well-established in the literature and directly cited
- **Medium Confidence**: The effectiveness of combining SAM with preprocessing techniques (label smoothing, weight decay, CELU, input patch whitening) for the specific 10-minute time constraint, as this is empirically demonstrated but relies on specific implementation details
- **Medium Confidence**: The meta-learning based training approach for small datasets, as the concept is theoretically sound but the specific MLTP implementation details are unclear

## Next Checks

1. **Reproduce the baseline results**: Implement ResNet-9 with standard SGD optimizer and all preprocessing techniques on the 5000 CIFAR-10 subset, measuring accuracy and training time to establish a baseline

2. **Validate SAM implementation**: Replace the optimizer with SAM while keeping all other components identical, and verify that the accuracy improvement is consistent with the reported 86.6% on Mini-CIFAR10

3. **Test individual component contributions**: Systematically remove each preprocessing technique (label smoothing, weight decay, CELU, input patch whitening) to measure their individual impact on both accuracy and training time within the 10-minute constraint