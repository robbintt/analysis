---
ver: rpa2
title: Are LSTMs Good Few-Shot Learners?
arxiv_id: '2310.14139'
source_url: https://arxiv.org/abs/2310.14139
tags:
- lstm
- uni00000013
- learning
- op-lstm
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the idea of using LSTMs for meta-learning,
  first proposed by Hochreiter et al. (2001).
---

# Are LSTMs Good Few-Shot Learners?

## Quick Facts
- arXiv ID: 2310.14139
- Source URL: https://arxiv.org/abs/2310.14139
- Reference count: 23
- Key outcome: A plain LSTM underperforms on few-shot image classification benchmarks but a new Outer Product LSTM (OP-LSTM) method achieves competitive performance on within-domain few-shot image classification and outperforms popular meta-learning baselines in cross-domain settings by 0.5% to 1.9% in accuracy score.

## Executive Summary
This paper investigates whether plain LSTMs can effectively perform few-shot learning on modern benchmarks. While surprisingly competitive on a simple sine wave regression task, plain LSTMs fall short on complex few-shot image classification. The authors identify two key limitations: lack of permutation invariance in processing support data and the intertwining of input representation with learning procedures. They propose a new method called Outer Product LSTM (OP-LSTM) that addresses these issues through permutation-invariant support processing and separation of base-learner and meta-learner functions. OP-LSTM achieves competitive performance on standard few-shot image classification benchmarks and outperforms existing meta-learning methods in cross-domain settings.

## Method Summary
The authors propose OP-LSTM, a meta-learning method that uses LSTMs to update weight matrices through outer products. Unlike plain LSTMs that process support examples sequentially, OP-LSTM first embeds each support example through a base-learner network, then aggregates these embeddings using average pooling for permutation invariance. Separate LSTMs then update each weight matrix of the base-learner using outer products between hidden states and input activations. This architecture decouples the input representation from the learning procedure. The method is trained end-to-end using backpropagation through time across multiple tasks sampled from the training distribution.

## Key Results
- Plain LSTM surprisingly outperforms MAML on sine wave regression but underperforms on few-shot image classification
- OP-LSTM achieves competitive performance on within-domain few-shot image classification benchmarks
- OP-LSTM outperforms popular meta-learning baselines (MAML, ProtoNet) in cross-domain settings by 0.5% to 1.9% accuracy
- OP-LSTM can approximate both MAML and Prototypical Network update rules through its outer product mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation invariance in support data processing improves training stability and performance
- Mechanism: By using average pooling over support examples rather than sequential processing, the hidden state becomes invariant to the order of examples, preventing the LSTM from overfitting to spurious sequences
- Core assumption: The learning algorithm should not depend on the order in which support examples are presented
- Evidence anchors:
  - [abstract] "We identify two potential causes for this underperformance, namely 1) the fact that it is not invariant to permutations of the training data"
  - [section 5.1] "we propose a permutation-invariant method of feeding training examples into recurrent neural networks and perform a detailed study"
  - [corpus] Weak - corpus lacks direct evidence about permutation invariance benefits
- Break condition: If the learning algorithm inherently requires temporal dependencies between support examples (e.g., ordered sequences), permutation invariance would harm performance

### Mechanism 2
- Claim: Decoupling input representation from learning procedure resolves optimization difficulties
- Mechanism: By separating the base-learner network (input representation) from the meta-network (learning updates), each component can specialize without interference, reducing overfitting risk and improving optimization
- Core assumption: The optimal input embedding and optimal learning procedure are orthogonal concerns that should be learned independently
- Evidence anchors:
  - [abstract] "2) that the input representation and learning procedures are intertwined"
  - [section 3.3] "we create two separate procedures: 1) the embedding procedure, and 2) the learning procedure"
  - [section 4.1] "we effectively decouple the learning algorithm implemented by hidden state dynamics from the input representation"
- Break condition: If the learning procedure inherently requires complex input representations, or if the base-learner cannot adequately represent the input space

### Mechanism 3
- Claim: Outer product updates enable learning of both parametric and non-parametric meta-learning algorithms
- Mechanism: The outer product of LSTM hidden states with input activations allows the meta-network to learn update rules that can approximate gradient descent (MAML) and prototype-based classification (ProtoNet)
- Core assumption: The LSTM hidden states can encode the necessary information to compute both gradient-like and prototype-like updates
- Evidence anchors:
  - [section 6] "OP-LSTM can approximate MAML [Finn et al., 2017] as well as Prototypical network [Snell et al., 2017]"
  - [section 4.1] "we use the outer product, which requires only one hidden vector of size din that can be outer-multiplied with a vector of size dout"
  - [corpus] Weak - corpus lacks evidence about outer product update mechanisms
- Break condition: If the base-learner architecture or task complexity exceeds what outer product updates can effectively represent

## Foundational Learning

- Concept: Permutation invariance in set processing
  - Why needed here: Support sets are unordered collections of examples; the learning algorithm should not depend on presentation order
  - Quick check question: What happens to the LSTM's hidden state if you randomly permute the support examples?

- Concept: Separation of concerns in neural network design
  - Why needed here: The plain LSTM fails because it conflates input representation with learning dynamics; OP-LSTM solves this by separating them
  - Quick check question: How does OP-LSTM's base-learner differ functionally from the LSTM's hidden state?

- Concept: Outer product weight updates
  - Why needed here: Enables the meta-network to learn update rules that can approximate different meta-learning algorithms without explicitly computing gradients
  - Quick check question: How does the outer product update rule in OP-LSTM differ from standard gradient descent?

## Architecture Onboarding

- Component map: Base-learner network -> OP-LSTM blocks -> Average pooling -> Meta-training loop
- Critical path:
  1. Sample support and query sets for a task
  2. Feed support examples through base-learner
  3. Update weight matrices using OP-LSTM outer product rules
  4. Compute predictions on query set
  5. Backpropagate query loss to update meta-parameters

- Design tradeoffs:
  - LSTM vs coordinate-wise LSTM: Single LSTM per layer vs. shared LSTM across coordinates (parameter efficiency vs. expressiveness)
  - Number of passes over support data: More passes allow more complex updates but increase computation
  - Architecture of base-learner: Convolutional for images vs. fully-connected for simpler data

- Failure signatures:
  - Poor performance on cross-domain tasks: Indicates meta-learned updates don't generalize
  - Instability during training: Suggests learning rate or architecture needs adjustment
  - Random guessing: Points to fundamental issues with update rules or base-learner capacity

- First 3 experiments:
  1. Implement permutation-invariant LSTM on sine wave regression to verify performance improvement over sequential LSTM
  2. Compare OP-LSTM with MAML on Omniglot 5-way 1-shot classification to validate competitive performance
  3. Measure cosine similarity between OP-LSTM updates and gradient descent on miniImageNet to confirm approximation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the permutation invariance introduced by mean pooling in OP-LSTM affect the performance on datasets with strong temporal or sequential dependencies?
- Basis in paper: [explicit] The paper demonstrates that mean pooling improves performance on few-shot image classification by making the LSTM invariant to permutations of the support data.
- Why unresolved: The paper only tests on datasets (Omniglot, miniImageNet, CUB) where the order of training examples is not inherently meaningful. It doesn't explore tasks where temporal order might carry important information.
- What evidence would resolve it: Experiments comparing OP-LSTM with and without mean pooling on datasets like time-series classification or video action recognition where sequence order is crucial.

### Open Question 2
- Question: Can the outer product weight update rule in OP-LSTM be generalized to update convolutional layers in CNNs, and how would this affect performance?
- Basis in paper: [explicit] The paper states that OP-LSTM is currently only applied on top of convolutional feature embeddings, not on the convolutional layers themselves, due to difficulties with message passing through pooling layers.
- Why unresolved: The paper acknowledges this as a limitation but doesn't provide a solution or investigate the potential benefits of updating convolutional layers directly.
- What evidence would resolve it: Implementation of OP-LSTM that updates convolutional layer weights, followed by comparison with standard CNNs and OP-LSTM with frozen convolutional layers on few-shot image classification benchmarks.

### Open Question 3
- Question: What is the theoretical relationship between the expressivity of the LSTM hidden state dimensionality and its ability to learn effective learning algorithms in meta-learning?
- Basis in paper: [inferred] The paper mentions that a single-layer LSTM may be too limited for complex learning tasks, while stacking multiple layers may lead to overfitting due to overly complex input representations.
- Why unresolved: The paper doesn't provide a theoretical analysis of how LSTM architecture (number of layers, hidden state size) affects its meta-learning capability, only empirical observations.
- What evidence would resolve it: Theoretical analysis of the relationship between LSTM capacity and learning algorithm expressivity, potentially using techniques from neural tangent kernel or mean-field theory, validated with empirical studies across different LSTM architectures.

## Limitations
- Permutation invariance claims lack comprehensive empirical validation through ablation studies
- Outer product approximation capability remains primarily theoretical with limited empirical validation
- The paper doesn't investigate OP-LSTM's performance on tasks with inherent temporal dependencies

## Confidence

- **Permutation invariance mechanism**: Medium - supported by ablation studies but lacks comprehensive empirical validation
- **Separation of concerns**: High - well-documented architectural distinction with clear performance improvements
- **Outer product approximation capability**: Low - primarily theoretical with limited empirical validation of approximation quality

## Next Checks
1. Conduct ablation studies measuring performance variance across different support set orderings for both LSTM and OP-LSTM
2. Perform quantitative analysis comparing OP-LSTM update vectors to actual MAML and ProtoNet gradients across multiple tasks
3. Test OP-LSTM with alternative base-learner architectures (e.g., transformer-based) to verify the decoupling claims hold beyond the tested convolutional and fully-connected models