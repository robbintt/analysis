---
ver: rpa2
title: A Closer Look into Automatic Evaluation Using Large Language Models
arxiv_id: '2310.05657'
source_url: https://arxiv.org/abs/2310.05657
tags:
- evaluation
- ratings
- chatgpt
- prompts
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies two methods for using large language models
  (LLMs) to automatically evaluate text quality, finding that forcing LLMs to output
  only numeric ratings is suboptimal, and that asking LLMs to explain or analyze their
  ratings consistently improves correlation with human ratings, with state-of-the-art
  correlations achieved on two datasets.
---

# A Closer Look into Automatic Evaluation Using Large Language Models

## Quick Facts
- **arXiv ID**: 2310.05657
- **Source URL**: https://arxiv.org/abs/2310.05657
- **Reference count**: 40
- **Primary result**: Asking LLMs to explain their ratings consistently improves correlation with human ratings compared to forcing numeric-only outputs

## Executive Summary
This paper systematically investigates methods for using large language models (LLMs) as automatic evaluation metrics for text quality. Through extensive experiments on two datasets (SummEval and Topical-Chat), the authors demonstrate that restricting LLMs to output only numeric ratings is suboptimal, and that asking LLMs to explain or analyze their ratings consistently yields better alignment with human judgments. The study identifies that auto Chain-of-Thought methods often merely paraphrase evaluation criteria without adding substantive reasoning, and establishes that explanation-based prompting achieves state-of-the-art correlation coefficients across multiple evaluation attributes.

## Method Summary
The study evaluates ChatGPT's performance as an automatic metric through controlled experiments on two meta-evaluation datasets. Researchers systematically vary prompt formats (score-only vs. free text vs. explanation-based), test auto Chain-of-Thought effectiveness, and measure correlation with human ratings using Pearson's r and Kendall's τ. The experimental pipeline involves prompt generation with task descriptions and evaluation criteria, LLM API calls to generate ratings, and statistical analysis of correlation coefficients. The paper compares six prompting strategies across 11 attributes, testing robustness through temperature variation and persona prompts.

## Key Results
- Forcing LLMs to output only numeric ratings yields lower correlation with human ratings than allowing free text or explanation formats
- Asking LLMs to explain their ratings consistently improves correlation across both SummEval and Topical-Chat datasets
- Auto Chain-of-Thought methods often merely paraphrase evaluation criteria rather than adding substantive reasoning
- Explanation-based prompting achieves state-of-the-art correlation coefficients on both datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forcing LLMs to output only numeric ratings reduces correlation with human ratings.
- Mechanism: Restricting output form prevents the model from generating intermediate reasoning that could calibrate its judgment.
- Core assumption: The model's internal rating process benefits from explicit reasoning steps even if not directly used in final output.
- Evidence anchors: abstract states "forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal"; section 3.2 shows "allowing ChatGPT to respond to the question freely yields Pearson's r and Kendall's τ much higher than restricting the model to output a single numeric score"; tested on two datasets.

### Mechanism 2
- Claim: Auto Chain-of-Thought (CoT) does not consistently improve correlation with human ratings.
- Mechanism: Auto-generated evaluation steps often merely paraphrase existing criteria without adding substantive reasoning depth.
- Core assumption: The quality of evaluation steps matters more than their mere presence.
- Evidence anchors: abstract states "the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings"; section 3.1 shows "the evaluation steps generated with auto CoT often merely paraphrases the evaluation criterion and instructions given to the LLM"; moderate evidence from two datasets with mixed results.

### Mechanism 3
- Claim: Asking LLMs to explain their ratings improves correlation with human ratings.
- Mechanism: Explicit explanation prompts the model to generate ratings that are easier to justify, leading to better alignment with human judgment patterns.
- Core assumption: Ratings that are easier to explain are more likely to align with human reasoning.
- Evidence anchors: abstract states "asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings"; section 3.2 shows "asking ChatGPT to explain/analyze when rating, ChatGPT's correlation can be better than or comparable to the state-of-the-art correlation coefficients"; strong evidence from consistent improvement across both datasets and multiple attributes.

## Foundational Learning

- Concept: Correlation coefficient calculation (Pearson's r and Kendall's τ)
  - Why needed here: To measure alignment between LLM ratings and human ratings
  - Quick check question: What's the difference between Pearson's r and Kendall's τ in terms of what they measure?

- Concept: Meta-evaluation of evaluation metrics
  - Why needed here: To assess the quality of LLM-based evaluation methods
  - Quick check question: Why is it important to evaluate evaluation metrics rather than just using them?

- Concept: Prompt engineering and instruction following
  - Why needed here: Core mechanism for controlling LLM behavior in evaluation tasks
  - Quick check question: How does prompt format affect the quality of LLM outputs?

## Architecture Onboarding

- Component map: Load datasets → preprocess → split into samples → prompt generation → LLM evaluation → correlation calculation → significance testing
- Critical path: Prompt generation → LLM evaluation → correlation calculation → significance testing
- Design tradeoffs: Single numeric output vs. free text (simplicity vs. potential for better alignment); Auto CoT vs. manual criteria (automation vs. control over evaluation steps); Number of samples per rating (cost vs. stability of results)
- Failure signatures: Low correlation coefficients indicate poor alignment; Inconsistent results across runs suggest instability; Statistical insignificance indicates unreliable differences
- First 3 experiments: 1) Baseline: G-Eval method (auto CoT + score only) on both datasets; 2) Output format variation: Compare score only vs. free text vs. explain formats; 3) Auto CoT effectiveness: Compare with and without auto CoT for each output format

## Open Questions the Paper Calls Out

1. How do different large language models (LLMs) compare in terms of their effectiveness as automatic evaluation metrics for various NLP tasks? (Explicitly stated by the authors)

2. What are the optimal prompt designs and evaluation criteria for different attributes when using LLMs for automatic evaluation? (Explicitly stated by the authors)

3. How does the performance of LLMs as automatic evaluation metrics vary across different datasets and tasks? (Explicitly stated by the authors)

4. What are the limitations and potential biases of using LLMs for automatic evaluation, and how can these be addressed? (Explicitly stated by the authors)

5. How can the robustness of LLMs as automatic evaluation metrics be improved with respect to variations in prompts and sampling temperatures? (Explicitly stated by the authors)

## Limitations

- Tested only two datasets (SummEval and Topical-Chat), limiting generalizability to other NLP tasks
- Auto CoT analysis is limited to current implementations without exploring enhanced methods
- Causal mechanism for why explanations improve correlation remains unclear (could be token usage rather than explanatory content)
- Does not systematically investigate the impact of different scoring scales on performance

## Confidence

- Forcing numeric-only outputs is suboptimal: **High confidence**
- Auto CoT often merely paraphrases criteria: **Medium confidence**
- Explanations improve correlation: **High confidence**
- Generalizability across diverse tasks: **Medium confidence**
- Token usage vs. explanation content: **Low confidence**

## Next Checks

1. Test the proposed methods on at least three additional diverse evaluation datasets (e.g., machine translation, dialogue, and summarization tasks) to assess generalizability

2. Conduct ablation studies varying prompt length and token count to isolate whether improvements come from explanations or simply more computation

3. Implement and test enhanced auto CoT methods that generate substantively different evaluation steps rather than paraphrasing criteria, to determine if the failure is with auto CoT itself or just current implementations