---
ver: rpa2
title: 'Quantizable Transformers: Removing Outliers by Helping Attention Heads Do
  Nothing'
arxiv_id: '2306.12929'
source_url: https://arxiv.org/abs/2306.12929
tags:
- attention
- outliers
- layer
- quantization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of quantization difficulty in transformer
  models caused by strong outliers in activations, which require higher bitwidths
  or workarounds for acceptable performance. The core method introduces two modifications
  to the attention mechanism based on analysis showing outliers arise when attention
  heads try to learn "no-op" behavior.
---

# Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing

## Quick Facts
- **arXiv ID:** 2306.12929
- **Source URL:** https://arxiv.org/abs/2306.12929
- **Reference count:** 40
- **Primary result:** Introduces clipped softmax and gated attention to suppress outliers in transformers, enabling full INT8 quantization without performance degradation.

## Executive Summary
This paper addresses quantization difficulty in transformer models caused by strong activation outliers. Through analysis, the authors discover that outliers arise when attention heads learn to suppress updates by attending heavily to uninformative tokens with low-magnitude values. They propose two modifications to the attention mechanism: clipped softmax that allows exact zero/one outputs, and gated attention with explicit gating modules to nullify updates. These methods significantly reduce outliers while maintaining or improving floating-point task performance, enabling full INT8 quantization of activations without additional effort. The approach is demonstrated effective on both language models (BERT, OPT) and vision transformers (ViT).

## Method Summary
The authors introduce two modifications to address outliers in transformer attention mechanisms. First, clipped softmax replaces the standard softmax with a variant that can produce exact zero or one outputs, preventing the need for large input dynamic ranges. Second, gated attention adds a sigmoid gating module that explicitly controls whether an attention head should contribute to the residual update. Both methods are integrated during pre-training and shown to reduce outlier magnitudes while maintaining model performance. The training uses standard transformer architectures with symmetric weight quantization and asymmetric activation quantization, employing static range estimation with running min-max statistics.

## Key Results
- Clipped softmax and gated attention significantly reduce activation outliers during pre-training
- Both methods maintain or improve floating-point task performance on BERT, OPT, and ViT
- Models trained with these modifications enable full INT8 quantization without additional effort
- Quantized performance approaches original floating-point performance levels
- Methods work across both language and vision transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
Attention heads learn to suppress updates by attending heavily to uninformative tokens (delimiters/background patches) with low-magnitude values. The attention head allocates most of its probability mass to fixed, common tokens that have low information content and can be learned to have small value function outputs, effectively creating a "soft no-op" that adds only noise to the residual.

### Mechanism 2
The softmax function requires large input dynamic range to produce near-zero outputs, creating outliers in the FFN output. To achieve the exact zeros needed for a no-update, the input to softmax is pushed to be larger and larger during training, causing outliers in other parts of the network due to the mathematical relationship between softmax inputs and outputs.

### Mechanism 3
Layer Normalization amplifies outliers by normalizing the attention output, forcing the FFN to produce even larger activations. Since Layer Normalization normalizes the outliers, the magnitude of the FFN output in the previous layer has to be very high to still produce a sufficiently big dynamic range after the LayerNorm, creating a feedback loop.

## Foundational Learning

- **Quantization fundamentals** (uniform affine quantization, symmetric vs asymmetric, min-max vs MSE range estimation) - Why needed: The entire paper addresses quantization challenges in transformers, requiring understanding of how different quantization schemes affect outlier handling and model performance. Quick check: What's the difference between symmetric and asymmetric quantization, and why might asymmetric be preferred for activations with outliers?

- **Attention mechanism and multi-head self-attention** (Q, K, V projections, attention probability computation) - Why needed: The proposed modifications directly target the attention mechanism, requiring deep understanding of how Q, K, V projections work and how attention probabilities are computed. Quick check: How does the multi-head attention mechanism partition the hidden dimension, and what happens at the boundaries between heads?

- **Transformer architecture and residual connections** (residual add, LayerNorm placement) - Why needed: The "no-op" behavior analysis relies on understanding how residual connections interact with attention outputs, and how LayerNorm affects the overall flow. Quick check: In a transformer layer, what is the exact order of operations: attention → residual add → LayerNorm, or does LayerNorm come first?

## Architecture Onboarding

- **Component map:** Attention mechanism with clipped softmax variant → Gated attention variant with sigmoid gating → LayerNorm placement considerations → Weight quantization (symmetric) → Activation quantization (asymmetric) → Range estimation (running min-max)
- **Critical path:** Pre-training → outlier analysis → method implementation → quantization → evaluation
- **Design tradeoffs:** Clipped softmax vs gated attention: computational overhead vs flexibility; Static vs dynamic quantization ranges: calibration complexity vs accuracy; LayerNorm placement: affects outlier dynamics but changes model behavior; Stretch factor γ: too small ineffective, too large degrades performance
- **Failure signatures:** Outliers persist despite modifications: indicates incorrect γ initialization or insufficient training; Quantization performance degrades: suggests range estimation issues or inappropriate quantization scheme; Floating-point performance drops significantly: indicates gating parameters not properly initialized
- **First 3 experiments:** 1) Baseline BERT pre-training with vanilla attention to establish outlier baseline metrics; 2) Clipped softmax with varying γ values to find optimal range for outlier suppression; 3) Gated attention with different bias initializations to find sweet spot for πinit values

## Open Questions the Paper Calls Out

### Open Question 1
Why does clipped softmax with γ < 0 fail to improve quantization performance for OPT models while succeeding for BERT and ViT? The authors note this specific failure case but do not provide any analysis or hypotheses for why it occurs.

### Open Question 2
What is the optimal range for the πinit hyperparameter in gated attention across different model architectures? The authors found reasonable ranges but the wide range indicates relative robustness rather than optimization, and they didn't systematically explore the full hyperparameter space.

### Open Question 3
Do convolutional neural networks learn similar "no-op" behavior patterns that lead to activation outliers? The authors speculate that this behavior is likely not limited to transformers, but the paper only analyzes transformer architectures without providing evidence for this claim about CNNs.

## Limitations
- The core mechanism linking outlier formation to "no-op" attention behavior lacks direct empirical validation beyond observational correlation
- The choice of quantization scheme and range estimation method is presented as effective but not systematically compared against alternatives
- Claims about generalizability across architectures are based on only three model families without testing on architectures with different attention mechanisms or normalization schemes

## Confidence

- **Outlier Formation Mechanism (Medium):** The claim that attention heads learning "no-op" behavior causes outliers is supported by observational data but lacks causal proof.
- **Method Effectiveness (High):** The empirical results showing improved quantization performance are robust and clearly demonstrated across multiple model types.
- **Generalizability (Low):** Claims about working for both language and vision transformers are based on three model families without broader testing.
- **Mathematical Foundation (High):** The observation that Layer Normalization amplifies outliers through the feedback loop mechanism is mathematically sound and consistent with presented data.

## Next Checks

1. **Causal Outlier Reduction Study:** Design an experiment where attention heads are explicitly constrained to avoid "no-op" patterns (e.g., minimum attention entropy regularization) and measure whether this reduces outlier formation without the proposed modifications.

2. **Architecture Ablation Analysis:** Apply the proposed methods to a broader range of transformer variants including models with different normalization placements, attention mechanisms, and architectural choices to assess generalizability.

3. **Alternative Quantization Scheme Comparison:** Systematically compare the proposed methods against alternative quantization strategies including dynamic range estimation, different bitwidths, and post-training vs quantization-aware training.