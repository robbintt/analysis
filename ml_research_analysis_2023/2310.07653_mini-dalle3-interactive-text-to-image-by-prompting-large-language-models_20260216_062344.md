---
ver: rpa2
title: 'Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models'
arxiv_id: '2310.07653'
source_url: https://arxiv.org/abs/2310.07653
tags:
- image
- arxiv
- generation
- it2i
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces interactive text-to-image (iT2I), a new task
  where users can iteratively communicate with language models (LLMs) to generate,
  edit, and refine images using natural language instructions. This addresses the
  limitation of traditional text-to-image models that require complex, technical prompts.
---

# Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models

## Quick Facts
- arXiv ID: 2310.07653
- Source URL: https://arxiv.org/abs/2310.07653
- Reference count: 40
- Key outcome: Interactive text-to-image system using prompting techniques to augment LLMs with iT2I capabilities, working across different LLMs and T2I models without training

## Executive Summary
This paper introduces interactive text-to-image (iT2I), a new task where users can iteratively communicate with language models to generate, edit, and refine images using natural language instructions. The proposed Mini-DALLE3 approach leverages prompting techniques to transform iT2I into a multi-turn textual description generation task, using off-the-shelf T2I models for image generation and editing. The system supports multi-turn conversations, maintains content consistency, and can be composed with other LLM abilities like question answering. Experiments show that the approach works well across different LLMs and enables various use cases such as storytelling, concept prototyping, and logo design, requiring no training and only minimally impacting the LLMs' inherent capabilities.

## Method Summary
The Mini-DALLE3 approach uses prompting techniques to augment LLMs for iT2I by transforming the task into multi-turn textual description generation. The system consists of a router that analyzes the LLM's response and dispatches image generation requests, and an adapter that transforms image representations for T2I models. Few-shot prompts guide the LLM to generate image descriptions enclosed in special tags (e.g., `<image>` or `<edit>`), which are then parsed and fed into T2I models. A hierarchical content consistency control strategy uses different T2I models for varying levels of content changes - small changes use Prompt-to-prompt or MasaCtrl, while large changes use IP-Adapter. The method requires no training and preserves the LLM's general reasoning and QA capabilities while adding iT2I functionality.

## Key Results
- The approach successfully enables iT2I capabilities in existing language models without any training
- The system works across different LLMs (ChatGPT, LLaMA, Baichuan, InternLM) and T2I models (Stable Diffusion XL, DALL-E 3)
- Evaluation shows minimal degradation on LLM performance for unrelated tasks like question answering and code generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prompting LLMs with few-shot examples can transform the iT2I task into a multi-turn textual description generation task, leveraging the LLM's in-context learning ability.
- **Mechanism**: The LLM is prompted to generate image descriptions enclosed in special tags (e.g., `<image>` or `<edit>`). These descriptions are then parsed and fed into T2I models. The LLM's strong context understanding allows it to maintain conversational flow and interpret editing instructions.
- **Core assumption**: LLMs can reliably distinguish between generation and editing instructions based on context and the presence of special tags.
- **Evidence anchors**: [abstract] "We propose a simple approach that augments LLMs for iT2I with prompting techniques and off-the-shelf T2I models." [section] "We observe that this approach yields favorable outcomes. The LM successfully generates images accompanied by coherent textual responses..."
- **Break condition**: If the LLM fails to correctly identify when to use `<edit>` vs. `<image>` tags, leading to inappropriate image regeneration instead of refinement.

### Mechanism 2
- **Claim**: Hierarchical content consistency control using different T2I models for varying levels of content changes ensures both quality and consistency in multi-turn editing.
- **Mechanism**: Small changes (style, weighting) use Prompt-to-prompt or MasaCtrl; large changes use IP-Adapter. This allows precise control without retraining models.
- **Core assumption**: Off-the-shelf models can be effectively combined in a hierarchical fashion to handle different editing granularities.
- **Evidence anchors**: [section] "We introduce a hierarchical control strategy that utilizes different models for different levels of content changes." [section] "We utilize IP-Adapter to perform large content changes as these models are more flexible for the input textual prompts."
- **Break condition**: If the hierarchy introduces latency or quality degradation due to mismatched model outputs.

### Mechanism 3
- **Claim**: No training requirement and minimal impact on LLM's inherent abilities makes this approach widely applicable and cost-effective.
- **Mechanism**: By using prompting rather than fine-tuning, the approach preserves the LLM's general reasoning and QA capabilities while adding iT2I functionality.
- **Core assumption**: Prompting does not significantly degrade performance on unrelated tasks.
- **Evidence anchors**: [abstract] "without any training while bringing little degradation on LLMs' inherent capabilities in, e.g., question answering and code generation." [section] "Our results demonstrate that our approach can easily enable iT2I capabilities in any existing language model..." [section] "Evaluation results of different models on the subtasks of MMLU... it can be observed that the iT2I prompt only brings minor degradations."
- **Break condition**: If repeated iT2I interactions cause drift in the LLM's general reasoning abilities.

## Foundational Learning

- **Concept**: In-context learning in LLMs
  - Why needed here: The system relies on LLMs generating correct image descriptions without fine-tuning, based on few-shot examples.
  - Quick check question: Can the LLM generate a `<image>` tag description for "a red apple on a table" after seeing one or two examples?

- **Concept**: Diffusion model conditioning and prompt engineering
  - Why needed here: The adapter module transforms LLM outputs into prompts suitable for T2I models, requiring understanding of how text conditions image synthesis.
  - Quick check question: What happens to the generated image if you swap "cute" with "scary" in the prompt?

- **Concept**: Multimodal consistency in iterative generation
  - Why needed here: Multi-turn editing must preserve identity across iterations; otherwise, the conversation loses coherence.
  - Quick check question: If the user asks to "make the dog smaller," does the new image still depict the same dog?

## Architecture Onboarding

- **Component map**: LLM -> Router -> Adapter -> T2I Model(s); optionally LLM (prompt refinement) -> Adapter
- **Critical path**: User input -> LLM generation (with tags) -> Router dispatch -> Adapter transform -> T2I model -> Image output
- **Design tradeoffs**: Prompt-based augmentation vs. fine-tuning: lower cost and generality vs. potential lower precision in edge cases
- **Failure signatures**: 
  - LLM outputs malformed tags -> Router fails to parse
  - Adapter produces prompts incompatible with T2I model -> Generation errors
  - T2I model outputs low quality -> Poor user experience
- **First 3 experiments**:
  1. Single-turn generation: Prompt "Generate a cat" -> Verify `<image>` tag output and image quality
  2. Multi-turn editing: Start with cat -> "Make it bigger" -> Verify `<edit>` tag usage and consistency
  3. Cross-LLM robustness: Run same prompts on ChatGPT, LLaMA, Baichuan -> Compare tag detection and output quality

## Open Questions the Paper Calls Out
- How does the proposed iT2I approach compare in terms of computational efficiency and latency to existing text-to-image systems?
- What are the limitations of the iT2I approach when handling complex or ambiguous user instructions, and how can these be addressed?
- How does the iT2I approach scale with increasing complexity of image editing tasks, and what are the potential bottlenecks?

## Limitations
- The method's effectiveness depends heavily on the LLM's ability to correctly interpret and execute editing instructions through special tags, which may not generalize well across different conversational styles
- The hierarchical content consistency control strategy lacks empirical validation through comparative studies against single-model approaches
- The paper does not address potential scalability issues when handling very long conversations or the impact of model size on response quality

## Confidence

- **High Confidence**: The basic mechanism of using few-shot prompts to transform image generation into textual description generation is well-supported by LLM capabilities and has clear implementation paths.
- **Medium Confidence**: The hierarchical content consistency control strategy is theoretically sound but lacks direct experimental validation.
- **Low Confidence**: The claim about minimal impact on LLM's inherent capabilities needs more rigorous testing, particularly regarding long-term degradation across multiple interaction sessions.

## Next Checks
1. **Cross-LLM Consistency Test**: Run identical conversation sequences across multiple LLM providers (ChatGPT, Claude, Gemini) and measure consistency in tag generation and editing interpretation accuracy.

2. **Long Conversation Stability**: Execute extended multi-turn conversations (>10 turns) to evaluate content drift and tag interpretation consistency over time, measuring both image quality and conversation coherence.

3. **Edge Case Robustness**: Test the system with challenging editing scenarios including contradictory instructions, abstract concepts, and complex spatial relationships to identify failure modes in tag interpretation and content preservation.