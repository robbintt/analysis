---
ver: rpa2
title: 'FusDom: Combining In-Domain and Out-of-Domain Knowledge for Continuous Self-Supervised
  Learning'
arxiv_id: '2312.13026'
source_url: https://arxiv.org/abs/2312.13026
tags:
- pre-training
- fusdom
- learning
- speech
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FusDom is a simple and novel methodology for continuous self-supervised
  learning of speech representations that addresses the problem of catastrophic forgetting
  in continued pre-training on out-of-domain distributions. FusDom employs two identical
  pre-trained SSL models, a teacher and a student, with a modified pre-training head
  to solve the CP SSL pre-text task.
---

# FusDom: Combining In-Domain and Out-of-Domain Knowledge for Continuous Self-Supervised Learning

## Quick Facts
- **arXiv ID:** 2312.13026
- **Source URL:** https://arxiv.org/abs/2312.13026
- **Reference count:** 0
- **Key outcome:** FusDom significantly outperforms all baselines across settings, with WER improvements in the range of 0.2 WER - 7.3 WER in the target domain while retaining performance in the earlier domain.

## Executive Summary
FusDom is a novel methodology for continuous self-supervised learning of speech representations that addresses catastrophic forgetting during continued pre-training on out-of-domain distributions. The approach employs two identical pre-trained SSL models—a frozen teacher and an updated student—connected through a cross-attention pre-training head. This architecture enables the student to learn domain-adapted representations while retaining knowledge from previously encountered domains, achieving significant WER improvements across multiple Indian language datasets.

## Method Summary
FusDom uses two identical pre-trained SSL models (teacher and student) with a modified pre-training head that employs cross-attention between their representations. The teacher remains frozen while the student receives gradient updates during continued pre-training. The cross-attention mechanism uses teacher representations as queries and student representations as keys/values, forcing the student to align with the teacher's in-domain knowledge while adapting to the target domain. After continued pre-training, the student model is fine-tuned for ASR using either CTC or as a feature extractor for an Encoder-Decoder model.

## Key Results
- FusDom achieves WER improvements ranging from 0.2 to 7.3 compared to baselines across target domains
- The method successfully retains performance on earlier domains while adapting to new ones
- Significant improvements observed across multiple Indian language datasets including MSR Gujarati, MSR Tamil, MSR Telugu, and Gramvani Hindi

## Why This Works (Mechanism)

### Mechanism 1
FusDom prevents catastrophic forgetting by making the student model's representations aware of the teacher's in-domain knowledge during continued pre-training. The cross-attention pre-training head uses teacher representations as queries and student representations as keys/values, forcing the student to align its outputs with the frozen teacher's representations while still adapting to the target domain. This assumes neural networks have sufficient capacity to retain information about all domains they encounter during continued SSL.

### Mechanism 2
The teacher model acts as a stable reference point that preserves knowledge from previous domains. By keeping the teacher frozen while only updating the student, FusDom ensures that the student cannot overwrite previously learned representations that the teacher maintains. This creates a stable reference point for knowledge preservation through the dual-model approach.

### Mechanism 3
The cross-domain attention mechanism allows the model to solve the SSL pretext task in an "out-of-domain-aware" fashion. By incorporating both in-domain (teacher) and out-of-domain (student) representations into the pretext task solution, the model learns representations that are simultaneously adapted to the target domain while retaining knowledge of previous domains.

## Foundational Learning

- **Self-Supervised Learning (SSL) in speech**: FusDom builds upon existing SSL frameworks like Wav2Vec2, XLSR, and Vakyansh, modifying their pre-training heads for continued pre-training. *Quick check:* What is the primary difference between contrastive learning and reconstruction-based SSL approaches in speech?

- **Catastrophic forgetting in continual learning**: The paper addresses the fundamental problem of neural networks losing previously acquired knowledge when trained on new, non-IID data. *Quick check:* How does the IID assumption violation lead to catastrophic forgetting in neural network training?

- **Cross-attention mechanisms in transformers**: FusDom's key innovation is the modified pre-training head that uses cross-attention between teacher and student representations. *Quick check:* In standard transformer attention, what are the roles of queries, keys, and values?

## Architecture Onboarding

- **Component map**: Pre-trained SSL model → Teacher (frozen) + Student (updated) → Cross-attention pre-training head → ASR fine-tuning (CTC/Encoder-Decoder)

- **Critical path**: 1) Initialize teacher and student with pre-trained SSL model, 2) For each domain, perform continued pre-training using cross-attention head, 3) Fine-tune the student model on target ASR dataset, 4) Evaluate performance on both current and previous domains

- **Design tradeoffs**: Memory overhead of two model copies vs. single model; training complexity of additional cross-attention vs. simpler adaptation; domain relevance of teacher's frozen knowledge vs. student's adaptation flexibility

- **Failure signatures**: WER degradation on previous domains indicates forgetting; minimal WER improvement on target domain suggests insufficient adaptation; training instability or convergence issues

- **First 3 experiments**: 1) Replicate baseline results (no continued pre-training) on a simple domain adaptation task, 2) Implement vanilla continued pre-training and compare with FusDom, 3) Test FusDom with different teacher-student initialization strategies (random vs. pre-trained)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several important questions unresolved regarding scalability to larger numbers of domains, performance with different domain orderings, and comparison with other continual learning methods.

## Limitations

- The mechanism relies heavily on cross-attention architecture without extensive ablation studies to isolate its specific contribution
- Evaluation focuses primarily on WER metrics without deeper analysis of representation quality or robustness to domain shifts
- The paper assumes sufficient model capacity to retain information about all encountered domains without empirical validation

## Confidence

**High Confidence Claims:**
- FusDom outperforms baselines across all tested settings (Section 5.2)
- The dual-model architecture with frozen teacher and updated student is correctly implemented (Section 3.2)
- Significant WER improvements are observed across domain adaptation scenarios (Section 5.3)

**Medium Confidence Claims:**
- The cross-attention mechanism effectively prevents catastrophic forgetting (Section 4.1)
- FusDom's approach is more effective than alternative regularization methods (Section 5.4)
- The method scales well across different pre-trained model families (Section 5.2)

**Low Confidence Claims:**
- The specific cross-attention architecture is optimal for this task (Section 3.2)
- FusDom's performance generalizes to domains beyond those tested (Section 5.5)
- The method would maintain effectiveness with significantly larger domain gaps

## Next Checks

1. **Ablation Study**: Remove the cross-attention component and compare performance with the full FusDom architecture to quantify the specific contribution of the cross-attention mechanism to preventing catastrophic forgetting.

2. **Memory Efficiency Analysis**: Measure and report the actual memory overhead of maintaining two model copies, and evaluate whether performance degrades when using smaller student models relative to the teacher.

3. **Generalization Test**: Evaluate FusDom on a significantly different domain pair (e.g., speech to non-speech audio or cross-lingual adaptation between unrelated language families) to assess robustness beyond the tested Indian language datasets.