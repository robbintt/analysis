---
ver: rpa2
title: Fully Hyperbolic Convolutional Neural Networks for Computer Vision
arxiv_id: '2303.15919'
source_url: https://arxiv.org/abs/2303.15919
tags:
- hyperbolic
- lorentz
- euclidean
- space
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first fully hyperbolic convolutional neural
  network (HCNN) for computer vision tasks. It extends the Lorentz model with novel
  formulations of the convolutional layer, batch normalization, and multinomial logistic
  regression, enabling fully hyperbolic architectures.
---

# Fully Hyperbolic Convolutional Neural Networks for Computer Vision

## Quick Facts
- arXiv ID: 2303.15919
- Source URL: https://arxiv.org/abs/2303.15919
- Authors: 
- Reference count: 40
- Key outcome: This paper proposes the first fully hyperbolic convolutional neural network (HCNN) for computer vision tasks. It extends the Lorentz model with novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression, enabling fully hyperbolic architectures. Experiments on image classification and generation tasks demonstrate that HCNN outperforms hybrid and Euclidean baselines, with improved stability and performance. The Lorentz model shows superior properties compared to the Poincaré ball for HNNs. The authors believe HCNN provides a foundation for developing more powerful hyperbolic neural networks that can better represent complex hierarchical structures in visual data.

## Executive Summary
This paper introduces the first fully hyperbolic convolutional neural network (HCNN) for computer vision, overcoming limitations of hybrid approaches that mix Euclidean and hyperbolic representations. The authors develop novel Lorentz model-based implementations of convolutional layers, batch normalization, and multinomial logistic regression, enabling end-to-end hyperbolic architectures. Experiments on image classification and generation tasks demonstrate that HCNN outperforms both hybrid HNNs and purely Euclidean baselines, with particular advantages in stability and performance. The Lorentz model is shown to provide better numerical properties than the Poincaré ball representation.

## Method Summary
The paper proposes HCNN, a fully hyperbolic convolutional neural network based on the Lorentz model of hyperbolic geometry. The key innovations include Lorentz convolutional layers using hyperbolic correlation operations, Lorentz batch normalization with efficient centroid computation and parallel transport, and Lorentz multinomial logistic regression for classification. The architecture replaces all Euclidean components with hyperbolic counterparts, enabling direct learning of hierarchical representations in hyperbolic space. The method is evaluated on image classification (CIFAR-10, CIFAR-100, Tiny-ImageNet) and generation (CelebA) tasks, comparing fully hyperbolic, hybrid, and Euclidean approaches.

## Key Results
- HCNN outperforms hybrid and Euclidean baselines on image classification tasks across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets
- Lorentz model provides superior numerical stability compared to Poincaré ball representation in hyperbolic neural networks
- Fully hyperbolic architecture mitigates gradient vanishing issues present in hybrid approaches that mix Euclidean and hyperbolic features
- Lorentz batch normalization with closed-form centroid computation offers computational advantages over Fréchet mean-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lorentz model provides superior numerical stability compared to Poincaré ball in hyperbolic neural networks.
- Mechanism: The Lorentz model uses 64-bit precision but achieves comparable stability to 32-bit Poincaré ball, while the Poincaré ball requires 64-bit precision for similar stability. This is because the Lorentz model's closed-form formulae for exponential/logarithmic maps and distance functions are less prone to rounding errors in floating-point arithmetic.
- Core assumption: The exponential growth of volume with radius in hyperbolic space introduces numerical instability that is better managed by the Lorentz model's mathematical formulation.
- Evidence anchors:
  - [abstract]: "Due to its hard numerical constraint, the Poincaré ball is much more susceptible to numerical instability than the Lorentz model"
  - [section 2]: "we forgo the typically used Poincaré ball in favor of the Lorentz model, as it is considered to offer better properties for stability and optimization"
  - [corpus]: No direct evidence about numerical stability comparison found
- Break condition: If experiments show Poincaré ball matching or exceeding Lorentz stability with proper optimization techniques, or if Lorentz model introduces other numerical issues not present in Poincaré ball.

### Mechanism 2
- Claim: Fully hyperbolic architecture outperforms hybrid architectures by learning latent representations directly in hyperbolic space throughout the network.
- Mechanism: By replacing all Euclidean components with Lorentz counterparts, the network can leverage hyperbolic geometry properties at every layer rather than only at task heads. This avoids gradient vanishing issues that occur when mixing Euclidean and hyperbolic features, and allows the network to learn hierarchical structures more effectively throughout the feature extraction process.
- Core assumption: Hierarchical structures in visual data can be better captured when all network layers operate in the same geometric space rather than mixing Euclidean and hyperbolic representations.
- Evidence anchors:
  - [abstract]: "current methods in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry"
  - [section 4]: "Our HCNN naturally learns latent hyperbolic feature representations in every layer, mitigating these issues"
  - [corpus]: No direct evidence about hybrid vs fully hyperbolic performance comparison found
- Break condition: If hybrid architectures with carefully tuned feature clipping and projection strategies match or exceed fully hyperbolic performance, or if the computational overhead of fully hyperbolic networks outweighs their benefits.

### Mechanism 3
- Claim: Lorentz batch normalization using Lorentzian centroid and parallel transport operations provides more efficient normalization than Fréchet mean-based approaches.
- Mechanism: The Lorentzian centroid can be calculated efficiently in closed form using weighted sums, while Fréchet mean requires iterative optimization. The parallel transport operation preserves variance while re-centering, and the special case of transporting to/from origin allows efficient computation compared to single parallel transport operations.
- Core assumption: Efficient batch normalization is critical for training deep hyperbolic networks, and the Lorentzian centroid provides a computationally tractable alternative to Fréchet mean while maintaining mathematical rigor.
- Evidence anchors:
  - [section 4.2]: "we propose to use the centroid with respect to the squared Lorentzian distance, which can be calculated efficiently in closed form"
  - [section 4.2]: "transporting towards and from the origin are special cases that can be computed efficiently, reducing the overhead significantly"
  - [corpus]: No direct evidence about normalization efficiency comparison found
- Break condition: If Fréchet mean-based approaches become computationally tractable through optimization, or if Lorentz batch normalization introduces other training instabilities.

## Foundational Learning

- Concept: Hyperbolic geometry and Riemannian manifolds
  - Why needed here: Understanding the mathematical foundation of hyperbolic spaces, their curvature properties, and how they differ from Euclidean geometry is essential for implementing and debugging HCNN components.
  - Quick check question: What is the relationship between the curvature K and the distance scaling in hyperbolic space, and how does this enable better representation of hierarchical structures?

- Concept: Lorentz model vs Poincaré ball representations
  - Why needed here: The paper specifically uses the Lorentz model, requiring understanding of its mathematical formulation, operations, and advantages over alternative hyperbolic models.
  - Quick check question: How do you convert between Lorentz and Poincaré representations, and what are the computational trade-offs of each approach?

- Concept: Parallel transport and exponential/logarithmic maps
  - Why needed here: These operations are fundamental to implementing hyperbolic neural network components like batch normalization and residual connections.
  - Quick check question: How does parallel transport preserve geometric properties, and why is it necessary for operations like batch normalization in hyperbolic space?

## Architecture Onboarding

- Component map: Input → Lorentz conv layers → Lorentz BN → Lorentz activations → Repeat → Lorentz MLR output
- Critical path: Input → Lorentz conv layers → Lorentz BN → Lorentz activations → Repeat → Lorentz MLR output
- Design tradeoffs: Fully hyperbolic vs hybrid architectures (performance vs implementation complexity), Lorentz vs Poincaré models (stability vs existing implementations), feature map dimensionality (expressiveness vs computational cost)
- Failure signatures: NaN values in training (numerical instability), gradient vanishing (improper mixing of Euclidean/hyperbolic spaces), poor convergence (incorrect curvature parameters)
- First 3 experiments:
  1. Implement basic Lorentz fully connected layer and verify it produces valid hyperbolic points
  2. Replace one Euclidean conv layer in a simple CNN with Lorentz conv and compare training stability
  3. Test Lorentz batch normalization on synthetic hyperbolic data and verify mean/variance statistics are preserved under parallel transport

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fully hyperbolic architectures compare to hybrid approaches when using more complex CNN architectures beyond ResNet-18?
- Basis in paper: [inferred] The paper only tested HCNN-ResNet on ResNet-18 architecture, leaving open whether the performance gains generalize to deeper or more complex models.
- Why unresolved: The experiments were limited to ResNet-18, which may not capture the full potential of fully hyperbolic networks.
- What evidence would resolve it: Comparative experiments on ResNet-50/101 and other CNN architectures like DenseNet or EfficientNet would clarify if the performance benefits extend to larger models.

### Open Question 2
- Question: What is the optimal curvature K for different vision tasks and datasets, and does learning K during training consistently improve performance?
- Basis in paper: [explicit] The ablation study shows that learning K does not show to be effective in the experimental setting, but this needs further investigation.
- Why unresolved: The paper found that learning K did not improve performance in their experiments, but this could be due to limited hyperparameter tuning or task selection.
- What evidence would resolve it: Systematic experiments across diverse vision tasks with comprehensive hyperparameter search for K would determine if learned curvature can be beneficial.

### Open Question 3
- Question: How does the computational overhead of fully hyperbolic networks scale with increasing model depth and input resolution?
- Basis in paper: [explicit] The paper notes that "two major drawbacks of HNNs are relatively high runtime and memory requirements" and that "hybrid HNNs only add little overhead compared to the significantly slower HCNN."
- Why unresolved: While the paper identifies the runtime issue, it doesn't provide a systematic analysis of how computational costs scale with model complexity.
- What evidence would resolve it: Benchmarking HCNN performance across different model depths, input resolutions, and hardware configurations would quantify the scaling behavior and identify practical limitations.

## Limitations
- Limited empirical validation on larger-scale datasets beyond CIFAR/Tiny-ImageNet, with no ImageNet-scale experiments to confirm scalability
- No ablation studies comparing Lorentz vs Poincaré models under identical training conditions with comprehensive hyperparameter sweeps
- Missing analysis of computational overhead relative to accuracy gains across different network depths and dataset complexities

## Confidence
- **High confidence**: Numerical stability advantages of Lorentz over Poincaré models (supported by mathematical formulation and cited literature)
- **Medium confidence**: Performance benefits of fully hyperbolic vs hybrid architectures (based on limited experimental evidence, requires broader validation)
- **Low confidence**: Generalization claims to diverse visual tasks (primarily validated on classification and generation tasks only)

## Next Checks
1. Scale experiments to ImageNet to verify that Lorentz model stability advantages persist at higher resolution and class complexity
2. Conduct controlled ablation studies comparing Lorentz and Poincaré implementations with identical optimization strategies and hyperparameter tuning
3. Measure and report computational overhead (FLOPs, memory) across network depths to quantify the efficiency-performance tradeoff