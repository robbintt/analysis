---
ver: rpa2
title: 'A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing'
arxiv_id: '2310.08433'
source_url: https://arxiv.org/abs/2310.08433
tags:
- llms
- ignatius
- stories
- human
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models were evaluated on creative writing using
  a challenging zero-shot task involving a combat between Ignatius J. Reilly and a
  pterodactyl.
---

# A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing

## Quick Facts
- arXiv ID: 2310.08433
- Source URL: https://arxiv.org/abs/2310.08433
- Reference count: 40
- Large language models were evaluated on creative writing using a challenging zero-shot task involving a combat between Ignatius J. Reilly and a pterodactyl.

## Executive Summary
This study evaluates 12 large language models and 5 human writers on creative writing using a zero-shot prompt about a combat between Ignatius J. Reilly and a pterodactyl. Human raters scored stories on 10 rubric criteria including fluency, creativity, humor, and style. Commercial LLMs (GPT-4, Claude, Bing) matched or slightly outperformed human writers on most criteria, while humans excelled in creativity and humor. Open-source models lagged significantly behind commercial offerings. The results suggest LLMs can produce competent creative writing but humans retain an edge in originality and humor generation.

## Method Summary
The study evaluated 12 LLMs (including GPT-4, Claude, Bing Chat, ChatGPT with GPT-3.5/4, Alpaca, Bard, Dolly, GPT4All-J, Koala, OpenAssistant, StableLM, and Vicuna) and 5 human writers on a zero-shot creative writing task. Each LLM generated 5 stories using the same prompt about a combat between Ignatius J. Reilly and a pterodactyl. Human evaluators rated stories using a 10-item rubric (overall readability, narrative elements, structural elements, plot logic, creativity, style, epic genre, combat description, character accuracy, humor), each scored 1-10 for an overall score out of 100. Inter-rater agreement was measured using weighted Cohen's kappa.

## Key Results
- Commercial LLMs (GPT-4, Claude, Bing) achieved average scores between 6-6.5, outperforming humans on most rubric items
- Humans excelled in creativity and humor, showing a binary divide with models that could handle these elements versus those that could not
- Open-source models lagged significantly, with the best (Koala) achieving 60.0 overall score compared to GPT-4's 80.2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms humans on most rubric items due to emergent humor and coherence abilities in large models
- Mechanism: Larger parameter counts and instruction-tuning enable models to generate contextually coherent narratives with style mimicry, creativity, and humor
- Core assumption: Performance differences arise from model scale and training data diversity, not just prompt quality
- Evidence anchors:
  - [abstract] "Humans excelled in creativity and humor, which showed a binary divide between models that could handle it and those that could not"
  - [section] "Claude, Bing and GPT-4... obtain average scores between 6 and 6.5; whereas the rest of the models achieve very low scores of 3.4 or less"
  - [corpus] Weak—corpus only shows related humor-generation papers, no direct evidence for humor emergence in large models
- Break Condition: If model scale is reduced or training data lacks humor examples, performance drops sharply

### Mechanism 2
- Claim: Commercial LLMs consistently outperform open-source models due to proprietary fine-tuning and RLHF
- Mechanism: Reinforcement Learning from Human Feedback and specialized instruction datasets improve instruction-following and stylistic adherence
- Core assumption: Commercial models have access to higher-quality alignment data and iterative feedback loops
- Evidence anchors:
  - [abstract] "Commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind"
  - [section] "The best performances correspond to commercial offerings... Open-source models are clearly behind, with the best (Koala) achieving 60.0 overall score, contrasting with the 80.2 obtained by GPT-4"
  - [corpus] Weak—corpus lists related evaluation studies but lacks direct comparison of commercial vs open-source training pipelines
- Break Condition: If open-source models receive comparable alignment fine-tuning, performance gap narrows

### Mechanism 3
- Claim: Humor detection and generation is a binary emergent capability in LLMs, not a gradual improvement
- Mechanism: Humor requires nuanced contextual understanding and incongruity resolution, which only emerge in larger models
- Core assumption: Humor is a high-level cognitive task requiring cross-domain knowledge and cultural context
- Evidence anchors:
  - [abstract] "Humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it"
  - [section] "There is a rather stark binary divide... Claude, Bing and GPT-4, together with the human writers, obtain average scores between 6 and 6.5; whereas the rest of the models achieve very low scores of 3.4 or less"
  - [corpus] Weak—corpus contains related humor studies but no quantitative evidence for binary emergence
- Break Condition: If task is reframed or simplified, binary divide may disappear

## Foundational Learning

- Concept: Zero-shot instruction following
  - Why needed here: Models are prompted without in-context examples, relying solely on alignment training to interpret and execute the task
  - Quick check question: What training method allows a model to perform well on unseen tasks without examples in the prompt?

- Concept: Inter-rater agreement and reliability
  - Why needed here: Human evaluation is subjective; understanding agreement metrics ensures validity of comparative results
  - Quick check question: What statistical measure assesses consistency between two independent raters on ordinal scales?

- Concept: Creative writing rubric design
  - Why needed here: Rubric must capture multiple dimensions of writing quality, including creativity, style, coherence, and humor
  - Quick check question: Why is a holistic scale preferred over purely analytic scoring in creative writing assessment?

## Architecture Onboarding

- Component map: Prompt generation → LLM inference → Story output → Human evaluation → Rubric scoring → Statistical analysis
- Critical path: Prompt → LLM → Output → Evaluation
- Design tradeoffs: Zero-shot setting maximizes generalizability but limits control over output; human evaluation ensures qualitative depth but is resource-intensive
- Failure signatures: Low inter-rater agreement indicates ambiguous rubric; poor LLM performance on humor indicates lack of training data or scale
- First 3 experiments:
  1. Run same prompt on multiple model sizes to map performance scaling
  2. Test humor-specific prompts to isolate humor capability
  3. Compare zero-shot vs few-shot prompts to quantify in-context learning benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of humor do LLMs struggle with, and what distinguishes the few models that succeed from those that fail?
- Basis in paper: [explicit] The paper identifies humor as a challenging aspect where most LLMs fail but a few (GPT-4, Claude, Bing) succeed comparably to humans
- Why unresolved: The study only identifies the binary divide in humor performance but doesn't analyze the specific elements of humor (e.g., wordplay, situational irony, dark humor) that different models handle
- What evidence would resolve it: Comparative analysis of humor elements in successful vs. unsuccessful stories, plus ablation studies on humor-specific prompts

### Open Question 2
- Question: How do different levels of "epicness" in stories correlate with specific narrative techniques or vocabulary choices?
- Basis in paper: [explicit] Rubric item 7 (understanding and habitation of the epic genre) shows significant variation between models, with ChatGPT versions outperforming humans
- Why unresolved: The paper identifies which models excel at epic narration but doesn't specify what narrative elements (e.g., elevated diction, heroic themes, mythic references) contribute to higher ratings
- What evidence would resolve it: Linguistic analysis of successful epic stories to identify common features, plus controlled experiments varying these elements

### Open Question 3
- Question: What is the relationship between model size and performance on creative writing tasks?
- Basis in paper: [inferred] The study shows commercial LLMs outperforming open-source models, but doesn't explicitly analyze the correlation between parameter count and creative writing ability
- Why unresolved: The paper doesn't provide a systematic comparison of model sizes and their corresponding creative writing performance across all evaluated models
- What evidence would resolve it: Regression analysis correlating model parameters with rubric scores, controlling for other factors like training data and fine-tuning approaches

## Limitations
- The study relies on a single creative writing prompt, limiting generalizability to other genres or styles
- Human evaluation involved a small pool of raters (10) and student writers (5), potentially limiting statistical power
- The study does not control for creative writing experience level of human writers, which could confound comparisons with LLM outputs

## Confidence
- **High Confidence**: Commercial LLMs (GPT-4, Claude, Bing) consistently outperform open-source models across multiple rubric criteria
- **Medium Confidence**: The binary divide in humor performance is real but may be partially attributed to prompt-specific factors
- **Low Confidence**: Claims about emergent humor capabilities being strictly binary rather than gradual improvements

## Next Checks
1. Replicate the evaluation with 3-5 diverse creative writing prompts across different genres (horror, romance, mystery) to determine if the commercial vs open-source performance gap persists across varied creative tasks
2. Repeat the evaluation with human writers categorized by experience level (novice, intermediate, expert) to isolate the impact of human skill on performance comparisons with LLMs
3. Systematically test the same prompt across multiple model sizes within the same architecture family (e.g., LLaMA-7B, LLaMA-13B, LLaMA-33B) to map the precise relationship between model scale and creative writing performance, particularly for humor generation