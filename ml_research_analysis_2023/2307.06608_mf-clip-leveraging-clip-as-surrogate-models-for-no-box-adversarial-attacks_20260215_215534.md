---
ver: rpa2
title: 'MF-CLIP: Leveraging CLIP as Surrogate Models for No-box Adversarial Attacks'
arxiv_id: '2307.06608'
source_url: https://arxiv.org/abs/2307.06608
tags:
- surrogate
- adversarial
- clip
- attack
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MF-CLIP, a framework that enhances CLIP's
  effectiveness as a surrogate model for no-box adversarial attacks by addressing
  its lack of discriminative capability through margin-based feature space optimization.
  The method fine-tunes CLIP using target images and a margin-based loss function,
  then applies basic FGSM attacks.
---

# MF-CLIP: Leveraging CLIP as Surrogate Models for No-box Adversarial Attacks

## Quick Facts
- **arXiv ID**: 2307.06608
- **Source URL**: https://arxiv.org/abs/2307.06608
- **Reference count**: 37
- **Primary result**: Improves attack success rates by 15.23% over standard models and 9.52% over adversarially trained models

## Executive Summary
MF-CLIP addresses the challenge of using CLIP as an effective surrogate model for no-box adversarial attacks by enhancing its discriminative capability at the feature level. The framework fine-tunes CLIP using margin-based loss functions on target images, then applies basic FGSM attacks to generate adversarial examples. The method demonstrates significant improvements in attack success rates while highlighting surrogate model selection as a critical factor in no-box attack performance.

## Method Summary
MF-CLIP fine-tunes CLIP's image encoder using margin-based loss functions (specifically ArcFace) on target images that are not part of the target model's training set. After fine-tuning, the method generates adversarial examples using FGSM attacks in the feature space. The framework evaluates attack success rates on target models including ResNet-18, EfficientNet-B0, and RegNetX-1.6GF. The approach focuses on creating larger inter-class margins in CLIP's feature space to improve transferability of adversarial examples to target models.

## Key Results
- Achieves 15.23% improvement in attack success rate over standard models
- Outperforms adversarially trained models by 9.52%
- Shows effectiveness across both Pets and Cars datasets
- Demonstrates that basic FGSM attacks on fine-tuned CLIP can match or exceed more complex baseline methods

## Why This Works (Mechanism)

### Mechanism 1
CLIP's contrastive training creates high representational capacity but narrow inter-class margins in feature space. The contrastive loss focuses on image-text alignment rather than maximizing class separation, resulting in features that are difficult to exploit for adversarial attacks.

### Mechanism 2
Margin-based loss functions like ArcFace can reorganize CLIP's feature space to create larger inter-class margins while maintaining class compactness. This enhances discriminative capability at the feature level, making it more suitable for generating transferable adversarial examples.

### Mechanism 3
In no-box settings where attackers have no access to target model architecture or training data, surrogate model selection becomes the dominant factor in attack success. Fine-tuned foundation models provide better transferability by balancing broad representational capability with discriminative features.

## Foundational Learning

- **Concept**: Contrastive learning vs supervised classification
  - Why needed here: Explains why CLIP's training objective leads to representational but not discriminative features
  - Quick check question: What is the key difference between contrastive loss (used in CLIP) and cross-entropy loss (used in standard classification models) in terms of feature space organization?

- **Concept**: Non-robust features and adversarial examples
  - Why needed here: Frames adversarial noise as "non-robust features" and explains why models need to recognize target images to generate valid non-robust features
  - Quick check question: How does the concept of "non-robust features" explain why a model unfamiliar with cats cannot generate effective adversarial examples targeting cat images?

- **Concept**: Transferability in adversarial attacks
  - Why needed here: Explains the assumption that features learned by surrogate models transfer to fool target models
  - Quick check question: Why does the paper suggest that greater divergence between surrogate and target model knowledge leads to weaker transferability of adversarial examples?

## Architecture Onboarding

- **Component map**: CLIP image encoder → Margin-based loss head → FGSM attack generation → Attack success evaluation
- **Critical path**: CLIP → Margin-based fine-tuning → FGSM attack generation → Attack success evaluation
- **Design tradeoffs**: Computational cost of fine-tuning CLIP vs using native CLIP; choice between different margin-based loss functions; trade-off between fine-tuning dataset size and attack effectiveness; backbone architecture selection (ResNet vs ViT)
- **Failure signatures**: Low attack success rates despite fine-tuning; overfitting during fine-tuning; degraded performance on original classification tasks; poor transferability when target model architecture differs significantly
- **First 3 experiments**:
  1. Compare attack success rates using native CLIP vs fine-tuned CLIP on a small dataset (e.g., Pets) with FGSM attack
  2. Test different margin-based loss functions (ArcFace, AdaCos, CircleLoss) on the same dataset to identify optimal choice
  3. Vary the percentage of target images used for fine-tuning (e.g., 12.5%, 25%, 50%, 100%) to determine minimum effective dataset size

## Open Questions the Paper Calls Out

### Open Question 1
How does MF-CLIP's performance compare to white-box and black-box attack settings? The paper mentions that no-box attack success rates are much lower than white-box and black-box settings but does not provide direct comparisons.

### Open Question 2
Can other margin-based loss functions beyond those tested further improve MF-CLIP's performance? The paper tested five margin-based losses but suggests there may be room for further improvement with additional loss functions.

### Open Question 3
How does MF-CLIP's performance scale with larger or more diverse target image datasets? Experiments were limited to two relatively small datasets, leaving questions about generalizability to larger-scale or more varied datasets.

## Limitations

- Experiments conducted on only two datasets (Pets and Cars) and three target models, limiting generalizability
- No quantification of computational overhead for fine-tuning CLIP for each target dataset
- Absence of ablation studies on the margin-based loss component to isolate its specific contribution

## Confidence

- **High confidence**: CLIP's contrastive training creates high representational capacity but limited discriminative capability at the feature level
- **Medium confidence**: Effectiveness of margin-based fine-tuning in improving attack success rates (limited experimental scope)
- **Low confidence**: Claim that surrogate model selection is now the "dominant factor" in no-box attack performance compared to other attack settings

## Next Checks

1. Replicate experiments across 5-7 additional datasets spanning different visual domains to assess generalizability beyond Pets and Cars
2. Evaluate attack transferability when targeting models with architectures significantly different from CLIP's backbone
3. Conduct systematic comparison of ArcFace against alternative margin-based losses and traditional classification losses to quantify the specific contribution of margin-based approaches