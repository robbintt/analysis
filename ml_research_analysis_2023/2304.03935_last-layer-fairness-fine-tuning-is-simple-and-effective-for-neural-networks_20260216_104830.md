---
ver: rpa2
title: Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks
arxiv_id: '2304.03935'
source_url: https://arxiv.org/abs/2304.03935
tags:
- fairness
- dataset
- ft-l
- neural
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple but novel framework to train fair
  neural networks in an efficient and inexpensive way - last-layer fine-tuning alone
  can effectively promote fairness in deep neural networks. This framework offers
  valuable insights into representation learning for training fair neural networks.
---

# Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks

## Quick Facts
- arXiv ID: 2304.03935
- Source URL: https://arxiv.org/abs/2304.03935
- Reference count: 26
- Key outcome: Last-layer fine-tuning alone can effectively promote fairness in deep neural networks by avoiding overfitting to fairness criteria

## Executive Summary
This paper proposes a simple but novel framework to train fair neural networks efficiently by demonstrating that last-layer fine-tuning alone can effectively promote fairness. The authors observe that over-parameterization of neural networks leads to overfitting of fairness criteria, as flexible decision boundaries can memorize spurious fairness patterns. By fixing the representation learned through standard ERM training and only updating the last layer with fairness constraints on a balanced dataset, the method avoids these overfitting problems while maintaining computational efficiency.

## Method Summary
The method involves first training a full neural network using empirical risk minimization on imbalanced data, then extracting the representation from the penultimate layer and fine-tuning only the last layer on a small balanced dataset with fairness constraints. The approach leverages the observation that standard ERM training can already encode core task-relevant features well, and only requires a linear transformation (last layer) to recover these features for fair prediction. This avoids the overfitting problems associated with adding fairness constraints during full-network training.

## Key Results
- Last-layer fine-tuning with fairness constraints achieves competitive fairness metrics while maintaining accuracy on CelebA and CheXpert datasets
- The method demonstrates significant improvements in generalization gap compared to full-network training with fairness constraints
- Surgical fine-tuning of intermediate blocks shows diminishing returns compared to last-layer-only fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-parameterization causes fairness overfitting when constraints are applied during full-network training
- Mechanism: Large neural networks have highly flexible decision boundaries that can memorize fairness criteria for training data but fail to generalize
- Core assumption: The model's capacity exceeds the effective complexity needed for the task, enabling memorization of spurious fairness patterns
- Evidence anchors:
  - [abstract] "Recent research has shown that adding fairness constraints to the objective function leads to severe over-fitting to fairness criteria in large models"
  - [section] "Cherepanova et al. [2] observe that large models overfit to fairness objectives and produce a range of unintended and undesirable consequences"
  - [corpus] Weak evidence - no direct mention of overfitting mechanisms in related papers
- Break condition: When the representation learning phase fails to capture core features, or when the last layer requires significant non-linear transformations

### Mechanism 2
- Claim: Standard ERM training on imbalanced data still learns core features relevant to the task
- Mechanism: The representation layer captures task-relevant patterns that remain useful even when minority groups are underrepresented in training
- Core assumption: Core discriminative features exist and can be learned even with class imbalance, as long as the minority class is not completely absent
- Evidence anchors:
  - [section] "We also explore whether fine-tuning other structures beyond the last layer of neural networks can perform well"
  - [section] "Our Inspiration: standard training can still learn core features on imbalanced datasets"
  - [corpus] Weak evidence - related papers focus on fairness but not on feature learning in imbalanced settings
- Break condition: When minority groups are too small to provide meaningful signal, or when features are entirely spurious

### Mechanism 3
- Claim: Last-layer fine-tuning with data reweighting corrects group-level biases without overfitting
- Mechanism: By fixing the representation and only adjusting the linear classifier, the model can learn balanced decision boundaries using a small balanced dataset
- Core assumption: The representation contains sufficient information for fair classification, and a linear classifier can learn to separate groups fairly
- Evidence anchors:
  - [abstract] "We conduct comprehensive experiments on two popular image datasets with state-of-art architectures under different fairness notions to show that last-layer fine-tuning is sufficient for promoting fairness"
  - [section] "We then fixΦ and improve model fairness through last-layer fine-tuning by incorporating fairness constraints (Eq. 2 & 4 & 6) and data reweighting"
  - [corpus] Weak evidence - related work focuses on last-layer retraining but not specifically for fairness
- Break condition: When the representation is too biased or when non-linear transformations are required for fairness

## Foundational Learning

- Concept: Empirical Risk Minimization (ERM)
  - Why needed here: ERM is used to train the initial representation before fine-tuning for fairness
  - Quick check question: What is the objective function minimized during standard ERM training?

- Concept: Fairness metrics (EO, AE, MMF)
  - Why needed here: The paper evaluates fairness through multiple notions including Equalized Odds, Accuracy Equality, and Max-Min Fairness
  - Quick check question: How does Equalized Odds differ from Demographic Parity in terms of conditional independence?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The method relies on pre-training a representation and then fine-tuning the last layer for fairness
  - Quick check question: What is the key difference between fine-tuning all layers versus only the last layer?

## Architecture Onboarding

- Component map:
  - Representation network (Φ): Fixed after ERM training
  - Last layer (w): Updated during fine-tuning with fairness constraints
  - Balanced dataset: Small subset used for fine-tuning
  - Fairness constraints: Applied only during fine-tuning phase

- Critical path:
  1. Train full network with ERM on imbalanced dataset
  2. Extract representation Φ from penultimate layer
  3. Create balanced subset from training and validation data
  4. Fine-tune last layer with fairness constraints on balanced data
  5. Evaluate on test set

- Design tradeoffs:
  - Pros: Computationally efficient, avoids overfitting, leverages pre-training benefits
  - Cons: Requires good initial representation, may not work if representation is too biased
  - Alternatives: Full network fine-tuning (computationally expensive), surgical fine-tuning of multiple blocks

- Failure signatures:
  - Poor fairness metrics on test set despite good training performance
  - Significant drop in accuracy when adding fairness constraints
  - Last-layer fine-tuning fails to improve minority group performance

- First 3 experiments:
  1. Train ResNet-18 on CelebA with ERM, then fine-tune last layer on balanced subset with EO constraints
  2. Compare BASE (full network with constraints) vs FT-LAST-RW-FC on CheXpert dataset
  3. Test surgical fine-tuning by updating Block 1 instead of just last layer on CelebA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which last-layer fine-tuning with fairness constraints mitigates overfitting to fairness criteria compared to full-network training?
- Basis in paper: [explicit] The paper states that "over-parameterization of a neural network lead to the over-fitting of fairness criteria since over-parameterization makes the learned neural network's decision boundary highly flexible" and that last-layer fine-tuning avoids this by only updating the last layer.
- Why unresolved: The paper provides empirical evidence but does not offer a formal theoretical explanation for why restricting updates to the last layer prevents overfitting to fairness constraints.
- What evidence would resolve it: A mathematical proof or formal analysis showing how restricting parameter updates to the last layer bounds the model's flexibility in satisfying fairness constraints while preserving representation quality.

### Open Question 2
- Question: How does the effectiveness of last-layer fairness fine-tuning vary with the complexity of the fairness notion being enforced?
- Basis in paper: [inferred] The paper evaluates three different fairness notions (Equalized Odds, Accuracy Equality, Max-Min Fairness) and shows varying performance, suggesting that the complexity of the fairness criterion may impact the effectiveness of last-layer fine-tuning.
- Why unresolved: While the paper demonstrates effectiveness across multiple fairness notions, it does not systematically analyze how the complexity or number of constraints affects the method's performance.
- What evidence would resolve it: A controlled study varying the complexity of fairness constraints (e.g., number of protected attributes, strictness of constraints) while measuring the performance of last-layer fine-tuning versus full-network training.

### Open Question 3
- Question: What are the limitations of last-layer fairness fine-tuning when applied to fairness notions that require changes to intermediate representations?
- Basis in paper: [inferred] The method relies on fixing the representation learned by standard ERM and only fine-tuning the last layer, which may not be sufficient for fairness notions that require modifying the learned features themselves.
- Why unresolved: The paper demonstrates effectiveness for certain fairness notions but does not explore whether the approach would fail for notions requiring representation-level changes.
- What evidence would resolve it: Experiments applying the method to fairness notions that explicitly require representation changes (e.g., individual fairness metrics) and comparing the results to methods that modify intermediate layers.

## Limitations

- The effectiveness of the method depends heavily on the quality of the initial representation learned through ERM, which may not generalize across all datasets
- The paper lacks specific implementation details for fairness constraint formulations and hyperparameter settings, making exact reproduction challenging
- The method may not work when minority groups are severely underrepresented or when the representation contains fundamental biases that cannot be corrected through linear transformations

## Confidence

- **High confidence**: The observation that over-parameterization can lead to overfitting on fairness criteria is well-supported by related work and the paper's empirical results
- **Medium confidence**: The effectiveness of last-layer fine-tuning for fairness is demonstrated on the tested datasets but may not generalize to all scenarios
- **Low confidence**: The claim that representation learning from standard ERM captures "core features" well enough for fairness correction through linear reweighting, as this depends on specific data characteristics

## Next Checks

1. Test the method on datasets with extreme class imbalance (e.g., 1% minority representation) to evaluate the breaking point where last-layer fine-tuning fails
2. Compare performance when using pre-trained representations from ImageNet versus training from scratch on the target dataset to isolate the representation quality effect
3. Implement surgical fine-tuning of multiple blocks rather than just the last layer to quantify the optimal trade-off between computational cost and fairness improvement