---
ver: rpa2
title: A mixed policy to improve performance of language models on math problems
arxiv_id: '2307.08767'
source_url: https://arxiv.org/abs/2307.08767
tags:
- math
- language
- problems
- policy
- generate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of using language models to
  solve deterministic math problems, where sampling-based prediction may lead to incorrect
  answers. The authors propose a mixed policy exploration approach that combines abstract-level
  probabilistic exploration of operators with deterministic greedy selection of operands.
---

# A mixed policy to improve performance of language models on math problems

## Quick Facts
- arXiv ID: 2307.08767
- Source URL: https://arxiv.org/abs/2307.08767
- Reference count: 15
- This paper proposes a mixed policy exploration approach that achieves over 2% performance improvement on GSM8K compared to standard cross-entropy training

## Executive Summary
This paper addresses the challenge of using language models to solve deterministic math problems, where sampling-based prediction may lead to incorrect answers. The authors propose a mixed policy exploration approach that combines abstract-level probabilistic exploration of operators with deterministic greedy selection of operands. This two-level token exploration policy samples whether a token is an operator or operand, then deterministically selects the highest-scoring token for non-operators. Tested on the GSM8K dataset with GPT-2, this method achieves over 2% performance improvement compared to standard cross-entropy training, demonstrating that incorporating deterministic elements in policy exploration can improve math problem-solving accuracy in language models.

## Method Summary
The method uses a two-level token exploration policy: an abstract level that probabilistically explores whether the next token is an operator or operand, and a second level that deterministically selects the highest-scoring token for non-operators. The approach is trained using a combination of cross-entropy loss and reinforcement learning with outcome-based rewards, where the reward is 1 if the final answer matches the ground truth and 0 otherwise. The algorithm uses a linear layer to map hidden states to operator category probabilities (4 operators plus all other tokens), then applies mixed deterministic/stochastic selection based on the predicted category.

## Key Results
- Achieved over 2% performance improvement on GSM8K dataset compared to standard cross-entropy training
- Demonstrated that combining probabilistic operator exploration with deterministic operand selection improves math problem-solving accuracy
- Validated the approach using GPT-2 (124M parameters) with reinforcement learning framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-level token exploration policy improves math problem accuracy by decoupling operator selection from operand selection.
- Mechanism: The abstract level uses probabilistic sampling to explore different operators (e.g., +, -, *, /), while the second level deterministically selects the highest-scoring operand tokens. This addresses the mismatch between language models' generative nature and math's deterministic requirements.
- Core assumption: Operator tokens are the primary source of error in math problem solving, and probabilistic exploration of operators can find better reasoning paths.
- Evidence anchors:
  - [abstract] "we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic"
  - [section] "we propose a two level policy exploration approach: the abstract level policy will decide whether the token is operator or operand and sample from operators, and the second level will take the greedy token selection for non-operators"
  - [corpus] Weak evidence - corpus shows related work on step-by-step planning but no direct evidence about this specific two-level mechanism

### Mechanism 2
- Claim: Reinforcement learning with outcome-based rewards guides the language model toward more deterministic behavior in math reasoning.
- Mechanism: The RL framework uses the final answer as a reward signal, which is discounted back through the sequence to shape the policy toward correct reasoning paths. The max(0, ...) term suppresses policies below average performance.
- Core assumption: Outcome-based supervision is sufficient to guide the reasoning process when combined with appropriate reward shaping.
- Evidence anchors:
  - [section] "we define r(X, ˆyT ) = 1 if the final result ˆyT in ˆY matches its ground truth from Y"
  - [section] "discount the reward from the final step to each previous step to guide the reasoning process with RL"
  - [corpus] Moderate evidence - corpus includes related work on outcome-based vs process-based supervision comparisons

### Mechanism 3
- Claim: The abstract level policy mapping tokens to operator/operand categories enables targeted exploration where it's most beneficial.
- Mechanism: A linear layer maps hidden states to an action space of 5 categories (4 operators + V for all other tokens). This creates a structured exploration space that focuses randomness on the most critical decisions.
- Core assumption: Categorizing tokens into operators vs operands is a meaningful abstraction that captures the key decision points in math problem solving.
- Evidence anchors:
  - [section] "we categorize the tokens into 5 classes: 4 operators and the rest from V, where operators can be '+', '-', '*' and '/'"
  - [section] "In the abstract level, we build a linear model to model the distribution over O"
  - [corpus] Weak evidence - corpus doesn't provide direct support for this specific categorization approach

## Foundational Learning

- Concept: Reinforcement Learning with Policy Gradients
  - Why needed here: The paper uses REINFORCE-style updates to incorporate reward signals from final answers back into the policy
  - Quick check question: What is the key difference between policy gradient methods and value-based RL approaches?

- Concept: Sequence Modeling with Transformers
  - Why needed here: The base model is GPT-2, which uses transformer architecture to model conditional probabilities of token sequences
  - Quick check question: How does causal attention in transformers differ from bidirectional attention, and why is it important for autoregressive generation?

- Concept: Categorical Probability Distributions
  - Why needed here: The abstract level policy outputs probabilities over the operator categories using softmax, requiring understanding of categorical distributions
  - Quick check question: What property must the output of a softmax layer satisfy, and why is this important for modeling probability distributions?

## Architecture Onboarding

- Component map: GPT-2 base model -> Abstract level linear layer -> Second level greedy selector -> Reward function -> Baseline network

- Critical path: Question → GPT-2 hidden states → Abstract level sampling → Token generation → Answer comparison → Reward signal → Policy update

- Design tradeoffs:
  - Operator vs operand exploration: Focusing exploration on operators assumes they're the primary error source
  - Deterministic vs probabilistic: The second level is fully deterministic, which may miss some valid solutions but ensures consistency
  - Reward shaping: Using only final answer reward is simple but may provide sparse feedback for complex reasoning steps

- Failure signatures:
  - Performance plateaus early: May indicate insufficient exploration or poor reward shaping
  - Degradation in non-math tasks: The mixed policy may overfit to math problem structure
  - High variance in training: Could indicate learning rate issues or unstable RL updates

- First 3 experiments:
  1. Baseline comparison: Train with only cross-entropy loss, then with cross-entropy + RL (no mixed policy) to isolate the benefit of the mixed exploration approach
  2. Operator-only exploration: Modify the algorithm to use probabilistic selection for all tokens, not just operators, to test if the operator-focused approach is optimal
  3. Reward shaping variations: Test different reward functions (e.g., partial credit for intermediate steps) to see if more granular feedback improves performance beyond the simple final answer reward

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the mixed policy approach perform on larger language models beyond GPT-2?
- Basis in paper: [explicit] The paper mentions that larger models could better understand semantic meaning, suggesting potential performance improvements.
- Why unresolved: The authors only tested their approach on the smallest GPT-2 model (124M parameters) due to difficulties in attributing performance gains to model size versus algorithm.
- What evidence would resolve it: Empirical results comparing the mixed policy approach across different model sizes (e.g., GPT-2 small, medium, large) on the GSM8K dataset.

### Open Question 2
- Question: Can the mixed policy exploration be extended to explore both operators and operands rather than just operators?
- Basis in paper: [explicit] The authors mention in the conclusion that their approach "only explore operators" and suggest future work should consider exploring both operators and operands.
- Why unresolved: The current implementation focuses solely on operator exploration while keeping operand selection deterministic, leaving the potential benefits of operand exploration untested.
- What evidence would resolve it: Experimental results comparing the current mixed policy (operator-only exploration) with an extended version that includes operand exploration on math problem datasets.

### Open Question 3
- Question: How does the performance of the mixed policy approach compare to outcome-based supervision methods that use process-level feedback?
- Basis in paper: [inferred] The authors discuss both outcome-based and process-based approaches in related work, noting that process supervision significantly outperforms outcome supervision in some studies, suggesting this comparison would be valuable.
- Why unresolved: The paper only compares against cross-entropy loss and outcome-based RL without including process-based supervision baselines.
- What evidence would resolve it: Head-to-head comparison of the mixed policy approach against process-based supervision methods on the same math problem datasets with identical model architectures.

## Limitations

- The approach was only tested on GSM8K dataset with the smallest GPT-2 model (124M parameters), limiting generalizability to other datasets and larger models
- The operator categorization assumes only four basic arithmetic operators, which may not extend to more complex mathematical expressions involving functions, parentheses, or advanced operations
- The deterministic second-level selection for operands could potentially miss valid alternative solutions that require different operand choices, limiting exploration of the solution space

## Confidence

**High Confidence**: The claim that combining abstract-level probabilistic exploration with deterministic operand selection improves math problem-solving accuracy. This is directly supported by the reported experimental results showing over 2% improvement on GSM8K compared to cross-entropy training alone.

**Medium Confidence**: The assertion that operator tokens are the primary source of error in math problem solving. While the two-level approach assumes this, the paper doesn't provide ablation studies or error analysis to confirm that focusing exploration on operators is optimal.

**Low Confidence**: The claim that this mixed policy approach would scale effectively to more complex math problems or larger language models. The paper only tests on GSM8K with GPT-2 (124M parameters), and the operator categorization may not extend well to advanced mathematical expressions.

## Next Checks

1. **Cross-dataset validation**: Test the mixed policy approach on multiple math datasets (e.g., MATH, ASDiv, MultiArith) to assess generalizability. Compare performance improvements across datasets with varying difficulty levels and problem types.

2. **Operator importance ablation**: Conduct ablation studies where the algorithm explores different token types (operands vs operators) to determine which actually contributes more to performance gains. This could involve testing variants that use probabilistic selection for all tokens, deterministic selection for all tokens, or focus exploration on different token categories.

3. **Larger model scalability**: Evaluate the mixed policy approach with larger language models (GPT-3, GPT-4, or similar) to determine if the performance gains scale with model size. This would test whether the mixed policy approach remains effective as the base model becomes more capable, or if it becomes redundant with larger models' inherent reasoning abilities.