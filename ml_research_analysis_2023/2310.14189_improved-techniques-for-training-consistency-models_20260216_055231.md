---
ver: rpa2
title: Improved Techniques for Training Consistency Models
arxiv_id: '2310.14189'
source_url: https://arxiv.org/abs/2310.14189
tags:
- consistency
- training
- song
- noise
- cifar-10
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents improved techniques for consistency training,
  an emerging generative modeling approach that learns to map noisy data samples to
  clean data without distillation. The authors identify and fix theoretical issues
  in previous work, including the use of Exponential Moving Average for teacher networks
  and learned metrics like LPIPS.
---

# Improved Techniques for Training Consistency Models

## Quick Facts
- arXiv ID: 2310.14189
- Source URL: https://arxiv.org/abs/2310.14189
- Reference count: 40
- Key outcome: Improved consistency training achieves FID scores of 2.51 (CIFAR-10) and 3.25 (ImageNet 64x64) in one sampling step, rivaling leading diffusion models and GANs.

## Executive Summary
This paper presents significant improvements to consistency training (CT), an emerging generative modeling approach that learns to map noisy data samples to clean data without distillation. The authors identify and fix theoretical issues in previous work, including the use of Exponential Moving Average (EMA) for teacher networks and learned metrics like LPIPS. By eliminating EMA, adopting Pseudo-Huber losses, using lognormal noise schedules, and implementing curriculum learning for discretization steps, consistency models achieve state-of-the-art FID scores that rival leading diffusion models and GANs.

## Method Summary
The paper proposes several improvements to consistency training: (1) eliminating EMA from the teacher network to avoid theoretical inconsistencies, (2) replacing learned metrics like LPIPS with Pseudo-Huber losses for robustness and reduced training variance, (3) implementing a lognormal noise schedule that properly weights consistency losses across noise levels, and (4) using an exponential curriculum for discretization steps. The training uses Score SDE architecture for CIFAR-10 and ADM architecture for ImageNet 64x64, with RAdam optimizer and careful hyperparameter tuning including Fourier noise embeddings, dropout regularization, and specific weighting functions.

## Key Results
- FID scores of 2.51 (CIFAR-10) and 3.25 (ImageNet 64x64) in one sampling step
- Two-step sampling further improves FID to 2.24 and 2.77
- Outperforms previous consistency training approaches by 3.5x and 4x
- Results rival leading diffusion models and GANs without requiring distillation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing EMA from the teacher network eliminates bias that prevents CT from properly learning consistency models
- Mechanism: When θ' ≠ θ (teacher ≠ student), the CM/CT objective becomes independent of the data distribution, providing no learning signal. With EMA, θ' ≠ θ by construction, causing this failure.
- Core assumption: The theoretical analysis in Song et al. (2023) has an overlooked flaw where Argument (i) requires conditions that don't hold when EMA is used
- Evidence anchors: [abstract]: "identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model"; [section 3.2]: "We show this inconsistency in requirements for Argument (i) and (ii) to hold is caused by flawed theoretical analysis of the former"
- Break condition: If the regularization benefits of EMA outweigh the bias it introduces, or if the theoretical analysis doesn't actually contain the claimed flaw

### Mechanism 2
- Claim: Pseudo-Huber losses replace learned metrics like LPIPS while reducing training variance
- Mechanism: Pseudo-Huber metrics are continuously differentiable and more robust to outliers than squared ℓ2, leading to lower variance in parameter updates during training
- Core assumption: The robustness of Pseudo-Huber to large errors translates to reduced variance in optimization dynamics
- Evidence anchors: [abstract]: "To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics"; [section 3.3]: "we posit that this added robustness can reduce variance during training" and "our observations confirm that the Pseudo-Huber metric results in reduced variance relative to the squared ℓ2 metric"
- Break condition: If the reduced variance doesn't translate to better sample quality, or if LPIPS's learned features provide essential information not captured by Pseudo-Huber

### Mechanism 3
- Claim: Lognormal noise schedules improve sample quality by properly weighting consistency losses across noise levels
- Mechanism: The default uniform sampling over log σ assigns too much weight to high noise levels, while the lognormal schedule with mean=-1.1 and std=2.0 reduces emphasis on high noise levels where learning is harder
- Core assumption: Consistency losses at lower noise levels should be weighted more heavily because errors accumulate to higher noise levels
- Evidence anchors: [abstract]: "we propose a new schedule for sampling noise levels in the CT objective based on lognormal distributions"; [section 3.5]: "This is at odds with the intuition that consistency losses at lower noise levels influence subsequent ones and cause error accumulation" and "this lognormal distribution assigns significantly less weight to high noise levels"
- Break condition: If the optimal noise schedule depends on dataset or architecture in ways not captured by the lognormal distribution

## Foundational Learning

- Concept: Probability flow ODEs and consistency functions
  - Why needed here: The entire consistency model framework is built on transforming samples between different noise levels using the probability flow ODE
  - Quick check question: What is the mathematical relationship between the consistency function f* and the probability flow ODE in equation (1)?

- Concept: Score matching and score-based generative models
  - Why needed here: Consistency distillation (CD) requires a pre-trained diffusion model that estimates score functions, and understanding the connection between scores and consistency is crucial
  - Quick check question: How does the consistency training objective relate asymptotically to the consistency matching objective as N → ∞?

- Concept: Fréchet Inception Distance (FID) and learned perceptual metrics
  - Why needed here: The paper's results are measured using FID, and understanding why LPIPS causes bias in evaluation is central to the motivation
  - Quick check question: Why might using LPIPS (trained on ImageNet) to evaluate models on ImageNet introduce bias in FID scores?

## Architecture Onboarding

- Component map: Data → noise sampling → forward pass through consistency model (F_θ(x, σ) with boundary conditions c_skip(σ) and c_out(σ)) → metric computation → loss → gradient update
- Critical path: The noise schedule and discretization curriculum are critical hyperparameters that affect training dynamics
- Design tradeoffs: Using EMA for the teacher provides regularization but introduces bias (mechanism 1). Learned metrics like LPIPS provide better perceptual quality but require extra compute and introduce evaluation bias. Deeper architectures improve quality but increase training cost.
- Failure signatures: High FID scores indicate poor sample quality. Training instability (divergence) suggests issues with noise embedding sensitivity or learning rate. If EMA is used incorrectly, the model may not learn anything meaningful about the data distribution.
- First 3 experiments:
  1. Train a basic consistency model with squared ℓ2 loss, uniform weighting, and EMA with default parameters to establish baseline performance
  2. Remove EMA from the teacher network and observe the impact on FID scores and training stability
  3. Replace squared ℓ2 with Pseudo-Huber loss and adjust the weighting function to 1/(σ_i+1 - σ_i) to test improvements in sample quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal value of the Pseudo-Huber loss parameter c scale with image resolution and dataset characteristics?
- Basis in paper: [explicit] The paper shows that c=0.03 works best for CIFAR-10 and proposes a heuristic of c=0.00054√d for d-dimensional images, but only tests on CIFAR-10 and ImageNet 64x64.
- Why unresolved: The paper only explores a limited range of c values and datasets. Different image resolutions and dataset statistics could require different c values for optimal performance.
- What evidence would resolve it: Systematic experiments varying c across multiple datasets with different resolutions and image statistics, along with analysis of how c should scale with image dimensions.

### Open Question 2
- Question: What is the theoretical explanation for why consistency models benefit from higher dropout rates compared to diffusion models?
- Basis in paper: [explicit] The paper observes that dropout rate 0.3 improves consistency models while diffusion models typically use lower rates, but only offers a hypothesis about overfitting differences.
- Why unresolved: The authors provide empirical observations but no theoretical justification for why consistency models would need more regularization than diffusion models.
- What evidence would resolve it: Mathematical analysis comparing the optimization landscapes and generalization properties of consistency models versus diffusion models, potentially through PAC-Bayes bounds or other generalization frameworks.

### Open Question 3
- Question: How does the exponential curriculum schedule compare to other scheduling strategies in terms of sample quality and training efficiency?
- Basis in paper: [explicit] The paper proposes an exponential schedule for N(pk) and shows it outperforms constant, square root, linear, square, and cosine schedules, but doesn't explore other potential strategies.
- Why unresolved: While the exponential schedule is shown to be superior to several alternatives, the space of possible scheduling strategies is vast and not fully explored.
- What evidence would resolve it: Systematic comparison of the exponential schedule against other curriculum learning strategies, including adaptive schedules that adjust based on training progress or validation metrics.

## Limitations

- The theoretical analysis of EMA removal assumes no regularization benefits from EMA, which may not hold in all cases
- Pseudo-Huber losses may not capture all the perceptual information that learned metrics like LPIPS provide
- The optimal lognormal noise schedule parameters may be dataset-dependent and require further exploration

## Confidence

- Mechanism 1 (EMA removal): Medium - strong theoretical argument but requires empirical validation across different model scales
- Mechanism 2 (Pseudo-Huber): Medium - theoretical justification is sound but comparative studies with LPIPS are limited
- Mechanism 3 (Lognormal schedules): Low-Medium - intuitive improvement but optimal schedule likely dataset-dependent

## Next Checks

1. Test consistency models with and without EMA on larger-scale datasets (e.g., 256x256 ImageNet) to verify if the theoretical bias persists at scale
2. Compare Pseudo-Huber against multiple learned metrics (LPIPS, CLIP) across different domains to assess generalizability
3. Systematically vary lognormal parameters (mean, std) to determine sensitivity and identify optimal schedules for different noise levels and architectures