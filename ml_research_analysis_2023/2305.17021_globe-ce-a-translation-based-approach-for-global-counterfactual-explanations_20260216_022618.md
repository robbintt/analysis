---
ver: rpa2
title: 'GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations'
arxiv_id: '2305.17021'
source_url: https://arxiv.org/abs/2305.17021
tags:
- ares
- globe-ce
- cost
- explanations
- recourse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for global counterfactual explanations
  (GCEs) that scales fixed translation directions by input-dependent magnitudes, significantly
  improving coverage and reducing costs compared to prior methods. Unlike previous
  approaches that fix both direction and magnitude, GLOBE-CE learns a single direction
  and scales it per instance to reach the decision boundary efficiently.
---

# GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations

## Quick Facts
- arXiv ID: 2305.17021
- Source URL: https://arxiv.org/abs/2305.17021
- Reference count: 40
- Primary result: GLOBE-CE achieves higher coverage and lower average costs than state-of-the-art baselines while running orders of magnitude faster

## Executive Summary
GLOBE-CE introduces a novel framework for generating global counterfactual explanations that scales fixed translation directions by input-dependent magnitudes. Unlike previous approaches that fix both direction and magnitude, this method learns a single direction and scales it per instance to reach decision boundaries efficiently. The framework also provides the first mathematical treatment of categorical feature translations, proving that one-hot encoded vectors can be expressed as interpretable If-Then rules. Evaluated across four datasets and three model types, GLOBE-CE demonstrates superior performance in coverage, cost, and runtime metrics while enabling effective bias detection in user studies.

## Method Summary
GLOBE-CE generates global counterfactual explanations by learning a fixed translation direction and scaling it with input-specific magnitudes to reach decision boundaries. The method accepts a black-box model, dataset, number of explanations, and generation algorithm as inputs. It produces scaled translations, counterfactuals, predictions, and costs for evaluation. The framework bridges local and global explanations by applying variable scaling to a shared translation direction, while providing mathematical proofs for converting categorical translations into interpretable If-Then rules.

## Key Results
- Achieves higher coverage than AReS and Fast AReS across all tested datasets
- Reduces average cost by orders of magnitude compared to baseline methods
- Runs significantly faster than optimization-based approaches while maintaining performance
- Enables effective detection of recourse biases in user studies

## Why This Works (Mechanism)

### Mechanism 1
Fixed translation directions scaled by input-dependent magnitudes significantly improve global counterfactual coverage while reducing average cost. The method learns a single direction δ and scales it per input by scalar kj to reach the decision boundary efficiently. This mitigates the trade-off between coverage and cost inherent in fixed-magnitude approaches. However, if decision boundaries are highly nonlinear or discontinuous, a single fixed direction may fail to cover diverse input regions effectively.

### Mechanism 2
Arbitrary translations on one-hot encoded categorical features can be expressed as interpretable If-Then rules. Theorems prove that any translation vector added to a one-hot encoding can be converted into rules with one Then condition per feature, and scaling the translation adds If conditions cumulatively. This works because one-hot encodings and their arithmetic follow predictable patterns that map to rule-based representations. The theorem may not apply directly to non-one-hot encodings like label encoding.

### Mechanism 3
Scaling translations input-wise bridges local and global explanations by applying variable scaling to a shared translation direction. Treating groups of inputs as single instances, generating directions, and then scaling per input captures variation efficiently while retaining a fixed global direction. This reveals that local and global explanations are more intimately connected than current research implies. However, if inputs within a group have highly diverse feature distributions, a single direction may not adequately represent the group.

## Foundational Learning

- Concept: Counterfactual explanations (CEs) and their properties (sparsity, diversity, actionability)
  - Why needed here: GLOBE-CE builds on existing CE methods by generalizing them to global settings with scaling
  - Quick check question: What are the key desiderata for counterfactual explanations, and how do they differ between local and global settings?

- Concept: One-hot encoding and its arithmetic properties
  - Why needed here: The method relies on mathematical proofs about translating one-hot encoded vectors into interpretable rules
  - Quick check question: How does adding a translation vector to a one-hot encoded feature change the feature value, and how can this be expressed as a rule?

- Concept: Submodular maximization and its applications in optimization
  - Why needed here: The method uses submodular optimization to select diverse and high-performing translations from candidate sets
  - Quick check question: What is submodular maximization, and how does it guarantee polynomial-time convergence for selecting diverse explanations?

## Architecture Onboarding

- Component map: Black-box model B -> Dataset X -> Generation algorithm G -> Translation generation -> Scaling (k) -> Categorical rule conversion (Theorems 4.1-4.2) -> Scaled translations δi, scalars ki -> Counterfactuals X', predictions Y', costs C -> Evaluation (coverage, cost, runtime)

- Critical path: 1. Generate n translation directions using algorithm G 2. Scale each direction across all inputs with scalars k 3. Convert categorical translations to If-Then rules using theorems 4. Compute counterfactuals, predictions, and costs 5. Evaluate coverage, cost, and runtime

- Design tradeoffs:
  - Fixed direction vs. multiple directions: Single direction with scaling is more interpretable but may miss diverse regions
  - Random sampling vs. model-specific generation: Random sampling is simpler but model-specific methods may yield better results
  - Categorical rule complexity vs. interpretability: More complex rules may be less interpretable but cover more cases

- Failure signatures:
  - Low coverage: Translation directions don't reach decision boundary for many inputs
  - High average cost: Scaling factors are too large, making explanations impractical
  - Slow runtime: Ground set generation or submodular optimization takes too long
  - Poor categorical conversion: Translation rules don't map cleanly to interpretable If-Then statements

- First 3 experiments:
  1. Run GLOBE-CE with n=1 on German Credit dataset using random sampling; measure coverage, cost, and runtime
  2. Compare GLOBE-CE translations to AReS triples on COMPAS dataset; evaluate interpretability and performance
  3. Test categorical translation theorems by applying translations to one-hot encoded features and verifying rule conversion

## Open Questions the Paper Calls Out

### Open Question 1
How does the GLOBE-CE framework's performance scale when applied to high-dimensional datasets with complex decision boundaries, such as those found in image classification or natural language processing tasks? The paper demonstrates GLOBE-CE's efficacy on tabular datasets with continuous and categorical features but does not explore its application to high-dimensional domains like images or text. Extending the framework to these spaces may require significant modifications to handle increased complexity and potential non-linearities. Applying GLOBE-CE to image or text datasets and evaluating its performance compared to existing methods would provide insights into its scalability and effectiveness in high-dimensional settings.

### Open Question 2
How sensitive is the GLOBE-CE framework to the choice of the number of global counterfactual explanations (n) and the range of scaling factors (k) used in the algorithm? The paper mentions that the choice of n and k can impact the framework's performance and interpretability but does not provide a comprehensive analysis of their sensitivity. Conducting experiments with varying values of n and k across different datasets and model types would reveal the framework's sensitivity to these parameters and guide practitioners in selecting appropriate settings.

### Open Question 3
How does the GLOBE-CE framework perform in terms of robustness and stability when applied to counterfactual explanations in the presence of noisy or adversarial inputs? The paper acknowledges the importance of robustness in counterfactual explanations but does not explicitly evaluate GLOBE-CE's performance under such conditions. Evaluating GLOBE-CE's performance on datasets with injected noise or adversarial examples and comparing it to existing methods in terms of robustness and stability would provide insights into its resilience under challenging conditions.

## Limitations
- Scalability to high-dimensional datasets beyond the four tested cases remains uncertain
- Performance on non-tabular data (images, text) has not been evaluated
- Single fixed direction may not effectively cover diverse regions for highly nonlinear decision boundaries

## Confidence

**High confidence**: The mathematical proofs for categorical translation rules (Theorems 4.1-4.2) are sound and verifiable

**Medium confidence**: The claim that GLOBE-CE outperforms AReS and Fast AReS on coverage and cost metrics, based on the limited dataset evaluation

**Low confidence**: The assertion that GLOBE-CE bridges local and global explanations through variable scaling, as this theoretical connection requires further empirical validation

## Next Checks

1. Test GLOBE-CE on a high-dimensional dataset (e.g., >100 features) to verify scalability claims
2. Evaluate the method on non-tabular data types to assess generalizability beyond structured data
3. Conduct ablation studies to quantify the individual contributions of direction generation and scaling to overall performance