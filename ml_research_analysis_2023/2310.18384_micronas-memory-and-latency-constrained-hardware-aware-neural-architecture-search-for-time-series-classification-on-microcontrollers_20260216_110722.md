---
ver: rpa2
title: 'MicroNAS: Memory and Latency Constrained Hardware-Aware Neural Architecture
  Search for Time Series Classification on Microcontrollers'
arxiv_id: '2310.18384'
source_url: https://arxiv.org/abs/2310.18384
tags:
- search
- latency
- memory
- neural
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MicroNAS is the first differentiable neural architecture search
  system for time series classification on microcontrollers. It introduces a novel
  search space with two searchable cells tailored for temporal aggregation and sensor
  fusion, uses dynamic convolutions with a lookup table for precise latency estimation,
  and optimizes for user-defined memory and latency constraints.
---

# MicroNAS: Memory and Latency Constrained Hardware-Aware Neural Architecture Search for Time Series Classification on Microcontrollers

## Quick Facts
- arXiv ID: 2310.18384
- Source URL: https://arxiv.org/abs/2310.18384
- Reference count: 40
- Key outcome: First differentiable NAS system for MCU time series classification with lookup table latency estimation, achieving 97.35% F1-score on SkodaR and 94.59% accuracy on UCI-HAR

## Executive Summary
MicroNAS introduces the first differentiable neural architecture search system specifically designed for time series classification on microcontrollers. It addresses the challenge of finding efficient neural architectures that meet strict memory and latency constraints on resource-constrained MCUs. The system employs a novel lookup table approach for precise latency estimation, dynamic convolutions for efficient filter count search, and specialized cell architectures tailored for temporal aggregation and sensor fusion. MicroNAS achieves near state-of-the-art classification performance while ensuring deployment feasibility on embedded devices.

## Method Summary
MicroNAS uses differentiable neural architecture search (DNAS) with a hardware-aware loss function that incorporates user-defined latency and memory constraints. The search space consists of linear stacks of two specialized cell types: Time-Reduce cells for temporal aggregation and Sensor-Fusion cells for cross-channel feature extraction. A lookup table approach pre-measures operator latencies on the target MCU for accurate hardware cost estimation. Dynamic convolutions enable efficient search over filter counts through weight sharing. The system generates architectures that meet constraints after quantization-aware retraining using TFLM framework.

## Key Results
- Achieved 97.35% F1-score on SkodaR dataset and 94.59% accuracy on UCI-HAR dataset
- Lookup table latency estimation achieves R²-score of 99.97 with mean absolute error of 1.59 ms
- Outperformed DARTS-based baselines while meeting strict MCU constraints
- Post-training quantization reduces memory by 3.28x with only 0.78% average accuracy drop

## Why This Works (Mechanism)

### Mechanism 1: Lookup Table Latency Estimation
- Claim: The lookup table latency estimation method achieves higher precision than FLOPs-based proxy metrics for MCU deployment.
- Mechanism: By pre-executing each operator on the target MCU and storing actual latency measurements in a lookup table, MicroNAS avoids the non-linear relationship between FLOPs and execution time that arises from MCU-specific factors like memory access patterns and instruction-level parallelism.
- Core assumption: Latency of an operator is deterministic for a given MCU, dataset shape, and quantization settings, allowing reliable table-based lookup.
- Evidence anchors:
  - [abstract]: "accurate execution latency calculations, with a minimum error of only ±1.02ms"
  - [section 4.1.1]: "Our lookup table approach achieves an R2-score of 99.97 with a mean absolute error of 1.59 ms"
  - [corpus]: Missing direct evidence; corpus neighbors focus on zero-shot NAS rather than latency estimation, so no anchor found.
- Break condition: If operator latency becomes data-dependent (e.g., dynamic tensor shapes or runtime branching), lookup table entries become insufficient.

### Mechanism 2: Dynamic Convolutions with Mask-Based Filter Selection
- Claim: Dynamic convolutions with mask-based filter selection enable fine-grained search for the number of filters without exploding search space size.
- Mechanism: A convolution is executed once with maximum filters, then output is masked according to architectural weights. This shares weights across different filter counts, avoiding separate training for each configuration.
- Core assumption: The same convolutional kernel weights can produce valid outputs for different numbers of filters when masked appropriately.
- Evidence anchors:
  - [section 5.1.2]: "A dynamic convolution is a convolution whose number of filters can be searched for efficiently by using weight sharing"
  - [figure 5]: Visual representation of mask-based filter selection process
  - [corpus]: No direct anchor found; corpus focuses on zero-shot and evolutionary search rather than dynamic convolution mechanisms.
- Break condition: If mask patterns create invalid tensor shapes or if hardware kernels don't support arbitrary channel reduction efficiently.

### Mechanism 3: Specialized Cell Types for Time Series Classification
- Claim: Two specialized cell types (Time-Reduce and Sensor-Fusion) capture domain-specific patterns in time series classification while maintaining hardware efficiency.
- Mechanism: Time-Reduce cells use strided convolutions to aggregate temporal information and reduce sequence length early, minimizing computational cost. Sensor-Fusion cells apply parallel convolutions at different temporal scales to capture cross-sensor interactions, inspired by InceptionTime.
- Core assumption: Time series classification on MCUs benefits from early temporal aggregation and multi-scale feature extraction across sensor channels.
- Evidence anchors:
  - [section 5.1]: "The newly introduced Time-Reduce cell aggregates information in the temporal domain and reduces the length of the time series while the Sensor-Fusion cell allows for cross-channel interaction"
  - [section 5.2]: Detailed architectural description of both cell types with temporal reduction and sensor fusion rationale
  - [corpus]: No direct anchor found; corpus neighbors don't discuss time series-specific NAS architectures.
- Break condition: If dataset characteristics don't align with early temporal reduction (e.g., need for long-range dependencies) or if cross-sensor patterns aren't present.

## Foundational Learning

- Concept: Differentiable Neural Architecture Search (DNAS)
  - Why needed here: Provides efficient gradient-based search through continuous relaxation of architectural decisions, essential for handling the large search space (≈10^13 to 10^22 architectures) within reasonable time on MCUs
  - Quick check question: How does DNAS differ from traditional RL-based NAS in terms of computational requirements and search efficiency?

- Concept: Hardware-aware optimization with differentiable constraints
  - Why needed here: Enables joint optimization of accuracy and hardware metrics (latency, memory) through differentiable loss terms, ensuring found architectures meet strict MCU constraints
  - Quick check question: What mathematical form does the hardware-aware loss take, and how do the scaling factors γ control constraint satisfaction?

- Concept: Post-training quantization impact on accuracy and performance
  - Why needed here: Critical for MCU deployment as quantization reduces model size and latency while maintaining acceptable accuracy loss (average 0.78% accuracy drop, 3.28x memory reduction)
  - Quick check question: What are the typical accuracy and performance trade-offs when applying int8 quantization to found architectures?

## Architecture Onboarding

- Component map: Dataset → Lookup table characterization → DNAS search → Architecture extraction → Retraining → Quantization → Deployment
- Critical path: Dataset → Lookup table characterization → DNAS search → Architecture extraction → Retraining → Quantization → Deployment
- Design tradeoffs:
  - Lookup table granularity vs. characterization cost: Higher granularity provides more precise latency estimation but requires more pre-measurement steps
  - Cell complexity vs. search efficiency: More sophisticated cells capture better patterns but increase search space size and characterization burden
  - Hardware constraint weighting vs. accuracy: Higher constraint penalties ensure compliance but may limit optimal architectures
- Failure signatures:
  - Architecture search fails to converge: Check Gumbel-Softmax temperature decay schedule and constraint scaling factors
  - Found architectures exceed memory limits: Verify analytical memory estimation matches actual TFLM requirements
  - Poor accuracy after quantization: Review quantization-aware retraining procedure and dataset scaling parameters
- First 3 experiments:
  1. Characterize latency lookup table on target MCU with all operators in search space using dataset-specific input shapes
  2. Run DNAS search with relaxed hardware constraints to verify search space connectivity and convergence
  3. Deploy a found architecture on MCU and measure actual latency/memory vs. predicted values to validate estimation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MicroNAS perform when applied to time series classification tasks beyond human activity recognition, such as anomaly detection or medical diagnostics?
- Basis in paper: [inferred] The paper evaluates MicroNAS on UCI-HAR and SkodaR datasets, both of which are human activity recognition datasets. There is no mention of its performance on other time series classification tasks.
- Why unresolved: The paper does not provide any results or discussion on the application of MicroNAS to other time series classification tasks. This is a gap in the evaluation of the system's generalizability.
- What evidence would resolve it: Performance metrics (accuracy, F1-score, latency, memory usage) on a variety of time series classification tasks, including anomaly detection, medical diagnostics, and other domains.

### Open Question 2
- Question: What is the impact of different hardware-aware NAS search spaces on the performance and efficiency of MicroNAS? Specifically, how would incorporating self-attention layers or other advanced neural network components affect the results?
- Basis in paper: [inferred] The paper mentions that incorporating more sophisticated layer types, such as self-attention, could enhance the search system further, though this may necessitate adding specific operators to the TFLM-framework. This suggests that the current search space might be limited and that exploring other architectures could yield better results.
- Why unresolved: The paper does not explore different search spaces or evaluate the impact of incorporating advanced neural network components on the performance of MicroNAS.
- What evidence would resolve it: Comparative analysis of MicroNAS performance using different search spaces, including those with self-attention layers or other advanced components, on various time series classification tasks.

### Open Question 3
- Question: How does the latency lookup table approach in MicroNAS compare to other latency estimation methods in terms of accuracy and computational cost for different microcontroller architectures?
- Basis in paper: [explicit] The paper presents a comparison between the lookup table approach and the flops-based latency estimation method, showing that the lookup table approach achieves higher accuracy with a mean absolute error of 1.59 ms compared to 15.57 ms for the flops-based method. However, the comparison is limited to a single microcontroller (Nucleo-L552ZE-Q) and a single search space.
- Why unresolved: The paper does not provide a comprehensive comparison of the lookup table approach with other latency estimation methods across different microcontroller architectures and search spaces.
- What evidence would resolve it: Comparative analysis of the lookup table approach with other latency estimation methods (e.g., analytical models, machine learning-based approaches) in terms of accuracy and computational cost for various microcontroller architectures and search spaces.

## Limitations

- The lookup table latency estimation method's generalizability across different MCUs and datasets remains untested beyond the evaluated Cortex-M4F
- The dynamic convolution mechanism's effectiveness depends on specific hardware kernel implementations that may vary across MCU vendors
- The specialized cell designs for time series classification may not transfer well to other sequence modeling tasks

## Confidence

- **High confidence** in the lookup table latency estimation mechanism (validated by R²=99.97 and ±1.02ms error)
- **Medium confidence** in dynamic convolution effectiveness (mechanism described but hardware kernel dependency unclear)
- **Medium confidence** in cell specialization benefits (domain rationale provided but cross-dataset validation limited)
- **Low confidence** in zero-shot generalization claims (not extensively validated across different MCU architectures)

## Next Checks

1. Measure latency estimation accuracy on a different MCU architecture (e.g., Cortex-M7) with the same lookup table approach
2. Benchmark dynamic convolution performance on multiple hardware backends to assess kernel dependency
3. Test the found architectures on additional time series classification datasets to verify domain transfer capability