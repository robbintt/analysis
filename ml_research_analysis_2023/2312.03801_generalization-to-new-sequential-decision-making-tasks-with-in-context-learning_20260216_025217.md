---
ver: rpa2
title: Generalization to New Sequential Decision Making Tasks with In-Context Learning
arxiv_id: '2312.03801'
source_url: https://arxiv.org/abs/2312.03801
tags:
- learning
- tasks
- task
- agent
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates in-context learning (ICL) for sequential
  decision making. While transformers have been shown to enable ICL for language and
  vision tasks, applying them to sequential decision making is challenging due to
  the environment's stochasticity and the need to handle unseen states.
---

# Generalization to New Sequential Decision Making Tasks with In-Context Learning

## Quick Facts
- arXiv ID: 2312.03801
- Source URL: https://arxiv.org/abs/2312.03801
- Reference count: 40
- Key outcome: Training transformers on sequences of trajectories with distributional properties like burstiness and stochasticity enables in-context learning for new sequential decision making tasks

## Executive Summary
This paper demonstrates that transformers can learn in-context sequential decision making by training on sequences of trajectories with specific distributional properties. Unlike traditional few-shot learning that requires weight updates, the proposed approach enables zero-shot and few-shot learning of new tasks without any parameter updates. The key insight is that multi-trajectory sequences, trajectory burstiness, and environment stochasticity during training lead to better generalization to unseen sequential decision making tasks.

## Method Summary
The authors train causal transformers on sequences of expert trajectories from 12 training tasks, systematically varying factors like trajectory burstiness, environment stochasticity, task diversity, model size, and dataset size. They evaluate on 4 unseen tasks in both zero-shot and one-shot settings (1-7 demonstrations), measuring episodic return. The training data is constructed to contain multi-trajectory sequences with burstiness, where multiple trajectories from the same level appear together to force generalization rather than copying.

## Key Results
- Multi-trajectory transformers outperform single-trajectory variants on ICL for sequential tasks
- Higher trajectory burstiness, environment stochasticity, and task diversity all improve ICL performance
- Larger model and dataset sizes lead to better generalization to unseen tasks
- One-shot ICL with 1-7 demonstrations can achieve near-expert performance on new tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning in sequential decision making requires trajectories in the context to cover the wide range of states an agent may encounter.
- Mechanism: Unlike supervised learning where a few labeled examples suffice, sequential environments demand full or partial trajectories in the context to provide relevant state-action pairs for unseen situations. This is because agents rarely revisit the same state within an episode, making isolated transitions insufficient for effective generalization.
- Core assumption: The context must contain trajectories from the same environment level as the query trajectory to provide relevant information.
- Evidence anchors:
  - [abstract] "in sequential decision making it is crucial for the context to contain full/partial trajectories (or sequences of predictions) to cover the potentially wide range of states the agent may find itself in at deployment."
  - [section] "in sequential decision making, multi-trajectory transformers enable better in-context learning than single-trajectory transformers" - demonstrated through the comparison experiment.
- Break condition: If the context contains trajectories from entirely different tasks or environments with disjoint state spaces, the model cannot leverage this information effectively.

### Mechanism 2
- Claim: Trajectory burstiness (having multiple trajectories from the same level in the context) improves in-context learning by providing similar but not identical trajectories.
- Mechanism: Burstiness ensures the context contains demonstrations similar to the query, but with variation due to environment stochasticity. This forces the model to generalize from similar trajectories rather than simply memorizing or copying actions.
- Core assumption: The trajectories in the context, despite being from the same level, will vary due to inherent environment stochasticity.
- Evidence anchors:
  - [abstract] "training on sequences of trajectories with certain distributional properties leads to in-context learning of new sequential decision making tasks" - specifically mentioning burstiness.
  - [section] "Burstiness was first introduced in Chan et al. (2022a) to describe the quality of a dataset sample where the context contains examples that are similar to the query... Here, we extend this idea to trajectory burstiness, which we define as the probability of a given input sequence containing at least two trajectories from the same level."
- Break condition: If the environment is fully deterministic with no stochasticity, trajectories from the same level would be identical copies, eliminating the need for generalization.

### Mechanism 3
- Claim: Environment stochasticity improves in-context learning by increasing dataset diversity and preventing pure copying behavior.
- Mechanism: Stochastic environments introduce variation in trajectories from the same level, forcing the model to learn generalized policies rather than simply copying actions from the context. This diversity helps the model handle unseen states during deployment.
- Core assumption: Stochasticity introduces meaningful variation in trajectories from the same level, making identical copies rare.
- Evidence anchors:
  - [abstract] "We investigate different design choices and find that... more task diversity, environment stochasticity... all result in better in-context learning"
  - [section] "Models trained with stochastic dynamics exhibit superior performance on unseen tasks... when the training environment dynamics are stochastic, the pretraining dataset exhibits greater diversity and identical copies of the trajectory become rarer, forcing the model to generalize"
- Break condition: If stochasticity is too high, the trajectories may become too dissimilar, making it difficult for the model to learn useful patterns.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) - formal framework for modeling sequential decision making problems with states, actions, rewards, and transition dynamics.
  - Why needed here: The paper operates in the sequential decision making setting, which requires understanding MDP fundamentals to grasp the problem formulation and evaluation methodology.
  - Quick check question: What are the key components of an MDP tuple and how do they relate to the sequential decision making problem?

- Concept: In-context learning (ICL) - the ability of models to learn new tasks from context examples without weight updates.
  - Why needed here: The paper's core contribution is demonstrating ICL in sequential decision making, building on prior work that showed ICL in language and vision tasks.
  - Quick check question: How does ICL differ from traditional few-shot learning in terms of model adaptation?

- Concept: Trajectory representation - sequences of (state, action, reward) tuples that capture the temporal structure of decision making.
  - Why needed here: The paper's key insight is that training on sequences of trajectories (rather than single trajectories) enables ICL in sequential settings.
  - Quick check question: Why are single-trajectory sequences insufficient for enabling ICL in sequential decision making?

## Architecture Onboarding

- Component map: Observation encoder -> Action/reward encoder -> Causal transformer -> Action prediction
- Critical path: Context preparation → Trajectory encoding → Causal attention processing → Action prediction
  - Context preparation: Collect expert trajectories, construct multi-trajectory sequences with burstiness
  - Trajectory encoding: Process observations and actions through respective encoders
  - Causal attention: Transformer processes sequence with causal masking
  - Action prediction: Predict actions for each state in the sequence
- Design tradeoffs:
  - Single vs multi-trajectory sequences: Multi-trajectory provides better ICL but increases computational cost
  - Context length: Longer contexts capture more information but increase memory requirements
  - Burstiness probability: Higher burstiness improves ICL but may reduce task diversity in training
- Failure signatures:
  - Low in-context action accuracy: Model cannot effectively use demonstration information
  - Good in-context accuracy but poor episodic return: Model copies actions but fails in stochastic/unforgiving environments
  - Poor performance on all metrics: Model fails to learn either in-context or in-weights
- First 3 experiments:
  1. Compare single-trajectory vs multi-trajectory transformers on a simple task to verify the key insight
  2. Test effect of trajectory burstiness (pb=0, 0.6, 1.0) on ICL performance
  3. Evaluate impact of environment stochasticity by training on deterministic vs stochastic versions of the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of trajectory burstiness on the emergence of in-context learning (ICL) in sequential decision making, and is there an optimal level of burstiness?
- Basis in paper: [explicit] The paper investigates trajectory burstiness and finds that more burstiness leads to better ICL, but it doesn't pinpoint the exact relationship or optimal level.
- Why unresolved: The paper shows a positive correlation between burstiness and ICL but doesn't explore the full spectrum of burstiness levels or provide a detailed analysis of the relationship.
- What evidence would resolve it: Experiments varying burstiness levels systematically and analyzing the resulting ICL performance to identify the optimal burstiness level.

### Open Question 2
- Question: How does the model's performance in terms of in-context learning change when trained on datasets with varying degrees of task diversity?
- Basis in paper: [explicit] The paper mentions that task diversity has a positive impact on ICL but doesn't explore the relationship in detail or provide a concrete measure of task diversity.
- Why unresolved: The paper shows a general trend of improved ICL with increased task diversity but doesn't quantify the diversity or explore its impact in depth.
- What evidence would resolve it: Experiments training models on datasets with different levels of task diversity and measuring the resulting ICL performance.

### Open Question 3
- Question: What is the role of environment stochasticity in the emergence of in-context learning, and how does it interact with other factors like task diversity and trajectory burstiness?
- Basis in paper: [explicit] The paper finds that stochastic environments lead to better ICL but doesn't explore the interaction between stochasticity and other factors.
- Why unresolved: The paper shows a positive impact of stochasticity on ICL but doesn't investigate how it interacts with other factors or explore the optimal level of stochasticity.
- What evidence would resolve it: Experiments varying the level of stochasticity while controlling for other factors like task diversity and burstiness, and analyzing the resulting ICL performance.

### Open Question 4
- Question: What are the specific failure modes of in-context learning in sequential decision making, and how can they be addressed?
- Basis in paper: [explicit] The paper identifies some failure modes like "unforgiving environment" and "distributional drift" but doesn't provide a comprehensive analysis or solutions.
- Why unresolved: The paper mentions some failure modes but doesn't explore them in detail or propose solutions to address them.
- What evidence would resolve it: A systematic analysis of different failure modes and experiments testing potential solutions to mitigate them.

## Limitations
- Findings are based primarily on relatively simple grid-based and procedurally generated environments
- Model sizes used (100M-210M parameters) are modest compared to modern LLMs
- Evaluation focuses on episodic return as the primary metric with limited analysis of sample efficiency or computational costs

## Confidence
- High confidence: Multi-trajectory sequences enable better ICL than single-trajectory sequences; trajectory burstiness improves ICL by forcing generalization; larger models and datasets improve ICL performance; environment stochasticity enhances dataset diversity and prevents copying
- Medium confidence: Task diversity is crucial for generalization to unseen tasks; the proposed training approach generalizes to both MiniHack and Procgen domains; one-shot ICL with 1-7 demonstrations can achieve near-expert performance

## Next Checks
1. Test whether the same approach works for more complex sequential decision making domains like continuous control or text-based games with longer horizons
2. Evaluate the computational efficiency of inference-time ICL compared to fine-tuning, including memory requirements for longer context lengths
3. Investigate whether the model can handle distributional shifts beyond task novelty, such as changes in observation space or action space definitions