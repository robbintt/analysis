---
ver: rpa2
title: Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks
arxiv_id: '2310.17805'
source_url: https://arxiv.org/abs/2310.17805
tags:
- tricks
- learning
- reward
- symlog
- twohot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies the model-based reinforcement learning tricks
  from DreamerV3 to the PPO algorithm, aiming to improve its stability and performance.
  The authors implement symlog predictions, twohot encoding, percentile scaling, critic
  EMA regularization, and unimix categoricals in PPO.
---

# Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks

## Quick Facts
- arXiv ID: 2310.17805
- Source URL: https://arxiv.org/abs/2310.17805
- Reference count: 38
- One-line primary result: Applying DreamerV3 tricks to PPO generally doesn't improve performance, except symlog predictions significantly help when reward clipping is disabled.

## Executive Summary
This paper investigates whether the stability tricks from the model-based DreamerV3 algorithm can improve the performance of the model-free PPO algorithm. The authors implement symlog predictions, twohot encoding, percentile scaling, critic EMA regularization, and unimix categoricals in PPO and evaluate them across diverse environments. Surprisingly, these tricks generally do not improve PPO performance compared to the baseline, except for symlog predictions which significantly help when reward clipping is disabled. The results suggest that while these tricks may not be generally beneficial, they can improve robustness to varying reward scales in specific cases.

## Method Summary
The authors implement five DreamerV3 tricks in PPO: symlog predictions (logarithmic compression of large values), twohot encoding (categorical value predictions), percentile scaling (normalization by return spread), critic EMA regularization (smoothed value predictions), and unimix categoricals (stochastic action sampling). They evaluate these modifications on Atari 100M (57 games) and DeepMind Control Suite (35 environments), comparing against baseline PPO with and without reward clipping. The experiments include full implementation with all tricks and ablation studies (add-one and drop-one) to isolate individual effects.

## Key Results
- Symlog predictions significantly improve PPO performance when reward clipping is disabled, particularly in Atari environments
- Two-hot encoding and unimix categoricals generally underperform compared to baseline PPO
- Percentile scaling performs comparably or better than baseline in Atari without reward clipping
- None of the tricks consistently improve performance across all environments and settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symlog predictions compress large return values while preserving small ones, stabilizing critic learning when rewards are unbounded.
- Mechanism: The symlog transform applies a logarithmic scale to magnitudes greater than 1 while remaining nearly linear near zero, reducing variance in large targets without distorting small returns.
- Core assumption: The transformed targets still preserve the relative ordering and gradients needed for effective value prediction.
- Evidence anchors:
  - [abstract] states that symlog predictions "significantly help PPO when reward clipping is disabled"
  - [section] provides the transform equations: symlog(x) = sign(x) * ln(|x| + 1)
  - [corpus] no direct evidence for symlog in PPO, weak signal
- Break condition: If the inverse transform (symexp) introduces numerical instability or if the critic architecture cannot handle the compressed dynamic range.

### Mechanism 2
- Claim: Twohot encoding enables the critic to predict value distributions over discretized bins, improving robustness to outlier returns.
- Mechanism: Continuous values are represented as weighted combinations of two adjacent discrete bins, allowing the critic to learn a categorical distribution rather than a single scalar.
- Core assumption: The discretization granularity (number and range of bins) is appropriate for the environment's return distribution.
- Evidence anchors:
  - [section] defines twohot encoding: "represents continuous values as a weighting between two equal buckets"
  - [abstract] notes that twohot encoding "underperforms compared to the baseline" in general
  - [corpus] no direct evidence for twohot encoding improving PPO, weak signal
- Break condition: When the chosen bin range is too narrow or too wide relative to actual returns, causing the critic to saturate or underutilize the representation.

### Mechanism 3
- Claim: Percentile scaling normalizes advantages by their spread rather than absolute magnitude, improving exploration in sparse reward settings.
- Mechanism: Advantages are divided by the difference between the 5th and 95th percentile of returns in each batch, scaling by relative spread rather than absolute magnitude.
- Core assumption: The percentile estimates are stable enough across batches to provide consistent scaling.
- Evidence anchors:
  - [section] explains percentile scaling avoids "amplifying the noise in small returns" unlike standard deviation scaling
  - [abstract] shows percentile scaling performs comparably or better in "Atari without reward clipping"
  - [corpus] no direct evidence for percentile scaling in PPO, weak signal
- Break condition: If the percentile EMA updates too quickly or too slowly relative to return distribution changes, causing unstable scaling.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO) objective and clipped surrogate loss
  - Why needed here: The tricks are applied to PPO's policy and value networks, so understanding the base algorithm is essential
  - Quick check question: What is the purpose of the clipping parameter ϵ in PPO's objective function?

- Concept: Advantage estimation and Generalized Advantage Estimation (GAE)
  - Why needed here: Percentile scaling modifies how advantages are computed and normalized
  - Quick check question: How does GAE balance bias and variance compared to simple Monte Carlo returns?

- Concept: Categorical vs. continuous regression in neural networks
  - Why needed here: Twohot encoding changes the critic from predicting continuous values to categorical distributions
  - Quick check question: What loss function is used when the critic outputs logits for discrete bins instead of a single value?

## Architecture Onboarding

- Component map:
  - Policy network: unchanged from baseline PPO, may use unimix categoricals for action sampling
  - Value network: modified to output either symlog-transformed values or twohot-encoded distributions
  - Data preprocessing: symlog transformation applied to observations in continuous vector spaces
  - Advantage computation: percentile scaling applied to GAE advantages before policy update
  - EMA tracking: separate exponential moving averages maintained for value predictions and percentile statistics

- Critical path: Observation → Network forward pass → Symlog/Twohot processing → Loss computation → Gradient update → EMA update
- Design tradeoffs:
  - Symlog predictions: improved stability for unbounded rewards vs. added computational overhead
  - Twohot encoding: better handling of outliers vs. sensitivity to bin range selection
  - Percentile scaling: robust normalization vs. potential instability from percentile EMA updates
  - Critic EMA regularizer: smoother learning vs. potential underfitting if decay is too high
  - Unimix categoricals: guaranteed exploration vs. reduced exploitation efficiency

- Failure signatures:
  - Vanishing or exploding gradients in symlog-transformed value predictions
  - Critic consistently predicting extreme bin indices in twohot encoding
  - Percentile scaling factors approaching zero or NaN values
  - Policy collapse when unimix ratio is too high

- First 3 experiments:
  1. Run PPO with symlog predictions enabled on an Atari environment without reward clipping to verify stability improvement
  2. Test twohot encoding with varying bin ranges on a simple continuous control task to find optimal configuration
  3. Compare percentile scaling against standard advantage normalization on a sparse reward environment to measure exploration benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the stability tricks from DreamerV3 specifically beneficial for world model learning, or do they also improve performance in other reinforcement learning settings?
- Basis in paper: [explicit] The authors mention that "It's possible that symlog predictions, twohot encoding, or unimix categoricals are specifically beneficial to world model learning, or that the world model-specific tricks not studied in this paper are the source of its improved performance."
- Why unresolved: The authors did not test these tricks in a world model learning context, so it is unclear if their benefits extend beyond model-free learning.
- What evidence would resolve it: Implementing these tricks in a world model learning algorithm like DreamerV3 and comparing its performance to the original algorithm would provide insight into their specific benefits for world model learning.

### Open Question 2
- Question: How do the stability tricks from DreamerV3 interact with different activation functions, such as SiLU, and architectural choices like layer normalization?
- Basis in paper: [explicit] The authors note that "their use of SiLU activation functions, layer normalization, and a 200 million parameter architecture may also contribute to its state-of-the-art results."
- Why unresolved: The authors only experimented with the default PPO architecture and did not explore how these tricks interact with different architectural choices.
- What evidence would resolve it: Implementing these tricks in PPO with different activation functions and architectural choices, such as SiLU activations and layer normalization, and comparing their performance to the original PPO would provide insight into their interactions with different architectures.

### Open Question 3
- Question: What is the optimal range and number of bins for twohot encoding in PPO, and how do these choices affect performance?
- Basis in paper: [explicit] The authors found that "the choice of range for twohot encodings has a significant impact on performance" and that "Large bounds also seem to have a detrimental effect on learning."
- Why unresolved: The authors only experimented with a limited range of bin sizes and ranges, and did not perform an exhaustive search to find the optimal configuration.
- What evidence would resolve it: Conducting an extensive hyperparameter search over the range and number of bins for twohot encoding in PPO and comparing the performance across different configurations would provide insight into the optimal settings for these parameters.

## Limitations

- Limited hyperparameter tuning: The tricks were not extensively tuned for PPO, making it difficult to distinguish between suboptimal configurations and inherent ineffectiveness
- Lack of mechanistic analysis: The study doesn't explain why certain tricks underperform in PPO compared to DreamerV3, particularly regarding PPO's clipped objective interactions
- Sensitivity to bin range selection: Two-hot encoding performance is highly dependent on bin range choices without clear guidance on optimal configurations across different environment types

## Confidence

**High Confidence**: The empirical finding that symlog predictions significantly help PPO when reward clipping is disabled, as this is directly demonstrated across multiple environments with statistical significance.

**Medium Confidence**: The general claim that DreamerV3 tricks do not improve PPO performance in standard settings, as this depends heavily on implementation details and hyperparameter choices.

**Low Confidence**: The assertion that these tricks are "generally not beneficial" for PPO, given the sensitivity to reward clipping settings and potential for improved performance with proper tuning.

## Next Checks

1. Conduct systematic hyperparameter sweeps for each trick individually on a diverse set of environments to determine optimal configurations and identify conditions under which each trick becomes beneficial.

2. Implement ablation studies focusing on the interaction between symlog predictions and reward clipping, testing intermediate clipping thresholds to understand the precise mechanism behind symlog's effectiveness.

3. Compare the gradient flow and value prediction stability between PPO with symlog predictions versus standard PPO using gradient norm analysis and value prediction error tracking across training episodes.