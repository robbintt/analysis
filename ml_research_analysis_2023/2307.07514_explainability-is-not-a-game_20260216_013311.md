---
ver: rpa2
title: Explainability is NOT a Game
arxiv_id: '2307.07514'
source_url: https://arxiv.org/abs/2307.07514
tags:
- values
- shapley
- feature
- features
- explainability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical argument demonstrating that Shapley
  values, a popular measure of feature importance in explainable AI (XAI), can produce
  misleading results. The authors show that Shapley values may assign higher importance
  to irrelevant features than to relevant ones for a given prediction.
---

# Explainability is NOT a Game

## Quick Facts
- arXiv ID: 2307.07514
- Source URL: https://arxiv.org/abs/2307.07514
- Reference count: 4
- This paper presents a theoretical argument demonstrating that Shapley values can produce misleading results in explainable AI

## Executive Summary
This paper presents a theoretical argument demonstrating that Shapley values, a popular measure of feature importance in explainable AI (XAI), can produce misleading results. The authors show that Shapley values may assign higher importance to irrelevant features than to relevant ones for a given prediction. They identify seven potential issues with Shapley values for explainability and demonstrate through analysis of all 4-variable boolean functions that these issues occur frequently. The authors conclude that Shapley values are fundamentally flawed for feature attribution in XAI and argue that tools like SHAP, which approximate Shapley values, cannot guarantee rigorous explanations.

## Method Summary
The paper employs theoretical analysis and empirical validation on boolean functions. The method involves computing abductive explanations (AXp) and contrastive explanations (CXp) to determine feature relevance, then computing exact Shapley values for comparison. The authors use polynomial-time algorithms to analyze all 4-variable boolean functions represented as truth tables. They compare the Shapley value rankings with logical relevance classifications to identify discrepancies and demonstrate the inadequacy of Shapley values for explainability.

## Key Results
- Shapley values can assign higher importance to irrelevant features than to relevant ones
- Over 99% of 4-variable boolean functions exhibit at least one of seven identified issues with Shapley values
- The paper identifies seven specific issues (I1-I7) where Shapley values produce misleading feature importance rankings
- SHAP and other approximation methods are argued to be fundamentally flawed due to their reliance on Shapley values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley values assign misleading importance by counting coalitions that should be disallowed in explainability
- Mechanism: In game theory, Shapley values average contributions over all possible feature coalitions. In explainability, certain coalitions should be excluded because they violate logical sufficiency constraints. Including these coalitions inflates the Shapley value for irrelevant features.
- Core assumption: Feature coalitions in explainability must respect logical consistency with the prediction (sufficiency and minimality)
- Evidence anchors:
  - [abstract] "by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction"
  - [section 4.3] "one reason that causes Shapley values to produce misleading information is the fact that some disallowed set of features are accounted for"

### Mechanism 2
- Claim: Formal explanations use subset-minimality while Shapley values don't, causing discrepancy
- Mechanism: Abductive explanations require subset-minality (no feature can be removed without breaking sufficiency). Shapley values treat all subsets equally, including non-minimal ones that contain irrelevant features. This causes irrelevant features to accumulate non-zero Shapley values.
- Core assumption: Subset-minimality is essential for meaningful feature importance in explanations
- Evidence anchors:
  - [section 2.2] "An AXp is defined as a subset-minimal (or irreducible) set of features X ‚äÜ F such that the features in X are sufficient for the prediction"
  - [section 4] "The logic statement above clearly states that, if we fix the values of the features identified by any AXp then, no matter the value picked for feature ùëù, the prediction is guaranteed to be ùëê = ùúÖ (v)"

### Mechanism 3
- Claim: Shapley values treat all input distributions equally while explainability requires uniform treatment of irrelevant features
- Mechanism: For irrelevant features, their value can vary freely without affecting the prediction. Shapley values assign contributions based on how feature values change the average prediction, but don't account for the fact that irrelevant features can take any value. This leads to non-zero Shapley values for irrelevant features.
- Core assumption: The treatment of irrelevant features in Shapley value computation should reflect their logical irrelevance
- Evidence anchors:
  - [section 3] "A feature ùëñ ‚àà F is relevant if it is contained in some AXp, i.e. ùëñ ‚àà FA( E ) = FC( E ); otherwise it is irrelevant, i.e. ùëñ ‚àâ FA( E )"
  - [section 4.1] "The feature with the largest absolute Shapley value is irrelevant for the prediction"

## Foundational Learning

- Concept: Feature relevance vs irrelevance in formal explanations
  - Why needed here: The paper's core argument relies on distinguishing relevant features (those that appear in at least one minimal sufficient explanation) from irrelevant ones (those that never appear). Understanding this distinction is crucial for grasping why Shapley values fail.
  - Quick check question: How does the paper define when a feature is relevant vs irrelevant for a given prediction?

- Concept: Abductive explanations (AXp) and contrastive explanations (CXp)
  - Why needed here: These formal explanation types are used to establish feature relevance and form the basis for the logical argument against Shapley values. AXp identifies features sufficient for a prediction, while CXp identifies features that could change it.
  - Quick check question: What is the relationship between AXp and CXp sets according to the paper?

- Concept: Shapley value computation mechanics
  - Why needed here: Understanding how Shapley values calculate feature contributions (averaging marginal contributions over all possible coalitions) is essential to see why they can assign importance to irrelevant features.
  - Quick check question: What is the formula for computing the Shapley value of a feature, and what does each component represent?

## Architecture Onboarding

- Component map:
  - Feature space representation (F, D_i, F)
  - Classifier function (Œ∫: F ‚Üí K)
  - Formal explanation engine (AXp/CXp computation)
  - Shapley value calculator
  - Analysis pipeline (truth table generation, relevance determination, Shapley value computation)

- Critical path:
  1. Load classifier and instance
  2. Generate truth table or circuit representation
  3. Compute AXp/CXp sets to determine feature relevance
  4. Compute Shapley values for all features
  5. Compare Shapley values with relevance classification
  6. Identify and report issues

- Design tradeoffs:
  - Exact vs approximate Shapley value computation: Exact computation is exponential in feature count but necessary for rigorous analysis
  - Truth table vs circuit representation: Truth tables are feasible for small feature counts but circuit representation enables larger problems
  - Comprehensive vs targeted analysis: Analyzing all boolean functions exhaustively provides strong evidence but is computationally intensive

- Failure signatures:
  - Issue I1: Irrelevant features with non-zero Shapley values
  - Issue I2: Irrelevant features with higher absolute Shapley values than relevant ones
  - Issue I3: Relevant features with zero Shapley values
  - Issue I4: Combination of I1 and I3
  - Issue I5: Irrelevant feature with highest absolute Shapley value
  - Issue I6: Irrelevant and relevant features with same sign Shapley values
  - Issue I7: Combination of I2 and I6

- First 3 experiments:
  1. Verify the running example: Compute AXp/CXp for the decision tree in Figure 1, determine feature relevance, compute Shapley values, and check if feature 1 has the largest absolute value
  2. Test on small boolean functions: Analyze all 3-variable boolean functions to see if issues appear with fewer features
  3. Compare with SHAP: Compute exact Shapley values and SHAP approximations for the same classifier to quantify correlation differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative metrics or methods could be developed that respect feature (ir)relevancy while providing accurate feature importance measures for explainable AI?
- Basis in paper: [explicit] The authors state "Given the demonstrated inadequacy of Shapley values for explainability, a natural line of research is whether an alternative metric could be devised which respects feature (ir)relevancy."
- Why unresolved: While the paper identifies issues with Shapley values and suggests the need for alternatives, it does not propose or explore specific alternative metrics that could solve these problems.
- What evidence would resolve it: Development and validation of new metrics through theoretical analysis and empirical testing showing they correctly identify relevant vs. irrelevant features in various scenarios.

### Open Question 2
- Question: How do the identified issues with Shapley values for explainability manifest in real-world, high-stakes application domains like medical diagnostics or legal decision-making?
- Basis in paper: [explicit] The authors argue that "the continued use of XAI approaches based on Shapley values in high-risk domains will inevitably cause human decision makers to assign importance to unimportant features, and to overlook important features."
- Why unresolved: The paper focuses on theoretical analysis and boolean functions, but does not provide empirical evidence of how these issues affect real-world applications.
- What evidence would resolve it: Case studies and empirical analyses of XAI systems using Shapley values in critical domains, demonstrating the practical impact of the identified issues.

### Open Question 3
- Question: What is the relationship between the sign of a feature's Shapley value and its perceived impact on the prediction in explainability contexts?
- Basis in paper: [explicit] The authors mention that "one might be tempted to argue that the sign of Sv(1) differs from the sign of Sv(2), Sv(3), Sv(4), and that that could explain the reported issue. However, the hypothetical relationship between the sign of the Shapley values and their perceived impact of the value of the prediction is a flawed argument."
- Why unresolved: The paper dismisses the relationship between sign and impact but does not provide a thorough analysis of this relationship or its implications for explainability.
- What evidence would resolve it: Systematic investigation of the correlation between Shapley value signs and feature relevance across various classifiers and datasets, potentially leading to a refined understanding of how to interpret Shapley values in explainability.

## Limitations
- The paper's argument is primarily theoretical, with empirical validation limited to 4-variable boolean functions
- Computational complexity of exact Shapley value computation limits the scope of empirical validation
- While seven issues are identified, formal definitions and complete enumeration are not fully specified

## Confidence

- **High confidence**: The theoretical argument that Shapley values can assign misleading importance rankings is well-founded and logically consistent with the definitions of feature relevance in formal explanations.
- **Medium confidence**: The empirical claim that issues occur in over 99% of 4-variable boolean functions, as this is based on a specific and limited computational experiment.
- **Low confidence**: The broader claim that all approximate Shapley value methods (like SHAP) are fundamentally flawed, as this extrapolates from exact computation results to approximation methods.

## Next Checks

1. Implement the polynomial-time algorithms for computing AXp's and CXp's from truth tables to verify the feature relevance determination process and reproduce the 99% failure rate claim.
2. Test the Shapley value computation on larger boolean functions (5-6 variables) to assess how computational complexity affects the empirical validation and whether the identified issues persist at scale.
3. Compare the exact Shapley values with SHAP approximations on the same classifiers to quantify the correlation between them and determine if SHAP's approximation quality degrades in cases where exact Shapley values are misleading.