---
ver: rpa2
title: 'Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to
  Demystify Image Recognition'
arxiv_id: '2311.04940'
source_url: https://arxiv.org/abs/2311.04940
tags:
- framework
- geoscience
- recognition
- proposed
- earth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an interpretable geoscience artificial intelligence
  (XGeoS-AI) framework to unravel the mystery of image recognition in Earth sciences.
  The framework is inspired by human visual decision-making and uses local threshold
  generation combined with various AI engines (SVR, MLP, CNN) to achieve interpretable
  recognition of geoscience images.
---

# Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to Demystify Image Recognition

## Quick Facts
- arXiv ID: 2311.04940
- Source URL: https://arxiv.org/abs/2311.04940
- Reference count: 23
- One-line primary result: XGeoS-AI framework achieves Dice coefficients >0.98 for soil crack CT images and >0.96 for biotite distribution in granite, while maintaining interpretability.

## Executive Summary
This study introduces the XGeoS-AI framework, an interpretable artificial intelligence approach for geoscience image recognition inspired by human visual decision-making. The framework generates local thresholds from image patches using AI engines (SVR, MLP, CNN) to segment cracks or geological features, replacing complex black-box models with a transparent, single-step process. Applied to soil crack and biotite recognition tasks, XGeoS-AI achieves state-of-the-art performance while providing visual explanations via SHAP values. The method addresses the opacity of traditional AI models in geoscience, offering a pathway for wider adoption of AI in Earth sciences.

## Method Summary
The XGeoS-AI framework uses local threshold generation from image patches, trained via AI engines (SVR, MLP, CNN) to predict a single threshold value for segmentation. The method employs a heuristic to generate training pairs (image patch, optimal threshold) and applies the predicted threshold to segment cracks or geological features. Performance is evaluated using Dice, precision, recall, and Jaccard metrics, with SHAP values used to validate interpretability. The approach is tested on CT images of soil cracks and biotite in granite, demonstrating high accuracy and transparency.

## Key Results
- Achieved Dice coefficients >0.98 for soil crack recognition across most test images.
- Successfully recognized biotite distribution in granite CT images with Dice >0.96.
- SHAP visualizations confirmed the interpretability of threshold predictions for both CNN and MLP models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local threshold generation from image patches enables interpretable crack detection without heavy black-box models.
- Mechanism: The XGeoS-AI framework mimics human visual decision-making by first learning a threshold value from a small region of the image using an AI engine (SVR, MLP, or CNN). This threshold is then applied across the image to segment cracks, replacing complex encoder-decoder pipelines with a single interpretable step.
- Core assumption: Crack pixels have a distinct grayscale range that can be captured by a locally learned threshold.
- Evidence anchors:
  - [abstract] "Inspired by the mechanism of human vision, the proposed XGeoS-AI framework generates a threshold value from a local region within the whole image to complete the recognition."
  - [section] "In the procedure of human decision in classifying vision signals, it often takes two steps to make an accurate decision... The first step is often the process of getting visional representation from the environment... Then, high level semantic information is generated in the second step to make the decision according to the knowledge stored in the brain."
- Break condition: If crack pixels do not exhibit consistent local intensity patterns, or if noise dominates the local region, threshold prediction fails.

### Mechanism 2
- Claim: Using simple, well-understood AI engines (SVR, MLP, CNN) with interpretable post-processing preserves model transparency while maintaining performance.
- Mechanism: Each AI engine predicts a threshold value instead of a full segmentation map. The interpretability comes from the clear, single-step application of this threshold, which is traceable and explainable compared to multi-layer neural networks.
- Core assumption: Threshold prediction can be framed as a regression task solvable by standard ML models.
- Evidence anchors:
  - [abstract] "Different kinds of artificial intelligence (AI) methods, such as Support Vector Regression (SVR), Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), can be adopted as the AI engines of the proposed XGeoS-AI framework to efficiently complete geoscience image recognition tasks."
  - [section] "Inspired by the process of human decision, we propose a XGeoS-AI framework for demystifying the working mechanism of geoscience image recognition."
- Break condition: If the regression target (threshold) is not learnable from the training data, or if the AI engines are too simple for complex patterns.

### Mechanism 3
- Claim: Visual explanations (SHAP values) validate the interpretability of the XGeoS-AI framework even for deep models.
- Mechanism: SHAP (Shapley Additive Explanations) is applied to the CNN and MLP engines to visualize which image regions contribute most to the threshold prediction, confirming the decision process is traceable.
- Core assumption: SHAP can meaningfully attribute model decisions to input regions for regression tasks like threshold prediction.
- Evidence anchors:
  - [section] "We adopt a Shapley Additive Explanations (SHAP) approach to assess the interpretability of the proposed framework for geoscience image recognition... The visualization of the SHAP values of these two engines further demonstrates the interpretability of the proposed XGeoS-AI framework."
- Break condition: If SHAP values are noisy or uninformative, or if the model decisions are too complex to be captured by local explanations.

## Foundational Learning

- Concept: Local threshold generation
  - Why needed here: Allows crack detection without full segmentation models, making the process interpretable and computationally efficient.
  - Quick check question: Can you explain how a single threshold value can segment an image, and why this is simpler than a full pixel-wise classifier?

- Concept: Support Vector Regression (SVR) for threshold prediction
  - Why needed here: SVR is a well-understood ML model that can predict a continuous threshold value, fitting within the interpretable framework.
  - Quick check question: What is the main difference between SVR and a standard regression model, and how does that help in threshold prediction?

- Concept: SHAP for model interpretability
  - Why needed here: Provides visual, quantitative explanations for model decisions, validating the transparency of the XGeoS-AI framework.
  - Quick check question: How do SHAP values help in understanding which parts of an image influence the model's threshold prediction?

## Architecture Onboarding

- Component map:
  Data loader -> Threshold generator -> AI engine (SVR/MLP/CNN) -> Segmentation module -> Evaluation

- Critical path:
  1. Load image and ground truth.
  2. Generate training pairs (image patch, optimal threshold).
  3. Train AI engine to predict threshold.
  4. Apply predicted threshold to new image.
  5. Evaluate segmentation and visualize SHAP.

- Design tradeoffs:
  - Simplicity vs. accuracy: Simple AI engines are interpretable but may miss complex patterns; deeper models increase accuracy but reduce transparency.
  - Local vs. global: Local threshold generation is interpretable but may fail on heterogeneous regions.
  - Preprocessing: Denoising improves threshold prediction but adds complexity.

- Failure signatures:
  - Predicted thresholds far from true values.
  - Segmentation masks with many false positives/negatives.
  - SHAP visualizations show no clear pattern.
  - Dice/Jaccard scores drop sharply on certain images.

- First 3 experiments:
  1. Run threshold generation heuristic on a sample image and compare with manual annotation.
  2. Train a simple SVR on a small dataset and test threshold prediction accuracy.
  3. Visualize SHAP values for CNN on a sample image to confirm interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed XGeoS-AI framework be further optimized to handle images with multiple noisy spots or artifacts, such as those seen in test images T3 and T4?
- Basis in paper: [explicit] The paper mentions that the proposed framework's performance is affected by the presence of multiple noisy spots in images T3 and T4, and suggests that more advanced AI engines or preprocessing techniques like denoising could potentially improve results.
- Why unresolved: The study demonstrates the framework's effectiveness but does not explore advanced techniques or modifications to specifically address noisy or artifact-laden images.
- What evidence would resolve it: Comparative experiments using more advanced AI engines (e.g., deeper CNNs) or preprocessing methods (e.g., denoising algorithms) to evaluate their impact on recognition performance in noisy images.

### Open Question 2
- Question: What are the trade-offs between interpretability and accuracy when applying the XGeoS-AI framework to more complex geoscience problems, such as real-time environmental monitoring or large-scale geological mapping?
- Basis in paper: [inferred] The paper emphasizes the importance of interpretability but does not address potential trade-offs in more complex applications where high accuracy might be critical.
- Why unresolved: The framework's interpretability is demonstrated in controlled experiments, but its performance and interpretability in more complex, real-world scenarios are not explored.
- What evidence would resolve it: Case studies or field applications of the XGeoS-AI framework in complex geoscience tasks, comparing interpretability and accuracy with traditional black-box models.

### Open Question 3
- Question: How can the XGeoS-AI framework be adapted to handle multi-modal geoscience data, such as combining seismic, satellite, and in-situ sensor data, while maintaining interpretability?
- Basis in paper: [explicit] The paper focuses on single-modal image recognition tasks and does not explore the integration of multi-modal data sources.
- Why unresolved: Geoscience problems often involve multi-modal data, and the framework's ability to handle such data while preserving interpretability is not addressed.
- What evidence would resolve it: Experiments integrating multi-modal data sources (e.g., seismic and satellite imagery) and evaluating the framework's interpretability and performance in these scenarios.

## Limitations
- Performance drops on images with multiple noisy spots (e.g., T3, T4), suggesting sensitivity to artifacts.
- Lack of detailed CNN and MLP architecture specifications hinders reproducibility.
- Limited validation to two specific geoscience tasks (soil cracks, biotite), raising questions about generalizability.

## Confidence
- **High**: The interpretability mechanism via local threshold generation and SHAP visualizations is well-supported by the results and aligns with human visual decision-making theory.
- **Medium**: The performance metrics are strong, but the absence of detailed model hyperparameters and architectures introduces uncertainty in replication.
- **Low**: The framework's generalization to other geoscience domains beyond soil cracks and biotite remains unproven.

## Next Checks
1. Replicate threshold prediction: Train the SVR, MLP, and CNN models on the provided dataset and verify that predicted thresholds match ground truth within acceptable error margins.
2. Test on noisy images: Apply the framework to images with known noise (e.g., T3, T4) and analyze whether predicted thresholds deviate significantly from true values.
3. Visualize SHAP values: Generate SHAP explanations for the CNN and MLP models on test images to confirm that the model decisions are traceable and interpretable.