---
ver: rpa2
title: Counterfactually Fair Representation
arxiv_id: '2311.05420'
source_url: https://arxiv.org/abs/2311.05420
tags:
- counterfactual
- fairness
- causal
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning fair machine learning
  models under the Counterfactual Fairness (CF) criterion, which requires that an
  individual's outcome remains the same in the real world as it would be in a counterfactual
  world with a different sensitive attribute. Existing CF methods discard features
  that are descendants of the sensitive attribute, leading to performance loss.
---

# Counterfactually Fair Representation

## Quick Facts
- arXiv ID: 2311.05420
- Source URL: https://arxiv.org/abs/2311.05420
- Authors: [List of authors]
- Reference count: 40
- Primary result: Novel method for learning counterfactually fair representations using all available features, achieving near-zero total effect while maintaining high accuracy.

## Executive Summary
This paper proposes a novel method for learning counterfactually fair representations in machine learning models. The approach generates counterfactual samples based on a causal structure and applies a symmetric function to factual and counterfactual features. This method allows the model to leverage all available features, including descendants of the sensitive attribute, which are typically discarded in existing counterfactual fairness methods. The authors theoretically prove that models trained with these representations satisfy perfect counterfactual fairness and demonstrate significant improvements in both fairness and performance compared to existing baselines on real-world datasets.

## Method Summary
The proposed method involves generating counterfactual samples for each data point using a causal model, then applying a symmetric function (like averaging) to both factual and counterfactual features to create counterfactually fair representations. These representations are used to train machine learning models that inherently satisfy counterfactual fairness. The approach extends to path-dependent counterfactual fairness by considering only the descendants of the sensitive attribute along specific paths. The method uses variational autoencoders (VAEs) to model the causal structure and generate counterfactual samples, with the symmetric function ensuring that the resulting representations are invariant to changes in the sensitive attribute.

## Key Results
- Achieves near-zero total effect (TE) while maintaining high accuracy/MSE, improving fairness by up to 92% compared to the best-performing baseline.
- Outperforms existing counterfactual fairness baselines (CE, CR, ICA) on both fairness (TE) and performance metrics (accuracy/MSE) across two real-world datasets.
- Demonstrates the effectiveness of using all available features, including descendants of sensitive attributes, for improved fairness and performance compared to methods that discard these features.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The symmetric function applied to factual and counterfactual features ensures counterfactual fairness by averaging out the effect of the sensitive attribute.
- Mechanism: By generating counterfactual samples where the sensitive attribute is flipped and applying a symmetric function (like averaging) to both factual and counterfactual features, the representation becomes invariant to the sensitive attribute's value.
- Core assumption: The symmetric function effectively neutralizes the influence of the sensitive attribute on the representation.
- Evidence anchors:
  - [abstract] "The idea is to first generate counterfactual samples of each data point based on the causal structure, and the fair representations can be generated subsequently by applying a symmetric function to both factual (i.e., original data) and counterfactual samples."
  - [section] "The idea is to first apply a symmetric function s(·) to both factual feature x and counterfactual features {ˇx[j]}|A|−1j=1."
- Break condition: If the symmetric function is not truly symmetric or if the counterfactual samples are not generated correctly according to the causal model, the fairness guarantee may not hold.

### Mechanism 2
- Claim: Training models on representations that satisfy counterfactual fairness ensures the model itself satisfies counterfactual fairness.
- Mechanism: Since the representation h(x, a; M, s) is constructed to satisfy counterfactual fairness by design, any model trained on this representation will inherit this property.
- Core assumption: The fairness property of the representation is preserved when used as input for downstream models.
- Evidence anchors:
  - [abstract] "We theoretically and empirically show that models trained with this method can satisfy CF."
  - [section] "We theoretically show that ML models (or any other downstream tasks) trained with counterfactually fair representations can satisfy perfect CF."
- Break condition: If the model training process introduces biases or if the representation is altered in a way that breaks the fairness property, the model may not satisfy counterfactual fairness.

### Mechanism 3
- Claim: The use of counterfactual samples allows the model to learn patterns that are invariant to the sensitive attribute.
- Mechanism: By exposing the model to both factual and counterfactual samples during training, it learns to predict outcomes based on features that are causally independent of the sensitive attribute.
- Core assumption: The model can effectively learn from the augmented dataset containing both factual and counterfactual samples.
- Evidence anchors:
  - [abstract] "The idea is to first generate counterfactual samples of each data point based on the causal structure, and the fair representations can be generated subsequently by applying a symmetric function to both factual (i.e., original data) and counterfactual samples."
  - [section] "The idea is to first apply a symmetric function s(·) to both factual feature x and counterfactual features {ˇx[j]}|A|−1j=1."
- Break condition: If the counterfactual samples are not representative or if the model cannot effectively learn from them, the fairness may not be achieved.

## Foundational Learning

- Concept: Causal graphs and structural equations
  - Why needed here: Understanding the causal relationships between variables is crucial for generating counterfactual samples and defining counterfactual fairness.
  - Quick check question: Can you explain what a directed acyclic graph (DAG) is and how it relates to causal models?

- Concept: Counterfactual inference
  - Why needed here: Counterfactual inference is the process of determining what would have happened under different conditions, which is essential for generating counterfactual samples.
  - Quick check question: What are the three steps of counterfactual inference as described in the paper?

- Concept: Fairness notions in machine learning
  - Why needed here: Understanding different fairness definitions helps in comparing counterfactual fairness with other approaches and in interpreting the results.
  - Quick check question: How does counterfactual fairness differ from demographic parity or equalized odds?

## Architecture Onboarding

- Component map:
  Causal model M -> Encoder -> Decoder -> Symmetric function s -> Predictor gw

- Critical path:
  1. Sample latent variables from the causal model.
  2. Generate counterfactual samples using the decoder.
  3. Apply the symmetric function to factual and counterfactual features.
  4. Train the predictor on the fair representation.

- Design tradeoffs:
  - Using all features vs. discarding descendants of sensitive attributes: Our method uses all features, potentially improving performance but requiring more complex processing.
  - Choice of symmetric function: Different symmetric functions may affect the fairness and performance of the model.
  - Complexity of the causal model: A more complex causal model may capture more relationships but could be harder to learn and use.

- Failure signatures:
  - High total effect (TE) values: Indicates that the model is not achieving counterfactual fairness.
  - Significant drop in accuracy: Suggests that the fair representation is losing important information.
  - Inconsistent performance across different causal models: May indicate that the method is sensitive to the assumptions of the causal model.

- First 3 experiments:
  1. Train a model using only non-descendants of the sensitive attribute (CE baseline) and compare its performance and fairness to our method.
  2. Test the method with different symmetric functions (e.g., median, max) to see how they affect fairness and accuracy.
  3. Apply the method to a dataset with a different causal structure to verify its generalizability.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, based on the content, some potential open questions include:
- How does the proposed method perform on datasets with different levels of correlation between sensitive attributes and other features?
- Can the proposed method be extended to handle multiple sensitive attributes?
- How does the proposed method compare to other fairness notions, such as demographic parity or equal opportunity?

## Limitations
- The method requires a known causal graph, which may not be available or accurate in many real-world scenarios.
- The symmetric function approach may not generalize well to highly nonlinear relationships or complex feature interactions.
- The method's performance depends on the quality of counterfactual generation, which is constrained by the capacity and assumptions of the underlying VAE models.

## Confidence
- Confidence in core claims: High for the theoretical framework and empirical improvements, but Medium for real-world robustness due to the dependence on assumed causal structures.

## Next Checks
1. Test the method on datasets with different causal graph structures (e.g., including feedback loops or hidden confounders) to assess robustness to causal model assumptions.
2. Evaluate the method on datasets with noisy or incomplete causal knowledge to understand performance degradation when the causal model is imperfect.
3. Conduct ablation studies removing specific features or relationships from the causal graph to quantify the impact of model misspecification on fairness and accuracy.