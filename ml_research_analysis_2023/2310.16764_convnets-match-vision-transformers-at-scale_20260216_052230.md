---
ver: rpa2
title: ConvNets Match Vision Transformers at Scale
arxiv_id: '2310.16764'
source_url: https://arxiv.org/abs/2310.16764
tags:
- vision
- training
- compute
- learning
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConvNets are competitive with Vision Transformers on large-scale
  pre-training. The authors pre-train NFNet models on the JFT-4B dataset across varying
  compute budgets (0.4k-110k TPU-v4 core hours) and observe log-log scaling laws between
  validation loss and compute.
---

# ConvNets Match Vision Transformers at Scale

## Quick Facts
- arXiv ID: 2310.16764
- Source URL: https://arxiv.org/abs/2310.16764
- Reference count: 11
- ConvNets achieve competitive performance with ViTs on large-scale pre-training

## Executive Summary
This paper challenges the prevailing notion that Vision Transformers are inherently superior to ConvNets at scale by demonstrating that ConvNet architectures (specifically NFNet) can match ViT performance when pre-trained on large datasets like JFT-4B. The authors systematically vary compute budgets and observe log-log scaling laws between validation loss and compute, finding that both architectures achieve similar performance when given comparable training resources. The key insight is that model size and training epochs should be scaled together at the same rate as compute increases.

## Method Summary
The authors pre-train NFNet models (F0, F1, F3, F7 variants) on JFT-4B across compute budgets ranging from 0.4k to 110k TPU-v4 core hours, varying depth and width configurations. They observe log-log scaling laws between validation loss and compute budget, finding that optimal model size and epoch budget increase proportionally with compute. After pre-training, models are fine-tuned on ImageNet for 50 epochs at 384x384 resolution using techniques like SAM, stochastic depth, and dropout. Learning rate tuning is critical, with optimal rates decreasing as model size and training duration increase.

## Key Results
- NFNet models achieve 90.4% top-1 accuracy on ImageNet after fine-tuning, matching ViT performance with similar compute budgets
- Log-log scaling laws observed between validation loss and compute budget across architectures
- Optimal model size and epoch budget both increase proportionally with compute budget following simple scaling rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ConvNets achieve competitive performance with ViTs when pre-trained on large-scale datasets with sufficient compute.
- Mechanism: Both ConvNets and ViTs benefit from large-scale pre-training, and the performance difference between them diminishes when trained with comparable compute budgets on datasets like JFT-4B.
- Core assumption: The computational efficiency and architectural expressivity of ConvNets scale similarly to ViTs when given access to large datasets and compute.
- Evidence anchors:
  - [abstract] "After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets."
  - [section] "The performance of pre-trained NFNets at scale is remarkably similar to the performance of pre-trained Vision Transformers."
  - [corpus] Weak corpus evidence; no direct comparison of ConvNet vs ViT scaling in related papers.
- Break condition: If ConvNets cannot match ViTs in pre-training efficiency or scaling behavior on even larger compute budgets or datasets.

### Mechanism 2
- Claim: Larger models and longer training epochs lead to better performance, following log-log scaling laws.
- Mechanism: Increasing model depth and width, as well as training epochs, results in improved validation loss and downstream accuracy, with predictable scaling behavior.
- Core assumption: The relationship between model size, training time, and performance is consistent across architectures and scales predictably.
- Evidence anchors:
  - [section] "We observe a log-log scaling law between validation loss and compute budget."
  - [section] "The optimal model size and the optimal epoch budget (which achieve the lowest validation loss) both increase in size as the compute budget increases."
  - [corpus] Weak corpus evidence; no scaling law analysis in related papers.
- Break condition: If the scaling law breaks down at larger scales or if the relationship between model size and performance becomes non-linear.

### Mechanism 3
- Claim: Tuning the learning rate is critical for optimizing performance, especially as model size and training epochs increase.
- Mechanism: The optimal learning rate decreases as model size and training epochs increase, and can be efficiently tuned within a small range.
- Core assumption: The learning rate can be predicted and tuned based on model size and training duration.
- Evidence anchors:
  - [section] "We find that all models in the NFNet family show a similar optimal learning rate α ≈ 1.6 for small epoch budgets. However the optimal learning rate falls as the epoch budget rises, and for large models the optimal learning rate falls more quickly."
  - [section] "In practice one can efficiently tune the learning rate within 2 trials by assuming that the optimal learning rate falls slowly but monotonically as both the model size and the epoch budget increases."
  - [corpus] Weak corpus evidence; no learning rate analysis in related papers.
- Break condition: If the learning rate tuning strategy fails to generalize across different model architectures or training regimes.

## Foundational Learning

- Concept: Log-log scaling laws
  - Why needed here: Understanding how performance scales with compute and model size is crucial for optimizing training strategies.
  - Quick check question: What does a log-log scaling law imply about the relationship between compute and validation loss?
- Concept: Pre-training on large-scale datasets
  - Why needed here: Pre-training on large datasets like JFT-4B is essential for achieving competitive performance with modern architectures.
  - Quick check question: Why is pre-training on a dataset with billions of images beneficial for model performance?
- Concept: Fine-tuning on downstream tasks
  - Why needed here: Fine-tuning pre-trained models on specific tasks like ImageNet classification is necessary to achieve high accuracy.
  - Quick check question: How does fine-tuning a pre-trained model on a smaller dataset improve its performance on that task?

## Architecture Onboarding

- Component map: NFNet model family -> JFT-4B dataset -> ImageNet dataset -> TPU-v4 cores
- Critical path:
  1. Pre-train NFNet models on JFT-4B with varying compute budgets
  2. Fine-tune pre-trained models on ImageNet
  3. Evaluate and compare performance with ViTs
- Design tradeoffs:
  - Model size vs. compute budget: Larger models require more compute but achieve better performance
  - Training epochs vs. compute budget: Longer training improves performance but increases computational cost
  - Learning rate tuning: Balancing learning rate with model size and training duration
- Failure signatures:
  - Suboptimal performance: If models do not achieve competitive accuracy with ViTs
  - Scaling law violations: If performance does not follow log-log scaling with compute
  - Learning rate issues: If models fail to converge or overfit due to improper learning rate tuning
- First 3 experiments:
  1. Pre-train a small NFNet model on JFT-4B with a limited compute budget and evaluate its performance on ImageNet.
  2. Increase the model size and training epochs, and observe the impact on validation loss and downstream accuracy.
  3. Fine-tune the pre-trained model with different learning rates and assess the impact on ImageNet accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical relationship between model size, training epochs, and compute budget for optimal performance in ConvNets and Vision Transformers?
- Basis in paper: [explicit] The paper observes that optimal model size and epoch budget both increase with compute budget, following a rule of thumb to scale both at the same rate. It also notes that for large models, the optimal learning rate falls more quickly as epoch budget increases.
- Why unresolved: The paper provides observations but doesn't derive a precise mathematical relationship or formula for the optimal scaling of model size, epochs, and learning rate with compute budget.
- What evidence would resolve it: Systematic experiments varying model size, epochs, and compute budget in a controlled manner to derive the exact scaling laws, potentially using techniques like grid search or Bayesian optimization to find optimal configurations across a wider range of parameters.

### Open Question 2
- Question: How do ConvNets and Vision Transformers compare in terms of pre-training efficiency across different hardware architectures beyond TPU-v4?
- Basis in paper: [explicit] The paper notes that NFNets were optimized for TPU-v4 and perform less well on other devices. It mentions that NFNet-F7+ would require 250 TPU-v3 core hours to pre-train, while the paper only provides estimates for ViTs on TPU-v4.
- Why unresolved: The paper only provides limited comparisons of pre-training efficiency between NFNets and ViTs on different hardware. It doesn't explore a wide range of hardware architectures or provide comprehensive efficiency comparisons.
- What evidence would resolve it: Extensive benchmarking of both ConvNets and Vision Transformers across various hardware architectures (e.g., GPUs, different TPU generations, specialized AI accelerators) to determine their relative efficiency and optimal hardware choices for different model families.

### Open Question 3
- Question: What is the impact of data quality and distribution on the scaling laws observed for ConvNets and Vision Transformers?
- Basis in paper: [explicit] The paper uses JFT-4B, a large labelled dataset of 4 billion images from 30k classes, for pre-training. It mentions removing near-duplicates between ImageNet and JFT-4B, but doesn't explore how data quality or distribution affects scaling.
- Why unresolved: The paper doesn't investigate how variations in data quality, class distribution, or the presence of duplicates/semi-supervised examples affect the observed scaling laws. It also doesn't compare the effects of different pre-training datasets on ConvNet vs. Transformer performance.
- What evidence would resolve it: Controlled experiments varying data quality, class balance, and dataset size for both ConvNets and Vision Transformers. This could include using synthetic data with known properties, introducing controlled amounts of noise or duplicates, or comparing performance on datasets with different class distributions.

## Limitations
- Findings based on a single ConvNet architecture (NFNet) and one large-scale dataset (JFT-4B), limiting generalizability
- Scaling law observations confined to tested compute range (0.4k-110k TPU-v4 core hours), with unknown behavior at larger scales
- Learning rate tuning strategy effectiveness across different architectures and training regimes remains unverified

## Confidence
- **High confidence**: ConvNets can achieve competitive performance with ViTs on large-scale pre-training, given sufficient compute and appropriate architecture design
- **Medium confidence**: The observed log-log scaling laws between validation loss and compute will hold at scales beyond those tested
- **Low confidence**: The proposed learning rate tuning strategy generalizes effectively across diverse model architectures and training scenarios

## Next Checks
1. **Cross-architecture validation**: Test the scaling laws and learning rate tuning strategy with other ConvNet architectures (e.g., EfficientNet, ConvNeXt) on JFT-4B or similar large-scale datasets
2. **Extreme scale testing**: Evaluate model performance and scaling behavior at compute budgets exceeding 110k TPU-v4 core hours to identify potential scaling law break points
3. **Dataset generalization**: Pre-train NFNet and ViT models on alternative large-scale datasets (e.g., Instagram-3.5B, YFCC-100M) to assess the robustness of performance parity findings across different data distributions