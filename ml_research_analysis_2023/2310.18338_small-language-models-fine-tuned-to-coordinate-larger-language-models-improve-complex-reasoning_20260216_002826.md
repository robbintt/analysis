---
ver: rpa2
title: Small Language Models Fine-tuned to Coordinate Larger Language Models improve
  Complex Reasoning
arxiv_id: '2310.18338'
source_url: https://arxiv.org/abs/2310.18338
tags:
- daslam
- solver
- answer
- reasoning
- three
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DaSLaM introduces a modular approach to complex reasoning by separating
  problem decomposition from solution generation, using a small fine-tuned language
  model as a decomposer to guide a larger solver model through subproblems. The decomposer
  is trained with reinforcement learning to optimize interactions with the solver,
  enabling solver-agnostic performance improvements.
---

# Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning

## Quick Facts
- arXiv ID: 2310.18338
- Source URL: https://arxiv.org/abs/2310.18338
- Authors: 
- Reference count: 40
- Key outcome: DaSLaM improves GPT-3.5 accuracy from 41.6% to 54.5% on reasoning benchmarks by using a small fine-tuned decomposer model to guide problem-solving.

## Executive Summary
DaSLaM introduces a modular approach to complex reasoning by separating problem decomposition from solution generation, using a small fine-tuned language model as a decomposer to guide a larger solver model through subproblems. The decomposer is trained with reinforcement learning to optimize interactions with the solver, enabling solver-agnostic performance improvements. On multiple reasoning benchmarks (MATH, AQuA, JEEBench), DaSLaM boosts GPT-3.5's accuracy from 41.6% to 54.5%, surpassing few-shot GPT-4 in some tasks and generalizing effectively across different solver model scales.

## Method Summary
DaSLaM trains a small 13B parameter LLaMA model as a problem decomposer using two-stage fine-tuning: first with supervised fine-tuning on synthetically generated subquestions, then with policy gradient optimization (PPO) using rewards based on entity coverage, answer consistency, operation order, CoT proximity, and final answer correctness. The decomposer generates subproblems conditioned on both the original question and the solver's initial response, enabling targeted decomposition that addresses the solver's specific mistakes. The system is designed to work with any solver LM as a black box, making it solver-agnostic.

## Key Results
- GPT-3.5 accuracy improves from 41.6% to 54.5% on MATH, AQuA, and JEEBench benchmarks
- DaSLaM surpasses few-shot GPT-4 performance on some tasks
- The 13B decomposer outperforms much larger models when used as a decomposer
- Performance generalizes across solver scales, improving smaller LLaMA models as well

## Why This Works (Mechanism)

### Mechanism 1
Separation of decomposition and solution generation into two specialized modules allows each to be optimized for its specific task without interference from the other's errors. The decomposer LM generates subproblems conditioned on both the original question and the solver's initial response, enabling targeted decomposition that addresses the solver's specific mistakes. This assumes problem decomposition and solution generation are distinct capabilities that benefit from independent optimization.

### Mechanism 2
Reinforcement learning from the solver's feedback enables the decomposer to specialize in generating subproblems that the solver can actually solve. The decomposer is trained using policy gradient optimization (PPO) with rewards based on entity coverage, answer consistency, operation order, CoT proximity, and final answer correctness. This assumes the decomposer can learn an effective policy for generating subproblems by observing the solver's responses without needing access to the solver's internal reasoning.

### Mechanism 3
Solver-agnostic decomposition enables performance improvements across different solver scales without requiring fine-tuning of the solver. The decomposer is trained to work with any solver LM as a black box, using only the solver's outputs as feedback, allowing the same decomposer to improve solvers of varying sizes. This assumes effective problem decomposition is largely independent of the solver's scale and architecture.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) prompting**
  - Why needed here: Understanding CoT is essential to grasp why DaSLaM's decomposition approach is necessary and how it differs from simply asking the model to explain its reasoning steps.
  - Quick check question: What is the main limitation of CoT prompting that DaSLaM aims to address?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: The decomposer is trained using a similar approach to RLHF, where it learns to generate better decompositions based on feedback from the solver's performance.
  - Quick check question: How does the reward function in DaSLaM differ from typical RLHF reward functions used for aligning language models?

- **Concept: Policy Gradient Methods (PPO)**
  - Why needed here: Understanding PPO is crucial for implementing the training of the decomposer LM, as it uses this algorithm to optimize the decomposition policy.
  - Quick check question: Why might PPO be preferred over other policy gradient methods for training the decomposer in this setup?

## Architecture Onboarding

- **Component map**: Problem → Initial solver response → Decomposer generates subproblems → Solver solves subproblems iteratively → Final answer consolidation

- **Critical path**: Question → Initial solver response → Subproblem generation → Iterative subproblem solving → Final answer generation

- **Design tradeoffs**:
  - Small decomposer vs. large decomposer: Smaller decomposer reduces computational cost but may have limited decomposition capabilities
  - Supervised fine-tuning vs. pure RL: Supervised pre-training provides a better starting point but requires labeled data
  - Feedback frequency: More frequent feedback may improve learning but increases computational cost
  - Subproblem granularity: Finer decomposition may help but risks generating too many subproblems

- **Failure signatures**:
  - Decomposers generating subproblems that are too complex for the solver to handle
  - Subproblems that don't provide sufficient guidance for the solver to correct its initial mistakes
  - Reinforcement learning failing to converge due to sparse or noisy rewards
  - Solver overfitting to specific decomposition patterns and failing on novel problem types

- **First 3 experiments**:
  1. Implement the supervised fine-tuning stage with the LLaMA 13B model using the provided dataset of questions and subquestions
  2. Set up the PPO training loop with the reward functions (entity coverage, answer consistency, etc.) and train the decomposer with a small solver (LLaMA 13B)
  3. Evaluate the trained decomposer on a simple reasoning task (e.g., GSM8K) with both the original solver and different solver scales to test solver-agnostic generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DaSLaM scale when using even smaller decomposer models (e.g., 1-3 billion parameters) compared to the 13B LLaMA model?
- Basis in paper: [explicit] The paper states that a finetuned LLaMA 13B decomposer outperforms the much larger GPT-3.5 when used as a decomposer, but doesn't explore smaller decomposer sizes
- Why unresolved: The paper only tests with LLaMA 13B as the decomposer and doesn't systematically investigate the minimum effective size for the decomposer model
- What evidence would resolve it: Experiments comparing DaSLaM performance with decomposer models of varying sizes (1B, 3B, 7B, 13B) while keeping the solver model constant

### Open Question 2
- Question: Can DaSLaM's decomposition capabilities generalize to non-mathematical reasoning tasks like common sense reasoning or narrative understanding?
- Basis in paper: [inferred] The paper demonstrates DaSLaM on mathematical and physics problems but doesn't test it on broader reasoning domains that require different types of decomposition
- Why unresolved: The evaluation is limited to mathematical and physics reasoning datasets, leaving open whether the decomposition strategy generalizes to other reasoning types
- What evidence would resolve it: Testing DaSLaM on benchmarks for common sense reasoning (e.g., HellaSwag), narrative understanding (e.g., MCTest), or logical reasoning tasks

### Open Question 3
- Question: What is the theoretical upper bound on the complexity of problems that DaSLaM can effectively decompose and solve?
- Basis in paper: [inferred] The paper shows improvements on existing benchmarks but doesn't characterize the limits of problem complexity that DaSLaM can handle
- Why unresolved: The experiments use problems of moderate complexity, and there's no analysis of how problem structure or depth affects DaSLaM's performance
- What evidence would resolve it: Systematic testing of DaSLaM on problems of increasing structural complexity, with analysis of decomposition quality and success rates across problem types

## Limitations

- Evaluation relies primarily on synthetic subquestion generation using GPT-3.5, which may not generalize to real-world decomposition scenarios
- Success depends on the assumption that complex reasoning problems can be meaningfully decomposed into independently solvable subproblems
- The reinforcement learning approach requires careful reward engineering, and the reported reward functions may not capture all relevant aspects of effective decomposition

## Confidence

- **High Confidence**: The core architectural claim that separating decomposition from solution generation can improve reasoning performance is well-supported by the empirical results across multiple benchmarks
- **Medium Confidence**: The solver-agnostic generalization claim is demonstrated across different model scales but lacks testing with architecturally diverse solvers
- **Medium Confidence**: The reinforcement learning mechanism for training the decomposer is theoretically sound, but the specific reward functions and their relative weights could significantly impact performance

## Next Checks

1. **Cross-Architecture Solver Testing**: Evaluate DaSLaM with solver models that have fundamentally different architectures (e.g., convolutional models, graph neural networks, or rule-based systems) to verify the solver-agnostic claim beyond scale variations.

2. **Real-World Decomposition Validation**: Test the system using datasets where ground truth decompositions are available from human experts, rather than relying solely on synthetic decompositions generated by GPT-3.5, to assess performance on authentic decomposition tasks.

3. **Reward Function Ablation Study**: Systematically vary the relative weights of different reward components (entity coverage, answer consistency, operation order, CoT proximity) to identify which aspects are most critical for effective decomposition and whether simpler reward functions could achieve comparable results.