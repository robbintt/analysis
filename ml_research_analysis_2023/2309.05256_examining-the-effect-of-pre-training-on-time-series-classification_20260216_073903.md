---
ver: rpa2
title: Examining the Effect of Pre-training on Time Series Classification
arxiv_id: '2309.05256'
source_url: https://arxiv.org/abs/2309.05256
tags:
- pre-training
- time
- series
- data
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the effect of unsupervised pre-training followed
  by fine-tuning on time series classification. The authors conduct a thorough analysis
  of 150 classification datasets derived from the Univariate Time Series (UTS) and
  Multivariate Time Series (MTS) benchmarks, using three model structures and five
  pre-training tasks.
---

# Examining the Effect of Pre-training on Time Series Classification

## Quick Facts
- arXiv ID: 2309.05256
- Source URL: https://arxiv.org/abs/2309.05256
- Reference count: 13
- Primary result: Pre-training improves optimization for under-fitted models but not well-fitted ones

## Executive Summary
This paper examines unsupervised pre-training followed by fine-tuning for time series classification across 150 datasets. The authors systematically evaluate three model architectures (LSTM, Dilated CNN, Time-Series Transformer) with five pre-training tasks to understand when and why pre-training helps. Their analysis reveals that pre-training's benefits are limited to improving optimization for under-fitted models, does not provide regularization effects given sufficient training time, and can only speed up convergence for sufficiently capable models. The study also finds that while both model structure and pre-training task matter, model architecture plays a more significant role in determining effectiveness.

## Method Summary
The authors conduct experiments on 150 classification datasets from UTS and MTS benchmarks, using three model structures and five pre-training tasks. For each combination, they pre-train encoders on unlabeled data, then fine-tune on labeled data with a classification head. They evaluate test accuracy, training loss, and convergence speed, analyzing how pre-training affects under-fitted versus well-fitted models. The study also examines whether pre-training acts as regularization and how additional pre-training data affects generalization.

## Key Results
- Pre-training only improves optimization for under-fitted models, not well-fitted ones
- Pre-training does not act as regularization when given sufficient training time
- Model structure plays a more significant role than pre-training task in determining effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training can only improve optimization for under-fitted models, not for well-fitted ones.
- Mechanism: Pre-training provides a better initialization point in parameter space, reducing the distance the optimizer must travel to find a good local minimum. However, if the model already has sufficient capacity and is close to convergence, this initialization benefit is negligible.
- Core assumption: The optimization landscape is non-convex and sensitive to initialization; under-fitted models are farther from good solutions.
- Evidence anchors:
  - [abstract] "Pre-training can only help improve the optimization process for models that fit the data poorly, rather than those that fit the data well."
  - [section] "We find that when pre-training the models with simple structures, pre-training can, in some cases, improve the model's fitting ability of the data. In contrast, provided with a poorly fit model with a large number of parameters, pre-training does not bring any benefit to the final optimization result."
- Break condition: If the pre-training task is poorly aligned with the downstream task, even under-fitted models may not benefit due to initialization in a misleading region of parameter space.

### Mechanism 2
- Claim: Pre-training does not act as regularization given sufficient training time.
- Mechanism: Pre-training does not impose a penalty on model complexity or constrain the hypothesis space in a way that reduces overfitting; instead, it simply provides a starting point. With enough fine-tuning epochs, the model can overcome any implicit bias from pre-training initialization.
- Core assumption: The effect of pre-training is transient and does not structurally constrain the model's capacity to overfit.
- Evidence anchors:
  - [abstract] "Pre-training does not exhibit the effect of regularization when given sufficient training time."
  - [section] "no pre-training task significantly improves the generalization ability of the model on both the UTS and the MTS datasets."
- Break condition: If fine-tuning is stopped early or computational resources are limited, pre-training might appear to have a regularization effect simply because the model has not had time to overfit.

### Mechanism 3
- Claim: Pre-training can speed up convergence only if the model has sufficient ability to fit the data.
- Mechanism: A well-specified model (adequate capacity) can leverage the pre-trained parameters to quickly adapt to the downstream task. An under-specified model cannot, because the initialization is irrelevant if the model lacks the representational capacity to solve the problem.
- Core assumption: Model capacity and pre-training initialization interact multiplicatively; both must be sufficient for convergence speedup.
- Evidence anchors:
  - [abstract] "Pre-training can only speed up convergence if the model has sufficient ability to fit the data."
  - [section] "Since TsTransformer is under-fitted (high training loss) on most datasets, we conjecture that pre-training cannot speed up the convergence of the model when it is under-fitted."
- Break condition: If the pre-training task is too domain-specific or the model architecture is too rigid, even a well-specified model may not realize speedup.

## Foundational Learning

- Concept: Time series representation learning
  - Why needed here: Time series data have temporal dependencies and domain-specific characteristics that must be captured before effective classification.
  - Quick check question: What distinguishes univariate from multivariate time series in terms of feature dimensionality?

- Concept: Inductive bias in model architectures
  - Why needed here: Different architectures (LSTM, CNN, Transformer) encode different assumptions about temporal patterns; these biases determine how well pre-training transfers.
  - Quick check question: How does a dilated CNN's receptive field differ from an LSTM's in modeling long-range dependencies?

- Concept: Generalization measures (sharpness, path norm, distance traveled)
  - Why needed here: These metrics help diagnose whether pre-training is helping optimization vs. generalization, which is central to the paper's claims.
  - Quick check question: Why might a flatter loss landscape (lower sharpness) correlate with better generalization?

## Architecture Onboarding

- Component map:
  - Data pipeline: Load UTS/MTS datasets → pad sequences → split into Dpre/Dtrain/Dval
  - Encoder: LSTM / Dilated CNN / Time-Series Transformer
  - Pre-training head: Task-specific (e.g., masked reconstruction, contrastive loss)
  - Fine-tuning head: 1D conv + MLP for classification
  - Optimizer: Adam with learning rate schedules

- Critical path:
  1. Pre-train encoder on Dpre with chosen task
  2. Save best parameters by validation loss
  3. Initialize encoder with θpre, add classification head
  4. Fine-tune on Dtrain, track validation accuracy
  5. Evaluate on held-out test set

- Design tradeoffs:
  - Model complexity vs. training speed: Simpler models (LSTM-underfit) may benefit more from pre-training but have lower capacity.
  - Pre-training data volume vs. domain relevance: More data can reinforce existing advantages but does not guarantee better generalization.
  - Task alignment vs. generality: Tasks like Ts2Vec are more broadly useful; domain-specific tasks may overfit to pre-training.

- Failure signatures:
  - No improvement in training loss after pre-training → poor task alignment or insufficient model capacity
  - Worse test accuracy than random init → pre-training led to bad local minima or overfitting to pre-training data
  - No speedup in convergence → model under-specified or pre-training task too dissimilar

- First 3 experiments:
  1. Run LSTM with Random-Cls pre-training on a small UTS dataset; compare training loss curves to random init.
  2. Test Ts2Vec on D.Conv; measure if epoch-1 accuracy improves over baseline.
  3. Evaluate adding extra pre-training data (e.g., Food type) to LSTM; check if test accuracy improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does unsupervised pre-training significantly improve optimization for under-fitted models with simple structures?
- Basis in paper: [explicit] The paper finds that pre-training can improve optimization for under-fitted models with simple structures but not for complex models.
- Why unresolved: The paper identifies the effect but doesn't provide a comprehensive analysis of the boundary conditions that determine when pre-training helps optimization for simple models.
- What evidence would resolve it: Systematic experiments varying model complexity, dataset characteristics, and pre-training task types to identify precise conditions where pre-training improves optimization for under-fitted models.

### Open Question 2
- Question: How does the effectiveness of pre-training tasks compare when evaluated on larger models and datasets beyond the low-resource setting studied in this paper?
- Basis in paper: [inferred] The paper explicitly acknowledges its limitations to medium-sized models and limited data, suggesting potential differences in pre-training effectiveness with larger models.
- Why unresolved: The study focuses on constrained computational resources and doesn't explore scaling effects on pre-training effectiveness.
- What evidence would resolve it: Comparative experiments using larger models (e.g., transformer-based architectures with millions of parameters) and significantly expanded pre-training datasets across the same 150 classification tasks.

### Open Question 3
- Question: What are the fundamental characteristics of time series data that make certain pre-training tasks more effective than others for specific domains?
- Basis in paper: [explicit] The paper notes that different pre-training tasks have specific applicability scenarios and limitations based on implicit assumptions about time series properties.
- Why unresolved: While the paper identifies domain-specific effectiveness, it doesn't systematically analyze which data characteristics correlate with pre-training task success.
- What evidence would resolve it: Detailed analysis correlating time series properties (seasonality, stationarity, frequency content, inter-variable correlation) with the effectiveness of each pre-training task across domains.

## Limitations

- Limited scope of model architectures: The study examines only three model structures, which may not generalize to other architectures like GNNs or hybrid models.
- Fixed pre-training tasks: The five pre-training tasks are relatively standard and do not explore more domain-specific or novel tasks.
- Dataset bias and heterogeneity: The 150 datasets' characteristics (e.g., length, frequency, noise levels) are not reported, potentially limiting the conclusions about under-fitting vs. well-fitting models.

## Confidence

- High confidence: The claim that pre-training improves optimization for under-fitted models is well-supported by training loss curves and consistent across multiple datasets.
- Medium confidence: The claim that pre-training does not act as regularization is plausible but depends on the definition of "sufficient training time."
- Low confidence: The claim that model structure plays a more significant role than pre-training task is based on a limited set of architectures and tasks.

## Next Checks

1. Expand model architectures: Test the same pre-training tasks on a broader set of architectures (e.g., GNNs, hybrid CNN-LSTM) to verify if model structure consistently dominates task choice.

2. Quantify "sufficient training time": Run fine-tuning with varying epoch budgets to identify the point at which pre-training's regularization effect vanishes. Report the convergence curves for early vs. late stopping.

3. Analyze dataset heterogeneity: Stratify results by dataset characteristics (length, frequency, noise) to check if the under-fitting vs. well-fitting distinction holds across different data regimes.