---
ver: rpa2
title: 'Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through Dialect
  Identification using Transformer-based Approach'
arxiv_id: '2311.18739'
source_url: https://arxiv.org/abs/2311.18739
tags:
- arabic
- dialect
- language
- task
- identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a system for the Nuanced Arabic Dialect Identification
  (NADI) 2023 Shared Task. The goal is to classify Arabic tweets into one of 18 country-level
  dialects.
---

# Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through Dialect Identification using Transformer-based Approach

## Quick Facts
- arXiv ID: 2311.18739
- Source URL: https://arxiv.org/abs/2311.18739
- Reference count: 10
- This paper presents a system for the Nuanced Arabic Dialect Identification (NADI) 2023 Shared Task, classifying Arabic tweets into 18 country-level dialects using transformer-based models and ensemble methods, achieving an F1 score of 76.65 on the test set.

## Executive Summary
This paper addresses the Nuanced Arabic Dialect Identification (NADI) 2023 Shared Task by developing a system that classifies Arabic tweets into 18 country-level dialects. The authors employ several transformer-based models pre-trained on Arabic, including AraBERT and CAMeLBERT variants, and experiment with both individual model performance and ensembling approaches. The final system uses hard voting ensembling, which combines predictions from multiple models to yield more stable and improved results, ranking 11th on the leaderboard with an F1 score of 76.65 on the test set.

## Method Summary
The method involves fine-tuning several transformer-based models pre-trained on Arabic language for the dialect identification task. The models are trained on the NADI 2023 dataset, which contains 18,000 training samples, 1,800 development samples, and 3,600 test samples. The authors experiment with different pre-trained models, including AraBERTv02-Twitter-base, CAMeLBERT-DA, and AraBERTv02-base, and use hard voting ensembling to combine their predictions. The hyperparameters used include 10 epochs, a learning rate of 1e-5, a batch size of 32, and the AdamW optimizer. The text is preprocessed by removing USER, NUM, and URL tokens using regex.

## Key Results
- AraBERTv02-Twitter-base achieved the highest individual model performance with an F1 score of 77.03 on the development set.
- The hard voting ensemble of multiple models achieved an F1 score of 77.62 on the development set and 76.65 on the test set.
- The ensemble approach ranked 11th on the NADI 2023 Shared Task leaderboard.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training transformer models on Arabic-specific corpora improves dialect identification performance compared to generic multilingual models.
- Mechanism: Transformer models like AraBERT and CAMeLBERT are pre-trained on large-scale Arabic data, capturing linguistic patterns unique to Arabic dialects. Fine-tuning on task-specific data allows the model to adapt these patterns to distinguish between 18 country-level dialects.
- Core assumption: The pre-training data adequately represents the dialectal variation present in the NADI dataset.
- Evidence anchors:
  - [abstract] "Numerous transformer-based models, pre-trained on Arabic language, are employed for identifying country-level dialects."
  - [section 4.1] "The 70 million phrases that make up the pre-training dataset...are used to train the models. The news in the data covers a wide range of topics that is valuable for many downstream applications."
  - [corpus] Weak evidence: Only mentions related NADI papers, no direct evidence about pre-training corpora quality.
- Break condition: If the pre-training corpus does not cover certain dialects or if dialectal variation is too subtle for the model to capture.

### Mechanism 2
- Claim: Ensembling multiple transformer models using hard voting reduces prediction volatility and improves overall accuracy.
- Mechanism: Individual models may make different errors due to their training data or architecture. Hard voting aggregates predictions by majority vote, smoothing out these individual errors and producing more stable, robust predictions.
- Core assumption: Model errors are not perfectly correlated; different models make different mistakes.
- Evidence anchors:
  - [abstract] "The ensembling method is leveraged to yield improved performance of the system."
  - [section 5] "In hard voting, the final prediction is chosen based on the majority vote or the 'mode' of all the predictions. It reduces the volatility in the outcomes and aids in strengthening the system's robustness."
  - [corpus] No direct evidence found.
- Break condition: If all models consistently make the same errors on certain dialect classes, ensembling will not improve performance.

### Mechanism 3
- Claim: Pre-training on dialect-specific data (e.g., multi-dialect tweets) provides a performance advantage for dialect identification tasks.
- Mechanism: Models pre-trained on data similar to the target task learn representations that are more relevant to the task, requiring less adaptation during fine-tuning. AraBERTv02-Twitter-base, pre-trained on 60M multi-dialect tweets, outperforms other models.
- Core assumption: The distribution of dialects in the pre-training data matches the distribution in the NADI dataset.
- Evidence anchors:
  - [section 4.1] "AraBERTv02-Twitter-base is pre-trained on 60M multi-dialect tweets besides the usual datasets used for AraBERT models, giving it an edge over other models for this particular task."
  - [section 6] "AraBERTv02-Twitter-base outperforms the other models with an F1 score of 77.03 on the development dataset and 75.17 on the test dataset."
  - [corpus] No direct evidence found.
- Break condition: If the pre-training data contains dialects not present in NADI or if the tweet-style text differs significantly from NADI's tweet data.

## Foundational Learning

- Concept: Multi-class classification with transformer models
  - Why needed here: The task requires assigning each tweet to one of 18 dialect classes, a classic multi-class classification problem well-suited for transformer architectures.
  - Quick check question: What is the difference between multi-class and multi-label classification, and why is NADI a multi-class problem?

- Concept: Fine-tuning pre-trained language models
  - Why needed here: Instead of training from scratch, the authors leverage pre-trained models and adapt them to the dialect identification task using the provided dataset.
  - Quick check question: What are the key hyperparameters to consider when fine-tuning a transformer model, and how might they affect performance?

- Concept: Ensembling methods and voting strategies
  - Why needed here: Combining multiple model predictions can reduce variance and improve robustness, as demonstrated by the superior performance of the hard voting ensemble.
  - Quick check question: What is the difference between hard voting and soft voting in model ensembling, and when might one be preferred over the other?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Text cleaning using regex to remove "USER", "NUM", and "URL" tokens
  - Base models: AraBERTv02-Twitter-base, CAMeLBERT-DA, AraBERTv02-base
  - Ensembling layer: Hard voting mechanism that aggregates predictions
  - Evaluation: F1-score metric on development and test sets

- Critical path:
  1. Load and preprocess the NADI dataset
  2. Fine-tune each transformer model on the training data
  3. Generate predictions on the development set
  4. Implement hard voting ensemble on model predictions
  5. Evaluate ensemble performance on development set
  6. Generate final predictions on test set using ensemble

- Design tradeoffs:
  - Using pre-trained models vs. training from scratch: Pre-trained models offer better performance with less data and training time but may have biases from pre-training data.
  - Hard voting vs. soft voting: Hard voting is simpler and often effective, but soft voting (averaging probabilities) can capture more nuanced confidence information.
  - Model selection: Including more diverse models in the ensemble could improve robustness but increases computational cost.

- Failure signatures:
  - Poor performance on specific dialects: May indicate class imbalance or insufficient representation of those dialects in pre-training data
  - Large gap between development and test performance: Could suggest overfitting or domain shift between development and test data
  - Ensemble performs worse than individual models: Might indicate high correlation between model errors or inappropriate ensembling strategy

- First 3 experiments:
  1. Compare performance of individual models (AraBERTv02-Twitter-base, CAMeLBERT-DA, AraBERTv02-base) on the development set to identify the best base model
  2. Implement hard voting ensemble with the top 2-3 models and evaluate on the development set to assess ensembling benefits
  3. Experiment with different ensemble sizes (top 2, top 3, all models) to find the optimal balance between performance and complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the transformer-based models pre-trained on Arabic dialects perform on country-level dialect identification compared to models trained on Modern Standard Arabic (MSA) only?
- Basis in paper: [inferred] The paper mentions that various transformer-based models pre-trained on Arabic language are employed, including CAMeLBERT variants which are trained on variations of MADAR and NADI datasets.
- Why unresolved: The paper does not directly compare the performance of models pre-trained on Arabic dialects versus MSA only.
- What evidence would resolve it: Conduct experiments using models pre-trained on MSA only and compare their performance with models pre-trained on Arabic dialects on the country-level dialect identification task.

### Open Question 2
- Question: What is the impact of using different ensembling strategies, such as weighted voting or stacking, on the performance of the dialect identification system?
- Basis in paper: [explicit] The paper mentions that hard voting ensemble strategy is found to be the most effective and precise among the strategies used for ensembling.
- Why unresolved: The paper does not explore other ensembling strategies or compare their performance with hard voting.
- What evidence would resolve it: Implement and evaluate different ensembling strategies, such as weighted voting or stacking, on the dialect identification task and compare their performance with hard voting.

### Open Question 3
- Question: How does the performance of the dialect identification system vary with different sizes of training data?
- Basis in paper: [inferred] The paper mentions that the training data has 18000 tweet samples, but does not explore the impact of varying the size of training data on the system's performance.
- Why unresolved: The paper does not investigate the relationship between the size of training data and the performance of the dialect identification system.
- What evidence would resolve it: Conduct experiments with different sizes of training data and analyze the impact on the performance of the dialect identification system.

## Limitations
- The performance metrics are based on a single dataset with a specific distribution of dialects, which may not represent the full diversity of Arabic dialects across different contexts and time periods.
- The preprocessing steps are minimal, focusing only on removing USER, NUM, and URL tokens, which may not address other important aspects of tweet normalization such as spelling variations, code-switching, or dialect-specific morphological features.
- The ensemble approach uses hard voting without exploring alternative strategies like weighted voting or stacking, potentially leaving performance gains on the table.

## Confidence
- High Confidence: The claim that pre-trained transformer models outperform generic multilingual models on dialect identification tasks.
- Medium Confidence: The claim that pre-training on dialect-specific data (multi-dialect tweets) provides a performance advantage.
- Low Confidence: The claim that hard voting is the optimal ensembling strategy.

## Next Checks
1. **Dialect-specific performance analysis**: Break down model performance by individual dialect classes to identify systematic weaknesses and potential biases in the models, particularly for underrepresented dialects in the pre-training data.
2. **Alternative ensembling strategies**: Implement and compare soft voting, weighted voting, and stacking approaches to determine if the hard voting ensemble is truly optimal or if other strategies could yield better performance.
3. **Preprocessing robustness test**: Experiment with more comprehensive preprocessing pipelines including dialect-specific normalization, handling of code-switching, and spelling variation correction to assess the impact of preprocessing on model performance.