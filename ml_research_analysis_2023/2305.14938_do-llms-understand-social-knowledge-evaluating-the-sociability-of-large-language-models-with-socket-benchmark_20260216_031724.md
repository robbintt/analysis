---
ver: rpa2
title: Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language
  Models with SocKET Benchmark
arxiv_id: '2305.14938'
source_url: https://arxiv.org/abs/2305.14938
tags:
- tasks
- social
- language
- task
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SOCKET, a new benchmark for evaluating large
  language models'' understanding of social language. SOCKET contains 58 NLP tasks
  across five categories: humor & sarcasm, offensiveness, sentiment & emotion, trustworthiness,
  and social factors.'
---

# Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark

## Quick Facts
- arXiv ID: 2305.14938
- Source URL: https://arxiv.org/abs/2305.14938
- Authors: [Not specified in source]
- Reference count: 40
- Primary result: SOCKET benchmark reveals LLMs perform moderately (just above 0.7) on social language tasks with significant room for improvement

## Executive Summary
This paper introduces SOCKET, a comprehensive benchmark for evaluating large language models' understanding of social knowledge. SOCKET contains 58 NLP tasks across five categories: humor & sarcasm, offensiveness, sentiment & emotion, trustworthiness, and social factors. The authors demonstrate that current models achieve only moderate performance on social language tasks, with the best models scoring just above 0.7 overall. The benchmark reveals significant potential for task transfer among different types and categories of social language tasks. Through zero-shot evaluations, the paper shows that pretrained models possess some innate but limited capabilities for social language understanding, while training on one category of tasks can improve zero-shot testing on others. SOCKET provides a systematic way to analyze model performance on an important dimension of language and points to clear opportunities for building more socially-aware LLMs.

## Method Summary
The authors construct SOCKET by compiling 58 existing social language tasks across five categories. They evaluate model performance through fine-tuning BERT-based models (BERT, RoBERTa, DeBERTa-V3) on individual tasks and testing zero-shot performance on multiple LLMs (GPT, GPT-J, OPT, T5, LLaMA, BLOOM, FLAN-T5, Alpaca). The study quantifies task dependencies using prediction correlations and investigates multi-task training effectiveness through pre-finetuning on thematically grouped tasks. Models are evaluated using standard metrics (F1, accuracy, correlation) across classification, regression, pairwise comparison, and span identification tasks.

## Key Results
- Current LLMs achieve only moderate performance (just above 0.7 overall) on SOCKET social language tasks
- Zero-shot models perform close to baseline, indicating limited innate social knowledge capabilities
- Cross-task transfer potential is significant, with tasks within the same category showing strong dependencies
- Multi-task training on thematically grouped tasks improves performance on individual social language tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-task transfer among social language tasks is feasible due to shared underlying social constructs
- Mechanism: Tasks within the same category exhibit strong task dependencies, allowing models trained on one task to improve performance on related tasks
- Core assumption: Social language constructs are interrelated and models can learn generalizable representations of these constructs
- Evidence anchors: [abstract]: "We demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory."

### Mechanism 2
- Claim: Multi-task training on thematically grouped tasks can improve model performance on individual social language tasks
- Mechanism: Pre-finetuning on multiple tasks within the same category allows the model to learn a more robust representation of the category, which can then be fine-tuned for individual tasks
- Core assumption: Tasks within the same category share underlying social knowledge that can be jointly learned
- Evidence anchors: [abstract]: "Training on one category of tasks can improve zero-shot testing on others."

### Mechanism 3
- Claim: Zero-shot performance of LLMs on social language tasks is limited due to the lack of social knowledge in their parameters
- Mechanism: LLMs trained on general text data may not have learned the specific social constructs required for social language tasks, leading to poor zero-shot performance
- Core assumption: Social language understanding requires specific social knowledge that may not be present in general text data
- Evidence anchors: [section 4.2]: "Zeroshot models experience close-to-baseline performances, indicating that prompts alone cannot lead to correct predictions in identifying social knowledge without further finetuning."

## Foundational Learning

- Concept: Social language constructs (e.g., humor, sarcasm, offensiveness, sentiment, emotion, trustworthiness)
  - Why needed here: SOCKET benchmark is designed to evaluate LLMs' understanding of these constructs, which are crucial for interpersonal communication
  - Quick check question: Can you define each of the five social language categories in SOCKET and provide an example of a task for each?

- Concept: Task dependencies and cross-task transfer
  - Why needed here: Understanding task dependencies is crucial for designing effective multi-task training strategies and evaluating the potential for cross-task transfer
  - Quick check question: How would you quantify the dependency between two social language tasks, and what would a strong dependency indicate?

- Concept: Multi-task training and pre-finetuning
  - Why needed here: Multi-task training on thematically grouped tasks is a key strategy for improving model performance on individual social language tasks
  - Quick check question: What are the two stages of pre-finetuning, and how do they differ from standard fine-tuning?

## Architecture Onboarding

- Component map: SOCKET benchmark -> 58 tasks -> 5 categories -> BERT/RoBERTa/DeBERTa-V3/T5 models -> Fine-tuning -> Evaluation

- Critical path: 1. Preprocess datasets and split into train/test/dev sets 2. Fine-tune LLM models on individual tasks or multiple tasks (pre-finetuning) 3. Evaluate model performance on SOCKET benchmark 4. Analyze task dependencies and cross-task transfer potential 5. Investigate the effectiveness of multi-task training on social language tasks

- Design tradeoffs: Task diversity vs. task similarity: Balancing the number of tasks and their relatedness within each category; Model size vs. performance: Larger models may have more parameters but may not always guarantee better performance; Zero-shot vs. fine-tuned performance: Zero-shot models may have limited social knowledge, while fine-tuned models require additional training

- Failure signatures: Poor performance on span identification tasks: May indicate difficulty in open-ended tasks; Low cross-task transfer: May suggest lack of shared social constructs among tasks; Ineffective multi-task training: May indicate tasks are too diverse or lack shared knowledge

- First 3 experiments: 1. Fine-tune a BERT model on a single social language task (e.g., humor detection) and evaluate its performance on the SOCKET benchmark 2. Pre-finetune a model on all tasks within a single category (e.g., Offensiveness) and then fine-tune it on an individual task within that category. Compare its performance to the single-task fine-tuned model 3. Investigate the task dependencies among social language tasks by computing the correlation between predictions from models trained on different tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which LLMs can improve their social knowledge understanding beyond current training methods?
- Basis in paper: [explicit] The paper states that current LLMs perform moderately on social knowledge tasks and leave "clear room for improvement" to build more socially-aware models
- Why unresolved: The paper provides benchmark results showing current limitations but does not propose specific architectural or training innovations to address these gaps
- What evidence would resolve it: Empirical studies comparing novel model architectures, training paradigms, or evaluation frameworks specifically designed to enhance social knowledge understanding

### Open Question 2
- Question: How do cultural and demographic factors influence the performance of LLMs on social knowledge tasks across different populations?
- Basis in paper: [explicit] The paper acknowledges limitations regarding cross-cultural and multilingual expansions, noting that social knowledge interpretation varies by demographic and contextual factors
- Why unresolved: The current SOCKET benchmark is primarily English-based and doesn't account for cultural variations in social knowledge interpretation
- What evidence would resolve it: Cross-cultural validation studies with multilingual datasets and demographic-specific performance analyses of LLMs on social knowledge tasks

### Open Question 3
- Question: What is the relationship between model size and social knowledge understanding, and at what scale do LLMs show emergent social reasoning capabilities?
- Basis in paper: [explicit] The paper notes a weak correlation between parameter count and performance, with some larger models performing worse than smaller ones, and mentions that LLMs may show emergent abilities above certain thresholds
- Why unresolved: The study only tested models up to 13B parameters and couldn't explore potential emergent properties at larger scales due to computational constraints
- What evidence would resolve it: Systematic evaluation of social knowledge task performance across a broader range of model sizes, particularly focusing on very large models (50B+ parameters) to identify potential scaling laws or tipping points

## Limitations

- The SOCKET benchmark is limited to English-only datasets, constraining findings to Western-centric social constructs
- The selection criteria for the 58 specific tasks and their representativeness of broader social knowledge remains unclear
- Zero-shot evaluation results are limited by specific prompting strategies used, without exploring alternative approaches

## Confidence

- **High Confidence:** The benchmark construction methodology and evaluation framework are sound. The finding that LLMs show only moderate performance (scoring just above 0.7 overall) on social language tasks is well-supported by the experimental results
- **Medium Confidence:** The cross-task transfer findings show promise but require further validation across different model architectures and training regimes. The theoretical grounding for task dependencies, while referenced, could be more explicitly connected to the empirical results
- **Low Confidence:** The zero-shot evaluation results are limited by the specific prompting strategies used, and the paper does not explore alternative prompting approaches that might yield different outcomes

## Next Checks

1. Replicate the cross-task transfer experiments with additional model architectures beyond BERT-based models to verify the generalizability of observed task dependencies
2. Conduct ablation studies on the zero-shot prompts to determine how different prompt formulations affect social knowledge evaluation performance
3. Extend the benchmark to include non-English social language tasks to test whether the observed LLM limitations are universal or culturally specific