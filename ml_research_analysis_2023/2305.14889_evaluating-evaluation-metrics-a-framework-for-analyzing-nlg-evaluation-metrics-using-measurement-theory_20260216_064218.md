---
ver: rpa2
title: 'Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics
  using Measurement Theory'
arxiv_id: '2305.14889'
source_url: https://arxiv.org/abs/2305.14889
tags:
- evaluation
- metrics
- test
- validity
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MetricEval, a framework informed by measurement
  theory, to conceptualize and evaluate the reliability and validity of NLG evaluation
  metrics. The framework addresses the limitations of existing metrics and the subjective
  nature of human evaluation.
---

# Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory

## Quick Facts
- arXiv ID: 2305.14889
- Source URL: https://arxiv.org/abs/2305.14889
- Reference count: 10
- Primary result: Proposes MetricEval framework using measurement theory to quantify reliability and validity of NLG evaluation metrics

## Executive Summary
This paper introduces MetricEval, a framework that applies measurement theory to evaluate Natural Language Generation (NLG) evaluation metrics. The framework addresses fundamental limitations of existing metrics by systematically quantifying sources of measurement error and providing statistical tools for reliability and validity assessment. By decomposing observed metric scores into true scores and random error, researchers can better interpret results and identify issues related to conflated validity and reliability in both human and LLM-based metrics.

## Method Summary
The MetricEval framework applies classical test theory to NLG evaluation by treating observed metric scores as combinations of true scores and random error. It provides statistical tools including reliability coefficients (test-retest correlation, internal consistency), validity assessment across multiple dimensions (content, criterion-related, construct), and multivariate techniques like factor analysis and MTMM tables. The framework enables quantification of uncertainty in metric scores and offers guidance for metric design and interpretation based on empirical data from summarization tasks and human evaluations.

## Key Results
- Framework formalizes measurement error sources and offers statistical tools for evaluating NLG metrics
- Reliability coefficients can be estimated using test-retest or internal consistency methods
- Multi-dimensional validity framework distinguishes content, criterion-related, and construct validity
- Statistical tools like factor analysis can reveal latent structures in metric scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Measurement theory provides systematic framework to decompose NLG metric scores into true signal and random error
- Mechanism: Framework treats observed scores as sum of true score and random error following classical test theory, enabling reliability estimation
- Core assumption: Random error component is uncorrelated with true score and has zero mean across repeated measures
- Evidence anchors: Abstract mentions formalizing measurement error; section 4 defines random measurement errors as transient fluctuations that balance out to zero

### Mechanism 2
- Claim: Framework enables assessment of whether metrics measure intended constructs rather than just correlating with human judgment
- Mechanism: Distinguishes content validity, criterion-related validity, and construct validity to evaluate construct measurement
- Core assumption: Intended construct can be clearly defined and operationalized in measurable terms
- Evidence anchors: Abstract discusses conceptualizing validity and reliability; section 5 defines validity as reflecting intended measurement

### Mechanism 3
- Claim: Statistical tools reveal latent structures in metric scores, improving interpretability and guiding metric design
- Mechanism: Factor analysis identifies underlying factors explaining covariation among metrics; MTMM tables test discriminant and convergent validity
- Core assumption: Observed metric scores are linear combinations of latent factors plus unique variance
- Evidence anchors: Section 5.4.2 discusses factor analysis for identifying constructs explaining covariation; abstract mentions statistical tools

## Foundational Learning

- Concept: Classical Test Theory (CTT)
  - Why needed here: Provides mathematical foundation for decomposing observed scores into true scores and error
  - Quick check question: If an NLG metric has observed score variance of 0.25 and estimated error variance of 0.10, what is the reliability coefficient under CTT?

- Concept: Types of Validity
  - Why needed here: Ensures framework evaluates whether metrics measure intended constructs, not just correlate with human judgments
  - Quick check question: Which type of validity would you use to assess whether a summarization metric's scores predict real-world user satisfaction?

- Concept: Factor Analysis
  - Why needed here: Enables identification of latent constructs underlying multiple evaluation metrics
  - Quick check question: If two summarization metrics have high correlation, what might factor analysis reveal about their relationship?

## Architecture Onboarding

- Component map:
  Data Input Layer -> Metric Evaluation Engine -> Statistical Analysis Module -> Reporting Interface

- Critical path:
  1. Load benchmark and metric scores
  2. Compute reliability estimates (test-retest or internal consistency)
  3. Assess validity (content, criterion-related, construct)
  4. Apply factor analysis or MTMM if multiple metrics
  5. Generate uncertainty quantification and diagnostic reports

- Design tradeoffs:
  - Granularity vs. computational cost: More tasks improve reliability but increase runtime
  - Model-specific vs. general metrics: Task-specific metrics may have higher validity but lower generalizability
  - Simplicity vs. comprehensiveness: Single metrics are easier to interpret but may miss construct nuances

- Failure signatures:
  - Low reliability (<0.5): High random error, consider more tasks or robust metrics
  - Low validity despite high reliability: Metric may be precise but not measuring intended construct
  - Factor analysis yields many small factors: Possible overfactoring or noisy data

- First 3 experiments:
  1. Compute test-retest reliability on fixed summarization benchmark using two sets of outputs from same model
  2. Perform split-half reliability on summarization dataset with 100 tasks to estimate internal consistency
  3. Apply factor analysis to scores from ROUGE, BERTScore, and MoverScore on summarization benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between reliability of evaluation metrics and their predictive validity in real-world applications?
- Basis in paper: Explicit discussion of importance of reliability and its impact on predictive power
- Why unresolved: Paper does not provide empirical evidence or specific methodologies to quantify this relationship
- What evidence would resolve it: Studies comparing NLG model performance in real-world applications with their scores on evaluation metrics, considering reliability

### Open Question 2
- Question: How can measurement theory methodologies be adapted to handle scale and complexity of modern NLG models and datasets?
- Basis in paper: Inferred from mentions of challenges applying traditional methodologies to large-scale datasets and computationally intensive models
- Why unresolved: Paper does not provide specific solutions or examples of adapted methodologies
- What evidence would resolve it: Development and testing of scalable measurement theory-based evaluation methods for NLG models

### Open Question 3
- Question: What are most effective ways to combine multiple evaluation metrics to create composite score reflecting NLG model quality?
- Basis in paper: Explicit suggestion to use statistical indices from educational testing to curate subset of metrics
- Why unresolved: Paper does not provide specific guidance on determining weights or combining metrics effectively
- What evidence would resolve it: Empirical studies comparing performance of composite scores created using different combination methods

## Limitations
- Framework effectiveness depends heavily on availability of high-quality human judgments, which remain subjective and inconsistent
- Statistical tools assume linear relationships and may not capture complex interactions between metrics and constructs
- Framework's applicability to domains beyond summarization (dialogue, translation) remains untested

## Confidence
- High confidence: Decomposition of observed scores into true scores and random error follows well-established classical test theory principles
- Medium confidence: Multi-dimensional validity framework provides systematic approach but requires careful operationalization of constructs
- Low confidence: Effectiveness of statistical tools like factor analysis and MTMM tables for NLG metric evaluation needs empirical validation across diverse benchmarks

## Next Checks
1. Apply framework to new NLG task (dialogue generation) with multiple human evaluation methods to test generalizability
2. Conduct sensitivity analysis by introducing controlled systematic errors to metric scores and measuring impact on reliability estimates
3. Compare framework predictions with actual downstream model selection performance to validate criterion-related validity claims