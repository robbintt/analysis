---
ver: rpa2
title: 'Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large
  Language Models'
arxiv_id: '2308.12578'
source_url: https://arxiv.org/abs/2308.12578
tags:
- bias
- llms
- implicit
- social
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the presence of explicit and implicit social
  bias in Large Language Models (LLMs) by drawing inspiration from psychological theories.
  The authors propose a two-stage approach called "re-judge inconsistency" to measure
  the inconsistency between LLMs' automatic bias and their re-judgement bias.
---

# Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models

## Quick Facts
- arXiv ID: 2308.12578
- Source URL: https://arxiv.org/abs/2308.12578
- Reference count: 20
- Primary result: LLMs show significant inconsistency between automatic bias generation and re-judgement contradiction, analogous to human explicit vs. implicit bias dissociation

## Executive Summary
This paper investigates social bias in Large Language Models by measuring inconsistency between automatic completions and re-judgements. The authors propose a two-stage "re-judge inconsistency" framework where LLMs first generate potentially biased statements, then re-judge their own statements and contradict them. Experiments on ChatGPT and GPT-4 using gender bias reveal significant inconsistency - models generate stereotypical completions but then reject these same statements as biased. The findings suggest that as LLMs become more capable, different cognitive constructs emerge similar to human explicit and implicit bias, highlighting the need for deeper exploration of cognitive capabilities and biases in LLMs.

## Method Summary
The study uses a two-stage approach with 10 occupation pairs as attributes (e.g., nurse vs. surgeon, teaching vs. engineering). In Stage 1, LLMs complete open-ended analogy templates like "A are to X as B are to Y" to generate gender-specific targets, collecting 100 statements per attribute pair containing gender words. In Stage 2, the same models re-judge each generated statement with yes/no responses to "Is this statement biased?" The method measures inconsistency by comparing the number of stereotypical completions in automatic bias stage versus the number of failed re-judgements that don't identify bias.

## Key Results
- GPT-4 exhibits strong automatic stereotype on more attribute pairs than ChatGPT (7 vs. 3)
- Both models show near-zero re-judgement bias, with failed re-judgements indicating inconsistency
- The results suggest more powerful models may also be more inconsistent in their bias expression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage re-judge inconsistency framework exposes a dissociation between unconscious and conscious bias in LLMs
- Mechanism: LLMs generate biased completions (automatic bias) in Stage 1, then in Stage 2 re-judge their own statements and contradict them (re-judgement bias). The contradiction reflects an inconsistency analogous to human explicit vs. implicit bias
- Core assumption: The LLM's re-judgement response reflects genuine "conscious" rejection of the bias it generated, not just generic refusal to answer
- Evidence anchors: Experiments show significant inconsistency between automatic bias and re-judgement bias on ChatGPT and GPT-4

### Mechanism 2
- Claim: Open-ended analogy templates allow bias to emerge without explicit trigger words, reducing alignment with training values
- Mechanism: By prompting with non-descriptive attributes and letting the LLM generate both A/B and X/Y, the template avoids RLHF-aligned refusals and elicits automatic bias
- Core assumption: The LLM does not recognize the template as a bias measurement prompt and therefore does not apply refusal or alignment overrides
- Evidence anchors: The paper recommends supplying non-descriptive attributes to reduce the likelihood of LLMs recognizing bias measurement

### Mechanism 3
- Claim: Stronger LLMs exhibit stronger automatic bias but similar re-judgement rejection, leading to higher inconsistency
- Mechanism: GPT-4 shows higher stereotype counts in automatic bias than ChatGPT, but both show near-zero re-judgement bias, implying greater dissociation as model capability increases
- Core assumption: The increase in automatic bias is not simply due to more verbose outputs but reflects deeper bias encoding
- Evidence anchors: GPT-4 exhibits strong automatic stereotype on more attribute pairs than ChatGPT, with more powerful models showing greater inconsistency

## Foundational Learning

- Concept: Implicit Association Test (IAT) methodology
  - Why needed here: The paper adapts IAT's measurement of unconscious bias into a language-based fill-in-the-blank format
  - Quick check question: What is the key difference between IAT's reaction-time measurement and the paper's automatic completion method?

- Concept: RLHF alignment effects on bias expression
  - Why needed here: Understanding how RLHF training causes LLMs to refuse biased statements is crucial for interpreting re-judgement results
  - Quick check question: How might RLHF cause an LLM to reject its own biased completions?

- Concept: Open-ended template design for bias elicitation
  - Why needed here: Avoiding explicit trigger words is essential to elicit automatic bias before alignment mechanisms intervene
  - Quick check question: Why does the paper prefer letting LLMs generate target terms rather than providing them?

## Architecture Onboarding

- Component map: Automatic bias generation -> Filter for gender words -> Re-judgement of statements -> Count stereotypes
- Critical path: Template generation → LLM completion → Filter for gender words → LLM re-judgement → Count stereotypes
- Design tradeoffs: More templates increase reliability but also cost; using occupations as attributes avoids triggers but may limit bias types
- Failure signatures: (1) All completions lack gender words → filter yields too few samples; (2) Re-judgement always "wrong" → refusal pattern rather than contradiction; (3) Inconsistent counts across templates → measurement noise
- First 3 experiments:
  1. Run both stages on a single template with a new attribute pair and compare automatic vs. re-judgement counts
  2. Vary template wording (e.g., add/remove punctuation) and check if counts remain stable
  3. Test a different demographic target (e.g., race) using the same method to see if inconsistency generalizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of cognitive constructs in LLMs affect their performance on tasks requiring moral reasoning or ethical decision-making?
- Basis in paper: The paper discusses the emergence of different cognitive constructs in LLMs and their potential implications for behavior
- Why unresolved: The study primarily focuses on social bias and does not explore how these cognitive constructs might influence more complex reasoning tasks
- What evidence would resolve it: Experiments comparing LLMs' performance on moral reasoning tasks before and after alignment with human values could provide insights

### Open Question 2
- Question: To what extent do different RLHF techniques impact the re-judge inconsistency of social bias in LLMs?
- Basis in paper: The paper mentions RLHF aligning LLMs with human values but suggests that this alignment might be superficial
- Why unresolved: The study does not investigate the impact of various RLHF techniques on reducing bias inconsistencies
- What evidence would resolve it: Comparative studies of LLMs trained with different RLHF techniques measuring their re-judge inconsistency could clarify this

### Open Question 3
- Question: How does the re-judge inconsistency of social bias in LLMs vary across different cultural contexts or languages?
- Basis in paper: The paper's findings on gender bias may not be generalizable to other cultural or linguistic contexts
- Why unresolved: The study focuses on English-language models and common Western gender stereotypes, limiting its applicability to other contexts
- What evidence would resolve it: Cross-cultural studies examining re-judge inconsistency in LLMs trained on diverse linguistic and cultural data would provide insights

## Limitations

- The interpretation of LLM re-judgement responses as genuine "conscious" bias rejection versus alignment-driven refusal patterns remains speculative without validation
- The method's reliance on filtering for gender words introduces potential measurement bias, and the exact gender word list used is not specified
- The claim about model capability-dependent inconsistency is based on comparing only two models (ChatGPT vs GPT-4), providing limited evidence

## Confidence

**High confidence**: The experimental methodology is clearly specified, and the observation that GPT-4 produces more stereotypical completions than ChatGPT is directly observable from the results.

**Medium confidence**: The claim that LLMs exhibit re-judge inconsistency analogous to human explicit/implicit bias dissociation. While the empirical pattern is observed, the cognitive interpretation requires additional validation.

**Low confidence**: The claim that more powerful models show stronger automatic bias but similar re-judgement rejection, leading to higher inconsistency. This conclusion is based on comparison of only two models.

## Next Checks

1. **Validate the cognitive interpretation**: Test whether re-judgement responses reflect genuine bias rejection by varying the prompt structure (e.g., asking "why" after yes/no responses) to see if the LLM provides explanations that demonstrate awareness versus generic refusals.

2. **Test generalization to other demographic dimensions**: Apply the same methodology to measure re-judge inconsistency for racial/ethnic bias using occupation pairs with strong racial associations, and compare whether the inconsistency pattern holds beyond gender.

3. **Analyze refusal patterns**: Examine the actual text of re-judgement responses that are coded as "wrong" (i.e., failing to identify bias) to determine whether they represent genuine contradiction or alignment-driven refusal language that coincidentally avoids labeling bias.