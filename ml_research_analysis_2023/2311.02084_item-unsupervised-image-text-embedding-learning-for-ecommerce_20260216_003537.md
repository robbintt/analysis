---
ver: rpa2
title: 'ITEm: Unsupervised Image-Text Embedding Learning for eCommerce'
arxiv_id: '2311.02084'
source_url: https://arxiv.org/abs/2311.02084
tags:
- item
- product
- embedding
- image
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ITEm, an unsupervised image-text embedding
  model designed for eCommerce applications. The model addresses the challenge of
  learning fine-grained embeddings from both product images and titles, especially
  when one modality (often titles) dominates the other.
---

# ITEm: Unsupervised Image-Text Embedding Learning for eCommerce

## Quick Facts
- **arXiv ID:** 2311.02084
- **Source URL:** https://arxiv.org/abs/2311.02084
- **Reference count:** 23
- **Key outcome:** ITEm achieves 67.53% Macro-Average Recall@10 and 60.56% Mean Average Precision@10 on same product recommendation in eCommerce.

## Executive Summary
This paper introduces ITEm, an unsupervised image-text embedding model designed for eCommerce applications. The model addresses the challenge of learning fine-grained embeddings from both product images and titles, especially when one modality dominates the other. ITEm extends BERT by incorporating both image and text modalities and introduces five pre-training tasks: image-text matching, masked language modeling, masked image modeling, and their global variants. The global variants force the model to summarize information in a global representation, promoting balanced attention to both modalities. The model is evaluated on two tasks: same product recommendation and leaf category prediction. Results show that ITEm significantly outperforms strong baselines, including ResNet50, RoBERTa, CLIP, and supervised ITEm, demonstrating the effectiveness of ITEm in learning fine-grained cross-modal embeddings for eCommerce applications.

## Method Summary
ITEm extends BERT with five pre-training tasks (ITM, MLM, MIM, GMLM, GMIM) using a single transformer encoder for both image and text. The model uses the whole image as input without region detection, processing 224x224 images as 16x16 patches. Training uses AdamW optimizer with specific learning rate schedule and batch size of 512 on 8 V100 GPUs for 100k steps. The model is evaluated on same product recommendation and leaf category prediction tasks using cosine similarity scoring.

## Key Results
- ITEm achieves 67.53% Macro-Average Recall@10 and 60.56% Mean Average Precision@10 on same product recommendation
- Outperforms strong baselines including ResNet50, RoBERTa, CLIP, and supervised ITEm
- Shows substantial gains in both same product recommendation and leaf category prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global variants of MLM and MIM force the model to summarize information across modalities, balancing attention between image and text.
- Mechanism: By reconstructing masked tokens/patches using only the global [CLS] representation and position embeddings (instead of individual token/patch representations), the model must integrate and compress cross-modal information into a shared representation.
- Core assumption: The global representation can effectively capture both image and text features when forced to predict masked elements.
- Evidence anchors:
  - [abstract]: "We extend BERT by (1) learning an embedding from text and image without knowing the regions of interest; (2) training a global representation to predict masked words and to construct masked image patches without their individual representations."
  - [section 4]: "To summarize both image and title information in the global output representation, we design a new objective, predicting the missing tokens with the global output representation and their corresponding position embeddings rather than their individual output representations."
  - [corpus]: Weak evidence; related works focus on modality alignment but not on global summarization as a balancing mechanism.
- Break condition: If the global representation cannot effectively encode both modalities, the reconstruction task will fail, leading to degraded performance.

### Mechanism 2
- Claim: Using the whole image as input without region detection broadens model applicability to out-of-domain datasets.
- Mechanism: By avoiding reliance on pre-trained object detectors, the model can process raw images directly, making it robust to datasets where object detection may perform poorly.
- Core assumption: The transformer encoder can learn effective image representations without explicit region proposals.
- Evidence anchors:
  - [abstract]: "We propose a method to learn fine-grained representation from product images and titles for eCommerce, trying to ease the embedding over-dominance problem caused by a single modality that contains dominant information."
  - [section 2]: "Inspired by Dosovitskiy et al. (2021), ITEm uses the whole image as input without the detection of the regions of interest, while the other methods employ a pre-trained model to detect the regions of interest and apply these regions as input."
  - [corpus]: Weak evidence; most related works assume region proposals are available and don't address out-of-domain robustness.
- Break condition: If the dataset contains highly complex images where local features are critical, the lack of region attention may limit performance.

### Mechanism 3
- Claim: Pre-training with unsupervised tasks improves fine-grained embedding for same product retrieval compared to supervised category prediction.
- Mechanism: Unsupervised pre-training focuses on learning detailed cross-modal correspondences without being biased toward category-level patterns, enabling better distinction between similar products.
- Core assumption: Fine-grained product differences are better captured by unsupervised cross-modal learning than by supervised category classification.
- Evidence anchors:
  - [abstract]: "We evaluate the pre-trained ITEm on two tasks: the search for extremely similar products and the prediction of product categories, showing substantial gains compared to strong baseline models."
  - [section 6.2]: "the models trained with unsupervised learning achieve better results than the ones with supervised learning. When training to predict the leaf category, in a supervised way, the model tends to learn the common pattern within each category."
  - [corpus]: Limited evidence; related works focus on retrieval but not specifically on same product vs. category-level distinctions.
- Break condition: If the dataset lacks fine-grained distinctions or if category labels are highly informative, supervised learning might outperform unsupervised pre-training.

## Foundational Learning

- Concept: Cross-modal representation learning
  - Why needed here: The model must align image and text features into a shared embedding space for retrieval and classification tasks.
  - Quick check question: Can the model effectively retrieve the same product using only image or only text embeddings, or is the combination necessary?

- Concept: Masked language modeling (MLM) and masked image modeling (MIM)
  - Why needed here: These tasks encourage the model to learn contextual representations by predicting missing information, which is crucial for understanding fine-grained details.
  - Quick check question: Does the model maintain performance when a large portion of the input is masked during inference?

- Concept: Global representation learning
  - Why needed here: The global [CLS] token must summarize cross-modal information to balance attention between image and text, preventing modality dominance.
  - Quick check question: Can the global representation effectively reconstruct masked tokens/patches when individual representations are not used?

## Architecture Onboarding

- Component map: Image/text → embeddings → transformer encoder → [CLS] → downstream task head
- Critical path: Image/text → embeddings → transformer encoder → [CLS] → downstream task head
- Design tradeoffs:
  - Using whole image vs. region proposals: Simpler, more robust to out-of-domain data but may miss local details
  - Global vs. individual reconstruction: Forces cross-modal summarization but may be harder to optimize
  - Unsupervised vs. supervised pre-training: Better for fine-grained retrieval but may underperform if category labels are highly informative
- Failure signatures:
  - Poor same product retrieval: Model may be dominated by the more informative modality (usually text)
  - Low category prediction accuracy: Pre-training tasks may not align well with category-level patterns
  - Unstable training: Imbalanced loss terms or learning rate issues
- First 3 experiments:
  1. Ablation study: Remove global variants (GMLM, GMIM) to test if modality balancing improves performance
  2. Modality dominance test: Compare retrieval performance using only image vs. only text embeddings
  3. Out-of-domain robustness: Evaluate on a dataset where object detection is known to perform poorly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ITEm's performance change when evaluated on datasets with different modality dominance patterns (e.g., image-dominant vs. title-dominant)?
- Basis in paper: [explicit] The paper discusses challenges in learning embeddings when one modality is more dominant, but evaluates primarily on ITOP where titles are generally more informative.
- Why unresolved: The paper focuses on a specific dataset where titles are more dominant, limiting generalizability to other modality dominance scenarios.
- What evidence would resolve it: Evaluating ITEm on diverse datasets with varying modality dominance patterns would reveal its robustness and adaptability.

### Open Question 2
- Question: What is the impact of using region-of-interest detection in ITEm, and how does it compare to the current approach of using the whole image?
- Basis in paper: [explicit] The paper contrasts ITEm's use of whole images with other methods that use region-of-interest detection, suggesting potential benefits for specific tasks.
- Why unresolved: The paper does not empirically test the impact of region-of-interest detection on ITEm's performance.
- What evidence would resolve it: Conducting experiments comparing ITEm with and without region-of-interest detection on relevant tasks would clarify the trade-offs.

### Open Question 3
- Question: How does ITEm's pre-training scale with larger datasets, and what are the computational requirements for such scaling?
- Basis in paper: [inferred] The paper mentions pre-training on a large dataset but does not explore scalability or computational implications.
- Why unresolved: The paper does not provide insights into the scalability of ITEm's pre-training process or its computational demands.
- What evidence would resolve it: Experiments with larger datasets and detailed analysis of computational resources would provide clarity on scalability.

### Open Question 4
- Question: How does ITEm handle products with minimal or no textual information, relying solely on visual data?
- Basis in paper: [explicit] The paper discusses learning from both modalities but does not address scenarios where textual information is absent.
- Why unresolved: The paper does not explore ITEm's performance in edge cases where titles are minimal or missing.
- What evidence would resolve it: Testing ITEm on products with varying levels of textual information would demonstrate its effectiveness in such scenarios.

## Limitations

- Limited ablation studies to isolate the contribution of each pre-training task and architectural component
- Evaluation conducted exclusively on the ITOP dataset, raising questions about generalizability to other domains
- Same product recommendation task relies on exact matches, which may not reflect real-world scenarios with product variations

## Confidence

**High confidence** (supported by direct evidence):
- ITEm architecture and training procedure are clearly specified
- Quantitative results on ITOP dataset show consistent improvements over baselines
- The five pre-training tasks are implemented as described

**Medium confidence** (supported by indirect evidence):
- The mechanism of global variants balancing modality attention is plausible but not definitively proven
- Claims about out-of-domain robustness are theoretically sound but lack empirical validation
- The superiority of unsupervised pre-training for fine-grained retrieval is demonstrated but could be dataset-specific

**Low confidence** (limited or no direct evidence):
- Generalizability to other eCommerce domains or product categories
- Performance on datasets with different image characteristics (complex backgrounds, multiple products)
- Long-term stability and effectiveness after extended deployment

## Next Checks

1. **Ablation study on pre-training tasks**: Remove each of the five pre-training tasks individually (especially GMLM and GMIM) to quantify their contribution to overall performance. This will help determine whether the global variants are essential for modality balancing or if simpler approaches could suffice.

2. **Cross-domain evaluation**: Test ITEm on product datasets from different eCommerce platforms, cultural contexts, or even non-eCommerce domains (fashion vs. electronics vs. furniture) to assess generalizability and identify potential domain-specific limitations.

3. **Robustness to product variations**: Evaluate retrieval performance on datasets where query products have minor variations from index products (different colors, sizes, or seasonal variants). This would test whether ITEm truly learns fine-grained product representations or simply memorizes exact matches.