---
ver: rpa2
title: 'Total Selfie: Generating Full-Body Selfies'
arxiv_id: '2308.14740'
source_url: https://arxiv.org/abs/2308.14740
tags:
- selfie
- image
- images
- full-body
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Total Selfie generates full-body images from arms-length selfies
  by combining pre-captured video of the user in an outfit, a background image, and
  a target pose image. The method uses a multi-concept DreamBooth fine-tuning approach
  with ControlNet to synthesize an initial full-body image, then refines appearance
  using local refinement and a selfie undistortion module to correct perspective distortion.
---

# Total Selfie: Generating Full-Body Selfies

## Quick Facts
- arXiv ID: 2308.14740
- Source URL: https://arxiv.org/abs/2308.14740
- Reference count: 25
- Primary result: LPIPS 0.205, SSIM 0.652, PSNR 19.28, FID 112.2 on 30 examples across 5 individuals

## Executive Summary
Total Selfie generates full-body images from arms-length selfies by combining pre-captured video of the user in an outfit, a background image, and a target pose image. The method uses a multi-concept DreamBooth fine-tuning approach with ControlNet to synthesize an initial full-body image, then refines appearance using local refinement and a selfie undistortion module to correct perspective distortion. An image harmonization stage improves shading and removes artifacts. On a dataset of 30 examples across 5 individuals and 2 scenes, Total Selfie achieves LPIPS 0.205, SSIM 0.652, PSNR 19.28, and FID 112.2, outperforming baselines including PIDM, Paint-By-Example, and DreamBooth+ControlNet.

## Method Summary
Total Selfie works by first collecting a pre-captured selfie video containing overhead, cloth, pants, and shoes views (15 images each), an on-site selfie, a background image, and a target pose image. The method trains a multi-concept DreamBooth model with ControlNet to generate an initial full-body image using region-aware generation that separately processes foreground and background. It then refines the appearance through local refinement and a selfie undistortion module to correct perspective distortion. Finally, an image harmonization stage improves shading and removes artifacts by projecting the refined image back onto the data manifold learned by Stable Diffusion.

## Key Results
- LPIPS 0.205, SSIM 0.652, PSNR 19.28, FID 112.2 on 30 examples
- Outperforms PIDM, Paint-By-Example, and DreamBooth+ControlNet baselines
- Achieves natural camera viewpoints through region-aware generation
- Corrects perspective distortion in on-site selfies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-concept DreamBooth learns correspondence among selfie images of different body parts to build complete outfit understanding without explicit geometric information
- Mechanism: Training separate concepts for overhead view, cloth, pants, and shoes allows the model to learn implicit relationships through shared identity embeddings
- Core assumption: Stable Diffusion's implicit prior contains sufficient body structure and clothing relationship information
- Evidence anchors: Abstract mentions implicit prior knowledge linking selfie images of different body parts; section explains advantages of using DreamBooth for implicit learning
- Break condition: If pre-captured video doesn't capture all necessary body parts or clothing styles vary too much across images

### Mechanism 2
- Claim: Region-aware inference generates full-body images with natural camera viewpoints by separately processing foreground and background
- Mechanism: ControlNet's Canny edge model guides person generation while background generation remains natural, blending at each denoising step
- Core assumption: ControlNet Canny edge model can effectively guide person generation in desired pose
- Evidence anchors: Abstract mentions multi-concept DreamBooth with ControlNet; section describes region-aware inference idea
- Break condition: If Canny edge detection is inaccurate or blending weights don't balance foreground/background properly

### Mechanism 3
- Claim: Image harmonization projects refined images back onto Stable Diffusion's data manifold to remove artifacts and improve shading
- Mechanism: Fine-tuning Stable Diffusion decoder on refined image with null-text inversion and classifier guidance
- Core assumption: Stable Diffusion has learned distribution including natural full-body shots with proper shading
- Evidence anchors: Abstract mentions image harmonization step; section explains key idea of reprojecting to data manifold
- Break condition: If initial refined image is too far from data manifold or guidance losses don't balance content preservation with style matching

## Foundational Learning

- Concept: Stable Diffusion denoising process
  - Why needed here: Understanding latent diffusion models' progressive denoising is crucial for region-aware generation and image harmonization
  - Quick check question: What is the role of classifier-free guidance scale in the denoising process?

- Concept: DreamBooth fine-tuning methodology
  - Why needed here: Multi-concept approach requires understanding personalized model training with multiple concepts and ControlNet usage
  - Quick check question: How does multi-concept DreamBooth differ from standard DreamBooth training?

- Concept: ControlNet integration
  - Why needed here: Method relies on ControlNet for pose conditioning in both region-aware generation and appearance refinement
  - Quick check question: What types of control signals can be used with ControlNet, and how are they integrated into denoising process?

## Architecture Onboarding

- Component map: Pre-capture video → Region-aware generation → Appearance refinement → Image harmonization → Output
- Critical path: Pre-capture video → Region-aware generation → Appearance refinement → Image harmonization → Output
- Design tradeoffs:
  - Multi-concept DreamBooth vs. single-concept with all body parts
  - Region-aware generation vs. direct generation with all conditions
  - Local refinement vs. global refinement
  - Image harmonization strength vs. content preservation
- Failure signatures:
  - Incorrect identity or expression: Issues in appearance refinement stage
  - Unnatural transitions between foreground/background: Issues in region-aware generation
  - Incorrect shading or lighting: Issues in image harmonization
  - Artifacts in hands or complex poses: Known Stable Diffusion limitations
- First 3 experiments:
  1. Test region-aware generation with synthetic data where ground truth full-body images are available
  2. Validate face undistortion module on dataset of distorted and undistorted selfies
  3. Evaluate image harmonization on images with known artifacts to verify artifact removal

## Open Questions the Paper Calls Out

- Question: How does performance vary with quality and resolution of input selfie video and on-site selfies?
- Basis in paper: Paper doesn't provide detailed analysis on how input quality affects output quality
- Why unresolved: Paper doesn't include experiments testing performance across different input qualities or resolutions
- What evidence would resolve it: Comparative experiments showing performance metrics across various input video qualities and selfie resolutions

- Question: What is minimum training data required for effective personalization, and how does this scale with diversity of clothing and poses?
- Basis in paper: Paper uses 15 images per body part but doesn't explore minimum required data or scaling with diversity
- Why unresolved: Paper doesn't investigate how performance changes with fewer training images or increased diversity
- What evidence would resolve it: Experiments showing performance degradation curves as training data decreases, analysis across different clothing and pose variations

- Question: How does method perform on individuals with significantly different body types or proportions compared to training data?
- Basis in paper: Paper tests on 5 individuals but doesn't discuss performance across different body types
- Why unresolved: No analysis of cross-body-type generalization or performance across different body proportions
- What evidence would resolve it: Testing on individuals with various body types and comparing performance metrics to baseline results

## Limitations

- Limited dataset size (30 examples across 5 individuals and 2 scenes) raises questions about generalizability to diverse body types and clothing styles
- Relatively high FID score (112.2) suggests generated images may contain noticeable artifacts or distribution mismatches
- Multi-concept training approach requires significant computational resources and careful hyperparameter tuning

## Confidence

- High confidence: Core methodology of combining pre-captured video, pose conditioning, and refinement stages is technically sound and well-explained
- Medium confidence: Quantitative results appear reasonable for this image generation task, though limited dataset makes metrics less reliable
- Low confidence: Claims about implicit learning of body part correspondences through multi-concept DreamBooth are not directly validated

## Next Checks

1. **Dataset expansion validation**: Test method on larger and more diverse dataset (100+ examples across 20+ individuals and 5+ scenes) to verify generalizability and identify failure cases

2. **Ablation study on multi-concept approach**: Conduct controlled experiments comparing single-concept vs. multi-concept DreamBooth training with varying numbers of concepts to quantify impact of concept granularity

3. **Long-range consistency testing**: Generate full-body selfies from on-site selfies taken at different times/locations to evaluate method's ability to maintain consistent appearance across varied conditions including different lighting, poses, and camera angles