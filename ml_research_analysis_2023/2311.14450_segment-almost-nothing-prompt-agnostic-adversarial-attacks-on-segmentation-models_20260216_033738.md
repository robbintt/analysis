---
ver: rpa2
title: 'Segment (Almost) Nothing: Prompt-Agnostic Adversarial Attacks on Segmentation
  Models'
arxiv_id: '2311.14450'
source_url: https://arxiv.org/abs/2311.14450
tags:
- mask
- score
- attacks
- segmentation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of adversarial attacks on general-purpose\
  \ segmentation models like SAM and SEEM, which can generate segmentation masks from\
  \ various prompts (visual, textual, etc.). The core method idea is to create prompt-agnostic\
  \ adversarial attacks by maximizing the \u21132-distance in the latent space between\
  \ the embeddings of the original and perturbed images, thus distorting the image\
  \ representation used by the segmentation model."
---

# Segment (Almost) Nothing: Prompt-Agnostic Adversarial Attacks on Segmentation Models

## Quick Facts
- arXiv ID: 2311.14450
- Source URL: https://arxiv.org/abs/2311.14450
- Authors: 
- Reference count: 36
- Key outcome: Even small ℓ∞-bounded perturbations of radius ε=1/255 are often sufficient to drastically modify the masks predicted with point, box, and text prompts, with mIoU dropping from ~60% to ~17% for image-specific attacks and from ~59% to ~45% for universal attacks.

## Executive Summary
This paper addresses the problem of adversarial attacks on general-purpose segmentation models like SAM and SEEM, which can generate segmentation masks from various prompts (visual, textual, etc.). The core method idea is to create prompt-agnostic adversarial attacks by maximizing the ℓ2-distance in the latent space between the embeddings of the original and perturbed images, thus distorting the image representation used by the segmentation model. The primary results show that even small ℓ∞-bounded perturbations of radius ε=1/255 are often sufficient to drastically modify the masks predicted with point, box, and text prompts, with mIoU dropping from ~60% to ~17% for image-specific attacks and from ~59% to ~45% for universal attacks. The paper also explores creating universal, non image-specific attacks that can be applied to any input without additional computational cost.

## Method Summary
The paper proposes a novel approach to generate adversarial attacks on segmentation models by maximizing the ℓ2-distance between the embeddings of clean and perturbed images in the latent space of the image encoder. For image-specific attacks, Projected Gradient Descent (PGD) with 100 steps is used to optimize the perturbation within an ℓ∞ bound. For universal attacks, a single perturbation is optimized over a batch of training images using PGD with 500 steps, then resized and applied to new images. The attack is designed to be prompt-agnostic, meaning it affects the segmentation quality regardless of the prompt type (point, box, text, etc.). The method is evaluated on SAM and SEEM models using ADE20K and SA-1B datasets, with mIoU as the primary metric for point prompts.

## Key Results
- Small ℓ∞-bounded perturbations (ε=1/255) can drastically reduce mIoU from ~60% to ~17% for image-specific attacks.
- Universal perturbations (ε=8/255) reduce mIoU from ~59% to ~45% without per-image optimization.
- The attack generalizes across prompt types (point, box, text) and tasks (instance, semantic segmentation).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Maximizing ℓ2-distance between embeddings of original and perturbed images distorts the segmentation model's internal representation enough to degrade mask quality across prompts.
- **Mechanism:** The image encoder ϕ processes the image independently of prompts; by optimizing adversarial perturbations to maximize ∥ϕ(x+δ) - ϕ(x)∥²₂, the attack corrupts the latent representation used downstream for mask generation, regardless of prompt type.
- **Core assumption:** The downstream segmentation model's performance depends heavily on the fidelity of the encoder's embedding; small changes in latent space propagate to large changes in mask predictions.
- **Evidence anchors:**
  - [abstract] "distort the embedding produced by the image encoder, i.e. maximize the ℓ2-distance between the representations, in latent space, of clean and adversarial image"
  - [section] "we propose to use instead the following attack objective: max δ∈R^{w×h×c} ∥ϕ(x + δ) − ϕ(x)∥²₂"
  - [corpus] Weak: No direct corroboration in corpus neighbors; only general segmentation attack papers, not prompt-agnostic ones.
- **Break condition:** If the mask generator uses local context or prompt-specific conditioning that is less sensitive to latent embedding drift, the attack's effect may diminish.

### Mechanism 2
- **Claim:** Universal perturbations can be found by optimizing a single perturbation over a batch of training images and applying it to new images after interpolation to match resolution.
- **Mechanism:** The attack optimizes δ to maximize the sum of ℓ2-distances across n training images; the same δ is resized to the target image via interpolation and added to perturb the encoder output universally.
- **Core assumption:** The image encoder's embedding is smooth enough that a single perturbation, even when resized, generalizes to other images.
- **Evidence anchors:**
  - [section] "we fix the shape of the perturbation to 1024x1024 pixels... and interpolate it to match the size of the target image"
  - [section] "When optimizing the attack, we compute the gradient of the target loss wrt each input image, normalize it wrt its ℓ2-norm... and finally sum them."
  - [corpus] Weak: No direct corroboration in corpus neighbors; only general universal attacks, not foundation model encoders.
- **Break condition:** If the encoder's embedding is highly input-specific or uses normalization that nullifies the effect of a fixed perturbation, the universal attack will fail.

### Mechanism 3
- **Claim:** Multi-crop PGD attacks can degrade mask quality even when the mask generator uses multiple crops and combines them, by attacking crops of the perturbed image.
- **Mechanism:** At each PGD step, with probability pcrop=0.8 a random crop of the current perturbed image is selected and used to compute the loss, ensuring that all spatial regions are attacked over iterations.
- **Core assumption:** Attacking multiple random crops over iterations ensures that the perturbation degrades the encoder's output for all regions, even if the mask generator only sees crops.
- **Evidence anchors:**
  - [section] "at each iteration, with probability pcrop = 0.8 we use a randomly cropped version of the image to compute the objective loss"
  - [section] "we first select a random rectangular subset of the current iterate... whose width and height are uniformly (and independently one from another) sampled between 30% and 90% of the original width and height respectively"
  - [corpus] Weak: No direct corroboration in corpus neighbors; only general adversarial segmentation attacks, not multi-crop generator specific ones.
- **Break condition:** If the mask generator's combination step is robust to crop-level noise or uses global features that are unaffected by local perturbations, the attack will be less effective.

## Foundational Learning

- **Concept:** Adversarial examples and Projected Gradient Descent (PGD) optimization
  - **Why needed here:** The core attack method relies on solving an ℓ∞-bounded optimization problem via PGD/APGD to maximize feature distortion.
  - **Quick check question:** What is the difference between FGSM, PGD, and APGD in terms of step size adaptation and convergence?

- **Concept:** Image encoder embeddings and latent space manipulation
  - **Why needed here:** Understanding how the encoder's output is used downstream and how perturbing it affects mask predictions is critical.
  - **Quick check question:** Why does maximizing ℓ2-distance in the latent space lead to degraded segmentation masks, and not just small perceptual changes?

- **Concept:** Universal adversarial perturbations and interpolation across resolutions
  - **Why needed here:** The universal attack relies on a fixed perturbation that must generalize across images of different resolutions via interpolation.
  - **Quick check question:** How does interpolation of a fixed perturbation affect its effectiveness when applied to images of varying aspect ratios?

## Architecture Onboarding

- **Component map:** Input image → Image encoder (ϕ) → Latent embedding → Mask generator (with optional multi-crop mode) → Segmentation masks
- **Critical path:** Image encoder → latent embedding → mask generator; perturbations target the encoder output to degrade all downstream predictions.
- **Design tradeoffs:**
  - Attack strength vs. imperceptibility: Larger ℓ∞ bounds yield stronger degradation but risk visibility.
  - Universal vs. image-specific: Universal perturbations generalize but are weaker; image-specific are stronger but require per-image computation.
  - Multi-crop PGD vs. APGD: Multi-crop ensures spatial coverage but increases variance in updates.
- **Failure signatures:**
  - Perturbations are too small → no noticeable degradation in masks.
  - Mask generator uses strong global features → perturbations ineffective.
  - Transfer to SEEM fails → encoder architectures too different.
- **First 3 experiments:**
  1. Generate ℓ∞-bounded perturbations (ε=1/255) for a single SAM image and evaluate mIoU drop across point, box, and text prompts.
  2. Create a universal perturbation (ε=8/255) using 100 ADE20K images and test on held-out images with Segment Everything mode.
  3. Apply multi-crop PGD (pcrop=0.8) to a SAM image and compare mask degradation with standard APGD under the multi-crop mask generator.

## Open Questions the Paper Calls Out
- **Open Question 1:** How do adversarial perturbations designed for SAM transfer to other promptable segmentation models beyond SEEM?
- **Open Question 2:** Can prompt-agnostic attacks be optimized for targeted attacks rather than untargeted degradation?
- **Open Question 3:** What is the minimal computational budget needed to generate effective universal adversarial perturbations?

## Limitations
- The paper's core claim of prompt-agnostic attacks rests on the assumption that the image encoder's embedding is the primary determinant of mask quality across all prompt types, but ablation studies isolating the encoder's contribution are missing.
- The universal attack's generalization across architectures (SAM to SEEM) is only briefly validated, with no analysis of why or how well the perturbation transfers between different encoder backbones.
- The multi-crop PGD variant's effectiveness against the multi-crop mask generator is demonstrated but not thoroughly validated against alternative attack strategies or robustness analyses.

## Confidence
- **High confidence**: The mechanism of maximizing ℓ2-distance in the latent space to degrade segmentation masks is well-supported by the mathematical formulation and experimental results.
- **Medium confidence**: The universal perturbation's effectiveness across different image resolutions via interpolation is plausible but lacks detailed analysis of resolution-specific degradation patterns.
- **Medium confidence**: The multi-crop PGD variant's effectiveness against the multi-crop mask generator is demonstrated but not thoroughly validated against alternative attack strategies or robustness analyses.

## Next Checks
1. **Ablation study on encoder contribution**: Run controlled experiments where the prompt encoder is held constant while varying the image encoder's embedding (e.g., using frozen vs. unfrozen encoders) to quantify how much of the attack's success depends on the encoder alone versus prompt-encoder interactions.
2. **Cross-architecture transfer analysis**: Systematically test universal perturbations optimized on SAM across a wider range of segmentation models with different encoder architectures (e.g., Swin, ConvNeXt) and quantify degradation patterns to establish generalization bounds.
3. **Resolution-specific degradation analysis**: For universal attacks, test perturbations at multiple target resolutions (e.g., 256x256, 512x512, 1024x1024) and measure the correlation between interpolation error and attack effectiveness to establish resolution sensitivity.