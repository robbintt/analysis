---
ver: rpa2
title: 'pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting'
arxiv_id: '2305.11304'
source_url: https://arxiv.org/abs/2305.11304
tags:
- distribution
- time
- series
- ensemble
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes pTSE, a multi-model ensemble method for probabilistic
  time series forecasting. It addresses the challenge of combining multiple probabilistic
  forecasting models, each with its own distribution assumptions.
---

# pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting

## Quick Facts
- arXiv ID: 2305.11304
- Source URL: https://arxiv.org/abs/2305.11304
- Reference count: 7
- Key outcome: A multi-model ensemble method using HMM framework with Mixture Quantile Estimation to combine probabilistic forecasts, achieving superior performance on benchmark datasets

## Executive Summary
This paper introduces pTSE (Probabilistic Time Series Ensemble), a novel multi-model ensemble method for probabilistic time series forecasting. The method addresses the challenge of combining multiple probabilistic forecasting models, each with its own distribution assumptions, by treating the ensemble as a Hidden Markov Model where hidden states correspond to member models. pTSE incorporates Mixture Quantile Estimation into the Baum-Welch algorithm to estimate model residuals and uses bootstrap bandwidth selection for kernel density estimation. The method demonstrates superior performance compared to individual member models and competitive ensemble methods on three benchmark datasets.

## Method Summary
pTSE constructs an ensemble of probabilistic forecasting models by modeling them as states in a Hidden Markov Model (HMM). The Baum-Welch algorithm with Mixture Quantile Estimation (MQE) is used to estimate the HMM parameters, including transition probabilities and emission distributions. Each member model's error distribution is estimated using Weighted Kernel Density Estimation (WKDE), where weights are the posterior probabilities of each model being optimal. Bootstrap bandwidth selection optimizes the KDE performance for each model. The ensemble distribution is then computed as a weighted combination of the member model distributions based on the learned stationary distribution of the HMM.

## Key Results
- pTSE outperforms individual member models (DeepAR, SimpleFeedForwardEstimator, TemporalFusionTransformer, Transformer) on three benchmark datasets
- Superior performance compared to baseline ensemble methods (FFORMA, ModelRank) on quantile risk metrics
- Theoretical convergence proof showing ensemble distribution converges to stationary distribution of underlying HMM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble distribution converges to the stationary distribution of the underlying HMM
- Mechanism: Baum-Welch algorithm estimates transition matrix A and initial distribution π, with empirical distribution converging to π* ∫_(-∞)^τ F(o)do as T → ∞
- Core assumption: Time series follows HMM structure with Markov model transitions
- Evidence anchors: Abstract theoretical claim with Lemma 3.2 proof
- Break condition: Markov property assumption fails for non-Markovian time series

### Mechanism 2
- Claim: Mixture Quantile Estimation ensures ensemble respects individual model quantile predictions
- Mechanism: WKDE with posterior probabilities γ_k(t) estimates error distributions, preserving q-th quantile at zero
- Core assumption: Model errors are stationary and KDE-approximable
- Evidence anchors: Abstract MQE integration claim
- Break condition: Non-stationary or multimodal error distributions

### Mechanism 3
- Claim: Bootstrap bandwidth selection optimizes KDE performance
- Mechanism: Selects bandwidth σ* minimizing bootstrap integrated mean squared error (BIMSE)
- Core assumption: Optimal bandwidth found by minimizing BIMSE on bootstrap samples
- Evidence anchors: Section description of bootstrap method
- Break condition: Insufficient data points for reliable bootstrap estimation

## Foundational Learning

- Concept: Hidden Markov Models and their properties
  - Why needed here: Entire framework relies on HMM structure where hidden states represent optimal member models
  - Quick check question: What are the key conditions required for an HMM to have a unique stationary distribution?

- Concept: Expectation-Maximization algorithm and Baum-Welch specifically
  - Why needed here: Parameter estimation uses MQE Baum-Welch algorithm extending standard EM
  - Quick check question: How does the E-step differ between standard Baum-Welch and MQE Baum-Welch?

- Concept: Kernel Density Estimation and bandwidth selection
  - Why needed here: Individual model error distributions estimated using WKDE with bootstrap bandwidth selection
  - Quick check question: What are the trade-offs between under-smoothing and over-smoothing when selecting KDE bandwidth?

## Architecture Onboarding

- Component map: Model outputs → Baum-Welch parameter estimation → MQE error estimation → Bootstrap bandwidth selection → Ensemble distribution inference → Quantile prediction

- Critical path: Model outputs → HMM parameter estimation → MQE error distribution estimation → Bootstrap bandwidth optimization → Ensemble distribution computation → Quantile prediction generation

- Design tradeoffs:
  - HMM state count K vs. computational complexity (EM iterations scale poorly with K)
  - KDE bandwidth selection method (bootstrap vs. cross-validation vs. rule-of-thumb)
  - Full distributional outputs vs. just quantiles from member models
  - Model interpretability vs. performance when increasing ensemble complexity

- Failure signatures:
  - Poor performance when individual models have highly correlated errors
  - Convergence issues in Baum-Welch with near-zero transition probabilities
  - Overfitting with aggressive bandwidth selection
  - Degraded performance when time series violates Markov assumptions

- First 3 experiments:
  1. Synthetic HMM data validation: Generate data from known HMM, apply pTSE, verify convergence to theoretical stationary distribution
  2. Ablation study: Compare ensemble performance with/without MQE vs. simple weighted average of model quantiles
  3. Bandwidth sensitivity: Test performance across different bootstrap sample sizes and bandwidth search grids

## Open Questions the Paper Calls Out
- How does pTSE performance scale with increasing number of member models in the ensemble?
- Can pTSE be effectively applied to multivariate time series forecasting problems?
- How does pTSE handle non-stationary time series data and adapt to changing data distributions over time?

## Limitations
- Assumes underlying time series follows Markov structure, which may not hold for many real-world scenarios
- Computational complexity scales poorly with number of ensemble members, limiting practical use to small ensembles
- No ablation studies to isolate contribution of each component (MQE, bootstrap bandwidth selection)

## Confidence
- HMM convergence claim: High confidence (rigorous theoretical proof provided)
- MQE framework integration: Medium confidence (well-motivated but lacks direct corpus evidence)
- Bootstrap bandwidth selection: Medium confidence (described but no comparative analysis)

## Next Checks
1. Validate theoretical convergence by testing on synthetic HMM data with known stationary distribution
2. Conduct ablation study comparing pTSE components against simpler ensemble baselines
3. Evaluate scalability by testing performance with varying numbers of member models in the ensemble