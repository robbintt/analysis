---
ver: rpa2
title: 'Acceleration of stochastic gradient descent with momentum by averaging: finite-sample
  rates and asymptotic normality'
arxiv_id: '2305.17665'
source_url: https://arxiv.org/abs/2305.17665
tags:
- sgdm
- convergence
- momentum
- have
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides finite-sample convergence rates for stochastic
  gradient descent with momentum (SGDM) under strongly convex loss functions. It proves
  that mini-batch SGDM with appropriate momentum weights and batch sizes converges
  faster than mini-batch SGD to a neighborhood of the true solution.
---

# Acceleration of stochastic gradient descent with momentum by averaging: finite-sample rates and asymptotic normality

## Quick Facts
- arXiv ID: 2305.17665
- Source URL: https://arxiv.org/abs/2305.17665
- Reference count: 40
- Mini-batch SGDM with appropriate momentum weights and batch sizes converges faster than mini-batch SGD under strongly convex loss functions

## Executive Summary
This paper provides finite-sample convergence rates for stochastic gradient descent with momentum (SGDM) under strongly convex loss functions. The analysis demonstrates that mini-batch SGDM with appropriate momentum weights and batch sizes converges faster than mini-batch SGD to a neighborhood of the true solution. The paper also establishes asymptotic normality for the Polyak-averaging version of SGDM, showing that when the learning rate is α = Θ(t⁻ϵ) for ϵ ∈ (1/2, 1), the averaged SGDM is asymptotically equivalent to averaged SGD. The results are supported by theoretical analysis and numerical experiments across synthetic quadratic loss, synthetic logistic regression, and MNIST hand-written digit classification datasets.

## Method Summary
The method analyzes Stochastic Gradient Descent with Momentum (SGDM) under strongly convex loss functions using mini-batch gradients. The analysis combines momentum-based updates with Polyak-averaging to establish both finite-sample convergence rates and asymptotic normality. The algorithm maintains a momentum term that accumulates past gradients with weight γ, and uses batch gradients of size s to reduce variance. The theoretical framework relies on Lyapunov functions, matrix analysis, and concentration inequalities to derive convergence guarantees. The learning rate α is typically chosen as a constant (α = 0.001 for synthetic data, α = 1.0 for MNIST) with batch size s = 0.2N for synthetic examples and s = 256 for MNIST.

## Key Results
- Mini-batch SGDM with large batch size converges faster than mini-batch SGD to a neighborhood of the optimal value under strongly convex loss functions
- Polyak-averaging of SGDM trajectories accelerates convergence, especially in early iterations, and achieves the same asymptotic normality as averaged SGD
- SGDM permits broader choices of learning rates compared to SGD, allowing for potentially faster convergence with appropriate momentum weights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mini-batch SGDM with large batch size converges faster than mini-batch SGD to a neighborhood of the optimal value
- Mechanism: Momentum introduces a weighted average of past gradients, which acts as a preconditioner that improves the effective condition number of the optimization problem. The large batch size ensures low gradient variance, allowing the momentum term to stabilize and accelerate convergence
- Core assumption: The loss function is strongly convex and smooth (Assumptions A1-A2), and the batch size is sufficiently large relative to the problem parameters
- Break condition: If the batch size is too small, gradient variance dominates and momentum may not provide acceleration. If the loss is not strongly convex, the theoretical guarantees no longer apply

### Mechanism 2
- Claim: Polyak-averaging of SGDM trajectories accelerates convergence, especially in early iterations, and achieves the same asymptotic normality as averaged SGD
- Mechanism: Averaging smooths out the stochastic fluctuations in the trajectory, reducing variance without increasing bias. The averaged iterate inherits the faster early-phase convergence of SGDM while achieving the optimal asymptotic variance of averaged SGD
- Core assumption: The learning rate is appropriately chosen (either constant with diverging batch size, or decaying at rate Θ(t^-ϵ) with ϵ ∈ (1/2,1)), and the loss function satisfies strong convexity
- Break condition: If the learning rate decays too quickly (ϵ ≤ 1/2), the variance term in the averaged iterate may not diminish sufficiently for asymptotic normality. If the batch size is fixed and too small, the non-vanishing bias prevents convergence to the true solution

### Mechanism 3
- Claim: SGDM permits broader choices of learning rates compared to SGD, allowing for potentially faster convergence
- Mechanism: The momentum term provides an additional degree of freedom that can compensate for larger learning rates, which would otherwise cause divergence in SGD. This allows for a wider range of learning rate schedules that still guarantee convergence
- Core assumption: The momentum weight γ is chosen appropriately relative to the learning rate α and the problem parameters (L, µ)
- Break condition: If the momentum weight is too large relative to the learning rate, the algorithm may become unstable and oscillate. If the momentum weight is too small, the benefits over SGD are minimal

## Foundational Learning

- Concept: Strong convexity
  - Why needed here: Strong convexity ensures the existence of a unique global minimum and allows for linear convergence rates. It is a key assumption for the theoretical analysis of both SGD and SGDM
  - Quick check question: What is the relationship between strong convexity and the condition number of the Hessian matrix?

- Concept: Stochastic gradient variance and its impact on convergence
  - Why needed here: The variance of the stochastic gradient determines the size of the neighborhood to which SGD/SGDM converge and the rate at which this neighborhood shrinks. Understanding how batch size and momentum affect this variance is crucial for interpreting the results
  - Quick check question: How does increasing the batch size affect the variance of the stochastic gradient and the convergence rate of SGD?

- Concept: Polyak-averaging and its variance reduction properties
  - Why needed here: Polyak-averaging is a key technique used to accelerate the convergence of stochastic optimization algorithms and to establish asymptotic normality. Understanding how it works and its limitations is essential for applying the results in practice
  - Quick check question: What is the bias-variance tradeoff in Polyak-averaging, and how does it depend on the choice of the averaging window?

## Architecture Onboarding

- Component map: SGDM update (momentum weight γ, learning rate α) -> Mini-batch gradient computation -> Polyak-averaging scheme
- Critical path: The computation of the momentum term and the update of the iterate, with convergence analysis depending on spectral properties of the update matrix and variance of stochastic gradients
- Design tradeoffs: Larger batch sizes reduce gradient variance but increase per-iteration cost. Larger momentum weights can accelerate convergence but may cause instability. The choice of learning rate and momentum weight must balance these factors
- Failure signatures: If the learning rate is too large, the algorithm may diverge. If the momentum weight is too large, the algorithm may oscillate around the optimum. If the batch size is too small, the convergence may be slow due to high gradient variance
- First 3 experiments:
  1. Implement SGDM with different momentum weights (γ = 0, 0.5, 0.9) on a quadratic loss function and compare convergence rates
  2. Implement Polyak-averaging for SGD and SGDM and compare their convergence and asymptotic variances
  3. Vary the batch size and learning rate for SGDM and observe the impact on convergence speed and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal momentum weight γ for general strongly convex loss functions under different learning rates and batch sizes?
- Basis in paper: [explicit] The paper states "As one increases γ from 0 to 1, the convergence rate of averaged SGDM is first faster and then slower" and provides theoretical bounds for optimal γ in different regimes
- Why unresolved: The paper provides theoretical bounds but does not provide a complete characterization of the optimal γ across all parameter regimes
- What evidence would resolve it: Numerical experiments comparing different momentum weights across various learning rates and batch sizes to identify the optimal γ for each regime

### Open Question 2
- Question: How does SGDM perform on non-convex loss functions, particularly in deep learning applications?
- Basis in paper: [inferred] The paper focuses on strongly convex loss functions and mentions that extending the analysis to non-convex functions would be an interesting direction for future work
- Why unresolved: The theoretical analysis in the paper assumes strong convexity, which is not satisfied by most deep learning loss functions
- What evidence would resolve it: Numerical experiments on deep learning tasks comparing SGDM with other optimization algorithms

### Open Question 3
- Question: How does the convergence rate of SGDM change when the learning rate decays over time?
- Basis in paper: [explicit] The paper mentions that "it is also of interest to extend the analysis to the case when the learning rate decays over time" as a future direction
- Why unresolved: The current analysis assumes constant learning rates, and the behavior under decaying rates is not characterized
- What evidence would resolve it: Theoretical analysis of the convergence rate under decaying learning rates, supported by numerical experiments

### Open Question 4
- Question: How does the asymptotic normality of averaged SGDM hold under more general conditions?
- Basis in paper: [explicit] The paper establishes asymptotic normality under specific conditions on the learning rate and batch size but mentions that the conditions could be relaxed
- Why unresolved: The current asymptotic normality results require restrictive conditions on the learning rate and batch size
- What evidence would resolve it: Theoretical analysis of the asymptotic normality under more general conditions, supported by numerical experiments

## Limitations
- The theoretical analysis relies heavily on strong convexity assumptions, which may not hold for many practical deep learning applications
- The finite-sample rates depend on specific relationships between batch size, momentum weight, and learning rate that may be difficult to tune in practice
- The asymptotic normality results require careful choice of learning rate decay schedules, with the critical threshold of ϵ > 1/2 not being easily interpretable for practitioners

## Confidence
- High confidence in the theoretical framework and convergence rate proofs, as they build on established stochastic optimization techniques and provide rigorous finite-sample bounds under well-specified assumptions
- Medium confidence in the practical applicability of the results, particularly regarding the choice of hyperparameters (momentum weight, learning rate, batch size) that satisfy the theoretical conditions while maintaining good empirical performance
- Medium confidence in the asymptotic normality claims, as they depend on specific learning rate decay schedules and may not hold uniformly across all problem instances or when the strong convexity assumption is only approximately satisfied

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary momentum weights (γ ∈ {0.1, 0.5, 0.9, 0.99}) and learning rates (α ∈ {0.01, 0.1, 1.0}) across different batch sizes to identify practical regions where the theoretical benefits manifest empirically

2. **Robustness to convexity assumptions**: Test the algorithm on problems with near-convexity or mild non-convexity to understand how violations of the strong convexity assumption affect convergence rates and asymptotic normality

3. **Early stopping validation**: Compare the practical benefits of momentum in early iterations against the theoretical predictions by measuring convergence speed up to various iteration thresholds (T ∈ {100, 1000, 10000}) on standard benchmark datasets