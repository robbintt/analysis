---
ver: rpa2
title: 'N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets'
arxiv_id: '2308.02092'
source_url: https://arxiv.org/abs/2308.02092
tags:
- boosting
- biasing
- targets
- n-gram
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A two-step keyword boosting mechanism was developed that applies
  contextual biasing on normalized n-gram targets rather than raw tokens. This approach
  resolves missing hits from under-represented rare words and prevents over-boosting
  of multi-token keywords by adjusting weight application logic.
---

# N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets

## Quick Facts
- arXiv ID: 2308.02092
- Source URL: https://arxiv.org/abs/2308.02092
- Reference count: 0
- Primary result: 26% relative B-WER improvement on proprietary data with no degradation to unbiased words

## Executive Summary
This paper presents a two-step keyword boosting mechanism that applies contextual biasing on normalized n-gram targets rather than raw tokens. The approach addresses missing hits from under-represented rare words and prevents over-boosting of multi-token keywords by adjusting weight application logic. The method achieves a 26% relative improvement in keyword recognition rate (B-WER) on proprietary in-domain data while avoiding degradation to unbiased words, and a 2% relative B-WER improvement on LibriSpeech.

## Method Summary
The method involves normalizing rare words containing non-alphabetic characters or non-standard pronunciations into common forms that can be handled by trie-based biasing. This includes converting digits to spoken form, symbols to spoken form, casing to lowercase, compound words to n-grams, initialisms to standalone letters, and acronyms to lowercase. The normalized forms are then processed by the trie-based biasing algorithm, with adjusted logic that only applies the biasing score when the entire n-gram matches the final output. Inverse text normalization is applied to restore the original target format in the final transcript.

## Key Results
- 26% relative improvement in keyword recognition rate (B-WER) on proprietary in-domain data
- 2% relative B-WER improvement on LibriSpeech test set
- No degradation to unbiased words (U-WER) while improving B-WER
- Particularly effective for targets with non-alphabetic characters or non-standard pronunciations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalization transforms rare and stylized tokens into common forms that can be handled by the trie-based biasing decoder
- Mechanism: Text normalization converts digits, symbols, casing, compound words, initialisms, and acronyms into spoken-form tokens. These normalized forms are then processed by the trie-based biasing algorithm, which can now recognize and apply biasing scores to tokens that would otherwise be pruned or missing from the decoding beam
- Core assumption: The normalization step produces forms that are present in the language model and decoder vocabulary
- Evidence anchors:
  - [abstract] "We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets."
  - [section 5.3] "The normalization module converts many different cases, including, but not limited to: digits (0-9, numerical sequence like 356) to their spoken form, symbols (&, -, +, *, x) to their spoken form or eliminated, casing to lowercase form, compound words to n-grams separated by space, initialisms (e.g. IBM) to standalone letters, and acronyms (e.g. NASA) to lowercase form."
  - [corpus] Weak evidence for normalization efficacy; neighbor papers focus on trie-based biasing but do not directly discuss normalization of rare or stylized tokens

### Mechanism 2
- Claim: Adjusted boosting weight logic prevents over-boosting when normalized n-grams are split into multiple tokens
- Mechanism: Instead of boosting each unigram separately, the algorithm only applies the biasing score when the entire n-gram matches the final output. This avoids inflating scores for individual letters in initialisms or common tokens in compound words
- Core assumption: The final hypothesis will contain the full n-gram as a contiguous sequence
- Evidence anchors:
  - [section 5.3] "We adjust the boosting logic for n-gram boosting to only apply the biasing score to the full matches of the entire n-grams in the final output in order to avoid over-boosting issues."
  - [section 6.1] "We resolve this issue with n-gram boosting by adjusting the boosting logic to only boost full-match n-grams in finals in the n-gram boosting approach."
  - [corpus] No direct evidence from corpus; this is a novel adjustment not mentioned in neighbor papers

### Mechanism 3
- Claim: Inverse text normalization restores the original target format in the final transcript, preserving customer expectations
- Mechanism: After the decoder emits the normalized form, a reverse mapping is applied to convert it back to the original stylized or alphanumeric form using a pronunciation-based lookup table
- Core assumption: The inverse mapping is one-to-one and reversible for all normalized forms
- Evidence anchors:
  - [section 5.3] "Finally, we apply inverse text normalization to map the boosted targets back to their expected written format."
  - [section 6] "Biased terms for scoring only include the non-normalized, original terms (e.g. 'square1' counts as B-WER, 'square' and 'one' count as U-WER)."
  - [corpus] No explicit mention in neighbor papers of inverse normalization; this appears to be a unique implementation detail

## Foundational Learning

- Concept: Trie-based biasing in CTC decoding
  - Why needed here: The method relies on trie-based biasing to inject contextual scores into the beam search; understanding this mechanism is essential to see how normalization interacts with it
  - Quick check question: How does trie-based biasing differ from shallow fusion in hybrid ASR systems?

- Concept: Text normalization and inverse normalization
  - Why needed here: Normalization is the core transformation that enables rare tokens to be recognized; inverse normalization ensures the output matches the original format
  - Quick check question: What are the common categories of normalization (e.g., digit-to-word, symbol removal) used in ASR preprocessing?

- Concept: Weighted finite state transducer (WFST) composition
  - Why needed here: Although not directly used here, understanding WFST helps contextualize why trie-based biasing is a streaming-friendly alternative
  - Quick check question: In what scenarios would WFST-based biasing be preferred over trie-based biasing?

## Architecture Onboarding

- Component map: Raw audio -> Log-mel filterbank features -> 18-layer Conformer-CTC model -> Pyctcdecode with 4-gram LM -> Normalizer -> Trie-based biasing with adjusted weight logic -> Inverse Normalizer -> Final transcript

- Critical path: Raw audio → Conformer → Decoder with LM → Apply trie-based biasing → Inverse normalization → Final transcript

- Design tradeoffs:
  - Normalization improves coverage but requires manual review for accuracy
  - Adjusted boosting logic reduces over-boosting but may miss targets if n-grams are split
  - Streaming model emits partials; partial boosting could introduce noise if not handled carefully

- Failure signatures:
  - Missing hits: Rare tokens not in final beam even after normalization
  - Over-boosting: U-WER increases due to inflated scores on common unigrams
  - Incorrect formatting: Inverse normalization fails to map back to original form

- First 3 experiments:
  1. Apply default boosting to normalized targets on a small subset; measure U-WER and B-WER changes
  2. Implement adjusted n-gram boosting logic; verify reduction in over-boosting on initialism-heavy test cases
  3. Test inverse normalization accuracy on a set of manually curated normalization pairs

## Open Questions the Paper Calls Out

- How does the performance of n-gram boosting compare to deep biasing methods for rare words with non-alphabetic characters or non-standard pronunciations?
  - Basis in paper: [inferred] The paper mentions that deep biasing solutions tend to underperform on rare words compared to trie-based biasing methods, but does not directly compare n-gram boosting to deep biasing
  - Why unresolved: The paper focuses on comparing n-gram boosting to default trie-based boosting, without exploring how it fares against deep biasing techniques
  - What evidence would resolve it: A controlled experiment comparing n-gram boosting to a deep biasing approach (e.g., contextual RNN-T) on the same dataset with rare words containing non-alphabetic characters or non-standard pronunciations

- What is the impact of the manual review process in the normalization mapping on the scalability of the n-gram boosting method?
  - Basis in paper: [explicit] The paper mentions that the normalization mapping requires a certain level of manual review, which could be a limiting factor for scaling in the future
  - Why unresolved: The paper acknowledges the limitation but does not provide any quantitative analysis of the manual review process's impact on scalability
  - What evidence would resolve it: An analysis of the time and resources required for manual review in the normalization mapping process, as well as an exploration of potential automation techniques to reduce the manual workload

- How does the n-gram boosting method perform on real-world data compared to the TTS-generated test set?
  - Basis in paper: [inferred] The paper uses TTS-generated samples for the in-house test set, which might not accurately reflect the acoustic characteristics of real-world data
  - Why unresolved: The paper does not provide any evaluation of the n-gram boosting method on real-world data, relying solely on TTS-generated samples
  - What evidence would resolve it: An evaluation of the n-gram boosting method on a real-world dataset with actual speech recordings, comparing the results to those obtained from the TTS-generated test set

## Limitations

- The evaluation relies entirely on a proprietary in-domain dataset of 1917 samples, limiting external validation
- The paper assumes perfect one-to-one mapping in inverse text normalization, which may not hold in real-world cases
- The boosting weight tuning process methodology and sensitivity to hyperparameters are not fully specified

## Confidence

- **High Confidence**: The core mechanism of normalizing rare tokens to improve trie-based biasing coverage is well-grounded in text normalization literature and the empirical results (26% B-WER improvement) are substantial and consistent across datasets
- **Medium Confidence**: The adjusted n-gram boosting logic to prevent over-boosting is plausible and theoretically sound, but the paper does not provide ablation studies isolating its contribution from the normalization effect
- **Low Confidence**: The inverse normalization step's reliability and the exact impact of partial hypothesis boosting are not empirically validated, making these components speculative without further testing

## Next Checks

1. **Ablation Study**: Run experiments comparing raw token boosting vs. normalized token boosting vs. n-gram boosting, isolating the contribution of each component to B-WER and U-WER changes

2. **Inverse Normalization Robustness**: Create a test set with ambiguous or complex normalization cases (e.g., multiple valid written forms) and measure the accuracy and consistency of the inverse normalization output

3. **Partial Hypothesis Impact**: Implement a controlled experiment where partial hypothesis boosting is toggled on/off in streaming mode, measuring changes in WER and the frequency of partial-induced errors