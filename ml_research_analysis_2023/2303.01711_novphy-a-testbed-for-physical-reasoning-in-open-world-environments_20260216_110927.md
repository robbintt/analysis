---
ver: rpa2
title: 'NovPhy: A Testbed for Physical Reasoning in Open-world Environments'
arxiv_id: '2303.01711'
source_url: https://arxiv.org/abs/2303.01711
tags:
- novelty
- agent
- physical
- agents
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NovPhy, a novel testbed for evaluating AI
  agents' physical reasoning capabilities in open-world environments with novelties.
  The testbed applies eight types of novelties (e.g., new objects, agents, actions,
  interactions, relations, environments, goals, events) to five physical scenarios
  (single force, multiple forces, rolling, falling, sliding) in the Angry Birds game.
---

# NovPhy: A Testbed for Physical Reasoning in Open-world Environments

## Quick Facts
- arXiv ID: 2303.01711
- Source URL: https://arxiv.org/abs/2303.01711
- Authors: 
- Reference count: 40
- Key outcome: NovPhy evaluates AI agents' physical reasoning in open-world environments with novelties; humans achieve near-perfect novelty detection (CDT ≈ 1) and high adaptation (AP > 0.8, AUS > 0.7), while baseline AI agents show significantly lower performance (detection 0.3-0.6, adaptation 0.01-0.21).

## Executive Summary
This paper introduces NovPhy, a novel testbed for evaluating AI agents' physical reasoning capabilities in open-world environments with novelties. The testbed applies eight types of novelties (new objects, agents, actions, interactions, relations, environments, goals, events) to five physical scenarios (single force, multiple forces, rolling, falling, sliding) in the Angry Birds game. Human players achieved near-perfect novelty detection (CDT ≈ 1) and high adaptation performance (AP > 0.8, AUS > 0.7) across most scenarios. Baseline AI agents showed significantly lower performance, with detection rates around 0.3-0.6 and adaptation scores of 0.01-0.21, indicating substantial room for improvement. The study demonstrates that current AI systems struggle with novelty detection and adaptation compared to humans, highlighting the need for better open-world learning approaches in physical reasoning tasks.

## Method Summary
NovPhy evaluates AI agents' physical reasoning capabilities by applying eight types of novelties to five physical scenarios in the Angry Birds game. The testbed uses a trial-based evaluation protocol where agents first perform normal tasks, then encounter novel tasks, allowing measurement of both novelty detection and adaptation. Performance is measured using CDT (correctly detected trials), DD (detection delay), AP (asymptotic performance), and AUS (area under success curve). The study compares human performance against 11 baseline AI agents (heuristic, learning, and random agents) across two settings: novelty-physical scenario (NPS) and physical scenario-novelty (SPN) evaluation modes.

## Key Results
- Human players achieved near-perfect novelty detection (CDT ≈ 1) and high adaptation performance (AP > 0.8, AUS > 0.7) across most scenarios.
- Baseline AI agents showed significantly lower performance, with detection rates around 0.3-0.6 and adaptation scores of 0.01-0.21.
- DQN Adapt and Relational Adapt agents showed better adaptation behavior than other agents but still relied on the provided trajectory planner.
- Agents failed to adapt to certain novelty types like Relations, Environments, and Events, indicating fundamental limitations in their learning approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The testbed enables orthogonal novelty-physical scenario evaluation, allowing isolation of novelty effects from physical reasoning performance.
- Mechanism: By designing novelties orthogonal to physical scenarios, the testbed can evaluate the same novelty across different physical scenarios and vice versa, enabling clear attribution of performance differences to either the novelty type or the physical scenario.
- Core assumption: Physical scenarios and novelties are independent dimensions that can be varied separately without confounding effects.
- Evidence anchors:
  - [abstract] "The testbed consists of tasks that require agents to detect and adapt to novelties in physical scenarios."
  - [section 3.2] "We believe that, if an agent can truly perform under a novelty, the agent should be able to perform with that novelty when the novelty is applied to different physical scenarios."
- Break condition: If certain novelty-physical scenario combinations create non-orthogonal interactions that confound the evaluation.

### Mechanism 2
- Claim: Human performance serves as a strong baseline for evaluating AI agent novelty detection and adaptation capabilities.
- Mechanism: Human players achieved near-perfect novelty detection (CDT ≈ 1) and high adaptation performance (AP > 0.8, AUS > 0.7), providing a performance ceiling that AI agents must approach.
- Core assumption: Human performance represents an achievable benchmark for AI systems in this domain.
- Evidence anchors:
  - [abstract] "Human players achieved near-perfect novelty detection (CDT ≈ 1) and high adaptation performance (AP > 0.8, AUS > 0.7) across most scenarios."
  - [section 6.1] "Overall, the participants were able to correctly detect the novelties in almost all the trials (CDT is close to 1)."
- Break condition: If human performance is not representative of optimal performance in this domain.

### Mechanism 3
- Claim: The trial-based evaluation protocol with pre-novelty and post-novelty phases effectively measures novelty adaptation.
- Mechanism: The trial structure (normal tasks followed by novel tasks) creates a clear baseline for measuring performance degradation and recovery, capturing both detection and adaptation capabilities.
- Core assumption: Performance on normal tasks provides a valid baseline for measuring novelty impact.
- Evidence anchors:
  - [section 4.6] "A trial is a sequence of tasks, which starts from normal tasks and after a random number of normal tasks switches to novel tasks."
  - [section 4.7] "We use the area under the pass rate curve (success curve) to measure novelty adaptation performance."
- Break condition: If the transition between normal and novel tasks is not sufficiently distinct to measure adaptation.

## Foundational Learning

- Concept: Physical reasoning scenarios (single force, multiple forces, rolling, falling, sliding)
  - Why needed here: These scenarios form the foundation for testing how novelties affect physical reasoning capabilities.
  - Quick check question: Can you explain how a circular object would roll differently when gravity is inverted versus normal?

- Concept: Novelty hierarchy (objects, agents, actions, interactions, relations, environments, goals, events)
  - Why needed here: Understanding these categories is crucial for comprehending the types of novelties tested and their potential impacts.
  - Quick check question: What's the difference between a novelty at the "objects" level versus the "environments" level?

- Concept: Trial-based evaluation methodology
  - Why needed here: This evaluation structure is fundamental to understanding how novelty detection and adaptation are measured.
  - Quick check question: In a trial with 40 normal tasks and 40 novel tasks, at what point would novelty detection be considered successful?

## Architecture Onboarding

- Component map: Task templates -> Novelty generators -> Trial creation -> Agent execution -> Performance metrics
- Critical path: The primary workflow involves generating tasks from templates, running trials with agents, collecting performance data, and calculating novelty detection and adaptation metrics. The evaluation system must track performance across both normal and novel task phases.
- Design tradeoffs: The testbed prioritizes comprehensive novelty coverage over computational efficiency. Generating unlimited tasks from templates enables robust evaluation but increases computational requirements. The orthogonal design enables clean separation of novelty and scenario effects but may not capture all real-world interactions.
- Failure signatures: Common failure modes include novelty detection systems failing to trigger on subtle changes, agents failing to adapt even when novelty is detected, and performance metrics not capturing meaningful adaptation. The evaluation system must distinguish between novelty detection failures and adaptation failures.
- First 3 experiments:
  1. Run a baseline human trial on a single novelty-scenario to establish expected performance ranges.
  2. Test a simple heuristic agent on the same scenario to verify the evaluation pipeline.
  3. Evaluate a learning agent pre-trained on normal tasks to establish baseline AI performance without adaptation capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop AI agents that can efficiently detect and adapt to novelties across all eight levels of the novelty hierarchy in physical reasoning tasks?
- Basis in paper: [explicit] The paper establishes that current AI agents struggle with novelty detection and adaptation compared to humans, with detection rates around 0.3-0.6 and adaptation scores of 0.01-0.21 across most scenarios. The authors promote the development of intelligent agents capable of performing at the human level or above when operating in open-world physical environments.
- Why unresolved: Current AI agents show significant performance gaps compared to humans in both novelty detection (CDT ≈ 0.3-0.6 vs 0.9-1.0) and adaptation (AP ≈ 0.01-0.21 vs 0.7-0.9). The baseline agents, including DQN Adapt and Relational Adapt, fail to adapt to certain novelty types like Relations, Environments, and Events, indicating fundamental limitations in their learning approaches.
- What evidence would resolve it: Development of AI agents that can achieve CDT > 0.8 and AP > 0.7 across all eight novelty types and five physical scenarios, matching or exceeding human performance levels demonstrated in the study.

### Open Question 2
- Question: What architectural modifications would enable AI agents to handle novelty detection without relying on external trajectory planners?
- Basis in paper: [explicit] The authors note that DQN Adapt and Relational Adapt agents show better adaptation behavior compared to other agents but still use the provided trajectory planner, which limits their ability to adapt to novelties like inverted gravity or magnetic interactions that require different planning approaches.
- Why unresolved: The current adaptive agents (DQN Adapt and Relational Adapt) rely on pre-defined trajectory planners that cannot handle novelties requiring fundamental changes in planning strategies. The Naive Adapt agent, which searches for solution tuples, also fails to achieve human-level performance due to its limited search strategy.
- What evidence would resolve it: Development of agent architectures that can learn novel planning strategies directly from experience without requiring pre-defined trajectory planners, demonstrated by successful adaptation to all novelty types including Environments and Relations novelties.

### Open Question 3
- Question: How does the efficiency of novelty adaptation correlate with the agent's ability to generalize across different physical scenarios?
- Basis in paper: [inferred] The paper evaluates agents in two settings: performance on a novelty when applied to different physical scenarios (NPS) and performance on a physical scenario when different novelties are applied to it (SPN). The authors note that humans can adapt faster than agents, but don't fully explore the relationship between adaptation efficiency and generalization ability.
- Why unresolved: While the paper measures adaptation performance separately for different novelty-scenario combinations, it doesn't establish whether agents that adapt efficiently to one novelty type in one scenario can transfer this ability to other scenarios, or whether efficient adapters show better overall generalization.
- What evidence would resolve it: Empirical analysis showing whether agents with high NPS scores across multiple scenarios also demonstrate high SPN scores, and whether there's a correlation between adaptation speed (measured by AUS) and generalization performance across different physical reasoning tasks.

## Limitations
- The testbed's evaluation framework assumes orthogonality between physical scenarios and novelties, but real-world interactions may not be fully independent.
- The performance gap between humans and AI agents could be partially attributed to the provided trajectory planner, which may limit agents' ability to fully adapt to novelty-induced changes in physics.
- The study focuses on Angry Birds as a single game environment, raising questions about generalizability to other physical reasoning domains.

## Confidence
- **High Confidence**: The human performance baseline results (CDT ≈ 1, AP > 0.8, AUS > 0.7) are well-supported by the experimental data with 47 participants across multiple trials.
- **Medium Confidence**: The baseline AI agent performance metrics are reliable within the constraints of the provided trajectory planner, but the true adaptation capabilities may be underestimated.
- **Medium Confidence**: The orthogonality assumption between novelties and physical scenarios is theoretically sound but may not capture all real-world interaction effects.

## Next Checks
1. **Trajectory Planner Validation**: Test agent performance with and without the trajectory planner to determine its impact on novelty adaptation capabilities.
2. **Generalizability Test**: Apply the same novelty types to a different physical reasoning environment (e.g., a different physics-based game) to assess cross-domain performance.
3. **Interaction Analysis**: Systematically test combinations of multiple novelties simultaneously to identify any non-orthogonal effects that the current evaluation framework might miss.