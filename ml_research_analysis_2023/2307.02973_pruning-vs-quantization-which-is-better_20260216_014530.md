---
ver: rpa2
title: 'Pruning vs Quantization: Which is Better?'
arxiv_id: '2307.02973'
source_url: https://arxiv.org/abs/2307.02973
tags:
- pruning
- quantization
- neural
- networks
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares the effectiveness of neural network pruning
  and quantization for model compression. It provides both analytical comparisons
  on general data distributions and empirical evaluations on real-world models.
---

# Pruning vs Quantization: Which is Better?

## Quick Facts
- **arXiv ID**: 2307.02973
- **Source URL**: https://arxiv.org/abs/2307.02973
- **Reference count**: 40
- **Primary result**: Quantization generally outperforms pruning for neural network compression across most compression ratios, except at very high compression levels where pruning may have a slight advantage.

## Executive Summary
This paper provides a comprehensive comparison between neural network pruning and quantization for model compression. Through both theoretical analysis and extensive empirical evaluation, the authors demonstrate that quantization consistently achieves higher accuracy than pruning at similar compression ratios across 8 large-scale models on 3 different tasks. The study uses signal-to-noise ratio (SNR) as a key metric to compare the two methods theoretically and validates these predictions through practical experiments. The findings suggest quantization should be the preferred choice for most practical compression applications, with pruning potentially offering advantages only at extreme compression levels.

## Method Summary
The paper employs a two-pronged approach combining theoretical analysis with empirical validation. Theoretically, it analyzes error characteristics of symmetric uniform quantization and magnitude pruning on standard normal and Student's t distributions, deriving SNR comparisons and lower bounds. Empirically, the authors conduct experiments across 8 large-scale models (ResNet-18/50, MobileNet-V2/V3, EfficientNet-lite, ViT, DeepLab-V3, EfficientDet) on three tasks (ImageNet classification, Pascal VOC segmentation, MS COCO object detection). The experiments use magnitude pruning with fine-tuning and quantization-aware training (QAT) with LSQ, matching compression ratios across methods and fine-tuning for equal epochs.

## Key Results
- Quantization consistently achieves higher accuracy than pruning at similar compression ratios across all tested models and tasks
- Theoretical SNR analysis correctly predicts the relative performance of quantization and pruning on real neural network weight distributions
- At very high compression ratios (equivalent to 2-3 bits), pruning may slightly outperform quantization due to quantization's clipping error from outliers
- The SNR metric strongly correlates with model accuracy for both compression methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quantization generally outperforms pruning for neural network compression across most compression ratios.
- **Mechanism:** Quantization uses a continuous mapping (scale factor + bit-width) to represent values, which preserves more information than pruning's binary mask approach. This allows quantization to maintain higher signal-to-noise ratios (SNR) for moderate compression levels.
- **Core assumption:** The distribution of neural network weights follows patterns (e.g., near-Gaussian) where quantization's error characteristics are superior to pruning's.
- **Evidence anchors:**
  - [abstract]: "Our results show that in most cases quantization outperforms pruning."
  - [section 3.3]: Theoretical analysis shows quantization achieves higher SNR than pruning for standard normal distributions and moderate compression ratios.
  - [corpus]: Weak evidence - no direct comparison of quantization vs pruning in corpus, but related work suggests both are commonly studied independently.
- **Break Condition:** At very high compression ratios (equivalent to 2-3 bits per value), pruning may become more accurate due to quantization's increased clipping error from outliers.

### Mechanism 2
- **Claim:** The signal-to-noise ratio (SNR) is a reliable predictor of model accuracy for both quantization and pruning.
- **Mechanism:** SNR measures the ratio of signal power to noise power. Higher SNR indicates better preservation of the original information, which correlates with maintaining model accuracy after compression.
- **Core assumption:** The relationship between per-layer SNR and full-model accuracy is strong and consistent across different network architectures.
- **Evidence anchors:**
  - [section 3]: "We use this analytic formulation for our distribution results below" - shows SNR used to compare quantization and pruning errors.
  - [section 4.3]: "We observe a strong correlation between SNR and accuracy" - confirms SNR correlates with model accuracy.
  - [corpus]: Weak evidence - no direct discussion of SNR as accuracy predictor in corpus.
- **Break Condition:** If the optimization algorithm for quantization or pruning is significantly suboptimal, the SNR may not accurately reflect the true compression error.

### Mechanism 3
- **Claim:** Fine-tuning after quantization leads to learning new representations, while fine-tuning after pruning tends to recover the original representation.
- **Mechanism:** Pruning removes weights but preserves the overall structure of the network, allowing fine-tuning to adjust remaining weights back toward the original solution. Quantization changes the numerical precision of weights, forcing the network to adapt to a different computational space during fine-tuning.
- **Core assumption:** The representations learned during fine-tuning depend on the nature of the compression technique applied.
- **Evidence anchors:**
  - [section H]: "For pruning, the representations tend to become closer to the original representation during fine-tuning. However, for quantization the fine-tuning rather learns a representation that is different from the original one."
  - [corpus]: Weak evidence - no direct discussion of representation changes during fine-tuning in corpus.
- **Break Condition:** If the network architecture or task is highly sensitive to weight precision, quantization might force representation changes even with pruning-like fine-tuning behavior.

## Foundational Learning

- **Concept:** Signal-to-Noise Ratio (SNR) as a measure of compression quality
  - Why needed here: SNR is the primary metric used throughout the paper to compare quantization and pruning errors. Understanding SNR is essential to interpret the results.
  - Quick check question: If a compression method achieves SNR of 20 dB while another achieves 10 dB for the same compression ratio, which method better preserves the original information?

- **Concept:** Symmetric uniform quantization and magnitude pruning
  - Why needed here: These are the specific compression techniques being compared. The paper's analysis and experiments are built around these methods.
  - Quick check question: In symmetric uniform quantization, what determines the quantization levels for a given bit-width?

- **Concept:** Kurtosis as a measure of distribution tail heaviness
  - Why needed here: Kurtosis is used to predict when pruning might outperform quantization, particularly in distributions with heavy tails/outliers.
  - Quick check question: How does increasing kurtosis in a distribution affect the relative performance of quantization versus pruning?

## Architecture Onboarding

- **Component map:** Theoretical Analysis -> Distribution analysis (standard normal, Student's t) -> Lower bounds for PTQ -> Experimental Framework -> PyTorch model zoo analysis -> Per-layer experiments -> Full-model fine-tuning -> Evaluation Metrics -> SNR, accuracy, mIoO, mAP -> Compression Methods -> QAT, magnitude pruning with fine-tuning

- **Critical path:**
  1. Analyze weight distributions theoretically to understand error characteristics
  2. Compute lower bounds for PTQ quantization and exact solutions for pruning
  3. Run per-layer experiments to validate theoretical predictions
  4. Conduct full-model experiments across 8 models on 3 tasks
  5. Analyze learned representations during fine-tuning

- **Design tradeoffs:**
  - Theoretical vs empirical: The paper balances analytical bounds with practical experiments
  - Hardware-agnostic vs hardware-aware: Focuses on accuracy rather than implementation specifics
  - Compression ratio matching: Compares methods at equivalent compression levels rather than absolute bit-widths

- **Failure signatures:**
  - If quantization shows poor results at low bit-widths (2-3 bits), this may indicate the distribution has heavy tails
  - If pruning shows competitive results, the network may have sparse or outlier-heavy weight distributions
  - If full-model accuracy doesn't correlate with per-layer SNR, the optimization algorithm may be suboptimal

- **First 3 experiments:**
  1. Replicate the standard normal distribution analysis from section 3.3 to verify quantization's SNR advantage
  2. Run the PyTorch model zoo analysis (section 3.4) to identify which layers benefit from pruning vs quantization
  3. Implement the PTQ lower bound computation (section 4.1) for a simple convolutional layer to validate the bound tightness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do different hardware implementations of pruning and quantization affect the relative performance and energy efficiency of each method in practice?
- **Basis in paper:** Explicit - The paper acknowledges hardware implications in section 6 but deliberately avoids discussing them in detail, focusing instead on accuracy comparisons.
- **Why unresolved:** The paper explicitly states it avoids discussing hardware aspects to focus on accuracy, leaving a gap in understanding real-world performance.
- **What evidence would resolve it:** Comprehensive benchmarking studies comparing energy consumption, latency, and accuracy trade-offs of pruning and quantization across various hardware platforms (CPUs, GPUs, specialized AI accelerators).

### Open Question 2
- **Question:** How does the combination of pruning and quantization techniques affect model performance compared to using either method alone?
- **Basis in paper:** Explicit - The paper states in section 6 that it does not study combinations of pruning and quantization apart from analyzing inherent sparsity in quantized tensors.
- **Why unresolved:** The authors explicitly leave this for future work, suggesting that the interaction between these two compression techniques is not well understood.
- **What evidence would resolve it:** Experimental comparisons of models compressed using combined pruning and quantization techniques against models compressed using only one method, measuring accuracy and compression ratios.

### Open Question 3
- **Question:** How do low-precision floating-point and logarithmic quantization formats compare to uniform quantization in terms of accuracy and efficiency?
- **Basis in paper:** Explicit - The paper states in section 6 that it considers only uniform quantization and ignores other formats like low-precision floating or logarithmic quantization.
- **Why unresolved:** The authors note that these formats are not likely to change the results but acknowledge they haven't been explored, leaving uncertainty about their potential benefits.
- **What evidence would resolve it:** Comparative studies of models quantized using different formats (uniform, low-precision floating, logarithmic) under identical conditions, measuring accuracy, compression ratio, and computational efficiency.

## Limitations
- Theoretical analysis assumes standard normal and Student's t distributions, which may not fully capture real neural network weight distributions
- Results primarily evaluated on convolutional and transformer-based architectures, with limited coverage of recurrent networks
- Hardware implementation considerations (energy efficiency, latency) are not addressed, focusing solely on accuracy metrics

## Confidence
- **High Confidence**: The general superiority of quantization over pruning at moderate compression ratios is well-supported by both theoretical analysis (SNR comparisons) and extensive empirical validation across 8 models
- **Medium Confidence**: The prediction that pruning may outperform quantization at very high compression ratios (2-3 bits) is theoretically sound but requires more empirical validation across diverse architectures
- **Medium Confidence**: The claim about different fine-tuning behaviors (representation recovery vs. learning new representations) is supported by single-layer experiments but needs full-model validation

## Next Checks
1. Test the theoretical prediction that pruning outperforms quantization at extreme compression ratios (2-3 bits) on additional architectures beyond those evaluated
2. Validate the SNR-accuracy correlation on recurrent neural networks and other architectures not covered in the original experiments
3. Conduct hardware-aware evaluation to assess whether the accuracy advantage of quantization translates to practical deployment benefits (energy, latency)