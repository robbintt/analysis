---
ver: rpa2
title: Machine Translation to Control Formality Features in the Target Language
arxiv_id: '2311.13475'
source_url: https://arxiv.org/abs/2311.13475
tags:
- formality
- language
- translation
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating from English
  to formality-sensitive languages like Hindi, where English lacks inherent formality
  markers. The core idea is to compare the performance of custom-built bilingual models
  with pre-trained multilingual models like IndicBERT, when fine-tuned in a formality-controlled
  setting.
---

# Machine Translation to Control Formality Features in the Target Language

## Quick Facts
- **arXiv ID**: 2311.13475
- **Source URL**: https://arxiv.org/abs/2311.13475
- **Reference count**: 28
- **One-line primary result**: Pre-trained multilingual models like IndicBERT, when fine-tuned for formality control, significantly outperformed custom-built bilingual models for English-to-Hindi translation, achieving 90.16% accuracy for formal Hindi and 94.06% for informal Hindi.

## Executive Summary
This paper addresses the challenge of translating from English to formality-sensitive languages like Hindi, where English lacks inherent formality markers. The core idea is to compare the performance of custom-built bilingual models with pre-trained multilingual models like IndicBERT, when fine-tuned in a formality-controlled setting. To overcome the lack of formality-annotated data, automated annotation techniques were employed to augment the training corpus. The primary modeling approach leveraged transformer models and evaluated formality accuracy by comparing predicted masked tokens with ground truth. Results showed that pre-trained multilingual models like IndicBERT, when fine-tuned for formality control, significantly outperformed custom-built bilingual models, achieving 90.16% accuracy for formal Hindi and 94.06% for informal Hindi. This demonstrates the effectiveness of formality-aware multilingual models for translation tasks in low-resource languages.

## Method Summary
The method involves fine-tuning pre-trained multilingual models (IndicBERT) on formality-annotated data for English-to-Hindi translation. Automated annotation techniques were used to label a large Hindi corpus (Samanantar) with formal and informal tags based on word-level formality markers extracted from a small annotated corpus (IWSLT2022). The models were evaluated using masked language modeling, where the accuracy of predicted formality-appropriate tokens was measured against ground truth. The fine-tuning process used a linear scheduler with warm-up, optimizing with Adam at a learning rate of 1e-4 over 30 epochs.

## Key Results
- Pre-trained multilingual models (IndicBERT) achieved 90.16% accuracy for formal Hindi and 94.06% for informal Hindi when fine-tuned for formality control.
- Custom-built bilingual models significantly underperformed compared to fine-tuned multilingual models in formality accuracy.
- Automated annotation of formality using word lists effectively expanded training data from 400 to millions of sentences.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning pre-trained multilingual models like IndicBERT on formality-annotated data significantly improves formality control in translation.
- Mechanism: Pre-trained models already capture rich linguistic representations across multiple languages, including Hindi. Fine-tuning these models on formality-labeled data allows them to adjust their internal representations to better handle formal and informal registers specific to the target language.
- Core assumption: The pre-trained model's general linguistic knowledge transfers effectively to formality-specific nuances when fine-tuned on small, formality-annotated datasets.
- Evidence anchors:
  - [abstract]: "Results showed that pre-trained multilingual models like IndicBERT, when fine-tuned for formality control, significantly outperformed custom-built bilingual models, achieving 90.16% accuracy for formal Hindi and 94.06% for informal Hindi."
  - [section]: "In this study, we opted to leverage pre-trained model IndicBERT v2, which is a Masked Language Model (MLM) instead of building a multilingual model from scratch. These models were fine-tuned using formality control settings to cater to our specific requirements."
  - [corpus]: Weak corpus support - no explicit mention of multilingual model performance in related papers.

### Mechanism 2
- Claim: Automated annotation of formality using extracted word lists effectively expands training data for formality-controlled translation.
- Mechanism: By identifying formal and informal words from a small annotated corpus (IWSLT2022) and applying them to a larger unlabeled corpus (Samanantar), the method creates synthetic formality labels that enable training on much larger datasets.
- Core assumption: The presence of formal or informal words in a sentence is a reliable indicator of the sentence's overall formality level.
- Evidence anchors:
  - [section]: "To automate the labelling process, we developed a script designed to identify the extracted words within the Samantar corpus. The script then accurately labeled each word as either 'Formal' or 'Informal' based on its respective set."
  - [section]: "The underlying assumption we made during this process was that sentences containing words from the formal set would be considered formal sentences, while sentences containing words from the informal set would be classified as informal sentences."
  - [corpus]: Weak corpus support - no explicit validation of the assumption that word-level formality reliably indicates sentence-level formality.

### Mechanism 3
- Claim: Masked Language Models (MLMs) are more effective than sequence-to-sequence models for formality control in translation tasks.
- Mechanism: MLMs like IndicBERT can predict missing tokens in context, making them naturally suited for formality transfer tasks where certain words need to be replaced with their formal or informal equivalents while maintaining grammatical coherence.
- Core assumption: The masked prediction task in MLMs aligns well with the formality substitution problem, where formal and informal variants need to be predicted based on context.
- Evidence anchors:
  - [section]: "MLMs are a powerful tool for natural language processing tasks, such as text generation, machine translation, and question answering. They are able to learn the meaning of words and phrases by seeing how they are used in context."
  - [section]: "Upon loading the fine-tuned IndicBERTv2 model, which has been trained to perform masked language modeling, we applied a basic text cleaning function to preprocess the dataset..."
  - [corpus]: Weak corpus support - no explicit comparison of MLM vs. Seq2Seq approaches in related papers.

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: The paper uses transformer-based models (IndicBERT, custom bilingual models) which rely on self-attention to capture long-range dependencies and contextual relationships in language.
  - Quick check question: How does multi-head attention in transformers help capture different aspects of formality in Hindi sentences?

- **Concept**: Masked Language Modeling (MLM) pretraining
  - Why needed here: The success of IndicBERT in formality control depends on its MLM pretraining, which teaches the model to understand word relationships and context.
  - Quick check question: What advantage does MLM pretraining provide over traditional language modeling for formality transfer tasks?

- **Concept**: Data augmentation through automated annotation
  - Why needed here: The paper addresses data scarcity by automatically labeling a large corpus based on word-level formality annotations, expanding training data from 400 to millions of sentences.
  - Quick check question: What are the risks of using automated annotation for formality labels, and how might these affect model performance?

## Architecture Onboarding

- **Component map**: IWSLT2022 (small annotated corpus) → word extraction → automated annotation of Samanantar corpus → training data → Custom bilingual transformer → IndicBERT v2 MLM (pre-trained) → fine-tuned models → Masked token prediction evaluation → Formality accuracy calculation
- **Critical path**: Data preparation → model fine-tuning → masked prediction evaluation → formality accuracy calculation
- **Design tradeoffs**: Using pre-trained models trades computational resources for better performance vs. building custom bilingual models from scratch
- **Failure signatures**: Low formality accuracy despite high general translation accuracy indicates the model isn't learning formality patterns; high loss during fine-tuning suggests poor annotation quality
- **First 3 experiments**:
  1. Fine-tune IndicBERT on 1000 formality-annotated sentences and measure masked prediction accuracy
  2. Compare formality accuracy of fine-tuned IndicBERT vs. custom bilingual model on the same test set
  3. Test model performance on sentences with mixed formality (both formal and informal words present)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of formality-controlled multilingual models like IndicBERT compare to bilingual models when translating from English to other low-resource languages with similar formality markers, such as Japanese or Korean?
- Basis in paper: [explicit] The paper compares the performance of bilingual models and pre-trained multilingual models like IndicBERT for English-to-Hindi translation.
- Why unresolved: The paper only tests the models on English-to-Hindi translation, so the performance on other low-resource languages with formality markers is unknown.
- What evidence would resolve it: Conduct similar experiments with English-to-Japanese or English-to-Korean translation tasks and compare the performance of bilingual and multilingual models.

### Open Question 2
- Question: How does the use of automated annotation techniques to augment training data with formality labels impact the model's ability to generalize to unseen formality variations in real-world scenarios?
- Basis in paper: [explicit] The paper uses automated annotation techniques to label a large corpus of Hindi sentences with formal and informal tags to train the models.
- Why unresolved: While the models perform well on the test data, it's unclear how well they would handle novel formality variations not present in the training data.
- What evidence would resolve it: Evaluate the models on a diverse set of real-world Hindi sentences with varying formality levels not seen during training.

### Open Question 3
- Question: What are the specific challenges and limitations of using pre-trained multilingual models like IndicBERT for formality control in low-resource languages, and how can these be addressed?
- Basis in paper: [explicit] The paper notes that fine-tuning pre-trained multilingual models like IndicBERT can be challenging due to resource constraints and the need for large amounts of training data.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges or potential solutions for using these models in low-resource settings.
- What evidence would resolve it: Conduct a thorough analysis of the challenges and limitations, and propose and evaluate potential solutions, such as transfer learning techniques or data augmentation strategies.

## Limitations
- The automated annotation approach assumes word-level formality reliably indicates sentence-level formality, but this assumption is not empirically validated and may not capture grammatical formality markers.
- The evaluation metric (masked token prediction accuracy) may not fully capture true translation quality, as formality control requires maintaining consistent formality throughout entire translations.
- The performance comparison between models may be confounded by other factors beyond formality control capabilities, such as general transfer learning benefits from pretraining.

## Confidence
- **Low**: The automated annotation approach assumes that word-level formality reliably indicates sentence-level formality, but this assumption is not empirically validated.
- **Medium**: While the results show clear performance differences between IndicBERT and custom bilingual models, the evaluation metric may not fully capture true translation quality.
- **Medium**: The paper claims that pre-trained multilingual models outperform custom-built bilingual models, but this comparison may be confounded by other factors beyond formality control.

## Next Checks
1. **Annotation Quality Validation**: Create a manually annotated validation set (separate from training data) of 100-200 sentences to measure the accuracy of the automated annotation system. Calculate precision, recall, and F1-score for formality classification to quantify annotation noise and its impact on model performance.

2. **End-to-End Formality Evaluation**: Conduct human evaluation studies where native Hindi speakers rate the formality appropriateness of full translated sentences (not just masked tokens). Compare ratings between model outputs for formal and informal settings to verify that the model maintains consistent formality throughout entire translations.

3. **Generalization Across Domains**: Test the fine-tuned models on formality-controlled translation tasks across different domains (news, conversation, literature) to assess whether the formality patterns learned from Samanantar generalize beyond the training corpus distribution. Measure domain-specific formality accuracy and identify any significant performance drops.