---
ver: rpa2
title: 'NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving
  Datasets using Markup Annotations'
arxiv_id: '2312.06352'
source_url: https://arxiv.org/abs/2312.06352
tags:
- language
- dataset
- driving
- tasks
- autonomous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NuScenes-MQA, a novel QA dataset for autonomous
  driving that addresses the challenge of evaluating both natural language generation
  and visual question answering (VQA) capabilities. The authors propose Markup-QA,
  a dataset annotation technique where QAs are enclosed within special markups, enabling
  simultaneous evaluation of textual quality and QA accuracy.
---

# NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations

## Quick Facts
- arXiv ID: 2312.06352
- Source URL: https://arxiv.org/abs/2312.06352
- Authors: 
- Reference count: 40
- Key outcome: Introduces NuScenes-MQA dataset with 1,459,933 QA pairs for evaluating both natural language generation and VQA accuracy in autonomous driving scenarios

## Executive Summary
This paper presents NuScenes-MQA, a novel QA dataset for autonomous driving that addresses the challenge of simultaneously evaluating natural language generation quality and visual question answering accuracy. The authors introduce Markup-QA, an annotation technique where QAs are enclosed within special markup tokens, enabling integrated evaluation within the same model output. Using this methodology, they constructed a dataset from the nuScenes autonomous driving dataset comprising over 1.4 million QA pairs focused on object presence, counts, proximity, and relative positions in driving scenes. Experimental results demonstrate that treating markup tokens as special tokens improves VQA performance while slightly reducing natural language generation quality.

## Method Summary
The method involves constructing a QA dataset using the nuScenes autonomous driving dataset by generating questions about object presence, counts, proximity, and relative positions. QA pairs are annotated using Markup-QA, where special tokens like `<obj>`, `<cnt>`, and `<target>` are embedded within full sentences. A vision-language model architecture combines CLIP ViT-large patch14 for feature extraction with OPT language models (125M, 1.3B, 6.7B parameters) connected through adapter modules. The model is trained using cross-entropy loss with a one-cycle scheduler for 10 epochs, and evaluation uses BLEU, METEOR, and ROUGE scores for text generation alongside accuracy metrics for VQA tasks.

## Key Results
- OPT-1.3B model achieved BLEU-1 score of 0.698 and BLEU-4 score of 0.404 for sentence generation
- Model showed accuracy of 0.714 for object category detection in VQA tasks
- Using markup tokens as special tokens improved VQA performance but slightly reduced natural language generation quality
- Spatial reasoning tasks (Loc. x and Loc. y) proved particularly challenging for all models tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The markup-based annotation allows simultaneous evaluation of both text generation quality and VQA accuracy within the same model output
- Mechanism: By embedding special tokens like `<obj>`, `<cnt>`, and `<target>` around specific answers within full sentences, the model can be evaluated on both fluent sentence generation (BLEU, METEOR scores) and precise VQA accuracy (object detection, counting) using the same generated text
- Core assumption: The model can learn to generate both fluent natural language and correctly formatted markup tokens without interference
- Evidence anchors:
  - [abstract]: "Using this annotation methodology, we designed the NuScenes-MQA dataset. This dataset empowers the development of vision language models, especially for autonomous driving tasks, by focusing on both descriptive capabilities and precise QA."
  - [section 3.2]: "By enveloping the target objects with these markups, we can easily extract the relevant words from the answers... Using our markup methodology, we can simultaneously evaluate both elements."
  - [corpus]: Weak - corpus neighbors don't discuss markup-based annotation approaches
- Break condition: If the model confuses natural language generation with markup token generation, causing either poor fluency or inaccurate QA extraction

### Mechanism 2
- Claim: Treating markup tokens as special tokens improves VQA performance by providing explicit visual grounding
- Mechanism: When markup tokens like `<obj>`, `<cnt>`, and `<dst>` are treated as additional tokens in the tokenizer, the model learns stronger associations between visual features and the semantic concepts they represent, leading to improved accuracy on detection and counting tasks
- Core assumption: The visual-linguistic associations learned for natural language can be effectively transferred to markup tokens
- Evidence anchors:
  - [section 5.3.2]: "When the special token was integrated, the OPT-125M model showed notable improvements in several metrics such as Yes/No, Cat., and Cat. & Count"
  - [section 2.3]: "Special words improve the efficiency of vision language tasks by better linking visual information to text"
  - [corpus]: Weak - corpus neighbors focus on VQA but don't discuss special token approaches
- Break condition: If the model overfits to markup tokens and fails to generalize to natural language descriptions

### Mechanism 3
- Claim: The dataset's focus on spatial relationships and relative positioning creates challenging VQA tasks that better reflect real autonomous driving needs
- Mechanism: By requiring the model to answer questions about object proximity, direction, and coordinates relative to the ego vehicle, the dataset tests spatial reasoning capabilities that are critical for safe autonomous navigation
- Core assumption: Models trained on spatial reasoning tasks will transfer better to real-world driving scenarios than those trained only on object classification
- Evidence anchors:
  - [abstract]: "The dataset focuses on object presence, counts, proximity, and relative positions in driving scenes"
  - [section 3.1]: "Our dataset is based on four core concepts... Relative Distance to Ego Vehicle... Relative location to Ego vehicle"
  - [section 5.3.2]: "Interestingly, for Loc. x and Loc. y, none of the models showed impressive results, indicating that the task was particularly challenging"
- Break condition: If the spatial reasoning performance remains consistently poor despite model scaling, suggesting the task formulation may be too difficult or the spatial features are not properly encoded

## Foundational Learning

- Concept: Vision-language model architecture (CLIP-like ViT + LLM decoder)
  - Why needed here: The paper combines a vision transformer for feature extraction with a language model for text generation, requiring understanding of how these components interact
  - Quick check question: What is the role of the adapter module between the ViT and language model?

- Concept: Special token handling in tokenization
  - Why needed here: The paper shows different results when markup tokens are treated as special tokens vs regular text, affecting both generation quality and VQA accuracy
  - Quick check question: How does adding markup tokens as special tokens change the tokenizer vocabulary and model behavior?

- Concept: VQA evaluation metrics beyond standard accuracy
  - Why needed here: The paper uses BLEU, METEOR, and ROUGE scores for generation quality alongside accuracy metrics for VQA, requiring understanding of when each metric is appropriate
  - Quick check question: When would you prefer BLEU-4 over BLEU-1 for evaluating VQA model outputs?

## Architecture Onboarding

- Component map: Vision transformer (CLIP ViT-large patch14) → Adapter module → Language model (OPT) with causal attention mask for text, non-causal for vision features
- Critical path: Image → ViT feature extraction → Adapter projection → Language model input → Text generation with markup tokens
- Design tradeoffs: Using markup tokens improves VQA accuracy but reduces generation fluency; using a single shared ViT for all camera views reduces parameters but may limit view-specific feature learning
- Failure signatures: Poor BLEU scores with special tokens indicate generation issues; low VQA accuracy with special tokens suggests grounding problems; consistent spatial task failures indicate feature encoding issues
- First 3 experiments:
  1. Test model with and without special tokens on a small subset to verify the trade-off between generation quality and VQA accuracy
  2. Evaluate adapter module size (dimensions) to find optimal balance between parameter efficiency and performance
  3. Test different positional encoding strategies for the multi-camera setup to improve spatial reasoning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of markup tokens as special tokens affect the model's ability to generate coherent and contextually relevant descriptions for driving scenes?
- Basis in paper: [explicit] The paper discusses the impact of using markup tokens as special tokens on the model's performance, noting that while it improves VQA performance, it slightly reduces natural language generation quality.
- Why unresolved: The paper provides initial insights but does not fully explore the long-term effects of this trade-off on the model's overall effectiveness in real-world autonomous driving scenarios.
- What evidence would resolve it: Comparative studies on model performance in practical applications, such as in simulated driving environments or real-world tests, could provide insights into the long-term impact of using markup tokens.

### Open Question 2
- Question: Can the NuScenes-MQA dataset be effectively used to train larger models, such as OPT-6.7B, given its rule-based construction and limited task variety?
- Basis in paper: [explicit] The paper mentions that the dataset's rule-based construction and limited task variety might make it less ideal for training larger models.
- Why unresolved: The paper does not provide empirical evidence on the dataset's scalability or its effectiveness in training larger models.
- What evidence would resolve it: Training larger models using the NuScenes-MQA dataset and evaluating their performance on various autonomous driving tasks could provide insights into its scalability and effectiveness.

### Open Question 3
- Question: How can the model's ability to capture expressions pertaining to positional data be improved, given the current limitations in task variety?
- Basis in paper: [explicit] The paper highlights that the limited variety of tasks addressing spatial information raises concerns about the model's ability to capture expressions related to positional data.
- Why unresolved: The paper does not propose specific methods or experiments to enhance the model's understanding of spatial expressions.
- What evidence would resolve it: Developing and testing new tasks or data augmentation techniques that focus on spatial expressions could improve the model's ability to capture positional data.

## Limitations
- The dataset's focus on relatively simple spatial relationships may limit its effectiveness for evaluating complex driving scenarios requiring deeper reasoning about traffic dynamics
- The trade-off between VQA accuracy and natural language generation quality remains unclear in terms of real-world applicability
- The rule-based construction and limited task variety might make the dataset less ideal for training larger models

## Confidence

**High Confidence**: The dataset construction methodology and the basic observation that special tokens improve VQA accuracy while reducing generation quality are well-supported by the experimental results presented.

**Medium Confidence**: The claim that Markup-QA enables "simultaneous evaluation" of both text quality and QA accuracy assumes that downstream applications will benefit from this dual capability, but this connection is not fully established.

**Low Confidence**: The assertion that this dataset "empowers the development of vision language models" for autonomous driving is forward-looking and not directly validated by the experiments, which only demonstrate performance on the dataset itself rather than transfer to real driving scenarios.

## Next Checks
1. **Real-world transfer evaluation**: Test whether models trained on NuScenes-MQA show improved performance on actual autonomous driving decision-making tasks compared to models trained on standard VQA datasets, measuring both accuracy and safety-critical metrics.

2. **Markup token generalization**: Evaluate whether models trained with special tokens can successfully generate natural language descriptions without markups when needed, or if they become overly dependent on the markup format.

3. **Spatial reasoning robustness**: Test model performance on out-of-distribution spatial configurations not well-represented in nuScenes (e.g., unusual camera angles, occluded objects, or complex multi-object scenarios) to assess the limits of current spatial reasoning capabilities.