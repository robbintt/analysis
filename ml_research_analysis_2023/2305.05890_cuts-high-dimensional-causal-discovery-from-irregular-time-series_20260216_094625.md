---
ver: rpa2
title: 'CUTS+: High-dimensional Causal Discovery from Irregular Time-series'
arxiv_id: '2305.05890'
source_url: https://arxiv.org/abs/2305.05890
tags:
- causal
- cuts
- time-series
- discovery
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the scalability challenge in high-dimensional
  causal discovery from irregular time-series data. The authors propose CUTS+, an
  extension of the CUTS method, which introduces two key techniques: Coarse-to-fine
  Discovery (C2FD) and a Message-passing-based Graph Neural Network (MPGNN).'
---

# CUTS+: High-dimensional Causal Discovery from Irregular Time-series

## Quick Facts
- arXiv ID: 2305.05890
- Source URL: https://arxiv.org/abs/2305.05890
- Reference count: 40
- Key outcome: CUTS+ achieves 0.9923-0.9927 AUROC on high-dimensional Air Quality dataset with 163 time-series

## Executive Summary
This paper addresses the scalability challenge in high-dimensional causal discovery from irregular time-series data. The authors propose CUTS+, an extension of the CUTS method, which introduces two key techniques: Coarse-to-fine Discovery (C2FD) and a Message-passing-based Graph Neural Network (MPGNN). C2FD reduces the parameter space exponentially in early training stages while maintaining convergence to true causal graphs, and MPGNN eliminates component-wise redundancy while preserving causal disentanglement. The method is evaluated on simulated, quasi-realistic, and real datasets with different types of irregular sampling, showing significant performance improvements especially on high-dimensional datasets.

## Method Summary
CUTS+ combines Coarse-to-fine Discovery (C2FD) with a Message-passing-based Graph Neural Network (MPGNN) to enable scalable causal discovery from irregular time-series. C2FD groups time-series initially and progressively refines the causal graph by doubling group numbers every 20 epochs. MPGNN replaces N separate LSTMs/MLPs with a parameter-efficient architecture using message passing layers integrated with GRU units. The method performs joint imputation and causal discovery, with missing data imputation providing better input for causal discovery while causal discovery identifies dependencies that improve imputation. Training alternates between Causal Discovery Stage and Prediction Stage using Gumbel-softmax sampling for graph optimization.

## Key Results
- CUTS+ achieves 0.9923 AUROC for Random Missing with p=0.3 on Air Quality dataset (163 time-series)
- CUTS+ achieves 0.9927 AUROC for Random Block Missing with pblk=0.15% on same dataset
- Significant performance improvement over existing methods, especially on high-dimensional datasets (N=512)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse-to-fine discovery (C2FD) reduces the parameter space exponentially in early training stages while maintaining convergence to true causal graph
- Mechanism: By grouping N time-series into Ng groups initially, the method learns a N×Ng adjacency matrix instead of N×N, then progressively splits groups doubling Ng until each contains one time-series
- Core assumption: Causal relationships are approximately preserved at group level (if group Granger causes j, at least one member does)
- Evidence anchors:
  - [abstract] "Coarse-to-fine-discovery (C2FD), a simple yet efficient technique to facilitate scalable causal graph optimization"
  - [section] "The parameter number to learn is greatly decreased in the initial stages. In the initial stages when Ng≪ N, only N· Ng instead of N2 parameters are required to be optimized"
  - [corpus] Weak - no direct corpus evidence for C2FD's convergence guarantees
- Break condition: If causal relationships are not preserved at group level (highly heterogeneous causal parents within groups), C2FD may converge to incorrect graph

### Mechanism 2
- Claim: Message-passing graph neural network (MPGNN) eliminates component-wise redundancy while preserving causal disentanglement
- Mechanism: Instead of N separate LSTMs/MLPs (O(N) parameters), MPGNN shares parameters across time-series using message passing layers integrated with GRU units
- Core assumption: Temporal dynamics can be shared across time-series while maintaining causal disentanglement through adjacency masking
- Evidence anchors:
  - [abstract] "leveraging a message-passing-based graph neural network (MPGNN) to avoid structural redundancy in CUTS+"
  - [section] "Scalability of MPGNN. The number of parameters needing to be optimized in MPGNN can be calculated with l (|νr| +|νu| +|νc| +|ψ|), where l is the number of MPGNN layers. Comparing CUTS+ with component-wise GRU, whose parameter number is N l(|νr| +|νu| +|νc| +|ψ|)"
  - [corpus] Weak - no direct corpus evidence for MPGNN's performance on irregular time-series
- Break condition: If temporal dynamics are highly specific to individual time-series (no shared patterns), MPGNN may underperform component-wise networks

### Mechanism 3
- Claim: Joint imputation and causal discovery creates mutual reinforcement improving both tasks
- Mechanism: Missing data imputation provides better input for causal discovery, while causal discovery identifies dependencies that improve imputation
- Core assumption: Causal relationships contain information about missingness patterns
- Evidence anchors:
  - [abstract] "raises the scalability by introducing a technique called Coarse-to-fine-discovery (C2FD) and leveraging a message-passing-based graph neural network (MPGNN)"
  - [section] "Our CUTS+ performs imputation and causal discovery jointly, and as a result accurate imputation is crucial for reliable causal discovery"
  - [corpus] Weak - no direct corpus evidence for this mutual reinforcement mechanism
- Break condition: If missingness is completely random with no causal structure, the mutual reinforcement may provide minimal benefit

## Foundational Learning

- Concept: Granger causality and its neural network formulation
  - Why needed here: The entire method builds on Granger causality framework extended with neural networks
  - Quick check question: What is the fundamental assumption of Granger causality that makes it compatible with neural network prediction?

- Concept: Irregular time-series imputation techniques
  - Why needed here: The method handles missing data through joint imputation, requiring understanding of time-series imputation
  - Quick check question: What are the two main types of irregular sampling addressed in the paper?

- Concept: Graph neural networks and message passing
  - Why needed here: MPGNN is the core architectural innovation for scalability
  - Quick check question: How does the message passing layer in MPGNN differ from standard GRU units?

## Architecture Onboarding

- Component map: Grouping matrix G -> Gumbel-softmax sampling -> MPGNN prediction with causal masking -> Alternate between causal discovery and prediction stages -> Double Ng and split groups every 20 epochs

- Critical path: 1) Initialize grouping matrix G with Ng groups, 2) Learn GCPG Q through Gumbel-softmax sampling, 3) Perform MPGNN prediction with causal masking, 4) Alternate between causal discovery and prediction stages, 5) Double Ng and split groups every 20 epochs

- Design tradeoffs: C2FD trades initial accuracy for scalability (coarser groups initially), MPGNN trades parameter efficiency for potential loss of time-series-specific patterns, joint imputation trades complexity for improved performance on irregular data

- Failure signatures: Poor performance on high-dimensional data indicates C2FD or MPGNN issues; degraded performance on irregular data suggests imputation module problems; failure to converge suggests hyperparameter issues with learning rates or Gumbel-softmax temperature

- First 3 experiments:
  1. Run on small VAR dataset (N=16) with both RM and RBM to verify basic functionality
  2. Test scalability by increasing N from 16 to 128 on VAR dataset, measuring AUROC degradation
  3. Validate imputation performance on AQI dataset with different missingness patterns (p=0.3, p=0.6)

## Open Questions the Paper Calls Out

- Question: How does the performance of CUTS+ scale with extremely high dimensions (N > 1000) and what are the theoretical limits of its scalability?
  - Basis in paper: [inferred] The paper demonstrates good scalability up to N=512, but doesn't explore performance beyond this point or provide theoretical limits
  - Why unresolved: The experiments only tested up to N=512, and the paper doesn't provide theoretical analysis of scalability limits
  - What evidence would resolve it: Additional experiments with N > 1000 time series and theoretical analysis of computational complexity and memory requirements

- Question: How robust is CUTS+ to the presence of latent confounders and instantaneous effects, which violate the assumptions of Granger causality?
  - Basis in paper: [explicit] The authors acknowledge this as a limitation, stating "CUTS+ is a Granger-causality-based causal discovery algorithm. A main limitation of our CUTS+ is the gap between Granger causality and real causality. Granger causality may fail when there exists latent confounders or sub-sampled causal effects, which are common in real datasets."
  - Why unresolved: The paper doesn't provide experiments or theoretical analysis on the performance degradation when these assumptions are violated
  - What evidence would resolve it: Experiments with synthetic data containing known latent confounders or instantaneous effects, and analysis of performance degradation in such scenarios

- Question: How does the choice of initial grouping in C2FD affect the final causal discovery results, and is there an optimal strategy for determining the initial number of groups?
  - Basis in paper: [inferred] The paper mentions that initial grouping is based on the number of nodes, but doesn't explore how different initial groupings affect the results or provide guidance on choosing the optimal number of groups
  - Why unresolved: The paper doesn't provide sensitivity analysis of initial grouping or guidelines for choosing the initial number of groups
  - What evidence would resolve it: Experiments comparing performance with different initial group numbers and analysis of how initial grouping affects convergence and final results

## Limitations

- The method's performance relies heavily on the assumption that causal relationships are approximately preserved at group level during C2FD, which may not hold for highly heterogeneous time-series
- The MPGNN architecture assumes temporal dynamics can be shared across time-series, potentially limiting performance when individual time-series exhibit unique patterns
- The joint imputation mechanism lacks rigorous validation of the claimed mutual reinforcement between imputation and causal discovery

## Confidence

- High confidence in scalability improvements (measured parameter reduction from O(N²) to O(N))
- Medium confidence in causal discovery performance on benchmark datasets
- Low confidence in theoretical guarantees of convergence due to limited analytical treatment

## Next Checks

1. Test CUTS+ on datasets with known heterogeneous causal parents to verify C2FD assumptions about group-level causality preservation
2. Compare MPGNN performance against component-wise networks on datasets with strong time-series-specific temporal dynamics
3. Conduct ablation studies removing the joint imputation component to quantify its contribution to overall performance