---
ver: rpa2
title: 'Automated Evaluation of Classroom Instructional Support with LLMs and BoWs:
  Connecting Global Predictions to Specific Feedback'
arxiv_id: '2310.01132'
source_url: https://arxiv.org/abs/2310.01132
tags:
- classroom
- class
- each
- what
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores using Large Language Models (LLMs) to automatically\
  \ estimate the \u201CInstructional Support\u201D domain scores of the CLassroom\
  \ Assessment Scoring System (CLASS), a widely used classroom observation protocol.\
  \ The authors use zero-shot prompting of Meta\u2019s Llama2 and a Bag of Words (BoW)\
  \ model to classify individual teacher utterances for 11 behavioral indicators of\
  \ Instructional Support."
---

# Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback

## Quick Facts
- arXiv ID: 2310.01132
- Source URL: https://arxiv.org/abs/2310.01132
- Authors: 
- Reference count: 36
- Key outcome: Automatic CLASS Instructional Support estimation accuracy using LLM and BoW methods approaches human inter-rater reliability (Pearson R up to 0.48 vs human R=0.55)

## Executive Summary
This paper presents a novel approach for automatically estimating CLASS (Class Assessment Scoring System) Instructional Support domain scores from classroom transcripts using Large Language Models (LLMs) and Bag of Words (BoW) models. The system classifies individual teacher utterances for 11 behavioral indicators of Instructional Support, then aggregates these utterance-level judgments to predict global CLASS scores. Experiments on two CLASS-coded datasets show that automatic CLASS score estimation accuracy approaches human inter-rater reliability, with LLMs generally outperforming BoW and combined models performing best. The method also provides explainable feedback by identifying which specific utterances most positively or negatively correlate with CLASS dimensions.

## Method Summary
The method uses zero-shot prompting of Meta's Llama2-7b-chat to classify individual teacher utterances for 11 CLASS Instructional Support behavioral indicators, producing binary presence/absence judgments. These utterance-level judgments are aggregated across the entire 15-minute observation session through summation and z-scoring, then regressed to CLASS scores using L1-regularized linear regression. A Bag of Words baseline uses 300 most frequent n-grams plus special tokens to characterize utterances. Both approaches are evaluated on two datasets (UVA Toddler and NCRECE PreK) using 5-fold cross-validation stratified by teacher, with performance measured by Pearson correlation and RMSE against human-annotated CLASS scores.

## Key Results
- Automatic CLASS Instructional Support estimation accuracy reaches Pearson R=0.48, approaching human inter-rater reliability of R=0.55
- LLMs (Llama2) generally yield slightly higher accuracy than BoW models for this task
- Best-performing models typically combined features extracted from both LLM and BoW approaches
- The system provides explainable feedback by visualizing marginal contributions of individual utterances to CLASS dimension scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing global CLASS scoring into utterance-level indicator judgments reduces semantic complexity for LLMs.
- Mechanism: Instead of asking Llama2 to score an entire transcript globally, the system breaks the task into 11 separate binary indicator judgments per utterance, each corresponding to a specific behavioral indicator in the CLASS framework.
- Core assumption: LLMs can reliably distinguish presence/absence of individual instructional support behaviors when prompted appropriately.
- Evidence anchors:
  - [abstract] "We design a machine learning architecture that uses either zero-shot prompting of Meta's Llama2... to classify individual utterances of teachers' speech... for the presence of 11 behavioral indicators of Instructional Support."
  - [section] "Our general approach to applying Llama2 was to ask it to analyze an individual utterance from a classroom transcript and infer whether or not the utterance exhibits one of the behavioral indicators... associated with CLASS Instructional Support."
  - [corpus] Weak - related papers focus on utterance classification but don't specifically address CLASS indicator decomposition.
- Break condition: If LLM performance on individual indicators doesn't correlate with actual CLASS scores, or if utterance boundaries are too coarse to capture behavioral indicators.

### Mechanism 2
- Claim: Aggregating utterance-level judgments with L1-regularized linear regression enables interpretable global CLASS scoring.
- Mechanism: Binary indicator judgments from each utterance are summed across the transcript, z-scored, and regressed to CLASS scores using L1 regularization, which produces sparse weights that highlight influential utterances.
- Core assumption: Simple linear aggregation of utterance-level features can approximate complex human holistic scoring.
- Evidence anchors:
  - [abstract] "These utterance-level judgments are aggregated over an entire 15-min observation session to estimate a global CLASS score."
  - [section] "We predict the CLASS scores for the session as ŷ = w⊤ ỹ + b, where w are the regression weights and b is the bias term."
  - [corpus] Weak - related papers use regression but not specifically L1 for CLASS score estimation.
- Break condition: If the relationship between utterance-level indicators and global scores is non-linear, or if the sparsity assumption fails.

### Mechanism 3
- Claim: Combining LLM and BoW features captures complementary information about classroom discourse.
- Mechanism: LLM provides semantic understanding of instructional support behaviors while BoW captures frequency-based patterns; their combination improves prediction accuracy over either method alone.
- Core assumption: Different feature extraction methods capture orthogonal aspects of classroom discourse quality.
- Evidence anchors:
  - [abstract] "(2) LLMs yield slightly greater accuracy than BoW for this task; and (3) the best models often combined features extracted from both LLM and BoW."
  - [section] "Results suggest that the system can estimate CLASS scores with an accuracy that is similar to human inter-rater reliability... the best models often combined features extracted from both LLM and BoW."
  - [corpus] Moderate - related papers show both LLM and BoW approaches work for classroom analysis tasks.
- Break condition: If LLM and BoW features are highly correlated, or if one method consistently dominates.

## Foundational Learning

- Concept: CLASS observation protocol and Instructional Support domain
  - Why needed here: The entire system is designed to estimate CLASS Instructional Support scores, so understanding what these scores measure is fundamental.
  - Quick check question: What are the three dimensions that comprise the CLASS Instructional Support domain?

- Concept: Large Language Models and zero-shot prompting
  - Why needed here: The primary method uses Llama2 zero-shot prompting to classify utterances for instructional support indicators.
  - Quick check question: How does zero-shot prompting differ from few-shot or fine-tuned approaches?

- Concept: Bag of Words feature extraction and n-gram analysis
  - Why needed here: The BoW baseline uses n-gram counts to characterize utterances for CLASS prediction.
  - Quick check question: What is the difference between unigrams, bigrams, and trigrams in BoW analysis?

## Architecture Onboarding

- Component map:
  Audio input -> Whisper ASR -> Utterance segmentation -> LLM branch (Llama2 prompt -> Binary indicator vector) OR BoW branch (n-gram counting -> Integer feature vector) -> Aggregation (Sum and z-score feature vectors) -> Regression (L1-regularized linear regression) -> CLASS score

- Critical path: Audio transcription -> Utterance classification (LLM or BoW) -> Feature aggregation -> Regression -> CLASS score

- Design tradeoffs:
  - Zero-shot prompting vs. fine-tuning: Zero-shot requires no training data but may be less accurate
  - Utterance-level vs. transcript-level analysis: More granular but loses context
  - L1 vs. L2 regularization: L1 provides sparsity and interpretability but may underfit

- Failure signatures:
  - Low Pearson correlation with human scores (<0.3)
  - Inconsistent predictions across similar utterances
  - Regression weights concentrated on trivial features (e.g., question marks)
  - LLM explanations that don't match its own classifications

- First 3 experiments:
  1. Test LLM classification accuracy on individual indicators using held-out utterances
  2. Compare BoW and LLM performance on a small subset of CLASS-coded data
  3. Validate aggregation method by correlating utterance-level judgments with human CLASS scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much improvement in CLASS Instructional Support prediction accuracy is needed for automated systems to be practically useful for teacher feedback?
- Basis in paper: [inferred] The paper notes that the highest Pearson correlation achieved was 0.46, which is approaching human inter-rater reliability but likely needs substantial improvement for practical use.
- Why unresolved: The paper does not specify what correlation threshold would be needed for the system to provide actionable feedback to teachers in real classroom settings.
- What evidence would resolve it: A study comparing different levels of prediction accuracy (e.g., R=0.5, 0.6, 0.7) with teacher satisfaction and improvement in classroom practices after receiving automated feedback.

### Open Question 2
- Question: Would multi-modal analysis (incorporating audio, video, and transcripts) significantly improve CLASS score prediction accuracy compared to transcript-only approaches?
- Basis in paper: [inferred] The authors suggest future work could explore how multi-modal LLMs could analyze audio and video in addition to transcripts to infer more about educational and emotional context.
- Why unresolved: The current study only used automatically generated transcripts, so the potential benefit of incorporating other modalities remains unknown.
- What evidence would resolve it: A comparative study using the same dataset with multi-modal analysis versus transcript-only analysis to measure the improvement in CLASS score prediction accuracy.

### Open Question 3
- Question: Are automated classroom analysis measures predictive of students' learning outcomes, or do they primarily reflect classroom observation scores from human coders?
- Basis in paper: [explicit] The authors note this as future work: "it would be valuable to explore whether the automated classroom analysis measures are predictive of students' learning outcomes, not just classroom observation scores from human coders."
- Why unresolved: The current study only validated the automated measures against human CLASS scores, not against actual student learning outcomes.
- What evidence would resolve it: A longitudinal study correlating automated CLASS scores with student achievement measures over time to establish predictive validity.

## Limitations

- Limited generalizability: Method validated only on two specific datasets (UVA Toddler and NCRECE PreK) covering limited age ranges
- ASR dependency: Reliance on automated Whisper transcription could introduce errors that compound through the analysis pipeline
- Sample size constraints: Moderate sample sizes (172 and 561 sessions) with limited statistical power for comparisons

## Confidence

- High Confidence: LLMs and BoW can be used to classify classroom utterances for CLASS indicators; the general architecture is sound
- Medium Confidence: The claimed accuracy levels (R=0.48) and their comparison to human reliability (R=0.55); the superiority of combined LLM+BoW features
- Low Confidence: The generalizability to other CLASS domains, age groups, or educational contexts; the robustness of zero-shot prompting to prompt variations

## Next Checks

1. Cross-Domain Validation: Test the exact methodology on CLASS data from different age groups (elementary, secondary) and other CLASS domains (Emotional Support, Classroom Organization) to assess generalizability.

2. Prompt Robustness Analysis: Systematically vary the zero-shot prompts across different formulations and evaluate how sensitive the predictions are to these changes, including testing with different LLM models of varying sizes.

3. Error Analysis on Transcription Quality: Conduct controlled experiments where the same audio is transcribed with different ASR systems (or human transcription) to quantify how transcription errors propagate through to final CLASS score predictions.