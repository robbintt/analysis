---
ver: rpa2
title: 'Prometheus: Inducing Fine-grained Evaluation Capability in Language Models'
arxiv_id: '2310.08491'
source_url: https://arxiv.org/abs/2310.08491
tags:
- score
- feedback
- evaluation
- gpt-4
- rubric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prometheus, a 13B open-source language model
  trained for fine-grained evaluation of text. The key innovation is the Feedback
  Collection dataset, which contains 1K customized score rubrics, 20K instructions,
  and 100K responses with feedback generated by GPT-4.
---

# Prometheus: Inducing Fine-grained Evaluation Capability in Language Models

## Quick Facts
- arXiv ID: 2310.08491
- Source URL: https://arxiv.org/abs/2310.08491
- Authors: 
- Reference count: 40
- Key outcome: Prometheus achieves Pearson correlation of 0.897 with human evaluators on 45 customized score rubrics, on par with GPT-4 and significantly outperforming ChatGPT (0.392)

## Executive Summary
This paper introduces Prometheus, a 13B open-source language model trained for fine-grained evaluation of text. The key innovation is the Feedback Collection dataset, which contains 1K customized score rubrics, 20K instructions, and 100K responses with feedback generated by GPT-4. Prometheus is fine-tuned on this dataset to evaluate any given long-form text based on a user-defined score rubric. Experimental results show that Prometheus achieves strong correlation with human evaluators and demonstrates potential as a universal reward model, while being fully open-source and inexpensive compared to proprietary LLMs.

## Method Summary
Prometheus fine-tunes Llama-2-Chat 13B on the Feedback Collection dataset, which includes 1K customized score rubrics, 20K instructions, and 100K responses with feedback generated by GPT-4. The model is trained to generate both detailed feedback and scores sequentially, using reference materials (score rubrics and reference answers) to guide evaluation. The training uses structured output formatting with a batch size of 20 and learning rate of 1e-5, optimizing for correlation with human evaluators across multiple benchmarks.

## Key Results
- Prometheus achieves Pearson correlation of 0.897 with human evaluators on 45 customized score rubrics
- Outperforms ChatGPT significantly (0.392 vs 0.897) and matches GPT-4 performance (0.882)
- Shows strong performance as a universal reward model, ranking human preferences correctly in 67.74% of cases
- Demonstrates ability to generate interpretable feedback explaining evaluation decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including reference materials enables the evaluator model to focus solely on assessment rather than solving the instruction
- Mechanism: Reference answers provide a clear target for evaluation, decomposing the task into solving the instruction and evaluating the response
- Core assumption: An evaluator that doesn't need to solve the problem can more effectively evaluate different responses
- Evidence anchors: [abstract] Feedback Collection dataset; [section 6.1] incorporating reference materials; [corpus] Weak - based on ablation experiments only
- Break condition: Performance degrades significantly when reference materials are removed

### Mechanism 2
- Claim: Training on thousands of customized score rubrics enables generalization to diverse, real-world evaluation criteria
- Mechanism: Exposure to wide variety of evaluation criteria through Feedback Collection enables flexible adaptation to different user-defined scoring rubrics
- Core assumption: Exposure to diverse rubrics during training enables generalization to unseen rubrics during inference
- Evidence anchors: [abstract] Pearson correlation of 0.897 with human evaluators; [section 5.2] +0.255 improvement over base model; [corpus] Weak - correlation metrics show effectiveness but don't directly prove mechanism
- Break condition: Performance significantly degrades when trained only on coarse-grained rubrics (12 options)

### Mechanism 3
- Claim: Sequential generation of feedback followed by score enables interpretable evaluation decisions
- Mechanism: Training the model to first generate detailed feedback explaining reasoning, then generate final score, leads to more transparent decisions
- Core assumption: Forcing model to articulate reasoning before scoring improves quality and interpretability of evaluation decisions
- Evidence anchors: [abstract] fine-tuned to evaluate based on user-defined score rubric; [section 6.3] generates detailed feedback criticizing response components; [corpus] Weak - qualitative examples show interpretable feedback but no quantitative comparison
- Break condition: Performance drops significantly when sequential generation is removed (w/o feedback distillation)

## Foundational Learning

- Concept: In-context learning (ICL) and its limitations for structured output generation
  - Why needed here: ICL was difficult due to token limitations (3072 tokens max), making it hard to include demonstrations in prompts
  - Quick check question: Why did authors choose fine-tuning over prompting strategies like ICL for Prometheus?

- Concept: Correlation metrics (Pearson, Kendall-Tau, Spearman) for evaluating evaluator performance
  - Why needed here: Primary metrics used to compare Prometheus against human evaluators and GPT-4 across multiple benchmarks
  - Quick check question: What does a Pearson correlation of 0.897 with human evaluators indicate about Prometheus's performance?

- Concept: Absolute grading vs ranking grading evaluation schemes
  - Why needed here: Prometheus trained on absolute grading (generating scores 1-5) but tested on ranking grading benchmarks to assess universal reward model potential
  - Quick check question: How does the paper evaluate Prometheus's ability to function as a reward model despite being trained on absolute grading?

## Architecture Onboarding

- Component map: Llama-2-Chat 13B -> Fine-tuned on Feedback Collection -> Prometheus evaluator -> Generates feedback + score
- Critical path: Instruction + Response + Score Rubric + Reference Answer -> Model input -> Feedback generation -> Score generation
- Design tradeoffs: Fine-tuning for structured output vs. prompting for flexible evaluation; training on absolute grading vs. ranking grading; including reference materials vs. requiring model to solve problems internally
- Failure signatures: Poor performance when reference materials are removed; inability to generate parseable scores without fine-tuning; performance degradation when trained on coarse-grained vs. fine-grained rubrics
- First 3 experiments:
  1. Test Prometheus correlation with human evaluators on Feedback Bench, Vicuna Bench, and MT Bench
  2. Conduct pairwise comparison of feedback quality between Prometheus, GPT-4, and GPT-3.5-Turbo
  3. Measure correlation between Prometheus evaluation and GPT-4 evaluation across 1222 score rubrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Prometheus perform on long-form responses compared to short-form ones?
- Basis in paper: [inferred] The paper mentions testing on datasets with varying response lengths but doesn't explicitly compare long vs. short responses
- Why unresolved: Paper doesn't provide detailed analysis of performance across different response lengths
- What evidence would resolve it: Experiment comparing Prometheus's performance on a dataset with both long and short responses, measuring correlation with human evaluation for each group

### Open Question 2
- Question: Can Prometheus be effectively used for cross-lingual evaluation?
- Basis in paper: [inferred] Paper focuses on English responses and doesn't explore Prometheus's capability in other languages
- Why unresolved: With increasing importance of multilingual models, understanding Prometheus's performance in different languages is crucial
- What evidence would resolve it: Testing Prometheus on a multilingual evaluation dataset and comparing performance across languages

### Open Question 3
- Question: How does Prometheus handle responses with implicit bias or controversial content?
- Basis in paper: [inferred] Paper mentions training on diverse dataset but doesn't explicitly address performance on biased or controversial content
- Why unresolved: As an evaluator model, Prometheus's ability to handle sensitive content is important for real-world applicability and ethical considerations
- What evidence would resolve it: Experiment testing Prometheus's evaluation of responses containing various forms of bias or controversial topics, comparing judgments with human evaluations

## Limitations

- Evaluation relies heavily on GPT-4-generated reference feedback as ground truth, introducing potential biases
- Human evaluation component (18 participants rating 6 instructions) is relatively small and may not capture full diversity of real-world scenarios
- Paper does not provide comprehensive analysis of performance degradation when reference materials are removed

## Confidence

**High Confidence**: Correlation results with human evaluators (0.897 Pearson) are well-supported by experimental design and multiple validation datasets. Claim that Prometheus outperforms ChatGPT significantly (0.392 vs 0.897) is robust across different correlation metrics.

**Medium Confidence**: Mechanism by which rubric diversity enables generalization is plausible but not definitively proven. While correlation improvements are demonstrated, causal link between training on 1000+ rubrics versus 12 coarse-grained options and generalization capability requires further validation.

**Low Confidence**: Interpretability claim for Chain-of-Thought Fine-tuning is primarily supported by qualitative examples rather than quantitative measures of interpretability. Paper does not establish clear metrics for what constitutes "interpretable" feedback.

## Next Checks

1. **Robustness Test**: Evaluate Prometheus performance when reference materials are systematically removed or corrupted to quantify model's dependence on these components and identify failure thresholds

2. **Generalization Stress Test**: Conduct cross-domain evaluation by testing Prometheus on evaluation rubrics from domains not represented in Feedback Collection training data (e.g., medical, legal, or technical domains) to assess true generalization capability

3. **Human Alignment Validation**: Expand human evaluation to include diverse demographic groups and expertise levels, and compare Prometheus outputs against multiple independent human annotators rather than relying on GPT-4-generated feedback as ground truth