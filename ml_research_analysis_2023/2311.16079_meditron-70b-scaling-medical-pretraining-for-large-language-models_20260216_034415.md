---
ver: rpa2
title: 'MEDITRON-70B: Scaling Medical Pretraining for Large Language Models'
arxiv_id: '2311.16079'
source_url: https://arxiv.org/abs/2311.16079
tags:
- medical
- training
- medi
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEDI TRON is an open-source medical LLM suite with 7B and 70B parameters,
  adapted from Llama-2 through continued pretraining on high-quality medical corpora.
  It incorporates PubMed papers, abstracts, and clinical guidelines to improve medical
  reasoning.
---

# MEDITRON-70B: Scaling Medical Pretraining for Large Language Models

## Quick Facts
- arXiv ID: 2311.16079
- Source URL: https://arxiv.org/abs/2311.16079
- Reference count: 40
- Primary result: MEDI TRON-70B achieves state-of-the-art performance among open-source medical LLMs, outperforming GPT-3.5 and Med-PaLM while approaching GPT-4 and Med-PaLM-2 performance.

## Executive Summary
MEDI TRON is an open-source medical LLM suite with 7B and 70B parameters, adapted from Llama-2 through continued pretraining on high-quality medical corpora. It incorporates PubMed papers, abstracts, and clinical guidelines to improve medical reasoning. Evaluations on four benchmarks show MEDI TRON outperforms baselines at both scales, achieving state-of-the-art performance among open-source models and competitive results versus commercial LLMs. The work emphasizes careful data curation, distributed training, and robust evaluation for domain-specific adaptation.

## Method Summary
MEDI TRON is developed through continued pretraining of Llama-2 on curated medical corpora using Megatron-LLM for distributed training, followed by supervised finetuning and inference using chain-of-thought and self-consistency methods. The approach leverages medical data from PubMed papers, clinical guidelines, and abstracts to enhance domain-specific reasoning capabilities, with training conducted on 128 A100 GPUs for the 70B parameter model.

## Key Results
- MEDI TRON-70B achieves state-of-the-art performance among open-source medical LLMs
- 6% absolute performance gain over best public baseline in 7B parameter class
- MEDI TRON-7B with in-context learning outperforms other pretrained baselines with 20% increase over base model on PubMedQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling up model parameters to 70B enables emergent medical reasoning capabilities not present in smaller models.
- Mechanism: Larger models have more parameters to encode complex medical knowledge patterns and relationships from pretraining data, enabling better generalization on medical benchmarks.
- Core assumption: Parameter count directly correlates with model's ability to capture and reason about medical knowledge.
- Evidence anchors:
  - [abstract] "MEDI TRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2"
  - [section] "At the 70B scale, the base model Llama-2-70B and MEDI TRON-70B's performances increase significantly compared to the 7B models"
- Break condition: If additional scaling beyond 70B shows diminishing returns on medical reasoning tasks

### Mechanism 2
- Claim: Continued pretraining on curated medical corpora improves domain-specific performance beyond base Llama-2 capabilities.
- Mechanism: Specialized medical data exposure during pretraining allows the model to learn medical terminology, reasoning patterns, and evidence-based knowledge that general pretraining misses.
- Core assumption: Medical corpora contains domain-specific patterns that general language pretraining cannot capture adequately.
- Evidence anchors:
  - [abstract] "MEDI TRON achieves a 6% absolute performance gain over the best public baseline in its parameter class"
  - [section] "MEDI TRON-7B with in-context learning outperforms other pretrained baselines... MEDI TRON-7B shows much higher performance on PubMedQA than the base model (20% increase)"
- Break condition: If continued pretraining on medical data shows no improvement over general pretraining

### Mechanism 3
- Claim: Chain-of-thought and self-consistency inference methods significantly improve medical reasoning performance.
- Mechanism: By generating intermediate reasoning steps and selecting answers through majority voting, the model can better navigate complex medical decision-making.
- Core assumption: Medical questions often require multi-step reasoning that benefits from explicit intermediate steps.
- Evidence anchors:
  - [abstract] "finetuning MEDI TRON-70B to support advanced prompting strategies such as chain-of-thought and self-consistency further improves over the best baseline by 3% and the best public baseline by 12%"
- Break condition: If SC-CoT shows no improvement on simpler medical questions

## Foundational Learning

- Concept: Distributed training with model parallelism
  - Why needed here: Training 70B parameter models requires splitting computation across multiple GPUs
  - Quick check question: What's the difference between data parallelism and tensor parallelism in distributed training?

- Concept: Domain-specific pretraining data curation
  - Why needed here: Medical language requires specialized vocabulary and reasoning patterns not present in general corpora
  - Quick check question: How does the quality vs quantity tradeoff affect pretraining performance in specialized domains?

- Concept: Chain-of-thought reasoning
  - Why needed here: Medical decision-making often requires multi-step logical reasoning that single-step generation struggles with
  - Quick check question: What's the difference between chain-of-thought prompting and self-consistency inference?

## Architecture Onboarding

- Component map: Megatron-LLM distributed training library -> Llama-2 architecture with modifications -> Custom medical corpus preprocessor -> Inference pipeline with CoT and self-consistency

- Critical path:
  1. Download and preprocess medical corpus
  2. Configure distributed training with proper parallelism settings
  3. Run continued pretraining with cosine learning rate schedule
  4. Fine-tune on downstream medical benchmarks
  5. Apply CoT/self-consistency inference for evaluation

- Design tradeoffs:
  - Model size vs. inference speed (70B vs 7B)
  - Medical data diversity vs. domain specificity
  - Training stability vs. learning rate aggressiveness
  - Inference accuracy vs. computational cost (CoT vs direct generation)

- Failure signatures:
  - Training loss plateaus early → check data quality or learning rate
  - Memory errors → adjust parallelism configuration
  - Poor downstream performance → verify data preprocessing or fine-tuning setup
  - Inconsistent inference results → check temperature and sampling settings

- First 3 experiments:
  1. Verify distributed training works by running small-scale pretraining on subset of data
  2. Test data preprocessing pipeline on sample PubMed articles
  3. Validate model can load and generate text after fine-tuning on single benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of including clinical guidelines from diverse geographic and resource settings on model performance?
- Basis in paper: Explicit
- Why unresolved: The paper mentions including guidelines from various settings but does not perform a detailed analysis of how this diversity affects performance.
- What evidence would resolve it: Conducting ablation studies comparing model performance trained on guidelines from different geographic/resource settings versus a more homogeneous set.

### Open Question 2
- Question: How does the performance of MEDI TRON compare to human medical experts on the same benchmarks?
- Basis in paper: Explicit
- Why unresolved: While the paper compares MEDI TRON to other LLMs, it does not provide a direct comparison to human performance on the medical benchmarks.
- What evidence would resolve it: Evaluating both MEDI TRON and human medical experts on the same set of benchmark questions and comparing their accuracy.

### Open Question 3
- Question: What is the effect of scaling the model size beyond 70B parameters on medical reasoning performance?
- Basis in paper: Inferred
- Why unresolved: The paper only evaluates models up to 70B parameters, leaving the potential benefits of larger models unexplored.
- What evidence would resolve it: Training and evaluating models with 100B+ parameters on the same medical benchmarks to determine if performance continues to improve with scale.

## Limitations

- The 70B model requires extensive computational resources (128 A100 GPUs) for pretraining, limiting accessibility for many research groups.
- Evaluation focuses primarily on multiple-choice question answering benchmarks, which may not fully capture clinical utility or safety-critical aspects of medical reasoning.
- The study does not address potential biases in the medical training data or evaluate model performance on underrepresented patient populations.

## Confidence

- **High Confidence**: Technical implementation of distributed training and core methodology for continued pretraining on medical corpora are well-established and reproducible.
- **Medium Confidence**: Claims about emergent reasoning capabilities at 70B scale are supported by benchmark results but lack ablation studies showing which specific capabilities emerge.
- **Low Confidence**: Claims about clinical utility and real-world applicability remain speculative, as evaluation is limited to multiple-choice questions rather than interactive clinical scenarios.

## Next Checks

1. Conduct ablation studies comparing performance at different parameter scales (7B vs 13B vs 70B) to isolate the contribution of model size versus continued pretraining on medical data.

2. Evaluate the model on clinical decision support tasks using real patient cases with known outcomes, rather than multiple-choice questions, to assess practical medical reasoning capabilities.

3. Perform bias and fairness analysis across different demographic groups using medical datasets with demographic metadata to identify potential disparities in model performance.