---
ver: rpa2
title: State-space Models with Layer-wise Nonlinearity are Universal Approximators
  with Exponential Decaying Memory
arxiv_id: '2309.13414'
source_url: https://arxiv.org/abs/2309.13414
tags:
- state-space
- memory
- function
- sequence
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the universality and memory property of state-space
  models. We prove that any continuous sequence-to-sequence functional can be approximated
  by multi-layer state-space models.
---

# State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory

## Quick Facts
- arXiv ID: 2309.13414
- Source URL: https://arxiv.org/abs/2309.13414
- Reference count: 35
- Primary result: Multi-layer state-space models with layer-wise nonlinearity can approximate any continuous sequence-to-sequence function and exhibit exponential decaying memory

## Executive Summary
This paper establishes that state-space models (SSMs) with layer-wise nonlinear activation can serve as universal approximators for continuous sequence-to-sequence functions. Unlike classical recurrent neural networks that require temporal nonlinearity, SSMs achieve universal approximation through stacking multiple layers with nonlinearities applied only across layers. The paper provides two theoretical constructions: one based on the Kolmogorov-Arnold theorem and another using Volterra series expansion. The authors also prove that SSMs inherit the exponential decaying memory property from linear RNNs, which is verified through numerical experiments on randomly initialized models.

## Method Summary
The paper proposes multi-layer state-space models where each layer performs a linear state-space operation followed by a nonlinear activation function applied element-wise. The universal approximation is demonstrated through two approaches: a Kolmogorov-Arnold decomposition that requires exponentially many neurons in sequence length, and a Volterra series expansion that provides sequence-length-independent results. The memory decay analysis examines the derivative of output with respect to time (memory function) and proves exponential decay for stable SSMs. Numerical experiments validate these theoretical findings using synthetic sequences and memory decay measurements.

## Key Results
- SSMs with layer-wise nonlinearity can approximate any continuous sequence-to-sequence functional
- Memory function of SSMs decays exponentially, consistent with classical RNNs
- Two theoretical constructions (Kolmogorov-Arnold and Volterra series) provide different tradeoffs in approximation efficiency

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Nonlinearity Suffices for Universal Approximation
The paper constructs a five-layer SSM that approximates any sequence-to-sequence function by decomposing it into element-wise functions and temporal convolutions. Each component is handled by different layers: two layers approximate element-wise functions using sigmoidal activation, one layer approximates temporal convolution, and the final layer combines these components. This layered decomposition allows the model to capture complex sequence patterns without nonlinear activation along the temporal direction.

### Mechanism 2: Exponential Memory Decay in SSMs
The memory function of SSMs is defined as the derivative of output with respect to time. For stable SSMs (eigenvalues bounded by 1), the memory function decays exponentially regardless of the nonlinearity in the layer-wise activations. This is proven by showing that both the state and output of SSMs converge exponentially to their limits when the input converges exponentially.

### Mechanism 3: Volterra Series Construction
The paper shows that any continuous time-invariant system can be expanded in a Volterra series. Each term in this series represents a higher-order convolution that can be approximated by SSMs using tensor products of first-order convolutions. This approach avoids the sequence-length dependency of the Kolmogorov-Arnold construction.

## Foundational Learning

- **Concept: Universal Approximation Theorem**
  - Why needed here: The paper builds its main result on universal approximation theory, showing that SSMs with layer-wise nonlinearity can approximate any continuous sequence-to-sequence function. Understanding the classical results for RNNs and their limitations is crucial for appreciating why this result matters.
  - Quick check question: Why do classical RNNs require nonlinear activation along the temporal direction, while SSMs only need layer-wise nonlinearity to achieve universal approximation?

- **Concept: Memory Function and Decay**
  - Why needed here: The paper investigates the memory properties of SSMs, showing they have exponential decaying memory like classical RNNs. Understanding how memory functions are defined and analyzed in sequence models is essential for interpreting the theoretical results.
  - Quick check question: How does the memory function definition (Equation 9) generalize from linear functionals to nonlinear sequence-to-sequence functions?

- **Concept: Volterra Series Expansion**
  - Why needed here: The paper presents an alternative construction for SSM approximation using Volterra series, which provides sequence-length-independent results. Understanding this expansion helps explain why SSMs can handle long sequences efficiently.
  - Quick check question: How does the Volterra series represent a nonlinear system, and why does this representation help avoid sequence-length dependencies in the approximation?

## Architecture Onboarding

- **Component map**: Input → First SSM layer (element-wise transformations) → Multiple SSM layers with layer-wise nonlinearities → Final SSM layer (combination and transformation) → Output
- **Critical path**: Input → Element-wise function approximation (2 layers) → Temporal convolution approximation (1 layer) → Combination and final transformation (2 layers) → Output. The critical path must handle both spatial (across layers) and temporal (across time steps) information processing.
- **Design tradeoffs**:
  - Depth vs. width: Deeper networks can use fewer neurons per layer but require careful initialization; wider networks may need fewer layers but scale poorly with sequence length
  - Linear vs. nonlinear recurrence: Linear recurrence enables efficient computation but requires layer-wise nonlinearity for expressive power
  - Stability vs. memory: Stable SSMs (bounded eigenvalues) have exponential memory decay, which may be too fast for some tasks requiring long-term dependencies
- **Failure signatures**: 
  - Poor approximation quality: Indicates insufficient depth or width, or that the layer-wise nonlinearity is not properly tuned
  - Vanishing gradients in deep networks: Suggests initialization issues or that the linear recurrence is too restrictive
  - Memory issues on long sequences: Could indicate that exponential decay is too fast for the task, or that the Volterra series order is insufficient
- **First 3 experiments**:
  1. **Element-wise function approximation**: Test a two-layer SSM on synthetic data where output depends only on current input (y[t] = f(x[t])). Verify it can learn arbitrary continuous functions.
  2. **Temporal convolution approximation**: Test a single-layer SSM on data with known convolution structure (y[t] = sum of weighted past inputs). Compare approximation quality with different hidden dimensions.
  3. **Memory decay verification**: Initialize random SSMs and measure memory decay on synthetic decaying input sequences. Verify exponential decay patterns match theoretical predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal depth and hidden dimension for state-space models in specific sequence modeling tasks?
- Basis in paper: [explicit] The paper discusses the universal approximation property of multi-layer state-space models but does not provide quantitative results on the efficiency of different model architectures.
- Why unresolved: The theoretical results on approximation do not have quantitative results, making it difficult to directly compare the efficiency of different types of models over a specific task.
- What evidence would resolve it: Empirical studies comparing the performance of state-space models with different depths and hidden dimensions on various sequence modeling tasks, along with a theoretical analysis of the approximation rates.

### Open Question 2
- Question: How does the layer-wise nonlinearity in state-space models affect their ability to learn long-term dependencies compared to classical RNNs with recurrent activation functions?
- Basis in paper: [inferred] The paper proves that multi-layer state-space models can approximate any continuous sequence-to-sequence functional, implying that nonlinear recurrent activation is not necessary when there are nonlinear activations across different hidden layers. However, it is unclear how this impacts the learning of long-term dependencies.
- Why unresolved: The paper does not provide a detailed comparison of the memory properties of state-space models and classical RNNs with recurrent activation functions.
- What evidence would resolve it: Empirical studies comparing the performance of state-space models and classical RNNs on tasks requiring long-term memory, along with an analysis of their memory functions.

### Open Question 3
- Question: Can the approximation rate of state-space models be improved by using more efficient kernel representations, such as those used in implicit convolution methods?
- Basis in paper: [explicit] The paper mentions that while the proof for approximation is provided through a polynomial method, polynomials may not be the most efficient means to parameterize kernel functions. It suggests that various techniques have been empirically investigated to represent convolutional layers.
- Why unresolved: The paper does not explore the use of alternative kernel representations in state-space models or provide a comparison of their approximation rates.
- What evidence would resolve it: Empirical studies comparing the performance of state-space models using different kernel representations on various sequence modeling tasks, along with a theoretical analysis of their approximation rates.

## Limitations
- Kolmogorov-Arnoff construction requires exponentially many neurons in sequence length, making it impractical for very long sequences
- Theoretical results assume continuous functions and stable SSMs, which may not always hold in practice
- Limited empirical validation beyond synthetic sequences and memory decay measurements

## Confidence
- **High confidence**: SSMs with layer-wise nonlinearity can approximate any continuous sequence-to-sequence function (theoretical proof provided)
- **Medium confidence**: Memory decay in SSMs is exponential and consistent with linear RNNs (theoretical proof with numerical verification)
- **Medium confidence**: Volterra series construction provides sequence-length-independent approximation (theoretical construction with limited empirical validation)

## Next Checks
1. **Practical efficiency evaluation**: Implement the Volterra series construction and compare its parameter efficiency against the Kolmogorov-Arnoff approach on real-world sequence modeling tasks.

2. **Memory capacity analysis**: Systematically vary SSM stability parameters and measure the actual memory retention on tasks requiring long-term dependencies (e.g., character-level language modeling).

3. **Layer-wise vs. temporal nonlinearity comparison**: Design controlled experiments comparing the approximation power of SSMs with layer-wise nonlinearity against architectures with temporal nonlinearity but fewer layers.