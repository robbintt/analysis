---
ver: rpa2
title: Outlier Robust Adversarial Training
arxiv_id: '2309.05145'
source_url: https://arxiv.org/abs/2309.05145
tags:
- orat
- training
- adversarial
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ORAT, an outlier-robust adversarial training
  method that combines robust learning with adversarial training. ORAT is based on
  a bi-level optimization formulation using a robust rank-based loss function.
---

# Outlier Robust Adversarial Training

## Quick Facts
- arXiv ID: 2309.05145
- Source URL: https://arxiv.org/abs/2309.05145
- Reference count: 40
- Key outcome: ORAT achieves up to 10% higher accuracy on adversarial attacks in the presence of outliers compared to standard and adversarial training methods

## Executive Summary
This paper introduces ORAT (Outlier Robust Adversarial Training), a method that combines robust learning with adversarial training to handle both label noise and adversarial attacks. ORAT uses a bi-level optimization formulation with a robust rank-based loss function that filters outliers by excluding high-loss and low-loss samples. The method is theoretically grounded with H-consistency guarantees, making it a proper surrogate for adversarial 0/1 loss. Experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that ORAT significantly outperforms standard and adversarial training methods in the presence of outliers.

## Method Summary
ORAT addresses the challenge of training robust models when training data contains both outliers and adversarial examples. The method uses a bi-level optimization framework where the inner maximization generates adversarial samples via PGD, and the outer minimization uses a robust rank-based loss that averages only the middle-ranked losses (excluding top-m and bottom-(n-k) losses). This ranking operation automatically filters both high-loss outliers and low-loss easy samples. The authors provide an equivalent reformulation that removes the explicit ranking operation using auxiliary variables, making the optimization more tractable. ORAT is theoretically shown to satisfy H-consistency in binary classification, ensuring that minimizing the surrogate loss leads to minimizing the true adversarial 0/1 loss.

## Key Results
- ORAT achieves up to 10% higher accuracy than standard and adversarial training on adversarial attacks in the presence of outliers
- The method outperforms AT + AoRR (remove outliers then train) by 1-2% on CIFAR-10 and CIFAR-100
- ORAT demonstrates robustness across different attack types (FGSM, PGD, CW, AutoAttack) and noise levels (10-50% contamination)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ORAT's ranked-range loss filters both extreme adversarial perturbations and majority-subgroup samples
- Mechanism: The optimization problem uses a bi-level structure where the inner maximization finds worst-case adversarial samples, then the outer minimization averages only the middle-ranked losses (excluding top-m and bottom-(n-k) losses). This automatically removes both high-loss outliers and low-loss easy samples.
- Core assumption: The ranking operation correctly identifies outliers and easy samples based on loss magnitude.
- Evidence anchors:
  - [abstract]: "ORAT is based on a bi-level optimization formulation of adversarial training with a robust rank-based loss function."
  - [section]: "The overall method can be viewed as a sample selection method and can filter incorrect data samples according to the top-k and top-m individual losses."
  - [corpus]: Weak connection - most corpus papers focus on outlier detection or adversarial robustness separately, not their combination through ranking.

### Mechanism 2
- Claim: ORAT satisfies H-consistency, making it a proper surrogate for adversarial 0/1 loss
- Mechanism: The ORAT loss is shown to be H-calibrated and H-consistent through theoretical analysis, ensuring that minimizing the surrogate loss leads to minimizing the true adversarial 0/1 loss in the limit.
- Core assumption: The loss function is non-negative, continuous, non-increasing, and the hypothesis set is symmetric.
- Evidence anchors:
  - [abstract]: "Theoretically, we show that the learning objective of ORAT satisfies the H-consistency in binary classification, which establishes it as a proper surrogate to adversarial 0/1 loss."
  - [section]: "We show ORAT loss satisfies H-calibration and H-consistency."
  - [corpus]: Weak - corpus papers focus on outlier detection or adversarial training but don't discuss surrogate loss consistency properties.

### Mechanism 3
- Claim: The equivalent reformulation removes the explicit ranking operation while preserving the optimization objective
- Mechanism: Theorem 1 shows that the ranking operation can be replaced with auxiliary variables λ and ˆλ, transforming the non-smooth optimization into a form amenable to gradient descent methods.
- Core assumption: The equivalence holds when optimal solutions satisfy ˆλ > λ.
- Evidence anchors:
  - [section]: "Although ORAT is designed by combining AoRR and AT, this combination has not been explored in the existing literature."
  - [section]: "Theorem 1 Denote [a]+ = max{0, a} as the hinge function. Eq.(3) is equivalent to..."
  - [corpus]: Weak - no direct corpus evidence for this specific reformulation technique.

## Foundational Learning

- Concept: Bi-level optimization
  - Why needed here: ORAT solves a nested optimization where the inner problem finds adversarial samples and the outer problem optimizes parameters based on ranked losses.
  - Quick check question: In ORAT, what are the two levels of optimization and what does each solve?

- Concept: H-consistency and H-calibration
  - Why needed here: These theoretical properties ensure that optimizing the ORAT loss leads to good performance on the true adversarial 0/1 loss.
  - Quick check question: What is the difference between H-calibration and H-consistency in the context of surrogate losses?

- Concept: Rademacher complexity and generalization bounds
  - Why needed here: The paper provides uniform convergence rates for ORAT using Rademacher complexity to bound the generalization error.
  - Quick check question: How does Rademacher complexity relate to the generalization ability of ORAT?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Network architecture -> Training loop -> Evaluation
  Normalize pixel values -> LeNet/Small-CNN/ResNet-18 -> Generate adversarial examples -> Compute losses -> Apply ORAT ranking -> Update parameters -> Evaluate on clean and adversarial test sets

- Critical path:
  1. Initialize network and ORAT hyperparameters (k, m, ϵ)
  2. For each epoch and batch:
     a. Generate adversarial examples using PGD
     b. Compute individual losses for all samples
     c. Find top-m and bottom-(n-k) losses
     d. Average the remaining (k-m) losses
     e. Update network parameters using this averaged loss
  3. Evaluate on clean and adversarial test sets

- Design tradeoffs:
  - Larger k-m values include more samples but may include more outliers
  - Smaller k-m values are more robust but may discard useful information
  - Larger ϵ values create stronger attacks but may push samples beyond meaningful regions

- Failure signatures:
  - Training loss plateaus early: May indicate k and m are too restrictive
  - Test accuracy drops significantly on clean data: May indicate over-aggressive outlier filtering
  - No improvement over standard AT: May indicate k and m settings don't match the outlier distribution

- First 3 experiments:
  1. Run ORAT with k=n, m=0 (reduces to standard AT) to verify baseline performance
  2. Run ORAT with small symmetric noise (10%) and various k, m combinations to find optimal settings
  3. Compare ORAT with AT + AoRR preprocessing (remove outliers then train) to validate end-to-end advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for automatically determining the hyperparameters k and m during training?
- Basis in paper: Explicit - The authors mention in the conclusion that "we plan to design an efficient method to automatically determine the hyperparameters k and m during the training."
- Why unresolved: The paper only discusses manually selecting k and m through grid search, which is computationally expensive and not scalable to larger datasets or more complex models.
- What evidence would resolve it: An adaptive algorithm that dynamically adjusts k and m based on training progress or dataset characteristics, with empirical validation showing improved performance or efficiency compared to manual selection.

### Open Question 2
- Question: How does ORAT perform on extremely large-scale datasets and very deep neural networks?
- Basis in paper: Explicit - The authors state in the conclusion "We will also evaluate our method on large datasets and large deep neural networks."
- Why unresolved: The experiments were conducted on relatively small datasets (MNIST, CIFAR-10, CIFAR-100) and moderately sized networks (LeNet, Small-CNN, ResNet-18).
- What evidence would resolve it: Comprehensive experiments on ImageNet-scale datasets and state-of-the-art architectures like Vision Transformers or very deep ResNets, demonstrating scalability and maintaining the performance advantages observed in smaller-scale experiments.

### Open Question 3
- Question: How does ORAT perform under distribution shift between training and test data?
- Basis in paper: Explicit - The authors mention "how to relax this assumption to a more realistic assumption, for example, n − m i.i.d. inliers with m outliers" in the discussion of generalization error.
- Why unresolved: The theoretical analysis assumes i.i.d. samples, which may not hold in real-world scenarios where training and test distributions differ.
- What evidence would resolve it: Theoretical extensions to non-i.i.d. settings and empirical validation on datasets with known distribution shifts (e.g., domain adaptation benchmarks) showing robustness to such shifts compared to standard adversarial training.

## Limitations
- Hyperparameter Sensitivity: ORAT performance heavily depends on the choice of k and m parameters, requiring extensive grid search for optimal settings
- Binary Classification Restriction: Theoretical framework is developed for binary classification but experiments only validate on binary tasks
- Computational Overhead: Bi-level optimization with ranking operations adds significant computational cost compared to standard adversarial training

## Confidence
- High Confidence: Theoretical properties (H-consistency, H-calibration) are well-established with sound proofs
- Medium Confidence: The mechanism by which ranked-range loss filters outliers is plausible but relies on loss magnitude correlating with outlier status
- Low Confidence: Practical effectiveness in real-world scenarios with complex, non-uniform outlier distributions remains unverified beyond synthetic noise

## Next Checks
1. Test ORAT on standard multi-class classification tasks (full MNIST, CIFAR-10, CIFAR-100) to verify theoretical claims extend beyond binary settings
2. Evaluate ORAT's performance when outliers are generated from non-uniform distributions (e.g., clustered label noise, semantic outliers) rather than uniform random noise
3. Conduct runtime comparisons between ORAT and standard adversarial training methods across different dataset sizes to quantify the practical overhead and identify optimization opportunities