---
ver: rpa2
title: Disposable Transfer Learning for Selective Source Task Unlearning
arxiv_id: '2308.09971'
source_url: https://arxiv.org/abs/2308.09971
tags:
- loss
- accuracy
- learning
- target
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of protecting pre-trained model weights
  from unauthorized piggyback learning by proposing disposable transfer learning (DTL),
  which selectively unlearns source task knowledge after transfer learning. The core
  method introduces Gradient Collision (GC) loss, which minimizes inner products between
  sample gradients to prevent knowledge leakage.
---

# Disposable Transfer Learning for Selective Source Task Unlearning

## Quick Facts
- **arXiv ID**: 2308.09971
- **Source URL**: https://arxiv.org/abs/2308.09971
- **Reference count**: 40
- **Primary result**: GC loss significantly outperforms baseline unlearning methods in retaining target task performance while reducing piggyback learning accuracy

## Executive Summary
This paper addresses the issue of protecting pre-trained model weights from unauthorized piggyback learning by proposing disposable transfer learning (DTL). The core method introduces Gradient Collision (GC) loss, which minimizes inner products between sample gradients to prevent knowledge leakage. GC loss is efficiently implemented using Hessian-vector products and supports distributed training. The approach is evaluated using piggyback learning (PL) accuracy, measuring a model's susceptibility to unauthorized fine-tuning on new tasks. Experiments show that GC loss significantly outperforms baseline unlearning methods in retaining target task performance while reducing PL accuracy (e.g., 24.53% lower than transfer learning baselines).

## Method Summary
The method introduces Gradient Collision (GC) loss to selectively unlearn source task knowledge while preserving target task performance. GC loss minimizes the inner product between gradients of source data samples, effectively leading gradient vectors in opposing directions. The knowledge disposal stage combines GC loss with knowledge distillation (KD) loss to retain target task knowledge. The approach uses Hessian-vector products for efficient computation and supports distributed training. Evaluation uses piggyback learning accuracy as a metric for measuring vulnerability to unauthorized fine-tuning.

## Key Results
- GC loss achieves 24.53% lower PL accuracy compared to transfer learning baselines
- Models retain target task performance while effectively preventing piggyback learning
- The method demonstrates robustness to membership inference attacks
- GC loss outperforms baseline unlearning methods across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gradient Collision (GC) loss selectively unlearns source task knowledge while preserving target task performance by minimizing inner products between gradients of source data samples.
- **Mechanism**: GC loss guides gradients of source data examples to point in opposing directions, preventing the model from learning the source task. This is achieved by minimizing the inner product of gradient vectors across mini-batch chunks, which disrupts the model's ability to retain source task knowledge.
- **Core assumption**: The model's ability to learn the source task is directly tied to the alignment of gradient vectors during training.
- **Evidence anchors**:
  - [abstract]: "GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions."
  - [section 3.2.2]: "GC loss guides the model towards abnormal convergence on the source task by minimizing inner-products between sample gradients."
  - [corpus]: No direct corpus evidence, but the mechanism aligns with known gradient-based learning methods (e.g., gradient episodic memory for continual learning).

### Mechanism 2
- **Claim**: Knowledge distillation (KD) loss retains target task performance by transferring knowledge from the pre-trained model to the unlearned model.
- **Mechanism**: KD loss minimizes the KL divergence between the soft target outputs of the pre-trained model and the unlearned model, ensuring that the target task knowledge is preserved during unlearning.
- **Core assumption**: The pre-trained model contains valuable knowledge about the target task that can be transferred to the unlearned model.
- **Evidence anchors**:
  - [section 3.2.1]: "KD loss is for transferring knowledge between different models by setting the output of the teacher model to a soft target and minimizing the KL divergence of the soft target and the output of a student model."
  - [section 4.6]: "KD-based knowledge retaining losses (SRC-KD, TGT-KD) outperform the non-KD methods."
  - [corpus]: No direct corpus evidence, but KD is a well-established technique in transfer learning literature.

### Mechanism 3
- **Claim**: Piggyback Learning (PL) accuracy measures the model's susceptibility to unauthorized fine-tuning on new tasks, providing a metric for evaluating the effectiveness of unlearning.
- **Mechanism**: PL accuracy is defined as the test accuracy of the unlearned model when fine-tuned on a piggyback task. A lower PL accuracy indicates that the model has successfully unlearned the source task and is less susceptible to piggyback learning.
- **Core assumption**: The ability of a model to perform well on a piggyback task is directly related to its retention of source task knowledge.
- **Evidence anchors**:
  - [abstract]: "PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data."
  - [section 3.3]: "We propose to benchmark DTL using PL accuracy."
  - [section 4.3]: "Our experiments show that the proposed method (GC model) is the most effective in preventing piggyback learning."

## Foundational Learning

- **Concept**: Gradient-based optimization and its role in deep learning.
  - Why needed here: Understanding how gradients drive model updates is crucial for grasping how GC loss manipulates gradients to unlearn source task knowledge.
  - Quick check question: How does the direction and magnitude of gradients affect the model's learning process?

- **Concept**: Knowledge distillation and its application in transfer learning.
  - Why needed here: KD loss is used to retain target task performance during unlearning, so understanding its mechanism is essential.
  - Quick check question: How does knowledge distillation transfer knowledge from a pre-trained model to a new model?

- **Concept**: Membership inference attacks (MIAs) and their relevance to model privacy.
  - Why needed here: Evaluating the robustness of unlearned models to MIAs is part of assessing the effectiveness of the unlearning process.
  - Quick check question: How do MIAs exploit model outputs to infer whether a sample was part of the training data?

## Architecture Onboarding

- **Component map**: Pre-trained model -> Fine-tuned model -> Unlearned model (with GC and KD losses) -> Evaluation (target accuracy, PL accuracy, MIA success rate)
- **Critical path**: 
  1. Pre-train the model on source task data
  2. Fine-tune the model on target task data
  3. Apply GC loss to selectively unlearn source task knowledge
  4. Evaluate the unlearned model using target accuracy, PL accuracy, and MIA success rate
- **Design tradeoffs**: Balancing the weight of KD loss and GC loss to optimize both target performance retention and source knowledge disposal; choosing the appropriate chunk size for computing GC loss to balance computational efficiency and effectiveness
- **Failure signatures**: High PL accuracy indicates insufficient unlearning of source task knowledge; low target accuracy suggests that the unlearning process has inadvertently degraded target task performance
- **First 3 experiments**:
  1. Compare the target accuracy and PL accuracy of models unlearned using different unlearning losses (e.g., GC loss, random target fooling loss, uniform target fooling loss)
  2. Evaluate the robustness of unlearned models to membership inference attacks using various attack strategies
  3. Investigate the impact of varying the weight of KD loss and GC loss on the trade-off between target performance retention and source knowledge disposal

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the Gradient Collision (GC) loss perform when applied to larger-scale datasets and more complex model architectures beyond CIFAR-100, STL-10, SVHN, and TinyImageNet?
- **Basis in paper**: [inferred] The paper notes that the studies were confined to relatively small-to-medium-scale datasets and model architectures, and encourages subsequent studies to investigate DTL within larger, more realistic contexts.
- **Why unresolved**: The current experimental scope is limited to small-to-medium datasets, and the scalability of GC loss to larger datasets and more complex models is not addressed.
- **What evidence would resolve it**: Experiments applying GC loss to larger datasets (e.g., ImageNet) and more complex models (e.g., ResNet-50, Vision Transformers) with performance comparisons to baseline methods.

### Open Question 2
- **Question**: What is the impact of different hyperparameter values for λ (the scalar controlling the level of unlearning) on the trade-off between target task performance and PL accuracy in the GC loss method?
- **Basis in paper**: [explicit] The paper discusses the trade-off between model performance on the target accuracy and PL accuracy across varying values of λ and characterizes the models.
- **Why unresolved**: While the paper provides insights into the behavior of different λ values, a comprehensive sensitivity analysis across a wider range of values and tasks is needed.
- **What evidence would resolve it**: A detailed sensitivity analysis showing the impact of different λ values on target accuracy, PL accuracy, and source accuracy across multiple tasks and datasets.

### Open Question 3
- **Question**: How does the Normalized Gradient Collision (NGC) loss compare to the GC loss in terms of effectiveness and computational efficiency for knowledge disposal in DTL?
- **Basis in paper**: [explicit] The paper conducts an extra investigation on NGC loss, focusing on the angle between gradients and ignoring the scale, and compares it to GC loss.
- **Why unresolved**: The paper notes that regularizing the gradient norm as well as minimizing the variance (which corresponds to the GC model) results in better performance, but a direct comparison of NGC and GC losses in terms of effectiveness and efficiency is not provided.
- **What evidence would resolve it**: Experiments comparing the performance of NGC and GC losses on target accuracy, PL accuracy, and computational efficiency across multiple tasks and datasets.

## Limitations

- The experiments are limited to image classification tasks on small-scale datasets, with effectiveness on larger-scale vision tasks and non-vision domains untested.
- The method requires careful tuning of λ values for KD loss and GC loss, with no systematic sensitivity analysis or practical guidelines for hyperparameter selection.
- The paper only evaluates one membership inference attack strategy and does not comprehensively assess different attack types or adaptive adversaries.

## Confidence

- **High Confidence**: The core mechanism of GC loss minimizing inner products between gradients is technically sound and the experimental results showing improved PL accuracy and maintained target performance are reliable within the evaluated scope.
- **Medium Confidence**: The claim that GC loss is more effective than baseline unlearning methods is supported by experiments, but the comparison could be strengthened with additional baselines and cross-domain validation.
- **Low Confidence**: The robustness claims against membership inference attacks need further validation, as the paper only evaluates one attack strategy (Yeom et al.) and does not comprehensively assess different attack types or adaptive adversaries.

## Next Checks

1. **Cross-Domain Generalization**: Evaluate DTL on non-vision tasks (NLP, speech) and larger-scale datasets to assess whether the gradient collision mechanism generalizes beyond the current experimental scope.
2. **Adaptive Attack Resistance**: Test the robustness of unlearned models against adaptive membership inference attacks that specifically target models with gradient collision-based unlearning, to verify the claimed privacy benefits.
3. **Hyperparameter Transferability**: Conduct systematic experiments to determine whether λ values can be transferred across different task pairs or whether they require task-specific tuning, and provide practical guidelines for hyperparameter selection.