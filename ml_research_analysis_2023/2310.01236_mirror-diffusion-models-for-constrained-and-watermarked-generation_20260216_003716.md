---
ver: rpa2
title: Mirror Diffusion Models for Constrained and Watermarked Generation
arxiv_id: '2310.01236'
source_url: https://arxiv.org/abs/2310.01236
tags:
- diffusion
- constrained
- arxiv
- mirror
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Mirror Diffusion Models (MDM) enable tractable diffusion models\
  \ for convex constrained sets by learning diffusions in a dual space via mirror\
  \ maps, avoiding boundary reflections. This approach achieves simulation-free training\
  \ and tractable conditional scores for constrained generation on sets like \u2113\
  2-balls, simplices, and polytopes."
---

# Mirror Diffusion Models for Constrained and Watermarked Generation

## Quick Facts
- arXiv ID: 2310.01236
- Source URL: https://arxiv.org/abs/2310.01236
- Reference count: 40
- Primary result: MDM achieves simulation-free training and tractable conditional scores for constrained generation on sets like ℓ2-balls, simplices, and polytopes while matching unconstrained diffusion models like DDPM.

## Executive Summary
Mirror Diffusion Models (MDM) address the challenge of learning diffusion processes within convex constrained sets without boundary violations. By leveraging mirror maps to transform the constrained generation problem into an unconstrained dual-space diffusion process, MDM avoids the computational complexity of boundary reflections while maintaining theoretical guarantees. The approach demonstrates strong empirical performance on synthetic datasets and competitive watermarked image generation results on FFHQ and AFHQv2.

## Method Summary
MDM transforms constrained generation problems by mapping data from convex constraint sets to an unconstrained dual space via mirror maps. This allows standard diffusion models to operate without boundary constraints, with the inverse mirror map ensuring samples remain within the original constraint set. The method supports various constraint types including ℓ2-balls, simplices, and polytopes through appropriate mirror map constructions (log-barrier, entropic, and hyperbolic tangent functions). For watermarking applications, constrained polytopes serve as private tokens that are embedded during generation.

## Key Results
- MDM outperforms reflected diffusion methods on synthetic datasets while maintaining zero constraint violation rates
- Matches unconstrained diffusion model performance (DDPM) without violating constraints
- Achieves competitive FID scores on watermarked image generation tasks for FFHQ and AFHQv2 datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mirror Diffusion Models (MDM) avoid boundary reflections by transforming constrained generation problems into unconstrained dual-space diffusion processes via mirror maps.
- **Mechanism**: The mirror map ∇ϕ lifts data from the constrained set M ⊆ Rd to an unconstrained dual space ∇ϕ(M) = Rd, where standard diffusion models can operate without boundary constraints. The inverse mirror map ∇ϕ* maps generated samples back to M, ensuring constraint satisfaction by construction.
- **Core assumption**: The mirror map is bijective and differentiable, and its inverse can be efficiently computed for common constraint sets.
- **Evidence anchors**:
  - [abstract]: "This is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard Euclidean space."
  - [section]: "MDM utilizes mirror maps ∇ϕ that transform data distributions on M to its dual space ∇ϕ(M)."
  - [corpus]: Weak evidence - no directly relevant corpus papers found for this specific mechanism.
- **Break condition**: The mirror map fails to be bijective or its inverse becomes computationally intractable for complex constraint sets.

### Mechanism 2
- **Claim**: MDM achieves tractable conditional scores and simulation-free training by leveraging the analytical properties of diffusion processes in the dual space.
- **Mechanism**: Since the dual space is unconstrained Euclidean space, MDM inherits the tractability of standard diffusion models. The mirror map transformation allows for efficient computation of conditional scores without requiring explicit boundary reflection simulations.
- **Core assumption**: The mirror map transformation preserves the necessary properties for tractable diffusion model training and inference.
- **Evidence anchors**:
  - [section]: The paper derives efficient computations of mirror maps for common constrained sets.
  - [corpus]: Weak evidence - limited corpus papers discussing this specific tractability claim.
- **Break condition**: The mirror map transformation introduces computational overhead that negates the tractability advantages, or the conditional score computations become intractable for certain constraint sets.

## Foundational Learning

**Mirror Maps**: Transformations that map points from a constraint set to an unconstrained dual space and vice versa.
*Why needed*: Enables standard diffusion models to operate in unconstrained space while ensuring generated samples satisfy original constraints.
*Quick check*: Verify that the mirror map and its inverse are differentiable and computationally efficient for the target constraint set.

**Reflected Diffusion Processes**: Standard diffusion models that explicitly handle boundary reflections when sampling from constrained sets.
*Why needed*: Provides baseline comparison for MDM's performance and constraint satisfaction.
*Quick check*: Compare constraint violation rates between reflected diffusion and MDM on simple constraint sets.

**Wasserstein Distances**: Metrics measuring the distance between probability distributions based on optimal transport theory.
*Why needed*: Quantifies the quality of generated samples by measuring distributional similarity to the true data distribution.
*Quick check*: Ensure implementation correctly computes Wasserstein distances for synthetic datasets.

## Architecture Onboarding

**Component Map**: Data distribution on constraint set M -> Mirror map ∇ϕ -> Dual space distribution -> Diffusion model training -> Inverse mirror map ∇ϕ* -> Generated samples on M

**Critical Path**: Mirror map computation → Diffusion model training in dual space → Inverse mirror map application → Constraint satisfaction verification

**Design Tradeoffs**: MDM trades computational complexity of boundary reflections for mirror map computations; performance depends on choice of mirror map function and its properties.

**Failure Signatures**: 
- Constraint violations indicate mirror map implementation errors
- Poor sample quality suggests incorrect dual-space diffusion training
- Numerical instability in mirror map computation

**First Experiments**:
1. Implement and test mirror maps for ℓ2-balls with synthetic Gaussian mixture data
2. Compare constraint violation rates between MDM and reflected diffusion on simple polytopes
3. Evaluate Wasserstein distance performance for MDM vs. DDPM on simplex-constrained data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of mirror map affect the quality of generated samples in MDM, particularly for complex constrained sets like polytopes?
- Basis in paper: [explicit] The paper derives efficient computations of mirror maps for common constrained sets like ℓ2-balls and simplices, and proposes a modification for polytopes using hyperbolic tangents to address numerical instability.
- Why unresolved: The paper does not explore the impact of different mirror map choices on the quality of generated samples, especially for more complex constrained sets like polytopes.
- What evidence would resolve it: Empirical studies comparing MDM performance with different mirror maps on various constrained sets, including complex polytopes, would help determine the impact of mirror map choice on sample quality.

### Open Question 2
- Question: Can MDM be extended to handle non-convex constrained sets, and if so, what modifications would be necessary?
- Basis in paper: [inferred] The paper focuses on convex constrained sets and mentions that MDM concerns mainly convex constrained sets, which limits its application to general constraints.
- Why unresolved: The paper does not address the possibility of extending MDM to non-convex constrained sets or the necessary modifications for such an extension.
- What evidence would resolve it: Theoretical analysis and empirical experiments demonstrating the feasibility and performance of MDM on non-convex constrained sets would help determine if and how MDM can be extended to handle such sets.

### Open Question 3
- Question: How does the watermarking capability of MDM compare to other watermarking techniques in terms of robustness against removal attempts and detection accuracy?
- Basis in paper: [explicit] The paper presents MDM as a novel approach for watermarking and compares its performance to prior watermarking techniques for diffusion models in terms of FID scores.
- Why unresolved: The paper does not evaluate the robustness of MDM's watermarking against removal attempts or its detection accuracy compared to other watermarking techniques.
- What evidence would resolve it: Comparative studies assessing the robustness of MDM's watermarking against various removal techniques and its detection accuracy relative to other watermarking methods would help determine its effectiveness in this regard.

## Limitations

- MDM is primarily designed for convex constraint sets, limiting its applicability to non-convex constraints
- Computational overhead from mirror map operations may negate simulation-free advantages for complex constraint sets
- Limited evaluation on high-resolution image datasets (64×64 resolution) restricts understanding of scalability

## Confidence

**Mechanism 1 Confidence: Medium**
- Theoretical foundation is well-established in optimization literature
- Limited empirical validation for complex constraint sets
- Computational efficiency claims require further verification

**Mechanism 2 Confidence: Low-Medium**
- Tractability claims depend heavily on specific mirror map choices
- Limited exploration of performance across different constraint types
- Computational overhead characterization is incomplete

**Empirical Results Confidence: Medium**
- Strong results on synthetic datasets
- Limited comparison with alternative approaches
- Small-scale image generation evaluation

## Next Checks

1. **Scalability Test**: Evaluate MDM on higher-resolution images (e.g., 256×256) and more complex constraint sets to verify if the computational advantages scale with problem complexity.

2. **Robustness Analysis**: Test MDM's performance under different noise schedules and training durations to quantify its sensitivity to hyperparameters compared to standard diffusion models.

3. **Constraint Generalization**: Implement MDM for non-convex constraint sets and measure performance degradation to determine the practical limits of the mirror map approach.