---
ver: rpa2
title: Removing Structured Noise with Diffusion Models
arxiv_id: '2302.05290'
source_url: https://arxiv.org/abs/2302.05290
tags:
- diffusion
- noise
- data
- inverse
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of removing structured noise
  from images in inverse problems, where the noise is highly correlated and not easily
  modeled by standard Gaussian assumptions. The authors propose a novel method using
  diffusion models that jointly learns score models for both the signal and the structured
  noise.
---

# Removing Structured Noise with Diffusion Models

## Quick Facts
- arXiv ID: 2302.05290
- Source URL: https://arxiv.org/abs/2302.05290
- Reference count: 19
- Primary result: Joint diffusion-based posterior sampling for signal and structured noise outperforms competitive methods on various inverse problems

## Executive Summary
This paper addresses the challenge of removing structured noise from images in inverse problems, where noise is highly correlated and not easily modeled by standard Gaussian assumptions. The authors propose a novel method using diffusion models that jointly learns score models for both the signal and the structured noise. This approach allows for conditional sampling of the posterior distribution, enabling effective denoising and reconstruction. The method is compared to state-of-the-art baselines including normalizing flows, GANs, and classical denoising techniques. Results demonstrate strong performance gains in various inverse problems with structured noise, outperforming competitive methods.

## Method Summary
The method proposes a joint conditional reverse diffusion process with learned scores for both signal and noise. It uses two separate NCSNv2 score networks trained independently on signal and noise datasets, then combines them during joint sampling via a conditional reverse-time SDE. The sampling process incorporates data consistency steps that enforce measurement constraints at each iteration. The approach allows flexible priors without requiring invertibility constraints like normalizing flows, and naturally incorporates conditional terms to guide denoising toward measurement consistency.

## Key Results
- Achieves strong performance gains in various inverse problems with structured noise
- Outperforms competitive methods including normalizing flows, GANs, BM3D, and LASSO
- Demonstrates improved robustness to out-of-distribution signals compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Joint posterior sampling of both signal and structured noise enables better inversion than treating noise as fixed or simple. By modeling x and n as a joint posterior, the sampler can trade off between signal fidelity and noise suppression, guided by data consistency steps which enforce y = Ax + n at each iteration. This works when the measurement model is linear and the noise distribution is tractable enough to sample from.

### Mechanism 2
Learning separate score models for data and noise allows flexible priors without requiring invertibility constraints like in normalizing flows. Independent training of sθ(xt, t) and sφ(nt, t) lets each capture its own manifold, with entanglement only happening during joint sampling via the measurement model. This relies on the assumption that data and noise distributions are independent a priori.

### Mechanism 3
The reverse-time SDE formulation naturally incorporates the conditional term ∇xt logp(ŷt|xt, nt) to guide denoising toward consistency with measurements. At each reverse step, the drift term is corrected by both the unconditional score and the gradient of the likelihood of the noisy observation under the current estimate. This requires that the transition kernel q(yt|y) is Gaussian and analytically tractable.

## Foundational Learning

- **Score matching and denoising score matching objective**: Needed to train score models that approximate ∇ log p(x) and ∇ log p(n). Quick check: What is the role of the time-dependent term in the DSM objective?
- **Stochastic differential equations (SDEs) for diffusion processes**: Required to describe forward and reverse diffusion processes and enable numerical sampling. Quick check: How does the drift coefficient f(t) affect the variance of the process over time?
- **Bayesian posterior inference for inverse problems**: Provides the framework for framing inversion as sampling from p(x, n | y) ∝ p(y|x, n)p(x)p(n). Quick check: Why is maximum a posteriori (MAP) insufficient when uncertainty quantification is needed?

## Architecture Onboarding

- **Component map**: Pretrained score models -> Joint conditional sampler -> Measurement model A and observation y -> Output x0
- **Critical path**: Load pretrained score models → Initialize xt and nt from base distribution → For each step: corrupt observation → data consistency → score step (data) → score step (noise) → Return x0 as denoised output
- **Design tradeoffs**: Separate training simplifies pipeline but assumes independence; joint sampling is slower than GAN/Flow but offers posterior samples; fixed λ, μ vs. learned step sizes
- **Failure signatures**: Mode collapse if score models overfit to training data; instability if λ or μ too large (overshooting in data consistency); poor performance if noise distribution is misspecified
- **First 3 experiments**: 1) Denoise synthetic Gaussian noise on MNIST → validate basic sampler 2) Remove MNIST digits from CelebA → reproduce main result 3) Test compressed sensing with sinusoidal noise → validate structured noise handling

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific architectural differences between the diffusion model used in this work and the normalizing flow model that contribute to the 50x speed difference in inference time? The paper mentions that the flow model requires substantially more trainable parameters compared to the diffusion method, mainly due to restrictive requirements imposed on the architecture to ensure tractable likelihood computation.

### Open Question 2
How does the performance of the proposed method change when using different types of structured noise, such as Poisson noise or impulse noise? The paper only tests the proposed method on Gaussian noise and sinusoidal noise, and does not explore other types of structured noise.

### Open Question 3
What is the impact of the choice of the SDE formulation on the performance of the proposed method? The paper uses the variance preserving (VP) SDE to define the diffusion trajectory, but does not explore the impact of using different SDE formulations.

### Open Question 4
How does the performance of the proposed method change when using different types of signal priors, such as sparsity or low-rank priors? The paper uses a learned signal prior based on a diffusion model, but does not explore the impact of using different types of signal priors.

### Open Question 5
What is the impact of the choice of the score-matching objective on the performance of the proposed method? The paper uses the denoising score matching (DSM) objective to train the score networks, but does not explore the impact of using different score-matching objectives.

## Limitations
- Strong assumptions about linear measurement model and tractable noise distribution for sampling
- Independence assumption between signal and noise priors may not hold for all real-world scenarios
- Method may struggle with non-linear forward operators or non-Gaussian noise distributions

## Confidence
- **High confidence**: Core algorithmic framework and theoretical justification (score matching, SDE formulation)
- **Medium confidence**: Empirical performance claims on benchmark datasets
- **Medium confidence**: Claims about robustness to out-of-distribution signals (limited validation presented)

## Next Checks
1. Test the method on non-linear forward operators (e.g., Poisson or compressive sensing) to verify robustness beyond the linear assumption
2. Evaluate performance when the noise distribution is misspecified or non-additive (e.g., multiplicative noise)
3. Conduct ablation studies on the weighting parameters λ and μ to understand their impact on reconstruction quality and convergence