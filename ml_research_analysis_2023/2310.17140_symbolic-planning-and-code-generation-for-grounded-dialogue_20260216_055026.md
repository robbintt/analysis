---
ver: rpa2
title: Symbolic Planning and Code Generation for Grounded Dialogue
arxiv_id: '2310.17140'
source_url: https://arxiv.org/abs/2310.17140
tags:
- code
- dialogue
- human
- symbolic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Symbolic Planning and Code-generation (SPC),
  a grounded dialogue system that combines large language models with symbolic planning
  and grounded code execution. The key idea is to interpret partner utterances into
  executable Python code, using code as a compositional knowledge representation.
---

# Symbolic Planning and Code Generation for Grounded Dialogue

## Quick Facts
- arXiv ID: 2310.17140
- Source URL: https://arxiv.org/abs/2310.17140
- Reference count: 11
- One-line primary result: Improves task success from 56% to 69% on OneCommon dialogue task using code generation and symbolic planning

## Executive Summary
This paper introduces Symbolic Planning and Code-generation (SPC), a grounded dialogue system that combines large language models with symbolic planning and grounded code execution. The system interprets partner utterances into executable Python code, executes this code to update a belief state over possible worlds, and uses a symbolic planner to optimize information gain when choosing the next action. Evaluated on the challenging OneCommon dialogue task involving collaborative reference resolution on abstract images, SPC substantially outperforms previous state-of-the-art, improving task success from 56% to 69% in human evaluations while being comparable to human-human pairs on average.

## Method Summary
SPC consists of two main components: a reader and a planner. The reader uses GPT-4 to translate partner utterances into executable Python code through a four-step process: dialogue act classification, reference prediction, constraint generation, and composition. This code is grounded through a manually defined perceptual library that maps to specific visual elements. The planner maintains a belief distribution over possible worlds and updates it based on interpreted utterances, then optimizes an information gain objective to select the next action. The system also employs a decomposed code generation approach that breaks the generation into smaller steps to improve speed without sacrificing accuracy.

## Key Results
- Improves task success rate from 56% to 69% compared to previous state-of-the-art on OneCommon task
- Outperforms human-machine pairs by 13% and is comparable to human-human pairs on average
- Achieves significant speed improvements (13.7s to 7.1s) through decomposed code generation while maintaining similar accuracy

## Why This Works (Mechanism)

### Mechanism 1
Code generation serves as a compositional knowledge representation that enables grounded language understanding in dialogue. By translating partner utterances into executable Python code, the system grounds spatial and contextual references to specific dots in the visual context. The generated code calls perceptual library functions that have clear semantic grounding, allowing the system to interpret ambiguous or context-dependent references accurately.

### Mechanism 2
Symbolic planning with information gain optimization enables efficient exploration and exploitation in collaborative reference games. The system maintains a belief distribution over possible worlds and updates it based on interpreted partner utterances. It then plans the next action by optimizing expected information gain, which encourages asking questions that maximize knowledge about the shared dot configuration. This balances exploration (gathering information) with exploitation (selecting the correct dot when confident).

### Mechanism 3
Decomposing code generation into smaller steps improves speed without sacrificing accuracy. Instead of generating the full meaning function in one step, the system breaks down the task into dialogue act classification, reference prediction, constraint generation, and composition. This reduces the output token count for each LLM call, significantly improving response time while maintaining comparable accuracy.

## Foundational Learning

- **Concept: Bayesian belief updating**
  - Why needed here: The system needs to maintain and update a probability distribution over possible world states based on evidence from partner utterances
  - Quick check question: How does the system update its belief state p(z|u) given a new utterance u and the prior belief p(z)?

- **Concept: Information theory and entropy**
  - Why needed here: The planning objective uses information gain, which is defined as the reduction in entropy about the world state
  - Quick check question: What is the mathematical definition of information gain used in the planning objective?

- **Concept: Few-shot learning with LLMs**
  - Why needed here: The system relies on few-shot prompting to generate code from natural language
  - Quick check question: How many few-shot examples are used in the constraint generation prompt, and how does changing this number affect performance?

## Architecture Onboarding

- **Component map**: Partner utterance → Reader (dialogue act classification → reference prediction → constraint generation → composition) → Planner (belief update → information gain optimization) → Writer → Response generation
- **Critical path**: The most critical sequence is: partner utterance → reader (code generation) → planner (belief update and action selection) → writer (response generation)
- **Design tradeoffs**: The system trades flexibility for interpretability and control. Using symbolic planning and code generation makes the system more interpretable and controllable than end-to-end language models, but requires manual engineering of the perceptual API and symbolic representations
- **Failure signatures**: Common failures include incorrect code generation leading to wrong belief updates, poor information gain calculation causing suboptimal questions, and template generation producing unnatural responses
- **First 3 experiments**:
  1. Test the reader component in isolation by feeding it sample utterances and checking if the generated code correctly interprets the meaning and grounds references to the visual context
  2. Test the planner component by providing it with simulated belief states and checking if it selects optimal questions according to the information gain objective
  3. Perform an end-to-end test with a simulated partner to verify that the full system can successfully complete the OneCommon task

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of few-shot examples affect the accuracy of the code constraint generation prompt? The paper only reports average agreement between 5-shot and 15-shot prompts but doesn't explore the impact of different example choices on accuracy.

### Open Question 2
How does the choice of few-shot examples affect the speed of the code constraint generation prompt? The paper reports accuracy with different numbers of examples but doesn't discuss the impact on speed.

### Open Question 3
How does the choice of few-shot examples affect the generalization of the code constraint generation prompt to unseen contexts? The paper mentions using the same prompts in every context but doesn't analyze how example choice affects generalization.

## Limitations
- The perceptual library requires substantial manual engineering for each new domain, limiting generalization to diverse real-world applications
- The system's reliance on GPT-4 raises questions about cost-effectiveness and whether smaller language models could achieve comparable results
- Evaluation is limited to a single abstract visual task, leaving open questions about performance in more complex, real-world scenarios

## Confidence
- Code generation as compositional knowledge representation: Medium-High
- Symbolic planning with information gain optimization: Medium-High
- Decomposed code generation for speed: High

## Next Checks
1. **Generalization Test**: Evaluate the system on a more diverse set of visual reference tasks with varying complexity, including real images and natural language descriptions, to assess the robustness of the code generation and grounding mechanisms.

2. **Scalability Analysis**: Test the system with smaller, more cost-effective language models (e.g., GPT-3.5, LLaMA) to determine the minimum model size required for maintaining performance, and analyze the impact on response time and cost.

3. **User Experience Evaluation**: Conduct a user study focusing on conversational naturalness and perceived intelligence, measuring metrics beyond task success rate such as engagement, clarity of responses, and subjective preference compared to baseline systems.