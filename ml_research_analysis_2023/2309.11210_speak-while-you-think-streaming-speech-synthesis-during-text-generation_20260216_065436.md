---
ver: rpa2
title: 'Speak While You Think: Streaming Speech Synthesis During Text Generation'
arxiv_id: '2309.11210'
source_url: https://arxiv.org/abs/2309.11210
tags:
- word
- text
- speech
- lookahead
- llm2speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM2Speech, a system that enables streaming
  speech synthesis while text is being generated by a Large Language Model (LLM).
  The key idea is to use LLM hidden embeddings as semantic context to compensate for
  limited future context in streaming, enabling low-latency TTS.
---

# Speak While You Think: Streaming Speech Synthesis During Text Generation

## Quick Facts
- arXiv ID: 2309.11210
- Source URL: https://arxiv.org/abs/2309.11210
- Reference count: 0
- Key outcome: LLM2Speech achieves streaming TTS with MOS of 4.12, matching offline teacher quality while reducing latency

## Executive Summary
This paper introduces LLM2Speech, a system that enables streaming speech synthesis while text is being generated by a Large Language Model (LLM). The key innovation is using LLM hidden embeddings as semantic context to compensate for limited future context in streaming scenarios. LLM2Speech achieves this through a three-component architecture: a frozen LLM, an adaptor (LLM2PnP) that converts LLM outputs to Phones and Prosody using restricted attention, and a streamable TTS system (PnP2Speech) based on Parallel Prosody Transfer. Experimental results demonstrate that LLM2Speech maintains the quality of a non-streaming teacher model while enabling low-latency streaming synthesis.

## Method Summary
LLM2Speech addresses streaming TTS during LLM text generation by exploiting hidden embeddings as semantic context. The method involves training LLM2PnP via offline-to-streaming knowledge distillation on large textual datasets, where it learns to predict phones and prosody using restricted attention with lookahead constraint L. PnP2Speech is modified to operate on small input chunks with lookahead-constrained attention and convolution layers. The system is trained on C4 dataset (3M samples) and evaluated on a 6.5-hour proprietary conversational corpus.

## Key Results
- LLM2Speech achieves MOS of 4.12 ± 0.04, matching teacher model's MOS of 4.10 ± 0.04
- Stream-teacher model with same lookahead achieves lower MOS of 3.46 ± 0.06
- LLM embeddings improve G2P performance but do not significantly impact prosodic quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM hidden embeddings compensate for limited future context in streaming speech synthesis
- Mechanism: The LLM generates text incrementally while producing hidden embeddings. These embeddings capture semantic context that would otherwise require future text tokens. LLM2PnP uses these embeddings as additional input features, allowing the model to make better phonetic and prosodic predictions with limited lookahead.
- Core assumption: LLM embeddings contain sufficient semantic information to compensate for missing future context in streaming scenarios
- Evidence anchors:
  - [abstract] "It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context"
  - [section] "LLM2Speech utilizes LLM embeddings, a by-product of the text generation that contains semantic information and might compensate for the lack of future context in streaming"
  - [corpus] Weak evidence - only 1 related paper mentions semantic context in streaming

### Mechanism 2
- Claim: Restricted attention with fixed lookahead enables streaming without quality degradation
- Mechanism: The LLM2PnP model implements restricted attention where tokens can only attend to future context within a fixed lookahead window L. This prevents the model from accessing tokens beyond L words ahead, enabling true streaming while maintaining quality through knowledge distillation from a full-context teacher model.
- Core assumption: Knowledge distillation from a full-context teacher can train a streaming model to match quality within the lookahead constraint
- Evidence anchors:
  - [section] "To restrict dependence on future context, we formalize restricted attention with a fixed word lookahead L"
  - [section] "LLM2PnP is trained on a large textual dataset via offline-to-streaming knowledge distillation during which it attempts to mimic the predictions of a teacher model that has access to the full text"
  - [corpus] Moderate evidence - 2 related papers discuss streaming with lookahead constraints

### Mechanism 3
- Claim: Chunked processing with lookahead-constrained layers enables low-latency streaming synthesis
- Mechanism: PnP2Speech uses LC-CNN and chunked BLSTM layers with controlled lookahead. This allows the model to process small input chunks while maintaining quality. The lookahead constraint ensures the system doesn't wait for future context, enabling streaming with minimal delay.
- Core assumption: Lookahead-constrained neural layers can maintain synthesis quality while operating on small chunks
- Evidence anchors:
  - [section] "PnP2Speech operates on small input chunks instead of the entire sequence, and requires the changes to [17] described below"
  - [section] "symmetric-kernel convolutions are used as long as the lookahead constraint is met; otherwise, skewed-kernel convolutions are applied, resulting in a constrained lookahead"
  - [corpus] Moderate evidence - 2 related papers discuss chunked processing for streaming

## Foundational Learning

- Concept: Transformer attention mechanisms and lookahead constraints
  - Why needed here: Understanding how to restrict attention to enable streaming while maintaining quality is fundamental to LLM2Speech's design
  - Quick check question: How does the lookahead constraint in Eq. 1 and Eq. 2 ensure streaming capability while preventing access to future context?

- Concept: Knowledge distillation from offline to streaming models
  - Why needed here: The training approach relies on transferring knowledge from a full-context teacher to a streaming student model
  - Quick check question: What are the key differences between offline and streaming models that knowledge distillation must address?

- Concept: Prosodic feature representation and synthesis
  - Why needed here: Understanding Hierarchical Prosodic Controls (HPCs) and their role in expressive speech synthesis is crucial for implementing PnP2Speech
  - Quick check question: How do duration and pitch HPCs contribute to the naturalness of synthesized speech?

## Architecture Onboarding

- Component map: LLM (T5) -> frozen text generation -> LLM2PnP (adaptor) -> tokens + embeddings -> Phones and Prosody (PnP) -> PnP2Speech (streaming TTS) -> PnP -> audio output
- Critical path: LLM output generation -> LLM2PnP processing -> PnP2Speech synthesis -> audio output
- Design tradeoffs:
  - Lookahead length vs. streaming latency: longer lookahead improves quality but increases delay
  - Chunk size vs. synthesis quality: smaller chunks enable lower latency but may degrade quality
  - Model complexity vs. inference speed: more complex models may provide better quality but slower streaming
- Failure signatures:
  - Audio quality degradation: indicates issues with lookahead constraints or knowledge distillation
  - Increased latency: suggests problems with chunk processing or model efficiency
  - Phonetic errors: points to issues with the G2P model or embedding utilization
  - Prosodic inconsistencies: indicates problems with prosody prediction or synthesis
- First 3 experiments:
  1. Baseline comparison: Run LLM2Speech with L=0 (no lookahead) and measure quality degradation vs. teacher model
  2. Lookahead sensitivity: Test different lookahead values (L=1, L=2) and measure quality/latency tradeoff
  3. Embedding ablation: Run LLM2PnP with and without LLM embeddings to quantify their contribution to quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM hidden embeddings impact prosodic quality in streaming TTS?
- Basis in paper: [explicit] The authors investigate the influence of LLM embeddings on G2P performance but find they do not yield a significant improvement in prosodic quality.
- Why unresolved: The paper only shows that LLM embeddings improve phonetic prediction but does not explain why they do not significantly impact prosodic quality.
- What evidence would resolve it: Further experiments comparing LLM embeddings to other semantic context features, and ablation studies isolating the contribution of embeddings to prosody.

### Open Question 2
- Question: What is the optimal number of LLM hidden layers to use for streaming TTS?
- Basis in paper: [explicit] The authors experiment with different numbers of LLM layers but do not determine an optimal number, only that larger LLMs and more layers improve performance.
- Why unresolved: The paper does not explore the tradeoff between model size, computational cost, and performance gains.
- What evidence would resolve it: Systematic evaluation of different LLM sizes and layer configurations, measuring both quality and latency.

### Open Question 3
- Question: How can streaming TTS be improved for languages with irregular orthography?
- Basis in paper: [explicit] The authors focus on English, which has irregular orthography, and use G2P models. The paper does not address how the approach generalizes to other languages.
- Why unresolved: The paper does not explore the challenges of streaming TTS for languages with different orthographic properties.
- What evidence would resolve it: Experiments with the proposed method on languages with different orthographic characteristics, comparing performance to non-streaming approaches.

### Open Question 4
- Question: How does the proposed streaming TTS method compare to end-to-end streaming approaches?
- Basis in paper: [explicit] The paper uses a two-step TTS approach (G2P followed by PnP2Speech) rather than an end-to-end method. The authors do not compare their method to end-to-end streaming approaches.
- Why unresolved: The paper does not explore the potential benefits or drawbacks of using an end-to-end streaming TTS model instead of the proposed two-step approach.
- What evidence would resolve it: Implementation and evaluation of an end-to-end streaming TTS model, comparing quality, latency, and computational requirements to the proposed method.

## Limitations
- Proprietary conversational speech corpus limits reproducibility and generalizability
- Lookahead constraint L=2 may not capture long-range dependencies in complex contexts
- Effectiveness of knowledge distillation across different languages and domains remains uncertain

## Confidence
- High confidence: The core mechanism of using LLM embeddings to compensate for limited lookahead is well-supported by experimental results
- Medium confidence: Generalization claims are supported by ablation studies but based on limited evaluation dataset
- Low confidence: Claims about handling arbitrary LLM-generated text streams without degradation are based on simulated conditions

## Next Checks
1. Cross-domain robustness test: Evaluate LLM2Speech on diverse conversational datasets (news, storytelling, customer service) to verify generalization beyond the proprietary corpus
2. Lookahead sensitivity analysis: Systematically vary lookahead constraint L (0, 1, 2, 3, 4) while measuring quality-latency tradeoff curve
3. Real-time streaming validation: Deploy LLM2Speech in actual streaming setup where LLM generates text incrementally while speech synthesis occurs, measuring real-world latency and quality metrics