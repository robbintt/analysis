---
ver: rpa2
title: Fairness Explainability using Optimal Transport with Applications in Image
  Classification
arxiv_id: '2308.11090'
source_url: https://arxiv.org/abs/2308.11090
tags: []
core_contribution: This work bridges algorithmic fairness and explainable AI by combining
  optimal transport theory with post-processing techniques to address bias in image
  classification. It first ensures demographic parity by projecting biased model scores
  to fair ones using Wasserstein barycenters, preserving predictive accuracy.
---

# Fairness Explainability using Optimal Transport with Applications in Image Classification

## Quick Facts
- arXiv ID: 2308.11090
- Source URL: https://arxiv.org/abs/2308.11090
- Reference count: 7
- This work bridges algorithmic fairness and explainable AI by combining optimal transport theory with post-processing techniques to address bias in image classification.

## Executive Summary
This paper presents a method that addresses algorithmic bias in image classification by combining optimal transport theory with post-processing techniques. The approach first ensures demographic parity by projecting biased model scores to fair ones using Wasserstein barycenters, preserving predictive accuracy. The second step extends this to generate bias-specific attention maps via Grad-CAM, identifying image regions most responsible for unfair predictions. Applied to the CelebA dataset with gender as a sensitive attribute, the method reduces unfairness with minimal accuracy loss.

## Method Summary
The method uses post-processing to enforce demographic parity in image classification. It projects biased classifier outputs onto the Wasserstein barycenter of conditional score distributions, ensuring fairness while preserving predictive rankings. The approach is model-agnostic and requires no retraining, operating on soft classifier outputs calibrated using a separate set. After achieving fair scores, the method trains a secondary classifier on the difference between biased and fair scores, using Grad-CAM to generate attention maps highlighting regions responsible for bias.

## Key Results
- Reduced unfairness from 0.896 to 0.531 in the beard prediction task on CelebA
- Maintained predictive accuracy while achieving demographic parity
- Generated interpretable attention maps identifying visual cues responsible for bias
- Demonstrated model-agnostic approach applicable to pre-trained classifiers

## Why This Works (Mechanism)

### Mechanism 1
Optimal transport can transform biased prediction scores into fair scores while preserving predictive accuracy. The method projects biased classifier outputs onto the Wasserstein barycenter of conditional score distributions, enforcing demographic parity by aligning CDFs across sensitive groups. Core assumption: The optimal transport map from biased to fair distributions is a function of order statistics and can be computed via quantile functions. Evidence anchors: Studies demonstrate that minimizing risk subject to zero unfairness yields the Wasserstein barycenter solution. Break condition: If sensitive groups have disjoint score distributions, the barycenter may collapse predictive power.

### Mechanism 2
The difference between biased and fair scores can be used as a proxy for detecting and visualizing discrimination. By computing cdB(X,S) = bf(X,S) - cfB(X,S), one obtains a per-instance unfairness score. Training a classifier on this target with Grad-CAM yields attention maps localizing visual cues responsible for bias. Core assumption: Gradient-based saliency from a secondary classifier trained on unfairness can isolate image regions causing bias. Evidence anchors: The unfairness formulation offers intuitive explanation where positive values indicate favoritism and negative values indicate unfavoritism. Break condition: If the unfairness signal is too diffuse, Grad-CAM may highlight irrelevant regions.

### Mechanism 3
Post-processing fairness projection works well with pre-trained models and requires no retraining. The method calibrates soft classifier outputs using a calibration set to compute CDFs and quantiles per sensitive group, then applies the transport mapping in a plug-in estimator. Core assumption: Pre-trained model's soft outputs can be treated as biased estimates and corrected via post-processing without loss of generalization. Evidence anchors: The solution is optimal with respect to relative rankings, independent of bias. Break condition: If the calibration set is unrepresentative or too small, quantile estimation fails.

## Foundational Learning

- **Optimal Transport (Wasserstein Distance)**: Provides principled way to measure and align distributions for fairness. Why needed: Enables computation of Wasserstein barycenters for fair score projection. Quick check: What does W2(νf1,νf2) represent in OT theory?
- **Demographic Parity (DP)**: Defines fairness criterion enforced by the method. Why needed: Serves as the target fairness constraint for score projection. Quick check: How does DP differ from equality of opportunity?
- **Wasserstein Barycenter**: Aggregates multiple conditional distributions into a single fair distribution. Why needed: Enables computation of fair scores that satisfy demographic parity. Quick check: What role do the weights ws play in computing a barycenter?

## Architecture Onboarding

- **Component map**: Feature block (pre-trained CNN) → Classification block (with S input) → Soft classifier output → Fairness calibrator (post-processing) → Fair classifier output → XAI pipeline (secondary classifier + Grad-CAM)
- **Critical path**: Data → Model inference → CDF/quantile estimation (calibration) → Wasserstein projection → Fair scores → Bias detection → Attention map generation
- **Design tradeoffs**:
  - Pre-trained vs fine-tuned: Pre-trained speeds deployment but may need more careful calibration
  - Soft vs hard classifier: Soft outputs preserve more information for OT but may require thresholding later
  - Calibration set size: Larger sets yield more stable quantile estimates but increase memory cost
- **Failure signatures**:
  - Calibration set too small → NaN or unstable quantile estimates
  - Sensitive feature not included in base model → Biased proxy variables dominate
  - Grad-CAM noise → Attention maps are diffuse or highlight background
- **First 3 experiments**:
  1. Verify OT projection reduces unfairness on a synthetic tabular dataset with known bias
  2. Test CDF/quantile estimation stability as calibration set size varies
  3. Run the XAI pipeline on a simple biased image dataset (e.g., modified MNIST with color bias) and inspect attention maps

## Open Questions the Paper Calls Out

### Open Question 1
How can optimal transport theory be extended to handle continuous sensitive attributes beyond binary ones? Basis: The authors note their method works for binary sensitive features and mention this as a natural extension. Why unresolved: The paper focuses on discrete binary sensitive attributes. What evidence would resolve it: A mathematical framework and empirical validation showing how Wasserstein barycenters can be computed for continuous sensitive attributes.

### Open Question 2
What is the impact of optimal transport-based fairness post-processing on downstream tasks beyond classification, such as object detection or segmentation? Basis: The authors mention their method is "model-agnostic" and can be extended to regression tasks, but don't test it on other vision tasks. Why unresolved: The paper only demonstrates the approach on binary classification of facial attributes. What evidence would resolve it: Experimental results showing the method's effectiveness on object detection, semantic segmentation, or other computer vision tasks.

### Open Question 3
How does the choice of τ threshold in the bias detection task affect the quality and interpretability of the generated attention maps? Basis: The authors use the 0.75 quantile of the bias distribution as τ but acknowledge it can be chosen based on specific goals. Why unresolved: The paper doesn't explore how different τ values impact the resulting XAI visualizations. What evidence would resolve it: A sensitivity analysis showing how varying τ affects the attention maps' ability to identify discriminatory patterns.

## Limitations
- Demonstrated primarily on a single dataset (CelebA) with specific bias (gender in beard prediction), limiting generalizability
- Effectiveness depends critically on representativeness of calibration set and assumption that optimal transport map can be accurately approximated
- Grad-CAM-based XAI component may produce diffuse attention maps if unfairness signal is not well-defined

## Confidence

- **High Confidence**: Core optimal transport theory for achieving demographic parity is well-established in literature
- **Medium Confidence**: Fairness improvement results are specific to CelebA dataset and may not generalize to other datasets or bias types
- **Medium Confidence**: XAI component using Grad-CAM is conceptually valid but effectiveness depends on quality of secondary classifier

## Next Checks

1. **Robustness to calibration set size**: Systematically vary calibration set size and measure stability of quantile estimates and resulting fairness metrics
2. **Cross-dataset generalization**: Apply method to different image classification dataset (e.g., FairFace or modified ImageNet) with different sensitive attribute (e.g., age or race) and evaluate fairness improvements
3. **Ablation study on XAI component**: Compare Grad-CAM-based attention maps with alternative XAI methods (e.g., LIME or SHAP) to assess whether proposed approach provides more meaningful explanations for observed bias