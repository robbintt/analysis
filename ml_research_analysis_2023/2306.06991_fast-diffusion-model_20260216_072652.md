---
ver: rpa2
title: Fast Diffusion Model
arxiv_id: '2306.06991'
source_url: https://arxiv.org/abs/2306.06991
tags:
- diffusion
- process
- training
- momentum
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Fast Diffusion Model (FDM) that accelerates
  diffusion models by incorporating momentum into the diffusion process. The key idea
  is to connect the diffusion process with stochastic optimization, and then leverage
  momentum SGD to improve the diffusion process.
---

# Fast Diffusion Model

## Quick Facts
- arXiv ID: 2306.06991
- Source URL: https://arxiv.org/abs/2306.06991
- Reference count: 40
- One-line primary result: FDM accelerates diffusion models by ~50% in training and 3x in sampling steps while maintaining sample quality.

## Executive Summary
This paper introduces the Fast Diffusion Model (FDM), which accelerates diffusion models by incorporating momentum into the diffusion process. The authors establish a theoretical connection between diffusion processes and stochastic optimization, then leverage momentum SGD to improve convergence speed. FDM achieves 1.6× training acceleration and 3× sampling speedup across CIFAR-10, FFHQ, and AFHQv2 datasets while maintaining or improving sample quality metrics like FID scores.

## Method Summary
FDM accelerates diffusion models by reformulating the forward diffusion process as stochastic gradient descent with momentum. The method introduces a momentum term into the diffusion update equation, then derives an analytical perturbation kernel by solving a Damped Oscillation ODE in its critically damped state. This allows direct sampling at any timestep without sequential noising. The approach is compatible with various diffusion model frameworks and includes a loss weight warm-up strategy to stabilize early training when momentum causes higher noise levels.

## Key Results
- 1.6× training acceleration across VP, VE, and EDM models on CIFAR-10, FFHQ, and AFHQv2
- 3× reduction in sampling steps while maintaining comparable FID scores
- FDM consistently outperforms baseline models in both training efficiency and inference speed
- Compatible with multiple diffusion model architectures (VP, VE, EDM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Momentum-based diffusion process accelerates training by reducing oscillation in the parameter trajectory.
- Mechanism: By introducing momentum into the diffusion process, the algorithm accumulates past gradients and provides a more stable descent direction, avoiding oscillation around high-curvature regions.
- Core assumption: The forward diffusion process of DMs aligns with stochastic gradient descent on a time-variant stochastic function.
- Evidence anchors:
  - [abstract] "momentum SGD uses both gradient and an extra momentum, achieving more stable and faster convergence"
  - [section] "momentum SGD introduces an extra momentum... which accumulates all past gradients, and thus provides a more stable descent direction than the single gradient"
- Break condition: If the critically damped state cannot be achieved due to hyperparameter mis-tuning, the system may oscillate instead of converging faster.

### Mechanism 2
- Claim: The Damped Oscillation ODE formulation allows analytical derivation of the perturbation kernel for efficient sampling.
- Mechanism: Reformulating the discrete momentum diffusion into a continuous Damped Oscillation ODE enables solving for the perturbation kernel by achieving critical damping, which avoids overshoot and speeds convergence.
- Core assumption: The deterministic part of the momentum diffusion can be modeled as a second-order ODE with controllable damping.
- Evidence anchors:
  - [section] "we frame the momentum-based process as a Damped Oscillation system whose critically damped state—the kernel solution—avoids oscillation and thus has a faster convergence speed"
  - [section] "By solving the Critically Damped ODE... we obtain the mean-varying process"
- Break condition: If the linear function extension of the damping parameter β(t) does not adequately capture the dynamics, the analytical solution may not yield the intended acceleration.

### Mechanism 3
- Claim: The equivalence between DDPM forward diffusion and SGD on a stochastic quadratic enables applying SGD acceleration techniques to diffusion models.
- Mechanism: By constructing a stochastic time-variant function whose SGD dynamics match the DDPM diffusion, techniques like momentum SGD can be directly transplanted to accelerate the diffusion process.
- Core assumption: The mapping f(x) = 1/2 Eζ∼N(0,bI)∥x − βt/(1−αt) ζ∥²₂ correctly represents the diffusion process dynamics.
- Evidence anchors:
  - [section] "the forward diffusion process of DMs accords with the stochastic optimization process of stochastic gradient descent (SGD) on a stochastic time-variant problem"
  - [section] "By setting α²_t + β²_t = 1, one can observe that the forward diffusion process of DDPM and stochastic optimization process of SGD share the same formulation"
- Break condition: If the assumed equivalence breaks down for more complex data distributions or model architectures, the momentum acceleration may not transfer effectively.

## Foundational Learning

- Concept: Stochastic Gradient Descent with Momentum
  - Why needed here: The core acceleration mechanism in FDM is directly inspired by momentum SGD; understanding its convergence benefits is essential to grasp why adding momentum to diffusion helps.
  - Quick check question: In momentum SGD, what role does the term γ(xt − xt−1) play compared to standard SGD?

- Concept: Diffusion Models and Perturbation Kernels
  - Why needed here: FDM modifies the perturbation kernel of diffusion models; understanding how p(xt|x0) is used in both training and sampling is critical.
  - Quick check question: How does the perturbation kernel p(xt|x0) in standard DDPM relate to the forward noising process?

- Concept: Ordinary Differential Equations and Critical Damping
  - Why needed here: The transition from discrete momentum diffusion to a continuous Damped Oscillation ODE requires familiarity with ODE stability and damping conditions.
  - Quick check question: What distinguishes a critically damped system from underdamped or overdamped in a second-order ODE?

## Architecture Onboarding

- Component map: Forward diffusion module → Perturbation kernel derivation → Score network module → Training loop → Sampling module
- Critical path: Forward diffusion → perturbation kernel derivation → score network training → reverse sampling
- Design tradeoffs:
  - Momentum parameter γ: Larger values accelerate training but risk instability; must be tuned carefully
  - Loss weight clamp λmax: Prevents early abnormal loss spikes due to high momentum noise; too strict a clamp slows convergence
  - ODE discretization: Balancing numerical stability and speed during sampling
- Failure signatures:
  - Training instability: Large loss spikes in early epochs suggest momentum noise is overwhelming the network capacity
  - Sampling artifacts: If the perturbation kernel is incorrectly derived, samples may show visible overshoot or ringing effects
  - Degraded FID with increased γ: Indicates that momentum is destabilizing rather than accelerating training
- First 3 experiments:
  1. Replace standard VP forward diffusion with momentum-based diffusion on CIFAR-10; measure training FID vs. number of samples to confirm 1.6× acceleration
  2. Test different γ values on a simple dataset to find the sweet spot between acceleration and stability
  3. Compare sampling NFEs for FDM vs. vanilla VP on AFHQv2; verify ~3× reduction in steps while maintaining FID

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the methodology and results presented.

## Limitations

- Hyperparameter sensitivity: FDM's performance heavily depends on careful tuning of momentum parameter γ and loss weight clamp λmax
- Limited dataset diversity: Experiments focus on three datasets (CIFAR-10, FFHQ, AFHQv2), requiring validation on more diverse data distributions
- Theoretical assumptions: The critical damping formulation assumes linear β(t) behavior which may not hold for all diffusion model variants

## Confidence

- Training acceleration claim: Medium confidence - well-supported by experiments but dependent on hyperparameter tuning
- Sampling speedup claim: Medium confidence - demonstrated but may vary with different architectures and samplers
- Theoretical framework: Medium confidence - mathematically sound but relies on specific assumptions about diffusion-SGD equivalence

## Next Checks

1. Test FDM across diverse diffusion architectures beyond VP, VE, and EDM (e.g., DDIM, DPM) to verify the claimed compatibility
2. Conduct ablation studies varying γ systematically to map the stability-acceleration tradeoff curve
3. Evaluate FDM performance on datasets with different characteristics (e.g., ImageNet, LSUN) to assess generalizability beyond the current three datasets