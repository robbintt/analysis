---
ver: rpa2
title: 'GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value in
  Similar Item Recommendation'
arxiv_id: '2310.17732'
source_url: https://arxiv.org/abs/2310.17732
tags:
- item
- graph
- items
- nodes
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of optimizing Gross Merchandise
  Value (GMV) in similar item recommendation for e-commerce platforms, which is crucial
  for increasing revenue but not directly addressed by existing graph neural network
  (GNN) architectures. The proposed GNN-GMVO (Graph Neural Network - Gross Merchandise
  Value Optimizer) introduces a new decoder function that incorporates item prices
  into the similarity scoring mechanism, allowing the model to optimize both relevance
  and revenue simultaneously.
---

# GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value in Similar Item Recommendation

## Quick Facts
- arXiv ID: 2310.17732
- Source URL: https://arxiv.org/abs/2310.17732
- Reference count: 40
- Key outcome: GNN-GMVO improves EGMV@4 by up to 3.6% on All Beauty category without degrading NDCG ranking metrics

## Executive Summary
This paper addresses the challenge of optimizing Gross Merchandise Value (GMV) in similar item recommendation for e-commerce platforms. The authors propose GNN-GMVO, a Graph Neural Network architecture that incorporates item prices into the similarity scoring mechanism through a custom decoder function. By balancing relevance and revenue objectives, the model achieves superior performance on both NDCG ranking metrics and expected GMV compared to traditional GNN models like GCN and GAT.

## Method Summary
GNN-GMVO uses GCN or GAT encoders with a custom price-weighted decoder: DEC(zu, zv) = (1 + λ(pu + pv))(zTu zv), where λ controls the trade-off between relevance and revenue. The model employs a novel edge construction method using co-view, view-bought, and co-purchase data with subtraction of complementary items. Node embeddings are initialized using Universal Sentence Encoder from item text features. Training uses binary cross-entropy loss with two-hop message passing and 256-dimensional embeddings. The model is evaluated on Walmart.com and Amazon datasets using NDCG and EGMV@K metrics.

## Key Results
- GNN-GMVO achieves up to 3.6% improvement in EGMV@4 on All Beauty category
- Maintains or improves NDCG ranking metrics while optimizing revenue
- Outperforms traditional GCN and GAT models in expected GMV
- Custom edge construction reduces noise from complementary items while preserving similarity signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decoder function incorporates item prices into the similarity scoring mechanism, allowing the model to optimize both relevance and revenue simultaneously.
- Mechanism: By adding a price-weighted term (1 + λ(pu + pv)) to the inner product of node embeddings, the model increases the similarity score for high-priced items when computing recommendations. This directly biases the model to favor items that generate higher GMV without degrading relevance.
- Core assumption: The price of an item is a valid proxy for its contribution to GMV and that users are willing to consider higher-priced similar items.
- Evidence anchors:
  - [abstract]: "The proposed GNN-GMVO (Graph Neural Network - Gross Merchandise Value Optimizer) introduces a new decoder function that incorporates item prices into the similarity scoring mechanism, allowing the model to optimize both relevance and revenue simultaneously."
  - [section]: "Under GCN-GMVO framework for the similar item recommendation problem, we aim to optimize on item similarity relevance while inflating the weight of the edges with higher item prices."
  - [corpus]: Weak evidence; corpus neighbors discuss revenue optimization but do not explicitly mention price-aware GNN architectures.
- Break condition: If price sensitivity is not uniform across user segments, the fixed λ may misalign with actual purchasing behavior, reducing both relevance and revenue.

### Mechanism 2
- Claim: Custom edge construction using co-view, view-bought, and co-purchase data reduces noise and better identifies true similarity relations between items.
- Mechanism: The edge construction metric Sc(u,v) = |cv(u,v)| + |vb(u,v)| + |vb(v,u)| - |cp(u,v)| captures directional and co-occurrence interactions while subtracting complementary item signals (|cp(u,v)|) to isolate similarity.
- Core assumption: Co-viewing and sequential view-then-buy behavior are strong indicators of item similarity, while co-purchasing often indicates complementarity rather than similarity.
- Evidence anchors:
  - [section]: "We propose a customized edge construction method to tailor the model toward similar item recommendation task and alleviate the noisy and complex item-item relations."
  - [section]: "Eq. (8) to detect strong similarity relations while removing noise created by complementary items."
  - [corpus]: No direct evidence in corpus; corpus neighbors focus on different aspects of recommendation (e.g., fairness, conversion) without discussing edge construction for similarity.
- Break condition: If the assumption that co-purchasing is predominantly complementary is wrong, the subtraction term may incorrectly remove genuine similarity edges.

### Mechanism 3
- Claim: The multi-hop message passing in GNN-GMVO allows the model to capture higher-order item-item relationships beyond direct edges, improving embedding quality.
- Mechanism: By performing two-hop message passing, the model aggregates information from neighbors-of-neighbors, allowing embeddings to reflect broader structural context in the item graph, which enhances similarity detection.
- Core assumption: Item similarity is not just local (direct neighbor) but also depends on shared neighborhoods and higher-order connectivity patterns.
- Evidence anchors:
  - [section]: "GNNs aggregate information from the graph's structure to create a deep representation for each node by using a form of message passing to transfer information between nodes to update each node's representation."
  - [section]: "We use a two-hop message passing GCN and GAT models with an output embedding of 256 dimensions."
  - [corpus]: No direct evidence in corpus; neighbors do not discuss message passing depth or higher-order graph features.
- Break condition: If the item graph is too sparse or noisy, multi-hop aggregation may propagate irrelevant signals, degrading embedding quality.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: The core innovation relies on using GNNs to model complex item-item relations and generate embeddings that encode both similarity and price information.
  - Quick check question: Can you explain how a single message passing step updates a node's embedding in a GCN?

- Concept: Edge construction and graph topology design
  - Why needed here: The performance of GNN-GMVO depends critically on how edges are defined; poor edge construction introduces noise that misleads the model.
  - Quick check question: Why does the proposed Sc(u,v) metric subtract co-purchase counts, and what would happen if this term were omitted?

- Concept: Multi-objective optimization and hyperparameter tuning
  - Why needed here: Balancing relevance (NDCG) and revenue (GMV) requires tuning λ; understanding how the loss function changes with λ is key to effective deployment.
  - Quick check question: What is the effect on NDCG and GMV when λ is increased from 0 to 1, and why does this trade-off occur?

## Architecture Onboarding

- Component map: Universal Sentence Encoder embeddings -> Custom edge construction (Sc(u,v)) -> GCN/GAT encoder -> Price-weighted decoder -> Binary cross-entropy loss

- Critical path:
  1. Load and preprocess item metadata → generate USE embeddings
  2. Construct item-item graph using Sc(u,v) threshold
  3. Train GNN encoder-decoder with binary cross-entropy loss
  4. During inference, encode items, rank candidates using weighted cosine similarity
  5. Apply cold-start handling if necessary

- Design tradeoffs:
  - GCN vs. GAT: GCN is simpler and faster but GAT can capture attention-weighted neighborhood influence
  - λ tuning: Higher λ boosts revenue but risks relevance; optimal λ is dataset and segment dependent
  - Edge sparsity: Strict Sc(u,v) thresholds reduce noise but may exclude valid edges; looser thresholds increase coverage but add noise

- Failure signatures:
  - NDCG drops sharply with increasing λ → revenue focus too aggressive
  - EGMV plateaus or decreases → price weighting not aligned with user behavior
  - Training loss oscillates → graph too noisy or edge construction flawed
  - Cold-start items receive poor recommendations → nearest neighbor linking ineffective

- First 3 experiments:
  1. Baseline: Train GCN-GMVO with λ = 0 (pure relevance) and measure NDCG on held-out set
  2. Revenue test: Incrementally increase λ (0.1, 0.5, 1.0) and observe NDCG/EGMV trade-off curve
  3. Edge construction ablation: Train with and without the co-purchase subtraction term to quantify noise reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the GNN-GMVO architecture perform when incorporating additional features such as image embeddings and item attributes beyond textual information?
- Basis in paper: [inferred] The paper mentions that future work could include adding features such as items' features (price, product type, product category, etc.) and image embedding data to the textual embedding data to enrich the initial embedding inputs of the model.
- Why unresolved: The current experiments are limited to using textual information for node embeddings. The potential benefits of incorporating multimodal data are hypothesized but not empirically validated.
- What evidence would resolve it: Comparative experiments showing NDCG and EGMV performance improvements when using multimodal features (text, image, attributes) versus text-only embeddings.

### Open Question 2
- Question: How does the GNN-GMVO model handle the cold-start problem for new items in a more comprehensive way, beyond the proposed edge construction method for items with zero or small user interactions?
- Basis in paper: [explicit] The paper discusses the edge construction for cold items but suggests that further investigation is needed for a more comprehensive solution.
- Why unresolved: The proposed method for handling cold items is preliminary and may not scale well or capture complex item relationships effectively.
- What evidence would resolve it: Evaluation of the model's performance on cold items using alternative strategies such as meta-learning, transfer learning, or hybrid models that combine content-based and collaborative filtering approaches.

### Open Question 3
- Question: How does the GNN-GMVO model's performance compare to other state-of-the-art recommendation models, such as DeepFM, xDeepFM, and wide & deep models, when used in conjunction with the GNN-GMVO architecture?
- Basis in paper: [explicit] The paper suggests that feeding the generated node embeddings by GNN-GMVO into other expert models like wide & deep, ProspectNet, DeepFM, and xDeepFM could be tested to examine if this further optimizes the model.
- Why unresolved: The paper does not provide empirical comparisons with these models, leaving their potential benefits unexplored.
- What evidence would resolve it: Head-to-head comparisons of GNN-GMVO embeddings integrated with DeepFM, xDeepFM, and wide & deep models against standalone implementations of these models in terms of NDCG and EGMV.

## Limitations

- Proprietary Walmart dataset prevents independent validation of claimed improvements
- Missing sensitivity analysis for λ hyperparameter across different price distributions
- No comparison against non-GNN revenue optimization baselines (e.g., collaborative filtering with price-aware ranking)
- Cold-start handling described but implementation details not provided

## Confidence

- Revenue-relevance trade-off optimization: High confidence
- Custom edge construction effectiveness: High confidence  
- Multi-hop message passing benefits: Medium confidence
- Generalizability to different price distributions: Low confidence

## Next Checks

1. Replicate the NDCG-EGMV trade-off curve on Amazon datasets by varying λ from 0.0 to 1.0 in 0.1 increments and verify monotonic degradation in relevance metrics
2. Conduct ablation studies removing the co-purchase subtraction term from edge construction to quantify noise reduction empirically
3. Test model performance on synthetic price distributions to verify that the price-weighted decoder generalizes beyond the observed price ranges in the datasets