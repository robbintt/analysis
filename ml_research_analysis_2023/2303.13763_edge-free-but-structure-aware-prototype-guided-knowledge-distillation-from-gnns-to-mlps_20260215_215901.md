---
ver: rpa2
title: 'Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from
  GNNs to MLPs'
arxiv_id: '2303.13763'
source_url: https://arxiv.org/abs/2303.13763
tags:
- graph
- pgkd
- glnn
- mlps
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PGKD, a method for distilling knowledge from
  graph neural networks (GNNs) to multilayer perceptrons (MLPs) without requiring
  graph edge information. The key idea is to use class prototypes to capture and transfer
  graph structural information from GNNs to MLPs.
---

# Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs

## Quick Facts
- **arXiv ID**: 2303.13763
- **Source URL**: https://arxiv.org/abs/2303.13763
- **Reference count**: 19
- **Primary result**: PGKD outperforms GLNN on 6 graph benchmarks in both transductive and inductive settings, achieving higher accuracy and robustness to noisy node features.

## Executive Summary
This paper introduces PGKD, a method for distilling knowledge from graph neural networks (GNNs) to multilayer perceptrons (MLPs) without requiring graph edge information. The key idea is to use class prototypes to capture and transfer graph structural information from GNNs to MLPs. PGKD employs two loss functions: an intra-class loss to draw nodes closer to their class prototypes, and an inter-class loss to align the distance patterns among class prototypes. Experimental results on six graph benchmarks demonstrate that PGKD outperforms the baseline method GLNN in both transductive and inductive settings, achieving higher accuracy and robustness, especially in the presence of noisy node features.

## Method Summary
PGKD distills knowledge from a trained GNN teacher to an MLP student using class prototypes. The method extracts prototypes from the GNN's output, then trains the MLP using two additional losses beyond standard KD: an intra-class loss that pulls node features toward their class prototype, and an inter-class loss that aligns the distance patterns between class prototypes. This allows the MLP to capture structural information encoded in the GNN without access to edge data. The approach is evaluated on six graph benchmarks (Cora, Citeseer, A-computer, Pubmed, Penn94, Twitch-gamer) in both transductive and inductive settings.

## Key Results
- PGKD outperforms GLNN on 6 graph benchmarks in both transductive and inductive settings
- PGKD shows better robustness to noisy node features compared to GLNN
- PGKD achieves higher node classification accuracy while requiring no edge information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intra-class edges enforce local smoothness by constraining representations of connected nodes to become similar, capturing homophily.
- Mechanism: GNNs use message passing to aggregate neighbor features, naturally enforcing smoothness over the graph. This makes representations of nodes in the same class cluster together.
- Core assumption: Connected nodes in the same class should have similar representations.
- Evidence anchors:
  - [abstract] "The Laplacian regularization guides the smoothness of H over G, where connected nodes share similar features."
  - [section] "The intra-class edges enforce the local smoothness by constraining the learned representations of two connected nodes to become similar, so that the homophily property for nodes from the same class can be captured."
- Break condition: If nodes from the same class are not connected or if the graph has high heterophily, the smoothness assumption fails.

### Mechanism 2
- Claim: Inter-class edges determine the pattern of distances among class prototypes; more edges between two classes means smaller distance.
- Mechanism: By analyzing the prototype distances in GNNs, PGKD aligns MLP prototypes to mimic the same inter-class distance patterns without needing actual edge information.
- Core assumption: The relative distances between class prototypes in GNNs reflect the structural connectivity patterns of inter-class edges.
- Evidence anchors:
  - [abstract] "The inter-class edges, connecting two nodes from different classes, determine the pattern of distances among these classes."
  - [section] "We define the class distance as the distance between class prototypes, and the class prototype is the prototypical vector for all nodes from the same class. The inter-class edges determine the pattern of class distances."
- Break condition: If inter-class edges do not correlate with class distances, the alignment fails.

### Mechanism 3
- Claim: Class prototypes provide a stable summary of each class, enabling efficient distillation of structural information without edge data.
- Mechanism: Prototypes aggregate node features within each class, reducing sensitivity to noise and computational cost compared to pairwise node distances.
- Core assumption: Class prototypes are less sensitive to outliers than individual node features and provide a good summary of intra-class structure.
- Evidence anchors:
  - [abstract] "We employ the class prototypes to analyze the impact of graph structures on GNN teachers, and then design two losses to distill such information from GNNs to MLPs."
  - [section] "Meanwhile, the computing cost is decreased into O(n×K) where K≪n typically."
- Break condition: If class prototypes do not capture the true distribution of node features, the distillation becomes inaccurate.

## Foundational Learning

- Concept: Graph Laplacian regularization
  - Why needed here: Explains why GNNs naturally enforce smoothness and why intra-class edges matter.
  - Quick check question: What is the role of the graph Laplacian in GNN optimization?

- Concept: Prototypical networks and metric learning
  - Why needed here: Shows how class prototypes can be used to measure and align intra- and inter-class structure.
  - Quick check question: How are class prototypes computed and why are they robust to noise?

- Concept: Knowledge distillation (KD) and logit matching
  - Why needed here: Provides the baseline framework that PGKD builds upon and improves.
  - Quick check question: What is the difference between vanilla KD and PGKD in terms of loss components?

## Architecture Onboarding

- Component map: GNN teacher -> prototype extraction -> intra-class loss + inter-class loss -> MLP student
- Critical path: Extract prototypes -> compute intra-class alignment -> compute inter-class alignment -> combine with KD loss
- Design tradeoffs: Prototype stability vs. computational overhead; intra-class vs. inter-class focus; temperature scaling for softmax
- Failure signatures: Low intra-class alignment leads to dispersed same-class features; low inter-class alignment leads to incorrect class distances; high noise in prototypes causes unstable training
- First 3 experiments:
  1. Verify prototype computation matches class means on labeled data.
  2. Check that intra-class loss pulls node features toward their class prototype.
  3. Confirm inter-class loss aligns MLP prototype distances with GNN prototype distances.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PGKD perform on graph tasks beyond node classification, such as link prediction or graph classification?
- Basis in paper: [inferred] The authors mention future work will generalize PGKD to other graph tasks beyond node classification.
- Why unresolved: The current paper only evaluates PGKD on node classification tasks, leaving its performance on other graph tasks unexplored.
- What evidence would resolve it: Experiments applying PGKD to link prediction and graph classification benchmarks would demonstrate its effectiveness on a wider range of graph tasks.

### Open Question 2
- Question: Can prototype generation in PGKD be improved by utilizing node representations instead of relying solely on class labels?
- Basis in paper: [explicit] The authors suggest future work will explore generating prototypes using node representations rather than class labels.
- Why unresolved: The current prototype generation strategy in PGKD uses class labels to group nodes, which may not fully capture the rich information in node representations.
- What evidence would resolve it: Comparing PGKD's performance using prototypes generated from node representations versus class labels would show if the former leads to better distillation of graph structural knowledge.

### Open Question 3
- Question: How does PGKD compare to other methods for distilling GNNs to MLPs in terms of computational efficiency?
- Basis in paper: [inferred] The authors mention that PGKD requires slightly more computational cost compared to the baseline GLNN method.
- Why unresolved: While the paper demonstrates PGKD's effectiveness, it does not provide a detailed comparison of computational efficiency with other GNN-to-MLP distillation methods.
- What evidence would resolve it: Empirical comparisons of PGKD's runtime and memory usage with other methods on the same benchmarks would quantify its computational efficiency relative to the state-of-the-art.

## Limitations

- The paper lacks detailed specifications of critical hyperparameters for PGKD, particularly the weights for intra-class and inter-class loss terms.
- The empirical evaluation is limited to six graph benchmarks, all with relatively clean node features, leaving uncertainty about PGKD's robustness to highly noisy or heterophilic graphs.
- The paper does not report training stability metrics or variance across multiple runs, making it difficult to assess the method's reliability in practice.

## Confidence

- **High confidence**: The core mechanism that class prototypes can capture structural information from GNNs is well-supported by the experimental results showing PGKD outperforms GLNN across multiple datasets and settings.
- **Medium confidence**: The claim that PGKD achieves better robustness to noisy node features is supported but needs more rigorous testing on datasets with controlled noise levels and different noise types.
- **Medium confidence**: The assertion that PGKD achieves computational efficiency through prototype-based aggregation is plausible given the O(n×K) complexity claim, but requires verification through runtime benchmarks.

## Next Checks

1. **Noise robustness validation**: Systematically evaluate PGKD on datasets with varying levels and types of feature noise (Gaussian, random, adversarial) to quantify its robustness advantage over GLNN.
2. **Hyperparameter sensitivity analysis**: Conduct ablation studies to determine the optimal weights for intra-class and inter-class loss terms, and assess how sensitive PGKD performance is to these hyperparameters.
3. **Runtime and scalability evaluation**: Measure actual training and inference times for PGKD compared to GLNN across datasets of increasing size to verify the claimed computational efficiency benefits.