---
ver: rpa2
title: The Optimal Approximation Factors in Misspecified Off-Policy Value Function
  Estimation
arxiv_id: '2307.13332'
source_url: https://arxiv.org/abs/2307.13332
tags:
- approximation
- function
- value
- which
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the optimal approximation factor in misspecified\
  \ off-policy value function estimation. The authors analyze linear estimators for\
  \ infinite-horizon discounted Markov Reward Processes under two norms: the weighted\
  \ L2(\xB5) norm and the L\u221E norm."
---

# The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation

## Quick Facts
- **arXiv ID**: 2307.13332
- **Source URL**: https://arxiv.org/abs/2307.13332
- **Reference count**: 40
- **One-line primary result**: This paper characterizes the optimal approximation factors for linear off-policy value function estimation under misspecification, establishing tight bounds under both L2(µ) and L∞ norms.

## Executive Summary
This paper studies the fundamental hardness of off-policy value function estimation when the true value function cannot be represented exactly by the chosen function class. The authors analyze linear estimators for infinite-horizon discounted Markov Reward Processes under two norms, establishing tight upper and lower bounds on the approximation ratio. For the L2(µ) norm, they show that the Least Squares Temporal Difference (LSTD) estimator achieves the optimal approximation factor, which depends on two problem-dependent parameters. For the L∞ norm, LSTD also achieves the optimal approximation factor in certain regimes. Under a full support assumption, a model-based estimator achieves a much better approximation ratio of 2/(1-γ). The results provide a complete characterization of the hardness of off-policy value function estimation under misspecification.

## Method Summary
The paper analyzes linear off-policy value function estimation in infinite-horizon discounted Markov Reward Processes. The method involves establishing tight upper and lower bounds on the approximation ratio (ratio of estimator error to best-in-class error) for both L2(µ) and L∞ norms. For the L2(µ) norm, the authors analyze the Least Squares Temporal Difference (LSTD) estimator and prove it achieves optimal approximation factors through a decomposition of the error into the action of a linear operator on the component orthogonal to the features. For the L∞ norm, similar analysis is performed. Under a full support assumption, a model-based estimator using Bayes value functions is analyzed and shown to achieve better approximation ratios. The paper also constructs specific MRPs to prove lower bounds matching the upper bounds, establishing tightness.

## Key Results
- LSTD achieves optimal approximation factor under L2(µ) norm when A matrix is invertible
- LSTD achieves optimal approximation factor under L∞ norm in certain regimes
- Model-based estimator achieves approximation ratio of 2/(1-γ) under full support assumption
- Tight upper and lower bounds are established, showing the hardness of the estimation problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Least Squares Temporal Difference (LSTD) estimator achieves the optimal approximation factor for linear off-policy value function estimation under the L2(µ) norm.
- Mechanism: LSTD's error can be decomposed exactly into the action of the linear operator ΦA⁻¹Φ⊤DP on the component of the value function orthogonal to the features. This decomposition allows for tight upper bounds involving two instance-dependent parameters: ∥ΠµP∥µ and σmin(Σ⁻¹/²AΣ⁻¹/²).
- Core assumption: The A matrix is invertible and the feature map φ is bounded with non-degenerate second moment matrix Σ.
- Evidence anchors:
  - [abstract]: "For the L2(µ) norm, they show that the Least Squares Temporal Difference (LSTD) estimator achieves the optimal approximation factor, which depends on two problem-dependent parameters."
  - [section 4.1]: "Theorem 4.1. Assume that the A matrix from Equation (8) is invertible. Then the population LSTD estimator of Equation (10) has an approximation factor upper bound of..."
  - [corpus]: No direct evidence found for LSTD optimality in related works; this appears to be a novel contribution.
- Break condition: If the A matrix is not invertible, no estimator can have a finite approximation ratio.

### Mechanism 2
- Claim: The approximation factor under the L∞ norm depends only on the minimum singular value of A, not on ∥ΠµP∥µ.
- Mechanism: The L∞ norm bounds the maximum error across all states, which makes the guarantee distribution-independent. This results in a simpler expression for the approximation factor that only involves σmin(A).
- Core assumption: The A matrix is invertible and the feature map φ is bounded with non-degenerate second moment matrix Σ.
- Evidence anchors:
  - [abstract]: "For the L∞ norm, LSTD also achieves the optimal approximation factor in certain regimes."
  - [section 5.1]: "Theorem 5.1. Assume that the A matrix from Equation (8) is invertible. Then the population LSTD estimator has an approximation factor upper bound of..."
  - [corpus]: No direct evidence found for L∞ norm analysis in related works; this appears to be a novel contribution.
- Break condition: If the A matrix is not invertible, no estimator can have a finite approximation ratio.

### Mechanism 3
- Claim: Under a full support assumption, a model-based estimator using Bayes value functions achieves an approximation ratio of 2/(1-γ), independent of 1/σmin(A).
- Mechanism: The Bayes model ignores the topology on the abstract space and instead learns a pointwise function. This approach is fundamentally different from LSTD and can achieve better approximation factors when the full support assumption holds.
- Core assumption: The off-policy distribution µ has full support over the entire state space.
- Evidence anchors:
  - [abstract]: "Under a full support assumption, a model-based estimator achieves a much better approximation ratio of (1-γ)⁻¹."
  - [section 5.2]: "Assumption 5.3 (Full support). The off-policy distribution µ is such that supp(µ) = S."
  - [corpus]: No direct evidence found for full support assumption in related works; this appears to be a novel contribution.
- Break condition: If the full support assumption is violated, this estimator's approximation ratio can be arbitrarily large.

## Foundational Learning

- Concept: Markov Reward Processes (MRPs)
  - Why needed here: The paper studies linear off-policy value function estimation in infinite-horizon discounted MRPs.
  - Quick check question: What are the components of an MRP and how is the value function defined?

- Concept: Linear function approximation
  - Why needed here: The learner uses a feature map φ to approximate the value function linearly, and the paper studies misspecified settings where the true value function is not linear in the features.
  - Quick check question: What is the difference between realizable and misspecified settings in linear function approximation?

- Concept: L2(µ) and L∞ norms
  - Why needed here: The paper studies approximation factors under both weighted L2(µ) norm and L∞ norm, which have different properties and applications.
  - Quick check question: What are the key differences between L2(µ) and L∞ norms in the context of value function estimation?

## Architecture Onboarding

- Component map: Feature map φ -> Offline dataset -> LSTD estimator / Model-based estimator -> Approximation factor analysis
- Critical path:
  1. Define the MRP and feature map
  2. Collect offline data
  3. Choose estimator (LSTD or model-based)
  4. Compute approximation factor bounds
  5. Analyze hardness of the estimation problem

- Design tradeoffs:
  - LSTD vs. model-based estimators: LSTD is more sample-efficient but may have larger approximation factors; model-based estimators can achieve better approximation factors under full support but are less sample-efficient.
  - L2(µ) vs. L∞ norms: L2(µ) norms are more natural for function estimation but depend on the offline distribution; L∞ norms provide distribution-independent guarantees but are harder to minimize.

- Failure signatures:
  - Infinite approximation ratio: Indicates that the true value function cannot be approximated within the given function class, even asymptotically.
  - Large approximation factor: Suggests that the estimation problem is inherently difficult due to the instance-dependent parameters (e.g., small σmin(A) or large ∥ΠµP∥µ).

- First 3 experiments:
  1. Implement LSTD estimator and verify its approximation factor bounds on a simple MRP with known parameters.
  2. Compare LSTD and model-based estimators on a problem with full support to demonstrate the difference in approximation factors.
  3. Construct examples where the approximation factor is infinite to illustrate the hardness of the estimation problem.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions on the feature mapping φ and transition dynamics P can the optimal approximation factor be reduced to 1 in the L2(µ) norm?
- Basis in paper: [explicit] The paper identifies conditions where LSTD achieves an approximation factor of 1, such as when ΦA⁻¹Φ⊤DP v⊥ = 0 or when the orthogonal subspace of col(Φ) is closed under P.
- Why unresolved: The paper provides sufficient conditions but does not characterize all possible scenarios where the approximation factor can be 1. A complete characterization remains open.
- What evidence would resolve it: A comprehensive mathematical proof identifying all feature mapping and transition dynamics configurations that guarantee an approximation factor of 1.

### Open Question 2
- Question: Can the optimal approximation factor in the L∞ norm be improved beyond the (1-γ)⁻¹ bound under the full support assumption?
- Basis in paper: [explicit] The paper establishes that under full support, the Bayes value function achieves an approximation ratio of 2/(1-γ), but also shows a lower bound approaching this value.
- Why unresolved: The paper demonstrates this is close to optimal but does not prove whether it is exactly optimal or if further improvements are possible.
- What evidence would resolve it: Either a matching upper bound proof showing 2/(1-γ) is indeed optimal, or a counterexample demonstrating a better achievable ratio.

### Open Question 3
- Question: How do the instance-dependent factors (such as ∥ΠµP∥µ and σmin(Σ⁻¹/²AΣ⁻¹/²)) behave in high-dimensional feature spaces?
- Basis in paper: [inferred] The paper discusses these factors extensively but focuses on theoretical bounds rather than their behavior as feature dimension increases.
- Why unresolved: The paper does not analyze how these factors scale with feature dimensionality, which is crucial for practical applications with high-dimensional features.
- What evidence would resolve it: Empirical studies or theoretical bounds showing how these factors grow or remain bounded as feature dimension increases, potentially revealing new hardness regimes.

### Open Question 4
- Question: Are there alternative assumptions or quantities that can replace the finiteness of ∥ΠµP∥µ and σmin(Σ⁻¹/²AΣ⁻¹/²) to guarantee finite approximation factors?
- Basis in paper: [explicit] The paper identifies these quantities as necessary for finite approximation factors and mentions that alternative assumptions like Bellman-completeness exist but behave differently.
- Why unresolved: The paper suggests these alternatives exist but does not explore whether they can provide better approximation factors or under what conditions they might be equivalent.
- What evidence would resolve it: A comparative analysis showing under which assumptions (traditional vs. alternative) better approximation factors can be achieved, or proving the impossibility of improvement under any assumption.

## Limitations

- The optimality of LSTD critically depends on the invertibility of the A matrix, which may be restrictive in practical applications.
- The improved approximation factor for the model-based estimator requires a full support assumption that is often violated in real-world offline datasets.
- The approximation factors depend on problem-specific parameters that may be difficult to estimate or control in practice.

## Confidence

**High Confidence Claims:**
- LSTD achieves the optimal approximation factor under L2(µ) norm when A is invertible (Theorem 4.1 and its upper bound)
- The model-based estimator achieves approximation factor 2/(1-γ) under full support assumption (Theorem 5.2)
- No estimator can achieve finite approximation ratio when A is singular

**Medium Confidence Claims:**
- LSTD achieves optimal approximation factor under L∞ norm in certain regimes (Theorem 5.1)
- The characterization of problem hardness through instance-dependent parameters is complete

**Low Confidence Claims:**
- The tightness of lower bounds in all regimes (requires checking specific counterexamples)
- Practical relevance of the full support assumption for real-world applications

## Next Checks

1. **Empirical Verification of LSTD Optimality**: Implement the LSTD estimator and test it on synthetic MRPs with controlled parameters (varying ∥ΠµP∥µ and σmin(A)) to verify that it achieves the theoretical approximation factor bounds and that no other estimator outperforms it.

2. **Sensitivity Analysis of Full Support Assumption**: Construct MRPs where the off-policy distribution has limited support and empirically measure how the approximation factor of the model-based estimator degrades as the support becomes sparser, validating the theoretical predictions.

3. **Numerical Stability Assessment**: Perform extensive numerical experiments with random feature maps and transition matrices to determine the probability that the A matrix becomes singular or nearly singular, and quantify how often this leads to infinite or very large approximation ratios in practice.