---
ver: rpa2
title: Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs
arxiv_id: '2303.10165'
source_url: https://arxiv.org/abs/2303.10165
tags:
- lemma
- algorithm
- function
- have
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reward-free reinforcement learning with linear
  function approximation, where the agent must explore an environment without access
  to rewards and later plan optimally for any given reward function. The authors propose
  a new algorithm for linear mixture MDPs that achieves horizon-free sample complexity,
  meaning the complexity does not depend on the planning horizon H.
---

# Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs

## Quick Facts
- arXiv ID: 2303.10165
- Source URL: https://arxiv.org/abs/2303.10165
- Reference count: 4
- Achieves first horizon-free sample complexity in reward-free RL with linear function approximation

## Executive Summary
This paper presents the first horizon-free algorithm for reward-free reinforcement learning with linear function approximation. The authors develop a method that requires only Õ(d²ε⁻²) episodes to find an ε-optimal policy in linear mixture MDPs, where d is the feature dimension. This is achieved through a novel high-order moment estimator that precisely controls aleatoric and epistemic uncertainties, combined with uncertainty-weighted value-targeted regression and exploration-driven pseudo-rewards. The algorithm operates in two phases: an exploration phase without access to rewards, followed by a planning phase where it can find optimal policies for any given reward function.

## Method Summary
The algorithm employs a two-phase approach to reward-free reinforcement learning. In the exploration phase, it generates pseudo-rewards based on trajectory-level uncertainty and uses a high-order moment estimator to track both aleatoric and epistemic uncertainties. This involves recursive weighted regression targeting higher-order moments of value functions, with weights incorporating both data uncertainty and estimated aleatoric uncertainty. The exploration is guided by maximizing overall uncertainty along trajectories. After collecting sufficient samples, the planning phase uses dynamic programming with the estimated transition kernel to find optimal policies for any given reward function.

## Key Results
- Achieves Õ(d²ε⁻²) sample complexity for horizon-free exploration, independent of planning horizon H
- Proves matching lower bound of Ω(d²ε⁻²), establishing optimality of the approach
- First algorithm to achieve horizon-free reward-free RL with linear function approximation
- High-order moment estimation provides precise uncertainty quantification for both aleatoric and epistemic components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-order moment estimation precisely controls both aleatoric and epistemic uncertainties, enabling horizon-free sample complexity.
- Mechanism: Recursive weighted regression targets higher-order moments of value functions (V, V², V⁴, etc.), with each order estimating the variance of the previous. Weights incorporate both data uncertainty (γ²∥φ∥Σ⁻¹) and estimated aleatoric uncertainty (variance estimates + error bounds).
- Core assumption: Transition kernel can be represented as linear combination of known feature mappings; sum of rewards per episode is bounded by 1.
- Evidence anchors: [abstract] "high-order moment estimator for the aleatoric and epistemic uncertainties"; [section] recursive definition of higher-order estimators in Algorithm 2.

### Mechanism 2
- Claim: Exploration-driven pseudo-rewards guide the agent to collect samples that minimize uncertainty in the transition kernel estimate.
- Mechanism: Constructs pseudo-reward function based on trajectory-level uncertainty rather than step-level uncertainty. At each episode start, optimizes over (θ, π, r) to maximize sum of uncertainties along trajectories, then follows resulting policy.
- Core assumption: Pseudo-reward function can be optimized to approximate uncertainty-maximizing trajectory.
- Evidence anchors: [abstract] "uncertainty-weighted value-targeted regression with exploration-driven pseudo-reward"; [section] construction of pseudo-reward function aiming to maximize overall uncertainty.

### Mechanism 3
- Claim: High-confidence sets ensure estimated transition kernel remains close to true kernel with high probability.
- Mechanism: After each episode, adds constraints ∥θ − ˆθk,m∥˙ˆΣ⁻¹ k,m ≤ βk and ∥θ − ˜θk,m∥˙˜Σ⁻¹ k,m ≤ βk to confidence set Uk. Constraints use covariance matrices updated with inverse-variance weighting.
- Core assumption: Confidence radius βk grows slowly enough with episodes while shrinking with more data.
- Evidence anchors: [abstract] "high-order moment estimator for the aleatoric and epistemic uncertainties"; [section] explicit form of βk in Theorem 5.1.

## Foundational Learning

- Concept: Linear function approximation in MDPs
  - Why needed here: Algorithm assumes transition probabilities are linear combinations of known features, enabling compact representation and efficient estimation
  - Quick check question: What is the form of the linear mixture MDP assumption P(s'|s,a) = Σφi(s'|s,a)θ*i?

- Concept: Value-targeted regression (VTR)
  - Why needed here: Algorithm uses VTR to estimate transition parameters by regressing value functions against feature mappings, which is more sample-efficient than direct transition estimation
  - Quick check question: How does VTR differ from direct maximum likelihood estimation of transition probabilities?

- Concept: High-order moment estimation
  - Why needed here: Higher-order moments provide increasingly accurate estimates of value function variance, crucial for proper uncertainty quantification
  - Quick check question: Why does the algorithm need to estimate moments up to order M = log(1/ε)/log(2)?

## Architecture Onboarding

- Component map: Exploration phase (Pseudo-reward generation -> Trajectory sampling -> High-order moment estimation -> Confidence set updates) -> Planning phase (Dynamic programming using estimated transition kernel)

- Critical path:
  1. Generate pseudo-reward via uncertainty maximization
  2. Execute policy and collect trajectory
  3. Update feature matrices and moment estimates using HOME
  4. Update confidence sets with new constraints
  5. Repeat until sample complexity bound reached
  6. Perform backward dynamic programming for given reward

- Design tradeoffs:
  - Higher M improves variance estimation but increases computational cost (O(Md²) per step)
  - Tighter confidence bounds (smaller βk) require more conservative exploration
  - Choice of α = H⁻¹/² and γ = d⁻¹/⁴ balances uncertainty terms but may need tuning for specific problems

- Failure signatures:
  - Slow decrease in trajectory uncertainty suggests poor exploration or feature misspecification
  - Large confidence bounds indicate insufficient data or inappropriate weight scaling
  - Suboptimal planning performance despite sufficient samples suggests estimation bias

- First 3 experiments:
  1. Verify uncertainty decreases monotonically with episodes on a simple linear mixture MDP
  2. Test sensitivity to M parameter by varying it and measuring sample complexity
  3. Compare exploration efficiency against Chen et al. (2021) baseline on a benchmark task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can horizon-free reward-free RL algorithms be extended to other function approximation settings beyond linear mixture MDPs?
- Basis in paper: [explicit] Paper specifically focuses on linear mixture MDPs and achieves horizon-free results in this setting
- Why unresolved: Demonstrates optimality for linear mixture MDPs but doesn't explore generalization to other function approximation classes like neural networks
- What evidence would resolve it: Development of horizon-free algorithms for other function approximation settings, or proofs that such horizon-free algorithms cannot exist for certain classes

### Open Question 2
- Question: What is the fundamental relationship between reward-free exploration and standard RL in terms of sample complexity?
- Basis in paper: [explicit] Authors note "reward-free MDP is more difficult than non-reward-free MDP by definitions" and provide lower bounds
- Why unresolved: While paper provides lower bounds showing reward-free RL requires at least as many samples as standard RL, exact quantitative relationship and whether gap can be closed for specific settings remains unclear
- What evidence would resolve it: Either matching upper and lower bounds for specific settings, or proof that certain gap is unavoidable

### Open Question 3
- Question: How do high-order moment estimation techniques perform in practice compared to simpler methods?
- Basis in paper: [explicit] Paper introduces novel high-order moment estimator as key technical contribution, claiming it provides more accurate estimation than previous methods
- Why unresolved: Paper provides theoretical analysis but doesn't include empirical comparisons showing practical benefits versus simpler uncertainty estimation methods
- What evidence would resolve it: Empirical studies comparing high-order moment estimation with alternative uncertainty estimation techniques on benchmark RL problems

## Limitations
- Algorithm relies heavily on linear mixture MDP assumption which may not hold in many practical settings
- Computational complexity scales with number of high-order moments M, potentially making algorithm impractical for very small ε targets
- Requires optimization oracle to find uncertainty-maximizing trajectories, which may not be efficiently implementable in all scenarios

## Confidence

- High confidence: Horizon-free sample complexity bound of Õ(d²ε⁻²) and its matching lower bound are well-supported by theoretical analysis in Theorems 5.1 and 5.2
- Medium confidence: Effectiveness of high-order moment estimator for uncertainty quantification is theoretically sound but relies on several assumptions about feature distributions and boundedness conditions
- Low confidence: Practical implementation details for optimization oracle and computational feasibility for large-scale problems are not fully specified

## Next Checks

1. Implement high-order moment estimator on simple linear mixture MDP and verify uncertainty estimates decrease monotonically with episodes while maintaining required confidence bounds
2. Test algorithm's sensitivity to M parameter by varying it on benchmark task and measuring both sample complexity and computational runtime to identify optimal trade-off
3. Compare exploration efficiency against Chen et al. (2021) baseline on standard reward-free RL benchmark, measuring both number of samples needed and quality of policies obtained during planning phase