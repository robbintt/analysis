---
ver: rpa2
title: Attending to Graph Transformers
arxiv_id: '2302.04181'
source_url: https://arxiv.org/abs/2302.04181
tags:
- graph
- graphs
- transformer
- attention
- encodings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive taxonomy of graph transformers,
  categorizing architectures based on their input features, tokenization strategies,
  and propagation mechanisms. The authors demonstrate that graph transformers can
  recover various graph properties, handle heterophilic graphs effectively, and mitigate
  over-squashing to some extent.
---

# Attending to Graph Transformers

## Quick Facts
- arXiv ID: 2302.04181
- Source URL: https://arxiv.org/abs/2302.04181
- Reference count: 12
- One-line primary result: Graph transformers can bypass over-squashing and handle heterophilic graphs, but require structural/positional encodings and face scalability limitations.

## Executive Summary
This survey provides a comprehensive taxonomy of graph transformers (GTs), categorizing architectures based on their input features, tokenization strategies, and propagation mechanisms. The authors demonstrate that graph transformers can recover various graph properties, handle heterophilic graphs effectively, and mitigate over-squashing to some extent. They show that GTs outperform traditional graph neural networks on heterophilic datasets and can perfectly fit the training set on the NEIGHBORS MATCH problem, which is challenging for GNNs. However, they also highlight that graph transformers still face limitations in terms of expressivity and generalization to larger graphs. The survey emphasizes the importance of structural and positional encodings in enhancing the performance of graph transformers.

## Method Summary
The survey analyzes graph transformers through a systematic taxonomy and empirical evaluation on various tasks including structural awareness (edge detection, triangle counting, CSL classification), heterophilic node classification, and the NEIGHBORS MATCH problem for over-squashing. Experiments compare different GT architectures with various structural/positional encodings (LapPE, RWSE, DEG) against GNN baselines (GIN, GCN) on benchmark datasets including ZINC, TRIANGLES, CSL, and heterophilic datasets (ACTOR, CORNELL, TEXAS, WISCONSIN, CHAMELEON, SQUIRREL). The method involves implementing or adapting GT and GNN models with different encoding strategies, training them with specified hyperparameters, and analyzing performance across tasks requiring different levels of structural awareness.

## Key Results
- Graph transformers outperform GNNs on heterophilic datasets, with hybrid architectures (GPS) showing the best performance
- GTs can perfectly fit the NEIGHBORS MATCH training set, demonstrating their ability to handle over-squashing
- Without structural and positional encodings, graph transformers are equivalent to DeepSets and cannot distinguish non-isomorphic graphs
- The quadratic computational complexity of full attention limits GT scalability to truly large graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph transformers can bypass over-squashing by modeling long-range interactions directly.
- Mechanism: Unlike GNNs, which aggregate information through bounded hops and suffer from exponentially growing receptive fields, GTs use global attention to directly connect any two nodes in a single layer, avoiding the bottleneck.
- Core assumption: The attention mechanism can effectively capture long-range dependencies without excessive noise or computational cost.
- Evidence anchors:
  - [abstract]: "many papers speculate that GTs [...] do not suffer from such effects as they aggregate information over all nodes in a given graph and hence are not limited to local structure bias."
  - [section]: "Our results demonstrate that the ability of transformers to model long-range interactions between nodes can circumvent the problem posed by Alon and Yahav [2021]."
  - [corpus]: Weak corpus evidence; only one neighbor paper mentions over-squashing, and no citations exist yet.
- Break condition: If the graph is extremely large, the quadratic cost of full attention becomes prohibitive, forcing approximation that may reintroduce bottlenecks.

### Mechanism 2
- Claim: Structural and positional encodings are necessary for GTs to capture non-trivial graph structure.
- Mechanism: Without encodings, a GT is equivalent to a DeepSets model and can only distinguish graphs by node count. Encodings inject structural or positional information that the attention mechanism can leverage.
- Core assumption: The encodings are informative and distinguishable for different graph structures.
- Evidence anchors:
  - [abstract]: "However, to make GTs aware of graph structure, one has to equip them with so-called structural and postional encodings."
  - [section]: "GTs can only become maximal expressive, i.e., universal function approximators, if they have access to maximally expressive structural bias."
  - [corpus]: No direct corpus evidence; the cited paper on enhancing GTs with spectral tokens is not yet available in the corpus.
- Break condition: If encodings are poorly designed or too simplistic, they may not provide enough discriminative power, causing GTs to underperform.

### Mechanism 3
- Claim: Hybrid architectures combining local GNN layers with global transformer attention can improve performance on heterophilic graphs.
- Mechanism: GNN layers capture local structure and community patterns, while transformer attention captures long-range dependencies that are crucial in heterophilic settings where neighbors may not share labels.
- Core assumption: Both local and global information are relevant and complementary for the task.
- Evidence anchors:
  - [abstract]: "Graph transformers have already shown promising performance [...] often attributed to their ability to circumvent graph neural networks’ shortcomings, such as over-smoothing and over-squashing."
  - [section]: "Adding global attention to the GCN, i.e., following the GPS model, universally improves the performance. Most interestingly, disabling the local GCN in GPS, i.e., becoming the Transformer model, increases the performance even further."
  - [corpus]: Weak corpus evidence; only one neighbor paper mentions combining GNNs with transformers, but lacks detail.
- Break condition: If the graph is highly homophilic, the extra complexity of hybrid models may not yield benefits over simpler GNNs.

## Foundational Learning

- Concept: Graph isomorphism and its relation to expressiveness
  - Why needed here: Understanding why GTs require encodings hinges on the graph isomorphism problem and the limitations of permutation-invariant functions.
  - Quick check question: Can a transformer without any structural bias distinguish two non-isomorphic graphs of the same size?

- Concept: Positional encodings vs. structural encodings
  - Why needed here: These two types of encodings serve different purposes; confusing them leads to suboptimal architecture choices.
  - Quick check question: What is the key difference between making a node aware of its local degree (structural) versus its shortest-path distance to a hub (positional)?

- Concept: Over-smoothing and over-squashing in GNNs
  - Why needed here: These are the primary issues GTs aim to solve; understanding them clarifies when GTs are beneficial.
  - Quick check question: Why does stacking too many GNN layers lead to over-smoothing, and how does this differ from over-squashing?

## Architecture Onboarding

- Component map: Input features → Tokenization (nodes/edges/subgraphs) → Positional/structural encodings → Attention mechanism (global/sparse/hybrid) → Output head
- Critical path: Ensure encodings are computed before attention; attention matrix must be materialized or approximated efficiently; output head must respect task equivariance/invariance.
- Design tradeoffs: Full attention gives best performance but O(n²) cost; sparse or kernelized attention reduces cost but may lose expressiveness; hybrid models balance both but add complexity.
- Failure signatures: Poor performance on triangle counting or edge detection indicates missing or ineffective encodings; degradation on large graphs suggests scalability bottlenecks; failure on heterophilic data may mean local patterns are undervalued.
- First 3 experiments:
  1. Compare a vanilla transformer (no encodings) vs. one with Laplacian positional encodings on the EDGES task to verify encoding necessity.
  2. Evaluate a hybrid GPS model vs. pure transformer on a heterophilic dataset to confirm the benefit of local GNN layers.
  3. Test sparse attention (e.g., GKAT) on a large molecular dataset to assess scalability without major accuracy loss.

## Open Questions the Paper Calls Out

- Question: How can graph transformers be made more expressive to distinguish non-isomorphic graphs without relying on maximally expressive structural encodings?
  - Basis in paper: [explicit] "GTs can only become maximal expressive... if they have access to maximally expressive structural bias... However, this is equivalent to solving the graph isomorphism problem."
  - Why unresolved: The graph isomorphism problem is computationally hard, and current GTs struggle with limited expressivity even with structural encodings.
  - What evidence would resolve it: Development of new graph transformer architectures or encodings that achieve high expressivity without requiring maximally expressive structural encodings, demonstrated through empirical results on graph isomorphism testing tasks.

- Question: What is the most effective way to incorporate edge features into graph transformer architectures?
  - Basis in paper: [explicit] "it still needs to be determined how best to incorporate edge-feature information into GT architectures."
  - Why unresolved: While node features are commonly used, the integration of edge features remains an open challenge due to the nature of the attention mechanism in transformers.
  - What evidence would resolve it: Comparative studies of different methods for incorporating edge features into graph transformers, evaluated on benchmark datasets with edge features, showing significant improvements in performance.

- Question: How can graph transformers be scaled to handle truly large graphs typical in real-world node-level prediction tasks?
  - Basis in paper: [explicit] "one major downside of GTs is their quadratic running time in the number of nodes, preventing them from scaling to truly large graphs."
  - Why unresolved: The quadratic computational complexity of standard graph transformers is a significant limitation for large-scale applications.
  - What evidence would resolve it: Successful implementations of graph transformers that can efficiently process large graphs (e.g., millions of nodes) while maintaining competitive performance, demonstrated through experiments on real-world large-scale graph datasets.

## Limitations

- The claims about over-squashing mitigation are primarily based on controlled experiments with specific datasets and may not generalize to all graph types
- The necessity of structural and positional encodings is demonstrated theoretically, but the optimal encoding design for different graph types remains an open question
- The quadratic computational complexity of full attention limits GT scalability to truly large graphs, requiring approximation methods that may reintroduce bottlenecks

## Confidence

- **High confidence**: The demonstration that vanilla transformers without structural bias are equivalent to DeepSets and cannot distinguish non-isomorphic graphs of the same size. The experimental evidence showing graph transformers' superior performance on heterophilic datasets compared to GNNs is also highly reliable.
- **Medium confidence**: The claims about over-squashing mitigation are supported by specific experiments but rely on assumptions about attention mechanisms' ability to capture long-range dependencies effectively. The hybrid architecture benefits are demonstrated but may vary significantly across different graph domains and tasks.
- **Low confidence**: The expressivity claims regarding universal approximation depend heavily on the quality and informativeness of structural encodings, which are not standardized across implementations. The survey's broader claims about graph transformers replacing GNNs entirely lack sufficient empirical support across diverse real-world applications.

## Next Checks

1. **Scalability validation**: Test graph transformers with sparse attention mechanisms (e.g., GKAT or GraphGPS) on graphs with 100K+ nodes to verify whether over-squashing mitigation holds at scale, and measure the trade-off between approximation quality and computational efficiency.

2. **Encoding sensitivity analysis**: Systematically vary the quality and type of structural encodings (Laplacian vs. random walk vs. degree-based) across multiple graph families to determine which encoding strategies are most critical for different graph properties and tasks.

3. **Heterophily boundary conditions**: Design experiments that systematically vary the homophily ratio in synthetic graphs to identify the precise threshold where graph transformers outperform GNNs, and determine whether hybrid architectures provide consistent benefits across this spectrum.