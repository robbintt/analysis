---
ver: rpa2
title: 'GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image
  Pre-training'
arxiv_id: '2308.11331'
source_url: https://arxiv.org/abs/2308.11331
tags:
- growth
- learning
- step
- image
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adapting cross-modal pre-trained
  models to continuously growing image-text data. The authors propose GrowCLIP, a
  data-aware automatic model growing algorithm that dynamically adjusts the model
  architecture as more data becomes available.
---

# GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training

## Quick Facts
- **arXiv ID**: 2308.11331
- **Source URL**: https://arxiv.org/abs/2308.11331
- **Reference count**: 40
- **Primary result**: Improves zero-shot image classification accuracy by up to 2.3% and zero-shot image retrieval recall by 1.2% compared to existing methods

## Executive Summary
GrowCLIP addresses the challenge of adapting cross-modal pre-trained models to continuously growing image-text data by introducing a data-aware automatic model growing algorithm. The method dynamically adjusts model architecture as more data becomes available through a combination of dynamic growth space expansion, parameter inheriting with momentum (PIM), and a shared encoder architecture. Experiments demonstrate significant improvements in zero-shot image classification and retrieval tasks compared to fixed-architecture baselines.

## Method Summary
GrowCLIP operates through a multi-step process where model architecture grows in response to increasing data volumes. At each growth step, the method uses parameter inheriting with momentum to initialize new model parameters while preserving knowledge from previous steps. A supernet is fine-tuned for two epochs, followed by growth architecture selection (GAS) to identify optimal subnetworks within the expanded search space. The shared encoder architecture allows image and text modalities to share transformer layers (except layernorm), reducing parameters while enhancing cross-modal fusion. The process repeats as data continues to grow, with each iteration building upon the previously trained model.

## Key Results
- Zero-shot image classification accuracy improved by up to 2.3% across 9 downstream tasks compared to fixed-architecture baselines
- Zero-shot image retrieval recall improved by 1.2% on Flickr30K and MSCOCO datasets
- Dynamic architecture growth consistently outperformed static models of equivalent parameter count
- Shared encoder architecture reduced model parameters while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic architecture growth adapts model capacity to data size
- Mechanism: The growth space expands transformer blocks, heads, and convolutional layers proportionally to incoming data volume
- Core assumption: Larger datasets require more complex architectures for optimal performance
- Evidence anchors:
  - [abstract] "we adopt a dynamic growth space and seek out the optimal architecture at each growth step to adapt to online learning scenarios"
  - [section] "the size of the training dataset is related to the optimal architecture selection" and "when data grows, the small-scale architecture may limit the final performance due to the limited parameters"
  - [corpus] Weak - no direct evidence in corpus neighbors about dynamic architecture scaling
- Break condition: If growth steps don't correlate with meaningful data increases, or if optimal architecture doesn't change with data volume

### Mechanism 2
- Claim: Parameter inheriting with momentum prevents local minimum issues
- Mechanism: New layers inherit parameters from previous model's corresponding layers, with momentum term balancing exploration vs exploitation
- Core assumption: Direct parameter copying creates local minima, but random initialization loses valuable knowledge
- Evidence anchors:
  - [abstract] "we employ parameter inheriting with momentum (PIM) to maintain the previous knowledge and address the issue of the local minimum dilemma"
  - [section] "the performance of model trained from scratch is much better than trained with pre-training... This is probably caused by the influence of the inheriting parameters trained with the previous smaller dataset"
  - [corpus] Weak - corpus doesn't address parameter inheriting strategies
- Break condition: If momentum term is misconfigured, leading to either insufficient exploration or loss of prior knowledge

### Mechanism 3
- Claim: Shared encoder enhances cross-modal fusion while reducing parameters
- Mechanism: Image and text encoders share transformer layers (except layernorm), forcing early interaction between modalities
- Core assumption: Cross-modal fusion benefits from shared representations rather than separate processing
- Evidence anchors:
  - [section] "we propose a shared encoderh(·), in which the image and text share the transformers except layernorm layers. The shared encoder not only can reduce model parameters, but also enhance the degree of cross-modal fusion"
  - [corpus] Weak - corpus neighbors don't discuss shared encoder architectures
- Break condition: If shared parameters become too specialized to one modality, hurting the other modality's representation quality

## Foundational Learning

- Concept: Contrastive learning objective
  - Why needed here: The model learns by maximizing similarity between matching image-text pairs while minimizing similarity with non-matching pairs
  - Quick check question: What is the mathematical form of the contrastive loss used in CLIP models?

- Concept: Neural architecture search (NAS)
  - Why needed here: GrowCLIP needs to efficiently search optimal architectures within the dynamic growth space
  - Quick check question: How does one-shot NAS differ from traditional NAS in terms of computational efficiency?

- Concept: Online learning vs continual learning
  - Why needed here: The paper distinguishes between online learning (previous data accessible) and continual learning (data disappears)
  - Quick check question: What is the key difference between online learning and continual learning settings in terms of data accessibility?

## Architecture Onboarding

- Component map:
  Image encoder (CNN + Transformers) -> Shared encoder (Transformers) -> Contrastive loss layer
  Text encoder (Transformers) -> Shared encoder (Transformers) -> Contrastive loss layer

- Critical path:
  1. PIM initialization of supernet at new growth step
  2. Supernet fine-tuning (2 epochs)
  3. Growth Architecture Selection (GAS) - train subnets then evaluate
  4. Selected model training with PIM initialization
  5. Final model evaluation

- Design tradeoffs:
  - Shared encoder reduces parameters but may hurt modality-specific feature extraction
  - Dynamic growth space increases search complexity but enables data-adaptive scaling
  - PIM balances knowledge retention vs exploration but requires careful momentum tuning

- Failure signatures:
  - Training instability or collapse during supernet training
  - GAS consistently selecting architectures that underperform compared to fixed baselines
  - Performance degradation when moving between growth steps

- First 3 experiments:
  1. Verify supernet training converges and subnets can be sampled
  2. Test GAS selection on a small validation set to ensure it picks reasonable architectures
  3. Run a single growth step with PIM and compare to training from scratch baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GrowCLIP scale with different growth step intervals (e.g., 2 vs. 4 steps) and what is the optimal interval for maximizing accuracy gains?
- Basis in paper: [inferred] The paper divides CC12M into 4 growth steps but does not explore alternative interval strategies or compare their impact on final performance.
- Why unresolved: The paper only evaluates one fixed growth interval (4 steps) without testing how performance varies with different numbers of growth steps or more gradual/rapid growth schedules.
- What evidence would resolve it: Experiments comparing final zero-shot classification accuracy and retrieval metrics across different growth step intervals (2, 4, 8 steps) using the same dataset would clarify the optimal growth schedule.

### Open Question 2
- Question: How does GrowCLIP perform on real-world continuously growing datasets compared to controlled synthetic growth scenarios like CC12M?
- Basis in paper: [explicit] The paper acknowledges that GrowCLIP is currently only validated on CC12M and notes the need to extend it to real-world scenarios where VLP models keep training with constantly updated web data.
- Why unresolved: The paper's experiments are limited to a static, pre-divided dataset rather than actual streaming data that grows over time in practice.
- What evidence would resolve it: Deploying GrowCLIP on a live web-crawled dataset that continuously grows over time, comparing performance metrics against baseline models, would demonstrate real-world effectiveness.

### Open Question 3
- Question: What is the optimal trade-off between exploitation (maintaining existing knowledge) and exploration (re-initializing parameters) in the PIM mechanism for different growth scenarios?
- Basis in paper: [explicit] The paper introduces PIM with hyperparameters β and γ for balancing exploitation and exploration, but does not provide a systematic analysis of how these parameters should be tuned for different data growth rates or model sizes.
- Why unresolved: While PIM is proposed as a solution to the local minimum dilemma, the paper does not explore sensitivity to these hyperparameters or provide guidance on setting them optimally.
- What evidence would resolve it: A comprehensive ablation study varying β and γ across different growth step sizes and data volumes, measuring performance impact, would identify optimal parameter settings.

## Limitations

- The paper doesn't specify how the growth space is constructed or what architectural dimensions are considered at each step
- The evaluation compares against fixed-architecture baselines but doesn't compare against other adaptive methods that could handle growing data
- PIM hyperparameters (β=0.3, γ=0.001) appear tuned for this specific setup; their generalizability to different model sizes or data distributions is untested

## Confidence

- **High confidence**: The core mechanism of parameter inheriting with momentum is technically sound and well-explained. The concept of sharing encoders to reduce parameters while maintaining cross-modal fusion is clearly articulated.
- **Medium confidence**: The claim that dynamic architecture growth outperforms fixed architectures needs more rigorous ablation studies, particularly to isolate the contribution of shared encoders versus dynamic scaling.
- **Low confidence**: The 2.3% zero-shot classification improvement is impressive but reported without statistical significance tests or comparison to the most recent CLIP variants that might use different training strategies.

## Next Checks

1. Re-run the ablation study removing the shared encoder component to quantify its exact contribution versus the dynamic architecture selection
2. Test PIM with different momentum values (β∈[0.1, 0.5]) to establish sensitivity and optimal configuration range
3. Compare GrowCLIP against a baseline that simply increases model size proportionally to data growth without any architectural search