---
ver: rpa2
title: Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial
  Attacks
arxiv_id: '2307.02828'
source_url: https://arxiv.org/abs/2307.02828
tags:
- gradient
- uni00000013
- adversarial
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Sampling-based Fast Gradient Rescaling Method
  (S-FGRM) to enhance the transferability of adversarial examples in black-box attacks.
  The key idea is to replace the commonly used sign function in gradient-based attacks
  with a data rescaling method that retains the disparities in gradient values.
---

# Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks

## Quick Facts
- arXiv ID: 2307.02828
- Source URL: https://arxiv.org/abs/2307.02828
- Reference count: 38
- Primary result: S-FGRM achieves up to 37% higher attack success rates in single-model settings and up to 48% higher success rates in ensemble-model settings compared to state-of-the-art baselines on ImageNet

## Executive Summary
This paper introduces a Sampling-based Fast Gradient Rescaling Method (S-FGRM) to enhance the transferability of adversarial examples in black-box attacks. The key innovation replaces the commonly used sign function in gradient-based attacks with a data rescaling method that preserves relative gradient magnitudes. Additionally, a Depth First Sampling method stabilizes gradient updates by averaging over perturbed inputs. The proposed method significantly outperforms state-of-the-art baselines on the ImageNet dataset, achieving substantial improvements in both single-model and ensemble-model attack scenarios.

## Method Summary
S-FGRM combines two core components: Fast Gradient Rescaling Method (FGRM) and Depth First Sampling Method (DFSM). FGRM replaces the sign function with a data rescaling approach that uses log transformation and normalization to preserve gradient magnitude disparities. DFSM samples N points around the input image using a depth-first strategy, averaging gradients from these samples with the original gradient to stabilize updates. The method is integrated into existing attack frameworks like MI-FGSM and NI-FGSM, with key parameters including max perturbation ε=16, iterations T=10, step size α=1.6, sampling N=12, sampling range β=1.5, and rescale factor c=2.

## Key Results
- S-FGRM achieves up to 37% higher attack success rates in single-model settings compared to state-of-the-art baselines
- S-FGRM achieves up to 48% higher attack success rates in ensemble-model settings
- The method demonstrates consistent improvements across normally trained and adversarially trained models including Inc-v3, Inc-v4, IncRes-v2, and Res-101

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using data rescaling (instead of sign) preserves relative gradient magnitudes, improving gradient update direction accuracy.
- Mechanism: The `rescale(g) = c * sign(g) ⊙ f(norm(log2|g|))` function applies a log transformation to scatter small gradient values, then normalizes and smooths them with a sigmoid. This retains the original gradient's relative scale while maintaining monotonicity and sufficient perturbation magnitude.
- Core assumption: The distribution of original gradients is approximately symmetric around zero and the log transformation meaningfully spreads closely packed small values.
- Evidence anchors:
  - [abstract] "We observe that the deviation between the original gradient and the generated noise may lead to inaccurate gradient update estimation"
  - [section] "For instance, if two dimensions of the actual gradient are (0.8, 10−8), the sign gradient will be (1, 1) while (1, 0) is a more accurate approximation."
  - [corpus] Weak: No direct corpus neighbor explicitly validates this specific log-normalization approach, but the general idea of improving gradient direction accuracy appears in related works.
- Break condition: If the original gradient distribution is highly asymmetric or contains many zero values, the log transformation could produce undefined or noisy outputs.

### Mechanism 2
- Claim: Depth First Sampling (DFS) stabilizes the rescaling process by averaging gradients over a trajectory of perturbed inputs rather than a single point.
- Mechanism: At each iteration, N samples are drawn in a local neighborhood around the current point, but each new sample is centered on the previous sample (depth-first), not the original image. The gradients from all samples (including the original) are averaged to produce a more stable update direction.
- Core assumption: Sampling around a trajectory reduces overfitting to a single decision boundary and mitigates the effect of local gradient fluctuations.
- Evidence anchors:
  - [abstract] "We further propose a Depth First Sampling method to eliminate the fluctuation of rescaling and stabilize the gradient update."
  - [section] "By sampling around the input image, the negative influence of small value fluctuations will be alleviated."
  - [corpus] No explicit neighbor validation; the method is novel within the corpus.
- Break condition: If the sampling radius is too large, the method may step outside the effective adversarial perturbation space, reducing attack efficacy.

### Mechanism 3
- Claim: Combining rescaling and DFS sampling mitigates the limitations of sign-based gradient updates in boosting transferability.
- Mechanism: The rescaling component provides a more accurate gradient direction, while DFS smooths out local noise. Together, they reduce overfitting to the surrogate model and produce adversarial examples that generalize better across models.
- Core assumption: Improved gradient approximation and stability translate directly into higher transferability metrics on black-box models.
- Evidence anchors:
  - [abstract] "Our method could be used in any gradient-based attacks and is extensible to be integrated with various input transformation or ensemble methods to further improve the adversarial transferability."
  - [section] "Our integrated methods mitigate the overfitting issue and further strengthen various gradient-based attacks."
  - [corpus] Weak: While related works exist, none in the corpus specifically validate the combination of rescaling + DFS for transferability.
- Break condition: If the target models have very different gradient landscapes, even improved gradient approximation may not yield significant transferability gains.

## Foundational Learning

- Concept: Gradient-based adversarial attacks (FGSM, I-FGSM, MI-FGSM, NI-FGSM)
  - Why needed here: S-FGRM is designed to replace the sign function in these attacks; understanding their update rules is essential to integrate S-FGRM correctly.
  - Quick check question: What is the update rule for MI-FGSM, and how does it differ from I-FGSM?

- Concept: Transferability in black-box attacks
  - Why needed here: The paper's performance gains are measured by transferability success rates across different models; understanding what affects transferability is key to interpreting results.
  - Quick check question: Why do adversarial examples often fail to transfer between architectures?

- Concept: Logarithmic scaling and normalization
  - Why needed here: The rescale function uses `log2(|g|)` and normalization; understanding how this affects gradient distributions is critical for debugging or tuning.
  - Quick check question: What happens to gradient values that are exactly zero when passed through `log2(|g|)`?

## Architecture Onboarding

- Component map: Input image x -> Compute gradient ∇J(x, y) -> Sample N points with DFS -> Average gradients -> Apply rescale(g) -> Update xadv with rescaled gradient

- Critical path:
  1. Compute gradient ∇J(x, y)
  2. Sample N points around x using DFS strategy
  3. Average gradients from samples + original
  4. Apply rescale(g) to averaged gradient
  5. Update xadv with rescaled gradient and step size α

- Design tradeoffs:
  - Sampling N adds computation but stabilizes updates; tuning N vs. performance is key.
  - Rescale factor c controls perturbation magnitude; too high risks clipping, too low may under-attack.
  - DFS sampling radius β balances local vs. global exploration; too small may not help, too large may diverge.

- Failure signatures:
  - If attack success rate drops sharply, check if sampling radius β is too large or c is clipping gradients.
  - If performance gain is minimal, verify that log transformation is not producing NaNs (e.g., from zero gradients).
  - If runtime is too high, reduce N or simplify the rescale function.

- First 3 experiments:
  1. Run SMI-FGRM with N=0 (no sampling) on a single model; compare to MI-FGSM to isolate rescaling effect.
  2. Run with N=12, β=1.5, c=2; compare to MI-FGSM + CTM to see combined impact.
  3. Vary c from 1 to 5; plot attack success rate vs. rescale factor to find optimal c.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the S-FGRM method perform on other datasets beyond ImageNet, such as CIFAR-10 or real-world data?
- Basis in paper: [inferred] The paper focuses on experiments using the ImageNet dataset and does not explore other datasets.
- Why unresolved: The paper does not provide evidence of S-FGRM's performance on other datasets, leaving its generalizability unclear.
- What evidence would resolve it: Testing S-FGRM on various datasets and comparing its performance with other methods.

### Open Question 2
- Question: What is the impact of different hyperparameters (e.g., sampling number N, rescale factor c) on the performance of S-FGRM?
- Basis in paper: [explicit] The paper mentions a parameter study but does not explore the impact of all hyperparameters in detail.
- Why unresolved: The paper does not provide a comprehensive analysis of how different hyperparameters affect S-FGRM's performance.
- What evidence would resolve it: Conducting experiments with various hyperparameter settings and analyzing their impact on S-FGRM's performance.

### Open Question 3
- Question: How does S-FGRM compare to other state-of-the-art adversarial attack methods in terms of computational efficiency?
- Basis in paper: [inferred] The paper focuses on the effectiveness of S-FGRM but does not provide a detailed comparison of computational efficiency with other methods.
- Why unresolved: The paper does not provide evidence of S-FGRM's computational efficiency compared to other methods, leaving its practicality unclear.
- What evidence would resolve it: Comparing the computational efficiency of S-FGRM with other state-of-the-art methods using benchmarks and metrics.

## Limitations
- The rescale function's log transformation could produce undefined outputs (log(0)) when gradients contain zero values, though the paper doesn't specify the handling of this edge case
- DFS sampling introduces additional hyperparameters (N, β) that require careful tuning, and the paper doesn't explore sensitivity to these parameters
- The method's performance on datasets other than ImageNet or on different model architectures remains unverified

## Confidence

- **High confidence** in the empirical results showing improved transferability on the tested ImageNet setup
- **Medium confidence** in the theoretical justification for why gradient rescaling improves update accuracy
- **Medium confidence** in DFS sampling's contribution to stability, as the mechanism is novel and lacks corpus validation

## Next Checks

1. Test S-FGRM with various rescale factors (c=1,2,5) on a subset of ImageNet to quantify the sensitivity of attack success to this parameter
2. Implement a variant with DFS sampling disabled (N=0) to isolate the contribution of gradient rescaling alone
3. Evaluate the method on a non-ImageNet dataset (e.g., CIFAR-10) to assess generalizability across different image domains