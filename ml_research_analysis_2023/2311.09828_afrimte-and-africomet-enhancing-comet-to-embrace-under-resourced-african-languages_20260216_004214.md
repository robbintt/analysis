---
ver: rpa2
title: 'AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced African
  Languages'
arxiv_id: '2311.09828'
source_url: https://arxiv.org/abs/2311.09828
tags:
- translation
- languages
- african
- evaluation
- eng-yor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating machine translation
  quality for under-resourced African languages, where existing metrics like BLEU
  have poor correlation with human judgments. The authors create AfriMTE, a high-quality
  human evaluation dataset with simplified Multidimensional Quality Metrics (MQM)
  guidelines and Direct Assessment (DA) scoring for 13 typologically diverse African
  languages.
---

# AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced African Languages

## Quick Facts
- arXiv ID: 2311.09828
- Source URL: https://arxiv.org/abs/2311.09828
- Reference count: 16
- Primary result: Developed AfriCOMET, achieving 0.406 Spearman correlation with human judgments for African language MT evaluation

## Executive Summary
This paper addresses the critical challenge of evaluating machine translation quality for under-resourced African languages, where traditional metrics like BLEU show poor correlation with human judgments. The authors introduce AfriMTE, a high-quality human evaluation dataset created using simplified Multidimensional Quality Metrics (MQM) guidelines and Direct Assessment (DA) scoring for 13 typologically diverse African languages. Building on this dataset, they develop AfriCOMET, a COMET evaluation metric specifically designed for African languages by leveraging transfer learning from well-resourced languages using an African-centric multilingual encoder (AfroXLM-Roberta-large). AfriCOMET establishes a new state-of-the-art for African language MT evaluation with a Spearman-rank correlation of 0.406 with human judgments.

## Method Summary
The study combines simplified MQM guidelines with Direct Assessment scoring to create AfriMTE, a human evaluation dataset for 13 African languages. Non-expert evaluators first identify error spans in translations using the simplified guidelines, then assign continuous DA scores. The authors develop AfriCOMET by fine-tuning pre-trained multilingual models on WMT DA datasets from well-resourced languages, using AfroXLM-Roberta-large as the encoder. The model is trained to predict normalized scores (0-1) from source, translation, and reference triplets, with validation on AfriMTE dev sets. The approach also explores reference-free quality estimation by adapting the COMET architecture.

## Key Results
- AfriCOMET achieves Spearman correlation of 0.406 with human judgments, establishing new state-of-the-art for African language MT evaluation
- AfroXLM-Roberta-large outperforms other pre-trained models (XLM-R, InfoXLM, AfriBERTa) in transfer learning from well-resourced to African languages
- Simplified MQM guidelines with error span highlighting improve annotation efficiency while maintaining reasonable inter-annotator agreement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The simplified MQM guideline improves DA scoring reliability by reducing cognitive load on non-expert evaluators.
- Mechanism: By breaking the evaluation process into two clear steps—first identifying error spans, then assigning a continuous DA score—the guideline provides concrete anchors for judgment, making the process more systematic and less subjective.
- Core assumption: Non-expert evaluators can reliably identify error spans even if they struggle with abstract severity ratings.
- Evidence anchors:
  - [abstract]: "we introduced a more simplified version harmonized with the principles of Direct Assessment, specifically adapted to suit non-expert evaluators."
  - [section]: "To address this issue, we develop a novel approach that combines the strengths of MQM and DA annotations. We propose a simplified version of the MQM guidelines, designed to be more accessible to non-expert evaluators."
  - [corpus]: Weak. The corpus shows related work on African MT evaluation but does not directly validate the simplified guideline's effectiveness.
- Break condition: If evaluators still struggle to consistently identify error spans, the two-step process may not yield reliable DA scores.

### Mechanism 2
- Claim: Transfer learning from high-resource languages to African languages is feasible using AfroXLM-Roberta-large.
- Mechanism: The model's pre-training on a diverse multilingual corpus, including some African languages, provides a foundation that can be fine-tuned with DA data from well-resourced languages to perform well on unseen African languages.
- Core assumption: The linguistic features learned during pre-training are transferable to African languages, even if they weren't explicitly included in the pre-training data.
- Evidence anchors:
  - [abstract]: "we develop AfriCOMET: COMET evaluation metrics for African languages by leveraging DA training data from well-resourced languages and an African-centric multilingual encoder (AfroXLM-R)."
  - [section]: "AfroXLM-R-L demonstrates a promising ability to transfer learning from other languages to African languages. It achieves the highest Spearman-rank correlation among various pre-trained models considered on a considerable portion of language pairs."
  - [corpus]: Weak. The corpus lists related work on African NLP but does not provide direct evidence for the effectiveness of AfroXLM-Roberta-large in this transfer learning scenario.
- Break condition: If the model fails to capture the unique linguistic features of African languages, transfer learning will not yield improved evaluation metrics.

### Mechanism 3
- Claim: Combining error span highlighting with DA scoring enhances the quality of human evaluation data.
- Mechanism: Error span highlighting forces evaluators to focus on specific translation issues, leading to more informed and consistent DA score assignments. The two-step process reduces the impact of subjective interpretation on the final score.
- Core assumption: Evaluators who identify specific errors are more likely to assign accurate overall quality scores than those who only provide holistic judgments.
- Evidence anchors:
  - [abstract]: "To address the complexities associated with the MQM framework, we introduced a more simplified version harmonized with the principles of Direct Assessment."
  - [section]: "The integration of MQM-based error detection aims to enhance the quality and accuracy of DA annotations, while making the process more manageable for non-expert evaluators."
  - [corpus]: Weak. The corpus does not provide direct evidence for the effectiveness of combining error span highlighting with DA scoring.
- Break condition: If evaluators find the two-step process cumbersome or confusing, it may lead to decreased annotation efficiency or quality.

## Foundational Learning

- Concept: Multidimensional Quality Metrics (MQM) framework
  - Why needed here: Understanding the original MQM framework is crucial for appreciating the need for simplification and the design of the new annotation guidelines.
  - Quick check question: What are the key components of the MQM framework, and how do they differ from the simplified version used in this study?

- Concept: Transfer learning in NLP
  - Why needed here: The study relies on transferring knowledge from high-resource languages to African languages, which is a fundamental concept in modern NLP.
  - Quick check question: How does transfer learning work in the context of multilingual language models, and what are the key factors that influence its success?

- Concept: Direct Assessment (DA) scoring
  - Why needed here: DA scoring is the primary method used for human evaluation in this study, and understanding its principles is essential for interpreting the results.
  - Quick check question: What are the advantages and disadvantages of DA scoring compared to other evaluation methods, such as ranking or Likert scales?

## Architecture Onboarding

- Component map: Simplified MQM guidelines -> DA scoring -> AfroXLM-R-L encoder -> COMET architecture -> Spearman correlation evaluation

- Critical path:
  1. Collect high-quality human evaluation data using simplified MQM guidelines and DA scoring
  2. Preprocess and normalize the data
  3. Fine-tune pre-trained models on the WMT Others dataset using the collected data for validation
  4. Evaluate the models on the AfriMTE devtest sets
  5. Develop reference-free QE models by adapting the COMET architecture

- Design tradeoffs:
  - Simplified MQM vs. original MQM: Simpler guidelines may reduce evaluator burden but might miss some nuanced errors
  - Transfer learning vs. training from scratch: Transfer learning leverages existing knowledge but may not capture language-specific features as well
  - Reference-based vs. reference-free evaluation: Reference-based methods are more accurate but require reference translations, while reference-free methods are more practical but may be less reliable

- Failure signatures:
  - Low inter-annotator agreement: Indicates that the guidelines or training process may need improvement
  - Poor correlation with human judgments: Suggests that the model is not capturing the relevant linguistic features or that the evaluation data is not representative
  - Overfitting to the training data: Model performs well on training data but poorly on unseen languages or domains

- First 3 experiments:
  1. Compare the simplified MQM guidelines with the original MQM framework by having a subset of evaluators use both and measuring inter-annotator agreement and correlation with human judgments
  2. Evaluate the impact of different pre-trained models (AfroXLM-Roberta-large, InfoXLM-large, AfriBERTa-large, XLM-Roberta-large) on the performance of the AfriCOMET metric
  3. Investigate the effect of including or excluding the WMT African dataset in the training data on the model's performance for African languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal approach to address the scarcity of evaluation data for under-resourced African languages in the context of machine translation evaluation?
- Basis in paper: [explicit] The paper discusses the challenges of evaluating machine translation quality for under-resourced African languages, particularly the lack of evaluation data with human ratings.
- Why unresolved: The paper introduces AfriMTE, a high-quality human evaluation dataset, but does not explore other potential methods for addressing the scarcity of evaluation data.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of different approaches (e.g., synthetic data generation, active learning, or leveraging existing resources) in addressing the data scarcity issue.

### Open Question 2
- Question: How does the performance of AfriCOMET vary across different African language families and typologies?
- Basis in paper: [explicit] The paper presents AfriCOMET, a COMET evaluation metric for African languages, but does not provide a detailed analysis of its performance across different language families and typologies.
- Why unresolved: The paper focuses on the overall performance of AfriCOMET but does not delve into its effectiveness for specific language families or typologies.
- What evidence would resolve it: A comprehensive analysis of AfriCOMET's performance across different African language families and typologies, highlighting any strengths or weaknesses.

### Open Question 3
- Question: What are the limitations of using simplified MQM guidelines for error detection in the context of machine translation evaluation for African languages?
- Basis in paper: [explicit] The paper introduces a simplified MQM guideline for error detection, but does not extensively discuss its limitations or potential drawbacks.
- Why unresolved: The paper focuses on the benefits of using simplified MQM guidelines but does not explore potential limitations or challenges associated with their implementation.
- What evidence would resolve it: A detailed analysis of the limitations and potential drawbacks of using simplified MQM guidelines, including their impact on evaluation accuracy and consistency.

## Limitations

- The AfriMTE dataset covers only 13 African languages, representing a small fraction of the continent's linguistic diversity
- The study relies on non-expert annotators whose agreement levels were moderate but varied across language pairs
- The transfer learning approach is limited by the relatively small size of available African language data (only 3 language pairs in WMT African vs. 112 in WMT Others)

## Confidence

- **High Confidence**: The core finding that AfroXLM-Roberta-large achieves superior transfer learning performance for African languages is well-supported by comparative experiments across multiple pre-trained models. The correlation results with human judgments (0.406 Spearman) are clearly reported and statistically significant.
- **Medium Confidence**: The effectiveness of the simplified MQM guidelines is supported by improved annotation efficiency, but the study lacks direct comparison with original MQM framework performance. The claim that combining error span highlighting with DA scoring improves data quality is plausible but not rigorously validated.
- **Low Confidence**: The assertion that AfroXLM-Roberta-large's training on 51 African languages directly explains its superior performance lacks empirical validation, as the paper doesn't analyze which specific language features contribute to improved evaluation metrics.

## Next Checks

1. **Inter-annotator Agreement Analysis**: Conduct detailed pairwise correlation analysis between annotators across different African language pairs to identify specific languages or error types where agreement is weakest, then refine guidelines accordingly.

2. **Cross-linguistic Transfer Study**: Systematically evaluate AfroXLM-Roberta-large's performance when trained on different subsets of African languages (e.g., Bantu vs. Afroasiatic families) to identify which language groups benefit most from transfer learning.

3. **Error Type Distribution Analysis**: Analyze the distribution of error types identified using simplified MQM guidelines to determine if certain error categories are systematically underrepresented compared to traditional MQM frameworks.