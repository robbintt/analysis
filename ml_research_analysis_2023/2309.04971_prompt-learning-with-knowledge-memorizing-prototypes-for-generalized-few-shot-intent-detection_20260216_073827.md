---
ver: rpa2
title: Prompt Learning With Knowledge Memorizing Prototypes For Generalized Few-Shot
  Intent Detection
arxiv_id: '2309.04971'
source_url: https://arxiv.org/abs/2309.04971
tags:
- intents
- knowledge
- learning
- seen
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Generalized Few-Shot Intent Detection
  (GFSID) task, which requires categorizing both seen and novel intents simultaneously
  with limited labeled data. The authors propose a two-stage learning framework that
  converts GFSID into a class incremental learning paradigm.
---

# Prompt Learning With Knowledge Memorizing Prototypes For Generalized Few-Shot Intent Detection

## Quick Facts
- arXiv ID: 2309.04971
- Source URL: https://arxiv.org/abs/2309.04971
- Reference count: 0
- This paper proposes a two-stage learning framework for Generalized Few-Shot Intent Detection (GFSID) that significantly outperforms previous methods on SNIPS and NLUE datasets.

## Executive Summary
This paper addresses the challenging task of Generalized Few-Shot Intent Detection (GFSID), where models must simultaneously classify both seen and novel intents with limited labeled data. The authors propose converting GFSID into a class incremental learning paradigm using a two-stage framework. In the first stage, the model learns seen intents with abundant labeled data using prompt learning with prototypes. In the second stage, novel intents are learned with few examples while preserving knowledge of seen intents through two preservation methods: Data-Agnostic (L2 penalization) and Data-Dependent (knowledge distillation with virtual memory). Experiments show significant improvements over previous methods, particularly in non-episodic evaluation settings.

## Method Summary
The paper proposes a two-stage learning framework that converts GFSID into class incremental learning. First, the model learns seen intents with abundant labeled data using prompt learning with predefined templates and prototype-based classification. Then, novel intents are learned with few examples while preserving seen intent knowledge through two mechanisms: Data-Agnostic Knowledge Preservation (L2 penalization) and Data-Dependent Knowledge Preservation (knowledge distillation with virtual memory). The approach leverages PLMs for semantic knowledge, prototypes for knowledge transfer, and explicit preservation methods to prevent catastrophic forgetting of seen intents during novel intent learning.

## Key Results
- Significant improvements over previous methods on SNIPS and NLUE datasets
- 10.18% and 6.49% improvements for 1-shot and 5-shot respectively on NLUE in non-episodic setting
- The two-stage framework effectively converts GFSID to class incremental learning paradigm
- Both Data-Agnostic and Data-Dependent knowledge preservation methods are effective, with DDKP showing more stable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting GFSID to class incremental learning allows the model to explicitly learn seen intent classification before adapting to novel intents
- Mechanism: Two-stage training where first stage learns seen intents with abundant labeled data, then second stage adds novel intents with few examples while preserving seen intent knowledge
- Core assumption: Knowledge learned about seen intents in stage one can be effectively transferred and preserved when learning novel intents in stage two
- Evidence anchors:
  - [abstract] "we propose to convert the GFSID task into the class incremental learning paradigm"
  - [section] "Specifically, we propose a two-stage learning framework, which sequentially learns the knowledge of different intents in various periods via prompt learning"
  - [corpus] Weak evidence - no direct corpus matches for class incremental learning in intent detection context
- Break condition: If preserving seen intent knowledge significantly interferes with learning novel intents, causing performance degradation

### Mechanism 2
- Claim: Prompt learning with predefined templates improves few-shot performance by leveraging PLM knowledge
- Mechanism: Using template functions to format input sentences and mask token representations for classification
- Core assumption: PLMs contain sufficient semantic knowledge that can be effectively triggered through appropriate prompting templates
- Evidence anchors:
  - [abstract] "we design a predefined template for improving performance with the use of PLMs"
  - [section] "Specifically, given a PLM M, the hidden vector at the position of the [MASK] token in the input sentence wrapped with the template is represented as: h[MASK] = M(T(x))"
  - [corpus] Moderate evidence - corpus contains related work on prompt learning for text classification
- Break condition: If template design is suboptimal, leading to poor masking position or template structure that doesn't leverage PLM strengths

### Mechanism 3
- Claim: Prototypes provide effective knowledge transfer between seen and novel intents in vector space
- Mechanism: Using prototype vectors as classification anchors that can be preserved and adapted across learning stages
- Core assumption: Intent representations can be effectively captured as prototype vectors that generalize across different intent categories
- Evidence anchors:
  - [abstract] "we exploit prototypes for categorizing both seen and novel intents"
  - [section] "Since prototypes have a good generalized capability [8] and are easy to transfer knowledge in vector space, we use prototypes for classification toward both seen and novel intents"
  - [corpus] Strong evidence - multiple corpus neighbors mention prototypical networks and prototype-based methods
- Break condition: If prototype representations don't capture sufficient intent variability, leading to poor generalization

## Foundational Learning

- Concept: Class incremental learning
  - Why needed here: Enables learning new intents while preserving knowledge of previously learned intents
  - Quick check question: What is the key challenge in class incremental learning that this paper addresses?

- Concept: Prompt learning with PLMs
  - Why needed here: Leverages pre-trained language models' semantic knowledge for few-shot intent detection
  - Quick check question: How does prompt learning differ from standard fine-tuning in this context?

- Concept: Prototype-based classification
  - Why needed here: Provides a mechanism for transferring knowledge between seen and novel intents
  - Quick check question: Why are prototypes particularly effective for few-shot learning scenarios?

## Architecture Onboarding

- Component map: Template generator → PLM encoder → Prototype space → Classification layer → Knowledge preservation (L2 penalization/knowledge distillation) → Virtual memory
- Critical path: Template generation → PLM encoding → Prototype computation → Knowledge preservation → Classification
- Design tradeoffs:
  - Data-Agnostic vs Data-Dependent preservation: Computational efficiency vs effectiveness
  - Template design: Specificity vs generalizability
  - Prototype initialization: Random vs informed initialization for novel intents
- Failure signatures:
  - Catastrophic forgetting of seen intents during novel intent learning
  - Poor generalization from few examples due to inadequate prototype representation
  - Template formatting that doesn't effectively leverage PLM capabilities
- First 3 experiments:
  1. Validate template effectiveness by comparing with and without template formatting
  2. Test knowledge preservation methods by measuring seen intent accuracy degradation
  3. Evaluate prototype quality by measuring classification accuracy on novel intents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of data-agnostic vs data-dependent knowledge preservation methods vary across different intent detection datasets with varying characteristics?
- Basis in paper: [explicit] The paper states "Employing the data-dependent knowledge preservation method yields superior performance in most cases" but also notes that DDKP provides more stable retention of old knowledge
- Why unresolved: The paper only tested on two datasets (SNIPS and NLUE) with specific characteristics. Different datasets might have different intents, label distributions, or data quality that could affect which preservation method is more effective
- What evidence would resolve it: Systematic testing across a diverse set of intent detection datasets with varying characteristics (number of intents, label distribution, data quality, domain specificity) while comparing both preservation methods

### Open Question 2
- Question: What is the optimal template design for prompt learning in intent detection, and how sensitive is performance to template variations?
- Basis in paper: [inferred] The paper uses a specific template format ("the intent is to [MASK]") but doesn't explore template variations or provide guidance on optimal template design
- Why unresolved: The paper presents results using one template format without exploring the design space or sensitivity analysis of template variations
- What evidence would resolve it: Systematic experimentation with different template formats, analyzing how variations in template structure affect performance across different datasets and few-shot settings

### Open Question 3
- Question: How does the proposed two-stage learning framework scale when dealing with larger numbers of novel intents or more complex intent hierarchies?
- Basis in paper: [inferred] The paper focuses on GFSID but doesn't explore scenarios with larger numbers of novel intents or hierarchical intent structures
- Why unresolved: The experimental setup and discussion focus on relatively simple intent detection scenarios without exploring scalability or hierarchical intent structures
- What evidence would resolve it: Testing the framework on datasets with larger numbers of novel intents, more complex hierarchical intent structures, and analyzing performance degradation or improvements as complexity increases

### Open Question 4
- Question: What is the minimum amount of labeled data needed in the first stage to achieve optimal performance in the second stage?
- Basis in paper: [inferred] The paper mentions learning "with a large amount of labeled data" in the first stage but doesn't investigate the minimum required amount
- Why unresolved: The paper doesn't explore the relationship between first-stage data quantity and second-stage performance, which is crucial for understanding practical deployment requirements
- What evidence would resolve it: Systematic experiments varying the amount of labeled data in the first stage while measuring the impact on second-stage performance across different few-shot settings

## Limitations

- The two-stage framework may not reflect continuous intent evolution in real-world applications where intent distributions shift gradually rather than discretely
- Performance improvements are primarily demonstrated on SNIPS and NLUE datasets, limiting generalizability to other domains or languages
- The template design for prompt learning is not fully specified, making exact reproduction challenging

## Confidence

**High Confidence**: The effectiveness of prototype-based classification for few-shot learning is well-established in the literature. The mechanism by which prototypes provide knowledge transfer between intent categories is theoretically sound and empirically validated across multiple domains.

**Medium Confidence**: The knowledge preservation methods (Data-Agnostic and Data-Dependent) are likely effective, but their relative performance may depend heavily on specific dataset characteristics and the degree of overlap between seen and novel intents. The virtual memory approach adds computational overhead that may not scale well to large intent vocabularies.

**Low Confidence**: The template design for prompt learning, while following established patterns, is not fully specified in the paper. Without exact template formats, the reproducibility of the prompt learning component is uncertain, and template quality could significantly impact overall performance.

## Next Checks

1. **Ablation on Knowledge Preservation Methods**: Systematically compare Data-Agnostic vs Data-Dependent preservation across multiple random seeds and dataset splits to establish robustness and identify which method performs better under different conditions.

2. **Template Sensitivity Analysis**: Test multiple template variations to determine how sensitive the model is to template design choices, and whether the performance gains are attributable to the prompt learning approach or specific template configurations.

3. **Long-term Stability Evaluation**: Extend evaluation beyond the two-stage framework to assess performance over multiple rounds of novel intent introduction, simulating more realistic deployment scenarios where intents continue to evolve over time.