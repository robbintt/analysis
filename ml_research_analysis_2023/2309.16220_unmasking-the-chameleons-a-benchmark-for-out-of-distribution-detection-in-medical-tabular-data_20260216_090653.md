---
ver: rpa2
title: 'Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in
  Medical Tabular Data'
arxiv_id: '2309.16220'
source_url: https://arxiv.org/abs/2309.16220
tags:
- data
- detection
- methods
- medical
- eicu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks out-of-distribution (OOD) detection in medical
  tabular data, comparing density-based and post-hoc methods across multiple architectures
  (MLP, ResNet, Transformer) on two large ICU datasets (eICU, MIMIC-IV). The key findings
  are: (1) OOD detection is effective for far-OODs but remains challenging for near-OODs;
  (2) post-hoc methods perform poorly alone but improve substantially when coupled
  with distance-based mechanisms; (3) the transformer architecture is far less overconfident
  compared to MLP and ResNet.'
---

# Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data

## Quick Facts
- **arXiv ID**: 2309.16220
- **Source URL**: https://arxiv.org/abs/2309.16220
- **Reference count**: 21
- **Primary result**: OOD detection is effective for far-OODs but remains challenging for near-OODs in medical tabular data

## Executive Summary
This paper presents a comprehensive benchmark for out-of-distribution (OOD) detection in medical tabular data, evaluating 24 different OOD detection methods across three neural architectures (MLP, ResNet, Transformer) on two large ICU datasets (eICU, MIMIC-IV). The study systematically examines far-OOD, near-OOD, and synthesized-OOD scenarios to understand the strengths and limitations of various detection approaches. Key findings reveal that while density-based methods perform well across settings, post-hoc methods require coupling with distance-based mechanisms to overcome their inherent over-confidence issues. The transformer architecture shows promising results in mitigating over-confidence compared to traditional architectures.

## Method Summary
The benchmark evaluates 24 OOD detection methods (7 density-based + 17 post-hoc) applied to three predictive architectures (MLP, ResNet, FT-Transformer) trained on mortality prediction from eICU and MIMIC-IV datasets. The methods are tested across four experimental settings: far-OOD (completely different patient cohorts), near-OOD (varying one clinical feature while keeping others constant), and synthesized-OOD (scaled features by factors of 10, 100, and 1000). Performance is measured using AUROC and FPR@95 metrics, with models trained for 10 epochs using AdamW optimizer and batch size of 64.

## Key Results
- OOD detection is effective for far-OODs but remains challenging for near-OODs
- Post-hoc methods perform poorly alone but improve substantially when coupled with distance-based mechanisms
- Transformer architecture is far less overconfident compared to MLP and ResNet
- Better closed-set classifiers do not necessarily translate to better OOD detection performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc OOD detectors perform poorly alone but improve substantially when coupled with distance-based mechanisms.
- Mechanism: Distance-based novelty scores (e.g., KNN, MDS) measure the distance of an input to the training set. When post-hoc detectors are combined with these distance-based mechanisms, the resulting score integrates both the classifier's confidence and the input's proximity to the training distribution, reducing over-confidence.
- Core assumption: Distance-based scores are more robust to over-confidence than softmax-based scores alone.
- Evidence anchors:
  - [abstract]: "ii) post-hoc methods alone perform poorly, but improve substantially when coupled with distance-based mechanisms;"
  - [section 5]: "methods like MDS applied on ResNet can even marginally outperform density-based methods... when used in conjunction with the distance-based mechanisms."
- Break condition: If the distance-based mechanism is not properly calibrated or if the distance metric is not discriminative for the given data distribution, the improvement may not be observed.

### Mechanism 2
- Claim: The transformer architecture mitigates the problem of over-confidence compared to MLP and ResNet.
- Mechanism: The attention mechanism in transformers allows the model to consider relationships between input elements, leading to a better understanding of ID data and more calibrated confidence scores. This reduces the tendency to assign high confidence to OOD samples far from the training data.
- Core assumption: The attention mechanism in transformers inherently provides better calibration of confidence scores than the fully connected layers in MLP and ResNet.
- Evidence anchors:
  - [abstract]: "iii) the transformer architecture is far less overconfident compared to MLP and ResNet."
  - [section 5.4]: "Our numerical results suggest that FT-Transformer could be a solution to this problem, however, our simple example with toy data showcases that transformers do not completely eliminate over-confidence."
- Break condition: If the transformer is not properly trained or if the data distribution is such that the attention mechanism does not provide a significant advantage, the over-confidence mitigation may not be effective.

### Mechanism 3
- Claim: Better closed-set classifiers do not necessarily translate to better OOD detection performance.
- Mechanism: OOD detection requires distinguishing between ID and OOD data, which is a different task than accurately classifying ID data. A model that performs well on ID data may not be able to reliably identify OOD data, especially if it is overconfident in its predictions.
- Core assumption: The ability to classify ID data accurately is not directly related to the ability to detect OOD data.
- Evidence anchors:
  - [abstract]: "better closed-set classifiers do not necessarily translate to better OOD detection performance"
  - [section 5]: "superior performance as a closed-set classifier on ID data does not necessarily translate to better OOD detection, challenging the claim in Vaze et al. (2021) that better closed-set classifiers lead to improved OOD detection performance."
- Break condition: If the OOD detection task is simplified or if the OOD data is very similar to the ID data, the relationship between closed-set performance and OOD detection may become more direct.

## Foundational Learning

- Concept: Understanding of OOD detection methods (post-hoc, density-based, distance-based)
  - Why needed here: The paper compares different types of OOD detection methods, and understanding their strengths and weaknesses is crucial for interpreting the results.
  - Quick check question: What are the three main categories of OOD detection methods discussed in the paper, and how do they differ in their approach to detecting OOD samples?

- Concept: Knowledge of transformer architectures and attention mechanisms
  - Why needed here: The paper highlights the transformer architecture as a potential solution to the over-confidence problem in OOD detection. Understanding how transformers work and how the attention mechanism contributes to their performance is essential for understanding this claim.
  - Quick check question: How does the attention mechanism in transformers potentially contribute to mitigating over-confidence in OOD detection?

- Concept: Familiarity with medical tabular data and ICU datasets (eICU, MIMIC-IV)
  - Why needed here: The experiments in the paper are conducted on medical tabular data from ICU datasets. Understanding the characteristics of these datasets and the challenges associated with OOD detection in this domain is important for interpreting the results.
  - Quick check question: What are the two ICU datasets used in the experiments, and what are some of the key features of medical tabular data that make OOD detection challenging in this domain?

## Architecture Onboarding

- Component map: Preprocessing -> Dataset Preparation -> Model Training -> OOD Detection Application -> Performance Evaluation
- Critical path: 1. Preprocess the eICU and MIMIC-IV datasets. 2. Train the predictive architectures (MLP, ResNet, FT-Transformer) on the ID data. 3. Apply the OOD detection methods to the trained models. 4. Evaluate the performance of the OOD detection methods using metrics such as AUROC and FPR@95.
- Design tradeoffs: The choice of OOD detection method and predictive architecture involves tradeoffs between performance, computational cost, and ease of implementation. Density-based methods may require more training time but can provide good performance across different settings. Post-hoc methods are easier to apply but may suffer from over-confidence issues. Transformers may mitigate over-confidence but may require more computational resources.
- Failure signatures: Poor OOD detection performance can manifest as high FPR@95 or low AUROC. Over-confidence can lead to high confidence scores for OOD samples, causing them to be misclassified as ID. Under-confidence can lead to low confidence scores for ID samples, causing them to be misclassified as OOD.
- First 3 experiments:
  1. Replicate the far-OOD experiments on the eICU dataset using MLP and MDS as the OOD detection method. Evaluate the performance using AUROC and FPR@95.
  2. Replicate the near-OOD experiments on the MIMIC-IV dataset using ResNet and KNN as the OOD detection method. Evaluate the performance using AUROC and FPR@95.
  3. Replicate the synthesized-OOD experiments on both datasets using FT-Transformer and VIM as the OOD detection method. Evaluate the performance using AUROC and FPR@95.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do OOD detection methods perform when applied to tabular medical data from different healthcare domains (e.g., primary care, radiology, genomics) beyond ICU data?
- Basis in paper: [inferred] The authors caution that their benchmark is built on intensive care datasets and suggest that findings should be transported with caution to other healthcare tabular datasets with different characteristics.
- Why unresolved: The study only evaluated OOD detection on two specific ICU datasets (eICU and MIMIC-IV), leaving the generalizability of results to other medical domains unexplored.
- What evidence would resolve it: Extending the benchmark to include datasets from different medical domains (e.g., primary care records, radiology reports, genomic data) and comparing OOD detection performance across these domains would provide insights into generalizability.

### Open Question 2
- Question: What is the impact of temporal dependencies and time-series characteristics in medical tabular data on OOD detection performance?
- Basis in paper: [inferred] The authors aggregated time-series data using various statistics but did not explore the impact of different temporal modeling approaches on OOD detection.
- Why unresolved: The study focused on static features derived from time-series data without investigating how temporal modeling techniques (e.g., recurrent networks, temporal attention) might affect OOD detection capabilities.
- What evidence would resolve it: Evaluating OOD detection methods on medical tabular data using different temporal modeling approaches (e.g., LSTMs, temporal transformers) and comparing their performance to static feature-based methods would clarify the role of temporal dependencies.

### Open Question 3
- Question: How do feature selection and dimensionality reduction techniques influence OOD detection performance in medical tabular data?
- Basis in paper: [inferred] The authors selected clinical variables based on importance and completeness but did not systematically investigate how different feature selection or dimensionality reduction methods affect OOD detection.
- Why unresolved: While the study used a fixed set of clinical variables, it did not explore whether optimizing feature selection or applying dimensionality reduction techniques could improve OOD detection performance.
- What evidence would resolve it: Conducting experiments with different feature selection methods (e.g., mutual information, L1 regularization) and dimensionality reduction techniques (e.g., PCA, autoencoders) to assess their impact on OOD detection performance would provide insights into optimal feature representation strategies.

## Limitations

- The benchmark is built on intensive care datasets and findings may not generalize to other healthcare tabular datasets with different characteristics
- The study only tests one specific transformer variant (FT-Transformer), limiting conclusions about transformer architectures in general
- Feature selection and dimensionality reduction techniques were not systematically investigated for their impact on OOD detection performance

## Confidence

- **High Confidence**: The experimental results showing post-hoc methods' improvement with distance-based mechanisms and the lack of correlation between closed-set performance and OOD detection performance.
- **Medium Confidence**: The transformer architecture's over-confidence mitigation and the generalizability of findings to other medical tabular datasets.

## Next Checks

1. **Replicate with alternative distance metrics**: Test the post-hoc + distance-based mechanism improvement using alternative distance metrics (e.g., Mahalanobis distance, cosine similarity) to confirm the robustness of the observed improvement across different distance measures.

2. **Cross-dataset generalization**: Apply the benchmark to additional medical tabular datasets (e.g., PhysioNet, MIMIC-III) to assess the generalizability of the findings beyond eICU and MIMIC-IV.

3. **Ablation study on transformer architecture**: Conduct an ablation study on the FT-Transformer architecture to identify which specific components (e.g., attention mechanism, feature-wise linear modulation) contribute most to the over-confidence mitigation, and test alternative transformer variants (e.g., ViT, DeiT) to confirm the robustness of the findings.