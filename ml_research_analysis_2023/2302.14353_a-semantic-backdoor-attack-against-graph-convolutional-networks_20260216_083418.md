---
ver: rpa2
title: A semantic backdoor attack against Graph Convolutional Networks
arxiv_id: '2302.14353'
source_url: https://arxiv.org/abs/2302.14353
tags:
- backdoor
- attack
- trigger
- graph
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a semantic backdoor attack against graph convolutional
  networks (GCNs) under graph classification tasks. The attack injects a backdoor
  into GCN models by poisoning training data, using a certain type of node in the
  samples as a backdoor trigger.
---

# A semantic backdoor attack against Graph Convolutional Networks

## Quick Facts
- **arXiv ID**: 2302.14353
- **Source URL**: https://arxiv.org/abs/2302.14353
- **Reference count**: 0
- **Primary result**: Achieves near-100% attack success rate with <5% poisoning rate while maintaining clean accuracy on graph classification tasks

## Executive Summary
This paper presents a novel semantic backdoor attack against Graph Convolutional Networks (GCNs) for graph classification tasks. The attack exploits naturally occurring node types as semantic triggers, poisoning training data by relabeling graphs containing specific node classes to a target class. The GCN learns to associate these trigger nodes with the target label, creating a backdoor that activates when unmodified samples contain enough trigger nodes. Experimental results demonstrate high attack success rates (near 100%) with minimal poisoning (less than 5%) while maintaining normal predictive accuracy on benign samples.

## Method Summary
The attack identifies a node class that naturally occurs in the dataset and uses it as a semantic trigger. During training, graphs containing this trigger node are selected and relabeled to a target class, poisoning the training set. A scoring GCN model ranks candidate graphs based on the impact of setting trigger node features to zero. The backdoored GCN is then trained on this poisoned data. During inference, any unmodified graph containing the trigger node is classified to the target class, activating the backdoor. The attack is evaluated on four real-world graph datasets (AIDS, NCI1, PROTEINS, ENZYMES) under different poisoning rates.

## Key Results
- Achieves near-100% attack success rate with poisoning rates below 5%
- Maintains close prediction accuracy to clean models on benign samples
- Demonstrates effectiveness across four real-world graph classification datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Poisoning training data by relabeling graphs containing a specific node type as the backdoor trigger embeds the backdoor into the GCN model.
- **Mechanism**: The attacker identifies a node type (trigger node) that naturally occurs in the dataset. During training, graphs containing this node type are relabeled to a target class. The GCN learns to associate the presence of this node type with the target label, creating a backdoor.
- **Core assumption**: The GCN model will learn to associate the presence of the trigger node type with the relabeled target class during training.
- **Evidence anchors**:
  - [abstract]: "The attack injects a backdoor into GCN models by poisoning training data, using a certain type of node in the samples as a backdoor trigger."
  - [section]: "In the training phase, we let the GCN model to train on a poisoned training set by re-labelling some graphs, thus injecting the backdoor into the target model."
  - [corpus]: Weak - the corpus neighbors discuss backdoor attacks but don't specifically address the semantic trigger mechanism using naturally occurring node types.
- **Break condition**: If the GCN model does not learn the association between the trigger node type and the target class, or if the model is trained on a sufficiently large and diverse dataset that prevents overfitting to this specific pattern.

### Mechanism 2
- **Claim**: The backdoor is activated during inference when the trigger node appears in unmodified samples.
- **Mechanism**: Since the trigger node is a naturally occurring semantic feature of the graph, the backdoored GCN model will classify any graph containing this node type to the target class, even without any modification to the graph structure or node features.
- **Core assumption**: The trigger node is a semantically meaningful feature that the GCN model has learned to associate with the target class.
- **Evidence anchors**:
  - [abstract]: "The backdoor will be activated, and the GCN models give malicious classification results specified by the attacker even on unmodified samples as long as the samples contain enough trigger nodes."
  - [section]: "Since our trigger is semantic, in the inference phase, the backdoor will be activated and the model will give the classification result specified by the attacker even on unmodified samples as long as the trigger appears, otherwise the model works normally."
  - [corpus]: Weak - the corpus neighbors discuss backdoor attacks but don't specifically address the activation of backdoors using naturally occurring semantic features without modification.
- **Break condition**: If the trigger node is not semantically meaningful or if the model's learned representation does not strongly associate the trigger node with the target class.

### Mechanism 3
- **Claim**: The attack achieves high success rates with low poisoning rates due to the semantic nature of the trigger.
- **Mechanism**: By using a naturally occurring node type as the trigger, the attacker can achieve high attack success rates with minimal poisoning (less than 5%). This is because the semantic trigger is inherently part of many graphs, and relabeling a small percentage of these graphs is sufficient to embed the backdoor.
- **Core assumption**: The semantic nature of the trigger allows for effective backdoor embedding with minimal poisoning.
- **Evidence anchors**:
  - [abstract]: "Experimental results on four graph datasets demonstrate that the proposed semantic backdoor attack is effective, achieving almost 100% attack success rate under the poisoning rate less than 5% while having no impact on normal predictive accuracy."
  - [section]: "Through evaluation on several real-world benchmark graph datasets, the experimental results demonstrate that our proposed SBA can achieve almost 100% attack success rate under the poisoning rate less than 5% while having close prediction accuracy to that of the clean model for benign samples."
  - [corpus]: Weak - the corpus neighbors discuss backdoor attacks but don't specifically address the relationship between semantic triggers and low poisoning rates.
- **Break condition**: If the semantic trigger is not prevalent enough in the dataset, or if the model requires a higher poisoning rate to learn the backdoor effectively.

## Foundational Learning

- **Concept**: Graph Convolutional Networks (GCNs) and their application to graph classification tasks.
  - **Why needed here**: Understanding GCNs is crucial to grasp how the backdoor attack exploits the model's learning process and graph structure.
  - **Quick check question**: How do GCNs aggregate information from neighboring nodes during the learning process?

- **Concept**: Adversarial attacks, specifically backdoor attacks, and their impact on machine learning models.
  - **Why needed here**: The paper focuses on a specific type of adversarial attack, so understanding the broader context of adversarial attacks is essential.
  - **Quick check question**: What is the difference between a backdoor attack and a poisoning attack?

- **Concept**: Graph data structures and node/edge features.
  - **Why needed here**: The attack relies on manipulating graph data and node features, so understanding these concepts is necessary.
  - **Quick check question**: How are node features typically represented in graph data for GCNs?

## Architecture Onboarding

- **Component map**: Graph datasets (AIDS, NCI1, PROTEINS, ENZYMES) -> GCN model (three-layer with one hidden layer) -> Trigger node identification algorithm -> Graph scoring function -> Poisoning mechanism (relabeling graphs) -> Evaluation metrics (ASR, CAD, poisoning rate)

- **Critical path**:
  1. Identify trigger node type
  2. Score and select graphs for poisoning
  3. Poison training data by relabeling
  4. Train backdoored GCN model
  5. Evaluate attack success rate and clean accuracy

- **Design tradeoffs**:
  - Poisoning rate vs. attack success rate: Lower poisoning rates are more stealthy but may reduce attack success.
  - Trigger node selection: Choosing a node type that is semantically meaningful but not too prevalent to avoid detection.
  - Model architecture: Balancing model complexity with the ability to learn the backdoor effectively.

- **Failure signatures**:
  - Low attack success rate: Indicates the backdoor was not effectively embedded.
  - High clean accuracy drop: Suggests the poisoning significantly impacts normal model performance.
  - Difficulty in identifying suitable trigger nodes: May indicate the dataset lacks semantically meaningful node types.

- **First 3 experiments**:
  1. Implement the trigger node identification algorithm on a small graph dataset and visualize the distribution of node types.
  2. Create a simple poisoning function that relabels a small percentage of graphs containing the trigger node and train a GCN model to observe the effect on classification.
  3. Implement the graph scoring function to select graphs for poisoning and evaluate its impact on the attack success rate compared to random selection.

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the effectiveness of semantic backdoor attacks on GCNs be mitigated or prevented without significantly impacting model performance on benign samples?
  - **Basis in paper**: [explicit] The paper concludes by stating that future work will study a defense method against the semantic backdoor attack, specifically mentioning the observation of gradient changes in the neural network.
  - **Why unresolved**: While the paper acknowledges the need for a defense mechanism, it does not provide any details or propose a specific solution, leaving the question of how to effectively defend against such attacks open.
  - **What evidence would resolve it**: A proposed defense method with experimental results demonstrating its effectiveness in mitigating semantic backdoor attacks while maintaining model performance on benign samples.

- **Open Question 2**: What are the potential impacts of semantic backdoor attacks on GCNs when applied to real-world graph datasets with more complex structures and larger sizes?
  - **Basis in paper**: [inferred] The paper evaluates the attack on four real-world datasets but does not explore the scalability or effectiveness of the attack on larger or more complex graph structures.
  - **Why unresolved**: The paper's experiments are limited to specific datasets, and there is no discussion on how the attack might perform or be detected in more complex or larger-scale scenarios.
  - **What evidence would resolve it**: Experimental results showing the attack's success rate and detectability on various real-world graph datasets with different sizes and complexities.

- **Open Question 3**: How does the choice of trigger node affect the attack's success rate and the model's ability to generalize on benign samples?
  - **Basis in paper**: [explicit] The paper describes a method for selecting a trigger node based on its frequency in graphs with different labels, but it does not explore how different choices of trigger nodes might impact the attack's effectiveness or the model's performance.
  - **Why unresolved**: While the paper provides a method for selecting a trigger node, it does not investigate the sensitivity of the attack to different trigger node choices or their impact on the model's generalization ability.
  - **What evidence would resolve it**: Experimental results comparing the attack's success rate and the model's performance on benign samples when using different trigger nodes selected through various criteria.

## Limitations
- The attack's effectiveness may be limited to graph classification tasks and may not transfer to other graph learning paradigms like node classification or link prediction.
- The reliance on naturally occurring node types as triggers may limit applicability to datasets where such semantic features are absent or where node type distributions are highly uniform.
- The experimental validation is restricted to four relatively small graph datasets, raising questions about scalability to larger, more complex graphs.

## Confidence
- **High Confidence**: The core mechanism of using naturally occurring node types as semantic triggers and the poisoning strategy are well-established and reproducible based on the provided methodology.
- **Medium Confidence**: The claim of achieving near-100% attack success with <5% poisoning rate is supported by experiments but may not generalize to all graph datasets or model architectures.
- **Medium Confidence**: The assertion that the attack has "no impact on normal predictive accuracy" is demonstrated but could vary with different model capacities or dataset characteristics.

## Next Checks
1. **Transferability Test**: Evaluate the attack's effectiveness on graph node classification tasks using standard benchmarks like Cora, Citeseer, or PubMed to assess cross-task applicability.
2. **Trigger Robustness Analysis**: Systematically vary the semantic meaning and frequency of trigger nodes across different datasets to determine attack sensitivity to trigger selection.
3. **Detection Vulnerability Assessment**: Implement and evaluate standard backdoor detection techniques (e.g., spectral signature analysis, activation clustering) to measure the attack's stealth against existing defenses.