---
ver: rpa2
title: Random Postprocessing for Combinatorial Bayesian Optimization
arxiv_id: '2309.02842'
source_url: https://arxiv.org/abs/2309.02842
tags:
- optimization
- nbocs
- postprocessing
- bayesian
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the slow convergence issue in Bayesian optimization
  for high-dimensional combinatorial problems. The authors propose a random postprocessing
  method that strictly prohibits duplicated samples in the dataset by rejecting points
  already in the dataset and proposing a new random point instead.
---

# Random Postprocessing for Combinatorial Bayesian Optimization

## Quick Facts
- arXiv ID: 2309.02842
- Source URL: https://arxiv.org/abs/2309.02842
- Reference count: 0
- Key outcome: Random postprocessing improves Bayesian optimization for high-dimensional combinatorial problems by preventing local optima trapping, particularly effective with MAP estimation acquisition functions

## Executive Summary
This paper addresses the slow convergence issue in Bayesian optimization for high-dimensional combinatorial problems by introducing a random postprocessing method that strictly prohibits duplicated samples in the dataset. The method rejects points already in the dataset and proposes new random points instead, which drives model parameters away from incorrect fixed points. Applied to the ground-state search of the Sherrington-Kirkpatrick spin glass model using a second-order surrogate model with both Thompson sampling and MAP estimation acquisition functions, the results show that random postprocessing significantly reduces the number of steps needed to find the ground state when using MAP estimation, with the algorithm typically finding the ground state within 10^3 steps. The number of steps scales algebraically with the number of spins N with an exponent of 2.12(3), which is very close to the optimal exponent of 2.

## Method Summary
The method uses a second-order surrogate model with normal prior to approximate the objective function, combined with two acquisition functions: Thompson sampling and MAP estimation. The random postprocessing mechanism rejects any query point that already exists in the dataset and instead samples a new point uniformly at random. This postprocessing is applied after the acquisition function selects a point but before the objective function is evaluated. The optimization algorithm uses simulated annealing to select query points based on the acquisition function, and the surrogate model parameters are updated using linear regression on the dataset. The approach is tested on the Sherrington-Kirkpatrick spin glass model, tracking normalized smallest energy and overlap with correct parameters to evaluate performance.

## Key Results
- Random postprocessing with MAP estimation typically finds the ground state within 10^3 steps for the Sherrington-Kirkpatrick model
- The number of steps scales algebraically with the number of spins N with exponent 2.12(3), close to the optimal exponent of 2
- Postprocessing shows significant improvement for MAP estimation but minimal effect on Thompson sampling due to the latter's inherent randomness
- The method drives model parameters away from metastable local optima, enabling better exploration of the parameter space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random postprocessing prevents the algorithm from getting stuck in local optima by injecting uniformly random samples when a duplicate point is proposed.
- Mechanism: When the acquisition function (MAP estimation) selects a point already in the dataset, the postprocessing rejects it and instead samples a new point uniformly at random. This action drives the model parameters away from incorrect local optima because it forces exploration into unvisited regions of the parameter space.
- Core assumption: The MAP estimation acquisition function is prone to propose the same or nearby points repeatedly, creating a fixed point in the parameter space that prevents further exploration.
- Evidence anchors:
  - [section] "Appending uniformly random samples to D drives w away from the incorrect fixed point we."
  - [abstract] "random postprocessing method that strictly prohibits duplicated samples in the dataset by rejecting points already in the dataset and proposing a new random point instead."
  - [corpus] Weak evidence - no direct citations found, but the claim aligns with general Bayesian optimization literature on exploration-exploitation balance.
- Break condition: If the random sampling does not provide sufficient diversity or if the parameter space is too constrained, the method may fail to escape local optima.

### Mechanism 2
- Claim: Random postprocessing significantly improves performance specifically when using MAP estimation as the acquisition function, but not when using Thompson sampling.
- Mechanism: MAP estimation deterministically selects the mode of the posterior distribution, which can lead to repeated selection of the same point. Random postprocessing breaks this determinism by introducing randomness when duplicates are detected. Thompson sampling already incorporates randomness by sampling from the posterior, so postprocessing has minimal additional effect.
- Core assumption: The acquisition function's behavior (deterministic vs. stochastic) determines the effectiveness of postprocessing.
- Evidence anchors:
  - [section] "nBOCS-Random (MAP) typically finds the ground state within 103 steps if βfinal is large enough, whereas [u(t)] of nBOCS (MAP) with any βfinal used in our runs gets stuck at [ u(t)] ≳ 10−1... indicating that nBOCS (MAP) cannot find the ground state even in the limit t → ∞."
  - [section] "This comes from the fact that αTS(x) (Eq. (8)) rarely yields a point already included in D(t) and triggers the postprocessing with a very small probability. Therefore, nBOCS (TS) and nBOCS-Random (TS) are virtually identical."
  - [corpus] Weak evidence - no direct citations found, but the claim is supported by the experimental results presented.
- Break condition: If the acquisition function inherently provides sufficient exploration (like Thompson sampling), postprocessing may not provide additional benefits.

### Mechanism 3
- Claim: The random postprocessing method improves the scaling of the number of steps needed to find the ground state, reducing the exponent from approximately 2.29 to 2.12.
- Mechanism: By preventing the algorithm from getting stuck in local optima, postprocessing allows more efficient exploration of the parameter space. This efficiency gain translates into a better scaling relationship between the number of steps and the problem size (number of spins N).
- Core assumption: The scaling exponent is sensitive to the algorithm's ability to explore the parameter space effectively.
- Evidence anchors:
  - [section] "Our results imply that random postprocessing can improve Bayesian optimization for high-dimensional problems."
  - [section] "Our algorithms, especially nBOCS-Random (MAP), are thus very close to the optimal."
  - [section] "The exponent z depends on algorithms, and nBOCS-Random (MAP) yields the smallest value, z = 2.12(3), while z = 2.29(4) for the other two algorithms, suggesting that the postprocessing qualitatively changes how the algorithm explores the parameter space."
  - [corpus] Weak evidence - no direct citations found, but the claim is supported by the experimental results presented.
- Break condition: If the problem structure changes significantly or if the random sampling does not provide sufficient diversity, the scaling improvement may not be observed.

## Foundational Learning

- Concept: Bayesian Optimization
  - Why needed here: The paper is fundamentally about improving Bayesian optimization for high-dimensional combinatorial problems.
  - Quick check question: What are the key components of Bayesian optimization, and how do they work together to optimize expensive black-box functions?

- Concept: Surrogate Models and Acquisition Functions
  - Why needed here: The paper uses a second-order surrogate model and compares two acquisition functions (Thompson sampling and MAP estimation).
  - Quick check question: How do different acquisition functions (like Thompson sampling and MAP estimation) balance exploration and exploitation in Bayesian optimization?

- Concept: Combinatorial Optimization and the Sherrington-Kirkpatrick Model
  - Why needed here: The paper applies the method to the ground-state search of the Sherrington-Kirkpatrick spin glass model, which is a combinatorial optimization problem.
  - Quick check question: What makes combinatorial optimization problems challenging for Bayesian optimization, and how does the Sherrington-Kirkpatrick model exemplify these challenges?

## Architecture Onboarding

- Component map: Surrogate model -> Acquisition function -> Postprocessing -> Objective function evaluation -> Dataset update
- Critical path: Initialize dataset with one random point → Update surrogate model parameters using linear regression → Select next query point using acquisition function and simulated annealing → Apply postprocessing if duplicate point is selected → Evaluate objective function and update dataset → Repeat until termination criterion is met
- Design tradeoffs: Postprocessing vs. no postprocessing (postprocessing improves MAP but not TS), Random sampling vs. deterministic selection (prevents local optima but may reduce exploitation efficiency), Second-order surrogate model vs. higher-order models (simpler model is easier to optimize but may be less expressive)
- Failure signatures: Algorithm gets stuck in local optima (no improvement in normalized smallest energy), Random sampling does not provide sufficient diversity, Surrogate model parameters do not converge to correct values
- First 3 experiments: 1) Run nBOCS (MAP) without postprocessing on a small Sherrington-Kirkpatrick instance and observe if it gets stuck in local optima, 2) Add random postprocessing to nBOCS (MAP) and compare the number of steps needed to find the ground state, 3) Compare the performance of nBOCS (TS) with and without postprocessing to verify that postprocessing has minimal effect on Thompson sampling

## Open Questions the Paper Calls Out
- Question: How does random postprocessing perform on constrained combinatorial optimization problems?
  - Basis in paper: [explicit] The authors state "applying the postprocessing to Bayesian optimization of other problems, e.g., constrained combinatorial optimization problems, is certainly an interesting direction."
  - Why unresolved: The paper only tested the method on unconstrained ground-state search of the SK model. Performance on problems with constraints is unknown.
  - What evidence would resolve it: Testing random postprocessing on benchmark constrained combinatorial optimization problems (e.g., knapsack, traveling salesman) and comparing convergence rates to standard Bayesian optimization.

- Question: How does the performance of random postprocessing scale with problem dimensionality beyond N=48?
  - Basis in paper: [inferred] The authors studied N up to 48, finding power-law scaling with exponent ~2.12. Scaling for much larger N is unknown.
  - Why unresolved: The paper only tested up to N=48 due to computational cost. Scaling for very high-dimensional problems is unknown.
  - What evidence would resolve it: Running the algorithm on larger instances (N >> 48) to empirically determine the scaling exponent and check if it remains close to the optimal value of 2.

- Question: How does random postprocessing affect exploration vs exploitation trade-off in Bayesian optimization?
  - Basis in paper: [explicit] The authors note random postprocessing "drives w to escape from a metastable, local optimum, yielding enhanced parameter space exploration."
  - Why unresolved: While the authors observe this effect, they do not quantify or analyze how postprocessing specifically changes the exploration-exploitation balance.
  - What evidence would resolve it: Measuring and comparing acquisition function diversity, entropy of selected points, and exploration metrics between standard and postprocessed Bayesian optimization runs.

## Limitations
- The effectiveness of random postprocessing is highly dependent on the specific acquisition function used, showing significant improvements only for MAP estimation while having minimal impact on Thompson sampling
- The paper does not explore whether this approach generalizes to other combinatorial optimization problems beyond the Sherrington-Kirkpatrick model
- The second-order surrogate model, while effective, may not capture more complex problem structures that require higher-order interactions

## Confidence
- Mechanism 1 (Local optima escape via random sampling): Medium - supported by experimental results but relies on theoretical assumptions about parameter space dynamics
- Mechanism 2 (MAP vs. Thompson sampling differential effect): High - directly demonstrated through comparative experiments with clear numerical evidence
- Mechanism 3 (Scaling improvement from 2.29 to 2.12): High - quantitatively measured across multiple problem sizes with statistical error bounds

## Next Checks
1. Implement and test random postprocessing on a different combinatorial optimization problem (e.g., Max-Cut or Traveling Salesman) to verify generalizability beyond the SK model
2. Compare computational overhead of postprocessing by measuring total wall-clock time and query efficiency, particularly for larger N values
3. Investigate whether adaptive postprocessing (varying random sampling probability based on exploration metrics) can further improve performance or reduce computational cost