---
ver: rpa2
title: Co-training and Co-distillation for Quality Improvement and Compression of
  Language Models
arxiv_id: '2311.02849'
source_url: https://arxiv.org/abs/2311.02849
tags:
- student
- teacher
- performance
- loss
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Co-training and Co-distillation (CTCD) is proposed to improve the
  performance of pre-trained language models (PLMs) while compressing them through
  knowledge distillation. CTCD involves jointly training a teacher and student model,
  allowing them to transfer knowledge to each other bidirectionally.
---

# Co-training and Co-distillation for Quality Improvement and Compression of Language Models

## Quick Facts
- arXiv ID: 2311.02849
- Source URL: https://arxiv.org/abs/2311.02849
- Reference count: 6
- Key outcome: CTCD improves BERT-6 performance by 1.66 GLUE points over baseline

## Executive Summary
This paper proposes Co-training and Co-distillation (CTCD), a framework for improving pre-trained language models (PLMs) while compressing them through knowledge distillation. CTCD involves jointly training a teacher and student model, allowing bidirectional knowledge transfer between them. The key innovation is that knowledge flows in both directions - the student distills knowledge to the teacher, which then enhances its ability to teach the student. This creates a positive feedback loop where both models improve together. The framework is evaluated on the GLUE benchmark, showing that the student model distilled by CTCD outperforms the original larger model by 1.66 points while maintaining inference efficiency.

## Method Summary
CTCD involves co-training a 6-layer BERT teacher and a 4-layer BERT student from scratch on a reduced pre-training corpus. The models are trained jointly using a combination of hard losses (cross-entropy with ground truth) and soft losses (KL divergence between model predictions). Knowledge is distilled bidirectionally between the models during pre-training, making the approach task-agnostic. The distilled student model is then fine-tuned on downstream GLUE tasks. The framework uses specific loss weighting values (αh:αs:βh:βs = 1:1:1:4) and standard hyperparameters (learning rate 5e-4, batch size 128, 10-20 epochs).

## Key Results
- Student model distilled by CTCD outperforms independently trained larger BERT-6 by 1.66 GLUE points
- Bidirectional knowledge transfer from student to teacher improves teacher performance
- Enhanced teacher performance further boosts student performance through positive feedback loop
- CTCD achieves comparable inference efficiency to one-way distillation while improving quality

## Why This Works (Mechanism)

### Mechanism 1
Distilling from the smaller model to the larger model during co-training improves the larger model's performance. The student model's predictions serve as "soft label" regularization for the teacher, providing complementary views of the data that differ from ground truth. This bidirectional knowledge transfer allows the teacher to learn more generalizable representations.

### Mechanism 2
The enhanced performance of the larger model further boosts the performance of the smaller model. The teacher model, now improved through co-distillation from the student, provides higher-quality soft labels to the student during training. This creates a positive feedback loop where both models benefit from each other's improved representations.

### Mechanism 3
Task-agnostic pre-training distillation creates transferable knowledge that benefits downstream fine-tuning. By performing KD during pre-training on a large text corpus rather than task-specific fine-tuning, the distilled knowledge is more general and can be effectively adapted to various downstream tasks during fine-tuning.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: CTCD builds directly on KD principles, extending them from one-way to bidirectional transfer
  - Quick check question: What is the key difference between soft labels and hard labels in KD?

- Concept: Bidirectional knowledge transfer
  - Why needed here: CTCD's core innovation is allowing knowledge to flow in both directions between teacher and student
  - Quick check question: Why might a smaller model's predictions be useful for improving a larger model?

- Concept: Pre-training vs. fine-tuning
  - Why needed here: CTCD performs distillation during pre-training, making the approach task-agnostic
  - Quick check question: What advantage does task-agnostic KD have over task-specific KD?

## Architecture Onboarding

- Component map: Teacher model (6-layer BERT, ϕ) <-soft loss-> Student model (4-layer BERT, θ) <-hard loss-> Ground truth labels

- Critical path:
  1. Initialize teacher and student from scratch
  2. Forward pass through both models
  3. Compute hard and soft losses for both models
  4. Backpropagate and update both sets of parameters
  5. Iterate for specified epochs

- Design tradeoffs:
  - Training time vs. performance: CTCD requires longer training than one-way KD but achieves better results
  - Model size ratio: Optimal performance may depend on the relative sizes of teacher and student
  - Loss weight balancing: The ratio of hard to soft loss weights affects the knowledge transfer dynamics

- Failure signatures:
  - Performance degradation: If the student is too weak, it may provide misleading signals to the teacher
  - Slow convergence: Improper loss weight balancing can lead to unstable training
  - Overfitting: Excessive emphasis on soft losses may cause models to fit each other's idiosyncrasies rather than generalize

- First 3 experiments:
  1. Compare standalone training vs. co-training without distillation to establish baseline performance
  2. Test different ratios of hard to soft loss weights (e.g., 1:1, 1:2, 1:4) to find optimal balancing
  3. Vary training length (10 vs. 20 epochs) to determine when the positive feedback loop saturates

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CTCD compare when applied to larger PLMs (e.g., GPT-3, T5) versus the BERT models used in the study?
- Basis in paper: [explicit] The paper focuses on BERT models and does not explore larger PLMs.
- Why unresolved: The paper does not provide any results or analysis for larger PLMs.
- What evidence would resolve it: Experiments comparing CTCD performance on various sizes of PLMs, including GPT-3 and T5, to establish scalability.

### Open Question 2
What is the impact of different architectural differences between teacher and student models (e.g., layer count, attention mechanisms) on the effectiveness of CTCD?
- Basis in paper: [inferred] The paper uses a 6-layer BERT as teacher and 4-layer BERT as student but does not explore other architectural variations.
- Why unresolved: The study is limited to specific architectures and does not investigate how other architectural choices might affect CTCD.
- What evidence would resolve it: Comparative experiments with various teacher-student architecture pairs to determine the impact on CTCD effectiveness.

### Open Question 3
How does CTCD perform in multilingual or cross-lingual settings, and can it improve the quality of multilingual PLMs?
- Basis in paper: [inferred] The paper does not address multilingual or cross-lingual aspects of CTCD.
- Why unresolved: The study is conducted on English datasets and does not explore multilingual capabilities.
- What evidence would resolve it: Experiments applying CTCD to multilingual PLMs and evaluating performance on multilingual tasks.

### Open Question 4
What are the long-term effects of CTCD on model robustness and generalization across diverse downstream tasks?
- Basis in paper: [explicit] The paper evaluates CTCD on the GLUE benchmark but does not explore long-term effects or diverse tasks.
- Why unresolved: The study is limited to a specific benchmark and does not investigate long-term robustness or generalization.
- What evidence would resolve it: Longitudinal studies and evaluations on a broader range of tasks and domains to assess CTCD's impact on model robustness and generalization.

## Limitations

- The theoretical understanding of why bidirectional distillation works remains limited
- Computational efficiency claims are qualified - co-training is more expensive than standalone training
- Evaluation is limited to GLUE benchmark, not extensively exploring diverse downstream tasks
- The task-agnostic pre-training distillation approach's generalizability is not fully proven

## Confidence

**High Confidence**: The empirical results demonstrating GLUE score improvements are robust and well-documented. The training procedure is clearly specified, and the comparison between CTCD and baseline methods is methodologically sound.

**Medium Confidence**: The explanation of why bidirectional distillation works is plausible but not fully proven. While the paper provides a mechanistic explanation, additional theoretical analysis or ablation studies would strengthen the claims about knowledge complementarity.

**Low Confidence**: Claims about the generalizability of task-agnostic pre-training distillation across diverse downstream tasks are not fully supported by the evidence. The paper evaluates on GLUE but doesn't provide extensive analysis of how well the distilled knowledge transfers to very different types of tasks.

## Next Checks

1. **Size Ratio Sensitivity**: Conduct experiments varying the teacher-student size ratio (e.g., 8-layer vs 4-layer, 6-layer vs 3-layer) to determine how sensitive the performance improvements are to the relative model sizes.

2. **Negative Transfer Analysis**: Systematically test cases where the student model is deliberately weakened to quantify the threshold at which bidirectional knowledge transfer becomes detrimental rather than beneficial.

3. **Cross-Task Generalization Study**: Evaluate CTCD on a diverse set of tasks beyond GLUE, including specialized domains like biomedical text or code understanding, to assess how well the pre-training distillation generalizes to out-of-distribution tasks.