---
ver: rpa2
title: 'On Bias and Fairness in NLP: Investigating the Impact of Bias and Debiasing
  in Language Models on the Fairness of Toxicity Detection'
arxiv_id: '2305.12829'
source_url: https://arxiv.org/abs/2305.12829
tags:
- bias
- fairness
- dataset
- text
- cation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the impact of different sources of bias\u2014\
  upstream, sample, and overamplification\u2014on the fairness of toxicity detection\
  \ in NLP. The authors propose debiasing methods such as data perturbation and stratified\
  \ sampling to mitigate these biases."
---

# On Bias and Fairness in NLP: Investigating the Impact of Bias and Debiasing in Language Models on the Fairness of Toxicity Detection

## Quick Facts
- arXiv ID: 2305.12829
- Source URL: https://arxiv.org/abs/2305.12829
- Authors: 
- Reference count: 13
- One-line primary result: Overamplification bias has the strongest impact on fairness in toxicity detection, and removing it by fine-tuning on balanced datasets significantly improves fairness metrics.

## Executive Summary
This paper investigates how different sources of bias—upstream (pre-trained model), sample (fine-tuning dataset), and overamplification (model amplifying existing biases)—affect fairness in toxicity detection. The authors propose and evaluate debiasing methods including data perturbation and stratified sampling to mitigate these biases. Their key finding is that overamplification bias has the strongest negative impact on fairness, and removing it through fine-tuning on balanced datasets leads to significant improvements in fairness metrics like FPR_gap, TPR_gap, and AUC_gap. The study provides practical guidelines for ensuring fairness in toxicity detection tasks.

## Method Summary
The study fine-tunes BERT-base, RoBERTa-base, and ALBERT-base models on the Jigsaw Toxicity Dataset (400K comments after preprocessing), evaluating fairness across marginalized and non-marginalized identity groups. The authors create balanced evaluation datasets using lexical word replacement and apply debiasing techniques including data perturbation and stratified sampling. They measure fairness using threshold-based metrics (FPR_gap, TPR_gap) and threshold-agnostic metrics (AUC_gap), comparing model performance before and after debiasing interventions.

## Key Results
- Overamplification bias has the strongest impact on fairness in toxicity detection models
- Fine-tuning models on balanced datasets significantly improves fairness metrics (FPR_gap, TPR_gap, AUC_gap)
- Downstream biases (sample and overamplification) have stronger impact on fairness than upstream model bias
- Balanced evaluation datasets provide more accurate assessment of model fairness compared to original unbalanced datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing overamplification bias by fine-tuning on balanced datasets improves fairness metrics in toxicity detection.
- Mechanism: Overamplification bias occurs when models learn spurious correlations between identity groups and toxicity labels due to imbalanced representation in training data. By creating a balanced dataset where identity groups have proportional toxic and non-toxic examples, the model is prevented from amplifying these spurious correlations, leading to fairer predictions.
- Core assumption: The imbalance in identity group representation in the original dataset causes the model to learn spurious correlations that manifest as fairness gaps.
- Evidence anchors:
  - [abstract]: "they find that overamplification bias has the strongest impact on fairness, and that removing it by fine-tuning models on balanced datasets significantly improves fairness metrics"
  - [section 7.2]: "We recommend removing the overamplification bias since it is the most impactful debiasing method on the models' fairness"
  - [corpus]: Weak evidence - no directly relevant corpus neighbors discussing overamplification bias specifically
- Break condition: If the original dataset imbalance does not reflect spurious correlations, or if the balancing process introduces new biases through synthetic data generation.

### Mechanism 2
- Claim: Using a balanced fairness dataset for evaluation improves measured fairness scores compared to the original unbalanced dataset.
- Mechanism: The original toxicity detection datasets contain imbalanced representations of identity groups, with some groups having more toxic examples than others. When evaluating fairness using metrics that compare performance across groups, this imbalance can create apparent fairness gaps. By creating a balanced dataset where each identity group has the same proportion of toxic examples, the measured fairness gaps are reduced, providing a more accurate assessment of the model's true fairness.
- Core assumption: The imbalance in the original evaluation dataset creates artificial fairness gaps that do not reflect the model's actual bias.
- Evidence anchors:
  - [abstract]: "they demonstrate that fairness scores, measured using metrics like FPR_gap, TPR_gap, and AUC_gap, improve when models are trained on datasets with balanced representations"
  - [section 4.2]: "We hypothesize that this imbalance in the dataset might have an impact on the measured fairness scores"
  - [corpus]: Weak evidence - no directly relevant corpus neighbors discussing balanced evaluation datasets specifically
- Break condition: If the balancing process for the fairness dataset does not accurately represent the real-world distribution of toxicity across identity groups.

### Mechanism 3
- Claim: Downstream biases (sample and overamplification) have a stronger impact on fairness than upstream bias in language models.
- Mechanism: While upstream bias in language models (pre-training) can introduce societal stereotypes, the authors find that downstream biases introduced during fine-tuning on toxicity datasets have a more direct and stronger impact on fairness metrics. This is evidenced by the stronger correlations between downstream bias measures and fairness gaps compared to upstream bias measures.
- Core assumption: The fairness gaps in toxicity detection are primarily caused by biases introduced during the fine-tuning process rather than pre-existing biases in the language model.
- Evidence anchors:
  - [abstract]: "Results show strong evidence that downstream sources of bias, especially overamplification bias, are the most impactful types of bias on the fairness of the task of text classification"
  - [section 6.1]: "Our results indicate that the two sources of downstream bias, sample and overamplification, have an impact on the fairness of the task of text classification. Overamplification bias seems to have slightly stronger impact than sample bias"
  - [corpus]: Weak evidence - no directly relevant corpus neighbors comparing upstream vs. downstream bias impacts
- Break condition: If the language model's pre-existing biases are more severe than the biases introduced during fine-tuning, or if the fine-tuning dataset is not representative of the language model's pre-training data distribution.

## Foundational Learning

- Concept: Group fairness metrics (FPR_gap, TPR_gap, AUC_gap)
  - Why needed here: These metrics are used to quantify the fairness of toxicity detection models by measuring the performance differences between marginalized and non-marginalized identity groups.
  - Quick check question: How is FPR_gap calculated between two groups g and ĝ?

- Concept: Data perturbation and synthetic data generation
  - Why needed here: These techniques are used to create balanced datasets by modifying existing examples to represent different identity groups proportionally.
  - Quick check question: What tool is mentioned for gender-based perturbations in the text?

- Concept: Counterfactual fairness and perturbation sensitivity score (SenseScore)
  - Why needed here: These are used to evaluate how the model's predictions change when the identity group in a sentence is altered, providing insight into the model's fairness.
  - Quick check question: What does a lower SenseScore indicate about a model's fairness?

## Architecture Onboarding

- Component map: BERT/RoBERTa/ALBERT model -> Fine-tuning on Jigsaw dataset -> Debiasing methods (data perturbation, stratified sampling) -> Evaluation on balanced fairness dataset -> Fairness metrics calculation (FPR_gap, TPR_gap, AUC_gap)
- Critical path: Fine-tune language model -> Measure baseline fairness -> Apply debiasing method -> Re-measure fairness -> Compare results
- Design tradeoffs: Balancing the dataset for fairness may reduce overall model performance on toxicity detection; using synthetic data for balancing may introduce new biases; focusing on fairness for specific identity groups may neglect other potential sources of bias.
- Failure signatures: If fairness metrics do not improve after debiasing, or if model performance significantly degrades; if the SenseScore does not decrease after debiasing; if the balanced dataset does not accurately represent real-world toxicity distribution.
- First 3 experiments:
  1. Fine-tune a language model on the original Jigsaw dataset and measure baseline fairness using FPR_gap, TPR_gap, and AUC_gap on both the original and balanced evaluation datasets.
  2. Apply overamplification bias removal by fine-tuning the model on a balanced dataset and re-measure fairness.
  3. Compare the SenseScore of the model's predictions on the original and balanced datasets to evaluate counterfactual fairness.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but rather focuses on presenting its findings and recommendations. The open questions discussed in the paper's broader context include: How do different sources of bias interact with each other to impact fairness in toxicity detection? Are the findings on bias and fairness generalizable across different languages and cultural contexts? What are the long-term effects of debiasing techniques on model performance and fairness?

## Limitations

- The study is limited to English language datasets and Western cultural contexts, potentially limiting generalizability to other languages and cultures.
- Only three pre-trained language models (BERT, RoBERTa, ALBERT) are investigated, without exploring the impact of model size or architecture on fairness.
- The paper does not address the potential trade-off between fairness improvements and model performance degradation, which could be significant in real-world applications.

## Confidence

- High Confidence: The finding that downstream biases (sample and overamplification) have a stronger impact on fairness than upstream bias in language models.
- Medium Confidence: The claim that removing overamplification bias by fine-tuning on balanced datasets significantly improves fairness metrics.
- Low Confidence: The generalizability of the proposed debiasing methods to other toxicity detection tasks or datasets beyond the Jigsaw Toxicity Dataset.

## Next Checks

1. Validate the proposed debiasing methods on additional toxicity detection datasets, such as the Civil Comments dataset or the HateXplain dataset, to assess the generalizability of the findings.

2. Conduct experiments to quantify the trade-off between fairness improvements and model performance degradation when applying the proposed debiasing techniques.

3. Investigate the sensitivity of the proposed fairness metrics (FPR_gap, TPR_gap, AUC_gap) to different types of bias beyond identity-based bias, such as political bias or regional bias.