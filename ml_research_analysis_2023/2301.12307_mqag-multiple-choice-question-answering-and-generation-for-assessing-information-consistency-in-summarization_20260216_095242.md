---
ver: rpa2
title: 'MQAG: Multiple-choice Question Answering and Generation for Assessing Information
  Consistency in Summarization'
arxiv_id: '2301.12307'
source_url: https://arxiv.org/abs/2301.12307
tags:
- summary
- source
- question
- linguistics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new way to measure information consistency
  between a source text and a summary using multiple-choice question answering and
  generation (MQAG). The key idea is to measure the similarity between probability
  distributions over multiple-choice answer options, instead of comparing answer spans
  directly.
---

# MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization

## Quick Facts
- arXiv ID: 2301.12307
- Source URL: https://arxiv.org/abs/2301.12307
- Reference count: 34
- Outperforms existing methods for evaluating information consistency with Pearson correlation coefficients up to 0.942 on the SummEval dataset.

## Executive Summary
This paper introduces MQAG, a novel framework for assessing information consistency between source texts and summaries using multiple-choice question answering and generation. Instead of comparing answer spans directly, MQAG measures the similarity between probability distributions over multiple-choice answer options generated from both the summary and source. The framework uses T5 models for question generation and a Longformer model for answering, computing the negative KL-divergence between answer distributions as the consistency score. Experiments on four datasets demonstrate that MQAG achieves state-of-the-art performance in measuring information consistency, with strong correlations to human judgments.

## Method Summary
MQAG is a framework that assesses information consistency between source texts and summaries by comparing probability distributions over multiple-choice answer options. The method involves two stages: question generation and answering. Two T5-large models (g1 for question+answer generation and g2 for distractor generation) are fine-tuned on RACE to generate multiple-choice questions from both the summary and source. A Longformer model, also fine-tuned on RACE, predicts probability distributions over answer options for each question. The negative KL-divergence between the answer distributions from the summary and source is computed and averaged across all questions to obtain the MQAG score, which serves as the information consistency measure.

## Key Results
- MQAG achieves Pearson correlation coefficients up to 0.942 with human judgments on the SummEval dataset.
- The framework outperforms existing methods for evaluating information consistency on multiple datasets.
- Applying temperature annealing to the answer probability distributions helps account for domain shift and model overconfidence.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MQAG measures information consistency by comparing the KL divergence between probability distributions over multiple-choice answer options.
- Mechanism: Instead of comparing answer spans directly, MQAG generates multiple-choice questions from the summary and source, then uses a question answering model to predict the probability distribution over answer options. The negative KL-divergence between these distributions serves as the consistency score.
- Core assumption: The probability distribution of answer options contains sufficient information to capture the semantic content of the source and summary.
- Evidence anchors:
  - [abstract] "We propose a Multiple-choice Question Answering and Generation framework, MQAG, which approximates the information consistency by computing the expected statistical distance between summary and source answer distributions over automatically generated multiple-choice questions."
  - [section 3] "We define information consistency IC(x, y) as ∫q,o −KL(Pa(o|q, x), Pa(o|q, y)) Pg(q, o|y)dodq"
- Break condition: If the question answering model fails to capture the semantic meaning of the context and questions, the probability distributions will not accurately represent the information content.

### Mechanism 2
- Claim: MQAG outperforms span-based QA approaches by avoiding lexical matching and representation-based methods.
- Mechanism: By comparing probability distributions over multiple-choice options, MQAG captures a more abstract understanding of information consistency, avoiding the lexical biases present in span-based approaches.
- Core assumption: Multiple-choice answer distributions provide a more robust and abstract representation of information than direct span comparison.
- Evidence anchors:
  - [abstract] "This approach exploits multiple-choice answer probabilities, as predicted answer distributions can be easily compared."
  - [section 1] "In contrast, we attempt to measure information using multiple-choice questions, which allows for a more abstract understanding of information and enables convenient use of standard information-theoretic measures."
- Break condition: If the question generation model produces questions that are not relevant to the information consistency task, the distributions will not reflect meaningful semantic differences.

### Mechanism 3
- Claim: MQAG can be calibrated to account for domain shift and model overconfidence.
- Mechanism: By applying temperature annealing to the answer probability distributions, MQAG can adjust for the overconfidence of the QA model and domain mismatch between training (RACE) and evaluation datasets.
- Core assumption: The QA model's probability outputs can be adjusted to better reflect the true uncertainty and improve consistency measurement.
- Evidence anchors:
  - [section 6] "As an initial result, we show in Appendix B.3 that: First, the accuracy drops from around 80% on RACE to around 60% (or below) on summarization datasets. Second, we observe a performance gain by applying temperature annealing as shown in Fig. 4."
- Break condition: If the temperature annealing is not properly tuned, it may either over-smooth the distributions or fail to correct the overconfidence.

## Foundational Learning

- Concept: KL divergence and information-theoretic measures
  - Why needed here: The core metric for measuring information consistency in MQAG is based on the negative KL divergence between probability distributions.
  - Quick check question: Can you explain what KL divergence measures and why it is used to compare probability distributions in information theory?

- Concept: Question generation and answering with T5 and Longformer architectures
  - Why needed here: MQAG relies on fine-tuned T5 models for question generation and Longformer models for question answering to produce the answer distributions.
  - Quick check question: What are the key differences between T5 and Longformer architectures, and why are they suitable for this task?

- Concept: Calibration of probability distributions
  - Why needed here: The QA model's probability outputs need to be calibrated to account for domain shift and overconfidence, which is done using temperature annealing.
  - Quick check question: How does temperature annealing affect the shape of a probability distribution, and what is its effect on model confidence?

## Architecture Onboarding

- Component map:
  - Question Generation (g1) -> Distractor Generation (g2) -> Multiple-choice Questions
  - Question Answering (a) -> Probability Distributions
  - KL Divergence Computation -> MQAG Score

- Critical path:
  1. Generate multiple-choice questions from summary (g1, g2)
  2. Generate multiple-choice questions from source (g1, g2)
  3. For each question, use QA model to get probability distributions for source and summary
  4. Compute KL divergence between distributions
  5. Average KL divergences across all questions to get MQAG score

- Design tradeoffs:
  - Using multiple-choice questions instead of span-based answers avoids lexical matching but requires generating distractors
  - T5-large vs T5-base for generation: T5-base may yield more diverse questions due to higher perplexity
  - Longformer vs Roberta for answering: Longformer handles longer inputs but is slower; Roberta is faster but limited to shorter contexts

- Failure signatures:
  - Low KL divergence but high lexical overlap: Indicates the QA model may not be capturing semantic differences
  - High variance in MQAG scores across different question sets: Suggests question generation may not be consistent or diverse enough
  - MQAG scores correlate with extractive summaries more than abstractive: Indicates potential bias towards extractive content

- First 3 experiments:
  1. Run MQAG with N=1 question per summary and compare to N=50 to measure impact of question diversity
  2. Swap T5-large with T5-base in question generation and measure performance change
  3. Apply temperature annealing with different T values to calibrated distributions and measure effect on correlation with human judgments

## Open Questions the Paper Calls Out
- [inferred] The impact of question diversity on MQAG performance is not fully explored, as the paper only investigates the effect of the number of questions (N) but not the diversity of questions.
- [explicit] The paper discusses the calibration of the answering model's probability distribution and mentions that temperature annealing does not improve performance, but does not explore other calibration methods or investigate the impact of calibration on MQAG's performance.
- [inferred] The paper mentions that some questions can be answered irrespective of context and suggests that the selection of questions and options is an area for improvement, but does not provide details on how questions and options are selected or explore the impact of selection on performance.

## Limitations
- MQAG's dependence on the RACE dataset for training raises concerns about domain generalization, with accuracy drops from 80% on RACE to around 60% on summarization datasets.
- The computational cost of generating 50 multiple-choice questions per summary and running them through the QA model may limit scalability for large-scale evaluation.
- The optimal values for temperature annealing and their sensitivity are not thoroughly explored, and the calibration approach may not generalize well to all domains.

## Confidence
- **High confidence**: The core mechanism of using KL-divergence between probability distributions is well-grounded in information theory and the mathematical formulation is sound. The experimental results showing strong Pearson correlation coefficients with human judgments (up to 0.942 on SummEval) provide empirical support.
- **Medium confidence**: The claim that MQAG outperforms span-based QA approaches is supported by the results but the comparison is primarily against metrics that measure different aspects (like QAGS for faithfulness). The advantage over existing consistency metrics could be more directly established.
- **Low confidence**: The scalability and practical deployment of MQAG for real-world summarization evaluation remains uncertain given the computational requirements and the need for domain-specific calibration.

## Next Checks
1. **Domain Adaptation Test**: Fine-tune the QA model on a small subset of the target summarization dataset (e.g., 100 examples) and measure the improvement in MQAG scores and correlation with human judgments compared to the RACE-only baseline.

2. **Calibration Sensitivity Analysis**: Systematically vary the temperature annealing parameter across a wide range (e.g., T=0.1 to T=2.0) and plot the resulting MQAG scores and correlations to identify optimal calibration ranges and potential overfitting to specific temperature values.

3. **Computational Efficiency Benchmark**: Measure the wall-clock time and memory usage of MQAG when processing a batch of 100 summaries with N=50 questions each, and compare this to the time required by simpler metrics like BERTScore or ROUGE to quantify the practical trade-off between accuracy and efficiency.