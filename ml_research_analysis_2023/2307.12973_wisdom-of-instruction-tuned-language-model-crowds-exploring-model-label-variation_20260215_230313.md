---
ver: rpa2
title: Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation
arxiv_id: '2307.12973'
source_url: https://arxiv.org/abs/2307.12973
tags:
- tasks
- language
- llms
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether aggregate labels from multiple instruction-tuned
  LLMs can outperform individual models, similar to how human annotator variation
  can be leveraged. The authors evaluate five state-of-the-art LLMs on five subjective
  text classification tasks across four languages using zero-shot learning.
---

# Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation

## Quick Facts
- arXiv ID: 2307.12973
- Source URL: https://arxiv.org/abs/2307.12973
- Reference count: 20
- Key outcome: Aggregating labels from multiple instruction-tuned LLMs improves classification accuracy by 8.3 F1 points on average compared to individual models.

## Executive Summary
This paper investigates whether aggregate labels from multiple instruction-tuned large language models (LLMs) can outperform individual models, similar to how human annotator variation can be leveraged. The authors evaluate five state-of-the-art LLMs on five subjective text classification tasks across four languages using zero-shot learning. They find that while individual models vary widely in performance, aggregating their labels via majority voting or Bayesian MACE substantially outperforms any single model. However, even the best aggregated zero-shot learning results are still well below supervised models by over 10 F1 points on average. The results suggest LLMs are not yet ready to replace human annotation, despite the potential benefits of leveraging their variation through aggregation.

## Method Summary
The study evaluates five instruction-tuned LLMs (Flan-T5, Flan-UL2, T0, mT0, Tk-Instruct, Alpaca) on five subjective text classification tasks (sentiment analysis, topic detection, age/gender prediction, hate speech detection) across four languages using zero-shot prompting. The authors use two aggregation methods: majority voting and Bayesian MACE. Performance is measured using macro-F1 scores, inter-model agreement metrics (Cohen's κ, Fleiss' κ, Krippendorff's α), and MACE competence scores. Results are compared against supervised baselines to assess the gap between zero-shot and supervised performance.

## Key Results
- Aggregated labels from multiple LLMs outperform individual models by 8.3 F1 points on average across all tasks.
- MACE aggregation performs better than majority voting in most cases by weighting models based on estimated competence.
- Even the best zero-shot LLM performance remains over 10 F1 points below simple supervised models on average.
- Inter-model agreement varies significantly across tasks, with sentiment analysis showing high agreement (κ ~ 0.8) while demographic and hate speech tasks show little agreement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating labels from multiple instruction-tuned LLMs improves classification accuracy compared to any individual model.
- Mechanism: Different LLMs have specialized strengths on different tasks and languages. By combining their predictions (via majority voting or Bayesian MACE), we can leverage these complementary strengths to achieve better overall performance than relying on a single model.
- Core assumption: The errors made by different LLMs are not perfectly correlated, so combining their predictions reduces overall error.
- Evidence anchors:
  - [abstract] "However, aggregation techniques designed for human annotators perform substantially better than any one individual model. On average across all tasks, aggregated labels perform substantially better than the average LLM (by 8.3 points F1) and better than majority voting."
  - [section] "In the vast majority of cases, aggregated labels outperform even the best individual LLM. On average across all tasks, aggregated labels perform substantially better than the average LLM (by 8.3 points F1) and better than majority voting."
- Break condition: If all LLMs make the same types of errors, or if the errors are highly correlated, aggregation will not improve performance.

### Mechanism 2
- Claim: The reliability of different LLMs varies across tasks, and this can be quantified and used to weight their contributions in aggregation.
- Mechanism: MACE (Multi-Annotator Competence Estimation) computes a competence score for each LLM, representing the probability it chooses the correct label. These scores can be used to weight the models' predictions, giving more influence to more reliable models.
- Core assumption: The competence scores accurately reflect the models' true reliability on the task.
- Evidence anchors:
  - [section] "MACE computes the competence of each LLM (i.e., the probability it chooses the correct label instead of guessing it) and uses these weights to select the most likely label, usually leading to a more accurate aggregation outcome than majority voting."
  - [section] "The MACE competence score does correlate (though not strongly) with the actual performance of the models: 0.45 Spearman ρ and 0.43 Pearson ρ."
- Break condition: If the competence scores are inaccurate or do not correlate with true performance, weighted aggregation will not improve results.

### Mechanism 3
- Claim: Zero-shot learning with LLMs can provide reasonable performance for text classification tasks, but is not yet competitive with supervised models.
- Mechanism: LLMs can perform zero-shot classification by prompting them with task descriptions, without requiring any labeled training data. This provides a quick and cost-effective way to get initial labels for a task.
- Core assumption: The LLMs have learned enough about language and the world to make reasonable zero-shot predictions on unseen tasks.
- Evidence anchors:
  - [abstract] "However, even the best ZSL performance is usually still well below that of even simple supervised models, and substantially lower than Transformer-based supervised models (by over 10 F1 points on average)."
  - [section] "However, even the best ZSL result we achieved, be it from an LLM or aggregation method. Except for 4 cases... even the simple ML models beat the best ZSL result."
- Break condition: If the LLMs have not learned enough general knowledge, or if the tasks are too far outside their training distribution, zero-shot performance will be poor.

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: The LLMs are given zero-shot prompts describing the task and asking for a label. The quality of these prompts can significantly impact performance.
  - Quick check question: How would you formulate a prompt for a sentiment analysis task?

- Concept: Inter-annotator agreement
  - Why needed here: The level of agreement between different LLMs provides insight into the difficulty of the task and the reliability of the models. Low agreement suggests the task is difficult or the models are making different types of errors.
  - Quick check question: What does it mean if the inter-annotator agreement between LLMs is very low for a particular task?

- Concept: Bayesian aggregation
  - Why needed here: MACE uses a Bayesian model to estimate the competence of each LLM based on their agreement with each other. Understanding Bayesian inference is key to understanding how MACE works.
  - Quick check question: How does MACE use Bayesian inference to estimate the competence of each LLM?

## Architecture Onboarding

- Component map: Flan-T5 -> Flan-UL2 -> T0 -> mT0 -> Tk-Instruct -> Alpaca (LLM inputs) -> Zero-shot prompts -> LLM predictions -> Majority voting/MACE aggregation -> Macro-F1 evaluation -> Trustpilot/HatEval datasets (evaluation)
- Critical path: Generating LLM predictions → Aggregating predictions (majority voting/MACE) → Computing evaluation metrics
- Design tradeoffs: Using more LLMs in the aggregation could improve performance but increases cost and complexity. Similarly, more sophisticated aggregation methods could help but require more computation. The choice of aggregation method (majority voting vs. MACE) is another tradeoff between simplicity and potential performance gains.
- Failure signatures: If all LLMs perform poorly on a task, aggregation will not help. If the LLMs have highly correlated errors, aggregation will not improve performance. If the competence scores from MACE are inaccurate, weighted aggregation will not work well.
- First 3 experiments:
  1. Compare the performance of individual LLMs on each task to establish a baseline.
  2. Implement majority voting aggregation and compare its performance to individual LLMs.
  3. Implement MACE aggregation and compare its performance to majority voting and individual LLMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance gaps between LLM-based zero-shot learning and supervised models vary across different task types and domains?
- Basis in paper: [explicit] The paper shows that even the best ZSL performance is usually well below that of simple supervised models, with an average performance gap of over 10 F1 points. However, it does not provide a detailed analysis of how this gap varies by task type or domain.
- Why unresolved: The paper presents aggregate results across multiple tasks but does not break down the performance differences by specific task categories or domains to identify which types of tasks are more challenging for LLMs in zero-shot settings.
- What evidence would resolve it: Detailed comparative analysis showing F1 score differences between ZSL and supervised models across various task categories (sentiment analysis, topic detection, demographic prediction, hate speech detection) and different domains or datasets.

### Open Question 2
- Question: What specific characteristics of tasks or datasets make them particularly challenging for LLM-based zero-shot learning?
- Basis in paper: [inferred] The paper notes that some tasks like sentiment analysis have relatively high agreement scores while others like gender, age, and hate speech detection have little to no agreement, suggesting inherent difficulty differences.
- Why unresolved: While the paper identifies that task difficulty varies, it does not systematically investigate which specific task characteristics (e.g., label granularity, subjectivity, cultural context) contribute most to LLM performance challenges.
- What evidence would resolve it: Controlled experiments varying task characteristics (number of labels, subjectivity levels, cultural specificity) while measuring LLM performance to identify which factors most impact zero-shot learning effectiveness.

### Open Question 3
- Question: How does the cost-benefit tradeoff between LLM aggregation and human annotation vary across different organizational contexts and application domains?
- Basis in paper: [explicit] The paper discusses tradeoffs in accuracy, cost, and moral/ethical considerations between LLM and human annotation, noting that while LLMs are cost-effective, they don't match supervised model performance.
- Why unresolved: The paper presents general tradeoffs but doesn't provide specific cost-benefit analyses for different organizational sizes, application domains, or use cases that would help practitioners make informed decisions.
- What evidence would resolve it: Case studies comparing total costs (including annotation time, model development, error correction) and benefits (speed, scalability, accuracy) of LLM aggregation versus human annotation across different industries and organizational contexts.

## Limitations

- The findings may not generalize to tasks beyond subjective classification, such as factual question answering or logical reasoning.
- The computational cost of running multiple LLMs for aggregation presents practical limitations not fully addressed in the paper.
- The study doesn't explore hybrid approaches combining LLM predictions with human judgment, which could potentially yield better results.

## Confidence

**High Confidence:** The finding that aggregated LLM labels outperform individual models by 8.3 F1 points on average is well-supported by the experimental results across multiple tasks and languages. The statistical significance is demonstrated through bootstrap sampling.

**Medium Confidence:** The claim that zero-shot LLM performance remains substantially below supervised models (by over 10 F1 points) is supported by the data, though this comparison is somewhat limited by the specific supervised baselines chosen.

**Low Confidence:** The generalizability of MACE competence scores as reliable indicators of model performance across different domains and tasks, given the observed correlation of only 0.45 Spearman ρ.

## Next Checks

1. **Cross-domain validation**: Test the aggregation approach on tasks outside the subjective classification domain (e.g., factual question answering, logical reasoning) to assess generalizability.

2. **Cost-benefit analysis**: Measure the computational cost and inference time of aggregation compared to single models and supervised approaches, including the trade-off between performance gains and resource usage.

3. **Hybrid evaluation**: Experiment with combining LLM predictions with a small number of human annotations to determine if hybrid approaches can bridge the performance gap with fully supervised models while maintaining the benefits of LLM diversity.