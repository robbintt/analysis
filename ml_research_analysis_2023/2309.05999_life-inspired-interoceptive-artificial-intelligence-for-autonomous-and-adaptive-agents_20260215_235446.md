---
ver: rpa2
title: Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive
  Agents
arxiv_id: '2309.05999'
source_url: https://arxiv.org/abs/2309.05999
tags:
- internal
- states
- interoceptive
- state
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an interoceptive AI framework that introduces
  internal environment states into artificial agents, enabling them to maintain homeostasis
  and adapt to non-stationary environments. By factorizing state variables into internal
  and external components, and mapping rewards onto internal state dynamics, the framework
  addresses challenges in reinforcement learning such as dynamically changing goals
  and exploration-exploitation dilemmas.
---

# Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents

## Quick Facts
- arXiv ID: 2309.05999
- Source URL: https://arxiv.org/abs/2309.05999
- Reference count: 0
- Key outcome: Proposes an interoceptive AI framework introducing internal environment states into artificial agents for homeostasis and adaptation

## Executive Summary
This paper introduces an interoceptive artificial intelligence framework that draws inspiration from biological organisms' ability to monitor and regulate internal states. The framework proposes factorizing state variables into internal (homeostatic) and external components, with sparse interactions mediated through boundary states. By mapping rewards onto internal state dynamics rather than external outcomes, the approach addresses key challenges in reinforcement learning including dynamically changing goals, exploration-exploitation dilemmas, and stability-plasticity trade-offs. The framework offers both a novel perspective for building autonomous agents and computational models for studying interoception and affect in neuroscience.

## Method Summary
The interoceptive AI framework implements a Markov Decision Process with factorized state space, separating internal (homeostatic) states from external environmental states. Internal states maintain set points and bounded ranges, with rewards derived from deviations from homeostasis rather than external outcomes. Sparse interactions between internal and external states occur through boundary states, preserving stability while enabling adaptation. The policy network conditions decisions on both internal and external states, allowing context-dependent behavior that naturally balances exploration and exploitation based on internal needs. This approach enables autonomous agents to maintain stable value representations while adapting to non-stationary environments.

## Key Results
- Introduces state factorization into internal and external components as a foundation for autonomous agents
- Maps reward functions onto internal state dynamics to resolve exploration-exploitation dilemmas
- Demonstrates how homeostatic set points enable generalization to novel states through state-dependent valuation
- Provides computational framework for studying interoception and affect in neuroscience

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Internal-external state factorization enables autonomous agents to maintain stable value representations while adapting to non-stationary environments
- Mechanism: By separating internal (homeostatic) states from external environmental states and limiting their interaction through sparse boundary states, the agent can maintain stable internal dynamics that provide consistent context signals regardless of external fluctuations
- Core assumption: Internal states can be conditionally independent of external states given boundary states, allowing for stable value representations
- Evidence anchors:
  - [section] "The first core idea of interoceptive AI is the factorization of the environment states into the internal and external states... This internal-external factorization allows for a separation between the agent and its surroundings"
  - [section] "The internal and external states interact with each other via boundary states... This insulation ensures that the dynamics of the internal states remain relatively stable despite variations in external environment states"
  - [corpus] Weak - related papers mention interoceptive robots but lack detailed mathematical formalization of state factorization
- Break condition: If boundary states cannot maintain conditional independence between internal and external states, or if internal state dynamics become too coupled with external states

### Mechanism 2
- Claim: Mapping rewards onto internal state dynamics resolves exploration-exploitation and stability-plasticity dilemmas
- Mechanism: By making rewards dependent on internal state trajectories rather than external outcomes, agents naturally balance exploration and exploitation based on their internal needs, while preserving learned knowledge during environmental changes
- Core assumption: Internal states can serve as continuous context signals that modulate reward functions dynamically
- Evidence anchors:
  - [section] "The second core idea is to map the reward function onto the agent's internal states and their dynamics... The continuous decision regarding the consumption of food or water as a potential reward can be contingent upon the agent's internal state dynamics"
  - [section] "The interoceptive AI framework tackles the dilemma differently... the interoceptive agents can balance exploration and exploitation by adopting internal state-dependent strategies"
  - [corpus] Missing - corpus lacks papers specifically addressing these dilemmas through interoceptive frameworks
- Break condition: If internal state dynamics cannot provide sufficient information for meaningful reward modulation, or if the mapping becomes too complex to compute efficiently

### Mechanism 3
- Claim: Homeostatic set points and bounded internal states enable generalization and emergent autonomous behavior
- Mechanism: Internal states with set points and bounded ranges create attractor dynamics that allow agents to evaluate novel states based on their deviation from homeostasis, enabling zero-shot generalization and emergent goal-seeking behavior
- Core assumption: Internal state dynamics can be designed with attractors and bounded regions that correspond to survival-relevant behaviors
- Evidence anchors:
  - [section] "Having a set point has important computational implications... This enables value estimation based only on the states (i.e., state-dependent valuation), even for states never previously experienced"
  - [section] "In other words, living organisms should keep the essential variables within a certain limit (i.e., homeostasis) to survive"
  - [corpus] Weak - corpus mentions homeostatic frameworks but lacks detailed discussion of attractor dynamics and generalization properties
- Break condition: If internal state attractors become unstable or if bounded regions cannot be meaningfully defined for the target domain

## Foundational Learning

- Concept: Markov Decision Processes and state factorization
  - Why needed here: The framework builds directly on MDP formalism but extends it with internal-external state factorization
  - Quick check question: How does factorizing state space into internal and external components change the transition probability structure in an MDP?

- Concept: Free Energy Principle and active inference
  - Why needed here: Provides theoretical foundation for sparse boundary-mediated interactions and homeostatic regulation
  - Quick check question: How does the Markov blanket concept relate to the boundary states mediating internal-external interactions?

- Concept: Dynamical systems and attractor dynamics
  - Why needed here: Essential for understanding how internal state set points create stable yet adaptive behavior patterns
  - Quick check question: What is the relationship between homeostatic set points and attractors in state space?

## Architecture Onboarding

- Component map:
  - Internal state module -> Boundary state interface -> External state processor -> Policy network -> Reward computation engine

- Critical path:
  1. Observe external states → process through boundary states
  2. Update internal state dynamics based on boundary inputs
  3. Compute reward from internal state deviation
  4. Select actions conditioned on both internal and external states
  5. Execute actions and update external environment

- Design tradeoffs:
  - Granularity vs. computational cost: finer internal state discretization enables better homeostasis but increases computational load
  - Boundary sparsity vs. adaptability: sparser boundaries ensure stability but may limit environmental responsiveness
  - Set point flexibility vs. goal consistency: dynamic set points enable adaptation but may create unpredictable behavior

- Failure signatures:
  - Internal states oscillating wildly or diverging → boundary state implementation issues
  - Agent stuck in repetitive patterns → reward function not properly linked to internal dynamics
  - Poor adaptation to environmental changes → insufficient coupling between internal and external states

- First 3 experiments:
  1. Implement simple homeostatic agent in grid world with single internal resource (energy) that must be maintained
  2. Test multi-resource scenario where agent must balance competing internal needs (hunger vs. thirst)
  3. Evaluate adaptation to non-stationary environment where resource locations change over time while internal needs remain constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the mathematical properties of the internal environment of self-organizing biological organisms differ from those of artificial systems? If there are differences, how might they influence cognitive and affective functions?
- Basis in paper: [explicit] The paper discusses adopting life-inspired mathematical properties of internal environment states and mentions that "the fact that we can arbitrarily define survival by designing essential variables is important for building AI aligned with human values."
- Why unresolved: The paper proposes adopting life-inspired properties but does not explicitly compare the mathematical properties of biological vs artificial internal environments or their impact on cognitive functions.
- What evidence would resolve it: Computational models comparing internal state dynamics in biological organisms and artificial systems, demonstrating differences in cognitive and affective outcomes.

### Open Question 2
- Question: What are the distinctive properties of the goals derived from the internal state dynamics, such as homeostasis and allostasis, compared to those of goals in traditional reinforcement learning?
- Basis in paper: [explicit] The paper states "The second core idea is to map the reward function onto the agent's internal states and their dynamics" and discusses homeostatic set points.
- Why unresolved: While the paper introduces the concept of mapping rewards to internal state dynamics, it does not explicitly compare these properties to traditional RL goals or fully explore their distinctive characteristics.
- What evidence would resolve it: Comparative studies of goal achievement and adaptation in traditional RL vs interoceptive AI frameworks, highlighting differences in goal properties and outcomes.

### Open Question 3
- Question: How do cognitive processes and neural representations transform as goal states transition from static to dynamic?
- Basis in paper: [inferred] The paper discusses the interoceptive AI framework's ability to adjust goals based on internal states and mentions the transformation of value representations.
- Why unresolved: The paper proposes a framework for dynamic goal adjustment but does not explicitly address how cognitive processes and neural representations change during this transition from static to dynamic goals.
- What evidence would resolve it: Neuroimaging studies and computational models tracking changes in neural representations and cognitive processes as goal states shift from static to dynamic in interoceptive AI systems.

## Limitations

- Theoretical framework lacks empirical validation and specific mathematical formalization of key mechanisms
- Reward mapping onto internal state dynamics not fully specified, making implementation challenging
- State factorization assumptions about conditional independence between internal and external states require further verification

## Confidence

- High Confidence: The conceptual framework of interoceptive AI and its potential to address key challenges in autonomous agent design
- Medium Confidence: The proposed state factorization approach and its ability to maintain stable value representations
- Low Confidence: The practical implementation details for reward mapping and boundary state mechanisms

## Next Checks

1. **Formal Mathematical Validation**: Develop and verify the mathematical conditions under which internal states can maintain conditional independence from external states through boundary states in Markov Decision Processes.

2. **Implementation Feasibility Study**: Create a minimal prototype to test the computational tractability of reward functions derived from internal state dynamics in non-stationary environments.

3. **Comparative Performance Analysis**: Design experiments comparing interoceptive agents against standard reinforcement learning approaches in tasks requiring both stability and adaptability, measuring trade-offs in exploration-exploitation balance and generalization.