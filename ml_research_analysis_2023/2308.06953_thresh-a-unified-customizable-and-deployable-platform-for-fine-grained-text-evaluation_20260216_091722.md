---
ver: rpa2
title: 'Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text
  Evaluation'
arxiv_id: '2308.06953'
source_url: https://arxiv.org/abs/2308.06953
tags:
- thresh
- annotation
- text
- evaluation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Thresh is a unified, customizable, and deployable platform for
  fine-grained text evaluation. It allows users to build and test annotation interfaces
  for any evaluation framework within minutes using a single YAML configuration file.
---

# Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation

## Quick Facts
- arXiv ID: 2308.06953
- Source URL: https://arxiv.org/abs/2308.06953
- Reference count: 21
- Primary result: Unified platform for building and deploying fine-grained text evaluation interfaces in minutes using YAML configuration

## Executive Summary
Thresh is a platform that enables rapid creation and deployment of annotation interfaces for fine-grained text evaluation tasks. By using a YAML configuration file, users can build complex annotation interfaces within minutes rather than weeks of custom coding. The platform supports three selection types (single-span, multi-span, composite) and three boundary options (char, whitespace, subword) while providing extensive customization including multi-language support and a decision tree structure for complex typologies.

## Method Summary
Users create a YAML configuration file to define their annotation interface typology and provide data through a JSON file. The platform compiles this configuration into an interactive web interface that annotators use to select text spans and answer hierarchical questions. Annotations are saved in a unified JSON format that can be processed using the provided Python library. The platform supports multiple deployment options including hosted, serverless, programmatic, and crowdsource methods.

## Key Results
- Enables rapid deployment of complex annotation interfaces through YAML configuration
- Supports three span selection types with three boundary options for flexible text selection
- Provides unified data format enabling multi-task learning research and framework interoperability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: YAML configuration enables rapid deployment of complex annotation interfaces
- Mechanism: The YAML file defines a hierarchical decision tree structure that recursively builds the interface, allowing users to create complex typologies in minutes rather than weeks of custom coding
- Core assumption: Users can translate their evaluation frameworks into the YAML template format without extensive technical expertise
- Evidence anchors:
  - [abstract]: "By simply creating a YAML configuration file, users can build and test an annotation interface for any framework within minutes"
  - [section 4]: "Users use a YAML template to construct their interface and provide data with a JSON file"
- Break condition: If the YAML schema becomes too rigid to accommodate novel annotation frameworks, or if users cannot translate their typologies into the decision tree structure

### Mechanism 2
- Claim: Sub-word boundary selection reduces annotation time and improves data quality
- Mechanism: By snapping selections to sub-word boundaries using tokenizers like RobertaTokenizer, annotators avoid partial word selections and the platform supports languages without whitespace boundaries
- Core assumption: Tokenizers provide appropriate segmentation for the target languages and tasks
- Evidence anchors:
  - [section 3.1]: "we introduce sub-word boundaries as a third option, in which users can use any LLMs tokenizer of their choice... to specify a boundary: subword flag in the Y AML configuration file"
  - [section 3.1]: "This approach is limited in: (1) punctuation gets selected with adjacent words... (2) languages with no whitespace boundaries between words (e.g., Chinese) cannot be supported"
- Break condition: If tokenizers produce inappropriate segmentations for specific languages or domains, or if the sub-word snapping interferes with intended span selections

### Mechanism 3
- Claim: Unified data format enables multi-task learning research and framework interoperability
- Mechanism: All annotations are stored in a consistent JSON format with conversion scripts that bidirectionally adapt existing annotations to the unified schema
- Core assumption: The unified format captures sufficient information from diverse frameworks without lossy conversion
- Evidence anchors:
  - [abstract]: "the absence of a unified annotated data format inhibits the research in multi-task learning"
  - [section 4]: "Our unified fine-grained data format allows smooth transfer of analysis, agreement calculation and modeling code between different projects"
- Break condition: If critical information is lost during format conversion, or if the unified format cannot represent novel annotation schemes

## Foundational Learning

- YAML configuration and templating
  - Why needed here: Thresh uses YAML as the primary mechanism for defining annotation interfaces, making understanding YAML essential for customizing the platform
  - Quick check question: What YAML structure represents a decision tree with nested questions for span annotation?

- Span selection mechanics and text tokenization
  - Why needed here: Understanding how span selection works with different boundary options (char, whitespace, subword) is crucial for configuring appropriate annotation interfaces
  - Quick check question: How does sub-word boundary selection differ from whitespace snapping in terms of handling punctuation and non-space-separated languages?

- JSON data serialization and parsing
  - Why needed here: All annotations are stored in JSON format, and the platform includes Python tools for loading and processing this data
  - Quick check question: What JSON structure represents a selected span with its associated annotations and metadata?

## Architecture Onboarding

- Component map:
  - Frontend: Web-based annotation interface builder with YAML editor and preview
  - Backend: Template compilation engine that converts YAML to interactive interfaces
  - Python library: thresh package for programmatic template creation and annotation processing
  - Community hub: GitHub repository for sharing templates and annotated data
  - Deployment layer: Multiple deployment options (hosted, serverless, Python, crowdsource)

- Critical path:
  1. User creates YAML configuration file
  2. System compiles YAML into interactive interface
  3. Annotators select spans and answer decision tree questions
  4. Annotations saved in unified JSON format
  5. Python library processes annotations for downstream tasks

- Design tradeoffs:
  - Flexibility vs. complexity: Extensive customization options may increase learning curve
  - Performance vs. features: Rich interface features may impact loading times for large datasets
  - Standardization vs. adaptability: Unified format may not capture all domain-specific annotation needs

- Failure signatures:
  - YAML compilation errors indicating schema violations
  - Inconsistent annotation formats suggesting parsing issues
  - Slow interface loading indicating performance bottlenecks
  - Missing data in JSON outputs suggesting serialization errors

- First 3 experiments:
  1. Create a simple YAML template with single-span selection and binary annotation to verify basic compilation
  2. Configure a multi-language interface with custom translations to test localization features
  3. Deploy a template with existing annotations to verify bidirectional conversion functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of sub-word boundaries in span selection affect the quality and consistency of annotations across different languages, particularly those without whitespace boundaries between words?
- Basis in paper: [explicit] The paper introduces sub-word boundaries as a third option for span selection, addressing limitations of whitespace-based selection for languages without whitespace boundaries.
- Why unresolved: The paper mentions the introduction of sub-word boundaries but does not provide empirical evidence on how this affects annotation quality across different languages, especially those without whitespace boundaries.
- What evidence would resolve it: Comparative studies of annotation quality and consistency using character, whitespace, and sub-word boundaries across multiple languages, including those without whitespace boundaries.

### Open Question 2
- Question: What are the long-term impacts of using fine-grained human evaluation frameworks on the development and improvement of large language models?
- Basis in paper: [explicit] The paper discusses the use of fine-grained human evaluation for evaluating large language models and improving language models, but does not explore the long-term impacts.
- Why unresolved: The paper focuses on the current state of fine-grained evaluation but does not investigate how these frameworks might influence the evolution of large language models over time.
- What evidence would resolve it: Longitudinal studies tracking the development of large language models that incorporate feedback from fine-grained human evaluation frameworks.

### Open Question 3
- Question: How can Thresh's unified data format and community hub facilitate multi-task learning research and improve the transfer of knowledge between different NLP tasks?
- Basis in paper: [explicit] The paper mentions that the absence of a unified annotated data format inhibits research in multi-task learning and that Thresh provides a unified format for annotations.
- Why unresolved: While the paper highlights the potential benefits of a unified format, it does not provide concrete examples or empirical evidence of how this format can enhance multi-task learning research.
- What evidence would resolve it: Case studies demonstrating the application of Thresh's unified data format in multi-task learning experiments and the resulting improvements in model performance.

## Limitations
- Scalability concerns with very large datasets remain unverified
- Unified data format may not capture all specialized annotation needs
- Multi-language support effectiveness across diverse writing systems requires validation

## Confidence
**High Confidence Claims**:
- Thresh provides a unified YAML-based configuration system for building annotation interfaces
- The platform supports three selection types (single-span, multi-span, composite) with three boundary options
- Thresh offers multiple deployment methods including hosted, serverless, and programmatic options

**Medium Confidence Claims**:
- Sub-word boundary selection significantly reduces annotation time and improves data quality
- The unified data format enables seamless multi-task learning research
- The Python library effectively streamlines the entire annotation workflow

**Low Confidence Claims**:
- Thresh eliminates weeks of custom coding for complex annotation interfaces (requires empirical time studies)
- The community hub will achieve critical mass for template sharing and collaboration
- Thresh's performance matches or exceeds specialized annotation platforms for all use cases

## Next Checks
1. **Performance Benchmarking**: Test Thresh with progressively larger datasets (10K, 100K, 1M tokens) to measure interface loading times, memory usage, and annotation throughput. Compare performance metrics against existing annotation platforms.

2. **Multi-language Validation**: Deploy identical annotation interfaces across 5+ diverse languages (e.g., English, Chinese, Arabic, Finnish, Japanese) and measure annotation accuracy, completion rates, and user satisfaction. Document any tokenization issues or language-specific limitations.

3. **Format Interoperability Test**: Convert annotations from 10 existing frameworks to Thresh's unified format and back, then measure information loss and conversion accuracy. Assess whether critical annotation details are preserved through the bidirectional conversion process.