---
ver: rpa2
title: Size Lowerbounds for Deep Operator Networks
arxiv_id: '2308.06338'
source_url: https://arxiv.org/abs/2308.06338
tags:
- neural
- training
- above
- have
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how the size of DeepONets relates to their\
  \ ability to learn operators from data. The authors derive a lower bound on the\
  \ common output dimension of the branch and trunk nets needed to achieve low training\
  \ error on n data points, showing it must scale as \u03A9(\u221An)."
---

# Size Lowerbounds for Deep Operator Networks

## Quick Facts
- arXiv ID: 2308.06338
- Source URL: https://arxiv.org/abs/2308.06338
- Reference count: 15
- Primary result: Establishes Ω(√n) lower bound on DeepONet output dimension needed for low training error

## Executive Summary
This paper establishes theoretical lower bounds on the size of Deep Operator Networks (DeepONets) required to achieve low training error on noisy data. The authors prove that the common output dimension of branch and trunk networks must scale as Ω(√n) where n is the number of data points. They also demonstrate through experiments that to leverage increases in this output dimension, training data size must scale quadratically with it. The work provides fundamental insights into the relationship between DeepONet architecture and data requirements.

## Method Summary
The authors use isoperimetry of data distributions and covering number arguments to derive lower bounds on the common output dimension q of branch and trunk networks. They analyze when DeepONets can achieve empirical training error below a label noise threshold, showing that q ≥ Ω(√n) is necessary. The theoretical framework considers DeepONets as a function class with bounded weights, applying generalization bounds to establish the scaling relationship.

## Key Results
- Proves q must scale as Ω(√n) for DeepONets to achieve low training error on n data points
- Shows experimental evidence that n must scale quadratically with q to maintain monotonic improvement
- Demonstrates the intimate connection between empirical error and correlation with label noise
- Validates findings on advection-diffusion-reaction PDE with varying q and n values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The necessary output dimension q must scale as Ω(√n) to achieve low training error
- Mechanism: Leverages isoperimetry of data distribution and bounded weights assumption
- Core assumption: i.i.d. sampling from bounded domain with bounded network weights
- Evidence anchors: Abstract statement on Ω(√n) scaling, Theorem 4.1 proof
- Break condition: Non-i.i.d. data or unbounded weights

### Mechanism 2
- Claim: Training data size n must scale quadratically with q to leverage increased expressiveness
- Mechanism: Power law relationship between model capacity and data requirements
- Core assumption: Data distribution requires quadratic coverage growth
- Evidence anchors: Abstract on quadratic scaling, section 8 experimental results
- Break condition: Highly redundant data or strong inductive biases

### Mechanism 3
- Claim: Low empirical error requires low correlation with label noise
- Mechanism: Proof connecting empirical error to noise correlation via covering numbers
- Core assumption: Label noise is independent of input features
- Evidence anchors: Lemma 5.4 on error-noise connection, section 7.4 proof
- Break condition: Correlated noise or high model capacity

## Foundational Learning

- Concept: Universal Approximation Theorem for Operators
  - Why needed here: DeepONets generalize this theorem to function spaces
  - Quick check question: Can DeepONets approximate any continuous operator between function spaces with sufficient capacity?

- Concept: Rademacher Complexity
  - Why needed here: Used to derive generalization bounds for DeepONets
  - Quick check question: How does Rademacher complexity relate to generalization error?

- Concept: Isoperimetry
  - Why needed here: Key tool for deriving lower bounds on output dimension
  - Quick check question: How does isoperimetric inequality bound probabilities in high dimensions?

## Architecture Onboarding

- Component map: Input functions → Branch Net (Rd1 → Rq) → Trunk Net (Rd2 → Rq) → Inner product → Output
- Critical path: Branch Net → Trunk Net → Output Layer
- Design tradeoffs: Higher q increases expressiveness but computational cost and data needs
- Failure signatures: Too small q → underfitting; too large q relative to n → overfitting
- First 3 experiments:
  1. Train with small q (5) and large training data to test minimum dimension
  2. Train with large q (50) and small training data to test overfitting
  3. Train with q and n scaling as q√n fixed to test monotonic improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise scaling relationship between q and n for monotonic improvement?
- Basis in paper: Authors state quadratic scaling might be necessary
- Why unresolved: Theoretical bound suggests √n scaling, experiments suggest quadratic
- What evidence would resolve it: Systematic experiments varying q and n to determine exact relationship

### Open Question 2
- Question: How do results generalize to different types of PDEs?
- Basis in paper: Theoretical results are PDE-independent but experiments limited to one example
- Why unresolved: Only one PDE tested
- What evidence would resolve it: Experiments on diverse PDE types comparing results

### Open Question 3
- Question: How to extend proof techniques to exploit DeepONet neural network structure?
- Basis in paper: Authors note current proof hasn't fully exploited architectural structure
- Why unresolved: Current bounds are general, not architecture-specific
- What evidence would resolve it: New proof techniques incorporating architectural details

## Limitations

- Theoretical analysis relies on specific assumptions about data distribution and weight bounds
- Quadratic scaling relationship needs more extensive experimental validation across diverse PDEs
- Isoperimetric approach may not capture all aspects of DeepONet generalization

## Confidence

- Ω(√n) lower bound claim: Medium - Theoretical derivation sound but relies on assumptions
- Quadratic scaling experiments: Low - Limited to one PDE with specific parameters
- Connection between empirical error and label noise correlation: Medium - Proof provided but minimal experimental validation

## Next Checks

1. Test Ω(√n) lower bound on multiple PDE problems with varying dimensions and noise levels
2. Systematically vary both q and n across wider range to verify quadratic scaling relationship
3. Implement additional metrics to measure actual correlation between model predictions and label noise in experiments