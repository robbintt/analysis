---
ver: rpa2
title: 'RelationMatch: Matching In-batch Relationships for Semi-supervised Learning'
arxiv_id: '2305.10397'
source_url: https://arxiv.org/abs/2305.10397
tags:
- matrix
- learning
- semi-supervised
- cross-entropy
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RelationMatch addresses semi-supervised learning by enforcing in-batch
  relational consistency using a Matrix Cross-Entropy (MCE) loss. Instead of only
  aligning predictions for augmented views of individual samples, it encourages the
  model to preserve relationships between samples within each batch.
---

# RelationMatch: Matching In-batch Relationships for Semi-supervised Learning

## Quick Facts
- arXiv ID: 2305.10397
- Source URL: https://arxiv.org/abs/2305.10397
- Reference count: 27
- Key outcome: RelationMatch achieves 15.21% higher accuracy than FlexMatch on STL-10 with 40 labels using in-batch relational consistency via Matrix Cross-Entropy loss

## Executive Summary
RelationMatch addresses semi-supervised learning by enforcing in-batch relational consistency between weakly and strongly augmented views of data. The method introduces a Matrix Cross-Entropy (MCE) loss that operates on relationship matrices computed from batch predictions, encouraging the model to preserve similarity structures within the batch. Derived from both matrix analysis and information geometry perspectives, MCE generalizes the standard cross-entropy loss while maintaining favorable theoretical properties. The approach shows significant performance gains, particularly in low-label regimes, with minimal computational overhead.

## Method Summary
RelationMatch extends existing semi-supervised learning frameworks by adding a Matrix Cross-Entropy (MCE) loss term that captures in-batch relationships between samples. The method computes relationship matrices R(A) = AA⊤ for both weakly and strongly augmented data within each batch, then minimizes the MCE loss between these matrices. This encourages consistency in the relational structure rather than just individual sample consistency. The approach is trained using SGD with cosine learning rate scheduling, combining MCE with standard cross-entropy loss for labeled data.

## Key Results
- Achieves 15.21% higher accuracy than FlexMatch on STL-10 with only 40 labels
- Shows consistent improvements across CIFAR-10 and CIFAR-100 datasets
- Demonstrates effectiveness in supervised settings as well
- Adds minimal computational overhead (< 2%) to training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Matrix Cross-Entropy (MCE) enforces consistency in in-batch relational structures between weakly and strongly augmented views.
- **Mechanism:** The method computes relationship matrices R(A) = AA⊤ for both weakly and strongly augmented data. By minimizing MCE(P, Q), it encourages these matrices to align, capturing higher-order similarity patterns between samples within the batch.
- **Core assumption:** That preserving in-batch relational consistency (beyond individual sample consistency) provides richer supervisory signals than pairwise consistency alone.
- **Evidence anchors:**
  - [abstract]: "RelationMatch addresses semi-supervised learning by enforcing in-batch relational consistency using a Matrix Cross-Entropy (MCE) loss."
  - [section]: "we propose RelationMatch, which uses the matrix cross-entropy loss (MCE) to capture the in-batch relationships between the images."
- **Break condition:** If the model predictions are already perfectly clustered by class, then R(Aw) and R(As) will be identical, and MCE provides no additional gradient information.

### Mechanism 2
- **Claim:** MCE generalizes the standard cross-entropy loss and retains favorable theoretical properties from both matrix analysis and information geometry.
- **Mechanism:** The MCE loss is derived as a natural extension of cross-entropy to positive semi-definite matrices. It has properties like convexity, linearity, and invariance under unitary transformations.
- **Core assumption:** That these mathematical properties ensure stable optimization and meaningful gradients.
- **Evidence anchors:**
  - [abstract]: "The proposed MCE loss is rigorously derived from both matrix analysis and information geometry perspectives, ensuring theoretical soundness and practical efficacy."
  - [section]: "we adopt two distinct theoretical perspectives to generalize the cross-entropy loss of vectors to MCE, deriving from both matrix analysis and information geometry."
- **Break condition:** If the matrices P and Q in MCE are not positive semi-definite or become ill-conditioned, the optimization can become unstable.

### Mechanism 3
- **Claim:** By encouraging relational consistency, RelationMatch improves performance especially in low-label regimes like STL-10 with 40 labels.
- **Mechanism:** The relational consistency term acts as an additional regularizer that helps the model learn more discriminative features when labeled data is scarce.
- **Core assumption:** That the additional structural information from in-batch relationships is most valuable when labeled data is limited.
- **Evidence anchors:**
  - [abstract]: "Notably, we observed a substantial enhancement of 15.21% in accuracy over FlexMatch on the STL-10 dataset using only 40 labels."
  - [section]: "we observe RelationMatch incurs a significant performance uplift for STL-10, and consistent improvements for CIFAR-10 as well."
- **Break condition:** In fully supervised settings or when labeled data is abundant, the benefit of relational consistency may diminish.

## Foundational Learning

- **Concept:** Matrix (von Neumann) entropy and its relation to Shannon entropy
  - Why needed here: MCE is built upon matrix entropy, and understanding its relationship to classical entropy helps grasp why MCE is a meaningful generalization.
  - Quick check question: What is the matrix entropy of a diagonal density matrix with eigenvalues λ₁, λ₂, ..., λₙ?
    - Answer: It equals the Shannon entropy of the probability distribution given by the eigenvalues.

- **Concept:** Positive semi-definite matrices and density matrices
  - Why needed here: MCE operates on positive semi-definite matrices (specifically density matrices), and understanding their properties (e.g., spectral decomposition) is essential.
  - Quick check question: What conditions must a matrix satisfy to be a density matrix?
    - Answer: It must be symmetric, positive semi-definite, and have trace equal to 1.

- **Concept:** Matrix logarithm and its properties
  - Why needed here: MCE uses the matrix logarithm in its definition, and knowing its properties (e.g., existence, uniqueness for density matrices) is crucial.
  - Quick check question: Under what condition does a matrix A have a unique principal logarithm?
    - Answer: When A has no eigenvalues on the negative real axis.

## Architecture Onboarding

- **Component map:** Backbone model (WideResNet) -> Weak/Strong augmentations -> Predictions -> Cross-entropy + MCE losses -> SGD optimizer with momentum

- **Critical path:**
  1. Sample a batch with labeled and unlabeled data
  2. Apply weak and strong augmentations to unlabeled data
  3. Compute predictions for all samples
  4. Calculate standard cross-entropy loss for labeled data and pseudo-labeled unlabeled data
  5. Compute MCE loss between relationship matrices of weak and strong augmentations
  6. Combine losses and update model parameters

- **Design tradeoffs:**
  - **Computational overhead:** MCE adds minimal overhead (< 2%) but requires matrix operations on the batch
  - **Numerical stability:** Regularization (λI) may be needed for stable matrix logarithm computation
  - **Hyperparameter tuning:** The MCE loss balancing hyperparameter (μᵤ) needs careful tuning

- **Failure signatures:**
  - **Poor performance:** Could indicate that the relational consistency is not beneficial for the dataset or that hyperparameters are not well-tuned
  - **Training instability:** May suggest issues with the matrix logarithm computation or ill-conditioned matrices

- **First 3 experiments:**
  1. **Sanity check:** Verify that MCE reduces to cross-entropy when predictions are one-hot encoded
  2. **Ablation study:** Train with and without MCE on a small dataset (e.g., CIFAR-10 with 40 labels) to measure performance impact
  3. **Robustness test:** Add noise to the relationship matrices and observe the effect on training stability and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the matrix cross-entropy loss (MCE) provide additional benefits in scenarios beyond semi-supervised learning, such as in contrastive learning or self-supervised learning?
- Basis in paper: [inferred] The paper mentions that Balestriero & LeCun (2022) reinterpret SimCLR as doing element-wise matrix cross-entropy between relation matrices, and the authors suggest leaving the utilization of MCE in self-supervised learning for future work.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis on the effectiveness of MCE in contrastive or self-supervised learning settings.
- What evidence would resolve it: Experiments comparing MCE to standard contrastive learning losses (e.g., InfoNCE) on benchmarks like CIFAR-10/100 or ImageNet, or theoretical analysis of MCE's properties in these contexts.

### Open Question 2
- Question: How does the choice of regularization parameter λ (for the λI regularization in MCE) affect the performance and stability of RelationMatch across different datasets and label settings?
- Basis in paper: [explicit] The paper mentions adding λI regularization to the matrices P and Q in MCE for more stable convergence but does not discuss its impact.
- Why unresolved: The paper does not report experiments varying λ or analyze its sensitivity.
- What evidence would resolve it: Ablation studies showing accuracy/consistency across a range of λ values on multiple datasets (e.g., CIFAR-10, STL-10) and label regimes.

### Open Question 3
- Question: Is the matrix cross-entropy loss theoretically optimal for preserving in-batch relational consistency, or are there alternative formulations that could better capture the desired properties?
- Basis in paper: [inferred] The paper derives MCE from matrix analysis and information geometry perspectives but does not compare it to other possible losses for relational consistency.
- Why unresolved: The paper focuses on deriving and validating MCE but does not explore or compare alternative loss functions for the same task.
- What evidence would resolve it: Comparative analysis of MCE against other potential losses (e.g., based on different divergences or similarity measures) in terms of theoretical properties and empirical performance.

## Limitations

- Weak corpus support for RelationMatch's core contributions - no direct mentions of matrix cross-entropy for semi-supervised learning or in-batch relational consistency found in neighbor papers
- Limited ablation studies isolating the contribution of MCE versus other factors
- Benchmark-specific results without systematic analysis of when relational consistency is most beneficial

## Confidence

- Theoretical claims: Medium - mathematical derivations provided but connections to broader literature not well-established
- Empirical validation: Medium - substantial improvements reported but lack comprehensive ablation studies
- Novelty claims: Low - weak corpus support raises questions about novelty and existing similar approaches

## Next Checks

1. **Theoretical validation**: Rigorously verify the Taylor expansion implementation of MCE against the exact matrix logarithm, particularly for small batch sizes where higher-order terms matter
2. **Ablation study**: Isolate the contribution of MCE by comparing against alternative consistency losses (VAT, PiModel) while controlling for all other factors
3. **Robustness testing**: Systematically vary the MCE balancing hyperparameter μᵤ to identify the sensitivity and optimal operating range for different datasets