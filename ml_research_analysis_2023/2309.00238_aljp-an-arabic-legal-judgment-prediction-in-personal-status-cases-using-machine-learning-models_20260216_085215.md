---
ver: rpa2
title: 'ALJP: An Arabic Legal Judgment Prediction in Personal Status Cases Using Machine
  Learning Models'
arxiv_id: '2309.00238'
source_url: https://arxiv.org/abs/2309.00238
tags:
- cases
- char09
- judgment
- char40
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ALJP, an Arabic Legal Judgment Prediction
  system for personal status cases, specifically custody and annulment of marriage
  cases. The authors developed a dataset combining Ministry of Justice cases and simulated
  data from legal experts, addressing a gap in Arabic legal NLP.
---

# ALJP: An Arabic Legal Judgment Prediction in Personal Status Cases Using Machine Learning Models

## Quick Facts
- arXiv ID: 2309.00238
- Source URL: https://arxiv.org/abs/2309.00238
- Reference count: 29
- Primary result: Arabic Legal Judgment Prediction system for custody and annulment cases using SVM with word2vec achieving 88% accuracy

## Executive Summary
This paper introduces ALJP, an Arabic Legal Judgment Prediction system specifically designed for personal status cases in Saudi Arabia. The authors address a gap in Arabic legal NLP by developing a novel dataset combining Ministry of Justice cases with simulated data from legal experts. They apply various machine learning models (SVM, Logistic Regression) and deep learning models (LSTM, BiLSTM) with different text representations (TF-IDF, word2vec) to predict judgment outcomes and associated law articles, achieving promising results for custody and annulment of marriage cases.

## Method Summary
The authors developed an Arabic legal prediction dataset for personal status cases using a combination of Ministry of Justice cases and simulated data from legal experts. They formulated the problem as multi-class classification, taking pleading text as input to generate judgments and reasons/evidences as output. The methodology employed multiple machine learning models (SVM, Logistic Regression) and deep learning models (LSTM, BiLSTM) using TF-IDF and word2vec text representations. Hyperparameter tuning was performed using grid search for SVM and LR, while LSTM/BiLSTM models used 300 neurons with appropriate activation functions.

## Key Results
- SVM with word2vec achieved 88% accuracy for predicting custody judgments
- Logistic Regression with TF-IDF achieved 78% accuracy for annulment of marriage judgments
- SVM and Logistic Regression with word2vec achieved 88% accuracy for predicting probabilities of outcomes from claim and answer text in custody cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining real dataset from Ministry of Justice with simulated dataset from domain experts increases model robustness
- Mechanism: Mixed dataset provides authentic case patterns and coverage of edge cases
- Core assumption: Simulated data generated by legal experts is sufficiently realistic without introducing harmful biases
- Evidence anchors: Dataset combines MOJ and expert-generated data; simulated data validated by law professors

### Mechanism 2
- Claim: Using multiple ML and DL models with different text representations allows selection of best-performing combination
- Mechanism: Different algorithms and representations capture different aspects of text semantics
- Core assumption: Best model combination may differ across prediction tasks
- Evidence anchors: Tables show SVM with word2vec best for custody, LR with TF-IDF best for annulment

### Mechanism 3
- Claim: Multi-class classification framework enables structured prediction of judgment outcomes and law articles
- Mechanism: Clear framework maps input text to discrete legal outcomes and evidence categories
- Core assumption: Legal outcomes can be adequately represented as discrete classes
- Evidence anchors: Problem formulated as multi-class classification; models predict both outcomes and evidence

## Foundational Learning

- Concept: Arabic text preprocessing and normalization
  - Why needed here: Arabic text contains diacritics and complex morphology affecting model performance
  - Quick check question: Can you explain why removing diacritics and stop words is important for Arabic NLP tasks?

- Concept: Text representation methods (TF-IDF vs word embeddings)
  - Why needed here: Different representations capture different semantic relationships in text
  - Quick check question: What are the key differences between TF-IDF and word2vec representations, and when might one be preferred over the other?

- Concept: Multi-class classification vs multi-label classification
  - Why needed here: Paper uses both approaches for different prediction tasks
  - Quick check question: When would you choose multi-class classification over multi-label classification for a legal prediction task?

## Architecture Onboarding

- Component map: Data preprocessing -> Text representation (TF-IDF/word2vec) -> Model training (SVM/LR/LSTM/BiLSTM) -> Evaluation (accuracy, precision, recall, F1)
- Critical path: Dataset preparation -> Text preprocessing -> Model selection and training -> Performance evaluation
- Design tradeoffs: Multiple models increase computational cost but improve performance through selection; simulated data increases coverage but may introduce bias
- Failure signatures: Low accuracy on real data despite high accuracy on simulated data; overfitting to specific word representations; poor generalization across case types
- First 3 experiments:
  1. Train SVM with TF-IDF on real MOJ data only to establish baseline
  2. Train SVM with word2vec on combined real and simulated data to test data augmentation benefits
  3. Compare LSTM and BiLSTM with both representations on custody cases to identify optimal deep learning approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would model performance change with larger and more diverse Arabic legal dataset?
- Basis in paper: Authors noted deep learning models didn't outperform SVM due to small dataset size
- Why unresolved: Current dataset size and diversity may constrain complex model performance
- What evidence would resolve it: Training on larger, more diverse dataset and comparing performance metrics

### Open Question 2
- Question: How would incorporating contextual information from related cases or legal precedents affect accuracy?
- Basis in paper: Study focused solely on textual content without considering contextual information
- Why unresolved: Legal judgments often rely on precedents and contextual information from similar cases
- What evidence would resolve it: Implementing model incorporating contextual information and evaluating performance

### Open Question 3
- Question: How would model performance differ when evaluated on separate test set from different jurisdiction?
- Basis in paper: Dataset developed using Saudi Arabian cases; no testing on other jurisdictions
- Why unresolved: Legal systems vary significantly between jurisdictions, affecting model generalization
- What evidence would resolve it: Evaluating models on test set from different jurisdiction and comparing performance

## Limitations
- Relatively small dataset size (49 real cases + 79 simulated cases) limiting generalizability
- Performance metrics for predicting law articles (25-50% accuracy) suggest significant room for improvement
- Study focuses exclusively on Saudi Arabian personal status law, limiting applicability to other jurisdictions

## Confidence
- **High confidence** in experimental methodology and reported accuracy metrics for judgment outcome prediction
- **Medium confidence** in generalizability of results due to limited dataset size and single-jurisdiction focus
- **Medium confidence** in effectiveness of combining real and simulated data

## Next Checks
1. Replicate experiments using only real Ministry of Justice dataset to quantify simulated data contribution
2. Test trained models on legal cases from different countries or regions to evaluate generalizability
3. Evaluate model performance on cases from different time periods to assess longitudinal stability