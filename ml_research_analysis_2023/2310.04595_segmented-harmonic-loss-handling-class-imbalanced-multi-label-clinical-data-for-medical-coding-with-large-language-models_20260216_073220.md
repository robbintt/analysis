---
ver: rpa2
title: 'Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical Data
  for Medical Coding with Large Language Models'
arxiv_id: '2310.04595'
source_url: https://arxiv.org/abs/2310.04595
tags:
- loss
- codes
- classes
- class
- segments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) for medical
  coding of clinical notes, a challenging task due to extreme class imbalance and
  noisy data. The authors develop Segmented Harmonic Loss, a new loss function that
  segments the classes based on frequency and decouples co-occurring classes.
---

# Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical Data for Medical Coding with Large Language Models

## Quick Facts
- arXiv ID: 2310.04595
- Source URL: https://arxiv.org/abs/2310.04595
- Reference count: 40
- Primary result: Achieves over 10 percentage points improvement in F1 score for medical coding using LLMs with Segmented Harmonic Loss

## Executive Summary
This paper addresses the challenge of medical coding using large language models (LLMs) on highly imbalanced clinical datasets. The authors introduce Segmented Harmonic Loss, a novel loss function that segments the long-tailed ICD-9 code distribution and decouples co-occurring classes to handle extreme class imbalance. By combining this with embedding similarity filtering to remove invalid codes, their approach significantly outperforms previous methods on MIMIC III and IV datasets. The work demonstrates the potential of LLMs for medical coding while acknowledging remaining challenges around computational costs and quality control requirements in healthcare settings.

## Method Summary
The methodology employs a three-stage approach: first, embedding similarity thresholding removes ICD-9 codes not represented in truncated clinical notes by comparing note embeddings with code description embeddings; second, a recursive segmentation algorithm partitions the frequency distribution into segments with tolerable variance using hyperparameter ηS; third, separate models are trained for each segment using Segmented Harmonic Focal Loss, which balances negative sample contributions across segments. The approach uses clinicalBERT as the base model with LoRA fine-tuning and mixed-precision training to manage computational costs.

## Key Results
- Achieves over 10 percentage points improvement in F1 score compared to state-of-the-art methods on MIMIC III and IV datasets
- Successfully handles extreme class imbalance where head class frequencies are approximately 700 times greater than tail classes
- Embedding similarity filtering removes 7924 unique codes from training labels, reducing noise in the dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Segmented Harmonic Loss addresses class imbalance by dividing the long-tailed ICD-9 code distribution into segments with comparable frequency variance within each segment.
- Mechanism: The algorithm recursively segments the sorted frequency list from the tail end, ensuring that the standard deviation of frequencies within each segment does not exceed a fraction of the smallest frequency in that segment. This creates groups of classes where the frequency differences are tolerable, allowing separate models to be trained for each segment.
- Core assumption: A single model cannot effectively learn from data with orders of magnitude frequency differences between classes.
- Evidence anchors:
  - [abstract] "extreme class imbalance that we found to prevail in most medical data in a multi-label scenario by segmenting and decoupling co-occurring classes of the dataset with a new segmentation algorithm"
  - [section] "The frequencies of the head classes are ≈ 700 times greater than those of the tail classes"
  - [corpus] Weak evidence - no direct citations found in the corpus supporting this specific segmentation approach for class imbalance.

- Break condition: If the segmentation hyperparameter ηS is set too high, the variance within segments becomes too large, defeating the purpose of segmentation.

### Mechanism 2
- Claim: Embedding similarity thresholding removes invalid codes that are not represented in the truncated clinical notes, reducing false positives in training.
- Mechanism: The method computes cosine similarities between the average embeddings of the input note (divided into chunks) and the embeddings of each ICD-9 code's description. Codes with similarity below a threshold (0.55) are removed from the label set.
- Core assumption: ICD-9 code descriptions that are semantically similar to the clinical note text are likely to be relevant diagnoses for that note.
- Evidence anchors:
  - [section] "We devised a technique based on embedding similarity to tackle noisy data" and "This process removed 7924 unique codes from the code lists"
  - [section] "We found it to be very efficient in incorporating bidirectional medical context and clinical meaning compared to similar encoder models"
  - [corpus] No direct corpus evidence found for this specific embedding similarity approach to code filtering.

- Break condition: If the similarity threshold is set too high, relevant codes may be incorrectly removed; if too low, irrelevant codes remain.

### Mechanism 3
- Claim: Segmented Harmonic Focal Loss balances loss contributions from negative samples across different segments while maintaining discrimination capability.
- Mechanism: The loss function incorporates a modulating factor (1/βSH) that weights negative samples inversely proportional to their occurrence rate across segments. This prevents the overwhelming contribution of negative samples from more frequent classes while maintaining the necessary discrimination learning.
- Core assumption: Negative samples from different segments occur at different rates relative to the positive samples of the segment being trained, and this rate difference needs to be accounted for in the loss function.
- Evidence anchors:
  - [abstract] "developed Segmented Harmonic Loss, a new loss function to address the extreme class imbalance that we found to prevail in most medical data in a multi-label scenario"
  - [section] "SH (qr) = - 1/βSHr Σ log(qr_i)" and "SHF ocal(qr) = - 1/βSHr Σ (1 - qr)γlog(qr_i)"
  - [corpus] No direct corpus evidence found for this specific Segmented Harmonic Focal Loss formulation.

- Break condition: If βSH cannot be computed correctly (e.g., division by zero), the loss function becomes undefined.

## Foundational Learning

- Concept: Class imbalance in multi-label classification
  - Why needed here: Medical coding datasets like MIMIC have extreme class imbalance with a few codes occurring frequently and many occurring rarely, which degrades model performance on minority classes.
  - Quick check question: What is the ratio between the most frequent and least frequent ICD-9 codes in the MIMIC dataset used in this paper?

- Concept: Embedding similarity and cosine distance
  - Why needed here: The method uses embedding similarity to filter out ICD-9 codes that are not represented in the clinical note text, requiring understanding of how embeddings capture semantic meaning.
  - Quick check question: How does the method handle the case where a clinical note contains only 300 tokens but the BERT input limit is 512 tokens?

- Concept: Segmentation algorithms and recursive partitioning
  - Why needed here: The segmentation algorithm recursively partitions the frequency distribution to create segments with tolerable variance, which is key to the Segmented Harmonic Loss approach.
  - Quick check question: What is the role of the hyperparameter ηS in controlling the segmentation process?

## Architecture Onboarding

- Component map: Clinical notes → embedding similarity filtering → dataset segmentation → segment-specific model training with SH-Focal Loss → ensemble predictions across segments
- Critical path: Clinical notes → embedding similarity filtering → dataset segmentation → segment-specific model training with SH-Focal Loss → ensemble predictions across segments
- Design tradeoffs: Using separate models for each segment allows better handling of class imbalance but increases computational cost and complexity compared to a single unified model
- Failure signatures: Poor performance on tail classes suggests incorrect segmentation or insufficient negative sampling balance; high false positives suggest embedding similarity threshold issues
- First 3 experiments:
  1. Implement the segmentation algorithm with different ηS values and verify the variance constraint within segments
  2. Test the embedding similarity filtering with different thresholds (0.5, 0.55, 0.6) and measure the reduction in label cardinality
  3. Compare BCE, Focal Loss, and Segmented Harmonic Focal Loss on a single segment to verify the balancing effect of the modulating factor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the hyperparameter ηS for segmenting the long-tailed distribution of ICD-9 codes?
- Basis in paper: [explicit] The paper mentions that ηS is a hyperparameter that controls the tolerable variance between class frequencies in a segment, and it is set to 0.5 in the experiments.
- Why unresolved: The paper does not explore the impact of different values of ηS on the performance of the Segmented Harmonic Loss. It only uses one value (0.5) and shows that it works well for their dataset.
- What evidence would resolve it: Conducting experiments with different values of ηS and comparing the performance of the Segmented Harmonic Loss on the medical coding task would provide insights into the optimal value of ηS for this specific problem.

### Open Question 2
- Question: How does the Segmented Harmonic Loss perform on other imbalanced datasets outside of medical coding?
- Basis in paper: [inferred] The paper mentions that the segmentation algorithm and the Segmented Harmonic Loss are designed to handle extreme class imbalance in a multi-label scenario, which is a common problem in many real-world datasets.
- Why unresolved: The paper only evaluates the performance of the Segmented Harmonic Loss on medical coding datasets (MIMIC III and IV). It does not explore its effectiveness on other imbalanced datasets from different domains.
- What evidence would resolve it: Applying the Segmented Harmonic Loss to other imbalanced datasets from various domains and comparing its performance with other loss functions designed for imbalanced data would provide insights into its generalizability and effectiveness across different problem settings.

### Open Question 3
- Question: What are the limitations of using LLM-based models for medical coding in terms of computational costs and quality control?
- Basis in paper: [explicit] The paper mentions that the healthcare sector is wary of using open-source software due to litigation concerns, and that domain-specific training of LLMs can be prohibitively expensive until the hardware catches up with the rapid progress in R&D.
- Why unresolved: The paper does not provide a detailed analysis of the computational costs and quality control measures required for using LLM-based models in medical coding. It only briefly mentions these concerns as potential challenges.
- What evidence would resolve it: Conducting a comprehensive study on the computational requirements, training costs, and quality control measures for LLM-based models in medical coding would provide insights into the practical feasibility and limitations of using these models in real-world healthcare settings.

## Limitations
- No ablation studies to isolate contributions of segmentation, embedding filtering, and SH loss to performance gains
- Lack of direct comparison between segmented approach and unified model with standard loss functions on the same data
- Computational complexity of training multiple models for each segment not adequately addressed for larger LLMs

## Confidence
**High Confidence**: The observation that medical coding datasets exhibit extreme class imbalance, with frequency ratios between head and tail classes on the order of 700:1. This is supported by the authors' frequency analysis and aligns with general understanding of long-tailed distributions in healthcare data.

**Medium Confidence**: The effectiveness of the Segmented Harmonic Loss function in improving model performance. While the authors report significant improvements over state-of-the-art methods, the lack of ablation studies and direct comparisons to simpler approaches reduces confidence in attributing gains specifically to the SH loss formulation rather than other factors like model architecture or training procedures.

**Low Confidence**: The claim that embedding similarity filtering is "very efficient in incorporating bidirectional medical context and clinical meaning." This assertion is made without comparative evaluation against alternative filtering methods or sensitivity analysis of different similarity thresholds, making it difficult to assess whether the chosen approach is optimal or even necessary.

## Next Checks
1. **Ablation Study Implementation**: Run experiments with identical training procedures but progressively disable components: first test without embedding similarity filtering, then without segmentation (using a single model with SH loss), and finally compare against standard BCE loss without any modifications. This will isolate the contribution of each innovation.

2. **Threshold Sensitivity Analysis**: Systematically vary the embedding similarity threshold (0.45, 0.50, 0.55, 0.60) and the segmentation hyperparameter ηS across a range of values to determine the robustness of performance to these critical hyperparameters.

3. **Computational Complexity Assessment**: Measure training time and resource requirements for the segmented approach versus a single unified model on the same hardware, including memory usage patterns and scalability with segment count, to quantify the practical limitations of the methodology.