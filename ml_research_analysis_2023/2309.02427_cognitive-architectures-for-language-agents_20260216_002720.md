---
ver: rpa2
title: Cognitive Architectures for Language Agents
arxiv_id: '2309.02427'
source_url: https://arxiv.org/abs/2309.02427
tags:
- language
- arxiv
- memory
- agents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a cognitive architecture framework, CoALA,
  for language agents that extends large language models (LLMs) with internal memory
  and external action capabilities. CoALA organizes language agents into memory modules
  (working, episodic, semantic, procedural), internal actions (reasoning, retrieval,
  learning), and external actions (grounding).
---

# Cognitive Architectures for Language Agents

## Quick Facts
- arXiv ID: 2309.02427
- Source URL: https://arxiv.org/abs/2309.02427
- Authors: Anonymous
- Reference count: 40
- Key outcome: Proposes CoALA framework organizing LLM agents into memory modules, action types, and decision cycles; maps existing agents to validate approach

## Executive Summary
This paper introduces Cognitive Architectures for Language Agents (CoALA), a framework that extends large language models with structured memory and action capabilities inspired by decades of cognitive architecture research. CoALA organizes agents into four memory modules (working, episodic, semantic, procedural), two action types (internal and external), and iterative decision-making cycles. The framework is validated by mapping existing language agents like ReAct, Voyager, and Generative Agents to their CoALA components, and highlights future directions including structured working memory, learning beyond retrieval, and deliberate planning.

## Method Summary
The paper proposes a conceptual framework (CoALA) for organizing language agents by drawing analogies between LLMs and classical production systems. The framework defines four memory modules (working, episodic, semantic, procedural), two action types (internal: reasoning, retrieval, learning; external: grounding), and iterative decision-making cycles. The method involves mapping existing language agents to CoALA components and using the framework to identify gaps and propose future extensions. No specific implementation details, training procedures, or evaluation metrics are provided, as this is a conceptual rather than empirical contribution.

## Key Results
- CoALA provides a structured framework for organizing language agents into memory modules, action types, and decision cycles
- Existing agents (ReAct, Voyager, Generative Agents) can be mapped to CoALA components, validating the framework's generality
- The framework highlights key research directions including structured working memory, learning beyond retrieval, diverse action spaces, and deliberate planning

## Why This Works (Mechanism)

### Mechanism 1
CoALA maps LLM behaviors to classical cognitive architecture components, making complex LLM agents interpretable and modular. By drawing analogies between LLMs and production systems, CoALA frames LLM outputs as probabilistic rule applications, enabling systematic decomposition into memory modules, action types, and decision cycles. The core assumption is that the analogy between LLM token generation and production system rule firing is sufficiently strong to justify applying decades of cognitive architecture design principles. If LLM outputs are too stochastic or context-dependent for deterministic mapping to discrete production rules, the analogy breaks and the modular decomposition loses explanatory power.

### Mechanism 2
Organizing LLM agents into structured memory modules enables targeted improvements in reasoning, learning, and grounding. By explicitly separating working, episodic, semantic, and procedural memory, CoALA allows designers to diagnose where information is lost or duplicated, and to optimize retrieval, reasoning, and learning procedures for each memory type. The core assumption is that clear separation of memory types reflects actual computational benefits rather than just theoretical neatness. If memory modules become too fine-grained or overlap, the overhead of managing them outweighs any performance gains.

### Mechanism 3
Defining both internal (reasoning, retrieval, learning) and external (grounding) actions gives agents richer decision-making capabilities than pure prompt chaining. Internal actions allow agents to reflect on past experiences, generate new knowledge, and plan multi-step strategies before acting, while external actions ground decisions in real-world feedback. The core assumption is that agents benefit from deliberative internal reasoning before committing to external actions, not just reactive prompt chaining. If the cost of internal reasoning (LLM calls, memory management) exceeds the benefit of better-planned actions, agents may perform worse than simpler prompt-chaining approaches.

## Foundational Learning

- Concept: Production systems and control flow
  - Why needed here: Understanding the analogy between LLM token generation and production system rule application is essential for grasping CoALA's design rationale.
  - Quick check question: Can you explain how a language model's next-token prediction is analogous to a production system selecting and applying a rule?

- Concept: Cognitive architecture memory modules
  - Why needed here: CoALA's effectiveness relies on correctly mapping LLM agent capabilities to working, episodic, semantic, and procedural memory, each with distinct roles.
  - Quick check question: What is the difference between episodic and semantic memory in CoALA, and how does each support agent decision-making?

- Concept: Decision-making cycles with planning and execution stages
  - Why needed here: CoALA agents use structured decision cycles, separating planning (proposal, evaluation, selection) from execution, which is key to their deliberative behavior.
  - Quick check question: How does the planning stage in CoALA differ from simple prompt chaining, and why is this distinction important?

## Architecture Onboarding

- Component map:
  - Memory modules: working (short-term, active), episodic (past experiences), semantic (world knowledge), procedural (agent code and skills)
  - Actions: internal (reasoning, retrieval, learning) and external (grounding in physical, dialogue, or digital environments)
  - Decision procedure: iterative cycles with proposal, evaluation, selection (planning), then execution

- Critical path:
  1. Initialize working memory with task context and observations
  2. Use reasoning/retrieval to propose candidate actions
  3. Evaluate and select best action
  4. Execute action (internal or external)
  5. Update memories based on outcome
  6. Repeat cycle

- Design tradeoffs:
  - Granularity of memory modules: more modules enable specialization but increase management complexity
  - Frequency of learning vs. acting: more learning improves adaptation but slows real-time performance
  - Cost of internal reasoning: richer deliberation improves decisions but increases computational overhead

- Failure signatures:
  - Memory bloat: episodic or semantic memory grows too large, slowing retrieval
  - Decision paralysis: too many candidate actions or slow evaluation prevents timely responses
  - Feedback loop failure: grounding actions not properly processed into working memory, breaking planning

- First 3 experiments:
  1. Implement a minimal CoALA agent with only working and procedural memory, test on a simple text game
  2. Add episodic memory and learning to store past game states, measure improvement in task completion
  3. Introduce semantic memory for external knowledge (e.g., game rules), evaluate impact on planning quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should language agents decide when to engage in planning versus execution, and what are the optimal trade-offs between planning time and execution quality?
- Basis in paper: [explicit] The paper discusses this as a key research direction, noting that "making a call to an LLM is both slow and computationally intensive" and that "relying on LLMs for decision-making thus requires balancing the cost of planning against the utility of the resulting improved plan."
- Why unresolved: Current approaches fix a search budget by specifying a depth of reasoning, but biological agents adaptively allocate computation based on expected utility. The paper notes that "future work should develop mechanisms to estimate the utility of planning and modify the decision procedure accordingly."
- What evidence would resolve it: Empirical studies comparing different planning strategies with varying depths and evaluation of their impact on task performance and computational efficiency.

### Open Question 2
- Question: What is the optimal boundary between internal and external actions in language agents, and how does this boundary affect agent design and capability?
- Basis in paper: [explicit] The paper raises this question directly, noting that "the boundary of language agents has become vaguer as a result of their digital grounding" and discussing examples like whether Wikipedia is internal or external.
- Why unresolved: The distinction between internal and external actions is increasingly blurry in digital environments, and the paper suggests using controllability and indispensability as criteria, but these are subjective and context-dependent.
- What evidence would resolve it: Systematic analysis of agent performance and safety across different boundary definitions, and empirical studies of how different boundary placements affect learning and decision-making capabilities.

### Open Question 3
- Question: How can language agents learn new decision-making procedures autonomously, and what are the risks and benefits of allowing such learning?
- Basis in paper: [explicit] The paper identifies this as an understudied area, noting that "learning new learning and decision procedures can be seen as ways of 'meta-learning' and 'meta-decision making' for language agents, and are currently understudied due to their difficulty and high risk."
- Why unresolved: While the framework allows for procedural memory updates, current agents avoid this due to safety concerns and technical challenges. The paper notes this is "theoretically possible" but "significantly riskier than writing to episodic or semantic memory."
- What evidence would resolve it: Development of safe mechanisms for procedural learning, empirical studies of agents that successfully learn new decision procedures, and analysis of the trade-offs between adaptability and stability.

## Limitations
- Framework lacks empirical validation and specific implementation details
- No evaluation metrics or training procedures provided for new agents
- Claims about benefits of structured memory and planning are forward-looking, not yet supported by results

## Confidence

- **High Confidence**: The cognitive architecture analogy and memory/action categorization are grounded in established cognitive science principles and provide a coherent organizational framework for thinking about language agent design.

- **Medium Confidence**: The mapping of existing language agents (ReAct, Voyager, etc.) to CoALA components is reasonable and helps illustrate the framework, but the paper does not provide quantitative evidence that this mapping reveals new insights or capabilities.

- **Low Confidence**: Claims about the benefits of structured working memory, learning beyond retrieval, diverse action spaces, and deliberate planning are forward-looking and not yet supported by empirical results or ablation studies.

## Next Checks

1. **Implement and benchmark a minimal CoALA agent** (working + procedural memory only) against a standard prompt-chaining baseline on a simple text game or reasoning task, measuring completion rate and decision quality.

2. **Ablation study of memory modules** - systematically remove or merge memory types (e.g., episodic vs. semantic) in an existing agent and measure the impact on retrieval accuracy, planning efficiency, and task performance.

3. **Cost-benefit analysis of internal reasoning** - compare agents with and without structured deliberation cycles (proposal/evaluation/selection) on tasks requiring multi-step planning, measuring both decision quality and computational overhead (LLM calls, memory management).