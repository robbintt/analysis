---
ver: rpa2
title: Axiomatic Preference Modeling for Longform Question Answering
arxiv_id: '2312.02206'
source_url: https://arxiv.org/abs/2312.02206
tags:
- answer
- answers
- question
- human
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an axiomatic framework for training preference
  models for long-form question answering, addressing the limitations of existing
  reward models that lack principled guidance and diverse training signals. The proposed
  method constructs contrastive training pairs based on five axioms: usefulness, relevance,
  groundedness, truthfulness, and thoroughness, using both human-written answers and
  LLM-generated responses.'
---

# Axiomatic Preference Modeling for Longform Question Answering

## Quick Facts
- arXiv ID: 2312.02206
- Source URL: https://arxiv.org/abs/2312.02206
- Reference count: 35
- Primary result: A 220M parameter axiomatic preference model outperforms GPT-4 and existing reward models on long-form question answering preference scoring

## Executive Summary
This paper addresses the challenge of training effective preference models for long-form question answering by introducing an axiomatic framework that encodes explicit principles of human preference. The approach constructs contrastive training pairs based on five axioms (usefulness, relevance, groundedness, truthfulness, and thoroughness) using both human-written answers and LLM-generated responses. The resulting preference model demonstrates superior performance in ranking and preference scoring compared to larger models like GPT-4, while being more efficient and interpretable.

## Method Summary
The method constructs a preference model using an axiomatic framework where training pairs are generated based on five principles: usefulness (upvotes), relevance (related questions), groundedness (evidence incorporation), truthfulness (fact corruption), and thoroughness (answer combination). The model is trained using margin loss on these contrastive pairs, with a T5-base architecture serving as the cross-encoder. The approach specifically addresses the challenge of scoring both human-written and LLM-generated answers on the same scale, which is critical for RLHF applications.

## Key Results
- A 220M parameter preference model outperforms GPT-4 in agreement with human preferences on long-form QA
- The axiomatic framework enables smaller models to learn nuanced preference distinctions through targeted training signals
- The model successfully scores human- and LLM-generated answers on the same scale, addressing a key limitation of naive reward models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Axiomatic preference models outperform naive reward models by encoding explicit principles rather than learning from noisy scalar preferences alone.
- **Mechanism**: Each axiom generates contrastive training pairs where one answer is clearly preferred based on a defined principle, providing richer training signals than simple upvote-based pairs.
- **Core assumption**: Human preferences can be decomposed into discrete, learnable principles that can be operationalized into training data.
- **Evidence anchors**: [abstract] "Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4"; [section 2.1] "We treat answers which have relatively higher upvotes as being more useful or helpful"

### Mechanism 2
- **Claim**: The axiomatic framework enables training signals that are transferable across human-written and LLM-generated answers.
- **Mechanism**: By generating pairs based on principles rather than style or length, the model learns to evaluate content quality independent of generation method.
- **Core assumption**: The same principles that govern human preference for human-written answers apply to LLM-generated answers.
- **Evidence anchors**: [abstract] "training a standalone preference model that can score human- and LLM-generated answers on the same scale"; [section 4.1] "PM 0 (a naive model trained only on upvotes) falls short of these expectations, which we believe is due to stylistic differences in LLM-generated answers"

### Mechanism 3
- **Claim**: Small preference models can outperform much larger models in preference scoring tasks.
- **Mechanism**: The axiomatic framework provides high-quality, principle-targeted training data that enables efficient learning. The preference task is simpler than generation, requiring fewer parameters to capture relevant distinctions.
- **Core assumption**: Identifying preference relationships is cognitively simpler than generation, and the axiomatic signals provide sufficient information for this task.
- **Evidence anchors**: [abstract] "showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring"; [section 4.3] "despite GPT-4's advantage, our 220M parameter PM 0-5 has higher agreement with gold human preferences"

## Foundational Learning

- **Concept: Contrastive learning**
  - Why needed here: The preference model learns by comparing pairs of answers, requiring understanding of how contrastive objectives work.
  - Quick check question: What is the difference between a contrastive loss and a standard regression loss in the context of preference modeling?

- **Concept: Axiomatic reasoning**
  - Why needed here: The framework builds training pairs based on explicitly defined principles (axioms), requiring understanding of how to operationalize abstract concepts.
  - Quick check question: How would you define an axiom for the principle of "conciseness" in long-form question answering?

- **Concept: Reward modeling and RLHF**
  - Why needed here: The work builds on RLHF techniques but improves upon them by addressing limitations in existing reward models.
  - Quick check question: What are the key differences between a naive reward model trained on scalar preferences versus an axiomatic preference model?

## Architecture Onboarding

- **Component map**: Question Encoder -> Answer Encoder -> Cross-Encoder -> Preference Score -> Margin Loss
- **Critical path**: Question → Answer → Cross-Encoder → Preference Score → Margin Loss → Model Update
- **Design tradeoffs**: 
  - Parameter efficiency vs. expressiveness (220M vs. 7B+ models)
  - Axiomatic signal quality vs. data quantity
  - Generalizability vs. overfitting to specific principles
- **Failure signatures**: 
  - Low agreement with human preferences
  - Inability to score human and LLM answers on same scale
  - Overfitting to spurious signals like length or style
- **First 3 experiments**:
  1. Train PM 0 (only on upvotes) and evaluate on held-out Stack Exchange data to establish baseline
  2. Add Axiom 1 (relevance) to create PM 0-1 and compare performance on hard negatives
  3. Add Axioms 2-5 to create PM 0-5 and evaluate on research questions and axiomatic held-out pairs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would the axiomatic preference model perform on non-longform QA tasks like classification or summarization?
- **Basis in paper**: Inferred - The paper discusses the PM's effectiveness for longform QA but doesn't explore other NLP tasks.
- **Why unresolved**: The axioms and training methodology were specifically designed for longform QA. Their generalizability to other tasks is unknown.
- **What evidence would resolve it**: Experiments applying the axiomatic framework to classification, summarization, and other NLP tasks to measure performance gains compared to standard training methods.

### Open Question 2
- **Question**: What is the impact of using different margin computation strategies on the PM's performance?
- **Basis in paper**: Explicit - The paper mentions using log10(votes(a+)/votes(a-)) as the margin for human-LLM pairs but uses a fixed margin of 0.25 for LLM-LLM pairs. It also discusses using GPT-4 to compute margins.
- **Why unresolved**: The paper only experiments with a few margin computation strategies. The optimal strategy is unclear.
- **What evidence would resolve it**: Ablation studies testing various margin computation methods (e.g., constant, log-ratio, learned, GPT-4 scores) on the PM's ranking and agreement metrics.

### Open Question 3
- **Question**: How sensitive is the PM's performance to the choice of LLM used for generating axiomatic pairs?
- **Basis in paper**: Explicit - The paper uses ChatGPT for generating most axiomatic pairs but doesn't explore other LLMs.
- **Why unresolved**: Different LLMs may generate pairs with varying quality and diversity, potentially impacting the PM's learned preferences.
- **What evidence would resolve it**: Training and evaluating PMs using axiomatic pairs generated by different LLMs (e.g., Vicuna, GPT-4) and comparing their performance on the same evaluation datasets.

## Limitations
- The axiomatic framework relies heavily on the quality of generated training pairs, which may introduce biases or miss complex preference dimensions
- The evaluation focuses on Stack Exchange data and specific research analysis questions, limiting generalizability to other domains
- The decomposition of human preferences into five discrete axioms may oversimplify the nuanced and context-dependent nature of preference judgments

## Confidence

**High Confidence**:
- The axiomatic framework provides richer training signals than naive upvote-based preference models
- Small preference models (220M parameters) can achieve competitive performance with larger models on preference scoring tasks
- The framework successfully addresses the issue of scoring human and LLM-generated answers on the same scale

**Medium Confidence**:
- The specific axioms chosen capture the most important dimensions of human preference for long-form QA
- The 220M parameter model will maintain its performance advantage as it scales to different domains and question types
- The model's interpretability advantages translate to practical benefits in real-world deployment

**Low Confidence**:
- The model would maintain its performance advantage against future, more advanced LLMs beyond GPT-4
- The axiomatic framework would be equally effective for other natural language generation tasks beyond long-form QA
- The training efficiency gains would scale proportionally as model size increases

## Next Checks

1. **Ablation study across diverse domains**: Conduct systematic ablation studies testing each axiom's contribution across multiple domains (e.g., technical support, creative writing, medical advice) to identify which principles are universally important versus domain-specific.

2. **Human preference alignment analysis**: Design experiments comparing model preference judgments against human preferences on a diverse set of answer pairs, including cases where human preferences may conflict with axiomatic principles (e.g., conciseness vs. thoroughness trade-offs).

3. **Robustness to adversarial inputs**: Test the preference model's performance on deliberately crafted answer pairs that violate multiple axioms simultaneously, assessing whether the model can still identify subtle quality differences or becomes confused when multiple principles conflict.