---
ver: rpa2
title: Imitation Learning from Purified Demonstrations
arxiv_id: '2310.07143'
source_url: https://arxiv.org/abs/2310.07143
tags:
- learning
- demonstrations
- imitation
- imperfect
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning from imperfect demonstrations
  in imitation learning. The key idea is to first purify the imperfect demonstrations
  by smoothing potential perturbations via a forward diffusion process and then recover
  optimal demonstrations via a reverse diffusion process.
---

# Imitation Learning from Purified Demonstrations

## Quick Facts
- arXiv ID: 2310.07143
- Source URL: https://arxiv.org/abs/2310.07143
- Reference count: 40
- Primary result: DP-BC achieves 1.3× higher average reward than best baseline on Ant-v2 with D3 demonstrations

## Executive Summary
This paper introduces Diffusion Purified Imitation Learning (DP-IL), a method that addresses the challenge of learning from imperfect demonstrations in imitation learning. The approach uses a two-step diffusion process: first, forward diffusion adds noise to imperfect demonstrations to smooth out perturbations, and then reverse diffusion recovers an optimal demonstration distribution. The method theoretically bounds the distance between purified and optimal distributions and empirically demonstrates consistent improvements over state-of-the-art methods on MuJoCo tasks.

## Method Summary
The method trains a diffusion model on a small set of optimal demonstrations to learn the denoising score function. Imperfect demonstrations undergo forward diffusion at a carefully selected timestep t*, then reverse diffusion recovers purified demonstrations. These purified demonstrations are used for imitation learning via either behavior cloning (BC) or generative adversarial imitation learning (GAIL). The approach reduces distribution mismatch between expert and agent policies by operating on data statistically closer to optimal demonstrations.

## Key Results
- DP-BC achieves 1.3× higher average reward (2547) than best baseline on Ant-v2 with D3 demonstrations
- DP-IL consistently outperforms state-of-the-art methods across MuJoCo tasks with imperfect demonstrations
- MMD distance between optimal and purified distributions is significantly reduced, validating purification effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Forward diffusion reduces total variance distance between purified and optimal distributions by smoothing perturbations
- **Mechanism:** Gradual noise injection progressively reduces statistical differences (KL divergence) between distributions while preserving semantic structure
- **Core assumption:** Perturbations manifest as distinct statistical features that can be averaged out
- **Evidence anchors:** [abstract] total variance distance upper-bounded; [section 3.3] gap between distributions decreases with timestep; corpus papers weak evidence
- **Break condition:** Large perturbations relative to diffusion noise, or fragile semantic structure lost during diffusion

### Mechanism 2
- **Claim:** Reverse diffusion recovers optimal demonstration distribution from diffused imperfect demonstrations
- **Mechanism:** Learned score function iteratively denoises diffused demonstrations back to original state space
- **Core assumption:** Denoising score function generalizes from optimal to diffused demonstrations
- **Evidence anchors:** [abstract] reverse process recovers optimal demonstrations; [section 3.2] denoising from timestep i* to 0; corpus papers weak evidence
- **Break condition:** Poor score function estimation or excessive semantic information loss during forward diffusion

### Mechanism 3
- **Claim:** Learning from purified demonstrations improves imitation learning performance
- **Mechanism:** Purification reduces distribution matching error between expert and agent policies
- **Core assumption:** Purification sufficiently reduces statistical distance to meaningfully improve learning
- **Evidence anchors:** [abstract] DP-IL outperforms state-of-the-art; [section 4.1] mixed demonstrations show substantial margin; corpus papers moderate evidence
- **Break condition:** Purification introduces artifacts or remaining distribution mismatch too large

## Foundational Learning

- **Concept: Diffusion probabilistic models**
  - Why needed here: Core mechanism for smoothing and recovering data distributions
  - Quick check question: What is the mathematical relationship between forward diffusion and reverse processes in diffusion models?

- **Concept: Occupancy measure and distribution matching**
  - Why needed here: Core objective for imitation learning and comparing expert/agent policies
  - Quick check question: How does occupancy measure ρπ relate to policy π in terms of one-to-one correspondence?

- **Concept: f-divergence and variational bounds**
  - Why needed here: Measures distribution differences and formulates optimization objective
  - Quick check question: What is the relationship between f-divergence and its variational lower bound in imitation learning?

## Architecture Onboarding

- **Component map:** Optimal demonstrations (Do) -> Diffusion model training -> Forward diffusion on imperfect demonstrations (Dn) -> Reverse diffusion -> Purified demonstrations -> Imitation learning (BC/GAIL) -> Policy

- **Critical path:** 1) Train diffusion model on optimal demonstrations; 2) Forward diffusion with optimal timestep selection; 3) Reverse diffusion to recover purified demonstrations; 4) Train imitation learning algorithm; 5) Evaluate policy performance

- **Design tradeoffs:** Timestep selection (larger for better smoothing vs. information preservation); Dataset size (more optimal demonstrations improve training vs. cost); Algorithm choice (BC simpler vs. GAIL more robust)

- **Failure signatures:** Poor performance despite purification (insufficient semantic preservation or wrong timestep); Diffusion model training instability (insufficient optimal demonstrations or wrong architecture); Reverse diffusion failure (poor score function or excessive forward noise)

- **First 3 experiments:** 1) Ablation study varying diffusion timestep t* on simple MuJoCo task with known optimal policy; 2) Compare purified vs non-purified performance on Ant-v2 with D3 demonstrations; 3) Evaluate different diffusion model architectures on same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of timestep t* affect trade-off between removing perturbations and preserving semantic information?
- Basis in paper: Explicit - discusses importance of t* and provides theoretical analysis
- Why unresolved: Optimal t* depends on demonstration characteristics requiring further investigation
- What evidence would resolve it: Systematic experiments varying t* across tasks and demonstration qualities

### Open Question 2
- Question: How does diffusion purification perform with different noise types (uniform, correlated) vs. Gaussian noise?
- Basis in paper: Inferred - only considers Gaussian noise in experiments
- Why unresolved: Performance with other noise types not explored
- What evidence would resolve it: Experiments with various noise types compared to baseline methods

### Open Question 3
- Question: Can diffusion purification handle multi-modal demonstrations (visual observations, proprioceptive states)?
- Basis in paper: Inferred - focuses on state-action pairs, doesn't discuss multi-modal application
- Why unresolved: Effectiveness on multi-modal demonstrations not explored
- What evidence would resolve it: Experiments with multi-modal demonstrations compared to baseline methods

## Limitations

- Timestep selection (t*) is critical and varies significantly across tasks without systematic guidance
- Method tested only on Gaussian noise models, limiting generalization to other demonstration imperfections
- Comparison limited to specific MuJoCo tasks without broader domain validation

## Confidence

**High confidence**: Core mechanism of using diffusion models for demonstration purification and empirical superiority over baselines on tested MuJoCo tasks.

**Medium confidence**: Theoretical bounds on total variance distance between purified and optimal distributions, as these rely on specific assumptions about diffusion process.

**Low confidence**: Robustness to different types of demonstration imperfections beyond tested Gaussian noise models, and optimal strategy for selecting diffusion timestep t*.

## Next Checks

1. **Timestep Sensitivity Analysis**: Systematically vary t* across wider range on each MuJoCo task to identify optimal ranges and assess sensitivity, including computational cost considerations.

2. **Alternative Imperfection Models**: Test method with demonstration imperfections beyond Gaussian noise (systematic biases, partial demonstrations, suboptimal experts) to assess generalizability.

3. **Transfer Learning Validation**: Evaluate whether diffusion model trained on optimal demonstrations from one task can effectively purify imperfect demonstrations from related but different task, testing scalability and transfer capability.