---
ver: rpa2
title: Nepali Video Captioning using CNN-RNN Architecture
arxiv_id: '2311.02699'
source_url: https://arxiv.org/abs/2311.02699
tags:
- video
- captioning
- captions
- dataset
- nepali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates Nepali video captioning using a CNN-RNN
  encoder-decoder framework, employing pre-trained EfficientNetB0, ResNet101, and
  VGG16 models with LSTM, GRU, and BiLSTM decoders. By leveraging the MSVD dataset
  translated into Nepali via Google Translate, the research explores model configurations
  and hyperparameters to optimize captioning performance.
---

# Nepali Video Captioning using CNN-RNN Architecture

## Quick Facts
- arXiv ID: 2311.02699
- Source URL: https://arxiv.org/abs/2311.02699
- Reference count: 0
- Primary result: EfficientNetB0 + BiLSTM with 1024 hidden dimensions achieves BLEU-4 of 17 and METEOR of 46

## Executive Summary
This study investigates Nepali video captioning using a CNN-RNN encoder-decoder framework. The research evaluates three pre-trained CNN models (EfficientNetB0, ResNet101, VGG16) combined with three RNN decoders (LSTM, GRU, BiLSTM) to determine optimal configurations for generating Nepali captions. Using the MSVD dataset translated into Nepali via Google Translate, the study systematically explores hyperparameter impacts and identifies the most effective model architecture. The work contributes both a novel Nepali video captioning dataset and demonstrates the viability of CNN-RNN approaches for improving accessibility in Nepali video content.

## Method Summary
The method employs a CNN-RNN encoder-decoder framework where pre-trained CNNs extract visual features from 30 evenly sampled frames per video (resized to 224x224). These features are then processed by RNN decoders to generate Nepali captions. The translated MSVD dataset is tokenized into a vocabulary of 15,585 words, with captions padded to a maximum of 10 tokens. Three CNN architectures (EfficientNetB0, ResNet101, VGG16) and three RNN architectures (LSTM, GRU, BiLSTM) are systematically combined and evaluated. The best-performing configuration uses EfficientNetB0 with BiLSTM (1024 hidden dimensions), achieving BLEU-4 of 17 and METEOR of 46. Training employs Adam optimizer with categorical crossentropy loss across various batch sizes and epoch counts.

## Key Results
- EfficientNetB0 consistently outperformed ResNet101 and VGG16 across all tested configurations
- BiLSTM decoder achieved the highest performance metrics among the three RNN options
- The optimal model configuration (EfficientNetB0 + BiLSTM, 1024 hidden dimensions) achieved BLEU-4 of 17 and METEOR of 46
- Vocabulary size of 15,585 words with 10-token maximum caption length proved effective for the dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained EfficientNetB0 extracts richer visual features than ResNet101 or VGG16 for video captioning tasks.
- Mechanism: EfficientNetB0 uses compound scaling and optimized architecture to capture fine-grained visual patterns, which are critical for generating accurate captions.
- Core assumption: The dataset contains enough diversity so that richer features translate to better caption accuracy.
- Evidence anchors:
  - [abstract]: "Results show that the EfficientNetB0 + BiLSTM combination with 1024 hidden dimensions achieves the highest BLEU-4 score of 17 and METEOR score of 46."
  - [section]: "Among the tested CNNs, EfficientNetB0 consistently outperformed ResNet101 and VGG16 across most model configurations indicating its effectiveness in extracting meaningful visual features..."
  - [corpus]: Weak evidence - related works focus on image captioning or other languages; no direct comparison to EfficientNetB0 in Nepali video captioning.

### Mechanism 2
- Claim: Bidirectional LSTM decoder captures both past and future context in caption generation, improving accuracy.
- Mechanism: BiLSTM processes caption tokens in both forward and backward directions, enabling richer context modeling and smoother sentence flow.
- Core assumption: Caption tokens are sufficient in length to benefit from backward context.
- Evidence anchors:
  - [abstract]: "the best model being EfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 score of 17 and METEOR score of 46."
  - [section]: "BiLSTM’s ability to understand both past and future information in the videos contributes to generating accurate captions."
  - [corpus]: Weak evidence - no direct BiLSTM comparison in related works; most use LSTM or GRU.

### Mechanism 3
- Claim: One-hot encoded target sequences with start/end tokens guide the decoder to produce grammatically coherent captions.
- Mechanism: Explicit BOS/EOS tokens bound the caption generation, preventing overprediction and ensuring sentence structure.
- Core assumption: Vocabulary size and token distribution are stable across training and test sets.
- Evidence anchors:
  - [section]: "Each caption is enriched by adding a 'beginning of sentence' (’bos’) token at the start and an 'end of sentence' (’eos’) token at the end."
  - [section]: "The generator creates training and validation data for each epoch and each caption in training and validation sequences... generates decoder input and target data by converting the caption tokens into categorical one-hot encoded vectors."
  - [corpus]: No direct evidence; this is a standard practice in sequence-to-sequence models.

## Foundational Learning

- Concept: Vocabulary construction and tokenization
  - Why needed here: Nepali captions are translated and tokenized into numeric sequences for model input; incorrect tokenization causes misalignment between video features and captions.
  - Quick check question: If the training vocabulary size is 15,585 but the test set contains unseen words, what happens to the model's output probabilities?

- Concept: Frame sampling and feature extraction
  - Why needed here: Selecting 30 evenly spaced frames balances computational load and visual coverage; too few frames miss context, too many add noise.
  - Quick check question: If you increase frames from 30 to 60, how does the encoder input shape change for EfficientNetB0?

- Concept: BLEU and METEOR metrics interpretation
  - Why needed here: These metrics evaluate caption quality; understanding their scaling (0–100) and what each n-gram captures guides model tuning.
  - Quick check question: Why might BLEU-4 be lower than BLEU-1 in this study, even if captions are semantically accurate?

## Architecture Onboarding

- Component map: Video preprocessing → 30 frames @ 224x224 → CNN (EfficientNetB0/ResNet101/VGG16) → feature vector (1280/2048/4096 dims) → RNN (LSTM/GRU/BiLSTM) → Dense(softmax) → word probabilities

- Critical path:
  1. Extract CNN features from each frame
  2. Feed features into encoder RNN
  3. Initialize decoder with encoder hidden states
  4. Decode padded caption tokens, predict next word until EOS

- Design tradeoffs:
  - CNN choice: EfficientNetB0 best accuracy but larger model; VGG16 simpler but weaker features
  - RNN choice: BiLSTM best accuracy but double parameters; LSTM/GRU lighter but less context
  - Sequence length: 10 tokens avoids excessive padding but may truncate long captions

- Failure signatures:
  - BLEU-4 << BLEU-1: model captures words but fails on longer phrase coherence
  - High METEOR, low BLEU: captions are semantically correct but word order mismatches reference
  - Vocabulary OOV spikes: translation or preprocessing introduces unseen words

- First 3 experiments:
  1. Swap EfficientNetB0 with ResNet101, keep BiLSTM(1024), measure BLEU/METEOR drop
  2. Replace BiLSTM with LSTM(1024), keep EfficientNetB0, compare context capture
  3. Increase max caption length from 10 to 15, retrain, observe padding vs truncation effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different hyperparameter settings, such as learning rate, dropout rate, and optimizer choice, impact the performance of CNN-RNN models for Nepali video captioning?
- Basis in paper: [inferred] The paper discusses the influence of batch size, hidden dimensions, and number of epochs on model performance, suggesting that other hyperparameters may also play a role.
- Why unresolved: The study focused on a limited set of hyperparameters and did not exhaustively explore the effects of learning rate, dropout, and optimizer choice.
- What evidence would resolve it: Conducting experiments with various learning rates, dropout rates, and optimizer choices to identify their impact on BLEU and METEOR scores.

### Open Question 2
- Question: Can transformer-based models, such as BERT or other large language models, outperform traditional CNN-RNN architectures in generating Nepali video captions?
- Basis in paper: [inferred] The paper mentions that transformer networks and other large language models are considered alternatives to traditional RNN models, indicating the potential for improved performance.
- Why unresolved: The study did not experiment with transformer-based models, leaving their effectiveness compared to CNN-RNN architectures unexplored.
- What evidence would resolve it: Implementing and evaluating transformer-based models for Nepali video captioning and comparing their performance to CNN-RNN models.

### Open Question 3
- Question: How does the quality of the translated Nepali captions affect the performance of the video captioning models, and can improving the translation process lead to better results?
- Basis in paper: [explicit] The paper acknowledges that the quality of the translated captions relies on the capabilities of Google Translate, which may introduce inaccuracies and variations in the Nepali captions.
- Why unresolved: The study used translated captions without assessing their impact on model performance or exploring ways to improve the translation process.
- What evidence would resolve it: Conducting experiments with manually verified or professionally translated captions to determine their effect on captioning accuracy and comparing the results with those obtained using machine-translated captions.

## Limitations

- Translation quality uncertainty: The study relies on Google Translate for Nepali captions without validation by native speakers, introducing potential semantic drift
- Single dataset constraint: Results are based on translated MSVD dataset only, limiting generalizability to other video content
- Vocabulary coverage limitations: 15,585-word vocabulary may still face out-of-vocabulary challenges in real-world deployment

## Confidence

- High Confidence: The architectural mechanism claims linking EfficientNetB0 feature extraction to improved captioning performance are well-supported by direct experimental comparisons
- Medium Confidence: The BiLSTM decoder advantage is supported by the best model results but lacks comparative evidence from the broader literature on Nepali video captioning
- Medium Confidence: The BOS/EOS token mechanism is standard practice with clearly specified implementation, though no ablation study quantified its specific contribution

## Next Checks

1. **Translation Quality Validation**: Conduct human evaluation of 100 randomly sampled Nepali captions against their English originals to quantify semantic drift and identify systematic translation errors

2. **Frame Sampling Sensitivity**: Systematically vary frame sampling rates (15, 30, 45, 60 frames) while keeping other parameters constant to determine optimal temporal coverage

3. **Vocabulary Coverage Analysis**: Analyze the frequency distribution of out-of-vocabulary tokens in the test set and measure performance degradation when limiting vocabulary to different threshold percentages (90%, 95%, 100% coverage)