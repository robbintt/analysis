---
ver: rpa2
title: 'CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance
  on Low-Resource Languages'
arxiv_id: '2310.13683'
source_url: https://arxiv.org/abs/2310.13683
tags:
- image
- capiv
- captions
- clip
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAPIVARA introduces a cost-efficient framework to improve multilingual
  CLIP performance in low-resource languages by generating synthetic captions via
  image captioning and machine translation, then fine-tuning with LiT, LoRA, and gradient
  checkpointing. It achieves state-of-the-art results on Portuguese zero-shot tasks,
  with up to 6.5 pp.
---

# CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages

## Quick Facts
- **arXiv ID:** 2310.13683
- **Source URL:** https://arxiv.org/abs/2310.13683
- **Reference count:** 27
- **Key outcome:** Achieves state-of-the-art zero-shot performance on Portuguese with up to 6.5 pp. improvement over baseline, while using only 8.5 GB GPU memory and 2 hours training.

## Executive Summary
CAPIVARA addresses the challenge of improving multilingual CLIP performance in low-resource languages through a cost-efficient framework. The approach generates synthetic captions using image captioning models and machine translation, then fine-tunes the text encoder with LiT, LoRA, and gradient checkpointing. This method significantly improves zero-shot performance on Portuguese and shows substantial gains for other low-resource languages like Xhosa and Hindi, all while maintaining modest computational requirements. The framework effectively bridges the performance gap between low-resource and high-resource languages without requiring expensive manual annotation.

## Method Summary
CAPIVARA improves multilingual CLIP performance by generating synthetic captions for images using BLIP2, then translating both original and synthetic captions into the target low-resource language. The method fine-tunes the text encoder using LiT (freezing the image encoder), LoRA (parameter-efficient adaptation), and gradient checkpointing to reduce memory usage. The framework is trained on a single RTX Quadro 8000 GPU with 8.5 GB memory for approximately 2 hours. The approach leverages the Conceptual Captions 3M dataset and evaluates performance on standard benchmarks like Flickr30k and MS COCO using mean recall metrics.

## Key Results
- Achieves 6.5 pp. improvement over baseline for Portuguese zero-shot tasks
- State-of-the-art performance on Portuguese image classification and retrieval
- Demonstrates significant gains for other low-resource languages (Xhosa, Hindi)
- Maintains computational efficiency with 8.5 GB GPU memory usage and 2 hours training time

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic captions generated by image captioning models (e.g., BLIP2) reduce annotation noise and improve language diversity in training data.
- **Mechanism:** Image captioning models produce multiple image-conditioned captions, addressing the limitation of sparse, English-only descriptions. These captions are then translated into the target low-resource language, increasing linguistic coverage and reducing noise in the dataset.
- **Core assumption:** BLIP2 can generate captions semantically aligned with images, and translation preserves meaning across languages.
- **Evidence anchors:**
  - [abstract] "CAPIVARA addresses this by augmenting text data using image captioning and machine translation to generate multiple synthetic captions in low-resource languages."
  - [section 3.4] "We propose to use BLIP2 to generate new captions conditioned on the images from CC3M."
  - [corpus] Weak evidence for BLIP2's performance in low-resource languages; no direct evaluation provided.
- **Break condition:** If BLIP2 generates captions that are not semantically aligned with the image, or if translation introduces significant errors, the noise reduction benefit diminishes.

### Mechanism 2
- **Claim:** LiT (Locked-image text tuning) with frozen image encoders accelerates training while preserving strong visual representations.
- **Mechanism:** The image encoder is kept frozen during fine-tuning, so only the text encoder is updated. This leverages the already well-trained visual representations from pre-training and reduces the number of trainable parameters.
- **Core assumption:** The image encoder produces good representations for images and does not need to be fine-tuned for better cross-modal alignment.
- **Evidence anchors:**
  - [section 3.5] "The motivation for training only the text encoder is that the image encoder has already undergone extensive pre-training and can produce good representations for images."
  - [abstract] "We optimize the training pipeline with LiT, LoRA, and gradient checkpointing to alleviate the computational cost."
- **Break condition:** If the frozen image encoder cannot adapt to nuances in the new language's visual-text alignment, performance gains may plateau.

### Mechanism 3
- **Claim:** LoRA (Low-Rank Adaptation) reduces trainable parameters and memory usage while maintaining model performance.
- **Mechanism:** LoRA decomposes dense layers into low-rank matrices, drastically reducing the number of parameters that need gradient updates. This allows fine-tuning on a single GPU with modest memory.
- **Core assumption:** Low-rank decomposition can capture the necessary changes in the model without full fine-tuning.
- **Evidence anchors:**
  - [section 3.5] "LoRA involves a re-parameterization of the dense layers... We use LoRA in the query (Q) and value (V) self-attention modules from the text encoder."
  - [abstract] "To the best of our knowledge, we are the first to employ LoRA for language adaptation in CLIP models, considerably reducing the number of trainable parameters."
- **Break condition:** If the rank of the decomposition is too low, LoRA may fail to capture necessary model changes, leading to degraded performance.

## Foundational Learning

- **Concept: Contrastive learning for vision-language tasks**
  - Why needed here: CAPIVARA relies on CLIP's contrastive learning framework to align image and text embeddings in a shared multimodal space.
  - Quick check question: How does the InfoNCE loss encourage the model to match correct image-text pairs over incorrect ones?

- **Concept: Parameter-efficient fine-tuning methods**
  - Why needed here: CAPIVARA uses LiT and LoRA to reduce computational cost, which requires understanding how these methods modify model training.
  - Quick check question: What is the key difference between LiT and LoRA in terms of which parameters are updated?

- **Concept: Image captioning and machine translation**
  - Why needed here: CAPIVARA generates synthetic captions in English and translates them to the target language, so understanding the capabilities and limitations of these tools is critical.
  - Quick check question: What are the potential sources of noise when using BLIP2 for caption generation followed by Google Translate?

## Architecture Onboarding

- **Component map:** BLIP2 (caption generation) -> Google Translate (caption translation) -> LiT (text encoder fine-tuning) -> LoRA (parameter-efficient adaptation) -> CLIP model
- **Critical path:** 1) Generate synthetic captions with BLIP2, 2) Translate captions to target language, 3) Fine-tune text encoder with LiT, LoRA, and gradient checkpointing
- **Design tradeoffs:** Using BLIP2 + translation vs. training a multilingual captioning model directly (simpler but potentially noisier); LiT + LoRA vs. full fine-tuning (faster and cheaper but may limit adaptation); generating many synthetic captions vs. fewer (more diversity but higher noise risk)
- **Failure signatures:** Poor cross-modal retrieval performance indicates issues with caption quality or alignment; memory errors during training suggest LoRA rank or batch size needs adjustment; slow convergence may indicate suboptimal learning rate or checkpointing configuration
- **First 3 experiments:** 1) Fine-tune OPEN CLIP on translated CC3M captions (baseline for CAPIVARA), 2) Apply CAPIVARA (synthetic captions + translation) on CC3M and evaluate retrieval performance, 3) Add LoRA to the CAPIVARA pipeline and measure memory usage and performance trade-offs

## Open Questions the Paper Calls Out

1. **How many training examples in a low-resource language are needed to match English performance?**
   - Basis in paper: Explicit - The paper states "how many examples annotated in a low-resource language are necessary to achieve a performance comparable to English?" as an open research question in the limitations section.
   - Why unresolved: The paper demonstrates significant performance gains using synthetic captions and translation, but doesn't establish a clear threshold of data quantity needed for performance parity with English.
   - What evidence would resolve it: Systematic experiments varying the number of annotated examples in the target language while measuring performance relative to English baselines.

2. **Does CAPIVARA scale effectively to larger models beyond the base ViT-B/32 architecture?**
   - Basis in paper: Inferred - The paper focuses on base model versions due to computational constraints and explicitly mentions this as a limitation, stating "Future work will explore different model sizes within our budget."
   - Why unresolved: The authors specifically limited their experiments to base models and noted infrastructure limitations, leaving scalability to larger architectures unexplored.
   - What evidence would resolve it: Experiments training CAPIVARA on larger CLIP models (ViT-L/16, etc.) with varying dataset sizes to determine performance scaling relationships.

3. **What is the optimal balance between synthetic caption generation and human annotation for low-resource languages?**
   - Basis in paper: Explicit - The paper uses BLIP2 for synthetic caption generation followed by machine translation, but notes that generating captions in target languages involves two steps due to lack of robust non-English models, and suggests this as future research.
   - Why unresolved: The current approach relies on synthetic captions from a monolingual model plus translation, which may introduce quality issues, but the paper doesn't explore hybrid approaches or determine when human annotation becomes more beneficial.
   - What evidence would resolve it: Comparative studies measuring performance trade-offs between pure synthetic generation, human annotation, and hybrid approaches across different resource levels.

## Limitations

- **Limited ablation of synthetic caption strategies:** The paper does not thoroughly explore alternative caption generation approaches or compare different synthetic caption selection strategies.
- **Translation quality concerns:** Heavy reliance on Google Translate without evaluating translation quality or quantifying how translation errors impact performance.
- **Dataset and language coverage gaps:** Evaluation focuses primarily on Portuguese with limited experiments on Xhosa and Hindi, leaving effectiveness across broader language ranges untested.

## Confidence

**High confidence** - Performance improvements on zero-shot tasks: Well-supported by experimental results on established benchmarks with state-of-the-art results on Portuguese.

**Medium confidence** - Cost-efficiency claims: Demonstrated on specific hardware but lacks comprehensive comparison framework with other approaches.

**Medium confidence** - Generalizability to other low-resource languages: Promising results on Xhosa and Hindi but limited scope prevents strong generalization claims.

## Next Checks

1. **Synthetic caption quality assessment:** Evaluate semantic alignment between BLIP2-generated captions and corresponding images in the target language, and measure how translation errors correlate with downstream performance degradation.

2. **Cross-lingual generalization test:** Apply CAPIVARA to a broader set of low-resource languages with varying linguistic properties (e.g., morphologically rich languages, languages with different word orders) to assess method robustness.

3. **Alternative synthetic caption generation comparison:** Compare CAPIVARA's approach against alternatives such as fine-tuning a multilingual image captioning model or using back-translation techniques to generate caption diversity.