---
ver: rpa2
title: 'It''s Not Easy Being Wrong: Large Language Models Struggle with Process of
  Elimination Reasoning'
arxiv_id: '2311.07532'
source_url: https://arxiv.org/abs/2311.07532
tags:
- answer
- choices
- question
- incorrect
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task, process of elimination (PoE)
  with chain-of-thought (COT), where large language models (LLMs) are prompted to
  reason toward incorrect answer choices on multiple-choice questions. The authors
  evaluate GPT-3.5, LLaMA-2, and Falcon on this task across four commonsense and scientific
  reasoning datasets.
---

# It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning

## Quick Facts
- arXiv ID: 2311.07532
- Source URL: https://arxiv.org/abs/2311.07532
- Authors: [Not specified in source]
- Reference count: 22
- Key outcome: Process of elimination (PoE) with chain-of-thought (COT) consistently underperforms direct answer selection, even with COT prompting, revealing challenges in LLM reasoning capabilities.

## Executive Summary
This paper investigates how well large language models can reason toward incorrect answers using process of elimination (PoE) with chain-of-thought (COT) prompting. The authors find that PoE with COT consistently underperforms direct answer selection across multiple models and datasets, and that the two strategies do not reliably agree. Error analysis reveals misaligned rationales and logical errors as common failure modes. The results suggest PoE with COT is a challenging task that may require larger models or additional training, and could serve as a reasoning/consistency benchmark.

## Method Summary
The authors evaluate three LLM families (GPT-3.5, LLaMA-2, and Falcon) on 2-choice multiple-choice questions from CommonsenseQA, Social IQa, ARC, and OpenBookQA datasets. They use few-shot prompting (10 examples) with and without COT for both direct answering (DA) and process of elimination (PoE) strategies. Accuracy, consistency between DA and PoE, and self-consistency are measured. Error analysis examines rationales for misaligned or incorrect reasoning.

## Key Results
- PoE with COT consistently underperforms direct answer selection across all tested models and datasets
- DA and PoE strategies fail to agree on 2-choice questions where logical consistency is expected
- Misaligned rationales and logical errors are common failure modes in PoE reasoning
- The performance gap between DA and PoE narrows with larger models, suggesting PoE might only be attainable for very large models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought (COT) prompting improves reasoning accuracy by forcing explicit step-by-step verbalization of the problem-solving process.
- Mechanism: COT prompts guide the model to decompose complex reasoning tasks into intermediate steps, reducing the likelihood of jumping directly to an incorrect conclusion.
- Core assumption: LLMs possess latent reasoning capabilities that can be unlocked through structured intermediate reasoning.
- Evidence anchors:
  - [abstract] "Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers"
  - [section] "Following the work on COT (Wei et al., 2022b), we use few-shot (10) prompts to implement the strategies"
- Break condition: If the model lacks the requisite factual knowledge or fails to understand the intermediate steps, COT may not improve accuracy and could introduce additional errors.

### Mechanism 2
- Claim: Process of elimination (PoE) with COT can enhance interpretability by providing rationales for why incorrect options are invalid.
- Mechanism: PoE with COT requires the model to explicitly reason about why a particular choice is incorrect, offering users insight into the model's decision-making process.
- Core assumption: Users benefit from understanding not just why an answer is correct, but also why other options are incorrect.
- Evidence anchors:
  - [abstract] "This strategy of process of elimination (PoE), when used with COT, has the potential to enhance interpretability in tasks like medical diagnoses of exclusion."
  - [section] "COT reasoning is often viewed as a rationale as to why the model thinks an option is correct, but users may also want to know why the model thinks other options are incorrect."
- Break condition: If the model cannot generate coherent rationales for incorrect choices, PoE with COT may fail to enhance interpretability and could even mislead users.

### Mechanism 3
- Claim: Comparing DA and PoE strategies can reveal logical inconsistencies in LLM reasoning, serving as a benchmark for model robustness.
- Mechanism: For 2-choice questions, DA and PoE should theoretically agree, as selecting one choice as correct implies the other is incorrect. Discrepancies indicate reasoning flaws.
- Core assumption: Logical consistency between strategies is a valid proxy for overall reasoning robustness.
- Evidence anchors:
  - [section] "For 2-choice MC questions, we can study if the decisions from PoE with COT and directly answering the question with COT agree, as both should arrive at the same answer"
  - [section] "we find that DA more often agrees with itself upon a repeated inference (i.e. self-consistency) than agrees with its PoE counterpart"
- Break condition: If the model's reasoning is inherently asymmetric or context-dependent, DA and PoE may not be expected to agree even in a 2-choice setting.

## Foundational Learning

- Concept: Chain-of-thought (COT) prompting
  - Why needed here: COT is the primary mechanism used to elicit reasoning from LLMs in both DA and PoE strategies.
  - Quick check question: What is the purpose of adding intermediate reasoning steps in COT prompting?
  - Answer: To guide the model to decompose complex problems into manageable steps, improving reasoning accuracy and interpretability.

- Concept: Process of elimination (PoE)
  - Why needed here: PoE is the alternative strategy being compared to DA, focusing on identifying incorrect choices rather than correct ones.
  - Quick check question: How does PoE differ from DA in a 2-choice multiple-choice question setting?
  - Answer: In a 2-choice setting, DA selects the correct answer while PoE selects the incorrect answer. Both strategies should theoretically arrive at the same conclusion.

- Concept: Self-consistency
  - Why needed here: Self-consistency measures the agreement of a model's predictions upon repeated inference, serving as a baseline for comparing DA-PoE consistency.
  - Quick check question: What does it mean if a model's DA predictions have high self-consistency but low agreement with its PoE predictions?
  - Answer: It suggests that the model's reasoning is internally consistent when using the same strategy but inconsistent when comparing different strategies, indicating a potential flaw in the model's logical reasoning.

## Architecture Onboarding

- Component map: Input question -> DA COT prompt -> DA output; Input question -> PoE COT prompt -> PoE output; Compare DA and PoE outputs
- Critical path: 1) Parse input question and choices 2) Apply DA COT prompt and generate answer 3) Apply PoE COT prompt and generate answer 4) Compare DA and PoE outputs for agreement 5) Evaluate accuracy and reasoning quality
- Design tradeoffs:
  - Prompt complexity vs. model performance: More detailed COT prompts may improve reasoning but also increase computational cost
  - Accuracy vs. interpretability: Focusing on PoE rationales may sacrifice some accuracy but provide more insight into the model's decision-making process
  - Model size vs. PoE ability: Larger models may be better equipped to handle PoE tasks, but at a higher computational cost
- Failure signatures:
  - Low accuracy in both DA and PoE strategies: Indicates the model may not understand the task or lack relevant knowledge
  - High DA accuracy but low PoE accuracy: Suggests a bias towards identifying correct answers rather than eliminating incorrect ones
  - Low DA-PoE agreement: Implies logical inconsistencies in the model's reasoning process
- First 3 experiments:
  1. Evaluate DA and PoE strategies with and without COT on a small subset of questions to establish baseline performance
  2. Measure DA-PoE agreement and self-consistency to identify potential logical inconsistencies
  3. Conduct error analysis on PoE rationales to categorize failure modes and inform model improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does PoE with COT improve model calibration compared to DA with COT?
- Basis in paper: [explicit] The paper suggests PoE with COT could be used for confidence calibration, as models that agree on DA and PoE might be more confident.
- Why unresolved: The paper does not evaluate the calibration properties of PoE with COT compared to DA with COT.
- What evidence would resolve it: Experiments measuring calibration metrics (e.g., expected calibration error, reliability diagrams) for PoE with COT vs. DA with COT across different model sizes and datasets.

### Open Question 2
- Question: How does fine-tuning LLMs specifically on PoE rationales affect their overall reasoning capabilities?
- Basis in paper: [explicit] The paper suggests fine-tuning LLMs on PoE rationales as a potential direction to enhance PoE abilities and overall reasoning.
- Why unresolved: The paper does not conduct any fine-tuning experiments or analyze the impact on general reasoning.
- What evidence would resolve it: Results from fine-tuning experiments comparing reasoning performance on standard benchmarks before and after PoE-specific fine-tuning.

### Open Question 3
- Question: Is there a correlation between model size and the ability to perform PoE with COT that plateaus at a certain scale?
- Basis in paper: [explicit] The paper observes that the gap between DA and PoE performance narrows with larger models, suggesting PoE might only be attainable for larger models.
- Why unresolved: The paper does not test models larger than Falcon-180B or analyze if the trend plateaus.
- What evidence would resolve it: Performance comparisons of PoE with COT across a wider range of model sizes, including very large models like GPT-4, to determine if there's a plateau effect.

## Limitations
- The study focuses exclusively on 2-choice questions, which may not generalize to multi-choice settings where PoE could be more effective
- The specific in-context examples and COT prompt quality are not provided, leaving uncertainty about whether suboptimal prompt engineering contributed to poor PoE performance
- The analysis only examined three LLM families without exploring fine-tuning approaches that could potentially improve PoE reasoning capabilities

## Confidence

- **High confidence** in the core finding that PoE with COT underperforms DA across all tested models and datasets, as this result is consistently observed and statistically robust
- **Medium confidence** in the error analysis conclusions about misaligned rationales and logical errors, as these are based on manual inspection of a limited sample of outputs
- **Low confidence** in the generalizability of these results to multi-choice questions, different domains, or larger model architectures not tested in this study

## Next Checks
1. Test PoE with COT on multi-choice questions (3+ options) to determine if the performance gap between PoE and DA narrows when more elimination opportunities exist
2. Conduct controlled experiments varying the number of in-context examples (1, 5, 10) and their quality to isolate the impact of prompt engineering on PoE performance
3. Evaluate whether fine-tuning LLMs specifically on PoE tasks improves accuracy and consistency with DA strategies, particularly for larger model variants