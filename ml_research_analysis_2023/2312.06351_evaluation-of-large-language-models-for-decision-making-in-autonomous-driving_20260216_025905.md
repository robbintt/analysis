---
ver: rpa2
title: Evaluation of Large Language Models for Decision Making in Autonomous Driving
arxiv_id: '2312.06351'
source_url: https://arxiv.org/abs/2312.06351
tags:
- llms
- driving
- autonomous
- vehicle
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study quantitatively evaluated the capabilities of Large
  Language Models (LLMs) for autonomous driving, focusing on two essential abilities:
  spatial-aware decision making (recognizing space from coordinates to avoid collisions)
  and following traffic rules. Three LLMs (LLaMA-2 7B, GPT-3.5, and GPT-4) were tested
  in both simulated highway environments and simplified real-world scenarios.'
---

# Evaluation of Large Language Models for Decision Making in Autonomous Driving

## Quick Facts
- arXiv ID: 2312.06351
- Source URL: https://arxiv.org/abs/2312.06351
- Reference count: 12
- Key outcome: GPT-4 demonstrated significantly higher accuracy than other models across all tasks: 0.618 for spatial-aware decision making, 0.792 for following traffic rules, and 0.640 for combined tasks in simulation.

## Executive Summary
This study evaluates Large Language Models (LLMs) for autonomous driving decision-making, focusing on spatial-aware decision making and traffic rule following. Three models were tested: LLaMA-2 7B, GPT-3.5, and GPT-4. The evaluation was conducted in both simulated highway environments and simplified real-world scenarios. GPT-4 showed significantly higher accuracy across all tasks, achieving 0.618 for spatial-aware decision making, 0.792 for following traffic rules, and 0.640 for combined tasks in simulation. The study also implemented a proof-of-concept system using GPT-4 to control an actual vehicle based on object detection and human instructions, successfully navigating toward designated targets while adhering to traffic rules.

## Method Summary
The study evaluated three LLMs (LLaMA-2 7B, GPT-3.5, and GPT-4) using manually created datasets with 34 SADM samples, 24 FTR samples, and 50 SADM&FTR samples for simulation testing, plus 20 samples for each task in real vehicle testing. Models received object coordinates and velocities as text prompts along with traffic rules and user instructions, then output one of five actions: accelerate, maintain, decelerate, change lane right, or change lane left. The evaluation compared LLM decisions to ground truth using accuracy metrics, with fixed random seeds for deterministic outputs.

## Key Results
- GPT-4 achieved significantly higher accuracy than LLaMA-2 7B and GPT-3.5 across all evaluation tasks
- In simulation: GPT-4 scored 0.618 (SADM), 0.792 (FTR), and 0.640 (combined) accuracy
- In real-world deployment: GPT-4 achieved 0.85, 0.90, and 0.90 accuracy respectively for the three task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 demonstrates significantly higher accuracy than smaller LLMs in spatial-aware decision making and traffic rule following due to its larger parameter count and training data diversity.
- Mechanism: The model's ability to reason about spatial relationships and traffic rules is enhanced by its extensive pretraining on diverse text data, allowing it to generalize better to novel driving scenarios.
- Core assumption: Larger models with more parameters and diverse training data perform better on complex reasoning tasks that require understanding spatial relationships and rules.
- Evidence anchors:
  - [abstract]: "GPT-4 demonstrated significantly higher accuracy than other models across all tasks: 0.618 for spatial-aware decision making, 0.792 for following traffic rules, and 0.640 for combined tasks in simulation."
  - [section]: "In our experiments, we manually created datasets... GPT-4 showed a significantly higher score than other LLMs by a large margin."
- Break condition: If the task requires real-time inference, the computational cost of GPT-4 may make it impractical despite its higher accuracy.

### Mechanism 2
- Claim: LLMs can effectively translate object coordinate information into driving decisions through prompt engineering that structures spatial information in natural language.
- Mechanism: By providing coordinate and velocity information as text prompts, LLMs can parse this information to understand spatial relationships and make appropriate driving decisions.
- Core assumption: LLMs can effectively process structured text prompts containing spatial information and translate this into actionable driving decisions.
- Evidence anchors:
  - [abstract]: "One strategy of using LLMs for autonomous driving involves inputting surrounding objects as text prompts to the LLMs, along with their coordinate and velocity information, and then outputting the subsequent movements of the vehicle."
  - [section]: "LLMs are used to output driving operations from the text description of the recognized objects."
- Break condition: If the coordinate information is too complex or the spatial relationships too subtle, the LLM may fail to make correct decisions.

### Mechanism 3
- Claim: The ability to generate explanations alongside decisions improves accuracy for more capable LLMs, suggesting reasoning transparency enhances performance.
- Mechanism: When LLMs are prompted to provide reasoning for their decisions, they engage in more thorough analysis, leading to better decision quality.
- Core assumption: Generating explanations forces the model to engage in more thorough reasoning, improving decision quality.
- Evidence anchors:
  - [abstract]: "asking for a reason along with the decision led to improved accuracy in GPT-3.5 and GPT-4, but this was not observed in LLaMA-2."
  - [section]: "It was observed that performance improved as LLM abilities increased... asking for a reason along with the decision led to improved accuracy in GPT-3.5 and GPT-4."
- Break condition: If the explanation prompt adds too much computational overhead, the accuracy gains may not justify the increased inference time.

## Foundational Learning

- Concept: Spatial reasoning from coordinate data
  - Why needed here: Autonomous driving requires understanding the spatial relationships between objects to avoid collisions and navigate effectively.
  - Quick check question: Given objects at coordinates (x1, y1) and (x2, y2), how would you determine if they're in the same lane or different lanes?

- Concept: Traffic rule comprehension and application
  - Why needed here: Vehicles must follow traffic laws and regulations to operate safely and legally on roads.
  - Quick check question: If a vehicle is in the right lane with a speed limit of 70 km/h and another vehicle ahead is going 50 km/h, what should the ego vehicle do according to traffic rules?

- Concept: Prompt engineering for structured task completion
  - Why needed here: LLMs require carefully structured prompts to produce accurate outputs for autonomous driving decisions.
  - Quick check question: How would you structure a prompt to ask an LLM to decide whether to change lanes, accelerate, or decelerate based on surrounding vehicle positions?

## Architecture Onboarding

- Component map: Perception module (object detection and coordinate calculation) -> Prompt generation system (translating perception data to text) -> LLM inference engine (decision making) -> Vehicle control interface (executing decisions) -> Safety monitoring system (overriding unsafe decisions)

- Critical path: Perception → Prompt Generation → LLM → Control Interface
  - The system must process object detection data, convert it to structured text prompts, generate decisions via LLM, and translate those decisions into vehicle control commands.

- Design tradeoffs:
  - Accuracy vs. latency: Larger models like GPT-4 offer better accuracy but slower inference times
  - Safety vs. autonomy: More autonomous systems require stronger safety guarantees
  - Local vs. cloud processing: Local models offer faster response but may lack accuracy of cloud models

- Failure signatures:
  - Incorrect spatial reasoning: Vehicle makes decisions that would cause collisions
  - Rule violation: Vehicle breaks traffic laws or safety protocols
  - Prompt misinterpretation: Vehicle responds inappropriately to instruction format
  - Latency issues: Vehicle cannot respond quickly enough to dynamic situations

- First 3 experiments:
  1. Test LLM decision accuracy on simplified spatial scenarios with known correct answers
  2. Evaluate rule-following accuracy with various traffic scenarios and conflicting instructions
  3. Measure inference latency of different model sizes under realistic input conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-off between computational efficiency and decision-making accuracy be effectively managed when deploying LLMs for autonomous driving in real-time applications?
- Basis in paper: [explicit] The paper discusses the challenges of using GPT-3.5 and GPT-4 via API due to communication through the Internet and inference time, making real-time use difficult. It also notes the low accuracy of LLaMA when tested on a local machine.
- Why unresolved: The paper highlights the need to balance computational efficiency and decision-making accuracy but does not provide a solution or method to achieve this balance in real-time applications.

### Open Question 2
- Question: What are the limitations of LLMs in handling unexpected or novel driving scenarios that are not covered by their training data?
- Basis in paper: [inferred] The paper mentions that LLMs are considered to possess general knowledge about the world and can potentially handle unfamiliar scenarios, but it does not explore the limitations or challenges in such situations.
- Why unresolved: While the paper suggests that LLMs can make driving decisions in unfamiliar scenarios, it does not investigate the specific limitations or performance degradation in handling truly novel situations not represented in the training data.

### Open Question 3
- Question: How can ethical judgments be effectively integrated into LLM-based autonomous driving systems, and what are the implications for decision-making in complex moral dilemmas?
- Basis in paper: [explicit] The paper discusses the potential for LLMs to handle ethical judgments and mentions the Trolley Problem as an example of a moral dilemma that may arise in autonomous driving.
- Why unresolved: While the paper acknowledges the possibility of LLMs making ethical judgments, it does not explore how this can be achieved or the implications for decision-making in complex moral dilemmas.

## Limitations
- The evaluation relies entirely on manually created datasets rather than real-world driving data, which may not capture the full complexity and variability of actual driving scenarios
- The real-world deployment validation is limited to simple navigation tasks without comprehensive safety validation under diverse conditions
- The study doesn't address computational latency implications of using large models like GPT-4 in time-critical driving scenarios

## Confidence

- High confidence: GPT-4 outperforms smaller LLMs (LLaMA-2 7B, GPT-3.5) on the specific evaluation metrics tested
- Medium confidence: The explanation-generation mechanism improves decision accuracy for more capable models
- Low confidence: The results generalize to real-world autonomous driving scenarios with diverse traffic conditions

## Next Checks

1. **Real-world safety validation**: Deploy the system in controlled real-world environments with diverse traffic scenarios and measure safety performance beyond accuracy metrics
2. **Latency and computational cost analysis**: Quantify the inference latency of different model sizes under realistic time constraints and evaluate the trade-off between accuracy and response time
3. **Robustness testing**: Evaluate model performance when presented with adversarial or ambiguous spatial scenarios that challenge coordinate interpretation and rule application