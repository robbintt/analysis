---
ver: rpa2
title: When Do Program-of-Thoughts Work for Reasoning?
arxiv_id: '2308.15452'
source_url: https://arxiv.org/abs/2308.15452
tags:
- code
- reasoning
- data
- complexity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method, Complexity-Impacted Reasoning
  Score (CIRS), to measure the relationship between code reasoning steps and reasoning
  abilities in large language models (LLMs). CIRS evaluates code complexity from both
  structural and logical perspectives, using abstract syntax trees and cyclomatic
  complexity.
---

# When Do Program-of-Thoughts Work for Reasoning?

## Quick Facts
- arXiv ID: 2308.15452
- Source URL: https://arxiv.org/abs/2308.15452
- Authors: 
- Reference count: 13
- Key outcome: A novel Complexity-Impacted Reasoning Score (CIRS) identifies optimal code complexity for improving LLM reasoning abilities, achieving favorable results on mathematical reasoning and code generation tasks.

## Executive Summary
This paper introduces the Complexity-Impacted Reasoning Score (CIRS) to measure how code complexity affects reasoning abilities in large language models. The authors propose that code data with an optimal level of structural and logical complexity is crucial for improving LLM reasoning capabilities. They develop an auto-synthesizing and stratifying algorithm that generates and filters code data to achieve this optimal complexity, demonstrating effectiveness across mathematical reasoning and code generation tasks.

## Method Summary
The method involves calculating CIRS scores that evaluate code complexity from both structural (using abstract syntax trees) and logical (using cyclomatic complexity and difficulty metrics) perspectives. An auto-synthesizing pipeline generates synthetic code data using templates and APIs, while a stratifying algorithm clusters this data based on complexity thresholds. The filtered optimal complexity subsets are then used to train LLaMA-based models for mathematical reasoning and code generation tasks, with performance evaluated across multiple datasets.

## Key Results
- Code data with intermediate complexity levels shows optimal performance for LLM reasoning
- Larger models (65B vs 7B parameters) demonstrate greater gains from complexity-optimized data
- Code format provides advantages over textual rationales for reasoning tasks
- The approach achieves favorable results on GSM8K, MultiArith, ASDiv, and SVAMP datasets

## Why This Works (Mechanism)

### Mechanism 1
Code data with optimal structural and logical complexity improves LLM reasoning. CIRS quantifies code complexity using structural metrics (AST node count, types, depth) and logical metrics (difficulty and cyclomatic complexity), identifying code samples with "just right" complexity that maximizes learning. The core assumption is a non-linear relationship between code complexity and reasoning ability with a performance peak at intermediate levels. Evidence shows models perform best with mid-range complexity data across multiple datasets. This could break if CIRS fails to capture true complexity characteristics or if LLMs suddenly learn from extremely complex code structures.

### Mechanism 2
Code format provides superior reasoning structure compared to textual rationales. Code inherently encodes logical semantics and procedural structure that flat text cannot capture. The CIRS framework leverages this by evaluating code's structural and logical complexity, which correlates with reasoning ability improvements. The core assumption is that the structured, executable nature of code provides cognitive scaffolding that enhances reasoning beyond textual chain-of-thought. Evidence shows code datasets demonstrate clear advantages across all tested datasets. This could break if code generation becomes unreliable or if LLMs develop better mechanisms for representing logical structure in text format.

### Mechanism 3
Larger models benefit more from complexity-optimized code data. As model size increases, their capacity to understand and learn from complex structural and logical information improves, making optimal complexity code data increasingly effective. The core assumption is that model parameter count correlates with ability to process and learn from complex symbolic knowledge embedded in code. Evidence shows performance improvement from 7B to 65B parameters, with larger models showing more significant reasoning capability gains. This could break if architectural limitations prevent larger models from effectively utilizing complex code data.

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs) and their structural properties
  - Why needed here: ASTs provide the structural representation of code that CIRS uses to measure complexity. Understanding nodes, depth, and types is essential for grasping how structural complexity is quantified.
  - Quick check question: What three AST features does CIRS use to measure structural complexity?

- Concept: Cyclomatic complexity and code difficulty metrics
  - Why needed here: These metrics form the logical complexity component of CIRS, measuring the control flow and operational complexity of code.
  - Quick check question: How does cyclomatic complexity relate to the number of decision points in code?

- Concept: Parameter scaling effects on model capabilities
  - Why needed here: The paper shows larger models benefit more from optimal complexity data, so understanding how scale affects learning capacity is crucial.
  - Quick check question: What trend does the paper observe when comparing 7B vs 65B parameter models on reasoning tasks?

## Architecture Onboarding

- Component map: CIRS calculation engine (AST analysis + complexity metrics) -> Auto-synthesizing pipeline (template filling + filtering) -> Stratifying algorithm (threshold-based k-means clustering) -> Training framework (LLaMA-based models) -> Evaluation suite (multiple reasoning datasets)

- Critical path: 1) Generate synthetic code data using templates and APIs 2) Calculate CIRS scores for all generated samples 3) Partition data into complexity-based subsets using auto-stratifying algorithm 4) Train models on optimal complexity subsets 5) Evaluate on held-out reasoning tasks

- Design tradeoffs: Complexity vs. data volume (optimal complexity may reduce available training data), Generalizability vs. specialization (complexity optimization may overfit to specific reasoning types), Computational cost vs. accuracy (more complex CIRS calculations may improve selection but increase preprocessing time)

- Failure signatures: Model performance plateaus regardless of complexity optimization, Auto-synthesizing generates low-quality or irrelevant code samples, Stratifying algorithm creates imbalanced clusters, CIRS scores don't correlate with actual reasoning performance

- First 3 experiments: 1) Run CIRS on existing code datasets to verify complexity distribution patterns 2) Test simple template-based synthesis with manual filtering to validate pipeline components 3) Train small model (7B) on low/medium/high complexity subsets to confirm optimal complexity hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CIRS-guided instruction generation scale with increasing model size and complexity? While the paper shows performance improvement with larger models (7B to 65B parameters), it doesn't provide comprehensive analysis of scaling effects. A thorough evaluation across a wide range of model sizes and complexities, including both in-distribution and out-of-distribution settings, would resolve this.

### Open Question 2
Can the auto-synthesizing and stratifying algorithm be extended to other domains beyond mathematical reasoning and code generation? The paper focuses on two specific applications but doesn't investigate potential for broader use cases. An evaluation in other domains like natural language understanding, computer vision, or robotics would determine generalizability.

### Open Question 3
How does the choice of complexity metric (CIRS) impact the performance of the auto-synthesizing and stratifying algorithm? The paper introduces CIRS but doesn't compare its performance to other complexity metrics or investigate the impact of different metrics. A comparison using different complexity metrics, including CIRS and alternatives, would determine the optimal metric for various applications.

## Limitations
- The auto-synthesizing and stratifying algorithm details are underspecified, making full replication challenging
- Limited evaluation scope - only tested on mathematical reasoning and code generation tasks
- No ablation studies on individual CIRS components to determine which complexity metrics matter most

## Confidence
- High confidence: The existence of an optimal complexity range for code data (supported by clear performance curves across multiple datasets)
- Medium confidence: That code format provides superior reasoning structure compared to text (supported by comparative results but limited to specific task types)
- Medium confidence: That larger models benefit more from complexity optimization (trend observed but with limited parameter scale testing)

## Next Checks
1. Implement and test the auto-synthesizing algorithm with varying template complexity to verify the optimal complexity hypothesis holds across different generation approaches
2. Conduct ablation studies removing individual CIRS components (AST vs. cyclomatic complexity) to identify which complexity measures drive performance improvements
3. Test the approach on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference) to assess generalizability beyond the current scope