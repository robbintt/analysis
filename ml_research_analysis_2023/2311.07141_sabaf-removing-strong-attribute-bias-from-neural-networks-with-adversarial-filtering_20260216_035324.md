---
ver: rpa2
title: 'SABAF: Removing Strong Attribute Bias from Neural Networks with Adversarial
  Filtering'
arxiv_id: '2311.07141'
source_url: https://arxiv.org/abs/2311.07141
tags:
- bias
- attribute
- dataset
- methods
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of removing strong attribute bias
  in neural networks, where the network relies heavily on protected attributes (e.g.,
  race, sex, age) for prediction. Existing methods for removing attribute bias are
  effective only when the bias strength is relatively weak, but their limitations
  in presence of strong bias remain under-explored.
---

# SABAF: Removing Strong Attribute Bias from Neural Networks with Adversarial Filtering

## Quick Facts
- **arXiv ID**: 2311.07141
- **Source URL**: https://arxiv.org/abs/2311.07141
- **Reference count**: 40
- **Primary result**: Proposed method achieves state-of-the-art performance in both strong and moderate bias settings for removing protected attribute bias from neural networks

## Executive Summary
This paper addresses the fundamental limitation of existing attribute bias removal methods when dealing with strong bias scenarios. The authors derive a theoretical necessary condition for any method to effectively remove attribute bias regardless of its strength, showing that access to a Universal distribution where the protected attribute and target are not perfectly correlated is essential. Building on this insight, they propose SABAF (Same-space Adversarial Bias Filtering), a novel method that uses adversarial objectives to filter out protected attributes directly in the input space while maximally preserving other attributes, without requiring specific target labels.

## Method Summary
The proposed method uses an encoder-decoder architecture to transform input data into a representation that minimizes mutual information with the protected attribute while preserving task-relevant information. The filter is trained using an adversarial objective with a Wasserstein GAN critic, attribute-conditioned reconstruction losses, and mutual information minimization. The key innovation is operating in the same input space, allowing the filter to be applied as a preprocessing step across various downstream tasks. The method requires access to a Universal distribution where the protected attribute and target are not perfectly correlated, which enables overcoming the fundamental trade-off between bias removal and model performance.

## Key Results
- Achieves state-of-the-art performance on Colored MNIST, CelebA, IMDB, and Adult datasets across strong and moderate bias settings
- Outperforms existing methods by significant margins when bias strength is high (e.g., 90% vs 70% accuracy on bias-conflicting test sets)
- Theoretical bound derived in the paper is validated empirically across multiple datasets and bias strengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial filtering can remove protected attribute information from latent representations while preserving other task-relevant attributes, enabling effective bias mitigation even in strong bias regimes.
- Mechanism: The method uses an encoder-decoder architecture with mutual information minimization between the latent representation and the protected attribute, combined with attribute-conditioned reconstruction and adversarial losses to ensure attribute removal while maintaining other attributes.
- Core assumption: There exists a Universal distribution where the protected attribute and target are not perfectly correlated (Hq(Y|A) > 0), which is necessary to overcome the trade-off between bias removal and model performance.
- Evidence anchors:
  - [abstract]: "Inspired by this condition, we then propose a new method using an adversarial objective that directly filters out protected attributes in the input space while maximally preserving all other attributes"
  - [section III]: Theorem 1 and Corollary 1 establish the theoretical foundation showing that without access to a Universal distribution, attribute bias removal methods are fundamentally limited in their performance
  - [corpus]: The corpus neighbors include several papers on debiasing and fairness that support the relevance of this approach, though specific empirical evidence for this mechanism is not directly cited
- Break condition: The mechanism breaks down if the Universal distribution does not exist (i.e., the protected attribute and target are perfectly correlated in all possible distributions), or if the filter cannot effectively disentangle the protected attribute from other attributes in the latent space.

### Mechanism 2
- Claim: Same-space filtering allows the bias removal method to be applied as a preprocessing step across various downstream tasks without requiring task-specific architecture modifications.
- Mechanism: By operating in the same input space (transforming images to images), the filter can be trained once and then applied to any downstream dataset, removing the protected attribute while preserving other information.
- Core assumption: The protected attribute information can be localized and removed from the input space without destroying task-relevant information that exists in other dimensions of the input.
- Evidence anchors:
  - [abstract]: "without requiring any specific target label" and the method "directly filters out protected attributes in the input space while maximally preserving all other attributes"
  - [section V]: The filter architecture is described as Gdec ◦ Genc : X → X, operating directly on the input space
  - [corpus]: Limited direct evidence in corpus, but the approach aligns with general domain adaptation and preprocessing techniques
- Break condition: The mechanism fails if the protected attribute is inherently entangled with all other information in the input representation, making it impossible to remove without destroying task-relevant information.

### Mechanism 3
- Claim: The trade-off between attribute bias removal and model performance can be overcome by utilizing a partially observable Universal distribution (with only protected attribute labels available).
- Mechanism: The filter is trained on a Universal distribution where H(Y|A) > 0, learning to remove the protected attribute while preserving task-relevant information. This learned filter can then be applied to the biased dataset for downstream tasks.
- Core assumption: Even with missing target labels, the Universal distribution provides sufficient information for the filter to learn effective bias removal that generalizes to the biased dataset.
- Evidence anchors:
  - [abstract]: "we then propose a new method using an adversarial objective that directly filters out protected attributes in the input space while maximally preserving all other attributes, without requiring any specific target label"
  - [section IV]: Corollary 1 establishes that access to a Universal distribution is necessary to overcome the performance-bias removal trade-off
  - [corpus]: Limited direct evidence in corpus, but the approach is consistent with semi-supervised learning and domain adaptation literature
- Break condition: The mechanism fails if the Universal distribution is too biased itself (Hq(Y|A) ≈ 0) or if the partial observability (missing target labels) prevents effective filter training.

## Foundational Learning

- Concept: Information bottleneck theory and mutual information
  - Why needed here: The method relies on minimizing mutual information between the latent representation and protected attribute while maximizing mutual information with the target
  - Quick check question: How does the data processing inequality relate to the performance bounds established in Theorem 1?

- Concept: Adversarial training and generative modeling
  - Why needed here: The filter uses adversarial objectives (WGAN) and reconstruction losses to learn effective attribute removal
  - Quick check question: What is the role of the critic network D in the WGAN loss function for this application?

- Concept: Domain adaptation and representation learning
  - Why needed here: The method operates on the principle that representations can be learned that are invariant to certain attributes while preserving task-relevant information
  - Quick check question: How does the Universal distribution concept relate to domain adaptation in transfer learning?

## Architecture Onboarding

- Component map: Input -> Encoder (Genc) -> Latent representation -> Decoder (Gdec) -> Filtered output -> Downstream classifier
- Critical path: Encoder → Latent representation → Decoder → Filtered output → Downstream classifier
- Design tradeoffs:
  - Reconstruction quality vs. attribute removal: Higher reconstruction loss preserves more information but may retain protected attributes
  - Mutual information minimization vs. model performance: Stronger MI minimization removes more attribute information but may reduce target prediction accuracy
  - Filter complexity vs. generalization: More complex filters may better remove attributes but may overfit to training distribution

- Failure signatures:
  - High attribute prediction accuracy on filtered data indicates incomplete attribute removal
  - Low target prediction accuracy on filtered data indicates over-aggressive attribute removal
  - Poor reconstruction quality indicates the filter is destroying too much information
  - Performance degradation on Universal distribution indicates filter overfitting to biased data

- First 3 experiments:
  1. Train filter on Colored MNIST with varying λmi, λpred, λrec parameters and measure attribute removal vs. reconstruction quality
  2. Apply trained filter to CelebA and measure target prediction accuracy and attribute removal on unbiased test set
  3. Compare performance with and without Universal distribution on Adult dataset to validate the necessity condition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise upper bound on the performance of any attribute bias removal method in terms of bias strength?
- Basis in paper: [explicit] The paper derives an information-theoretical upper bound on the performance of any attribute bias removal method in terms of the bias strength, but does not provide the exact mathematical expression of this bound.
- Why unresolved: The exact mathematical expression of the upper bound is not provided in the paper.
- What evidence would resolve it: Deriving the exact mathematical expression of the upper bound on the performance of any attribute bias removal method in terms of bias strength.

### Open Question 2
- Question: How does the proposed method compare to other methods in terms of its ability to remove strong attribute bias in real-world datasets?
- Basis in paper: [explicit] The paper conducts extensive experiments on synthetic, image, and census datasets to evaluate the effectiveness of the proposed method in removing strong attribute bias.
- Why unresolved: The paper does not provide a detailed comparison of the proposed method with other methods in terms of its ability to remove strong attribute bias in real-world datasets.
- What evidence would resolve it: Conducting additional experiments on real-world datasets to compare the proposed method with other methods in terms of its ability to remove strong attribute bias.

### Open Question 3
- Question: What are the limitations of the proposed method in terms of its ability to remove attribute bias in the presence of extreme bias?
- Basis in paper: [explicit] The paper discusses the limitations of existing methods in presence of extreme bias and proposes a new method that can mitigate this limitation.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of the proposed method in terms of its ability to remove attribute bias in the presence of extreme bias.
- What evidence would resolve it: Conducting experiments on datasets with extreme bias to evaluate the limitations of the proposed method in terms of its ability to remove attribute bias.

## Limitations

- The method's effectiveness depends critically on the existence and accessibility of a Universal distribution where the protected attribute and target are not perfectly correlated, which may not be available in many real-world scenarios.
- The same-space filtering approach assumes that protected attribute information can be localized and removed without destroying task-relevant information, which may not hold for attributes that are inherently entangled with all other information in the input representation.
- The theoretical framework and empirical validation rely heavily on synthetic data (Colored MNIST) where bias strength can be precisely controlled, which may not fully capture the complexities and nuances of real-world bias patterns.

## Confidence

- **High confidence**: Theoretical claims about the necessity of Universal distributions for overcoming the attribute bias removal trade-off are well-supported by information-theoretic reasoning
- **Medium confidence**: Empirical validation shows consistent improvements across datasets but relies heavily on synthetic data and may not fully capture real-world complexities
- **Medium confidence**: Claims about strong bias performance are supported by experimental results but with varying magnitudes across different bias levels and evaluation metrics

## Next Checks

1. **Universal Distribution Sensitivity Analysis**: Systematically vary the bias strength in the Universal distribution and measure the resulting performance trade-offs to validate the theoretical necessity condition across different bias regimes.

2. **Attribute Entanglement Assessment**: Design experiments to quantify how well the filter can separate protected attributes from task-relevant information when these are highly correlated, using metrics like conditional mutual information before and after filtering.

3. **Cross-Domain Transfer Validation**: Test the filter's performance when trained on one dataset/domain and applied to another with different attribute distributions to assess generalization capabilities and identify potential overfitting to specific bias patterns.