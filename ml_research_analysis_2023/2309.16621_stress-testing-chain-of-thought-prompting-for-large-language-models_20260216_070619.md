---
ver: rpa2
title: Stress Testing Chain-of-Thought Prompting for Large Language Models
arxiv_id: '2309.16621'
source_url: https://arxiv.org/abs/2309.16621
tags:
- performance
- prompting
- tasks
- answer
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how the correctness of Chain-of-Thought
  (CoT) prompting affects large language model performance on multi-step reasoning
  tasks. Researchers systematically perturbed CoT demonstrations by altering values,
  order, and operators while keeping final answers correct.
---

# Stress Testing Chain-of-Thought Prompting for Large Language Models

## Quick Facts
- arXiv ID: 2309.16621
- Source URL: https://arxiv.org/abs/2309.16621
- Reference count: 13
- One-line primary result: Incorrect numerical values in Chain-of-Thought demonstrations severely degrade large language model performance, sometimes below standard few-shot prompting levels.

## Executive Summary
This study systematically investigates how the correctness of Chain-of-Thought (CoT) prompting affects large language model performance on multi-step reasoning tasks. Researchers tested GPT-3 across numerical (math word problems) and non-numerical reasoning tasks by perturbing CoT demonstrations through altering values, order, and operators while maintaining correct final answers. The findings reveal that incorrect CoT values cause the most severe performance degradation, sometimes falling below standard few-shot prompting levels, while incorrect order or operators have less impact. Notably, for more difficult tasks, even incorrect CoT prompts retain some benefit over few-shot prompting. The study also observed that models sometimes retain incorrect CoT structures while producing correct answers, suggesting complex reasoning patterns.

## Method Summary
The researchers conducted controlled experiments using GPT-3 (text-davinci-002) with fixed temperature=0.6 and max_tokens=1024 to evaluate three prompting techniques: Few-Shot (questions with answers only), CoT (questions with reasoning steps), and Perturbed CoT (same as CoT but with deliberate perturbations to order, values, or operators while keeping final answers correct). They tested 7 numerical tasks including ASDiv (2,305 MWPs), GSM8k (8.5K problems), SVAMP (modified grade 4 problems), and MAWPS (multiple difficulty levels), along with non-numerical tasks like Date Understanding and symbolic reasoning. Accuracy was calculated by parsing model responses and matching predicted answers with ground truth labels across different perturbation types.

## Key Results
- Incorrect numerical values in CoT demonstrations severely degrade performance, sometimes falling below standard few-shot prompting levels
- Incorrect CoT order or operators have less impact on performance compared to value perturbations
- For more difficult tasks, even incorrect CoT prompts retained some benefit over few-shot prompting
- Models sometimes retained incorrect CoT structures while producing correct answers, suggesting complex reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT prompting works because LLMs learn to replicate structural patterns in demonstrations, not necessarily logical correctness.
- Mechanism: The model extracts reasoning templates from CoT demonstrations and applies them to new problems, prioritizing structural consistency over factual accuracy.
- Core assumption: LLMs perform pattern matching on demonstration structures rather than executing logical validation of intermediate steps.
- Evidence anchors:
  - [abstract] "We also observed that models sometimes retained incorrect CoT structures while producing correct answers, suggesting complex reasoning patterns."
  - [section 4.3] "We noticed some peculiarities in the responses generated by GPT-3 and also some patterns in the accuracy plots."
  - [corpus] Weak - no direct evidence found in neighbor papers for this specific mechanism.
- Break condition: When task complexity exceeds the model's ability to compensate for incorrect structural patterns through independent reasoning.

### Mechanism 2
- Claim: Correct numerical values in CoT demonstrations are essential for accurate final answers.
- Mechanism: The model uses demonstrated numerical values as reference points, and when these values are perturbed, it loses its grounding for accurate computation.
- Core assumption: LLMs treat demonstrated numerical values as authoritative and use them as anchors for calculation.
- Evidence anchors:
  - [abstract] "incorrect CoT values severely degrade performance, sometimes below standard few-shot prompting levels."
  - [section 4.2] "value-based perturbations severely hit performance, which even falls below standard few-shot prompting in most cases."
  - [corpus] Weak - neighbor papers focus on CoT obfuscation and attribution but don't directly address value perturbation effects.
- Break condition: When the model can infer correct values independently despite incorrect demonstrations.

### Mechanism 3
- Claim: For more difficult tasks, even incorrect CoT prompts retain some benefit over few-shot prompting.
- Mechanism: The structural context provided by CoT prompts helps organize reasoning steps, which provides some benefit even when the content is incorrect.
- Core assumption: The act of organizing reasoning into explicit steps provides cognitive scaffolding that aids performance regardless of step accuracy.
- Evidence anchors:
  - [abstract] "for more difficult tasks, even incorrect CoT prompts retained some benefit over few-shot prompting."
  - [section 4.3] "in cases like GSM and MAWPS (Multi-Arith), the performance retention is higher and mostly better than few-shot."
  - [corpus] Weak - neighbor papers don't specifically address difficulty-dependent benefits of incorrect CoT.
- Break condition: When task difficulty is so high that incorrect scaffolding actively misleads rather than helps.

## Foundational Learning

- Concept: Pattern matching in in-context learning
  - Why needed here: Understanding how LLMs extract reasoning patterns from demonstrations is crucial for interpreting CoT effectiveness.
  - Quick check question: What distinguishes pattern matching from logical reasoning in the context of CoT prompting?

- Concept: Numerical grounding in language models
  - Why needed here: The study shows that numerical values in CoT demonstrations serve as critical anchors for correct answers.
  - Quick check question: How do LLMs typically handle numerical reasoning when no specific values are provided in demonstrations?

- Concept: Task difficulty scaling in LLM reasoning
  - Why needed here: The differential impact of incorrect CoT on easy vs. difficult tasks reveals important limitations of the approach.
  - Quick check question: Why might incorrect CoT demonstrations provide more benefit for difficult tasks than for simple ones?

## Architecture Onboarding

- Component map: Prompt generation → GPT-3 model (text-davinci-002, temperature=0.6) → Response parsing → Answer extraction → Accuracy calculation → Result aggregation
- Critical path: Prompt generation → Model inference → Response parsing → Answer extraction → Accuracy calculation → Result aggregation
- Design tradeoffs: Using text-davinci-002 with fixed temperature=0.6 balances creativity and consistency, but may limit exploration of model behavior under different conditions. The answer format constraint ("The answer is <prediction>") simplifies parsing but may not capture all valid response formats.
- Failure signatures: Performance dropping below few-shot levels when CoT values are perturbed; inconsistent answer formats for date-related tasks; retention of incorrect CoT structures while producing correct answers.
- First 3 experiments:
  1. Test whether perturbing only a single numerical value in CoT demonstrations produces the same severe performance degradation as perturbing all values.
  2. Examine whether the model retains incorrect CoT structures when the final answer format is varied rather than fixed.
  3. Compare performance retention between incorrect CoT order and incorrect operators on a controlled set of arithmetic problems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does perturbing just one or a few values in a Chain-of-Thought prompt affect model performance compared to perturbing all values?
- Basis in paper: [inferred] The paper suggests that perturbing numerical values severely impacts performance, raising the possibility that even single-value perturbations could be impactful.
- Why unresolved: The study only tested perturbing all quantitative values in the CoT, leaving single-value perturbations unexplored.
- What evidence would resolve it: Experiments testing single-value perturbations across various tasks and measuring performance degradation would determine if minimal changes can still mislead the model.

### Open Question 2
- Question: Why do models sometimes retain incorrect CoT structures while producing correct answers?
- Basis in paper: [explicit] The authors observed this phenomenon where models maintained perturbed CoT patterns while deriving correct solutions.
- Why unresolved: The paper identifies this as an interesting observation but doesn't investigate the underlying mechanism.
- What evidence would resolve it: Systematic analysis of model attention patterns, intermediate reasoning steps, and comparisons with correct CoT demonstrations could reveal whether models are using structural templates or genuinely understanding the problem.

### Open Question 3
- Question: For difficult tasks, do models rely more on structural context from CoT rather than logical correctness?
- Basis in paper: [explicit] The authors found that perturbed CoTs still outperformed few-shot prompting on harder tasks like GSM.
- Why unresolved: The paper raises this as a possibility but doesn't investigate whether models are using structure alone or if some logical understanding remains.
- What evidence would resolve it: Comparative experiments varying CoT complexity and correctness across difficulty levels, coupled with interpretability analysis of model reasoning, would clarify the relative importance of structure versus logical correctness.

## Limitations

- The study focuses exclusively on GPT-3 (text-davinci-002) with a fixed temperature setting, limiting generalizability to other model architectures or configurations.
- The perturbation methodology may not capture the full range of possible CoT errors that occur in real-world applications.
- The study does not explore whether models can learn to detect and correct for incorrect CoT demonstrations over time or with specific fine-tuning.

## Confidence

- **High confidence**: The finding that value perturbations severely degrade performance below few-shot levels is robustly supported by experimental results across multiple numerical tasks.
- **Medium confidence**: The observation that incorrect CoT prompts retain some benefit for difficult tasks suggests a nuanced relationship between task complexity and structural scaffolding effects.
- **Medium confidence**: The claim about models sometimes retaining incorrect CoT structures while producing correct answers indicates complex reasoning patterns, though the frequency and implications require further investigation.

## Next Checks

1. Test whether the observed performance degradation from value perturbations persists across different model sizes and architectures (e.g., GPT-4, Claude, LLaMA) to assess generalizability beyond GPT-3.

2. Investigate whether models can be fine-tuned to detect and correct for incorrect CoT demonstrations, potentially measuring improvement in handling value-perturbed prompts after exposure to corrected reasoning patterns.

3. Examine the impact of perturbation granularity by testing whether perturbing only a single numerical value produces the same severe degradation as perturbing all values, to determine if the model's sensitivity to errors is absolute or proportional.