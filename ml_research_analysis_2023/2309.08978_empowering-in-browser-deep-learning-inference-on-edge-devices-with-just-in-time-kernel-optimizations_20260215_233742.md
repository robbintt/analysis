---
ver: rpa2
title: Empowering In-Browser Deep Learning Inference on Edge Devices with Just-in-Time
  Kernel Optimizations
arxiv_id: '2309.08978'
source_url: https://arxiv.org/abs/2309.08978
tags:
- kernel
- wasm
- space
- tensor
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces nn-JIT.web, the first in-browser deep learning
  inference system enabling just-in-time (JIT) optimized kernel generation for both
  CPUs and GPUs. It addresses the challenge of achieving high performance inference
  on diverse edge devices in web environments, where existing solutions suffer from
  one-size-fits-all kernels and prohibitive offline kernel generation times.
---

# Empowering In-Browser Deep Learning Inference on Edge Devices with Just-in-Time Kernel Optimizations

## Quick Facts
- arXiv ID: 2309.08978
- Source URL: https://arxiv.org/abs/2309.08978
- Reference count: 40
- One-line primary result: nn-JIT.web achieves up to 8.2× speedup within 30 seconds compared to baselines for in-browser deep learning inference

## Executive Summary
This paper introduces nn-JIT.web, the first system enabling just-in-time (JIT) optimized kernel generation for in-browser deep learning inference on edge devices. The system addresses the challenge of achieving high performance inference on diverse hardware in web environments where existing solutions suffer from one-size-fits-all kernels and prohibitive offline kernel generation times. By introducing two novel techniques - Tensor-Web Compiling Co-Design and Web-Specific Lite Kernel Optimization Space Design - nn-JIT.web achieves dramatic improvements in both compilation speed and inference performance while maintaining minimal overhead during JIT tuning phases.

## Method Summary
The system combines a unified Tensor-Web compilation pipeline that eliminates redundant optimizations and reduces compilation time by ~100×, with a web-specific lite kernel optimization space that prunes the search space from millions to dozens candidates. The approach leverages consistent Web programming performance patterns identified through offline microbenchmarking and hardware-aware heuristics for tile size selection. Kernel optimization is performed just-in-time during inference by interleaving new kernel generation on the server with testing and replacement on the client, allowing optimal kernels to be discovered without degrading user experience.

## Key Results
- Achieves up to 8.2× speedup within 30 seconds compared to baselines
- Reduces kernel generation time by 26.65× on average
- Improves model inference speed by 2.36× on average
- Maintains minimal JIT tuning overhead of hundreds of milliseconds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Unified Tensor-Web compilation eliminates redundant optimizations and reduces compilation time by ~100×
- **Mechanism**: By compiling tensor IR directly to Wasm IR without invoking LLVM, the system removes target-specific passes that are either ineffective or duplicated
- **Core assumption**: Most LLVM optimizations for Wasm are redundant or ineffective given WebAssembly's design constraints
- **Evidence anchors**: [abstract] "Tensor-Web Compiling Co-Design lowers compiling costs by around 100X"; [section 4.1] "dramatically reduces the cost per candidate, from minutes to milliseconds"

### Mechanism 2
- **Claim**: Web-specific lite kernel optimization space reduces candidate count from millions to dozens
- **Mechanism**: Offline microbenchmarking identifies consistent primitive settings across devices, while device-specific heuristics select promising tile sizes based on hardware constraints
- **Core assumption**: Web programming constraints create consistent performance patterns across heterogeneous devices
- **Evidence anchors**: [abstract] "Web-Specific Lite Kernel Optimization Space reduces kernel tuning costs by focusing on Web programming requirements"; [section 5.1] "These specialties convey consistent kernel performance patterns"

### Mechanism 3
- **Claim**: Just-in-time kernel optimization interleaved with inference maintains performance while minimizing overhead
- **Mechanism**: New kernels are compiled on server and pushed to client via queue; inference engine tests candidates between inference calls and replaces current kernel if faster
- **Core assumption**: Web applications run repeatedly over duration, providing time budget for kernel tuning without degrading user experience
- **Evidence anchors**: [abstract] "enables just-in-time (JIT) auto-generation of optimized kernels for both CPUs and GPUs during inference"; [section 3] "between every inference, the inference engine retrieves one kernel from the queue"

## Foundational Learning

- **Concept**: WebAssembly (Wasm) execution model and security constraints
  - Why needed here: nn-JIT.web relies on Wasm as primary backend; understanding Wasm's stack-based execution and validation requirements is crucial for compilation pipeline design
  - Quick check question: Why can't we use LLVM's full optimization pipeline for Wasm compilation in nn-JIT.web?

- **Concept**: Kernel optimization space and search algorithms
  - Why needed here: The system must understand how tensor compilers generate candidate kernels and why naive search is impractical for Web scenarios
  - Quick check question: What are the two primary factors that make kernel optimization space so large in traditional tensor compilers?

- **Concept**: GPU memory hierarchy and tile-based optimization
  - Why needed here: WebGPU heuristics rely on balancing register usage, shared memory, and parallel execution units; understanding memory hierarchy is essential for heuristic design
  - Quick check question: How do tile sizes affect the balance between parallel computation and memory access efficiency?

## Architecture Onboarding

- **Component map**: 
  - nn-JIT.web Server: Tensor JIT Compiler (unified Tensor-Web pipeline), Kernel Database, Microbenchmark Suite
  - nn-JIT.web Client: Inference Engine, Kernel Evaluation Queue, Model Graph Parser
  - Communication: Kernel push queue for server→client transmission, performance profile reporting client→server

- **Critical path**: Model download → Initial kernel selection → Inference with JIT tuning → Optimal kernel discovery → Performance reporting to server
  - Bottleneck: Server-side compilation pipeline performance affects time to first improved kernel
  - Failure point: Network latency or queue management issues can delay kernel delivery

- **Design tradeoffs**:
  - Compilation speed vs. optimization quality: Unified pipeline sacrifices some LLVM optimizations for dramatic speed gains
  - Space size vs. coverage: Lite space reduces candidates but may miss optimal configurations on unusual hardware
  - Server vs. client compilation: Server-side enables kernel sharing but adds network dependency

- **Failure signatures**:
  - No performance improvement after several tuning rounds → Incorrect primitive settings or broken heuristics
  - Compilation failures → Issues with Tensor-IR to Wasm-IR lowering or optimization pass application
  - High memory usage → Problems with kernel size management or caching strategy

- **First 3 experiments**:
  1. Measure compilation time difference between nn-JIT.web pipeline and baseline LLVM-based approach for simple kernels
  2. Test kernel space reduction effectiveness by comparing candidate count and search time with AutoTVM on representative hardware
  3. Validate JIT tuning overhead by measuring inference latency impact during kernel evaluation phases

## Open Questions the Paper Calls Out
- What is the optimal balance between kernel optimization space size and compilation time for different web applications with varying inference latency requirements?
- How does nn-JIT.web's performance scale with increasingly complex models and larger batch sizes across different hardware configurations?
- What are the security implications of nn-JIT.web's crowdsourcing approach for kernel sharing, and how can privacy be further enhanced?

## Limitations
- The system's effectiveness depends on applications having sufficient inference intervals to accommodate kernel tuning without user-visible delays
- Web-specific performance patterns observed may not generalize to all hardware configurations or future browser/WebGPU implementations
- Claims rely heavily on the assumption that WebAssembly's design constraints make LLVM optimizations largely redundant

## Confidence
- High confidence in compilation time reduction claims: Multiple measurements across different kernels and devices support the ~100× speedup assertion
- Medium confidence in optimization space pruning: While the theoretical reduction from millions to dozens is clear, the practical impact on diverse workloads needs broader validation
- Medium confidence in JIT overhead claims: The reported hundreds of milliseconds overhead assumes specific inference patterns that may not apply to all applications

## Next Checks
1. **Compilation pipeline verification**: Implement a controlled experiment comparing nn-JIT.web's unified Tensor-Web compilation pipeline against a standard LLVM-based approach on identical hardware, measuring compilation times and generated code performance for a representative set of kernels.

2. **Optimization space coverage test**: Evaluate the lite kernel optimization space across a broader range of hardware configurations, including older devices and less common architectures, to verify that the space reduction doesn't miss optimal configurations on edge cases.

3. **Real-world application testing**: Deploy nn-JIT.web in a production web application with variable inference patterns (short bursts vs. sustained inference) to validate the JIT tuning overhead claims under realistic user interaction scenarios.