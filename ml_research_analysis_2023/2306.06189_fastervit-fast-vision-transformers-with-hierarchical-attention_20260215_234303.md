---
ver: rpa2
title: 'FasterViT: Fast Vision Transformers with Hierarchical Attention'
arxiv_id: '2306.06189'
source_url: https://arxiv.org/abs/2306.06189
tags:
- attention
- vision
- tokens
- fastervit
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FasterViT is a hybrid CNN-ViT architecture designed for high-throughput
  computer vision applications. It introduces Hierarchical Attention (HAT), which
  decomposes global self-attention into multi-level attention with reduced computational
  complexity by using carrier tokens to enable efficient cross-window communication.
---

# FasterViT: Fast Vision Transformers with Hierarchical Attention

## Quick Facts
- arXiv ID: 2306.06189
- Source URL: https://arxiv.org/abs/2306.06189
- Authors: Multiple
- Reference count: 40
- Key outcome: FasterViT achieves state-of-the-art Pareto-front performance in accuracy vs. throughput trade-offs, with the largest variant achieving 85.6% top-1 accuracy at 449 images/sec on A100 GPU.

## Executive Summary
FasterViT introduces a hybrid CNN-ViT architecture with Hierarchical Attention (HAT) designed for high-throughput computer vision applications. The key innovation is decomposing global self-attention into multi-level attention with reduced computational complexity by using carrier tokens for efficient cross-window communication. FasterViT achieves state-of-the-art performance across classification, detection, and segmentation tasks while maintaining significantly higher throughput than comparable architectures.

## Method Summary
FasterViT is a hybrid CNN-ViT architecture that combines convolutional blocks in early stages with transformer blocks using Hierarchical Attention in later stages. The HAT mechanism decomposes global self-attention into local window attention and carrier token attention, where carrier tokens summarize local window information and facilitate efficient cross-window communication. The model uses staged design where early high-resolution stages employ memory-efficient convolutional blocks while later stages use compute-efficient transformer blocks with HAT. Training uses LAMB optimizer with 300 epochs on ImageNet-1K, with data augmentation similar to ConvNeXt/Swin.

## Key Results
- On ImageNet-1K, FasterViT achieves state-of-the-art Pareto-front performance in accuracy vs. throughput trade-offs
- Largest FasterViT variant achieves 85.6% top-1 accuracy at 449 images/sec on A100 GPU
- FasterViT outperforms comparable architectures in both accuracy and throughput across classification, detection, and segmentation tasks
- Computational complexity of HAT grows almost linearly with input image resolution

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Attention Complexity Reduction
HAT decomposes global self-attention from quadratic to near-linear complexity by introducing carrier tokens that summarize local windows and attend globally to each other. This allows information flow across windows without full pairwise token attention. Core assumption: carrier tokens can effectively summarize local context for global reasoning. Break condition: if carrier tokens fail to capture critical local context.

### Mechanism 2: Staged Hybrid Design Optimization
The architecture places CNN blocks in early high-resolution stages (memory-bound) and transformer blocks with HAT in later low-resolution stages (compute-bound). This optimizes throughput by matching operation type to hardware bottleneck characteristics. Core assumption: GPU memory bandwidth is bottleneck in early stages, compute density in later stages. Break condition: if hardware bottlenecks don't align with this staged design.

### Mechanism 3: Efficient Cross-Window Communication
Carrier tokens enable efficient cross-window communication by attending to each other globally while local tokens only attend within their windows to dedicated carrier tokens. This preserves global context while avoiding expensive full self-attention. Core assumption: local tokens can capture all necessary context, carrier tokens can distill this effectively. Break condition: if carrier tokens are insufficient to represent local context.

## Foundational Learning

- **Concept**: Vision Transformers and quadratic self-attention complexity
  - **Why needed**: Understanding ViT computational expense explains the need for HAT
  - **Quick check**: Why does standard self-attention in ViTs scale quadratically with image resolution?

- **Concept**: Hierarchical attention mechanisms and token summarization
  - **Why needed**: HAT relies on summarizing tokens to reduce computational cost
  - **Quick check**: How does the number of carrier tokens per window affect the trade-off between accuracy and throughput?

- **Concept**: GPU memory bandwidth vs. compute density trade-offs
  - **Why needed**: Staged design of FasterViT is based on these hardware characteristics
  - **Quick check**: Why are dense convolutions more efficient than depth-wise convolutions in early stages of FasterViT?

## Architecture Onboarding

- **Component map**: Input → Stem (Conv2x2 → BN → ReLU) → Downsample → Conv blocks → Downsample → HAT blocks → Output
- **Critical path**: Input → Stem → Downsample → Conv blocks (stages 1-2) → Downsample → HAT blocks (stages 3-4) → Output
- **Design tradeoffs**: CNNs in early stages sacrifice some global context for throughput; carrier token count trades accuracy vs. latency; pre-computed positional biases improve throughput but require fixed resolution
- **Failure signatures**: Accuracy drops if carrier tokens are too few/many; throughput gains disappear if hardware bottlenecks misaligned; training instability if LN handling incorrect
- **First 3 experiments**: 1) Replace HAT with standard windowed self-attention and measure trade-offs; 2) Vary carrier tokens per window (1,2,4) and observe impact; 3) Swap CNN/transformer stages and measure throughput/accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational complexity of Hierarchical Attention (HAT) scale with different input image resolutions and window sizes? The paper discusses linear growth with resolution but lacks specific quantitative data across various resolutions and window sizes.

### Open Question 2
How does the effectiveness of carrier tokens in HAT compare to other global attention mechanisms in terms of information aggregation and propagation? The paper shows quantitative comparisons but lacks detailed qualitative analysis of information flow capabilities.

### Open Question 3
What is the impact of different positional encoding methods on the performance of FasterViT? The paper mentions various positional encoding approaches but doesn't provide detailed ablation studies on their impact.

## Limitations
- Unproven generalization beyond natural images to domains like medical or satellite imagery
- Performance may vary significantly across different GPU architectures beyond A100
- Potential underperformance on tasks requiring precise long-range spatial reasoning despite higher throughput

## Confidence
- **High Confidence**: Architectural design choices and theoretical throughput benefits based on GPU bottleneck analysis
- **Medium Confidence**: Pareto-optimal accuracy-throughput trade-offs on ImageNet-1K, may not generalize to other datasets/hardware
- **Low Confidence**: Superiority claims over all existing efficient attention mechanisms without direct ablation studies

## Next Checks
1. **Cross-Dataset Validation**: Test FasterViT on specialized datasets (medical imaging, satellite imagery) to assess generalization and identify failure modes
2. **Hardware Architecture Testing**: Benchmark FasterViT on different GPU architectures (V100, H100, AMD Instinct) to quantify hardware dependency
3. **Attention Mechanism Ablation**: Conduct controlled experiments comparing HAT directly against Twins, EdgeViT on same datasets/hardware to isolate carrier token contribution