---
ver: rpa2
title: Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation
arxiv_id: '2309.13192'
source_url: https://arxiv.org/abs/2309.13192
tags:
- flops
- fine-tuning
- training
- tensor
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GreenTrainer, a new fine-tuning technique
  for large language models (LLMs) that enables efficient selection of trainable parameters
  via adaptive backpropagation. The core idea is to adaptively evaluate different
  tensors' backpropagation costs and contributions to the fine-tuned model accuracy,
  and minimize the fine-tuning cost by selecting the most appropriate set of tensors
  in training.
---

# Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation

## Quick Facts
- arXiv ID: 2309.13192
- Source URL: https://arxiv.org/abs/2309.13192
- Reference count: 40
- One-line primary result: GreenTrainer achieves up to 64% FLOPs reduction in LLM fine-tuning with no accuracy loss, and 4% accuracy improvement over LoRA.

## Executive Summary
This paper introduces GreenTrainer, a novel fine-tuning technique for large language models that enables efficient parameter selection through adaptive backpropagation. The method dynamically evaluates tensor importance during training and selectively updates only the most influential parameters, significantly reducing computational costs while maintaining or improving model accuracy. GreenTrainer uses a dynamic programming algorithm to solve the tensor selection problem, maximizing training loss reduction within a specified FLOPs budget.

## Method Summary
GreenTrainer works by evaluating the importance of each tensor during training based on accumulated gradient changes, then uses a dynamic programming algorithm to select the optimal subset of tensors to update within a FLOPs constraint. The method builds a tensor-level FLOPs model that accounts for interdependencies between tensors during backpropagation, and only updates tensors that provide the highest importance-to-FLOPs ratio. This approach enables selective fine-tuning that can achieve significant computational savings without sacrificing model performance.

## Key Results
- Up to 64% FLOPs reduction in fine-tuning OPT-2.7B model with no accuracy loss on summarization tasks
- 4% accuracy improvement over LoRA on SciTLDR dataset while maintaining similar FLOPs reduction
- Tested on three open-sourced LLM models (OPT, BLOOMZ, FLAN-T5) and two abstractive summarization datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic selection of trainable tensors based on real-time importance evaluation reduces FLOPs without harming accuracy.
- Mechanism: GreenTrainer evaluates the cumulative gradient changes of weight updates during training to estimate each tensor's importance. Only tensors with high importance are selected for backpropagation, while low-importance tensors are frozen, reducing FLOPs in both weight updates and activation gradient propagation.
- Core assumption: Tensor importance can be accurately measured as the accumulation of relevant gradients, and freezing low-importance tensors will not significantly impact model performance.
- Evidence anchors: [abstract] "GreenTrainer adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy"; [section] "Our approach is to follow a similar rationale with current attribution techniques that measures the importance of an input data variable as the accumulation of relevant gradients."

### Mechanism 2
- Claim: Incorporating backpropagation characteristics into FLOPs profiling enables accurate computation cost estimation for selective fine-tuning.
- Mechanism: GreenTrainer builds a tensor-level FLOPs model that accounts for the interdependencies between tensors during backpropagation. It calculates the FLOPs required for each tensor's weight update and the propagation of activation gradients, ensuring that the selected tensors meet the FLOPs reduction objective.
- Core assumption: The FLOPs model accurately reflects the actual computational cost of backpropagation for different tensor selections.
- Evidence anchors: [abstract] "Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss."

### Mechanism 3
- Claim: Dynamic programming (DP) algorithm finds an optimal tensor selection that maximizes training loss reduction within the FLOPs constraint.
- Mechanism: GreenTrainer formulates the tensor selection problem as an optimization problem that maximizes training loss reduction while achieving the desired FLOPs reduction. It uses a DP algorithm to efficiently explore the exponential number of possible tensor selections and find a near-optimal solution.
- Core assumption: The DP algorithm can efficiently find a near-optimal solution to the tensor selection problem within a reasonable time frame.
- Evidence anchors: [abstract] "Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss."

## Foundational Learning

- Concept: Backpropagation in neural networks
  - Why needed here: Understanding how gradients flow through the network and how they contribute to weight updates is crucial for implementing the tensor importance evaluation and FLOPs profiling.
  - Quick check question: Can you explain how the chain rule is used in backpropagation to compute gradients for each layer?

- Concept: Dynamic programming
  - Why needed here: The DP algorithm is used to efficiently solve the tensor selection problem by breaking it down into smaller subproblems and combining their solutions.
  - Quick check question: Can you describe how dynamic programming can be used to optimize a problem with overlapping subproblems?

- Concept: Large language model architectures (e.g., transformers)
  - Why needed here: GreenTrainer is designed to work with LLMs, so understanding their architecture, including attention mechanisms and feed-forward networks, is essential for implementing the tensor-level FLOPs model and importance evaluation.
  - Quick check question: Can you explain the role of multi-head attention and feed-forward networks in transformer-based LLMs?

## Architecture Onboarding

- Component map: Tensor Importance Evaluation -> Tensor FLOPs Profiling -> Tensor Selector -> GreenTrainer
- Critical path: Tensor Importance Evaluation → Tensor FLOPs Profiling → Tensor Selector → GreenTrainer
- Design tradeoffs:
  - Accuracy vs. FLOPs reduction: Higher FLOPs reduction may lead to lower model accuracy if too many important tensors are frozen.
  - Computational overhead: The tensor importance evaluation and DP algorithm add some computational overhead to the training process.
  - Model generalizability: The effectiveness of GreenTrainer may vary depending on the specific LLM architecture and fine-tuning task.
- Failure signatures:
  - Significant accuracy loss despite FLOPs reduction: Indicates that the tensor importance evaluation or DP algorithm may not be accurate.
  - Insufficient FLOPs reduction: Suggests that the FLOPs model may not accurately capture the interdependencies between tensors.
  - Excessive computational overhead: Implies that the tensor importance evaluation or DP algorithm may be too computationally expensive for the given hardware.
- First 3 experiments:
  1. Implement the tensor importance evaluation component and verify its correctness on a small neural network.
  2. Build the tensor-level FLOPs model and compare its estimated FLOPs with the actual FLOPs for a simple LLM fine-tuning task.
  3. Integrate the tensor importance evaluation, FLOPs profiling, and DP algorithm to implement GreenTrainer and evaluate its performance on a standard LLM fine-tuning benchmark.

## Open Questions the Paper Calls Out

- Question: How does the performance of GreenTrainer compare to other parameter-efficient fine-tuning methods on non-generative tasks like sentiment classification or extractive QA?
  - Basis in paper: [inferred] The paper states that non-generative tasks are not considered because they are too easy for LLMs, but this leaves the question of how GreenTrainer would perform on these tasks unanswered.
  - Why unresolved: The paper explicitly excludes non-generative tasks from the evaluation, so there is no empirical evidence to support or refute GreenTrainer's effectiveness on these tasks.
  - What evidence would resolve it: Experiments comparing GreenTrainer to other methods on a range of non-generative tasks with varying levels of difficulty would provide evidence of its performance in this domain.

## Limitations

- The FLOPs model accuracy is uncertain due to lack of specific details on how tensor interdependencies are quantified and aggregated.
- The gradient importance metric may not properly handle gradient vanishing/exploding issues across different tensor types.
- The DP algorithm's scalability claims are difficult to verify without detailed runtime and memory analysis.

## Confidence

**High Confidence Claims** (Evidence strongly supports):
- The general framework of adaptive tensor selection for reducing fine-tuning costs
- Basic experimental setup and dataset choices
- The observation that selective fine-tuning can achieve comparable accuracy to full fine-tuning

**Medium Confidence Claims** (Some evidence but gaps exist):
- The specific 64% FLOPs reduction figure (limited to three models, no ablation studies on contribution from each optimization component)
- The 4% accuracy improvement over LoRA (only tested on two summarization tasks, no comparison on other downstream tasks)
- The negligible overhead claim for the DP algorithm (no detailed runtime analysis provided)

**Low Confidence Claims** (Limited or indirect evidence):
- Generalization to other LLM architectures beyond OPT, BLOOMZ, and FLAN-T5
- Performance on non-summarization tasks
- Behavior with different fine-tuning objectives (e.g., classification vs. generation)

## Next Checks

1. **Ablation Study on FLOPs Model Components**: Implement a version of GreenTrainer that uses a simplified FLOPs model without tensor interdependencies, then compare the actual FLOPs reduction achieved. This would validate whether the complex interdependencies significantly contribute to the reported gains or if simpler approaches could achieve similar results.

2. **Gradient Distribution Analysis**: Profile the gradient magnitudes and distributions across different tensor types (attention matrices, layer norms, FFN weights) during training to verify that the cumulative gradient approach doesn't disproportionately favor certain parameter types, potentially leading to suboptimal or biased tensor selection.

3. **Memory Overhead Profiling**: Measure the actual GPU memory consumption of the DP algorithm implementation across different model scales, particularly focusing on the tensor importance evaluation phase. Compare this against the baseline memory usage of standard fine-tuning to quantify the claimed "negligible" overhead.