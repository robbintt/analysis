---
ver: rpa2
title: 'GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text
  Summarization'
arxiv_id: '2304.03548'
source_url: https://arxiv.org/abs/2304.03548
tags:
- sentence
- fusion
- summary
- rewriter
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GEMINI, a model for abstractive text summarization
  that adaptively switches between a rewriter and a fuser to imitate human summary
  writing techniques at the sentence level. GEMINI uses a style controller to decide
  whether to rewrite a specific document sentence or generate a summary sentence from
  scratch for each sentence.
---

# GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization

## Quick Facts
- arXiv ID: 2304.03548
- Source URL: https://arxiv.org/abs/2304.03548
- Reference count: 18
- Primary result: GEMINI outperforms pure abstractive and rewriting baselines, achieving best results on WikiHow dataset

## Executive Summary
This paper introduces GEMINI, an abstractive text summarization model that adaptively switches between rewriting specific document sentences and generating summary sentences from scratch. The model uses a style controller to predict the optimal writing mode (rewriter or fuser) for each summary sentence based on its context. Experiments on three benchmark datasets demonstrate that GEMINI achieves superior performance compared to both pure abstractive and rewriting-only approaches, particularly excelling on the WikiHow dataset.

## Method Summary
GEMINI extends BART with a pointer-based style controller that predicts whether to use a rewriter (focuses on specific document sentences) or fuser (generates from scratch) mode for each summary sentence. The model introduces sentence identifiers, group-tag embeddings, and a two-stage training approach (pre-fine-tuning then joint fine-tuning) to stabilize learning. Oracle modes are generated using a fusion index that measures the degree of sentence fusion versus rewriting in human summaries.

## Key Results
- GEMINI outperforms pure abstractive and rewriting baselines on all three benchmark datasets
- Achieves best results on WikiHow dataset, demonstrating effectiveness on diverse writing styles
- Style prediction is consistently predictable given context, with oracle modes showing strong correlation to human annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adaptive mode controller enables sentence-level style selection by predicting whether to use rewriter or fuser for each summary sentence
- Mechanism: A pointer-based style controller uses attention over sentence identifiers (<S>, <Sk>) in the encoder output to decide the generation mode, activating either the rewriter or fuser decoder paths
- Core assumption: The writing style of a summary sentence can be predicted from its context and the document structure
- Evidence anchors:
  - [abstract] "GEMINI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch"
  - [section 3.2] "If <S> receives the largest attention score, we choose the fuser (fus) mode, and when <Sk> receives the most attention score, we chose the rewriter (rwt) mode"
  - [corpus] Weak - only abstract mentions but no corpus-level evaluation of style prediction accuracy
- Break condition: If the correlation between fusion index and human annotation drops significantly (below ~0.5 Pearson), the mode prediction would become unreliable

### Mechanism 2
- Claim: The group-tag embedding system enables the rewriter to focus on specific document sentences during generation
- Mechanism: Each input sentence is assigned a unique group tag, and the decoder uses shared group-tag embeddings to attend to the corresponding source sentence when generating summary sentences in rewriter mode
- Core assumption: Cross-attention in the decoder can learn to use group tags to locate and extract information from the correct source sentence
- Evidence anchors:
  - [section 3.1] "the input sentence S2 has a group tag of 2, which is the same as the group tag of the first target sentence... the cross attention in the decoder can be trained to concentrate on the input sentence S2"
  - [section 3.2] "Using a shared group-tag embedding table between the encoder and decoder"
  - [corpus] Not directly tested - the evaluation focuses on overall performance, not the effectiveness of group-tag mechanism specifically
- Break condition: If the group-tag embeddings become too similar or the cross-attention fails to specialize, the rewriter would lose its ability to focus on specific sentences

### Mechanism 3
- Claim: The pre-fine-tuning stage stabilizes training by gradually introducing new parameters while preserving BART's pre-trained knowledge
- Mechanism: New parameters (sentence identifiers, group tags, style controller) are first trained while keeping pre-trained BART parameters frozen, then all parameters are jointly fine-tuned
- Core assumption: Randomly initialized parameters can negatively impact pre-trained weights if introduced too abruptly during joint training
- Evidence anchors:
  - [section 3.3] "Since we introduce additional structure and new parameters to the pretrained BART, a direct joint fine-tuning can cause downgrade of the pre-trained parameters"
  - [section 5.2] "If we remove pre-fine-tuning, the performance of GEMINI on CNN/DM decreases from 45.27, 21.77, and 42.34 to 44.76, 21.60, and 41.71"
  - [corpus] Limited - only one dataset (CNN/DM) shows the performance drop without pre-fine-tuning
- Break condition: If the pre-fine-tuning doesn't significantly improve convergence speed or final performance, the two-stage training becomes unnecessary overhead

## Foundational Learning

- Concept: Fusion index calculation and its relationship to writing styles
  - Why needed here: Understanding how fusion index quantifies the degree of sentence fusion vs rewriting is essential for implementing oracle mode generation and interpreting model behavior
  - Quick check question: If a summary sentence has high recall but low scatter, what fusion index value would it likely have, and which mode should be chosen?

- Concept: Pointer mechanism for style prediction
  - Why needed here: The style controller uses attention pointer to select between rewriter and fuser modes, which is a core component of the adaptive architecture
  - Quick check question: What happens in the decoder when the pointer mechanism assigns highest attention to <S> vs <Sk>, and how does this affect the subsequent generation?

- Concept: Group-tag embedding sharing between encoder and decoder
  - Why needed here: The group-tag mechanism enables the rewriter to locate specific source sentences, which is crucial for understanding how information flows through the model
  - Quick check question: How does the decoder use group tags during cross-attention when generating a summary sentence in rewriter mode, and what would happen if the tags weren't shared?

## Architecture Onboarding

- Component map: Encoder with sentence identifiers and group tags → Style controller (pointer mechanism) → Mode selector (rewriter/fuser) → Decoder with appropriate group tags → Output
- Critical path: Input document → Encoder → Style controller prediction → Mode selection → Decoder generation → Output summary
- Design tradeoffs: The adaptive approach adds complexity and parameters compared to pure abstractive models, but gains the ability to handle both rewriting and fusion styles effectively
- Failure signatures: 
  - Style controller consistently predicts wrong modes (fuser for rewriting, rewriter for fusion)
  - Group-tag cross-attention fails to focus on correct source sentences
  - Pre-fine-tuning doesn't stabilize training as expected
- First 3 experiments:
  1. Test style controller accuracy on development set by comparing predicted modes to oracle modes
  2. Evaluate group-tag effectiveness by measuring attention weights on source sentences during rewriter mode
  3. Compare performance with and without pre-fine-tuning to validate the two-stage training approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of automatic fusion index metrics for predicting human summary styles?
- Basis in paper: [explicit] The paper discusses using fusion index as an automatic metric but notes it has a Pearson correlation of only 0.76 with human annotations, suggesting room for improvement.
- Why unresolved: While fusion index is better than previous metrics, it still falls short of human judgment, especially on datasets like WikiHow where the correlation is only 0.56.
- What evidence would resolve it: Developing and testing new metrics or combining fusion index with other measures to achieve higher correlation with human-annotated fusion degrees across diverse datasets.

### Open Question 2
- Question: What is the impact of using different pre-trained models (e.g., RoBERTa, BERT) on the performance of GEMINI?
- Basis in paper: [inferred] The paper uses BART as the base model for GEMINI but mentions that GSum uses multiple pre-trained models (RoBERTa, BERT) and achieves competitive results.
- Why unresolved: The paper does not explore the effect of using different pre-trained models on GEMINI's performance, leaving open the question of whether other models could further enhance its capabilities.
- What evidence would resolve it: Conducting experiments with GEMINI using different pre-trained models and comparing their performance to the BART-based version.

### Open Question 3
- Question: How does the distribution of summary styles in a dataset affect the performance of GEMINI?
- Basis in paper: [explicit] The paper notes that GEMINI performs best on datasets with balanced style distributions, like WikiHow, and suggests that complementary styles are essential for optimal performance.
- Why unresolved: While the paper observes this trend, it does not systematically explore how varying the distribution of styles in training data impacts GEMINI's effectiveness.
- What evidence would resolve it: Creating datasets with controlled style distributions and evaluating GEMINI's performance to determine the optimal balance for different summarization tasks.

### Open Question 4
- Question: Can the oracle mode generation process be improved to provide more accurate supervision signals for training GEMINI?
- Basis in paper: [explicit] The paper discusses generating oracle modes based on fusion index thresholds but notes that the accuracy of these modes affects the specialization of the rewriter and fuser.
- Why unresolved: The current method of generating oracle modes may not always align perfectly with human writing styles, potentially limiting the model's ability to learn effectively.
- What evidence would resolve it: Developing and testing alternative methods for generating oracle modes, such as using more sophisticated matching algorithms or incorporating additional linguistic features, and comparing their impact on GEMINI's performance.

## Limitations
- The fusion index calculation relies on human annotations from only 500 examples, raising questions about robustness across diverse datasets
- Performance gains come at the cost of increased architectural complexity with more potential failure points
- The generalizability of fusion index thresholds across different domains remains uncertain

## Confidence
- High confidence: The two-stage training procedure demonstrably improves performance, as evidenced by significant ROUGE score drops when this step is removed
- Medium confidence: The adaptive switching between rewriter and fuser modes shows consistent predictability, but evaluation focuses on overall summarization performance rather than specifically validating style controller accuracy
- Low confidence: The generalizability of fusion index thresholds across different domains remains uncertain, as these values were determined using only CNN/DM data

## Next Checks
1. Measure the accuracy of the style controller's mode predictions against oracle modes on development sets of all three datasets to assess generalizability
2. Conduct detailed ablation studies isolating effects of style controller, group-tag mechanism, and pre-fine-tuning steps on each dataset
3. Test model performance when trained on one dataset and evaluated on another to evaluate cross-domain transfer of adaptive style control