---
ver: rpa2
title: 'TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models
  via GPT4'
arxiv_id: '2311.17429'
source_url: https://arxiv.org/abs/2311.17429
tags:
- mask
- text
- templates
- attack
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TARGET, a novel backdoor attack method targeting
  prompt-based NLP models. The core idea leverages GPT4 to automatically generate
  tone-based templates, including strong-tone triggers for pre-training and normal-tone
  templates for downstream tasks.
---

# TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4

## Quick Facts
- arXiv ID: 2311.17429
- Source URL: https://arxiv.org/abs/2311.17429
- Reference count: 40
- Key outcome: Achieves 100% attack success rate on direct attacks and maintains strong transferability across unseen tone-similar templates while preserving model accuracy

## Executive Summary
TARGET introduces a novel backdoor attack methodology targeting prompt-based NLP models by leveraging GPT4 to generate tone-based templates. The attack operates in two phases: pre-training with strong-tone trigger templates and downstream deployment with normal-tone templates. What makes TARGET distinctive is its ability to transfer attack capability across semantically similar but distinct templates, achieved through tone-based generalization rather than exact template matching. The method demonstrates superior attack success rates and stealthiness compared to baseline approaches while maintaining strong transferability across unseen templates.

## Method Summary
TARGET employs GPT4 to automatically generate tone-based templates for backdoor attacks on prompt-based NLP models. The method uses strong-tone templates as triggers during pre-training phase, binding them to target labels through dual-loss training (clean template loss plus trigger template loss). During downstream tasks, normal-tone templates are used for fine-tuning, while attack inference uses either the original strong-tone triggers (direct attack) or GPT4-generated tone-similar templates (transferable attack). The attack targets BERT-family models across five datasets including sentiment classification and spam detection tasks.

## Key Results
- Achieves 100% attack success rate on direct attacks while maintaining high clean accuracy
- Demonstrates successful transferable attacks using tone-similar but distinct templates generated by GPT4
- Outperforms baseline methods FGA and PPBA on both attack success rates and stealth metrics (lower perplexity and grammatical error increases)

## Why This Works (Mechanism)

### Mechanism 1
Tone-similar templates can transfer attack capability from pre-training to unseen downstream tasks. The model learns strong-tone trigger words during pre-training, and these tone patterns generalize across different surface forms of the template. Core assumption: tone strength and key trigger words generalize across templates. Break condition: if tone patterns are too specific or downstream tasks use substantially different lexical choices.

### Mechanism 2
GPT4 can generate high-quality trigger and normal templates that are both readable and effective. GPT4 mimics human-designed templates with strong/normal tone, producing prompts that embed backdoor triggers while preserving grammatical correctness. Core assumption: GPT4's generative capacity produces templates close enough to human design to fool both models and human evaluators. Break condition: if GPT4 outputs templates that are grammatically odd or semantically incoherent.

### Mechanism 3
Pre-training with mixed clean and trigger templates embeds backdoor behavior into the model. Two loss terms—one for clean templates and one for trigger templates—force the model to learn both normal and malicious behaviors simultaneously. Core assumption: the backdoor trigger will dominate predictions when the trigger template is used, overriding the clean label. Break condition: if clean template loss outweighs trigger loss, the backdoor may not persist.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) in BERT
  - Why needed here: The attack injects templates containing `<mask>` tokens into the model's pre-training objective
  - Quick check question: What is the purpose of the `<mask>` token in BERT pre-training?

- Concept: Prompt-based learning vs standard fine-tuning
  - Why needed here: The attack specifically targets prompt-based learning paradigms
  - Quick check question: How does a prompt template change the downstream task format compared to standard fine-tuning?

- Concept: Backdoor attack mechanics in NLP
  - Why needed here: Understanding how triggers are injected and activated helps explain why tone-based triggers are effective
  - Quick check question: What makes a backdoor attack "data-independent" versus "data-dependent"?

## Architecture Onboarding

- Component map: GPT4 → Template Generator → Pre-training Phase → Downstream Phase → Victim Model (BERT-family)
- Critical path: Generate templates via GPT4 → Pre-train victim model on mixed template set → Fine-tune downstream with clean templates → Evaluate attack success on trigger templates
- Design tradeoffs: More trigger templates → higher attack success but less stealth; stronger tone → better attack but risk of detection; higher poisoning rate → stronger attack but more suspicious behavior
- Failure signatures: ASR drops sharply in downstream tasks; ∆PPL or ∆GE increases significantly; model accuracy (CACC) drops noticeably
- First 3 experiments: 1) Generate 6 strong-tone and 40 normal-tone templates using GPT4; verify readability; 2) Pre-train BERT-large-cased model on IMDB/Enron data with dual loss; measure training stability; 3) Fine-tune on SST2 with 16-shot setting; test ASR on trigger templates vs clean templates

## Open Questions the Paper Calls Out
1. How does transferability vary across different language model architectures when using tone-based templates?
2. What is the minimum number of poisoned samples required in pre-training phase for successful attack across task complexities?
3. How do tone-based backdoor attacks perform against sophisticated defense mechanisms compared to traditional template-based attacks?

## Limitations
- Core transferability claims rely on untested assumptions about tone pattern generalization
- GPT4 template generation process lacks transparency in exact prompts and quality evaluation criteria
- Experimental validation limited to five datasets and three BERT variants, not exploring cross-model or cross-lingual scenarios

## Confidence
- High Confidence: Direct attack mechanism and dual-loss training approach are well-established in backdoor literature
- Medium Confidence: Attack success rates and stealth metrics follow standard NLP evaluation practices with reasonable baseline comparisons
- Low Confidence: Transferability claims depend heavily on tone-similarity hypothesis without direct empirical validation or controlled ablation studies

## Next Checks
1. Conduct transferability ablation study varying linguistic similarity between pre-training trigger templates and downstream attack templates
2. Replicate template generation using exact GPT4 prompts and compare GPT4-generated templates against human-designed baselines
3. Test cross-model transferability by attacking models pre-trained with TARGET templates using trigger templates designed for different model architectures