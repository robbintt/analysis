---
ver: rpa2
title: 'On permutation symmetries in Bayesian neural network posteriors: a variational
  perspective'
arxiv_id: '2310.10171'
source_url: https://arxiv.org/abs/2310.10171
tags:
- neural
- learning
- solutions
- bayesian
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the analysis of loss landscape geometry and
  mode connectivity to Bayesian neural networks (BNNs), focusing on the role of permutation
  symmetries. It proposes a method to align the distributions of two independent variational
  inference (VI) solutions by finding permutation matrices that minimize the Wasserstein
  distance between them.
---

# On permutation symmetries in Bayesian neural network posteriors: a variational perspective

## Quick Facts
- arXiv ID: 2310.10171
- Source URL: https://arxiv.org/abs/2310.10171
- Reference count: 40
- Key outcome: This paper extends the analysis of loss landscape geometry and mode connectivity to Bayesian neural networks (BNNs), focusing on the role of permutation symmetries. It proposes a method to align the distributions of two independent variational inference (VI) solutions by finding permutation matrices that minimize the Wasserstein distance between them. The method reframes the problem as a sum of bilinear assignment problems and uses a greedy coordinate descent algorithm to solve it. Experiments on various architectures and datasets, including MLPs and ResNet20 on MNIST, Fashion-MNIST, and CIFAR10, show that after alignment, the loss barriers between VI solutions are nearly zero, indicating linear connectivity. The results hold for different prior variances and temperatures, with wider models exhibiting lower barriers. However, the method fails to find zero-barrier solutions for CIFAR100.

## Executive Summary
This paper investigates the linear connectivity of approximate Bayesian neural network solutions by accounting for permutation symmetries in weight matrices and biases. The authors propose a matching algorithm to align the distributions of two independent variational inference solutions using permutation matrices that minimize the Wasserstein distance between them. By reframing the problem as a sum of bilinear assignment problems and solving it with a greedy coordinate descent algorithm, they demonstrate that the loss barriers between aligned VI solutions are nearly zero for various architectures and datasets, indicating linear connectivity. The results hold for different prior variances and temperatures, with wider models exhibiting lower barriers.

## Method Summary
The paper extends the analysis of loss landscape geometry and mode connectivity to Bayesian neural networks (BNNs), focusing on the role of permutation symmetries. It proposes a method to align the distributions of two independent variational inference (VI) solutions by finding permutation matrices that minimize the Wasserstein distance between them. The method reframes the problem as a sum of bilinear assignment problems and uses a greedy coordinate descent algorithm to solve it. Experiments on various architectures and datasets, including MLPs and ResNet20 on MNIST, Fashion-MNIST, and CIFAR10, show that after alignment, the loss barriers between VI solutions are nearly zero, indicating linear connectivity. The results hold for different prior variances and temperatures, with wider models exhibiting lower barriers. However, the method fails to find zero-barrier solutions for CIFAR100.

## Key Results
- The proposed permutation alignment method successfully finds zero-barrier solutions for MLPs on MNIST and CIFAR10, and nearly-zero barrier for ResNet20 on CIFAR10.
- Wider models generally exhibit lower loss barriers after alignment, both for MLPs and ResNet20 architectures.
- The method fails to find zero-barrier solutions for CIFAR100, indicating that the landscape complexity can exceed the alignment algorithm's capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variational inference solutions for Bayesian neural networks are linearly connected after accounting for permutation symmetries.
- Mechanism: The paper proposes aligning two independent VI solutions by finding permutation matrices that minimize the Wasserstein distance between their parameter distributions. This alignment transforms the problem into a sum of bilinear assignment problems, solvable via a greedy coordinate descent algorithm.
- Core assumption: The weight-space posteriors of two independently trained VI models are multimodal due to permutation symmetries, and there exists a permutation of one solution that places it in a linearly connected region with the other.
- Evidence anchors:
  - [abstract] "We first extend the formalism of marginalized loss barrier and solution interpolation to BNNs, before proposing a matching algorithm to search for linearly connected solutions. This is achieved by aligning the distributions of two independent approximate Bayesian solutions with respect to permutation matrices."
  - [section 4] "For the variational inference setting, propose a matching algorithm to search for linearly connected solutions by aligning the distributions of two independent solutions with respect to permutation matrices."
  - [corpus] Weak; no direct evidence from related papers, but topic alignment suggests relevance.
- Break condition: The alignment method fails to find zero-barrier solutions, as observed on CIFAR100, indicating that either the permutation map doesn't exist or the algorithm cannot find it.

### Mechanism 2
- Claim: The loss barriers between aligned VI solutions are nearly zero, indicating linear connectivity.
- Mechanism: After aligning the distributions using permutation matrices, the interpolated solutions along Wasserstein geodesics exhibit negligible loss barriers, as measured by the predictive likelihood.
- Core assumption: The predictive likelihood is a valid proxy for the true posterior loss barrier, and the Wasserstein geodesic interpolation accurately captures the loss landscape geometry.
- Evidence anchors:
  - [section 5.1] "With the alignment proposed in ยง 4 and Algorithm 1, we recover zero barrier solutions for MLPs on both MNIST and CIFAR10, and nearly-zero barrier for ResNet20 on CIFAR10."
  - [section 4] "We build on the results of Ainsworth et al. (2023), reframing the problem as a combinatorial optimization one, using an approximation to the sum of bilinear assignment problem."
  - [corpus] Weak; no direct evidence from related papers, but topic alignment suggests relevance.
- Break condition: The loss barriers remain significant even after alignment, as seen on CIFAR100, suggesting that the landscape geometry is more complex than assumed.

### Mechanism 3
- Claim: Wider models exhibit lower loss barriers after alignment.
- Mechanism: The width of the neural network affects the geometry of the loss landscape, with wider models having more symmetric and connected regions, leading to lower barriers after alignment.
- Core assumption: The width of the network is directly related to the complexity of the loss landscape and the ease of finding linearly connected solutions.
- Evidence anchors:
  - [section 5.1] "We see that wider models generally provide lower barriers: for MLPs this holds true with and without alignment, while for the ResNet20 this is happening only after alignment."
  - [section 4.2] "This optimization problem is more challenging than the one presented in Eq. (18): we are interested in finding permutation matrices to be applied concurrently to rows and columns of both means and standard deviations."
  - [corpus] Weak; no direct evidence from related papers, but topic alignment suggests relevance.
- Break condition: The loss barriers do not decrease with width, or the alignment method fails to find zero-barrier solutions even for wide models.

## Foundational Learning

- Concept: Permutation symmetries in neural networks
  - Why needed here: Permutation symmetries allow for functionally equivalent solutions in weight space, which is crucial for understanding the loss landscape geometry and finding linearly connected solutions.
  - Quick check question: Can you explain how permuting the rows and columns of weight matrices in a neural network results in a functionally equivalent model?

- Concept: Variational inference for Bayesian neural networks
  - Why needed here: Variational inference is used to approximate the posterior distribution over the parameters of the neural network, which is necessary for computing the loss barriers and finding linearly connected solutions.
  - Quick check question: What is the evidence lower bound (ELBO) in variational inference, and how is it used to optimize the approximate posterior?

- Concept: Wasserstein geodesics and loss barriers
  - Why needed here: Wasserstein geodesics provide a way to interpolate between two probability distributions in a geometrically meaningful way, which is used to measure the loss barriers between aligned VI solutions.
  - Quick check question: How does the Wasserstein distance differ from other measures of distance between probability distributions, and why is it suitable for this application?

## Architecture Onboarding

- Component map:
  - Variational inference module -> Permutation alignment module -> Loss barrier computation module -> Experimental evaluation module

- Critical path:
  1. Train two independent VI models on the same dataset.
  2. Extract the parameter distributions from the trained models.
  3. Use the permutation alignment module to find the permutation matrices that minimize the Wasserstein distance between the distributions.
  4. Interpolate the aligned distributions along Wasserstein geodesics.
  5. Compute the loss barriers using the predictive likelihood.
  6. Analyze the results and draw conclusions.

- Design tradeoffs:
  - Using the Wasserstein distance vs. other measures of distance between distributions for alignment.
  - Solving the sum of bilinear assignment problems exactly vs. using a greedy coordinate descent algorithm.
  - Computing the loss barriers exactly vs. using Monte Carlo approximations.

- Failure signatures:
  - The alignment method fails to find zero-barrier solutions, indicating that either the permutation map doesn't exist or the algorithm cannot find it.
  - The loss barriers remain significant even after alignment, suggesting that the landscape geometry is more complex than assumed.
  - The method fails to generalize to different architectures or datasets, indicating that the proposed approach may not be universally applicable.

- First 3 experiments:
  1. Train two independent VI models on a simple dataset (e.g., MNIST) using a small MLP architecture. Extract the parameter distributions and use the permutation alignment module to find the permutation matrices. Interpolate the aligned distributions and compute the loss barriers.
  2. Repeat the first experiment with a larger MLP architecture to study the effect of width on the loss barriers.
  3. Train two independent VI models on a more complex dataset (e.g., CIFAR10) using a ResNet architecture. Extract the parameter distributions and use the permutation alignment module to find the permutation matrices. Interpolate the aligned distributions and compute the loss barriers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do the proposed permutation alignment methods fail to find zero-barrier solutions for CIFAR100, despite working well on other datasets?
- Basis in paper: [explicit] The paper reports that the method fails to find zero-barrier solutions for CIFAR100, contrasting with its success on other datasets.
- Why unresolved: The paper does not provide a clear explanation for this discrepancy, leaving the underlying reasons unclear.
- What evidence would resolve it: Experiments testing the method on CIFAR100 with different model architectures, widths, or prior variances could provide insights into the specific factors causing the failure.

### Open Question 2
- Question: How does the width of a neural network affect the linear connectivity of Bayesian solutions after permutation alignment?
- Basis in paper: [explicit] The paper observes that wider models exhibit lower loss barriers after alignment, but does not fully explain the underlying reasons.
- Why unresolved: The relationship between width and linear connectivity is complex and may involve factors such as the limiting behavior of non-parametric models like Gaussian Processes and Deep Gaussian Processes.
- What evidence would resolve it: Further experiments comparing the linear connectivity of Bayesian solutions for networks of varying widths, combined with theoretical analysis of the posterior landscape geometry, could provide insights into this relationship.

### Open Question 3
- Question: What is the role of permutation symmetries in the generalization performance of Bayesian neural networks?
- Basis in paper: [inferred] The paper focuses on the linear connectivity of Bayesian solutions after permutation alignment, but does not explicitly discuss the implications for generalization.
- Why unresolved: The connection between linear connectivity and generalization is not straightforward and may depend on various factors such as the data distribution, model architecture, and prior choice.
- What evidence would resolve it: Experiments comparing the generalization performance of Bayesian neural networks with and without permutation alignment, along with theoretical analysis of the posterior geometry, could shed light on this relationship.

## Limitations
- The analysis is limited to permutation symmetries in fully connected layers, and the proposed alignment method may not generalize to other types of symmetries or architectures.
- The use of predictive likelihood as a proxy for the true posterior loss barrier may not capture all aspects of the loss landscape geometry.
- The method's failure on CIFAR100 suggests that the landscape complexity can exceed the alignment algorithm's capabilities.

## Confidence
- High confidence in the proposed alignment method's effectiveness for simple architectures (MLPs) on standard datasets (MNIST, Fashion-MNIST, CIFAR10).
- Medium confidence in the method's applicability to deeper architectures (ResNet20) and its ability to find zero-barrier solutions.
- Low confidence in the method's generalizability to more complex datasets (CIFAR100) and its ability to handle more intricate loss landscape geometries.

## Next Checks
1. Evaluate the alignment method's performance on a wider range of architectures, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to assess its generalizability beyond MLPs and ResNet20.
2. Investigate the impact of different priors and temperature values on the alignment process and the resulting loss barriers to understand the method's sensitivity to hyperparameter choices.
3. Develop and test alternative alignment methods that can handle more complex symmetries, such as scale and sign symmetries, to improve the method's robustness and applicability to a broader range of problems.