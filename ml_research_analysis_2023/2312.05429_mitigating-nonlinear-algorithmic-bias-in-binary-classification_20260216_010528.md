---
ver: rpa2
title: Mitigating Nonlinear Algorithmic Bias in Binary Classification
arxiv_id: '2312.05429'
source_url: https://arxiv.org/abs/2312.05429
tags:
- risk
- bias
- causal
- data
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses algorithmic bias that is nonlinear in a protected
  attribute, specifically age, within binary classification. The proposed method uses
  causal modeling to detect and mitigate this bias by treating the trained prediction
  model as a black box.
---

# Mitigating Nonlinear Algorithmic Bias in Binary Classification

## Quick Facts
- arXiv ID: 2312.05429
- Source URL: https://arxiv.org/abs/2312.05429
- Reference count: 22
- Primary result: Proposed causal modeling approach mitigates nonlinear age bias in credit risk classification while maintaining accuracy and improving equal opportunity across age groups.

## Executive Summary
This paper presents a novel approach to mitigating nonlinear algorithmic bias in binary classification using causal modeling. The method treats the trained prediction model as a black box and incorporates nonlinearity through quadratic terms in the protected attribute (age). Using the German Credit dataset, the authors demonstrate that their approach can detect and correct age-based bias patterns that linear methods miss, resulting in more equitable outcomes across age groups while maintaining classification accuracy.

## Method Summary
The approach involves two main stages: first, training a logistic regression model on the German Credit dataset to generate probability estimates for credit risk classification. Second, fitting a causal model with linear and quadratic age terms to quantify and remove direct age-based bias from predictions. The causal model treats the black-box classifier's outputs as endogenous variables and estimates path coefficients to identify and eliminate direct age-to-prediction relationships. De-biased probability estimates are then generated by setting these direct paths to zero, creating counterfactual predictions that remove age-based advantage.

## Key Results
- Nonlinear bias patterns show probability of correct "low risk" classification increases with age, lowest among young people (19-37) at 69.8% compared to 81.1% for 57-75 age group
- Causal de-biasing successfully equalizes opportunity across age groups while reducing overall accuracy by only 0.2-0.5%
- Method demonstrates robustness by achieving similar fairness improvements on both training and test data sets
- Interpretability of causal model enhances explicability and trust among AI stakeholders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating a quadratic term for age captures nonlinear bias patterns that linear models miss
- Mechanism: Path diagram allows both direct (age → prediction) and mediated (age → target → prediction) causal pathways. Age² term represents observed nonlinear increase in correct classification probability with age
- Core assumption: Observed nonlinearity is well-approximated by quadratic function over age range
- Evidence anchors: [abstract] causal model incorporates nonlinearity through quadratic term in age, [section] introduce higher order polynomial term
- Break condition: True bias curve is higher-order or non-polynomial, causing mis-specification

### Mechanism 2
- Claim: Treating trained model as black box enables post-hoc fairness correction without retraining
- Mechanism: Path diagram only uses black-box predictions and target as endogenous variables; classifier internals are irrelevant
- Core assumption: Black-box predictions are calibrated probability estimates or can be calibrated
- Evidence anchors: [abstract] proposed method uses causal modeling treating trained model as black box, [section] trained model treated as black box
- Break condition: Prediction model outputs uncalibrated scores, breaking causal model assumptions

### Mechanism 3
- Claim: Setting direct path from age to prediction to zero creates counterfactual fairness
- Mechanism: Fitted causal model shows β_age→pred ≈ 0 implies age should only affect predictions through target variable
- Core assumption: Causal model correctly identifies and isolates direct age→prediction path as sole source of algorithmic bias
- Evidence anchors: [section] absence of algorithmic bias means no direct relationship between age and prediction, [section] de-biased prediction computed by setting coefficients to zero
- Break condition: Other unobserved confounders create direct paths to prediction

## Foundational Learning

- Concept: Path analysis in structural equation modeling
  - Why needed here: Causal model specified and estimated using path analysis; understanding direct and indirect effects essential for interpreting β coefficients
  - Quick check question: In path diagram A→Y→Z, which coefficient represents direct effect of A on Z?

- Concept: Calibration of probability estimates
  - Why needed here: Black-box classifier outputs must be well-calibrated so causal de-biasing on probabilities is meaningful and thresholds remain interpretable
  - Quick check question: What does reliability diagram (calibration curve) show about classifier's probability estimates?

- Concept: Counterfactual fairness
  - Why needed here: De-biasing step assumes predictions in world where protected attribute is neutralized; understanding counterfactual reasoning clarifies why setting direct paths to zero achieves fairness
  - Quick check question: In counterfactual fairness, what does "same decision in alternate demographic world" mean for individual A?

## Architecture Onboarding

- Component map: Data pipeline → Black-box classifier → Probability calibration → Causal model fitting → De-biased probability generation → Classification with new threshold
- Critical path: Prediction model training → Calibration curve verification → Causal model specification (linear + quadratic) → Path analysis estimation → De-biased probability computation → Threshold adjustment → Fairness evaluation
- Design tradeoffs:
  - Higher-order polynomial terms increase flexibility but risk overfitting; discretization is simpler but loses granularity
  - Black-box treatment eases integration but requires calibrated outputs; transparency requires model internals
  - Post-processing avoids retraining but cannot correct bias already embedded in training data distributions
- Failure signatures:
  - Calibration curve deviates significantly from diagonal → biased probabilities → invalid causal de-biasing
  - Non-significant β_age terms but visible fairness gaps → model misspecification or confounding
  - Large accuracy drop after de-biasing → over-correction or threshold mis-specification
- First 3 experiments:
  1. Fit logistic regression on training set, plot calibration curve, verify probability reliability
  2. Fit causal model with age and age² terms, check significance of direct age→prediction path, compute de-biased probabilities
  3. Apply de-biased model to test set, recompute classification accuracy and equal opportunity across age groups, compare to baseline

## Open Questions the Paper Calls Out

- Question: How does performance of proposed causal modeling approach compare to other bias mitigation techniques like pre-processing or in-processing methods when handling nonlinear algorithmic bias?
  - Basis in paper: [inferred] Paper focuses on post-processing and compares it to pre-processing and in-processing but does not compare proposed causal modeling approach to these methods
  - Why unresolved: Paper does not provide direct comparison between proposed method and other bias mitigation techniques
  - What evidence would resolve it: Empirical results comparing proposed causal modeling approach to other bias mitigation techniques on same dataset

- Question: What are limitations of using higher order polynomial terms in causal model to capture nonlinearity, and how can these limitations be addressed?
  - Basis in paper: [explicit] Paper mentions there may be more effective ways to handle nonlinearity and suggests including higher order polynomial terms, but does not explore these limitations
  - Why unresolved: Paper does not provide detailed analysis of limitations of using higher order polynomial terms
  - What evidence would resolve it: Study exploring limitations of higher order polynomial terms in causal models and proposing alternative methods

- Question: How does proposed method perform on datasets with different characteristics, such as larger sample sizes or different distributions of protected attributes?
  - Basis in paper: [inferred] Paper uses German Credit dataset with specific sample size and distribution of protected attributes, but does not explore performance on other datasets
  - Why unresolved: Paper does not provide empirical results on datasets with different characteristics
  - What evidence would resolve it: Empirical results on multiple datasets with varying sample sizes and distributions of protected attributes

## Limitations
- Causal de-biasing effectiveness depends critically on correctly specifying nonlinearity (quadratic form) and assumes no other unobserved confounders affect predictions
- Paper provides limited validation of calibration curves and no external datasets were tested
- Quadratic approximation may not generalize to populations with different age distributions

## Confidence
- Mechanism 1 (Quadratic nonlinearity): Medium confidence - supported by path analysis theory but no empirical comparison to higher-order terms
- Mechanism 2 (Black-box treatment): Medium confidence - theoretically sound but no sensitivity analysis for uncalibrated outputs
- Mechanism 3 (Path removal for fairness): Medium confidence - mathematically valid but assumes correctly specified causal structure

## Next Checks
1. Test method on alternative datasets with different age distributions and bias patterns to assess generalizability
2. Compare quadratic specification against cubic and spline-based approaches to verify optimal functional form
3. Conduct sensitivity analysis by deliberately introducing miscalibration to evaluate robustness of de-biased estimates