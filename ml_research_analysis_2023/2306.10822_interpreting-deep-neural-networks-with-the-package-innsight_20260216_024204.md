---
ver: rpa2
title: Interpreting Deep Neural Networks with the Package innsight
arxiv_id: '2306.10822'
source_url: https://arxiv.org/abs/2306.10822
tags:
- input
- data
- output
- rule
- innsight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The innsight package provides the first R implementation of feature
  attribution methods for deep neural networks, offering a unified and flexible framework
  to interpret model predictions. It supports models from various R packages (keras,
  torch, neuralnet) and custom models, leveraging efficient torch computations.
---

# Interpreting Deep Neural Networks with the Package innsight

## Quick Facts
- arXiv ID: 2306.10822
- Source URL: https://arxiv.org/abs/2306.10822
- Reference count: 36
- The innsight package provides the first R implementation of feature attribution methods for deep neural networks

## Executive Summary
The innsight package addresses the gap in R for interpreting deep neural network predictions by implementing feature attribution methods including Gradient, SmoothGrad, Layer-wise Relevance Propagation (LRP), and DeepLift. The package operates independently of deep learning libraries, supporting models from keras, torch, neuralnet, and custom architectures through a unified conversion framework. By leveraging torch's LibTorch backend, innsight achieves computational efficiency while maintaining R-native workflow without Python dependencies. The package provides both local instance-wise explanations and global aggregated insights with interactive visualization capabilities for tabular, signal, and image data.

## Method Summary
The innsight package uses a three-step approach: convert the trained model to a torch-based representation, apply the selected interpretation method, and visualize the results. The Converter class analyzes any passed model and creates a torch-based replication with pre-implemented interpretation methods for each valid layer type. The package implements eight feature attribution methods through a unified InterpretingMethod super class, with specific classes for Gradient, SmoothGrad, Gradient×Input, DeepLift, and various LRP variants. Results are visualized using built-in ggplot2 and plotly integration, supporting complex scenarios with multiple input and output layers through facetted plots.

## Key Results
- Numerical validation against Python implementations shows near-identical results with mean absolute errors below 10^-6
- Runtime comparisons reveal innsight is generally faster than Keras-based implementations but slower than PyTorch-based ones for large image inputs
- The package successfully handles models with multiple input and output layers, demonstrated through real-world examples using the penguins and melanoma datasets
- LRP with α-β-rule and DeepLift methods show some numerical inaccuracies with hyperbolic tangent activations, though remaining below acceptable error thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: innsight leverages torch's LibTorch backend to achieve computational efficiency without requiring Python dependencies.
- Mechanism: The package internally uses the torch R package, which builds directly on LibTorch (PyTorch's C++ backend), enabling fast array calculations while maintaining R-native workflow.
- Core assumption: Torch computations in R can match PyTorch's performance when backed by the same C++ library.
- Evidence anchors:
  - [abstract]: "benefits internally from the torch package's fast and efficient array calculations, which builds on LibTorch – PyTorch's C++ backend – without a Python dependency"
  - [section 3]: "the package uses the R package torch (Falbel and Luraschi 2023), which builds on LibTorch"
  - [corpus]: Found related work on torch-based R packages, supporting this approach.
- Break condition: If torch R package fails to properly interface with LibTorch or introduces significant overhead compared to native PyTorch.

### Mechanism 2
- Claim: innsight achieves deep-learning-library-agnostic interpretation through a unified model conversion framework.
- Mechanism: The Converter class analyzes any passed model (keras, torch, neuralnet, or custom list) and creates a torch-based replication with pre-implemented interpretation methods for each valid layer type.
- Core assumption: Model architectures can be uniformly represented as lists of layers with weights, biases, and activation functions regardless of original library.
- Evidence anchors:
  - [abstract]: "operates independently of the deep learning library allowing the interpretation of models from any R package, including keras, torch, neuralnet, and even custom models"
  - [section 3.1]: "The passed trained models are not limited to a specific deep learning library"
  - [corpus]: Found packages implementing similar model conversion approaches.
- Break condition: If model architectures contain features that cannot be represented in the unified list structure or if conversion introduces errors.

### Mechanism 3
- Claim: innsight provides consistent results with Python implementations through numerical precision management.
- Mechanism: The package uses single-precision floating point calculations (IEEE 754 standard) and implements attribution methods following established mathematical formulations, achieving mean absolute errors within 10^-6 of reference implementations.
- Core assumption: Single-precision calculations are sufficient for feature attribution methods and numerical discrepancies are primarily due to floating point representation.
- Evidence anchors:
  - [section 5.1]: "all outliers with an error exceeding 10^-6 originate from models with the hyperbolic tangent as activation"
  - [section 5.1]: "the results from innsight compared to captum or zennit for the simple, ε-rule and α-β-rule differ negligibly and are far below the maximally tolerated error of 10^-6"
  - [corpus]: Found studies validating numerical precision in deep learning implementations.
- Break condition: If attribution methods require higher precision than single-precision floating point or if numerical instabilities exceed acceptable thresholds.

## Foundational Learning

- Concept: Feature attribution methods
  - Why needed here: The paper's core contribution is implementing these methods for R users, so understanding what they are and how they work is fundamental.
  - Quick check question: What is the key difference between local and global interpretability methods in the context of neural networks?

- Concept: R6 class system
  - Why needed here: innsight uses R6 extensively for its Converter and method classes, so understanding reference semantics and object-oriented design in R is crucial.
  - Quick check question: How does the R6 class system differ from S3/S4 in terms of object reference and method inheritance?

- Concept: Torch backend integration
  - Why needed here: The package's performance and compatibility depend on proper torch integration, requiring understanding of how R interfaces with C++ libraries.
  - Quick check question: What is the relationship between torch R package, LibTorch, and PyTorch, and how does this enable CPU/GPU computation?

## Architecture Onboarding

- Component map:
  - Converter class: Model analysis and torch-based replication
  - InterpretingMethod super class: Unified framework for all attribution methods
  - Specific method classes (Gradient, LRP, DeepLift, etc.): Implementation of individual techniques
  - Visualization classes (innsight_ggplot2, innsight_plotly): Plotting infrastructure
  - Core dependencies: torch, ggplot2, plotly, checkmate

- Critical path: Model → Converter → Method class → Results → Visualization
- Design tradeoffs:
  - Flexibility vs performance: Supporting multiple model types requires conversion overhead
  - R-native vs Python compatibility: Avoiding Python dependency limits some PyTorch features
  - Single-precision vs double-precision: Balancing speed with numerical accuracy
- Failure signatures:
  - Conversion errors: Model architecture cannot be represented in unified list structure
  - Numerical instability: Results show large errors or NaNs, especially with activation functions
  - Performance degradation: Runtime much slower than expected for large models or datasets
- First 3 experiments:
  1. Convert a simple neuralnet model and verify the Converter produces expected layer structure
  2. Apply Gradient×Input method to a small torch model and compare results with manual gradient calculation
  3. Test visualization with a model having multiple input layers (image + tabular) to verify facet handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does innsight's performance compare to PyTorch-based packages for large-scale image interpretation tasks with DeepLift?
- Basis in paper: [explicit] The paper states that innsight is slower than captum and zennit for larger image sizes in the DeepLift method.
- Why unresolved: The paper only provides runtime comparisons for a limited set of image sizes and does not explore the performance impact at truly large scales.
- What evidence would resolve it: Comprehensive runtime benchmarks of innsight vs. PyTorch-based packages for image interpretation tasks with DeepLift on datasets with images significantly larger than 404x404 pixels.

### Open Question 2
- Question: How do the numerical inaccuracies observed in innsight's results for saturated hyperbolic tangent activations affect the interpretability of the explanations?
- Basis in paper: [explicit] The paper mentions that 96.8% of cases with errors exceeding 10^-6 for DeepLift and LRP are caused by saturated hyperbolic tangent activations.
- Why unresolved: The paper does not explore the practical impact of these numerical inaccuracies on the interpretability of the explanations or provide guidance on when these inaccuracies become problematic.
- What evidence would resolve it: A study investigating the correlation between the magnitude of numerical inaccuracies and the quality of interpretability for saturated hyperbolic tangent activations, along with guidelines for when these inaccuracies are negligible.

### Open Question 3
- Question: How does the computational efficiency of innsight compare to other packages when interpreting models with complex architectures, such as those with skip connections or attention mechanisms?
- Basis in paper: [inferred] The paper focuses on comparing innsight's performance on basic dense and convolutional architectures and does not explore more complex model architectures.
- Why unresolved: The paper does not provide runtime comparisons for models with skip connections, attention mechanisms, or other complex architectural components.
- What evidence would resolve it: Comprehensive runtime benchmarks of innsight vs. other packages for interpreting models with various complex architectures, including those with skip connections, attention mechanisms, and other advanced components.

## Limitations
- Numerical inaccuracies occur with hyperbolic tangent activations, though remaining below acceptable error thresholds
- Runtime performance is slower than PyTorch-based implementations for large image datasets with DeepLift
- Model conversion framework may encounter difficulties with custom or complex layer types not represented in the unified list structure

## Confidence
- Numerical accuracy claims: High
- Runtime performance comparisons: Medium
- Model conversion reliability: Medium
- Visualization capabilities: High

## Next Checks
1. Test innsight with models containing recurrent layers and attention mechanisms to verify conversion framework handles sequential architectures
2. Evaluate numerical stability with double-precision calculations on challenging activation functions (tanh, sigmoid) to quantify precision-related discrepancies
3. Benchmark performance on multi-modal models combining image and tabular data with varying batch sizes to identify scaling bottlenecks