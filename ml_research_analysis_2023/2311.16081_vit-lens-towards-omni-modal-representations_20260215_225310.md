---
ver: rpa2
title: 'ViT-Lens: Towards Omni-modal Representations'
arxiv_id: '2311.16081'
source_url: https://arxiv.org/abs/2311.16081
tags:
- vit-l
- image
- modalities
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VIT-Lens-2, a method for learning omni-modal
  representations by leveraging pretrained Vision Transformers (ViTs) to process diverse
  data types like point clouds, depth maps, audio, tactile, and EEG. The key innovation
  is a modality-specific lens that projects raw data into an intermediate embedding
  space, which is then processed by frozen pretrained ViT layers and aligned with
  a modal-independent space defined by foundation models like CLIP.
---

# ViT-Lens: Towards Omni-modal Representations

## Quick Facts
- arXiv ID: 2311.16081
- Source URL: https://arxiv.org/abs/2311.16081
- Reference count: 40
- One-line primary result: VIT-Lens-2 achieves state-of-the-art zero-shot classification on 3D point clouds (50.1% accuracy on Objaverse-LVIS) and emergent multimodal capabilities through integration with foundation models.

## Executive Summary
This paper presents VIT-Lens-2, a method for learning omni-modal representations by leveraging pretrained Vision Transformers (ViTs) to process diverse data types like point clouds, depth maps, audio, tactile, and EEG. The key innovation is a modality-specific lens that projects raw data into an intermediate embedding space, which is then processed by frozen pretrained ViT layers and aligned with a modal-independent space defined by foundation models like CLIP. VIT-Lens-2 achieves state-of-the-art performance across various understanding tasks, including zero-shot classification on 3D point clouds (50.1% accuracy on Objaverse-LVIS, surpassing prior methods by 11.0%), depth-only scene classification, and audio classification. By integrating VIT-Lens-2 with Multimodal Foundation Models like InstructBLIP and SEED, the method enables emergent capabilities such as any-modality captioning, QA, and image generation without additional instruction tuning.

## Method Summary
VIT-Lens-2 learns omni-modal representations by using a modality-specific lens to project raw data from diverse modalities (point clouds, depth, audio, tactile, EEG) into intermediate embeddings compatible with pretrained ViT layers. The method employs cross-modal contrastive learning to align these representations with foundation model embeddings (e.g., CLIP). The modality encoder, consisting of a modality embedding module, lens, and frozen ViT, can be integrated into multimodal foundation models to enable emergent capabilities like any-modality captioning and QA. The approach requires minimal training data compared to traditional methods and scales effectively with larger foundation models and ViT architectures.

## Key Results
- Achieves 50.1% zero-shot classification accuracy on Objaverse-LVIS 3D point clouds, surpassing prior methods by 11.0%
- Demonstrates emergent capabilities (captioning, QA, image generation) when integrated with MFMs like InstructBLIP and SEED
- Shows data efficiency by requiring minimal training data compared to traditional supervised methods

## Why This Works (Mechanism)

### Mechanism 1
The modality-specific lens projects raw signals into an intermediate embedding space that is compatible with pretrained ViT layers, enabling the ViT to process non-image modalities effectively. Raw data from diverse modalities often lacks the structural format expected by ViT. The modality embedding module tokenizes the raw signal, and the lens maps these embeddings into a latent space resembling ViT input patches. This projection preserves modality-specific semantics while aligning with ViT's learned representations.

### Mechanism 2
Cross-modal alignment via foundation models (e.g., CLIP) supervises the modality encoder to produce representations that are semantically consistent across modalities. Anchor modalities (image, text) are encoded by a frozen foundation model to produce reference features. The modality encoder is trained to align its output with these reference features using contrastive loss, ensuring that representations from different modalities of the same concept are close in the joint embedding space.

### Mechanism 3
By integrating VIT-LENS-2 into multimodal foundation models (MFMs), the emergent capabilities of the MFM are extended to novel modalities without additional instruction tuning. The modality encoder trained with VIT-LENS-2 produces features compatible with the MFM's existing vision encoder (e.g., EVI0I-g14 CLIP-ViT). Plugging in the modality-specific lens and embedding module before the frozen ViT allows the MFM to process inputs from any modality, leveraging its pretrained instruction-following capabilities.

## Foundational Learning

- **Concept**: Cross-modal contrastive learning
  - **Why needed here**: Aligns representations from different modalities in a shared embedding space, enabling zero-shot generalization across modalities.
  - **Quick check question**: Can you explain how the contrastive loss ensures that features from the same concept across modalities are closer than features from different concepts?

- **Concept**: Vision Transformer (ViT) architecture and pretraining
  - **Why needed here**: VIT-LENS-2 leverages the rich visual knowledge in pretrained ViT layers to process non-image modalities after projection into an intermediate embedding space.
  - **Quick check question**: What are the key design choices in ViT (e.g., patch embedding, self-attention) that make it suitable for transfer to novel modalities?

- **Concept**: Modality-specific tokenization and embedding
  - **Why needed here**: Different modalities have distinct raw signal structures (e.g., point clouds vs. audio spectrograms) that must be converted into a format compatible with the ViT input space.
  - **Quick check question**: How does the modality embedding module for audio (e.g., log mel spectrogram + patch embedding) differ from that for 3D point clouds (e.g., FPS + kNN grouping + PointNet)?

## Architecture Onboarding

- **Component map**: Raw modality data → Modality embedding → Lens projection → Pretrained ViT encoding → Cross-modal alignment → Downstream task or MFM integration

- **Critical path**: Raw modality data → Modality embedding → Lens projection → Pretrained ViT encoding → Cross-modal alignment → Downstream task or MFM integration

- **Design tradeoffs**:
  - Freezing pretrained ViT layers vs. fine-tuning: Freezing reduces parameters and computation but may limit modality-specific adaptation.
  - Choice of lens architecture (S-Attn vs. Iter-CS-Attn): S-Attn is simpler for image-like inputs, while Iter-CS-Attn handles variable-length inputs more efficiently.
  - Anchor modalities for alignment: Using both image and text as anchors generally improves performance, but text-only alignment may be necessary for certain modalities (e.g., tactile).

- **Failure signatures**:
  - Poor performance on novel modalities: Could indicate issues with the lens projection or ViT generalization.
  - Misalignment in the joint embedding space: Could indicate problems with the contrastive loss or foundation model.
  - Incompatibility with MFM integration: Could indicate issues with feature scaling or abstractor module expectations.

- **First 3 experiments**:
  1. Ablation study: Train VIT-LENS-2 with and without the lens component to quantify its impact on modality representation quality.
  2. Modality encoder design: Compare different lens architectures (S-Attn vs. Iter-CS-Attn) on a held-out modality to determine the most effective design.
  3. MFM integration: Plug VIT-LENS-2 into a simple MFM (e.g., InstructBLIP) and evaluate emergent capabilities on a few-shot classification task for a novel modality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well can ViT-Lens generalize to completely unseen modalities beyond the five tested (3D point clouds, depth, audio, tactile, and EEG)?
- Basis in paper: The paper states they tested five modalities but mentions "extending beyond images and videos to encompass 3D point cloud, depth, audio, tactile, and EEG" and "a spectrum of modalities" in the abstract, suggesting potential for more.
- Why unresolved: The paper only evaluates five specific modalities, leaving open the question of performance on other types of data like thermal, hyperspectral, or medical imaging.
- What evidence would resolve it: Testing ViT-Lens on additional diverse modalities and comparing performance to modality-specific models or other general-purpose approaches.

### Open Question 2
- Question: What is the optimal balance between the number of trainable parameters in the Lens component versus the frozen pretrained-ViT layers for different modalities?
- Basis in paper: The ablation study explores different Lens architectures and their impact on performance, but doesn't systematically vary the parameter ratio between Lens and ViT.
- Why unresolved: The paper shows that different modalities benefit from different Lens designs, but doesn't provide a clear guideline for allocating parameters between the two components.
- What evidence would resolve it: Systematic ablation studies varying the parameter ratio for each modality and analyzing the trade-off between performance and computational cost.

### Open Question 3
- Question: How does the performance of ViT-Lens scale with the size and quality of the foundation model used for alignment?
- Basis in paper: The paper mentions scaling up the foundation model and ViT-Lens improves performance, but doesn't explore the relationship in detail.
- Why unresolved: While the paper shows that larger models help, it doesn't quantify how much improvement comes from scaling the foundation model versus scaling ViT-Lens itself.
- What evidence would resolve it: Controlled experiments scaling the foundation model and ViT-Lens independently and measuring their relative contributions to performance gains.

## Limitations

- Performance heavily dependent on quality of pretrained foundation models and their embedding spaces
- May not match fully supervised methods when abundant labeled data is available for specific modalities
- Modality-specific lens design requires architectural tuning rather than a universal solution

## Confidence

- **High confidence**: The core claim that VIT-LENS-2 achieves state-of-the-art zero-shot classification performance across multiple modalities is well-supported by extensive quantitative results (50.1% accuracy on Objaverse-LVIS, +11.0% over prior methods).
- **Medium confidence**: The emergent capability claims for any-modality captioning, QA, and image generation are demonstrated qualitatively but lack comprehensive quantitative evaluation.
- **Low confidence**: The paper claims data efficiency compared to traditional methods, but direct comparisons with other data-efficient approaches are limited.

## Next Checks

1. **Robustness Testing**: Evaluate VIT-LENS-2's performance across varying data quality levels and domain shifts for each modality to assess real-world applicability beyond controlled benchmark datasets.

2. **MFM Integration Benchmark**: Design quantitative benchmarks for the emergent capabilities (captioning, QA, image generation) across multiple MFMs and modalities, comparing against both unimodal and multimodal baselines.

3. **Ablation on Foundation Model Quality**: Systematically test how VIT-LENS-2's performance varies with foundation model quality (e.g., CLIP vs. smaller models) and analyze the relationship between foundation model capabilities and downstream task performance.