---
ver: rpa2
title: 'The Fourth International Verification of Neural Networks Competition (VNN-COMP
  2023): Summary and Results'
arxiv_id: '2312.16760'
source_url: https://arxiv.org/abs/2312.16760
tags:
- unsat
- nn4sys
- acasxu
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VNN-COMP 2023 was the fourth international competition for neural
  network verification tools, featuring 7 participating teams and 10 scored benchmarks.
  The competition used standardized formats (ONNX networks, VNN-LIB specifications)
  and equal-cost AWS hardware for fair comparison.
---

# The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results

## Quick Facts
- arXiv ID: 2312.16760
- Source URL: https://arxiv.org/abs/2312.16760
- Reference count: 40
- Top-performing tool: α,β-CROWN with score 930.9/1000

## Executive Summary
VNN-COMP 2023 was the fourth international competition for neural network verification tools, featuring 7 participating teams and 10 scored benchmarks. The competition used standardized formats (ONNX networks, VNN-LIB specifications) and equal-cost AWS hardware for fair comparison. Tools were evaluated on a diverse set of real-world benchmarks including ACAS Xu collision detection, image classification, and power system applications. The top-performing tool, α,β-CROWN, achieved a score of 930.9 out of 1000, verifying 721 instances with 570 UNSAT and 151 SAT results. Marabou placed second with 594.1 points.

## Method Summary
The competition used standardized ONNX networks and VNN-LIB specifications across 10 benchmark categories. Tools were evaluated on AWS hardware (p3.2xlarge, m5.16xlarge, g5.8xlarge) with scoring based on verified instances (10 points correct hold/violated, -150 points incorrect). The evaluation framework automatically installed and ran tools, collecting results in CSV format. Tools could choose which instances to attempt, with penalties for incorrect results and timeouts for failed instances.

## Key Results
- α,β-CROWN won with 930.9 points, verifying 721 instances
- Marabou placed second with 594.1 points
- 7 teams participated across 10 benchmark categories
- Top tools used GPU-accelerated linear bound propagation with branch-and-bound frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: α,β-CROWN's combination of GPU-accelerated linear bound propagation with branch-and-bound achieves the best performance by efficiently handling both ReLU and general nonlinear activation functions.
- Mechanism: The tool uses linear relaxation of the neural network through α-CROWN (optimized CROWN/LiRPA) to compute bounds on intermediate layers, then applies branch-and-bound (BaB) using β-CROWN to split constraints and propagate them efficiently. GPU acceleration via PyTorch and CUDA enables massively parallel computation of these bounds.
- Core assumption: Linear relaxation with bound propagation provides tight enough bounds to make BaB pruning effective, and GPU acceleration provides sufficient speedup to offset the overhead of BaB.
- Evidence anchors:
  - [abstract]: "The combination of efficient, optimizable and GPU-accelerated bound propagation with BaB produces a powerful and scalable neural network verifier."
  - [section 3.1]: "The core techniques in α,β-CROWN combine the efficient and GPU-accelerated linear bound propagation method with branch-and-bound methods."
- Break condition: If the linear relaxation becomes too loose for complex nonlinearities, BaB may not prune effectively, or if GPU memory becomes a bottleneck for very large networks.

### Mechanism 2
- Claim: Marabou's use of a uniform solving strategy combining convex optimization, abstract interpretation, and Split-and-Conquer parallelization provides robust performance across diverse benchmarks.
- Mechanism: Marabou encodes verification queries as constraint satisfaction problems and solves them using specialized convex optimization procedures and abstract interpretation techniques. The Split-and-Conquer algorithm enables parallelization across multiple cores.
- Core assumption: The combination of these techniques provides sufficient coverage and efficiency across the diverse set of benchmarks, and parallelization effectively utilizes available CPU resources.
- Evidence anchors:
  - [section 3.3]: "Under the hood, Marabou employs a uniform solving strategy for a given verification query. In particular, Marabou performs complete analysis that employs a specialized convex optimization procedure and abstract interpretation."
  - [section 3.3]: "It also uses the Split-and-Conquer algorithm for parallelization."
- Break condition: If the uniform strategy is not well-tuned for specific benchmark types, or if parallelization overhead outweighs benefits for smaller problems.

### Mechanism 3
- Claim: nnenum's multi-level abstraction refinement approach achieves high performance by balancing completeness with computational efficiency.
- Mechanism: nnenum uses reachability analysis with star sets combined with the ImageStar method to propagate sets through linear layers. It supports multiple abstraction levels including intervals, zonotopes, and optimized bound propagation methods. The tool uses GLPK or Gurobi for LP solving and adds refinement when needed.
- Core assumption: The multi-level abstraction approach provides a good tradeoff between precision and computational cost, and the refinement process effectively improves results when initial abstractions are insufficient.
- Evidence anchors:
  - [section 3.4]: "The nnenum tool uses multiple levels of abstraction to achieve high-performance verification of ReLU networks without sacrificing completeness."
  - [section 3.4]: "The core verification method is based on reachability analysis using star sets, combined with the ImageStar method to propagate sets through all linear layers."
- Break condition: If the abstraction levels are not sufficiently precise for certain benchmark types, or if the refinement process becomes too computationally expensive.

## Foundational Learning

- Concept: Neural network verification fundamentals (safety properties, robustness, adversarial examples)
  - Why needed here: All tools are solving neural network verification problems, so understanding the basic concepts is essential for understanding their approaches and results.
  - Quick check question: What is the difference between local robustness and global safety properties in neural network verification?

- Concept: Linear relaxation and bound propagation techniques
  - Why needed here: Many top-performing tools (α,β-CROWN, nnenum) rely on these techniques, so understanding how they work is crucial for understanding their effectiveness.
  - Quick check question: How does linear relaxation of ReLU activation functions enable bound propagation in neural networks?

- Concept: Branch-and-bound algorithms and constraint satisfaction
  - Why needed here: Several tools use BaB frameworks, so understanding how these algorithms work is important for understanding their approach.
  - Quick check question: How does the branch-and-bound algorithm prune the search space in neural network verification?

## Architecture Onboarding

- Component map: Benchmark system -> Tool interface -> Evaluation platform -> Result aggregation -> Competition framework
- Critical path: Benchmark definition → Tool submission → Automated evaluation → Result aggregation → Ranking
- Design tradeoffs:
  - Hardware standardization vs. tool flexibility
  - Timeout per instance vs. total benchmark runtime
  - Penalty severity for incorrect results vs. encouraging participation
  - Benchmark diversity vs. year-on-year comparability
- Failure signatures:
  - Tools failing to complete installation scripts
  - Timeout issues due to benchmark size vs. tool capability mismatch
  - Incorrect results due to tool bugs or specification misinterpretation
  - Scoring anomalies due to counterexample output issues
- First 3 experiments:
  1. Run a simple benchmark (e.g., ACAS Xu) with all tools to verify installation and basic functionality
  2. Test a medium-complexity benchmark (e.g., Collins RUL CNN) to assess performance differences
  3. Run a large benchmark (e.g., VGGNet16) to evaluate scalability and timeout handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific improvements in neural network verification techniques could be developed to handle transformers and other general nonlinearities more efficiently?
- Basis in paper: [explicit] The paper highlights the challenge of verifying transformers due to their complex architectures and general nonlinearities, and proposes a new benchmark with Vision Transformers (ViTs) to encourage the development of verification techniques for these models.
- Why unresolved: The paper presents the ViT benchmark but does not provide a comprehensive solution for verifying transformers and other general nonlinearities.
- What evidence would resolve it: A significant improvement in the runtime and scalability of verification tools when applied to transformer-based models and other networks with general nonlinearities, compared to existing methods.

### Open Question 2
- Question: How can the overhead of neural network verification tools be reduced without compromising their accuracy and completeness?
- Basis in paper: [explicit] The paper mentions the overhead correction process, where the overhead of tools was measured and subtracted from the total runtime. However, it did not influence the scores as no time bonus was awarded.
- Why unresolved: The paper does not provide a solution to reduce the overhead of verification tools while maintaining their performance.
- What evidence would resolve it: A demonstration of a significant reduction in the overhead of verification tools, leading to faster verification times without sacrificing accuracy or completeness.

### Open Question 3
- Question: What are the potential applications and limitations of neural network verification in safety-critical domains beyond those explored in the VNN-COMP 2023 benchmarks?
- Basis in paper: [inferred] The paper discusses the importance of neural network verification in safety-critical applications and mentions benchmarks related to autonomous driving, power systems, and aerospace. However, it does not explore the full range of potential applications and limitations.
- Why unresolved: The paper focuses on the results and findings of the VNN-COMP 2023, without extensively discussing the broader implications and limitations of neural network verification in various safety-critical domains.
- What evidence would resolve it: A comprehensive study analyzing the applicability and limitations of neural network verification in a wide range of safety-critical domains, such as healthcare, finance, and critical infrastructure, providing insights into the challenges and opportunities in each domain.

## Limitations
- Timeout-based scoring may favor tools handling large instances quickly over those with higher overall accuracy on smaller problems
- Penalty system (-150 points for incorrect results) could discourage tools from attempting particularly challenging benchmarks
- Results may not generalize to completely different domains or network architectures not represented in the 10 benchmark categories

## Confidence
- High confidence in core findings due to well-documented results and standardized evaluation procedures
- Medium confidence in conclusions about specific tool mechanisms without ablation studies isolating component contributions
- Low confidence in generalization to networks with different activation functions or architectures beyond the competition benchmarks

## Next Checks
1. Run the top three tools (α,β-CROWN, Marabou, nnenum) on a new benchmark set containing networks with different activation functions (e.g., sigmoid, tanh) to test generalization beyond ReLU networks.

2. Conduct an ablation study on α,β-CROWN by running it with and without GPU acceleration on the same benchmark set to quantify the contribution of parallelization.

3. Test the tools on networks with significantly more layers than those in VNN-COMP 2023 (e.g., ResNet-50 or larger) to evaluate scalability limits beyond the competition benchmarks.