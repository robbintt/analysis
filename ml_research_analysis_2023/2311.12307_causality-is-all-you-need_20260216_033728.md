---
ver: rpa2
title: Causality is all you need
arxiv_id: '2311.12307'
source_url: https://arxiv.org/abs/2311.12307
tags:
- causal
- layer
- which
- deconfounding
- adjustment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Causal Graph Routing (CGR), a novel framework\
  \ that applies causal inference principles to visual and language tasks by integrating\
  \ multiple deconfounding methods. CGR dynamically selects the most appropriate deconfounding\
  \ technique\u2014no confounder, back-door adjustment, or front-door adjustment\u2014\
  for each task using a novel \"sufficient cause\" concept."
---

# Causality is all you need

## Quick Facts
- arXiv ID: 2311.12307
- Source URL: https://arxiv.org/abs/2311.12307
- Reference count: 40
- Key result: State-of-the-art 75.46% VQA2.0 accuracy, 3.91% improvement over best competitor

## Executive Summary
This paper introduces Causal Graph Routing (CGR), a novel framework that applies causal inference principles to visual and language tasks by integrating multiple deconfounding methods. The framework dynamically selects the most appropriate deconfounding technique for each task using a novel "sufficient cause" concept, combining no confounder, back-door adjustment, and front-door adjustment blocks. CGR achieves state-of-the-art results on Visual Question Answering (VQA2.0) and Long Document Classification (ECtHR), demonstrating superior performance with fewer parameters compared to large pre-trained models.

## Method Summary
The Causal Graph Routing (CGR) framework implements a stacked architecture with parallel deconfounding blocks at each layer. Each layer contains three types of deconfounding blocks: no confounder, back-door adjustment, and front-door adjustment. The framework uses a sufficient cause concept to dynamically weight and route through these blocks based on task-specific causal relationships. A sharpening softmax function with temperature annealing enables differentiable routing weight learning, allowing the model to make discrete selections while maintaining end-to-end trainability. The framework is trained using Adam/AdamW optimizers with task-specific learning rates and decay schedules.

## Key Results
- Achieved 75.46% accuracy on VQA2.0 (3.91% improvement over best competitor)
- Reached 76.6% macro F1 score on ECtHR legal document classification
- Demonstrated state-of-the-art performance on Long Document Classification across ECtHR and 20 NewsGroups datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sufficient cause concept enables dynamic selection of deconfounding methods across different causal graphs
- Mechanism: By calculating the probability of sufficient cause between causal graphs (psP1→P2), the framework can weight different deconfounding blocks and route through the most appropriate causal graph for each specific task
- Core assumption: Different causal graphs are sufficient causes for each other in certain contexts, and these relationships can be quantified
- Break condition: If sufficient cause probabilities become uniform across all blocks, routing mechanism loses discriminative power

### Mechanism 2
- Claim: Stacked causal layers with parallel deconfounding blocks can handle diverse confounding scenarios in vision and language tasks
- Mechanism: Each layer contains three deconfounding blocks that can address different types of confounding relationships, and multiple layers allow for hierarchical deconfounding
- Core assumption: Different types of confounding require different deconfounding methods, and stacking captures complex causal relationships
- Break condition: If layers become too deep without sufficient regularization, model may overfit to specific confounding patterns

### Mechanism 3
- Claim: The sharpening softmax function enables differentiable routing weight learning through temperature annealing
- Mechanism: Temperature coefficient τ starts at 1 (soft weighting) and gradually decreases to 0 (hard argmax), allowing model to learn routing weights through backpropagation
- Core assumption: Differentiable approximation of discrete routing is possible through temperature-based sharpening
- Break condition: If temperature annealing is too aggressive, routing becomes too deterministic too early

## Foundational Learning

- Concept: Causal inference and do-calculus framework
  - Why needed here: Framework built on causal intervention principles rather than correlation, requiring understanding of do-operations
  - Quick check question: What is the key difference between P(Y|X) and P(Y|do(X)) in causal terms?

- Concept: Confounding and backdoor/front-door adjustment criteria
  - Why needed here: Framework uses classical causal inference techniques as building blocks
  - Quick check question: When would you use front-door adjustment instead of backdoor adjustment?

- Concept: Sufficient cause and probabilistic causation
  - Why needed here: Novel sufficient cause concept is central to routing mechanism
  - Quick check question: How does probability of sufficient cause differ from traditional probabilistic causation?

## Architecture Onboarding

- Component map: Input → Parallel deconfounding blocks → Sufficient cause weighting → Layer aggregation → Output
- Critical path: Input processing → Three deconfounding blocks per layer → Sufficient cause weighting → Layer aggregation with sharpening softmax → Output classification
- Design tradeoffs:
  - More layers provide better hierarchical deconfounding but increase computational cost
  - Sharper routing (lower temperature) gives better performance but reduces exploration
  - External knowledge retrieval adds performance but introduces dependency on knowledge base quality
- Failure signatures:
  - Uniform sufficient cause weights indicate routing failure
  - Performance degradation on specific task types suggests incorrect deconfounding method selection
  - Training instability may indicate temperature annealing schedule issues
- First 3 experiments:
  1. Compare single deconfounding block performance vs full CGR to validate necessity of multiple methods
  2. Test different temperature annealing schedules to find optimal balance between exploration and exploitation
  3. Evaluate ablation of external knowledge confounder extraction to measure its contribution to performance

## Open Questions the Paper Calls Out

1. How can the CGR framework be extended to incorporate additional deconfounding methods beyond the three currently implemented?
   - Basis: Authors mention "the causation community can offer numerous powerful deconfounding methods to further enhance our framework"
   - Resolution: Experiments comparing CGR performance with additional deconfounding methods across multiple tasks

2. How does dynamic selection of deconfounding methods affect interpretability and explainability of model predictions?
   - Basis: Authors propose framework that "dynamically selects the suitable deconfounding methods in each layer"
   - Resolution: Systematic study comparing interpretability of CGR predictions against baseline models

3. How does CGR performance scale with increasingly complex causal graphs and larger datasets?
   - Basis: Authors demonstrate effectiveness on two tasks with moderate-sized datasets
   - Resolution: Experiments on tasks with more complex causal graphs and larger datasets

## Limitations

- Performance claims based on limited competitor comparisons without detailed ablation studies
- Temperature annealing schedule for sharpening softmax lacks sensitivity analysis
- Sufficient cause probability calculation is novel but lacks extensive empirical validation

## Confidence

**High confidence**: General framework architecture and use of classical causal inference techniques
**Medium confidence**: Novel sufficient cause concept and performance improvements claims
**Low confidence**: Effectiveness of sufficient cause routing mechanism and scalability to complex tasks

## Next Checks

1. Conduct ablation study on sufficient cause routing comparing random routing, fixed routing, and full sufficient cause routing
2. Perform temperature annealing sensitivity analysis with different decay schedules and initial values
3. Apply CGR to at least two additional task types to validate generalization beyond vision and language tasks