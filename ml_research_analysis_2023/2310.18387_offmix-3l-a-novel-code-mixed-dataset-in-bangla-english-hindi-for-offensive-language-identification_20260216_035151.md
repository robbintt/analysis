---
ver: rpa2
title: 'OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for Offensive
  Language Identification'
arxiv_id: '2310.18387'
source_url: https://arxiv.org/abs/2310.18387
tags:
- data
- offensive
- language
- languages
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OffMix-3L, a novel dataset for offensive
  language identification in code-mixed Bangla-English-Hindi text, addressing the
  scarcity of such resources for three-language code-mixing. The dataset contains
  1,001 instances annotated by fluent speakers, with a focus on real-world code-mixing
  scenarios.
---

# OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for Offensive Language Identification

## Quick Facts
- arXiv ID: 2310.18387
- Source URL: https://arxiv.org/abs/2310.18387
- Reference count: 10
- Primary result: OffMix-3L is a novel dataset for offensive language identification in Bangla-English-Hindi code-mixed text with 1,001 annotated instances.

## Executive Summary
This paper introduces OffMix-3L, a novel dataset for offensive language identification in code-mixed Bangla-English-Hindi text, addressing the scarcity of such resources for three-language code-mixing. The dataset contains 1,001 instances annotated by fluent speakers, with a focus on real-world code-mixing scenarios. Experiments with monolingual, bilingual, and multilingual models show that BanglishBERT outperforms others, achieving an F1 score of 0.68 when trained on synthetic data and tested on OffMix-3L. Multilingual models like mBERT and XLM-R perform best (F1: 0.88) when trained and tested on synthetic data. The study highlights challenges like transliteration and typos, and plans to expand the dataset for training and develop pre-trained trilingual models. OffMix-3L serves as a valuable benchmark for advancing research in multi-level code-mixing tasks.

## Method Summary
The study introduces OffMix-3L, a novel dataset for offensive language identification in code-mixed Bangla-English-Hindi text. The dataset was collected from social media platforms and annotated by fluent speakers to ensure quality. Synthetic code-mixed data was generated using the Random Code-mixing Algorithm and r-CM to create a large training set. Various transformer-based models, including BanglishBERT, mBERT, and XLM-R, were fine-tuned on synthetic data and evaluated on OffMix-3L. The experiments aimed to assess the performance of these models on real code-mixed data and identify challenges such as transliteration and typos.

## Key Results
- OffMix-3L contains 1,001 instances annotated by fluent speakers for offensive language identification in Bangla-English-Hindi code-mixed text.
- BanglishBERT achieves an F1 score of 0.68 when trained on synthetic data and tested on OffMix-3L.
- Multilingual models like mBERT and XLM-R perform best (F1: 0.88) when trained and tested on synthetic data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic code-mixed data generated via Random Code-mixing Algorithm (Krishnan et al., 2022) and r-CM (Santy et al., 2021) provides a scalable training source for a three-language offensive language task.
- Mechanism: The synthetic pipeline takes monolingual offensive datasets (OLID, SOLID) and applies controlled language-switching rules to simulate realistic Bangla-English-Hindi code-mixed instances, producing 100k labeled samples.
- Core assumption: The statistical patterns learned from synthetic code-mixed data transfer effectively to real-world code-mixed text with similar token-level distributions.
- Evidence anchors:
  - [abstract] "We present OffMix-3L exclusively as a test set... We build a synthetic train and development set that contains Code-mixing for Bangla, English, and Hindi."
  - [section] "We use the Random Code-mixing Algorithm (Krishnan et al., 2022) and r-CM (Santy et al., 2021) to generate the synthetic Code-mixed dataset."
  - [corpus] Weak evidence; no published synthetic corpus directly validates cross-lingual transfer to OffMix-3L.
- Break condition: If the synthetic corpus lacks sufficient transliteration or typo variability, model performance will drop on natural code-mixed inputs.

### Mechanism 2
- Claim: BanglishBERT, trained on Bangla-English code-mixed data, generalizes to three-language code-mixing when fine-tuned on synthetic Bangla-English-Hindi data.
- Mechanism: The bilingual model's shared subword vocabulary and attention layers can absorb Hindi tokens through fine-tuning, enabling multilingual code-mixing understanding.
- Core assumption: Pre-trained bilingual models can absorb additional languages without catastrophic forgetting when the new language shares similar code-mixing patterns.
- Evidence anchors:
  - [abstract] "BanglishBERT outperforms other transformer-based models and GPT-3.5."
  - [section] "BanglishBERT (Bhattacharjee et al., 2022) and HingBERT (Nayak and Joshi, 2022) are used as bilingual models as they are trained on both Bangla-English and Hindi-English respectively."
  - [corpus] No corpus evidence shows cross-language transfer performance for BanglishBERT on three-language code-mixing.
- Break condition: If Hindi and Bangla tokenization schemes diverge significantly, BanglishBERT will fail to capture Hindi-English mixing patterns.

### Mechanism 3
- Claim: Multilingual models (mBERT, XLM-R) trained on 100+ languages perform well on synthetic three-language code-mixing but poorly on real OffMix-3L.
- Mechanism: Multilingual models' large vocabularies and shared representation space can handle synthetic multi-language input but struggle with real code-mixing nuances like transliteration and typos.
- Core assumption: Synthetic data mirrors real code-mixed linguistic patterns closely enough for effective multilingual model training.
- Evidence anchors:
  - [abstract] "Multilingual models like mBERT and XLM-R perform best (F1: 0.88) when trained and tested on synthetic data."
  - [section] "We use mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) as multilingual models which are respectively trained on 104 and 100 languages including Bangla-English-Hindi."
  - [corpus] Strong evidence; multilingual models' performance drop on real OffMix-3L (F1: 0.63 for mBERT) is well-documented in the results.
- Break condition: If synthetic code-mixing lacks real-world noise patterns, multilingual models will overfit to clean synthetic patterns.

## Foundational Learning

- Concept: Language identification with langdetect for filtering three-language code-mixed samples.
  - Why needed here: Ensures dataset purity by retaining only samples containing at least one token from each target language.
  - Quick check question: What happens if langdetect misclassifies transliterated words as Other tokens?

- Concept: Cohen's Kappa for inter-annotator agreement calculation.
  - Why needed here: Quantifies annotation reliability beyond raw agreement, accounting for chance agreement in binary classification.
  - Quick check question: How does Cohen's Kappa differ from raw agreement in imbalanced label distributions?

- Concept: Synthetic data generation via controlled code-mixing algorithms.
  - Why needed here: Enables training on large-scale multi-language offensive data where real data is scarce.
  - Quick check question: What are the key parameters for controlling language-switching probability in the Random Code-mixing Algorithm?

## Architecture Onboarding

- Component map: Data collection -> Annotation pipeline -> Synthetic generation -> Model training -> Evaluation -> Error analysis
- Critical path: Synthetic generation -> Model training -> Evaluation on OffMix-3L
- Design tradeoffs: Synthetic data quantity vs. real-world noise fidelity; bilingual model transfer vs. multilingual model generalization
- Failure signatures: Performance drop on transliterated tokens; high Other token counts; overfitting to synthetic patterns
- First 3 experiments:
  1. Fine-tune BanglishBERT on synthetic data, evaluate on OffMix-3L
  2. Fine-tune mBERT on synthetic data, evaluate on OffMix-3L
  3. Fine-tune mBERT on synthetic data, evaluate on synthetic test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multilingual models like mBERT and XLM-R compare when trained on natural versus synthetic code-mixed data for offensive language identification in Bangla-English-Hindi?
- Basis in paper: [explicit] The paper mentions that mBERT and XLM-R achieve an F1 score of 0.88 when trained and tested on synthetic data, but their performance on natural data is not directly compared.
- Why unresolved: The paper does not provide a direct comparison of model performance on natural versus synthetic data, leaving the effectiveness of synthetic data for training unclear.
- What evidence would resolve it: Conducting experiments that train and test models on both natural and synthetic data, comparing their F1 scores to determine the impact of data type on model performance.

### Open Question 2
- Question: What are the specific challenges and limitations of using transliterated words and misspelled tokens in code-mixed datasets for offensive language identification?
- Basis in paper: [inferred] The paper discusses the presence of transliterated words and misspelled tokens in the dataset, noting that these pose challenges for models not pre-trained on such data.
- Why unresolved: The paper does not provide a detailed analysis of how these tokens affect model performance or potential strategies to mitigate their impact.
- What evidence would resolve it: Analyzing model performance on instances with transliterated and misspelled tokens, and exploring methods to improve model handling of such tokens.

### Open Question 3
- Question: How can the OffMix-3L dataset be expanded to include more diverse and realistic code-mixing scenarios, and what impact would this have on model training and evaluation?
- Basis in paper: [explicit] The paper mentions plans to expand the dataset for training and develop pre-trained trilingual models, but does not detail the specific strategies or expected outcomes.
- Why unresolved: The paper outlines future plans but does not provide insights into the potential benefits or challenges of dataset expansion.
- What evidence would resolve it: Implementing dataset expansion strategies and evaluating the impact on model performance, diversity of code-mixing scenarios, and overall dataset quality.

## Limitations
- The synthetic data generation methods cannot fully capture real-world linguistic phenomena like transliteration patterns and typos, leading to performance gaps on natural code-mixed text.
- The study relies on monolingual offensive datasets (OLID, SOLID) as source material, which may introduce bias as offensive language patterns differ across languages and code-mixing contexts.
- The OffMix-3L dataset, while novel, is relatively small (1,001 instances) and exclusively serves as a test set, limiting its utility for training robust models.

## Confidence
- High Confidence: The dataset collection methodology and annotation process are well-documented and follow established practices. The inter-annotator agreement (Cohen's Kappa = 0.78) indicates reliable labeling. The performance differences between models on synthetic versus real data are empirically observed and statistically valid.
- Medium Confidence: The claim that BanglishBERT outperforms other models on OffMix-3L (F1 = 0.68) is supported by experimental results, though the confidence intervals are not reported. The superiority of multilingual models on synthetic data (F1 = 0.88) is demonstrated, but the real-world applicability remains questionable given the performance drop on natural code-mixed text.
- Low Confidence: The assertion that synthetic data generation methods can fully replace real code-mixed training data lacks sufficient empirical support. The study does not provide ablation studies on synthetic data size or quality parameters, making it difficult to assess the robustness of the synthetic generation approach.

## Next Checks
1. **Synthetic Data Quality Analysis**: Conduct a systematic comparison of token-level distributions between synthetic and real OffMix-3L data, specifically measuring transliteration frequency, typo rates, and language-switching patterns to quantify the synthetic-real gap.
2. **Cross-Validation with Expanded Dataset**: Expand OffMix-3L to include a training split and perform 5-fold cross-validation to assess model performance stability and determine whether the current 1,001-instance test set provides sufficient statistical power.
3. **Alternative Synthetic Generation Methods**: Implement and compare alternative synthetic data generation approaches (e.g., neural machine translation-based methods) against the current Random Code-mixing Algorithm and r-CM to evaluate whether different synthetic strategies yield better real-data transfer performance.