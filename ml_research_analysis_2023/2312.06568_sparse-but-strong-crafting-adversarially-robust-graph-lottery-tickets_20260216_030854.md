---
ver: rpa2
title: 'Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets'
arxiv_id: '2312.06568'
source_url: https://arxiv.org/abs/2312.06568
tags:
- graph
- sparsity
- args
- accuracy
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the vulnerability of Graph Lottery Tickets
  (GLTs) under adversarial structure perturbations. The authors propose an adversarially
  robust graph sparsification (ARGS) framework that prunes the adjacency matrix and
  GNN weights by optimizing a novel loss function capturing graph homophily and leveraging
  both true labels of train nodes and pseudo labels of test nodes.
---

# Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets

## Quick Facts
- arXiv ID: 2312.06568
- Source URL: https://arxiv.org/abs/2312.06568
- Reference count: 40
- Key outcome: This paper explores the vulnerability of Graph Lottery Tickets (GLTs) under adversarial structure perturbations and proposes an adversarially robust graph sparsification (ARGS) framework that achieves high sparsity while maintaining competitive performance under various attacks.

## Executive Summary
This paper addresses the vulnerability of Graph Lottery Tickets (GLTs) to adversarial structure perturbations. The authors propose an adversarially robust graph sparsification (ARGS) framework that prunes the adjacency matrix and GNN weights by optimizing a novel loss function capturing graph homophily and leveraging both true labels of train nodes and pseudo labels of test nodes. By iteratively applying ARGS, they identify adversarially robust GLTs (ARGLTs) that achieve competitive performance under various structure attacks like PGD, MetaAttack, Meta-PGD, and PR-BCD. The ARGLTs exhibit high sparsity (23%-61% for graphs and 64%-98% for GNN models) while maintaining similar accuracy to full models and graphs. Compared to GLTs from UGS, ARGLTs achieve the same accuracy with 2.4x more graph sparsity and 2.3x more model sparsity on average.

## Method Summary
The paper proposes an adversarially robust graph sparsification (ARGS) framework that identifies sparse and robust Graph Lottery Tickets (ARGLTs). ARGS iteratively prunes the adjacency matrix and GNN weights using a novel loss function that combines cross-entropy on training nodes, feature smoothness loss, and cross-entropy on pseudo-labeled test nodes. The framework leverages the observation that adversarial attacks modify more edges around training nodes than test nodes. ARGS achieves better robustness than standard unguided sparsification (UGS) by optimizing a combined loss function during iterative pruning rather than just magnitude-based pruning.

## Key Results
- ARGS achieves similar accuracy to full models while maintaining high sparsity (23%-61% for graphs, 64%-98% for GNN models)
- ARGLTs outperform UGS GLTs by achieving the same accuracy with 2.4x more graph sparsity and 2.3x more model sparsity on average
- ARGS maintains competitive performance under various structure attacks including PGD, MetaAttack, Meta-PGD, and PR-BCD across multiple datasets (Cora, Citeseer, PubMed, OGBN-ArXiv)
- The framework successfully identifies and removes adversarial edges while preserving important clean edges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ARGS loss function effectively identifies and removes adversarial edges by combining graph homophily and label consistency.
- Mechanism: ARGS combines three key loss terms: (1) cross-entropy on training nodes to preserve local structure, (2) feature smoothness loss to remove edges between dissimilar nodes, and (3) cross-entropy on pseudo-labeled test nodes to guide pruning in regions less affected by attack.
- Core assumption: Adversarial edges tend to connect nodes with dissimilar features, while clean edges connect nodes with similar features (homophily property).
- Evidence anchors:
  - [abstract] "we present an adversarially robust graph sparsification (ARGS) framework that prunes the adjacency matrix and the GNN weights by optimizing a novel loss function capturing graph homophily and information associated with both the true labels of the train nodes and the pseudo labels of the test nodes."
  - [section 3.2] "Adversarial attacks like MetaAttack, PGD, and PR-BCD poison the graph structure by either introducing new edges or deleting existing edges, resulting in changes in the original graph properties. We analyze the difference in the attribute features of the nodes connected by the clean and adversarial edges. Figure 3 depicts the density distribution of the attribute feature difference between connected nodes in the Citeseer graph dataset attacked by the PGD attack. We can observe from Figure 3 that the attack tends to connect nodes with large attribute feature differences."
  - [corpus] "Sparse Bayesian Message Passing under Structural Uncertainty" discusses leveraging uncertainty in graph structure for robust learning.

### Mechanism 2
- Claim: ARGS leverages the observation that poisoning attacks modify more edges around training nodes than test nodes to guide pruning.
- Mechanism: By including cross-entropy loss on both training nodes (which are heavily attacked) and pseudo-labeled test nodes (which are less affected), ARGS can differentiate between edges that need pruning and those that should be preserved.
- Core assumption: Structure poisoning attacks introduce most modifications around training nodes while leaving test node local structure relatively intact.
- Evidence anchors:
  - [abstract] "Attacks like the projected gradient descent (PGD) topology attack [27], the meta-learning-based graph attack (MetaAttack) [37], and Meta-PGD [23] often introduce many of the edge modifications around the training nodes [17] while the local structure of the test nodes is less affected."
  - [section 3.3] "Poisoning attacks like the MetaAttack and the PGD attack tend to modify more the local structure around the train nodes than that around the test nodes [17]."
  - [corpus] "Efficient Low-Rank GNN Defense Against Structural Attacks" focuses on defending GNNs by identifying and removing adversarial edges, supporting the importance of this observation.

### Mechanism 3
- Claim: ARGS achieves better robustness than UGS by optimizing a combined loss function during iterative pruning rather than just magnitude-based pruning.
- Mechanism: ARGS optimizes all parameters (adjacency mask, weight mask, and model weights) simultaneously using the combined loss function, while UGS only optimizes based on weight magnitudes.
- Core assumption: The loss function can effectively guide the pruning process to remove adversarial edges while preserving important clean edges.
- Evidence anchors:
  - [abstract] "ARGS archives similar accuracy with 30% more sparsity compared to UGS for the Cora dataset under PGD attack."
  - [section 3.3] "ARGS reformulates the loss function to include (a) a CE loss term on the train nodes, (b) a CE loss term on a set of test nodes, and (c) a square loss term on all edges. Pruning the edges based on this combined loss function results in the removal of adversarial as well as less-important non-adversarial edges from the graph."
  - [corpus] "Rethinking Graph Lottery Tickets: Graph Sparsity Matters" discusses limitations of existing GLT approaches and the importance of effective pruning strategies.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: Understanding how GNNs aggregate information from neighbors is crucial for grasping why adversarial edge modifications are so effective and how ARGS counters them.
  - Quick check question: How does a 2-layer GCN update node representations, and why does this make GNNs vulnerable to adversarial structure perturbations?

- Concept: Lottery Ticket Hypothesis and graph sparsification
  - Why needed here: ARGS builds on the graph lottery ticket framework by finding sparse, robust subnetworks that maintain performance under attack.
  - Quick check question: What is the difference between a standard graph lottery ticket and an adversarially robust graph lottery ticket (ARGLT)?

- Concept: Adversarial attacks on graphs and poisoning attacks
  - Why needed here: Understanding the types of attacks (PGD, MetaAttack, PR-BCD) and their characteristics is essential for comprehending why ARGS' approach is necessary.
  - Quick check question: What is the key difference between structure poisoning attacks and feature poisoning attacks on GNNs?

## Architecture Onboarding

- Component map:
  - ARGS Optimizer -> Loss Function Module -> Pruning Controller -> MLP Pseudo-label Generator -> GNN Backbone

- Critical path:
  1. Initialize masks and weights
  2. For each pruning iteration:
     a. Compute ARGS loss
     b. Update masks and weights via gradient descent
     c. Apply top-k pruning to masks
  3. After reaching target sparsity, train the sparse GNN with fixed masks

- Design tradeoffs:
  - Computational cost vs. robustness: ARGS is more computationally expensive than UGS due to the complex loss function and optimization
  - Sparsity vs. accuracy: Higher sparsity generally reduces accuracy, but ARGS aims to minimize this tradeoff
  - Hyperparameter sensitivity: ARGS has more hyperparameters (β, γ, λ1, λ2) that require tuning

- Failure signatures:
  - Low accuracy on clean graphs: Indicates over-pruning of important edges
  - High accuracy on attacked graphs but low sparsity: Suggests insufficient pruning of adversarial edges
  - Training instability: Could indicate improper hyperparameter settings or optimization issues

- First 3 experiments:
  1. Run ARGS on Cora with PGD attack (5% perturbation) and compare accuracy vs. graph sparsity against UGS baseline
  2. Perform ablation study by removing each component of the ARGS loss function and measuring impact on robustness
  3. Test ARGS on a heterophilic graph (where connected nodes have dissimilar features) to identify limitations of the feature smoothness component

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The framework may not generalize well to heterophilic graphs where connected nodes have dissimilar features, potentially causing the feature smoothness component to incorrectly penalize clean edges
- Computational overhead is higher compared to simpler pruning methods like UGS, which may limit scalability to very large graphs
- The effectiveness depends on proper hyperparameter tuning, with multiple parameters (β, γ, λ1, λ2) that require careful adjustment

## Confidence
- **High confidence**: Claims about ARGS achieving competitive accuracy under attacks while maintaining high sparsity (supported by extensive experiments across 4 datasets and 4 attack types)
- **Medium confidence**: Claims about ARGS' superiority over UGS in finding more sparse yet robust GLTs (requires careful hyperparameter tuning for fair comparison)
- **Low confidence**: Claims about ARGS' effectiveness on heterophilic graphs (not explicitly tested in the paper)

## Next Checks
1. **Heterophily testing**: Apply ARGS to datasets with known heterophilic structure (like Texas or Wisconsin) to verify if the feature smoothness component remains effective or needs modification.

2. **Computational efficiency analysis**: Compare wall-clock time and memory usage between ARGS and UGS across different graph sizes to quantify the overhead cost and identify optimization opportunities.

3. **Transfer learning validation**: Test whether ARGLTs found on one dataset can maintain robustness when transferred to structurally similar but different datasets, examining the robustness transferability properties.