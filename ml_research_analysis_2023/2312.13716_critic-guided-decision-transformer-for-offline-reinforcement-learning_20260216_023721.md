---
ver: rpa2
title: Critic-Guided Decision Transformer for Offline Reinforcement Learning
arxiv_id: '2312.13716'
source_url: https://arxiv.org/abs/2312.13716
tags:
- returns
- learning
- critic
- policy
- cgdt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations of return-conditioned supervised
  learning (RCSL) in offline reinforcement learning, particularly its struggle with
  stochastic environments and stitching suboptimal trajectories. The core issue is
  the inconsistency between sampled target returns and expected returns of actions
  due to environmental stochasticity and suboptimal data.
---

# Critic-Guided Decision Transformer for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.13716
- Source URL: https://arxiv.org/abs/2312.13716
- Authors: [List of authors from paper]
- Reference count: 20
- Key outcome: Critic-Guided Decision Transformer (CGDT) significantly outperforms traditional return-conditioned supervised learning methods in offline reinforcement learning, particularly in suboptimal data regimes and stochastic environments.

## Executive Summary
This paper addresses limitations of return-conditioned supervised learning (RCSL) in offline reinforcement learning, particularly its struggle with stochastic environments and stitching suboptimal trajectories. The core issue is the inconsistency between sampled target returns and expected returns of actions due to environmental stochasticity and suboptimal data. To resolve this, the paper proposes Critic-Guided Decision Transformer (CGDT), which combines trajectory modeling from Decision Transformer with value-based critic guidance. The critic estimates expected returns of actions and guides policy training through asymmetric loss functions that favor optimistic actions with higher expected returns. Empirical evaluations on Bernoulli bandit tasks and D4RL benchmark datasets show CGDT significantly outperforms traditional RCSL methods in suboptimal data regimes while maintaining strong performance in high-quality datasets. The approach also demonstrates improved consistency between expected and target returns and better handling of sparse rewards.

## Method Summary
CGDT integrates a value-based critic network into the Decision Transformer framework to address the inconsistency between sampled target returns and expected returns in RCSL. The critic estimates the distribution of returns conditioned on state-action pairs, providing guidance to align policy actions with desired target returns. During training, an asymmetric NLL loss biases the critic towards fitting either optimal or suboptimal trajectories based on a coefficient τc, while expectile regression with parameter τp favors optimistic actions with higher expected returns than the target returns. The policy is trained using a weighted combination of the original RCSL objective and the critic-guided objective, with a linearly increasing weight coefficient α' to balance these terms. This approach enables CGDT to effectively handle suboptimal data and stochastic environments while maintaining consistency between expected and target returns.

## Key Results
- CGDT outperforms traditional RCSL methods (DT) and baselines (CQL, IQL, TT, RvS, QDT) on D4RL datasets in medium, medium-replay, and medium-expert data qualities
- Significant performance improvements on suboptimal datasets (e.g., medium and medium-replay) where RCSL methods struggle
- Improved consistency between expected and target returns, particularly in stochastic environments like the Bernoulli bandit task
- Better handling of sparse rewards and trajectory stitching problems compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CGDT resolves inconsistency between sampled target returns and expected returns by incorporating a critic that estimates expected returns of actions.
- Mechanism: The critic approximates the distribution of returns conditioned on state-action pairs, providing guidance to align policy actions with desired target returns through asymmetric loss functions.
- Core assumption: The learned critic accurately estimates expected returns and can effectively guide policy learning.
- Evidence anchors:
  - [abstract] "The critic estimates expected returns of actions and guides policy training through asymmetric loss functions that favor optimistic actions with higher expected returns."
  - [section] "To address the limitations of RCSL, we propose a novel approach called the Critic-Guided Decision Transformer (CGDT). Our approach combines the predictability of long-term returns from value-based methods with the Decision Transformer framework."
  - [corpus] Weak evidence - no direct citations to support this specific mechanism.
- Break condition: If the critic cannot accurately estimate expected returns due to model limitations or insufficient data quality, the guidance would be unreliable.

### Mechanism 2
- Claim: Asymmetric critic training biases the critic towards fitting either optimal or suboptimal trajectories based on a coefficient τc.
- Mechanism: By adjusting the coefficient τc, the critic training loss is modified to emphasize fitting trajectories with higher or lower returns, improving the critic's ability to guide policy in the presence of mixed-quality data.
- Core assumption: The offline dataset contains sufficient examples of both optimal and suboptimal trajectories to enable effective asymmetric fitting.
- Evidence anchors:
  - [section] "We introduce an asymmetric NLL loss as the revised learning objective of fitting the critic... When τc > 0.5, the critic is biased towards fitting optimal trajectories, while τc < 0.5 biases the critic towards suboptimal trajectories."
  - [corpus] Weak evidence - no direct citations to support this specific mechanism.
- Break condition: If the dataset lacks sufficient diversity in trajectory quality, asymmetric training may not provide meaningful benefits.

### Mechanism 3
- Claim: Expectile regression in critic guidance favors optimistic actions with higher expected returns than the target returns.
- Mechanism: By using expectile regression with parameter τp, the policy is guided to select actions whose expected returns exceed the target returns, encouraging more optimistic behavior.
- Core assumption: The critic's estimated mean and variance are reliable enough to guide policy selection effectively.
- Evidence anchors:
  - [section] "To encourage the selection of optimistic actions with expected returns higher than the target returns, we adopt the approach of Expectile Regression... When τp = 0.5, it is equivalent to mean regression... By adjusting τp, we introduce asymmetry into the mean regression."
  - [corpus] Weak evidence - no direct citations to support this specific mechanism.
- Break condition: If the critic's estimates are highly uncertain or inaccurate, expectile regression may lead the policy astray.

## Foundational Learning

- Concept: Return-Conditioned Supervised Learning (RCSL)
  - Why needed here: Understanding RCSL is fundamental to grasping the limitations that CGDT addresses, particularly its struggle with stochastic environments and stitching suboptimal trajectories.
  - Quick check question: How does RCSL differ from traditional imitation learning, and what is the core limitation of RCSL in stochastic environments?

- Concept: Value-based reinforcement learning methods
  - Why needed here: CGDT integrates value-based methods to address RCSL's limitations, so understanding how value functions estimate expected returns is crucial.
  - Quick check question: What is the primary advantage of using value functions in reinforcement learning, especially in stochastic environments?

- Concept: Asymmetric loss functions and expectile regression
  - Why needed here: CGDT employs asymmetric loss functions in both critic training and policy guidance, which is key to its effectiveness in handling suboptimal data and encouraging optimistic actions.
  - Quick check question: How does asymmetric loss differ from standard loss functions, and what is the purpose of using expectile regression in the context of CGDT?

## Architecture Onboarding

- Component map:
  Input: States (s), Actions (a), Target Returns (R) -> Policy Network (Decision Transformer) -> Critic Network -> Loss Functions (Asymmetric NLL, Expectile Regression) -> Output: Actions

- Critical path:
  1. Train critic on offline dataset to estimate expected returns of actions
  2. Use trained critic to guide policy training through asymmetric loss functions
  3. Balance original RCSL objective with critic guidance to ensure policy stays close to data distribution
  4. Evaluate policy performance on test environments

- Design tradeoffs:
  - Tradeoff between fitting optimal vs. suboptimal trajectories in critic training (controlled by τc)
  - Tradeoff between conservative vs. optimistic action selection in policy training (controlled by τp)
  - Complexity of training two separate models (policy and critic) vs. potential performance gains

- Failure signatures:
  - Poor performance on stochastic environments despite high performance on deterministic tasks
  - Overfitting to suboptimal data leading to consistently poor policies
  - Critic overestimating returns for out-of-distribution actions, leading policy astray

- First 3 experiments:
  1. Implement and train the critic network on a small subset of the offline dataset to verify it can estimate expected returns.
  2. Integrate critic guidance into policy training with τc = 0.5 and τp = 0.5, evaluate on a simple stitching task.
  3. Tune τc and τp on a validation set, observe impact on policy performance and consistency between expected and target returns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of critic guidance effectiveness when dealing with highly stochastic environments where return distributions are multimodal?
- Basis in paper: [explicit] The paper demonstrates CGDT's effectiveness in Bernoulli bandit environments with stochastic rewards, but doesn't explore the limits of this approach when environmental stochasticity becomes more extreme
- Why unresolved: The paper only tests on a simple two-armed bandit with Bernoulli rewards and doesn't systematically vary the level of environmental stochasticity to find where critic guidance breaks down
- What evidence would resolve it: Experiments varying the degree of environmental stochasticity (e.g., different bandit arm reward distributions, multi-modal return distributions) to identify when CGDT's critic guidance fails to improve over standard DT

### Open Question 2
- Question: How does CGDT's performance scale with dataset size and diversity, particularly for offline RL problems where the available data is limited or represents only narrow slices of the state-action space?
- Basis in paper: [inferred] The paper evaluates CGDT on standard D4RL benchmarks with fixed dataset sizes but doesn't investigate how performance changes as a function of dataset size or diversity
- Why unresolved: The paper doesn't conduct ablation studies varying dataset size or diversity to understand the minimum data requirements for effective critic guidance
- What evidence would resolve it: Systematic experiments varying dataset size (sub-sampling D4RL datasets) and diversity (using different behavior policies to generate data) to characterize CGDT's data efficiency and requirements

### Open Question 3
- Question: Can the asymmetric critic training and guidance parameters (τc and τp) be learned automatically rather than requiring manual tuning, and what would be the implications for CGDT's practical applicability?
- Basis in paper: [explicit] The paper uses fixed values for τc and τp across different environments and datasets, noting that "not all combinations of τc and τp are tuned" and requiring manual adjustment
- Why unresolved: The paper relies on manual hyperparameter tuning for τc and τp without exploring adaptive or learned approaches to setting these parameters
- What evidence would resolve it: Experiments with learned or adaptive methods for setting τc and τp (e.g., meta-learning, Bayesian optimization, or data-driven approaches) and comparison to manual tuning across diverse environments

## Limitations

- Theoretical foundations for critic guidance effectiveness are not rigorously established, relying primarily on empirical results
- Asymmetric parameters (τc and τp) require manual tuning across different environments and data qualities, limiting practical applicability
- Evaluation is limited to D4RL benchmarks and a simple Bernoulli bandit task, leaving uncertainty about performance in more complex, real-world scenarios

## Confidence

- High confidence: The core mechanism of combining value-based critic guidance with RCSL is well-founded and addresses a real limitation in offline RL.
- Medium confidence: Empirical results show clear improvements over baselines in the tested scenarios, though the magnitude of improvement varies across datasets.
- Low confidence: The specific parameter choices (τc, τp, α') and their impact on different types of suboptimal data are not fully explained or theoretically justified.

## Next Checks

1. **Theoretical analysis of critic guidance**: Derive conditions under which the critic's expected return estimates reliably guide policy improvement, particularly for out-of-distribution actions.

2. **Ablation studies on asymmetric parameters**: Systematically vary τc and τp across a wider range to understand their impact on performance and identify optimal settings for different data quality regimes.

3. **Scalability testing**: Evaluate CGDT on more complex offline RL benchmarks (e.g., Atari, continuous control tasks with higher dimensional states) to assess robustness beyond the current D4RL suite.