---
ver: rpa2
title: 'GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation
  Language Models for Ancient Texts'
arxiv_id: '2307.05354'
source_url: https://arxiv.org/abs/2307.05354
tags:
- chinese
- ancient
- traditional
- simplified
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study constructs GujiBERT and GujiGPT, foundational language
  models for intelligent information processing of ancient texts. Trained on extensive
  datasets encompassing both simplified and traditional Chinese characters, these
  models handle various natural language processing tasks, including sentence segmentation,
  punctuation, word segmentation, part-of-speech tagging, entity recognition, and
  automatic translation.
---

# GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts

## Quick Facts
- arXiv ID: 2307.05354
- Source URL: https://arxiv.org/abs/2307.05354
- Reference count: 6
- Key outcome: GujiBERT and GujiGPT models achieve high F1 scores (93.53% NER, 86.31% sentence breaking) on ancient Chinese text processing tasks.

## Executive Summary
This study introduces GujiBERT and GujiGPT, foundational language models tailored for intelligent information processing of ancient Chinese texts. By employing self-supervised continued pre-training on large-scale ancient corpora, these models handle diverse NLP tasks such as named entity recognition, sentence segmentation, and translation. The research demonstrates that continued pre-training on domain-specific data significantly enhances model performance, with variations for traditional, simplified, and mixed Chinese scripts to accommodate different processing needs.

## Method Summary
The authors constructed GujiBERT and GujiGPT by first expanding the vocabulary of baseline models (BERT, RoBERTa, GPT) with ancient Chinese terms, then continuing pre-training using MLM for BERT/RoBERTa and CLM for GPT on the Daizhige corpus. The models were fine-tuned on downstream tasks using publicly available datasets for traditional, simplified, and mixed Chinese. Hyperparameters included a learning rate of 2e-5, block sizes of 512 or 1024, batch sizes of 32 or 16, and 5 epochs, with validation via perplexity and task-specific metrics like F1 and BLEU scores.

## Key Results
- GujiBERT_fan achieves 93.53% F1 score for named entity recognition on traditional Chinese texts.
- GujiRoBERTa_fan reaches 86.31% F1 for sentence breaking in traditional Chinese.
- The models demonstrate strong cross-script generalization, handling both simplified and traditional ancient Chinese with high accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model’s high F1 scores on named entity recognition are driven by self-supervised continued pre-training on large-scale ancient text corpora.
- Mechanism: The original pre-trained models (BERT, RoBERTa, GPT) are further trained on domain-specific ancient Chinese corpora using Masked Language Modeling (MLM) for comprehension models and Causal Language Modeling (CLM) for generation models. This self-supervised training enriches the models with contextual representations tailored to classical Chinese syntax and terminology.
- Core assumption: The corpus contains sufficient domain-specific linguistic patterns that the base models can learn to represent effectively during continued pre-training.
- Evidence anchors:
  - [abstract]: “Our research findings highlight the efficacy of employing self-supervised methods to further train the models using classical text corpora, thus enhancing their capability to tackle downstream tasks.”
  - [section]: “The masked language model (MLM) is used to train the BERT and RoBERTa models, and the causal language model (CLM) is used to train the GPT model…”
- Break condition: If the domain corpus is too small or lacks diversity, the continued pre-training may not improve or may even degrade performance due to overfitting or insufficient linguistic variety.

### Mechanism 2
- Claim: The dual training on traditional and simplified ancient texts allows the models to generalize across both character sets without performance loss.
- Mechanism: By creating parallel corpora in traditional and simplified Chinese and merging them, the models are exposed to both scripts during pre-training, which helps them learn unified representations that work well for either form.
- Core assumption: The simplified and traditional versions of the texts are sufficiently aligned in content so that shared training does not introduce conflicting patterns.
- Evidence anchors:
  - [abstract]: “These models have been trained on an extensive dataset that encompasses both simplified and traditional Chinese characters…”
  - [section]: “the two data are merged to form the mixed data of traditional and simplified, and the model is trained using the traditional, simplified and mixed data respectively.”
- Break condition: If the simplified and traditional versions diverge significantly in meaning or structure, the mixed training may confuse the model and hurt performance on one or both scripts.

### Mechanism 3
- Claim: The performance advantage over general models is due to both increased vocabulary coverage and domain-adapted embeddings.
- Mechanism: The study expands the vocabulary of the baseline models with domain-specific ancient Chinese terms and then continues pre-training so that embeddings for both common and rare ancient terms are better aligned with their usage context.
- Core assumption: Simply adding vocabulary entries without retraining embeddings would not yield the same performance gains; continued pre-training is necessary to integrate the new tokens into the model’s semantic space.
- Evidence anchors:
  - [section]: “the word list is expanded on the basis of the word list of the baseline model, and the expansion of the word list is shown in Table 1. After that, the cleaned Daizhige corpus is divided into training set and validation set… the model continues to be trained…”
  - [section]: “compared with the baseline model, the perplexity of the Guji series models… decreases significantly…”
- Break condition: If the vocabulary expansion is minimal or the continued training is too short, the embeddings may not adapt sufficiently and performance gains will be limited.

## Foundational Learning

- Concept: **Self-supervised pre-training with masked and causal language modeling**
  - Why needed here: Ancient Chinese texts lack large annotated datasets, so self-supervised objectives allow the model to learn rich representations without manual labeling.
  - Quick check question: What is the key difference between MLM and CLM objectives, and why does each suit comprehension vs. generation tasks?

- Concept: **Domain adaptation via continued pre-training**
  - Why needed here: General-purpose language models are trained on modern text; continued pre-training on classical corpora shifts the model’s knowledge toward the linguistic patterns of ancient texts.
  - Quick check question: How does continued pre-training differ from fine-tuning, and why is it more effective when labeled data is scarce?

- Concept: **Vocabulary expansion and embedding integration**
  - Why needed here: Ancient Chinese uses many characters and terms not present in standard vocabularies; expanding the vocabulary and retraining embeddings ensures these tokens are properly represented.
  - Quick check question: Why is it necessary to retrain embeddings after expanding the vocabulary, rather than just adding new token IDs?

## Architecture Onboarding

- Component map: Corpus cleaning and conversion between traditional/simplified scripts → vocabulary expansion on base models → continued pre-training using MLM (BERT/RoBERTa) or CLM (GPT) → task-specific fine-tuning → evaluation on held-out test sets.

- Critical path: Corpus preparation → vocabulary expansion → continued pre-training → task-specific fine-tuning → evaluation.

- Design tradeoffs: Using larger corpora and more parameters improves performance but increases compute cost; choosing MLM vs. CLM depends on the target task (understanding vs. generation); merging traditional and simplified data simplifies deployment but risks introducing noise if the scripts diverge.

- Failure signatures: High perplexity on validation data indicates over- or under-fitting during pre-training; poor F1 on downstream tasks despite low perplexity suggests a mismatch between pre-training and task objectives; performance gaps between traditional and simplified models may indicate imbalanced script representation.

- First 3 experiments:
  1. Pre-train a GujiBERT variant on only traditional corpus and evaluate perplexity vs. baseline to confirm domain adaptation effect.
  2. Fine-tune the same model on NER and compare F1 to a general BERT model to measure task-specific gains.
  3. Repeat with mixed traditional/simplified corpus to test cross-script generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GujiBERT and GujiGPT compare to other domain-specific language models in non-Chinese ancient text processing tasks?
- Basis in paper: [inferred] The paper focuses on Chinese ancient texts but mentions the potential for global dissemination of traditional culture, suggesting interest in cross-cultural applications.
- Why unresolved: The study does not provide comparative analysis with models handling other ancient languages or scripts.
- What evidence would resolve it: Benchmarking GujiBERT and GujiGPT against models like AncientBERT (for Greek/Latin) or similar domain-specific models on parallel tasks in different languages.

### Open Question 2
- Question: What is the impact of model size (parameter count) on the performance of GujiBERT and GujiGPT in ancient text processing tasks?
- Basis in paper: [inferred] The paper mentions the importance of model architecture and training data but does not explore the effect of varying model sizes on performance.
- Why unresolved: The study does not experiment with different parameter scales to determine optimal model size for ancient text tasks.
- What evidence would resolve it: Conducting experiments with scaled versions of GujiBERT and GujiGPT (e.g., small, base, large) to analyze performance trade-offs.

### Open Question 3
- Question: How do GujiBERT and GujiGPT handle ambiguity and polysemy in ancient Chinese texts compared to modern Chinese?
- Basis in paper: [inferred] The paper discusses the models' capabilities in various NLP tasks but does not address their handling of linguistic ambiguities specific to ancient texts.
- Why unresolved: The study lacks an analysis of the models' disambiguation abilities in contexts where ancient and modern meanings differ.
- What evidence would resolve it: Evaluating the models on datasets containing ambiguous terms with both ancient and modern interpretations to measure accuracy in context resolution.

### Open Question 4
- Question: What is the long-term sustainability and maintenance plan for the GujiBERT and GujiGPT models as new ancient texts are digitized?
- Basis in paper: [explicit] The authors mention the need for new breakthroughs in model construction and the diminishing returns of training on static classical corpora.
- Why unresolved: The paper does not outline strategies for updating the models with new data or adapting to evolving research needs.
- What evidence would resolve it: Proposing and implementing a framework for continuous learning and model updates as new ancient texts become available.

## Limitations

- Corpus details: The exact preprocessing steps for the Daizhige corpus, including how traditional and simplified text conversions were performed and merged, are not fully specified. This limits reproducibility and makes it difficult to assess potential biases introduced during data preparation.

- Fine-tuning specifics: While continued pre-training hyperparameters are given, fine-tuning details for downstream tasks (learning rates, epochs, batch sizes) are sparse. This ambiguity could lead to variability in results when reproducing the study.

- Evaluation scope: Most reported results are F1 scores for NER and sentence breaking, with fewer details on other tasks like translation (BLEU) or POS tagging. The paper lacks comprehensive ablation studies isolating the impact of individual factors (e.g., corpus size, vocabulary expansion, script mixing) on performance.

## Confidence

- **High confidence**: Claims about the general efficacy of continued pre-training for domain adaptation in ancient Chinese texts. The mechanism is well-grounded in self-supervised learning literature, and the reported perplexity reductions support this.

- **Medium confidence**: Claims about cross-script generalization (traditional vs. simplified) and the relative importance of corpus size, font choice, and initial model selection. These are supported by results but lack detailed ablation studies or error analysis to rule out confounding factors.

- **Low confidence**: Claims about the model’s ability to handle tasks beyond NER and sentence breaking (e.g., automatic translation, word segmentation) due to limited quantitative results and lack of comparative baselines.

## Next Checks

1. **Reproduce pre-training perplexity**: Re-run the continued pre-training step using the same hyperparameters and a subset of the Daizhige corpus; verify that perplexity decreases as reported, confirming effective domain adaptation.

2. **Fine-tune and evaluate NER on held-out data**: Use the released or reconstructed model to fine-tune on a standard NER dataset for ancient Chinese; compare F1 scores to the paper’s reported 93.53% to confirm reproducibility.

3. **Cross-script performance gap analysis**: Train separate models on only traditional, only simplified, and mixed corpora; measure and compare F1 scores for NER and sentence breaking to quantify the impact of script mixing and identify any degradation in either script.