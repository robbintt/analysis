---
ver: rpa2
title: Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal
  Medical Diagnosis
arxiv_id: '2310.09909'
source_url: https://arxiv.org/abs/2310.09909
tags:
- image
- case
- gpt-4v
- images
- tissue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive qualitative evaluation of OpenAI's
  GPT-4V(ision) for multimodal medical diagnosis across 17 human body systems and
  8 imaging modalities. While GPT-4V demonstrates proficiency in recognizing medical
  image modalities and anatomical structures, it faces significant challenges in disease
  diagnosis and generating accurate clinical reports.
---

# Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis

## Quick Facts
- arXiv ID: 2310.09909
- Source URL: https://arxiv.org/abs/2310.09909
- Reference count: 30
- Primary result: GPT-4V demonstrates proficiency in recognizing medical image modalities and anatomy but faces significant challenges in disease diagnosis and clinical report generation.

## Executive Summary
This study presents a comprehensive qualitative evaluation of OpenAI's GPT-4V(ision) for multimodal medical diagnosis across 17 human body systems and 8 imaging modalities. While GPT-4V demonstrates strong capabilities in recognizing medical image modalities and anatomical structures, it faces significant challenges in disease diagnosis and generating accurate clinical reports. The model shows inconsistent performance across different imaging modalities, with rare modalities like ultrasound and nuclear medicine posing particular difficulties. Despite its ability to identify medical devices and recognize text within images, GPT-4V struggles with localizing anatomical structures and anomalies, showing poor performance with an average IOU score of only 0.16.

## Method Summary
The study conducted a manual case-level evaluation of GPT-4V using 92 radiology cases from 17 body systems with 266 images across multiple modalities, plus 20 pathology cases. Each case was tested through the GPT-4V online chat interface with one or two questions in fresh chat windows. The evaluation covered seven key tasks: modality and anatomy recognition, disease diagnosis, report generation, medical device identification, multi-image analysis, patient history integration, and localization. For localization tasks, a step-by-step approach with IOU score calculation was used. Reference answers were obtained from Radiopaedia for comparison.

## Key Results
- GPT-4V demonstrates strong proficiency in recognizing medical image modalities and anatomical structures across standard imaging types.
- The model faces significant challenges in disease diagnosis and generating accurate clinical reports, with performance degrading on rare modalities like ultrasound and nuclear medicine.
- GPT-4V shows poor localization performance with an average IOU score of only 0.16, despite its visual recognition capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V demonstrates strong modality and anatomy recognition across most medical imaging modalities.
- Mechanism: The model leverages its vision-language foundation to identify imaging types and anatomical regions through visual feature extraction and cross-modal alignment.
- Core assumption: GPT-4V has been exposed to diverse medical imagery during pretraining, enabling it to generalize across standard imaging types.
- Evidence anchors:
  - [abstract] "GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy"
  - [section] "GPT-4V can recognize the modality and anatomy of medical images. Modality Recognition...Anatomy Recognition"
  - [corpus] Weak evidence; only general biomedical vision papers, no direct GPT-4V medical imaging validation
- Break condition: Performance degrades significantly on rare modalities or unusual anatomical presentations not well-represented in pretraining data.

### Mechanism 2
- Claim: GPT-4V struggles with precise localization and disease diagnosis despite visual recognition capabilities.
- Mechanism: While the model can identify visual features and patterns, it lacks the spatial reasoning and clinical knowledge integration needed for accurate anatomical localization and diagnostic conclusions.
- Core assumption: Visual recognition does not automatically translate to spatial understanding or clinical reasoning required for medical diagnosis.
- Evidence anchors:
  - [abstract] "it faces significant challenges in disease diagnosis and generating accurate clinical reports...struggles with localizing anatomical structures and anomalies"
  - [section] "GPT-4V cannot localize the anatomical structures or anomalies in medical images...average IOU score of only 0.16"
  - [corpus] No direct evidence; general observation about multimodal model limitations in medical domains
- Break condition: Model consistently fails to provide accurate bounding boxes or diagnoses even with multiple attempts and guidance.

### Mechanism 3
- Claim: GPT-4V's performance is highly dependent on prompt engineering, patient history, and visual context.
- Mechanism: The model's responses can be significantly improved through careful prompt design, inclusion of patient context, and use of visual markers that guide attention.
- Core assumption: GPT-4V processes visual and textual information together, so contextual cues substantially influence its outputs.
- Evidence anchors:
  - [abstract] "GPT-4V's prediction heavily relies on patient's medical history"
  - [section] "GPT-4V can change answers with guidance in multi-round interaction...performance variation and inconsistency"
  - [corpus] Weak evidence; only general observations about prompt engineering effects on multimodal models
- Break condition: Model remains inconsistent even with optimized prompts and contextual information.

## Foundational Learning

- Concept: Multimodal Vision-Language Models
  - Why needed here: Understanding how GPT-4V integrates visual and textual information is crucial for interpreting its medical imaging performance
  - Quick check question: How does a multimodal model like GPT-4V process both images and text simultaneously to generate responses?

- Concept: Medical Image Analysis Fundamentals
  - Why needed here: Knowledge of medical imaging modalities, anatomical structures, and diagnostic criteria is essential for evaluating GPT-4V's capabilities
  - Quick check question: What are the key differences between CT, MRI, X-ray, and ultrasound imaging in terms of tissue contrast and diagnostic applications?

- Concept: Localization and Segmentation Metrics
  - Why needed here: Understanding evaluation metrics like IOU (Intersection over Union) scores is critical for interpreting GPT-4V's localization performance
  - Quick check question: How is the IOU score calculated and what does a score of 0.16 indicate about localization accuracy?

## Architecture Onboarding

- Component map: Vision encoder -> Text encoder -> Cross-modal attention -> Generation decoder -> Safety mechanisms
- Critical path:
  1. Image preprocessing and normalization
  2. Visual feature extraction
  3. Context integration with prompt and patient history
  4. Cross-modal reasoning and analysis
  5. Structured response generation
  6. Safety filtering
- Design tradeoffs:
  - Input limitations (max 4 images) vs. comprehensive case analysis
  - Generalist pretraining vs. specialized medical knowledge
  - Safety constraints vs. diagnostic utility
  - Spatial reasoning vs. visual pattern recognition
- Failure signatures:
  - Normal hallucination (claiming normal findings when abnormalities exist)
  - Inconsistent responses to same image with different prompts
  - Poor localization performance (low IOU scores)
  - Rare modality confusion (ultrasound, nuclear medicine)
- First 3 experiments:
  1. Test modality recognition consistency across 10 random medical images from different modalities
  2. Evaluate localization accuracy on a standardized dataset with known ground truth bounding boxes
  3. Compare responses with and without patient history for the same medical imaging case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications would be needed to improve GPT-4V's performance on rare medical imaging modalities?
- Basis in paper: [explicit] The paper notes that GPT-4V shows significantly worse performance on rare imaging modalities like ultrasound and nuclear medicine compared to common ones like CT and MRI.
- Why unresolved: The paper only demonstrates GPT-4V's limitations without exploring potential solutions or architectural changes that could address these weaknesses.
- What evidence would resolve it: Comparative studies showing performance improvements after implementing targeted architectural modifications for rare modalities.

### Open Question 2
- Question: How does GPT-4V's performance on medical imaging tasks compare to specialized medical vision models trained specifically on those tasks?
- Basis in paper: [explicit] The paper concludes that GPT-4V is "far from being used to effectively support real-world medical applications" but doesn't compare its performance to existing specialized models.
- Why unresolved: The evaluation only benchmarks GPT-4V against clinical expectations without establishing a baseline comparison to domain-specific alternatives.
- What evidence would resolve it: Head-to-head performance comparison studies between GPT-4V and established medical imaging models across various clinical tasks.

### Open Question 3
- Question: What are the minimum dataset requirements for GPT-4V to achieve reliable performance on medical imaging tasks?
- Basis in paper: [inferred] The paper notes GPT-4V's poor performance on rare modalities and suggests this may be due to limited training data exposure.
- Why unresolved: The paper doesn't investigate the relationship between dataset size, diversity, and model performance on medical tasks.
- What evidence would resolve it: Systematic studies varying training dataset size and composition to determine performance thresholds.

## Limitations
- The study relies on qualitative evaluation methodology rather than standardized quantitative metrics for most tasks.
- Sample size of 92 radiology cases and 20 pathology cases may not capture full clinical complexity.
- Reference answers from Radiopaedia may not represent all possible correct interpretations in clinical practice.

## Confidence
- High Confidence: GPT-4V's proficiency in recognizing medical image modalities and anatomical structures is well-supported by multiple test cases and consistent observations.
- Medium Confidence: The findings about GPT-4V's struggles with disease diagnosis and clinical report generation are supported by qualitative analysis but could benefit from more standardized quantitative metrics.
- Low Confidence: The extent of GPT-4V's dependence on patient history and prompt engineering effects requires further systematic investigation with controlled experiments.

## Next Checks
1. Conduct a quantitative evaluation using standardized medical imaging datasets (e.g., MIMIC-CXR, RSNA challenges) with established metrics like AUC-ROC for diagnosis tasks.
2. Systematically test GPT-4V's performance across different prompt variations and patient history contexts using a controlled experimental design.
3. Implement a multi-expert review process for evaluating GPT-4V's responses to establish inter-rater reliability and reduce potential bias in qualitative assessment.