---
ver: rpa2
title: Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology
  Datasets
arxiv_id: '2310.01438'
source_url: https://arxiv.org/abs/2310.01438
tags:
- data
- minds
- cancer
- multimodal
- amazon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of integrating and managing large
  volumes of heterogeneous oncology data from diverse public sources to enable multimodal
  machine learning in cancer research. It proposes MINDS (Multimodal Integration of
  Oncology Data System), a scalable cloud-based data lakehouse architecture built
  on AWS.
---

# Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets

## Quick Facts
- arXiv ID: 2310.01438
- Source URL: https://arxiv.org/abs/2310.01438
- Reference count: 36
- Primary result: MINDS reduces storage from petabytes to 25.85 MB while enabling real-time multimodal oncology data integration and querying

## Executive Summary
This paper introduces MINDS (Multimodal Integration of Oncology Data System), a cloud-based data lakehouse architecture that addresses the challenge of integrating heterogeneous oncology data from multiple public repositories for machine learning research. Built on AWS infrastructure, MINDS consolidates over 41,000 open-access cancer cases from sources like GDC into a unified, patient-centric framework. The system combines the flexibility of a data lake with the performance of a data warehouse, enabling efficient cohort building, automated unstructured data extraction, and integrated analytics through SQL-based querying and QuickSight visualizations.

## Method Summary
MINDS is implemented as a three-stage pipeline: (1) Data Acquisition - pulling structured/semi-structured data from repositories like GDC using APIs, (2) Data Processing - using AWS Glue crawlers to automatically catalog data into a data lake and transform it into a structured warehouse via ETL, and (3) Data Serving - providing SQL querying through Athena and visualization via QuickSight while enabling on-demand retrieval of unstructured data from connected repositories. The architecture leverages S3 for raw storage, Glue for cataloging and ETL, Lambda for automation, Redshift for analytics, and RDS for structured data storage.

## Key Results
- Reduces storage requirements from petabytes to 25.85 MB through on-demand unstructured data retrieval
- Enables real-time updates by pulling source data directly from repositories like GDC
- Provides unified SQL-based querying across clinical, imaging, and genomic data modalities
- Supports rapid cohort construction with automated unstructured data extraction from connected repositories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MINDS enables real-time, up-to-date mapping of cancer cases across data repositories by pulling source data directly from repositories like GDC
- Mechanism: Direct API pulls from source repositories bypass the latency of intermediate mapping services like CDA, ensuring the latest case information is always available
- Core assumption: Source repositories provide stable, accessible APIs that can be queried programmatically without excessive throttling or authentication barriers
- Evidence anchors: [abstract] "MINDS pulls source data directly from repositories like GDC to ensure real-time, up-to-date mapping of all cases" and [section] "MINDS is designed as an end-to-end platform for users to build integrated multimodal datasets themselves rather than a fixed service"
- Break condition: If source APIs become rate-limited, unstable, or require additional authentication steps, real-time mapping will fail or degrade

### Mechanism 2
- Claim: Schema-on-write (data warehouse) combined with schema-on-read (data lake) provides both structured performance and raw flexibility for multimodal oncology data
- Mechanism: Data lakes store raw, heterogeneous data with minimal upfront structuring, while data warehouses enforce predefined schemas for high-performance querying. This dual approach allows flexible raw storage and efficient structured analysis
- Core assumption: The cost and complexity of maintaining both systems is justified by the analytical benefits they provide
- Evidence anchors: [section] "MINDS adopts a common two-tier data architecture, a data lake, and a data warehouse [22] to process data and derive meaningful insights efficiently" and [section] "The flexible schema of the data lake provides scalable storage for varied data types, including imaging, -omics, and electronic health records. Meanwhile, the warehouse's performance, governance, and extract-transform-load (ETL) capabilities facilitate structured access and analysis"
- Break condition: If the ETL pipeline becomes a bottleneck or the cost of maintaining both systems outweighs benefits, the dual architecture may need simplification

### Mechanism 3
- Claim: Automated ETL via AWS Glue crawlers and Lambda functions enables continuous, real-time updates without manual intervention
- Mechanism: Glue crawlers automatically catalog new data in S3, and Lambda triggers crawlers on new file arrival, ensuring the data lake is always synchronized with source repositories
- Core assumption: AWS services scale reliably and cost-effectively for the expected data volume and update frequency
- Evidence anchors: [section] "To achieve this, we use AWS Lambda serverless compute [27] to trigger Glue crawlers automatically whenever new data lands in the S3 bucket" and [section] "This ensures our data lake is always up-to-date with the latest data without explicit manual synchronization"
- Break condition: If Lambda throttling or Glue crawler limits are reached, updates will fail or lag, breaking the real-time synchronization promise

## Foundational Learning

- Concept: Data Lake vs. Data Warehouse
  - Why needed here: Understanding the distinction explains why MINDS uses both systems and how they complement each other
  - Quick check question: What is the primary difference between schema-on-write and schema-on-read data management approaches?

- Concept: Extract, Transform, Load (ETL) Pipeline
  - Why needed here: ETL is the core process that moves raw data from repositories into the structured warehouse, enabling analysis
  - Quick check question: In the MINDS pipeline, at which stage does the transformation from raw to structured data occur?

- Concept: Multimodal Data Integration
  - Why needed here: Integrating clinical, imaging, and molecular data modalities is the central goal of MINDS and requires understanding how different data types can be harmonized
  - Quick check question: Why is it challenging to integrate structured clinical data with unstructured imaging data in oncology research?

## Architecture Onboarding

- Component map: S3 Ingest Bucket → AWS Glue Crawler → Data Lake (Glue Catalog) → AWS Glue ETL → RDS MySQL + Redshift → QuickSight Dashboard + Athena Queries → Unstructured Data APIs (GDC, IDC, PDC)
- Critical path: Data Acquisition → ETL Processing → Structured Storage → Cohort Query → Unstructured Data Retrieval
- Design tradeoffs: Using both RDS and Redshift balances low-latency queries with high-performance analytics but increases complexity and cost; pulling unstructured data on-demand avoids storage bloat but introduces latency for cohort assembly; real-time synchronization ensures freshness but depends on stable source APIs
- Failure signatures: Delayed or missing cohort data → check Lambda triggers and Glue crawler logs; slow query performance → check Redshift cluster health and query optimization; inconsistent data across modalities → verify ETL schema mapping and data versioning
- First 3 experiments: 1) Deploy MINDS with a small synthetic dataset to validate the full ETL pipeline from S3 to Redshift, 2) Build a test cohort using SQL queries and verify unstructured data retrieval via APIs, 3) Load a subset of real GDC cases and test dashboard visualizations in QuickSight

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MINDS incorporate controlled-access data while maintaining privacy and security?
- Basis in paper: [inferred] The paper discusses MINDS' current focus on public datasets and mentions future plans to include controlled data
- Why unresolved: The paper doesn't provide specific details on how to integrate controlled data without compromising privacy
- What evidence would resolve it: A detailed implementation plan showing secure data integration methods and compliance with privacy regulations

### Open Question 2
- Question: What are the potential challenges and solutions for scaling MINDS to handle exponentially growing data volumes?
- Basis in paper: [explicit] The paper emphasizes MINDS' scalability and ability to handle data growth but doesn't delve into specific challenges or solutions
- Why unresolved: The paper mentions scalability as a key feature but lacks detailed strategies for addressing potential scaling issues
- What evidence would resolve it: Case studies or simulations demonstrating MINDS' performance under various data growth scenarios and proposed solutions

### Open Question 3
- Question: How can MINDS enhance its analytics and visualization capabilities to provide more actionable insights for researchers?
- Basis in paper: [inferred] The paper highlights MINDS' current analytics features but suggests room for improvement
- Why unresolved: While the paper mentions current capabilities, it doesn't provide specifics on potential enhancements or new features
- What evidence would resolve it: A roadmap of proposed analytics features, user feedback on current limitations, and prototype implementations of new visualization tools

## Limitations
- Technical assumptions about stable, high-throughput access to public oncology data repositories like GDC
- Scalability gaps when handling very large cohorts (>100,000 cases) due to API quota limits and network bandwidth constraints
- Generalizability concerns as performance across other oncology data sources beyond GDC remains unproven

## Confidence
- High Confidence: The data lakehouse architecture combining S3, Glue, and Redshift is a well-established pattern for multimodal analytics
- Medium Confidence: Claims about real-time updates depend on external factors (API stability, Lambda/GGlue scaling limits) not fully tested under production loads
- Low Confidence: The assertion that MINDS "facilitates reproducible research" lacks specific validation without version control for source data snapshots

## Next Checks
1. API Reliability Testing: Simulate high-frequency data pulls from GDC APIs to measure rate limits, response times, and failure modes under sustained load
2. Cross-Repository Schema Mapping: Test the ETL pipeline with heterogeneous data sources beyond GDC (e.g., TCGA, CPTAC) to validate schema harmonization
3. Cohort Retrieval Performance: Build test cohorts of varying sizes (1,000 to 100,000 cases) and measure end-to-end latency for unstructured data retrieval via GDC, IDC, and PDC APIs