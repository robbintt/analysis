---
ver: rpa2
title: 'MISA: Unveiling the Vulnerabilities in Split Federated Learning'
arxiv_id: '2312.11026'
source_url: https://arxiv.org/abs/2312.11026
tags:
- data
- learning
- bottom
- misa
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of split federated learning
  (SFL) to poisoning attacks, challenging the common belief that SFL is robust against
  such attacks. The authors propose MISA, a novel poisoning attack that simultaneously
  poisons both the top and bottom models in SFL, causing a misalignment in the global
  model and leading to a significant drop in accuracy.
---

# MISA: Unveiling the Vulnerabilities in Split Federated Learning

## Quick Facts
- arXiv ID: 2312.11026
- Source URL: https://arxiv.org/abs/2312.11026
- Reference count: 0
- Primary result: MISA poisoning attack causes significant accuracy drop in split federated learning by poisoning both top and bottom models simultaneously.

## Executive Summary
This paper challenges the prevailing belief that split federated learning (SFL) is inherently robust against poisoning attacks. The authors propose MISA, a novel attack that simultaneously poisons both the top and bottom models in SFL, causing a misalignment in the global model and leading to significant accuracy degradation. MISA employs a two-pronged approach: indirectly poisoning the top model by crafting malicious smashed data using mean and variance perturbation, and directly poisoning the bottom model through a Min-Sum optimization problem. The attack uses Thompson Sampling to dynamically determine the optimal perturbation direction and a Search and Locate algorithm to find the optimal perturbation magnitude.

## Method Summary
The MISA attack works by poisoning both components of the split federated learning architecture. For the top model, attackers craft malicious smashed data by blending each original sample with the mean of same-class samples and shifting by 1.5 standard deviations. For the bottom model, the attack formalizes poisoning as a Min-Sum optimization problem that ensures the malicious model appears no more distant than benign models from any other benign model. Thompson Sampling is used to dynamically select the optimal perturbation direction from three classical options, while a Search and Locate algorithm finds the optimal perturbation magnitude.

## Key Results
- MISA achieves higher accuracy drop compared to state-of-the-art attacks under various conditions
- Attack effectiveness increases with deeper model splits (V1 to V4)
- MISA maintains effectiveness even when robust defenses (Krum, TrMean, Median) are employed
- Attack performance is consistent across different data distributions and attacker ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MISA poisons the top model indirectly by crafting malicious smashed data using mean and variance perturbation
- Mechanism: Attackers compute mean and standard deviation of all smashed data labeled with the same class, then create poisoned data by blending original samples with the mean and shifting by 1.5×δ, scaled by λ=0.05
- Core assumption: Smashed data approximates normal distribution; 1.5 standard deviation shift remains within natural variation bounds
- Evidence anchors:
  - [abstract] "MISA employs a two-pronged approach: indirectly poisoning the top model by crafting malicious smashed data using mean and variance perturbation"
  - [section] "For a smashed data point xi with label y, we poison it as follows: ˆxi = λxi + (1 − λ)¯x − 1.5δ"

### Mechanism 2
- Claim: MISA poisons the bottom model by solving Min-Sum optimization that ensures malicious model remains indistinguishable from benign outliers
- Mechanism: Attack formalizes malicious bottom model as minimize sum of squared distances to all benign models, subject to upper bound on max distance to any benign model
- Core assumption: FedAvg aggregation will accept malicious model if it lies within convex hull of benign updates
- Evidence anchors:
  - [abstract] "directly poisoning the bottom model by formalizing it as a Min-Sum optimization problem"
  - [section] "we formalize the poisoning of the bottom model as a Min-Sum problem, where the sum of squared distances of the malicious bottom model from all the benign models is upper-bounded by the sum of squared distances of any benign bottom model from the other benign bottom models"

### Mechanism 3
- Claim: MISA uses Thompson Sampling to dynamically select optimal perturbation direction
- Mechanism: Attack treats choosing among three perturbation directions as Multi-Armed Bandit problem, using Thompson Sampling to select direction with highest expected reward measured by cosine similarity drop
- Core assumption: Optimal perturbation direction varies across rounds; Thompson Sampling can adaptively discover it without prior knowledge
- Evidence anchors:
  - [abstract] "Thompson Sampling is introduced for dynamically determining the optimal perturbation direction"
  - [section] "we propose an adaptive algorithm to search for the perturbation direction. This approach dynamically selects the optimal perturbation direction for different scenarios"

## Foundational Learning

- Concept: Split Federated Learning (SFL) architecture
  - Why needed here: Understanding how model is split and how smashed data flows is essential to see how top and bottom models can be poisoned
  - Quick check question: In SFL, which server updates the top model and which updates the bottom model?

- Concept: Min-Sum optimization and Byzantine-robust aggregation
  - Why needed here: Attack's effectiveness depends on mathematical formulation ensuring malicious updates remain indistinguishable from benign outliers under aggregation
  - Quick check question: What is the core constraint in Min-Sum formulation that allows malicious model to avoid detection?

- Concept: Thompson Sampling for Multi-Armed Bandits
  - Why needed here: Dynamic perturbation direction search is key differentiator from prior attacks and relies on bandit algorithms
  - Quick check question: What is the reward metric used by Thompson Sampling in MISA?

## Architecture Onboarding

- Component map: Clients -> Fed Server (bottom model aggregation) -> Main Server (top model update)
- Critical path:
  1. Attackers generate poisoned smashed data using mean/variance perturbation
  2. Attackers solve Min-Sum optimization for bottom model direction/magnitude
  3. Both poisoned components are sent to servers
  4. Servers aggregate and update global models, causing misalignment
- Design tradeoffs:
  - Perturbation magnitude vs. stealth: Larger shifts improve attack but risk detection
  - Exploration vs. exploitation in Thompson Sampling: More exploration improves direction choice but may reduce immediate attack impact
  - Attacker ratio vs. impact: More attackers increase attack strength but also increase chance of detection
- Failure signatures:
  - Accuracy drop plateaus despite sustained attack
  - Sudden detection of outlier bottom models by robust defenses
  - Smashed data statistics deviating significantly from benign clients
- First 3 experiments:
  1. Test MISA with λ=0.05, δ perturbation on CIFAR-10 with 20% attackers, measure accuracy drop vs. baseline
  2. Vary split position (V1 to V4) and record attack impact to confirm increased effectiveness with deeper splits
  3. Evaluate MISA under different robust defenses (Krum, TrMean, Median) to quantify relative performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of MISA compare to other poisoning attacks when different aggregation algorithms are used in SFL?
- Basis in paper: [explicit] The paper mentions that MISA outperforms state-of-the-art attacks under different aggregation algorithms like Krum, TrMean, and Median, but does not provide a comprehensive comparison across all possible aggregation algorithms
- Why unresolved: The paper only evaluates a limited set of aggregation algorithms, and there could be other algorithms that might affect the performance of MISA differently
- What evidence would resolve it: Conducting experiments with a broader range of aggregation algorithms to compare the effectiveness of MISA against other poisoning attacks

### Open Question 2
- Question: How does the performance of MISA change with different model architectures and split positions in SFL?
- Basis in paper: [inferred] The paper evaluates MISA on a VGG-like model with specific split positions, but does not explore how the attack's effectiveness varies with different model architectures or alternative split positions
- Why unresolved: The paper's focus on a single model architecture and limited split positions leaves uncertainty about MISA's adaptability to other configurations
- What evidence would resolve it: Testing MISA on various model architectures and exploring a wider range of split positions to determine its robustness and adaptability

### Open Question 3
- Question: What are the implications of MISA on the privacy and security of SFL systems in real-world applications?
- Basis in paper: [inferred] While the paper demonstrates MISA's effectiveness in controlled experiments, it does not address the potential real-world implications for privacy and security in practical SFL deployments
- Why unresolved: The experimental setup may not fully capture the complexities and challenges of real-world SFL systems, leaving questions about MISA's impact on actual applications
- What evidence would resolve it: Conducting real-world case studies and simulations to assess MISA's impact on privacy and security in practical SFL scenarios

## Limitations

- Limited evaluation scope: Experiments focus primarily on CIFAR-10 dataset with VGG-like architecture, leaving generalizability to other datasets and models uncertain
- Implementation details missing: Critical components like the Search and Locate algorithm and Thompson Sampling reward function lack specific implementation specifications
- Defense resistance quantification: While claimed effective against robust defenses, the extent of resistance is not thoroughly quantified beyond a few percentage points of accuracy drop

## Confidence

- High Confidence: The mechanism of poisoning the top model through mean/variance perturbation and the Min-Sum optimization formulation for the bottom model are well-specified and theoretically sound
- Medium Confidence: The Thompson Sampling approach for selecting perturbation direction is a reasonable adaptation of bandit algorithms, but its specific implementation details are missing
- Low Confidence: The exact numerical results and comparative performance against baseline attacks cannot be fully verified without the complete experimental setup and code

## Next Checks

1. Implement the Search and Locate (SnL) algorithm for finding optimal perturbation magnitude, ensuring it converges to a solution that satisfies the Min-Sum constraints
2. Verify the Thompson Sampling implementation using cosine similarity between global model updates, and validate its ability to select optimal perturbation direction across multiple rounds
3. Test MISA against a broader range of robust defenses (e.g., FoolsGold, RFA) and quantify the attack's effectiveness when the detection threshold is tightened