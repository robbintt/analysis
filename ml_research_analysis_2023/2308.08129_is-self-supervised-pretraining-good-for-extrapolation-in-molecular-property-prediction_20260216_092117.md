---
ver: rpa2
title: Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property
  Prediction?
arxiv_id: '2308.08129'
source_url: https://arxiv.org/abs/2308.08129
tags:
- data
- extrapolation
- pretraining
- training
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of self-supervised pretraining
  for improving extrapolation in molecular property prediction. The authors propose
  a novel experimental framework where data distributions are defined based on output
  labels rather than input features, addressing the practical need to predict unobserved
  physical property values in material development.
---

# Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?

## Quick Facts
- arXiv ID: 2308.08129
- Source URL: https://arxiv.org/abs/2308.08129
- Authors: 
- Reference count: 30
- Self-supervised pretraining improves relative property ranking in molecular property prediction, though absolute values remain difficult to predict

## Executive Summary
This study investigates whether self-supervised pretraining can improve extrapolation performance for molecular property prediction. The authors propose a novel experimental framework where extrapolation is defined based on output label distributions rather than input features, addressing the practical need to predict unobserved physical property values in material development. Using the PCQM4Mv2 dataset for HOMO-LUMO gap prediction, they compare seven training methods including baseline and variants with pretraining using three different pretext tasks. The key finding is that while models cannot accurately extrapolate absolute property values, self-supervised pretraining significantly improves the ability to learn relative tendencies of unobserved property values, as measured by rank correlation coefficients improving from 0.255 to 0.516.

## Method Summary
The study uses a Graphormer model with 12 encoder layers and 512-dimensional features for molecular property prediction. The experimental framework defines extrapolation based on output label distributions, where molecules with extreme property values are held out during training. Seven training methods are compared: a baseline without pretraining, and six variants with pretraining using three pretext tasks (node-level masking, geometric structure prediction, and motif prediction) with and without validation data. The pretraining stage runs for 30 epochs followed by fine-tuning for 80 epochs using the AdamW optimizer. Evaluation uses both Mean Absolute Error (MAE) and rank correlation coefficient to assess both absolute prediction accuracy and relative ordering capabilities.

## Key Results
- Models cannot accurately extrapolate absolute HOMO-LUMO gap values (baseline MAE of 0.692 remains around 0.7 across methods)
- Self-supervised pretraining improves rank correlation coefficients from 0.255 (baseline) to 0.516, demonstrating better relative property ranking
- Utilizing unlabeled validation data during pretraining provides additional benefits, particularly for node-level and geometric structure prediction tasks
- Pretraining with validation data improves rank correlation from almost 0.5 to 0.4 when removed, showing its importance for extrapolation

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pretraining helps models learn relative orderings of molecular property values even when absolute extrapolation fails. Pretraining on unlabeled molecular graphs builds rich structural representations that encode relative chemical relationships, enabling the model to rank molecules by property values despite poor absolute prediction accuracy. This works because structural features learned during pretraining transfer to downstream regression tasks and capture relative property tendencies. The mechanism breaks if pretraining pretext tasks do not capture meaningful structural relationships.

### Mechanism 2
Utilizing both labeled training data and unlabeled validation data during pretraining improves extrapolation performance. Exposure to validation distribution during pretraining prevents overfitting to training distribution and allows the model to learn features relevant to unseen property ranges. This works because validation data contains distributional information about extreme property values that training data lacks. The mechanism breaks if validation data distribution is too different from true extrapolation region, potentially misleading rather than helping the model.

### Mechanism 3
Pretraining helps mitigate the inability of standard models to extrapolate non-linear functions beyond training distribution. Self-supervised pretraining provides a different optimization objective that encourages learning non-linear representations before fine-tuning on the regression task. This works because non-linear function approximation learned during pretraining transfers to improved regression performance. The mechanism breaks if pretraining pretext tasks are too simple or structurally unrelated to the target task.

## Foundational Learning

- **Label-based data distribution for extrapolation**: Why needed here: Unlike traditional input-feature-based extrapolation, material property prediction requires predicting unseen label values (extreme properties) rather than new input types. Quick check question: How does defining extrapolation based on output labels differ from input-based definitions in terms of practical application to material discovery?

- **Self-supervised learning on graphs**: Why needed here: Molecular data has abundant unlabeled structures but expensive property labels, making self-supervised pretraining ideal for leveraging this structural data. Quick check question: What are the three pretext tasks used in this study and how do they capture different aspects of molecular structure?

- **Rank correlation as evaluation metric**: Why needed here: Absolute prediction error remains high, but relative ordering of molecules by property is practically valuable for material screening. Quick check question: Why might rank correlation be more informative than MAE for evaluating extrapolation performance in this context?

## Architecture Onboarding

- **Component map**: Graphormer encoder (12 layers, 512-dim features) → Linear regression head; Pretraining module with three pretext tasks; Training pipeline with pretraining + fine-tuning stages
- **Critical path**: Data split (forward holdout) → Pretraining (30 epochs) → Fine-tuning (80 epochs) → Evaluation (MAE and rank correlation)
- **Design tradeoffs**: Using validation data in pretraining improves extrapolation but risks overfitting; simpler pretext tasks may be more robust but less informative
- **Failure signatures**: MAE remains high (>0.6) while rank correlation improves; correlation drops sharply after early epochs indicating overfitting
- **First 3 experiments**:
  1. Run baseline (no pretraining) to establish performance floor and confirm inability to predict values >8
  2. Implement pretraining with node-level masking only, compare MAE and rank correlation
  3. Test pretraining with and without validation data for geometric structure prediction task

## Open Questions the Paper Calls Out

### Open Question 1
Why does the model's performance degrade after the first few epochs during fine-tuning, as observed in Figure 9? The paper notes that after 10 epochs, correlation coefficients fall into negative values for most methods, with a sharp drop particularly after the first 5 epochs. This remains unresolved as the paper suggests this might be due to overfitting to the training data, but doesn't provide a definitive explanation or solution for this phenomenon. Experiments comparing different regularization techniques, learning rate schedules, or early stopping criteria would resolve this question.

### Open Question 2
Would incorporating three-dimensional molecular coordinates (beyond the two-dimensional graphs used) improve extrapolation performance for HOMO-LUMO gap prediction? The PCQM4Mv2 dataset includes both 2D graph structures and 3D coordinates, but the paper only uses the 2D graphs, suggesting potential for improvement. This remains unresolved as the paper focuses exclusively on 2D graph structures without exploring whether the additional 3D spatial information could enhance extrapolation capabilities. Direct comparison of models trained with 2D-only versus 2D+3D input features on the same extrapolation task would resolve this question.

### Open Question 3
How does the choice of self-supervised pretext task affect extrapolation performance for different types of molecular properties beyond HOMO-LUMO gaps? The paper tests three different pretext tasks (node-level masking, geometric structure prediction, and motif prediction) specifically for HOMO-LUMO gap prediction. This remains unresolved as while the paper shows these tasks have varying effectiveness for one specific property, it doesn't explore whether the optimal task selection generalizes to other molecular properties or if different properties benefit from different pretext tasks. Systematic evaluation of the three pretext tasks across multiple molecular property prediction tasks would resolve this question.

## Limitations
- The study relies on a single dataset (PCQM4Mv2) and specific molecular property (HOMO-LUMO gap), limiting generalizability to other molecular properties or materials domains
- The definition of extrapolation based on output label distributions creates an artificial scenario where models never encounter extreme values during training, potentially underestimating real-world extrapolation capabilities
- The three pretext tasks, while showing differential performance, were not systematically compared for their relative effectiveness in capturing transferable structural information

## Confidence
**High Confidence**: Pretraining improves relative property ranking while absolute prediction accuracy remains poor
**Medium Confidence**: Utilizing validation data during pretraining provides additional benefits
**Low Confidence**: Non-linearity learning during pretraining explains extrapolation improvements

## Next Checks
1. Test pretraining effectiveness across multiple molecular properties beyond HOMO-LUMO gap to assess generalizability of relative ranking improvements
2. Compare the three pretext tasks systematically using ablation studies to identify which structural aspects most contribute to extrapolation performance
3. Implement an alternative extrapolation evaluation using input-feature-based distribution shifts to validate whether label-based extrapolation captures the full scope of real-world prediction challenges