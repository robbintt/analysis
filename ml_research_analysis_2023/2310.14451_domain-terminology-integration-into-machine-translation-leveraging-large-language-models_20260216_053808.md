---
ver: rpa2
title: 'Domain Terminology Integration into Machine Translation: Leveraging Large
  Language Models'
arxiv_id: '2310.14451'
source_url: https://arxiv.org/abs/2310.14451
tags:
- translation
- data
- language
- term
- terms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a system for the WMT 2023 Terminology Shared
  Task, focusing on integrating pre-approved domain terminology into machine translation
  for three language pairs: German-to-English, English-to-Czech, and Chinese-to-English.
  The authors propose a four-step approach that leverages large language models (LLMs).'
---

# Domain Terminology Integration into Machine Translation: Leveraging Large Language Models

## Quick Facts
- arXiv ID: 2310.14451
- Source URL: https://arxiv.org/abs/2310.14451
- Reference count: 14
- Key outcome: Nearly doubled terminology integration from 36.67% to 72.88% using LLM-based post-editing

## Executive Summary
This paper presents a system for the WMT 2023 Terminology Shared Task that leverages large language models to integrate pre-approved domain terminology into machine translation for three language pairs. The authors propose a four-step approach combining synthetic data generation, mixed fine-tuning, and LLM-based post-editing. The system significantly improves terminology adherence while maintaining translation quality, demonstrating the effectiveness of LLMs in terminology integration tasks.

## Method Summary
The authors propose a four-step approach: First, they use ChatGPT to generate bilingual synthetic data incorporating the provided terminology. Second, they fine-tune OPUS MT models on a mix of synthetic terminology data and original generic training data. Third, they generate translations using the fine-tuned models. Finally, they employ ChatGPT for terminology-constrained automatic post-editing on translations missing required terms. The system uses beam_size=4 for NLLB-200 evaluation and specific hyperparameters for fine-tuning (learning_rate=2e-5, batch_size=32, weight_decay=0.01, num_train_epochs=1).

## Key Results
- Terminology adherence increased from 36.67% to 72.88% on blind dataset
- No degradation in translation quality metrics (BLEU, chrF++, COMET)
- Successful integration for all three language pairs (German-to-English, English-to-Czech, Chinese-to-English)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating bilingual synthetic data with LLMs provides domain-specific parallel examples that improve fine-tuning.
- Mechanism: ChatGPT produces both source and target sentence pairs containing pre-approved terminology, creating aligned synthetic data for the target domain.
- Core assumption: LLM can generate linguistically valid parallel data that accurately incorporates specified terms.
- Evidence anchors: Abstract states use of LLM for bilingual synthetic data; section 2.1 describes ChatGPT usage; weak evidence from related work on synthetic data generation.

### Mechanism 2
- Claim: Mixed fine-tuning on synthetic and generic data adapts MT model to domain while preserving general fluency.
- Mechanism: OPUS model trained on mix of synthetic terminology data and randomly sampled generic data balances domain adaptation with general translation ability.
- Core assumption: Generic model can integrate domain patterns from synthetic data without catastrophic forgetting.
- Evidence anchors: Abstract describes mixed fine-tuning approach; section 2.2 details training procedure; moderate evidence from related work on mixed fine-tuning.

### Mechanism 3
- Claim: LLM-based terminology-constrained post-editing inserts missing terms without degrading translation quality.
- Mechanism: ChatGPT post-editing prompts insert missing pre-approved terms into MT outputs, refining for terminology adherence.
- Core assumption: LLM can accurately identify and insert missing terminology coherently.
- Evidence anchors: Abstract describes LLM post-editing; section 2.3 explains post-editing process; strong evidence from results showing increased terminology usage.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their capabilities in text generation
  - Why needed here: System relies on LLMs (ChatGPT) for both synthetic data generation and post-editing
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and when might each be used?

- Concept: Fine-tuning transformer-based MT models
  - Why needed here: OPUS model is fine-tuned on mixed data to adapt to domain
  - Quick check question: What is the purpose of using both synthetic and generic data in mixed fine-tuning?

- Concept: Terminology-constrained decoding and post-editing
  - Why needed here: System must ensure pre-approved terms are used in translations
  - Quick check question: Why might post-editing be preferred over direct terminology-constrained decoding?

## Architecture Onboarding

- Component map: ChatGPT (synthetic data) -> OPUS MT model (fine-tuning) -> ChatGPT (post-editing)
- Critical path: Synthetic data generation → Mixed fine-tuning → MT inference → Post-editing
- Design tradeoffs: Synthetic vs. authentic domain data; LLM post-editing vs. direct constrained decoding; model size vs. inference efficiency
- Failure signatures: Low terminology adherence after post-editing; degradation in BLEU/chrF++ scores; LLM post-editing fails to converge or introduces errors
- First 3 experiments:
  1. Generate synthetic data with ChatGPT for one language pair and inspect sample outputs for quality and terminology adherence
  2. Fine-tune OPUS on synthetic+generic mix and evaluate terminology usage and translation quality on a dev set
  3. Apply post-editing to translations missing terms and compare terminology adherence and quality before/after

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do GPT-4's performance characteristics compare to GPT-3.5-turbo for terminology-constrained MT post-editing in terms of translation quality and term integration accuracy?
- Basis in paper: Authors suggest GPT-4 could potentially improve performance but were restricted to GPT-3.5-turbo
- Why unresolved: Shared task limitations prevented testing GPT-4
- What evidence would resolve it: Comparative experiments using both models, measuring translation quality and term integration accuracy

### Open Question 2
- Question: What is the optimal balance between authentic in-domain data and synthetic terminology-based data for fine-tuning MT models?
- Basis in paper: Authors note authentic in-domain data could be beneficial but were limited to synthetic data
- Why unresolved: Shared task did not provide authentic in-domain data
- What evidence would resolve it: Experiments comparing MT models fine-tuned on authentic vs. synthetic data

### Open Question 3
- Question: How does the quality and diversity of synthetic bilingual data generated by LLMs impact MT model performance in terminology integration?
- Basis in paper: Authors acknowledge need for deeper analysis of synthetic data properties and their effect on fine-tuning results
- Why unresolved: No detailed analysis conducted of relationship between synthetic data characteristics and model performance
- What evidence would resolve it: Correlation studies between synthetic data quality metrics and model performance

## Limitations

- Heavy reliance on ChatGPT API with unspecified prompt engineering strategies limiting reproducibility
- Focus on terminology integration without extensive exploration of overall translation quality impact
- Evaluation limited to specific shared task dataset, potentially limiting generalizability to other domains

## Confidence

- **High Confidence**: LLM-based terminology-constrained post-editing mechanism is well-supported by results (36.67% to 72.88% terminology adherence increase)
- **Medium Confidence**: Mixed fine-tuning effectiveness supported by related work and overall results, but optimal data ratio not explicitly explored
- **Medium Confidence**: LLM synthetic data generation is reasonable approach supported by related work, but data quality validation is critical assumption

## Next Checks

1. **Prompt Engineering Validation**: Systematically vary ChatGPT prompts for synthetic data generation and post-editing, evaluating impact on terminology adherence and translation quality to identify optimal strategies

2. **Synthetic Data Quality Assessment**: Compare linguistic quality and domain relevance of ChatGPT-generated synthetic data with authentic domain parallel data using cross-entropy scores and human evaluation

3. **Generalization Study**: Evaluate approach on broader range of domains and terminology integration tasks beyond WMT 2023 to assess robustness and generalizability