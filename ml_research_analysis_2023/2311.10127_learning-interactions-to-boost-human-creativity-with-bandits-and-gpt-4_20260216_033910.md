---
ver: rpa2
title: Learning interactions to boost human creativity with bandits and GPT-4
arxiv_id: '2311.10127'
source_url: https://arxiv.org/abs/2311.10127
tags:
- human
- hint
- hints
- word
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how interactions with AI algorithms can
  enhance human creative thought. The authors use a semantic feature generation task,
  where participants list as many features as possible for a given concept.
---

# Learning interactions to boost human creativity with bandits and GPT-4

## Quick Facts
- arXiv ID: 2311.10127
- Source URL: https://arxiv.org/abs/2311.10127
- Reference count: 40
- Primary result: Bandits trained on GPT-4 behavior learn the same hinting strategy preferences as those trained on human participants

## Executive Summary
This paper investigates how AI interactions can enhance human creative thought through a semantic feature generation task. The authors compare human performance with and without algorithmically-generated hints administered by a multi-armed bandit (MAB) algorithm. Remarkably, both humans and GPT-4 benefit from hints, with bandits learning from AI responses preferring the same prompting strategy as those learning from human behavior - specifically, a semantic strategy that provides related words based on user input history.

## Method Summary
The study uses a semantic feature generation task where participants list features for given concepts (penguin, journalist). A multi-armed bandit (EXP3) algorithm administers hints from three strategies: semantic neighbors, English frequency, and diversity cover. The experiment involves 37 human participants and 60 GPT-4 runs, comparing feature generation with and without hints, and analyzing bandit arm preferences to identify the most effective hinting strategy.

## Key Results
- Both humans and GPT-4 benefit from hints in feature generation tasks
- Bandits learning from AI responses prefer the same prompting strategy as those learning from human behavior
- The semantic hinting strategy (providing semantically related words based on user history) is consistently preferred by bandits and leads to higher feature generation rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bandit learns to prefer the semantic hinting strategy because it leads to higher human feature generation rates.
- Mechanism: The bandit explores multiple hinting strategies and exploits the one that yields the most new features, which turns out to be the semantic strategy of providing semantically related words based on user history.
- Core assumption: Providing semantically related hints helps humans overcome mental blocks by guiding them to new semantic neighborhoods they know but can't access.
- Evidence anchors:
  - [abstract] "bandits learning from AI responses prefer the same prompting strategy as those learning from human behavior" and "MAB learns to prefer a 'semantic' hinting strategy"
  - [section] "The semantic arm had a mean weight of 0.40, reliably higher than both the frequency arm...and the diversity arm" and "When bandits learned larger weights on the semantic arm, the corresponding participant produced more features"
  - [corpus] Weak - corpus shows related work on LLM creativity but doesn't directly support this specific mechanism
- Break condition: If semantic hints don't actually help humans generate more features, the bandit would stop preferring this strategy.

### Mechanism 2
- Claim: GPT-4 can serve as a proxy for human participants in evaluating hinting strategies.
- Mechanism: GPT-4 exhibits similar behavioral patterns to humans in the feature generation task, including asking for hints when "stuck" and benefiting from hints, allowing bandits trained on GPT-4 to learn similar strategy preferences.
- Core assumption: Despite not having human cognitive constraints, GPT-4 can simulate human-like behavior patterns in open-ended language tasks.
- Evidence anchors:
  - [abstract] "bandits learning from AI responses prefer the same prompting strategy as those learning from human behavior" and "bandits trained in this way learn to prefer the same hinting strategy"
  - [section] "Remarkably human-like behavior of GPT-4" and "the same best strategy (semantic) was identified as the preferred arm" for both human and GPT-4 trained bandits
  - [corpus] Weak - corpus shows related work on LLM-human similarity but doesn't directly validate this specific mechanism
- Break condition: If GPT-4's behavior diverges significantly from human behavior in the task, bandits trained on it would learn different strategies.

### Mechanism 3
- Claim: The feature generation task reliably produces mental blocks that hints can help overcome.
- Mechanism: Humans know many more features than they can spontaneously generate, creating a gap that algorithmically generated hints can help bridge by providing semantic cues.
- Core assumption: The feature generation task creates a specific type of cognitive bottleneck where people know more than they can access without external cues.
- Evidence anchors:
  - [abstract] "semantic feature generation: given a concept name, respondents must list as many of its features as possible" and "people know many more properties than they can produce before getting stuck"
  - [section] "people know many more properties than they can produce before getting stuck" and "provision of random cues may push people out of the saturated part of the space"
  - [corpus] Weak - corpus doesn't provide direct evidence for this specific task's properties
- Break condition: If the task doesn't actually create the described mental blocks, hints wouldn't provide the observed benefits.

## Foundational Learning

- Concept: Multi-armed bandit algorithms (specifically EXP3)
  - Why needed here: The bandit framework allows learning which hinting strategy works best through trial and error without assuming which strategy is superior upfront
  - Quick check question: How does the EXP3 algorithm balance exploration vs. exploitation when selecting hinting strategies?

- Concept: Semantic similarity and word embeddings
  - Why needed here: The semantic hinting strategy relies on finding words semantically related to user responses using word embedding distance
  - Quick check question: How is semantic distance calculated between words in the embedding space used by the semantic arm?

- Concept: Human cognitive limitations in semantic memory search
  - Why needed here: Understanding why people get "stuck" in feature generation tasks is crucial for designing effective hinting strategies
  - Quick check question: What cognitive phenomenon explains why people know more features than they can spontaneously generate?

## Architecture Onboarding

- Component map: User interface for feature generation task -> Multi-armed bandit (EXP3) algorithm -> Three hinting strategies (semantic, frequency, diversity) -> GPT-4 simulation component -> Data collection and analysis pipeline

- Critical path: User request hint → Bandit selects strategy → Hint generated → User produces features → Bandit receives reward → Update strategy weights

- Design tradeoffs:
  - Simple hinting strategies vs. more complex ones (3 vs potentially hundreds)
  - Human participants vs. GPT-4 simulation for training (expensive vs. scalable)
  - Immediate reward vs. delayed reward for hint effectiveness

- Failure signatures:
  - Bandit doesn't learn preference for any strategy (all strategies equally ineffective)
  - GPT-4 doesn't exhibit human-like behavior in the task
  - Hints don't lead to increased feature generation in hinted condition

- First 3 experiments:
  1. Run bandit with all three strategies on human participants to verify it learns semantic preference
  2. Run bandit with all three strategies on GPT-4 to verify it learns same semantic preference
  3. Test bandit trained on GPT-4 with actual human participants to validate proxy effectiveness

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Limited generalizability due to small sample size (37 human participants, 60 GPT-4 runs) and specific task domain
- Potential task-specific effects - the semantic feature generation task may create unique cognitive bottlenecks not present in other creative tasks
- Fundamental differences in cognitive architecture between humans and AI may limit how well AI behavior translates to human outcomes

## Confidence
- Main finding: Medium-High confidence based on convergence of results across both human and GPT-4 experiments
- Generalizability: Low-Medium confidence due to limited scope and specific nature of the task
- Proxy effectiveness: Medium confidence in GPT-4 as proxy, but requires validation across multiple domains

## Next Checks
1. Test bandit-trained hinting strategies across multiple creative domains beyond semantic feature generation to assess generalizability
2. Compare hinting strategy effectiveness between GPT-4 and humans using cross-validation - train bandits on GPT-4, test on humans and vice versa
3. Implement more sophisticated hinting strategies (e.g., context-aware, personalized) and test whether bandits can learn these using GPT-4 proxies