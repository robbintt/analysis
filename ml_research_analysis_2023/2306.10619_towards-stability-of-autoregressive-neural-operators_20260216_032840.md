---
ver: rpa2
title: Towards Stability of Autoregressive Neural Operators
arxiv_id: '2306.10619'
source_url: https://arxiv.org/abs/2306.10619
tags:
- spectral
- neural
- these
- fourier
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies aliasing and spectral error accumulation
  as key causes of instability in autoregressive neural operators for spatiotemporal
  forecasting. The authors analyze this problem through the lens of pseudospectral
  numerical methods and propose several architectural improvements: frequency-domain
  spectral normalization, depthwise-separable spectral convolutions, a Double Fourier
  Sphere (DFS) representation for spherical domains, and data-driven spectral filters.'
---

# Towards Stability of Autoregressive Neural Operators

## Quick Facts
- arXiv ID: 2306.10619
- Source URL: https://arxiv.org/abs/2306.10619
- Reference count: 40
- Primary result: Novel architectural improvements enable neural operators to maintain stability for up to 800% longer forecast horizons in high-resolution global weather prediction

## Executive Summary
This paper addresses the critical problem of instability in autoregressive neural operators for spatiotemporal forecasting. The authors identify aliasing and spectral error accumulation as key causes of instability, drawing parallels between neural operators and pseudospectral numerical methods. Through theoretical analysis and empirical validation, they propose several architectural improvements including frequency-domain spectral normalization, depthwise-separable spectral convolutions, Double Fourier Sphere representation for spherical domains, and data-driven spectral filters. These improvements are evaluated on three physical systems: Navier-Stokes fluid flow, rotating shallow water equations, and high-resolution global weather forecasting (ERA5), demonstrating significantly improved stability and longer prediction horizons without divergence.

## Method Summary
The authors develop stability improvements for autoregressive neural operators by analyzing their connection to pseudospectral methods. The core innovations include frequency-domain spectral normalization that constrains the spectral norm of convolution filters to prevent error amplification, depthwise-separable spectral convolutions that reduce parameter count while maintaining expressiveness, and the Double Fourier Sphere (DFS) representation that eliminates artificial discontinuities at poles in spherical domains. Data-driven spectral filters are introduced to compensate for aliasing introduced by nonlinearities. These methods are implemented in a Fourier Neural Operator framework and evaluated on three physical systems, with particular emphasis on the challenging ERA5 weather forecasting task.

## Key Results
- Achieved up to 800% longer forecast horizons without divergence compared to baseline neural operators on ERA5 weather forecasting
- Demonstrated improved stability across three physical systems: Navier-Stokes, rotating shallow water equations, and high-resolution global weather forecasting
- Successfully eliminated cross-polar information flow artifacts in spherical domains using the Double Fourier Sphere representation
- Maintained computational efficiency while improving stability through depthwise-separable spectral convolutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive error accumulation in neural operators mirrors aliasing and spectral instabilities in pseudospectral numerical methods.
- Mechanism: The convolution theorem in neural operators allows nonlinear operations in the spatial domain to generate high-frequency components that alias back into lower frequencies when represented on a finite grid. This aliasing compounds over time steps, causing error growth that eventually destabilizes predictions.
- Core assumption: The Fourier neural operator's spectral-domain convolution is mechanistically equivalent to pseudospectral PDE solvers, inheriting their aliasing vulnerabilities.
- Evidence anchors:
  - [abstract]: "We analyze the sources of this autoregressive error growth using prototypical neural operator models for physical systems"
  - [section 2.2]: "The Fourier Neural Operator (FNO) is mechanistically very similar to pseudospectral methods... introduces aliasing which will become one of the key obstacles to stability."
  - [corpus]: Weak - no direct evidence in corpus of aliasing in neural operators; papers focus on other stability mechanisms.
- Break condition: If the model uses explicit spectral truncation after nonlinearities or employs methods that prevent high-frequency generation.

### Mechanism 2
- Claim: Spectral normalization in the frequency domain controls sensitivity to out-of-distribution inputs and prevents error amplification.
- Mechanism: Large singular values in frequency-domain convolutions can amplify small errors into instability. Spectral normalization rescales these weights to have bounded spectral norm, constraining error growth during autoregressive rollouts.
- Core assumption: The spectral norm of frequency-domain filters directly correlates with the model's sensitivity to perturbations in the input distribution.
- Evidence anchors:
  - [section 4.1]: "We introduce a frequency-domain convolution analog to spectral normalization... this constraint amounts to ensuring that|a| ≤1"
  - [abstract]: "introduce architectural... improvements that allow for careful control of instability-inducing operations"
  - [corpus]: Missing - no direct evidence in corpus of spectral normalization in neural operators.
- Break condition: If the model encounters inputs with spectral characteristics vastly different from training data.

### Mechanism 3
- Claim: The Double Fourier Sphere representation eliminates artificial discontinuities at the poles that cause instability in spherical domains.
- Mechanism: Standard 2D FFT assumes toroidal periodicity, creating artificial discontinuities at poles. DFS maps the sphere to a torus via reflection, eliminating these discontinuities and preventing cross-polar information flow that destabilizes predictions.
- Core assumption: The artificial discontinuities introduced by naive 2D FFT on spherical data are a primary source of instability in autoregressive forecasting.
- Evidence anchors:
  - [section 3.2]: "2D FFT used in this model for spectral convolution proves to be a problem in the spherical setting... allows for 'local' operators to exchange information across the poles"
  - [section 4.3]: "One simple approach which can correct the artificial discontinuity induced by the 2D FFT... is the Double Fourier Sphere (DFS) method"
  - [corpus]: Weak - DFS mentioned in literature but not in context of neural operator stability.
- Break condition: If the spherical data has natural periodicity that matches the toroidal assumption.

## Foundational Learning

- Concept: Pseudospectral methods and aliasing in numerical PDEs
  - Why needed here: Understanding the connection between spectral neural operators and pseudospectral methods is essential to diagnose where aliasing-induced instabilities arise in autoregressive forecasting.
  - Quick check question: In a pseudospectral method with bandlimit K and N collocation points, what happens to modes with frequency > N/2?

- Concept: Discrete Fourier transforms and the Nyquist frequency
  - Why needed here: The DFT's aliasing behavior directly explains why high-frequency components generated by nonlinearities corrupt lower frequencies in neural operators.
  - Quick check question: If a signal with bandlimit K is sampled at N points where K > N/2, how do the higher frequencies manifest in the spectrum?

- Concept: Spectral normalization and its frequency-domain implementation
  - Why needed here: Understanding how spectral normalization constrains the spectral norm of frequency-domain convolutions is crucial for implementing the proposed stability improvements.
  - Quick check question: How does frequency-domain spectral normalization differ from spatial-domain spectral normalization in terms of what it constrains?

## Architecture Onboarding

- Component map: Input → DFS transform (spherical) → Dynamic position embeddings → Spectral conv layers with depthwise-separable structure and spectral normalization → Nonlinearities → Residual connections with frequency filtering → Output
- Critical path: DFS padding → Spectral normalization → Post-nonlinearity spectral filtering → Dynamic filter generation
- Design tradeoffs: DFS introduces spatial distortion but eliminates pole discontinuities; depthwise-separable convolutions reduce parameters but may limit expressiveness; frequency filtering adds capacity for aliasing compensation
- Failure signatures: Spectral spikes at alias frequencies, polar artifacts in spherical domains, exponential error growth in autoregressive rollouts, sensitivity to small input perturbations
- First 3 experiments:
  1. Implement DFS transform on spherical data and verify elimination of cross-polar sensitivity patterns
  2. Add frequency-domain spectral normalization to spectral convolution layers and measure sensitivity to small input perturbations
  3. Apply post-nonlinearity spectral filtering and evaluate stability on autoregressive Navier-Stokes prediction task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we theoretically characterize the aliasing behavior of arbitrary nonlinear activation functions in autoregressive neural operators?
- Basis in paper: [inferred] The paper mentions that the Weierstrass Approximation Theorem suggests any continuous nonlinearity can be approximated by polynomials, which exhibit aliasing, but notes that "exactly characterizing this behavior is an open problem."
- Why unresolved: The authors state that "exactly characterizing the impact of aliasing is difficult to analytically model" and provide only empirical observations through experiments.
- What evidence would resolve it: A rigorous mathematical framework that characterizes how different nonlinearities introduce aliasing in the spectral domain of neural operators, potentially building on signal processing theory.

### Open Question 2
- Question: What are the fundamental limits of predictability for data-driven neural operators on complex systems like weather forecasting?
- Basis in paper: [explicit] The paper notes that "over sufficiently long roll-outs, we hit the limit of predictability and the error converges to that of the persistence forecast" and mentions that "work remains to fully understand that particular pathology" in multi-resolution architectures.
- Why unresolved: The paper demonstrates convergence to persistence but doesn't establish theoretical bounds on predictability horizon, and the multi-resolution architecture's limitations are acknowledged but not fully resolved.
- What evidence would resolve it: Empirical studies establishing predictability horizons across different system complexities, and theoretical analysis of the information-theoretic limits of data-driven forecasting models.

### Open Question 3
- Question: How can we design architectures that handle sharp discontinuities (like coastlines and mountain ranges) without accumulating energy in these regions?
- Basis in paper: [explicit] The authors note that "ERA5 has sharp discontinuities along coasts and mountain ranges" and that "with one-step-ahead training, our current methods still fail to learn sufficiently diffusive dynamics to avoid energy accumulation in these areas indefinitely."
- Why unresolved: Despite architectural improvements, the paper acknowledges persistent issues with handling discontinuities, suggesting the current approaches are insufficient for these edge cases.
- What evidence would resolve it: Development and demonstration of architectures that can maintain stability and physical realism near discontinuities, potentially through specialized loss functions or domain-specific architectural modifications.

## Limitations

- The proposed methods show performance degradation near coasts and mountain ranges in ERA5 data, suggesting limitations with sharp discontinuities
- Computational overhead of DFS transforms and data-driven spectral filters on large-scale systems is not fully quantified
- Effectiveness of methods on discontinuous or high-contrast datasets remains unclear despite success on relatively smooth physical systems

## Confidence

**High Confidence**: The core hypothesis that aliasing and spectral error accumulation cause instability in autoregressive neural operators is well-supported by the theoretical analysis connecting FNO to pseudospectral methods. The DFS method's effectiveness in eliminating polar discontinuities is directly demonstrated through ablation studies.

**Medium Confidence**: The frequency-domain spectral normalization's effectiveness in controlling error amplification during autoregressive rollouts is supported by theoretical reasoning and experimental results, though the exact relationship between spectral norm bounds and practical stability could be more rigorously quantified.

**Low Confidence**: The claim that depthwise-separable spectral convolutions provide equivalent representational power to full convolutions while reducing parameters is not thoroughly validated. The experiments show improved stability but don't conclusively demonstrate that the efficiency gains come from architectural improvements or reduced model capacity.

## Next Checks

1. **Cross-Platform Stability Test**: Implement the full stability pipeline (DFS + spectral normalization + depthwise-separable convolutions) on a discontinuous dataset like terrain elevation or sea-land masks to verify whether the methods maintain stability where traditional FNO fails.

2. **Spectral Norm Sensitivity Analysis**: Systematically vary the spectral normalization bounds and measure the corresponding changes in autoregressive error growth rates across all three physical systems to establish the relationship between bound tightness and stability.

3. **Parameter Efficiency Validation**: Compare the full depthwise-separable spectral convolution model against a baseline with equivalent parameter count but traditional spatial convolutions to determine whether the efficiency gains come from architectural improvements or reduced model capacity.