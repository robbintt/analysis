---
ver: rpa2
title: 'DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech
  Translation'
arxiv_id: '2310.07403'
source_url: https://arxiv.org/abs/2310.07403
tags:
- latexit
- speech
- decoder
- translation
- sha1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving high-quality and
  fast speech-to-speech translation (S2ST) due to the complex multimodal distribution
  of target speech. The proposed method, DASpeech, is a non-autoregressive two-pass
  S2ST model that uses a directed acyclic graph (DAG) to model translations and incorporates
  variation information to alleviate acoustic multi-modality.
---

# DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation

## Quick Facts
- arXiv ID: 2310.07403
- Source URL: https://arxiv.org/abs/2310.07403
- Reference count: 40
- Primary result: Achieves comparable or better performance than Translatotron 2 while preserving up to 18.53x speedup

## Executive Summary
This paper addresses the challenge of achieving high-quality and fast speech-to-speech translation (S2ST) by introducing DASpeech, a non-autoregressive two-pass model. The method decomposes the translation process into content translation and speech synthesis using a directed acyclic graph (DAG) to model translations and incorporates variation information to alleviate acoustic multi-modality. DASpeech achieves state-of-the-art performance while significantly improving decoding speed compared to autoregressive baselines, and demonstrates the ability to preserve speaker voice characteristics during translation.

## Method Summary
DASpeech employs a two-pass architecture where a linguistic decoder first generates target text from source speech, and an acoustic decoder then generates target speech based on the linguistic decoder's hidden states. The linguistic decoder uses a Directed Acyclic Transformer (DA-Transformer) that models translations as paths in a DAG, with expected hidden states calculated via dynamic programming during training. The acoustic decoder follows FastSpeech 2 architecture, taking linguistic hidden states as input and using variance predictors (duration, pitch, energy) to generate mel-spectrograms. The model is trained in two stages: pretraining the speech encoder and linguistic decoder on speech-to-text translation, and the acoustic decoder on text-to-speech, followed by end-to-end fine-tuning on the full S2ST task.

## Key Results
- Achieves comparable or better translation quality than Translatotron 2 on CVSS Fr→En benchmark
- Preserves up to 18.53x speedup compared to autoregressive baselines
- Demonstrates better speaker voice preservation than Translatotron and Translatotron 2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-pass architecture decomposes generation into content translation and speech synthesis, simplifying the modeling of complex multimodal target speech distribution.
- Mechanism: Linguistic decoder generates target text from source speech; acoustic decoder generates target speech from hidden states of linguistic decoder, separating linguistic and acoustic variations.
- Core assumption: The target speech distribution can be effectively factorized into linguistic and acoustic components.
- Evidence anchors:
  - [abstract] "DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder."
  - [section 1] "Due to the linguistic diversity during translation, as well as the diverse acoustic variations (e.g., duration, pitch, energy, etc.), the target speech follows a complex multimodal distribution. To address this issue, Jia et al. [6], Inaguma et al. [7] propose the two-pass architecture..."
- Break condition: If linguistic and acoustic variations are highly correlated or interdependent, the factorization assumption fails.

### Mechanism 2
- Claim: Directed Acyclic Graph (DAG) in DA-Transformer captures multiple translation paths simultaneously, addressing linguistic multi-modality.
- Mechanism: Last-layer hidden states organized as DAG; different translations assigned to different paths; expected hidden states calculated via dynamic programming to consider all paths during training.
- Core assumption: The set of possible translations can be represented as paths in a DAG with unidirectional edges.
- Evidence anchors:
  - [abstract] "DA-Transformer models translations with a directed acyclic graph (DAG)."
  - [section 2.1] "DA-Transformer models the translation probability Pθ(Y |X) by marginalizing all possible paths in DAG..."
  - [section 3.2] "During training, we consider all possible paths in the DAG by calculating the expected hidden state for each target token via dynamic programming..."
- Break condition: If the number of possible translations is too large or the DAG structure cannot adequately represent them.

### Mechanism 3
- Claim: Non-autoregressive acoustic decoder with variance predictors (duration, pitch, energy) alleviates acoustic multi-modality.
- Mechanism: FastSpeech 2-style decoder takes linguistic decoder hidden states as input; variance adaptor predicts duration, pitch, and energy to condition mel-spectrogram generation.
- Core assumption: Acoustic variations can be explicitly modeled as variance information separate from the linguistic content.
- Evidence anchors:
  - [abstract] "To consider all potential paths in the DAG during training, we calculate the expected hidden states for each target token via dynamic programming, and feed them into the acoustic decoder to predict the target mel-spectrogram."
  - [section 3.1] "The acoustic decoder adopts the design of FastSpeech 2 [12], which takes the hidden states of the linguistic decoder as input and generates the target mel-spectrogram."
  - [section 2.2] "The introduction of variation information greatly alleviates the acoustic multi-modality problem, which leads to better voice quality."
- Break condition: If variance information is insufficient to capture all acoustic variations or if modeling it introduces too much complexity.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) and dynamic programming
  - Why needed here: DAG structure allows modeling multiple translation paths; dynamic programming efficiently computes expected hidden states across all paths.
  - Quick check question: How does the forward-backward algorithm compute expected hidden states in a DAG?

- Concept: Non-autoregressive sequence generation and multi-modality problem
  - Why needed here: Understanding why non-autoregressive models struggle with multi-modal distributions and how this work addresses it.
  - Quick check question: What is the multi-modality problem in non-autoregressive machine translation and how does DA-Transformer address it?

- Concept: Speech-to-text and text-to-speech (TTS) systems
  - Why needed here: Understanding the two-pass architecture and how it builds on existing speech and text processing technologies.
  - Quick check question: What are the key differences between direct S2ST and cascaded ASR-MT-TTS systems?

## Architecture Onboarding

- Component map: Speech encoder (Conformer blocks + subsampler) → Linguistic decoder (DA-Transformer) → Acoustic decoder (FastSpeech 2-style) → HiFi-GAN vocoder
- Critical path: Speech encoder → Linguistic decoder (last layer) → Acoustic decoder (expect-path training) → Mel-spectrogram generation
- Design tradeoffs:
  - DAG size λ: Larger λ improves S2TT but makes end-to-end training harder
  - Expect-path vs best-path training: Expect-path considers all paths but is more complex; best-path is simpler but may under-train some hidden states
  - Joint-Viterbi vs Lookahead decoding: Joint-Viterbi finds global optimum but is more complex; Lookahead is greedy but faster
- Failure signatures:
  - Poor translation quality: Check DAG structure, λ value, and expect-path computation
  - Slow decoding: Verify non-autoregressive implementation and batch processing
  - Voice preservation issues: Examine variance predictor training and acoustic decoder conditioning
- First 3 experiments:
  1. Verify DAG structure and path enumeration with small toy example
  2. Test expect-path computation with known probability distributions
  3. Validate acoustic decoder output with ground truth hidden states from linguistic decoder

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of graph size (λ) affect the performance of DASpeech in multilingual settings, and what is the optimal graph size for balancing translation quality and decoding speed across different language pairs?
- Basis in paper: [explicit] The paper mentions that larger graph sizes can improve S2TT performance but make end-to-end training more challenging. It also notes that the cascaded model prefers larger graph sizes while DASpeech prefers smaller graph sizes.
- Why unresolved: The paper only provides limited results for multilingual settings and does not thoroughly explore the impact of different graph sizes on translation quality and decoding speed across various language pairs.
- What evidence would resolve it: Conducting extensive experiments with different graph sizes (λ) for each language pair in the multilingual setting and analyzing the trade-off between translation quality and decoding speed would provide insights into the optimal graph size for DASpeech in multilingual scenarios.

### Open Question 2
- Question: Can the voice preservation ability of DASpeech be further enhanced by incorporating additional techniques or model modifications?
- Basis in paper: [explicit] The paper mentions that DASpeech can better preserve the speaker's voice than Translatotron and Translatotron 2, as its acoustic decoder explicitly introduces variation information of the target speech.
- Why unresolved: The paper does not explore potential techniques or model modifications that could further improve the voice preservation ability of DASpeech.
- What evidence would resolve it: Investigating and implementing additional techniques or model modifications, such as incorporating speaker embedding or using a more sophisticated voice conversion model, and evaluating their impact on the voice preservation ability of DASpeech would provide insights into potential improvements.

### Open Question 3
- Question: How does the training cost of DASpeech compare to other state-of-the-art S2ST models, and what strategies can be employed to reduce the training time without compromising performance?
- Basis in paper: [explicit] The paper mentions that the training cost of DASpeech is higher than Translatotron 2 (96 vs. 18 GPU hours) due to the use of dynamic programming during training.
- Why unresolved: The paper does not provide a detailed comparison of the training costs of DASpeech with other state-of-the-art S2ST models or explore strategies to reduce the training time.
- What evidence would resolve it: Conducting a comprehensive comparison of the training costs of DASpeech with other S2ST models and investigating techniques such as model parallelism, distributed training, or more efficient dynamic programming algorithms would provide insights into strategies to reduce the training time of DASpeech.

## Limitations
- Computational complexity of expect-path training using dynamic programming over all DAG paths during both forward and backward passes
- Potential inability to find optimal translations that autoregressive models might discover through iterative refinement
- Limited evaluation of performance with language pairs having significantly different phonological structures

## Confidence
- High confidence: Core architectural claims about two-pass decomposition, DAG-based linguistic modeling, and variance-based acoustic modeling
- Medium confidence: Voice preservation claims based on variance predictor's ability to capture speaker characteristics
- Low confidence: Performance claims with language pairs having divergent phonological and syntactic structures

## Next Checks
1. **Scaling analysis**: Systematically evaluate how decoding speed and translation quality degrade as DAG size λ increases, and identify the practical limits of expect-path training for longer sequences.

2. **Cross-lingual robustness**: Test the model on language pairs with more divergent phonological and syntactic structures than French-to-English to assess generalization beyond the primary experimental setup.

3. **Speaker similarity validation**: Implement a comprehensive speaker verification evaluation using both automatic metrics (e.g., speaker embedding similarity) and human perceptual studies to rigorously validate the voice preservation claims.