---
ver: rpa2
title: 'Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models'
arxiv_id: '2311.01442'
source_url: https://arxiv.org/abs/2311.01442
tags:
- time
- series
- deep
- descent
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the occurrence of epoch-wise deep double
  descent in Transformer models for long sequence time series forecasting. The authors
  demonstrate that overfitting can be reverted using more training epochs, challenging
  the common practice of early stopping.
---

# Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models

## Quick Facts
- arXiv ID: 2311.01442
- Source URL: https://arxiv.org/abs/2311.01442
- Reference count: 40
- This paper demonstrates epoch-wise deep double descent in Transformer models for long sequence time series forecasting, achieving state-of-the-art results in nearly 70% of 72 benchmarks by extending training epochs beyond typical early stopping practices.

## Executive Summary
This paper investigates the phenomenon of epoch-wise deep double descent in Transformer models applied to long sequence time series forecasting (LSTF). The authors demonstrate that overfitting can be reverted by continuing training beyond the first minimum, challenging the common practice of early stopping. By leveraging these findings, they achieve state-of-the-art results for Transformers on LSTF in nearly 70% of the 72 benchmarks tested. The study introduces a taxonomy for classifying training schema modifications and systematically explores how extended training epochs can unlock untapped potential in existing models.

## Method Summary
The method involves training Transformer-based models (FEDformer, Informer, Autoformer) on long sequence time series forecasting tasks with dramatically extended training epochs (up to 1000) and reduced early stopping. The key modification is maintaining a constant learning rate after epoch 5, compared to the original setups that used early stopping. The models were trained on 9 public time series datasets with various prediction lengths, and performance was evaluated using MSE and MAE metrics on test data.

## Key Results
- Demonstrated epoch-wise deep double descent curves in Transformer models for LSTF
- Achieved state-of-the-art results for long sequence time series forecasting in nearly 70% of 72 benchmarks
- Showed that overfitting can be reverted using more training epochs, challenging common early stopping practices
- Verified the double descent phenomenon is independent of model architecture across FEDformer, Informer, and Autoformer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Epoch-wise deep double descent allows models to escape overfitting by continuing training beyond the first minimum.
- Mechanism: As training epochs increase, the validation loss first decreases, then increases due to overfitting, but can decrease again in a "second descent" phase, potentially reaching lower loss than the first minimum.
- Core assumption: The time series data contains sufficient complexity and noise to trigger the double descent phenomenon.
- Evidence anchors:
  - [abstract] "We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs."
  - [section] "The underlying theoretical principles that explain the occurrence and conditions of deep double descent remain less comprehensively understood [24, 3]."
  - [corpus] Weak evidence: No direct corpus papers mention double descent in time series; most focus on architecture or augmentation.
- Break condition: If the data is too simple or noiseless, the double descent may not occur, and continued training will only lead to overfitting.

### Mechanism 2
- Claim: Increasing the number of training epochs beyond typical limits (e.g., from 10 to 1000) reveals untapped potential in existing transformer models for long sequence time series forecasting.
- Mechanism: By extending training duration and patience, models that previously stopped at early overfitting can enter a second descent phase, achieving lower validation loss and better generalization.
- Core assumption: The original hyperparameter settings (low epochs, low patience) were prematurely stopping training before the second descent could occur.
- Evidence anchors:
  - [abstract] "Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested."
  - [section] "The models were trained on an NVIDIA A100-80GB. To accommodate the higher epoch count we make one hyperparameter change to the original setups from the respective papers: the learning rate decay."
  - [corpus] Weak evidence: No corpus papers directly address extending training epochs for LSTF; most focus on model architecture or data augmentation.
- Break condition: If the computational budget is insufficient to train for the extended epochs, or if the model architecture is fundamentally limited, the second descent may not be reachable.

### Mechanism 3
- Claim: The double descent phenomenon is independent of model architecture, as it was observed across multiple transformer models (FEDformer, Informer, Autoformer).
- Mechanism: Different transformer architectures, when trained with sufficient epochs, can all exhibit the double descent behavior, suggesting a universal property of deep learning models on time series data.
- Core assumption: The double descent is a property of the training process and data complexity, not specific to a particular model architecture.
- Evidence anchors:
  - [abstract] "We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets."
  - [section] "We also include Informer [35] and Autoformer [29] to assess whether the double descent phenomenon is independent of model architecture."
  - [corpus] Weak evidence: No corpus papers directly test double descent across multiple transformer architectures for time series.
- Break condition: If a particular architecture has inherent limitations that prevent it from benefiting from extended training, the double descent may not occur for that model.

## Foundational Learning

- Concept: Deep double descent
  - Why needed here: Understanding the double descent phenomenon is crucial for interpreting the experimental results and the motivation behind extending training epochs.
  - Quick check question: What are the three phases of the double descent curve, and how do they relate to model performance?

- Concept: Long sequence time series forecasting (LSTF)
  - Why needed here: The paper focuses on LSTF, which involves predicting future values of time series with long history and horizon sizes, making it relevant to the choice of models and benchmarks.
  - Quick check question: How does LSTF differ from standard time series forecasting, and why are transformer models particularly suited for this task?

- Concept: Model architecture vs. training schema
  - Why needed here: The paper distinguishes between modifications to the model architecture (e.g., Informer, Autoformer) and modifications to the training schema (e.g., number of epochs, patience), emphasizing the latter as the focus of the study.
  - Quick check question: What are some examples of training schema modifications, and how do they differ from architecture modifications in their impact on model performance?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model architecture (Transformer models) -> Training schema (epochs, patience, learning rate decay) -> Evaluation (MSE, MAE)

- Critical path:
  1. Load and preprocess time series data
  2. Initialize transformer model with original hyperparameters
  3. Modify training schema (increase epochs, patience, adjust learning rate decay)
  4. Train model on data
  5. Evaluate model performance on test set
  6. Compare results to original implementations

- Design tradeoffs:
  - Longer training epochs vs. computational cost
  - Higher patience vs. risk of overfitting in early stages
  - Model complexity vs. ability to benefit from double descent

- Failure signatures:
  - Validation loss continues to increase without a second descent
  - Model performance does not improve despite extended training
  - Computational constraints prevent training for sufficient epochs

- First 3 experiments:
  1. Train FEDformer on ILI dataset with 1000 epochs and patience, compare to original 10 epochs
  2. Train Informer on Traffic dataset with 1000 epochs, observe if double descent occurs
  3. Train Autoformer on Exchange dataset with 1000 epochs, compare MSE and MAE to original results

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- The theoretical understanding of deep double descent remains incomplete
- Experiments are limited to 9 public benchmark datasets and 3 specific Transformer architectures
- Computational requirements for training with 1000 epochs may limit practical applicability

## Confidence
**High Confidence:**
- The empirical observation of double descent curves in transformer models for time series forecasting
- The effectiveness of extended training epochs in improving state-of-the-art results on tested benchmarks
- The reproducibility of the double descent phenomenon across multiple transformer architectures

**Medium Confidence:**
- The universality of epoch-wise double descent across all time series forecasting tasks
- The claim that early stopping is generally suboptimal for transformer models
- The practical implications for computational resource allocation

**Low Confidence:**
- The theoretical mechanisms underlying the double descent phenomenon in time series contexts
- The scalability of these findings to industrial-scale time series datasets
- The robustness of results across different hardware configurations

## Next Checks
1. **Theoretical Analysis**: Conduct a mathematical analysis to derive conditions under which epoch-wise deep double descent occurs in time series forecasting, building on existing statistical learning theory frameworks.

2. **Architecture-Agnostic Testing**: Extend experiments to include non-transformer architectures (such as LSTMs or temporal convolutional networks) to verify whether the double descent phenomenon is truly architecture-independent.

3. **Computational Efficiency Study**: Evaluate the trade-off between extended training epochs and computational cost by measuring performance gains relative to training time and energy consumption across different hardware configurations.