---
ver: rpa2
title: Improving Automatic Quotation Attribution in Literary Novels
arxiv_id: '2307.03734'
source_url: https://arxiv.org/abs/2307.03734
tags:
- character
- attribution
- quotation
- speaker
- quotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates quotation attribution systems for literary
  novels by breaking down the task into four subtasks: character identification, coreference
  resolution, quotation identification, and speaker attribution. Using the Project
  Dialogism Novel Corpus, a dataset of annotated coreferences and quotations in 22
  literary novels, the authors benchmark state-of-the-art models on each subtask independently.'
---

# Improving Automatic Quotation Attribution in Literary Novels

## Quick Facts
- arXiv ID: 2307.03734
- Source URL: https://arxiv.org/abs/2307.03734
- Reference count: 9
- Key outcome: Breaking down quotation attribution into four subtasks reveals that character identification and coreference resolution models struggle with literary text, while a simple sequential RNN achieves state-of-the-art speaker attribution accuracy on the PDNC dataset.

## Executive Summary
This paper evaluates quotation attribution systems for literary novels by decomposing the complex task into four interconnected subtasks: character identification, coreference resolution, quotation identification, and speaker attribution. Using the Project Dialogism Novel Corpus (PDNC), a dataset of 22 annotated literary novels, the authors benchmark state-of-the-art models on each subtask independently. They find that while character identification and coreference resolution models perform poorly on literary text, restricting candidate mentions to resolved character entities significantly improves speaker attribution accuracy. A simple sequential prediction model achieves accuracy scores on par with or exceeding more complex contextual models when trained on the PDNC dataset.

## Method Summary
The authors evaluate existing NLP models on each of the four subtasks independently using the PDNC dataset. They benchmark character identification using BookNLP and spaCy, coreference resolution using BookNLP and other models, quotation identification using GutenTag, and speaker attribution using both the BookNLP pipeline and a sequential RNN model. For speaker attribution, they train a one-layer RNN that predicts the speaker based on the sequence of recently mentioned characters. They evaluate all models using 5-fold cross-validation, either splitting by quotations or by entire novels (80/20 split), and measure accuracy by comparing predicted speakers to annotated character entities.

## Key Results
- Character identification models achieve only 0.23-0.27 CR scores on literary text, significantly lower than on other domains
- Coreference resolution successfully links only 37% of resolved mention clusters to character entities
- Restricting candidate mentions to resolved character entities increases speaker attribution accuracy from 0.42 to 0.61
- A simple sequential RNN speaker attribution model achieves 0.72 accuracy, outperforming BERT-based contextual models (0.64) on the PDNC dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Breaking down quotation attribution into four sub-tasks allows evaluation of component performance independently
- Mechanism: The paper decomposes the complex task of quotation attribution into character identification, coreference resolution, quotation identification, and speaker attribution, enabling targeted analysis of each component's performance
- Core assumption: Each sub-task can be evaluated independently without significant cross-task interference
- Evidence anchors:
  - [abstract] "Here, we approach quotation attribution as a set of four interconnected sub-tasks: character identification, coreference resolution, quotation identification, and speaker attribution"
  - [section 4] "We contend that most use-cases of a quotation attribution system involve resolving the speaker mention to one among a list of character entities"
- Break condition: When sub-task outputs are highly interdependent, making isolated evaluation misleading

### Mechanism 2
- Claim: Restricting candidate mentions to resolved character entities improves speaker attribution accuracy
- Mechanism: By limiting the candidate speaker mentions to only those that can be successfully linked to a character entity, the model eliminates spurious candidates and improves attribution performance
- Core assumption: Most attribution errors stem from considering incorrect candidate mentions
- Evidence anchors:
  - [section 6] "We evaluate a simple sequential prediction model that predicts the speaker of a quotation simply by looking at the sequence of speakers and mentions that occur in some window around the quotation"
  - [section 7] "The end-to-end pretrained BookNLP pipeline... achieves an accuracy of 0.42. When we restrict the set of candidate mentions... the attribution accuracy increases to 0.61"
- Break condition: When character identification or coreference resolution models perform so poorly that too few candidates remain for consideration

### Mechanism 3
- Claim: Sequential prediction based on mention context can match or exceed complex contextual models for speaker attribution
- Mechanism: A simple RNN model that predicts the speaker based on the sequence of recently mentioned characters can achieve accuracy comparable to BERT-based contextual models
- Core assumption: Local context patterns in mention sequences contain sufficient information for speaker attribution
- Evidence anchors:
  - [section 5.4] "We implement this as a one-layer RNN that is fed a sequence of tokens representing the five characters mentioned most recently prior to the quotation text"
  - [section 7] "However, the RNN model still beats this performance with an accuracy of 0.72 on the random data split"
- Break condition: When quotation attribution requires understanding of more distant context or complex linguistic patterns

## Foundational Learning

- Concept: Coreference resolution
  - Why needed here: Essential for linking mentions like "she" or "Elizabeth's sister" to character entities
  - Quick check question: What is the difference between a mention and a character entity in coreference resolution?

- Concept: Named entity recognition (NER)
  - Why needed here: Required to identify character names and aliases in the text
  - Quick check question: How does NER differ from character identification in the context of literary texts?

- Concept: Speaker attribution
  - Why needed here: The final task of linking quotations to their speakers
  - Quick check question: What makes speaker attribution in literary novels more challenging than in dialogue transcripts?

## Architecture Onboarding

- Component map:
  - Character Identification: Identifies all character entities and aliases
  - Coreference Resolution: Links mentions to character entities
  - Quotation Identification: Detects dialogue spans
  - Speaker Attribution: Links quotations to speakers
  - Pipeline Integration: Chains the components together

- Critical path: Character Identification → Coreference Resolution → Speaker Attribution
- Design tradeoffs:
  - Granularity vs. performance: More granular character lists improve accuracy but require better coreference resolution
  - Candidate restriction: Limiting candidates improves accuracy but may miss valid speakers
  - Context window size: Larger windows capture more context but increase computational cost

- Failure signatures:
  - Low character identification scores indicate problems with entity recognition
  - High unresolved mention rates suggest coreference resolution issues
  - Poor speaker attribution despite good mention resolution points to attribution model problems

- First 3 experiments:
  1. Run each component independently on a small novel sample and measure individual performance metrics
  2. Test the pipeline with restricted candidates (only resolved mentions) to measure attribution improvement
  3. Compare sequential prediction model performance against contextual models on explicit vs. implicit quotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve character identification models for literary texts to better handle complex naming patterns, such as characters being referred to by their husbands' or fathers' names?
- Basis in paper: [explicit] The paper highlights that character identification models struggle with literary texts due to complex naming patterns, such as characters being referred to by their husbands' or fathers' names (e.g., Mrs. Archer referring to May Welland).
- Why unresolved: Current character identification models are not designed to handle the complex naming patterns found in literary texts, leading to errors in identifying characters correctly.
- What evidence would resolve it: Developing and evaluating new character identification models that are specifically designed to handle complex naming patterns in literary texts, and testing their performance on a diverse set of literary novels.

### Open Question 2
- Question: How can we improve coreference resolution models for literary texts to better handle pronominal phrases and relationships between characters?
- Basis in paper: [explicit] The paper notes that coreference resolution models struggle with literary texts due to the abundance of pronominal phrases and relationships between characters, such as "her husband" and "their father."
- Why unresolved: Current coreference resolution models are not designed to handle the complex relationships between characters and pronominal phrases found in literary texts, leading to errors in resolving coreferences correctly.
- What evidence would resolve it: Developing and evaluating new coreference resolution models that are specifically designed to handle pronominal phrases and relationships between characters in literary texts, and testing their performance on a diverse set of literary novels.

### Open Question 3
- Question: How can we improve speaker attribution models for literary texts to better handle implicit and anaphoric quotations?
- Basis in paper: [explicit] The paper shows that speaker attribution models struggle with implicit and anaphoric quotations, where the speaker is not explicitly indicated by a referring expression.
- Why unresolved: Current speaker attribution models are not designed to handle the complex patterns of implicit and anaphoric quotations found in literary texts, leading to errors in attributing speakers correctly.
- What evidence would resolve it: Developing and evaluating new speaker attribution models that are specifically designed to handle implicit and anaphoric quotations in literary texts, and testing their performance on a diverse set of literary novels.

### Open Question 4
- Question: How can we improve the overall performance of quotation attribution systems for literary texts by addressing the limitations of individual sub-tasks?
- Basis in paper: [inferred] The paper suggests that improving the performance of individual sub-tasks, such as character identification, coreference resolution, and speaker attribution, could lead to better overall performance of quotation attribution systems for literary texts.
- Why unresolved: The current quotation attribution systems for literary texts are limited by the performance of individual sub-tasks, and addressing these limitations could lead to better overall performance.
- What evidence would resolve it: Developing and evaluating integrated quotation attribution systems that address the limitations of individual sub-tasks, and testing their performance on a diverse set of literary novels.

## Limitations

- Domain generalization concerns: The PDNC dataset of 22 novels may not capture the full diversity of quotation attribution challenges across different literary traditions and historical periods.
- Coreference resolution bottleneck: Only 37% of resolved mention clusters successfully link to character entities, creating a cascading effect that may underestimate speaker attribution model performance.
- Model comparison challenges: The sequential RNN model benefits from restricted candidate sets, making fair comparison with contextual models difficult.

## Confidence

**High Confidence**: The finding that character identification and coreference resolution models struggle with literary text (as evidenced by low CR scores and incomplete mention resolution) is well-supported by the empirical results across multiple evaluation metrics.

**Medium Confidence**: The claim that sequential prediction can match or exceed complex contextual models for speaker attribution is supported by the experimental results, though the restricted candidate set may artificially inflate performance differences.

**Low Confidence**: The generalizability of these findings to broader literary corpora and the extent to which the sequential model's performance represents a true advance versus an artifact of the evaluation setup.

## Next Checks

1. **Cross-Dataset Validation**: Test the sequential RNN model and baseline approaches on an independent literary corpus from a different period or style to assess domain generalization and identify potential overfitting to the PDNC dataset.

2. **Candidate Set Ablation Study**: Re-run speaker attribution experiments with varying candidate set sizes (including all potential mentions, not just resolved ones) to isolate the impact of candidate restriction on model performance comparisons.

3. **Error Analysis on Implicit Quotations**: Conduct a detailed error analysis focusing on implicit quotations where attribution is more challenging, comparing failure modes between sequential and contextual approaches to understand which model handles complex cases better.