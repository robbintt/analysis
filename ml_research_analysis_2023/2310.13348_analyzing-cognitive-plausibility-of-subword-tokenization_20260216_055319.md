---
ver: rpa2
title: Analyzing Cognitive Plausibility of Subword Tokenization
arxiv_id: '2310.13348'
source_url: https://arxiv.org/abs/2310.13348
tags:
- computational
- tokenization
- language
- vocabulary
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates subword tokenization algorithms from a cognitive
  perspective, using human lexical decision task data to measure how well tokenization
  correlates with response times and accuracy. The authors introduce a chunkability
  metric that quantifies how many tokens a word is split into relative to its length.
---

# Analyzing Cognitive Plausibility of Subword Tokenization

## Quick Facts
- arXiv ID: 2310.13348
- Source URL: https://arxiv.org/abs/2310.13348
- Reference count: 14
- Key outcome: WordPiece and BPE produce tokenizations more aligned with human lexical decision performance than UnigramLM

## Executive Summary
This study evaluates subword tokenization algorithms from a cognitive perspective by correlating tokenizer outputs with human lexical decision task performance. The authors introduce a chunkability metric that quantifies how much words are split into subword tokens, finding that fewer splits correlate with faster response times and higher accuracy. They demonstrate that WordPiece and BPE produce more cognitively plausible tokenizations than UnigramLM, and show that larger vocabularies improve morphological coverage, especially for morphologically rich languages.

## Method Summary
The study trains BPE, WordPiece, and UnigramLM tokenizers on 100,000 sentences from the Leipzig corpus, then evaluates cognitive plausibility by computing chunkability scores (1 - tokens/characters) for lexical decision stimuli across four languages. The researchers correlate these scores with human response times and accuracy from established lexical decision datasets, while also analyzing morpheme coverage using the SIGMORPHON dataset to assess morphological richness.

## Key Results
- WordPiece and BPE produce tokenizations more aligned with human lexical decision performance than UnigramLM
- UnigramLM's top-down pruning approach disrupts morpheme preservation, reducing cognitive plausibility
- Larger vocabularies improve morphological coverage, especially for morphologically rich languages
- Multilingual vocabularies generally reduce cognitive plausibility compared to monolingual ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chunkability metric correlates with human lexical decision performance because it captures cognitive effort of tokenizing complex words
- Mechanism: The chunkability metric (1 - tokens/characters) quantifies how much a word is split into subword tokens. Lower splits mean less cognitive load during recognition, aligning with faster response times and higher accuracy
- Core assumption: Subword tokenization reflects cognitive processing of word recognition, where fewer splits correspond to easier recognition
- Evidence anchors:
  - [abstract] "We analyze the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task"
  - [section] "Our results indicate that the UnigramLM algorithm yields less cognitively plausible tokenization behavior"
  - [corpus] "The corpus evidence shows that lexical decision task data from multiple languages correlates with chunkability scores"

### Mechanism 2
- Claim: UnigramLM produces less cognitively plausible tokenization because its top-down pruning disrupts morpheme preservation
- Mechanism: UnigramLM starts with an overly large vocabulary and prunes tokens that minimally affect likelihood. This approach may remove morphemes that are cognitively significant, leading to less plausible splits
- Core assumption: Morphemes are cognitively significant units, and preserving them in tokenization improves cognitive plausibility
- Evidence anchors:
  - [abstract] "Our results indicate that the UnigramLM algorithm yields less cognitively plausible tokenization behavior"
  - [section] "We find a lower cognitive correlation for the UnigramLM algorithm than for WordPiece and BPE"
  - [corpus] "The corpus evidence from morphological segmentation tasks shows that UnigramLM has worse morpheme coverage"

### Mechanism 3
- Claim: Larger vocabularies improve cognitive plausibility and morphological coverage by better representing morphological richness
- Mechanism: Increasing vocabulary size allows tokenizers to include more morphemes and complex subword units, which aligns better with human cognitive processing
- Core assumption: Morphological richness requires larger vocabularies to be adequately represented, and this representation aids cognitive processing
- Evidence anchors:
  - [abstract] "We also show that larger vocabularies improve morphological coverage, especially for morphologically rich languages"
  - [section] "We see that WordPiece tokens overall provide better coverage of morphemes than UnigramLM"
  - [corpus] "The corpus evidence from SIGMORPHON shared task annotations indicates that derivational morpheme coverage increases with vocabulary size"

## Foundational Learning

- Concept: Subword tokenization algorithms (BPE, WordPiece, UnigramLM)
  - Why needed here: Understanding how different algorithms build vocabularies and split words is crucial to interpreting cognitive plausibility results
  - Quick check question: How does BPE differ from UnigramLM in building its vocabulary?

- Concept: Lexical decision tasks and cognitive processing of words
  - Why needed here: The study uses lexical decision task data to measure cognitive plausibility, so understanding how these tasks work and what they measure is essential
  - Quick check question: What does a lexical decision task measure in terms of cognitive processing?

- Concept: Morphological segmentation and morpheme coverage
  - Why needed here: The study evaluates how well tokenizers cover derivational morphemes, which is related to cognitive plausibility and morphological richness
  - Quick check question: Why is morpheme coverage important for cognitive plausibility in tokenization?

## Architecture Onboarding

- Component map: Leipzig corpus -> Tokenizer training (BPE, WordPiece, UnigramLM) -> Chunkability calculation -> Lexical decision datasets -> Cognitive correlation analysis -> Morpheme coverage analysis (SIGMORPHON)

- Critical path:
  1. Train tokenizers on Leipzig corpus
  2. Compute chunkability for lexical decision stimuli
  3. Correlate chunkability with human response times and accuracy
  4. Analyze morpheme coverage across vocabulary sizes
  5. Compare monolingual vs multilingual tokenizer performance

- Design tradeoffs:
  - Vocabulary size vs computational efficiency
  - Morpheme preservation vs compression rate
  - Monolingual vs multilingual vocabularies for cognitive plausibility
  - Use of chunkability vs other evaluation metrics

- Failure signatures:
  - Low or negative correlations between chunkability and cognitive measures
  - Inconsistent results across languages or tokenizer algorithms
  - Poor morpheme coverage even with large vocabularies
  - Multilingual vocabularies performing worse than monolingual ones

- First 3 experiments:
  1. Train BPE, WordPiece, and UnigramLM tokenizers on Leipzig corpus and compare chunkability scores on lexical decision stimuli
  2. Analyze the correlation between chunkability and human response times/accuracy for each tokenizer and language
  3. Vary vocabulary sizes and measure the impact on cognitive plausibility and morpheme coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does chunkability correlate with lexical decision task results across more languages and language families beyond the four studied?
- Basis in paper: [inferred] The authors acknowledge that their cognitive analyses are limited to two Romance and two Germanic languages
- Why unresolved: The study's scope was limited by available lexical decision task datasets and computational resources
- What evidence would resolve it: Conduct the same analysis on lexical decision task datasets from languages of diverse families and typological features

### Open Question 2
- Question: What is the relative importance of vocabulary size versus tokenization algorithm for morphological coverage and cognitive plausibility?
- Basis in paper: [inferred] The authors find both vocabulary size and algorithm affect coverage and correlation, but don't disentangle their relative contributions
- Why unresolved: The study varied vocabulary size and algorithm simultaneously rather than isolating their individual effects
- What evidence would resolve it: Conduct a factorial experiment varying vocabulary size and algorithm independently while measuring morphological coverage and cognitive plausibility

### Open Question 3
- Question: Can alternative cognitive measures beyond lexical decision tasks better capture the relationship between tokenization and human language processing?
- Basis in paper: [inferred] The authors use lexical decision tasks as a proxy for processing complexity, acknowledging limitations
- Why unresolved: Lexical decision tasks have inherent limitations in capturing the full complexity of human language processing
- What evidence would resolve it: Conduct parallel analyses using alternative cognitive measures like reading time, eye-tracking, or neuroimaging data

## Limitations

- Cognitive plausibility findings rely heavily on lexical decision task data, which represents only one aspect of human language processing
- The chunkability metric, while intuitive, may not fully capture nuanced ways humans decompose and recognize words
- Comparison between monolingual and multilingual vocabularies assumes cognitive plausibility should be evaluated within language-specific contexts
- SIGMORPHON dataset represents a specific annotation scheme that may not align perfectly with how humans actually process morphemes cognitively

## Confidence

**High Confidence Claims:**
- WordPiece and BPE produce tokenizations more aligned with human lexical decision performance than UnigramLM
- Larger vocabularies improve morphological coverage, especially for morphologically rich languages
- Multilingual vocabularies generally reduce cognitive plausibility compared to monolingual ones

**Medium Confidence Claims:**
- The chunkability metric effectively captures cognitive processing effort in lexical decision tasks
- Morpheme preservation directly correlates with cognitive plausibility
- The specific vocabulary sizes chosen (50,000) represent optimal tradeoffs for cognitive processing

**Low Confidence Claims:**
- The findings generalize across all language processing tasks beyond lexical decision
- The observed patterns would hold with different corpus sizes or preprocessing methods
- The specific ranking of algorithms would remain consistent with different evaluation metrics

## Next Checks

1. **Cross-task validation**: Test whether the cognitive plausibility rankings (WordPiece/BPE > UnigramLM) hold when evaluated against other cognitive measures such as reading time corpora, eye-tracking data, or production tasks, to determine if lexical decision is representative of broader cognitive processing.

2. **Corpus size sensitivity analysis**: Systematically vary the training corpus size (e.g., 10K, 50K, 100K, 200K sentences) to determine whether the observed cognitive plausibility patterns are robust to different amounts of training data, particularly for morphologically rich languages.

3. **Alternative morphological evaluation**: Compare the SIGMORPHON-based morpheme coverage analysis with human intuition-based morphological segmentation tasks or psycholinguistic norms to validate whether the morphological annotation scheme used aligns with actual cognitive processing of morphemes.