---
ver: rpa2
title: Large Language Models as Generalizable Policies for Embodied Tasks
arxiv_id: '2310.17722'
source_url: https://arxiv.org/abs/2310.17722
tags:
- language
- arxiv
- llarp
- training
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We demonstrate that large language models (LLMs) can be adapted
  to be generalizable policies for embodied visual tasks. Our approach, called Large
  LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen
  LLM to take as input text instructions and visual egocentric observations and output
  actions directly in the environment.
---

# Large Language Models as Generalizable Policies for Embodied Tasks

## Quick Facts
- **arXiv ID**: 2310.17722
- **Source URL**: https://arxiv.org/abs/2310.17722
- **Reference count**: 40
- **Key outcome**: LLaRP achieves 42% success rate on 1,000 unseen tasks, 1.7x better than baselines, by adapting frozen LLMs for embodied visual tasks.

## Executive Summary
This paper introduces LLaRP, a method that adapts pre-trained frozen large language models (LLMs) to serve as generalizable policies for embodied visual tasks. The approach uses reinforcement learning to train lightweight adapter modules that process text instructions and visual observations, while keeping the LLM backbone frozen to preserve its language reasoning capabilities. LLaRP is evaluated on the novel Language Rearrangement benchmark consisting of 150,000 training and 1,000 testing tasks, demonstrating superior generalization to novel instructions and robust performance on unseen tasks.

## Method Summary
LLaRP adapts a frozen pre-trained LLM (e.g., LLaMA-7B) by adding learned input and output adapter layers. The observation encoder processes visual inputs and projects them to the language model token embedding dimension, while the action output MLP predicts actions and value estimates. During training, only the adapter modules are updated using reinforcement learning with DD-PPO (Distributed Deep Deterministic Policy Gradient), while the LLM backbone and visual encoder remain frozen. This approach leverages the LLM's pre-existing language understanding and reasoning capabilities while learning task-specific embodied control through environmental interactions.

## Key Results
- LLaRP achieves 42% success rate on 1,000 unseen Language Rearrangement tasks, 1.7x better than other learned baselines
- Demonstrates robust performance on complex paraphrasings of task instructions
- Shows strong generalization to novel tasks requiring previously unseen optimal behaviors
- Learns more efficiently than baselines, converging in 50M-100M steps while being substantially larger than LSTM-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained frozen LLM backbones preserve language reasoning capabilities while adapter layers learn task-specific embodied control
- Mechanism: By freezing the LLM backbone and only training lightweight adapter modules (observation encoder and action output MLP), the system maintains the general language understanding and reasoning capabilities of the pre-trained LLM while adapting it to process visual observations and generate environment actions
- Core assumption: The frozen LLM contains sufficient general knowledge about language understanding and reasoning to handle novel instructions without fine-tuning
- Evidence anchors:
  - [abstract]: "Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior"
  - [section 3.2]: "During training, we freeze the LLM backbone and the visual encoder. A frozen LLM backbone helps maintain language reasoning capabilities, which can be lost during fine tuning"
  - [corpus]: Weak - no direct corpus evidence found about frozen LLM preservation of reasoning capabilities

### Mechanism 2
- Claim: Large language models provide superior generalization to novel behaviors compared to traditional RL architectures
- Mechanism: The LLM's exposure to diverse language patterns and reasoning during pre-training enables it to handle instructions that require novel behaviors not seen during training, while traditional architectures like LSTMs struggle with such generalization
- Core assumption: The diversity and scale of LLM pre-training data provides better generalization foundations than smaller, task-specific architectures
- Evidence anchors:
  - [abstract]: "LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines"
  - [section 5.2]: "LLaRP is almost 1.7x better than the next best performing baseline, 42% vs 25%. This trend is true for both Paraphrastic and Behavior generalizations"
  - [corpus]: Weak - no direct corpus evidence found about LLM vs traditional architecture performance comparison in embodied tasks

### Mechanism 3
- Claim: Reinforcement learning with frozen LLM backbones is more sample-efficient than imitation learning approaches
- Mechanism: The frozen LLM provides a strong prior that guides exploration and learning, reducing the number of samples needed to achieve good performance compared to learning from scratch or from demonstrations
- Core assumption: The pre-trained LLM's knowledge provides useful inductive biases that accelerate learning in the RL setting
- Evidence anchors:
  - [abstract]: "LLaRP also learns efficiently, learning faster during training and comparing favorably to training with expert demonstrations"
  - [section 5.2]: "LLaRP is the most sample efficient model, converging in around 50M-100M steps before LSTM-Flan, despite being a substantially larger model"
  - [section 5.2]: "LLaRP with RL vs LLaRP with IL with the same number of episodes/demonstrations. In Figure 4c, we see that for any number of episodes (or demonstrations) RL outperforms IL"
  - [corpus]: Weak - no direct corpus evidence found about RL vs IL efficiency comparison with frozen LLM backbones

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The embodied agent only has partial information through egocentric visual observations and must make decisions without full state knowledge
  - Quick check question: How does the POMDP formulation differ from a fully observable MDP in this context?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: Understanding how the LLM backbone processes sequences of token embeddings from both text and visual inputs
  - Quick check question: What is the role of the attention mask during training and how does it relate to the episode structure?

- Concept: Reinforcement learning with proximal policy optimization (PPO)
  - Why needed here: The method uses DD-PPO (distributed PPO) to train the policy through environmental interactions
  - Quick check question: How does freezing the LLM backbone affect the PPO update process compared to training all parameters?

## Architecture Onboarding

- Component map: Frozen LLM backbone (e.g., LLaMA-7B) → frozen visual encoder (VC1) → adapter modules (observation encoder MLP, action output MLP) → environment interaction
- Critical path: Text instruction → LLM text embedding → observation encoder → combined embeddings → LLM backbone → action output MLP → environment action
- Design tradeoffs: Freezing LLM weights preserves reasoning capabilities but limits adaptation; using adapter modules reduces parameter count but may limit expressivity
- Failure signatures: Poor performance on novel instructions suggests insufficient LLM knowledge; slow learning suggests ineffective adapter training; instability suggests issues with frozen backbone compatibility
- First 3 experiments:
  1. Test zero-shot performance of frozen LLM on a subset of Language Rearrangement tasks to verify reasoning capabilities
  2. Train with only the observation encoder adapter module frozen to assess impact on learning efficiency
  3. Compare performance with and without the action output MLP to verify its necessity

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the following areas are identified as potential directions for future research:

- Scaling potential: How does performance scale with increasing LLM size beyond 13B parameters?
- Real-world deployment: How does LLaRP perform on real-world robot platforms compared to simulation?
- Task complexity: How does LLaRP handle tasks with significantly larger numbers of objects and receptacles compared to the benchmark?

## Limitations

- The evaluation is limited to synthetic benchmarks rather than real-world deployment scenarios
- The frozen LLM assumption may limit performance on tasks requiring knowledge acquired after the LLM's training cutoff
- The scaling relationship between model size and performance is not thoroughly explored

## Confidence

- **High confidence**: The technical approach of freezing LLM backbones and using adapter modules is sound and well-motivated by existing literature on parameter-efficient fine-tuning
- **Medium confidence**: The claim of superior generalization to novel behaviors is supported by empirical results but limited by the synthetic nature of the benchmark tasks
- **Low confidence**: The sample efficiency claims relative to imitation learning are based on a single benchmark and would benefit from validation across multiple environments and task distributions

## Next Checks

1. **Cross-environment validation**: Test LLaRP performance on at least two additional embodied AI benchmarks beyond Language Rearrangement to assess generalizability
2. **Ablation on adapter architecture**: Systematically vary the size and complexity of adapter modules to determine the minimal architecture needed for strong performance
3. **Knowledge cutoff study**: Design tasks that specifically test knowledge acquired after the LLM's training cutoff to quantify the impact of the frozen weights constraint