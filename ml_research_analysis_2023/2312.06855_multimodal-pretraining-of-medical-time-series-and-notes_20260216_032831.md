---
ver: rpa2
title: Multimodal Pretraining of Medical Time Series and Notes
arxiv_id: '2312.06855'
source_url: https://arxiv.org/abs/2312.06855
tags:
- data
- notes
- pretraining
- learning
- measurement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel pretraining method that leverages both
  clinical measurements and notes to improve representation learning for ICU data
  analysis. Their approach combines contrastive learning for aligning the two modalities
  with masked token prediction, allowing the model to learn rich representations even
  when labels are scarce.
---

# Multimodal Pretraining of Medical Time Series and Notes

## Quick Facts
- **arXiv ID**: 2312.06855
- **Source URL**: https://arxiv.org/abs/2312.06855
- **Reference count**: 18
- **Key outcome**: Pretraining method combines contrastive alignment and masked prediction for clinical measurements and notes, achieving strong semi-supervised performance with minimal labels

## Executive Summary
This paper introduces a multimodal pretraining approach that leverages both clinical measurements and notes to improve representation learning for ICU data analysis. The method combines contrastive learning for aligning the two modalities with masked token prediction, enabling the model to learn rich representations even when labels are scarce. The authors demonstrate significant improvements in semi-supervised settings on the MIMIC-III dataset, particularly when only 1% of labels are available.

## Method Summary
The method involves two phases: pretraining and fine-tuning. During pretraining, the model learns to align clinical measurements and notes through a contrastive objective while also performing masked token prediction. Two separate transformer encoders process each modality independently, with class tokens aligned through the contrastive objective. The pretraining combines alignment and masked prediction components, allowing the model to learn shared representations across modalities. After pretraining, the model is fine-tuned on downstream tasks like mortality prediction and phenotyping using either modality independently.

## Key Results
- In-hospital mortality AUC-ROC improves by 0.17 compared to randomly initialized model in 1% label setting
- Phenotyping AUC-PR improves by 0.1 in 1% label setting
- Strong performance gains persist across different label fractions (1%, 10%, 50%)
- Dual-encoder architecture enables flexible use of either modality independently after pretraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive alignment between clinical measurements and notes enables learning shared representations when labels are scarce
- **Mechanism**: Maps both modalities into joint embedding space using positive pairs (same ICU stay) and negative pairs (different stays), maximizing similarity for aligned data
- **Core assumption**: Clinical measurements and notes describing same ICU stay contain complementary but related information
- **Evidence anchors**: Weak corpus support; relies primarily on proposed methodology
- **Break condition**: Large modality gap (temporal misalignment or semantic unrelatedness) may prevent meaningful shared representations

### Mechanism 2
- **Claim**: Masked prediction tasks help learn robust representations by forcing reconstruction of missing tokens
- **Mechanism**: Random tokens in notes and measurements are masked and replaced with special tokens; model learns to predict these using context from both modalities
- **Core assumption**: Reconstructing masked content requires understanding full context, encouraging richer representations
- **Evidence anchors**: Weak corpus support; methodology-driven
- **Break condition**: Excessive masking rate prevents accurate predictions due to insufficient context

### Mechanism 3
- **Claim**: Dual-encoder architecture allows flexible use of either modality independently after pretraining
- **Mechanism**: Separate transformers process measurements and notes independently with class tokens aligned through contrastive objective
- **Core assumption**: Aligned representations preserve modality-specific information while enabling cross-modal transfer
- **Evidence anchors**: No corpus evidence supporting dual-encoder design choice
- **Break condition**: If one modality is significantly noisier, shared representation may be dominated by cleaner modality

## Foundational Learning

- **Concept**: Contrastive learning
  - **Why needed here**: To align two different modalities (measurements and notes) into shared embedding space where similar samples are close and dissimilar ones are far apart
  - **Quick check question**: What defines a positive pair versus a negative pair in the contrastive objective for multimodal clinical data?

- **Concept**: Masked language modeling
  - **Why needed here**: To force model to learn contextual representations by predicting masked tokens, improving ability to capture dependencies within each modality
  - **Quick check question**: How does masking measurements differ from masking text tokens, and why is different loss function used?

- **Concept**: Transformer architecture
  - **Why needed here**: To process sequential data (time series measurements and text notes) with self-attention, capturing long-range dependencies within each modality
  - **Quick check question**: Why use sinusoidal positional embeddings for measurements instead of learned embeddings?

## Architecture Onboarding

- **Component map**: Two separate transformer encoders (measurements and notes) → shared contrastive alignment objective → optional masked prediction task → class token alignment layer → downstream linear classifier
- **Critical path**: Data preprocessing → multimodal pretraining (contrastive + masking) → linear evaluation or semi-supervised fine-tuning → task-specific evaluation
- **Design tradeoffs**: Dual encoders allow independent use but may miss cross-modal interactions; joint encoder would capture interactions but lose flexibility; masking increases training pairs but adds complexity
- **Failure signatures**: Poor recall in retrieval tasks indicates weak alignment; high variance in semi-supervised results suggests overfitting or catastrophic forgetting; low zero-shot performance means pretraining didn't capture relevant features
- **First 3 experiments**:
  1. Pretrain with contrastive alignment only (no masking) and evaluate retrieval performance
  2. Add masking to pretraining and compare retrieval and downstream task performance
  3. Compare semi-supervised performance with 1%, 10%, and 50% labels against baseline models

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do learned embeddings generalize to clinical settings outside ICU data, such as outpatient care or chronic disease management?
- **Basis in paper**: [inferred] Methodology could potentially apply to other healthcare domains
- **Why unresolved**: Study limited to ICU data from MIMIC-III; no exploration of performance on other clinical data types
- **What evidence would resolve it**: Testing model on diverse healthcare datasets (outpatient records, chronic disease management) and comparing performance to other pretraining methods

### Open Question 2
- **Question**: What specific clinical features or patterns are most influential in model's predictions, and how can these be interpreted by clinicians?
- **Basis in paper**: [explicit] Mentions importance of learned representations but lacks interpretability analysis
- **Why unresolved**: Model shows improved performance but no detailed analysis of which clinical features drive predictions
- **What evidence would resolve it**: Feature importance analysis or SHAP values to identify and visualize most influential clinical features

### Open Question 3
- **Question**: How does model's performance change when applied to real-time clinical decision-making, and what are potential risks or benefits?
- **Basis in paper**: [inferred] Demonstrates effectiveness in semi-supervised settings but doesn't address real-time clinical environments
- **Why unresolved**: Focuses on offline evaluation metrics; doesn't explore behavior in dynamic, real-time clinical scenarios
- **What evidence would resolve it**: Implementing model in simulated or real-time ICU environment to assess performance, reliability, and impact on clinical outcomes

## Limitations

- Weak corpus evidence supporting specific design choices for multimodal contrastive learning in medical domains
- Insufficient detail for independent reproduction of methodology and hyperparameters
- No comparative ablations to isolate contributions of contrastive versus masked prediction components
- Limited exploration of model interpretability and clinical feature importance

## Confidence

- Multimodal pretraining methodology: Medium
- Semi-supervised performance claims: Medium
- Contrastive alignment effectiveness: Low
- Masked prediction contribution: Low

## Next Checks

1. **Ablation Study Validation**: Implement and test model with only contrastive learning (no masking) and only masked prediction (no contrastive alignment) to quantify independent contributions of each mechanism

2. **Corpus Evidence Gap**: Conduct systematic literature review to identify whether similar multimodal contrastive approaches have been validated in medical domains

3. **Hyperparameter Sensitivity Analysis**: Replicate experiments while systematically varying key hyperparameters (learning rate, batch size, masking rate) to determine if improvements are robust or configuration-dependent