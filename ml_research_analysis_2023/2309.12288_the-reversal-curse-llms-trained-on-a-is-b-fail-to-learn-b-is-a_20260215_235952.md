---
ver: rpa2
title: 'The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"'
arxiv_id: '2309.12288'
source_url: https://arxiv.org/abs/2309.12288
tags:
- name
- reversal
- curse
- description
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLMs trained on \u201CA is B\u201D fail to generalize to the reverse\
  \ \u201CB is A\u201D relationship. We show this reversal curse across multiple model\
  \ families and sizes."
---

# The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"

## Quick Facts
- arXiv ID: 2309.12288
- Source URL: https://arxiv.org/abs/2309.12288
- Authors: 
- Reference count: 40
- Key outcome: LLMs trained on "A is B" fail to generalize to the reverse "B is A" relationship across multiple model families and sizes

## Executive Summary
The Reversal Curse demonstrates that auto-regressive language models fundamentally fail to learn bidirectional relationships from unidirectional training. When trained on facts like "A is B", models cannot generalize to answer "B is A" even when they can perfectly answer the original direction. This failure persists across model sizes and families, suggesting it's an inherent limitation of auto-regressive training rather than a capacity issue. The phenomenon is demonstrated both on synthetic data (near-zero accuracy on reversed prompts) and real-world data (79% vs 33% accuracy for parent vs. child queries in GPT-4).

## Method Summary
The study uses finetuning experiments on GPT-3 and Llama-1 models with synthetic datasets of 30 fictitious celebrity facts, each with 30 paraphrases (900 examples total). Models are trained on either NameToDescription, DescriptionToName, or Both subsets with hyperparameter sweeps across learning rates (0.05-0.4) and batch sizes (1-16) for 10-20 epochs. Evaluation uses temperature 0 generation on held-out prompts, measuring exact match accuracy and comparing log-probabilities of correct vs random completions. Real-world validation uses 1573 child-parent pairs from IMDB's top 1000 celebrities, comparing GPT-4's accuracy on parent identification (79%) versus child identification (33%).

## Key Results
- Models trained on synthetic facts achieve near-perfect accuracy on forward prompts but near-zero accuracy on reversed prompts
- No statistically significant difference in log-probabilities between correct names and random names for reversed prompts
- GPT-4 answers parent queries correctly 79% of the time but child queries only 33% of the time
- The Reversal Curse persists across model families (GPT-3, Llama) and sizes, with flat scaling plots
- Data augmentation with both directions present does not resolve the curse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auto-regressive models learn asymmetric key-value mappings for entity relationships
- Mechanism: When trained on "A is B", the model stores a mapping from A's representation to B's representation in feed-forward layers, but not vice versa
- Core assumption: Factual associations are stored as key-value pairs in middle MLP layers, with keys and values corresponding to different entities
- Evidence anchors:
  - [abstract]: "Geva et al. (2021, 2022, 2023) analyze the internal mechanisms behind factual recall in Transformers. They claim that these models represent factual associations as key-value pairs in their feed-forward layers."
  - [section]: "It would make rational sense for this gradient update to also alter the representation of B to contain information about A. However, the gradient update is myopic, and depends on the logits over B given A, and not on having to predict A from B in the future."
  - [corpus]: Weak evidence - corpus contains related work on model editing and knowledge storage but no direct mechanism studies
- Break condition: If models use symmetric attention mechanisms or bidirectional training that updates both directions simultaneously

### Mechanism 2
- Claim: Models fail to meta-learn the pattern that "A is B" implies "B is A" occurs more frequently in natural text
- Mechanism: While "A is B" and "B is A" often co-occur in training data, models don't learn this correlation because they treat each sentence independently during training
- Core assumption: Humans vary sentence order in documents, making both directions more likely when one appears
- Evidence anchors:
  - [abstract]: "Thus, a good meta-learner would increase the probability of an instance of '<description> is <name>' after being trained on '<name> is <description>'. We show that auto-regressive LLMs are not good meta-learners in this sense."
  - [section]: "Sentences of the form '<name> is <description>' and '<description> is <name>' often co-occur in pretraining datasets; if the former appears in a dataset, the latter is more likely to appear."
  - [corpus]: Moderate evidence - multiple papers discuss meta-learning limitations and the Reversal Curse
- Break condition: If models are trained with explicit meta-learning objectives or contrastive learning between forward and reverse pairs

### Mechanism 3
- Claim: The gradient update for "A is B" doesn't propagate information back to B's representation in a way that enables B→A inference
- Mechanism: During training on "A is B", gradients flow to update A's representation to include B information, but not vice versa because the prediction target is B, not A
- Core assumption: Auto-regressive training is fundamentally unidirectional in its gradient flow
- Evidence anchors:
  - [abstract]: "if 'A is B' (or equivalently 'A=B') is true, then 'B is A' follows by the symmetry property of the identity relation."
  - [section]: "When a model is updated on 'A is B', this gradient update may slightly alter the representation of A such that it contains information about B... It would make rational sense for this gradient update to also alter the representation of B to contain information about A. However, the gradient update is myopic."
  - [corpus]: Weak evidence - corpus mentions influence functions and gradient effects but doesn't directly study this mechanism
- Break condition: If models use bidirectional training objectives or explicit symmetry regularization

## Foundational Learning

- Concept: Bidirectional relationships in knowledge representation
  - Why needed here: The Reversal Curse is fundamentally about understanding that relationships can be expressed in both directions
  - Quick check question: If "Paris is the capital of France" is true, what other statement must also be true?

- Concept: Meta-learning and generalization beyond training distribution
  - Why needed here: Models need to recognize that if they've seen "A is B" frequently, "B is A" should be more likely even if not explicitly trained on
  - Quick check question: If you train a model to recognize dogs in images, what pattern might it learn about cats if they often appear in similar contexts?

- Concept: Gradient flow and representation learning in transformers
  - Why needed here: Understanding how information flows during training helps explain why "A is B" doesn't automatically enable "B is A" inference
  - Quick check question: In a transformer predicting token B after A, which token's representation gets updated during backpropagation?

## Architecture Onboarding

- Component map: Input tokenizer → Embedding layer → Transformer blocks (attention + MLP) → Output logits → Loss function
- Critical path: 1. Training data preparation (synthetic facts in specific orders) 2. Model finetuning with one-directional facts 3. Evaluation with reversed prompts 4. Analysis of log-probabilities for correct vs random completions
- Design tradeoffs: Training efficiency vs. bidirectional knowledge capture; Model size vs. generalization capability (scaling plots are flat, suggesting size doesn't help); Synthetic data control vs. real-world applicability
- Failure signatures: Near-zero accuracy on reversed prompts despite perfect accuracy on forward prompts; No statistically significant difference in log-probabilities for correct vs random names; Consistency across model families and sizes
- First 3 experiments: 1. Finetune GPT-3 on synthetic facts in one direction, evaluate on reversed prompts 2. Try data augmentation with both directions present, check if meta-learning occurs 3. Test with different model families (Llama, GPT-3) and sizes to verify scaling behavior

## Open Questions the Paper Calls Out

- Question: What mechanisms cause the Reversal Curse in autoregressive language models?
  - Basis in paper: Explicit - The paper states "We mostly leave this for future work" and provides a brief sketch suggesting gradient updates may store information asymmetrically
  - Why unresolved: The paper acknowledges that the exact mechanisms behind the Reversal Curse are unknown and only provides a preliminary explanation
  - What evidence would resolve it: Detailed analysis of internal model representations during training to identify asymmetric information storage, or ablation studies testing specific architectural components

- Question: Does the Reversal Curse affect non-autoregressive models as well?
  - Basis in paper: Explicit - The paper explicitly states "Do non-autoregressive models suffer from it as well?" as an open question
  - Why unresolved: The paper only tests autoregressive models and does not investigate other model architectures
  - What evidence would resolve it: Direct experimentation comparing autoregressive and non-autoregressive models on the same reversal tasks

- Question: How can models be trained to overcome the Reversal Curse?
  - Basis in paper: Explicit - The paper tests various methods (data augmentation, hyperparameter sweeps, different model sizes) but none resolve the curse
  - Why unresolved: The paper concludes that "The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation"
  - What evidence would resolve it: Development and validation of a training method that successfully enables models to generalize to reversed relationships

## Limitations

- The Reversal Curse may be less severe for naturally occurring relationships, as GPT-4 still achieves 79% accuracy on parent identification
- The paper doesn't explore whether larger context windows or chain-of-thought prompting could mitigate the effect
- The corpus analysis reveals only 25 related papers, suggesting this phenomenon may be under-studied or the terminology not widely adopted

## Confidence

**High confidence**: The experimental results showing synthetic data failure (near-zero accuracy on reversed prompts, no log-probability advantage for correct answers) are robust and well-supported by multiple model families and sizes. The real-world GPT-4 comparison (79% vs 33% accuracy) provides strong external validation.

**Medium confidence**: The proposed mechanism about asymmetric key-value storage in MLP layers is plausible given the evidence from Geva et al. (2021-2023), but direct empirical validation of this specific claim is limited in the paper. The corpus analysis shows related work but doesn't confirm this mechanism.

**Low confidence**: The meta-learning hypothesis about frequency patterns in natural text is speculative and lacks direct evidence. While the paper mentions that forward and reverse sentences often co-occur in training data, it doesn't provide quantitative analysis of this claim.

## Next Checks

1. **Mechanism validation**: Use influence function analysis or model editing techniques to directly test whether "A is B" training updates only A's representation with B information, not vice versa. This would confirm or refute the asymmetric key-value storage hypothesis.

2. **Context window effects**: Test whether increasing context window size or using retrieval-augmented generation enables models to recover reversed relationships by accessing the original facts. This would determine if the curse is architectural or can be mitigated with better retrieval.

3. **Cross-dataset generalization**: Evaluate the Reversal Curse on additional real-world datasets beyond IMDB parent-child relationships (e.g., geographical facts, historical relationships) to determine if the 79% vs 33% gap generalizes or is specific to celebrity knowledge.