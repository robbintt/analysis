---
ver: rpa2
title: 'The language of prompting: What linguistic properties make a prompt successful?'
arxiv_id: '2311.01967'
source_url: https://arxiv.org/abs/2311.01967
tags:
- movie
- given
- could
- prompts
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how linguistic properties of prompts affect
  LLM performance, systematically varying grammatical mood, tense, aspect, modality
  and synonymy in semantically equivalent prompts. Contrary to common assumptions,
  model performance does not correlate with prompt perplexity, word frequency, ambiguity
  or length.
---

# The language of prompting: What linguistic properties make a prompt successful?

## Quick Facts
- **arXiv ID**: 2311.01967
- **Source URL**: https://arxiv.org/abs/2311.01967
- **Reference count**: 22
- **Key outcome**: LLM performance is highly sensitive to small linguistic variations in prompts, even when semantic content is held constant, with no correlation between performance and prompt perplexity, word frequency, ambiguity, or length.

## Executive Summary
This paper systematically investigates how linguistic properties of prompts affect LLM performance across sentiment classification, NLI, and question answering tasks. The authors construct 550 semantically equivalent prompts that systematically vary in grammatical mood, tense, aspect, modality, and synonymy. Contrary to common assumptions, they find that model performance does not correlate with prompt perplexity, word frequency, ambiguity, or length. Performance varies dramatically across semantically equivalent prompts, and prompts transfer poorly between datasets and models. The work highlights the need for more robust evaluation frameworks that account for linguistic variability.

## Method Summary
The authors conduct zero-shot evaluations of five LLMs (LLaMA 30b, OPT 1.3b, OPT-IML 1.3b, OPT 30b, OPT-IML 30b) across six datasets (SST-2, IMDB, RTE, CB, BoolQ, ARC-E). They construct 550 manually crafted prompts varying in grammatical mood, tense, aspect, modality, and synonymy. For each prompt-dataset-model combination, they measure accuracy and calculate perplexity. They perform statistical tests (Friedman, Wilcoxon) and correlation analysis (Spearman, Pearson) to examine relationships between linguistic properties and performance. The study focuses on decoder-only models but includes preliminary results for an encoder-decoder model (Flan-T5).

## Key Results
- Performance varies significantly across semantically equivalent prompts, with no single prompt structure performing best across all models and tasks
- No correlation found between performance and perplexity, word frequency, ambiguity, or prompt length
- Prompts transfer poorly between datasets and models, with large performance drops when optimal prompts from one dataset are applied to another
- Large performance variance exists even among instruction-tuned models, suggesting limited robustness to linguistic variations

## Why This Works (Mechanism)

### Mechanism 1
LLMs treat linguistic form (tense, mood, modality, synonyms) as meaningful signals that influence probability distributions over outputs. A change in prompt structure alters the model's internal representations enough to shift predictions, regardless of semantic equivalence. The model has not fully "learned" to ignore superficial form variations, and still responds to them as predictive cues.

### Mechanism 2
LLMs do not rely solely on frequency-based or perplexity-based heuristics to choose correct outputs; instead, they may use more abstract, learned syntactic or semantic patterns that override statistical cues. The training process has led to representations that encode structure beyond surface-level statistics.

### Mechanism 3
LLMs' learned representations are highly context-dependent; a prompt that works well for one dataset/model combination may fail on another because the relevant features or correlations differ across training distributions. The model's understanding of "correctness" is dataset-specific and not purely task-general.

## Foundational Learning

- **Linguistic structure (mood, tense, aspect, modality)**: Needed to understand the systematic variations tested; Quick check: Can you explain the difference between indicative and imperative mood, and give an example of how each might be used in a prompt?

- **Perplexity as a measure of language model confidence**: Needed to understand why the absence of correlation with performance is surprising; Quick check: What does it mean if a prompt has high perplexity for a model? How might this relate to the model's familiarity with that language?

- **Spearman and Pearson correlation coefficients**: Needed to understand the statistical analysis; Quick check: When would you use Spearman vs Pearson correlation? What does a significant correlation tell you about the relationship between two variables?

## Architecture Onboarding

- **Component map**: Input prompt → Tokenizer → LLM → Log probabilities for each answer option → Argmax over options → Predicted answer. Key variables: prompt formulation, model parameters, answer space.

- **Critical path**: Prompt generation → Model inference → Log probability calculation → Prediction. Bottlenecks: tokenization time, GPU memory for large models.

- **Design tradeoffs**: Manual prompt crafting vs. automatic generation. Manual gives control but is slow; automatic is faster but may introduce noise or atypical language.

- **Failure signatures**: Large performance variance across semantically equivalent prompts; poor transfer between datasets; lack of correlation between expected metrics (perplexity, frequency) and accuracy.

- **First 3 experiments**:
  1. Run the same prompt across all five models and measure accuracy variance; confirm instability.
  2. Generate parallel prompts varying only in mood (indicative vs interrogative) for a fixed dataset; measure performance difference.
  3. Replace content words in a high-performing prompt with rare synonyms; measure change in accuracy and perplexity.

## Open Questions the Paper Calls Out

### Open Question 1
How does performance vary across different model sizes beyond 30B parameters, and what scaling patterns emerge? The paper's computational constraints prevented testing larger models, leaving the scaling relationship between model size and prompt sensitivity unexplored. Experiments comparing models of 50B+ parameters across the same linguistic variations would reveal whether larger models show reduced sensitivity to prompt variations.

### Open Question 2
Do encoder-decoder architectures show similar or different patterns of prompt sensitivity compared to decoder-only models? The paper only provides illustrative results for one encoder-decoder model, making it unclear whether the observed patterns generalize across architectures. Systematic experiments across multiple encoder-decoder models (T5, T0, Flan-T5) using the same linguistic variations would reveal architecture-specific patterns.

### Open Question 3
How do different multilingual models process linguistic variations in prompts across languages with varying morphological complexity? All experiments were conducted in English, and the paper suggests that linguistic properties might interact differently with model behavior in other languages. Experiments applying the same linguistic variations to prompts in morphologically rich languages (e.g., German, Finnish, Turkish) would reveal cross-linguistic differences in model behavior.

## Limitations
- Focus on zero-shot evaluation may not generalize to few-shot or fine-tuned scenarios
- Manual prompt construction limits scale and diversity of linguistic variations tested
- Experiments only examine English prompts, limiting applicability to multilingual contexts
- Correlation analysis may not capture complex nonlinear relationships between linguistic features and model behavior

## Confidence
- **High confidence**: Performance varies significantly across semantically equivalent prompts; poor cross-dataset and cross-model prompt transfer
- **Medium confidence**: Perplexity, word frequency, ambiguity, and length do not correlate with performance; proposed evaluation framework represents methodological advancement
- **Low confidence**: This work "breaks ground for a more robust evaluation of LLM performance" is somewhat overstated

## Next Checks
1. Replicate the correlation analysis between perplexity and accuracy using a larger sample of prompts (n > 1000) to verify the absence of correlation is not due to sample size limitations.

2. Test prompt transfer performance after instruction-tuning the base models on a meta-dataset containing examples from all six target datasets to isolate whether dataset-specific learning versus general task knowledge drives the poor transfer.

3. Conduct a controlled experiment where semantically equivalent prompts are automatically generated using linguistic templates to assess whether manual prompt crafting introduces biases in the observed performance variance patterns.