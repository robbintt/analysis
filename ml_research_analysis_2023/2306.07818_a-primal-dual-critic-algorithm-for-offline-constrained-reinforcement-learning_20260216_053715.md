---
ver: rpa2
title: A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning
arxiv_id: '2306.07818'
source_url: https://arxiv.org/abs/2306.07818
tags:
- algorithm
- policy
- offline
- learning
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PDCA, a primal-dual algorithm for offline constrained
  reinforcement learning with function approximation. PDCA learns a policy that maximizes
  the expected cumulative reward subject to constraints on expected cumulative cost
  using an existing dataset.
---

# A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2306.07818
- **Source URL**: https://arxiv.org/abs/2306.07818
- **Reference count**: 0
- **Primary result**: PDCA learns a policy that maximizes expected cumulative reward subject to constraints on expected cumulative cost using an existing dataset, requiring only concentrability and realizability assumptions.

## Executive Summary
This paper introduces PDCA, a primal-dual algorithm for offline constrained reinforcement learning with function approximation. PDCA learns a policy that maximizes the expected cumulative reward subject to constraints on expected cumulative cost using an existing dataset. The algorithm runs a primal-dual algorithm on the Lagrangian estimated by critics, with the primal player using a no-regret policy optimization oracle and the dual player using a no-regret online linear optimization oracle. The main result shows that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem, under weaker assumptions than previous work.

## Method Summary
PDCA is a primal-dual algorithm for offline constrained reinforcement learning that uses function approximation. It estimates the Lagrangian using critics and runs a primal-dual optimization algorithm on this estimate. The primal player uses a no-regret policy optimization oracle to maximize the Lagrangian estimate, while the dual player uses a no-regret online linear optimization oracle to minimize it. The algorithm requires only concentrability and realizability assumptions for sample-efficient learning, unlike previous work that requires Bellman completeness. PDCA finds a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem.

## Key Results
- PDCA achieves sample-efficient learning under only concentrability and realizability assumptions, without requiring Bellman completeness.
- The algorithm successfully finds a near saddle point of the Lagrangian using no-regret primal and dual updates.
- Experiments on RWRL environments show PDCA performs comparably to the state-of-the-art algorithm COptiDICE.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PDCA can find a near saddle point of the Lagrangian without requiring Bellman completeness, only using concentrability and realizability assumptions.
- **Mechanism**: The algorithm decomposes the Lagrangian estimation into separate reward and cost critics, each solving a minimax optimization problem that bounds Bellman errors via marginalized importance weights. The primal player uses no-regret updates on the estimated Lagrangian, while the dual player uses no-regret linear optimization on cost estimates. This structure allows convergence to a near saddle point under weaker assumptions.
- **Core assumption**: Value functions and marginalized importance weights are realizable in the chosen function classes; concentrability bounds the distribution shift between candidate policies and the behavior policy.
- **Evidence anchors**:
  - [abstract] "Unlike previous work that requires concentrability and a strong Bellman completeness assumption, PDCA only requires concentrability and realizability assumptions for sample-efficient learning."
  - [section 3.1] "The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player."
  - [corpus] Weak - only general RL papers found, no direct evidence on Bellman completeness relaxation.
- **Break condition**: If the function class fails to contain the value functions or marginalized importance weights (realizability fails), or if concentrability is violated, the critic estimates become biased and the saddle point approximation breaks down.

### Mechanism 2
- **Claim**: The cost critic used by the λ-player accurately estimates the expected cumulative cost of a policy with high probability.
- **Mechanism**: The cost critic solves a minimax problem that minimizes the maximum Bellman error over the marginalized importance weight class. Concentration inequalities show that with enough samples, the empirical minimizer is close to the true cost in ℓ∞ norm.
- **Core assumption**: The marginalized importance weight class W contains the true weight functions and is bounded.
- **Evidence anchors**:
  - [section 4.1] "We provide a concentration bound of the value estimate returned by the OPE algorithm... Theorem 2. Let π be any policy with wπ ∈ W where W satisfies the boundedness assumption (Assumption C)."
  - [section 3.5] "The λ-player receives estimates hk(s0, πk) for JC(πk) from a cost critic and invokes a no-regret online linear optimization oracle on the linear function λ 7→ λ · (τ − hk(s0, πk))."
  - [corpus] Weak - no direct citations for cost critic concentration; relies on general offline RL bounds.
- **Break condition**: If W does not contain the true marginalized importance weights (realizability fails) or if the sample size is too small, the cost estimates become inaccurate and the λ-player's updates are misguided.

### Mechanism 3
- **Claim**: The no-regret properties of the primal and dual players ensure that the averaged policies and Lagrange multipliers form a near saddle point of the Lagrangian.
- **Mechanism**: The primal player's no-regret updates guarantee sublinear regret against any competing policy, while the dual player's no-regret updates guarantee sublinear regret against any competing Lagrange multiplier. Combining these with statistical error bounds from the critics yields a near saddle point.
- **Core assumption**: The no-regret oracles (e.g., natural policy gradient, online gradient descent) have known regret bounds and are correctly implemented.
- **Evidence anchors**:
  - [section 3.1] "The π-player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player."
  - [section 4.2] "We show that PDCA, which runs a primal-dual algorithm on the estimated Lagrangian, successfully finds a near saddle point of the Lagrangian."
  - [corpus] Weak - general RL papers found, no specific evidence on no-regret guarantees in constrained RL.
- **Break condition**: If the no-regret oracles fail to converge (e.g., bad learning rates, non-convex landscapes), or if the critics' errors are too large, the regret bounds fail and the near saddle point approximation breaks down.

## Foundational Learning

- **Concept**: Lagrangian duality in constrained optimization
  - **Why needed here**: The constrained RL problem is transformed into an unconstrained problem by introducing Lagrange multipliers; understanding the saddle point property is crucial for analyzing convergence.
  - **Quick check question**: What is the Lagrangian of a constrained optimization problem, and what does it mean for a point to be a saddle point?

- **Concept**: Bellman equation and value function realizability
  - **Why needed here**: The algorithm relies on the assumption that the true Q-value functions of policies are representable in the chosen function class; this underpins the critic's ability to estimate the Lagrangian.
  - **Quick check question**: What is the Bellman equation, and how does realizability of value functions differ from Bellman completeness?

- **Concept**: Marginalized importance weights and concentrability
  - **Why needed here**: These are used to correct for distribution shift between the behavior policy and candidate policies, ensuring that the critics can evaluate policies accurately.
  - **Quick check question**: How do marginalized importance weights relate to the concentrability coefficient, and why are they needed in offline RL?

## Architecture Onboarding

- **Component map**:
  - Data loader -> Reward critic, Cost critics, OPE critic -> Policy network -> Dual variable
  - Data loader -> Reward critic, Cost critics, OPE critic -> Policy network -> Dual variable
  - Policy network -> Natural policy gradient -> Dual variable -> Exponentiated gradient

- **Critical path**:
  1. Sample minibatch from dataset.
  2. Update reward and cost critics via stochastic gradient descent.
  3. Update policy network via natural policy gradient on estimated Lagrangian.
  4. Update dual variable via exponentiated gradient on cost estimates.
  5. Repeat for K iterations.

- **Design tradeoffs**:
  - **Critic accuracy vs. sample efficiency**: More expressive critics improve accuracy but require more data.
  - **Learning rates**: Fast rates for critics vs. slow rates for policy/dual updates to balance stability and speed.
  - **Function class expressiveness**: Larger classes improve realizability but increase overfitting risk.

- **Failure signatures**:
  - **High variance in cost estimates**: Indicates insufficient data or poor critic design.
  - **Policy collapse**: May signal bad learning rates or critic bias.
  - **Constraint violation**: Could mean λ-player is not updating correctly or critics are inaccurate.

- **First 3 experiments**:
  1. **Sanity check on critics**: Train critics on a small dataset with known value functions; verify that estimates are close to true values.
  2. **Policy optimization test**: Fix critics to known values; run policy updates and check if the estimated Lagrangian improves.
  3. **Dual variable test**: Fix policy and critics; run dual updates and check if cost estimates align with true values.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the PDCA algorithm scale to larger, more complex MDPs with high-dimensional state and action spaces?
- **Basis in paper**: [inferred] The paper focuses on theoretical analysis and experiments on tabular and RWRL environments, but does not explore scaling to high-dimensional settings.
- **Why unresolved**: The current experiments use function approximation with neural networks, but the analysis is limited to finite function classes. Scaling to truly high-dimensional problems requires understanding how the algorithm performs with continuous or infinite-dimensional function classes.
- **What evidence would resolve it**: Empirical evaluation of PDCA on benchmark high-dimensional continuous control tasks (e.g., MuJoCo environments) with varying function class sizes and sample complexities.

### Open Question 2
- **Question**: Can the realizability assumptions on value functions and marginalized importance weights be relaxed without compromising sample efficiency?
- **Basis in paper**: [explicit] The authors note that relaxing the all-policy MIG realizability to single-policy MIG realizability is an interesting future direction.
- **Why unresolved**: The current algorithm requires all-policy MIG realizability for the cost critic used by the λ-player. This is a strong assumption that may not hold in practice, and relaxing it could make the algorithm more widely applicable.
- **What evidence would resolve it**: Developing a variant of PDCA that uses pessimistic estimates for costs and modifying the λ-player to provide the same guarantee even with single-policy MIG realizability, followed by empirical validation.

### Open Question 3
- **Question**: How does the choice of the no-regret policy optimization oracle and online linear optimization oracle affect the performance of PDCA in practice?
- **Basis in paper**: [explicit] The paper mentions using natural policy gradient and exponentiated gradient algorithms as examples, but does not explore the impact of different choices.
- **Why unresolved**: The choice of optimization oracles can significantly impact the practical performance of the algorithm. Understanding which oracles work best for different problem settings is crucial for effective implementation.
- **What evidence would resolve it**: Systematic empirical comparison of PDCA with different optimization oracles (e.g., natural policy gradient, mirror descent, exponentiated gradient) on a range of constrained RL benchmarks, analyzing convergence speed and final performance.

## Limitations
- Experimental validation is limited to three RWRL tasks, providing minimal evidence of practical performance.
- Theoretical claims about no-regret properties are not directly verified in experiments.
- Strong realizability assumptions for marginalized importance weights may not hold in practice.
- Specific no-regret oracles required for implementation are not specified.

## Confidence
- **High**: The theoretical framework for Lagrangian-based constrained RL is sound, and the algorithmic structure is well-defined.
- **Medium**: The claims about relaxing Bellman completeness assumptions appear valid based on the analysis, though experimental evidence is limited.
- **Low**: The practical performance claims relative to state-of-the-art methods, given the minimal experimental validation.

## Next Checks
1. **Ablation study on function class size**: Systematically vary the expressiveness of the critic function classes to test the realizability assumptions and their impact on performance.
2. **Additional environment benchmarks**: Evaluate PDCA on more diverse constrained RL environments (e.g., safety-critical control tasks) to validate generalization.
3. **Oracle verification**: Implement and test specific no-regret policy optimization algorithms (e.g., natural policy gradient) to verify the theoretical regret bounds in practice.