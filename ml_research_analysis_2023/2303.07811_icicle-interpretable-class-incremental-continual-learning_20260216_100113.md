---
ver: rpa2
title: 'ICICLE: Interpretable Class Incremental Continual Learning'
arxiv_id: '2303.07811'
source_url: https://arxiv.org/abs/2303.07811
tags:
- task
- learning
- tasks
- prototypical
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ICICLE, an interpretable approach to class-incremental
  learning that addresses the problem of interpretability concept drift. ICICLE uses
  a prototypical part-based methodology and introduces three key novelties: interpretability
  regularization to retain previously learned concepts, proximity-based prototype
  initialization for fine-grained settings, and task-recency bias compensation.'
---

# ICICLE: Interpretable Class Incremental Continual Learning

## Quick Facts
- arXiv ID: 2303.07811
- Source URL: https://arxiv.org/abs/2303.07811
- Authors: 
- Reference count: 40
- This paper introduces ICICLE, an interpretable approach to class-incremental learning that addresses the problem of interpretability concept drift.

## Executive Summary
ICICLE addresses the critical challenge of maintaining interpretable explanations in class-incremental learning scenarios. The method builds upon prototypical part-based models and introduces three key innovations: interpretability regularization to preserve previously learned concepts, proximity-based prototype initialization for fine-grained classification, and task-recency bias compensation. ICICLE outperforms existing exemplar-free continual learning methods when applied to concept-based models, achieving significantly better performance and interpretability preservation on standard datasets.

## Method Summary
ICICLE extends prototypical part-based models (ProtoPNet) for class-incremental learning by introducing interpretability regularization (LIR loss) that minimizes changes in prototype similarities across tasks, proximity-based prototype initialization that clusters similar representations for better prototype placement, and task-recency bias compensation that equalizes class activations across tasks. The method trains task-specific prototypical part layers and fully-connected layers while maintaining shared backbone and add-on layers, with regularization terms to preserve interpretability and prevent forgetting.

## Key Results
- ICICLE achieves 72.8% average incremental accuracy on CUB-200-2011 dataset compared to 33.4% for EWC, 18.8% for LWF, and 32.5% for LWM
- ICICLE demonstrates better interpretability concept drift preservation with an IoU of 0.728 compared to 0.334 for EWC, 0.188 for LWF, and 0.325 for LWM
- ICICLE maintains strong performance across multiple task configurations (4, 10, 20 tasks) on both CUB-200-2011 and Stanford Cars datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interpretability regularization preserves prototype-based explanations by minimizing similarity changes between tasks.
- Mechanism: The LIR loss function measures the difference in similarity between a previous task's prototype and the current task's image representations, applying a mask to focus only on the highest similarity regions.
- Core assumption: Maintaining consistent prototype activations across tasks preserves the interpretability of explanations.
- Evidence anchors:
  - [abstract] "interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning"
  - [section 3.2] "We propose an additional regularization cost LIR... that minimizes the changes in the prototype similarities"
  - [corpus] Weak evidence - the corpus neighbors don't specifically address interpretability drift in continual learning
- Break condition: If the similarity mask threshold (γ) is set too low, the regularization may not effectively capture the most relevant regions, reducing its impact on preserving explanations.

### Mechanism 2
- Claim: Proximity-based prototype initialization improves performance by placing new task prototypes near relevant existing concepts.
- Mechanism: The method clusters representations from the new task that are most similar to existing prototypes, using these cluster centers as initial locations for new prototypes.
- Core assumption: New concepts in fine-grained classification are typically similar to previously learned concepts, making proximity initialization beneficial.
- Evidence anchors:
  - [abstract] "proximity-based prototype initialization strategy dedicated to the fine-grained setting"
  - [section 3.2] "This results in many candidates for new task prototypical parts. To obtain K · C t prototypes, we perform KMeans++ clustering"
  - [corpus] Weak evidence - no corpus neighbors directly address prototype initialization strategies in continual learning
- Break condition: If the α threshold for selecting similar representations is too high, the method may select irrelevant background regions, leading to poor initialization.

### Mechanism 3
- Claim: Task-recency bias compensation balances class activation across tasks by adjusting logits based on last task data.
- Mechanism: After training the final task, the method calculates constants for each task that equalize the average class activation across all tasks.
- Core assumption: When learning new tasks, similarities to previous task prototypes drop, creating a bias toward recently learned classes.
- Evidence anchors:
  - [abstract] "task-recency bias compensation devoted to prototypical parts"
  - [section 3.2] "we compensate for this using T − 1 constants obtained using the last tasks data"
  - [corpus] Weak evidence - corpus neighbors don't specifically address task-recency bias in prototypical part models
- Break condition: If the compensation constants are calculated on too small a validation set, they may not accurately represent the true activation distribution across tasks.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why continual learning methods are necessary for maintaining performance across tasks
  - Quick check question: What happens to performance on previous tasks when a neural network is trained on new tasks without any mitigation strategy?

- Concept: Prototype-based image recognition
  - Why needed here: ICICLE builds on prototypical part methodology where classification is based on similarity to learned prototypes
  - Quick check question: How does a prototypical part model make classification decisions differently from standard neural networks?

- Concept: Interpretability drift
  - Why needed here: The paper specifically addresses the problem of explanations becoming inconsistent over time as the model learns new tasks
  - Quick check question: What is interpretability concept drift and why is it particularly problematic in class-incremental learning scenarios?

## Architecture Onboarding

- Component map: Input image → Backbone f → Add-on fA → Task-specific gt → Task-specific ht → Classification output

- Critical path: Input image → Backbone f → Add-on fA → Task-specific gt → Task-specific ht → Classification output

- Design tradeoffs:
  - Separate gt and ht layers per task increase memory usage but enable better task isolation
  - Interpretability regularization adds computational overhead during training but preserves explanation consistency
  - Proximity initialization requires additional clustering computation but improves prototype placement

- Failure signatures:
  - Poor task-agnostic accuracy indicates task-recency bias not properly compensated
  - Low interpretability IOU suggests interpretability regularization is ineffective
  - Large performance gaps between task-aware and task-agnostic evaluation reveal task-specific prototype bias

- First 3 experiments:
  1. Test interpretability regularization alone by training with only the LIR loss and measuring explanation consistency across tasks
  2. Evaluate proximity initialization by comparing random vs. proximity-initialized prototypes on a single task learning scenario
  3. Validate task-recency compensation by measuring class activation balance before and after compensation on a multi-task model

## Open Questions the Paper Calls Out

- Question: How would incorporating exemplar-based replay (a replay buffer) affect the performance of ICICLE compared to the exemplar-free approach?
  - Basis in paper: [explicit] The paper states that ICICLE is evaluated in an exemplar-free scenario and mentions that analyzing how having a replay buffer would influence the method's performance is a limitation.
  - Why unresolved: The paper explicitly states this as a limitation and does not conduct experiments with exemplar-based replay.
  - What evidence would resolve it: Experiments comparing ICICLE's performance with and without a replay buffer on the same datasets would provide direct evidence.

- Question: Can ICICLE be effectively extended to other interpretable architectures beyond prototypical parts, such as B-COS networks?
  - Basis in paper: [explicit] The paper mentions that ICICLE was demonstrated with ProtoPNet and TesNet, and future work plans to investigate other interpretable architectures like B-COS.
  - Why unresolved: While ICICLE was tested with two specific architectures, its generalization to other concept-based models remains unexplored.
  - What evidence would resolve it: Implementing ICICLE with other interpretable architectures like B-COS and evaluating performance on standard continual learning benchmarks would provide evidence.

- Question: What is the optimal number of prototypes per class for ICICLE across different datasets and task numbers?
  - Basis in paper: [explicit] The paper uses 10 prototypes per class but does not explore the sensitivity of this hyperparameter or provide justification for this choice.
  - Why unresolved: The paper does not investigate how varying the number of prototypes affects performance or whether different settings might be optimal for different scenarios.
  - What evidence would resolve it: Conducting systematic experiments varying the number of prototypes per class on multiple datasets with different task configurations would identify optimal settings.

## Limitations
- Limited to exemplar-free continual learning scenarios without exploration of how replay buffers would affect performance
- Tested only on fine-grained classification datasets (birds and cars) without validation on broader vision tasks
- Does not investigate scalability to larger datasets with thousands of classes

## Confidence

**Confidence Assessment:**
- High confidence in interpretability regularization mechanism and its impact on preserving explanations
- Medium confidence in proximity-based initialization's general applicability across datasets
- Medium confidence in task-recency compensation effectiveness due to limited ablation studies
- Low confidence in scalability to large-scale datasets (>1000 classes) without modification

**Next Validation Checks:**
1. Test ICICLE on a larger dataset (e.g., ImageNet-1K) to evaluate scalability and performance in more realistic settings
2. Conduct an ablation study isolating each novelty (interpretability regularization, proximity initialization, task-recency compensation) to quantify individual contributions
3. Evaluate performance under severe class imbalance to assess robustness beyond balanced incremental scenarios