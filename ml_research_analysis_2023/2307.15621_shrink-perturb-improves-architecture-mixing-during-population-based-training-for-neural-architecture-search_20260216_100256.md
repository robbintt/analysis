---
ver: rpa2
title: Shrink-Perturb Improves Architecture Mixing during Population Based Training
  for Neural Architecture Search
arxiv_id: '2307.15621'
source_url: https://arxiv.org/abs/2307.15621
tags:
- search
- architecture
- training
- weights
- pbt-nas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PBT-NAS, a neural architecture search (NAS)
  method that simultaneously trains and mixes a population of neural networks. The
  core idea is to improve architectures by replacing poorly-performing networks with
  mixes of well-performing ones, inheriting weights using a shrink-perturb technique.
---

# Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search

## Quick Facts
- arXiv ID: 2307.15621
- Source URL: https://arxiv.org/abs/2307.15621
- Reference count: 40
- Key outcome: PBT-NAS achieves superior performance in neural architecture search by mixing well-performing networks and inheriting weights using shrink-perturb, outperforming random search and mutation-based baselines on GAN and RL tasks.

## Executive Summary
This paper introduces PBT-NAS, a neural architecture search method that adapts Population Based Training to NAS by mixing layers from different architectures during training rather than relying on random perturbations. The core innovation is using shrink-perturb to inherit weights when mixing networks, which allows the algorithm to efficiently explore large search spaces by reusing partially trained weights. Experiments on challenging tasks - GAN training (CIFAR-10, STL-10) and reinforcement learning for visual control (Quadruped Run, Walker Run, Humanoid Run) - demonstrate PBT-NAS's effectiveness with statistically significant improvements over baselines.

## Method Summary
PBT-NAS maintains a population of N neural networks trained in parallel. Each network is trained for a fixed number of epochs before evaluation, and the bottom τ% performers are replaced by mixing two top performers. The mixing operation randomly replaces layers from one network with layers from another, with probability p, then applies shrink-perturb to inherited weights (multiplying by λ and adding noise scaled by γ). This approach allows PBT-NAS to efficiently search large architecture spaces by reusing partially trained weights rather than training each architecture from scratch.

## Key Results
- PBT-NAS outperforms random search and mutation-based PBT on GAN tasks (CIFAR-10, STL-10) with statistically significant FID improvements
- For reinforcement learning visual control tasks (Quadruped Run, Walker Run, Humanoid Run), PBT-NAS achieves higher scores than comparison methods
- Mixing networks during training is more effective than simply cloning good networks
- Shrink-perturb weight inheritance is superior to copying or random reinitialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixing networks during training creates new architectures faster than random search by inheriting and adapting useful sub-components.
- Mechanism: When poorly-performing networks are replaced with mixes of top performers, useful architectural patterns (e.g., good layer configurations) are preserved and combined, accelerating the search.
- Core assumption: Layers from well-performing networks can be effectively mixed if they occupy the same position in the architecture and are compatible.
- Evidence anchors:
  - [abstract] "replacing poorly-performing networks in a population with the result of mixing well-performing ones"
  - [section 3.3] "we propose to adapt PBT to NAS by modifying the search to rely not on random perturbations but on mixing layers of the networks in the population"
  - [corpus] Weak: No direct neighbor papers discussing layer mixing in PBT-NAS; strongest related is evolutionary NAS, but that focuses on crossover of architecture encodings, not trained weights.
- Break condition: If layers are not interchangeable (e.g., due to incompatible tensor shapes or semantic mismatch), mixing will fail or produce invalid architectures.

### Mechanism 2
- Claim: Shrink-perturb enables better weight inheritance by softening the transfer from parent to child architectures.
- Mechanism: When copying weights from a donor network, shrink-perturb reduces their magnitude and adds noise, making them more adaptable to the new architecture rather than being stuck in the old representation space.
- Core assumption: The copied weights retain useful information even after magnitude reduction and noise addition, and gradient descent can adapt them effectively.
- Evidence anchors:
  - [abstract] "inheriting the weights using the shrink-perturb technique"
  - [section 3.3] "Shrink-perturb [3] is potentially helpful... shrinking (multiplying by a constant λ) and perturbing them (adding noise multiplied by a constant γ)"
  - [corpus] Weak: No neighbor papers explicitly mention shrink-perturb in NAS context; related work on model soups averages weights but does not shrink or perturb.
- Break condition: If λ is too small or γ too large, useful information is lost; if too conservative, adaptation may be too slow.

### Mechanism 3
- Claim: PBT-NAS is more efficient than training each architecture from scratch because it reuses partially trained weights.
- Mechanism: During the search, each network is trained for a fixed number of epochs before mixing; when replaced, the weights from the donor network are reused rather than reinitialized, saving computation.
- Core assumption: Partial training provides enough signal to guide mixing decisions and weight inheritance, and the reused weights still contribute to performance.
- Evidence anchors:
  - [abstract] "reusing the partially trained weights allows for efficient search"
  - [section 3.2] "N networks are trained in parallel... In each iteration... every network is trained for e_step epochs"
  - [corpus] Moderate: PBT literature supports weight reuse for hyperparameter optimization, but adaptation to NAS (where architecture changes) is novel and not directly covered by neighbors.
- Break condition: If e_step is too short, mixing decisions are based on insufficient training; if too long, search efficiency decreases.

## Foundational Learning

- Concept: Population Based Training (PBT) workflow
  - Why needed here: PBT-NAS builds on PBT's idea of parallel training with periodic replacement and weight inheritance; understanding PBT is essential to grasp how mixing fits into the loop.
  - Quick check question: In PBT, how are poorly-performing networks replaced, and what happens to their weights?

- Concept: Neural Architecture Search (NAS) search spaces
  - Why needed here: The effectiveness of PBT-NAS depends on designing search spaces where layers can be meaningfully mixed; understanding variable encoding and layer compatibility is crucial.
  - Quick check question: What makes two layers "mixable" in the context of PBT-NAS?

- Concept: Weight inheritance strategies (copy, shrink-perturb, reinitialize)
  - Why needed here: The choice of how to inherit weights after mixing directly impacts search performance; knowing the trade-offs helps tune or debug the algorithm.
  - Quick check question: Why might shrink-perturb be better than direct copying or random reinitialization when mixing networks?

## Architecture Onboarding

- Component map:
  - Population manager -> Training loop -> Selection logic -> Mixing operator -> Evaluation -> Configuration

- Critical path:
  1. Initialize population with random architectures from search space.
  2. For each iteration:
     a. Train all networks for e_step epochs in parallel.
     b. Evaluate fitness of each network.
     c. Sort population by fitness.
     d. Replace each bottom τ% network with a mix of two top τ% networks (using mixing operator).
  3. After e_total epochs, return the best-performing architecture.

- Design tradeoffs:
  - Population size N vs. compute budget: larger N explores more architectures but increases wall-clock time.
  - e_step length vs. mixing quality: too short and mixing is premature; too long and efficiency drops.
  - p (layer replacement probability) vs. architecture diversity: higher p yields more radical changes, lower p preserves more of donor.
  - Shrink-perturb parameters (λ, γ) vs. weight adaptability: aggressive shrink/perturb may lose useful information; conservative may slow adaptation.

- Failure signatures:
  - Population converges too quickly to one architecture (low diversity): likely p too low or selection pressure too high.
  - No improvement over random search: mixing is ineffective—check if layers are truly compatible or if shrink-perturb parameters are off.
  - Training instability or divergence: verify that e_step is sufficient for networks to learn before mixing, and that weight inheritance does not introduce too much noise.

- First 3 experiments:
  1. Run PBT-NAS on a simple search space (e.g., RL actor/critic with 3 searchable layers) with default parameters; verify that mixing produces new architectures and that performance improves over random search.
  2. Vary p (e.g., 0.1, 0.25, 0.5) and observe impact on architecture diversity and performance; identify if too low p leads to stagnation.
  3. Test shrink-perturb parameters: compare copying weights exactly, random reinitialization, and shrink-perturb with default [0.4, 0.1]; confirm shrink-perturb yields best results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PBT-NAS scale with increasingly larger and more complex search spaces?
- Basis in paper: [explicit] The paper notes that "we think that pivoting to more challenging search spaces and tasks could lead to NAS having a larger impact" and that current NAS research focuses on relatively restrictive cell-based search spaces where random search achieves competitive results.
- Why unresolved: While the paper demonstrates PBT-NAS works on two challenging search spaces (GanHard and the RL space), it doesn't systematically explore how performance scales with search space complexity or size.
- What evidence would resolve it: Experiments showing PBT-NAS performance on progressively larger and more complex search spaces, ideally including a benchmark with known scaling properties, would clarify this.

### Open Question 2
- Question: What is the optimal balance between the probability of replacing layers (p) and the shrink-perturb parameters (λ, γ) for different tasks and search spaces?
- Basis in paper: [explicit] The paper states "We aim to avoid unnecessary hyperparameter tuning to see if our approach is robust enough to perform well without it" and used default values (p=25%, λ=0.4, γ=0.1) across all experiments.
- Why unresolved: The paper demonstrates these default parameters work well across multiple settings, but doesn't explore whether task-specific tuning could yield better results or if there are general principles for setting these parameters.
- What evidence would resolve it: Systematic experiments varying p and shrink-perturb parameters across different tasks and search spaces, potentially revealing patterns or optimal settings for different scenarios.

### Open Question 3
- Question: How does PBT-NAS compare to specialized NAS algorithms when they can be applied to the same search space?
- Basis in paper: [explicit] The paper notes that "AdversarialNAS cannot be used to search in GanHard because some of the options cannot be searched for via continuous relaxation" and that their approach is "a good fit for multi-objective NAS."
- Why unresolved: While PBT-NAS outperforms baselines on challenging search spaces where specialized algorithms cannot be applied, the paper doesn't compare performance on search spaces where both general and specialized approaches could be used.
- What evidence would resolve it: Head-to-head comparisons of PBT-NAS versus specialized NAS algorithms (like DARTS for cell-based architectures) on search spaces where both can be applied would clarify relative strengths and weaknesses.

## Limitations
- Weak experimental validation with only two GAN datasets and three RL tasks
- Lack of ablation studies isolating impact of mixing vs. shrink-perturb
- Insufficient detail on hyperparameter tuning and robustness to initialization
- Limited comparison to specialized NAS algorithms

## Confidence
- **High confidence**: The core mechanism of mixing layers during training and inheriting weights with shrink-perturb is clearly described and theoretically sound.
- **Medium confidence**: The experimental results show PBT-NAS outperforms baselines, but the sample size is limited and some results (e.g., STL-10 GAN) are not reproducible.
- **Low confidence**: The claim that shrink-perturb is universally better than copying or reinitialization is not robustly tested across different tasks or architectures.

## Next Checks
1. **Ablation study**: Run PBT-NAS with only shrink-perturb (no mixing) and only mixing (no shrink-perturb) to isolate the contribution of each mechanism.
2. **Robustness test**: Evaluate PBT-NAS on additional datasets or tasks (e.g., CIFAR-100 GAN, different RL environments) to check generalizability.
3. **Hyperparameter sensitivity**: Systematically vary population size, step size, and shrink-perturb parameters to identify optimal settings and assess stability.