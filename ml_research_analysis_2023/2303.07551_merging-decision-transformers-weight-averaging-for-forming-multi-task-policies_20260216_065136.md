---
ver: rpa2
title: 'Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies'
arxiv_id: '2303.07551'
source_url: https://arxiv.org/abs/2303.07551
tags:
- merging
- attention
- parameters
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates merging Decision Transformers (DTs) trained
  on different MuJoCo tasks to create multi-task policies without centralized training.
  The authors analyze parameter similarity across DTs, showing that attention layers
  can be directly swapped with minimal performance loss.
---

# Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies

## Quick Facts
- arXiv ID: 2303.07551
- Source URL: https://arxiv.org/abs/2303.07551
- Reference count: 15
- Primary result: Attention layers in Decision Transformers trained on different MuJoCo tasks can be directly swapped with minimal performance loss, enabling multi-task policy creation without centralized training

## Executive Summary
This paper explores merging Decision Transformers trained on different MuJoCo locomotion tasks to create multi-task policies without centralized training objectives or data. The authors find that attention layers are particularly swappable between models, maintaining high performance when parameters are averaged. They propose Merge-Freeze-Finetune (MFF) to recover original performance after merging and demonstrate that language pre-training can improve merging outcomes. The work achieves 96.27% of original performance across three environments while using only 1.0052x the parameters of a single model.

## Method Summary
The paper investigates parameter merging strategies for Decision Transformers by training individual models on different MuJoCo tasks and then averaging their parameters. The core approach involves layer-wise merging analysis, where attention, MLP, and entire transformer layers are merged at different ratios (p=0.5 for averaging, p=1.0 for direct swapping). They introduce Merge-Freeze-Finetune (MFF) where merged layers are frozen and unmerged parts are finetuned to recover performance. The method is tested with and without language pre-training and co-training, using D4RL MuJoCo locomotion datasets.

## Key Results
- Attention layers can be directly swapped between models trained on different MuJoCo tasks with minimal performance loss
- Merge-Freeze-Finetune recovers original performance after merging with high success rates
- Language pre-training and co-training achieve 96.27% of original performance across three environments
- Merging policies achieve multi-task performance with only 1.0052x the parameters of a single model

## Why This Works (Mechanism)

### Mechanism 1
Attention layers in Decision Transformers trained on different MuJoCo tasks learn functionally similar parameters that can be directly swapped with minimal performance loss. Attention weights capture task-agnostic representations of sequential decision-making patterns, making them robust to domain shifts between locomotion tasks. The underlying dynamics of MuJoCo locomotion tasks share sufficient structural similarity that attention mechanisms can generalize across them. When attention becomes task-specific (e.g., in environments with fundamentally different dynamics or reward structures), swapping would cause significant performance degradation.

### Mechanism 2
Language model pre-training and co-training create a common initialization that improves merging performance. Shared pre-training establishes a common parameter space geometry, reducing the distance between task-specific fine-tuned models and making their average more likely to be in a high-performance region. Language pre-training provides useful inductive biases for decision-making tasks that transfer to RL domains. If the pre-training objective diverges too far from the downstream task distribution, the common initialization may not provide meaningful benefits.

### Mechanism 3
Merge-Freeze-Finetune (MFF) recovers original performance by allowing task-specific adaptation to merged representations. Freezing merged layers prevents catastrophic forgetting of shared representations while finetuning unmerged layers adapts to the new parameter configuration. The merged parameters contain useful shared information that shouldn't be overwritten during finetuning. If the merged parameters are too dissimilar from either original model, finetuning may not be able to recover performance.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how attention layers work is crucial for interpreting why they can be swapped between models
  - Quick check question: What is the mathematical operation performed in a self-attention layer, and how does the causal mask affect it?

- Concept: Offline reinforcement learning and return-to-go (RTG) conditioning
  - Why needed here: Decision Transformers frame RL as sequence modeling, which is fundamental to understanding the experimental setup
  - Quick check question: How does the RTG conditioning work during training vs inference in Decision Transformers?

- Concept: Parameter merging and interpolation in neural networks
  - Why needed here: The core contribution involves merging model parameters, requiring understanding of how parameter space interpolation works
  - Quick check question: What happens when you linearly interpolate between parameters of two randomly initialized models?

## Architecture Onboarding

- Component map:
  - Input projection layers (unique per environment due to different state/action spaces)
  - Transformer blocks (3 layers, 1 head, 128 embedding size)
  - Output projection layers (unique per environment)
  - Attention parameters (query/key/value projections and output projection)
  - MLP parameters (feed-forward networks within each transformer block)
  - Layer normalization parameters (per-layer affine transformations)

- Critical path:
  1. Initialize models (either randomly or with language pre-training)
  2. Train individual DTs on different MuJoCo tasks
  3. Analyze parameter similarity through layer-wise merging
  4. Apply Merge-Freeze-Finetune strategy for multi-task policy creation
  5. Evaluate performance across original and merged tasks

- Design tradeoffs:
  - Merging attention vs MLP parameters: Attention merging preserves more performance but offers less parameter reduction
  - Shared vs unique layer normalization: Keeping unique layer norms maintains task-specific normalization while sharing heavier components
  - Language pre-training benefits vs training cost: Pre-training improves merging but adds upfront computational overhead

- Failure signatures:
  - Large performance drop when swapping MLP parameters indicates task-specific feature extraction
  - Inconsistent merging results across environment pairs suggest domain gap too large for parameter averaging
  - Inability to recover performance with MFF indicates merged parameters are too dissimilar from original models

- First 3 experiments:
  1. Layer-wise merging analysis: Merge one layer at a time between two trained models to identify which components are most swappable
  2. Attention perturbation study: Replace attention parameters with random/identity parameters to measure their importance in each environment
  3. Cross-environment initialization test: Initialize a new DT with parameters from a model trained on a different environment to assess transfer potential

## Open Questions the Paper Calls Out

### Open Question 1
How do Decision Transformers trained on different tasks compare in terms of learned parameter similarities beyond MuJoCo locomotion tasks? The study is limited to a specific set of environments and tasks. It is unclear whether the observed parameter similarities and successful merging extend to other types of tasks or environments, such as different robotic control tasks or Atari games. Conducting similar merging experiments with DTs trained on a broader range of tasks and environments would provide evidence on the generalizability of the findings.

### Open Question 2
What is the role of attention mechanisms in Decision Transformers, and how does it vary across different tasks and datasets? The paper provides initial insights into the role of attention but does not fully explain why attention mechanisms vary in importance across different tasks and datasets. Understanding the underlying reasons for these variations is crucial for improving the design and training of DTs. Detailed analysis of attention mechanisms across a wider variety of tasks and datasets would help elucidate the role of attention in DTs.

### Open Question 3
How can merging Decision Transformers be extended to other reinforcement learning settings, such as continual learning or multi-task learning with more diverse objectives? The paper discusses the potential of merging DTs for creating multi-task policies without centralized training and mentions the possibility of exploring merging for continual reinforcement learning. The paper focuses on merging DTs trained on similar types of tasks and does not explore the application of merging to more diverse reinforcement learning settings or objectives. Experiments applying merging techniques to DTs in continual learning scenarios, multi-task learning with diverse objectives, and other reinforcement learning settings would demonstrate the versatility and effectiveness of merging in broader contexts.

## Limitations

- The analysis is limited to MuJoCo locomotion tasks, raising questions about generalizability to more diverse environments
- The specific implementation details for Fisher information weighting are not provided, making reproduction difficult
- The benefits of language pre-training versus other initialization methods are not directly compared

## Confidence

- High: Attention layer swapping works with minimal performance loss - supported by direct experimental evidence across multiple task pairs
- Medium: Merge-Freeze-Finetune recovers original performance - demonstrated but with limited ablation studies on freezing vs finetuning ratios
- Low: Language pre-training significantly improves merging - claimed but not directly compared to alternatives like random initialization or other pre-training methods

## Next Checks

1. Cross-architecture validation: Apply the attention swapping approach to other transformer-based RL methods (like PPO with transformers) to test generalizability beyond Decision Transformers
2. Dynamic parameter weighting: Implement and compare different parameter importance weighting schemes (Fisher information, magnitude-based, random) to isolate the most effective approach
3. Scaling experiment: Test the merging approach on more diverse environment pairs (e.g., locomotion + manipulation tasks) to identify the limits of parameter similarity across fundamentally different domains