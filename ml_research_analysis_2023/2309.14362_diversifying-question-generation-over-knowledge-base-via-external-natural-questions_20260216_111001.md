---
ver: rpa2
title: Diversifying Question Generation over Knowledge Base via External Natural Questions
arxiv_id: '2309.14362'
source_url: https://arxiv.org/abs/2309.14362
tags:
- questions
- question
- diverse
- diversity
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of diversifying question generation
  from knowledge bases, where previous methods focused on generating a single high-quality
  question rather than diverse questions. The authors propose a new diversity evaluation
  metric called Diverse@k that measures diversity among top-k generated questions
  while ensuring relevance to the ground truth.
---

# Diversifying Question Generation over Knowledge Base via External Natural Questions

## Quick Facts
- arXiv ID: 2309.14362
- Source URL: https://arxiv.org/abs/2309.14362
- Reference count: 0
- This paper introduces a diversity evaluation metric called Diverse@k and a dual model framework to generate diverse questions from knowledge bases using external natural questions.

## Executive Summary
This paper addresses the problem of diversifying question generation from knowledge bases, where previous methods focused on generating a single high-quality question rather than diverse questions. The authors propose a new diversity evaluation metric called Diverse@k that measures diversity among top-k generated questions while ensuring relevance to the ground truth. They introduce a dual model framework interwoven by two selection strategies to leverage external natural questions for enhancing diversity. The approach uses a forward model to generate questions from knowledge base triplets and a backward model to generate triplets from questions, with selection strategies filtering reliable pseudo pairs. Experiments on widely used benchmarks show the approach generates highly diverse questions while maintaining relevance, outperforming existing baselines in diversity metrics and improving downstream question answering performance.

## Method Summary
The method introduces a dual model framework with forward and backward models to transform knowledge base triplets into natural questions and vice versa. It leverages external natural questions to inject diverse semantic patterns through iterative fine-tuning. Two selection strategies filter reliable pseudo pairs based on semantic similarity (using simCSE) and diversity scores. The forward model generates questions from triplets while the backward model generates triplets from questions, with both models iteratively training on filtered pseudo pairs from each other and external sources to accumulate diverse expressions.

## Key Results
- Proposed Diverse@k metric achieves Pearson correlation of 0.935-0.949 with human evaluation for diversity measurement
- Outperforms baseline models on diversity metrics (Distinct-n, Diverse@k) while maintaining relevance through simCSE scores above 0.7 threshold
- Improves downstream question answering performance with better Hits@1 and F1 scores compared to existing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual model framework (forward and backward models) enables diversity by leveraging external natural questions to inject varied expressions.
- Mechanism: The forward model generates questions from knowledge base triplets, while the backward model generates triplets from questions. They iteratively fine-tune each other using reliable pseudo pairs selected from external natural questions, accumulating diverse semantic patterns.
- Core assumption: External natural questions contain richer semantic and linguistic expressions than paraphrased training questions, and the dual models can effectively learn and integrate these expressions.
- Evidence anchors:
  - [abstract]: "We introduce a dual model framework interwoven by two selection strategies to generate diverse questions leveraging external natural questions."
  - [section 3.3]: "To extract rich expressions and squeeze them into the generation model, we design two dual models, namely the forward model and the backward model, to transform the formatted facts like triplets into the natural question and vice versa respectively."
  - [corpus]: Weak, only 1 of 8 neighbors mentions "diversifying" explicitly; most are about KBQA rather than question generation diversity.

### Mechanism 2
- Claim: Selection strategies filter reliable pseudo pairs to maintain relevance while enhancing diversity.
- Mechanism: First, simCSE scores filter backward model outputs to keep external questions semantically close to training data. Second, simCSE plus Diverse(Si,Sj) select top diverse yet relevant pseudo pairs for backward model training.
- Core assumption: Semantic similarity via simCSE reliably indicates both relevance to ground truth and compatibility for training.
- Evidence anchors:
  - [section 3.6]: "We first calculate the simCSE score between Qj and fθ(bφ(Qj))... Then we calculate the diverse score Diverse(fθ(Gi), qi) to select top-1 scored pseudo pair."
  - [section 4.4]: "We conduct an experiment to verify this fact... the result is 0.703... Actually, diversity and relevance conflict with one another."
  - [corpus]: Weak; no corpus neighbors discuss selection strategies for diversity-relevance trade-off.

### Mechanism 3
- Claim: Iterative fine-tuning of dual models progressively enriches the generation model with diverse expressions.
- Mechanism: After initialization, forward model fine-tuned on pseudo pairs from backward model and external questions; backward model fine-tuned on pseudo pairs from forward model and training data. This cycle repeats, each iteration expanding the semantic space.
- Core assumption: Each iteration meaningfully adds new expressions without catastrophic forgetting of original patterns.
- Evidence anchors:
  - [section 3.5]: "Through this iterative fine-tuning, fθ and bφ accumulate a wide range of diverse patterns and expressions from DQ, which endows the forward model with diversity."
  - [section 4.2]: "We fix the number of iterations to 2 and 1 for the datasets WQ and PQ, which are chosen based on the results of the validation set."
  - [corpus]: Weak; no neighbor papers discuss iterative dual-model training for diversity.

## Foundational Learning

- Concept: Knowledge Base Question Generation (KBQG)
  - Why needed here: The paper builds on and extends KBQG by focusing on diversity; understanding the base task is essential.
  - Quick check question: What is the input and output of a KBQG model?

- Concept: Paraphrasing and semantic similarity
  - Why needed here: Diversity is defined as different expressions of same semantics; simCSE measures semantic similarity.
  - Quick check question: How does simCSE differ from BLEU in evaluating semantic relevance?

- Concept: Dual learning / back-translation in NLP
  - Why needed here: The dual model framework is inspired by back-translation; understanding this helps grasp the iterative training idea.
  - Quick check question: In back-translation, what roles do the forward and backward models play?

## Architecture Onboarding

- Component map:
  - BART-based forward model (fθ) -> generates questions from triplets
  - BART-based backward model (bφ) -> generates triplets from questions
  - External natural questions (DQ) -> unpaired questions from sources like Natural Questions
  - Selection strategies -> semantic filtering and diversity filtering
  - Training pipeline -> initialization → iterative fine-tuning on pseudo pairs

- Critical path:
  1. Pretrain fθ and bφ on training data D.
  2. Generate pseudo pairs {(bφ(Qj), Qj)} from DQ using bφ.
  3. Apply first selection strategy to filter reliable pseudo pairs.
  4. Fine-tune fθ on filtered pseudo pairs.
  5. Generate pseudo pairs {(fθ(Gi), Gi)} from D using fθ.
  6. Apply second selection strategy to filter and select diverse pseudo pairs.
  7. Fine-tune bφ on selected pseudo pairs.
  8. Repeat steps 2-7 for fixed iterations.

- Design tradeoffs:
  - More external questions → more diversity but higher noise risk.
  - Higher simCSE threshold → more relevance, less diversity.
  - More iterations → richer diversity, risk of overfitting.

- Failure signatures:
  - Low simCSE scores on validation → relevance collapse.
  - Distinct-n close to 1 → lack of diversity.
  - QA performance drops → diversity at expense of quality.

- First 3 experiments:
  1. Run forward and backward models on small held-out D to check pretraining quality.
  2. Generate pseudo pairs from DQ, inspect simCSE distribution to tune threshold α.
  3. Compare top-3 generated questions from baseline vs ours on 10 instances, manually check diversity and relevance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Diverse@k metric compare to other diversity metrics like Self-BLEU in terms of correlation with human judgments of diversity?
- Basis in paper: [explicit] The paper states that Diverse@k shows a Pearson correlation coefficient of 0.935-0.949 with human evaluation, while Self-BLEU is mentioned as an alternative metric.
- Why unresolved: The paper does not directly compare Diverse@k to Self-BLEU in terms of correlation with human judgments.
- What evidence would resolve it: Empirical comparison of the correlation coefficients between both metrics and human judgments on the same dataset.

### Open Question 2
- Question: What is the impact of different threshold values (α) for semantic relevance on the performance of the proposed approach?
- Basis in paper: [explicit] The paper mentions using a threshold of 0.7 for the semantic relevance score but does not explore the impact of different threshold values.
- Why unresolved: The choice of threshold value could affect the trade-off between diversity and relevance, but this was not systematically investigated.
- What evidence would resolve it: Empirical results showing the performance of the approach with different threshold values on the validation set.

### Open Question 3
- Question: How does the proposed approach perform on knowledge bases with different structures or domains?
- Basis in paper: [inferred] The experiments were conducted on two specific benchmarks (WQ and PQ), but the paper does not discuss the generalizability of the approach to other knowledge bases.
- Why unresolved: The performance of the approach might depend on the characteristics of the knowledge base, such as its size, complexity, or domain.
- What evidence would resolve it: Empirical results showing the performance of the approach on a diverse set of knowledge bases with varying structures and domains.

## Limitations
- The Diverse@k metric's reliance on simCSE similarity thresholds may be dataset-specific and require tuning for other knowledge base domains
- Effectiveness depends heavily on semantic alignment between external questions and training data
- Dual model framework introduces complexity that may make it harder to diagnose when diversity gains come at expense of generation quality

## Confidence
- High confidence: The core mechanism of using dual models with selection strategies is well-supported by experimental results
- Medium confidence: The claim that external natural questions significantly enhance diversity is supported, but specific contribution versus selection strategies is unclear
- Medium confidence: The iterative fine-tuning approach appears effective, but optimal number of iterations (1-2) seems dataset-specific

## Next Checks
1. Conduct an ablation study removing external natural questions while keeping selection strategies to quantify their specific contribution to diversity gains
2. Test the Diverse@k metric with different similarity thresholds (α values) on validation sets to determine optimal threshold ranges for different domains
3. Generate questions for a small subset using the forward model before and after each iteration, then manually annotate whether new semantic patterns are actually being introduced or if the model is just varying surface forms