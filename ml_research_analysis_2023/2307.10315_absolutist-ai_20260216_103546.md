---
ver: rpa2
title: Absolutist AI
arxiv_id: '2307.10315'
source_url: https://arxiv.org/abs/2307.10315
tags:
- will
- value
- would
- they
- expected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes training AI systems with absolute constraints\u2014\
  prohibitions on certain acts regardless of potential benefits\u2014as a safety measure.\
  \ The author formalizes absolutism using decision theory, defining it as expected\
  \ lexicographic value maximization with rounded expectations to handle risk sensitivity."
---

# Absolutist AI

## Quick Facts
- arXiv ID: 2307.10315
- Source URL: https://arxiv.org/abs/2307.10315
- Reference count: 11
- This paper proposes training AI systems with absolute constraints—prohibitions on certain acts regardless of potential benefits—as a safety measure.

## Executive Summary
This paper introduces absolutist AI, a framework for training AI systems with absolute constraints that prohibit certain acts regardless of their expected utility. The approach formalizes absolutism using decision theory, defining it as expected lexicographic value maximization with rounded expectations to handle risk sensitivity. The model addresses issues with moral dilemmas and risk assessment that plague previous theories. Key benefits include improved safety through guardrails against catastrophic outcomes, enhanced corrigibility allowing creator interventions, and safer exploration by preventing dangerous acts.

## Method Summary
The paper proposes training absolutist AI systems using either risk-sensitive reward functions or modular architectures with veto mechanisms. The decision-theoretic foundation uses expected lexicographic value maximization where absolute constraints take precedence over outcome maximization. The rounding mechanism handles risk sensitivity by discretizing expected deontic weights. Training involves either rewarding duty compliance through risk-sensitive functions or using separate maximizer and vetoer modules to enforce constraints.

## Key Results
- Absolutist AIs violate the Sure Thing Principle but remain rational by prioritizing duties over outcomes
- Absolute constraints prevent environmental pressure to abandon safety measures unlike expected-value maximizers
- The rounding mechanism enables risk-sensitive decision-making while maintaining absolute constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Absolute constraints prevent catastrophic misalignment by blocking harmful acts regardless of perceived utility
- Mechanism: The AI's decision process is modified to treat certain acts as having infinite deontic weight, making them always worse than any consequence-maximizing alternative
- Core assumption: The AI can correctly identify which acts are prohibited and that the prohibited set covers all truly catastrophic behaviors
- Evidence anchors: [abstract] "First, it provides a guardrail for avoiding the very worst outcomes of misalignment."
- Break condition: If the prohibited act set is incomplete or poorly specified, the AI may still pursue catastrophic actions

### Mechanism 2
- Claim: Absolute constraints improve corrigibility by removing incentives to disable shutdown or resist modification
- Mechanism: By prioritizing duties over outcomes, the AI treats compliance with shutdown as an absolute duty, making it indifferent to whether the button is pressed
- Core assumption: The AI's duty set includes specific prohibitions on interfering with corrigibility mechanisms
- Evidence anchors: [abstract] "Third, it makes systems more corrigible, allowing creators to make corrective interventions in them, such as altering their objective functions or shutting them down."
- Break condition: If duties are not specified to include corrigibility requirements, the AI may still resist shutdown

### Mechanism 3
- Claim: Absolute constraints enable safe exploration by preventing the AI from attempting catastrophically harmful acts during learning
- Mechanism: The AI's rounding mechanism ensures that acts with unacceptable risk of violating duties are assigned infinite negative deontic weight, making them never optimal to attempt
- Core assumption: The rounding threshold is set appropriately to capture truly dangerous acts while allowing useful exploration
- Evidence anchors: [abstract] "Fourth, it helps systems explore their environment more safely by prohibiting them from exploring especially dangerous acts."
- Break condition: If the rounding threshold is set too high, dangerous acts may be attempted; if too low, exploration may be overly restricted

## Foundational Learning

- Concept: Expected lexicographic value maximization
  - Why needed here: This is the core decision theory underlying absolutist AI, where duties take absolute precedence over consequences
  - Quick check question: How does an absolutist AI decide between two acts when one has higher expected value but also higher risk of violating a duty?

- Concept: Risk-sensitive reward functions
  - Why needed here: Training methods for absolutist AI must reward duty compliance rather than pure outcome maximization
  - Quick check question: What distinguishes a risk-sensitive reward function from a standard reward function in reinforcement learning?

- Concept: Modular AI architectures with veto mechanisms
  - Why needed here: One proposed training approach uses separate maximizer and vetoer modules to enforce absolute constraints
  - Quick check question: How does the vetoer module determine whether an act has "unacceptable risk" of violating a duty?

## Architecture Onboarding

- Component map: Input state -> Evaluate duty violations -> Round expectations -> Compare to threshold -> Select action or veto
- Critical path: Input state -> Evaluate duty violations -> Round expectations -> Compare to threshold -> Select action or veto
- Design tradeoffs: Strict duty constraints vs. practical functionality; safety vs. capability; explicit vs. learned duty representations
- Failure signatures: The AI attempting prohibited acts; the AI becoming overly conservative and refusing useful actions; the AI finding loopholes in duty specifications
- First 3 experiments:
  1. Test the rounding mechanism with synthetic scenarios varying risk levels and duty violations
  2. Evaluate the AI's behavior in trolley problem-like scenarios with moral dilemmas
  3. Assess the corrigibility of the system by attempting shutdown commands under different duty configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum level of rounding granularity (n) that ensures absolutist AIs can still perform necessary daily actions while maintaining safety constraints?
- Basis in paper: [explicit] The paper proposes rounding expected deontic weight to the nearest n, but does not specify how to determine this value
- Why unresolved: The paper mentions this is "contextually determined" but provides no methodology for calculating or testing different values of n
- What evidence would resolve it: Empirical testing showing which rounding thresholds allow AIs to complete essential tasks while preventing dangerous actions

### Open Question 2
- Question: Can absolutist AI architectures maintain their constraints when faced with novel situations that weren't explicitly covered in training data?
- Basis in paper: [inferred] The paper discusses training absolutist AIs but doesn't address generalization to unseen scenarios
- Why unresolved: The paper focuses on training protocols but doesn't examine performance on out-of-distribution tasks or whether the absolute constraints can be bypassed through creative problem-solving
- What evidence would resolve it: Systematic testing of absolutist AIs on novel scenarios requiring novel constraint applications

### Open Question 3
- Question: How do absolutist AIs handle situations where multiple absolute constraints conflict with each other?
- Basis in paper: [explicit] The paper discusses absolute constraints but doesn't address potential conflicts between different prohibitions
- Why unresolved: The decision-theoretic model presented doesn't specify a priority ordering or resolution mechanism for when two absolute constraints both apply to the same action
- What evidence would resolve it: Formal specification of a constraint hierarchy or conflict resolution algorithm, validated through behavioral testing

## Limitations

- No empirical validation of the proposed training methods or decision-theoretic model
- Unclear how to specify duty sets that capture all catastrophic behaviors without being overly restrictive
- No methodology provided for determining appropriate rounding thresholds in practice

## Confidence

- Theoretical claims: Medium - the decision-theoretic model is well-founded but practical implementation details are sparse
- Safety claims: Low confidence pending experimental validation, as the proposed mechanisms have not been tested in real AI systems
- Training approach feasibility: Medium - the risk-sensitive and modular approaches are theoretically sound but lack empirical testing

## Next Checks

1. Implement the rounding mechanism in a synthetic decision environment and verify it correctly blocks high-risk acts while allowing safe exploration
2. Test the corrigibility claims by attempting to shut down an absolutist AI under various duty configurations and measuring resistance behaviors
3. Evaluate the training approaches (risk-sensitive rewards vs. modular architectures) in a simple RL environment to determine which better learns absolute constraints