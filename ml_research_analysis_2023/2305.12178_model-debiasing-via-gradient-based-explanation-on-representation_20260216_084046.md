---
ver: rpa2
title: Model Debiasing via Gradient-based Explanation on Representation
arxiv_id: '2305.12178'
source_url: https://arxiv.org/abs/2305.12178
tags:
- sensitive
- task
- downstream
- latent
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new fairness framework that leverages gradient-based
  explanation to address the challenges of information loss and incomplete coverage
  of sensitive information in debiasing methods based on disentangled representation
  learning. The proposed method, DVGE, uses gradient-based explanation to identify
  model focuses for predicting sensitive attributes and downstream task labels, and
  then perturbs the latent code to guide the training of downstream task models towards
  fairness and utility goals.
---

# Model Debiasing via Gradient-based Explanation on Representation

## Quick Facts
- arXiv ID: 2305.12178
- Source URL: https://arxiv.org/abs/2305.12178
- Reference count: 40
- Key outcome: DVGE achieves better fairness-accuracy trade-off than state-of-the-art methods by using gradient-based explanations to identify sensitive information without requiring complete disentanglement.

## Executive Summary
This paper introduces DVGE, a novel fairness framework that addresses limitations in disentangled representation learning for debiasing. Unlike previous methods that require complete separation of sensitive attributes from other information, DVGE uses gradient-based explanations to identify model focuses for both sensitive attributes and downstream tasks, then applies bidirectional perturbation to guide training toward fairness and utility goals. The framework maintains flexibility by separating encoding and debiasing processes, allowing reuse of the same latent code for different sensitive attributes.

## Method Summary
DVGE trains a VAE to encode input data into latent codes, then uses gradient-based explanations from both a sensitive classifier and downstream task model to identify which dimensions of the latent code contain sensitive information versus task-relevant information. During training, it applies bidirectional perturbation by adding the downstream task focus while subtracting the sensitive focus, creating a balanced adjustment that promotes fairness while preserving accuracy. The framework is flexible because the VAE can be fixed and reused when sensitive attributes change.

## Key Results
- DVGE outperforms baseline methods on both CelebA and South German Credit datasets in terms of fairness-accuracy trade-off
- The framework achieves better coverage of sensitive information compared to disentangled representation approaches
- DVGE maintains flexibility by not requiring VAE retraining when sensitive attributes or downstream tasks change

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based explanations can identify model focuses for predicting sensitive attributes without requiring complete disentanglement.
- Mechanism: The sensitive classifier is trained to use every dimension of the latent code to predict sensitive attributes. The gradient of this classifier with respect to the latent code highlights which dimensions contain sensitive information, even when factors of variation are mixed.
- Core assumption: The gradient-based explanation effectively captures all sensitive information distributed across the latent code dimensions, regardless of disentanglement quality.

### Mechanism 2
- Claim: Bidirectional perturbation can guide downstream task models toward fairness while preserving accuracy by using opposing gradient directions.
- Mechanism: The perturbation adds the downstream task focus (positive perturbation) to emphasize task-relevant information while subtracting the sensitive focus (negative perturbation) to de-emphasize sensitive information, creating a balanced adjustment to the latent code.
- Core assumption: The gradient directions from the downstream task model and sensitive classifier are orthogonal enough that their combination can achieve both fairness and utility goals simultaneously.

### Mechanism 3
- Claim: The framework maintains flexibility by separating encoding and debiasing processes, allowing reuse of the same latent code for different sensitive attributes.
- Mechanism: Since the VAE is trained once and fixed, different sensitive classifiers can be trained and used to generate different sensitive focuses without retraining the encoder when sensitive attributes change.
- Core assumption: The latent code captures sufficient information about various sensitive attributes that different classifiers can extract their respective focuses.

## Foundational Learning

- Concept: Gradient-based explanation and saliency maps
  - Why needed here: The framework relies on computing gradients of predictions with respect to input features to identify which parts of the latent code are important for specific predictions
  - Quick check question: What is the mathematical relationship between the gradient of a classifier's output and the importance of input features?

- Concept: Variational Autoencoders (VAEs) and latent space representation
  - Why needed here: The framework uses VAEs to encode input data into a latent space that serves as the basis for both sensitive attribute detection and downstream task prediction
  - Quick check question: How does the evidence lower bound (ELBO) in VAEs balance reconstruction quality with latent space regularization?

- Concept: Fairness metrics (Demographic Parity and Equal Opportunity)
  - Why needed here: The framework's effectiveness is evaluated using these standard fairness metrics to measure the distance from ideal fairness conditions
  - Quick check question: What is the key difference between demographic parity and equal opportunity in terms of their fairness requirements?

## Architecture Onboarding

- Component map: Input Data -> VAE Encoder -> Latent Code -> Sensitive Classifier + Downstream Task Model -> Gradient-based Explainer -> Bidirectional Perturbation -> Trained Downstream Model

- Critical path:
  1. Train VAE on input data to produce fixed latent representations
  2. Train sensitive classifier on latent code to predict sensitive attributes
  3. During downstream model training: compute both focuses, apply bidirectional perturbation, train model on perturbed latent code
  4. During inference: encode input, pass latent code directly to trained downstream model

- Design tradeoffs:
  - Using fixed VAE provides flexibility but may limit adaptation to specific fairness requirements
  - Gradient-based perturbation is interpretable but may introduce noise if gradients are noisy
  - Separating encoding and debiasing simplifies training but requires the VAE to capture sufficient information upfront

- Failure signatures:
  - If downstream accuracy drops significantly, the bidirectional perturbation may be too aggressive
  - If fairness metrics don't improve, the sensitive classifier may not be capturing relevant information
  - If both accuracy and fairness suffer, the VAE may not be producing useful latent representations

- First 3 experiments:
  1. Verify gradient-based explanations correctly identify sensitive information by testing if removing highlighted dimensions reduces sensitive classifier accuracy
  2. Test bidirectional perturbation in isolation by applying it to pre-trained downstream models and measuring fairness-accuracy trade-off
  3. Validate the full pipeline on a simple dataset with known sensitive attributes to ensure all components work together correctly

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, several areas warrant further investigation regarding performance on highly entangled datasets, VAE architecture choices, and gradient-based explanation methods.

## Limitations
- Framework effectiveness depends heavily on VAE's ability to capture relevant information in latent space
- Bidirectional perturbation assumes sufficient orthogonality between gradient directions, which may not hold in all cases
- Computational overhead of gradient-based explanations and bidirectional perturbation is not addressed

## Confidence
- **High Confidence**: Framework's flexibility advantage is well-supported by design and experimental results
- **Medium Confidence**: Gradient-based explanation mechanism works well in practice, though sensitive classifier quality significantly impacts results
- **Low Confidence**: Bidirectional perturbation's ability to simultaneously optimize for fairness and utility in all scenarios

## Next Checks
1. Conduct ablation studies varying perturbation intensity and measuring impact on fairness-accuracy trade-off across different datasets
2. Test performance when downstream task model and sensitive classifier share highly correlated gradients to assess bidirectional perturbation's robustness
3. Evaluate computational overhead of gradient-based explanation and bidirectional perturbation compared to baseline methods