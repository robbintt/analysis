---
ver: rpa2
title: Domain constraints improve risk prediction when outcome data is missing
arxiv_id: '2312.03878'
source_url: https://arxiv.org/abs/2312.03878
tags:
- cancer
- risk
- constraints
- prevalence
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of predicting disease risk in settings
  where outcome data is selectively observed due to human decision-making. It proposes
  a Bayesian model class that captures this setting, along with two domain constraints
  (prevalence and expertise) to improve parameter inference.
---

# Domain constraints improve risk prediction when outcome data is missing

## Quick Facts
- arXiv ID: 2312.03878
- Source URL: https://arxiv.org/abs/2312.03878
- Reference count: 40
- The model outperforms baselines in terms of AUC and quintile ratio when predicting disease risk with selectively observed outcomes

## Executive Summary
This paper addresses the challenge of predicting disease risk when outcome data is selectively observed due to human decision-making, such as medical testing decisions. The authors propose a Bayesian model class that incorporates domain constraints - specifically prevalence and expertise constraints - to improve parameter inference in this selective labels setting. Applied to breast cancer risk prediction using UK Biobank data, the model successfully predicts cancer diagnoses, aligns with public health testing policies, and identifies suboptimalities in test allocation.

## Method Summary
The paper proposes a Bayesian model that addresses selective labels problems where outcomes are only observed for a subset of the population chosen by human decision-makers. The model incorporates two key domain constraints: a prevalence constraint that fixes the overall disease prevalence, and an expertise constraint that limits which features can affect testing decisions beyond disease risk. Using MCMC sampling via Stan, the model estimates posterior distributions over parameters while respecting these constraints. The approach is validated on UK Biobank data for breast cancer risk prediction.

## Key Results
- The model's inferred risks successfully predict cancer diagnoses
- The inferred testing policy correlates with actual public health policies
- The prevalence constraint prevents misleading inferences like implausible age trends in cancer risk
- The model outperforms baselines in terms of AUC and quintile ratio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The prevalence constraint improves parameter inference by reducing uncertainty in the disease prevalence for untested patients
- Mechanism: By fixing the overall disease prevalence, the model is prevented from inferring implausible distributions of disease among untested patients, which could otherwise lead to incorrect estimates of disease risk and testing behavior
- Core assumption: The true disease prevalence in the population is known or can be accurately estimated
- Evidence anchors:
  - [abstract]: "The prevalence constraint prevents misleading inferences, such as an implausible age trend in cancer risk"
  - [section 2]: "Prevalence constraint: The average value of Y across the entire population is known (E[Y ])"
  - [corpus]: Weak - corpus papers focus on outcome prediction rather than selective labels or prevalence constraints
- Break condition: If the assumed prevalence is significantly incorrect, the constraint could bias parameter estimates

### Mechanism 2
- Claim: The expertise constraint improves parameter inference by limiting how much human decision-making can deviate from purely risk-based allocation
- Mechanism: By restricting which features can affect testing decisions beyond disease risk, the model reduces the complexity of the decision-making function, leading to more stable parameter estimates
- Core assumption: There exist features that do not affect testing probability when controlling for disease risk
- Evidence anchors:
  - [section 2]: "Expertise constraint: Because doctors and patients are informed decision-makers, we can assume that tests are allocated mostly based on disease risk"
  - [section 5.1]: "We do not place the expertise constraint on (i) racial/socioeconomic features, due to disparities in healthcare access"
  - [corpus]: Weak - corpus papers don't discuss expertise constraints or deviation from risk-based allocation
- Break condition: If the constraint incorrectly excludes features that actually affect testing decisions beyond risk

### Mechanism 3
- Claim: The Bayesian framework improves parameter inference by incorporating uncertainty quantification and regularization
- Mechanism: The posterior distribution over parameters naturally penalizes extreme values and provides confidence intervals, leading to more stable estimates than point estimates
- Core assumption: The priors used in the Bayesian framework are reasonable approximations of the true parameter distributions
- Evidence anchors:
  - [section 2]: "We propose a Bayesian model class which captures this setting"
  - [section 3.1]: "In our Bayesian formulation, we estimate a posterior distribution for parameter θ given the observed data"
  - [section 4]: "We use the Bayesian inference package Stan... We report results across 200 trials"
- Break condition: If the priors are severely misspecified, they could dominate the posterior and lead to biased estimates

## Foundational Learning

- Concept: Selective labels problem
  - Why needed here: The model addresses a specific type of distribution shift where outcomes are only observed for a subset of the population selected by human decision-makers
  - Quick check question: In what ways does the selective labels setting differ from standard supervised learning with missing data?

- Concept: Bayesian inference
  - Why needed here: The model uses Bayesian methods to estimate posterior distributions over parameters, incorporating both data and prior knowledge
  - Quick check question: How does the law of total variance apply to showing that constraining parameters reduces posterior variance?

- Concept: Domain constraints
  - Why needed here: The prevalence and expertise constraints encode domain knowledge to improve parameter estimation in the presence of distribution shift
  - Quick check question: What are the key assumptions underlying the prevalence and expertise constraints, and how could they be violated?

## Architecture Onboarding

- Component map: Data preprocessing -> Model specification -> MCMC sampling -> Posterior analysis -> Validation
- Critical path:
  1. Load and preprocess UK Biobank data
  2. Define Bernoulli-sigmoid model with uniform unobservables
  3. Specify prevalence and expertise constraints
  4. Run MCMC sampling to estimate posterior distribution
  5. Analyze posterior to extract parameter estimates and predictions
  6. Validate model using held-out test data and qualitative checks

- Design tradeoffs:
  - Model complexity vs. identifiability: Adding more parameters or complex interactions can improve fit but may make the model harder to estimate
  - Constraint specificity vs. flexibility: More specific constraints can improve estimation but may exclude valid patterns
  - Computational cost vs. accuracy: MCMC sampling provides accurate posterior estimates but can be slow for large datasets

- Failure signatures:
  - Poor MCMC convergence: Indicates model misspecification or overly complex model for available data
  - Implausible parameter estimates: Suggests constraints are too restrictive or priors are misspecified
  - Poor predictive performance: Could indicate distribution shift not captured by model or insufficient features

- First 3 experiments:
  1. Fit model with only prevalence constraint on synthetic data to verify improvement in parameter estimation
  2. Fit model with only expertise constraint on synthetic data to verify improvement in parameter estimation
  3. Fit full model with both constraints on UK Biobank data and analyze posterior estimates and predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed domain constraints (prevalence and expertise) perform when applied to other selective labels settings beyond healthcare, such as criminal justice or lending?
- Basis in paper: [inferred] The paper mentions that selective labels settings occur in many domains and suggests that the general class of domain constraints revealed by the analysis could improve model estimation in many settings. However, the proposed constraints are only validated on a healthcare case study.
- Why unresolved: The paper does not empirically validate the performance of the proposed constraints in other selective labels domains beyond healthcare.
- What evidence would resolve it: Conducting experiments applying the proposed constraints to other selective labels datasets (e.g., criminal justice, lending) and comparing the performance to unconstrained models and other baselines would provide evidence of the constraints' generalizability.

### Open Question 2
- Question: How sensitive are the model's inferences to the assumed value of the expertise constraint (i.e., which features are allowed to have non-zero coefficients in the testing decision equation)?
- Basis in paper: [explicit] The paper states that the expertise constraint is implemented by allowing the β∆ coefficients to deviate from 0 only for features that plausibly influence testing probability beyond disease risk. It is acknowledged that the choice of features for the expertise constraint could affect the model's inferences.
- Why unresolved: The paper does not explore how sensitive the model's inferences are to different choices of features for the expertise constraint.
- What evidence would resolve it: Conducting experiments with different choices of features for the expertise constraint and comparing the resulting inferences (e.g., age trends, β∆ coefficients) would reveal the sensitivity of the model to this assumption.

### Open Question 3
- Question: How does the model's performance and inferences change when using a more complex model with nonlinear interactions between features, compared to the linear model used in the paper?
- Basis in paper: [explicit] The paper mentions that the linear model used in the case study is sufficient for the low-dimensional features, but the approach naturally extends to more complex inputs and deep learning models sometimes used in breast cancer risk prediction. It also presents synthetic experiments with pairwise nonlinear interactions between features.
- Why unresolved: The paper does not empirically compare the performance and inferences of the linear model to a more complex model with nonlinear interactions on the real-world case study.
- What evidence would resolve it: Fitting the model to the breast cancer dataset using a more complex model with nonlinear interactions between features and comparing its performance (e.g., AUC, quintile ratio) and inferences (e.g., age trends, β∆ coefficients) to the linear model would provide evidence of the benefits and limitations of using more complex models.

## Limitations
- The model relies on accurate prevalence estimates that aren't directly validated
- Strong assumptions about healthcare decision-making behavior may not generalize to other selective labels domains
- The approach requires careful specification of prior distributions and hyperparameters

## Confidence

High confidence in the Bayesian framework's ability to improve parameter inference through uncertainty quantification and regularization.

Medium confidence in the prevalence constraint's effectiveness, as it depends heavily on accurate prevalence estimation which isn't directly validated.

Low confidence in the expertise constraint due to strong assumptions about decision-making behavior that aren't empirically verified, particularly the exclusion of certain features from affecting testing decisions.

## Next Checks

1. Compare the assumed prevalence values with independent estimates from public health data to verify the accuracy of this key input to the prevalence constraint.

2. Systematically vary which features are excluded from the expertise constraint and measure the impact on parameter estimates and predictive performance to test the robustness of this assumption.

3. Apply the model to a different healthcare dataset with selective testing to evaluate whether the observed improvements in AUC and quintile ratio transfer to other contexts.