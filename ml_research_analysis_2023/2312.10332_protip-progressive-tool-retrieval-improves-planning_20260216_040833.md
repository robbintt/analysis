---
ver: rpa2
title: 'ProTIP: Progressive Tool Retrieval Improves Planning'
arxiv_id: '2312.10332'
source_url: https://arxiv.org/abs/2312.10332
tags:
- tool
- tools
- arxiv
- retrieval
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProTIP, a novel tool retrieval framework
  that outperforms state-of-the-art methods for improving multi-step planning with
  large language models. ProTIP addresses the limitations of single-step and task
  decomposition-based approaches by implicitly performing task decomposition without
  requiring explicit subtask labels, while maintaining subtask-tool atomicity alignment.
---

# ProTIP: Progressive Tool Retrieval Improves Planning

## Quick Facts
- arXiv ID: 2312.10332
- Source URL: https://arxiv.org/abs/2312.10332
- Reference count: 17
- Key outcome: ProTIP achieves 24% improvement in Recall@K=10 for tool retrieval and 41% enhancement in tool accuracy for plan generation compared to ChatGPT task decomposition approach

## Executive Summary
This paper introduces ProTIP, a novel framework for improving multi-step planning with large language models by addressing the limitations of both single-step and task decomposition-based approaches. ProTIP uses contrastive learning to iteratively transform query embeddings in vector space, implicitly performing task decomposition without requiring explicit subtask labels. The framework demonstrates significant improvements on the ToolBench dataset, achieving better tool retrieval and plan generation accuracy while maintaining subtask-tool atomicity alignment.

## Method Summary
ProTIP is a lightweight, contrastive learning-based framework that implicitly performs task decomposition (TD) without explicit subtask labels. It uses a BERT-base-uncased model to encode queries and tool descriptions, fine-tuned using margin-based contrastive loss. The framework iteratively transforms query embeddings by subtracting previously retrieved tool embeddings, enabling progressive retrieval that maintains subtask-tool atomicity. The approach addresses the "subtask-tool atomicity alignment" problem that plagues explicit task decomposition methods.

## Key Results
- 24% improvement in Recall@K=10 for tool retrieval on ToolBench dataset
- 41% enhancement in tool accuracy for plan generation compared to ChatGPT task decomposition approach
- Achieves Recall@K=10 of 0.61 on ToolBench test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative embedding transformation improves retrieval accuracy by aligning query representations with subtask-specific tool embeddings.
- Mechanism: The framework initializes with the full query embedding and iteratively subtracts tool embeddings from the query embedding. This subtraction operation progressively removes the influence of already addressed subtasks, resulting in an embedding that better matches the next subtask's requirements.
- Core assumption: Vector subtraction in embedding space corresponds to removing addressed subtasks from the query's semantic representation.
- Evidence anchors:
  - [abstract]: "Subsequently, the ProTIP module iteratively transforms the query embedding by subtracting previously retrieved tool description embedding from the query embedding."
  - [section 3.1]: "For subsequent retrieval steps, starting with subtask-2, we iteratively subtract Ew(tpos1) to Ew(tposi) from Ew(q) to arrive at an embedding that approximates a query q′ that only represents subtasks from i + 1 to n."

### Mechanism 2
- Claim: Contrastive learning enables the model to learn embedding transformations that implicitly perform task decomposition without requiring explicit subtask labels.
- Mechanism: The framework uses margin-based contrastive loss to train the encoder to minimize distance between query embeddings and relevant tool embeddings while maximizing distance from irrelevant ones. This creates embeddings where subtraction operations naturally eliminate addressed subtasks.
- Core assumption: The embedding space can be structured such that simple vector operations (subtraction) correspond to semantic operations (removing completed subtasks).
- Evidence anchors:
  - [abstract]: "ProTIP is a lightweight, contrastive learning-based framework that implicitly performs TD without the explicit requirement of subtask labels"
  - [section 3.1]: "We use contrastive loss (Hadsell et al., 2006) to fine-tune our retrieval which is suited for metric-based learning."

### Mechanism 3
- Claim: Progressive retrieval maintains subtask-tool atomicity while avoiding the need for explicit task decomposition models.
- Mechanism: By iteratively transforming query embeddings rather than decomposing queries into text subtasks, the framework avoids the "subtask-tool atomicity alignment" problem while still maintaining the correct ordering and matching of tools to subtasks.
- Core assumption: Vector space operations can maintain semantic relationships that text-based decomposition would explicitly preserve.
- Evidence anchors:
  - [abstract]: "ProTIP is a lightweight, contrastive learning-based framework that implicitly performs TD without the explicit requirement of subtask labels, while simultaneously maintaining subtask-tool atomicity."

## Foundational Learning

- Concept: Vector space semantics and embedding transformations
  - Why needed here: The entire framework relies on understanding how semantic relationships can be manipulated through vector operations in embedding space.
  - Quick check question: If you subtract the embedding of "hammer" from "build a table with hammer and nails," what semantic relationship should the resulting vector represent?

- Concept: Contrastive learning and metric-based optimization
  - Why needed here: The framework uses contrastive loss to train embeddings that support the iterative subtraction mechanism.
  - Quick check question: How does contrastive loss encourage similar examples to be close and dissimilar ones to be far apart in embedding space?

- Concept: Task decomposition and multi-step planning
  - Why needed here: Understanding the limitations of explicit task decomposition helps explain why the implicit approach via embeddings is valuable.
  - Quick check question: What challenges arise when trying to maintain "subtask-tool atomicity alignment" in dynamic toolboxes?

## Architecture Onboarding

- Component map: Encoder model (BERT-base-uncased) → Contrastive learning module → Progressive retrieval engine → LLM planner → Tool database

- Critical path: Query → Encoder → Initial retrieval → Embedding transformation → Subsequent retrieval → Tool interleaving → LLM planner

- Design tradeoffs:
  - Vector operations vs. text decomposition: Vector approach avoids atomicity alignment issues but requires careful embedding training
  - Single encoder vs. separate encoders: Using the same encoder simplifies implementation but may limit optimization flexibility
  - Iterative subtraction vs. direct multi-step prediction: Subtraction is more interpretable but may accumulate errors

- Failure signatures:
  - Poor Recall@K metrics indicate embedding transformation isn't capturing subtask relationships
  - High Tool Hallucination in planner suggests retrieved tools don't match actual subtask requirements
  - Degraded performance on complex queries indicates subtraction operations are introducing noise

- First 3 experiments:
  1. Test embedding subtraction on synthetic queries with known subtask structure to verify semantic relationships are preserved
  2. Compare contrastive loss training with alternative objectives (triplet loss, softmax-based) to optimize embedding quality
  3. Evaluate tool interleaving strategy against sequential retrieval to understand impact on planner performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProTIP's performance scale with increasing numbers of tools and API complexity in real-world applications?
- Basis in paper: [inferred] The paper mentions the potential for tool distribution discrepancies between train and test sets and the limitations of the current ToolBench dataset, which may not fully capture the complexity of real-world scenarios.
- Why unresolved: The experiments were conducted on a specific dataset (ToolBench) with a limited number of tools and API calls. The paper does not explore ProTIP's performance on larger, more complex datasets or real-world applications with a broader range of tools and APIs.
- What evidence would resolve it: Testing ProTIP on larger, more diverse datasets and real-world applications with varying numbers of tools and API complexity would provide insights into its scalability and generalization capabilities.

### Open Question 2
- Question: What are the long-term effects of incorporating execution history on the model's performance and potential for overfitting?
- Basis in paper: [explicit] The paper acknowledges that incorporating execution history can lead to truncation issues and potential overfitting, especially when the input size approaches the maximum context window size.
- Why unresolved: The paper does not investigate the long-term effects of incorporating execution history on the model's performance and generalization capabilities. It is unclear how the model's performance would evolve over time with continuous use and accumulation of execution history.
- What evidence would resolve it: Conducting long-term experiments with continuous use of the model and monitoring its performance and generalization capabilities over time would provide insights into the effects of incorporating execution history.

### Open Question 3
- Question: How can the performance of ProTIP be further improved by incorporating additional tool metadata and optimizing the prompt construction?
- Basis in paper: [inferred] The paper suggests that incorporating additional tool metadata, such as tool descriptions, parameters, and API signatures, could potentially improve the model's performance. It also mentions that the prompt construction could be further optimized to be more specific to different scenarios.
- Why unresolved: The paper does not explore the impact of incorporating additional tool metadata and optimizing the prompt construction on ProTIP's performance. It is unclear how these factors would affect the model's ability to accurately retrieve tools and generate plans.
- What evidence would resolve it: Conducting experiments with different combinations of tool metadata and prompt constructions, and evaluating their impact on ProTIP's performance, would provide insights into how these factors can be optimized to further improve the model's capabilities.

## Limitations

- The framework's reliance on vector subtraction assumes linear properties in embedding space that may not hold across all semantic relationships
- Preprocessing steps for hallucination removal are not fully specified, affecting reproducibility
- Limited exploration of performance on queries with varying complexity or domain shifts

## Confidence

- High confidence in the core claim that iterative embedding transformation improves tool retrieval accuracy
- Medium confidence in the mechanism explanation, as the paper provides theoretical justification but limited ablation studies
- Low confidence in the robustness claims, as the evaluation does not extensively test performance on complex queries or domain shifts

## Next Checks

1. Conduct controlled experiments testing embedding subtraction on synthetic queries with known subtask structures to verify that semantic relationships are preserved and subtraction operations don't introduce noise.

2. Perform ablation studies comparing the contrastive loss training objective with alternative approaches (triplet loss, softmax-based objectives) to determine the optimal training strategy for supporting iterative retrieval.

3. Evaluate the framework's performance on queries with varying complexity levels and language phenomena to assess robustness and identify failure modes in the progressive retrieval approach.