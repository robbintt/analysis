---
ver: rpa2
title: Crosslingual Retrieval Augmented In-context Learning for Bangla
arxiv_id: '2311.00587'
source_url: https://arxiv.org/abs/2311.00587
tags:
- language
- bangla
- text
- prompt
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cross-lingual retrieval augmented in-context
  learning method to improve multilingual large language models for low-resource languages,
  specifically Bangla. The approach uses a cross-lingual retriever to find semantically
  similar prompts from high-resource languages like English and incorporates them
  into prompts for multilingual language models (BLOOM, BLOOMZ, mBERT).
---

# Crosslingual Retrieval Augmented In-context Learning for Bangla

## Quick Facts
- arXiv ID: 2311.00587
- Source URL: https://arxiv.org/abs/2311.00587
- Reference count: 21
- Key outcome: Cross-lingual retrieval augmented in-context learning improves Bangla text classification and summarization tasks, with BLOOMZ-3b achieving 5-10% performance gains over zero-shot baselines

## Executive Summary
This paper introduces a cross-lingual retrieval augmented in-context learning method to improve multilingual large language models for low-resource languages, specifically Bangla. The approach uses a cross-lingual retriever to find semantically similar prompts from high-resource languages like English and incorporates them into prompts for multilingual language models (BLOOM, BLOOMZ, mBERT). The method shows consistent improvements over zero-shot performance, with BLOOMZ-3b achieving F1 scores of 0.24 and 0.44 for two Bangla classification tasks, representing 5-10% improvements. The results demonstrate the effectiveness of retrieval-augmented prompting for low-resource languages.

## Method Summary
The approach employs a cross-lingual retriever using a multilingual sentence transformer to map Bangla input text to a shared embedding space and retrieve k semantically similar examples from high-resource languages (English/Hindi). These retrieved examples are integrated into prefix prompt templates alongside the original Bangla text. The augmented prompts are then fed to multilingual pretrained language models (BLOOM, BLOOMZ, mBERT, mT5) for either generative (decoder-only) or masked prediction (encoder) modes. The method is evaluated on Bangla text classification (Vio-Lens for violence detection, SentNoB for sentiment analysis) and summarization (XL-Sum dataset) tasks, comparing zero-shot performance against retrieval-augmented results using F1-scores and Rouge metrics.

## Key Results
- BLOOMZ-3b achieves F1 scores of 0.24 and 0.44 for Bangla violence detection and sentiment analysis tasks, representing 5-10% improvements over zero-shot baselines
- Cross-lingual retrieval consistently improves multilingual models' performance on Bangla tasks compared to zero-shot approaches
- BLOOMZ models benefit more from retrieval augmentation than BLOOM models, likely due to instruction tuning
- Classification tasks show consistent improvements while summarization tasks show mixed or degraded results with retrieval augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual retrieval augments low-resource language prompts by leveraging semantically similar high-resource language examples.
- Mechanism: A cross-lingual retriever maps the input Bangla text to a vector in a shared embedding space and retrieves the most similar English examples, which are then used to enhance the prompt for the multilingual model.
- Core assumption: The cross-lingual retriever can effectively find semantically similar examples across languages, and these examples improve the model's understanding of the low-resource language task.
- Evidence anchors:
  - [abstract]: "By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs)... to successfully boost performance on Bangla tasks."
  - [section 3]: "Using semantic similarities with qembed, the retriever returns the most similar k examples from high-resource languages..."
  - [corpus]: Weak evidence - related papers focus on cross-lingual retrieval but do not specifically address the mechanism described here.
- Break condition: The cross-lingual retriever fails to find semantically similar examples, or the retrieved examples are not relevant to the task at hand.

### Mechanism 2
- Claim: BLOOMZ models benefit more from retrieval-augmented prompting than BLOOM models due to their instruction tuning.
- Mechanism: BLOOMZ models, which are instruction-tuned, are better able to leverage the semantically rich prompts retrieved from high-resource languages, leading to improved performance on low-resource language tasks.
- Core assumption: Instruction tuning enables models to better understand and utilize the context provided by the retrieved examples.
- Evidence anchors:
  - [abstract]: "especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks."
  - [section 4.3]: "BLOOMZ takes a novel approach in the MPLM landscape by applying Bloom filters in the context of language models (Muennighoff et al., 2023). This allows the model to use high-resource languages to improve embeddings for low-resource languages..."
  - [corpus]: Weak evidence - related papers do not specifically address the impact of instruction tuning on retrieval-augmented prompting.
- Break condition: The instruction tuning does not significantly improve the model's ability to leverage the retrieved examples, or the BLOOMZ model is not available.

### Mechanism 3
- Claim: Retrieval-augmented prompting is more effective for classification tasks than for summarization tasks.
- Mechanism: The model's ability to distinguish between nuanced categories in classification tasks is enhanced by the additional context provided by the retrieved examples, while in summarization tasks, the retrieved examples may introduce noise or distraction.
- Core assumption: The additional context provided by the retrieved examples is more beneficial for classification tasks that require distinguishing between specific categories than for summarization tasks that require generating coherent text.
- Evidence anchors:
  - [abstract]: "Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance."
  - [section 5.1]: "With the instructions of k = 3 retrieval augmented English prompts, we enhance the F1-scores of Bloomz-3b on the two tasks by 5% and 10% respectively."
  - [section 5.2]: "Retrieval augmentation seems to drastically affect the performance of mt5-base, reducing its score considerably... For the Bloomz models, bloomz-1b1 still retains decent performance, although there's a drop when compared to its zero-shot performance. Surprisingly, blommz-3b shows a sharper drop..."
- Break condition: The retrieved examples do not provide relevant context for the classification task, or the summarization task benefits from the additional context provided by the retrieved examples.

## Foundational Learning

- Concept: Cross-lingual retrieval and its application in natural language processing.
  - Why needed here: The paper relies on cross-lingual retrieval to find semantically similar examples in high-resource languages that can be used to improve the performance of multilingual models on low-resource language tasks.
  - Quick check question: How does cross-lingual retrieval differ from monolingual retrieval, and what are the challenges associated with cross-lingual retrieval?

- Concept: In-context learning and its application in few-shot learning scenarios.
  - Why needed here: The paper uses in-context learning to enable multilingual models to learn from the retrieved examples without the need for fine-tuning on the low-resource language task.
  - Quick check question: What are the advantages and limitations of in-context learning compared to traditional fine-tuning approaches?

- Concept: Multilingual pretrained language models and their capabilities in handling multiple languages.
  - Why needed here: The paper evaluates the performance of multilingual models (BLOOM, BLOOMZ, and mBERT) on low-resource language tasks and investigates the impact of retrieval-augmented prompting on their performance.
  - Quick check question: How do multilingual models differ from monolingual models in terms of their architecture and training process, and what are the challenges associated with developing effective multilingual models?

## Architecture Onboarding

- Component map: Cross-lingual retriever -> Prompt engineering -> Multilingual pretrained language models
- Critical path: 1) Input Bangla text is processed by the cross-lingual retriever. 2) The retriever maps the input text to a vector in a shared embedding space and retrieves the most similar examples from high-resource languages. 3) The retrieved examples and input text are combined using a prefix prompt template. 4) The prompt is fed into the multilingual pretrained language model for prediction. 5) The model's output is evaluated against the ground truth labels.
- Design tradeoffs: Model selection (BLOOM, BLOOMZ, or mBERT) impacts retrieval-augmented prompting effectiveness; number of retrieved examples (k) affects context quality vs. noise; prompt template design influences model's ability to leverage retrieved examples.
- Failure signatures: Cross-lingual retriever fails to find semantically similar examples or retrieves irrelevant examples; multilingual model does not effectively utilize retrieved examples or is negatively impacted by them; prompt template design is not optimal for the given task or model.
- First 3 experiments: 1) Evaluate cross-lingual retriever performance on held-out dataset to ensure effective semantic matching. 2) Test impact of varying k values on model performance to determine optimal retrieval amount. 3) Experiment with different prompt template designs to identify most effective configuration for task and model.

## Open Questions the Paper Calls Out
- What is the optimal number of retrieval samples (k) for cross-lingual retrieval augmented in-context learning across different tasks and languages?
- How does the choice of high-resource language for retrieval impact performance in cross-lingual retrieval augmented in-context learning?
- How does the performance of cross-lingual retrieval augmented in-context learning scale with model size and architecture?

## Limitations
- Cross-lingual retrieval effectiveness depends on semantic matching quality between Bangla and English, which may not generalize across all domains
- Classification tasks show consistent improvements while summarization tasks show mixed or degraded results, suggesting approach may not generalize uniformly across task types
- The paper uses relatively small model variants (3B parameters) rather than larger models that might show different behavior patterns

## Confidence
- High confidence in core finding that cross-lingual retrieval-augmented prompting provides consistent improvements over zero-shot performance for Bangla classification tasks
- Medium confidence in mechanism explaining why BLOOMZ benefits more due to instruction tuning
- Low confidence in generalizability to other low-resource languages or more complex task types beyond tested classification and summarization

## Next Checks
1. Conduct ablation studies varying the number of retrieved examples (k) systematically across different values to identify optimal retrieval-augmented prompting configurations
2. Test the approach on additional low-resource languages with similar characteristics to Bangla to determine cross-lingual transfer effectiveness
3. Implement human evaluation of retrieved examples to verify semantic relevance and quality, particularly focusing on cases where retrieval-augmentation either succeeded dramatically or failed