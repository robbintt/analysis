---
ver: rpa2
title: 'SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation'
arxiv_id: '2305.13194'
source_url: https://arxiv.org/abs/2305.13194
tags:
- dataset
- metrics
- seahorse
- palm
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEAHORSE, a large-scale multilingual dataset
  for summarization evaluation. The dataset consists of 96K summaries with human ratings
  across 6 quality dimensions (comprehensibility, repetition, grammar, attribution,
  main ideas, conciseness) in 6 languages, generated by 9 systems from 4 summarization
  datasets.
---

# SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation

## Quick Facts
- arXiv ID: 2305.13194
- Source URL: https://arxiv.org/abs/2305.13194
- Reference count: 19
- Primary result: SEAHORSE dataset enables training of multilingual summarization evaluation metrics that generalize to 45 languages without fine-tuning

## Executive Summary
SEAHORSE is a large-scale multilingual dataset for summarization evaluation, containing 96K summaries with human ratings across 6 quality dimensions (comprehensibility, repetition, grammar, attribution, main ideas, conciseness) in 6 languages. The dataset covers summaries generated by 9 systems from 4 summarization datasets and enables training learned metrics that achieve strong performance on out-of-domain meta-evaluation benchmarks. By using reference-free annotations and separating training and test data from different splits, SEAHORSE provides a foundation for developing robust, generalizable evaluation metrics across languages and quality dimensions.

## Method Summary
The authors collected human ratings on 96K summaries generated by 9 systems across 4 datasets in 6 languages, annotating each summary on 6 quality dimensions without reference comparison. They finetuned separate mT5-xxl models for each quality dimension using binary classification objectives to predict human ratings. The training data came from validation splits of original datasets while test data came from test splits, preventing test set contamination. Metrics were evaluated on both the SEAHORSE test set and external benchmarks (mFACE with 45 languages, TRUE) to assess generalization capabilities.

## Key Results
- SEAHORSE-trained metrics achieved strong correlation with human ratings on SEAHORSE test set
- mt5SEAHORSE showed zero-shot multilingual generalization, performing well on 45-language mFACE benchmark without fine-tuning
- SEAHORSE metrics outperformed strong baselines on both in-domain (SEAHORSE) and out-of-domain (mFACE, TRUE) evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset enables training of metrics that generalize across languages without direct supervision.
- Mechanism: SEAHORSE contains ratings across 6 languages, allowing a learned metric to capture language-agnostic patterns in quality assessment that transfer to unseen languages in benchmarks like mFACE.
- Core assumption: Quality dimensions such as comprehensibility and attribution share similar underlying features across languages.
- Evidence anchors:
  - [abstract] shows that metrics trained with SEAHORSE achieve strong performance on the mFACE benchmark, which consists of 45 languages, thus exhibiting the zero-shot multilingual generalization potential.
  - [section 4.3] demonstrates mt5SEAHORSE performs well on 5 languages in mFACE that overlap with SEAHORSE and also on the full 45-language mFACE set.
- Break condition: If quality assessment patterns are language-specific rather than universal, zero-shot generalization would fail.

### Mechanism 2
- Claim: Separating training and test data from different splits of original datasets prevents overfitting during metric development.
- Mechanism: By using validation splits for training SEAHORSE metrics and test splits for evaluation, the learned metrics avoid memorizing test examples, enabling reliable hyperparameter selection.
- Core assumption: Test set leakage is a primary source of overfitting in learned metric development.
- Evidence anchors:
  - [section 2.1] explicitly states the training and validation splits come from the original datasets' validation sets, and the test set contains summaries from test sets, to prevent test set contamination when training metrics.
  - [section 4] mentions using a filtered version of the dataset for training metrics, with train/dev/test splits.
- Break condition: If the validation and test splits are not sufficiently independent, leakage could still occur.

### Mechanism 3
- Claim: Using reference-free annotations allows metrics to reward diverse, relevant summaries rather than forcing similarity to a single reference.
- Mechanism: Annotators evaluate each summary directly without comparison to a reference, enabling metrics trained on these annotations to prefer summaries that are appropriate even if different from the reference.
- Core assumption: Reference-based evaluation unfairly penalizes valid but divergent summaries.
- Evidence anchors:
  - [section 2.2] states the annotation process is reference-less, i.e., the annotator is never comparing a model-generated summary with the reference summary.
  - [section 2.2] notes this enables training reference-less metrics, which have the added benefit of being able to be used at inference time for re-ranking.
- Break condition: If reference similarity is a necessary component of quality, reference-free training would miss this signal.

## Foundational Learning

- Concept: Zero-shot multilingual generalization
  - Why needed here: SEAHORSE metrics achieve strong results on mFACE without any fine-tuning, showing they can generalize to languages not seen during training.
  - Quick check question: Can a metric trained on 6 languages perform well on 45 languages it has never seen?

- Concept: Reference-free evaluation
  - Why needed here: SEAHORSE uses reference-less human annotations, allowing metrics to evaluate summaries based on intrinsic quality rather than similarity to a reference.
  - Quick check question: Does removing reference comparison improve the ability to detect diverse but relevant summaries?

- Concept: Test set contamination prevention
  - Why needed here: SEAHORSE explicitly separates training and test data from different splits to ensure metrics can be used for model development without overfitting.
  - Quick check question: Are the training and test splits of SEAHORSE drawn from different original dataset splits?

## Architecture Onboarding

- Component map: Data collection pipeline -> Metric training module -> Evaluation framework
- Critical path: Data collection → Metric training → Evaluation → Generalization assessment
- Design tradeoffs: Reference-free vs. reference-based evaluation; multilingual coverage vs. annotation depth per language
- Failure signatures: Poor generalization to unseen languages; overfitting indicated by high in-domain but low out-of-domain performance
- First 3 experiments:
  1. Train mt5SEAHORSE on SEAHORSE and evaluate on SEAHORSE test set to confirm learning capability
  2. Apply mt5SEAHORSE to mFACE without fine-tuning to test zero-shot multilingual generalization
  3. Compare mt5SEAHORSE to t5NLI on TRUE benchmark to assess attribution metric performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SEAHORSE-trained metrics generalize to other NLG tasks beyond summarization, such as machine translation or dialogue generation?
- Basis in paper: [inferred] The paper mentions that mt5SEAHORSE performs competitively to t5NLI on dialogue datasets (BEGIN, Q2, and DialFact), indicating its suitability for evaluating tasks outside of summarization.
- Why unresolved: While the paper shows some evidence of generalization to dialogue tasks, it does not extensively explore performance on other NLG tasks like machine translation.
- What evidence would resolve it: Comprehensive evaluation of SEAHORSE-trained metrics on a variety of NLG tasks, including machine translation, dialogue generation, and text simplification, would provide clearer evidence of their generalization capabilities.

### Open Question 2
- Question: What are the specific error types and patterns that different summarization models (e.g., t5_xxl vs. mt5_xxl) exhibit in the SEAHORSE dataset, and how do these errors relate to the human ratings across the six quality dimensions?
- Basis in paper: [explicit] The paper discusses the choice of systems covering a range of performance to capture diverse model error types and mentions analyzing positive response rates to inspect dataset quality.
- Why unresolved: The paper does not provide a detailed breakdown of specific error types and patterns across models and their relationship to human ratings.
- What evidence would resolve it: A fine-grained error analysis of model-generated summaries in the SEAHORSE dataset, correlating specific error types with human ratings across all six dimensions, would provide deeper insights into model strengths and weaknesses.

### Open Question 3
- Question: How does the performance of SEAHORSE-trained metrics vary across different languages, and what factors contribute to any observed differences in performance?
- Basis in paper: [explicit] The paper mentions evaluating mt5SEAHORSE on mFACE, which consists of 45 languages, and notes its performance on the subset of 5 languages common to both datasets.
- Why unresolved: While the paper shows some cross-lingual performance, it does not provide a comprehensive analysis of how metric performance varies across all six languages in SEAHORSE or the factors contributing to any differences.
- What evidence would resolve it: A detailed cross-lingual analysis of SEAHORSE-trained metric performance across all six languages, examining factors such as language family, dataset characteristics, and model architecture, would provide insights into the challenges and opportunities for multilingual evaluation.

## Limitations

- Annotation consistency across languages is uncertain due to potential cultural or linguistic biases not fully quantified
- The leap from 6 training languages to 45 test languages in mFACE represents a significant generalization gap
- Metric robustness to summary diversity and alternative valid expressions is not extensively tested

## Confidence

- High confidence: SEAHORSE provides a valuable multilingual dataset for summarization evaluation
- Medium confidence: Zero-shot multilingual generalization claim, as detailed analysis of which languages benefit is lacking
- Medium confidence: Superiority of reference-free evaluation, without extensive ablation studies

## Next Checks

1. **Cross-linguistic annotation consistency**: Conduct a small-scale study where the same summaries are annotated by speakers of different languages to quantify how cultural and linguistic background affects quality ratings across the 6 dimensions.

2. **Zero-shot generalization analysis**: For each language in mFACE, analyze the linguistic distance from the 6 training languages (using features like syntax, morphology, or language family) and correlate this with metric performance to understand which types of languages generalize better.

3. **Reference-free vs. reference-based comparison**: Train parallel versions of the mt5SEAHORSE metric - one using only reference-free annotations and another incorporating reference comparison - and compare their performance on both in-domain and out-of-domain benchmarks to quantify the trade-offs of each approach.