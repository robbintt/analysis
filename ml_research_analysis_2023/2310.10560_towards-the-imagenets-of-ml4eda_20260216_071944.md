---
ver: rpa2
title: Towards the Imagenets of ML4EDA
arxiv_id: '2310.10560'
source_url: https://arxiv.org/abs/2310.10560
tags:
- synthesis
- verilog
- code
- logic
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the lack of standardized datasets and learning
  tasks in the electronic design automation (EDA) domain. To spur progress in machine
  learning for EDA, the authors curate two large-scale, high-quality datasets: VeriGen,
  a corpus of Verilog code for training models to generate hardware description language
  (HDL) code, and OpenABC-D, a dataset of 870,000 And-Inverter-Graphs (AIGs) for logic
  synthesis tasks.'
---

# Towards the Imagenets of ML4EDA

## Quick Facts
- arXiv ID: 2310.10560
- Source URL: https://arxiv.org/abs/2310.10560
- Reference count: 37
- This work addresses the lack of standardized datasets and learning tasks in the electronic design automation (EDA) domain by curating two large-scale, high-quality datasets: VeriGen for Verilog code generation and OpenABC-D for logic synthesis.

## Executive Summary
This paper addresses the critical need for standardized datasets in machine learning for electronic design automation (ML4EDA). The authors curate two large-scale, high-quality datasets: VeriGen, a corpus of Verilog code for training models to generate hardware description language (HDL) code, and OpenABC-D, a dataset of 870,000 And-Inverter-Graphs (AIGs) for logic synthesis tasks. These datasets, along with open-source evaluation frameworks, aim to provide the EDA community with standardized benchmarks to spur research progress, similar to how ImageNet catalyzed advances in computer vision.

## Method Summary
The authors curate VeriGen by collecting Verilog code from GitHub repositories and 70 Verilog-centric textbooks, then developing an evaluation framework to assess syntactic and functional accuracy of generated code. For OpenABC-D, they synthesize AIGs from 29 open-source hardware projects using the ABC logic synthesis tool and provide an open-source framework for generating labeled data. The datasets are designed to enable ML models to learn from diverse, real-world hardware design scenarios and evaluate performance on standard tasks like Verilog code generation and PPA prediction.

## Key Results
- Curated VeriGen dataset with 65k Verilog files (~1GB) from GitHub and textbooks
- Generated OpenABC-D dataset with 870,000 AIGs from 1500 synthesis runs on 29 open-source IPs
- Demonstrated promising results using graph convolutional networks for PPA prediction tasks on OpenABC-D
- Open-sourced datasets and evaluation tools to enable further research in ML for EDA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing standardized, large-scale, labeled datasets removes the primary bottleneck in ML for EDA research.
- Mechanism: Researchers can train and evaluate ML models without first spending months curating data, enabling faster iteration and comparison of methods.
- Core assumption: The availability of high-quality, domain-specific datasets is a prerequisite for meaningful progress in ML applications.
- Evidence anchors:
  - [abstract] "Experience from the computer vision community suggests that such datasets are crucial to spur further progress in ML for EDA."
  - [section I] "there is a noticeable gap: the lack of standard datasets or typical learning tasks for the EDA domain."
  - [corpus] Weak - only mentions "Found 25 related papers" without quantifying the specific impact of dataset availability on research progress.
- Break condition: If the datasets are not representative of real-world hardware designs or lack sufficient diversity, models trained on them may not generalize to practical EDA tasks.

### Mechanism 2
- Claim: Open-sourcing both datasets and evaluation frameworks enables reproducibility and community-driven improvements.
- Mechanism: By providing VeriGen and OpenABC-D along with evaluation tools, the authors create a common ground for researchers to build upon, compare results, and extend the work.
- Core assumption: Reproducibility and open access are essential for collaborative advancement in scientific research.
- Evidence anchors:
  - [abstract] "The datasets and evaluation tools are open-sourced to enable further research in ML for EDA."
  - [section I] "we are making our extensive training and evaluation resources, available to the global community"
  - [corpus] Weak - corpus neighbors do not directly address the impact of open-sourcing on community progress.
- Break condition: If the open-source tools are difficult to use or have limited documentation, adoption and contribution from the broader community may be hindered.

### Mechanism 3
- Claim: Diverse data sources (GitHub, textbooks, open-source IPs) ensure the datasets capture the complexity and variety of real-world hardware design challenges.
- Mechanism: By curating data from multiple sources, the authors create datasets that expose ML models to a wide range of design patterns, complexities, and edge cases.
- Core assumption: ML models generalize better when trained on diverse, real-world data that reflects the full spectrum of application scenarios.
- Evidence anchors:
  - [section II-A] "The primary corpus for training was derived from open-source Verilog code found in public GitHub repositories... To augment the primary training dataset, 70 Verilog-centric textbooks were procured"
  - [section III] "we use 29 open source IPs covering diverse functions... Unlike ISCAS [31], [32] and EPFL [33] benchmarks focused on limited function IPs, these benchmarks are more diverse."
  - [corpus] Weak - corpus neighbors do not directly address the diversity of data sources used in the datasets.
- Break condition: If the data sources are not sufficiently diverse or representative of the full range of hardware design scenarios, models may fail to generalize to unseen cases.

## Foundational Learning

- Concept: Hardware Description Languages (HDLs) and Verilog syntax
  - Why needed here: Understanding the structure and semantics of Verilog is essential for interpreting the VeriGen dataset and evaluating generated code.
  - Quick check question: What are the key components of a Verilog module declaration (module name, ports, functionality)?

- Concept: Logic synthesis and And-Inverter Graphs (AIGs)
  - Why needed here: Grasping the concepts of logic synthesis and AIGs is crucial for understanding the OpenABC-D dataset and its application to ML for logic optimization.
  - Quick check question: What is the purpose of logic synthesis, and how do AIGs represent combinational logic circuits?

- Concept: Graph Neural Networks (GNNs) and their application to EDA
  - Why needed here: Familiarity with GNNs is necessary for understanding how the OpenABC-D dataset is used to train models for PPA prediction tasks.
  - Quick check question: How do GNNs operate on graph-structured data, and what are their advantages for EDA applications?

## Architecture Onboarding

- Component map:
  - VeriGen dataset: Verilog code corpus (GitHub + textbooks)
  - VeriGen evaluation framework: Problem set, test benches, LLM inference mechanism
  - OpenABC-D dataset: AIGs, synthesis recipes, PPA labels
  - OpenABC-D framework: RTL synthesis, graph processing, ML preprocessing
  - ML models: Graph convolutional networks (GCNs) for PPA prediction

- Critical path:
  1. Curate and preprocess Verilog code for VeriGen
  2. Develop problem set and test benches for evaluation
  3. Generate AIGs and labels for OpenABC-D
  4. Preprocess data for ML model input
  5. Train and evaluate ML models on the datasets

- Design tradeoffs:
  - VeriGen: Balancing dataset size and quality (e.g., filtering large files, ensuring functional correctness)
  - OpenABC-D: Managing computational resources for synthesis runs and labeling
  - ML models: Choosing appropriate network architectures and hyperparameters for PPA prediction tasks

- Failure signatures:
  - VeriGen: Generated Verilog code fails to compile or passes test benches but lacks functional correctness
  - OpenABC-D: ML models fail to generalize to unseen synthesis recipes or IPs, or have high prediction errors for PPA metrics

- First 3 experiments:
  1. Evaluate LLM-generated Verilog code on VeriGen's problem set using the provided test benches
  2. Train a simple regression model on a subset of OpenABC-D to predict AIG node counts
  3. Visualize the distribution of AIG structures in OpenABC-D to identify potential biases or gaps in the dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size and composition of training datasets for ML models to achieve high accuracy in Verilog code generation?
- Basis in paper: [explicit] The paper discusses the challenges of dataset quality and security for ML models in EDA, and the need for extensive and diverse datasets for effective model training and evaluation.
- Why unresolved: The paper does not provide specific guidelines or metrics for determining the optimal dataset size and composition. It only mentions that commercial LLMs like GitHub Copilot are susceptible to errors due to the absence of rich and varied training data.
- What evidence would resolve it: Comparative studies evaluating the performance of ML models trained on datasets of varying sizes and compositions, along with analysis of the impact on accuracy and generalization.

### Open Question 2
- Question: How can data augmentation techniques be tailored for the hardware domain to improve the performance of ML models in EDA tasks?
- Basis in paper: [explicit] The paper mentions the use of novel data augmentation tools that are tailored for the hardware domain, but does not provide details on their effectiveness or implementation.
- Why unresolved: The paper does not provide specific examples or evaluation of data augmentation techniques for EDA tasks, nor does it discuss the challenges or limitations of such techniques in the hardware domain.
- What evidence would resolve it: Experimental studies comparing the performance of ML models trained with and without domain-specific data augmentation techniques, along with analysis of the impact on accuracy and efficiency.

### Open Question 3
- Question: What are the key factors influencing the effectiveness of ML models in predicting PPA (Power, Performance, Area) metrics for logic synthesis tasks?
- Basis in paper: [explicit] The paper discusses the OpenABC-D dataset and its use in benchmarking ML models for PPA prediction tasks, but does not provide insights into the factors influencing model effectiveness.
- Why unresolved: The paper does not analyze the impact of factors such as AIG encoding, synthesis recipe encoding, or graph-level processing on the performance of ML models in PPA prediction tasks.
- What evidence would resolve it: Ablation studies evaluating the impact of different factors on model performance, along with analysis of the relationship between AIG structure, synthesis recipes, and PPA metrics.

## Limitations
- The paper doesn't quantify how representative these datasets are of the full range of real-world hardware design scenarios
- Limited discussion of potential biases introduced by the GitHub scraping methodology
- No validation of the datasets against industrial EDA tools or workflows

## Confidence
- High confidence in the dataset curation methodology and the need for standardized benchmarks in ML4EDA
- Medium confidence in the evaluation framework's ability to capture real-world design complexity
- Low confidence in the generalizability of results to industrial EDA workflows

## Next Checks
1. Analyze the distribution of design complexities and functionalities in both datasets to identify potential gaps or biases
2. Validate the VeriGen evaluation framework by testing it on a held-out set of textbook examples not used in training
3. Conduct a small-scale comparison between models trained on these datasets versus models trained on synthetic or smaller-scale benchmarks to quantify the benefit of dataset scale