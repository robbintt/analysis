---
ver: rpa2
title: 'M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large
  Language Models'
arxiv_id: '2306.05179'
source_url: https://arxiv.org/abs/2306.05179
tags:
- questions
- language
- llms
- question
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces M3Exam, a multilingual, multimodal, and multilevel
  benchmark dataset for evaluating large language models (LLMs) using real human exam
  questions. M3Exam addresses limitations of existing benchmarks by including 12,317
  questions across 9 languages (en, zh, it, pt, vi, th, sw, af, jv) at three educational
  levels (low, mid, high), with 23% requiring image processing.
---

# M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models

## Quick Facts
- **arXiv ID**: 2306.05179
- **Source URL**: https://arxiv.org/abs/2306.05179
- **Reference count**: 40
- **Primary result**: GPT-4 achieves 72.92% accuracy on M3Exam, outperforming other models but still struggling with non-Latin and low-resource languages

## Executive Summary
M3Exam is a comprehensive benchmark dataset designed to evaluate large language models (LLMs) on real human exam questions across multiple languages, modalities, and educational levels. The dataset contains 12,317 questions in 9 languages spanning three educational levels, with 23% requiring image processing. The authors systematically evaluate top-performing LLMs including GPT-4, ChatGPT, Claude, BLOOM, Vicuna, and various multimodal models. The benchmark reveals that while GPT-4 achieves the highest overall accuracy, all models struggle significantly with non-Latin scripts and low-resource languages, and multimodal models fail to effectively process complex exam images.

## Method Summary
The M3Exam benchmark is constructed from real human exam questions across 9 diverse languages (English, Chinese, Italian, Portuguese, Vietnamese, Thai, Swahili, Afrikaans, and Javanese) at three educational levels (low, mid, high). The dataset includes 12,317 questions with 23% requiring image processing. Evaluation uses a zero-shot approach where models are prompted with language-specific instructions for each question. Text-only models are assessed on all questions, while multimodal models handle image-based questions through constraint decoding to generate valid multiple-choice answers. Performance is measured by accuracy across different language groups, educational levels, and question types.

## Key Results
- GPT-4 achieves the highest accuracy at 72.92% across all questions
- All models show significant performance degradation on non-Latin and low-resource languages
- Multimodal models score below 50% accuracy on image-based questions
- LLM performance does not consistently decrease with educational level, unlike human performance
- Current models struggle with complex image details required for subjects like mathematics and science

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: M3Exam achieves comprehensive evaluation of LLMs by integrating multilingual, multimodal, and multilevel exam data.
- **Mechanism**: The dataset combines real-world human exam questions from 9 diverse languages, across 3 educational levels, with 23% requiring image processing, enabling holistic assessment of LLM capabilities in language understanding, domain knowledge, and problem-solving.
- **Core assumption**: Real exam questions from different countries provide authentic, culturally grounded testbeds that better reflect practical LLM deployment scenarios than synthetic or translated data.
- **Evidence anchors**:
  - [abstract]: "M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries... (2) multimodality... (3) multilevel structure..."
  - [section]: "Exams are widely used to assess human intelligence... Consequently, exam questions offer an ideal testbed for evaluating the general intelligence of LLMs."
  - [corpus]: **Weak** - no direct mention of M3Exam; corpus neighbors discuss related multilingual/multimodal benchmarks but not M3Exam specifically.

### Mechanism 2
- **Claim**: M3Exam exposes LLM limitations in non-Latin scripts and low-resource languages.
- **Mechanism**: By including languages like Thai (non-Latin) and Javanese (low-resource), the benchmark reveals that current models, even GPT-4, struggle with character encoding, cultural context, and limited training data for these languages.
- **Core assumption**: The performance gap between Latin-script and non-Latin/low-resource languages in M3Exam directly reflects the model's linguistic and cultural understanding gaps.
- **Evidence anchors**:
  - [abstract]: "GPT-4 achieves the highest accuracy (72.92%), but still underperforms on non-Latin and low-resource languages."
  - [section]: "We observe that existing models generally perform worse for non-Latin languages, such as Chinese (despite being relatively high-resource), as well as low-resource languages like Javanese."
  - [corpus]: **Missing** - corpus does not discuss performance differences by script type.

### Mechanism 3
- **Claim**: M3Exam reveals that LLM intelligence development does not mirror human educational progression.
- **Mechanism**: Unlike humans, whose performance typically decreases with educational level, LLMs show inconsistent performance across low, mid, and high school levels, suggesting different learning dynamics.
- **Core assumption**: The monotonic decrease in human performance with educational level is a valid baseline for comparing LLM development patterns.
- **Evidence anchors**:
  - [abstract]: "Surprisingly, LLM performance does not consistently decrease with educational level, unlike human performance, suggesting differences in intelligence development between LLMs and humans."
  - [section]: "This result suggests that although LLMs show impressive results... the emergence and development of intelligence in LLMs have significant differences from that of human intelligence."
  - [corpus]: **Missing** - corpus does not discuss educational level effects on LLM performance.

## Foundational Learning

- **Concept**: Multilingual evaluation design
  - Why needed here: To ensure the benchmark fairly assesses LLM performance across languages with different scripts, resource levels, and cultural contexts.
  - Quick check question: Does the dataset include both high-resource and low-resource languages, as well as Latin and non-Latin scripts?

- **Concept**: Multimodal question processing
  - Why needed here: To test whether LLMs can integrate visual and textual information, a key aspect of real-world problem-solving.
  - Quick check question: Are images in the dataset varied in complexity and integrated naturally into the questions?

- **Concept**: Educational level stratification
  - Why needed here: To evaluate whether LLM performance correlates with increasing cognitive demands, as it does for humans.
  - Quick check question: Do the questions at higher educational levels require more advanced reasoning or domain knowledge?

## Architecture Onboarding

- **Component map**: OCR + annotation -> preprocessing -> model inference -> evaluation
- **Critical path**: Annotator quality -> data consistency -> prompt clarity -> model output parsing -> metric calculation
- **Design tradeoffs**: Real exam data vs. synthetic data (authenticity vs. control), multilingual prompts vs. English instructions (cultural fidelity vs. model familiarity), zero-shot vs. few-shot evaluation (naturalness vs. performance boost)
- **Failure signatures**: OCR errors -> annotation drift -> prompt ambiguity -> model hallucination -> metric misalignment
- **First 3 experiments**:
  1. Run a small multilingual subset through multiple models to check language consistency
  2. Validate OCR and annotation quality on a random sample of questions
  3. Test prompt variations (zero-shot vs. few-shot) on a held-out development set to confirm robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific reasons for the discrepancy between LLM performance and human performance across educational levels?
- Basis in paper: [explicit] The paper notes that LLM performance does not show a monotonic decrease with educational level, unlike human performance, implying differences in intelligence development between LLMs and humans.
- Why unresolved: The paper identifies the phenomenon but does not provide a detailed explanation for why LLMs behave differently from humans in this regard.
- What evidence would resolve it: Further research analyzing the underlying reasons for LLM failures at different educational levels, comparing the learning processes of LLMs and humans, and investigating the impact of training data on LLM performance.

### Open Question 2
- Question: How can the multilingual capabilities of LLMs be improved, particularly for low-resource and non-Latin script languages?
- Basis in paper: [explicit] The paper finds that current models struggle with multilingual text, particularly in low-resource and non-Latin script languages, suggesting that there is still room for improvement in their multilingual capabilities.
- Why unresolved: While the paper identifies the challenge, it does not propose specific solutions or strategies to enhance LLM performance in these languages.
- What evidence would resolve it: Research exploring new training techniques, data augmentation methods, or model architectures that can better handle low-resource and non-Latin script languages, along with empirical evaluations demonstrating their effectiveness.

### Open Question 3
- Question: What are the key factors that contribute to the difficulty of multimodal understanding in exam questions, and how can multimodal models be improved to address these challenges?
- Basis in paper: [explicit] The paper observes that multimodal models struggle to comprehend complex image details in exam questions, which are vital for various subjects, and suggests that pre-training on multiple images does not necessarily guarantee better multimodal understanding abilities.
- Why unresolved: The paper highlights the challenges but does not provide a detailed analysis of the specific factors that make multimodal understanding difficult or propose concrete solutions to improve model performance.
- What evidence would resolve it: Research investigating the types of image details and reasoning tasks that pose the greatest challenges for multimodal models, as well as studies exploring new model architectures, training strategies, or data preprocessing techniques that can enhance multimodal understanding in the context of exam questions.

## Limitations

- The benchmark construction process may introduce biases from source materials that affect model evaluation
- OCR and annotation pipelines could introduce errors, particularly for non-Latin scripts
- Zero-shot evaluation may not capture full model capabilities that emerge with fine-tuning or few-shot approaches
- Performance differences across languages may reflect cultural context familiarity rather than pure language understanding

## Confidence

- **High Confidence**: The core finding that GPT-4 achieves highest overall accuracy (72.92%) is well-supported by direct comparison across multiple models using standardized evaluation procedures. The multimodal performance gap between text-only and multimodal models is clearly demonstrated.

- **Medium Confidence**: The observation of systematic underperformance on non-Latin and low-resource languages is supported by data, but may be confounded by dataset quality variations and OCR accuracy differences. The claim about developmental differences between LLMs and human intelligence is suggestive but requires more controlled studies to establish causation.

- **Low Confidence**: The specific performance metrics for individual languages and educational levels should be interpreted cautiously due to potential data quality variations and the limited sample sizes for some language-level combinations.

## Next Checks

1. **Data Quality Validation**: Conduct a systematic audit of 100 randomly sampled questions across all languages and levels to verify OCR accuracy, annotation consistency, and cultural appropriateness of questions and answers.

2. **Prompt Robustness Test**: Implement a controlled experiment varying prompt formats (zero-shot vs. few-shot) and instructions across the same question subsets to quantify the impact of evaluation methodology on performance metrics.

3. **Cultural Context Analysis**: Design a follow-up study isolating questions that require specific cultural knowledge versus those testing pure language ability, to better understand the sources of performance variation across languages.