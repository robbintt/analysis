---
ver: rpa2
title: 'FALCON: Feature-Label Constrained Graph Net Collapse for Memory Efficient
  GNNs'
arxiv_id: '2312.16542'
source_url: https://arxiv.org/abs/2312.16542
tags:
- graph
- falcon
- node
- training
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FALCON addresses the challenge of memory-efficient training of
  Graph Neural Networks (GNNs) on large datasets. It introduces a topology-aware graph
  reduction technique that preserves feature-label distribution by combining K-Means
  clustering with a dimension-normalized Euclidean distance.
---

# FALCON: Feature-Label Constrained Graph Net Collapse for Memory Efficient GNNs

## Quick Facts
- arXiv ID: 2312.16542
- Source URL: https://arxiv.org/abs/2312.16542
- Authors: 
- Reference count: 40
- Key outcome: FALCON reduces graph size by up to 66% while maintaining equal prediction quality, achieving up to 97% GPU memory reduction compared to GCN on PPI dataset.

## Executive Summary
FALCON introduces a topology-aware graph reduction technique that preserves feature-label distribution through K-Means clustering with a novel dimension-normalized Euclidean distance. By combining graph reduction with mini-batching and quantization methods, FALCON-QSIGN achieves significant memory efficiency gains while maintaining prediction quality across various GNN models. The framework demonstrates substantial improvements in GPU memory usage and training speed on benchmark datasets like PPI and Flickr.

## Method Summary
FALCON addresses memory bottlenecks in GNN training by implementing a feature-label constrained graph reduction technique. The method uses K-Means clustering with a dimension-normalized Euclidean distance to group nodes based on similar features and labels, then applies topology-aware node collapse using edge contraction. This is combined with SIGN's mini-batching approach and 2-bit activation quantization to create a comprehensive memory-efficient framework that reduces computational requirements without sacrificing model performance.

## Key Results
- Graph reduction preserves prediction quality while collapsing graphs to as low as 34% of original nodes
- FALCON-QSIGN achieves up to 97% GPU memory reduction compared to standard GCN on PPI dataset
- Training speed improves by up to 72% reduction in epoch times on Flickr dataset

## Why This Works (Mechanism)

### Mechanism 1
Preserving feature-label distribution during graph collapse maintains prediction quality despite reduced graph size. The method clusters nodes by features and labels, then distributes node collapse proportionally within each cluster to prevent label or feature imbalance.

### Mechanism 2
Combining graph reduction with quantization and mini-batching achieves maximal memory efficiency without accuracy loss. FALCON reduces graph size, SIGN decouples aggregation from transformation for efficient mini-batching, and activation quantization compresses layer data to low-bit precision.

### Mechanism 3
Topology-aware node collapse preserves graph connectivity and information flow while reducing node count. Instead of removing nodes directly, FALCON merges the least important node into its most important neighbor, contracting edges to maintain connectivity.

## Foundational Learning

- Concept: Graph Neural Networks and their memory bottlenecks
  - Why needed here: Understanding why GNNs are memory-intensive is crucial to appreciating the need for FALCON and its impact.
  - Quick check question: Why is mini-batching more complex for GNNs compared to traditional neural networks?

- Concept: Graph centrality measures (degree, betweenness, closeness, PageRank, eigenvector)
  - Why needed here: These measures are used to identify important nodes for topology-aware collapse.
  - Quick check question: How does PageRank centrality differ from simple degree centrality in evaluating node importance?

- Concept: K-Means clustering and distance metrics
  - Why needed here: Clustering is used to preserve feature-label distribution, and the dimension-normalized Euclidean distance is a key innovation.
  - Quick check question: Why is a dimension-normalized distance metric necessary when clustering nodes based on both features and labels?

## Architecture Onboarding

- Component map: Original graph (nodes, edges, features, labels) -> FALCON (centrality-based collapse with feature-label constraint) -> SIGN model (decoupled aggregation and transformation) -> 2-bit activation quantization -> Reduced graph and trained GNN model
- Critical path: Graph reduction → mini-batching construction → layer compression → model training
- Design tradeoffs:
  - Centrality measure choice: Different measures prioritize different aspects of node importance, affecting both collapse quality and computational cost.
  - Number of clusters: More clusters better preserve distribution but increase clustering complexity.
  - Quantization bits: Lower bits save more memory but may introduce more error.
- Failure signatures:
  - OOM errors during training: Indicates insufficient graph reduction or compression.
  - Significant performance drop: Suggests poor feature-label distribution preservation or excessive quantization error.
  - Slow epoch times: May indicate inefficient mini-batching or overhead from compression/decompression.
- First 3 experiments:
  1. Run FALCON with default parameters on a small dataset (e.g., Cora) and verify graph reduction and feature-label distribution preservation.
  2. Implement SIGN model with the reduced graph and verify mini-batching works as expected.
  3. Add 2-bit activation quantization and measure memory usage and epoch time compared to the unquantized version.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of centrality measure impact the feature-label distribution preservation across different graph types and sizes?
- Basis in paper: The paper shows that different centrality measures (DC, BC, CC, PR, EC) have varying impacts on prediction quality, with EC generally performing best.
- Why unresolved: The paper primarily focuses on EC's performance and does not systematically compare the effectiveness of different centrality measures in preserving feature-label distributions across diverse datasets.
- What evidence would resolve it: Comparative studies across multiple graph types (e.g., social, biological, citation networks) with varying sizes and characteristics, evaluating the preservation of feature-label distributions using each centrality measure.

### Open Question 2
- Question: Can FALCON's graph reduction technique be extended to dynamic graphs where the structure and features evolve over time?
- Basis in paper: The paper focuses on static graph datasets and does not address the applicability of FALCON to dynamic graphs.
- Why unresolved: The paper does not explore the potential of adapting FALCON's methodology to handle temporal changes in graph structure and node features.
- What evidence would resolve it: Experimental results demonstrating FALCON's performance on dynamic graph datasets, comparing its effectiveness against existing dynamic graph reduction techniques.

### Open Question 3
- Question: What is the impact of different clustering algorithms (e.g., DBSCAN, hierarchical clustering) on FALCON's feature-label distribution preservation compared to K-Means?
- Basis in paper: The paper mentions using K-Means clustering for grouping nodes based on features and labels but does not explore other clustering algorithms.
- Why unresolved: The paper does not investigate whether alternative clustering algorithms might offer advantages in terms of preserving feature-label distributions or computational efficiency.
- What evidence would resolve it: Comparative analysis of FALCON's performance using various clustering algorithms on multiple datasets, evaluating both the preservation of feature-label distributions and computational costs.

## Limitations

- The method's performance on extremely large graphs (millions of nodes) remains unverified, as current benchmarks are relatively moderate in size.
- The choice of centrality measure for node importance is shown to be dataset-dependent, but the paper doesn't provide a systematic way to select the optimal measure for new datasets.
- While 97% memory reduction is impressive, the computational overhead of FALCON's preprocessing (clustering, centrality calculation) is not thoroughly characterized.

## Confidence

- Graph reduction preserving prediction quality: **High**
- Memory efficiency gains: **High**
- Feature-label distribution preservation mechanism: **Medium**

## Next Checks

1. Test FALCON on a graph with severe class imbalance (e.g., 95% of nodes belong to one class) to verify if the feature-label constrained clustering still preserves the original distribution effectively.
2. Measure and report the preprocessing time for FALCON's clustering and centrality calculation on graphs of increasing size to understand scalability limitations.
3. Evaluate FALCON-QSIGN on a graph that changes over time (e.g., a dynamic social network) to assess how well the method adapts to structural changes without requiring complete recomputation.