---
ver: rpa2
title: 'Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection
  and Mitigation'
arxiv_id: '2305.15852'
source_url: https://arxiv.org/abs/2305.15852
tags:
- sentence
- https
- text
- chatgpt
- self-contradictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive investigation into self-contradictory
  hallucinations of large language models (LMs), focusing on evaluation, detection,
  and mitigation. The authors define self-contradiction as the generation of two logically
  inconsistent sentences within the same context by an LM, which guarantees non-factuality.
---

# Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation

## Quick Facts
- **arXiv ID**: 2305.15852
- **Source URL**: https://arxiv.org/abs/2305.15852
- **Reference count**: 40
- **Key outcome**: Proposes a three-step approach to detect and mitigate self-contradictory hallucinations in LMs, achieving 82.0-85.4% F1 score for detection and reducing self-contradictions across ChatGPT, GPT-4, and Vicuna-13B

## Executive Summary
This paper presents a comprehensive investigation into self-contradictory hallucinations in large language models (LMs), defining them as the generation of logically inconsistent sentences within the same context. The authors propose a three-step approach—trigger, detect, and mitigate—that works entirely through prompt engineering without requiring external grounded knowledge. The method is evaluated on three state-of-the-art instruction-tuned LMs (ChatGPT, GPT-4, and Vicuna-13B), demonstrating that self-contradictions occur frequently (14.3% for ChatGPT, 11.8% for GPT-4, and 18.5% for Vicuna-13B) and can be effectively detected and reduced while maintaining text quality.

## Method Summary
The approach consists of three steps: (1) Triggering self-contradictions by generating sentence pairs with controlled constraints using context tuples derived from relation triples; (2) Detecting contradictions through chain-of-thought prompting where an LM evaluates whether two sentences refer to the same subject with conflicting attributes; and (3) Mitigating contradictions through iterative local revision where only the problematic sentences are rewritten until no more contradictions are detected. The method works as a black-box tool applicable to any LM without requiring fine-tuning or external knowledge.

## Key Results
- Self-contradictions occur in 14.3% of ChatGPT, 11.8% of GPT-4, and 18.5% of Vicuna-13B generated text
- Detection achieves high accuracy with F1 scores ranging from 82.0% to 85.4% for ChatGPT and GPT-4
- Vicuna-13B shows significantly lower detection recall (22.5%) compared to proprietary models
- Mitigation successfully reduces self-contradictions while maintaining fluency (perplexity ~2.5) and informativeness
- The approach works as a push-button tool available at https://chatprotect.ai/

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-contradictions are guaranteed non-factual hallucinations because two logically inconsistent sentences cannot both be true.
- **Mechanism**: The approach leverages logical reasoning to detect contradictions without requiring external grounded knowledge. When two sentences contradict each other, at least one must be false, revealing the LM's non-factuality.
- **Core assumption**: LMs can perform logical reasoning about their own generated content when properly prompted.
- **Evidence anchors**: [abstract] states detecting self-contradiction "is guaranteed to reveal the LM's non-factuality, because the two sentences cannot be simultaneously correct"; [section 3] confirms "their contradiction is guaranteed to expose that the LM produces non-factual content"
- **Break condition**: If the LM cannot perform logical reasoning on its own outputs, the detection mechanism fails.

### Mechanism 2
- **Claim**: Iterative local revision effectively removes self-contradictions while preserving fluency and informativeness.
- **Mechanism**: The approach identifies contradictory sentence pairs, prompts the LM to revise only the problematic sentences, and iterates until convergence. This preserves non-contradictory information and maintains text quality.
- **Core assumption**: The LM can revise its own text to remove contradictions while maintaining coherence when given appropriate prompts.
- **Evidence anchors**: [abstract] states "The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness"; [section 6.3] shows "Each iteration progressively removes self-contradictions"
- **Break condition**: If the LM cannot revise its own text coherently, the iterative process fails to converge.

### Mechanism 3
- **Claim**: Appropriate constraint levels in prompts trigger self-contradictions effectively without over- or under-constraining the LM.
- **Mechanism**: The approach uses context tuples with relation triples, omitting the object element to create a cloze-style prompt that encourages the LM to retrieve internal knowledge that may contradict existing content.
- **Core assumption**: The LM's internal knowledge contains contradictory information that can be elicited through carefully crafted prompts.
- **Evidence anchors**: [section 5] states "Our prompt significantly outperforms all baselines because it enforces an appropriate level of constraint"; [section 6.3] attributes success to "the appropriate level of constraint our prompt enforces"
- **Break condition**: If the LM's internal knowledge is too consistent or lacks contradictory information, self-contradictions cannot be triggered.

## Foundational Learning

- **Concept**: Logical consistency and contradiction
  - Why needed here: The entire approach relies on detecting when two statements cannot both be true, which is fundamental to identifying hallucinations
  - Quick check question: Can you explain why two contradictory statements guarantee that at least one is false?

- **Concept**: Prompt engineering and zero-shot learning
  - Why needed here: The approach works entirely through prompting without fine-tuning, requiring understanding of how to craft effective prompts
  - Quick check question: How does chain-of-thought prompting differ from direct prompting in terms of detection accuracy?

- **Concept**: Iterative refinement algorithms
  - Why needed here: The mitigation process uses iterative local edits, requiring understanding of convergence and quality preservation
  - Quick check question: Why does the approach iterate multiple times rather than revising all sentences in one step?

## Architecture Onboarding

- **Component map**: Topic → Text generation → Context extraction → Alternative sentence generation → Contradiction detection → Local revision → Iteration → Final output

- **Critical path**: The pipeline flows from generating open-domain text to extracting contexts, creating alternative sentences, detecting contradictions, revising problematic sentences, and iterating until convergence.

- **Design tradeoffs**:
  - Constraint level vs. trigger rate: Too loose yields no contradictions, too tight prevents contradictions
  - Detection precision vs. recall: Higher precision reduces false positives but may miss some contradictions
  - Iteration count vs. computational cost: More iterations improve quality but increase API costs
  - Local vs. global revision: Local changes preserve fluency but may miss systemic contradictions

- **Failure signatures**:
  - Low contradiction trigger rate: Context extraction failing or prompts too restrictive
  - High false positive rate: Detector over-sensitive or prompt not specific enough
  - Poor fluency after mitigation: Revisions not coherent with context or too aggressive
  - No convergence: Contradictions too pervasive or detector missing key pairs

- **First 3 experiments**:
  1. Test context extraction on a simple topic (e.g., "William T. Freeman") to verify relation triple accuracy
  2. Run single iteration of detection and revision on one sentence pair to check prompt effectiveness
  3. Compare constraint levels (continue, rephrase, Q&A, our method) on the same topic to measure trigger rates

## Open Questions the Paper Calls Out

- **Question**: How can we effectively handle false negatives in the detection step of self-contradiction mitigation?
- **Basis in paper**: [inferred] The paper mentions that even though detection is accurate, false negatives still remain. Since mitigation targets predicted self-contradictions, these false negatives are not removed from the text.
- **Why unresolved**: The paper acknowledges the existence of false negatives but does not provide a concrete solution for addressing them. It only mentions that these false negatives remain in the text after the mitigation process.
- **What evidence would resolve it**: Developing and evaluating a method to identify and mitigate false negatives in the detection step would provide a solution to this open question.

- **Question**: How can we improve the performance of Vicuna-13B in detecting and mitigating self-contradictions?
- **Basis in paper**: [explicit] The paper shows that Vicuna-13B struggles with both detection and mitigation tasks, primarily due to its low recall for detection (22.5%).
- **Why unresolved**: The paper suggests that this can be addressed with future efforts on instruction-tuning open source LMs, but does not provide a concrete solution or evaluation of such efforts.
- **What evidence would resolve it**: Improving the instruction-tuning of Vicuna-13B or other open source LMs to enhance their performance in detecting and mitigating self-contradictions would resolve this open question.

- **Question**: How can we extend the approach to handle longer texts or documents?
- **Basis in paper**: [inferred] The paper focuses on open-domain text generation and sentence-level detection and mitigation of self-contradictions. It does not address the challenges of handling longer texts or documents.
- **Why unresolved**: The paper does not provide any discussion or evaluation of the approach's scalability to longer texts or documents, which is an important aspect of practical applications.
- **What evidence would resolve it**: Evaluating the approach on longer texts or documents and developing methods to handle the increased complexity and potential for self-contradictions would resolve this open question.

## Limitations

- The approach shows significantly lower performance on open-source models like Vicuna-13B (22.5% recall) compared to proprietary models, suggesting architecture-dependent limitations
- The method relies on prompt engineering without external grounded knowledge, potentially missing factual errors that don't manifest as explicit contradictions
- The iterative mitigation process increases computational costs through multiple API calls, which may be prohibitive for large-scale applications

## Confidence

- **High confidence**: Core claim that self-contradictions guarantee non-factuality and that the three-step approach can detect and mitigate these contradictions (detection shows strong precision 85.9-94.2% and acceptable F1 scores)
- **Medium confidence**: Generalization across different LM architectures (significant performance gap between proprietary and open-source models suggests limitations)
- **Low confidence**: Completeness of evaluation (test sets contain only 30-100 topics, potentially missing diverse contradiction patterns)

## Next Checks

1. **Cross-model consistency test**: Apply the same detection pipeline to Vicuna-13B using ChatGPT as the detector instead of its own outputs to isolate whether the low recall stems from Vicuna's generation quality or the detection method itself.

2. **Long-context evaluation**: Generate extended passages (500+ words) to assess whether self-contradictions become more frequent or detectable in longer contexts where internal knowledge conflicts may compound.

3. **Domain transfer validation**: Test the approach on specialized domains (medical, legal, technical) where factual consistency is critical to determine if the prompting strategy generalizes beyond general Wikipedia topics.