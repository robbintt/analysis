---
ver: rpa2
title: Towards Mitigating Hallucination in Large Language Models via Self-Reflection
arxiv_id: '2310.06271'
source_url: https://arxiv.org/abs/2310.06271
tags:
- knowledge
- answer
- question
- medical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates hallucinations in medical generative question
  answering (GQA) systems using large language models (LLMs) like Vicuna, Alpaca-LoRA,
  and ChatGPT. Through comprehensive analysis of five datasets, they categorize problematic
  answers into fact inconsistency, query inconsistency, and tangentiality.
---

# Towards Mitigating Hallucination in Large Language Models via Self-Reflection

## Quick Facts
- **arXiv ID:** 2310.06271
- **Source URL:** https://arxiv.org/abs/2310.06271
- **Reference count:** 40
- **Primary result:** Iterative self-reflection method significantly reduces hallucinations in medical GQA across multiple LLMs and datasets

## Executive Summary
This paper addresses the critical problem of hallucination in medical generative question answering systems using large language models. Through comprehensive analysis of five medical datasets, the authors categorize problematic answers into fact inconsistency, query inconsistency, and tangentiality. They propose an iterative self-reflection methodology that generates background knowledge, scores its factuality, and refines it until satisfactory, then uses this knowledge to generate answers with consistency scoring and refinement. Experiments demonstrate significant reductions in hallucination across different LLMs including Vicuna, Alpaca-LoRA, and ChatGPT, achieving higher factuality and consistency scores compared to direct generation baselines.

## Method Summary
The paper proposes an iterative self-reflection methodology for mitigating hallucination in medical generative question answering. The approach uses a three-loop system: (1) Factual Knowledge Acquiring Loop generates background knowledge and scores its factuality, refining iteratively until reaching a threshold; (2) Knowledge-Consistent Answering Loop generates answers conditioned on vetted knowledge and scores consistency, refining as needed; (3) Question-Entailment Answering Loop evaluates answer entailment with the original question, potentially returning to knowledge acquisition if unsatisfactory. The method leverages LLMs' multitasking ability across multiple turns, using generate-score-refine strategies with factuality scorers based on in-context instruction learning. Experiments run on five medical GQA datasets (PubMedQA, MedQuAD, MEDIQA2019, LiveMedQA2017, MASH-QA) using five LLMs (Vicuna, Alpaca-LoRA, ChatGPT, MedAlpaca, Robin-medical).

## Key Results
- The self-reflection method achieves significantly higher factuality and consistency scores compared to direct generation baselines across all tested LLMs and datasets
- Human evaluation shows substantial reduction in fact inconsistency, query inconsistency, and tangentiality categories
- The approach demonstrates effectiveness with multiple model architectures including Vicuna, Alpaca-LoRA, and ChatGPT
- Iterative refinement loops show consistent improvement in answer quality across the evaluation pipeline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative self-reflection improves factuality by forcing the model to detect and correct its own hallucinations
- **Mechanism:** The method generates background knowledge, scores its factuality, and if below a threshold, prompts the model to refine the knowledge. This loop continues until satisfactory factuality is reached, creating a feedback-driven improvement cycle
- **Core assumption:** LLMs possess sufficient introspective ability to recognize and correct their own hallucinated content when prompted appropriately
- **Evidence anchors:**
  - "Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers."
  - "Upon detection of discrepancies, the model is urged to self-correct, utilizing its inherent reflective capacities to refine the knowledge."
- **Break condition:** The mechanism breaks if the model cannot identify factual errors even after refinement prompts, or if the factuality scorer is unreliable

### Mechanism 2
- **Claim:** Consistency between generated answers and vetted background knowledge reduces hallucination
- **Mechanism:** After acquiring factual knowledge, the model generates answers conditioned on that knowledge. A consistency scorer evaluates alignment, and if insufficient, prompts refinement. This ensures answers are grounded in verified information
- **Core assumption:** Conditioning answer generation on high-quality background knowledge constrains the model to produce more accurate responses
- **Evidence anchors:**
  - "Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers."
  - "A consistency evaluation of the generated answer is conducted with CTRLEval... If the generated answer's consistency score lowers the threshold, the model is prompted to introspect, self-correct, and revise the answer."
- **Break condition:** The mechanism breaks if the background knowledge itself contains errors or if the consistency scorer fails to capture meaningful alignment

### Mechanism 3
- **Claim:** Multi-turn interactivity and multitasking ability of LLMs enable effective self-reflection and refinement
- **Mechanism:** The method leverages LLMs' capacity to handle multiple tasks (generation, scoring, refinement) across multiple turns, allowing iterative improvement of both knowledge and answers
- **Core assumption:** LLMs can effectively multitask and maintain context across multiple interaction turns to refine outputs
- **Evidence anchors:**
  - "Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers."
  - "Our self-reflective methodology initiates the generation of pertinent background knowledge for a given question, followed by a factuality evaluation... This cyclical process is repeated until a satisfactory level of factuality is achieved."
- **Break condition:** The mechanism breaks if the LLM loses context over multiple turns or cannot effectively handle the multiple tasks required

## Foundational Learning

- **Factuality scoring and evaluation**
  - **Why needed here:** To objectively measure whether generated knowledge is grounded in verifiable facts, enabling automated detection of hallucinations
  - **Quick check question:** How does the factuality scorer determine if knowledge is supported by empirical evidence?

- **Consistency evaluation between generated content and reference material**
  - **Why needed here:** To ensure that answers align with vetted background knowledge, reducing the risk of hallucination even when the knowledge itself is accurate
  - **Quick check question:** What metric is used to measure consistency between generated answers and background knowledge?

- **Iterative refinement loops in language model prompting**
  - **Why needed here:** To create a feedback mechanism where the model can progressively improve its outputs through multiple rounds of generation and correction
  - **Quick check question:** How many refinement loops are typically needed before satisfactory factuality is achieved?

## Architecture Onboarding

- **Component map:**
  - Input: Medical question from dataset
  - Factual Knowledge Acquiring Loop: Knowledge generation → Factuality scoring → Refinement (repeat until threshold met)
  - Knowledge-Consistent Answering Loop: Answer generation → Consistency scoring → Refinement (repeat until threshold met)
  - Question-Entailment Answering Loop: Entailment evaluation → Return to knowledge acquisition if unsatisfactory
  - Output: Refined answer with reduced hallucination

- **Critical path:** Question → Knowledge generation → Factuality scoring → Knowledge refinement (if needed) → Answer generation → Consistency scoring → Answer refinement (if needed) → Entailment evaluation → Final answer

- **Design tradeoffs:** The method trades computational efficiency (multiple generation and scoring rounds) for improved factuality and reduced hallucination. It also depends heavily on the quality of scoring mechanisms.

- **Failure signatures:**
  - Knowledge loop fails to converge (knowledge remains below factuality threshold)
  - Answer loop fails to converge (answer remains below consistency threshold)
  - Entailment loop triggers repeatedly without improvement
  - Scorers provide inconsistent or unreliable evaluations

- **First 3 experiments:**
  1. Run the full pipeline on a single question from PubMedQA and manually verify each loop iteration
  2. Compare factuality scores before and after knowledge refinement on 10 sample questions
  3. Test the effect of removing the refinement step to quantify its contribution to hallucination reduction

## Open Questions the Paper Calls Out

- **What are the underlying causes of hallucinations in large language models (LLMs) during medical generative question answering (GQA) tasks?**
  - **Basis in paper:** [inferred]
  - **Why unresolved:** The paper identifies that hallucinations in medical GQA are a significant issue but does not delve deeply into the underlying causes. While it mentions that uncommon professional concepts and potential social risks complicate the task, it does not explore the specific reasons why LLMs generate hallucinated content.
  - **What evidence would resolve it:** Detailed analysis of the LLM's internal processes during GQA tasks, including studies on the model's knowledge retrieval mechanisms, attention patterns, and contextual understanding, could provide insights into the causes of hallucinations.

- **How can the proposed self-reflection methodology be extended to other generation tasks beyond medical GQA?**
  - **Basis in paper:** [explicit]
  - **Why unresolved:** The paper focuses on the application of the self-reflection methodology to medical GQA tasks and mentions the potential for extension to other tasks. However, it does not provide a detailed exploration of how this methodology could be adapted and applied to different domains or generation tasks.
  - **What evidence would resolve it:** Empirical studies testing the self-reflection methodology on various generation tasks, such as creative writing, technical documentation, or conversational AI, would demonstrate its versatility and effectiveness across different domains.

- **What are the limitations of the current self-reflection methodology in completely eliminating hallucinations?**
  - **Basis in paper:** [explicit]
  - **Why unresolved:** The paper acknowledges that while the self-reflection methodology shows promise in mitigating hallucinations, it does not entirely eliminate the possibility. The paper mentions that the model might still generate ungrounded information, especially in complex or ambiguous scenarios, but does not explore the specific limitations and challenges of the methodology.
  - **What evidence would resolve it:** Further research and experimentation to identify the specific scenarios where the self-reflection methodology fails, along with the development of additional techniques or improvements to the methodology, could provide a clearer understanding of its limitations and potential solutions.

## Limitations

- The effectiveness critically depends on the reliability of factuality and consistency scorers, which are not fully specified in the paper
- The method requires multiple rounds of generation and evaluation, raising computational efficiency concerns for practical deployment
- The generalizability to non-medical domains and performance with models having limited introspective capabilities requires further validation

## Confidence

- **High confidence:** The empirical results showing hallucination reduction across multiple datasets and LLMs are well-supported by the reported metrics and human evaluation scores
- **Medium confidence:** The proposed iterative self-reflection mechanism is plausible given the known capabilities of LLMs for multi-turn interaction, but the exact implementation details and scorer reliability remain uncertain
- **Low confidence:** The generalizability of the method to non-medical domains and its performance with models that have limited introspective capabilities requires further validation

## Next Checks

1. Test the self-reflection method on non-medical domains (e.g., legal, scientific, or technical question answering) to assess generalizability beyond the medical GQA context
2. Compare the hallucination reduction achieved through self-reflection against alternative approaches like retrieval-augmented generation or chain-of-thought prompting to establish relative effectiveness
3. Conduct ablation studies to quantify the individual contributions of each refinement loop and scorer to the overall hallucination reduction, particularly examining scenarios where scorers provide inconsistent evaluations