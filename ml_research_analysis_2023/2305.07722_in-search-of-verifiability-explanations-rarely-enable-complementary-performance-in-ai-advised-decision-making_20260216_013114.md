---
ver: rpa2
title: 'In Search of Verifiability: Explanations Rarely Enable Complementary Performance
  in AI-Advised Decision Making'
arxiv_id: '2305.07722'
source_url: https://arxiv.org/abs/2305.07722
tags:
- explanations
- decision
- human
- performance
- making
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a theory to explain why AI explanations often
  fail to produce complementary performance in AI-advised decision making. The key
  idea is that explanations are only useful to the extent they allow a human decision
  maker to verify the correctness of an AI's recommendation.
---

# In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making

## Quick Facts
- arXiv ID: 2305.07722
- Source URL: https://arxiv.org/abs/2305.07722
- Authors: [Not specified in input]
- Reference count: 40
- Key outcome: AI explanations often fail to produce complementary performance because they rarely enable verification of AI recommendations

## Executive Summary
This paper proposes a theory explaining why AI explanations frequently fail to enable complementary performance in AI-advised decision making. The central argument is that explanations are only useful when they allow humans to verify the correctness of AI recommendations. Drawing parallels to NP-complete problem verification, the authors argue that most AI explanations do not support such verification, and most real-world tasks fundamentally lack verifiable answers. The paper distinguishes between outcome-graded and strategy-graded appropriate reliance, advocating for the latter as a more appropriate evaluation metric. It contextualizes this theory within broader research on demonstrability and social combination processes in collective decision making.

## Method Summary
The paper synthesizes existing literature on human-AI decision making and AI explanations to develop a theoretical framework. Rather than presenting new empirical results, it analyzes patterns across studies to argue that verification is the key mechanism through which explanations can enable complementary performance. The authors draw analogies to computational complexity theory and examine various types of explanations across different task domains to support their claims about the limitations of current explanation approaches.

## Key Results
- Explanations only enable complementary performance when they allow humans to verify AI recommendation correctness
- Most AI explanations (like feature importance scores) do not support verification of correctness
- The verifiability of explanations matters more than their faithfulness to AI reasoning for single isolated decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanations enable complementary performance only when they allow humans to verify the correctness of AI recommendations
- Mechanism: Verification reduces the human's verification cost from solving the entire problem to checking the solution, similar to how NP-complete problems work
- Core assumption: The human has sufficient task-specific knowledge to understand what constitutes a correct solution
- Break condition: If the task lacks a verifiable answer or the human lacks necessary task-specific knowledge, this mechanism fails

### Mechanism 2
- Claim: AI explanations that reveal model errors can enable complementary performance by allowing humans to reject incorrect recommendations
- Mechanism: When explanations expose egregious model errors, humans can identify when to disregard AI advice, preventing over-reliance
- Core assumption: Explanations can clearly reveal fatal flaws in the AI's decision-making process
- Break condition: If AI errors are systematic rather than random, or if humans cannot recognize the revealed errors as problematic

### Mechanism 3
- Claim: Explanations must lower the cognitive cost of verification enough to justify the effort of using them
- Mechanism: Even partial verification paths (e.g., 90% of a maze solution) can reduce verification costs sufficiently to enable complementary performance
- Core assumption: There exists a threshold of verification completeness where the benefit outweighs the cost
- Break condition: If the verification cost remains too high relative to the benefit, or if humans don't engage cognitively with partial explanations

## Foundational Learning

- Concept: Verification vs. Faithfulness in explanations
  - Why needed here: The paper argues that verifiability matters more than faithfulness for single isolated decisions
  - Quick check question: If an explanation perfectly represents the AI's reasoning process but doesn't help verify correctness, is it useful for complementary performance?

- Concept: Outcome-graded vs. Strategy-graded reliance
  - Why needed here: The paper distinguishes between these two notions of appropriate reliance and advocates for strategy-graded
  - Quick check question: Which definition of appropriate reliance would say it was "appropriate" to trust a doctor who later made a rare mistake?

- Concept: NP-complete problem verification analogy
  - Why needed here: The paper draws parallels between explanation verification and NP-complete problem verification
  - Quick check question: Why can humans quickly verify NP-complete problem solutions even though finding them is computationally hard?

## Architecture Onboarding

- Component map:
  Human decision maker (with task-specific knowledge) -> AI recommendation generator -> Explanation generator (must support verification) -> Verification mechanism (checking correctness) -> Performance measurement system (accuracy, error rate)

- Critical path:
  1. AI generates recommendation
  2. AI generates explanation that supports verification
  3. Human uses explanation to verify recommendation
  4. Human decides whether to accept/reject recommendation
  5. System measures complementary performance

- Design tradeoffs:
  - Explanation completeness vs. cognitive load
  - Verifiability vs. faithfulness to AI reasoning
  - Static vs. interactive explanations
  - Single-turn vs. multi-turn explanation dialogs

- Failure signatures:
  - Humans over-relying on AI despite errors (over-reliance)
  - Humans under-relying on correct AI recommendations
  - Humans not engaging cognitively with explanations
  - Explanations revealing no useful information for verification

- First 3 experiments:
  1. Implement maze-solving task with varying explanation completeness (0%, 50%, 90%, 100% path shown) and measure complementary performance
  2. Create sentiment analysis task where explanation highlights are sometimes misleading vs. factoid QA task where highlights enable verification
  3. Build interactive explanation system where users can request different levels of detail and measure verification accuracy and cognitive load

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do AI explanations enable verification of recommendations, and how can we identify these conditions in advance?
- Basis in paper: [explicit] The paper argues that explanations are only useful to the extent they allow verification of AI recommendations, and provides examples like maze solving and factoid QA where verification is possible
- Why unresolved: The paper acknowledges that most real-world tasks do not allow easy verification regardless of explanation method, but does not provide a comprehensive framework for predicting when verification is possible
- What evidence would resolve it: A systematic study categorizing tasks based on their verifiability potential, identifying common characteristics of verifiable vs. non-verifiable tasks, and testing different explanation methods across these categories

### Open Question 2
- Question: How does the presence of aleatoric uncertainty in decision-making tasks affect the utility of AI explanations for verification?
- Basis in paper: [explicit] The paper mentions that real-world decision making tasks involving inferences over human action can exhibit aleatoric uncertainty, which limits the utility of verifying recommendations
- Why unresolved: The paper only briefly touches on this concept and does not explore how different levels of uncertainty might impact the effectiveness of verification-based explanations
- What evidence would resolve it: Empirical studies measuring the impact of varying levels of aleatoric uncertainty on the effectiveness of verification-based explanations across different types of decision-making tasks

### Open Question 3
- Question: How do interactive and on-demand explanations compare to static, single-turn explanations in terms of enabling verification and complementary performance?
- Basis in paper: [inferred] The paper mentions that current interaction paradigms are typically unilateral and static, and suggests that interactive explanations might be more beneficial for verification
- Why unresolved: While the paper acknowledges the potential benefits of interactive explanations, it does not provide empirical evidence comparing their effectiveness to static explanations
- What evidence would resolve it: Comparative studies evaluating the impact of interactive vs. static explanations on verification rates and complementary performance across various decision-making tasks and user populations

## Limitations
- The framework assumes humans have sufficient task-specific knowledge to verify AI recommendations
- Many real-world AI tasks lack clear ground truth for verification
- The cognitive cost of verification may remain prohibitive even with partial explanations

## Confidence
- High confidence in the core hypothesis that verification enables complementary performance
- Medium confidence in the NP-complete analogy as the primary mechanism
- Medium confidence in the distinction between outcome-graded and strategy-graded reliance
- Low confidence in the practical feasibility of implementing verifiable explanations for most real-world AI systems

## Next Checks
1. Design an experiment testing whether explanations that enable verification (e.g., path highlights in mazes) produce complementary performance while those that don't (e.g., feature importance in sentiment analysis) do not
2. Conduct a systematic review of existing AI explanation studies to classify them by verifiability potential and correlate with performance outcomes
3. Develop a benchmark task where explanation verifiability can be manipulated independently of other explanation properties to isolate its effect on complementary performance