---
ver: rpa2
title: Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question
  Answering Performance of the GPT LLM Family
arxiv_id: '2303.07992'
source_url: https://arxiv.org/abs/2303.07992
tags:
- chatgpt
- language
- question
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a large-scale experimental evaluation of ChatGPT's
  performance in answering complex questions using its own knowledge base, compared
  to similar models and current state-of-the-art models. The authors design a feature-driven
  multi-label annotation method to categorize complex questions and use the CheckList
  testing specification to evaluate the functionality, robustness, and controllability
  of ChatGPT in reasoning for answering complex questions.
---

# Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family

## Quick Facts
- arXiv ID: 2303.07992
- Source URL: https://arxiv.org/abs/2303.07992
- Authors: Multiple researchers (names not provided)
- Reference count: 40
- Key outcome: ChatGPT significantly outperforms other LLMs on 7 of 8 test datasets but remains inferior to state-of-the-art traditional KBQA models

## Executive Summary
This paper presents a large-scale experimental evaluation of ChatGPT's performance in answering complex questions using its own knowledge base, compared to similar models and current state-of-the-art models. The authors design a feature-driven multi-label annotation method to categorize complex questions and use the CheckList testing specification to evaluate functionality, robustness, and controllability of ChatGPT in reasoning for answering complex questions. The evaluation is conducted on 8 real-world KB-based complex question answering datasets, including 6 English and 2 multilingual datasets, with a total of approximately 190,000 test cases. The results show that while ChatGPT significantly outperforms other participating LLMs on 7 of the test datasets, its performance is still significantly inferior to the state-of-the-art models on other datasets. The paper provides valuable insights and references for the development and downstream research of large-scale language models represented by ChatGPT.

## Method Summary
The study employs a black-box testing framework to evaluate ChatGPT's question answering performance on 8 KB-based complex question answering datasets (6 English, 2 multilingual) totaling approximately 190,000 test cases. The authors use a feature-driven multi-label annotation method to categorize questions by answer types, reasoning types, and language features. Evaluation follows CheckList specifications with three test types: MFT (Minimal Functionality Test), INV (Invariance Test), and DIR (Directional Expectation Test). An answer matching strategy uses phrase extraction and alias collection to handle the gap between ChatGPT's generative answers and dataset answer phrases. Comparative models include GPT-3, GPT-3.5 (v2, v3), and FLAN-T5.

## Key Results
- ChatGPT significantly outperforms other LLMs on 7 of 8 test datasets
- ChatGPT's performance is still significantly inferior to state-of-the-art traditional KBQA models on other datasets
- ChatGPT shows better performance on multilingual datasets, especially in low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ChatGPT's performance on complex question answering is superior to other LLMs but still inferior to state-of-the-art traditional KBQA models due to its reliance on unsupervised learning from Wikipedia knowledge.
- **Mechanism**: ChatGPT leverages its extensive Wikipedia-based knowledge to answer questions without requiring explicit knowledge base queries, allowing it to perform well on many question types. However, it lacks the structured reasoning capabilities of traditional KBQA models that use SPARQL queries and knowledge base lookups.
- **Core assumption**: The knowledge contained in ChatGPT's training data is sufficient and accurate for answering complex questions that would typically require structured knowledge base reasoning.
- **Evidence anchors**:
  - [abstract] "ChatGPT significantly outperforms other participating LLMs on 7 of the test datasets, but its performance is still significantly inferior to the state-of-the-art models on other datasets."
  - [section 3.2] "ChatGPT has difficulties in generating SPARQL queries with unified entity and relation IDs, making SPARQL matching difficult to automate."
  - [corpus] "Found 25 related papers... average citations=0.0" - indicates limited direct comparative evidence in the corpus for this specific claim.
- **Break condition**: If the questions require reasoning patterns or knowledge not well-represented in Wikipedia, or if the answer requires precise structured data not captured in ChatGPT's training, its performance will degrade significantly.

### Mechanism 2
- **Claim**: The feature-driven multi-label annotation method effectively categorizes complex questions by answer types, reasoning types, and language features, enabling targeted evaluation of ChatGPT's capabilities.
- **Mechanism**: By labeling questions with answer types (e.g., DATE, PER, LOC), reasoning types (e.g., Multi-hop, Counting, Comparison), and language features, the evaluation framework can systematically test ChatGPT's performance across different question characteristics and identify specific strengths and weaknesses.
- **Core assumption**: The multi-label annotation accurately captures the key features of complex questions that determine the difficulty and type of reasoning required.
- **Evidence anchors**:
  - [section 3.1] "We design three categories of labels, including 'Answer type', 'Reasoning type' and 'Language type' to describe the characteristics contained within a complex question."
  - [section 4.3] "We select test cases that only contained single inference labels or multi-labels of the same type... The MFT test results indicate that when a question involves only one type of reasoning operation, ChatGPT's performance decreases."
  - [corpus] Weak evidence - the corpus contains related papers but lacks direct evidence about the effectiveness of this specific annotation method.
- **Break condition**: If the multi-label annotation fails to capture important features that distinguish question difficulty or if the labels are too coarse to differentiate between different types of complex reasoning.

### Mechanism 3
- **Claim**: The answer matching strategy using phrase extraction and alias collection improves the evaluation of ChatGPT's question answering performance by handling the gap between ChatGPT's generative answers and dataset answer phrases.
- **Mechanism**: Instead of exact match, the evaluation extracts noun phrases from ChatGPT's output and compares them to a multilingual set of aliases for the correct answers, allowing for more flexible and accurate matching of semantic equivalence.
- **Core assumption**: The extracted noun phrases and answer aliases sufficiently capture the semantic content of the answers, and the similarity threshold appropriately handles paraphrases and near-misses.
- **Evidence anchors**:
  - [section 3.2] "We adopted a naive approach of enhancing answer matching generalization by expanding the matching scope... By utilizing the subtree labels provided by the constituent tree, all noun phrases within the textual answer can be extracted."
  - [section 4.3] "The experimental results on the multilingual test set... show that ChatGPT outperforms the comparison models in all language tests, especially in low-resource languages."
  - [corpus] No direct evidence in the corpus about this specific answer matching strategy.
- **Break condition**: If the phrase extraction misses key information, if the alias collection is incomplete or inaccurate, or if the similarity threshold is set too high or too low, the evaluation accuracy will suffer.

## Foundational Learning

- **Concept**: Knowledge Base Question Answering (KBQA)
  - Why needed here: Understanding KBQA is essential to grasp the context of comparing ChatGPT to traditional KBQA models and to understand the evaluation framework.
  - Quick check question: What is the difference between KBQA and open-domain question answering, and why is KBQA considered a more challenging task?

- **Concept**: Semantic Parsing
  - Why needed here: Semantic parsing is the core technique used by traditional KBQA models to translate natural language questions into executable queries (e.g., SPARQL), and understanding it helps explain why ChatGPT's unsupervised approach has limitations.
  - Quick check question: How does semantic parsing work in KBQA, and what are the key challenges in translating natural language questions into structured queries?

- **Concept**: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT prompting is used in the evaluation to guide ChatGPT through reasoning steps, and understanding it is crucial for interpreting the DIR results and the effectiveness of prompting strategies.
  - Quick check question: What is Chain-of-Thought prompting, and how does it help large language models perform complex reasoning tasks?

## Architecture Onboarding

- **Component map**: Data Collection (8 datasets) -> Multi-label Annotation (answer types, reasoning types, language features) -> Evaluation Framework (MFT, INV, DIR) -> Answer Matching (phrase extraction + alias collection) -> Comparative Models (ChatGPT, GPT-3.5, GPT-3, FLAN-T5)

- **Critical path**: Data collection → Multi-label annotation → Question answering with ChatGPT and comparison models → Answer matching and evaluation → Analysis of results by feature categories

- **Design tradeoffs**:
  - Using unsupervised ChatGPT vs. fine-tuned traditional KBQA models: trade-off between ease of use and performance on structured reasoning tasks
  - Answer matching strategy: trade-off between exact match precision and semantic match recall
  - Multi-label annotation: trade-off between granularity of features and annotation complexity

- **Failure signatures**:
  - Poor performance on questions requiring precise structured data or reasoning patterns not well-represented in Wikipedia
  - Inconsistent outputs across similar questions (low stability in INV tests)
  - Failure to follow answer type or reasoning operation prompts (poor controllability in DIR tests)
  - Low accuracy on numerical, temporal, or causal questions

- **First 3 experiments**:
  1. **MFT experiment**: Test ChatGPT's performance on single-reasoning-type questions (e.g., only counting, only multi-hop) to identify weaknesses in basic reasoning capabilities
  2. **INV experiment**: Test ChatGPT's stability by introducing spelling errors and paraphrases to questions and checking for consistent outputs
  3. **DIR experiment**: Test ChatGPT's controllability by adding answer type prompts and modifying reasoning-related phrases in questions, then checking if outputs change as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ChatGPT's performance on complex question answering be improved by fine-tuning on specific reasoning types or answer types where it currently underperforms?
- Basis in paper: [inferred] The paper shows ChatGPT's weaknesses in handling numerical, causal, and temporal answers, as well as in multi-hop reasoning tasks. It also notes that ChatGPT performs better on combined reasoning types than single ones.
- Why unresolved: The study only evaluates ChatGPT in its pre-trained form without any fine-tuning on specific reasoning or answer types. The potential for improvement through targeted fine-tuning remains unexplored.
- What evidence would resolve it: Conducting experiments where ChatGPT is fine-tuned on datasets focusing on numerical, causal, and temporal answers, as well as multi-hop reasoning, and comparing its performance to the current results.

### Open Question 2
- Question: How does the stability of ChatGPT's responses change when dealing with different languages or language families, particularly for low-resource languages?
- Basis in paper: [explicit] The paper mentions that ChatGPT outperforms other models in low-resource languages but also notes a puzzlingly low score in Chinese tests, questioning whether this is due to insufficient resources or low resource quality.
- Why unresolved: The study provides initial observations on multilingual performance but doesn't delve into the specific reasons behind the stability issues or the factors affecting performance across different languages.
- What evidence would resolve it: Analyzing the stability of ChatGPT's responses across a broader range of languages and language families, including a detailed examination of the quality and quantity of training data for each language.

### Open Question 3
- Question: Can the use of Chain-of-Thought (CoT) prompting be optimized to further improve ChatGPT's performance on complex reasoning tasks?
- Basis in paper: [explicit] The paper demonstrates that CoT prompting improves ChatGPT's performance, especially on counting and numerical questions, but it uses a general, naive CoT approach.
- Why unresolved: The study only explores the effects of a basic CoT prompting strategy without investigating more sophisticated or tailored approaches that might yield better results.
- What evidence would resolve it: Experimenting with various CoT prompting strategies, including more structured or domain-specific approaches, and measuring their impact on ChatGPT's performance across different types of complex reasoning tasks.

## Limitations
- ChatGPT's unsupervised knowledge base limits its ability to perform precise structured reasoning required by traditional KBQA models
- The answer matching strategy may introduce evaluation noise through its phrase extraction and alias collection approach
- Limited comparative evidence in the corpus for the effectiveness of the specific multi-label annotation method used

## Confidence
- **High confidence**: ChatGPT's superior performance compared to other LLMs on 7 of 8 test datasets
- **Medium confidence**: The claim about Wikipedia-based knowledge limitations
- **Medium confidence**: The effectiveness of the multi-label annotation method

## Next Checks
1. Conduct error analysis on ChatGPT's failures to identify whether they stem from knowledge gaps, reasoning limitations, or answer format mismatches
2. Test ChatGPT's performance on additional complex reasoning types (e.g., temporal, causal) not well-represented in the current datasets
3. Evaluate the stability and generalizability of the answer matching strategy across different language pairs and answer formats