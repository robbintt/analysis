---
ver: rpa2
title: 'How to Bridge the Gap between Modalities: Survey on Multimodal Large Language
  Model'
arxiv_id: '2311.07594'
source_url: https://arxiv.org/abs/2311.07594
tags:
- multimodal
- arxiv
- language
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey explores Multimodal Large Language Models (MLLMs) that
  integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data including
  text, images, audio, and more. MLLMs demonstrate capabilities such as generating
  image captions and answering image-based questions, bridging the gap towards real-world
  human-computer interactions and hinting at a potential pathway to artificial general
  intelligence.
---

# How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model

## Quick Facts
- **arXiv ID:** 2311.07594
- **Source URL:** https://arxiv.org/abs/2311.07594
- **Reference count:** 40
- **Primary result:** Survey of four modality alignment approaches for MLLMs: converters, perceivers, tool learning, and data-driven methods to bridge semantic gaps between modalities.

## Executive Summary
This survey explores Multimodal Large Language Models (MLLMs) that integrate Large Language Models like GPT-4 to handle multimodal data including text, images, audio, and more. MLLMs demonstrate capabilities such as generating image captions and answering image-based questions, bridging the gap towards real-world human-computer interactions and hinting at a potential pathway to artificial general intelligence. However, MLLMs still face challenges in addressing the semantic gap in multimodal data, which may lead to erroneous outputs, posing potential risks to society. The study surveys existing modality alignment methods for MLLMs, categorizing them into four groups: (1) Multimodal Converters that transform data into a format that LLMs can understand; (2) Multimodal Perceivers that improve how LLMs perceive different types of data; (3) Tool Learning that leverages external tools to convert data into a common format, usually text; and (4) Data-Driven Methods that teach LLMs to understand specific data types within datasets. The paper highlights the importance of modality alignment methods in enabling MLLMs to address environmental issues and enhance accessibility.

## Method Summary
The paper surveys modality alignment methods for MLLMs, categorizing them into four approaches: Multimodal Converters, Multimodal Perceivers, Tool Learning, and Data-Driven Methods. The study analyzes the mechanisms, advantages, and limitations of each approach, providing a comprehensive overview of how these methods enable LLMs to process multimodal data. The survey draws from existing literature and datasets to illustrate the effectiveness and challenges of each method, emphasizing the importance of addressing the semantic gap between modalities.

## Key Results
- MLLMs can generate image captions and answer image-based questions by bridging the semantic gap between modalities.
- Four main modality alignment approaches exist: converters, perceivers, tool learning, and data-driven methods.
- Current MLLMs face challenges with complex modalities like medical images and biological molecules due to limited domain-specific data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal converters align different modalities into a common representation space that LLMs can process.
- Mechanism: They transform raw multimodal inputs (e.g., images, audio) into text or embeddings that match the semantic space of the LLM, reducing the semantic gap.
- Core assumption: LLMs can effectively process transformed multimodal features if those features are semantically aligned with their training distribution.
- Evidence anchors:
  - [abstract]: "The converter facilitates the transformation of multimodal information into objects comprehensible or learnable by LLM"
  - [section]: "the most straightforward approach to tackling multi-modal tasks is to directly input multimodal features into the LLM, allowing it to learn and comprehend these multimodal features"
  - [corpus]: Weak - corpus contains only survey papers that describe converters but no direct experimental validation.
- Break condition: If the transformation introduces too much information loss or misalignment, the LLM will generate hallucinations or incorrect outputs.

### Mechanism 2
- Claim: Multimodal perceivers bridge the semantic gap by introducing specialized modules that map multimodal features into LLM-compatible embeddings.
- Mechanism: Perceivers like Q-former or VAE-based encoders learn to fuse multimodal signals into a shared embedding space, preserving fine-grained details.
- Core assumption: A dedicated perceiver can learn to encode multimodal features that align well with LLM embeddings without requiring massive parameter updates.
- Evidence anchors:
  - [abstract]: "These methods focus on designing multimodal perceivers intended to interface with LLM, primarily to enhance the perceptual abilities toward multimodal information"
  - [section]: "The perception model is a multi-modal perceiver that bridges the gap between text modality and other modalities by transforming multimodal features into multimodal tokens"
  - [corpus]: Moderate - corpus shows multiple perceiver-based approaches but limited comparative evidence of superiority over converters.
- Break condition: If the perceiver's capacity is too limited, it may fail to capture complex multimodal relationships, leading to degraded performance.

### Mechanism 3
- Claim: Tool learning leverages external models or APIs to convert multimodal data into text, outsourcing the alignment problem.
- Mechanism: LLMs act as coordinators or controllers, invoking tools (e.g., vision models, OCR, captioning) to transform multimodal inputs into unified text representations.
- Core assumption: External tools can reliably convert multimodal data into accurate text that preserves essential semantic content.
- Evidence anchors:
  - [abstract]: "Tool Learning that leverages external tools to convert data into a common format, usually text"
  - [section]: "LLMs are encouraged to harness tools for the conversion of different modalities into a unified modality, predominantly text"
  - [corpus]: Moderate - corpus lists several tool-based approaches but lacks empirical comparison of tool quality.
- Break condition: If the external tools are unreliable or introduce latency, the overall system performance degrades.

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: Understanding how different modalities can be encoded into shared spaces is essential for designing converters and perceivers.
  - Quick check question: How do you measure alignment quality between image embeddings and text embeddings?

- Concept: Cross-modal attention mechanisms
  - Why needed here: These mechanisms enable the fusion of multimodal features and are core to perceiver designs.
  - Quick check question: What is the difference between self-attention and cross-attention in multimodal contexts?

- Concept: In-context learning and prompt engineering
  - Why needed here: Many MLLM approaches rely on LLMs' ability to understand and generate outputs from carefully crafted prompts.
  - Quick check question: How does prompt structure affect the quality of multimodal reasoning in LLMs?

## Architecture Onboarding

- Component map: Raw multimodal inputs -> Encoder(s) -> Perceiver/Converter -> LLM backbone -> Output
- Critical path:
  1. Encode raw multimodal inputs
  2. Align or convert to LLM-compatible format
  3. Generate prompt for LLM
  4. LLM produces final output
- Design tradeoffs:
  - End-to-end fine-tuning vs. frozen LLM with lightweight adapters
  - Single unified encoder vs. modality-specific encoders
  - Heavy perceiver modules vs. simple converters
- Failure signatures:
  - Hallucinations in output
  - Slow inference due to external tool calls
  - Poor generalization to unseen modalities
- First 3 experiments:
  1. Compare a frozen LLM with a converter vs. a tuned LLM on a standard VQA dataset.
  2. Evaluate a perceiver-based model vs. a tool-based model on multimodal reasoning tasks.
  3. Test robustness by introducing noisy or out-of-distribution multimodal inputs and measuring degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective method for bridging the semantic gap between multimodal data and LLM comprehension, considering the limitations of current multimodal converters, perceivers, and tool-based approaches?
- Basis in paper: explicit - "Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with minimal performance improvement."
- Why unresolved: Current methods (multimodal converters, perceivers, tools) each have limitations in handling the semantic gap between modalities, and no clear consensus exists on the optimal approach.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of different alignment methods across diverse multimodal tasks, focusing on performance, computational efficiency, and semantic alignment quality.

### Open Question 2
- Question: How can MLLMs be developed to handle more complex and diverse modalities beyond images and text, such as medical images or point cloud data, without compromising their general performance?
- Basis in paper: explicit - "However, when faced with more complex multi-modal information, such as medical images or the structure of biological molecules, the data related to these modalities is few."
- Why unresolved: Current MLLMs are primarily trained on generic datasets and lack the ability to effectively process domain-specific or complex modalities.
- What evidence would resolve it: Development of high-quality, domain-specific datasets and evaluation of MLLMs trained on these datasets for improved performance in handling complex modalities.

### Open Question 3
- Question: What are the most effective strategies for addressing the hallucination, privacy, ethics, and bias issues inherent in MLLMs, considering the challenges posed by the vast and diverse training data?
- Basis in paper: explicit - "Security has been an ongoing and inevitable concern... current security issues primarily encompass hallucinations, privacy, ethics, and biases."
- Why unresolved: Existing approaches to mitigate these issues are often costly or insufficient, and there is a need for more efficient and comprehensive solutions.
- What evidence would resolve it: Implementation and evaluation of novel techniques, such as advanced data filtering, model-level interventions, and ethical guidelines, to effectively address these concerns while maintaining model performance.

## Limitations

- The survey is theoretical and lacks direct experimental validation of the proposed mechanisms.
- Insufficient quantitative comparison between converter and perceiver performance.
- Limited analysis of tool reliability in real-world conditions and absence of systematic evaluation of failure modes across different alignment strategies.

## Confidence

- **High Confidence**: The categorization of modality alignment methods into four distinct approaches (converters, perceivers, tool learning, and data-driven methods) is well-supported by the corpus and aligns with current literature. The identification of semantic gap challenges in MLLMs is also strongly evidenced.
- **Medium Confidence**: The proposed mechanisms for how each alignment approach works are logically sound but lack direct experimental validation. While the theoretical foundations are reasonable, the practical superiority of any particular approach remains unproven.
- **Low Confidence**: Claims about which alignment method is "best" for specific use cases or datasets are not substantiated by comparative experiments in the survey itself.

## Next Checks

1. **Empirical Comparison Study**: Design and execute controlled experiments comparing at least two alignment approaches (e.g., converter vs. perceiver) on standardized multimodal benchmarks, measuring both accuracy and efficiency metrics.

2. **Robustness Analysis**: Systematically test MLLM outputs under varying conditions of semantic gap (e.g., noisy inputs, domain shift) to quantify hallucination rates and identify failure thresholds for different alignment methods.

3. **Tool Reliability Assessment**: Evaluate the consistency and accuracy of external tools used in tool-based alignment approaches across diverse multimodal inputs, measuring both quality of conversion and latency impacts.