---
ver: rpa2
title: One-Shot Labeling for Automatic Relevance Estimation
arxiv_id: '2302.11266'
source_url: https://arxiv.org/abs/2302.11266
tags:
- relevance
- documents
- systems
- evaluation
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The problem of missing relevance judgments in test collections
  is addressed by using one-shot models to infer labels for unjudged documents. The
  approach uses a single known relevant document per query to estimate the relevance
  of other documents through techniques including MaxRep, DuoT5, and Prompt-Duo.
---

# One-Shot Labeling for Automatic Relevance Estimation

## Quick Facts
- arXiv ID: 2302.11266
- Source URL: https://arxiv.org/abs/2302.11266
- Reference count: 40
- One-line primary result: One-shot labeling achieves strong system ranking correlations despite low individual label accuracy, reducing false positive t-test rates from 15-30% down to under 5%

## Executive Summary
This paper addresses the problem of missing relevance judgments in test collections by using one-shot models to infer labels for unjudged documents. The approach uses a single known relevant document per query to estimate the relevance of other documents through techniques including MaxRep, DuoT5, and Prompt-Duo. The results show that while the one-shot models are not yet capable of reliably identifying all relevant documents (F1 scores up to 0.63), their use in evaluation measures yields strong correlation with full human assessments (consistently above 0.85, often exceeding 0.98). Additionally, using these approaches reduces the false positive rate in statistical tests comparing systems from 15-30% down to under 5%. This demonstrates the potential of one-shot labeling to replace deep manual assessments in precision-oriented evaluation settings.

## Method Summary
The method uses one-shot labeling techniques to fill "holes" in test collections where relevance judgments are missing. Given a single known relevant document per query, the approach applies three different models (MaxRep using lexical/semantic similarity, DuoT5 using supervised relative relevance, and Prompt-Duo using prompting) to estimate relevance scores for unjudged documents. These predicted scores are then used to compute evaluation measures like SDCG@10, P@10, and RBP, replacing the 0-gain typically assigned to non-relevant documents. The approach is tested on TREC DL 2019-2021 datasets, comparing system rankings and statistical test reliability against full human assessments.

## Key Results
- One-shot labeling achieves system ranking correlations consistently above 0.85 (often exceeding 0.98) with full human assessments
- False positive t-test rates drop from 15-30% with holey assessments to under 5% when using one-shot labeling
- F1 scores for individual label prediction reach up to 0.63, showing models are not yet reliable for identifying all relevant documents
- Precision-oriented measures benefit most from one-shot labeling, while recall-sensitive measures show more modest improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-shot labeling achieves strong system ranking correlations despite low individual label accuracy
- Mechanism: Even when individual relevance predictions are noisy, the relative ordering of systems remains stable because errors tend to be consistent across systems
- Core assumption: System performance differences dominate labeling noise in rank ordering
- Evidence anchors:
  - [abstract] "the labels they produce yield a far more reliable ranking of systems than the single labels do alone... can consistently reach system ranking correlations of over 0.86"
  - [section] "although the predictions of these One-Shot Labelers (1SL) frequently disagree with human assessments, the labels they produce yield a far more reliable ranking of systems than the single labels do alone"
  - [corpus] Weak evidence - corpus neighbors discuss similar problems but don't directly support this mechanism
- Break condition: When system performance differences are small relative to labeling noise, or when systems have different error patterns

### Mechanism 2
- Claim: Filling holes with predicted relevance scores reduces false positive t-test results
- Mechanism: By replacing missing judgments with estimated relevance scores, the evaluation becomes more complete, reducing the impact of missing data on statistical significance tests
- Core assumption: Missing data creates false positives in significance testing, and filling holes addresses this
- Evidence anchors:
  - [abstract] "substantially increases the reliability of t-tests due to filling holes in relevance assessments, giving researchers more confidence in results they find to be significant"
  - [section] "yields far fewer false positive statistical tests when comparing system performance (from 15-30% of tests with holey assessments, down to under 5%)"
  - [corpus] Weak evidence - corpus neighbors discuss similar problems but don't directly support this mechanism
- Break condition: When the automatic labeling introduces systematic bias that correlates with system performance differences

### Mechanism 3
- Claim: Prompt-Duo and DuoT5 models provide better relative relevance estimates than document similarity approaches
- Mechanism: Supervised models trained on relative relevance judgments can better capture query-document relationships than simple similarity metrics
- Core assumption: Models trained on relative relevance judgments capture more nuanced relevance signals than document similarity alone
- Evidence anchors:
  - [section] "Prompt-Duo and DuoT5 are moderately effective at the tasks, achieving an F1-score of at most 0.58 and 0.61 (AP: 0.63 and 0.59) respectively"
  - [section] "MaxRep methods achieve notably worse F1 and Average Precision (AP) than other methods. This is likely due to MaxRep not considering query text"
  - [corpus] Weak evidence - corpus neighbors discuss similar problems but don't directly support this mechanism
- Break condition: When query-document relationships are too complex for the model architectures to capture, or when training data is insufficient

## Foundational Learning

- Concept: Cranfield evaluation paradigm and pooling methodology
  - Why needed here: Understanding the baseline evaluation setup and the problem of incomplete judgments is crucial for grasping why one-shot labeling is valuable
  - Quick check question: What is the main limitation of traditional pooling methods in Cranfield-style evaluations?

- Concept: C/W/L framework and recall-agnostic measures
  - Why needed here: The paper focuses on measures that don't require full recall estimation, which is important for understanding why one-shot labeling works even with imperfect recall
  - Quick check question: Why does the paper focus on recall-agnostic measures rather than measures like MAP or nDCG?

- Concept: Statistical significance testing in IR evaluation
  - Why needed here: Understanding t-tests and false positive rates is crucial for interpreting the results about statistical reliability
  - Quick check question: How does the false positive rate of t-tests change when using one-shot labeling versus traditional holey assessments?

## Architecture Onboarding

- Component map:
  - Input: Single known relevant document per query
  - Models: MaxRep (BM25/TCT), DuoT5, Prompt-Duo
  - Output: Estimated relevance scores for unjudged documents
  - Evaluation: System ranking correlation, t-test reliability

- Critical path:
  1. Extract single known relevant document from BM25 baseline
  2. Apply 1SL models to estimate relevance of other documents
  3. Generate gains for evaluation measures
  4. Compute system rankings and statistical tests

- Design tradeoffs:
  - Accuracy vs. cost: One-shot labeling is much cheaper than full manual assessment but less accurate
  - Recall vs. precision: Models are better at identifying some relevant documents than others
  - Model choice: Different 1SL approaches have different strengths (e.g., MaxRep vs. supervised models)

- Failure signatures:
  - Low correlation with full qrels indicates model failure
  - High false positive t-test rates suggest systematic bias
  - Poor precision-recall curves indicate unreliable labeling

- First 3 experiments:
  1. Run each 1SL model on a small subset of queries and compare precision-recall curves
  2. Test system ranking correlation on a held-out dataset
  3. Evaluate t-test false positive rates with different significance thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the one-shot labeling approach be extended to document-level retrieval, where documents are significantly longer than passages?
- Basis in paper: [explicit] The paper notes that it only explores passage retrieval and suggests that providing relative relevance estimates over entire documents is "likely a far more challenging task, especially with context window limitations of neural language models."
- Why unresolved: The paper does not explore document-level retrieval, and the context window limitations of current language models present a technical challenge.
- What evidence would resolve it: Experiments demonstrating the effectiveness of one-shot labeling on document-level retrieval tasks, with comparison to human judgments.

### Open Question 2
- Question: How can one-shot labeling be adapted to handle multiple known relevant documents per query, potentially with different relevance grades?
- Basis in paper: [explicit] The paper states it only explores situations where a single relevant document is known and suggests that techniques for making use of multiple labels need to be explored.
- Why unresolved: The paper does not provide a method for aggregating scores from multiple relevant documents or handling different relevance grades.
- What evidence would resolve it: A framework or algorithm that effectively incorporates multiple relevant documents with varying grades into the one-shot labeling process, validated through experiments.

### Open Question 3
- Question: What potential biases might one-shot labeling models introduce when the systems being evaluated use the same base language model?
- Basis in paper: [explicit] The paper mentions the need for more work to explore potential biases these models introduce, especially when the systems being evaluated use the same base language model.
- Why unresolved: The paper does not provide a detailed analysis of the types of biases or their impact on evaluation outcomes.
- What evidence would resolve it: A study that identifies and quantifies the biases introduced by one-shot labeling models, including scenarios where evaluated systems share the same base language model.

## Limitations
- One-shot labeling works best for precision-oriented measures but shows more modest improvements for recall-sensitive measures
- The approach requires access to specific model checkpoints and prompt templates for exact reproduction
- Effectiveness depends on having sufficiently different systems being evaluated; similar systems may break the assumption that performance differences dominate labeling noise

## Confidence

**High Confidence**: The claim that one-shot labeling reduces false positive t-test rates from 15-30% down to under 5% is well-supported by direct experimental evidence in the paper. The mechanism of filling holes to reduce statistical noise is clearly demonstrated.

**Medium Confidence**: The assertion that system ranking correlations consistently exceed 0.85 when using one-shot labeling is supported by results but depends on specific conditions (e.g., using recall-agnostic measures, certain system differences).

**Low Confidence**: The general claim that one-shot labeling can "replace deep manual assessments" in precision-oriented settings is promising but not fully validated across diverse evaluation scenarios and system types.

## Next Checks

**Validation 1**: Test system ranking correlation across varying levels of system similarity to identify when one-shot labeling breaks down. This will validate the assumption about system performance differences dominating labeling noise.

**Validation 2**: Implement a reproducibility test using publicly available checkpoints for DuoT5 and Prompt-Duo models to verify that the reported F1 scores (0.58-0.61) and ranking correlations (>0.86) can be achieved with the specified implementations.

**Validation 3**: Extend experiments to include recall-sensitive measures like MAP and nDCG to determine the limitations of one-shot labeling for different evaluation objectives.