---
ver: rpa2
title: Machine learning with tree tensor networks, CP rank constraints, and tensor
  dropout
arxiv_id: '2305.19440'
source_url: https://arxiv.org/abs/2305.19440
tags:
- tensor
- learning
- low-rank
- tensors
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a machine learning method that uses tree tensor
  networks (TTN) with low-rank tensor constraints and tensor dropout for image classification.
  The key idea is to approximate the weight tensor of a classifier as a TTN built
  from tensors with limited canonical polyadic (CP) rank, which significantly reduces
  the number of parameters and computation costs.
---

# Machine learning with tree tensor networks, CP rank constraints, and tensor dropout

## Quick Facts
- arXiv ID: 2305.19440
- Source URL: https://arxiv.org/abs/2305.19440
- Reference count: 0
- Key outcome: Achieves 90.3% test accuracy on Fashion-MNIST using low-rank tree tensor networks with tensor dropout

## Executive Summary
This paper introduces a novel machine learning approach using tree tensor networks (TTN) with low-rank tensor constraints and tensor dropout for image classification. The method approximates classifier weight tensors as TTNs built from tensors with limited canonical polyadic (CP) rank, significantly reducing parameters and computation costs while maintaining competitive accuracy. The authors demonstrate that this approach outperforms other tensor-network-based methods on Fashion-MNIST and avoids the vanishing gradient problem inherent in deep neural networks. The key innovation is the combination of CP rank constraints and tensor dropout, which improves generalization and enables larger branching ratios, substantially enhancing the model's representation power.

## Method Summary
The method maps input images to high-dimensional feature spaces using tensor networks, specifically tree tensor networks (TTN) with branching ratio b. The weight tensor W is decomposed as a TTN with tensors constrained to have limited CP rank r. This low-rank decomposition significantly reduces the number of parameters from O(N m^(b+1)) to O(N b m r), where N is the number of pixels, m is the bond dimension, and r is the CP rank. Tensor dropout regularization is applied during training to improve generalization. The model is trained using the Adam optimizer with learning rate decay on the Fashion-MNIST dataset, achieving 90.3% test accuracy.

## Key Results
- Achieves 90.3% test accuracy on Fashion-MNIST with low computational cost
- Outperforms other tensor-network-based methods on the same dataset
- Avoids vanishing gradient problem through mostly linear intermediate layers
- Enables larger branching ratios (b=4) that substantially improve representation power

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Imposing CP rank constraints on TTN tensors reduces the number of parameters from O(N m^(b+1)) to O(N b m r)
- Mechanism: By decomposing each tensor into a sum of rank-1 terms, we can compute contractions more efficiently by first contracting smaller vectors before combining them
- Core assumption: The low-rank approximation captures sufficient expressive power for the classification task, and r ≈ m is sufficient
- Evidence anchors: [abstract], [section II.3]
- Break condition: If the CP rank r needs to be much larger than m to maintain accuracy, the parameter reduction benefit diminishes

### Mechanism 2
- Claim: Low-rank TTN classifiers avoid the vanishing gradient problem present in deep neural networks
- Mechanism: Since the only nonlinear elements are in the input and output layers, gradients can flow through the mostly linear intermediate layers without exponential decay
- Core assumption: The locality properties of the learned features align with the tensor network structure
- Evidence anchors: [abstract], [section I]
- Break condition: If the feature map or class label prediction introduces significant nonlinearity that disrupts gradient flow

### Mechanism 3
- Claim: Larger branching ratios b improve classification accuracy by reducing the average graph distance between pixels
- Mechanism: With larger b, more pixels are directly connected in the tensor network, allowing the classifier to capture non-local features more efficiently without needing to propagate information through many layers
- Core assumption: The features relevant for classification depend on interactions between distant pixels that benefit from shorter graph distances
- Evidence anchors: [abstract], [section II.1], [section IV]
- Break condition: If the relevant features for classification are primarily local, the benefit of larger b diminishes

## Foundational Learning

- Concept: Tensor networks and their role in quantum many-body physics
  - Why needed here: Understanding the motivation for using tensor networks in machine learning and the connection to quantum state approximation
  - Quick check question: What problem in quantum many-body physics do tensor networks solve, and how is this relevant to machine learning?

- Concept: Canonical Polyadic (CP) decomposition and low-rank tensor approximation
  - Why needed here: The core technique used to constrain the TTN tensors and reduce parameters
  - Quick check question: How does the CP decomposition express a tensor as a sum of rank-1 terms, and what is the significance of the CP rank?

- Concept: Tree tensor network (TTN) structure and contraction
  - Why needed here: Understanding how the TTN processes input data through its layered structure and how contractions are performed
  - Quick check question: How does a TTN with branching ratio b process an input image through its layers, and how does the number of layers relate to the image size?

## Architecture Onboarding

- Component map: Input image → Pixel feature map → TTN contractions through layers → Output vector → Class probabilities → Compare to true labels → Compute loss → Backpropagate gradients → Update tensor parameters

- Critical path: Input image → Pixel feature map → TTN contractions through layers → Output vector → Class probabilities via Born's rule (5) → Compare to true labels → Compute loss (7) → Backpropagate gradients → Update tensor parameters

- Design tradeoffs:
  - Bond dimension m vs. CP rank r: Higher m increases expressiveness but risks overfitting; higher r improves approximation quality but increases parameters
  - Branching ratio b vs. number of layers: Larger b reduces graph distance but increases tensor order; more layers increase depth but may introduce vanishing gradients
  - CP rank constraints vs. full-rank tensors: Constraints reduce parameters and improve generalization but may limit expressiveness

- Failure signatures:
  - Training accuracy much higher than validation/test accuracy: Overfitting, consider reducing m or increasing r
  - Training/validation accuracy plateauing early: Insufficient model capacity, consider increasing m or r
  - Diverging gradients or NaN values: Learning rate too high or insufficient regularization, consider reducing learning rate or adding penalty term

- First 3 experiments:
  1. Train a low-rank TTN with small m (e.g., m=4) and varying r on Fashion-MNIST, observe the tradeoff between r and accuracy
  2. Compare the performance of low-rank TTN with b=2 vs. b=4 on a small image dataset, verify the benefit of larger branching ratio
  3. Implement tensor dropout with varying dropout rates p on a trained low-rank TTN, observe its effect on generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the low-rank TTN classifier suffer from the vanishing gradient problem when the depth of the tree increases?
- Basis in paper: [explicit] The authors state that "Consisting of mostly linear elements, tensor network classifiers avoid the vanishing gradient problem of deep neural networks."
- Why unresolved: The authors claim that TTN classifiers avoid the vanishing gradient problem, but they do not provide empirical evidence or theoretical analysis to support this claim. It is unclear whether this property holds for very deep TTN classifiers with many layers.
- What evidence would resolve it: Empirical studies comparing the gradient magnitudes during training of TTN classifiers with varying depths, or theoretical analysis of the gradient flow in TTN classifiers, would help resolve this question.

### Open Question 2
- Question: What is the optimal branching ratio for the low-rank TTN classifier, and how does it depend on the dataset and problem complexity?
- Basis in paper: [explicit] The authors observe that "The data in Fig. 7 confirms the benefits of large b, which can be achieved much more easily with the low-rank TTN classifiers." They also state that "It will hence be interesting to investigate the performance of low-rank TTN classifiers with even larger b in more challenging machine learning problems."
- Why unresolved: The authors only test the low-rank TTN classifier with branching ratios b = 2 and b = 4. It is unclear whether the optimal branching ratio depends on the dataset size, problem complexity, or other factors. More extensive empirical studies are needed to determine the optimal branching ratio for different scenarios.
- What evidence would resolve it: Empirical studies comparing the performance of low-rank TTN classifiers with various branching ratios on diverse datasets and problem types would help determine the optimal branching ratio and its dependence on problem characteristics.

### Open Question 3
- Question: How does the low-rank TTN classifier compare to other state-of-the-art deep learning models on larger and more complex datasets?
- Basis in paper: [explicit] The authors compare the low-rank TTN classifier to other tensor-network-based methods and some traditional machine learning approaches (XGBoost, AlexNet) on the Fashion-MNIST dataset. They state that "Despite its simple structure and low computation costs, the low-rank TTN with tensor dropout is also competitive with more prominent machine-learning approaches like the convolutional-neural-network based AlexNet."
- Why unresolved: The authors only benchmark the low-rank TTN classifier on the relatively small Fashion-MNIST dataset. It is unclear how the classifier would perform on larger and more complex datasets, such as ImageNet or CIFAR-100, where deep learning models like convolutional neural networks have achieved state-of-the-art results. More extensive comparisons on larger datasets are needed to assess the competitiveness of the low-rank TTN classifier.
- What evidence would resolve it: Empirical studies comparing the performance of the low-rank TTN classifier with state-of-the-art deep learning models on larger and more complex datasets, such as ImageNet or CIFAR-100, would help resolve this question.

## Limitations
- Experimental validation restricted to single dataset (Fashion-MNIST), limiting generalizability
- CP rank constraints set to r ≈ m without exploring full tradeoff space or theoretical justification
- Tensor dropout implementation details are sparse, making exact reproduction challenging
- Computational complexity claims rely on idealized tensor contractions that may not account for practical overhead

## Confidence
- Mechanism 1 (CP rank parameter reduction): Medium confidence
- Mechanism 2 (vanishing gradient avoidance): Medium confidence
- Mechanism 3 (larger branching ratio benefits): Low confidence

## Next Checks
1. Perform systematic ablation studies varying CP rank r from 0.5m to 2m while keeping other parameters fixed to quantify the accuracy-parameter tradeoff
2. Implement gradient flow analysis comparing TTN classifiers with conventional deep networks to empirically verify the vanishing gradient claim
3. Test the low-rank TTN architecture on multiple image classification datasets (CIFAR-10, MNIST) to assess generalizability beyond Fashion-MNIST