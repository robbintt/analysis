---
ver: rpa2
title: Location-Aware Visual Question Generation with Lightweight Models
arxiv_id: '2310.15129'
source_url: https://arxiv.org/abs/2310.15129
tags:
- questions
- question
- engaging
- dataset
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LocaVQG, a novel task of generating engaging
  questions from location-aware information, specifically GPS coordinates and street-view
  images captured by on-car cameras. To address this task, the authors propose a dataset
  generation pipeline leveraging GPT-4 to produce diverse questions, followed by filtering
  using an engaging question classifier.
---

# Location-Aware Visual Question Generation with Lightweight Models

## Quick Facts
- arXiv ID: 2310.15129
- Source URL: https://arxiv.org/abs/2310.15129
- Reference count: 40
- Primary result: Introduces LocaVQG task and FDT5 model achieving competitive performance with only 15M parameters

## Executive Summary
This paper introduces LocaVQG, a novel task of generating engaging questions from location-aware information, specifically GPS coordinates and street-view images captured by on-car cameras. To address this task, the authors propose a dataset generation pipeline leveraging GPT-4 to produce diverse questions, followed by filtering using an engaging question classifier. They then present FDT5, a lightweight model that outperforms baselines on human evaluation (engagement, coherence, grounding) and automatic metrics (BERTScore, ROUGE-2), with only 15M parameters compared to GPT-4's 1T+. The proposed method achieves competitive performance, highlighting the effectiveness of the dataset generation pipeline and training techniques for developing in-car intelligent assistant systems.

## Method Summary
The method involves a three-stage pipeline: (1) GPT-4 generates questions from street-view images and GPS coordinates with reverse-geocoded addresses, (2) an engaging question classifier filters non-engaging questions, and (3) lightweight models (T5 variants, VL-T5) are trained using knowledge distillation from T5-Large to T5-Tiny, with engaging question filtering during inference. The dataset consists of 3,759 coordinates with 4 directional images each, and the FDT5 model achieves state-of-the-art performance on automatic metrics while maintaining a small parameter count.

## Key Results
- FDT5 achieves best performance on ROUGE-2, BERTScore, and BLEURT metrics
- Human evaluation shows FDT5 outperforms baselines on engagement, naturalness, coherence, common sense, and grounding
- FDT5 uses only 15M parameters compared to GPT-4's 1T+ parameters
- Engaging question classifier successfully filters out non-engaging questions with high accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4's large parameter count enables it to generate diverse, contextually rich questions when given structured prompts.
- **Mechanism**: GPT-4 processes multimodal inputs (via captions and reverse-geocoded addresses) to infer location-specific knowledge and generate engaging questions.
- **Core assumption**: GPT-4's training data includes sufficient location-specific and cultural context to produce relevant questions.
- **Evidence anchors**:
  - [abstract] "leverages GPT-4 to produce diverse and sophisticated questions"
  - [section 4.1] "we empirically find that some generated questions are not particularly engaging"
  - [corpus] Weak: no direct corpus citations provided
- **Break condition**: GPT-4 lacks location-specific knowledge or the prompt fails to provide sufficient contextual cues.

### Mechanism 2
- **Claim**: The engaging question classifier filters out non-engaging questions by distinguishing between task-oriented (SQuAD) and engagement-focused (MVQG) question styles.
- **Mechanism**: BERT-based classifier trained on SQuAD (non-engaging) and MVQG (engaging) datasets learns to score question engagement.
- **Core assumption**: SQuAD questions are inherently less engaging than MVQG questions for casual conversation.
- **Evidence anchors**:
  - [section 4.2] "we propose to learn a BERT-based engaging question classifier to filter out non-engaging questions"
  - [section 5.5.1] "The accuracy evaluates if the classifier can correctly distinguish the questions in the MVQG dataset from those in the SQuAD dataset"
  - [corpus] Weak: no direct corpus citations provided
- **Break condition**: Classifier fails to generalize beyond training data or engagement criteria shift.

### Mechanism 3
- **Claim**: Knowledge distillation from T5-Large to T5-Tiny transfers engagement capabilities while maintaining lightweight inference.
- **Mechanism**: FDT5 uses hard-label loss (ground truth) and soft-label loss (teacher model) to learn from both GPT-4-generated and T5-Large-generated questions.
- **Core assumption**: T5-Large's generated questions capture engagement patterns that T5-Tiny can learn through distillation.
- **Evidence anchors**:
  - [section 5.2] "we propose to learn a T5-Tiny model by distilling a learned T5-Large model"
  - [section 5.4] "Our proposed FDT5 achieves the best performance on ROUGE-2, BERTScore, and BLEURT"
  - [corpus] Weak: no direct corpus citations provided
- **Break condition**: T5-Large fails to generate engaging questions or T5-Tiny cannot capture distilled knowledge.

## Foundational Learning

- **Concept**: Multimodal question generation from visual and textual inputs
  - **Why needed here**: LocaVQG requires combining street-view images with GPS coordinates to generate contextually relevant questions
  - **Quick check question**: Can you explain how image captions and address information complement each other in question generation?

- **Concept**: Knowledge distillation in transformer models
  - **Why needed here**: Enables lightweight models (T5-Tiny) to achieve performance close to larger models (T5-Large) through soft-label learning
  - **Quick check question**: What is the difference between hard-label and soft-label loss in knowledge distillation?

- **Concept**: Engaging question classification using BERT
  - **Why needed here**: Filters GPT-4-generated questions to ensure only engaging ones are included in the dataset
  - **Quick check question**: How does the classifier distinguish between engaging and non-engaging questions based on training data?

## Architecture Onboarding

- **Component map**: GPT-4 (question generation) → Engaging question classifier (filtering) → T5-Base/Tiny (model training) → FDT5 (distilled lightweight model) → External components: Google Street View API, Google Reverse Geocoding API, image captioning model

- **Critical path**: GPT-4 prompt → Question generation → Classifier filtering → Dataset creation → Model training → Inference
- **Design tradeoffs**: Larger models (GPT-4, T5-Large) provide better question quality but are computationally expensive; lightweight models (T5-Tiny) are efficient but require distillation
- **Failure signatures**: Poor question engagement → Check classifier accuracy; slow inference → Check model size and filtering overhead
- **First 3 experiments**:
  1. Test GPT-4 question generation with sample prompts and verify output quality
  2. Evaluate engaging question classifier on validation set to ensure proper filtering
  3. Train T5-Tiny with and without distillation to compare performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the engaging question classifier effectively distinguish between engaging and non-engaging questions across diverse geographical locations and cultural contexts?
- **Basis in paper**: Inferred from the authors' mention of potential biases in AMT workers and the need for diverse backgrounds to ensure fairness.
- **Why unresolved**: The paper does not provide evidence of the classifier's performance across diverse locations or cultural contexts. It only reports performance on the training, validation, and test sets from the SQuaD and MVQG datasets.
- **What evidence would resolve it**: Evaluating the classifier's performance on a dataset containing questions from diverse geographical locations and cultural contexts would provide insights into its generalizability.

### Open Question 2
- **Question**: How does the inclusion of additional location-aware information, such as local news and weather, affect the diversity and engagingness of the generated questions?
- **Basis in paper**: Inferred from the authors' discussion of potential future work to incorporate more detailed location-aware information beyond GPS coordinates and street view images.
- **Why unresolved**: The paper does not explore the impact of incorporating additional location-aware information on the quality of generated questions.
- **What evidence would resolve it**: Experimenting with different types of location-aware information and evaluating their impact on the diversity and engagingness of generated questions would provide insights into their potential benefits.

### Open Question 3
- **Question**: How does the distractingness of generated questions impact driver safety, and what methods can be developed to produce engaging yet non-distracting questions?
- **Basis in paper**: Explicitly mentioned in the limitations section, where the authors acknowledge the potential for engaging questions to distract drivers and encourage future research to address this concern.
- **Why unresolved**: The paper does not explore the relationship between question engagingness and driver distraction, nor does it propose methods to mitigate this issue.
- **What evidence would resolve it**: Conducting experiments to measure the impact of generated questions on driver distraction and developing evaluation metrics to assess question distractingness would provide insights into this important aspect of the task.

## Limitations
- Reliance on GPT-4 for question generation introduces computational cost and potential knowledge gaps
- Engaging question classifier's effectiveness is demonstrated on limited datasets (SQuAD, MVQG) without real-world LocaVQG data validation
- Knowledge distillation process lacks detailed hyperparameter specifications affecting reproducibility
- Human evaluation relies on limited evaluators (3-4 per question) which may not capture diverse user perspectives

## Confidence
- **High Confidence**: Dataset generation pipeline and FDT5's lightweight architecture (15M parameters) are well-documented and reproducible. Performance gains over baselines on automatic metrics are clearly demonstrated.
- **Medium Confidence**: Engaging question classifier and knowledge distillation process effectiveness require further validation on larger datasets and diverse user groups.
- **Low Confidence**: GPT-4's question generation generalization to new locations and cultural contexts remains uncertain without additional testing.

## Next Checks
1. Evaluate the engaging question classifier on a held-out test set of LocaVQG data to assess its ability to filter questions effectively beyond the training datasets.
2. Test the GPT-4 question generation pipeline with street-view images and GPS coordinates from diverse geographic regions to verify its ability to produce culturally relevant and engaging questions.
3. Measure the performance and efficiency of FDT5 when scaled to larger datasets (e.g., 10x the current size) to ensure that the knowledge distillation process remains effective under increased data volume.