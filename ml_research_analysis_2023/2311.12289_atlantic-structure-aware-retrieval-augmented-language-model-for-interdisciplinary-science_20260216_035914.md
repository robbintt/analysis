---
ver: rpa2
title: 'ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary
  Science'
arxiv_id: '2311.12289'
source_url: https://arxiv.org/abs/2311.12289
tags:
- passages
- structural
- document
- scientific
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ATLANTIC, a structure-aware retrieval-augmented
  language model that improves scientific document understanding by incorporating
  document structure into the retrieval process. The method constructs a heterogeneous
  document graph capturing relationships (citation, co-authorship, etc.) across 15+
  scientific disciplines, and uses a pretrained graph neural network to provide structural
  embeddings for retrieved passages.
---

# ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science

## Quick Facts
- arXiv ID: 2311.12289
- Source URL: https://arxiv.org/abs/2311.12289
- Reference count: 4
- Primary result: Improves scientific document understanding by incorporating document structure into retrieval-augmented language models, achieving better evidence quality and faithfulness scores

## Executive Summary
This paper introduces ATLANTIC, a structure-aware retrieval-augmented language model that enhances scientific document understanding by incorporating document structure into the retrieval process. The method constructs a heterogeneous document graph capturing relationships (citation, co-authorship, etc.) across 15+ scientific disciplines, and uses a pretrained graph neural network to provide structural embeddings for retrieved passages. These structural embeddings are fused with text embeddings before feeding to the language model. ATLANTIC is evaluated on scientific benchmarks including Fields of Study classification and MMLU, showing improved retrieval of contextually relevant and faithful passages while maintaining comparable generation accuracy.

## Method Summary
ATLANTIC constructs a heterogeneous document graph from the S2ORC corpus capturing multiple relationship types (citation, co-authorship, co-topic, co-venue, co-institution) across 19 scientific domains. A pretrained Heterogeneous Graph Transformer (HGT) generates structural embeddings from this graph, which are then concatenated with text embeddings from Contriever for each retrieved passage. The fused embeddings are fed to a T5 language model, which is trained end-to-end using perplexity distillation loss. The model employs query-side finetuning to address scalability issues by freezing document embeddings while updating query encoder parameters.

## Key Results
- Achieves improved evidence quality with relevance scores up to 1.163 vs 0.825 for text-only baselines
- Maintains comparable generation accuracy (EM and F1 scores) while improving retrieval faithfulness
- Excels at retrieving passages from related scientific domains compared to text-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous document graph structural embeddings improve retrieval relevance by capturing cross-document relationships
- Mechanism: The model constructs a heterogeneous document graph capturing multiple relationship types (citation, co-authorship, co-topic, co-venue, co-institution) across 15+ scientific disciplines. A pretrained Heterogeneous Graph Transformer (HGT) generates structural embeddings that encode these relationships, which are then concatenated with text embeddings before feeding to the language model
- Core assumption: Scientific documents have meaningful structural relationships that correlate with semantic relevance for retrieval tasks
- Evidence anchors:
  - [abstract] "We create a heterogeneous document graph capturing multiple types of relationships (e.g., citation, co-authorship, etc.) that connect documents from more than 15 scientific disciplines"
  - [section] "HDG offers plenty of new information that is otherwise ignored in standard semantic-only RALM"
- Break condition: If scientific documents lack meaningful structural relationships or if the graph construction introduces noise that degrades retrieval quality

### Mechanism 2
- Claim: Fusing text and structural embeddings provides more contextually relevant passages for interdisciplinary science tasks
- Mechanism: The model concatenates semantic embeddings (from Contriever) with structural embeddings (from HGT) for each passage, creating aggregate embeddings that capture both lexical content and document relationships. This allows retrieval to consider both semantic similarity and structural proximity
- Core assumption: Combining multiple information sources (text + structure) provides better retrieval signals than text alone for scientific documents
- Evidence anchors:
  - [abstract] "along with text embeddings of the retrieved passages, we obtain structural embeddings of the documents (passages) and fuse them together before feeding them to the language model"
  - [section] "The structural embeddings are then concatenated with text embeddings and passed to the reader (i.e., LM)"
- Break condition: If structural information adds noise rather than signal, or if the fusion approach overwhelms the semantic information

### Mechanism 3
- Claim: Query-side finetuning enables scalable training while maintaining retrieval quality
- Mechanism: Instead of updating document embeddings during training (computationally expensive), the model freezes the document encoder and only updates the query encoder parameters. This allows frequent embedding updates without the computational cost of full index updates
- Core assumption: Query encoder updates are sufficient to learn effective retrieval while keeping document embeddings frozen
- Evidence anchors:
  - [section] "we opt for query side finetuning approach, which was originally introduced in the ATLAS model (Izacard et al. 2022)"
  - [section] "This approach is very efficient for model training since it keeps the document encoder frozen while only training the parameters of the query encoder"
- Break condition: If frozen document embeddings become stale or if query-side updates cannot adequately capture the relationship between queries and documents

## Foundational Learning

- Concept: Heterogeneous Graph Transformers (HGT)
  - Why needed here: HGT can explicitly model different types of relationships in the document graph (citation, co-authorship, etc.) with type-specific attention mechanisms, capturing the complex structure of scientific literature
  - Quick check question: How does HGT handle different edge types differently from standard GNNs?

- Concept: Dual encoder architecture for retrieval
  - Why needed here: Separates query and document encoding into independent components, enabling efficient similarity search via dot product while maintaining retrieval quality through learned representations
  - Quick check question: What are the trade-offs between dual encoder and cross-attention architectures for retrieval?

- Concept: Perplexity distillation loss for retriever training
  - Why needed here: Provides a differentiable signal from the language model back to the retriever, enabling end-to-end training where the retriever learns to select passages that improve LM generation quality
  - Quick check question: How does perplexity distillation differ from standard contrastive learning objectives for retrieval?

## Architecture Onboarding

- Component map: Query → Contriever (text encoder) → HGT (structural encoder) → Concatenation → T5 Reader → Generation
- Critical path: Query processing → Contriever encoding → HGT encoding → Embedding fusion → Reader input → Generation
- Design tradeoffs: 
  - Structural vs. semantic information balance: Too much structural emphasis may retrieve topically related but semantically irrelevant passages
  - Pretrained vs. learned structural encoders: Pretrained HGT reduces computational cost but may not be optimally tuned for the specific retrieval task
  - Frozen vs. trainable components: Freezing HGT and document encoder improves scalability but may limit adaptation to task-specific patterns
- Failure signatures:
  - Poor relevance scores despite high accuracy: Structural information may be overwhelming semantic signals
  - Slow inference due to structural encoding: HGT may be too computationally expensive for the benefit it provides
  - Degraded performance on non-scientific tasks: The heterogeneous graph may not generalize well beyond scientific domains
- First 3 experiments:
  1. Ablation study comparing ATLANTIC with text-only baseline on Fields of Study classification to measure structural contribution
  2. Retrieval quality analysis comparing structural embeddings from HGT vs. random graph embeddings to validate structural signal importance
  3. Scaling experiment varying the number of retrieved passages (k) to find optimal trade-off between computational cost and generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop benchmarks that effectively test retrieval-augmented language models on interdisciplinary science tasks that require knowledge from multiple domains?
- Basis in paper: Explicit - The paper concludes "We urge the scientific community to develop benchmarks that test the ability of the models to perform on interdisciplinary science tasks."
- Why unresolved: Current benchmarks like MMLU and FoS focus on single-domain classification or question answering, but real scientific problems often require integrating knowledge across multiple disciplines. The paper notes that many questions are fact-based and don't fully leverage retrieval augmentation's potential.
- What evidence would resolve it: Creation and validation of new benchmark datasets containing interdisciplinary science tasks where successful completion requires retrieving and synthesizing information from multiple scientific domains, with clear evaluation metrics for interdisciplinary reasoning.

### Open Question 2
- Question: What is the optimal trade-off between retrieval corpus scalability and effectiveness when using query-side finetuning instead of full index updates in RALMs?
- Basis in paper: Explicit - The paper states "the retrieval corpus was frozen during model training, but the query encoder was allowed to receive the gradient updates to address scalability issues" and identifies this as a limitation requiring future work.
- Why unresolved: The paper notes that this configuration may lead to models being less able to generalize to scientific data than originally tested for general web-quality data, but doesn't explore alternatives or quantify the impact.
- What evidence would resolve it: Empirical comparison of model performance and generalization capabilities using different retrieval corpus update strategies (full index updates vs query-side finetuning) across various dataset sizes and scientific domains.

### Open Question 3
- Question: How does the performance of structure-aware RALMs vary across different types of scientific reasoning tasks (e.g., hypothesis generation, experimental design, interdisciplinary problem-solving)?
- Basis in paper: Inferred - The paper focuses on classification and question-answering tasks and concludes with "we will test our model on a wider range of scientific benchmarks and tasks (e.g., hypothesis generation)", suggesting this hasn't been explored.
- Why unresolved: The current evaluation focuses on fact-based question answering and classification tasks where the language model may be less sensitive to retrieved context. The paper notes that "the impact of retrieval will be evident in those benchmarks (queries) that are very context dependent."
- What evidence would resolve it: Systematic evaluation of structure-aware RALMs on diverse scientific reasoning tasks requiring different cognitive processes, measuring both retrieval quality and model performance to identify which task types benefit most from structural information.

## Limitations

- Heavy computational overhead from Heterogeneous Graph Transformer (HGT) for structural encoding may not scale efficiently to larger document collections
- Reliance on pretrained HGT weights without evidence of how well this pretraining generalizes to specific scientific retrieval tasks
- Query-side finetuning approach may limit model's ability to generalize to scientific data compared to full index updates

## Confidence

**High Confidence (Experimental evidence provided):**
- ATLANTIC achieves improved retrieval relevance scores compared to text-only baselines on scientific benchmarks
- The heterogeneous document graph successfully captures meaningful relationships across 15+ scientific disciplines
- Query-side finetuning provides computational efficiency benefits while maintaining retrieval quality

**Medium Confidence (Evidence suggests but doesn't definitively prove):**
- Fusing text and structural embeddings provides more contextually relevant passages for interdisciplinary science tasks
- The structural embeddings from HGT encode meaningful document relationships that improve retrieval quality
- ATLANTIC maintains comparable generation accuracy while improving evidence quality

## Next Checks

1. **Scalability Validation**: Test ATLANTIC on a larger corpus (e.g., 1B+ passages) to measure computational overhead and verify whether the HGT-based structural encoding remains practical at scale.

2. **Structural Contribution Isolation**: Perform an ablation study comparing ATLANTIC against variants with random structural embeddings, pretrained HGT embeddings from different domains, and fine-tuned HGT embeddings to quantify the specific contribution of structural information versus the quality of the HGT pretraining.

3. **Cross-Domain Generalization**: Evaluate ATLANTIC on non-scientific domains (e.g., legal, news, or technical documentation) to determine whether the heterogeneous graph approach generalizes beyond scientific literature or if it's specialized to the scientific domain's unique relationship patterns.