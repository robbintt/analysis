---
ver: rpa2
title: Efficient Memory Management for Large Language Model Serving with PagedAttention
arxiv_id: '2309.06180'
source_url: https://arxiv.org/abs/2309.06180
tags:
- memory
- block
- vllm
- cache
- request
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high memory usage in large
  language model (LLM) serving, particularly due to the key-value (KV) cache memory
  that grows dynamically during generation. The authors propose PagedAttention, an
  attention algorithm inspired by virtual memory and paging techniques in operating
  systems, which allows the KV cache to be stored in non-contiguous memory blocks.
---

# Efficient Memory Management for Large Language Model Serving with PagedAttention

## Quick Facts
- arXiv ID: 2309.06180
- Source URL: https://arxiv.org/abs/2309.06180
- Reference count: 40
- Throughput improvement: 2-4x compared to state-of-the-art systems

## Executive Summary
This paper addresses the problem of high memory usage in large language model (LLM) serving, particularly due to the key-value (KV) cache memory that grows dynamically during generation. The authors propose PagedAttention, an attention algorithm inspired by virtual memory and paging techniques in operating systems, which allows the KV cache to be stored in non-contiguous memory blocks. They build vLLM, an LLM serving system on top of PagedAttention, that achieves near-zero waste in KV cache memory and flexible sharing within and across requests. Experiments show that vLLM improves the throughput of popular LLMs by 2-4x compared to state-of-the-art systems like FasterTransformer and Orca, without affecting model accuracy.

## Method Summary
The authors propose PagedAttention, an attention algorithm inspired by virtual memory and paging techniques in operating systems. PagedAttention divides the KV cache into fixed-size blocks and uses a block table to map logical blocks to physical memory locations, enabling non-contiguous memory allocation. The vLLM system implements a centralized block manager for physical block allocation, reference counting, and copy-on-write sharing across sequences within a request. The system also includes preemptive request scheduling to maximize GPU utilization and supports model-parallel execution across multiple GPU workers.

## Key Results
- vLLM achieves 2-4x higher throughput compared to FasterTransformer and Orca on OPT models (13B, 66B, 175B parameters)
- Near-zero waste in KV cache memory through non-contiguous block allocation
- Significant memory savings (30-50%) with copy-on-write sharing in parallel sampling and beam search decoding
- Improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PagedAttention reduces memory fragmentation by storing KV cache in non-contiguous fixed-size blocks instead of contiguous pre-allocated chunks.
- Mechanism: The algorithm partitions KV cache into fixed-size blocks (pages) and uses a block table to map logical blocks to physical memory locations. This allows dynamic allocation on demand without reserving large contiguous spaces upfront.
- Core assumption: Attention computation can be restructured to read key/value vectors from non-contiguous memory locations efficiently.
- Evidence anchors:
  - [abstract]: "PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems."
  - [section 4.1]: "PagedAttention divides the request's KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens."
  - [corpus]: No direct corpus evidence found; this is a novel architectural approach.
- Break condition: If block size is too large, internal fragmentation increases; if too small, kernel launch overhead and memory indirection dominate.

### Mechanism 2
- Claim: Copy-on-write sharing of KV cache blocks across sequences within a request significantly reduces memory usage for decoding algorithms like parallel sampling and beam search.
- Mechanism: Multiple sequences share physical blocks via reference counting; when a sequence needs to modify a shared block, a new physical block is allocated and the reference count is decremented.
- Core assumption: KV cache blocks for the prompt portion are identical across sequences in the same request until divergence occurs.
- Evidence anchors:
  - [section 4.4]: "vLLM implements a copy-on-write mechanism at the block granularity for the physical blocks that need modification by multiple sequences."
  - [section 6.3]: "By sharing physical blocks across multiple samples, memory usage can be greatly reduced, especially for long input prompts."
  - [corpus]: No direct corpus evidence found; this is a novel sharing mechanism.
- Break condition: High divergence rate between sequences reduces sharing benefits; reference counting overhead may become significant.

### Mechanism 3
- Claim: Centralized block management with distributed execution enables efficient memory sharing across model-parallel GPU workers without redundant memory copies.
- Mechanism: A single KV cache manager maintains the logical-to-physical block mapping for all workers, who share the same mapping and only store relevant portions of each block for their model shards.
- Core assumption: Model-parallel execution still requires the same logical positions to be cached across all workers for attention computation.
- Evidence anchors:
  - [section 4.6]: "Different GPU workers share the manager, as well as the mapping from logical blocks to physical blocks."
  - [section 6]: Evaluations show improvements with distributed configurations.
  - [corpus]: No direct corpus evidence found; this is a novel distributed memory management approach.
- Break condition: High communication overhead between scheduler and workers; inconsistent block mapping updates.

## Foundational Learning

- Concept: Virtual memory and paging in operating systems
  - Why needed here: PagedAttention directly adapts OS virtual memory concepts (pages, block tables, swapping) to GPU memory management for KV cache.
  - Quick check question: How does OS virtual memory solve fragmentation, and what are the analogous components in PagedAttention?

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding how attention computes scores and outputs is essential to see why KV cache exists and how PagedAttention restructures it.
  - Quick check question: What are the key, value, and query vectors in attention, and why must they be cached for autoregressive generation?

- Concept: Memory management in GPU serving systems
  - Why needed here: Knowing how traditional systems allocate contiguous memory for KV cache helps understand why PagedAttention's non-contiguous approach is innovative.
  - Quick check question: What are the main sources of memory waste in traditional LLM serving systems, and how does contiguous allocation contribute?

## Architecture Onboarding

- Component map:
  - Scheduler: Centralized control, block table management, request scheduling
  - KV Cache Manager: Physical block allocation, reference counting, copy-on-write
  - GPU Workers: Model execution, PagedAttention kernels, tensor communication
  - Block Allocator: CPU and GPU block pools, swap space management

- Critical path: Request arrival → Scheduler assigns logical blocks → Block manager allocates physical blocks → Workers execute with PagedAttention → Blocks freed or shared based on decoding algorithm

- Design tradeoffs:
  - Block size vs. fragmentation vs. kernel efficiency
  - Centralized vs. distributed block management
  - Swapping vs. recomputation for evicted blocks
  - Memory indirection overhead vs. flexibility

- Failure signatures:
  - High memory fragmentation: Block utilization drops, batch size decreases
  - Excessive swapping: Latency spikes, CPU-GPU bandwidth saturated
  - Reference counting bugs: Memory leaks or premature block freeing
  - Kernel launch overhead: Poor performance with small block sizes

- First 3 experiments:
  1. Compare memory usage and batch size with different block sizes (16, 32, 64) on ShareGPT trace
  2. Measure performance impact of copy-on-write with parallel sampling (2, 4, 6 sequences)
  3. Benchmark recomputation vs. swapping overhead for different block sizes and sequence lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the block size affect the trade-off between memory utilization and GPU parallelization efficiency in PagedAttention?
- Basis in paper: [explicit] The paper discusses the impact of block size on internal fragmentation, sharing probability, and GPU parallelism in §7.2.
- Why unresolved: The paper only tests a limited range of block sizes and doesn't provide a comprehensive analysis of the optimal block size for different workloads and hardware configurations.
- What evidence would resolve it: A detailed study varying block sizes across a wider range and evaluating the impact on memory utilization, sharing efficiency, and GPU utilization for different workloads and hardware configurations.

### Open Question 2
- Question: What are the potential performance bottlenecks and scalability limitations of vLLM when serving extremely large models (e.g., 1 trillion parameters) or handling extremely long sequences (e.g., 100K tokens)?
- Basis in paper: [inferred] The paper demonstrates vLLM's performance on models up to 175B parameters and sequences up to 2048 tokens, but doesn't explore the scalability limits.
- Why unresolved: The paper doesn't provide any analysis of vLLM's performance characteristics at extreme scales, which is crucial for understanding its applicability to future large-scale LLM deployments.
- What evidence would resolve it: Benchmarking vLLM on models and sequences significantly larger than those tested in the paper, and analyzing the performance bottlenecks and scalability limitations.

### Open Question 3
- Question: How does the choice of recovery mechanism (recomputation vs. swapping) impact the end-to-end performance of vLLM under different workload characteristics and hardware configurations?
- Basis in paper: [explicit] The paper discusses the trade-offs between recomputation and swapping in §7.3 and provides some experimental results.
- Why unresolved: The paper only provides a limited comparison of the two mechanisms under specific conditions and doesn't explore the impact of different workload characteristics and hardware configurations.
- What evidence would resolve it: A comprehensive study comparing the performance of recomputation and swapping under various workload patterns (e.g., different request rates, sequence lengths, and decoding algorithms) and hardware configurations (e.g., different GPU memory sizes and CPU-GPU interconnect bandwidths).

## Limitations

- The evaluation focuses primarily on models up to 175B parameters, leaving scalability to trillion-parameter models untested
- The absolute memory overhead of the block management system (block tables, reference counters, metadata) is not explicitly quantified
- Performance benefits under highly diverse prompts with minimal sharing opportunities are not thoroughly explored

## Confidence

### High Confidence Claims
- **PagedAttention reduces memory fragmentation**: Well-supported by the virtual memory analogy and the evaluation showing near-zero waste in KV cache memory
- **vLLM achieves 2-4x throughput improvement**: Multiple experiments across different models and decoding algorithms consistently show significant improvements
- **Copy-on-write sharing reduces memory usage**: Demonstrated improvements with parallel sampling and beam search decoding scenarios

### Medium Confidence Claims
- **Performance improvements scale with sequence length**: Supported by experiments but limited to sequences up to 2048 tokens
- **Distributed execution benefits**: Evaluation shows improvements but with limited configuration details
- **Block size optimization**: Claims supported by experiments but with narrow range of tested block sizes

### Low Confidence Claims
- **General applicability to all decoding algorithms**: Limited evaluation scope
- **Performance on trillion-parameter models**: Not tested
- **Behavior under extreme load conditions**: Limited concurrency testing

## Next Checks

1. **Memory Overhead Analysis**: Quantify the absolute memory overhead of the block management system (block tables, metadata, reference counters) across different model sizes and workloads to verify the near-zero waste claim.

2. **Scalability Testing**: Evaluate system performance and throughput with 500+ concurrent requests on 175B parameter models to identify potential bottlenecks in the centralized scheduler architecture.

3. **Diverse Workload Evaluation**: Test vLLM performance on workloads with highly diverse prompts (minimal sharing opportunities) and alternative decoding strategies (greedy decoding, top-k sampling) to assess generalization of the claimed improvements.