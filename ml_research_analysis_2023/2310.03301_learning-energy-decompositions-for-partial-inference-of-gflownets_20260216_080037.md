---
ver: rpa2
title: Learning Energy Decompositions for Partial Inference of GFlowNets
arxiv_id: '2310.03301'
source_url: https://arxiv.org/abs/2310.03301
tags:
- energy
- learning
- generation
- led-gfn
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach for training GFlowNets called
  Learning Energy Decompositions for GFlowNets (LED-GFN). The key idea is to decompose
  the energy of an object into learnable potential functions defined on state transitions,
  and reparameterize the flow functions using these potentials.
---

# Learning Energy Decompositions for Partial Inference of GFlowNets

## Quick Facts
- arXiv ID: 2310.03301
- Source URL: https://arxiv.org/abs/2310.03301
- Reference count: 16
- One-line primary result: LED-GFN outperforms existing methods on tasks like molecule generation and set generation by learning energy decompositions for partial inference in GFlowNets

## Executive Summary
This paper introduces Learning Energy Decompositions for GFlowNets (LED-GFN), a novel approach that addresses the credit assignment problem in GFlowNets by decomposing terminal state energies into learnable transition potentials. The method enables partial inference by providing informative local credit signals without requiring expensive or misleading intermediate state energy evaluations. LED-GFN achieves state-of-the-art performance on multiple tasks including molecule generation, RNA sequence generation, and set generation by training potential functions to approximate the true energy while minimizing variance along trajectories.

## Method Summary
LED-GFN decomposes the terminal energy function E(x) into a sum of learnable potential functions ϕθ defined on state transitions. These potentials are trained online during GFlowNet training using a least-squares objective with dropout-based regularization to minimize variance along trajectories. The learned potentials serve as local credit signals for training the GFlowNet policy, enabling partial inference where intermediate state energies are not evaluated. The potentials are updated iteratively within the GFlowNet training loop, allowing them to adapt to the current policy and provide relevant credit signals throughout training.

## Key Results
- LED-GFN discovers more modes (samples with energy below threshold) than GFlowNet and FL-GFN baselines on molecule generation tasks
- LED-GFN achieves higher average top-100 scores on bag generation and RNA sequence generation tasks
- LED-GFN shows consistent improvements across multiple domains including set generation and maximum independent set problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing terminal energy into learnable potentials provides more informative local credits than evaluating intermediate state energies directly.
- Mechanism: The energy decomposition framework splits the terminal energy E(x) into a sum of potentials ϕθ defined on state transitions. This allows training GFlowNets with partial inference by using these potentials as local credit signals, avoiding the need to evaluate potentially expensive or misleading intermediate state energies.
- Core assumption: The terminal energy can be accurately approximated by summing potentials over transitions, and these potentials can be learned to provide dense, informative signals along trajectories.
- Evidence anchors:
  - [abstract] "decompose the energy of an object into learnable potential functions defined on state transitions, and reparameterize the flow functions using the potential functions"
  - [section 3.2] "We decompose the energy function E associated with the terminal state into learnable potential functions ϕθ as follows: E(x) ≈ Φθ(τ) = PT−1t=0 ϕθ(st → st+1)"
  - [corpus] Weak - no direct evidence in corpus papers about energy decomposition for GFlowNets

### Mechanism 2
- Claim: Regularizing potentials to minimize variance along trajectories produces more informative local credits.
- Mechanism: The potential functions are trained with a least-squares objective that includes dropout-based regularization. This encourages potentials to change smoothly over sequences of actions, reducing sparsity and producing denser credit signals.
- Core assumption: Minimizing variance of potentials along trajectories leads to more consistent and informative local credit signals for training GFlowNets.
- Evidence anchors:
  - [section 3.2] "we propose to regularize the potential to change smoothly over the sequence of actions"
  - [section 3.2] "Dividing by T and C = PT−1t=0 zt aligns the scales to compensate for the scale reduction induced by dropout"
  - [corpus] Weak - no direct evidence in corpus papers about variance regularization for GFlowNet potentials

### Mechanism 3
- Claim: Online learning of potentials during GFlowNet training enables adaptive credit assignment.
- Mechanism: The potential functions are trained iteratively within the GFlowNet training loop, using trajectories collected during training. This allows the potentials to adapt to the current policy and provide relevant local credit signals.
- Core assumption: Online learning of potentials can keep pace with policy updates and provide useful credit signals throughout training.
- Evidence anchors:
  - [section 3.2] "To train the potential function, we define its training as online learning within GFlowNet training, i.e., learning from trajectories obtained during GFlowNet training"
  - [Algorithm 1] Shows the alternating training of potentials and GFlowNet policies
  - [corpus] Weak - no direct evidence in corpus papers about online learning of potentials for GFlowNets

## Foundational Learning

- Concept: Boltzmann distribution and energy-based modeling
  - Why needed here: GFlowNets aim to sample objects proportional to exp(-E(x)), so understanding energy-based models is crucial for grasping the training objective
  - Quick check question: What distribution does a GFlowNet aim to sample from, and how is it related to the energy function?

- Concept: Credit assignment in sequential decision making
  - Why needed here: The paper addresses the challenge of identifying which actions contribute to achieving low-energy terminal states, similar to credit assignment problems in RL
  - Quick check question: Why is credit assignment challenging in GFlowNets, and how does partial inference help?

- Concept: Function decomposition and parameterization
  - Why needed here: The core idea involves decomposing a complex function (energy) into simpler components (transition potentials) that can be learned
  - Quick check question: How does decomposing the energy function into transition potentials enable partial inference?

## Architecture Onboarding

- Component map:
  Energy function E(x) -> Potential function ϕθ -> GFlowNet policy PF -> Flow estimator F -> Buffer

- Critical path:
  1. Sample trajectories using current policy
  2. Update potential function using least-squares objective with dropout regularization
  3. Use learned potentials to train GFlowNet policy via modified flow balance objectives
  4. Repeat until convergence

- Design tradeoffs:
  - Energy evaluation vs. learned potentials: Direct energy evaluation provides ground truth but may be expensive or misleading; learned potentials are cheaper but require additional training
  - Variance regularization strength: Stronger regularization produces smoother potentials but may reduce their ability to capture important transitions
  - Online vs. offline potential learning: Online learning adapts to policy changes but may be less stable; offline learning could be more stable but less adaptive

- Failure signatures:
  - Potentials fail to approximate energy: Large decomposition error E(x) - Φθ(τ) across trajectories
  - Sparse or uninformative potentials: Most potentials are near zero or show little variation along trajectories
  - Training instability: Oscillations in policy performance or potential values during training

- First 3 experiments:
  1. Implement LED-GFN on the bag generation task and compare with GFlowNet and FL-GFN baselines on mode discovery
  2. Test LED-GFN on the molecule generation task, measuring both diversity (Tanimoto similarity) and quality (average top-100 score)
  3. Perform ablation study on the regularization strength (dropout probability) to find optimal variance reduction for potentials

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal regularization strength λ for the potential function to minimize variance while preserving energy decomposition accuracy?
- Basis in paper: [explicit] The paper mentions using λ = 0.1 for trajectories with length < 10 and λ = 0.2 otherwise, but does not provide a systematic study of the optimal value
- Why unresolved: The paper only uses fixed values based on trajectory length, without exploring the full parameter space or analyzing the impact on performance
- What evidence would resolve it: An ablation study varying λ across a range of values (e.g., 0.1-0.9) and measuring its effect on both energy decomposition accuracy and GFlowNet training performance

### Open Question 2
- Question: How does LED-GFN perform compared to model-based GFlowNet approaches when the energy function is expensive to evaluate?
- Basis in paper: [inferred] The paper mentions that LED-GFN avoids evaluating intermediate state energies, which is a key motivation, but does not directly compare to model-based approaches
- Why unresolved: The paper focuses on comparing LED-GFN to FL-GFN and other variants, but does not include model-based GFlowNet baselines in its experiments
- What evidence would resolve it: A head-to-head comparison of LED-GFN vs model-based GFlowNet on tasks with expensive energy evaluations, measuring both performance and computational cost

### Open Question 3
- Question: Can the potential function be generalized across different tasks or domains, or does it need to be learned from scratch each time?
- Basis in paper: [inferred] The paper presents LED-GFN as a task-specific method that learns potentials online during training, but does not explore transferability
- Why unresolved: The paper does not investigate whether potentials learned on one task can be transferred to similar tasks, or whether there are common patterns in the potential functions across domains
- What evidence would resolve it: Experiments showing the performance of LED-GFN when initializing with potentials pre-trained on related tasks, or analyzing the similarity of learned potentials across different domains

### Open Question 4
- Question: What is the theoretical relationship between the variance of the learned potentials and the quality of the final GFlowNet policy?
- Basis in paper: [explicit] The paper argues that minimizing potential variance produces "informative local credits" but does not provide a formal theoretical analysis
- Why unresolved: The paper provides empirical evidence that regularization helps, but does not establish a theoretical link between potential variance and policy quality
- What evidence would resolve it: A theoretical analysis proving bounds on the policy performance as a function of the variance of the learned potentials, or a more extensive empirical study correlating potential variance with downstream performance across many tasks

## Limitations

- The decomposition approach relies heavily on the assumption that terminal energies can be accurately approximated by summing transition potentials, which may not hold for all energy landscapes.
- The variance regularization technique could potentially oversmooth the potentials and reduce their ability to distinguish between important and unimportant transitions.
- The online learning of potentials within the GFlowNet training loop introduces additional hyperparameters and potential instability that may affect reproducibility.

## Confidence

- **High Confidence**: The core mathematical framework of energy decomposition and its connection to GFlowNet training objectives is well-established and rigorously proven.
- **Medium Confidence**: The effectiveness of variance regularization for improving local credit assignment is supported by theoretical arguments but would benefit from more extensive empirical validation.
- **Medium Confidence**: The experimental results show consistent improvements across multiple tasks, though the relatively small number of random seeds and comparison with only a few baselines limits generalizability.

## Next Checks

1. **Ablation Study on Regularization**: Systematically vary the dropout probability and regularization strength to identify the optimal balance between variance reduction and potential expressiveness.
2. **Energy Decomposition Error Analysis**: Quantitatively measure the decomposition error (E(x) - Φθ(τ)) across different tasks to understand when and why the approximation breaks down.
3. **Cross-Domain Transfer**: Test LED-GFN on energy functions from domains not represented in the current experiments (e.g., protein structure prediction) to evaluate robustness across different energy landscapes.