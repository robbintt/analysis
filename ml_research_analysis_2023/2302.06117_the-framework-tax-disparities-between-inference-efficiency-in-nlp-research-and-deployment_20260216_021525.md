---
ver: rpa2
title: 'The Framework Tax: Disparities Between Inference Efficiency in NLP Research
  and Deployment'
arxiv_id: '2302.06117'
source_url: https://arxiv.org/abs/2302.06117
tags:
- latency
- framework
- batch
- operations
- hardware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a phenomenon called the "framework tax" where
  improvements in hardware speed and reductions in computational operations do not
  translate to improved inference latency due to bottlenecks introduced by deep learning
  frameworks. The authors conduct a systematic investigation across multiple GPU generations,
  model architectures (ResNet and BERT variants), and frameworks (PyTorch, TorchScript,
  ONNX Runtime) to demonstrate that inference workloads are increasingly framework-bound
  rather than compute-bound at small batch sizes.
---

# The Framework Tax: Disparities Between Inference Efficiency in NLP Research and Deployment

## Quick Facts
- **arXiv ID**: 2302.06117
- **Source URL**: https://arxiv.org/abs/2302.06117
- **Reference count**: 17
- **Primary result**: Hardware improvements don't proportionally reduce inference latency due to framework overhead, with PyTorch showing 93.35% framework overhead at batch size 1 for ResNet-50 on RTX-8000 GPU.

## Executive Summary
This paper identifies a phenomenon called the "framework tax" where improvements in hardware speed and reductions in computational operations do not translate to improved inference latency due to bottlenecks introduced by deep learning frameworks. Through systematic investigation across multiple GPU generations, model architectures, and frameworks, the authors demonstrate that inference workloads are increasingly framework-bound rather than compute-bound at small batch sizes. They show that framework overhead becomes a larger proportion of total latency as hardware improves, creating a growing tax that prevents hardware improvements from delivering expected performance gains in real-world deployment settings.

## Method Summary
The authors conduct systematic benchmarking of ResNet and BERT model variants across different batch sizes, frameworks (PyTorch eager, TorchScript, ONNX Runtime), and hardware platforms (RTX-8000, V100, A100 GPUs). They measure inference latency, GPU utilization, and calculate framework overhead percentage by profiling CPU and GPU execution times. The experiments compare FP32 and FP16 precision across eager execution and optimized runtimes, with particular focus on batch size 1 scenarios where framework overhead is most pronounced.

## Key Results
- Framework overhead reaches 93.35% at batch size 1 for ResNet-50 on RTX-8000 GPU in PyTorch
- Increasing model width can improve utilization without affecting latency for framework-bound models
- Newer GPUs with Tensor Core support show higher framework-boundness than older GPUs lacking FP16 support
- ONNX Runtime consistently outperforms PyTorch eager execution at small batch sizes
- Hardware improvements paradoxically worsen framework-bound behavior as GPU compute speed increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "framework tax" emerges because GPU kernel dispatch operations become bottlenecked by CPU overhead, not compute capacity.
- Mechanism: As GPU hardware accelerates tensor operations, the time spent on CPU framework operations (kernel launches, graph construction, synchronization) becomes proportionally larger relative to actual GPU computation time, creating a fixed latency floor that dominates at small batch sizes.
- Core assumption: CPU-GPU communication overhead remains relatively constant while GPU compute speed increases, creating growing imbalance.
- Evidence anchors:
  - "Profiling shows that this suboptimal utilization is due to the rapid execution of small computer kernels is not enough to saturate the fixed cost framework overhead from time spent on CPU operations to launch appropriate kernels, construct computational graphs, handle control flow, and device synchronization."
  - "When the execution of almost all GPU kernel computation is being blocked on framework operations, the model's execution becomes framework-bound."

### Mechanism 2
- Claim: Model width can be increased without affecting latency for framework-bound models because the total execution time is dominated by fixed framework overhead.
- Mechanism: In framework-bound regimes, total latency is constant regardless of the number of operations performed. Therefore, doubling the width of layers does not increase latency since the framework overhead remains the bottleneck.
- Core assumption: The framework overhead is truly fixed and not affected by model width changes.
- Evidence anchors:
  - "Counter-intuitively, the wider variations of ResNet-50 and BERT see no increase in latency at low batch sizes. As this computation is framework bound, total runtime is constant despite the wider operations requiring more floating point operations."
  - "In practice, the hidden dimensions of both BERT and ResNet-50's largest and most computationally intensive linear and convolution layers can be doubled without affecting the single-example latency speed."

### Mechanism 3
- Claim: Hardware improvements paradoxically worsen framework-bound behavior because faster GPUs complete kernels more quickly, leaving relatively more time spent on CPU dispatch operations.
- Mechanism: As GPU computational throughput increases, the ratio of GPU compute time to CPU framework overhead decreases, making the framework overhead a larger proportion of total execution time, especially at small batch sizes.
- Core assumption: CPU framework overhead remains relatively constant while GPU speed increases.
- Evidence anchors:
  - "We observe that GPUs that lack Tensor Core support for half precision operations, such as the 1080Ti, are notably slower and less framework-bound than newer GPUs."
  - "These observations indicate that framework bounds on model execution will continue to worsen as hardware improves unless commensurate improvements are made to deep learning frameworks."

## Foundational Learning

- Concept: Difference between framework-bound and compute-bound execution
  - Why needed here: The paper's core argument depends on distinguishing when total latency is dominated by framework overhead versus actual computation time.
  - Quick check question: In framework-bound execution, does increasing the number of operations increase latency?

- Concept: GPU kernel dispatch and CPU-GPU synchronization
  - Why needed here: Understanding the mechanism of framework overhead requires knowing how frameworks launch operations on accelerators.
  - Quick check question: What happens during the CPU-GPU synchronization step in framework execution?

- Concept: Batch size effects on GPU utilization
  - Why needed here: The paper shows framework boundedness is most prominent at small batch sizes, which relates to GPU kernel size and utilization.
  - Quick check question: Why do small batch sizes lead to poor GPU utilization?

## Architecture Onboarding

- Component map: Model architectures (ResNet, BERT variants) → Deep learning frameworks (PyTorch eager, TorchScript, ONNX Runtime) → GPU hardware across multiple generations (Pascal, Turing, Ampere)
- Critical path: Model definition → Framework overhead (CPU operations) → GPU kernel dispatch → GPU execution → Result synchronization → Framework overhead (CPU operations)
- Design tradeoffs: Eager execution frameworks offer flexibility but incur high overhead; static runtimes reduce overhead but sacrifice dynamic features; model width scaling can improve utilization without latency cost in framework-bound regimes.
- Failure signatures: When increasing hardware performance doesn't reduce latency proportionally, when models exhibit constant latency across different batch sizes, or when framework overhead dominates total execution time (as measured by profiling).
- First 3 experiments:
  1. Profile a ResNet-50 model in PyTorch at batch sizes 1, 8, 16, 32 to measure framework overhead percentage
  2. Compare latency of BERT-Base in PyTorch vs TorchScript vs ONNX Runtime at batch size 1
  3. Test ResNet-50 latency on different GPU generations (RTX 2080Ti vs A100) to observe framework-bound behavior changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term trends in framework tax as hardware continues to improve - will framework overhead eventually become negligible at all batch sizes?
- Basis in paper: The authors note that "these inefficiencies are becoming more apparent as hardware accelerator speeds increase" and show that faster GPUs like the A100 remain framework bound at larger batch sizes than older GPUs
- Why unresolved: The paper only examines current hardware generations and doesn't project how framework improvements or hardware advances might interact over time.
- What evidence would resolve it: Systematic benchmarking across multiple future GPU generations with corresponding framework updates, measuring framework overhead ratios over time.

### Open Question 2
- Question: How do framework taxes differ across different types of models beyond CNNs and transformers, such as recurrent architectures or hybrid models?
- Basis in paper: The study focuses specifically on ResNet and BERT variants but acknowledges this is "the rapidly growing space of models" - suggesting other architectures may behave differently
- Why unresolved: The paper explicitly states "An exhaustive comparison of the rapidly growing space of models" is out of scope, limiting conclusions to just two major architectural paradigms.
- What evidence would resolve it: Systematic profiling of diverse model families (RNNs, GNNs, hybrid architectures) across the same framework/hardware combinations to identify whether framework taxes manifest differently based on computational patterns.

### Open Question 3
- Question: What specific framework optimizations would most effectively reduce framework taxes at small batch sizes?
- Basis in paper: The authors discuss various optimization strategies like operator fusion and graph rewrites but don't identify which would be most impactful
- Why unresolved: While the paper identifies that framework overhead is the core problem, it doesn't prioritize or quantify which optimization approaches would yield the greatest latency improvements
- What evidence would resolve it: Controlled experiments testing individual optimization techniques (operator fusion granularity, memory layout optimizations, kernel launch strategies) with ablation studies measuring their impact on framework overhead percentages across different model types.

## Limitations

- The analysis is based on specific hardware-software combinations (PyTorch 1.12.1, CUDA 11.6, RTX-8000, V100, A100 GPUs), limiting generalizability
- The study focuses on ResNet and BERT architectures, and findings may not extend to all model families or newer architectures like Mamba or state-space models
- The framework overhead measurements rely on careful profiling, but the exact methodology for isolating CPU versus GPU time is not fully specified

## Confidence

- **High**: The empirical observation that hardware improvements do not proportionally reduce inference latency at small batch sizes
- **Medium**: The claim that framework overhead becomes a larger proportion of total latency as hardware improves
- **Medium**: The recommendation to use inference runtimes and scale model width for framework-bound models

## Next Checks

1. Replicate framework overhead measurements on a different GPU generation (e.g., RTX 4090) to verify the trend continues across newer hardware
2. Profile framework overhead breakdown by operation type to identify which framework components contribute most heavily to the tax
3. Test whether framework overhead patterns persist across different model architectures beyond ResNet and BERT (e.g., ViT, Mamba)