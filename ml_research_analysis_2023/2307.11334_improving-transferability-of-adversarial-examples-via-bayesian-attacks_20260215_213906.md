---
ver: rpa2
title: Improving Transferability of Adversarial Examples via Bayesian Attacks
arxiv_id: '2307.11334'
source_url: https://arxiv.org/abs/2307.11334
tags:
- adversarial
- bayesian
- examples
- input
- transferability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper improves the transferability of adversarial examples
  by incorporating Bayesian formulations for both model parameters and inputs. The
  key idea is to introduce randomness into both the substitute model and input during
  the iterative attack process, enabling joint diversification.
---

# Improving Transferability of Adversarial Examples via Bayesian Attacks

## Quick Facts
- **arXiv ID**: 2307.11334
- **Source URL**: https://arxiv.org/abs/2307.11334
- **Reference count**: 40
- **Primary result**: Proposed Bayesian attack method achieves 19.14% improvement in average success rate on ImageNet and 2.08% improvement on CIFAR-10

## Executive Summary
This paper addresses the challenge of improving adversarial transferability by introducing a Bayesian framework that incorporates uncertainty into both model parameters and inputs. The key insight is that by sampling from posterior distributions over model parameters and inputs, the attack can achieve greater diversity and thus better generalization across different victim models. The authors demonstrate that this approach significantly outperforms existing state-of-the-art methods on both ImageNet and CIFAR-10 datasets, with improvements of 19.14% and 2.08% respectively in average success rates across multiple victim models.

## Method Summary
The proposed method samples from posterior distributions over model parameters and inputs, then maximizes the mean prediction loss across these samples to generate adversarial examples. It uses Monte Carlo sampling with M parameter samples and S input perturbations, and can optionally employ SWAG (Stochastic Weight Averaging with Gaussians) for more accurate posterior approximation. The approach also includes a fine-tuning phase that encourages flat minima in both parameter and input spaces to further improve transferability. The attack is based on I-FGSM with iterative updates and ℓ∞ constraints.

## Key Results
- 19.14% improvement in average success rate on ImageNet across 11 victim models compared to state-of-the-art methods
- 2.08% improvement in average success rate on CIFAR-10 across 6 victim models
- SWAG approximation provides 8.16% improvement in success rates when using single-step attacks
- Fine-tuning with flat minima objective further enhances transferability across both datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Introducing Bayesian formulations for both model parameters and input during the attack process enhances adversarial transferability by increasing diversity in both the model and input spaces.
- **Mechanism**: The method samples from distributions over model parameters and inputs, then maximizes the mean prediction loss across these samples. This creates a joint diversification effect that improves transferability.
- **Core assumption**: Sampling from posterior distributions over parameters and inputs captures the variability that exists across different models and preprocessing pipelines, making adversarial examples more generalizable.
- **Break condition**: If the posterior distributions poorly approximate the true variability in model parameters and inputs, the diversity benefits would diminish.

### Mechanism 2
- **Claim**: Fine-tuning the Bayesian model with an optimization objective that encourages flat minima in both parameter and input spaces improves transferability.
- **Mechanism**: The fine-tuning process optimizes parameters to maximize the worst-case performance across the posterior distributions, which is shown to encourage flat minima. Flat minima are known to improve generalization.
- **Core assumption**: Flat minima in both parameter and input spaces correlate with better transferability of adversarial examples across different models.
- **Break condition**: If the flat minima optimization becomes too computationally expensive or the approximation breaks down, the fine-tuning benefits may not materialize.

### Mechanism 3
- **Claim**: Using advanced approximations of the posterior distribution (SWAG) over model parameters and inputs further enhances adversarial transferability compared to simple isotropic Gaussian assumptions.
- **Mechanism**: SWAG decomposes the covariance matrix into diagonal, low-rank, and scaled identity terms, providing a more flexible and data-driven approximation of the posterior distribution.
- **Core assumption**: More accurate posterior approximations lead to better diversity in the sampled models and inputs, which translates to improved transferability.
- **Break condition**: If the SWAG approximation becomes computationally prohibitive or fails to capture the true posterior structure, the performance gains would diminish.

## Foundational Learning

- **Concept: Bayesian inference and posterior distributions**
  - Why needed here: The entire method is built on Bayesian formulations for both model parameters and inputs. Understanding how to compute and sample from posterior distributions is fundamental.
  - Quick check question: What is the difference between a prior distribution and a posterior distribution in Bayesian inference?

- **Concept: Adversarial examples and transferability**
  - Why needed here: The paper's goal is to improve the transferability of adversarial examples, which requires understanding how adversarial examples are generated and why transferability is challenging.
  - Quick check question: Why are adversarial examples typically less effective when attacking models with different architectures?

- **Concept: Optimization techniques and gradient-based methods**
  - Why needed here: The attack process involves optimizing perturbations to maximize prediction loss, requiring knowledge of gradient-based optimization methods like I-FGSM.
  - Quick check question: How does the I-FGSM algorithm differ from the basic FGSM algorithm?

## Architecture Onboarding

- **Component map**: Input preprocessing -> Bayesian posterior approximation (SWAG/isotropic) -> Monte Carlo sampling engine -> Loss computation and backpropagation -> Fine-tuning optimization -> Adversarial example generation

- **Critical path**: 
  1. Sample parameters from posterior distribution
  2. Sample input perturbations from input distribution
  3. Compute prediction loss across all samples
  4. Backpropagate and update adversarial perturbations
  5. Clip perturbations to stay within ℓ∞ constraint
  6. Repeat for multiple iterations

- **Design tradeoffs**:
  - Sampling more models (M) and inputs (S) improves diversity but increases computational cost
  - Using SWAG provides better posterior approximation but adds complexity
  - Fine-tuning improves transferability but requires additional training data and computation

- **Failure signatures**:
  - Low success rates across victim models indicate poor diversity or posterior approximation
  - High computational cost with marginal gains suggests sampling parameters are too large
  - Training instability during fine-tuning may indicate poor hyper-parameter choices

- **First 3 experiments**:
  1. Implement basic Bayesian attack with isotropic Gaussian posterior (M=5, S=5) on CIFAR-10 with ResNet-18
  2. Add SWAG-based posterior approximation for parameters only, compare success rates
  3. Implement fine-tuning with flat minima objective, test on both CIFAR-10 and ImageNet

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Bayesian attack framework perform against adversarial defenses beyond adversarial training, such as certified defenses or detection-based methods?
- Basis in paper: The paper mentions conducting experiments on robust models obtained through randomized smoothing, which is a certified defense method, but does not explore other types of defenses.
- Why unresolved: The paper primarily focuses on attacking models trained with adversarial training, leaving the effectiveness of the method against other defense mechanisms unexplored.
- What evidence would resolve it: Comprehensive experiments testing the method's transferability against a wide range of defensive models, including those based on certified defenses, detection methods, and other advanced techniques.

### Open Question 2
- Question: What is the impact of the number of Monte Carlo samples (M and S) on the trade-off between attack success rate and computational efficiency in the Bayesian attack framework?
- Basis in paper: The paper discusses the effect of varying M and S on the success rate of adversarial examples but does not provide a detailed analysis of the computational cost associated with different sample sizes.
- Why unresolved: While the paper demonstrates that increasing M and S can enhance transferability, it does not quantify the computational overhead or provide guidelines for selecting optimal values.
- What evidence would resolve it: Empirical studies that measure the computational time and resources required for different M and S values, along with an analysis of the diminishing returns in success rate as the number of samples increases.

### Open Question 3
- Question: How does the proposed method's performance compare to ensemble-based attacks that use a fixed set of diverse models, in terms of both success rate and computational efficiency?
- Basis in paper: The paper mentions that ensemble-based methods are closely related to the proposed approach and that the Bayesian framework can be seen as a statistical ensemble of infinitely many models.
- Why unresolved: The paper does not provide a direct comparison between the Bayesian attack and traditional ensemble-based attacks, leaving the relative strengths and weaknesses of each approach unclear.
- What evidence would resolve it: Comparative experiments that evaluate both the success rate and computational efficiency of the Bayesian attack and traditional ensemble-based attacks under similar conditions.

## Limitations
- Computational complexity increases significantly with larger numbers of Monte Carlo samples, potentially limiting real-world applicability
- The method's effectiveness against advanced adaptive defenses beyond adversarial training remains unexplored
- Performance gains may diminish with very large numbers of samples, suggesting a trade-off between diversity and efficiency

## Confidence
- **High**: The core claim that Bayesian formulations improve adversarial transferability is well-supported by significant performance improvements across multiple victim models and datasets
- **Medium**: The specific mechanisms involving SWAG approximation and fine-tuning with flat minima objectives are promising but lack extensive ablation studies to quantify their individual contributions
- **Low**: The claim about flat minima in input spaces improving transferability is theoretically sound but lacks comprehensive empirical validation across different attack scenarios

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of Bayesian parameter sampling, input diversification, SWAG approximation, and fine-tuning to overall performance gains.

2. Test the method's robustness against adaptive defenses and varying attack strengths to assess its practical security implications beyond transferability metrics.

3. Investigate the computational efficiency trade-offs by systematically varying the number of Monte Carlo samples (M and S) and measuring the corresponding impact on success rates and execution time.