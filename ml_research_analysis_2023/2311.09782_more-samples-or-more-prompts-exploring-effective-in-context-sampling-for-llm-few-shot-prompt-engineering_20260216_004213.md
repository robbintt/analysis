---
ver: rpa2
title: More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM
  Few-Shot Prompt Engineering
arxiv_id: '2311.09782'
source_url: https://arxiv.org/abs/2311.09782
tags:
- data
- strategy
- arxiv
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces In-Context Sampling (ICS), a method to improve
  LLM predictions by constructing multiple ICL prompt inputs and selecting the most
  confident prediction. Unlike standard ICL that uses a single set of demonstrations,
  ICS augments predictions by querying the LLM multiple times with different demonstration
  combinations and then applying majority voting to identify the most confident label.
---

# More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering

## Quick Facts
- arXiv ID: 2311.09782
- Source URL: https://arxiv.org/abs/2311.09782
- Reference count: 10
- Key outcome: ICS improves LLM predictions by using multiple ICL demonstration sets with majority voting, showing consistent accuracy gains across NLI tasks.

## Executive Summary
This paper introduces In-Context Sampling (ICS), a method that improves few-shot learning performance in large language models (LLMs) by constructing multiple ICL prompt inputs and selecting the most confident prediction. Unlike standard ICL that uses a single set of demonstrations, ICS augments predictions by querying the LLM multiple times with different demonstration combinations and then applying majority voting to identify the most confident label. Experiments with FlanT5-XL and Mistral-7B on e-SNLI, Multi-NLI, and ANLI datasets show that ICS consistently improves prediction accuracy and robustness compared to baseline ICL. The optimal configuration uses 100 demonstration candidates and 10 augmented ICL combinations.

## Method Summary
ICS constructs multiple ICL prompt inputs by sampling demonstration candidates from the unlabeled pool, generating augmented ICL combinations, and then querying the LLM multiple times for the same test instance. The method uses majority voting across these predictions to select the most confident label. The pipeline involves three main steps: subsampling demonstration candidates from the unlabeled pool, augmenting ICL combinations from these candidates, and verifying the most confident label through voting mechanisms. The approach can use either random sampling or diversity-based sampling strategies for demonstration selection.

## Key Results
- ICS consistently improves prediction accuracy across e-SNLI, Multi-NLI, and ANLI datasets compared to baseline ICL
- The optimal configuration uses 100 demonstration candidates and 10 augmented ICL combinations per test example
- Diversity-based sampling strategy further boosts ICS performance compared to random sampling
- Standard deviation of predictions decreases as the number of augmented combinations increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple ICL prompt inputs provide complementary knowledge representations that converge on more confident predictions
- Mechanism: When an LLM processes different ICL demonstrations, each set activates different knowledge pathways and reasoning patterns. The convergence of predictions across multiple inputs indicates higher confidence in the underlying knowledge structure
- Core assumption: Different ICL demonstrations activate distinct but overlapping knowledge representations in the LLM
- Evidence anchors:
  - [abstract] "We hypothesize that different ICL demonstrations provide LLMs with distinct knowledge about the task, leading to disparate understanding and predictions for the same data"
  - [section 3] "We hypothesize that different sets of ICL demonstrations provide LLMs with disparate knowledge about the task; thus, LLMs may alter their predictions for the same data x given different ICL prompt inputs, but the variation of predictions will eventually converge to a most confident prediction"
  - [corpus] Weak evidence - corpus lacks specific studies on knowledge pathway activation differences

### Mechanism 2
- Claim: Majority voting across multiple ICL predictions reduces variance and improves prediction robustness
- Mechanism: By querying the LLM multiple times with different demonstration combinations and applying majority voting, the method filters out noise and outlier predictions, similar to ensemble methods in traditional machine learning
- Core assumption: Individual ICL predictions contain both signal and noise, and the signal component is consistent across different demonstration sets
- Evidence anchors:
  - [abstract] "ICS augments predictions by querying the LLM multiple times with different demonstration combinations and then applying majority voting to identify the most confident label"
  - [section 3.3] "Once we acquire a set of labels from the aforementioned ICS step, a few verification algorithms can be applied to find the most confident label. Here, we obtain the most confident prediction with a majority vote"
  - [section 4.2] "We observe that the standard deviation reduces the most when k = 10 for both models"

### Mechanism 3
- Claim: Data diversity in demonstration sampling improves the representativeness of knowledge coverage
- Mechanism: Using diversity-based sampling strategies (like core-set approaches) ensures that the selected demonstrations span the semantic space of the unlabeled data, providing more comprehensive task understanding
- Core assumption: Demonstrations that are diverse in the embedding space will provide more comprehensive task understanding than random samples
- Evidence anchors:
  - [section 3.1] "Our strategy calculates the cosine similarity for each data xi, encoded with sentence-transformer... Subsequently, we rank the candidates by similarity score and retrieve n examples with the same interval, ensuring the sample set's diversity"
  - [section 4.3] "the diversity-based strategy for demonstration candidate sampling and combination augmentation can effectively increase ICS performance gain"
  - [corpus] Weak evidence - corpus lacks specific studies on diversity-based demonstration sampling for ICL

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: ICS builds directly on ICL by extending from single to multiple demonstration sets
  - Quick check question: What is the key difference between zero-shot and few-shot learning in LLMs?

- Concept: Active Learning and Sample Selection
  - Why needed here: ICS uses sampling strategies similar to active learning to select demonstration candidates
  - Quick check question: How does core-set selection differ from random sampling in active learning?

- Concept: Ensemble Methods and Majority Voting
  - Why needed here: ICS uses majority voting across multiple ICL predictions, similar to ensemble techniques
  - Quick check question: Why does majority voting often improve prediction robustness in ensemble methods?

## Architecture Onboarding

- Component map: Demonstration Candidate Sampling -> ICL Combination Augmentation -> Confident Label Verification
- Critical path:
  1. Sample n demonstration candidates from unlabeled pool
  2. Generate k ICL combinations from candidates
  3. Query LLM k times for same test instance
  4. Apply voting mechanism to select most confident prediction
- Design tradeoffs:
  - Sample size n vs. annotation cost: Larger n provides more diversity but requires more oracle annotations
  - Number of combinations k vs. computation: More combinations improve confidence but increase inference cost
  - Sampling strategy (random vs. diversity-based) vs. performance: Diversity-based improves performance but adds sampling complexity
- Failure signatures:
  - Minimal accuracy improvement: Demonstration sets too similar or LLM already overfits task
  - Increased variance: Voting mechanism not effective (predictions too correlated)
  - High computational cost: k too large relative to performance gains
- First 3 experiments:
  1. Baseline comparison: Run standard ICL vs. ICS with n=50, k=3 on e-SNLI to verify basic mechanism
  2. Diversity impact: Compare random vs. diversity-based sampling with n=100, k=10 on Multi-NLI
  3. Sensitivity analysis: Vary k from 1 to 20 with fixed n=100 on ANLI to find optimal combination count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the number of demonstration candidates (n) and the number of ICL combinations (k) for maximizing ICS performance across different tasks?
- Basis in paper: [explicit] The paper investigates the influence of different sample sizes (n) and ICL combination amounts (k) on performance.
- Why unresolved: The paper shows that n=100 and k=10 are optimal for the tested NLI tasks, but the generalizability to other task types and model architectures remains unexplored.
- What evidence would resolve it: Systematic experiments varying n and k across diverse NLP tasks (e.g., summarization, QA, classification) and LLM families, measuring both accuracy and computational efficiency.

### Open Question 2
- Question: How does the choice of verification algorithm (beyond majority voting) impact the effectiveness of ICS?
- Basis in paper: [explicit] The paper mentions that different verification algorithms could be applied to find the most confident label, but only uses majority voting.
- Why unresolved: The paper's focus is on demonstrating ICS's validity rather than exhaustively comparing verification strategies.
- What evidence would resolve it: Comparative studies of verification methods (e.g., weighted voting, confidence thresholds, ensemble techniques) on ICS performance and robustness.

### Open Question 3
- Question: Can ICS be effectively extended to multi-class classification tasks beyond the three-class NLI setup?
- Basis in paper: [explicit] Experiments are limited to three-class NLI tasks, with mention of one QA dataset.
- Why unresolved: The paper doesn't explore ICS's scalability to tasks with more classes or different label distributions.
- What evidence would resolve it: Experiments applying ICS to multi-class tasks (e.g., sentiment analysis with 5 classes, topic classification) and analyzing performance trends as class count increases.

## Limitations

- Limited task scope: Experiments only cover NLI tasks, not other NLP domains like question answering or text classification
- Oracle dependency: Requires labeled demonstration candidates, creating practical deployment challenges
- Computational overhead: Multiple LLM queries per instance may be prohibitive for large-scale applications
- Scalability uncertainty: Does not investigate performance with smaller, more practical LLMs or across different model sizes

## Confidence

**High Confidence**: The core mechanism of using multiple ICL demonstration sets with majority voting is technically sound and well-supported by the empirical results. The improvement in accuracy and reduction in variance across datasets is clearly demonstrated.

**Medium Confidence**: The claim that diversity-based sampling provides additional performance gains is supported by the ablation study, but the magnitude of improvement and its consistency across different task types remains uncertain given the limited evaluation scope.

**Low Confidence**: The hypothesis that different ICL demonstrations activate distinct knowledge pathways in LLMs is largely theoretical. While the convergence of predictions is observed, the underlying mechanism connecting demonstration diversity to knowledge representation differences lacks direct empirical validation.

## Next Checks

1. **Generalization Test**: Apply ICS to a diverse set of NLP tasks beyond NLI (e.g., sentiment analysis, text summarization, question answering) to assess whether the performance improvements generalize across different problem types and reasoning requirements.

2. **Scalability Analysis**: Conduct experiments varying model sizes (e.g., testing with smaller models like LLaMA-7B, LLaMA-13B) and measuring both accuracy improvements and computational overhead to determine the practical deployment threshold where ICS becomes cost-effective.

3. **Zero-Shot Candidate Selection**: Implement and evaluate alternative demonstration candidate selection methods that don't require oracle labels (e.g., using model confidence scores, clustering-based selection, or heuristic rules) to address the practical deployment limitation of requiring annotated candidates.