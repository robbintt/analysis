---
ver: rpa2
title: When and How Does Known Class Help Discover Unknown Ones? Provable Understanding
  Through Spectral Analysis
arxiv_id: '2308.05017'
source_url: https://arxiv.org/abs/2308.05017
tags:
- data
- case
- class
- novel
- known
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes novel class discovery (NCD) as a graph-theoretic
  problem, where both labeled and unlabeled data form a similarity graph. The authors
  introduce a new NCD Spectral Contrastive Loss (NSCL) that, when minimized, performs
  spectral decomposition on the graph's adjacency matrix.
---

# When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis

## Quick Facts
- arXiv ID: 2308.05017
- Source URL: https://arxiv.org/abs/2308.05017
- Reference count: 40
- Key outcome: NSCL achieves competitive performance on CIFAR-10 and CIFAR-100 benchmarks, outperforming the best baseline by 10.6% on CIFAR-100-50

## Executive Summary
This paper formalizes novel class discovery (NCD) as a graph-theoretic problem, where both labeled and unlabeled data form a similarity graph. The authors introduce a new NCD Spectral Contrastive Loss (NSCL) that, when minimized, performs spectral decomposition on the graph's adjacency matrix. This approach allows them to derive a provable error bound for NCD and establish sufficient and necessary conditions for perfect discovery. Empirically, NSCL achieves competitive performance on CIFAR-10 and CIFAR-100 benchmarks, outperforming the best baseline by 10.6% on CIFAR-100-50.

## Method Summary
The method introduces NCD Spectral Contrastive Loss (NSCL), which minimizes a spectral decomposition of the augmentation graph's adjacency matrix constructed from both labeled and unlabeled data. The framework assumes augmentation graph construction, adjacency matrix normalization, and minimization of a spectral contrastive loss that decomposes into positive and negative pair terms. The approach is implemented with ResNet-18 and a two-layer MLP projection head, trained using SGD with momentum 0.95, cosine annealing (lr=0.03), weight decay 5e-4, and batch size 512 for 1200 epochs.

## Key Results
- NSCL achieves competitive performance on CIFAR-10 and CIFAR-100 benchmarks
- Outperforms the best baseline by 10.6% on CIFAR-100-50
- Establishes provable error bounds and sufficient/necessary conditions for NCD
- Theoretical framework shows labeled data helps reduce "ignorance space" when semantic relationships exist

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Labeled data helps reduce the "ignorance space" of unlabeled data when there is semantic overlap between known and novel classes
- **Mechanism:** The labeled data provides additional knowledge that projects the ignorance space onto a lower-dimensional subspace, reducing the residual in linear probing
- **Core assumption:** The semantic relationship between known and novel classes can be captured by the linear span of labeled samples' features
- **Evidence anchors:**
  - [abstract] "Our main result (Theorem 5.5) suggests that the linear probing error can be significantly reduced (even to 0) when the linear span of known samples' feature covers the 'ignorance space' of unlabeled data"
  - [section] "The bound of residual in Ineq. (10) is composed of two projections: U ♭⊤ and (I − PL♭). Multiplying the second projection matrix (I − PL♭) further reduces the norm of the ignorance space by considering the extra knowledge from labeled data"
  - [corpus] Weak - no direct citations found
- **Break condition:** If the labeled data provides no semantic overlap with novel classes, or if the semantic relationship is too complex to be captured by linear span

### Mechanism 2
- **Claim:** Minimizing the NCD Spectral Contrastive Loss (NSCL) is equivalent to performing spectral decomposition on the graph's adjacency matrix
- **Mechanism:** The loss function decomposes into terms that encourage positive pairs to be close and negative pairs to be far apart, which corresponds to the top-k singular vectors of the adjacency matrix
- **Core assumption:** The augmentation graph constructed from labeled and unlabeled data has a meaningful structure that can be captured by spectral decomposition
- **Evidence anchors:**
  - [abstract] "Minimizing this objective is equivalent to factorizing the graph's adjacency matrix, which allows us to derive a provable error bound and provide the sufficient and necessary condition for NCD"
  - [section] "According to the Eckart–Young–Mirsky theorem (Eckart & Young, 1936), the minimizer of this loss function is F ∗ ∈ RN ×k such that F ∗F ∗⊤ contains the top-k components of ˙A's SVD decomposition"
  - [corpus] Weak - no direct citations found
- **Break condition:** If the augmentation graph is too sparse or noisy, the spectral decomposition may not capture meaningful structure

### Mechanism 3
- **Claim:** The coverage between the ignorance space and extra knowledge from labeled data determines the quality of novel class discovery
- **Mechanism:** The cosine distance between the ignorance space (U ♭⊤⃗ y) and the extra knowledge (L♭) determines how much of the unknown information can be explained by the known classes
- **Core assumption:** The ignorance space and extra knowledge can be meaningfully represented as vectors in the same space
- **Evidence anchors:**
  - [abstract] "The theoretical framework shows that labeled data helps reduce the 'ignorance space' of unlabeled data when there is a meaningful semantic relationship between known and novel classes, formalized through the coverage between the ignorance space and extra knowledge from labeled data"
  - [section] "Our Theorem 5.6 thus meaningfully shows that the linear probing error can be bounded more tightly as κ(⃗ y) increases (i.e., when labeled data provides more useful information for the unlabeled data)"
  - [corpus] Weak - no direct citations found
- **Break condition:** If the ignorance space and extra knowledge are orthogonal, the coverage will be zero and labeled data provides no benefit

## Foundational Learning

- **Concept: Spectral graph theory**
  - Why needed here: The paper uses spectral decomposition of the adjacency matrix to learn representations for novel class discovery
  - Quick check question: What is the relationship between the eigenvectors of a graph's adjacency matrix and its clustering structure?

- **Concept: Contrastive learning**
  - Why needed here: The NSCL loss is a form of contrastive learning that encourages similar samples to have similar representations
  - Quick check question: How does the NSCL loss differ from standard contrastive losses like SimCLR or MoCo?

- **Concept: Linear probing**
  - Why needed here: The paper evaluates the quality of learned representations using linear probing on novel classes
  - Quick check question: Why is linear probing a good metric for evaluating representation quality in self-supervised learning?

## Architecture Onboarding

- **Component map:** Input -> Augmentation -> Graph construction -> Adjacency matrix computation -> NSCL loss computation -> Optimization -> Feature extraction -> Linear probing evaluation

- **Critical path:**
  1. Construct augmentation graph from labeled and unlabeled data
  2. Compute adjacency matrix and normalize
  3. Compute NSCL loss from adjacency matrix
  4. Train neural network to minimize NSCL loss
  5. Extract learned representations from unlabeled data
  6. Evaluate using linear probing on novel classes

- **Design tradeoffs:**
  - Tradeoff between α and β: Controls the relative importance of labeled vs unlabeled data
  - Choice of k: Number of singular vectors to keep affects representation quality
  - Graph construction: Different augmentation strategies may yield different results

- **Failure signatures:**
  - Poor performance on novel classes: May indicate insufficient semantic overlap between known and novel classes
  - Unstable training: May indicate issues with graph construction or augmentation strategy
  - High linear probing error: May indicate insufficient coverage between ignorance space and extra knowledge

- **First 3 experiments:**
  1. Train NSCL with different values of α and β to see their effect on performance
  2. Compare NSCL with purely unsupervised counterpart (SCL) trained on Du only
  3. Visualize learned representations using UMAP to verify semantic structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of the augmentation strength parameters (τ₁, τs, τc) on the linear probing error in real-world datasets beyond the toy example?
- Basis in paper: [explicit] The paper derives theoretical bounds based on augmentation parameters in the toy example, showing their impact on the residual.
- Why unresolved: The paper only analyzes these parameters in a simplified toy setting and does not empirically validate their effects on actual benchmark datasets.
- What evidence would resolve it: Empirical studies varying τ parameters on CIFAR-10/100 datasets and measuring corresponding changes in linear probing accuracy.

### Open Question 2
- Question: How does the proposed method perform on datasets with more than two novel classes or more complex class hierarchies?
- Basis in paper: [inferred] The theoretical framework and experiments focus on cases with limited class numbers (5-50 novel classes).
- Why unresolved: The scalability of NSCL to datasets with larger class counts or hierarchical structures is not explored.
- What evidence would resolve it: Experiments on datasets like ImageNet with hundreds of classes, or hierarchical datasets where sub-class relationships matter.

### Open Question 3
- Question: What is the effect of labeled data quality and diversity on novel class discovery performance?
- Basis in paper: [explicit] The paper mentions that labeled data with "undesirable" features (e.g., gray cylinders) can be harmful, suggesting importance of labeled data quality.
- Why unresolved: The paper doesn't systematically explore how varying the relevance or diversity of labeled classes affects NCD performance.
- What evidence would resolve it: Controlled experiments varying labeled data quality/diversity on benchmark datasets and measuring NCD accuracy.

## Limitations
- Theoretical framework relies heavily on linear span assumptions about semantic relationships
- Paper lacks empirical validation of the theoretical bounds, particularly regarding coverage metrics
- Augmentation strategy details are sparse, making reproducibility challenging

## Confidence
- Theoretical framework (Medium): The spectral analysis approach is mathematically sound, but the assumptions about linear span coverage may be overly restrictive
- Empirical results (High): Strong quantitative performance on CIFAR benchmarks with clear improvements over baselines
- Mechanism claims (Medium): The three proposed mechanisms are logically consistent but lack direct empirical validation of the intermediate steps

## Next Checks
1. **Coverage quantification**: Measure the actual coverage between ignorance space and extra knowledge on CIFAR datasets and correlate with performance improvements to validate the theoretical claims
2. **Augmentation sensitivity**: Systematically vary augmentation strategies and quantify their impact on both the spectral decomposition quality and final clustering performance
3. **Transferability test**: Apply NSCL to non-image domains (e.g., tabular data from the corpus) to assess whether the linear span assumption holds across different data types and whether performance degrades as expected when semantic overlap is limited