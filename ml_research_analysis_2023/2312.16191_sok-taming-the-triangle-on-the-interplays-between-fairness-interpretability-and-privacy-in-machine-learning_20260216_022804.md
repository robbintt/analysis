---
ver: rpa2
title: 'SoK: Taming the Triangle -- On the Interplays between Fairness, Interpretability
  and Privacy in Machine Learning'
arxiv_id: '2312.16191'
source_url: https://arxiv.org/abs/2312.16191
tags:
- fairness
- privacy
- learning
- explanations
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically surveys the interplays between fairness,
  interpretability, and privacy in machine learning. The authors review tensions and
  synergies between these three desiderata, focusing on supervised learning tasks.
---

# SoK: Taming the Triangle -- On the Interplays between Fairness, Interpretability and Privacy in Machine Learning

## Quick Facts
- arXiv ID: 2312.16191
- Source URL: https://arxiv.org/abs/2312.16191
- Reference count: 40
- Key outcome: This paper systematically surveys the interplays between fairness, interpretability, and privacy in machine learning, identifying tensions and synergies between these three desiderata.

## Executive Summary
This survey paper examines the complex relationships between fairness, interpretability, and privacy in machine learning, particularly in supervised learning contexts. The authors systematically review how these three desirable properties interact, finding both fundamental conflicts and potential synergies. The paper highlights that while these properties are often studied in isolation, they create significant challenges when pursued simultaneously, requiring careful design decisions to balance competing requirements.

## Method Summary
The paper conducts a systematic literature survey, reviewing 40 relevant references to identify and categorize the interactions between fairness, interpretability, and privacy. The authors analyze three pairwise combinations: fairness-interpretability, fairness-privacy, and interpretability-privacy. The survey identifies synergies such as interpretability aiding privacy audits and potential joint enforcement of differential privacy and approximate fairness, while also highlighting fundamental tensions that arise when attempting to satisfy all three desiderata simultaneously.

## Key Results
- Fairness and interpretability intrinsically conflict due to simplicity constraints, making joint enforcement computationally challenging
- Privacy concerns arise from using sensitive attributes for fairness and post-hoc explanations potentially leaking training information
- Differential privacy and approximate fairness can be jointly enforced with tunable trade-offs using Lagrangian dual approaches
- Careful design is needed to handle these concerns while preserving utility, requiring interdisciplinary approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Privacy, fairness, and interpretability each require explicit constraints that trade off with utility, and these constraints interact multiplicatively.
- Mechanism: The paper shows that when two constraints are active, the third must be weakened or the utility drops sharply. For example, enforcing differential privacy increases vulnerability to fairness violations, and enforcing interpretability reduces the model space available for achieving fairness.
- Core assumption: Each desideratum can be modeled as a hard constraint that reduces the feasible set of models.
- Evidence anchors:
  - [abstract] "they were mainly considered in isolation, while in practice they interplay with each other, either positively or negatively."
  - [section IV-A] "it is provably impossible to build ML models strictly respecting a given group fairness constraint while respecting DP."
  - [corpus] Weak, only 0 citations; needs empirical backing.
- Break condition: If any constraint is relaxed to a soft penalty, the trade-off becomes less severe and utility can be preserved.

### Mechanism 2
- Claim: Post-hoc explanations can leak information about training data, creating privacy risks even when the model itself is private.
- Mechanism: Gradient-based explanations expose gradients that encode per-example influence; counterfactual explanations reveal local decision boundaries that can be queried to reconstruct training points.
- Core assumption: The explanation method is a differentiable function of the model parameters and training data.
- Evidence anchors:
  - [section V-A] "Feature-based explanations are leveraged... to perform MIAs" and "example-based explanations allow for dataset reconstruction attacks."
  - [section II-C] "differential privacy... bounds the amount of information the output of a computation leaks."
  - [corpus] Moderate, mentions reconstruction attacks in literature but no direct citations.
- Break condition: If explanations are made differentially private, the utility loss may outweigh the benefit of interpretability.

### Mechanism 3
- Claim: Fairness and differential privacy can be jointly enforced under approximate fairness and approximate DP, with tunable trade-offs.
- Mechanism: Use a Lagrangian dual approach where the fairness violation and privacy loss are penalized jointly, allowing the optimization to find a Pareto frontier.
- Core assumption: The optimization landscape is smooth enough for dual methods to converge to a good trade-off point.
- Evidence anchors:
  - [section IV-B] "it is possible for a DP learning algorithm to output a model approximately satisfying given fairness criteria" and describes Lagrangian dual approaches.
  - [abstract] "we also discuss possible conciliation mechanisms."
  - [corpus] Limited, no citation data yet; relies on described methods.
- Break condition: If the hypothesis class is too large, the exponential mechanism becomes intractable and the joint solution is not computable.

## Foundational Learning

- Concept: Group vs individual fairness definitions
  - Why needed here: The paper's analysis hinges on whether constraints are applied at the group or individual level; some are theoretically incompatible (e.g., pure DP + exact group fairness).
  - Quick check question: What is the difference between statistical parity and individual fairness in terms of the protected entities?

- Concept: Differential privacy mechanisms (Laplace, Gaussian, exponential)
  - Why needed here: Understanding how noise addition and sensitivity scaling work is essential to reason about the trade-offs with fairness and interpretability.
  - Quick check question: How does the sensitivity of a query affect the amount of noise added in the Laplace mechanism?

- Concept: Post-hoc explanation types and their vulnerabilities
  - Why needed here: The paper shows that different explanation forms (gradient, counterfactual, example-based) have different privacy leakage profiles; choosing the right one matters for joint desiderata.
  - Quick check question: Which explanation type is most vulnerable to membership inference attacks and why?

## Architecture Onboarding

- Component map:
  Input -> Fairness module -> Privacy module -> Interpretability module -> Output
- Critical path:
  1. Data sanitization for fairness (if pre-processing)
  2. Train with DP constraints (DP-SGD or PATE)
  3. Apply interpretability constraints or generate explanations
  4. Validate fairness, privacy, and interpretability jointly
- Design tradeoffs:
  - Stronger DP → more noise → larger accuracy drop for minorities
  - More interpretable model → simpler → harder to satisfy fairness
  - Richer explanations → more leakage risk → need DP for explanations
- Failure signatures:
  - Disproportionate accuracy drop in minority subgroups
  - Explanations with high variance indicating membership leakage
  - Optimizer stalls at trivial constant classifier (exact fairness + DP)
- First 3 experiments:
  1. Train a logistic regression with DP-SGD, then measure fairness metrics across subgroups.
  2. Replace the logistic regression with a decision tree, compare interpretability vs fairness trade-off.
  3. Generate gradient-based explanations for the DP model and test membership inference attack success rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we theoretically characterize the fundamental tension between simplicity (as a proxy for interpretability) and statistical fairness in machine learning models?
- Basis in paper: [explicit] The paper states that "Simplicity and fairness intrinsically conflict" and references a framework showing that every non-trivial group-agnostic simplification has a more complex classifier that improves both accuracy and fairness.
- Why unresolved: While the paper establishes this theoretical incompatibility, it does not provide a complete characterization of the underlying mathematical or structural reasons for this conflict across different model classes and fairness definitions.
- What evidence would resolve it: A formal proof showing that for any interpretable model class and any statistical fairness metric, there exists a trade-off curve that cannot be improved without either increasing model complexity or reducing fairness.

### Open Question 2
- Question: What are the precise conditions under which differentially private mechanisms disproportionately affect utility for minority subgroups in machine learning models?
- Basis in paper: [explicit] The paper discusses how DP mechanisms disproportionately affect underrepresented subgroups and mentions that "gradient clipping and random noise addition, the key mechanisms of DP-SGD, disproportionately affect underrepresented subgroups."
- Why unresolved: While empirical observations are provided, the paper does not offer a complete theoretical framework explaining when and why these disparities occur across different data distributions and DP mechanisms.
- What evidence would resolve it: A mathematical analysis showing the relationship between subgroup size, data distribution characteristics, and the magnitude of utility degradation under various DP mechanisms.

### Open Question 3
- Question: Can we design post-hoc explanation methods that are both differentially private and maintain high explanation quality across all subgroups?
- Basis in paper: [explicit] The paper notes that "Providing useful yet privacy-protective explanations remains an open challenge" and discusses how DP disproportionately affects explanation quality for minority groups.
- Why unresolved: While some methods attempt to address this, the paper shows that there is always a trade-off between privacy guarantees and explanation quality, particularly for minority groups.
- What evidence would resolve it: Empirical studies demonstrating explanation methods that achieve strong DP guarantees (ϵ ≤ 1) while maintaining explanation fidelity above 90% across all demographic subgroups in real-world datasets.

## Limitations
- The analysis relies on survey-style synthesis rather than empirical validation, limiting direct verification of claimed incompatibilities
- Confidence in the core mechanisms is low to medium due to absence of quantitative trade-off studies and lack of direct measurements
- The paper does not address how to empirically detect failure signatures or calibrate trade-offs in practice

## Confidence
- Mechanism 1 (fairness vs DP): Low
- Mechanism 2 (privacy leakage via explanations): Medium
- Mechanism 3 (joint DP + fairness): Medium

## Next Checks
1. Recreate the Lagrangian dual experiments for DP + fairness on a standard dataset (e.g., Adult) and measure the Pareto frontier.
2. Implement a gradient-based explanation method on a DP-trained model and run a membership inference attack to quantify leakage.
3. Train interpretable models (e.g., decision trees) under fairness constraints and measure the resulting accuracy gaps across subgroups.