---
ver: rpa2
title: 'Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and Statistical
  Approach'
arxiv_id: '2312.10750'
source_url: https://arxiv.org/abs/2312.10750
tags:
- translation
- chatgpt
- features
- translations
- txtmt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study uses machine learning and multidimensional analysis to
  examine linguistic differences among human, NMT, and ChatGPT translations of diplomatic
  texts. While clustering fails to distinguish the three translation types, supervised
  classifiers achieve high accuracy in their separation.
---

# Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and Statistical Approach

## Quick Facts
- **arXiv ID**: 2312.10750
- **Source URL**: https://arxiv.org/abs/2312.10750
- **Reference count**: 18
- **Primary result**: Machine learning classifiers distinguish human, NMT, and ChatGPT translations with high accuracy, while MDA reveals greater similarity between NMT and ChatGPT than either shows to human translation

## Executive Summary
This study investigates linguistic differences among human, NMT, and ChatGPT translations of Chinese diplomatic texts using machine learning and multidimensional analysis. The research processes 147 spokesperson remarks into 210 text samples using rolling stylometry, then extracts 121 linguistic features for analysis. While unsupervised clustering fails to distinguish the three translation types, supervised classifiers achieve high accuracy. Multi-dimensional analysis reveals five interpretable dimensions, showing that ChatGPT and NMT translations share greater similarity than either does to human translation, particularly in formality and stance expression.

## Method Summary
The study uses rolling stylometry to process Chinese diplomatic texts into 5000-word samples with 500-word overlap, creating 210 text samples across three translation types. It extracts 121 linguistic features using MAT and MFTE, then applies statistical filtering (Kruskal-Wallis H test) to identify 74 significant features. The analysis employs factor analysis with Varimax rotation to obtain five interpretable dimensions, followed by supervised classification (five classifiers) and distance calculations. The methodology is completed with t-SNE visualization to compare translation type distributions in reduced dimensional space.

## Key Results
- Supervised classifiers achieve high accuracy in distinguishing human, NMT, and ChatGPT translations, while clustering fails to separate them
- Multi-dimensional analysis reveals five interpretable dimensions capturing translation differences
- ChatGPT and NMT show greater similarity than either does to human translation across most MDA dimensions
- Distance calculations and t-SNE visualization confirm closer proximity between ChatGPT and NMT translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rolling stylometry with overlapping windows reveals translation type patterns by ensuring each sample contains representative linguistic features
- Mechanism: Concatenating translations into long blocks and sliding a fixed-length window with overlap ensures that each sample captures both high-frequency features and contextual variations across different spokesperson remarks
- Core assumption: Fixed-size blocks (5000 words) with 500-word overlap are sufficient to capture stable linguistic patterns while avoiding stylistic noise from individual speakers
- Evidence anchors:
  - [section] "To address this concern, we resorted to a technique called rolling stylometry (Eder, 2015) to process the textual data. This approach involved the following steps: First, we concatenated all the translated texts in each sub-corpus into a single file. Next, we split the three concatenated files into equal-sized blocks of 5000 words. To ensure overlap and continuity between samples, we set the moving window size as 500 words."
  - [corpus] "We created a total of 210 samples: 67 from Human_Trans, 71 from Machine_Trans, and 72 from ChatGPT_Trans. Then we conducted random sampling to select 50 samples for each sub-corpus."

### Mechanism 2
- Claim: Supervised classifiers achieve high accuracy because linguistic features encode systematic differences between translation types
- Mechanism: Feature vectors extracted from each translation sample capture distributional differences in lexical density, formality markers, stance expressions, and syntactic complexity, which supervised models can learn to distinguish
- Core assumption: The 121 linguistic features used (after filtering for statistical significance) are sufficient to capture the systematic differences between human, NMT, and ChatGPT translations
- Evidence anchors:
  - [abstract] "After extracting a wide range of linguistic features, supervised classifiers demonstrate high accuracy in distinguishing the three translation types, whereas unsupervised clustering techniques do not yield satisfactory results."
  - [section] "Following the implementation of Kruskal-Wallis H, we found that 74 out of 121 linguistic features exhibited statistical significance."

### Mechanism 3
- Claim: Multi-dimensional analysis reveals interpretable dimensions that separate translation types based on co-occurring feature patterns
- Mechanism: Factor analysis groups linguistically correlated features into dimensions (e.g., formality vs. interactivity, stance vs. action, evaluative vs. non-evaluative discourse), allowing comparison of translation types along these interpretable axes
- Core assumption: The underlying structure of translation differences can be captured by a small number of interpretable dimensions rather than requiring examination of each feature individually
- Evidence anchors:
  - [abstract] "Another major finding is that ChatGPT-produced translations exhibit greater similarity with NMT than HT in most MDA dimensions, which is further corroborated by distance computing and visualization."
  - [section] "We found that in four out of five dimensions, the z-transformed dimension scores of ChatGPT-produced translations were very close to those of NMT, as supported by their relative locations in the boxplots."

## Foundational Learning

- Concept: Statistical significance testing with multiple comparison correction
  - Why needed here: To identify which linguistic features actually distinguish translation types without inflating false positive rates when testing 121 features simultaneously
  - Quick check question: What happens to the p-value threshold when applying Bonferroni correction to 121 simultaneous tests?

- Concept: Factor analysis and dimensionality reduction
  - Why needed here: To transform 74 significant linguistic features into interpretable dimensions that capture the main patterns of variation between translation types
  - Quick check question: How do you determine the optimal number of factors to retain in factor analysis?

- Concept: Distance metrics in multidimensional space
  - Why needed here: To quantify the similarity between translation types by measuring how close their factor score distributions are in the reduced dimensional space
  - Quick check question: Why use Euclidean distance rather than Manhattan distance for comparing translation type distributions?

## Architecture Onboarding

- Component map: Data pipeline → Feature extraction (MAT/MFTE) → Statistical filtering → Factor analysis → Classification/Clustering → Distance calculation → Visualization
- Critical path: Feature extraction → Statistical filtering → Factor analysis → Classification → Interpretation
- Design tradeoffs:
  - More features → better discrimination but risk of overfitting
  - Larger window size → more stable patterns but less granularity
  - More factors → more detail but harder interpretation
- Failure signatures:
  - Low classification accuracy → feature set inadequate or training/test imbalance
  - Uninterpretable factors → too many factors or poor feature selection
  - No cluster separation → translation types too similar or feature scaling issues
- First 3 experiments:
  1. Run feature extraction on small sample to verify MAT/MFTE output format and feature counts
  2. Perform Kruskal-Wallis test on extracted features to identify statistically significant ones
  3. Run factor analysis on significant features to determine optimal number of dimensions and examine factor loadings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-feature MDA reveal systematic differences in register handling between ChatGPT, NMT, and human translators beyond diplomatic texts?
- Basis in paper: [explicit] The study demonstrates that multi-feature MDA can distinguish translation types and reveals that ChatGPT and NMT share more similarities in register handling (formality, stance expression) than either does with human translation. The authors note their analysis was restricted to Chinese-to-English diplomatic translation.
- Why unresolved: The study only examined one register (diplomatic discourse). Different text types (literary, technical, conversational) may show different patterns of AI vs human translation differences.
- What evidence would resolve it: Applying the same MDA methodology to translations across multiple registers and comparing dimension patterns between translation types.

### Open Question 2
- Question: What specific architectural or training data differences between ChatGPT and NMT lead to their greater similarity in translation output compared to human translation?
- Basis in paper: [explicit] The paper notes ChatGPT is a decoder-only model trained on general-domain monolingual corpora, while NMT uses encoder-decoder architecture trained on parallel corpora. Despite these differences, they produce more similar translations than either does to human translation.
- Why unresolved: The study identifies the phenomenon but doesn't investigate the underlying mechanisms causing ChatGPT and NMT to converge in linguistic patterns despite different architectures.
- What evidence would resolve it: Controlled experiments varying training data composition and model architecture while measuring how linguistic patterns in translations change.

### Open Question 3
- Question: How can NMT and LLMs be optimized to better capture human-like register awareness and cultural sensitivity in translation?
- Basis in paper: [inferred] The authors note that human translators demonstrate superior "text tailoring," cultural sensitivity, and awareness of translation norms compared to NMT and ChatGPT. They suggest incorporating these characteristics into AI training.
- Why unresolved: The paper identifies the gap but doesn't specify which features or training approaches would effectively bridge it.
- What evidence would resolve it: Comparative studies testing whether incorporating register-specific features or cultural knowledge into NMT/ChatGPT training improves translation quality according to human evaluation metrics.

## Limitations
- The rolling stylometry approach may introduce artifacts if windows span significantly different topics within spokesperson remarks
- The paper does not adequately address whether linguistic differences reflect translation-specific patterns or broader discourse-level variation
- The claim of ChatGPT-NMT similarity is based on factor score comparisons that may be sensitive to specific feature selection and rotation methods

## Confidence
- **High Confidence**: Supervised classifier performance and t-SNE visualization methodology are well-specified and reproducible
- **Medium Confidence**: MDA dimension interpretation and Euclidean distance calculations are methodologically appropriate but rely on assumptions about factor structure
- **Low Confidence**: The claim that ChatGPT and NMT are more similar than either is to human translation is based on factor score comparisons that may be sensitive to specific feature selection and rotation method

## Next Checks
1. **Feature Stability Analysis**: Test whether the 74 statistically significant features remain stable across different random train/test splits and whether their loadings in the factor analysis are consistent when the analysis is repeated with different seeds

2. **Alternative Dimensionality Reduction**: Apply multiple dimensionality reduction techniques (PCA, t-SNE, UMAP) to the feature space and compare whether the same five interpretable dimensions emerge, or if the factor analysis is imposing structure that alternative methods do not reveal

3. **Cross-Validation of Classification**: Implement k-fold cross-validation for the supervised classifiers rather than a single train/test split to assess whether the high accuracy is robust to different data partitions and to detect potential overfitting to the specific training set