---
ver: rpa2
title: Functional Interpolation for Relative Positions Improves Long Context Transformers
arxiv_id: '2310.04418'
source_url: https://arxiv.org/abs/2310.04418
tags:
- length
- fire
- sequence
- encoding
- positional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIRE, a new functional relative positional
  encoding method that uses progressive interpolation to enable Transformers to generalize
  better to longer input sequences. FIRE uses a learnable MLP to map input positions
  to biases, with progressive interpolation normalizing the query-key relative distance
  by the query position to ensure bounded input for all sequence lengths.
---

# Functional Interpolation for Relative Positions Improves Long Context Transformers

## Quick Facts
- **arXiv ID**: 2310.04418
- **Source URL**: https://arxiv.org/abs/2310.04418
- **Reference count**: 40
- **Primary result**: FIRE improves Transformer length generalization through progressive interpolation of relative positional encodings

## Executive Summary
This paper introduces FIRE (Functional Interpolation for Relative Positional Encoding), a novel method that uses progressive interpolation to enable Transformers to generalize better to longer input sequences. The approach employs a learnable MLP to map input positions to attention biases, with progressive interpolation normalizing the query-key relative distance by the query position to ensure bounded input for all sequence lengths. FIRE is theoretically proven to represent several popular relative positional encodings and empirically demonstrates significant improvements in zero-shot length generalization on language modeling tasks while maintaining competitive performance on short sequence benchmarks.

## Method Summary
FIRE implements relative positional encoding through a learnable MLP that maps normalized position inputs to attention biases. The method uses progressive interpolation, which normalizes the query-key relative distance by the query position, constraining the normalized distance to [0, 1] regardless of sequence length. As sequence length increases, positional inputs form progressively finer grids, effectively interpolating the encoding function on [0, 1]. The approach is theoretically proven to represent popular position encodings like T5's RPE, Alibi, and Kerple.

## Key Results
- FIRE significantly improves zero-shot length generalization on language modeling (C4 dataset) compared to baselines
- Models maintain competitive performance on short sequence tasks (GLUE/SuperGLUE benchmarks)
- FIRE outperforms existing methods (T5's RPE, Alibi, Kerple, RoPE) on long context question answering (NarrativeQA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive interpolation ensures bounded input for the position encoding function across all sequence lengths, enabling length generalization.
- Mechanism: The method normalizes the query-key relative distance by the query position, constraining the normalized relative distance to [0, 1] regardless of sequence length. As sequence length increases, positional inputs form progressively finer grids, interpolating the encoding function on [0, 1].
- Core assumption: The relative distance in causal attention is always between 0 and the query position, making this normalization scheme valid.
- Evidence anchors:
  - [abstract]: "progressive interpolation normalizing the query-key relative distance by the query position to ensure bounded input for all sequence lengths"
  - [section]: "In particular, with increasingly longer sequence lengths, the positional inputs will form progressively finer grids, interpolating the positional encoding function on [0, 1]."
  - [corpus]: Weak - corpus papers focus on different interpolation strategies rather than this specific normalization approach.
- Break condition: If the attention mechanism allows query-key distances outside the [0, query position] range, the normalization would no longer constrain inputs to [0, 1].

### Mechanism 2
- Claim: The functional approach using a learnable MLP allows the model to adapt position encoding to the given task instead of using fixed inductive biases.
- Mechanism: Instead of hard-coding specific attention patterns (like locality bias), FIRE learns a mapping from positions to biases through an MLP that can represent various existing position encoding methods.
- Core assumption: A sufficiently expressive function class (MLP) can learn task-appropriate position biases when given normalized position inputs.
- Evidence anchors:
  - [abstract]: "a learnable MLP to map input positions to biases" and "learnable function to map the input positions to biases"
  - [section]: "A functional approach to learn the biases allows the model to adapt to the given task instead of always having the same inductive bias"
  - [corpus]: Weak - corpus papers focus on interpolation but don't directly address the learnable functional approach advantage.
- Break condition: If the task requires position biases that cannot be represented by the chosen function class (e.g., discontinuous functions with the given MLP architecture).

### Mechanism 3
- Claim: FIRE can represent several popular relative positional encoding methods including T5's RPE, Alibi, and Kerple.
- Mechanism: The paper proves that with appropriate choice of transformation ψ, threshold L, and MLP configuration, FIRE can exactly match these existing methods for any sequence length up to a given bound.
- Core assumption: The FIRE framework is sufficiently expressive to represent these specific functional forms through appropriate parameter choices.
- Evidence anchors:
  - [abstract]: "We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple"
  - [section]: "We theoretically prove that FIRE can represent popular position encodings such as T5's RPE, Alibi, and Kerple"
  - [corpus]: Weak - corpus papers don't discuss representational capacity of FIRE compared to existing methods.
- Break condition: If new position encoding methods emerge with functional forms outside FIRE's representational capacity, the claim would no longer hold.

## Foundational Learning

- Concept: Relative positional encoding in Transformers
  - Why needed here: Understanding how Transformers use relative positions (rather than absolute positions) is fundamental to grasping FIRE's approach
  - Quick check question: What is the key difference between absolute and relative positional encoding in Transformers?

- Concept: Length generalization in language models
  - Why needed here: FIRE specifically targets the problem of models trained on shorter sequences but tested on longer ones
  - Quick check question: Why do models typically perform worse on sequences longer than those seen during training?

- Concept: Causal attention masks in decoder-only Transformers
  - Why needed here: FIRE's normalization relies on the constraint that in causal attention, relative distances are always non-negative and bounded by the query position
  - Quick check question: In a causal attention mask, what is the maximum possible relative distance for a query at position i?

## Architecture Onboarding

- Component map:
  Input positions -> Normalization function (progressive interpolation) -> Learnable MLP -> Attention bias matrix

- Critical path:
  1. Receive query and key positions
  2. Compute relative distance and normalize by query position
  3. Apply transformation ψ (log in standard FIRE)
  4. Pass through learnable MLP to get attention bias
  5. Add bias to attention logits before softmax

- Design tradeoffs:
  - MLP expressiveness vs computational efficiency (32 hidden neurons chosen)
  - Progressive interpolation vs fixed normalization schemes
  - Learnable vs fixed transformation function ψ

- Failure signatures:
  - Poor length generalization indicates normalization not working correctly
  - Degradation on short sequences suggests threshold L not well-tuned
  - Performance worse than baselines suggests MLP capacity insufficient

- First 3 experiments:
  1. Implement basic FIRE with identity normalization (no progressive interpolation) and test on C4 language modeling
  2. Add progressive interpolation normalization and compare length generalization performance
  3. Test with different MLP capacities (varying hidden neurons) to find sweet spot for expressiveness vs efficiency

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the analysis, several important questions remain:

1. How does FIRE's performance on short sequence tasks (e.g., GLUE/SuperGLUE) compare to its performance on long sequence tasks?
2. What is the impact of the MLP activation function choice (ReLU vs. GeLU) on FIRE's length generalization performance?
3. How does FIRE compare to RoPE with positional interpolation (RoPE-PI) when both methods are applied to zero-shot length generalization on long context question answering tasks?

## Limitations

- The theoretical claims about representational capacity show medium confidence due to limited formal proof details and focus on bounded rather than infinite sequence lengths
- The mechanism explaining why progressive interpolation enables length generalization shows medium confidence, as no other papers use this exact normalization approach
- Empirical results on length generalization show high confidence, but the claim about maintaining competitive performance on short sequences shows medium confidence due to limited ablation studies

## Confidence

- Theoretical claims about representational capacity: **Medium** confidence
- Empirical results on length generalization: **High** confidence
- Claims about maintaining short sequence performance: **Medium** confidence
- Mechanism explanation for progressive interpolation: **Medium** confidence

## Next Checks

1. **Ablation on threshold L**: Run FIRE without the learnable threshold mechanism to verify whether progressive interpolation is actually necessary for the length generalization benefits, or if simpler normalization schemes achieve similar results.

2. **Cross-architecture testing**: Implement FIRE in encoder-only Transformers (like BERT) and bidirectional attention patterns to test whether the normalization scheme's assumption about causal attention bounds is essential to the method's success.

3. **Scaling analysis**: Systematically vary the MLP capacity (hidden neurons) and measure the tradeoff between representational power and length generalization performance to identify whether the chosen architecture (32 hidden neurons) is optimal or conservative.