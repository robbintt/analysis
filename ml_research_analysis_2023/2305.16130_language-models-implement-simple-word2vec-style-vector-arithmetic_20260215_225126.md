---
ver: rpa2
title: Language Models Implement Simple Word2Vec-style Vector Arithmetic
arxiv_id: '2305.16130'
source_url: https://arxiv.org/abs/2305.16130
tags:
- intervention
- layer
- which
- tasks
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents evidence that large language models implement\
  \ a simple vector arithmetic mechanism, similar to static word embeddings, for solving\
  \ relational tasks during in-context learning. The core idea is that feedforward\
  \ networks construct context-independent vectors that transform input representations\
  \ (e.g., \"Poland\" \u2192 \"Warsaw\") through additive updates."
---

# Language Models Implement Simple Word2Vec-style Vector Arithmetic

## Quick Facts
- arXiv ID: 2305.16130
- Source URL: https://arxiv.org/abs/2305.16130
- Reference count: 40
- Large language models use vector arithmetic in feedforward networks for relational tasks during in-context learning

## Executive Summary
This paper demonstrates that large language models implement a simple vector arithmetic mechanism, similar to static word embeddings, to solve relational tasks during in-context learning. The core finding is that feedforward networks construct context-independent vectors that transform input representations (e.g., "Poland" → "Warsaw") through additive updates. This mechanism is specific to tasks requiring retrieval from pretraining memory rather than local context. The authors show that these transformations can be isolated, extracted, and transferred to new contexts, enabling controlled generation and improved zero-shot performance.

## Method Summary
The authors investigate how language models solve relational tasks by implementing vector arithmetic mechanisms during in-context learning. They evaluate three tasks (World Capitals, Colored Objects, Past Tense Verb Mapping) across multiple model scales using one-shot prompting. The methodology employs early decoding to observe hidden states at each layer, FFN output vector (o-vector) intervention experiments to extract and apply transformations, and ablation studies comparing abstractive vs extractive tasks. They use mean reciprocal rank and accuracy metrics to measure performance, with systematic analysis of FFN vs attention contributions.

## Key Results
- Language models implement Word2Vec-style vector arithmetic in FFN layers for relational tasks
- This mechanism is specific to out-of-context retrieval, not local context copying
- FFN output vectors can be extracted and transferred to new contexts for controlled generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models use vector arithmetic in FFN layers to transform an input argument into its relational output
- Mechanism: The model represents the input token in the residual stream, then the FFN at a specific layer computes a context-independent vector that is added to transform it into the output token
- Core assumption: The embedding space has learned linear relationships between related tokens such that additive updates can retrieve relational outputs
- Evidence anchors: [abstract] "LMs sometimes exploit...simple vector arithmetic in order to encode abstract relations (e.g., Poland:Warsaw::China:Beijing)"; [section 4.1] "We find that ⃗o19 will apply the get_capital function regardless of the content of ˜x19"
- Break condition: If the embedding space does not encode linear relationships for the target relation, or if the FFN layer does not compute the correct transformation vector

### Mechanism 2
- Claim: FFN layers in mid-to-late layers are specialized for out-of-context retrieval while attention layers handle local context copying
- Mechanism: FFNs apply vector arithmetic to recall facts from pretraining memory, while attention modules copy tokens from the prompt when the answer is present in context
- Core assumption: The model has learned a division of labor between FFNs and attention for different types of retrieval tasks
- Evidence anchors: [abstract] "We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context"; [section 5.2] "We find that performance plummets on the abstractive task as FFNs are ablated, while accuracy on the extractive task drops much more slowly"
- Break condition: If FFNs are also used for local context tasks, or if attention is involved in out-of-context retrieval

### Mechanism 3
- Claim: The model processes relational tasks through distinct stages: argument formation, function application, and saturation
- Mechanism: The model first represents the argument token in the residual stream, then abruptly applies a function (via FFN vector addition) to transform it to the output, and finally stops updating once the answer is found
- Core assumption: The model follows a predictable processing pattern for relational tasks that can be observed through early decoding
- Evidence anchors: [section 3] "We see several discrete stages of processing that the next token undergoes before reaching the final answer"; [section 3] "During Function Application we find that the model abruptly switches from the argument to the output of the function"
- Break condition: If the model uses a different processing pattern for relational tasks, or if the stages are not clearly observable

## Foundational Learning

- Concept: Vector arithmetic in embedding spaces
  - Why needed here: The core mechanism relies on the model using linear vector operations to transform related tokens
  - Quick check question: Can you explain how word2vec's "king - man + woman = queen" demonstrates vector arithmetic for relations?

- Concept: Residual connections and residual streams
  - Why needed here: The mechanism depends on FFNs adding vectors to the residual stream to transform token representations
  - Quick check question: What does the residual connection allow each layer to do to the token representation?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The tasks are evaluated in a few-shot setting where the model must learn the pattern from examples
  - Quick check question: How does in-context learning differ from fine-tuning, and why is it relevant for evaluating this mechanism?

## Architecture Onboarding

- Component map: Input tokens → Embedding layer → Transformer blocks (Attention + FFN) → Language modeling head
- Critical path: Token enters model → undergoes attention and FFN updates → FFN in mid-to-late layer computes transformation vector → vector is added to residual stream → output token is produced
- Design tradeoffs: Using FFNs for out-of-context retrieval vs. attention for local context provides modularity but may limit flexibility; early decoding provides interpretability but adds computational overhead
- Failure signatures: If the model fails to go through argument formation → function application → saturation stages, or if ablating FFNs does not affect abstractive task performance
- First 3 experiments:
  1. Apply early decoding to a simple relational task (e.g., country-capital) and observe the stages of processing
  2. Extract the FFN output vector from one example and test if it transforms other arguments in new contexts
  3. Ablate FFNs in different layers and measure impact on abstractive vs. extractive task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions do language models learn to use the vector arithmetic mechanism for factual recall versus other retrieval strategies like attention-based copying?
- Basis in paper: [explicit] The paper states "Although how language models store and use knowledge has been studied more generally... our work builds on the finding that FFN layers promote concepts... by breaking down the process the model uses to do this in context." The discussion section also notes "Our findings invite future work aimed at understanding why, and under what conditions, LMs learn to use this mechanism when they are capable of solving such tasks using, e.g., adhoc memorization."
- Why unresolved: The paper demonstrates the existence of the vector arithmetic mechanism but doesn't systematically characterize when models prefer it over alternatives
- What evidence would resolve it: Systematic experiments comparing retrieval strategies across different task types, model sizes, and training conditions

### Open Question 2
- Question: How do tokenization effects impact the effectiveness of vector arithmetic interventions across different language models and tasks?
- Basis in paper: [explicit] The paper notes in Section 4.1 that "We also find that the way tokenization splits the argument and target words affects the ability of the ⃗ ovector to work and is another source of errors" and provides a detailed discussion in Appendix E
- Why unresolved: While the paper identifies this as a source of errors, it doesn't provide a comprehensive analysis of how tokenization patterns affect different types of vector arithmetic interventions
- What evidence would resolve it: Systematic experiments measuring intervention success rates across different tokenization patterns, vocabulary sizes, and language models

### Open Question 3
- Question: Can the vector arithmetic mechanism be generalized across different model architectures beyond decoder-only transformers?
- Basis in paper: [inferred] The paper focuses exclusively on decoder-only transformer architectures (GPT2 variants, GPT-J, and BLOOM). The discussion mentions "Our experiments on stages of processing with GPT-J suggest that the same phenomena is in play, although... the procedures we derive for interventions on GPT2-Medium do not transfer perfectly."
- Why unresolved: The paper provides evidence for this mechanism in a specific class of models but doesn't explore whether it generalizes to encoder-decoder models, convolutional architectures, or other neural network designs
- What evidence would resolve it: Replicating the vector arithmetic intervention experiments across diverse model architectures (BERT, T5, ConvNets, etc.)

## Limitations

- Manual analysis required to extract o-vectors for intervention experiments introduces potential subjectivity and limits reproducibility
- Focus on specific relational tasks leaves open whether mechanism extends to more complex semantic relationships or multi-hop reasoning
- Uncertainty about generalizability across different model architectures and sizes

## Confidence

High confidence: The existence of the vector arithmetic mechanism in FFN layers for relational tasks, supported by multiple ablation studies and early decoding observations across different model scales.

Medium confidence: The claim that FFNs are specialized for out-of-context retrieval while attention handles local context, as this relies on indirect evidence from task ablation studies.

Low confidence: The assertion that all relational tasks follow the same three-stage processing pattern (argument formation → function application → saturation), as this may be an oversimplification of more complex processing dynamics.

## Next Checks

1. **Cross-Architecture Validation**: Test the vector arithmetic mechanism on a diverse set of model architectures (e.g., OPT, LLaMA, PaLM) to determine if the mechanism is universal or architecture-specific.

2. **Scaling Analysis**: Systematically evaluate how the strength and reliability of the vector arithmetic mechanism changes across model scales, particularly focusing on transition points where the mechanism appears/disappears.

3. **Multi-Relation Complexity**: Design experiments with tasks requiring multiple relational transformations (e.g., country → capital → continent → language) to test whether the vector arithmetic mechanism can compose and whether it remains interpretable in more complex reasoning chains.