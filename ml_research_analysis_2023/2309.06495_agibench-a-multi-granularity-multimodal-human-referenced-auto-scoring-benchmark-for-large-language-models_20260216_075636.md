---
ver: rpa2
title: 'AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring
  Benchmark for Large Language Models'
arxiv_id: '2309.06495'
source_url: https://arxiv.org/abs/2309.06495
tags:
- accuracy
- llms
- prompt
- questions
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGIBench addresses the lack of comprehensive, multi-dimensional
  evaluation for large language models (LLMs) by providing a multi-granularity, multimodal,
  human-referenced, and auto-scoring benchmark. It overcomes limitations of existing
  benchmarks by using a four-tuple labeling system to classify questions across ability
  branches, knowledge categories, difficulty levels, and modalities.
---

# AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2309.06495
- Source URL: https://arxiv.org/abs/2309.06495
- Reference count: 20
- Key outcome: AGIBench is a comprehensive benchmark that evaluates LLMs across multiple dimensions using a four-tuple labeling system, achieving 67% auto-scoring with 70% cost reduction.

## Executive Summary
AGIBench addresses critical gaps in LLM evaluation by providing a multi-granularity, multimodal, human-referenced, and auto-scoring benchmark. Unlike existing benchmarks that focus on single dimensions or blended questions, AGIBench decomposes questions into four orthogonal attributes (ability branch, knowledge category, difficulty level, and modality) to enable comprehensive performance analysis. The benchmark uses zero-shot learning and a hybrid scoring approach combining regex extraction with GPT-4 validation to achieve significant cost savings while maintaining accuracy. Experimental results reveal important insights about LLM reliability, the importance of architecture over size, and the persistent gap between human and machine reasoning capabilities.

## Method Summary
AGIBench evaluates LLMs using 927 questions labeled with a four-tuple system (ability branch, knowledge category, difficulty level, modality). The benchmark employs zero-shot learning to ensure fair comparison across models, avoiding prompt engineering biases. Answers are extracted using a heuristic regex algorithm for 67% of responses, with GPT-4 handling the remaining 33%. Human validation is only required for GPT-4 judgments. The benchmark defines five multi-dimensional metrics (average, worst-case, best-case, majority voting accuracy, and repeatability) to provide comprehensive performance insights beyond simple average accuracy.

## Key Results
- AGIBench achieves 67% automatic answer extraction using regex patterns, reducing manual evaluation costs by 70%
- LLM performance gaps between average and worst-case accuracy reveal significant reliability issues
- Architecture improvements matter more than model size, with ChatGLM v2-6B outperforming ChatGLM-130B
- Humans outperform LLMs on simple questions but underperform on difficult ones, highlighting reasoning limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AGIBench achieves reliable multi-granularity evaluation by labeling each question with a four-tuple rather than treating questions as blended collections.
- Mechanism: The four-tuple decomposition creates orthogonal dimensions for evaluation, allowing performance to be measured at per-question, per-ability branch, per-knowledge, per-difficulty level, per-modal, and per-dataset granularities.
- Core assumption: Question attributes can be cleanly separated into these four dimensions without losing contextual meaning.
- Evidence anchors:
  - [abstract] "Instead of a collection of blended questions, AGIBench focuses on three typical ability branches and adopts a four-tuple <ability branch, knowledge, difficulty, modal> to label the attributes of each question."
  - [section 3.1] "we use a four-tuple <ability branch, knowledge, difficulty, modal> to label the attributes of each question."
- Break condition: If questions cannot be cleanly categorized into these dimensions (e.g., questions requiring multiple inseparable ability branches), the evaluation granularity would become misleading.

### Mechanism 2
- Claim: AGIBench achieves cost-effective auto-scoring by combining heuristic regex pattern extraction with GPT-4 validation.
- Mechanism: The heuristic regex (HRE) algorithm identifies frequent response patterns from LLM outputs and extracts answers automatically. For the remaining 33% of responses that don't match patterns, GPT-4 is used with human validation only for these cases.
- Core assumption: LLM response formats follow enough recurring patterns to make regex extraction effective for the majority of cases.
- Evidence anchors:
  - [section 3.2] "We use the HRE algorithm illustrated in Algorithm 1 and GPT-4 to avoid the unpredictable performance impacts of prompt engineering."
  - [section 3.2] "By adopting HRE, we reduce about seventy percent labor costs."
- Break condition: If LLM response patterns become too diverse or unpredictable, the regex extraction rate would drop significantly, requiring more manual evaluation.

### Mechanism 3
- Claim: AGIBench provides comprehensive multi-dimensional metrics that reveal LLM reliability beyond simple average accuracy.
- Mechanism: AGIBench defines five metrics: average accuracy, worst-case accuracy, best-case accuracy, majority voting accuracy, and repeatability.
- Core assumption: Evaluating LLMs using multiple metrics provides more actionable insights than single average accuracy metrics.
- Evidence anchors:
  - [abstract] "Finally, it defines multi-dimensional metrics, including accuracy under the average, worst, best, and majority voting cases, and repeatability."
  - [section 3.2] "We widely include the average accuracy, the worst-case accuracy, the best-case accuracy, the majority voting accuracy, and the repeatability to indicate the performance of LLMs under different cases."
- Break condition: If the additional metrics don't reveal meaningful differences between models, the complexity added by multi-dimensional metrics may not justify their use.

## Foundational Learning

- Concept: Zero-shot learning methodology
  - Why needed here: AGIBench avoids prompt engineering like few-shot learning and chain-of-thought to prevent unpredictable performance impacts and ensure fair benchmarking across different models.
  - Quick check question: Why does AGIBench use zero-shot learning instead of few-shot or chain-of-thought prompting?

- Concept: Human-referenced difficulty classification
  - Why needed here: AGIBench classifies questions into five difficulty levels based on accuracy rates of educated humans, providing a standardized way to measure LLM performance relative to human capabilities.
  - Quick check question: How does AGIBench determine the difficulty level of each question?

- Concept: Multimodal question processing
  - Why needed here: AGIBench includes text-only, image-only, text with images, text with tables, and combinations thereof, requiring understanding of how different LLMs handle various input modalities.
  - Quick check question: What types of multimodal inputs does AGIBench support?

## Architecture Onboarding

- Component map: Question Dataset (927 labeled questions) → LLM Zero-shot Evaluation → HRE Regex Extraction (67% success) → GPT-4 Extraction (33%) → Human Validation → Metric Computation → Report Generation

- Critical path: Question → LLM zero-shot response → HRE regex extraction (67% success) → GPT-4 extraction for remaining (33%) → Human validation of GPT-4 results → Metric computation → Report generation. The HRE extraction step is the most critical for performance and cost efficiency.

- Design tradeoffs: Zero-shot vs. prompt engineering (zero-shot ensures fairness but may reduce accuracy); HRE vs. GPT-4 extraction (HRE is faster and cheaper but less comprehensive); human-referenced vs. algorithmic difficulty (human-referenced is more meaningful but requires more data collection).

- Failure signatures: Low HRE extraction rate (<50%) indicates changing LLM response patterns; large gaps between best-case and worst-case accuracy suggest poor reliability; consistent human outperformance on simple questions but underperformance on difficult ones may indicate reasoning limitations.

- First 3 experiments:
  1. Run the same question through multiple LLMs using zero-shot prompting and verify the HRE extraction successfully processes at least 60% of responses.
  2. Compare AGIBench metrics (average, worst-case, best-case, majority voting) against a simple average accuracy benchmark to validate the additional insights.
  3. Test the multimodal processing by evaluating a text-with-image question on models with and without image processing capabilities to verify the response handling logic.

## Open Questions the Paper Calls Out

- Question: What specific architectural improvements lead to better performance than simply increasing model size in LLMs?
  - Basis in paper: [explicit] The paper observes that ChatGLM v2-6B outperforms ChatGLM-130B despite having fewer parameters, suggesting architecture improvements matter more than model size.
  - Why unresolved: The paper identifies this as an observation but does not investigate which specific architectural features contribute to better performance.
  - What evidence would resolve it: Comparative analysis of different architectural components across models of similar size but different architectures would identify which specific improvements drive performance gains.

- Question: How can we improve LLM reliability to reduce the gap between best-case and worst-case accuracy?
  - Basis in paper: [explicit] The paper finds that worst-case accuracy is significantly below average accuracy, indicating poor reliability, while best-case accuracy is much higher.
  - Why unresolved: The paper identifies the reliability problem but does not propose solutions for making LLMs more consistent in their responses.
  - What evidence would resolve it: Experimental studies testing various techniques like temperature adjustment, ensembling methods, or confidence scoring to identify approaches that minimize performance variance across multiple runs.

- Question: What specific training data or fine-tuning strategies would improve LLM reasoning abilities?
  - Basis in paper: [explicit] The paper observes that GPT-4 outperforms humans on common sense and understanding but underperforms on reasoning by 25.94%, indicating reasoning as a key weakness.
  - Why unresolved: The paper identifies reasoning as a critical weakness but does not investigate what types of data or training approaches would address this gap.
  - What evidence would resolve it: Controlled experiments varying training data composition and fine-tuning strategies to determine optimal approaches for improving reasoning performance.

## Limitations

- The benchmark's reliance on zero-shot learning may underestimate model capabilities compared to well-engineered prompts
- The effectiveness of the heuristic regex extraction depends on consistent LLM response patterns, which may vary across model updates
- The human-referenced difficulty classification requires substantial human effort to establish and may not generalize perfectly to all LLM capabilities

## Confidence

- High Confidence: The benchmark's multi-dimensional metric framework and its ability to reveal performance gaps between average and worst-case accuracy
- Medium Confidence: The generalizability of the four-tuple labeling system across diverse question types
- Medium Confidence: The benchmark's ability to fairly compare models with different architectural capabilities

## Next Checks

1. Test the HRE algorithm's extraction success rate across multiple LLM versions and architectures to verify that the 67% baseline holds under different response patterns.
2. Evaluate the benchmark's sensitivity by measuring how performance metrics change when questions are randomly reassigned to different difficulty categories.
3. Compare benchmark results using zero-shot learning against results using carefully engineered prompts to quantify the performance gap and validate the fairness claim.