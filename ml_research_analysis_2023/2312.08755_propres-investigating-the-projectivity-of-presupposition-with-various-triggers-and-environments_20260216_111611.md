---
ver: rpa2
title: 'PROPRES: Investigating the Projectivity of Presupposition with Various Triggers
  and Environments'
arxiv_id: '2312.08755'
source_url: https://arxiv.org/abs/2312.08755
tags:
- doctor
- tears
- shed
- triggers
- projectivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the projectivity of presupposition in language
  models by introducing a new dataset, PROPRES, which includes 12k premise-hypothesis
  pairs crossing six presupposition triggers with five environments. Human evaluation
  reveals variable projectivity in some cases, while the best-performed model, DeBERTa,
  does not fully capture it.
---

# PROPRES: Investigating the Projectivity of Presupposition with Various Triggers and Environments

## Quick Facts
- arXiv ID: 2312.08755
- Source URL: https://arxiv.org/abs/2312.08755
- Reference count: 23
- Primary result: Human evaluation reveals variable projectivity in presupposition triggers, while best-performed model (DeBERTa) does not fully capture it

## Executive Summary
This study investigates the projectivity of presupposition in language models by introducing a new dataset, PROPRES, which includes 12k premise-hypothesis pairs crossing six presupposition triggers with five environments. Human evaluation reveals variable projectivity in some cases, while the best-performed model, DeBERTa, does not fully capture it. The results suggest that probing studies on pragmatic inferences should consider human judgment variability and the combination of linguistic items. The study's contributions include introducing PROPRES for comprehensive investigation, providing evidence for variable projectivity, and revealing differences in model and human behavior in understanding presuppositions.

## Method Summary
The study employs a template-based approach to generate 12k premise-hypothesis pairs covering six presupposition triggers (again, stop, before, continue, manage, manner adverbs) and five environments (entailment, negation, interrogative, conditional, modal). Human evaluation is conducted on subsets of the IMPPRES and PROPRES datasets using Amazon Mechanical Turk and PCIbex, with at least 9.4 labels per item on average. Models (RoBERTa and DeBERTa) are fine-tuned on MNLI and evaluated on control and target conditions. The study analyzes projectivity by comparing model predictions against human judgments and identifies cases where models fail to capture variable projectivity.

## Key Results
- Human evaluation reveals variable projectivity in some presupposition trigger-environment combinations
- DeBERTa model does not fully capture variable projectivity exhibited by humans
- Models and humans behave differently in understanding presuppositions, particularly for manner adverbs in negation and conditional environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projectivity varies depending on the combination of presupposition triggers and environments
- Mechanism: Different triggers interact with environments in non-uniform ways, causing presuppositions to project with varying strength
- Core assumption: Linguistic items (triggers) and contexts (environments) have combinatorial effects on pragmatic inference
- Evidence anchors:
  - [abstract] "The projectivity may vary depending on the combination of presupposition triggers and environments"
  - [section] "Importantly, previous linguistic studies show that the projectivity of presupposition can vary depending on factors such as context, lexical items, prior beliefs, a speaker's social identity, and prosodic focus"
  - [corpus] Weak signal; no direct neighbor papers address this specific combinatorial claim
- Break condition: If projectivity is found to be invariant across trigger-environment combinations, the combinatorial model fails

### Mechanism 2
- Claim: Human judgments on projectivity are variable and unsystematic
- Mechanism: Humans use context, prior beliefs, and pragmatic reasoning to make judgments, leading to variability
- Core assumption: Pragmatic inferences are not deterministic and depend on individual interpretation
- Evidence anchors:
  - [abstract] "Our human evaluation reveals that humans exhibit variable projectivity in some cases"
  - [section] "it is possible that projectivity varies depending on the combination of triggers and environments"
  - [corpus] Neighbor paper "Understand the Implication: Learning to Think for Pragmatic Understanding" suggests variability in pragmatic inference is a known challenge
- Break condition: If human judgments were perfectly consistent across participants and items, the variability mechanism would not apply

### Mechanism 3
- Claim: Models fail to capture variable projectivity because they lack pragmatic knowledge
- Mechanism: Models rely on lexical overlap and negation heuristics rather than understanding presupposition projection
- Core assumption: Current NLI models do not encode pragmatic knowledge about presuppositions
- Evidence anchors:
  - [abstract] "the best-performed model, DeBERTa, does not fully capture it"
  - [section] "Our model evaluation against human results reveals that the models and humans behave differently in the understanding of presuppositions"
  - [corpus] Neighbor paper "Simple Linguistic Inferences of Large Language Models (LLMs): Blind Spots and Blinds" indicates LLMs have blind spots in linguistic inference tasks
- Break condition: If models could be trained to achieve human-level performance on variable projectivity tasks, the current lack of pragmatic knowledge would be disproven

## Foundational Learning

- Concept: Projectivity of presupposition
  - Why needed here: The study's core focus is on whether presuppositions project out of entailment-canceling environments
  - Quick check question: Does the presupposition "John had cut the tree before" project out of the negation "John did not cut the tree again"?

- Concept: Entailment-canceling environments
  - Why needed here: These environments (negation, interrogative, conditional, modal) are the contexts in which projectivity is tested
  - Quick check question: Which of these environments would cancel the entailment but not necessarily the presupposition: "John cut the tree again"?

- Concept: Presupposition triggers
  - Why needed here: Different triggers (again, stop, before, etc.) introduce presuppositions that may behave differently across environments
  - Quick check question: Does the trigger "again" introduce the same presupposition as "before" when embedded under negation?

## Architecture Onboarding

- Component map:
  - Template creation -> Sentence generation -> Human evaluation -> Model prediction -> Comparative analysis

- Critical path:
  1. Template creation and sentence generation
  2. Human evaluation data collection and cleaning
  3. Model predictions on generated data
  4. Comparative analysis of human vs model behavior

- Design tradeoffs:
  - Template-based generation ensures control but may lack natural language diversity
  - Human evaluation provides ground truth but is expensive and slow
  - Model evaluation on MNLI-finetuned models tests transfer but may not capture specialized pragmatic knowledge

- Failure signatures:
  - High model accuracy on controls but poor performance on targets suggests heuristic reliance
  - Consistent human judgment across all conditions suggests lack of variability to investigate
  - Low inter-annotator agreement suggests ambiguous or poorly designed stimuli

- First 3 experiments:
  1. Test model performance on control conditions to identify heuristic reliance
  2. Compare model vs human projectivity ratings on a single trigger-environment combination
  3. Analyze within-trigger variability by testing different lexical items of the same trigger type

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do lexical items and participant variability influence the projectivity of presuppositions across different triggers and environments?
- Basis in paper: [inferred] The paper mentions that judgments on presuppositions can vary depending on factors like lexical items and participants, but does not conduct thorough analyses of between-item and between-participant variability.
- Why unresolved: The study does not investigate the influence of lexical items and participant variability on projectivity, leaving it unclear how these factors contribute to the observed variability in human judgments.
- What evidence would resolve it: Conducting item-by-item and participant-by-participant analyses of the human judgments collected in the study would provide insights into how lexical items and participant variability influence the projectivity of presuppositions.

### Open Question 2
- Question: How does contextual information affect the projectivity of presuppositions introduced by different triggers embedded under various environments?
- Basis in paper: [explicit] The paper acknowledges that the projectivity of presuppositions can depend on context, but the study does not investigate how contextual information affects the projectivity of presuppositions.
- Why unresolved: The study focuses on presuppositions without context, leaving it unclear how contextual information would influence the projectivity of presuppositions in different scenarios.
- What evidence would resolve it: Designing experiments that manipulate the context in which presuppositions occur and analyzing the resulting projectivity patterns would shed light on the role of contextual information in presupposition projectivity.

### Open Question 3
- Question: What are the underlying mechanisms that lead to the observed differences in projectivity between human judgments and model predictions, particularly for manner adverbs in negation and conditional environments?
- Basis in paper: [inferred] The paper highlights that DeBERTa does not capture the variable projectivity exhibited by humans in some cases, such as manner adverbs in negation and conditional environments, but does not explore the reasons behind these discrepancies.
- Why unresolved: The study identifies the differences in projectivity between humans and models but does not investigate the underlying mechanisms that lead to these discrepancies.
- What evidence would resolve it: Conducting further analyses to understand how humans and models process manner adverbs in negation and conditional environments, and identifying the factors that contribute to the observed differences, would provide insights into the underlying mechanisms.

## Limitations

- Template-based data generation may not capture full complexity and variability of natural language
- Human evaluation variability raises questions about reliability as gold standard for pragmatic inference tasks
- Model evaluation scope limited to MNLI-finetuned models without exploring specialized training or alternative architectures

## Confidence

**High confidence**: Models perform differently from humans on presupposition tasks requiring pragmatic reasoning
**Medium confidence**: Projectivity varies depending on trigger-environment combinations
**Low confidence**: Variable projectivity reflects genuine linguistic phenomena rather than evaluation artifacts

## Next Checks

1. **Natural corpus validation**: Test whether PROPRES patterns appear in naturally occurring text corpora
2. **Cross-linguistic replication**: Replicate study with non-English language data to test universality
3. **Model architecture ablation**: Conduct controlled experiments ablating different model components to identify features correlating with better presupposition task performance