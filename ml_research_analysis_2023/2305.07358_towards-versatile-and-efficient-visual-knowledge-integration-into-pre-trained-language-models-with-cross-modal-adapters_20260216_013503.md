---
ver: rpa2
title: Towards Versatile and Efficient Visual Knowledge Integration into Pre-trained
  Language Models with Cross-Modal Adapters
arxiv_id: '2305.07358'
source_url: https://arxiv.org/abs/2305.07358
tags:
- visual
- language
- tasks
- clip
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel plug-and-play module called X-adapter
  to efficiently inject visual knowledge into pre-trained language models (PLMs).
  The X-adapter consists of two sub-modules, V-expert and T-expert, that can flexibly
  leverage the aligned visual and textual knowledge learned in pre-trained vision-language
  models (VLMs) such as CLIP.
---

# Towards Versatile and Efficient Visual Knowledge Integration into Pre-trained Language Models with Cross-Modal Adapters

## Quick Facts
- arXiv ID: 2305.07358
- Source URL: https://arxiv.org/abs/2305.07358
- Reference count: 26
- Primary result: X-adapters significantly improve performance on object-color reasoning and NLU tasks while being parameter-efficient

## Executive Summary
This paper introduces X-adapters, a plug-and-play module for injecting visual knowledge into pre-trained language models (PLMs). The X-adapter consists of two sub-modules (V-expert and T-expert) that selectively activate based on downstream task requirements, leveraging aligned visual and textual knowledge from pre-trained vision-language models like CLIP. By freezing PLM parameters and only updating adapter parameters, the method achieves significant performance improvements on both visual reasoning and natural language understanding tasks while maintaining parameter efficiency.

## Method Summary
The X-adapter architecture inserts specialized adapter modules into PLM transformer layers, with each adapter containing two sub-modules: V-expert for visual feature processing and T-expert for textual feature processing. During adaptation, only adapter parameters are updated while PLM parameters remain frozen. The V-expert retrieves relevant images using CLIP's text encoder and processes their visual features, while T-expert processes textual features directly from CLIP's text encoder. Both sub-modules use cross-attention mechanisms to fuse features with linguistic representations. The method is trained using Masked Language Modeling on specialized corpora (COCO captions for V-expert, Wiki103 for T-expert) and evaluated on both zero-shot object-color reasoning tasks and fine-tuned NLU tasks.

## Key Results
- X-adapters significantly outperform baseline PLMs on object-color reasoning tasks (MemoryColor, ColorTerms benchmarks)
- The method achieves competitive performance on GLUE NLU tasks while using fewer parameters than full fine-tuning
- Task-specific activation (V-expert for visual reasoning, T-expert for NLU) provides performance benefits over unified approaches
- The parameter-efficient design reduces memory footprint by updating only adapter parameters during adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: X-adapters selectively inject visual knowledge by activating different sub-modules (V-expert/T-expert) depending on downstream task requirements
- Mechanism: The dual-expert architecture allows the model to route relevant visual information through V-expert for visual reasoning tasks and T-expert for language understanding tasks, avoiding information overload
- Core assumption: Different downstream tasks require different types of visual knowledge, and mixing them indiscriminately would degrade performance
- Evidence anchors:
  - [abstract] "We can opt for activating different sub-modules depending on the downstream tasks"
  - [section 4.2] "We activate V-expert in X-adapters for zero-shot object color reasoning tasks"
  - [section 4.3] "For NLU tasks, we activate T-expert in X-adapters"
- Break condition: If downstream tasks require both visual reasoning AND language understanding simultaneously, this selective activation approach may be suboptimal

### Mechanism 2
- Claim: Freezing original PLM parameters while only updating adapter parameters preserves learned language knowledge while integrating new visual knowledge
- Mechanism: By freezing the original transformer weights and only training the X-adapter parameters, the model maintains its original language understanding capabilities while adding visual reasoning capabilities as an overlay
- Core assumption: The original language knowledge in PLMs is valuable and should be preserved rather than overwritten during visual knowledge injection
- Evidence anchors:
  - [abstract] "only the added parameters are updated during adaptation"
  - [section 3.2] "we freeze the original parameters of PLMs"
  - [section 4.4] "setting 1...show that tuning all the parameters of the PLMs on small datasets...disturbs the knowledge learned on massive corpus"
- Break condition: If the visual knowledge fundamentally contradicts the language knowledge, simply adding it as an overlay may not be sufficient

### Mechanism 3
- Claim: The cross-attention mechanism in X-adapters enables effective fusion of visual features with linguistic representations at different semantic levels
- Mechanism: The multi-head cross-attention module (Equation 1-4) allows the adapter to attend to relevant visual features conditioned on the linguistic context, creating contextualized visual-linguistic representations
- Core assumption: Visual features contain complementary information to linguistic representations that can be effectively integrated through attention mechanisms
- Evidence anchors:
  - [section 3.1] "we use a multi-head cross-attention module to fuse the visual features"
  - [section 4.5] "when inserting all blue or red images, the logit of blue or red becomes the largest"
- Break condition: If visual features are too noisy or irrelevant, the cross-attention mechanism may amplify noise rather than useful information

## Foundational Learning

- Concept: Masked Language Modeling (MLM) objective
  - Why needed here: X-adapters are trained using MLM on specialized corpora to integrate visual knowledge while preserving language understanding
  - Quick check question: Why does X-adapter use a higher mask ratio (45%) for V-expert compared to the standard 15% for T-expert?

- Concept: Cross-modal representation learning
  - Why needed here: X-adapters rely on aligned visual-text representations from pre-trained VLMs (CLIP) to transfer visual knowledge to PLMs
  - Quick check question: What is the purpose of using both CLIP's image encoder and text encoder in the X-adapter architecture?

- Concept: Adapter-based parameter-efficient fine-tuning
  - Why needed here: X-adapters follow the adapter paradigm of adding small trainable modules while freezing original model parameters to achieve efficient knowledge transfer
  - Quick check question: How does freezing PLM parameters while training only adapter parameters affect memory usage during adaptation?

## Architecture Onboarding

- Component map: Text sequence → X-adapter layer → Cross-attention fusion → PLM transformer → Output
- Critical path: Text sequence → X-adapter layer → Cross-attention fusion → PLM transformer → Output
- Design tradeoffs:
  - Flexibility vs. complexity: Two sub-modules allow task-specific activation but increase architectural complexity
  - Parameter efficiency vs. capacity: Small adapter size saves parameters but may limit knowledge integration capacity
  - Training corpus specificity vs. generalization: Specialized corpora improve task performance but may reduce general applicability
- Failure signatures:
  - Performance degradation on language-only tasks when V-expert is activated unnecessarily
  - Insufficient visual knowledge integration when T-expert is used for visual reasoning tasks
  - Training instability when mask ratio is set too high or too low
  - Memory issues when inserting too many X-adapter layers
- First 3 experiments:
  1. Insert one X-adapter layer before the last transformer layer and test on MemoryColor benchmark with V-expert activated
  2. Compare performance of X-adapter with 15% vs 45% mask ratio on V-expert training
  3. Test cross-modal retrieval effectiveness by measuring cosine similarity between input text and retrieved images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of X-adapters vary when using different pre-trained VLMs other than CLIP?
- Basis in paper: [inferred] The paper uses CLIP as the pre-trained VLM for injecting visual knowledge into PLMs. It is not clear how the choice of VLM affects the performance of X-adapters.
- Why unresolved: The paper does not explore the use of other pre-trained VLMs such as ALIGN or Flamingo for injecting visual knowledge into PLMs using X-adapters.
- What evidence would resolve it: Conducting experiments using different pre-trained VLMs with X-adapters and comparing their performance on the same tasks would provide insights into the impact of the choice of VLM on the effectiveness of X-adapters.

### Open Question 2
- Question: How do the X-adapters perform on tasks that require reasoning about more complex visual concepts beyond object colors?
- Basis in paper: [inferred] The paper focuses on evaluating X-adapters on object-color reasoning tasks and NLU tasks. It is not clear how well X-adapters can handle more complex visual reasoning tasks, such as spatial relationships or object attributes.
- Why unresolved: The paper does not provide experiments or analysis on the performance of X-adapters for more complex visual reasoning tasks beyond object colors.
- What evidence would resolve it: Conducting experiments on a diverse set of visual reasoning tasks, such as visual question answering or visual entailment, and comparing the performance of X-adapters with other methods would provide insights into their ability to handle more complex visual concepts.

### Open Question 3
- Question: How does the performance of X-adapters change when using larger PLMs or VLMs as the base models?
- Basis in paper: [inferred] The paper uses base-sized BERT and RoBERTa models as the PLMs and CLIP as the VLM. It is not clear how the performance of X-adapters scales with larger base models or more powerful VLMs.
- Why unresolved: The paper does not explore the use of larger PLMs or VLMs as the base models for X-adapters, nor does it provide any analysis on the impact of model size on the performance of X-adapters.
- What evidence would resolve it: Conducting experiments using larger PLMs and VLMs as the base models for X-adapters and comparing their performance on the same tasks would provide insights into the scalability and effectiveness of X-adapters with larger models.

## Limitations

- The paper lacks ablation studies demonstrating whether task-specific routing is truly necessary versus a unified adapter architecture
- Claims about preserving language knowledge by freezing PLM parameters lack direct comparison with alternative parameter-efficient methods like LoRA
- The effectiveness of cross-attention mechanisms is demonstrated qualitatively but lacks quantitative analysis of attention patterns
- Performance improvements vary significantly across tasks without clear explanation for this variance

## Confidence

- **High Confidence**: The parameter efficiency claims are well-supported with clear evidence that updating only adapter parameters reduces memory usage while maintaining performance
- **Medium Confidence**: The core claim that X-adapters improve performance on both visual reasoning and NLU tasks is supported by experimental results, but the magnitude of improvement varies significantly across tasks
- **Low Confidence**: The claim that task-specific activation of V-expert vs T-expert is optimal for different downstream tasks lacks direct experimental validation through head-to-head comparisons

## Next Checks

1. **Ablation Study on Adapter Architecture**: Compare the dual-expert X-adapter architecture against a single unified adapter trained on combined visual and textual knowledge to determine if selective activation provides meaningful benefits

2. **Attention Pattern Analysis**: Quantitatively analyze the cross-attention weights learned by V-expert and T-expert to verify that they are indeed focusing on task-relevant visual features and to understand how attention patterns differ between the two sub-modules

3. **Comparison with Alternative Parameter-Efficient Methods**: Benchmark X-adapters against other parameter-efficient fine-tuning approaches like LoRA or prefix-tuning on the same downstream tasks to establish whether the specific architecture provides advantages beyond the general parameter-efficient fine-tuning paradigm