---
ver: rpa2
title: A Holistic Evaluation of Piano Sound Quality
arxiv_id: '2310.04722'
source_url: https://arxiv.org/abs/2310.04722
tags:
- piano
- sound
- quality
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a CNN-based method for piano sound quality
  evaluation to assist beginners in purchasing decisions. The approach uses mel-spectrogram
  images of piano recordings as input to fine-tuned pre-trained CNN models, treating
  audio classification as a computer vision problem.
---

# A Holistic Evaluation of Piano Sound Quality

## Quick Facts
- arXiv ID: 2310.04722
- Source URL: https://arxiv.org/abs/2310.04722
- Reference count: 30
- Best performing model achieved 98.3% accuracy

## Executive Summary
This paper proposes a CNN-based method for piano sound quality evaluation to assist beginners in purchasing decisions. The approach uses mel-spectrogram images of piano recordings as input to fine-tuned pre-trained CNN models, treating audio classification as a computer vision problem. Subjective questionnaire surveys were conducted with music professionals to obtain quality ratings, and ERB analysis was applied to improve model interpretability. Focal loss was used to address data imbalance caused by audio slicing.

## Method Summary
The method transforms piano audio recordings into mel-spectrogram images using STFT, then fine-tunes pre-trained CNN models (SqueezeNet, VGG19-BN, AlexNet) for classification. Focal loss is applied to handle data imbalance from slicing longer recordings into fixed-duration segments. ERB analysis provides psychoacoustic interpretability by modeling human auditory filtering.

## Key Results
- SqueezeNet achieved the highest accuracy of 98.3% on the test dataset
- Musically trained individuals can better distinguish sound quality differences between piano brands
- Differences within the same piano are less perceptible to listeners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming audio spectrograms into mel-spectrogram images enables CNNs trained on visual data to effectively classify piano sound quality
- Mechanism: By converting time-series audio signals into two-dimensional mel-spectrogram images, the problem is reframed from audio classification to image classification
- Core assumption: Visual feature hierarchies in ImageNet-trained CNNs transfer to acoustic frequency-space representations when mel-spectrograms are used as inputs
- Evidence anchors:
  - [abstract]: "The method selects the optimal piano classification models by comparing the fine-tuning results of different pre-training models of Convolutional Neural Networks (CNN)."
  - [section]: "To handle audio data, we transform it into images through a short-time Fourier transform (STFT) that generates a mel spectrogram as the model input."
- Break condition: If mel-spectrogram pixel patterns do not align with learned visual feature detectors, performance degrades

### Mechanism 2
- Claim: Focal loss mitigates the imbalance introduced by slicing longer piano recordings into fixed-duration segments
- Mechanism: Slicing audio recordings into 0.2-second windows creates unequal sample counts across piano types because original recordings vary in length
- Core assumption: The imbalance in sample counts after slicing is proportional to the original recording lengths and affects class balance enough to hurt performance
- Evidence anchors:
  - [abstract]: "However, the dataset is limited, and the audio is sliced to increase its quantity, resulting in a lack of diversity and balance, so we use focal loss to reduce the impact of data imbalance."
  - [section]: "To mitigate the effects of sample imbalance, we utilized focal loss as the loss function for CNN backpropagation."
- Break condition: If class imbalance is negligible or if the slicing strategy is modified to equalize counts per class

### Mechanism 3
- Claim: ERB analysis provides psychoacoustic interpretability by modeling human auditory filtering when distinguishing piano sound quality
- Mechanism: ERB filters approximate the human ear's frequency resolution
- Core assumption: Human listeners perceive differences in piano timbre through variations in ERB-filtered frequency power, and these perceptual differences align with classification model decisions
- Evidence anchors:
  - [abstract]: "To improve the interpretability of the models, the study applies Equivalent Rectangular Bandwidth (ERB) analysis."
  - [section]: "Brian Moore proposed a polynomial equation for estimating the bandwidth of the auditory filter, specifically for young listeners and moderate sound levels."
- Break condition: If ERB analysis shows no clustering by piano type, the model may be relying on non-perceptual features

## Foundational Learning

- Concept: Short-Time Fourier Transform (STFT) and Mel-scale filtering
  - Why needed here: Converts raw audio waveforms into mel-spectrogram images suitable for CNN input
  - Quick check question: What is the purpose of mapping linear frequency bins to mel scale before feeding into the CNN?

- Concept: Transfer learning and fine-tuning of pretrained CNNs
  - Why needed here: Leverages ImageNet-pretrained models to avoid training from scratch and to exploit learned visual feature hierarchies for spectrogram classification
  - Quick check question: Why is it beneficial to freeze early CNN layers during initial fine-tuning?

- Concept: Loss functions for class imbalance (focal loss)
  - Why needed here: Addresses the unequal number of training samples per piano type after audio slicing
  - Quick check question: How does focal loss differ from standard cross-entropy in handling imbalanced datasets?

## Architecture Onboarding

- Component map: Audio file -> STFT -> Mel-spectrogram -> Resize -> Normalization -> CNN input
- Critical path:
  1. Load and preprocess audio into mel-spectrogram images
  2. Fine-tune CNN backbone on balanced dataset with focal loss
  3. Evaluate classification accuracy and interpretability via ERB analysis
  4. Integrate into mobile app for real-time scoring
- Design tradeoffs:
  - Mel-spectrogram resolution vs. model input size: Higher resolution increases feature richness but also computational cost
  - Backbone size vs. deployment constraints: SqueezeNet offers 98.3% accuracy with low resource use, suitable for mobile deployment
  - Focal loss vs. data augmentation: Focal loss handles imbalance without altering original audio content, preserving quality cues
- Failure signatures:
  - Low accuracy despite high training accuracy: Overfitting due to small dataset; mitigate with stronger regularization or data augmentation
  - Poor clustering in ERB visualization: Model may rely on non-perceptual features; check spectrogram preprocessing and model architecture
  - High training time with minimal accuracy gain: Inefficient fine-tuning schedule; try lower learning rates or layer-wise learning rates
- First 3 experiments:
  1. Compare classification accuracy of mel-spectrograms vs. raw waveform CNN inputs on the same small dataset
  2. Evaluate the effect of focal loss vs. class-balanced sampling on model robustness to imbalanced data
  3. Test ERB-based interpretability by visualizing t-SNE embeddings of CNN embeddings vs. ERB features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed CNN-based piano sound quality evaluation method be further improved to handle larger and more diverse datasets?
- Basis in paper: [explicit] The paper mentions that the dataset used is limited and that the method will be optimized by expanding the dataset or employing few-shot learning techniques in future research
- Why unresolved: The paper does not provide specific details on how to expand the dataset or implement few-shot learning techniques
- What evidence would resolve it: Future studies could demonstrate improved performance using larger, more diverse datasets or showcase the effectiveness of few-shot learning techniques in this context

### Open Question 2
- Question: Can the proposed method be extended to evaluate the sound quality of other musical instruments beyond pianos?
- Basis in paper: [inferred] The paper discusses the application of the method to piano sound quality evaluation, but it does not explicitly mention its potential for other instruments
- Why unresolved: The paper focuses on piano sound quality evaluation and does not explore the method's applicability to other instruments
- What evidence would resolve it: Future research could demonstrate the method's effectiveness in evaluating the sound quality of other musical instruments, such as violins, guitars, or drums

### Open Question 3
- Question: How can the interpretability of the CNN model be further improved to provide more detailed insights into the factors affecting piano sound quality?
- Basis in paper: [explicit] The paper mentions the use of ERB analysis to improve model interpretability, but it does not explore other potential methods for enhancing interpretability
- Why unresolved: The paper does not provide a comprehensive analysis of all possible methods for improving model interpretability
- What evidence would resolve it: Future studies could investigate additional techniques, such as feature importance analysis or attention mechanisms, to provide more detailed insights into the factors influencing piano sound quality

## Limitations
- Small dataset size (~1300 recordings) limits generalizability
- Study focuses only on grand piano brands, excluding upright pianos more relevant for beginners
- Slicing approach creates data imbalance that requires focal loss, potentially introducing artifacts

## Confidence

- **High confidence**: The CNN-based classification approach using mel-spectrograms achieves high accuracy (98.3%) on the test dataset
- **Medium confidence**: The focal loss effectively addresses data imbalance from audio slicing, though alternative sampling strategies could achieve similar results
- **Medium confidence**: ERB analysis provides meaningful interpretability for model decisions, but the psychoacoustic validity needs more rigorous validation

## Next Checks

1. **Dataset expansion validation**: Test model performance on a larger, more diverse dataset including upright pianos and recordings from different acoustic environments to verify scalability
2. **A/B testing with beginners**: Conduct user studies where actual beginners use the mobile app for real piano purchase decisions, comparing outcomes with and without the evaluation tool
3. **Alternative imbalance handling**: Compare focal loss performance against class-balanced sampling or data augmentation strategies to determine if the focal loss is essential or if simpler methods suffice