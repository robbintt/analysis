---
ver: rpa2
title: 'Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming'
arxiv_id: '2311.06237'
source_url: https://arxiv.org/abs/2311.06237
tags:
- language
- teaming
- what
- participants
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a grounded theory of large language model (LLM)
  red teaming in the wild, based on 28 in-depth interviews with practitioners. The
  study explores how and why people engage in deliberate generation of abnormal LLM
  outputs through adversarial activities like prompt engineering and jailbreaking.
---

# Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming

## Quick Facts
- arXiv ID: 2311.06237
- Source URL: https://arxiv.org/abs/2311.06237
- Reference count: 40
- Primary result: Grounded theory of LLM red teaming based on 28 interviews, identifying 12 strategies and 35 techniques

## Executive Summary
This paper presents a grounded theory of large language model (LLM) red teaming in the wild, based on 28 in-depth interviews with practitioners. The study explores how and why people engage in deliberate generation of abnormal LLM outputs through adversarial activities like prompt engineering and jailbreaking. The research defines red teaming as a limit-seeking, non-malicious, manual activity that depends on team effort and an alchemist mindset, and identifies 12 strategies and 35 techniques used in red teaming.

## Method Summary
The study employed qualitative grounded theory methodology, conducting 28 in-depth interviews with LLM red teaming practitioners from diverse backgrounds. Participants were recruited and interviewed about their definitions, motivations, goals, and strategies. The data was analyzed through open coding, axial coding, and selective coding to develop a grounded theory model of LLM red teaming in the wild.

## Key Results
- Red teaming defined as limit-seeking, non-malicious, manual activity requiring team effort and alchemist mindset
- 12 strategies and 35 techniques identified across language, rhetoric, possible worlds, fictionalizing, and stratagem categories
- Community knowledge sharing plays essential role in accelerating red teaming effectiveness
- Wicked problem framing explains why red teaming resists automation and requires human intuition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Red teaming becomes possible through the convergence of low technical barriers and natural language interfaces
- Mechanism: LLMs allow manipulation via plain text input, removing need for specialized coding or image processing skills
- Core assumption: LLM interface accepts natural language commands that can be crafted to override internal safeguards
- Evidence anchors:
  - [abstract] "one can enter natural language and work with the target through that medium alone"
  - [section 1] "appearance in late 2022 of accessible interfaces...led to explosion of grassroots approaches"
  - [corpus] Weak corpus evidence: only 25 related papers found

### Mechanism 2
- Claim: Community functions as distributed knowledge base that accelerates red teaming effectiveness
- Mechanism: Practitioners share jailbreak examples, techniques, and failed attempts openly on social platforms
- Evidence anchors:
  - [section 4.3] "online community...plays essential role in sharing knowledge and shaping red teaming strategies"
  - [section 4.3] "Many participants mentioned finding inspiration in other people's prompts and jailbreaks"
  - [corpus] No direct corpus support for community dynamics

### Mechanism 3
- Claim: Wicked problem framing explains why red teaming resists automation and requires human intuition
- Mechanism: Each adversarial prompt is one-off problem with no clear stopping rule; solutions not transferable across contexts
- Evidence anchors:
  - [section 4.4.1] "tasks that someone tackles in in-the-wild red teaming are often 'one-off' problems"
  - [section 4.4.1] "Many attacks or tasks have characteristics of wicked problems"
  - [section 4.4.1] "Intuition and experiential knowledge are considered essential skills in red teaming"

## Foundational Learning

- Concept: Grounded theory methodology
  - Why needed here: Study aims to define emergent practice without imposing pre-existing categories
  - Quick check question: What is difference between hypothesis and grounded theory in this context?

- Concept: Wicked problems in design
  - Why needed here: Red teaming tasks lack clear stopping rules and solutions, matching wicked problem archetype
  - Quick check question: Name two characteristics of wicked problem and relate them to red teaming

- Concept: Prompt injection mechanics
  - Why needed here: Core technique for bypassing LLM safeguards; understanding token-level processing essential
  - Quick check question: How does tokenizer's behavior enable stop sequence attacks?

## Architecture Onboarding

- Component map: Data source (28 qualitative interviews) -> Analysis pipeline (open coding → axial coding → selective coding) -> Output (grounded theory model with core category, strategies, techniques) -> Taxonomy (12 strategies, 35 techniques across language, rhetoric, possible worlds, fictionalizing, stratagem categories)

- Critical path:
  1. Recruit diverse participants
  2. Conduct semi-structured interviews with screen sharing
  3. Transcribe and annotate with open coding
  4. Cluster tags into categories
  5. Synthesize into grounded theory
  6. Validate through selective coding and peer review

- Design tradeoffs:
  - Open-ended interviews vs. structured surveys: Chose open-ended to capture tacit knowledge
  - Single vs. multiple coders: Used three coders to reduce bias but increased coordination overhead
  - Manual vs. automated coding: Chose manual for depth, accepting scalability limits

- Failure signatures:
  - Category saturation not reached: Continue interviewing
  - Contradictory participant definitions: Refine core category
  - Insufficient technique examples: Expand sampling to more practitioners

- First 3 experiments:
  1. Replicate one technique from each strategy category on public LLM to verify feasibility
  2. Conduct small-scale red teaming session with 3-5 participants to test community knowledge transfer
  3. Build simple prompt injection demo to illustrate tokenizer behavior and stop sequence exploitation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are long-term societal impacts of LLM red teaming becoming widespread activity?
- Basis in paper: [inferred]
- Why unresolved: Paper focuses on defining and characterizing current red teaming practices but does not explore potential future societal implications
- What evidence would resolve it: Longitudinal studies tracking evolution of red teaming practices and broader societal impacts over time

### Open Question 2
- Question: How do different demographic groups perceive and engage with LLM red teaming activities?
- Basis in paper: [explicit]
- Why unresolved: Paper notes significant gender skew in participants but does not deeply explore how different demographic groups perceive or engage with red teaming
- What evidence would resolve it: Surveys and interviews with diverse demographic groups about their perceptions and engagement with red teaming

### Open Question 3
- Question: What are most effective defense strategies against emerging red teaming techniques?
- Basis in paper: [inferred]
- Why unresolved: Paper provides taxonomy of red teaming strategies but does not explore potential defenses against these techniques
- What evidence would resolve it: Comparative studies testing various defense strategies against different red teaming approaches

## Limitations
- Relies entirely on self-reported data from 28 practitioners, introducing potential recall bias and social desirability effects
- Interpretation of tacit knowledge remains inherently subjective despite iterative coding and peer review
- Rapid evolution of LLM capabilities means some reported techniques may already be obsolete

## Confidence

*High Confidence*:
- Definition of red teaming as limit-seeking, non-malicious, manual activity requiring team effort and alchemist mindset
- Characterization of red teaming tasks as wicked problems requiring intuition and experiential knowledge
- Role of community knowledge sharing in accelerating red teaming effectiveness

*Medium Confidence*:
- 12 strategies and 35 techniques taxonomy, which may not capture all possible approaches
- Motivations for engaging in red teaming, which may vary significantly across different practitioner populations
- Challenges of managing and evaluating red teaming efforts in rapidly evolving field

## Next Checks
1. Conduct quantitative survey of 100+ practitioners to validate motivations and perceived effectiveness of different red teaming strategies, comparing results with qualitative findings

2. Perform longitudinal study tracking evolution of red teaming techniques over 6-12 months, documenting which strategies remain effective as models adapt and which become obsolete

3. Replicate study with practitioners from non-Western contexts and different linguistic backgrounds to assess cultural and language-specific variations in red teaming approaches and community dynamics