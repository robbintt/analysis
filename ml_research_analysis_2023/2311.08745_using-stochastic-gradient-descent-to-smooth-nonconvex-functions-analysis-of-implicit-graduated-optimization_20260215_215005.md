---
ver: rpa2
title: 'Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis
  of Implicit Graduated Optimization'
arxiv_id: '2311.08745'
source_url: https://arxiv.org/abs/2311.08745
tags:
- learning
- function
- rate
- optimization
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how stochastic gradient descent (SGD) with
  mini-batch gradients smooths non-convex functions. The authors show that the degree
  of smoothing is determined by the ratio of the learning rate to the batch size,
  and that smaller batch sizes and larger learning rates lead to more smoothing.
---

# Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization

## Quick Facts
- arXiv ID: 2311.08745
- Source URL: https://arxiv.org/abs/2311.08745
- Reference count: 40
- Primary result: SGD with mini-batch gradients smooths non-convex functions, with smoothing degree determined by learning rate to batch size ratio

## Executive Summary
This paper analyzes how stochastic gradient descent (SGD) with mini-batch gradients smooths non-convex objective functions during optimization. The authors show that the degree of smoothing is determined by the ratio of learning rate to batch size, with smaller batch sizes and larger learning rates producing more smoothing. They propose an implicit graduated optimization framework that gradually decreases learning rate and increases batch size during training, converging to an ε-neighborhood of the global optimum in O(1/ε^(1/p+2)) rounds for new σ-nice functions. Experiments on image classification tasks demonstrate the superiority of this approach compared to fixed learning rate and batch size methods.

## Method Summary
The paper proposes an implicit graduated optimization framework that leverages SGD's natural smoothing effect through carefully designed noise scheduling. The method uses decaying learning rates and increasing batch sizes during training, which implicitly solves a sequence of increasingly less smoothed versions of the original nonconvex problem. This approach starts from a globally convex-like landscape and gradually transitions to the original multimodal function. The framework is applied to CIFAR100 and ImageNet datasets using ResNet18, ResNet34, and WideResNet28-10 architectures, with four variants of SGD compared: constant learning rate and batch size, decaying learning rate with constant batch size, constant learning rate with increasing batch size, and decaying learning rate with increasing batch size.

## Key Results
- SGD with mini-batch gradients smooths non-convex functions, with smoothing degree determined by η/√b
- Large batch sizes lead to solutions falling into sharp local minima due to insufficient function smoothing
- Decaying learning rates and increasing batch sizes implicitly perform graduated optimization by gradually reducing noise levels
- The framework converges to an ε-neighborhood of the global optimum in O(1/ε^(1/p+2)) rounds for new σ-nice functions
- Experiments show superior performance compared to fixed learning rate and batch size methods on CIFAR100

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic gradient descent with mini-batch gradients smooths the objective function, with the degree of smoothing determined by the ratio of learning rate to batch size.
- Mechanism: The stochastic noise in SGD (difference between mini-batch gradient and full gradient) acts as a convolution with uniform noise, creating a smoothed version of the objective function. The noise level is proportional to η/√b where η is learning rate and b is batch size.
- Core assumption: The stochastic gradient noise can be modeled as Gaussian-like noise that effectively smooths the function when convolved with the objective.
- Evidence anchors:
  - [abstract]: "stochastic gradient descent (SGD) with mini-batch stochastic gradients has the effect of smoothing the function, the degree of which is determined by the learning rate and batch size"
  - [section 3.2]: "we can say that the degree of smoothing by the stochastic gradients in SGD is determined by the learning rate η and batch size b"
  - [corpus]: Weak - no direct citations about SGD smoothing mechanisms in related papers
- Break condition: If the stochastic noise distribution deviates significantly from the assumed uniform/gaussian-like noise, or if the learning rate is too high causing divergence rather than smoothing.

### Mechanism 2
- Claim: Large batch sizes lead to solutions falling into sharp local minima due to insufficient function smoothing.
- Mechanism: Larger batch sizes reduce the noise level (η/√b), resulting in weaker smoothing. This keeps the optimization landscape closer to the original multimodal function, making it easier to get trapped in sharp local minima with poor generalization.
- Core assumption: Sharp local minima correspond to regions of the loss landscape that require significant smoothing to escape, and the original function's multimodality is preserved with large batch sizes.
- Evidence anchors:
  - [abstract]: "why large batch sizes fall into sharp local minima"
  - [section 3.2.1]: "using a large batch size, the smoothing is weak and the function is close to the original multimodal function, so it is easy for the solution to fall into sharp local minima"
  - [corpus]: Weak - related papers don't discuss batch size effects on local minima sharpness
- Break condition: If the optimization landscape doesn't contain sharp local minima, or if other factors (like architecture or data distribution) dominate the generalization performance.

### Mechanism 3
- Claim: Decaying learning rates and increasing batch sizes during training implicitly perform graduated optimization by gradually reducing noise levels.
- Mechanism: By reducing the noise level (through smaller η or larger b) over time, the algorithm implicitly solves a sequence of increasingly less smoothed versions of the original problem, starting from a globally convex-like landscape and ending at the original multimodal function.
- Core assumption: The sequence of noise levels created by decaying learning rates and increasing batch sizes provides a smooth path from a convex-like optimization problem to the original nonconvex problem.
- Evidence anchors:
  - [abstract]: "using a decaying learning rate and increasing the batch size makes sense in terms of avoiding local minima"
  - [section 3.2.2]: "using a decaying learning rate or increasing batch size during training is equivalent to decreasing the noise level of the smoothed function, so using a decaying learning rate or increasing the batch size is an implicit graduated optimization"
  - [corpus]: Weak - related papers don't discuss graduated optimization or noise scheduling
- Break condition: If the noise reduction schedule is too aggressive or too gradual, preventing proper convergence to the global optimum.

## Foundational Learning

- Concept: Stochastic gradient descent and its noise properties
  - Why needed here: Understanding how mini-batch gradients introduce noise that affects optimization behavior is fundamental to the paper's main claim about function smoothing
  - Quick check question: What is the difference between the full gradient and mini-batch gradient, and how does this difference affect optimization?

- Concept: Graduated optimization and homotopy methods
  - Why needed here: The paper builds on the concept of graduated optimization, where optimization problems are solved by gradually changing parameters to transition from simple to complex problems
  - Quick check question: How does graduated optimization differ from standard optimization approaches, and what are its key benefits for nonconvex problems?

- Concept: Strongly convex functions and their properties
  - Why needed here: The convergence analysis relies on the smoothed functions being strongly convex in certain neighborhoods, which guarantees convergence to the optimum
  - Quick check question: What are the key properties of strongly convex functions that make them easier to optimize than general convex or nonconvex functions?

## Architecture Onboarding

- Component map: Mini-batch stochastic gradients -> Noise generation (ωt) -> Function smoothing (f̂δ) -> Noise scheduling (η decay, b increase) -> Implicit graduated optimization -> Convergence to global optimum
- Critical path: Noise generation → Function smoothing → Graduated optimization → Convergence to global optimum
- Design tradeoffs:
  - Smaller batch sizes provide more smoothing but require more parameter updates
  - Larger learning rates provide more smoothing but risk instability
  - Noise scheduling must be gradual enough to maintain strong convexity in neighborhoods
- Failure signatures:
  - Poor generalization with large batch sizes (sharp local minima)
  - Slow convergence with aggressive noise reduction
  - Divergence with excessive learning rates
  - Failure to converge with insufficient noise reduction
- First 3 experiments:
  1. Compare training with constant learning rate and batch size vs decaying learning rate with constant batch size on a simple nonconvex problem
  2. Compare training with constant learning rate and increasing batch size vs decaying learning rate and increasing batch size on CIFAR100
  3. Test different noise scheduling strategies (polynomial decay with different powers) on WideResNet to verify optimal scheduling claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact conditions under which a function is "new σ-nice" beyond the sufficient conditions provided in Proposition 3.2?
- Basis in paper: [explicit] The paper provides sufficient conditions but states "Note that δ− does not always exist."
- Why unresolved: The paper only provides sufficient conditions, not necessary and sufficient conditions, and notes that δ− may not exist.
- What evidence would resolve it: A complete characterization of the set of functions that are "new σ-nice," including both necessary and sufficient conditions, would resolve this.

### Open Question 2
- Question: How does the optimal noise scheduling in graduated optimization compare to the optimal learning rate scheduling in standard deep learning?
- Basis in paper: [explicit] The paper states "polynomial decay with power less than or equal to 1 is the optimal learning rate schedule" and discusses the equivalence between noise decay and learning rate/batch size scheduling.
- Why unresolved: While the paper establishes a theoretical link, it doesn't empirically compare the optimal noise scheduling in graduated optimization to the optimal learning rate scheduling in standard deep learning.
- What evidence would resolve it: Experiments comparing different noise scheduling strategies in graduated optimization to different learning rate scheduling strategies in standard deep learning would help resolve this.

### Open Question 3
- Question: What is the precise relationship between the parameters η, b, and C in the smoothing property of SGD, and how do they interact in practice?
- Basis in paper: [explicit] The paper derives that the smoothing effect is determined by the ratio ηC/√b, where C is a constant from the variance of the stochastic gradients.
- Why unresolved: While the paper provides the theoretical relationship, it doesn't explore how these parameters interact in practice or how to choose them optimally for a given problem.
- What evidence would resolve it: Empirical studies investigating the interaction of η, b, and C in various deep learning tasks, along with guidelines for choosing these parameters, would help resolve this.

## Limitations

- The convergence analysis assumes the smoothed functions remain strongly convex in certain neighborhoods, which may not hold for all nonconvex functions
- The paper lacks empirical validation of the noise scheduling strategies on a diverse set of tasks and architectures beyond image classification
- The theoretical guarantees are based on assumptions about stochastic gradient noise that may not hold in practice for complex neural networks

## Confidence

- **High Confidence**: The mechanism by which SGD smooths functions through stochastic noise is well-established in the optimization literature
- **Medium Confidence**: The proposed implicit graduated optimization framework is theoretically sound, but its practical effectiveness depends on the specific noise scheduling and problem characteristics
- **Low Confidence**: The convergence guarantees for the new σ-nice functions are based on theoretical assumptions that may not hold in practice, especially for complex neural networks and datasets

## Next Checks

1. **Noise Distribution Analysis**: Empirically analyze the distribution of stochastic gradient noise for different batch sizes and learning rates on CIFAR100 using ResNet18. Verify that the noise approximates Gaussian-like noise and that the smoothing effect increases with the ratio of learning rate to batch size.

2. **Convergence Analysis**: Conduct a convergence analysis of the implicit graduated optimization framework on a simple nonconvex function with known global optimum. Vary the noise scheduling parameters (polynomial decay power, batch size increase rate) and measure the convergence rate and final solution quality.

3. **Generalization Study**: Compare the generalization performance of the implicit graduated optimization framework against standard SGD with constant learning rate and batch size on a diverse set of tasks (e.g., CIFAR100, ImageNet, NLP tasks). Analyze the effect of noise scheduling on the sharpness of local minima and the distance to the global optimum.