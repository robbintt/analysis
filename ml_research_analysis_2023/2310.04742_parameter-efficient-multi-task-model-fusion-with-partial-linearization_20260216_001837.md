---
ver: rpa2
title: Parameter Efficient Multi-task Model Fusion with Partial Linearization
arxiv_id: '2310.04742'
source_url: https://arxiv.org/abs/2310.04742
tags:
- tasks
- fine-tuning
- task
- lora
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method to improve multi-task model
  fusion for parameter-efficient fine-tuning techniques. The key idea is to partially
  linearize only the adapter modules (like LoRA) and apply model fusion algorithms
  over the linearized adapters.
---

# Parameter Efficient Multi-task Model Fusion with Partial Linearization

## Quick Facts
- arXiv ID: 2310.04742
- Source URL: https://arxiv.org/abs/2310.04742
- Reference count: 40
- This paper proposes a novel method to improve multi-task model fusion for parameter-efficient fine-tuning techniques by partially linearizing adapter modules like LoRA.

## Executive Summary
This paper introduces a novel approach for multi-task model fusion when using parameter-efficient fine-tuning methods like LoRA. The key insight is that by partially linearizing only the adapter modules during fine-tuning, task vectors become more orthogonal, reducing destructive interference during model fusion. This enables more effective combination of multiple task-specific models into a single unified model while maintaining computational efficiency. Experiments on both vision (CLIP) and language (Flan-T5) tasks demonstrate that this partial linearization technique outperforms standard LoRA fine-tuning when applying model fusion algorithms.

## Method Summary
The proposed method involves creating a linearized version of LoRA adapter modules and performing fine-tuning in the tangent space of the pre-trained model. Specifically, a LinearizedModelWrapper is applied to LoRA modules, allowing fine-tuning to occur in a linearized space where task vectors are more orthogonal. After individual task fine-tuning, model fusion algorithms (simple averaging, task arithmetic, ties-merging, and LoraHub) are applied to these linearized task vectors. The approach maintains computational efficiency by only linearizing the adapter modules rather than the entire model, while still enabling effective multi-task fusion through improved weight disentanglement.

## Key Results
- Partial linearization of LoRA adapters improves weight disentanglement for multi-task fusion
- L-LoRA fine-tuning achieves higher average normalized scores across tasks compared to standard LoRA
- The method outperforms both standard adapter tuning and task arithmetic alone
- Results demonstrate benefits on both vision (CLIP) and language (Flan-T5) tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linearizing LoRA modules improves weight disentanglement for multi-task fusion
- Mechanism: Task vectors from LoRA fine-tuning are more entangled (higher cosine similarity) than those from full fine-tuning. Linearizing the adapter modules creates a linear subspace where fine-tuning occurs, reducing destructive interference between task-specific representations during model fusion.
- Core assumption: Weight disentanglement improves when task vectors are more orthogonal in weight space
- Evidence anchors:
  - [abstract] "our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters"
  - [section 2] "Naively combining models that were fine-tuned in a parameter-efficient manner can more readily result in representational interference between tasks"
  - [corpus] "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic" supports that targeted linearization improves disentanglement

### Mechanism 2
- Claim: Partial linearization provides computational efficiency while maintaining effectiveness
- Mechanism: Only the adapter modules are linearized and fine-tuned, while the backbone remains fixed. This reduces computational cost compared to full linearization while still capturing task-specific knowledge in the adapters.
- Core assumption: Adapter modules contain sufficient task-specific information for effective multi-task fusion
- Evidence anchors:
  - [abstract] "partially linearizes only the adapter modules" and "while still performing fine-tuning and inference efficiently"
  - [section 4.1] "Our key insight is that we can perform efficient fine-tuning and disentangle task representations by only linearizing a subset of parameters"
  - [corpus] Weak - no direct evidence found for computational efficiency claims

### Mechanism 3
- Claim: L-LoRA fine-tuning enables more effective multi-task fusion than standard LoRA
- Mechanism: By fine-tuning in tangent space, L-LoRA creates task vectors that are more orthogonal, reducing interference during arithmetic operations for multi-task fusion.
- Core assumption: Orthogonal task vectors enable better preservation of specialized knowledge during model merging
- Evidence anchors:
  - [section 4.1] "task vectors from L-LoRA fine-tuning are closer to orthogonal than those from LoRA"
  - [section 5.1] "L-LoRA fine-tuning surpasses LoRA fine-tuning with all four model fusion algorithms"
  - [corpus] "Beyond Task Vectors: Selective Task Arithmetic Based on Importance Metrics" suggests selective task vector arithmetic improves fusion

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and linearized models
  - Why needed here: The paper relies on understanding how linearized models approximate neural network behavior in tangent space
  - Quick check question: What is the key difference between a nonlinear model f(θ) and its linearized approximation f_lin(θ)?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: The paper builds on LoRA and other PEFT techniques as the foundation for the proposed approach
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Task arithmetic and model fusion
  - Why needed here: The paper extends task arithmetic techniques from fully fine-tuned models to PEFT settings
  - Quick check question: What is the primary assumption underlying the effectiveness of task arithmetic?

## Architecture Onboarding

- Component map: Pre-trained backbone (fixed) -> LoRA adapters (linearized during fine-tuning) -> Task arithmetic operations (performed in linearized space) -> Model fusion algorithms (applied to task vectors)

- Critical path: LoRA modules → Linearization → Fine-tuning in tangent space → Task vector extraction → Model fusion → Evaluation

- Design tradeoffs:
  - Linearization vs. expressiveness: More linearization improves fusion but may reduce single-task performance
  - Computational cost vs. effectiveness: Full linearization is expensive; partial linearization balances cost and benefit
  - Adapter capacity vs. task complexity: Larger adapters may capture more complex task knowledge but increase parameters

- Failure signatures:
  - Poor single-task performance: Indicates linearization may be too restrictive for the task
  - Interference in multi-task models: Suggests task vectors are not sufficiently orthogonal
  - Computational inefficiency: May indicate incorrect implementation of linearization or Jacobian computation

- First 3 experiments:
  1. Implement linearization wrapper and verify it produces correct linearized outputs on a simple MLP
  2. Compare task vector similarity between LoRA and L-LoRA on a pair of simple tasks
  3. Test multi-task fusion performance on 2-3 tasks using both LoRA and L-LoRA with simple averaging

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of parameter-efficient fine-tuning (PEFT) method affect the effectiveness of partial linearization for multi-task model fusion?
- Basis in paper: [explicit] The paper mentions that the proposed partial linearization technique is applied to LoRA modules, but it does not explore other PEFT methods like adapters or prefix tuning.
- Why unresolved: The paper only focuses on LoRA fine-tuning and does not compare or analyze the impact of using different PEFT methods on the effectiveness of partial linearization for multi-task model fusion.
- What evidence would resolve it: Experiments comparing the performance of partial linearization with different PEFT methods, such as adapters or prefix tuning, would provide evidence on the generalizability and effectiveness of the proposed approach across various PEFT techniques.

### Open Question 2
- Question: How does the degree of linearization (i.e., the proportion of parameters linearized) impact the trade-off between task-specific performance and multi-task fusion capability?
- Basis in paper: [inferred] The paper proposes partial linearization of PEFT modules, but it does not explore the effect of varying the degree of linearization on the performance of single-task models versus multi-task fusion.
- Why unresolved: The paper does not investigate how different levels of linearization (e.g., linearizing a larger or smaller subset of parameters) affect the balance between maintaining high task-specific performance and enabling effective multi-task fusion.
- What evidence would resolve it: Experiments systematically varying the proportion of parameters linearized and evaluating the resulting trade-off between single-task accuracy and multi-task fusion performance would provide insights into the optimal degree of linearization for different use cases.

### Open Question 3
- Question: Can the proposed partial linearization technique be extended to other model architectures beyond vision and language transformers?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of partial linearization on CLIP and Flan-T5 models, but it does not explore its applicability to other model architectures such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs).
- Why unresolved: The paper only focuses on transformer-based models and does not investigate whether the proposed technique can be effectively applied to other model architectures commonly used in different domains.
- What evidence would resolve it: Experiments applying partial linearization to a diverse set of model architectures, including CNNs, RNNs, and other non-transformer models, and evaluating their performance in multi-task model fusion would provide evidence on the generalizability of the proposed technique across different model types.

## Limitations
- Computational efficiency gains are not explicitly quantified
- Evaluation is limited to CLIP and Flan-T5 architectures
- Does not investigate the impact of adapter capacity on multi-task fusion performance
- Long-term stability of fused models and performance on unseen tasks is not evaluated

## Confidence

**High confidence**: The core mechanism of linearizing adapter modules for improved task vector orthogonality is well-supported by theoretical foundations in neural tangent kernels and demonstrated through experimental results showing improved cosine similarity between task vectors.

**Medium confidence**: The claim that partial linearization provides computational efficiency while maintaining effectiveness is plausible but lacks direct quantitative evidence comparing computational costs across different approaches.

**Low confidence**: The assertion that L-LoRA consistently outperforms standard LoRA across all fusion algorithms and task combinations is based on limited experimental results and may not generalize to different model architectures or task distributions.

## Next Checks

1. **Computational overhead analysis**: Measure and compare the actual computational costs (memory usage and training time) of L-LoRA versus standard LoRA fine-tuning across different adapter sizes and task complexities.

2. **Cross-architecture generalization**: Test the partial linearization approach on additional model architectures (e.g., BERT, ResNet) to verify that the benefits observed with CLIP and Flan-T5 extend to other vision and language models.

3. **Long-term stability evaluation**: Evaluate the performance of fused multi-task models on held-out tasks not seen during the fusion process, and assess performance degradation over extended inference periods to validate the robustness of the approach.