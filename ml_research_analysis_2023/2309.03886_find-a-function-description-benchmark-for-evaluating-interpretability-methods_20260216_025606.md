---
ver: rpa2
title: 'FIND: A Function Description Benchmark for Evaluating Interpretability Methods'
arxiv_id: '2309.03886'
source_url: https://arxiv.org/abs/2309.03886
tags:
- function
- functions
- find
- neural
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIND is a benchmark suite for evaluating automated interpretability
  methods on functions whose structure is known a priori. It contains over 2000 procedurally
  generated function interpretation problems spanning numeric, string, and synthetic
  neural module domains, with accompanying ground-truth descriptions.
---

# FIND: A Function Description Benchmark for Evaluating Interpretability Methods

## Quick Facts
- arXiv ID: 2309.03886
- Source URL: https://arxiv.org/abs/2309.03886
- Reference count: 29
- Key outcome: GPT-4 fails to fully characterize 48% of functions in FIND benchmark despite interactive experimentation capabilities

## Executive Summary
FIND is a benchmark suite for evaluating automated interpretability methods on functions with known a priori structure. It contains over 2000 procedurally generated function interpretation problems spanning numeric, string, and synthetic neural module domains, with ground-truth descriptions. The benchmark tests black-box function description capabilities including hypothesis formation, experimentation, and description generation. Evaluation of several language model interpreters shows that while GPT-4 can infer function structure through experimentation, it fails to fully characterize 48% of functions and tends to miss local details while capturing global behavior.

## Method Summary
The benchmark evaluates interpretability methods by having interpreters interact with black-box functions to produce descriptions that are compared against ground-truth descriptions. Interpreters can evaluate functions on chosen inputs to form and test hypotheses about structure. The benchmark includes numeric functions (arithmetic and logical operations), string functions (text transformations and pattern matching), and synthetic neural modules (factored representations of neural network components). Evaluation uses success indicators including NMSE thresholds for code-based interpretations and unit testing protocols for language-based descriptions.

## Key Results
- GPT-4 fails to fully characterize 48% of functions despite being able to conduct experiments that reveal behavior
- LM-based descriptions capture global function behavior but miss local details like domain corruptions
- Initialization with exemplars dramatically improves LM interpreter performance
- Composed functions are significantly more difficult to interpret than atomic functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark works by providing ground-truth function structures that allow rigorous evaluation of interpretability methods.
- Mechanism: FIND generates functions with known internal structures and compares interpreter outputs against these ground truths using standardized metrics.
- Core assumption: The ground-truth descriptions accurately capture the intended function behavior and are sufficiently comprehensive for evaluation purposes.
- Evidence anchors:
  - [abstract] "FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate."
  - [section 3.2] "To evaluate the accuracy of function descriptions produced by an interpreter, we use success indicators for individual function interpretations and calculate average success rates across all functions."
- Break condition: If ground-truth descriptions are incomplete, ambiguous, or do not capture all relevant aspects of function behavior, evaluation results become unreliable.

### Mechanism 2
- Claim: The benchmark works by testing interpreters' ability to form and test hypotheses through experimentation.
- Mechanism: Interpreters interact with black-box functions by evaluating them on chosen inputs, forming hypotheses about structure, proposing experiments, and updating descriptions based on results.
- Core assumption: The experimental process effectively reveals function structure and interpreters can translate experimental results into accurate descriptions.
- Evidence anchors:
  - [abstract] "FIND also reveals that LM-based descriptions tend to capture global function behavior and miss local details."
  - [section 3.1] "The interpreter is prompted to run experiments on FIND functions to produce data that it can use to explain function behavior."
- Break condition: If interpreters cannot design effective experiments or fail to extract meaningful information from experimental results, the evaluation cannot properly assess their capabilities.

### Mechanism 3
- Claim: The benchmark works by providing diverse function categories that exercise different interpretability capabilities.
- Mechanism: FIND includes numeric functions, string functions, and synthetic neural modules, each requiring different interpretation strategies.
- Core assumption: The diversity of function types adequately represents the range of interpretability challenges found in real neural networks.
- Evidence anchors:
  - [abstract] "The functions span textual and numeric domains, and involve a range of real-world complexities."
  - [section 2] Detailed description of different function categories and their purposes
- Break condition: If the function categories are too narrow or do not capture important interpretability challenges, the benchmark provides an incomplete evaluation.

## Foundational Learning

- Concept: Black-box function interpretation
  - Why needed here: The benchmark tests interpreters' ability to understand functions without access to internal structure, which is the core challenge in neural network interpretability.
  - Quick check question: Can you explain how an interpreter would determine the behavior of a function that takes a country name and returns its capital, but returns undefined for South American countries?

- Concept: Experimental hypothesis testing
  - Why needed here: Interpreters must form hypotheses about function behavior and design experiments to test these hypotheses, which is a fundamental scientific method applied to function understanding.
  - Quick check question: Given a function that appears to reverse strings, what experimental inputs would you use to confirm this hypothesis and identify any exceptions?

- Concept: Ground-truth evaluation metrics
  - Why needed here: The benchmark requires standardized ways to compare interpreter outputs against ground-truth descriptions, which is essential for objective evaluation.
  - Quick check question: How would you evaluate whether an interpreter's description of a numeric function is accurate enough to reproduce the function's behavior?

## Architecture Onboarding

- Component map: Function generation system -> Interpreter interface -> Hypothesis formation -> Experimental testing -> Description generation -> Ground-truth comparison -> Success scoring
- Critical path: Function generation → Interpreter interaction → Hypothesis formation → Experimental testing → Description generation → Ground-truth comparison → Success scoring
- Design tradeoffs:
  - Black-box vs white-box access: Black-box forces experimental approaches but may miss structural details
  - Ground-truth comprehensiveness vs practicality: More detailed ground truths are better but harder to create
  - Function complexity vs interpreter capability: More complex functions test limits but may be too difficult for baseline interpreters
- Failure signatures:
  - Interpreter generates descriptions that don't match ground truth but pass unit tests (overly general descriptions)
  - Vicuna evaluator fails to distinguish between similar function descriptions
  - Function generation produces ambiguous or underspecified ground truths
- First 3 experiments:
  1. Run a simple numeric function through GPT-4 interpreter and verify the interpretation dialogue and success scoring work correctly
  2. Test the Vicuna evaluator on ground-truth function descriptions to confirm it achieves near-ceiling accuracy
  3. Run the initialization experiment with exemplars on a synthetic neural module to verify the performance improvement effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between interactive experimentation and exemplar initialization for automated interpretability methods?
- Basis in paper: [explicit] The paper shows that initialization with exemplars dramatically improves LM interpreter performance, but also demonstrates that interactive experimentation is valuable for uncovering complex function behaviors.
- Why unresolved: The paper presents both approaches separately and shows their individual benefits, but does not explore optimal combinations or sequences of initialization and experimentation.
- What evidence would resolve it: A systematic study comparing different initialization-experimentation schedules, measuring both efficiency (number of function calls) and accuracy of final descriptions.

### Open Question 2
- Question: How do different types of noise (normal, uniform, Poisson) affect the ability of interpretability methods to recover underlying function structure?
- Basis in paper: [explicit] The paper includes additive noise in numeric functions but only mentions testing with different noise types without reporting comparative results.
- Why unresolved: The paper tests noise presence but doesn't analyze how different noise distributions impact interpretation success rates or what strategies work best for each type.
- What evidence would resolve it: A detailed analysis of interpretation success rates across different noise types, including examination of how interpreters detect and characterize noise versus signal.

### Open Question 3
- Question: What is the relationship between function complexity (atomic vs. composed) and the depth of interpretation required for accurate characterization?
- Basis in paper: [inferred] The paper shows that composed functions are more difficult to interpret and that interpreters use deeper reasoning chains, but doesn't quantify this relationship.
- Why unresolved: While the paper demonstrates that composition increases difficulty, it doesn't establish whether this follows a predictable pattern or how many levels of composition can be reliably handled.
- What evidence would resolve it: A systematic study measuring interpretation success rates as a function of composition depth, potentially revealing limits of current interpretability methods.

## Limitations
- Benchmark evaluation relies heavily on completeness and accuracy of ground-truth descriptions
- Synthetic neural module functions depend on Vicuna inference server implementation
- Black-box evaluation may not capture all aspects of interpretability possible with white-box access

## Confidence
- **High confidence:** The benchmark's core methodology (ground-truth comparison with success indicators) is clearly specified and technically sound
- **Medium confidence:** Results showing GPT-4's 48% failure rate and the benefits of exemplar initialization are reproducible
- **Medium confidence:** The finding that LMs capture global but miss local behavior is well-supported

## Next Checks
1. Conduct ablation studies on ground-truth description quality by systematically removing details and measuring impact on evaluation accuracy
2. Test the benchmark with additional interpreter models beyond the three evaluated (GPT-4, GPT-3.5, Llama-2) to verify general applicability
3. Implement a white-box version of the evaluation to compare black-box interpretation performance against full access scenarios, measuring the information loss from black-box constraints