---
ver: rpa2
title: 'CLadder: Assessing Causal Reasoning in Language Models'
arxiv_id: '2312.04350'
source_url: https://arxiv.org/abs/2312.04350
tags:
- causal
- cited
- page
- inference
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CLadder, a new dataset for evaluating causal
  reasoning in large language models (LLMs). Unlike prior work on commonsense causality,
  CLadder focuses on formal causal reasoning, using an oracle causal inference engine
  to generate ground truth answers to questions spanning all three rungs of Pearl's
  Ladder of Causation.
---

# CLadder: Assessing Causal Reasoning in Language Models

## Quick Facts
- arXiv ID: 2312.04350
- Source URL: https://arxiv.org/abs/2312.04350
- Authors: 
- Reference count: 40
- Key outcome: Introduces CLadder dataset with 10,000 questions testing formal causal reasoning across Pearl's three rungs; even GPT-4 achieves only 66.6% accuracy

## Executive Summary
This paper introduces CLadder, a benchmark dataset designed to evaluate large language models' (LLMs) ability to perform formal causal reasoning across all three rungs of Pearl's Ladder of Causation. Unlike prior work focusing on commonsense causality, CLadder uses an oracle causal inference engine to generate ground truth answers to questions involving association, intervention, and counterfactuals. The dataset includes 10,000 questions verbalized from symbolic queries, with stories spanning commonsensical, anti-commonsensical, and nonsensical scenarios to isolate true causal reasoning from pattern matching.

Experiments show that even the best-performing model (GPT-4) achieves only 66.6% accuracy, revealing significant limitations in LLM causal reasoning capabilities. The authors introduce CausalCoT, a bespoke chain-of-thought prompting strategy that improves performance by 2.36 points over vanilla GPT-4. Detailed analysis reveals that LLMs struggle particularly with the formalization and evaluation steps of causal inference, suggesting that current models lack robust formal reasoning capabilities despite their impressive performance on other reasoning tasks.

## Method Summary
CLadder uses a two-part data generation pipeline: a formal component that employs an oracle causal inference engine to generate symbolic causal queries and ground truth answers, and a natural language component that verbalizes these queries into stories across three empirical alignment types (commonsensical, anti-commonsensical, nonsensical). The evaluation framework tests multiple LLMs including GPT-4, GPT-3.5, LLaMA, and Alpaca using a custom chain-of-thought prompting strategy called CausalCoT that guides models through five subquestions: extracting causal graph structure, classifying query type, deriving estimand, parsing available data, and solving the inference problem.

## Key Results
- GPT-4 achieves only 66.6% accuracy on CLadder, demonstrating significant challenges in formal causal reasoning
- CausalCoT prompting strategy improves GPT-4 performance by 2.36 percentage points over vanilla prompting
- Performance on anti-commonsensical and nonsensical stories is substantially lower than on commonsensical stories, suggesting LLMs rely on pattern matching rather than formal causal inference
- LLMs struggle particularly with formalization and evaluation steps of causal inference, despite relatively better performance on graph extraction and query classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The oracle causal inference engine provides formally correct ground truth answers that enable principled evaluation of LLM causal reasoning.
- Mechanism: By generating symbolic questions and ground truth answers through a CI engine that implements Pearl's causal inference framework, the dataset ensures that every question has a mathematically verifiable correct answer. This grounds the evaluation in formal causal reasoning rather than commonsense knowledge.
- Core assumption: The CI engine correctly implements all necessary causal inference rules (do-calculus, counterfactual prediction, etc.) and that the causal graphs and data provided are sufficient for identifiability.
- Evidence anchors:
  - [abstract]: "using an oracle causal inference engine to generate ground truth answers to questions spanning all three rungs of Pearl's Ladder of Causation"
  - [section]: "The ground truth causal models, which specify all quantities which are considered measurable in our questions, are causal Bayesian networks (CBNs)... The estimand also specifies exactly which terms are necessary to include in the prompt as 'available data'"

### Mechanism 2
- Claim: CLadder's multi-story verbalization approach (commonsensical, anti-commonsensical, nonsensical) isolates LLM performance on formal causal reasoning vs. pattern matching.
- Mechanism: By creating verbalized versions of the same formal causal question using different types of stories, the dataset can distinguish between LLMs that correctly apply causal inference rules versus those that rely on memorized commonsense patterns. Anti-commonsensical and nonsensical stories prevent successful pattern matching.
- Core assumption: The verbalization templates preserve the formal structure of the causal question while changing only the surface-level story, and that LLMs cannot easily generalize causal inference rules across drastically different story contexts.
- Evidence anchors:
  - [abstract]: "To probe whether LLMs employ amortized causal inference, we construct stories with commonsensical, as well as anti-commonsensical and with nonsensical causal relations"
  - [section]: "we also generate various anti-common sense and nonsensical variants of the stories, meant to isolate the effects of memorization"

### Mechanism 3
- Claim: CausalCoT's multi-step prompting strategy guides LLMs through the formal causal inference process, improving performance on challenging questions.
- Mechanism: By breaking down causal reasoning into sequential subquestions (extract causal graph, classify query type, derive estimand, collect data, solve), CausalCoT imposes an inductive bias toward systematic causal inference rather than pattern matching. This mirrors the CI engine's reasoning process.
- Core assumption: LLMs have sufficient internal reasoning capabilities to perform each of the sub-steps when properly prompted, and that the chain-of-thought format helps maintain reasoning coherence across steps.
- Evidence anchors:
  - [abstract]: "we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT"
  - [section]: "We therefore explore the combination of this CI engine idea with LLMs to address causal questions asked in natural language"

## Foundational Learning

- Concept: Pearl's Ladder of Causation (Rung 1: Association, Rung 2: Intervention, Rung 3: Counterfactuals)
  - Why needed here: CLadder explicitly evaluates all three rungs, requiring understanding of the distinctions between statistical associations, interventional queries (do-calculus), and counterfactual reasoning.
  - Quick check question: What is the key difference between P(Y|X) and P(Y|do(X)) in terms of what causal information they capture?

- Concept: Do-calculus and identification of causal effects
  - Why needed here: Many CLadder questions require applying do-calculus rules to transform interventional queries into estimable expressions using observational data.
  - Quick check question: Given a simple confounding structure X ← Z → Y, what adjustment formula would you use to identify E[Y|do(X=1)] - E[Y|do(X=0)]?

- Concept: Structural Causal Models (SCMs) and counterfactual reasoning
  - Why needed here: Rung 3 questions require understanding how SCMs support counterfactual queries through abduction, action, and prediction steps.
  - Quick check question: In a simple SCM with X → Y and exogenous U, how would you compute P(Yx=1 | X=0, U=u) for a specific value of u?

## Architecture Onboarding

- Component map: Data generation pipeline (formal part → natural language part) -> Model evaluation (multiple LLMs + CausalCoT) -> Analysis framework (roscoe metrics + ablation studies)
- Critical path: Data generation → Model evaluation → Error analysis → Insight generation
  The data generation must be complete and correct before any model evaluation can occur, and error analysis depends on having model outputs to analyze.
- Design tradeoffs: Formal correctness vs. natural language fluency in verbalization
  The dataset prioritizes formal causal correctness but must also maintain natural language quality to properly evaluate LLM capabilities. This creates tension in template design.
- Failure signatures: Low performance on anti-commonsensical/nonsensical stories indicates pattern matching rather than true causal reasoning; errors in specific sub-steps of CausalCoT reveal particular reasoning weaknesses.
- First 3 experiments:
  1. Evaluate vanilla GPT-4 on the full CLadder dataset to establish baseline performance
  2. Apply CausalCoT prompting to the same questions and measure improvement
  3. Analyze performance differences across story types (commonsensical vs. anti-commonsensical vs. nonsensical) to identify pattern matching vs. reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend CLadder to include more complex causal queries, such as path-specific effects other than natural direct and indirect effects?
- Basis in paper: [explicit] The paper mentions that future work may extend the dataset to include further queries like path-specific effects, moving closer to the ideal mini-Turing test for causality.
- Why unresolved: The current dataset covers only some commonly studied causal queries. Including more complex queries would require developing new methods for generating and verbalizing questions, as well as expanding the causal inference engine.
- What evidence would resolve it: A new version of CLadder that includes a wider range of causal queries, along with experiments demonstrating the performance of LLMs on these new query types.

### Open Question 2
- Question: Can plug-ins or external modules significantly enhance the ability of LLMs to perform formal causal reasoning?
- Basis in paper: [explicit] The discussion section suggests that providing LLMs with access to an actual implementation of the causal inference engine, similar to how plug-ins enhance math abilities, could be an interesting direction for future research.
- Why unresolved: While the idea is promising, it remains unclear how effective plug-ins would be for causal inference tasks and what challenges might arise in the language-to-tool interface.
- What evidence would resolve it: Experiments comparing the performance of LLMs with and without plug-ins on causal reasoning tasks, along with an analysis of the challenges and limitations of the language-to-tool interface.

### Open Question 3
- Question: What are the specific weaknesses of current LLMs in performing formal causal reasoning, and how can these be addressed?
- Basis in paper: [explicit] The error analysis in the paper reveals that LLMs struggle with the formalization and evaluation steps of causal inference, indicating a notable weakness in formal causal reasoning.
- Why unresolved: While the paper identifies these weaknesses, it does not provide concrete solutions for improving LLM performance on these specific steps.
- What evidence would resolve it: A detailed analysis of the types of errors made by LLMs in each step of the causal inference process, along with experiments testing different methods for addressing these weaknesses.

## Limitations
- The evaluation framework's reliability depends critically on the correctness of the oracle causal inference engine, which may contain implementation errors
- The verbalization process may inadvertently introduce biases that correlate with answer correctness beyond the underlying causal structure
- The study focuses primarily on GPT-4 and similar models, leaving uncertainty about how these findings generalize to other LLM architectures

## Confidence
- **High confidence**: The methodology for dataset generation using causal Bayesian networks and formal estimand derivation is sound and well-documented
- **Medium confidence**: The CausalCoT prompting strategy shows consistent improvements, but the absolute performance levels (66.6% for GPT-4) suggest fundamental limitations in LLM causal reasoning that may vary across different model families
- **Medium confidence**: The distinction between formal causal reasoning and pattern matching through anti-commonsensical stories is theoretically compelling, but the effectiveness depends on careful template design to prevent transfer learning across contexts

## Next Checks
1. Cross-validate ground truth answers using alternative causal inference engines to verify consistency and identify potential implementation bugs
2. Test zero-shot transfer from commonsensical to anti-commonsensical stories to assess whether models can generalize causal reasoning across contexts
3. Perform detailed step-by-step error analysis across multiple model families to identify universally challenging reasoning steps and architecture-specific limitations