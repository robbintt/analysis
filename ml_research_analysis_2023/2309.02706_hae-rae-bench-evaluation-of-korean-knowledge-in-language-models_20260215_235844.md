---
ver: rpa2
title: 'HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models'
arxiv_id: '2309.02706'
source_url: https://arxiv.org/abs/2309.02706
tags:
- language
- korean
- knowledge
- subset
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HAE-RAE Bench, a Korean-specific benchmark
  dataset for evaluating large language models (LLMs). The dataset includes six tasks
  across four domains: vocabulary, history, general knowledge, and reading comprehension.'
---

# HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models

## Quick Facts
- arXiv ID: 2309.02706
- Source URL: https://arxiv.org/abs/2309.02706
- Reference count: 3
- Key outcome: Korean-specific models outperform multilingual models on Korean knowledge tasks even when 13× smaller

## Executive Summary
This paper introduces HAE-RAE Bench, a Korean-specific benchmark designed to evaluate large language models' ability to recall Korean-specific knowledge and cultural contexts. The benchmark includes six tasks across four domains: vocabulary, history, general knowledge, and reading comprehension. The evaluation reveals that language-specific models like HyperClova LK-D2 and Polyglot-Ko-12.8B significantly outperform multilingual models like GPT-3.5, even when they are 13 times smaller. The study also identifies an optimal point for knowledge retention, suggesting that beyond this point, smaller models may begin to forget previously learned information.

## Method Summary
The HAE-RAE Bench benchmark evaluates language models on Korean-specific knowledge tasks using two methods: "3-shot" for models with API access (Text-Davinci-003, HyperClova LK-D2) and "log-likelihood" for other models. The 3-shot method provides three instructive examples and an incomplete example, while the log-likelihood method computes the log-likelihood of each response. The benchmark includes six tasks across four domains: vocabulary (Loan Words, Rare Words, Standard Nomenclature), history, general knowledge, and reading comprehension. Due to copyright constraints, the dataset is not publicly redistributable, but preprocessing code is available.

## Key Results
- Language-specific models (HyperClova LK-D2, Polyglot-Ko) outperform multilingual models (GPT-3.5, XGLM) on Korean knowledge tasks despite being 13× smaller
- XGLM models exhibit near-random performance across most tasks due to limited Korean token representation in pretraining
- Smaller models (12.8B) can outperform larger models (1.3B) when trained on more Korean-specific tokens, suggesting an optimal knowledge retention point

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific models outperform multilingual models on Korean knowledge tasks even when much smaller.
- Mechanism: Homogeneous training corpora aligned with target language allows more efficient knowledge encoding and retrieval.
- Core assumption: Cultural and linguistic specificity of training data directly translates to higher task performance.
- Evidence anchors:
  - [abstract]: "models approximately 13 times smaller than GPT-3.5 can exhibit similar performance levels in terms of language-specific knowledge retrieval."
  - [section]: "models roughly 13 times smaller than GPT-3.5 can achieve comparable performance in language-specific knowledge retrieval."
  - [corpus]: Weak - no direct corpus composition data; inference drawn from model size/performance gap.
- Break condition: If Korean-specific data is not well-represented in pretraining, or if evaluation tasks inadvertently overlap with English-centric benchmarks.

### Mechanism 2
- Claim: Beyond a certain model size, smaller models may forget previously learned information.
- Mechanism: Scaling laws predict a point where increasing parameters without proportional data increase leads to diminishing returns or forgetting.
- Core assumption: Model capacity and training token count must be balanced; otherwise, knowledge retention suffers.
- Evidence anchors:
  - [section]: "This observation suggests a new optimal point for knowledge memorization, beyond which smaller models may start to forget previously learned information."
  - [section]: "Polyglot-Ko-12.8B... is trained on a mere 167B tokens... even with a smaller corpus-derived knowledge base, the 12.8B model outperforms the 1.3B model."
  - [corpus]: Weak - model token counts are reported but not broken down by language.
- Break condition: If the model's architecture or regularization methods prevent forgetting despite imbalanced scaling.

### Mechanism 3
- Claim: Lack of sufficient Korean language data in multilingual pretraining leads to near-random performance.
- Mechanism: Without adequate language-specific exposure, multilingual models cannot form robust representations for culturally nuanced tasks.
- Core assumption: Pretraining data diversity and volume per language is a strong predictor of downstream performance on language-specific benchmarks.
- Evidence anchors:
  - [section]: "XGLM models exhibit near-random performance... trained on a substantial corpus of 500 billion tokens, it leverages merely 20 billion Korean tokens."
  - [abstract]: "XGLM models exhibit near-random performance across the majority of the benchmark subsets."
  - [corpus]: Weak - Korean token proportion is mentioned but not detailed training methodology.
- Break condition: If evaluation is biased toward English knowledge or if Korean data is superficially included but not deeply integrated.

## Foundational Learning

- Concept: Levenshtein distance as a proxy for lexical similarity
  - Why needed here: Used to generate plausible distractors in multiple-choice questions for vocabulary tasks.
  - Quick check question: If the correct answer is "사과", what would be an acceptable incorrect option based on Levenshtein distance ≤ 3?

- Concept: Pretraining corpus scaling laws
  - Why needed here: Determines the relationship between model size, training tokens, and optimal knowledge retention.
  - Quick check question: What happens to a model's performance if you double its parameters but only increase tokens by 10%?

- Concept: Evaluation methodology distinction (3-shot vs. log-likelihood)
  - Why needed here: Different model APIs and architectures require tailored evaluation strategies.
  - Quick check question: Why can't log-likelihood be used for models that only return API responses without raw logits?

## Architecture Onboarding

- Component map:
  Tokenizer (Korean-specific subwords) -> Transformer backbone (attention, feed-forward layers) -> Evaluation wrapper (3-shot instruction formatting vs. log-likelihood scoring) -> Dataset pipeline (multiple-choice formatting, distractor generation)

- Critical path:
  1. Load pretrained Korean model
  2. Tokenize and format input per evaluation method
  3. Run inference and extract answers
  4. Compute accuracy per task subset

- Design tradeoffs:
  - Homogeneous vs. multilingual corpora: trade-off between cultural specificity and general applicability
  - Model size vs. data volume: risk of forgetting vs. parameter efficiency
  - Evaluation method: structured output vs. probability ranking

- Failure signatures:
  - Near-random scores across all tasks → likely insufficient Korean pretraining data
  - Systematic bias toward one option → formatting or tokenizer issue
  - Memory errors during evaluation → mismatched batch/tokenization settings

- First 3 experiments:
  1. Compare a small Korean-specific model vs. a large multilingual model on a subset of the vocabulary task.
  2. Evaluate a scaled-up Korean model trained on fewer tokens vs. a smaller one trained on more tokens to probe the forgetting threshold.
  3. Run the same evaluation using both 3-shot and log-likelihood methods on a Korean-specific model to confirm method consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which smaller models begin to forget previously learned information when trained beyond a certain point?
- Basis in paper: [explicit] The paper mentions that "beyond a certain point in training, smaller models may start to lose previously learned information, even as their problem-solving and reasoning abilities continue to develop."
- Why unresolved: The paper does not provide a detailed explanation of the forgetting mechanism in smaller models when trained beyond an optimal point.
- What evidence would resolve it: Further research investigating the internal workings of smaller models during extended training periods, focusing on the retention and forgetting of information.

### Open Question 2
- Question: How does the performance of language-specific models compare to multilingual models when evaluated on tasks that require reasoning abilities, rather than knowledge recall?
- Basis in paper: [inferred] The paper focuses on the performance of language-specific models in knowledge retrieval tasks but does not explicitly compare their reasoning abilities to those of multilingual models.
- Why unresolved: The paper does not provide a direct comparison of reasoning abilities between language-specific and multilingual models.
- What evidence would resolve it: Conducting evaluations that measure the reasoning capabilities of both language-specific and multilingual models across various tasks.

### Open Question 3
- Question: What is the impact of the volume of language-specific data used during pretraining on the performance of language models, and how does it compare to the effect of model size?
- Basis in paper: [explicit] The paper states that "the volume of language-specific data utilized during the pretraining phase is equally crucial" as model parameter counts in determining performance.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the volume of language-specific data and model performance, nor does it compare this effect to that of model size.
- What evidence would resolve it: Investigating the performance of models with varying amounts of language-specific pretraining data, while controlling for model size, to determine the relative importance of each factor.

## Limitations
- Benchmark dataset is not publicly redistributable due to copyright constraints, limiting independent verification
- Exact parameter count of HyperClova LK-D2 remains undisclosed, preventing precise comparative analysis
- Optimal knowledge retention point is theoretically proposed but not empirically validated due to hardware constraints

## Confidence
- **High Confidence**: Language-specific models demonstrate superior performance on Korean knowledge tasks compared to multilingual models of similar or larger size, supported by consistent experimental results across multiple model comparisons.
- **Medium Confidence**: The existence of an optimal knowledge retention point is supported by observed performance patterns, though the exact location and generalizability of this threshold remains unverified.
- **Low Confidence**: Claims about corpus composition and Korean token proportions lack direct supporting data, relying instead on inference from performance gaps.

## Next Checks
1. Replicate the performance comparison using open Korean-specific models of varying sizes trained on controlled token distributions to verify the optimal knowledge retention hypothesis.
2. Conduct ablation studies by systematically varying Korean language representation in pretraining data while holding model architecture constant to isolate the effect of language-specific exposure.
3. Implement cross-lingual transfer experiments where knowledge from English tasks is explicitly mapped to Korean contexts to test whether performance gaps stem from cultural specificity or pure language exposure differences.