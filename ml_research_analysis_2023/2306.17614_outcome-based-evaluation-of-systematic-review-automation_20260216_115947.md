---
ver: rpa2
title: Outcome-based Evaluation of Systematic Review Automation
arxiv_id: '2306.17614'
source_url: https://arxiv.org/abs/2306.17614
tags:
- review
- outcomes
- systematic
- reviews
- publications
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an outcome-based evaluation framework for automated
  citation screening in systematic literature reviews. Instead of treating all relevant
  publications equally, the framework accounts for each publication's impact on the
  final review outcome by analyzing differences in meta-analysis results when publications
  are excluded.
---

# Outcome-based Evaluation of Systematic Review Automation

## Quick Facts
- arXiv ID: 2306.17614
- Source URL: https://arxiv.org/abs/2306.17614
- Reference count: 40
- This paper proposes an outcome-based evaluation framework for automated citation screening in systematic literature reviews

## Executive Summary
This paper introduces an outcome-based evaluation framework for automated citation screening in systematic literature reviews. The framework evaluates screening performance by measuring how much each publication affects the final review outcome, rather than treating all relevant publications equally. By analyzing differences in meta-analysis results when publications are excluded, the method accounts for each publication's true contribution to the review. When tested on 32 Cochrane reviews from CLEF TAR 2019, the approach found that removing just 5 publications (average recall 63%) changed 24% of outcomes, demonstrating that the relationship between recall and review outcomes is non-linear.

## Method Summary
The method extracts study-level effect sizes and weights from Cochrane review data, then simulates the impact of removing publications from automated screening runs. For each CLEF TAR 2019 run, the framework calculates review outcomes by applying the arbitrary ranking to included publications and computing meta-analysis results for each outcome. These calculated outcomes are compared with original review outcomes across five analysis aspects: magnitude of difference in effect sizes, distance from confidence intervals, overestimation/underestimation, sign preservation, and estimability of outcomes. The framework also employs Pareto frontier optimization to balance estimability and outcome accuracy.

## Key Results
- Removing just 5 publications (average recall 63%) changed 24% of outcomes in tested reviews
- Outcome-based evaluation produced different assessments than traditional IR metrics
- The relationship between recall and review outcomes is non-linear
- High recall doesn't necessarily translate to high outcome accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighting citations by meta-analysis effect sizes reveals true contribution differences
- Mechanism: Each study in a Cochrane review has an associated weight and effect size; removing a study changes the weights but not the effect sizes of remaining studies, so the impact on final outcomes varies by study
- Core assumption: The meta-analysis weights and effect sizes in RevMan files accurately reflect each study's contribution to the review outcome
- Evidence anchors: "extracts study-level effect sizes and weights from Cochrane review data" [abstract]; "a value of 1 represents no difference between the groups... Values higher or lower than these 'null' values may indicate either benefit or harm" [section 3.2]

### Mechanism 2
- Claim: Binary relevance evaluation masks outcome differences caused by missing publications
- Mechanism: Traditional evaluation treats all included publications equally, but our framework calculates how much each publication changes the final review conclusion when removed
- Core assumption: Changes in meta-analysis outcomes directly correspond to changes in review conclusions
- Evidence anchors: "the relationship between recall and review outcomes is non-linear" [abstract]; "failing to retrieve publications with little to no impact on the review outcome leads to the same decrease in recall as failing to retrieve crucial publications" [section 1]

### Mechanism 3
- Claim: Pareto frontier optimization can balance estimability and outcome accuracy
- Mechanism: The evaluation has two objectives - minimizing non-estimable outcomes and minimizing relative difference in estimable outcomes - creating a multi-objective optimization problem
- Core assumption: Both objectives are equally important and can be meaningfully combined in a Pareto analysis
- Evidence anchors: "evaluate both estimability and relative difference implemented, for instance, using the Pareto frontier" [section 5.3]; "On the x-axis, we show the number of non-estimable outcomes for each run. On the y-axis, there is a sum of relative difference for estimable outcomes" [section 5.3]

## Foundational Learning

- Concept: Meta-analysis statistics and forest plots
  - Why needed here: The framework relies on understanding how effect sizes and weights combine in meta-analysis to calculate outcome impacts
  - Quick check question: If a study has a risk ratio of 2.0 with weight 50% in a meta-analysis, what happens to the overall effect when this study is removed?

- Concept: Systematic review screening process
  - Why needed here: Understanding how citation screening fits into systematic reviews explains why binary relevance evaluation is insufficient
  - Quick check question: In systematic reviews, what's the difference between a "study" and a "publication"?

- Concept: Information retrieval evaluation metrics
  - Why needed here: The paper compares outcome-based evaluation with traditional IR metrics like MAP and Recall
  - Quick check question: How does Precision@K differ from Recall@K in terms of what they measure?

## Architecture Onboarding

- Component map: Data extraction -> Model evaluation -> Results analysis -> Pareto optimization
- Critical path: Extract RevMan data -> Match publications to PubMed -> Calculate outcomes for runs -> Compare with original outcomes
- Design tradeoffs: Using full-text qrels (more accurate) vs. title/abstract runs (what systems were trained on)
- Failure signatures: High proportion of non-estimable outcomes indicates data quality issues or insufficient studies per outcome
- First 3 experiments:
  1. Run simulation with random publication removal on a small review to verify relative difference calculations
  2. Test PubMed matching algorithm on 10 publications to check accuracy
  3. Calculate outcomes for one CLEF TAR run to verify integration of all components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum acceptable threshold for outcome-based evaluation metrics (like relative difference in effect sizes) that would indicate a sufficiently accurate automated systematic review screening system?
- Basis in paper: The paper shows that removing just 5 publications (average recall 63%) changed 24% of outcomes, suggesting the relationship between recall and outcomes is non-linear, but doesn't establish a specific threshold for acceptable outcome differences
- Why unresolved: The paper focuses on demonstrating the framework and comparing it to traditional metrics rather than establishing practical thresholds for implementation
- What evidence would resolve it: Empirical studies testing different threshold values across multiple review types and domains, measuring both accuracy and practical utility

### Open Question 2
- Question: How can the proposed outcome-based evaluation framework be effectively adapted for systematic reviews that don't follow the intervention study structure (e.g., diagnostic test accuracy, qualitative reviews)?
- Basis in paper: "Future work should investigate how this outcome-based evaluation framework can be extended to these other types of reviews"
- Why unresolved: The framework was only tested on systematic reviews of interventions with clear experimental/control groups
- What evidence would resolve it: Successful application and validation of the framework across different review types with varying outcome measures

### Open Question 3
- Question: What is the optimal balance between minimizing non-estimable outcomes and minimizing relative difference in outcomes when using Pareto frontier optimization for automated systematic review screening?
- Basis in paper: The paper demonstrates Pareto frontier optimization but shows no single run dominates on both dimensions, leaving the trade-off unresolved
- Why unresolved: The paper presents the optimization approach but doesn't provide guidance on how to weight these competing objectives in practice
- What evidence would resolve it: Comparative studies showing how different weighting schemes affect review quality and practical utility in real-world systematic review creation

## Limitations

- The framework relies on accurate meta-analysis data extraction from Cochrane RevMan files, which may not capture all relevant study characteristics
- Publications must be successfully matched to PubMed IDs for the framework to work, creating potential data quality issues
- The approach is currently limited to systematic reviews of interventions and requires adaptation for other review types

## Confidence

- **High Confidence**: The core finding that 5 publications (63% recall) changed 24% of outcomes demonstrates the non-linear relationship between recall and review outcomes
- **Medium Confidence**: The claim that outcome-based evaluation produces different assessments than traditional IR metrics is supported but could be strengthened with more systematic comparisons
- **Low Confidence**: The assertion that the framework can guide systematic reviewers in identifying impactful publications requires field validation beyond the simulation approach used here

## Next Checks

1. **Cross-validation of matching accuracy**: Manually verify PubMed matches for 20 randomly selected publications to establish confidence intervals for matching accuracy rates

2. **Sensitivity analysis on estimability thresholds**: Systematically vary the minimum number of studies required for estimable outcomes (1, 2, 3, 4) and measure how this affects the distribution of non-estimable outcomes across different review types

3. **Ground truth comparison study**: Conduct a small-scale validation with systematic reviewers who can identify which publications they consider most impactful, then compare this expert judgment with the framework's outcome-based rankings