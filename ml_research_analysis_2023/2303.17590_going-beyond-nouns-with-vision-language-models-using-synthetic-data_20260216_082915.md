---
ver: rpa2
title: Going Beyond Nouns With Vision & Language Models Using Synthetic Data
arxiv_id: '2303.17590'
source_url: https://arxiv.org/abs/2303.17590
tags:
- human
- data
- synthetic
- syvic
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large-scale vision-and-language models struggle to understand non-object
  words (attributes, actions, relations) and compositional reasoning. This work introduces
  Synthetic Visual Concepts (SyViC), a million-scale synthetic dataset with rich textual
  captions generated via physics-based 3D simulation.
---

# Going Beyond Nouns With Vision & Language Models Using Synthetic Data

## Quick Facts
- arXiv ID: 2303.17590
- Source URL: https://arxiv.org/abs/2303.17590
- Reference count: 40
- Primary result: Up to 9.9% gains in ARO and 4.3% in VL-Checklist using synthetic data for VL model fine-tuning

## Executive Summary
Large-scale vision-and-language models struggle with non-object words and compositional reasoning. This work introduces SyViC, a million-scale synthetic dataset generated through physics-based 3D simulation, to improve VLC understanding. The dataset features diverse objects, human avatars, randomized attributes, and detailed captions describing scenes and relationships. A fine-tuning strategy using LoRA adapters, domain stylization, and caption splitting significantly improves performance on VL-Checklist, ARO, and Winoground benchmarks with minimal zero-shot accuracy loss.

## Method Summary
The method uses Synthetic Visual Concepts (SyViC), a million-scale dataset generated through physics-based 3D simulation in ThreeDWorld. Diverse scenes are created with randomized objects, human avatars, and detailed captions generated via metadata-driven text synthesis. A fine-tuning strategy combines LoRA parameter-efficient adaptation, domain stylization through AdaIN, and caption splitting for handling long captions. The approach is evaluated on VL-Checklist, ARO, and Winoground benchmarks to measure improvements in VLC understanding and compositional reasoning.

## Key Results
- Up to 9.9% improvement on ARO benchmark for compositional reasoning
- 4.3% gain on VL-Checklist for VLC understanding
- Less than 1% drop in zero-shot accuracy on Elevator benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data enables explicit teaching of compositional reasoning by generating controlled variations in object attributes, relations, and actions.
- Mechanism: Physics-based 3D simulation generates multiple views of the same scene with consistent objects but varying attributes (e.g., color, material) and human actions, forcing the model to attend to compositional details during contrastive training.
- Core assumption: The model learns to associate text descriptions with the corresponding visual elements when the same objects appear across frames with different attribute compositions.
- Evidence anchors:
  - [abstract] "We generate synthetic videos that leverage realistic physical 3D simulation including diverse 3D environments and different 3D objects, human motions and actions assets..."
  - [section 3.1] "We randomly place 1 to 8 3D object models in the scene, randomizing their material and color attributes... We randomly place 0 to 4 human avatars in the scene, randomizing their gender and clothing."
- Break condition: If the model overfits to synthetic domain statistics and fails to generalize to real-world attribute distributions.

### Mechanism 2
- Claim: Caption splitting enables the model to process and learn from long, detailed captions that describe scene composition.
- Mechanism: Captions are split into sub-captions that fit within the model's max sequence length, then averaged to preserve full semantic information while training.
- Core assumption: Averaging text features from sub-captions maintains compositional meaning without losing context.
- Evidence anchors:
  - [abstract] "we propose and extensively ablate a combination of domain adaptation by stylization, parameter efficient fine-tuning, long captions handling, and model averaging methods..."
  - [section 3.2] "During training we handle arbitrary caption lengths by splitting a given caption into sub-captions that can each be encoded separately and averaging the text features obtained from each sub-caption."
- Break condition: If splitting breaks semantic coherence of captions, leading to loss of compositional meaning.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning with LoRA adapters preserves zero-shot capabilities while adapting to synthetic data.
- Mechanism: Low-rank adapters are added to the model layers and trained while freezing original weights, limiting forgetting of pre-trained knowledge.
- Core assumption: Small adapter matrices can capture necessary domain adaptation without overwriting base model knowledge.
- Evidence anchors:
  - [abstract] "we propose and extensively ablate a combination of domain adaptation by stylization, parameter efficient fine-tuning..."
  - [section 3.2] "We apply LoRA to adapt the encoders of a pre-trained VL model by parameterizing the adapted weights... These low-rank residual adapters can be applied efficiently during training..."
- Break condition: If rank is too low to capture synthetic domain shifts, or too high causing forgetting.

## Foundational Learning

- Concept: Compositional reasoning
  - Why needed here: The paper targets improvement in understanding non-object words (attributes, actions, relations) and the significance of word order in sentences.
  - Quick check question: Can the model correctly identify that "red car" refers to a car with the attribute "red" versus "car red" being a compositional error?

- Concept: Contrastive learning objectives
  - Why needed here: The paper relies on contrastive loss between image-text pairs to teach the model to align visual and textual features.
  - Quick check question: Does the model learn to pull together an image of a "blue ball" with its caption while pushing away a "red ball" image-caption pair?

- Concept: Domain adaptation
  - Why needed here: Synthetic data may differ from real data in visual style, requiring adaptation to maintain real-world generalization.
  - Quick check question: After stylization, can the model still recognize objects in real images that it only saw in stylized synthetic form?

## Architecture Onboarding

- Component map:
  3D simulation engine (TDW/Unity) -> Image generation pipeline
  Caption generation grammar -> Text description pipeline
  CLIP/CyCLIP backbone -> Pre-trained VL model
  LoRA adapters -> Fine-tuning modules
  AdaIN stylization -> Domain adaptation
  Caption splitting -> Text preprocessing
  Evaluation benchmarks (VL-Checklist, ARO, Winoground) -> Performance validation

- Critical path: 3D simulation -> Caption generation -> Synthetic dataset -> LoRA fine-tuning -> Evaluation

- Design tradeoffs:
  - Caption length vs. model capacity: Long captions improve detail but may exceed sequence limits.
  - LoRA rank vs. forgetting: Higher rank improves adaptation but risks forgetting base knowledge.
  - Stylization strength vs. realism: Too strong stylization may distort object appearance.

- Failure signatures:
  - Zero-shot performance drops >1%: Indicates forgetting of base model knowledge.
  - VL-Checklist performance flat: Suggests synthetic data not teaching compositional concepts.
  - Winoground performance drops: Indicates loss of compositional reasoning.

- First 3 experiments:
  1. Generate 1000 synthetic image-text pairs with varied object attributes and evaluate on VL-Checklist to check compositional learning.
  2. Fine-tune CLIP on synthetic data with LoRA rank 16 and compare zero-shot retention vs. full fine-tuning.
  3. Apply AdaIN stylization to synthetic images and measure domain gap reduction on downstream tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SyViC improve with increasing the number of synthetic samples and 3D object models used for synthesis?
- Basis in paper: [explicit] The paper includes an ablation study (Section H) that explores the effect of the number of synthetic samples and the number of 3D object models used for SyViC generation.
- Why unresolved: The paper does not provide a quantitative analysis of how the performance scales with the number of samples and models.
- What evidence would resolve it: A plot showing the performance of SyViC on a benchmark (e.g., VL-Checklist) as a function of the number of synthetic samples and 3D object models used for synthesis.

### Open Question 2
- Question: How does the performance of SyViC compare to text augmentation methods for improving VLC understanding?
- Basis in paper: [explicit] The paper mentions that text augmentation methods (e.g., [12, 66]) are an orthogonal approach to improving VLC understanding, and the authors conduct an experiment to compare the performance of their approach to text augmentation.
- Why unresolved: The paper does not provide a comprehensive comparison of SyViC to text augmentation methods on a variety of benchmarks and tasks.
- What evidence would resolve it: A detailed comparison of SyViC and text augmentation methods on multiple benchmarks (e.g., VL-Checklist, ARO, Winoground) and tasks, including both VLC understanding and compositional reasoning.

### Open Question 3
- Question: How does the performance of SyViC improve with more powerful language models for caption generation?
- Basis in paper: [explicit] The paper mentions that they experiment with using language models for caption generation, but they do not observe significant performance gains with the openly available language models they tried.
- Why unresolved: The paper does not explore the use of more powerful language models or the combination of visual grounding with caption generation.
- What evidence would resolve it: An experiment comparing the performance of SyViC using different language models for caption generation, including more powerful models and models that incorporate visual grounding.

## Limitations

- Synthetic data may not capture full complexity of real-world compositional reasoning scenarios
- Caption splitting mechanism lacks quantitative validation of specific impact on compositional understanding
- Fine-tuning strategy effectiveness primarily demonstrated on limited evaluation benchmarks

## Confidence

**High Confidence Claims:**
- SyViC dataset successfully generates diverse synthetic image-text pairs with controlled attribute variations
- LoRA-based fine-tuning effectively improves VLC understanding on benchmark tasks
- Proposed fine-tuning strategy achieves measurable performance gains on VL-Checklist, ARO, and Winoground

**Medium Confidence Claims:**
- Caption splitting specifically improves compositional reasoning understanding
- Synthetic data approach generalizes well to real-world scenarios
- Domain stylization method effectively reduces synthetic-to-real domain gap

**Low Confidence Claims:**
- Exact contribution of each component in fine-tuning strategy
- Scalability to more complex compositional reasoning tasks
- Robustness to out-of-distribution compositional scenarios

## Next Checks

1. Evaluate the fine-tuned model on additional real-world compositional reasoning benchmarks not used in training to assess true generalization capability beyond the synthetic domain.

2. Conduct a systematic ablation study isolating the contribution of each component (LoRA, stylization, caption splitting) to determine their individual impact on performance improvements.

3. Assess model performance over extended training periods and after deployment to identify potential forgetting of base capabilities or degradation in compositional reasoning abilities.