---
ver: rpa2
title: Boosting Norwegian Automatic Speech Recognition
arxiv_id: '2307.01672'
source_url: https://arxiv.org/abs/2307.01672
tags:
- speech
- language
- norwegian
- npsc
- nst-npsc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates and compares several automatic speech recognition\
  \ (ASR) models for the two official written languages in Norway: Bokm\xE5l and Nynorsk.\
  \ The authors fine-tune wav2vec 2.0 models of varying sizes (300M and 1B parameters)\
  \ on Norwegian speech datasets, including the Norwegian Parliamentary Speech Corpus\
  \ (NPSC) and the NST dataset."
---

# Boosting Norwegian Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2307.01672
- Source URL: https://arxiv.org/abs/2307.01672
- Reference count: 28
- Best WER: 5.81% (Bokmål) and 11.54% (Nynorsk) on NPSC

## Executive Summary
This paper evaluates and compares automatic speech recognition models for Norwegian, focusing on the two official written languages: Bokmål and Nynorsk. The authors fine-tune wav2vec 2.0 models of varying sizes on Norwegian speech datasets, including the Norwegian Parliamentary Speech Corpus (NPSC) and the NST dataset. They also experiment with combining these datasets and adding language models. The best performing models achieve a word error rate (WER) of 5.81% for Bokmål and 11.54% for Nynorsk on NPSC, improving upon the previous state of the art. The models also generalize well to out-of-domain data.

## Method Summary
The authors fine-tune pre-trained wav2vec 2.0 models (300M and 1B parameters) on Norwegian speech datasets, including the Norwegian Parliamentary Speech Corpus (NPSC) and the NST dataset. They experiment with combining these datasets and adding a 5-gram Kneser-Ney language model for rescoring. The models are evaluated on NPSC, NST, and an out-of-domain Norwegian subset of the FLEURS dataset. The fine-tuning process involves training the models on the combined NPSC and NST datasets, followed by language model integration and hyperparameter tuning.

## Key Results
- Best WER of 5.81% for Bokmål and 11.54% for Nynorsk on NPSC
- Out-of-domain FLEURS performance significantly improved by adding NST data
- Language model integration yields 1-5 point WER improvements across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning wav2vec 2.0 on both planned (NST) and semi-improvised (NPSC) Norwegian speech datasets improves generalization more than using either dataset alone.
- Mechanism: The combination provides complementary acoustic patterns—NST offers clean, articulated speech, while NPSC contains natural prosody and hesitations. This multi-domain exposure helps the model learn robust representations that generalize across varied real-world conditions.
- Core assumption: The acoustic characteristics of planned speech and parliamentary-style spontaneous speech are sufficiently distinct yet complementary for acoustic modeling.
- Evidence anchors:
  - [abstract] "We compare the performance of models of varying sizes and pre-training approaches on multiple Norwegian speech datasets."
  - [section 4] "Adding over 400 hours of extra planned speech to the semi-improvised speech part of NPSC, performance does not plummet, but actually increases."
  - [corpus] "corpus signals found 25 related papers, average neighbor FMR=0.419" (weak signal)
- Break condition: If the planned speech data does not provide enough acoustic variation (e.g., overly scripted or similar to NPSC), the gain from combination would be minimal.

### Mechanism 2
- Claim: Adding a 5-gram Kneser-Ney language model to the CTC output of wav2vec 2.0 significantly improves WER.
- Mechanism: The acoustic model predicts phoneme sequences, which are mapped to words via CTC. A language model rescoring step applies syntactic and lexical priors to correct acoustically plausible but contextually unlikely word sequences, especially for homophones and morphological variants in Norwegian.
- Core assumption: The confusion patterns in the CTC output are amenable to reranking via n-gram statistics without introducing significant latency.
- Evidence anchors:
  - [section 4] "Adding a 5-gram language model yields significant improvements across the board, ranging from a 5 points increase on the worst performing pairs of model and dataset, to a 1 point increase for the best performing pairs."
  - [section 5] "we established α = 0.5 and β = 0.001" (hyperparameters tuned for LM integration)
  - [corpus] "average citations=0.0" (no supporting citations for this claim)
- Break condition: If the language model is trained on mismatched domain data or if beam search exploration is too narrow, improvements may not materialize.

### Mechanism 3
- Claim: Larger wav2vec 2.0 models (1B vs 300M parameters) improve WER, especially on out-of-domain FLEURS data.
- Mechanism: Larger models have higher representational capacity, allowing them to capture finer-grained acoustic distinctions and long-range dependencies, which helps in handling the stylistic differences between in-domain (NPSC/NST) and out-of-domain (FLEURS) speech.
- Core assumption: The training data and compute budget are sufficient to realize the representational benefits of the larger model without overfitting.
- Evidence anchors:
  - [section 5] "For Nynorsk, as shown in Table 4, our NST-NPSC 300M model with a Nynorsk 5-gram language model attached did not beat the existing NPSC-Nynorsk 300M model. However, our newer NPSC-Nynorsk 1B model outperforms the NPSC-Nynorsk 300M model by 1.14 points."
  - [section 5] "the out of domain performance of the models is also greatly improved by adding the planned speech in NST to NPSC."
  - [corpus] "Found 25 related papers" (weak contextual support)
- Break condition: If the 1B model overfits to the training domain, the expected generalization gains on FLEURS may not appear.

## Foundational Learning

- Concept: Self-supervised representation learning via masked prediction in raw audio (wav2vec 2.0 architecture).
  - Why needed here: wav2vec 2.0 enables training without full transcriptions by predicting masked latent speech representations, crucial for leveraging large-scale unlabeled speech.
  - Quick check question: What is the purpose of the contrastive loss in wav2vec 2.0 pretraining?

- Concept: Connectionist Temporal Classification (CTC) loss for sequence-to-sequence alignment without explicit frame-to-symbol alignment.
  - Why needed here: CTC allows the fine-tuned wav2vec 2.0 to output character or phoneme sequences that can be decoded into words, bypassing the need for frame-level alignment.
  - Quick check question: How does CTC handle multiple alignments for the same output sequence?

- Concept: Language model rescoring via weighted combination of CTC and n-gram scores (shallow fusion).
  - Why needed here: Rescoring with a 5-gram Kneser-Ney model corrects acoustically plausible but contextually unlikely hypotheses, especially important for Norwegian's morphological richness.
  - Quick check question: What role do the α and β hyperparameters play in shallow fusion?

## Architecture Onboarding

- Component map: wav2vec 2.0 encoder -> CTC output layer -> KenLM 5-gram Kneser-Ney LM -> Decoder (beam search with shallow fusion)
- Critical path:
  1. Load pretrained wav2vec 2.0 weights
  2. Fine-tune on NPSC/NST with CTC loss
  3. Train LM on transcriptions from NPSC + external corpus
  4. Tune α, β on validation set
  5. Evaluate on test sets
- Design tradeoffs:
  - Larger models: better accuracy but higher memory/compute
  - LM integration: better fluency but adds inference latency
  - Data combination: broader coverage but risk of domain shift
- Failure signatures:
  - Overfitting: WER improves on training domain but worsens on FLEURS
  - LM mismatch: LM over-penalizes rare words, causing under-generation
  - Beam search too narrow: LM cannot correct top hypotheses
- First 3 experiments:
  1. Fine-tune 300M model on NPSC only, evaluate WER.
  2. Add NST data, fine-tune same model, compare WER.
  3. Attach 5-gram LM, sweep α, β, measure impact on WER.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would training wav2vec 2.0 models directly on non-normalized text impact their performance compared to models trained on normalized text?
- Basis in paper: [explicit] The paper mentions "the prospect of training wav2vec 2.0 directly on non-normalized text is an interesting avenue for research, as it would make the models directly usable without having to transform the output of the models to make them more readable."
- Why unresolved: The paper does not explore this approach and only discusses it as a potential future direction.
- What evidence would resolve it: Experiments comparing the performance of wav2vec 2.0 models trained on normalized vs. non-normalized text, using the same architecture and datasets.

### Open Question 2
- Question: How would using other more data-hungry architectures, such as Whisper, impact the performance of Norwegian ASR models compared to the wav2vec 2.0 models presented in this paper?
- Basis in paper: [explicit] The paper states "With enough transcribed speech, even other more data-hungry architectures could be tested, such as Whisper."
- Why unresolved: The paper does not explore other architectures and only mentions Whisper as a potential future direction.
- What evidence would resolve it: Experiments comparing the performance of Norwegian ASR models using different architectures (e.g., wav2vec 2.0, Whisper) on the same datasets and with similar amounts of training data.

### Open Question 3
- Question: How would incorporating more diverse and dialect-rich datasets impact the performance of Norwegian ASR models, particularly in terms of handling the complex phonetics and morphology of different dialects?
- Basis in paper: [inferred] The paper discusses the challenges of Norwegian ASR models in handling the complex phonetics and morphology of different dialects, and mentions the limited availability of high-quality datasets for Norwegian speech.
- Why unresolved: The paper does not explore the impact of using more diverse and dialect-rich datasets on model performance.
- What evidence would resolve it: Experiments comparing the performance of Norwegian ASR models trained on different datasets with varying levels of dialectal diversity, using the same architecture and evaluation metrics.

## Limitations

- Dataset completeness: The exact preprocessing steps for handling hesitations in NPSC are not fully specified, which could impact reproducibility.
- Language model generalization: The 5-gram Kneser-Ney LM improvements are not evaluated on other Norwegian dialects or domains beyond the tested FLEURS subset.
- Model size scaling: The superiority of the 1B parameter model over the 300M model is demonstrated, but the computational cost-benefit analysis is absent.

## Confidence

- **High confidence**: The baseline wav2vec 2.0 fine-tuning results on NPSC are reproducible given the model sizes and datasets are clearly specified.
- **Medium confidence**: The language model integration mechanism is well-understood, but hyperparameter sensitivity and generalization are not fully characterized.
- **Low confidence**: The "greatly improved" out-of-domain performance claim lacks comparative baselines and addresses only one dataset.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Re-run the language model rescoring experiments with a grid search around α∈[0.3,0.7] and β∈[0.0005,0.002] to determine if the reported improvements are robust to hyperparameter choice.

2. **Cross-dialect generalization**: Evaluate the best 1B model on a held-out Norwegian dialect dataset (e.g., Northern Norwegian speech) to verify that the claimed generalization extends beyond the tested domains.

3. **Data ablation study**: Train models on NST-only and NPSC-only subsets with matched hours to quantify the exact contribution of each dataset type, testing whether the combination effect is additive or synergistic.