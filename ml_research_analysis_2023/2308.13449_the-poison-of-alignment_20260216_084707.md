---
ver: rpa2
title: The Poison of Alignment
arxiv_id: '2308.13449'
source_url: https://arxiv.org/abs/2308.13449
tags:
- dataset
- arxiv
- data
- alignment
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the effect of alignment on the performance
  of large language models during instruction tuning. The authors argue that alignment
  in fine-tuning datasets can act as a form of data poisoning, harming model performance.
---

# The Poison of Alignment

## Quick Facts
- arXiv ID: 2308.13449
- Source URL: https://arxiv.org/abs/2308.13449
- Reference count: 29
- Primary result: Alignment in fine-tuning datasets degrades model reasoning performance by 4-33% on standard benchmarks

## Executive Summary
This study challenges the assumption that alignment is always beneficial for instruction-tuned models. The authors collect a high-quality dataset from their GoatChat app, perform extensive cleaning, and specifically remove aligned responses. They demonstrate that alignment acts as a form of data poisoning, significantly degrading performance across multiple reasoning benchmarks. The model trained without alignment outperforms the aligned model by 4-33% on MMLU, BBH, HumanEval, and DROP, highlighting the importance of careful dataset curation.

## Method Summary
The authors collected user-bot interactions from the GoatChat app and performed extensive cleaning, removing API failures, short messages, repeated queries, and aligned responses. They merged this cleaned dataset with Guanaco and fine-tuned Llama 2 7B using supervised fine-tuning for 1 epoch with batch size 512, bfloat16, DeepSpeed ZeRO-3, and learning rate 1e-4. An ablation study compared models trained with and without alignment to measure the impact on reasoning performance.

## Key Results
- Alignment removal improves model performance by 4-33% on MMLU, BBH, HumanEval, and DROP
- The cleaned dataset outperforms the base model on MMLU and BBH
- Models trained on aligned datasets show no improvement over base models on reasoning benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Alignment acts as dataset poisoning by introducing non-informative, passive responses that degrade model reasoning performance. Aligned responses often deflect or refuse rather than answer queries, and when included in supervised fine-tuning, the model learns to suppress informative responses.

### Mechanism 2
Dataset cleaning restores reasoning performance by eliminating misleading training signals. By removing aligned responses and other low-quality data, the dataset becomes more focused on informative interactions, leading to better model performance on reasoning benchmarks.

### Mechanism 3
Alignment introduces a distributional shift that biases the model toward non-informative outputs. Aligned responses change the training data distribution toward refusals and deflections, causing the model to adapt to this distribution and reduce its tendency to generate informative answers.

## Foundational Learning

- **Supervised fine-tuning (SFT) and model behavior**: Understanding how SFT modifies model behavior is key to interpreting alignment effects. Quick check: What is the difference between SFT and unsupervised pretraining in terms of data requirements and model adaptation?

- **Data poisoning and model performance**: The paper frames alignment as data poisoning; understanding poisoning mechanisms helps explain results. Quick check: How does data poisoning differ from benign dataset contamination in terms of model behavior changes?

- **Dataset cleaning techniques**: The paper's core contribution relies on dataset cleaning to remove alignment. Quick check: What are common methods for dataset cleaning, and how do they affect model generalization?

## Architecture Onboarding

- **Component map**: Dataset collection -> Cleaning (remove failures, low-quality, aligned) -> Merge with Guanaco -> Deduplication -> SFT training -> Evaluation on MMLU, BBH, HumanEval, DROP

- **Critical path**: The sequence from dataset collection through cleaning and fine-tuning to evaluation on reasoning benchmarks represents the critical path for demonstrating alignment's negative effects.

- **Design tradeoffs**: Removing aligned responses improves reasoning but may reduce safety; keeping them improves safety but harms reasoning. The choice depends on the intended use case.

- **Failure signatures**: No improvement over base model on reasoning benchmarks, high variance in results, or model collapse during training indicate potential issues.

- **First 3 experiments**:
  1. Train with full GoatAssistant + Guanaco dataset (with alignment) and evaluate on reasoning benchmarks
  2. Train with cleaned dataset (without alignment) and compare results to experiment 1
  3. Ablation study: Train with only Guanaco dataset to isolate the effect of GoatAssistant data

## Open Questions the Paper Calls Out

### Open Question 1
How does the degree of alignment in fine-tuning datasets quantitatively correlate with specific types of performance degradation across different reasoning benchmarks?

### Open Question 2
What specific characteristics of aligned responses make them particularly harmful for model reasoning capabilities compared to other types of low-quality data?

### Open Question 3
Are certain types of alignment (e.g., safety-based vs. format-based) more harmful than others, and how does this vary by model size or task type?

### Open Question 4
How does the timing and method of alignment removal during data processing affect the final model performance?

### Open Question 5
What is the long-term stability of alignment removal benefits, and do models exhibit any form of "alignment drift" over time or with continued fine-tuning?

## Limitations

- The study relies on a private, proprietary dataset that is not publicly available, making independent verification challenging
- The definition and criteria for identifying and removing "aligned responses" are not fully specified
- The study focuses on a single model architecture (Llama 2 7B) and dataset, limiting generalizability

## Confidence

- **High confidence**: The empirical observation that dataset cleaning improves reasoning performance is well-supported and aligns with general principles of supervised fine-tuning
- **Medium confidence**: The claim that alignment specifically acts as data poisoning is plausible but not definitively proven, as the study doesn't rigorously ablate different types of aligned responses
- **Low confidence**: The broader claim that alignment is universally harmful for instruction-tuned models is overstated, as the study only examines one model and limited benchmarks

## Next Checks

1. **Independent replication**: Replicate the study using a publicly available instruction-tuning dataset (e.g., Alpaca, ShareGPT) and a standard model (e.g., Llama 2 7B). Apply the same alignment removal process and evaluate on the same benchmarks.

2. **Safety evaluation**: Fine-tune two models—one with and one without alignment removal—and conduct a comprehensive safety evaluation using standard benchmarks (e.g., AdvPrompt, ToxiGen) to quantify the trade-off between reasoning performance and safety.

3. **Ablation study on alignment types**: Systematically vary the criteria for alignment removal (e.g., remove only refusals, remove all aligned responses, remove only certain categories) and measure the impact on both reasoning and safety benchmarks.