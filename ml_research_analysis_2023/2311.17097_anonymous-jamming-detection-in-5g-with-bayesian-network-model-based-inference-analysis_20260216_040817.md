---
ver: rpa2
title: Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference
  Analysis
arxiv_id: '2311.17097'
source_url: https://arxiv.org/abs/2311.17097
tags:
- jamming
- detection
- data
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of jamming detection in 5G networks,
  focusing on maintaining reliability, preventing user experience degradation, and
  avoiding infrastructure failure. The proposed approach uses supervised and unsupervised
  learning techniques to detect jamming in real-time with high accuracy, including
  unknown types.
---

# Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference Analysis

## Quick Facts
- arXiv ID: 2311.17097
- Source URL: https://arxiv.org/abs/2311.17097
- Reference count: 14
- Primary result: Unsupervised auto-encoder-based anomaly detection achieves AUC of 0.987 for detecting unknown jamming types in 5G networks

## Executive Summary
This paper addresses the challenge of jamming detection in 5G networks by proposing a comprehensive machine learning approach that combines supervised and unsupervised techniques. The method achieves high accuracy in detecting both known and unknown jamming types through cross-layer feature analysis, with supervised models reaching AUC values of 0.964-1.0 and unsupervised auto-encoders achieving 0.987 AUC. The approach is further enhanced with a Bayesian Network Model that provides causation analysis and root cause identification, offering transparency to the machine learning models. The system is validated on a 5G testbed and demonstrates resistance to adversarial training samples while maintaining interpretability.

## Method Summary
The method employs a hybrid approach combining supervised learning (decision trees, random forests, and LSTM models) for known jamming detection with unsupervised auto-encoder-based anomaly detection for unknown jamming types. Cross-layer features from 5G protocol stacks (PHY, MAC, RLC, PDCP) are extracted as input, including parameters like SNR, CQI, MCS, bitrate, and retransmission rates. A Bayesian Network Model is constructed to provide causation analysis by mapping jamming effects to root causes through a directed acyclic graph structure. The system is evaluated on a 5G NSA testbed with controlled jamming scenarios generated by specialized equipment.

## Key Results
- Supervised models achieve AUC of 0.964 to 1.0 for known jamming types, with random forest achieving 100% accuracy
- Unsupervised auto-encoder-based anomaly detection reaches AUC of 0.987 for detecting unknown jamming types
- Bayesian Network Model provides transparent causation analysis linking jamming effects to protocol layer degradation
- System demonstrates resistance to adversarial training samples while maintaining high detection accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised discriminative models achieve high AUC (0.964-1.0) for known jamming types by leveraging cross-layer protocol features.
- Mechanism: The model uses labeled training data from the 5G testbed to learn the statistical patterns of known jamming signals. Decision trees and random forests partition the high-dimensional feature space (e.g., bitrate, SNR, MCS variance) into regions that distinguish jammed vs. non-jammed states.
- Core assumption: The statistical signatures of known jamming types are distinct enough in the feature space to be separated by supervised learning.
- Evidence anchors:
  - [abstract]: "Supervised models reach an AUC of 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1."
  - [section]: "When detecting the listed jamming types from the no jamming scenarios, the random forest algorithm gives 100% accuracy for all jamming types."
  - [corpus]: Weak; corpus neighbors focus on intrusion detection but not specifically 5G jamming with cross-layer features.

### Mechanism 2
- Claim: Unsupervised anomaly detection using ensemble autoencoders detects unknown jamming types with AUC 0.987 by learning normal traffic patterns.
- Mechanism: Autoencoders are trained only on non-jamming data to reconstruct expected behavior. Unknown jamming introduces reconstruction errors that exceed a learned threshold, triggering detection.
- Core assumption: Jamming introduces statistically anomalous patterns not seen in normal operation, which the autoencoder cannot reconstruct well.
- Evidence anchors:
  - [abstract]: "unsupervised auto-encoder-based anomaly detection is presented with an AUC of 0.987."
  - [section]: "The introduced unsupervised approach has an AUC of 0.987 with training samples collected without any jamming or interference."
  - [corpus]: Weak; corpus neighbors discuss anomaly detection but not autoencoder-based methods for RF jamming.

### Mechanism 3
- Claim: Bayesian Network Model (BNM) enhances detection accuracy and provides causation analysis by integrating domain knowledge of 5G protocol behavior.
- Mechanism: The DAG structure encodes causal relationships (e.g., CCH jamming → incorrect CQI → MCS variance → throughput degradation). Empirical data populates conditional probability tables, allowing inference of root causes.
- Core assumption: Domain knowledge of protocol-layer interactions accurately reflects real-world jamming effects.
- Evidence anchors:
  - [abstract]: "a Bayesian network model based causation analysis is further introduced."
  - [section]: "A Bayesian Network Model (BNM) is constructed for revealing causation and root, direct, and indirect causing of the jamming effects on the performance, thereby providing transparency to machine learning models."
  - [corpus]: Weak; corpus neighbors mention Bayesian methods but not for causal analysis in RF jamming contexts.

## Foundational Learning

- Concept: Cross-layer feature extraction from 5G protocol stacks (PHY, MAC, RLC, PDCP)
  - Why needed here: Detection relies on statistical parameters from multiple protocol layers to capture the full impact of jamming on communication quality.
  - Quick check question: Which layer-level parameters (e.g., CQI, MCS, SNR) are most sensitive to jamming in your testbed?

- Concept: Supervised vs. unsupervised learning trade-offs
  - Why needed here: Supervised models need labeled data for known jamming types (high accuracy), while unsupervised models handle unknown types but may have lower specificity.
  - Quick check question: How does the availability of labeled jamming data influence your choice of model for a new jamming scenario?

- Concept: Bayesian Network inference and causality
  - Why needed here: BNM provides interpretable detection by mapping jamming effects to root causes, aiding in response and mitigation.
  - Quick check question: Can you trace a throughput degradation back to its most likely root cause using the DAG structure?

## Architecture Onboarding

- Component map:
  UE -> BS -> CN -> CAC -> JARG -> JDCIM -> Cross-layer feature extraction -> Supervised models (RF, LSTM) -> Unsupervised AE ensemble -> BNM inference engine

- Critical path:
  1. Jamming signal generation (CAC → JARG)
  2. Feature collection from protocol stacks
  3. Real-time inference (supervised/unsupervised models)
  4. Causation analysis (BNM)
  5. Alert/adaptation decision

- Design tradeoffs:
  - Supervised: High accuracy for known types, but requires labeled data and cannot detect unknowns.
  - Unsupervised: Detects unknowns but may have higher false positives; sensitive to training data distribution.
  - BNM: Adds interpretability but requires accurate DAG and data-driven CPTs; increases complexity.

- Failure signatures:
  - Supervised model: High false negatives for unknown jamming; accuracy drops if feature distributions shift.
  - Unsupervised model: High false positives if normal traffic exhibits anomalous patterns; misses if jamming mimics normal.
  - BNM: Inaccurate root cause if DAG is incomplete or CPTs are poorly estimated.

- First 3 experiments:
  1. Baseline: Run supervised model on labeled jamming data; measure AUC per jamming type.
  2. Unknown test: Generate new jamming types not in training; evaluate unsupervised model's detection rate and false positives.
  3. BNM validation: Inject known jamming; verify that BNM correctly identifies root cause and maps to observed KPI degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed unsupervised auto-encoder-based anomaly detection perform under varying network conditions (e.g., different traffic loads, multiple simultaneous users) in 5G environments?
- Basis in paper: [inferred] The paper focuses on the effectiveness of the model under specific conditions but does not extensively evaluate its performance under diverse network scenarios.
- Why unresolved: The study primarily tests the model under controlled jamming scenarios, without addressing the complexity of real-world network conditions.
- What evidence would resolve it: Empirical data showing the model's accuracy and robustness under different network loads and user scenarios would provide insights into its real-world applicability.

### Open Question 2
- Question: What are the limitations of the Bayesian Network Model (BNM) in capturing the full complexity of cross-layer interactions in 5G networks?
- Basis in paper: [explicit] The paper acknowledges that methods of probabilistic inference may have shortcomings such as high computational complexity and cumulative error as the number of nodes increases.
- Why unresolved: The study simplifies the BNM by focusing on specific factors from the physical layer, which may not fully represent the intricate interactions across all layers in 5G networks.
- What evidence would resolve it: A detailed analysis comparing the BNM's predictions with actual cross-layer interactions under various network conditions would highlight its limitations and areas for improvement.

### Open Question 3
- Question: How does the system handle false positives in jamming detection, and what mechanisms are in place to minimize their impact on network performance?
- Basis in paper: [inferred] The paper does not explicitly discuss strategies for handling false positives, which are critical for maintaining network reliability.
- Why unresolved: The focus is on detection accuracy, but the implications of false positives on network performance are not addressed.
- What evidence would resolve it: Data on the rate of false positives and their impact on network performance, along with strategies to mitigate them, would provide a clearer picture of the system's reliability.

### Open Question 4
- Question: How scalable is the proposed system for deployment in large-scale 5G networks with diverse user equipment and base stations?
- Basis in paper: [inferred] The paper describes the system's implementation on a testbed but does not discuss its scalability to larger, more complex networks.
- Why unresolved: The study is limited to a controlled environment, and scalability to real-world networks with varying configurations is not explored.
- What evidence would resolve it: Performance metrics from large-scale network deployments would demonstrate the system's scalability and adaptability to different network sizes and configurations.

## Limitations
- Requires detailed cross-layer protocol data which may not be available in all operational 5G deployments
- Supervised models are vulnerable to novel jamming techniques not present in training data
- Bayesian Network Model accuracy depends on correct DAG structure and sufficient empirical data for CPTs

## Confidence
- High confidence in supervised model performance claims (AUC 0.964-1.0) due to empirical validation on controlled testbed experiments
- Medium confidence in unsupervised model claims (AUC 0.987) as performance depends critically on training data representativeness
- Medium confidence in BNM causation analysis as it requires validation of real-world causal relationship capture

## Next Checks
1. Test supervised model performance on a new, previously unseen jamming type not in the training set to measure degradation in detection accuracy
2. Perform sensitivity analysis on the BNM by systematically removing edges from the DAG to quantify impact on root cause identification accuracy
3. Evaluate model robustness by introducing synthetic adversarial training samples at different stages of the training process and measuring AUC degradation