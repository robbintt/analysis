---
ver: rpa2
title: 'Large Language Models on the Chessboard: A Study on ChatGPT''s Formal Language
  Comprehension and Complex Reasoning Skills'
arxiv_id: '2308.15118'
source_url: https://arxiv.org/abs/2308.15118
tags:
- chatgpt
- move
- moves
- language
- chess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates ChatGPT's ability to play chess and its
  limitations in complex reasoning tasks involving formal language. Using chess as
  a case study, the authors evaluate ChatGPT's performance through various metrics
  assessing move legality, quality, and strategic behavior.
---

# Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills

## Quick Facts
- arXiv ID: 2308.15118
- Source URL: https://arxiv.org/abs/2308.15118
- Authors: 
- Reference count: 8
- ChatGPT struggles with chess rules and move quality despite strong natural language processing

## Executive Summary
This study investigates ChatGPT's ability to play chess and understand formal language by using chess as a case study for evaluating move legality, quality, and strategic behavior. The research reveals that despite ChatGPT's proficiency in natural language processing, it struggles significantly with adhering to chess rules and generating high-quality moves. The authors identify limitations in ChatGPT's attention mechanism and self-regulation abilities, which hinder its performance in formal language tasks. The study demonstrates that consistent repetition of relevant information can partially alleviate these limitations, highlighting the need for further refinement of language models before they can be considered reliable tools for complex reasoning tasks beyond natural language processing.

## Method Summary
The research uses ChatGPT (gpt-3.5-turbo-0301) without fine-tuning, instead employing various prompting strategies to evaluate chess performance. The methodology involves 1000 baseline games where ChatGPT plays as black against predetermined white moves (e4, d4, Nf3, or e3). Stockfish 15.1 serves as the chess engine for evaluation. Different experimental variations include providing additional information (Int-Illegal, Int-Rules), allowing natural language reasoning (Rsn-Simple, Rsn-CoT, Rsn-DropCoT), and implementing token repetition strategies (Move-Repeat, Move-IlgRem). Performance is measured across five metrics: Illegal Move Ratio (IMR), Retries Before Legal Move (RBLM), Game Length (GL), Board Evaluation (BE), and Move Repetition Score (MRS).

## Key Results
- ChatGPT demonstrates high Illegal Move Ratio (IMR) averaging 1.68 across all variations, indicating frequent rule violations
- Natural language reasoning improves decision-making "intent" but introduces erroneous information that reduces move quality
- The Move-Repeat variation, which consistently repeats board state information, significantly reduces IMR and RBLM, suggesting token repetition mitigates attention decay
- ChatGPT shows strategic consistency in move selection but struggles with fundamental chess rules, particularly in longer games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's self-attention mechanism allocates less attention to SAN tokens that are farther from the latest input, leading to decreased board state comprehension in longer games.
- Mechanism: As game length increases, tokens representing earlier moves receive progressively less attention weight, causing the model to lose track of board state information necessary for valid move generation.
- Core assumption: The self-attention mechanism's attention distribution follows a distance-based decay pattern where tokens further from the current position receive proportionally less attention.
- Evidence anchors:
  - [section]: "As highlighted by [St¨ockl, 2021], GPT-2 models are found to devote less attention to SAN notation tokens that are farther away from the latest input."
  - [section]: "Since a complete game memory is paramount for models to accurately track the board state [Toshniwalet al., 2022], we postulate that ChatGPT's disproportionate attention allocation might be the cause of a significant portion of its mistakes."
- Break condition: If the model employs positional encoding schemes that explicitly counter distance-based decay, or if the model uses attention mechanisms with fixed attention distribution regardless of token position.

### Mechanism 2
- Claim: LLMs trained on natural language data show reduced performance on formal language tasks due to their tendency to neglect tokens used in unconventional ways.
- Mechanism: When encountering formal language like chess notation, the model treats these tokens as noise or artifacts, diverting attention away from them toward more familiar natural language patterns, leading to rule violations and poor move quality.
- Core assumption: The model's training data distribution heavily biases it toward natural language patterns, making it treat formal language tokens as less relevant or as noise.
- Evidence anchors:
  - [section]: "Maynez et al. [2020] noted that LMs typically remain indifferent to noises or artifacts in training data, which we argue may also apply to formal languages like chess notations."
  - [section]: "This issue is particularly evident in the Int-Rules variation, where despite the introduction of helpful data, ChatGPT's performance dropped substantially."
- Break condition: If the model demonstrates equivalent performance on formal language tasks after fine-tuning on formal language data, or if the model can explicitly recognize and prioritize formal language tokens.

### Mechanism 3
- Claim: Providing natural language reasoning improves the model's decision-making "intent" but not necessarily move quality due to the introduction of erroneous information.
- Mechanism: When allowed to reason in natural language, the model generates more focused move selections (lower RBLM) by reducing the range of considered moves, but the reasoning process introduces hallucinations that mislead the model into making suboptimal moves.
- Core assumption: The model's reasoning process creates a feedback loop where the generated explanations influence subsequent move selection, even when the explanations contain errors.
- Evidence anchors:
  - [section]: "In all variations involving natural language reasoning (NL reasoning), we observed significant decrements in RBLM, indicating that NL reasoning also helps LLMs generate 'intent.'"
  - [section]: "However, the presence of intent does not necessarily correlate with better game performance. Allowing NL reasoning significantly impaired the model's move quality, which we attribute to the excessive amount of erroneous information in the model's reasoning."
- Break condition: If the model can be trained to generate only accurate reasoning, or if the model can distinguish between reliable and unreliable information in its own reasoning process.

## Foundational Learning

- Concept: Self-attention mechanism in transformer models
  - Why needed here: Understanding how attention weights are distributed across tokens is crucial for explaining why ChatGPT's performance degrades in longer games
  - Quick check question: How does the self-attention mechanism in transformers compute attention weights between tokens, and what factors influence this distribution?

- Concept: Formal vs. natural language processing
  - Why needed here: The study compares ChatGPT's performance on formal language (chess notation) versus natural language, requiring understanding of how these differ in computational processing
  - Quick check question: What are the key differences between formal and natural language that might cause a language model trained on natural language to struggle with formal language tasks?

- Concept: Chain-of-thought reasoning in LLMs
  - Why needed here: The study investigates how allowing natural language reasoning affects model performance, which requires understanding how chain-of-thought prompting works
  - Quick check question: How does chain-of-thought prompting work in large language models, and why might it improve performance on certain reasoning tasks while potentially introducing errors?

## Architecture Onboarding

- Component map: ChatGPT (language model) -> Stockfish (chess engine) -> Prompt templates (experimental variations) -> Evaluation metrics (IMR, RBLM, BE, GL) -> Analysis tools
- Critical path: Generate move → Check legality → If illegal, regenerate or terminate → Update board state → Evaluate position → Repeat until game end or termination
- Design tradeoffs: The choice between allowing natural language reasoning (which improves intent but introduces errors) versus strict formal language (which maintains consistency but lacks depth) represents a key tradeoff in the system design.
- Failure signatures: High IMR indicates rule comprehension issues, high RBLM indicates move selection uncertainty, decreasing game length indicates performance degradation, and poor manual analysis results indicate reasoning quality problems.
- First 3 experiments:
  1. Baseline experiment: Run 1000 games with minimal prompt information to establish performance baseline
  2. Move-Repeat variation: Implement token repetition strategy to test attention decay mitigation
  3. Int-Rules variation: Add chess rules to prompt to test formal language comprehension enhancement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of token repetition on the attention mechanism of language models in tasks requiring complex formal language comprehension?
- Basis in paper: [explicit] The paper suggests that consistent repetition of relevant information throughout a conversation can partially alleviate the limitations of language models' attention mechanism in tasks requiring complex formal language comprehension.
- Why unresolved: While the paper demonstrates the positive effect of token repetition in the context of chess, it does not provide a comprehensive analysis of its impact on other tasks requiring complex formal language comprehension.
- What evidence would resolve it: Conducting experiments with token repetition on various tasks involving complex formal language comprehension, such as programming languages or mathematical notation, would provide insights into the generalizability of this approach.

### Open Question 2
- Question: How does the introduction of natural language board descriptions affect the model's performance in tasks involving formal language comprehension?
- Basis in paper: [explicit] The paper introduces an experiment where natural language board descriptions are provided alongside formal chess notations, resulting in a significant decrease in performance.
- Why unresolved: The paper does not explore the underlying reasons for this performance decrease or investigate potential ways to improve the model's ability to integrate natural language descriptions with formal language comprehension.
- What evidence would resolve it: Further analysis of the model's internal representations and attention patterns when processing natural language board descriptions could shed light on the factors contributing to the performance decrease and suggest potential improvements.

### Open Question 3
- Question: To what extent does the model's decision-making focus, or "intent," influence its performance in tasks requiring complex formal language comprehension?
- Basis in paper: [explicit] The paper introduces the concept of "intent" as the model's decision-making focus and finds that allowing natural language reasoning, providing natural language chessboard descriptions, or enabling a clearer representation of the game board can strengthen the model's intent.
- Why unresolved: The paper does not provide a detailed analysis of how intent influences the model's performance in different aspects of complex formal language comprehension tasks or investigate potential ways to optimize intent for better performance.
- What evidence would resolve it: Conducting experiments that systematically manipulate the model's intent through various prompting strategies and analyzing the corresponding changes in performance across different aspects of complex formal language comprehension tasks would provide insights into the role of intent in the model's abilities.

## Limitations

- The study focuses exclusively on chess as a case study, limiting generalizability to other formal language tasks
- The research does not explore the potential benefits of fine-tuning ChatGPT on chess-specific data to address identified limitations
- The exact quantification of attention weight decay over token distance is not directly measured, relying instead on indirect evidence and prior research

## Confidence

- **High**: ChatGPT's performance on chess tasks - The metrics and experimental results clearly demonstrate consistent performance issues across multiple variations and conditions
- **Medium**: Attention mechanism as primary cause of performance degradation - While supported by evidence and literature, direct measurement of attention weights would strengthen this claim
- **High**: Natural language reasoning's dual effect - The data clearly shows improved intent but reduced move quality when allowing natural language reasoning
- **Low**: Generalizability to other formal language tasks - The study focuses exclusively on chess, and extending these findings to other formal languages requires additional validation

## Next Checks

1. **Direct attention weight analysis**: Implement methods to measure and visualize ChatGPT's attention weight distribution across tokens in chess games, particularly comparing attention patterns between legal and illegal moves to directly test the attention decay hypothesis.

2. **Cross-engine evaluation**: Replicate the experiments using multiple chess engines (e.g., Stockfish, Leela Chess Zero, Komodo) to verify that the observed performance issues are consistent across different evaluation standards and playing styles.

3. **Fine-tuning experiment**: Conduct a controlled experiment where ChatGPT is fine-tuned on chess-specific data for a limited number of epochs, then re-evaluate its performance to determine whether the identified limitations can be mitigated through domain adaptation.