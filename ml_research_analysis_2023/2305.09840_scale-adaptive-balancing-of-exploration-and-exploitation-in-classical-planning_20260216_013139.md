---
ver: rpa2
title: Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning
arxiv_id: '2305.09840'
source_url: https://arxiv.org/abs/2305.09840
tags:
- search
- planning
- algorithms
- guct
- gbfs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing exploration and
  exploitation in classical planning algorithms. The authors identify that existing
  Monte Carlo Tree Search (MCTS) methods, such as GreedyUCT, use UCB1 Multi-Armed
  Bandit algorithms in an ad hoc manner, failing to satisfy UCB1's theoretical requirements
  for fixed bounded support reward distributions.
---

# Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning

## Quick Facts
- arXiv ID: 2305.09840
- Source URL: https://arxiv.org/abs/2305.09840
- Reference count: 12
- Key outcome: GreedyUCT-Normal algorithm significantly outperforms traditional planning algorithms by properly adapting exploration rates to reward scales

## Executive Summary
This paper addresses a fundamental issue in Monte Carlo Tree Search (MCTS) for classical planning: the ad hoc application of UCB1 Multi-Armed Bandit algorithms fails to satisfy theoretical requirements for fixed bounded support reward distributions. The core problem is UCB1's lack of adaptation to different reward scales in planning heuristics. The authors propose GreedyUCT-Normal, which uses UCB1-Normal bandit algorithm that considers reward variance to automatically adjust exploration rates. This marks the first time MCTS-based algorithms properly outperformed traditional algorithms in classical planning settings, solving 36 more instances than Greedy Best First Search under the same expansion budget.

## Method Summary
The paper introduces GreedyUCT-Normal, an MCTS/Trial Based Heuristic Tree Search algorithm that replaces the standard UCB1 bandit with UCB1-Normal bandit. This change allows the algorithm to handle reward distributions with different scales by incorporating variance estimates into the exploration term. The algorithm maintains a tree-based open list and uses recursive action selection for node selection, backpropagation of statistics for updates, and handles locked nodes and subtree merging appropriately. The implementation uses the Pyperplan framework and is evaluated on International Planning Competition benchmark domains with multiple heuristics including hFF, hadd, hmax, and hGC.

## Key Results
- GreedyUCT-Normal solved 36 more instances than GBFS using hFF heuristic under the same expansion budget
- This marks the first time MCTS-based algorithms properly outperformed traditional algorithms in classical planning settings
- The algorithm shows consistent quality degradation interpreted as sufficient exploration, particularly in GreedyUCT-Normal2 variant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UCB1-Normal adapts exploration rates to reward scale automatically
- Mechanism: Uses reward variance estimate to scale exploration bonus term
- Core assumption: Reward distributions in planning have different variances
- Evidence anchors: The algorithm handles distributions with different scales by taking reward variance into consideration; appropriate MAB algorithms should automatically adjust to different scales

### Mechanism 2
- Claim: Scale-adaptive exploration improves node selection in planning
- Mechanism: Larger heuristic values indicate greater uncertainty about goal distance
- Core assumption: Heuristic accuracy correlates inversely with distance to goal
- Evidence anchors: In planning, arms with larger scale require more explorations because reward scale correlates to distance to goal; if goal is far, heuristic value tends to be large and inaccurate (large variance), so algorithm should explore more

### Mechanism 3
- Claim: Proper MAB application enables MCTS to outperform traditional planning algorithms
- Mechanism: Correct theoretical foundation eliminates need for ad-hoc enhancements
- Core assumption: Theoretical soundness translates to practical performance gains
- Evidence anchors: This marks the first time MCTS-based algorithms properly outperformed traditional algorithms in classical planning settings; consistent quality degradation interpreted as indication of sufficient exploration

## Foundational Learning

- Concept: Multi-Armed Bandit problem formulation
  - Why needed here: Provides theoretical foundation for balancing exploration and exploitation
  - Quick check question: What is the key difference between UCB1 and UCB1-Normal bandit algorithms?

- Concept: Variance estimation in reward distributions
  - Why needed here: Critical for scale-adaptive exploration rate adjustment
  - Quick check question: How does UCB1-Normal use variance estimates to modify its exploration term?

- Concept: Tree search versus priority queue search
  - Why needed here: Understanding how MCTS/THTS differs from traditional search algorithms
  - Quick check question: What is the key difference between selection in MCTS and in GBFS?

## Architecture Onboarding

- Component map: Root node → Selection (recursive action selection) → Expansion (generate successors) → Evaluation (heuristic calculation) → Backpropagation (update statistics) → Repeat
- Critical path: Node selection → Expansion → Backpropagation of statistics
- Design tradeoffs: Tree-based search allows efficient non-monotonic updates but requires more memory than priority queue
- Failure signatures: Poor performance on domains with highly accurate heuristics, excessive exploration in shallow search
- First 3 experiments:
  1. Compare GUCT-Normal2 vs GBFS on a simple domain with large heuristic values
  2. Test variance estimation accuracy in different parts of the search tree
  3. Measure exploration-exploitation balance by comparing solution lengths and node expansions

## Open Questions the Paper Calls Out

1. What specific enhancements (beyond the algorithms themselves) are needed to achieve state-of-the-art results in classical planning? The paper focuses on algorithmic efficiency rather than low-level performance, and future work includes combinations with existing ad-hoc or principled enhancements.

2. How does the performance of GUCT-Normal2 compare to state-of-the-art planners when implemented in a high-performance language like C++? The current Python implementation is deliberately unoptimized and significantly slower than competitive planners.

3. Are there bandit algorithms that better reflect the specific assumptions of classical planning (e.g., non-negative cost-to-go estimates) than UCB1-Normal? The Gaussian reward assumption made by UCB1-Normal is sufficient but not necessary for classical planning applications.

## Limitations

- The empirical validation is limited to specific IPC domains and heuristics, raising questions about generalizability
- The handling of variance estimation with limited samples and the impact of random tie-breaking on reproducibility are not fully addressed
- The Python implementation is deliberately unoptimized, making direct comparison with C++-based state-of-the-art planners impossible

## Confidence

- **High Confidence:** The theoretical foundation for using UCB1-Normal over UCB1 in planning contexts is sound
- **Medium Confidence:** The empirical results showing improved performance on benchmark domains are credible but may not generalize
- **Medium Confidence:** The interpretation of consistent quality degradation as evidence of proper exploration is reasonable but could have alternative explanations

## Next Checks

1. Test GreedyUCT-Normal on additional planning domains with varying heuristic characteristics to assess robustness across different problem types
2. Conduct ablation studies to isolate the contribution of variance-based exploration from other algorithmic differences
3. Measure the impact of sample size on variance estimation accuracy and resulting search behavior