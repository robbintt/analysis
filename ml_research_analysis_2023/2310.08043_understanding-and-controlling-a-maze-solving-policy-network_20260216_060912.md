---
ver: rpa2
title: Understanding and Controlling a Maze-Solving Policy Network
arxiv_id: '2310.08043'
source_url: https://arxiv.org/abs/2310.08043
tags:
- cheese
- maze
- policy
- figure
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the goals and goal representations of a
  maze-solving policy network that exhibits goal misgeneralization. Through behavioral
  analysis and examination of network activations, the authors demonstrate that the
  policy pursues multiple context-dependent goals and identify 11 residual channels
  that track the location of the cheese goal.
---

# Understanding and Controlling a Maze-Solving Policy Network

## Quick Facts
- **arXiv ID**: 2310.08043
- **Source URL**: https://arxiv.org/abs/2310.08043
- **Reference count**: 40
- **Primary result**: A maze-solving policy network exhibits goal misgeneralization, pursuing context-dependent goals (cheese vs. historical cheese location) tracked by 11 specific residual channels, which can be partially controlled through activation engineering.

## Executive Summary
This paper investigates a maze-solving reinforcement learning policy that exhibits goal misgeneralization, where it sometimes pursues a historical cheese location instead of the current cheese position. Through behavioral analysis and examination of network activations, the authors demonstrate that the policy uses context-dependent goals based on maze features, with 11 residual channels tracking the absolute position of the cheese goal. The study shows that these channel activations can be manipulated through activation engineering techniques to partially control the policy's behavior without retraining, revealing distributed and retargetable goal representations in the network.

## Method Summary
The researchers analyzed a 3.5M-parameter deep convolutional network trained via Proximal Policy Optimization to solve maze navigation tasks. They generated 5,000 mazes where the agent must choose between navigating to the cheese or the historical cheese location. The method combined behavioral analysis using logistic regression to predict goal pursuit based on maze features, visual inspection of residual channel activations to identify cheese-tracking circuits, and activation engineering techniques including manual edits to channel activations and combining forward passes to control behavior.

## Key Results
- The policy network exhibits goal misgeneralization, pursuing different goals (cheese vs. historical cheese location) based on context-dependent maze features
- 11 residual channels were identified that track the absolute position of the cheese goal through their activation patterns
- Activation engineering techniques can partially control the policy's behavior by modifying these channel activations, without requiring retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The policy network uses context-dependent goals based on maze features
- Mechanism: The network learns to pursue different goals (cheese vs. historical cheese location) based on environmental cues like distance metrics
- Core assumption: The policy can distinguish between different goal states and select appropriate actions
- Evidence anchors:
  - [abstract] "we can predict whether the policy navigates to the cheese or the historical cheese location based on maze factors"
  - [section 2.1] "Logistic regression on these features achieves an average accuracy of 82.4%"

### Mechanism 2
- Claim: Cheese location is tracked by specific residual channels in the network
- Mechanism: 11 channels encode the absolute position of cheese through their activation patterns
- Core assumption: Network activations can be interpreted as spatial representations
- Evidence anchors:
  - [abstract] "we identified eleven channels that track the location of the cheese"
  - [section 2.2] "Fig. 3 shows the activations of channel 55 for mazes where the goal is placed in different locations"

### Mechanism 3
- Claim: Activation engineering can control policy behavior without retraining
- Mechanism: Modifying specific channel activations or combining forward passes changes goal pursuit
- Core assumption: Network representations are modular and can be edited independently
- Evidence anchors:
  - [abstract] "By modifying these channels... we can partially control the policy"
  - [section 3.1] "We set just one activation in channel 55 to a large positive value (+5.55)"

## Foundational Learning

- Concept: Convolutional neural network architecture
  - Why needed here: Understanding the policy network's structure is essential for interpreting channel activations
  - Quick check question: What is the difference between a residual block and a regular convolutional layer?

- Concept: Reinforcement learning and goal-directed behavior
  - Why needed here: The policy network was trained using RL, so understanding how agents learn goals is crucial
  - Quick check question: How does proximal policy optimization differ from basic policy gradient methods?

- Concept: Activation engineering and interpretability
  - Why needed here: The paper's main contribution is controlling behavior through activation modifications
  - Quick check question: What is the difference between causal scrubbing and activation resampling?

## Architecture Onboarding

- Component map: Input image → 15 convolutional layers → residual channels → action probabilities
- Critical path: The policy network has 15 convolutional layers with 3.5M parameters, organized in IMPALA blocks with residual connections
- Design tradeoffs: Large network allows complex behavior but makes interpretability harder; IMPALA architecture balances efficiency with expressiveness
- Failure signatures: Misgeneralization (going to wrong location), inconsistent channel activation patterns, poor retargetability
- First 3 experiments:
  1. Visualize channel activations for mazes with cheese in different positions
  2. Test logistic regression prediction of goal selection using maze features
  3. Apply single activation edits to channel 55 and observe behavioral changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the other mechanisms in the policy network that track the cheese location, beyond the 11 identified channels?
- Basis in paper: The authors note that moving the cheese directly achieves even stronger retargetability than their activation edits, suggesting additional unidentified cheese-seeking mechanisms exist.
- Why unresolved: The study only identified 11 channels that track cheese location. The authors acknowledge there may be more cheese-tracking circuits they did not find.

### Open Question 2
- Question: How do the 11 cheese-tracking channels interact with each other and other parts of the network to influence the policy's behavior?
- Basis in paper: The authors show that the cheese-tracking channels primarily affect behavior based on cheese location, but also have some weak effects on other factors. The mechanisms by which these channels interact and influence the policy are not fully explored.
- Why unresolved: The study focuses on identifying and manipulating the cheese-tracking channels individually. The complex interactions between these channels and the rest of the network are not examined.

### Open Question 3
- Question: How generalizable are the findings from this maze-solving policy to other trained reinforcement learning policies?
- Basis in paper: The authors state that "policy networks may be well-understood as pursuing multiple context-dependent goals" based on their findings. However, the study only examines one specific maze-solving policy.
- Why unresolved: The research focuses on a single policy network. It is unclear whether the identified goal representations and control methods would apply to other policies trained on different tasks or environments.

## Limitations
- The findings are based on behavioral analysis and activation inspection rather than formal causal attribution
- Confidence is medium for the activation engineering control results, as the effects are partial and sometimes inconsistent
- The study does not address potential adversarial scenarios or distributional shift effects

## Confidence
- **High**: Identification of channels tracking cheese location through visual patterns
- **Medium**: Claim that the policy uses context-dependent goals based on maze features
- **Medium**: Activation engineering techniques can partially control behavior

## Next Checks
1. **Causal Intervention Test**: Use causal scrubbing or activation resampling to verify that modifying channel 55 activations directly causes changes in goal-directed behavior, rather than just correlating with it.

2. **Generalization Test**: Apply the activation engineering techniques to mazes with novel configurations (different cheese positions, wall layouts) to test whether the control methods transfer beyond the training distribution.

3. **Adversarial Robustness Test**: Systematically perturb the identified cheese-tracking channels to determine if an adversary could exploit these representations to induce goal misgeneralization or unintended behaviors.