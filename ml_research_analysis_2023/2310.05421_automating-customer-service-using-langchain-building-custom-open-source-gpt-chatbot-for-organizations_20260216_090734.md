---
ver: rpa2
title: 'Automating Customer Service using LangChain: Building custom open-source GPT
  Chatbot for organizations'
arxiv_id: '2310.05421'
source_url: https://arxiv.org/abs/2310.05421
tags:
- customer
- service
- langchain
- research
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research presents a custom open-source framework for automating
  customer service using LangChain and Google's Flan T5 XXL model. The approach employs
  web scraping for data collection, embeddings via HuggingFace Instructor, and FAISS
  for vector storage.
---

# Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations

## Quick Facts
- arXiv ID: 2310.05421
- Source URL: https://arxiv.org/abs/2310.05421
- Reference count: 8
- This research presents a custom open-source framework for automating customer service using LangChain and Google's Flan T5 XXL model.

## Executive Summary
This paper introduces a custom open-source framework for automating customer service using LangChain and Google's Flan T5 XXL model. The approach employs web scraping for data collection, embeddings via HuggingFace Instructor, and FAISS for vector storage. The model is integrated with Gradio API for deployment. Comparative evaluation shows that the Flan T5 XXL model outperforms Flan T5 Base and Small models in response quality, with higher accuracy in retrieving information from educational institution data. The system demonstrates improved customer interactions by providing personalized, context-aware responses, available 24/7.

## Method Summary
The method involves web scraping an organization's homepage using BeautifulSoup to extract relevant text content, generating embeddings with HuggingFace Instructor, and storing them in FAISS for efficient retrieval. The Flan T5 XXL model is fine-tuned on these embeddings and integrated with Gradio API for web deployment. The framework aims to automate customer service by providing context-aware responses based on the organization's knowledge base.

## Key Results
- Flan T5 XXL model outperforms Flan T5 Base and Small models in response quality
- Higher accuracy in retrieving information from educational institution data
- Improved customer interactions with personalized, context-aware responses available 24/7

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instructor embeddings fine-tuned for classification/retrieval enable more accurate context matching without requiring additional fine-tuning of the LLM.
- Mechanism: The instructor-large model maps text into dense vectors that preserve semantic meaning, allowing FAISS to efficiently retrieve the most relevant knowledge chunks for a given query. This reduces the burden on the LLM to interpret raw text and instead gives it precise context.
- Core assumption: The embedding space captures domain-specific semantics relevant to the educational institution data used in the demo.
- Evidence anchors:
  - [abstract] "We have used HuggingFace Instuct Embeddings – 'hkunlp/instructor-large' a text embedding model fine-tuned for specific tasks and domains, such as classification, retrieval, clustering, and text evaluation."
  - [section] "Embeddings play a pivotal role in the development of any LLM powered. They are vector representations of words or phrases in a continuous mathematical space that capture semantic and contextual information."
- Break condition: If the domain vocabulary is too specialized or out-of-distribution, embedding similarity may not reflect true relevance, causing retrieval errors that propagate to the final answer.

### Mechanism 2
- Claim: The Flan-T5 XXL model achieves better response quality than Base or Small variants due to its larger capacity and instruction tuning.
- Mechanism: Larger model size allows it to better understand nuanced queries and maintain conversational context, especially when fine-tuned with instruction datasets that guide its generation toward task-appropriate outputs.
- Core assumption: The instruction tuning and scale directly translate into improved performance on the given retrieval-and-generate task, not just on general language tasks.
- Evidence anchors:
  - [abstract] "Comparative evaluation shows that the Flan T5 XXL model outperforms Flan T5 Base and Small models in response quality, with higher accuracy in retrieving information from educational institution data."
  - [section] "We have chosen Google's Flan T5 XXL as the most appropriate language model after comparing with other Flan T5 distributions to retrieve knowledge from the vectorspace and chat_history."
- Break condition: If the context window is too small relative to the amount of retrieved data, even XXL cannot maintain coherence, or if the retrieved chunks are poorly ordered, the model's advantage diminishes.

### Mechanism 3
- Claim: Gradio integration provides a low-friction web interface for testing and deployment without custom front-end development.
- Mechanism: Gradio's high-level API lets the engineer bind the LLM pipeline to a simple chat window UI, enabling quick iteration and demonstration of the system.
- Core assumption: The target users (e.g., BVM staff or students) need only a browser-based interface and do not require advanced features like multi-session state or complex authentication.
- Evidence anchors:
  - [abstract] "The model is integrated with Gradio API for deployment."
  - [section] "However, for the demonstration purpose of this paper, we are using Gradio API framework."
- Break condition: Gradio's simplicity becomes a bottleneck if the application needs to scale to production-level concurrency, custom styling, or enterprise integrations.

## Foundational Learning

- Embeddings and similarity search
  - Why needed here: They enable the system to quickly find relevant context from large knowledge bases, which is essential for context-aware responses.
  - Quick check question: What is the role of FAISS in this pipeline, and why is it paired with embeddings?

- Instruction tuning and model selection
  - Why needed here: Different model sizes and instruction sets yield different trade-offs in accuracy and resource usage; selecting the right one is key for production feasibility.
  - Quick check question: Why might XXL outperform Base and Small models in this use case, and what are the cost implications?

- Web scraping and data preprocessing
  - Why needed here: The knowledge base is dynamically built from the institution's website, so robust scraping and cleaning are prerequisites for reliable embeddings and retrieval.
  - Quick check question: What kinds of data are extracted from the website, and how are they structured for the embedding pipeline?

## Architecture Onboarding

- Component map:
  Data collection: BeautifulSoup scraper → raw HTML/text → preprocessing: cleaning, chunking → embeddings: HuggingFace instructor-large → vectors → storage: FAISS index → retrieval: similarity search → context chunks → generation: Flan-T5 XXL + chat history → response → interface: Gradio chat UI

- Critical path:
  Scrape → preprocess → embed → store in FAISS → query → retrieve → feed to LLM with chat history → generate → display

- Design tradeoffs:
  - Model size vs. latency: XXL gives better quality but higher compute cost.
  - Chunk size vs. retrieval precision: larger chunks preserve context but may dilute relevance scores.
  - Gradio vs. custom UI: faster prototyping but less scalable.

- Failure signatures:
  - Empty or irrelevant retrievals → embedding or FAISS indexing issue.
  - Repetitive or off-topic responses → context window or chat history handling bug.
  - High latency → oversized model or inefficient retrieval step.

- First 3 experiments:
  1. Run the scraper on a small set of known pages, inspect extracted text for quality.
  2. Generate embeddings for the small set, verify FAISS search returns semantically similar chunks.
  3. Pass a test query through the full pipeline (retrieval + LLM) and evaluate response relevance manually.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the quantitative performance differences between Flan T5 XXL, Base, and Small models in terms of accuracy, response time, and resource consumption?
- Basis in paper: [inferred] The paper mentions that XXL outperforms Base and Small models but does not provide specific metrics or quantitative comparisons.
- Why unresolved: The authors state that XXL outperforms other models but lack concrete numerical data to support this claim.
- What evidence would resolve it: A detailed comparison table showing accuracy percentages, response times, and resource usage (e.g., memory, CPU/GPU utilization) for each model variant would provide the necessary quantitative evidence.

### Open Question 2
- Question: How does the system handle updates to the website content and ensure the chatbot's knowledge remains current?
- Basis in paper: [inferred] The paper describes using web scraping for data collection but does not address how frequently this is done or how the system manages dynamic content changes.
- Why unresolved: The methodology section focuses on initial data collection but lacks details on maintenance and updates to keep the chatbot's knowledge base current.
- What evidence would resolve it: Information on the frequency of web scraping, mechanisms for detecting and incorporating content changes, and validation processes to ensure the chatbot's responses remain accurate would address this question.

### Open Question 3
- Question: What are the limitations of the current implementation in terms of handling complex queries or domain-specific terminology?
- Basis in paper: [explicit] The authors mention that human knowledge is relied upon to evaluate performance, suggesting potential limitations in automated evaluation metrics.
- Why unresolved: The paper acknowledges the difficulty in evaluating LLM performance but does not provide specific examples of query types that pose challenges or how the system handles specialized terminology.
- What evidence would resolve it: A comprehensive analysis of query types that result in inaccurate or incomplete responses, along with examples of domain-specific terms that are misinterpreted, would clarify the system's limitations.

## Limitations
- Evaluation relies on qualitative assessments without specific quantitative metrics
- Fine-tuning procedure for Flan T5 models is not detailed
- Demonstration uses a single educational institution's website, limiting generalizability

## Confidence
- Medium confidence in Mechanism 1 (Instructor embeddings for retrieval)
- Medium confidence in Mechanism 2 (Flan-T5 XXL superiority)
- High confidence in Mechanism 3 (Gradio integration)

## Next Checks
1. Conduct a blind human evaluation study comparing XXL, Base, and Small model responses on identical queries, using standardized relevance and coherence rubrics.
2. Perform ablation testing by varying chunk sizes and measuring retrieval precision/recall to identify optimal preprocessing parameters for the embedding-retrieval pipeline.
3. Test the system on a different organizational domain (e.g., healthcare or retail) to assess cross-domain robustness and identify potential embedding or retrieval failures.