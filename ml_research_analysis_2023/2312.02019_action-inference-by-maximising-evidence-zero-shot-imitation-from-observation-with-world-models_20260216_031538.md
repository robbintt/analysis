---
ver: rpa2
title: 'Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation
  with World Models'
arxiv_id: '2312.02019'
source_url: https://arxiv.org/abs/2312.02019
tags:
- aime
- learning
- dataset
- world
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AIME, a novel method for imitation learning
  from observation (ILO) using world models. The method learns a world model from
  embodiment data in phase 1, then uses it to infer expert actions from observation-only
  demonstrations in phase 2.
---

# Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models

## Quick Facts
- arXiv ID: 2312.02019
- Source URL: https://arxiv.org/abs/2312.02019
- Authors: 
- Reference count: 40
- Outperforms BCO(0) baseline by up to 40% on Walker and Cheetah tasks

## Executive Summary
This paper presents AIME, a novel method for imitation learning from observation (ILO) using world models. The method learns a world model from embodiment data in phase 1, then uses it to infer expert actions from observation-only demonstrations in phase 2. AIME maximizes the evidence lower bound (ELBO) in both phases, with the key difference being that actions are known in phase 1 but must be inferred in phase 2. On Walker and Cheetah tasks, AIME outperforms state-of-the-art baselines like BCO(0) by large margins, achieving up to 40% better normalized returns. The method demonstrates strong zero-shot transfer capabilities, even outperforming oracle behavioral cloning in low-data scenarios. AIME shows robustness to observation modalities and scalability to larger demonstration datasets.

## Method Summary
AIME is a two-phase approach to imitation learning from observation that uses world models to infer expert actions from observation-only demonstrations. In phase 1, a variational world model (RSSM) is learned from embodiment data by maximizing the ELBO, capturing the relationship between actions and observations. In phase 2, this frozen world model is used to infer actions from demonstration data by maximizing the evidence p(o1:T) under the world model. A policy network amortizes this inference process, enabling efficient zero-shot imitation. The method is evaluated on Walker and Cheetah tasks from DeepMind Control Suite across three observation modalities (MDP, LPOMDP, Visual) and compared against the BCO(0) baseline.

## Key Results
- Achieves up to 40% better normalized returns than BCO(0) baseline on Walker and Cheetah tasks
- Demonstrates strong zero-shot transfer capabilities, outperforming oracle behavioral cloning in low-data scenarios
- Shows robustness to observation modalities, with consistent performance across MDP, LPOMDP, and Visual settings
- Scales to larger demonstration datasets, with continual improvement as more trajectories are added

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AIME learns a world model that generalizes across tasks and observation modalities by maximizing evidence in phase 1.
- Mechanism: The variational world model (RSSM) learns to model p(o1:T|a0:T-1) from embodiment data, capturing the causal relationship between actions and observations. This learned model is then frozen and used to infer actions from observation-only demonstrations by maximizing evidence p(o1:T).
- Core assumption: The world model can effectively capture the embodiment dynamics and generalize to new tasks and observation modalities.
- Evidence anchors:
  - [abstract] "AIME consists of two distinct phases. In the first phase, the agent learns a world model from its past experience to understand its own body by maximising the ELBO."
  - [section 3.1] "we define a world model as a probability distribution over sequences of observations"
  - [corpus] Weak - related works focus on imitation learning but don't specifically discuss world model generalization across modalities.
- Break condition: The world model fails to generalize if the embodiment data is insufficient or the observation modalities are too different.

### Mechanism 2
- Claim: AIME's action inference is more robust than IDM-based methods because it maximizes evidence rather than recovering true actions.
- Mechanism: By maximizing the evidence lower bound (ELBO) in phase 2, AIME finds an action sequence that best explains the observations under the world model, rather than trying to recover the true actions. This is more robust to non-invertible dynamics and observation noise.
- Core assumption: Maximizing evidence leads to better imitation performance than recovering true actions, especially in non-invertible systems.
- Evidence anchors:
  - [abstract] "AIME achieves this by defining a policy as an inference model and maximising the evidence of the demonstration under the policy and world model."
  - [section 3.2] "We are mainly interested in mimicking the expert's demonstration and may be better able to do so with a different action sequence."
  - [section 4.1] "we attribute the good performance of AIME to two reasons. First, the world model has a better data utilisation rate than the IDM..."
- Break condition: The method fails if the true action sequence is crucial for task success and cannot be inferred from maximizing evidence.

### Mechanism 3
- Claim: AIME's zero-shot performance is enabled by amortizing action inference through a policy network conditioned on the world model's latent state.
- Mechanism: Instead of solving an optimization problem for each demonstration sequence, AIME learns a policy πψ(at|st) that directly samples actions given the latent state st. This amortizes the inference process and enables efficient zero-shot imitation.
- Core assumption: A policy network can effectively amortize the action inference process and generalize across demonstrations.
- Evidence anchors:
  - [section 3.2] "To make it more efficient, we use amortised inference. We directly define a policy πψ(at|st) under the latent state of the world model."
  - [section 4.2] "AIME demonstrates continual improvement with as few as 2 trajectories."
  - [corpus] Weak - related works don't explicitly discuss amortization of action inference in imitation learning.
- Break condition: The method fails if the policy network cannot effectively learn the action inference mapping from the world model's latent state.

## Foundational Learning

- Concept: Variational inference and the evidence lower bound (ELBO)
  - Why needed here: AIME uses ELBO maximization in both phases to learn the world model and infer actions.
  - Quick check question: What is the difference between the ELBO and the true log-likelihood, and why is maximizing ELBO useful in this context?

- Concept: State-space models (SSMs) and recurrent neural networks
  - Why needed here: AIME uses an RSSM to model the dynamics of the environment and infer latent states from observations.
  - Quick check question: How does the RSSM architecture differ from a standard recurrent neural network, and what are the benefits of using it in this context?

- Concept: Imitation learning from observation (ILO) and behavioral cloning
  - Why needed here: AIME is an ILO method that learns to imitate expert behavior from observation-only demonstrations.
  - Quick check question: What are the key challenges in ILO compared to traditional imitation learning with expert actions, and how does AIME address these challenges?

## Architecture Onboarding

- Component map:
  - Phase 1: World model learning
    - Encoder: Extracts features from observations
    - Posterior and prior: Model latent state distribution
    - Decoder: Generates observations from latent state
    - Policy: Samples actions given latent state
  - Phase 2: Action inference
    - Frozen world model from phase 1
    - Policy network: Amortized action inference
    - Objective: Maximize ELBO on demonstration data

- Critical path:
  1. Train world model on embodiment data by maximizing ELBO
  2. Freeze world model and train policy on demonstration data by maximizing ELBO
  3. Evaluate policy on task

- Design tradeoffs:
  - World model complexity vs. sample efficiency: Larger models may generalize better but require more data
  - Latent state dimension: Higher dimensions may capture more information but increase computational cost
  - Policy architecture: More complex policies may better approximate action inference but risk overfitting

- Failure signatures:
  - World model fails to generalize: Low ELBO on demonstration data, poor task performance
  - Policy fails to learn: High reconstruction error, poor task performance
  - Overfitting: Low training ELBO but high validation ELBO, poor task performance

- First 3 experiments:
  1. Train world model on embodiment data and evaluate ELBO on held-out data
  2. Train policy on demonstration data and evaluate ELBO on held-out demonstrations
  3. Evaluate policy on task and compare to baseline methods

## Open Questions the Paper Calls Out

- Question: How does the performance of AIME vary with different world model architectures, particularly those without decoders (e.g., contrastive models)?
  - Basis in paper: [inferred] The paper mentions that using only the KL term in the objective function yields results close to using both terms, suggesting the latent state captures essential information. It also discusses the potential of incorporating decoder-free models.
  - Why unresolved: The paper only uses RSSM with encoders and decoders, and doesn't explore alternative world model architectures.
  - What evidence would resolve it: Experiments comparing AIME with different world model architectures, including decoder-free models, on the same tasks and datasets.

- Question: How does AIME perform in settings where the embodiment and sensor layout differ between the embodiment and demonstration datasets?
  - Basis in paper: [explicit] The paper discusses this as a limitation, noting that humans can imitate animals with different body structures and observe from a third-person perspective.
  - Why unresolved: The paper only studies the simplest setting where the embodiment and sensor layout are fixed across tasks.
  - What evidence would resolve it: Experiments applying AIME to cross-embodiment imitation tasks, such as imitating human videos with a robotic embodiment.

- Question: How does the performance of AIME change when incorporating an online learning phase to handle tasks that require new skills beyond observation?
  - Basis in paper: [explicit] The paper mentions that some tasks are too complex for zero-shot imitation even for humans, motivating an online learning phase 3 as an extension to the framework.
  - Why unresolved: The paper only presents the two-phase AIME algorithm without an online learning component.
  - What evidence would resolve it: Experiments comparing AIME with and without an online learning phase on tasks that require novel skills beyond observation.

## Limitations
- The method's performance heavily depends on the quality of the world model learned in phase 1, and insufficient embodiment data could lead to poor generalization
- The paper provides limited analysis of how different world model architectures or hyperparameters affect performance
- While the method shows robustness across observation modalities, the visual setting experiments use only 1000 trajectories, which may not reflect real-world complexity

## Confidence
- High confidence in AIME's basic two-phase ELBO maximization framework
- Medium confidence in claims about robustness to observation modalities
- Medium confidence in scalability claims based on limited visual experiments

## Next Checks
1. Test AIME's performance with varying amounts of embodiment data to quantify the minimum requirements for effective world model learning
2. Evaluate the method on more complex visual tasks with higher-dimensional observations to verify scalability claims
3. Compare AIME's performance when using true actions versus inferred actions in phase 2 to quantify the impact of the inference approach