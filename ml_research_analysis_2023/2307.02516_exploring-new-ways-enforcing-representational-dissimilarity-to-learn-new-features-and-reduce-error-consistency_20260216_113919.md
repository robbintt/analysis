---
ver: rpa2
title: 'Exploring new ways: Enforcing representational dissimilarity to learn new
  features and reduce error consistency'
arxiv_id: '2307.02516'
source_url: https://arxiv.org/abs/2307.02516
tags:
- similarity
- learning
- metrics
- dissimilarity
- learn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to training diverse ensembles
  by enforcing representational dissimilarity between models at intermediate layers.
  The method uses similarity metrics from representational similarity analysis to
  regularize intermediate representations during training, promoting models to learn
  distinct features and reduce error consistency.
---

# Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency

## Quick Facts
- arXiv ID: 2307.02516
- Source URL: https://arxiv.org/abs/2307.02516
- Authors: 
- Reference count: 37
- Key outcome: Novel approach using representational dissimilarity regularization to train diverse ensembles, reducing error consistency by up to 7% (Cohen's Kappa) while improving ensemble accuracy by up to 0.2% absolute.

## Executive Summary
This paper introduces a novel approach to training diverse ensembles by enforcing representational dissimilarity between models at intermediate layers. The method uses similarity metrics from representational similarity analysis to regularize intermediate representations during training, promoting models to learn distinct features and reduce error consistency. Experiments with ResNet models on CIFAR10 and CIFAR100 demonstrate that highly dissimilar intermediate representations lead to lower error consistency (up to 7% reduction in Cohen's Kappa) and slightly improved ensemble accuracy (up to 0.2% absolute gain) compared to independently trained models, while maintaining single model performance. The approach provides new insights into the relationship between intermediate representations and predictive behavior in ensemble learning.

## Method Summary
The method involves sequential training of neural networks where each new model is regularized to have dissimilar intermediate representations from all previously trained models in the ensemble. A dissimilarity loss is added to the standard classification loss, using metrics like explained variance, LinCKA, or L2 correlation to measure similarity between intermediate layer activations. The regularization is applied at chosen intermediate layers, with a projection layer used to align channels between models. Models are trained one after another, with each new model conditioned on all preceding models through the dissimilarity regularization.

## Key Results
- Highly dissimilar intermediate representations result in up to 7% reduction in error consistency (Cohen's Kappa) compared to independently trained models
- Ensemble accuracy improves by up to 0.2% absolute when using dissimilarity-regularized models versus baseline ensembles
- Single model performance is maintained or only slightly degraded (up to 0.4% drop) while achieving higher dissimilarity
- Later ensemble members achieve higher dissimilarity than earlier members, validating the sequential training approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularizing intermediate representations with dissimilarity metrics forces models to learn non-overlapping feature subspaces.
- Mechanism: The auxiliary loss penalizes similarity between internal activations of two models at chosen layers. This pushes the new model's representations away from the existing model's subspace, reducing representational overlap.
- Core assumption: The compositional nature of neural networks means that changes to intermediate representations will propagate to influence final predictions.
- Evidence anchors:
  - [abstract] "promote intermediate representations to be dissimilar at different depths between architectures"
  - [section] "we promote intermediate representations to be dissimilar at different depths between architectures"
  - [corpus] Weak evidence - neighboring papers focus on intermediate supervision and multi-layer representations, not dissimilarity regularization specifically.
- Break condition: If the regularization weight λ is too high, the model may sacrifice accuracy to achieve dissimilarity, or if the similarity metric cannot capture meaningful representational differences, the regularization may have no effect.

### Mechanism 2
- Claim: Lower representational similarity at intermediate layers leads to lower error consistency in final predictions.
- Mechanism: By learning dissimilar intermediate features, models develop different decision boundaries and fail on different samples, reducing overlap in error patterns.
- Core assumption: Intermediate feature similarity is correlated with final prediction similarity, such that forcing dissimilarity at intermediate layers reduces output prediction correlation.
- Evidence anchors:
  - [abstract] "highly dissimilar intermediate representations result in less correlated output predictions"
  - [section] "we wonder if high internal dissimilarity translates to less correlated predictive behavior between the model pairs"
  - [corpus] No direct evidence - corpus papers discuss intermediate representations but not their connection to error consistency.
- Break condition: If the models learn to compensate for dissimilar intermediate representations by converging at later layers, or if the task structure inherently requires similar features, error consistency may not decrease.

### Mechanism 3
- Claim: Sequential training with dissimilarity regularization allows building ensembles with increasingly diverse members without retraining from scratch.
- Mechanism: Each new model is trained to be dissimilar from all previous models in the ensemble, creating a sequence where each member explores a different region of the solution space.
- Core assumption: The dissimilarity regularization is additive across models - each new model only needs to be dissimilar from existing ones, not from all possible solutions.
- Evidence anchors:
  - [abstract] "we train sequentially, ending up with one unregularized model, the first one, and multiple regularized models conditioned on all preceding models"
  - [section] "When training multiple models we train sequentially, ending up with one unregularized model, the first one, and multiple regularized models conditioned on all preceding models"
  - [corpus] Weak evidence - corpus papers mention multi-layer representations and architectural agnostic training, but not sequential ensemble building through dissimilarity.
- Break condition: If the regularization becomes too constraining as ensemble size grows, or if early models dominate the solution space making it impossible for later models to find valid dissimilar solutions.

## Foundational Learning

- Concept: Representational Similarity Analysis (RSA)
  - Why needed here: The paper directly applies RSA methods (CKA, correlation analysis) to measure and regularize intermediate representations
  - Quick check question: What is the key difference between aligned metrics like correlation and subspace metrics like CKA in measuring representational similarity?

- Concept: Ensemble Learning and Error Consistency
  - Why needed here: The goal is to improve ensemble performance by reducing error consistency (Cohen's Kappa) between models
  - Quick check question: Why does reducing error consistency between ensemble members typically improve overall ensemble accuracy?

- Concept: Deep Network Compositionality
  - Why needed here: The paper assumes that changes to intermediate representations will affect final predictions due to the compositional nature of neural networks
  - Quick check question: How does the compositional structure of neural networks create a dependency between intermediate and final representations?

## Architecture Onboarding

- Component map: Base model (T) -> New model (U) -> Projection layer (1x1 conv) -> Similarity metric (ExpVar/LinCKA/L2Corr) -> Dissimilarity loss -> Combined loss with classification loss

- Critical path:
  1. Extract intermediate representations zT,l and zU,l at layer l
  2. Apply projection layer to zT,l to get aligned approximation
  3. Calculate similarity metric between zU,l and projected zT,l
  4. Backpropagate both classification and dissimilarity losses
  5. Update both model U and projection layer parameters

- Design tradeoffs:
  - Regularization strength (λ) vs. model accuracy - higher λ increases dissimilarity but may hurt accuracy
  - Layer position - earlier layers are harder to regularize but affect more of the network; later layers are easier but have less impact
  - Single vs. multiple layers - regularizing multiple layers increases regularization area but decreases per-layer effect
  - Aligned vs. subspace metrics - aligned metrics are more interpretable but require channel alignment; subspace metrics are more general but less interpretable

- Failure signatures:
  - Training instability - occurs when λ is too high, causing numerical issues or divergence
  - No dissimilarity achieved - occurs when the metric cannot capture meaningful differences or when the solution space constraints are too tight
  - Accuracy degradation - occurs when regularization forces the model away from optimal solutions
  - Diminishing returns - occurs when later ensemble members cannot find sufficiently dissimilar solutions

- First 3 experiments:
  1. Verify intermediate dissimilarity: Train two models with and without dissimilarity regularization at layer 8, measure LinCKA similarity at that layer and neighboring layers
  2. Test error consistency: Using models from experiment 1, calculate Cohen's Kappa and Jensen-Shannon Divergence between their predictions on test set
  3. Ensemble performance: Combine models from experiment 1 into ensembles, compare ensemble accuracy and error consistency against independently trained models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific features do dissimilar models learn compared to their baseline models, and how do these features differ across regularization positions?
- Basis in paper: [explicit] The paper mentions "we could explore only a small subset of all experiments we deem interesting" and explicitly lists this as a future work question.
- Why unresolved: The study focuses on measuring representational dissimilarity and its effects on ensemble performance, but does not analyze the actual features learned by dissimilar models or compare them across different regularization positions.
- What evidence would resolve it: Detailed feature visualization and analysis of dissimilar models' internal representations, comparing learned filters, activation patterns, and feature maps across different regularization positions and architectures.

### Open Question 2
- Question: Does enforcing representational dissimilarity lead models to converge to genuinely disjoint loss minima, or do they remain functionally similar with different parameterizations?
- Basis in paper: [explicit] The paper asks "Does the dissimilar model end up in a disjoint loss minimum? Can it still be stitched?"
- Why unresolved: While the paper demonstrates reduced representational similarity and error consistency, it does not investigate whether the dissimilar models have fundamentally different optimization trajectories or if they could be combined through model stitching.
- What evidence would resolve it: Analysis of loss landscapes, linear mode connectivity tests, and model stitching experiments between dissimilar models to determine if they occupy truly distinct regions of the solution space.

### Open Question 3
- Question: What is the optimal trade-off between representational dissimilarity strength and model accuracy across different regularization positions and architectures?
- Basis in paper: [inferred] The paper shows that regularization strength affects both dissimilarity and accuracy, noting that "different layers require different λ values as no one value dominates the others" and that "optimally one would like to achieve Cohen's Kappa scores of < 0."
- Why unresolved: The study explores various regularization positions and weights but does not systematically optimize for the best accuracy-dissimilarity trade-off or identify optimal combinations for different architectures.
- What evidence would resolve it: Systematic hyperparameter search across architectures, regularization positions, and loss weights to identify configurations that maximize ensemble performance while minimizing accuracy penalties.

## Limitations

- Limited architectural exploration - experiments only conducted on ResNet architectures, leaving generalizability to other model families (Vision Transformers, ConvNext) untested
- Computational overhead not characterized - the paper does not quantify the additional training time or memory requirements from calculating dissimilarity losses at intermediate layers
- Sequential training scalability unclear - the approach may degrade with larger ensembles, but experiments only tested up to 5 models

## Confidence

- **High confidence**: The empirical demonstration that dissimilarity regularization reduces representational similarity at intermediate layers, and that this correlates with reduced error consistency in final predictions.
- **Medium confidence**: The mechanism by which intermediate representational changes propagate to affect final predictions, and the scalability of sequential ensemble building to larger ensembles.
- **Low confidence**: The general applicability of this approach across different model architectures beyond ResNets, and the optimal choice of regularization strength and layer positions.

## Next Checks

1. **Metric sensitivity analysis**: Systematically compare the three proposed metrics (ExpVar, LinCKA, L2Corr) across multiple runs to determine which is most effective at achieving representational dissimilarity while maintaining accuracy.

2. **Ensemble size scalability**: Extend experiments to ensembles of 10+ models to identify when the sequential training approach breaks down or shows diminishing returns in terms of achieved dissimilarity and ensemble performance.

3. **Architectural generalization**: Test the approach on different backbone architectures (e.g., Vision Transformers, ConvNext) to validate whether the mechanism generalizes beyond ResNets and to what extent architecture-specific design choices are necessary.