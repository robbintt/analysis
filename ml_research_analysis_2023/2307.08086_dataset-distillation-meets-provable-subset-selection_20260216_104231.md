---
ver: rpa2
title: Dataset Distillation Meets Provable Subset Selection
arxiv_id: '2307.08086'
source_url: https://arxiv.org/abs/2307.08086
tags:
- distillation
- data
- uni00000013
- kernel
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of dataset distillation, where
  the goal is to compress a large training dataset into a smaller synthetic one that
  retains performance. The authors propose improving both phases of dataset distillation:
  initialization and learning.'
---

# Dataset Distillation Meets Provable Subset Selection

## Quick Facts
- arXiv ID: 2307.08086
- Source URL: https://arxiv.org/abs/2307.08086
- Authors: 
- Reference count: 40
- Primary result: Dataset distillation technique achieving up to 5% accuracy improvement over baselines

## Executive Summary
This paper addresses dataset distillation by improving both initialization and learning phases. The authors propose provable initialization using coresets - Gaussian kernel coresets for NTK-based methods and K-means coresets for non-NTK methods. During training, they implement smart sampling that selects examples based on their relative contribution to the loss function rather than random sampling. Their method, Distil-Boost, demonstrates accuracy improvements up to 5% on various datasets, with notable results like 56.96% accuracy on SVHN compared to 51.4% for the baseline RFAD method.

## Method Summary
The approach consists of two key innovations: (1) Provable initialization using coresets that identify important and remove redundant points from the training data, with different coresets for NTK-based (Gaussian kernel) and non-NTK-based (K-means) distillation methods, and (2) Smart sampling during training that selects batches based on example importance with respect to the loss function rather than random selection. The method is implemented through two algorithms - SMART-INITIALIZATION for coreset construction and SMART-PICK for importance-based batch selection. The approach is validated across multiple datasets including MNIST, Fashion-MNIST, SVHN, CIFAR10, and CIFAR100 using ConvNet architectures with three 3x3 convolution layers, 2x2 average pooling, instance normalization, and embedding size 128.

## Key Results
- Achieved 56.96% accuracy on SVHN compared to 51.4% for baseline RFAD method
- Demonstrated up to 5% accuracy improvements across various datasets
- Validated effectiveness on MNIST, Fashion-MNIST, SVHN, CIFAR10, and CIFAR100
- Showed consistent performance gains across different distillation methods (KIP, RFAD, DC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NTK-based initialization uses Gaussian kernel coresets to identify representative data points that preserve the learning behavior of wide neural networks.
- Mechanism: The algorithm approximates the neural tangent kernel with a Gaussian kernel and constructs a coreset by sampling points based on their sensitivity with respect to kernel density estimation.
- Core assumption: As neural network width increases, the NTK converges to a Gaussian process, so approximating this kernel with a coreset preserves the model's learning trajectory.
- Evidence anchors:
  - [abstract]: "we present a provable, sampling-based approach for initializing the distilled set by identifying important and removing redundant points in the data"
  - [section]: "Claim 1 (ANNs represent Gaussian kernels [17]). In the infinite-width limit, artificial neural networks have a Gaussian distribution described by a kernel."
  - [corpus]: Weak - no direct neighbor papers address NTK-coreset initialization specifically.
- Break condition: If the neural network width is not sufficiently large, the NTK may not approximate a Gaussian kernel well, breaking the coreset guarantees.

### Mechanism 2
- Claim: NNLMDT methods benefit from K-means coresets for initialization, which better capture data structure than K-center initialization.
- Mechanism: Lightweight K-means coresets are constructed to approximate the K-means clustering objective, providing a more informative initialization than deterministic K-center selection.
- Core assumption: K-means clustering better captures the underlying data distribution than K-center, leading to more effective synthetic dataset initialization.
- Evidence anchors:
  - [section]: "we propose using K-means coresets (specifically lightweight) as an initialization technique for non-NTK-based distillation techniques"
  - [section]: "the K-means problem differs from the K-center problem by the fact that instead of using the max operator over the distance between each point and its closest center, the problem revolves around averaging the distance of points to their closest center"
  - [corpus]: Weak - no neighbor papers directly compare K-means coresets to K-center for distillation initialization.
- Break condition: If the data has very high intrinsic dimensionality or irregular cluster structure, K-means coresets may not provide meaningful initialization.

### Mechanism 3
- Claim: Smart sampling during training improves distillation by focusing on examples the current distilled set poorly represents.
- Mechanism: Importance sampling selects training examples based on their relative loss contribution, forming batches that target the model's weaknesses.
- Core assumption: Examples with high loss relative to the current distilled set are underrepresented and thus most informative for improving the synthetic dataset.
- Evidence anchors:
  - [abstract]: "we further merge the idea of data subset selection with dataset distillation, by training the distilled set on 'important' sampled points during the training procedure instead of randomly sampling the next batch"
  - [section]: "Let ϕ be the loss function used by the dataset distillation technique... For each sample p in the training set P, we assign a probability P(p) := ϕ(p,y(p), ˜P ,˜y)/P(q∈P ϕ(q,y(q), ˜P ,˜y))"
  - [corpus]: Weak - no neighbor papers specifically address importance sampling for dataset distillation.
- Break condition: If the loss function becomes too uniform across examples (e.g., late in training), importance sampling provides little advantage over uniform sampling.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its convergence to Gaussian processes
  - Why needed here: The paper's NTK-based initialization relies on the theoretical foundation that wide neural networks behave like Gaussian processes, enabling coreset construction.
  - Quick check question: What happens to the NTK of a neural network as its width approaches infinity, and why does this matter for dataset distillation?

- Concept: Coreset theory and sensitivity sampling
  - Why needed here: Both initialization methods use coresets to identify representative subsets, and sensitivity sampling is the key algorithmic framework for constructing them.
  - Quick check question: How does the sensitivity of a data point relate to its importance in approximating a loss function with a smaller subset?

- Concept: K-means clustering vs K-center optimization
  - Why needed here: The paper contrasts these two approaches for NNLMDT initialization, arguing K-means coresets better capture data structure.
  - Quick check question: What is the fundamental difference between the optimization objectives of K-means and K-center problems, and how might this affect initialization quality?

## Architecture Onboarding

- Component map: Data preprocessing -> SMART-INITIALIZATION -> Dataset distillation algorithm (KIP, RFAD, DC) -> SMART-PICK -> Evaluation
- Critical path:
  1. Load dataset and preprocess
  2. Run SMART-INITIALIZATION to create initial synthetic dataset
  3. Train distilled dataset using chosen distillation algorithm
  4. At each iteration, use SMART-PICK to sample important batches
  5. Evaluate final distilled dataset by training a model on it

- Design tradeoffs:
  - NTK vs K-means initialization: NTK provides theoretical guarantees but requires kernel approximation; K-means is simpler but may be less theoretically grounded
  - Importance sampling vs uniform sampling: Importance sampling focuses learning but adds computational overhead for loss computation
  - Coreset size vs approximation quality: Larger coresets provide better guarantees but reduce the benefits of dataset distillation

- Failure signatures:
  - Poor initialization quality: Model trained on distilled data performs significantly worse than random sampling baseline
  - Importance sampling collapse: Batch diversity decreases dramatically, leading to poor gradient estimates
  - Computational overhead: Importance sampling and coreset construction add significant time without performance gains

- First 3 experiments:
  1. Compare K-center vs K-means coresets initialization on CIFAR10 with DC method, measuring final test accuracy
  2. Test NTK-based initialization on SVHN with RFAD, comparing against random initialization
  3. Evaluate importance sampling vs uniform sampling on Fashion-MNIST with KIP method, measuring convergence speed and final accuracy

## Open Questions the Paper Calls Out

- Question: How does the choice of coreset size affect the trade-off between computational efficiency and distillation performance?
- Basis in paper: [explicit] The paper discusses the size of the coreset required for different distillation techniques and the impact on performance.
- Why unresolved: The paper provides theoretical guarantees on the coreset size but does not extensively explore the practical trade-offs between computational cost and performance as the coreset size varies.
- What evidence would resolve it: Empirical studies comparing distillation performance and computational time for different coreset sizes on various datasets.

- Question: Can the provable initialization and sampling techniques be extended to other machine learning tasks beyond dataset distillation?
- Basis in paper: [inferred] The paper focuses on dataset distillation but mentions the potential applicability of the techniques to other tasks.
- Why unresolved: The paper does not explore the extension of the techniques to other tasks, leaving open the question of their broader applicability.
- What evidence would resolve it: Experiments demonstrating the effectiveness of the techniques in other machine learning tasks, such as active learning or transfer learning.

- Question: How does the choice of kernel function affect the performance of the NTK-based initialization technique?
- Basis in paper: [explicit] The paper uses Gaussian kernels for NTK-based methods but mentions the potential use of other shift-invariant kernels.
- Why unresolved: The paper does not explore the impact of different kernel choices on the initialization technique's performance.
- What evidence would resolve it: Comparative experiments using different shift-invariant kernels (e.g., Laplacian, Cauchy) in the NTK-based initialization technique.

## Limitations

- The NTK-based initialization relies on theoretical convergence that may not hold in practical, finite-width scenarios
- K-means coresets approach for non-NTK methods lacks extensive validation against simpler baselines
- Importance sampling mechanism may introduce computational overhead that wasn't thoroughly analyzed

## Confidence

- NTK-based initialization: Medium confidence - relies heavily on theoretical NTK convergence to Gaussian processes
- K-means coresets for non-NTK methods: Medium confidence - promising but limited validation
- Importance sampling: High confidence - conceptually sound but potential computational concerns
- Experimental results: High confidence - clear accuracy improvements demonstrated across multiple datasets

## Next Checks

1. Test NTK-based initialization with varying network widths to verify the Gaussian kernel approximation holds in practical settings
2. Compare K-means coresets initialization against simpler baselines (random, K-center) across multiple dataset characteristics
3. Measure the computational overhead of importance sampling across different dataset sizes and determine the break-even point where benefits outweigh costs