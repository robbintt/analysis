---
ver: rpa2
title: On Using Distribution-Based Compositionality Assessment to Evaluate Compositional
  Generalisation in Machine Translation
arxiv_id: '2311.08249'
source_url: https://arxiv.org/abs/2311.08249
tags:
- test
- data
- language
- splits
- generalisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel benchmark for evaluating compositional
  generalization in machine translation. The authors employ a distribution-based compositionality
  assessment framework to partition the Europarl corpus into training and test sets
  with divergent distributions of dependency relations.
---

# On Using Distribution-Based Compositionality Assessment to Evaluate Compositional Generalisation in Machine Translation

## Quick Facts
- arXiv ID: 2311.08249
- Source URL: https://arxiv.org/abs/2311.08249
- Reference count: 13
- Key outcome: Introduces a benchmark for evaluating compositional generalization in machine translation by creating train-test splits with divergent distributions of dependency relations

## Executive Summary
This paper presents a novel benchmark for evaluating compositional generalization in machine translation using a distribution-based compositionality assessment framework. The authors create training and test sets with divergent distributions of dependency relations from the Europarl corpus, allowing them to test whether neural machine translation systems can translate novel syntactic structures they haven't been trained on. Experiments with Transformer models on English-to-German, French, Greek, and Finnish translation tasks demonstrate significant performance drops when translating novel dependency relations, highlighting the importance of assessing compositional generalization in real-world NLP tasks.

## Method Summary
The authors employ a distribution-based compositionality assessment framework to partition the Europarl corpus into training and test sets with divergent distributions of dependency relations. They use a greedy data splitting algorithm to minimize atom divergence (ensuring similar lemma distributions) while maximizing compound divergence (ensuring novel dependency relations in test). Transformer models are trained using OpenNMT-py with standard hyperparameters and BPE tokenization. Performance is evaluated using chrF2++ scores, with a generalization score calculated as the ratio of performance on minimum vs maximum compound divergence splits.

## Key Results
- NMT models show significant performance drops (up to 25% relative) when translating novel dependency relations not seen during training
- The chrF2++ generalization score effectively captures the relative performance deterioration between minDC and maxDC splits
- Results are consistent across multiple target languages (German, French, Greek, Finnish), demonstrating the benchmark's applicability to different language families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Divergent dependency relation distributions in train and test sets force the model to generalize compositional structures it has not seen before.
- Mechanism: The data splitting algorithm minimizes atom divergence while maximizing compound divergence, creating a test set with unseen syntactic combinations while maintaining the same vocabulary.
- Core assumption: The system's compositional generalization capacity can be measured by its performance drop when encountering novel dependency relations.
- Evidence anchors:
  - [abstract] "Specifically, the training and test sets have divergent distributions of dependency relations, testing NMT systems' capability of translating dependencies that they have not been trained on."
  - [section] "The core idea of DBCA is to control the distributions of atoms and compounds to get approximately the same atom distributions but divergent compound distributions in the training and test sets."
- Break condition: If the system relies heavily on memorization of specific phrase patterns rather than understanding dependency relations, it may fail to generalize even with similar atom distributions.

### Mechanism 2
- Claim: By excluding the most frequent and infrequent lemmas from divergence calculations, the benchmark focuses on compositional generalization in the mid-frequency range where most dependency relations occur.
- Mechanism: The filtering of lemmas (excluding the 200 most frequent and those appearing fewer than 10 times) creates a more manageable atom set while maintaining realistic natural language characteristics.
- Core assumption: The mid-frequency range of lemmas contains sufficient complexity to test compositional generalization without the noise of extremely rare or common words.
- Evidence anchors:
  - [section] "To make the number of atoms manageable, we exclude from the distribution calculations lemmas that appear either very frequently... or fewer than 10 times in total in the corpus."
  - [section] "A downside of this choice is that the test sentences in the maxDC splits don't contain exclusively novel dependencies."
- Break condition: If the excluded extreme frequency ranges contain critical dependency relations, the benchmark may miss important generalization failures.

### Mechanism 3
- Claim: The chrF2++ score ratio between minDC and maxDC splits provides a meaningful generalization score that normalizes for overall translation quality.
- Mechanism: By comparing performance on the most compositionally similar split (minDC) to the most divergent split (maxDC), the ratio captures how much compositional generalization ability degrades the model's performance.
- Core assumption: The relative performance drop between these two conditions is a valid measure of compositional generalization capacity.
- Evidence anchors:
  - [section] "To get a sense of the generalisation capacity as a part of the system's translation capacity in general, it may be more meaningful to assess how the performance deteriorates between the minDC and maxDC splits."
  - [section] "Since this is a relative score, the absolute chrF2++ results should be reported in addition to this generalisation score."
- Break condition: If the model's performance on minDC is artificially low due to other factors, the generalization score may be misleading.

## Foundational Learning

- Concept: Distribution-based compositionality assessment (DBCA)
  - Why needed here: This framework provides the theoretical foundation for creating train-test splits that test compositional generalization rather than just memorization.
  - Quick check question: What is the key difference between atom divergence and compound divergence in the DBCA framework?

- Concept: Dependency parsing and syntactic structures
  - Why needed here: Understanding how dependency relations work is essential for interpreting what the benchmark actually tests and how to design meaningful experiments.
  - Quick check question: How does a dependency relation differ from a phrase structure in representing syntactic relationships?

- Concept: Natural language Zipfian distribution
  - Why needed here: Understanding the long tail of word frequencies helps explain why the benchmark's approach to handling frequency distributions is significant and what trade-offs it makes.
  - Quick check question: Why might neural models struggle with handling the long tail of natural language distributions?

## Architecture Onboarding

- Component map: Data preparation pipeline (dependency parsing, filtering, splitting) -> Training pipeline (Transformer NMT with BPE tokenization) -> Evaluation pipeline (chrF2++ scoring and generalization ratio calculation)
- Critical path: Data splitting → Model training → Evaluation → Generalization score calculation
- Design tradeoffs: Using the DBCA framework trades some natural language realism for better control over the compositional generalization test, while excluding extreme frequency words trades comprehensive coverage for computational feasibility.
- Failure signatures: Large performance drops on maxDC splits indicate poor compositional generalization; inconsistent results across random seeds suggest sensitivity to initialization or data partitioning; performance correlated with sentence length may indicate confounding factors.
- First 3 experiments:
  1. Train a baseline model on a random split and measure chrF2++ on both random and maxDC splits to establish baseline generalization capability
  2. Train models on minDC splits and compare their generalization scores to those from random splits to validate the minDC/maxDC comparison
  3. Test models on different target languages to verify that the compositional generalization challenge is consistent across language families

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the data partitioning method be adapted to work with pretrained language models that cannot be retrained on new data?
- Basis in paper: [explicit] The paper notes that the method requires controlling the training data and test data, which is not directly applicable to a pretrain-finetune training scheme with a fixed pretraining dataset.
- Why unresolved: The paper acknowledges this limitation but does not propose a concrete solution for adapting the method to pretrained models.
- What evidence would resolve it: A demonstration of how to create novel test sets with specific divergence values for pretrained models without modifying the training corpus.

### Open Question 2
- Question: Does the benchmark actually test what it assumes to test, i.e., compositional generalization ability?
- Basis in paper: [explicit] The paper states that they have not conclusively assessed whether the benchmark actually tests compositional generalization ability and discusses potential confounding variables.
- Why unresolved: The authors did not find large differences in sentence lengths between the test sets, but other potential confounding variables may exist.
- What evidence would resolve it: Additional experiments to rule out other potential confounding variables and confirm that the benchmark is testing compositional generalization ability.

### Open Question 3
- Question: How does the choice of leaving out the most frequent and infrequent lemmas affect the results of the benchmark?
- Basis in paper: [explicit] The paper notes that they left out some lemmas from the divergence calculations due to memory requirements, but have not assessed how this choice affects the results.
- Why unresolved: The authors acknowledge this limitation but have not conducted experiments to determine the impact of this choice.
- What evidence would resolve it: Experiments comparing the results of the benchmark with and without the exclusion of the most frequent and infrequent lemmas.

## Limitations

- The data splitting algorithm's implementation details are not fully specified, making exact replication challenging
- The benchmark uses only one type of syntactic representation (dependency relations) and one NMT architecture (Transformer)
- The use of English as source language may limit generalizability to other source languages with different syntactic properties

## Confidence

- NMT models struggle with novel dependency relations: High confidence
- DBCA framework effectiveness for creating meaningful benchmarks: Medium confidence
- chrF2++ generalization score as valid measure: Medium confidence

## Next Checks

1. Implement the greedy data splitting algorithm with exact weight thresholds and divergence calculation details to verify reproducibility
2. Test the benchmark with additional NMT architectures (e.g., RNN-based models) to assess generalizability
3. Conduct ablation studies on the lemma filtering criteria to determine the impact of excluding extreme frequency words on benchmark results