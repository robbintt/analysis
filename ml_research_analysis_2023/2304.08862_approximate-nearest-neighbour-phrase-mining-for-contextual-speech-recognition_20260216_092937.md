---
ver: rpa2
title: Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition
arxiv_id: '2304.08862'
source_url: https://arxiv.org/abs/2304.08862
tags:
- phrases
- catt
- context
- contextual
- ann-p
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for mining hard negative phrases for
  contextual speech recognition by using approximate nearest neighbour (ANN) search
  on the latent space of the context encoder. During training, given a reference query,
  the method mines similar phrases using ANN search and uses these as negative examples
  alongside random and ground truth contextual information.
---

# Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition

## Quick Facts
- **arXiv ID:** 2304.08862
- **Source URL:** https://arxiv.org/abs/2304.08862
- **Reference count:** 0
- **Primary result:** ANN-P mining improves contextual ASR accuracy by 7% relative WER reduction

## Executive Summary
This paper introduces Approximate Nearest Neighbour Phrase Mining (ANN-P) for contextual speech recognition, addressing the challenge of disambiguating between phonetically similar biasing phrases. The method mines hard negative examples from the context encoder's latent space during training, encouraging the model to distinguish between similar phrases. Experiments show up to 7% relative word error rate reductions on contextual test data, with particular benefits for streaming applications where limited audio context makes disambiguation more difficult.

## Method Summary
The approach extends Context-Aware Transformer Transducer (CATT) models by mining approximate nearest neighbour phrases during training. For each reference query, the method encodes biasing phrases into latent representations using the context encoder, builds an ANN index, and mines similar phrases as hard negatives. These ANN-P are included alongside random and ground truth phrases during training, forcing the model to learn discriminative representations. The method is efficient as it uses the existing context encoder rather than requiring external ASR models, and shows particular benefits for streaming scenarios with limited audio context.

## Key Results
- Up to 7% relative WER reduction on contextual portion of test data
- Streaming CATT models benefit more than non-streaming models
- Method improves disambiguation between phonetically similar biasing phrases
- Computational efficient compared to alternatives requiring full decodes

## Why This Works (Mechanism)

### Mechanism 1
Mining ANN-P improves disambiguation between phonetically similar phrases by forcing the model to distinguish between close latent representations. The context encoder's embeddings capture phonetic similarity, making mined phrases effective hard negatives.

### Mechanism 2
Streaming models benefit more due to limited audio context (240ms). The restricted window makes it harder to match audio to correct phrases, so explicit hard negative training is more valuable.

### Mechanism 3
The method is computationally efficient by using existing context encoder embeddings and ANN indexing rather than external ASR models or full data decodes.

## Foundational Learning

- **Transformer Transducer (TT) architecture**: CATT extends TT, so understanding base architecture is crucial. Quick check: What are the three main components of a TT model?
- **Cross-attention in contextual biasing**: CATT uses cross-attention to integrate biasing phrases. Quick check: How does multi-head attention measure similarity between context phrases and audio?
- **Approximate Nearest Neighbor (ANN) search**: ANN-P mining relies on efficient similarity search. Quick check: What similarity metric is used and why is it appropriate?

## Architecture Onboarding

- **Component map**: Audio → Audio Encoder → Label Encoder → Joint Network → Output; Context phrases processed in parallel via Context Encoder and integrated through Biasing Cross-Attention
- **Critical path**: Speech signal processed through conformer layers, label predictions through transformer layers, combined in joint network for output
- **Design tradeoffs**: Single embedding per phrase vs. per-subword, caching all phrases vs. on-demand mining, sampling per word vs. per phrase
- **Failure signatures**: No streaming improvement suggests context window limitation; degradation on generic queries suggests over-biasing; high variance suggests poor phonetic capture
- **First 3 experiments**: 1) Compare streaming vs non-streaming with/without ANN-P on small dataset; 2) Vary n and k sampling parameters; 3) Test different similarity metrics (dot vs cosine)

## Open Questions the Paper Calls Out

### Open Question 1
How does ANN-P perform with different context encoder embedding spaces (key/value projections vs output)? Only output representation is experimentally evaluated.

### Open Question 2
What impact does ANN index rebuilding frequency have on performance? Only every-two-epoch schedule is reported.

### Open Question 3
How does performance scale with larger biasing inventories? Only fixed-size inventory is evaluated.

## Limitations

- Key hyperparameters (ANN index configuration, sampling parameters, model details) are underspecified
- Limited evaluation to single large-scale dataset; generalization to other domains/languages untested
- Computational efficiency claims lack quantitative benchmarks against alternatives

## Confidence

- **High confidence**: Core mechanism of ANN-P mining is well-explained and theoretically sound
- **Medium confidence**: 7% WERR improvements are promising but evaluation details are incomplete
- **Low confidence**: Efficiency claims lack runtime comparisons to alternative methods

## Next Checks

1. Ablation study varying n and k sampling parameters to find optimal hyperparameters
2. Cross-dataset evaluation on public corpus to assess generalization
3. Runtime overhead measurement comparing ANN-P mining to alternative hard negative generation methods