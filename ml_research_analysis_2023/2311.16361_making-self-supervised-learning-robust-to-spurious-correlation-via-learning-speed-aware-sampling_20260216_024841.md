---
ver: rpa2
title: Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed
  Aware Sampling
arxiv_id: '2311.16361'
source_url: https://arxiv.org/abs/2311.16361
tags:
- learning
- spurious
- la-ssl
- training
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robustness to spurious correlations
  in self-supervised learning (SSL) representations. The authors observe that SSL
  tends to learn faster from examples that align with spurious correlations, leading
  to biased representations that fail to capture important attributes for downstream
  tasks.
---

# Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling

## Quick Facts
- **arXiv ID:** 2311.16361
- **Source URL:** https://arxiv.org/abs/2311.16361
- **Reference count:** 40
- **Primary result:** LA-SSL improves SSL robustness to spurious correlations by dynamically upsampling examples with slower learning speeds, achieving 48.02% accuracy on corrupted CIFAR-10 with 95% correlation vs 44.08% for standard SimCLR.

## Executive Summary
This paper addresses the challenge of spurious correlations in self-supervised learning (SSL) representations, where models tend to learn faster from examples that align with spurious attributes while neglecting other important features. The authors propose Learning-speed Aware Self-supervised Learning (LA-SSL), a method that dynamically adjusts sampling probabilities based on the learning speed of each example, measured as the similarity between augmentations of the same example. By upsampling examples with slower learning speeds (typically those conflicting with spurious correlations), LA-SSL forces the model to learn more discriminative features from underrepresented examples. Experiments on corrupted CIFAR-10, CelebA, and MIMIC-CXR datasets show consistent improvements in downstream classification tasks compared to standard SSL methods.

## Method Summary
LA-SSL dynamically adjusts sampling probabilities during SSL pretraining based on each example's learning speed, which is measured as the similarity between two augmentations of the same example. Examples that conflict with spurious correlations exhibit slower learning speeds and are thus upsampled more frequently. This approach forces the model to learn more discriminative features from underrepresented examples, improving robustness to spurious correlations. The method is compatible with various SSL frameworks including SimCLR, BarlowTwins, SimSiam, DINO, and MAE, and requires tuning of hyperparameters r and γ in the scaling function that converts similarity scores to sampling weights.

## Key Results
- LA-SSL achieves 48.02% accuracy on corrupted CIFAR-10 with 95% correlation vs 44.08% for standard SimCLR
- Consistent improvements across three diverse datasets (CIFAR-10, CelebA, MIMIC-CXR) with varying spurious correlation levels
- Generalizes to multiple SSL frameworks including BarlowTwins, SimSiam, DINO, and MAE
- Effectively improves subgroup performance in MIMIC-CXR, particularly for older patients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SSL training loss can be minimized by learning only a subset of conspicuous features that align with spurious correlations, neglecting other important predictive features.
- **Mechanism:** The feature suppression phenomenon in SSL causes the model to prioritize learning easy-to-learn features (those aligned with spurious correlations) while suppressing features related to other attributes. This creates bias in the learned representation.
- **Core assumption:** The training data distribution is imbalanced with respect to spurious attributes, making correlation-aligned examples dominate the optimization process.
- **Evidence anchors:**
  - [abstract]: "SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features"
  - [section]: "Feature suppression is a common phenomenon in SSL, where the model prioritizes learning easy-to-learn features as shortcuts, while neglecting features related to other attributes"
- **Break condition:** If the data distribution becomes balanced across spurious attributes, or if the spurious features are not easier to learn than genuine predictive features, this mechanism would break down.

### Mechanism 2
- **Claim:** Samples that conflict with spurious correlations are learned more slowly in SSL compared to samples that align with them.
- **Mechanism:** The learning speed difference is measured as the similarity between two augmentations of the same example. Examples conflicting with spurious correlations show slower learning speeds, which LA-SSL exploits by upsampling them.
- **Core assumption:** The learning speed (measured via augmentation similarity) reliably proxies for whether a sample aligns with or conflicts against spurious correlations.
- **Evidence anchors:**
  - [abstract]: "we observe that the learning is slower for samples that conflict with such correlations"
  - [section]: "We observe a similar phenomenon in SSL pretraining even in the absence of labeled supervision... s(x) = sim(xaug 1, xaug 2)"
  - [corpus]: Weak - The corpus neighbors don't directly address learning speed measurement in SSL
- **Break condition:** If the augmentation similarity doesn't correlate with learning progress, or if the learning dynamics change significantly across different SSL frameworks.

### Mechanism 3
- **Claim:** Dynamic sampling based on learning speed improves robustness by forcing the model to learn more discriminative features from underrepresented examples.
- **Mechanism:** LA-SSL dynamically adjusts sampling probabilities inversely related to learning speed, effectively upsampling examples that conflict with spurious correlations. This balances the learning dynamics between aligned and conflicting examples.
- **Core assumption:** By increasing the representation of slow-learning examples in the training data, the model will learn more diverse and robust features.
- **Evidence anchors:**
  - [abstract]: "we sample each training data with a probability that is inversely related to its learning speed"
  - [section]: "LA-SSL incorporates conditional sampling based on learning speed, assigning each example a probability that is inversely related to its learning speed"
  - [corpus]: Weak - No direct evidence in corpus about dynamic sampling based on learning speed
- **Break condition:** If the learning speed proxy becomes unreliable over training epochs, or if the dynamic sampling creates instability in the optimization process.

## Foundational Learning

- **Concept:** Self-supervised learning and contrastive learning frameworks (SimCLR, BarlowTwins, SimSiam, DINO)
  - **Why needed here:** The paper builds upon these SSL frameworks and extends them with learning-speed aware sampling. Understanding their loss functions and training dynamics is essential.
  - **Quick check question:** How does the InfoNCE loss in SimCLR encourage similar representations for augmented views of the same image?

- **Concept:** Spurious correlation and its impact on generalization
  - **Why needed here:** The paper addresses the problem of SSL robustness to spurious correlations, where certain attributes are correlated with labels due to imbalanced data distribution rather than causal relationships.
  - **Quick check question:** Why do neural networks tend to pick up spurious correlations as shortcuts during training?

- **Concept:** Feature suppression and shortcut learning in neural networks
  - **Why needed here:** The paper observes that SSL exhibits feature suppression, where models prioritize learning easy-to-learn features aligned with spurious correlations while neglecting other important features.
  - **Quick check question:** How does feature suppression manifest in the learned representations, and why is it problematic for downstream tasks?

## Architecture Onboarding

- **Component map:** Input augmentation -> Feature extractor -> Learning speed monitor -> Sampling probability calculator -> SSL loss function -> Parameter update
- **Critical path:** Input augmentation → Feature extraction → Learning speed computation → Sampling probability update → Loss computation → Parameter update
- **Design tradeoffs:**
  - Tradeoff between exploration (learning diverse features) and exploitation (optimizing current features)
  - Sensitivity of learning speed estimation to augmentation randomness vs. stability across epochs
  - Computational overhead of dynamic sampling vs. improved robustness
- **Failure signatures:**
  - If learning speed estimates become noisy or unstable, sampling probabilities may fluctuate excessively
  - If the scaling function h is too aggressive, it may oversample slow-learning examples and destabilize training
  - If the warmup period is too short, the model may not have learned sufficient features before dynamic sampling begins
- **First 3 experiments:**
  1. Implement learning speed monitoring on a simple SimCLR setup with corrupted CIFAR-10 and visualize learning speed distributions across aligned vs. conflicting examples
  2. Test different scaling functions h (varying γ and r) on a small dataset to observe their impact on sampling distributions and downstream performance
  3. Compare LA-SSL with uniform sampling on MIMIC-CXR, measuring improvements in subgroup performance (young vs. old patients)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the proposed learning-speed aware sampling method (LA-SSL) generalize to other types of biases beyond spurious correlations, such as class imbalance or domain shift?
- **Basis in paper:** [inferred] The paper focuses on addressing spurious correlations, but the method relies on measuring learning speed differences, which could potentially be applicable to other types of biases that affect learning dynamics.
- **Why unresolved:** The paper only evaluates LA-SSL on datasets with spurious correlations and does not explore its effectiveness on other types of biases. The authors acknowledge that the method could potentially be adapted to other scenarios, but this remains untested.
- **What evidence would resolve it:** Experiments applying LA-SSL to datasets with class imbalance or domain shift, and comparing its performance to existing methods for addressing these biases.

### Open Question 2
- **Question:** How does the performance of LA-SSL vary with different choices of the scaling function h and its hyperparameters (r and γ)?
- **Basis in paper:** [explicit] The paper mentions a sensitivity analysis on the hyperparameters r and γ, but the results are only briefly mentioned and not thoroughly discussed. The authors acknowledge that the choice of these hyperparameters can impact the performance of LA-SSL.
- **Why unresolved:** The sensitivity analysis in the paper is limited and does not provide a comprehensive understanding of how the performance of LA-SSL varies with different choices of h and its hyperparameters. The authors do not explore the impact of different scaling functions or alternative hyperparameter tuning strategies.
- **What evidence would resolve it:** A more extensive sensitivity analysis exploring a wider range of hyperparameter values and different scaling functions. Additionally, experiments comparing the performance of LA-SSL with different hyperparameter tuning strategies, such as grid search or Bayesian optimization.

### Open Question 3
- **Question:** How does the proposed method compare to other approaches for addressing spurious correlations in self-supervised learning, such as conditional sampling or adversarial training?
- **Basis in paper:** [inferred] The paper mentions that LA-SSL is inspired by conditional sampling methods, but it does not directly compare its performance to these approaches. The authors also briefly mention adversarial training as a potential alternative, but do not provide a detailed comparison.
- **Why unresolved:** The paper focuses on presenting and evaluating LA-SSL, but does not provide a comprehensive comparison with other existing methods for addressing spurious correlations in self-supervised learning. The authors do not discuss the strengths and weaknesses of LA-SSL relative to these alternative approaches.
- **What evidence would resolve it:** Experiments comparing the performance of LA-SSL to conditional sampling methods and adversarial training on datasets with spurious correlations. Additionally, a discussion of the trade-offs and advantages of each approach in terms of robustness, computational efficiency, and ease of implementation.

## Limitations

- LA-SSL's effectiveness relies heavily on the assumption that learning speed (measured via augmentation similarity) reliably indicates whether an example aligns with or conflicts spurious correlations, with limited empirical validation of this proxy
- The method introduces computational overhead through dynamic sampling and requires careful hyperparameter tuning of r and γ in the scaling function h
- Robustness gains may diminish when spurious correlations are weak or when the data distribution is balanced across attributes

## Confidence

- **High Confidence:** The core observation that SSL learns faster from examples aligned with spurious correlations, and that dynamic sampling can improve robustness. The experimental results on three diverse datasets showing consistent improvements.
- **Medium Confidence:** The mechanism by which learning speed proxies for correlation alignment, as this depends on the specific SSL framework and augmentation strategy used.
- **Medium Confidence:** The generalizability of LA-SSL across different SSL frameworks (SimCLR, BarlowTwins, SimSiam, DINO, MAE), though the paper demonstrates this empirically.

## Next Checks

1. Conduct ablation studies to validate whether learning speed is indeed a reliable proxy for spurious correlation alignment by comparing LA-SSL performance with oracle sampling that uses ground-truth correlation labels.
2. Test LA-SSL on datasets with multiple spurious correlations simultaneously to assess scalability and identify potential conflicts between different correlation structures.
3. Evaluate the sensitivity of LA-SSL to hyperparameter choices (r, γ) through systematic grid searches and analyze how these parameters affect the learning dynamics and final performance.