---
ver: rpa2
title: On the Second-Order Convergence of Biased Policy Gradient Algorithms
arxiv_id: '2311.02546'
source_url: https://arxiv.org/abs/2311.02546
tags:
- gradient
- policy
- lemma
- have
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence of biased policy gradient methods,
  including both the vanilla gradient estimator (GPOMDP) and the actor-critic algorithm,
  to second-order stationary points. The main result shows that with a finite-horizon
  biased gradient estimator, the algorithm can converge to an $\epsilon$-second-order
  stationary point in $\tilde{O}(\epsilon^{-6.5})$ iterations.
---

# On the Second-Order Convergence of Biased Policy Gradient Algorithms

## Quick Facts
- arXiv ID: 2311.02546
- Source URL: https://arxiv.org/abs/2311.02546
- Reference count: 40
- This paper analyzes convergence of biased policy gradient methods to second-order stationary points in $\tilde{O}(\epsilon^{-6.5})$ iterations.

## Executive Summary
This paper provides the first finite-time convergence analysis for biased policy gradient methods in reinforcement learning. The authors show that despite the bias introduced by finite-horizon sampling, the GPOMDP estimator can converge to second-order stationary points in infinite-horizon discounted reward settings. The key insight is that the truncation bias can be bounded and treated as a deterministic perturbation that doesn't prevent saddle point escape. The analysis applies techniques from nonconvex optimization to establish convergence guarantees.

## Method Summary
The paper analyzes biased policy gradient methods using finite-horizon sampling to approximate infinite-horizon gradients. The method involves sampling trajectories of length H, computing the biased GPOMDP gradient estimator, and updating parameters. The analysis constructs coupled sequences comparing biased gradients to unbiased gradients on a Taylor expansion, showing they remain close enough (O(μ)) for convergence guarantees to transfer. The bias is bounded exponentially with trajectory horizon H, allowing it to be controlled while still maintaining sufficient noise for exploration.

## Key Results
- Biased GPOMDP estimator converges to ε-second-order stationary points in $\tilde{O}(\epsilon^{-6.5})$ iterations
- Truncation bias decreases exponentially with trajectory horizon: ||d_{i+1}|| ≤ DHγ^H
- Convergence rate is stronger than previous unbiased analyses (which achieved $\tilde{O}(\epsilon^{-9})$) but weaker than some unbiased methods ($\tilde{O}(\epsilon^{-4.5})$)
- Analysis applies to infinite-horizon discounted reward setting, more practical than finite-horizon cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias can be bounded such that it does not prevent escape from saddle points
- Mechanism: The paper bounds the truncation bias using Lemma 3.4, showing ||d_{i+1}|| ≤ DHγ^H where H = O(log(1/μ)). This bounded bias can be treated as a deterministic perturbation that doesn't prevent the stochastic noise from providing sufficient curvature information for saddle point escape
- Core assumption: The bias decreases exponentially with trajectory horizon H and is therefore controllable by choosing H = O(log(1/μ))
- Break condition: If H cannot be chosen large enough relative to μ (e.g., computational constraints), the bias may dominate the noise needed for saddle point escape

### Mechanism 2
- Claim: Coupled sequence analysis allows transfer of second-order convergence results from unbiased to biased settings
- Mechanism: The proof constructs two sequences - {θ_i} using biased gradients and {θ'_i} using unbiased gradients on a Taylor expansion. By showing ||θ'_i - θ_i|| = O(μ), the second-order convergence properties of {θ'_i} transfer to {θ_i}, enabling escape from saddle points despite bias
- Core assumption: The distance between biased and unbiased sequences remains bounded by O(μ) throughout the optimization process
- Break condition: If the moment bounds fail to control the sequence difference, the transfer argument breaks down

### Mechanism 3
- Claim: Noise covariance structure ensures sufficient exploration in all directions near saddle points
- Mechanism: Assumption 6 requires that the noise covariance projected onto the negative curvature subspace has minimum eigenvalue σ²_l > 0. This guarantees that noise contains components in directions of negative curvature, enabling escape from saddle points
- Core assumption: The gradient estimator noise has sufficient correlation with negative curvature directions
- Break condition: If the noise becomes isotropic or aligned with positive curvature directions, saddle point escape may fail

## Foundational Learning

- Concept: Second-order stationary points and strict saddle property
  - Why needed here: The paper's convergence guarantee is to second-order stationary points, requiring understanding of when ∇J(θ) = 0 and λ_max(∇²J(θ)) ≤ 0
  - Quick check question: What is the difference between a first-order stationary point and a second-order stationary point in nonconvex optimization?

- Concept: Bias-variance decomposition in gradient estimation
  - Why needed here: The analysis separates gradient noise s_i from truncation bias d_i, treating them differently in the convergence proof
  - Quick check question: In the decomposition θ_i+1 = θ_i + μ∇J(θ_i) + μs_{i+1} + μd_{i+1}, which term represents the bias and which represents the variance?

- Concept: Coupling arguments in stochastic optimization
  - Why needed here: The proof constructs a coupled sequence using unbiased gradients on a Taylor expansion and shows it stays close to the biased sequence
  - Quick check question: Why is it useful to compare a biased optimization sequence to an unbiased sequence on a Taylor approximation of the objective?

## Architecture Onboarding

- Component map: Trajectory sampling -> Biased gradient estimation -> Parameter update -> Bias bounding -> Noise covariance analysis -> Coupled sequence analysis
- Critical path: 1) Sample trajectories of length H, 2) Compute biased gradient estimator, 3) Update parameters, 4) Analyze convergence using bias bounds and noise properties
- Design tradeoffs: Longer trajectories reduce bias but increase computational cost; larger step sizes speed convergence but may violate stability conditions
- Failure signatures: Slow convergence despite many iterations suggests either insufficient noise for saddle escape or excessive bias; oscillations indicate step size too large
- First 3 experiments:
  1. Implement the biased GPOMDP gradient estimator with varying horizon H and measure the bias ||∇J(θ) - ∇JH(θ)|| empirically
  2. Test convergence on a simple nonconvex test function with known saddle points, varying the noise level to verify saddle escape
  3. Compare convergence rates with and without the bias correction to quantify the impact of truncation bias on practical performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the second-order convergence rate of the biased policy gradient estimator compare to that of the unbiased estimator in practice, particularly in terms of sample complexity and computational efficiency?
- Basis in paper: The paper mentions that the sample complexity of the biased estimator is $\tilde{O}(\epsilon^{-6.5})$, which is stronger than $\tilde{O}(\epsilon^{-9})$ from [20] and weaker than $\tilde{O}(\epsilon^{-4.5})$ from [18], both of which only analyze unbiased estimators.
- Why unresolved: The paper does not provide a direct comparison of the practical performance of the biased and unbiased estimators, nor does it discuss the implications of the different sample complexities on real-world applications.
- What evidence would resolve it: Empirical studies comparing the performance of biased and unbiased estimators in various RL tasks, measuring both sample complexity and computational efficiency, would provide evidence to answer this question.

### Open Question 2
- Question: What are the specific conditions under which the biased policy gradient estimator outperforms the unbiased estimator, and vice versa?
- Basis in paper: The paper discusses the convergence of the biased policy gradient estimator but does not explicitly compare its performance to the unbiased estimator in different scenarios.
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-offs between biased and unbiased estimators in various RL settings, such as different reward structures, state-action spaces, and initial state distributions.
- What evidence would resolve it: A thorough theoretical and empirical analysis of the performance of biased and unbiased estimators under different RL settings, identifying the conditions under which each estimator is preferable, would resolve this question.

### Open Question 3
- Question: How does the presence of bias in the policy gradient estimator affect the exploration-exploitation trade-off in reinforcement learning algorithms?
- Basis in paper: The paper discusses the biased nature of the policy gradient estimator due to finite-horizon sampling and critic approximation, but does not explore its impact on the exploration-exploitation trade-off.
- Why unresolved: The paper focuses on the convergence properties of the biased estimator but does not investigate how the bias influences the algorithm's ability to explore the state-action space effectively.
- What evidence would resolve it: Studies examining the exploration strategies of biased and unbiased policy gradient algorithms in various RL tasks, measuring the balance between exploration and exploitation, would provide insights into this question.

### Open Question 4
- Question: Can the techniques developed for analyzing the convergence of biased policy gradient methods be extended to other biased gradient-based optimization algorithms in reinforcement learning and beyond?
- Basis in paper: The paper mentions that the approach for bounding the truncation bias and leveraging techniques from nonconvex optimization can be applied to biased gradient estimators, including the double-loop actor-critic algorithm.
- Why unresolved: The paper does not discuss the potential applicability of these techniques to other biased optimization algorithms in RL or other domains, such as federated learning or distributed optimization.
- What evidence would resolve it: Theoretical and empirical studies demonstrating the effectiveness of the developed techniques in analyzing the convergence of various biased gradient-based optimization algorithms in different domains would answer this question.

## Limitations
- The analysis relies on strong assumptions about noise covariance structure that may not hold in practical RL settings
- Coupled sequence argument depends on maintaining bounded distance between biased and unbiased trajectories, which may deteriorate in high-dimensional spaces
- Practical implications for actor-critic algorithms are less certain due to function approximation assumptions

## Confidence

**High Confidence**: The fundamental mechanism of bias decay with trajectory length (O(γ^H)) is well-established in the RL literature. The truncation bias bounds and their exponential decay with H are mathematically rigorous.

**Medium Confidence**: The second-order convergence framework leveraging negative curvature directions is standard in nonconvex optimization. However, the specific application to biased policy gradients introduces coupling effects that require careful verification.

**Low Confidence**: The practical implications for actor-critic algorithms are less certain, as the analysis assumes idealized conditions that may not translate to the function approximation setting common in deep RL.

## Next Checks

1. **Empirical bias verification**: Implement the biased gradient estimator on a known MDP and measure the actual bias ||∇J(θ) - ∇JH(θ)|| across different values of H and discount factors γ to validate the theoretical decay rate.

2. **Noise covariance analysis**: Compute the empirical noise covariance matrix during policy gradient updates and verify whether it satisfies the eigendecomposition condition required by Assumption 6 across different MDPs and policy parameterizations.

3. **Saddle point escape experiments**: Construct test MDPs with explicit saddle points and systematically vary the gradient estimator horizon H and noise level to empirically measure saddle escape rates, comparing biased vs. unbiased estimators.