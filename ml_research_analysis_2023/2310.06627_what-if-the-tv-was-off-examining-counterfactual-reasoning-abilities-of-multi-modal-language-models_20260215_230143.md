---
ver: rpa2
title: What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal
  Language Models
arxiv_id: '2310.06627'
source_url: https://arxiv.org/abs/2310.06627
tags:
- counterfactual
- questions
- reasoning
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C-VQA, a novel dataset designed to evaluate
  the counterfactual reasoning abilities of multimodal language models (MLLMs). The
  dataset consists of over 2k image-question-answer triplets derived from the VQAv2
  dataset, where questions are modified by adding counterfactual presuppositions.
---

# What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models

## Quick Facts
- arXiv ID: 2310.06627
- Source URL: https://arxiv.org/abs/2310.06627
- Reference count: 26
- This paper introduces C-VQA, a novel dataset designed to evaluate the counterfactual reasoning abilities of multimodal language models (MLLMs).

## Executive Summary
This paper introduces C-VQA, a novel dataset designed to evaluate the counterfactual reasoning abilities of multimodal language models (MLLMs). The dataset consists of over 2k image-question-answer triplets derived from the VQAv2 dataset, where questions are modified by adding counterfactual presuppositions. The authors assess several state-of-the-art MLLMs on C-VQA, observing substantial performance drops—up to 40%—compared to the original VQAv2 dataset. This significant decline indicates that current MLLMs struggle with counterfactual reasoning, highlighting a gap in their capabilities compared to human-like vision reasoning. The study underscores the need for further development in MLLMs to improve their ability to handle counterfactual scenarios effectively.

## Method Summary
The paper introduces C-VQA, a dataset of over 2k counterfactual questions derived from VQAv2 by adding counterfactual presuppositions to existing questions. The dataset was created using ChatGPT for initial generation, followed by manual verification at two stages (image-relatedness and answer-reasonability). The authors evaluate several state-of-the-art MLLMs (ViperGPT, VisProg, InstructBLIP, LLaVA, BLIP2) on C-VQA and compare their performance to the original VQAv2 dataset, measuring the performance drop to assess counterfactual reasoning capabilities.

## Key Results
- MLLMs show substantial performance drops (up to 40%) on C-VQA compared to original VQAv2 questions
- Current MLLMs struggle with counterfactual reasoning, failing to properly adjust their representations when hypothetical changes are introduced
- The performance gap is consistent across multiple state-of-the-art MLLMs, indicating a fundamental limitation in their reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The dataset design directly probes the counterfactual reasoning gap by modifying VQA questions with counterfactual presuppositions while keeping image content constant.
- **Mechanism:** By injecting counterfactual presuppositions into existing VQA-v2 questions, the models must reason about hypothetical changes to the scene (e.g., "if the TV was off") rather than just grounding concepts. This creates a controlled test where failure indicates lack of counterfactual reasoning rather than visual understanding.
- **Core assumption:** Current MLLMs rely primarily on visual grounding and pattern matching rather than true causal or counterfactual reasoning.
- **Evidence anchors:**
  - [abstract] "We evaluate recent vision language models on our newly collected test dataset and found that all models exhibit a large performance drop compared to the results tested on questions without the counterfactual presupposition."
  - [section] "When counterfactual presuppositions are added, ViperGPT gives wrong answers to the questions that it can answer correctly without the counterfactual parts."
- **Break condition:** If models can achieve similar performance on counterfactual questions without additional training, it would suggest the gap is due to dataset artifacts rather than fundamental reasoning limitations.

### Mechanism 2
- **Claim:** The manual verification process ensures high-quality counterfactual questions that truly test reasoning rather than pattern matching.
- **Mechanism:** Human verification at two stages (image-relatedness and answer reasonableness) filters out questions that ChatGPT generates which may be grammatically correct but logically disconnected from the image context or have ambiguous answers.
- **Core assumption:** Automated generation without verification would produce many questions that don't genuinely test counterfactual reasoning.
- **Evidence anchors:**
  - [section] "We further verify the questions and answers manually to address the errors caused by ChatGPT. Our verification mainly consists of two stages: (i) image-related verification and (ii) answer-reasonability verification."
  - [section] "Each image is at least verified by two people to ensure the correctness."
- **Break condition:** If manual verification is relaxed and dataset performance drops significantly, it would validate this mechanism's importance.

### Mechanism 3
- **Claim:** The dataset's construction methodology (starting from VQA-v2 and adding counterfactual presuppositions) provides a fair comparison baseline.
- **Mechanism:** By using the same images and only modifying questions/answers, the dataset isolates the counterfactual reasoning component from visual understanding capabilities, allowing researchers to measure the specific impact of counterfactual reasoning.
- **Core assumption:** The original VQA-v2 questions are solvable by current models, so any performance drop on C-VQA directly measures counterfactual reasoning ability.
- **Evidence anchors:**
  - [section] "We remove examples in which ViperGPT fails to give a correct answer to the original question in VQAv2. The rationale is that, by removing those examples, we keep only the examples that can be correctly processed by both the visual perception module and the reasoning module of ViperGPT."
  - [section] "ViperGPT [6] is used to further filter the dataset. We remove examples in which ViperGPT fails to give a correct answer to the original question in VQAv2."
- **Break condition:** If models that perform well on VQA-v2 show similar performance on C-VQA, it would suggest the counterfactual modification doesn't actually increase difficulty.

## Foundational Learning

- **Concept:** Counterfactual reasoning and causal inference
  - **Why needed here:** Understanding the difference between factual observation and hypothetical reasoning is central to interpreting why models fail on C-VQA
  - **Quick check question:** What is the key difference between answering "How many cats are in the image?" versus "How many cats would be in the image if the TV was off?"

- **Concept:** Visual grounding and concept detection
  - **Why needed here:** The dataset builds on VQA, so understanding how models map language to visual concepts is essential for appreciating what the counterfactual modification adds
  - **Quick check question:** How does a model typically identify "cats" in an image, and why would this approach fail when asked about "cats if the TV was off"?

- **Concept:** Dataset construction and evaluation methodology
  - **Why needed here:** The paper's contribution relies on careful dataset construction; understanding annotation, verification, and filtering processes is crucial for evaluating the results
  - **Quick check question:** Why did the authors filter out examples that ViperGPT couldn't answer correctly in the original VQA-v2 dataset?

## Architecture Onboarding

- **Component map:** VQA-v2 question filtering → ChatGPT counterfactual generation → manual verification → dataset assembly → model evaluation → performance analysis
- **Critical path:** Image → original VQA question → counterfactual presupposition addition → new question/answer generation → manual verification → model evaluation → performance analysis
- **Design tradeoffs:** 
  - Using ChatGPT for generation trades off potential quality issues for scalability, addressed by manual verification
  - Manual verification ensures quality but limits dataset size compared to fully automated approaches
  - Focusing on numerical and boolean questions simplifies evaluation but may miss other reasoning types
- **Failure signatures:** 
  - Large performance drops (up to 40%) on counterfactual questions vs original questions
  - Generated code that ignores counterfactual presuppositions or includes redundant reasoning steps
  - Incorrect arithmetic calculations in generated answers
- **First 3 experiments:**
  1. Evaluate a simple baseline model (e.g., CLIP-based VQA) on both original VQA-v2 and C-VQA to establish baseline performance drops
  2. Test the same model with and without access to the counterfactual presupposition text to determine if the drop is due to reasoning or simply missing information
  3. Create a synthetic dataset where counterfactual presuppositions are trivially true/false to test if models can handle any counterfactual reasoning at all

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The C-VQA dataset is relatively small at 2,217 examples compared to the original VQAv2 dataset, raising questions about generalizability
- Manual verification introduces potential human bias and limits scalability
- The study focuses specifically on numerical and boolean questions, which may not capture the full spectrum of counterfactual reasoning capabilities

## Confidence
- **High Confidence:** The methodology for creating counterfactual questions and the observed performance drops (up to 40%) are well-supported by the data presented
- **Medium Confidence:** The claim that current MLLMs fundamentally lack counterfactual reasoning capabilities is supported but could benefit from additional ablation studies
- **Low Confidence:** The generalizability of findings to broader counterfactual reasoning scenarios beyond the specific types tested in C-VQA

## Next Checks
1. **Scale-up Validation:** Expand the C-VQA dataset to include more diverse question types (e.g., "why" and "how" questions) and increase the total number of examples to better assess model performance across a wider range of counterfactual scenarios.

2. **Cross-dataset Consistency:** Test whether models that perform well on C-VQA also show improved performance on other counterfactual reasoning benchmarks, ensuring that improvements on C-VQA translate to broader counterfactual reasoning capabilities.

3. **Ablation Studies:** Conduct experiments to isolate whether the performance drops are due to counterfactual reasoning limitations or other factors such as language understanding, by testing models on variations of the questions that maintain the counterfactual structure but simplify the reasoning required.