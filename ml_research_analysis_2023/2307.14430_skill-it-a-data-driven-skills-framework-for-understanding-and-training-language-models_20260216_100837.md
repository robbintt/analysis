---
ver: rpa2
title: Skill-it! A Data-Driven Skills Framework for Understanding and Training Language
  Models
arxiv_id: '2307.14430'
source_url: https://arxiv.org/abs/2307.14430
tags:
- skill
- skills
- data
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a skills-based framework for understanding
  and training language models, motivated by the idea that LMs learn skills in a particular
  order, similar to humans. It defines skills and ordered skill sets, showing their
  existence in synthetic and real data, and demonstrates that training on prerequisite
  skills enables more efficient learning of advanced skills.
---

# Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models

## Quick Facts
- **arXiv ID:** 2307.14430
- **Source URL:** https://arxiv.org/abs/2307.14430
- **Authors:** Not specified
- **Reference count:** 40
- **Primary result:** SKILL-IT achieves higher accuracy with 1B tokens than uniform sampling with 3B tokens on RedPajama dataset

## Executive Summary
This paper introduces SKILL-IT, a data-driven skills framework for understanding and training language models. The framework is motivated by the observation that language models learn skills in a particular order, similar to humans. By defining skills as units of behavior with prerequisite relationships, SKILL-IT exploits skill ordering to minimize validation loss through an online data selection algorithm. The approach demonstrates significant improvements across continual pre-training, fine-tuning, and out-of-domain settings, achieving up to 36.5-point accuracy gains on synthetic tasks and 13.6% loss reduction on fine-tuning tasks.

## Method Summary
SKILL-IT operates by first identifying skills within datasets and constructing a directed acyclic graph representing prerequisite relationships between skills. The framework uses loss trajectories during training to cluster data points into skills, finding that loss-based clustering outperforms embedding-based approaches. An online mirror descent optimization algorithm dynamically adjusts sampling weights based on validation losses, prioritizing prerequisite skills and skills that are not yet learned. The method is evaluated across synthetic datasets (LEGO, Addition), multi-task NLP datasets (Natural Instructions), and large-scale pre-training datasets (RedPajama), showing consistent improvements over random, curriculum, and skill-stratified baselines.

## Key Results
- SKILL-IT outperforms random sampling with 36.5-point accuracy gain on LEGO synthetic dataset
- Achieves 13.6% loss reduction on Spanish QG fine-tuning task
- Lowest loss on 11 of 12 Natural Instructions test tasks in out-of-domain settings
- Reaches higher accuracy with 1B tokens than uniform sampling with 3B tokens on RedPajama

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on prerequisite skills reduces the amount of data needed to learn advanced skills
- Mechanism: The framework defines skills as units of behavior with associated data. When skills are ordered in a directed graph where edges represent reduced learning cost, training on prerequisite skills enables more efficient learning of downstream skills
- Core assumption: There exists a directed acyclic graph of skills where prerequisite skills reduce the data required to learn advanced skills
- Evidence anchors:
  - [abstract] "demonstrating that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills"
  - [section 2.2] "we find that in 35.9% fewer training steps, training on a balanced mixture of Xs1, Xs2, and Xs3 resulted in the same validation loss of 0.01 as training solely on Xs3"
  - [corpus] Weak evidence - only mentions general skill-based approaches in robotics and RL, not LM skill ordering
- Break condition: If the skills graph is complete or empty, the ordering advantage disappears and random sampling becomes optimal

### Mechanism 2
- Claim: SKILL-IT dynamically allocates training weight to skills based on their current learning status
- Mechanism: Online mirror descent optimization adjusts sampling weights based on current validation losses, prioritizing prerequisite skills and skills that are not yet learned
- Core assumption: Validation loss trajectories provide reliable signals about skill learning progress
- Evidence anchors:
  - [section 3.3.2] "we adjust the weight on skill i based on the losses of skills that i influences, with the assumption that more training data helps decrease loss"
  - [section 4] "SKILL-IT outperforms skill-stratified sampling by initially allocating more weight to prerequisite skills and eventually allocating more weight to skills that are learned more slowly"
  - [corpus] No direct evidence - the corpus discusses skill libraries in RL but not online weight allocation
- Break condition: If skill learning rates are unpredictable or validation losses plateau without meaningful progress

### Mechanism 3
- Claim: Embedding-based clustering fails to recover skills while loss-based clustering succeeds
- Mechanism: Loss trajectories during training capture the underlying skill structure better than semantic embeddings
- Core assumption: Points from the same skill exhibit similar loss trajectories during training
- Evidence anchors:
  - [section 2.3] "we find that taking 10 random training runs and clustering data by their loss per timestep per run recovers the skills with 61% accuracy"
  - [section 2.3] "embedding-based approaches do not adequately recover synthetic skills" with pretraining embeddings achieving only 24-38% accuracy
  - [corpus] No evidence - corpus neighbors discuss skill discovery in RL but not LM clustering methods
- Break condition: If loss trajectories are too noisy or if skills share similar learning patterns that cannot be distinguished by loss alone

## Foundational Learning

- Concept: Directed acyclic graphs (DAGs) for representing skill dependencies
  - Why needed here: The skills framework relies on representing prerequisite relationships between skills using a directed graph structure
  - Quick check question: Can you explain why the skills graph must be neither complete nor empty to be useful for data selection?

- Concept: Online optimization and mirror descent
  - Why needed here: SKILL-IT uses online mirror descent to dynamically adjust sampling weights based on validation losses
  - Quick check question: How does the exponential weight update rule in SKILL-IT relate to the standard multiplicative weights update algorithm?

- Concept: Curriculum learning and pacing functions
  - Why needed here: The framework builds on curriculum learning concepts but extends them to the skill level rather than sample level
  - Quick check question: What's the key difference between skill-stratified sampling and traditional curriculum learning?

## Architecture Onboarding

- Component map: Data preprocessing -> Skill identification and graph construction -> Training loop with online weight adjustment -> Validation loss tracking -> Evaluation
- Critical path: Graph learning → Initial weight calculation → Training rounds with weight updates → Evaluation
- Design tradeoffs:
  - Graph learning cost vs. data selection benefit
  - Static vs. dynamic sampling (skill-stratified vs. SKILL-IT)
  - Number of training rounds vs. convergence quality
  - Batch size vs. weight update frequency
- Failure signatures:
  - Uniform weights throughout training (graph not being utilized)
  - Rapid weight convergence to extremes (overfitting to early observations)
  - Weights oscillating without convergence (learning rate too high)
  - No improvement over random sampling (graph construction failure)
- First 3 experiments:
  1. Implement SKILL-IT with a synthetic dataset where the skill graph is known (like LEGO with k=3)
  2. Compare static skill-stratified sampling vs. SKILL-IT on a small subset of Natural Instructions
  3. Test ablation: SKILL-IT with identity matrix (no graph) vs. learned graph on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the skills graph affect the performance of SKILL-IT?
- Basis in paper: [explicit] The paper discusses two approaches for learning the skills graph: brute-force and linear approximation.
- Why unresolved: The paper does not provide a comprehensive analysis of how the choice of the skills graph impacts the performance of SKILL-IT.
- What evidence would resolve it: A thorough evaluation of SKILL-IT using different skills graphs, including those learned using the brute-force and linear approximation approaches, would provide insights into the impact of the skills graph choice.

### Open Question 2
- Question: Can the skills graph learned on a smaller model be effectively used for data selection with a larger model?
- Basis in paper: [explicit] The paper mentions that the skills graph can be learned on a smaller model and used for data selection with a larger model.
- Why unresolved: The paper does not provide a detailed analysis of the effectiveness of transferring the skills graph from a smaller model to a larger model.
- What evidence would resolve it: A comprehensive evaluation of SKILL-IT using skills graphs learned on smaller models and applied to larger models would provide insights into the effectiveness of this approach.

### Open Question 3
- Question: How does the choice of the Bregman divergence term affect the performance of SKILL-IT?
- Basis in paper: [explicit] The paper mentions that the Bregman divergence term is used in the update rule of SKILL-IT.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of the Bregman divergence term impacts the performance of SKILL-IT.
- What evidence would resolve it: A comprehensive evaluation of SKILL-IT using different Bregman divergence terms would provide insights into the impact of this choice.

## Limitations
- Graph learning scalability: The brute-force algorithm has exponential complexity, making it impractical for real-world datasets with many skills
- Dataset specificity: All experiments use synthetic or curated datasets where skill boundaries are relatively clear
- Model size constraints: Experiments are conducted on relatively small models (125M to 1.3B parameters)

## Confidence

**High Confidence** (supported by direct experimental evidence):
- SKILL-IT outperforms random sampling in controlled synthetic experiments (LEGO, Addition datasets)
- Loss-based clustering successfully recovers synthetic skills while embedding-based methods fail
- Skill-stratified sampling is superior to random sampling for fine-tuning scenarios

**Medium Confidence** (supported by experimental evidence but with limitations):
- SKILL-IT achieves 36.5-point accuracy gain on LEGO (limited to synthetic data)
- SKILL-IT shows 13.6% loss reduction on Spanish QG (single fine-tuning task)
- SKILL-IT achieves lowest loss on 11 of 12 NI test tasks (aggregate results, not per-skill)

**Low Confidence** (largely based on aggregated metrics or indirect evidence):
- SKILL-IT outperforms random sampling with 3B tokens using only 1B tokens on RedPajama (aggregate LM evaluation, not per-skill analysis)
- The framework generalizes to real-world datasets without extensive validation across diverse domains

## Next Checks

1. **Graph Learning Validation**: Implement Algorithm 3 on a real-world dataset (e.g., Pile of Law) and validate the learned skill graph by measuring whether training on prerequisite skills consistently reduces the data required to learn advanced skills. Compare brute-force (Algorithm 2) vs. approximate (Algorithm 3) results on a small subset.

2. **Model Scaling Experiment**: Test SKILL-IT on a 7B or 13B parameter model using a subset of RedPajama or a similar large-scale dataset. Measure whether the accuracy gains and token efficiency improvements observed with smaller models (1.3B) persist at scale.

3. **Cross-Domain Robustness**: Apply SKILL-IT to a diverse set of tasks beyond the current scope (e.g., code generation, mathematical reasoning, and creative writing) to validate that the skill ordering framework generalizes across fundamentally different skill types and that the online optimization remains stable.