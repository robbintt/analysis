---
ver: rpa2
title: Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural
  Networks
arxiv_id: '2302.14311'
source_url: https://arxiv.org/abs/2302.14311
tags:
- time
- training
- sltt
- bptt
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Spatial Learning Through Time (SLTT)
  method to reduce the computational cost of training Spiking Neural Networks (SNNs)
  using backpropagation through time (BPTT). The key insight is that the temporal
  components of backpropagation contribute minimally to the final gradients compared
  to spatial components.
---

# Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks

## Quick Facts
- arXiv ID: 2302.14311
- Source URL: https://arxiv.org/abs/2302.14311
- Reference count: 40
- Primary result: Achieves 70%+ memory reduction and 50%+ training time reduction on ImageNet while maintaining competitive accuracy

## Executive Summary
This paper introduces the Spatial Learning Through Time (SLTT) method to significantly reduce the computational cost of training Spiking Neural Networks (SNNs) using backpropagation through time (BPTT). The key insight is that temporal components of backpropagation contribute minimally to final gradients compared to spatial components. By ignoring these unimportant temporal routes during backpropagation, SLTT achieves constant memory usage independent of time steps and reduces training time from O(T) to O(K). The method maintains competitive accuracy with BPTT while achieving state-of-the-art performance on ImageNet with far fewer time steps than competing methods.

## Method Summary
SLTT reduces the computational cost of training SNNs by decomposing gradients into spatial and temporal components, then ignoring the temporal components during backpropagation. This approach enables online gradient calculation at each time step without requiring intermediate states to be stored, resulting in constant memory usage regardless of the number of time steps. The method also introduces SLTT-K, a variant that further reduces training time by limiting backpropagation to only K recent time steps. The key insight is that for typical surrogate gradient functions, the temporal components have negligible impact on final gradients due to their small spectral norms, allowing significant efficiency gains without substantial accuracy loss.

## Key Results
- Achieves >70% memory cost reduction compared to standard BPTT
- Reduces training time by >50% on ImageNet while maintaining competitive accuracy
- Achieves state-of-the-art accuracy on ImageNet using far fewer time steps than competing methods
- Maintains competitive performance across CIFAR-10, CIFAR-100, DVS-Gesture, and DVS-CIFAR10 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal components of backpropagation contribute minimally to final gradients compared to spatial components
- Mechanism: Gradients can be decomposed into spatial and temporal components, with temporal dependencies having negligible contribution due to small spectral norms of the diagonal matrix representing temporal dependencies
- Core assumption: The diagonal matrix representing temporal dependencies has a small spectral norm for typical surrogate function settings
- Evidence anchors: [abstract] temporal components contribute minimally; [section 4.1] temporal components contribute little due to small spectral norm; [corpus] Weak - no direct evidence in corpus papers

### Mechanism 2
- Claim: Ignoring unimportant temporal routes reduces scalar multiplications from O(T²) to O(T)
- Mechanism: By ignoring temporal components, the number of scalar multiplications required for gradient calculation is significantly reduced since spatial components dominate
- Core assumption: Spatial and temporal components have similar directions, making temporal components less important
- Evidence anchors: [abstract] reduces scalar multiplications; [section 4.2] required number of scalar multiplications reduced from Ω(T²) to Ω(T); [corpus] Weak - no direct evidence in corpus papers

### Mechanism 3
- Claim: Online gradient calculation enables time-steps-independent memory occupation
- Mechanism: Calculating gradients instantaneously at each time step without requiring information from other time steps allows intermediate states to be discarded, making memory usage constant
- Core assumption: Instantaneous loss calculation at each time step can be defined such that gradients can be computed independently
- Evidence anchors: [abstract] memory occupation independent of time steps; [section 4.2] error signals can be calculated independently; [section 4.2] required memory overhead is constant

## Foundational Learning

- Concept: Backpropagation Through Time (BPTT)
  - Why needed here: Understanding how BPTT works in SNNs is crucial to grasping why SLTT can improve efficiency
  - Quick check question: How does BPTT in SNNs differ from BPTT in standard RNNs, particularly regarding the non-differentiable spike generation function?

- Concept: Surrogate Gradients
  - Why needed here: Surrogate gradients are essential for training SNNs since the spike generation function is non-differentiable
  - Quick check question: What properties make a good surrogate gradient function for SNNs, and how do different surrogate functions affect training efficiency?

- Concept: Leaky Integrate-and-Fire (LIF) Neuron Model
  - Why needed here: The LIF model is the specific neuron model used in this work, and understanding its dynamics is crucial for understanding the gradient calculations
  - Quick check question: How does the membrane potential update equation in the LIF model contribute to the temporal dependencies in backpropagation?

## Architecture Onboarding

- Component map: SNN layers with LIF neurons → Loss function (cross-entropy + MSE) → Gradient calculation (spatial components only) → Weight update
- Critical path: Forward pass through SNN layers → Instantaneous loss calculation at each time step → Gradient calculation (spatial components only) → Weight update
- Design tradeoffs: Accuracy vs. efficiency (ignoring temporal components may slightly reduce accuracy but greatly improves efficiency), memory usage vs. training time (SLTT uses less memory but may require more careful hyperparameter tuning), and flexibility vs. performance (SLTT-K trades some accuracy for further time reduction)
- Failure signatures: Poor performance on tasks requiring strong temporal dependencies, instability in training due to aggressive gradient approximation, and potential issues with very long sequences where temporal components might become more significant
- First 3 experiments:
  1. Implement SLTT on a simple SNN task (e.g., MNIST) and compare memory usage and training time with standard BPTT
  2. Test SLTT-K with varying K values on a medium-scale task to find the optimal tradeoff between accuracy and efficiency
  3. Evaluate the method on a task with strong temporal dependencies to identify break conditions where temporal components become significant

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the text provided. However, based on the content and discussion, several implicit open questions emerge regarding the theoretical foundations, generalizability across different surrogate functions, and performance on neuromorphic hardware platforms.

## Limitations

- The method relies on the assumption that temporal components contribute negligibly to final gradients, which may not hold for all network architectures or tasks
- Performance on tasks with strong temporal dependencies has not been thoroughly explored, potentially limiting applicability to certain domains
- The paper doesn't validate claims about neuromorphic hardware suitability on actual neuromorphic platforms

## Confidence

- **High confidence**: Memory reduction claims (>70%) and training time improvements (>50%) on standard benchmarks like CIFAR-10/100 and ImageNet, as these are directly measurable and experimentally verified
- **Medium confidence**: Theoretical justification for ignoring temporal components, as it relies on assumptions about spectral norms and gradient decomposition that may not hold universally
- **Medium confidence**: State-of-the-art accuracy claims, as they depend on specific hyperparameter tuning and may not generalize to all network architectures

## Next Checks

1. Test SLTT on temporal sequence tasks (e.g., DVS-Gesture with longer sequences) to identify break conditions where temporal components become significant
2. Compare different surrogate gradient functions under SLTT to verify the assumption about temporal component insignificance across various settings
3. Conduct ablation studies systematically varying K in SLTT-K to map the accuracy-efficiency tradeoff curve and identify optimal configurations for different task complexities