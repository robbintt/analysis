---
ver: rpa2
title: Image Clustering using Restricted Boltzman Machine
arxiv_id: '2312.13845'
source_url: https://arxiv.org/abs/2312.13845
tags:
- image
- clustering
- face
- images
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Agglomerative Hierarchical Clustering
  based Method for Image Clustering using Restricted Boltzmann Machine (AHC-RBM).
  The proposed approach involves two major steps.
---

# Image Clustering using Restricted Boltzman Machine

## Quick Facts
- arXiv ID: 2312.13845
- Source URL: https://arxiv.org/abs/2312.13845
- Reference count: 40
- Primary result: AHC-RBM method achieves superior clustering performance on MS-Celeb-1M and DeepFashion datasets, particularly for small and sparse clusters

## Executive Summary
This paper introduces an Agglomerative Hierarchical Clustering based Method for Image Clustering using Restricted Boltzmann Machine (AHC-RBM). The approach involves training a universal RBM on all training data, then adapting individual RBMs for each test image. RBM vectors are generated by concatenating weight matrices and bias vectors from these adapted models. These vectors preserve class-specific information and are used for image clustering. The method outperforms established clustering algorithms like k-means and spectral clustering, particularly in challenging scenarios with small and sparse clusters.

## Method Summary
The method uses a two-step process: first, a universal RBM is trained on the full training dataset to capture class-independent features. Then, for each test image, an adapted RBM is trained using the universal RBM as initialization to capture class-specific features. RBM vectors are created by concatenating the visible-to-hidden weight matrices and bias vectors from these adapted models. Agglomerative Hierarchical Clustering with cosine or PLDA similarity is then applied to these vectors to form the final clusters. The approach was evaluated on MS-Celeb-1M and DeepFashion datasets using Pairwise F-score and BCubed F-score metrics.

## Key Results
- AHC-RBM outperforms k-means, spectral clustering, and approximate Rank-order on MS-Celeb-1M and DeepFashion datasets
- The method shows particular effectiveness for small and sparse clusters
- RBM vectors generated from concatenated weight matrices and bias vectors preserve class-specific information effectively
- Performance is robust across different dataset sizes and characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universal RBM captures class-independent features that serve as a strong initialization for adapted RBMs
- Mechanism: The universal RBM is trained on the full training dataset, learning broad feature representations. This model is then used to initialize adapted RBMs, which fine-tune these features for specific test images, allowing them to capture class-specific information
- Core assumption: The universal RBM learns generalizable features that can be effectively specialized through adaptation
- Evidence anchors:
  - [abstract] "Initially, a universal RBM model is trained using all available training dataset"
  - [section] "The URBM primary purpose is to capture class-independent information"
  - [corpus] Weak evidence - corpus focuses on different RBM applications but doesn't directly address universal RBM adaptation for clustering

### Mechanism 2
- Claim: Concatenating visible-to-hidden weight matrices and bias vectors creates discriminative RBM embedding vectors
- Mechanism: Each adapted RBM learns class-specific features. By concatenating the weight matrices and bias vectors from these adapted models, the resulting RBM vectors preserve class-specific information while maintaining the discriminative power of the original features
- Core assumption: The weight matrices and bias vectors contain sufficient class-specific information to distinguish between different classes
- Evidence anchors:
  - [abstract] "RBM vectors which is the embedding vector is generated by concatenating the visible-to-hidden weight matrices of these adapted models, and the bias vectors"
  - [section] "These vectors effectively preserve class-specific information"
  - [corpus] No direct evidence in corpus about concatenating RBM parameters for clustering

### Mechanism 3
- Claim: Bottom-up AHC with cosine or PLDA scoring effectively clusters RBM vectors based on class-specific information
- Mechanism: The AHC algorithm starts with each image as its own cluster and iteratively merges the most similar clusters based on cosine or PLDA similarity scores. This process leverages the class-specific information preserved in RBM vectors to form accurate clusters
- Core assumption: The similarity measures (cosine or PLDA) effectively capture the class-specific information in RBM vectors
- Evidence anchors:
  - [abstract] "These vectors effectively preserve class-specific information and are utilized in image clustering tasks"
  - [section] "We employ the conventional bottom-up Agglomerative Hierarchical Clustering (AHC) technique"
  - [corpus] No direct evidence in corpus about using AHC with RBM vectors

## Foundational Learning

- Concept: Restricted Boltzmann Machines
  - Why needed here: RBMs are the core mechanism for converting images into embedding vectors that preserve class-specific information
  - Quick check question: What are the two types of units in a standard RBM and what do they represent?

- Concept: Agglomerative Hierarchical Clustering
  - Why needed here: AHC is used to cluster the RBM vectors based on their similarity, forming the final image clusters
  - Quick check question: How does the linkage method (single vs. average) affect the merging decisions in AHC?

- Concept: Similarity measures (cosine and PLDA)
  - Why needed here: These measures are used to compute the similarity between RBM vectors during the clustering process
  - Quick check question: What is the main difference between cosine similarity and PLDA in terms of what they measure?

## Architecture Onboarding

- Component map: Universal RBM -> Adapted RBMs -> RBM Vector Extractor -> AHC Clustering -> Evaluation

- Critical path:
  1. Train universal RBM on full training dataset
  2. For each test image, adapt the universal RBM to create class-specific model
  3. Extract RBM vectors by concatenating weight matrices and bias vectors
  4. Cluster RBM vectors using AHC with cosine or PLDA similarity
  5. Evaluate clustering performance

- Design tradeoffs:
  - Dimensionality of RBM vectors vs. computational cost of clustering
  - Number of hidden units in RBM vs. representational power and overfitting
  - Choice of similarity measure (cosine vs. PLDA) vs. clustering accuracy
  - Training epochs for universal vs. adapted RBMs vs. feature quality

- Failure signatures:
  - Poor clustering performance despite high-dimensional RBM vectors (possible loss of discriminative information)
  - Extremely long training times (potential issues with RBM architecture or training parameters)
  - Clustering results dominated by a few large clusters (possible bias in similarity measures or AHC parameters)

- First 3 experiments:
  1. Train universal RBM on full dataset and visualize learned features to verify it captures meaningful patterns
  2. Adapt universal RBM for a few test images and compare adapted features to original to verify specialization
  3. Extract RBM vectors and compute pairwise similarities to verify they capture class-specific information before full clustering

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- The computational complexity of generating per-image adapted RBMs is not discussed, raising concerns about scalability
- The choice of 200 epochs for both universal and adapted RBM training appears arbitrary without sensitivity analysis
- The comparison against baselines like k-means and spectral clustering may be unfair since these are not specifically designed for face or fashion clustering tasks

## Confidence

- Mechanism 1 (Universal RBM initialization): Medium - reasonable but under-validated
- Mechanism 2 (Concatenated parameter vectors): Low - novel approach with limited theoretical grounding
- Mechanism 3 (AHC clustering effectiveness): Medium - standard technique but effectiveness with RBM vectors unproven

## Next Checks
1. Perform ablation study comparing clustering performance using: only weight matrices, only bias vectors, and concatenated vectors to isolate the contribution of each component
2. Test computational scalability by measuring training time and memory usage when scaling from 0.5M to 5M training images
3. Compare against specialized clustering methods for faces (e.g., triplet loss with k-means) to ensure fair baseline comparison