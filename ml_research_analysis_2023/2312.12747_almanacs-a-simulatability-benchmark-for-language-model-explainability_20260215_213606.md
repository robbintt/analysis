---
ver: rpa2
title: 'ALMANACS: A Simulatability Benchmark for Language Model Explainability'
arxiv_id: '2312.12747'
source_url: https://arxiv.org/abs/2312.12747
tags:
- explanations
- explanation
- methods
- behavior
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ALMANACS, a benchmark for evaluating language
  model explainability methods through simulatability - measuring how well explanations
  improve behavior prediction on new inputs. The benchmark consists of 12 safety-relevant
  topics with idiosyncratic premises designed to elicit model-specific behavior, using
  a train-test distributional shift to encourage faithful explanations.
---

# ALMANACS: A Simulatability Benchmark for Language Model Explainability

## Quick Facts
- arXiv ID: 2312.12747
- Source URL: https://arxiv.org/abs/2312.12747
- Reference count: 40
- Primary result: None of four explanation methods consistently improve simulatability over no-explanation control across 12 safety-relevant topics

## Executive Summary
ALMANACS introduces a benchmark for evaluating language model explainability methods through simulatability - measuring how well explanations improve behavior prediction on new inputs. The benchmark uses distributional shift between train and test sets with safety-relevant topics designed to elicit model-specific behavior. Using GPT-4 as an automated predictor, the study evaluates counterfactual, rationalization, attention, and Integrated Gradients explanations, finding that none consistently outperform a no-explanation control baseline, with mean Kullback-Leibler divergence scores between 0.08-0.10.

## Method Summary
The benchmark generates templated questions on 12 safety-relevant topics, with train and test sets using different placeholder values to create distributional shift. Four explanation methods are evaluated: counterfactuals (finding input variations that change model output), rationalizations (using model's own explanations), attention visualizations (verbalized from attention weights), and Integrated Gradients (feature attribution). GPT-4 serves as a predictor using in-context learning from examples, and performance is measured using Kullback-Leibler divergence, total variation distance, and Spearman correlation coefficient.

## Key Results
- No explanation method consistently outperforms NOEXPL control across all 12 topics
- Mean KLD IV scores range from 0.08-0.10 for both evaluated models
- Counterfactual and attention explanations show mixed results depending on topic
- ATTENTION explanations perform well on "Human or Not" and "Sycophancy" topics but poorly on others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can predict model behavior using in-context learning from examples alone
- Mechanism: The predictor GPT-4 is prompted with training examples (x, y) where y is the probability of "Yes" answer, allowing it to learn the model's behavior patterns through few-shot prompting
- Core assumption: GPT-4's in-context learning capability is sufficient to capture model behavior without explanations
- Evidence anchors:
  - [abstract] "none of the explanation methods consistently improve simulatability"
  - [section 2.1] "we use GPT-4 prompted in-context with 10 examples from D"
  - [corpus] Weak - no direct corpus evidence on GPT-4's in-context learning for this specific task
- Break condition: If GPT-4's in-context learning fails on novel distributional shifts or if the model behavior is too complex for few-shot learning

### Mechanism 2
- Claim: Distributional shift in test data prevents simple interpolation-based prediction
- Mechanism: Train and test sets use different placeholder values, forcing predictors to extrapolate rather than interpolate
- Core assumption: The distributional shift is sufficient to prevent naive prediction methods from working well
- Evidence anchors:
  - [section 2.2] "we evaluate simulatability under a distributional shift between questions in a train and test set"
  - [section 3.1] "LOGISTIC REGRESSION is the best naive baseline, with a KLD IV of 0.11"
  - [corpus] Strong - multiple related papers mention distributional shift as a key challenge
- Break condition: If the shift is too small or if predictors can still interpolate effectively

### Mechanism 3
- Claim: Counterfactual explanations work by showing input variations that change model output
- Mechanism: For each (x, y) pair, find nearest neighbor (x', y') with |y' - y| > 0.2 and use (x', y') as explanation
- Core assumption: The distance metric (cosine similarity of embeddings) captures meaningful conceptual differences
- Evidence anchors:
  - [section 3.2] "we generate counterfactual explanations by identifying, for each (x, y) ∈ D, the nearest neighbor (x′, y′) that satisfies |y′ − y| > δ"
  - [section 5] "COUNTERFACTUAL explanations...decrease KLD IV from 0.19 to 0.15 in Sycophancy"
  - [corpus] Moderate - related work supports counterfactuals but specific evidence for this approach is limited
- Break condition: If the embedding space doesn't capture relevant semantic differences or if δ is set too high/low

## Foundational Learning

- Concept: Kullback-Leibler Divergence as a prediction metric
  - Why needed here: KLD IV measures how well predicted probabilities match actual model probabilities
  - Quick check question: What does KLD IV=0.10 tell us about prediction quality?

- Concept: Total Variation Distance
  - Why needed here: TVD IST provides an alternative bounded metric for comparing probability distributions
  - Quick check question: How does TVD IST differ from KLD IV in interpretation?

- Concept: Spearman's rank correlation
  - Why needed here: Measures whether predictor can correctly rank inputs by probability rather than predict exact values
  - Quick check question: When would Spearman correlation be more useful than KLD IV?

## Architecture Onboarding

- Component map: Dataset (D) -> Explainer (E) -> Predictor (P) -> Evaluation metrics
- Critical path: Generate train/test data → Apply explanation method → Prompt GPT-4 with examples → Evaluate prediction quality
- Design tradeoffs: Fully automated vs human evaluation (speed/reproducibility vs accuracy), distributional shift (realism vs difficulty)
- Failure signatures: KLD IV scores not improving over NOEXPL control, GPT-4 predictor failing to learn from explanations
- First 3 experiments:
  1. Run baseline with NOEXPL control to establish performance floor
  2. Test counterfactual explanations on a single topic to verify implementation
  3. Compare attention vs integrated gradients on same topic to understand method differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do the results of ALMANACS transfer to human evaluations of explanation quality?
- Basis in paper: [explicit] The paper explicitly states that "it remains unclear how its results will transfer to human studies" and that "An interesting direction for future work would be to investigate this correspondence between automated and human predictors."
- Why unresolved: The benchmark uses GPT-4 as an automated predictor, which may not align with human judgment of explanation quality.
- What evidence would resolve it: Conducting human studies comparing human and GPT-4 predictor performance on the same explanation methods and topics would provide direct evidence of the correspondence.

### Open Question 2
- Question: Can an easier version of ALMANACS be developed while preserving its key properties?
- Basis in paper: [explicit] The paper suggests that "If the field of explainability is not yet up to this challenge, then it could be valuable for future work to develop an easier version of the ALMANACS benchmark."
- Why unresolved: The current benchmark's difficulty prevents explanation methods from improving over the no-explanation control, suggesting a need for a less challenging version.
- What evidence would resolve it: Creating and evaluating a modified benchmark with simpler tasks while maintaining distributional shift and non-objective properties would demonstrate whether easier versions are feasible.

### Open Question 3
- Question: Why do the tested explanation methods fail to improve simulatability in ALMANACS?
- Basis in paper: [inferred] The paper provides qualitative analysis of explanation methods suggesting they focus on specific examples rather than providing generalizable insights, and that saliency tokens may not provide clear guidance for behavior prediction.
- Why unresolved: The predictor is a black box, making definitive conclusions difficult, and the paper only offers hypotheses about why explanations fail.
- What evidence would resolve it: Systematic ablation studies removing specific components of explanations, or human studies analyzing what information is actually useful for prediction, would provide more definitive answers.

## Limitations
- The benchmark focuses on safety-relevant topics with idiosyncratic premises that may not represent general language model behavior
- The automated evaluation using GPT-4 as predictor may not translate to human evaluation
- The absolute performance (KLD IV of 0.08-0.10) suggests the problem of developing effective explanations for simulatability remains challenging

## Confidence
- Medium: The methodology is sound and results are reproducible, but the benchmark's specialized focus and automated evaluation limit generalizability

## Next Checks
1. **Human Evaluation Validation**: Conduct human studies to verify that GPT-4's prediction performance correlates with human ability to predict model behavior using the same explanations.

2. **Cross-Benchmark Generalization**: Test whether explanation methods that perform poorly in ALMANACS show better results on other simulatability benchmarks or real-world prediction tasks.

3. **Method Ablation Studies**: Systematically vary the distributional shift parameters and explanation quality to identify specific failure modes and potential improvements.