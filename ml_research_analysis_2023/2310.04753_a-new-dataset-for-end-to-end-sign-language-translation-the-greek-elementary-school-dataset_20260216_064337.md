---
ver: rpa2
title: 'A New Dataset for End-to-End Sign Language Translation: The Greek Elementary
  School Dataset'
arxiv_id: '2310.04753'
source_url: https://arxiv.org/abs/2310.04753
tags:
- dataset
- language
- sign
- translation
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Greek Elementary School dataset (Elementary23),
  a high-quality collection of 29,653 Greek Sign Language (GSL) video-translation
  pairs covering the official syllabus of Greek elementary schools. The dataset features
  professional-grade recordings, expert signers, and a broad thematic spectrum across
  six subjects.
---

# A New Dataset for End-to-End Sign Language Translation: The Greek Elementary School Dataset

## Quick Facts
- **arXiv ID**: 2310.04753
- **Source URL**: https://arxiv.org/abs/2310.04753
- **Reference count**: 40
- **Primary result**: Introduces Elementary23 dataset (29,653 GSL video-translation pairs) and achieves BLEU-4 score of 5.69 on end-to-end SLT using stochastic LWTA Transformers

## Executive Summary
This paper introduces the Greek Elementary School dataset (Elementary23), a high-quality collection of 29,653 Greek Sign Language (GSL) video-translation pairs covering the official syllabus of Greek elementary schools. The dataset features professional-grade recordings, expert signers, and a broad thematic spectrum across six subjects. To address challenges in end-to-end Sign Language Translation (SLT), the authors also present Elementary23-SLT, a curated subset optimized for deep learning. They train state-of-the-art Transformer-based models, including deterministic and stochastic LWTA Transformers, achieving BLEU-4 scores of 5.69 on the test set. The dataset and results demonstrate its potential as a benchmark for advancing SLT research with realistic content and measurable translation quality.

## Method Summary
The authors created Elementary23 by recording 29,653 video-translation pairs of GSL signs covering the Greek elementary school curriculum, with professional signers and high-quality 720p recordings. They developed Elementary23-SLT, a filtered subset of 8,372 pairs, to address word sparsity and singleton issues by removing rare words and singleton-only sentences. For translation, they used OpenPose to extract body landmarks and trajectories as video features, bypassing the need for gloss annotations. They trained Transformer-based models including deterministic and stochastic LWTA Transformers with cross-entropy and ELBO objectives respectively, using beam search inference.

## Key Results
- Elementary23 dataset contains 29,653 GSL video-translation pairs covering 6 school subjects
- Elementary23-SLT subset reduces word sparsity by filtering singletons and rare words, resulting in 8,372 pairs
- Stochastic LWTA Transformer achieves BLEU-4 score of 5.69 on test set, outperforming deterministic baseline by over 1.5 points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using stochastic LWTA Transformers yields better BLEU-4 scores than deterministic Transformers for end-to-end SLT.
- Mechanism: LWTA introduces local competition among output units within blocks, creating sparse and stochastic activations that improve generalization and translation quality.
- Core assumption: The stochasticity and sparsity from LWTA units help the model capture the complex structure of sign language translation better than deterministic activations.
- Evidence anchors:
  - [abstract] "they present Elementary23-SLT, a curated subset optimized for deep learning. They train state-of-the-art Transformer-based models, including deterministic and stochastic LWTA Transformers, achieving BLEU-4 scores of 5.69 on the test set."
  - [section] "The stochastic LWTA Transformer appears to be superior to the deterministic, achieving a BLEU-4 score of more than 1.5 units higher than the latter."
  - [corpus] Weak or missing: no corpus evidence provided directly comparing LWTA vs deterministic results.
- Break condition: If the dataset lacks sufficient diversity or if the model architecture cannot leverage the sparsity/stochasticity effectively, the advantage may disappear.

### Mechanism 2
- Claim: The Elementary23-SLT subset improves translation performance by reducing vocabulary sparsity and singleton words compared to using the full Elementary23 dataset.
- Mechanism: By filtering out singleton-only sentences and iteratively removing sentences with rare words, the subset increases the frequency of common words, making it easier for deep networks to learn patterns.
- Core assumption: Modern deep networks require multiple examples of each word to learn effectively; reducing singletons and rare words improves training stability and translation quality.
- Evidence anchors:
  - [abstract] "To overcome this issue, in this work we also present an appropriate representative subsample of the dataset, which we dub Elementary23-SLT... This is more suitable for training end-to-end SLT deep networks..."
  - [section] "The aforementioned process left us with a sample of 7168 sentences... The final SLT set contains 8372 video-sentence pairs..."
  - [corpus] Weak or missing: no corpus evidence showing direct comparison of BLEU scores between full dataset and subset before/after filtering.
- Break condition: If the filtering process removes too much content, reducing dataset size below a viable threshold, or if the retained sentences do not represent the full linguistic diversity, performance gains may be lost.

### Mechanism 3
- Claim: Using OpenPose to extract body landmarks and trajectories as features enables effective end-to-end SLT without requiring gloss annotations.
- Mechanism: OpenPose tracks key body parts and hand shapes, providing spatial and temporal features that capture the core elements of sign language, bypassing the need for gloss-level supervision.
- Core assumption: The visual patterns extracted by OpenPose are sufficient to represent the linguistic content of sign language for translation into spoken language.
- Evidence anchors:
  - [abstract] "To address this issue, we follow an alternative, yet established approach that involves using the OpenPose engine..."
  - [section] "The OpenPose engine is a convolutional neural network that has been trained to track and extract the trajectories of key human body parts. We use OpenPose to track landmarks related to the 2D positioning of upper body movements, facial expressions, and hand shapes..."
  - [corpus] Weak or missing: no corpus evidence comparing OpenPose-based features to gloss-based features or other feature extraction methods.
- Break condition: If OpenPose fails to capture subtle or nuanced sign elements critical for translation, or if the feature representation is too coarse, translation quality will suffer.

## Foundational Learning

- Concept: Sign Language Translation (SLT) fundamentals
  - Why needed here: Understanding that SLT involves converting visual sign language into spoken language text, and recognizing the challenges of modality gaps and data scarcity.
  - Quick check question: What is the main difference between Sign Language Recognition (SLR) and Sign Language Translation (SLT)?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The models used are Transformer-based; understanding self-attention, encoder-decoder structure, and how they handle sequential data is crucial.
  - Quick check question: How does the self-attention mechanism in Transformers help process sequential sign language video frames?

- Concept: Data preprocessing and feature extraction
  - Why needed here: The pipeline uses OpenPose for landmark extraction; knowing how to preprocess video data and extract meaningful features is essential.
  - Quick check question: Why might using OpenPose landmarks be preferable to using raw video frames for SLT?

## Architecture Onboarding

- Component map: Video input → OpenPose landmark extraction → Frame-wise feature vectors → Encoder (Transformer layers) → Context representations → Decoder (Transformer layers) → Text output
- Critical path: Landmark extraction → Encoder processing → Decoder generation → Beam search inference
- Design tradeoffs:
  - Using OpenPose avoids gloss annotation but may lose fine-grained linguistic detail
  - Stochastic LWTA improves performance but adds complexity and training instability risk
  - Dataset filtering improves training efficiency but may reduce linguistic coverage
- Failure signatures:
  - Low BLEU scores despite training → possible issues with feature quality or model capacity
  - High variance in predictions → stochastic components may be too noisy or poorly calibrated
  - Overfitting on small dataset → insufficient regularization or too complex model
- First 3 experiments:
  1. Train deterministic Transformer on full Elementary23 dataset; measure BLEU-4 score.
  2. Train deterministic Transformer on Elementary23-SLT subset; compare BLEU-4 improvement.
  3. Train stochastic LWTA Transformer on Elementary23-SLT; compare BLEU-4 against deterministic baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of Gloss annotations affect the performance of end-to-end Sign Language Translation (SLT) models on the Elementary23 dataset?
- Basis in paper: [explicit] The authors mention that "the preprocessing phase requires laborious dataset annotation in terms of auxiliary Glosses" and that models using Glosses can only generalize on other datasets of similar Gloss structure.
- Why unresolved: The Elementary23 dataset does not include Gloss annotations, making it impossible to directly evaluate the impact of Glosses on this specific dataset.
- What evidence would resolve it: Conducting experiments by training and evaluating SLT models on Elementary23 with and without Gloss annotations, and comparing the results.

### Open Question 2
- Question: What is the impact of the high word sparsity and large number of singletons in the Elementary23 dataset on the performance of end-to-end SLT models?
- Basis in paper: [explicit] The authors note that "the dataset's high word sparsity; the high number of singletons (words appearing only once in the corpus)" presents significant modeling challenges for deep networks.
- Why unresolved: The paper does not provide a detailed analysis of how these linguistic characteristics affect model performance.
- What evidence would resolve it: Performing ablation studies by varying the levels of word sparsity and singletons in the dataset, and measuring the impact on SLT model performance.

### Open Question 3
- Question: How do the translation quality and usability of SLT models trained on Elementary23 compare to those trained on other benchmark datasets like Phoenix2014T and OpenASL?
- Basis in paper: [explicit] The authors compare the BLEU-4 scores of their models trained on Elementary23 with those reported for other datasets, but do not provide a comprehensive comparison of translation quality and usability.
- Why unresolved: The paper focuses primarily on the technical aspects of the dataset and does not extensively evaluate the practical usability of the trained models.
- What evidence would resolve it: Conducting user studies or real-world evaluations of SLT models trained on different datasets, and comparing their translation quality and usability in practical scenarios.

## Limitations
- Reported BLEU-4 score of 5.69 remains relatively low for practical deployment
- Reliance on OpenPose landmarks may miss nuanced linguistic elements that gloss annotations would capture
- Dataset filtering for Elementary23-SLT may reduce linguistic coverage by removing rare words and singleton sentences

## Confidence
- High confidence in dataset novelty and collection methodology
- Medium confidence in LWTA Transformer performance claims
- Low confidence in generalization of results to other sign languages

## Next Checks
1. Conduct ablation studies comparing OpenPose-based features versus gloss annotations on a subset of the dataset to quantify information loss
2. Test the stochastic LWTA Transformer on an external sign language dataset to evaluate cross-linguistic generalization
3. Perform qualitative analysis of translation errors to identify whether failures stem from feature extraction, model architecture, or data limitations