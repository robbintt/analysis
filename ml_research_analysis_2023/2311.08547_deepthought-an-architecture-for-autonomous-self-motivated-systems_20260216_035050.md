---
ver: rpa2
title: 'DeepThought: An Architecture for Autonomous Self-motivated Systems'
arxiv_id: '2311.08547'
source_url: https://arxiv.org/abs/2311.08547
tags:
- learning
- attention
- language
- systems
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that large language models (LLMs), despite their
  impressive performance in dialogue tasks, lack key properties of agency such as
  self-motivation, adaptability, and the ability to learn from interactions. The authors
  propose a biologically-inspired architecture called DeepThought that combines LLMs
  with other neural networks to create cognitive language agents with properties akin
  to agency, self-motivation, and meta-cognition.
---

# DeepThought: An Architecture for Autonomous Self-motivated Systems

## Quick Facts
- arXiv ID: 2311.08547
- Source URL: https://arxiv.org/abs/2311.08547
- Reference count: 30
- One-line primary result: Proposes an architecture combining LLMs with specialized neural networks to create cognitive agents with agency, self-motivation, and meta-cognition

## Executive Summary
This paper argues that large language models, despite their impressive performance in dialogue tasks, fundamentally lack key properties of agency such as self-motivation, adaptability, and the ability to learn from interactions. The authors propose DeepThought, a biologically-inspired architecture that integrates LLMs with other neural networks to create cognitive language agents exhibiting properties akin to agency, self-motivation, and meta-cognition. The architecture draws on complementary learning systems, global neuronal workspace, and attention schema theories to create a system that can direct its own attention, learn from experiences, and potentially reflect on its reasoning processes.

## Method Summary
The DeepThought architecture combines multiple specialized neural network modules (language, vision, auditory, world models) with a supervisory attention schema that controls focus and behavior. The supervisor module acts as a fast-learning hub similar to the medial temporal lobe, directing attention and enabling rapid adaptation. This integrates with slower-learning modules that consolidate long-term knowledge through reinforcement learning. The architecture also includes episodic memory that stores the sequence of attended tokens, enabling meta-cognition and introspection of reasoning processes. The system uses deep reinforcement learning to adjust model parameters and improve predictions of the world model.

## Key Results
- Proposes integration of LLMs with specialized neural networks to create cognitive agents
- Introduces attention schema to control focus and behavior based on GWT and AST
- Implements dual learning system (fast MTL-like supervisor + slow neocortical modules) based on CLS theory
- Includes episodic memory for attention stream to enable meta-cognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating multiple specialized neural network modules with a supervisory attention schema enables behavior that simulates agency
- Mechanism: The supervisor module acts as a fast-learning, attention-directing hub that selects which sensory or internal information deserves focus, similar to the MTL in biological systems
- Core assumption: Attention mechanisms can direct learning and decision-making in a way that creates the appearance of agency even without intrinsic motivation
- Evidence anchors:
  - [abstract] "proposes to integrate LLMs and other deep learning systems into an architecture for cognitive language agents able to exhibit properties akin to agency, self-motivation, even some features of meta-cognition"
  - [section 4] "The supervisor module, at the top, enables short-term attention and memory and acts as the medium temporal lobe in the CLS theory, displaying fast adaptability, and controlling the global attention mechanism posited by GWT/GNW and AST"
  - [corpus] Weak evidence: corpus neighbors focus on agency but do not discuss architectural integration with attention schemas
- Break condition: If the attention schema fails to consistently prioritize relevant inputs, the system may behave erratically or fail to exhibit coherent goal-directed behavior

### Mechanism 2
- Claim: A two-system architecture (fast learning MTL-like supervisor + slow learning neocortical modules) allows both rapid adaptation and stable knowledge consolidation
- Mechanism: The supervisor rapidly encodes new episodic experiences and directs attention, while the lower-level modules slowly adapt through reinforcement learning to consolidate long-term knowledge
- Core assumption: Fast and slow learning systems must be integrated to avoid catastrophic forgetting and support both flexibility and stability
- Evidence anchors:
  - [section 4] "The supervisor module... acts as the medium temporal lobe in the CLS theory, displaying fast adaptability... The deep reinforcement learning (DRL) module... compares them with the internal world and self models, adjusting, as required, the model parameters"
  - [section 3] "CLS theory argues that these two systems work together in a complementary manner to support memory and learning"
  - [corpus] Weak evidence: corpus neighbors do not reference complementary learning systems or dual-process architectures
- Break condition: If the slow-learning modules cannot consolidate experiences effectively, the system will fail to build stable long-term knowledge and will act inconsistently

### Mechanism 3
- Claim: Storing the sequence of attended tokens in episodic memory enables meta-cognition and introspection of reasoning
- Mechanism: By logging the attention stream over time, the system can later reconstruct its reasoning path, describe its decision process, and potentially refine future attention strategies
- Core assumption: Introspection requires an explicit record of attentional focus over time
- Evidence anchors:
  - [section 4] "The succession of tokens that are the focus of attention is stored in long-term memory (the attention stream) for future retrieval and also to make meta-cognition possible"
  - [section 2] "we believe they fall short of achieving the full potential of self-motivated agents because... they lack attention mechanisms and meta-cognition"
  - [corpus] No direct evidence; this claim is novel to the paper
- Break condition: If the episodic memory is incomplete or noisy, the system cannot reliably reconstruct its reasoning, undermining meta-cognitive capabilities

## Foundational Learning

- Concept: Complementary Learning Systems (CLS)
  - Why needed here: Explains how fast episodic learning and slow semantic learning can coexist without interference
  - Quick check question: How does the hippocampus differ from the neocortex in terms of learning speed and stability?

- Concept: Global Neuronal Workspace Theory (GNW)
  - Why needed here: Provides the framework for how attended information becomes globally available for conscious processing
  - Quick check question: What is the role of the global workspace in broadcasting attended information to all processing modules?

- Concept: Attention Schema Theory (AST)
  - Why needed here: Describes how an internal model of attention can create the experience of consciousness and control over focus
  - Quick check question: How does Graziano's theory explain the subjective experience of attention?

## Architecture Onboarding

- Component map:
  - Supervisor Module: Fast-learning attention and memory controller (MTL analog)
  - DRL Module: Reinforcement learner that updates world and self-models
  - Language Model: Processes and generates text
  - Vision Model: Processes visual inputs
  - Auditory Model: Processes sound inputs
  - World Model: Maintains internal model of environment dynamics
  - Embedding Store: Shared representation space for multimodal tokens
  - Episodic Memory: Stores attention stream for meta-cognition

- Critical path: Supervisor → Attention Schema → Module Selection → Action Generation → Environment Feedback → DRL Update

- Design tradeoffs:
  - Fast vs. slow learning: Rapid adaptation may come at the cost of stability; balancing is key
  - Attention bandwidth: Over-attention can overwhelm the supervisor; under-attention can miss critical signals
  - Episodic memory size: Larger memory enables better introspection but increases storage and retrieval costs

- Failure signatures:
  - Erratic behavior: Indicates attention schema misfiring or poor prioritization
  - Forgetting: Suggests slow-learning modules are not consolidating effectively
  - Lack of introspection: Implies episodic memory is incomplete or inaccessible

- First 3 experiments:
  1. Test supervisor attention control by varying attention window size and measuring coherence of output sequences
  2. Validate dual learning by introducing rapid input changes and checking for catastrophic forgetting in lower modules
  3. Evaluate meta-cognition by querying the system to describe its reasoning path and checking consistency with stored attention stream

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DeepThought architecture, with its proposed integration of multiple neural networks and attention mechanisms, effectively replicate the agency and self-motivation properties observed in biological systems?
- Basis in paper: [explicit] The authors propose the DeepThought architecture to address the limitations of LLMs and enable properties akin to agency, self-motivation, and meta-cognition
- Why unresolved: The paper presents a theoretical framework and draws inspiration from biological theories, but does not provide empirical evidence or experimental results to validate the effectiveness of the architecture
- What evidence would resolve it: Empirical studies demonstrating the architecture's ability to exhibit agency, self-motivation, and meta-cognition in various tasks and environments

### Open Question 2
- Question: How does the DeepThought architecture handle the trade-off between the fast, unconscious processing of system 1 and the slow, deliberate processing of system 2, and what are the implications for decision-making and goal pursuit?
- Basis in paper: [explicit] The paper emphasizes the importance of a two-system architecture inspired by complementary learning systems theory and dual process theories
- Why unresolved: The paper does not provide a detailed explanation of how the two systems interact or how the trade-off is managed within the architecture
- What evidence would resolve it: Experiments or simulations that demonstrate the architecture's ability to balance and integrate the processing of both systems in different scenarios

### Open Question 3
- Question: Can the attention schema in the DeepThought architecture effectively control the focus of attention and guide the behavior of the system in a way that is analogous to the attention mechanisms in biological systems?
- Basis in paper: [explicit] The paper proposes the use of an attention schema to control the focus of attention and the behavior of the system, inspired by attention schema theory
- Why unresolved: The paper does not provide a detailed description of the attention schema or evidence of its effectiveness in controlling attention and behavior
- What evidence would resolve it: Empirical studies demonstrating the attention schema's ability to guide attention and behavior in a manner similar to biological attention mechanisms

## Limitations
- The biological grounding draws on well-established theories but specific implementation details connecting these theories to neural network components remain underspecified
- Critical details about module interfaces, learning algorithms, and system dynamics are missing from the paper
- The paper provides conceptual architecture diagrams but lacks concrete algorithmic descriptions of how the attention schema operates

## Confidence
- Medium confidence in the conceptual framework: The integration of multiple neural networks with attention control is theoretically sound and draws on established neuroscience
- Low confidence in implementation feasibility: Critical details about module interfaces, learning algorithms, and system dynamics are missing
- Medium confidence in potential outcomes: The architecture could enable agency-like behavior if attention mechanisms function as described, but this remains speculative without empirical validation

## Next Checks
1. Implement a simplified version of the attention schema with two competing inputs and verify that it can consistently prioritize relevant information over time
2. Create a test environment where rapid input changes occur and measure whether the supervisor module can maintain coherent behavior while lower modules gradually adapt without catastrophic forgetting
3. Design an evaluation protocol where the system must explain its decision-making process, then compare these explanations against stored attention streams to assess meta-cognitive accuracy