---
ver: rpa2
title: 'The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning'
arxiv_id: '2311.02940'
source_url: https://arxiv.org/abs/2311.02940
tags:
- hume
- learning
- dataset
- samples
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HUME is a framework for unsupervised learning that infers human
  labeling of datasets without external supervision. It exploits the observation that
  human-labeled classes are linearly separable across different representation spaces.
---

# The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning

## Quick Facts
- arXiv ID: 2311.02940
- Source URL: https://arxiv.org/abs/2311.02940
- Reference count: 40
- Key outcome: HUME framework achieves state-of-the-art unsupervised clustering performance by inferring human labeling without external supervision

## Executive Summary
HUME is a novel framework for unsupervised learning that discovers human-labeled tasks in datasets without requiring any external supervision. The key insight is that human-labeled classes are linearly separable across different representation spaces, allowing HUME to use generalization-based objectives to search for labelings that match human annotations. By training only linear classifiers on top of fixed pretrained representations, HUME achieves strong performance while being model-agnostic and benefiting from increasingly stronger pretrained models.

## Method Summary
HUME optimizes task parameters by minimizing test error of linear classifiers on two fixed pretrained representations, using MAML-style updates to propagate gradients through the inner optimization loop. The framework relies on the observation that human-labeled classes are linearly separable in sufficiently strong representation spaces, and uses cross-validation accuracy as a proxy for labeling quality. To avoid degenerate solutions where all samples are assigned to one class, HUME incorporates entropy regularization. The method aggregates multiple runs to produce stable predictions, and requires knowing the number of classes (K) in advance.

## Key Results
- Outperforms supervised linear classifier on STL-10 by 5%
- Achieves state-of-the-art performance on multiple benchmark datasets including ImageNet-1000
- Generates reliable samples for semi-supervised learning, achieving near-perfect accuracy in low-data regimes
- Demonstrates strong correlation between optimization objective and ground truth labeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-labeled tasks are linearly separable across different representation spaces
- Core assumption: Human-labeled tasks are invariant to the choice of representation space when that space is sufficiently strong
- Evidence anchors: The key insight behind HUME is that classes defined by many human labelings are linearly separable regardless of the representation space used

### Mechanism 2
- Claim: HUME's optimization objective is well-correlated with ground truth labeling
- Core assumption: The generalization error computed using cross-validation on two different representation spaces is a reliable proxy for how close a labeling is to human labeling
- Evidence anchors: HUME shows that the proposed optimization objective is strikingly well-correlated with the ground truth labeling of the dataset

### Mechanism 3
- Claim: Aggregation over multiple labelings stabilizes predictions
- Core assumption: Different random initializations lead to labelings that capture different aspects of the underlying human labeling, and majority voting over these captures the consensus
- Evidence anchors: HUME aggregates top-n labelings w.r.t. generalization error and shows that this produces more stable predictions than any single labeling

## Foundational Learning

- Concept: Linear separability of classes in representation space
  - Why needed here: The framework relies on the observation that human-labeled classes are linearly separable in sufficiently strong representation spaces
  - Quick check question: Can you explain why linear separability of classes in one representation space implies linear separability in another sufficiently strong space?

- Concept: Generalization error as a proxy for labeling quality
  - Why needed here: HUME uses generalization error computed via cross-validation as its optimization objective to find human-labeled tasks
  - Quick check question: How does measuring generalization error on held-out data help identify whether a labeling matches human labeling?

- Concept: Cross-validation for model evaluation
  - Why needed here: HUME uses cross-validation to estimate generalization error without requiring labeled data
  - Quick check question: Why is cross-validation particularly useful in unsupervised learning where labeled data is unavailable?

## Architecture Onboarding

- Component map: Self-supervised representations (ϕ1) -> Large pretrained model (ϕ2) -> Task encoder (τW1) -> Linear classification -> Generalization error computation -> Parameter update -> Aggregation
- Critical path: Representation → Task encoding → Linear classification → Generalization error computation → Parameter update → Aggregation
- Design tradeoffs:
  - Using fixed representations vs. fine-tuning: Fixed representations make framework model-agnostic but may limit performance if representations are weak
  - Linear vs. nonlinear classifiers: Linear classifiers ensure computational efficiency and interpretability but may miss complex patterns
  - Single vs. multiple runs: Single runs are faster but aggregation over multiple runs provides more stable results
- Failure signatures:
  - All samples assigned to one class: Indicates regularization parameter too low or representations too weak
  - Poor correlation with ground truth: Suggests representations are not sufficiently strong or optimization is stuck in poor local optima
  - High variance across runs: May indicate need for more aggregation or stronger regularization
- First 3 experiments:
  1. Run HUME with MOCOv2 and BiT on CIFAR-10, check if top-1 accuracy correlates with ground truth labeling
  2. Compare performance using different large pretrained models (BiT vs CLIP vs DINO) on STL-10
  3. Test aggregation strategy by varying number of top labelings included in majority vote on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to aggregate labelings from multiple runs of HUME to maximize accuracy?
- Basis in paper: The paper mentions that they do not optimize for different aggregation strategies and simply use the majority vote of all tasks in all experiments
- Why unresolved: The paper does not systematically compare different aggregation strategies (majority vote, top-n tasks) across various datasets and model strengths
- What evidence would resolve it: Systematic experiments comparing different aggregation strategies across various datasets and model strengths to determine the optimal approach

### Open Question 2
- Question: How does the choice of self-supervised method (e.g., MOCO, SimCLR, BYOL) impact HUME's performance?
- Basis in paper: The paper shows that HUME instantiated with MOCO consistently outperforms HUME instantiated with SimCLR, and that using BYOL shows consistent improvements over MOCO representations
- Why unresolved: While the paper demonstrates the impact of different self-supervised methods, it does not explore the reasons behind these differences or investigate other self-supervised methods
- What evidence would resolve it: Further experiments comparing HUME's performance using various self-supervised methods (e.g., DINO, VICReg) and analyzing the reasons behind performance differences

### Open Question 3
- Question: How does the number of classes (K) affect HUME's performance, and how can HUME be adapted to handle datasets with unknown or varying numbers of classes?
- Basis in paper: The paper assumes the number of classes is known a priori, which is a common assumption in existing unsupervised learning approaches
- Why unresolved: The paper does not explore the impact of varying the number of classes or investigate methods for estimating the number of classes in a dataset
- What evidence would resolve it: Experiments evaluating HUME's performance on datasets with varying numbers of classes and exploring methods for estimating the number of classes, such as deep clustering approaches

## Limitations
- Heavy dependency on sufficiently strong pretrained representations for the linear separability property to hold
- Computational cost is high due to multiple independent runs with inner optimization loops
- Requires knowing the number of classes (K) in advance, limiting applicability to truly unsupervised settings

## Confidence

**High confidence**: The core claim that human-labeled tasks are linearly separable in sufficiently strong representation spaces, supported by extensive empirical evidence across multiple datasets and representation models.

**Medium confidence**: The effectiveness of aggregation over multiple runs to stabilize predictions, though theoretical justification for why different runs capture complementary information is limited.

**Low confidence**: The claim that HUME can discover entirely new human-labeled tasks that were not part of the original dataset annotations, as the framework optimizes for known K classes.

## Next Checks

1. **Cross-domain robustness test**: Apply HUME to a dataset from a completely different domain (e.g., medical imaging or satellite imagery) where pretrained representations may not be as strong, to test the limits of the linear separability assumption.

2. **Correlation stability analysis**: Systematically vary the strength of one representation space (e.g., by reducing its dimensionality or training it on noisy data) and measure how the correlation between HUME's objective and ground truth labeling degrades.

3. **Scalability benchmark**: Compare HUME's computational cost and clustering performance against simpler methods like K-means and modern contrastive learning approaches on increasingly large datasets to establish practical limitations.