---
ver: rpa2
title: 'Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks'
arxiv_id: '2311.01038'
source_url: https://arxiv.org/abs/2311.01038
tags:
- graph
- pre-training
- graphs
- data
- micro
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies the "curse of big data" phenomenon in graph
  pre-training, where more training data does not necessarily lead to better downstream
  performance. To address this, the authors propose a data-active graph pre-training
  (APT) framework that consists of a graph selector and a pre-training model.
---

# Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks

## Quick Facts
- arXiv ID: 2311.01038
- Source URL: https://arxiv.org/abs/2311.01038
- Reference count: 40
- Key outcome: The paper proposes a data-active graph pre-training (APT) framework that achieves superior performance on node and graph classification tasks compared to state-of-the-art models while using fewer training samples and being 2.2× faster in training time.

## Executive Summary
This paper addresses the "curse of big data" phenomenon in graph pre-training, where more training data does not necessarily lead to better downstream performance. The authors propose a data-active graph pre-training (APT) framework consisting of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on predictive uncertainty and graph properties. The pre-training model then learns from the chosen data in a progressive and iterative way. Experimental results show that APT achieves superior performance on node and graph classification tasks compared to state-of-the-art graph pre-training models, while using fewer training samples and being 2.2× faster in training time.

## Method Summary
The paper proposes a data-active graph pre-training (APT) framework to address the "curse of big data" in graph pre-training. The framework consists of two main components: a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on predictive uncertainty and graph properties. The pre-training model then learns from the chosen data in a progressive and iterative way, with a proximal term regularization to prevent catastrophic forgetting. The APT framework is evaluated on node and graph classification tasks, and experimental results show that it achieves superior performance compared to state-of-the-art graph pre-training models while using fewer training samples and being 2.2× faster in training time.

## Key Results
- APT achieves superior performance on node and graph classification tasks compared to state-of-the-art graph pre-training models
- APT uses fewer training samples than baseline models while achieving better performance
- APT is 2.2× faster in training time compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "curse of big data" phenomenon in graph pre-training occurs because simply increasing the number of training samples or graph datasets does not guarantee better downstream performance.
- Mechanism: The model's ability to generalize is limited by the quality and representativeness of the data, not just the quantity. Randomly adding more data can introduce noise and reduce the model's focus on key structural patterns.
- Core assumption: The downstream task's performance is more dependent on the model's understanding of transferable structural patterns than on the sheer volume of training data.
- Evidence anchors:
  - [abstract] identifies the curse of big data phenomenon in graph pre-training
  - [section] provides experimental evidence showing scaling up sample size or graph datasets does not consistently improve performance
  - [corpus] does not directly support this mechanism
- Break condition: If the model's performance consistently improves with more data, regardless of quality or diversity, this mechanism would be invalidated.

### Mechanism 2
- Claim: The graph selector component effectively identifies the most instructive data points by considering both predictive uncertainty and graph properties.
- Mechanism: Predictive uncertainty measures the model's confidence in its predictions for a given data point, while graph properties (like network entropy, density, etc.) capture the inherent informativeness of the graph. By combining these two criteria, the graph selector can choose data points that are both uncertain for the current model and representative of the overall data distribution.
- Core assumption: The combination of predictive uncertainty and graph properties is a reliable indicator of a data point's suitability for pre-training.
- Evidence anchors:
  - [abstract] describes the graph selector's role in choosing data based on predictive uncertainty and graph properties
  - [section] provides details on the graph selector's criteria and their theoretical justification
  - [corpus] does not directly support this mechanism
- Break condition: If the graph selector consistently chooses data points that do not lead to improved downstream performance, this mechanism would be invalidated.

### Mechanism 3
- Claim: The progressive and iterative pre-training approach, with the proximal term regularization, prevents catastrophic forgetting and allows the model to effectively learn from new data while retaining knowledge from previous data.
- Mechanism: By training on data points sequentially and adding a proximal term to the loss function, the model is encouraged to remember knowledge from previous data while learning new patterns. This prevents the model from overwriting previous knowledge when learning new data.
- Core assumption: The proximal term regularization is effective in preventing catastrophic forgetting in the sequential training setting.
- Evidence anchors:
  - [abstract] mentions the pre-training model's ability to learn from new data while remembering previous knowledge
  - [section] provides details on the loss function with the proximal term
  - [corpus] does not directly support this mechanism
- Break condition: If the model consistently forgets previous knowledge when learning new data, despite the proximal term, this mechanism would be invalidated.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their application to graph pre-training
  - Why needed here: The paper is about improving the pre-training of GNNs, so a solid understanding of GNNs and their pre-training process is essential.
  - Quick check question: What is the main difference between supervised and self-supervised pre-training in the context of GNNs?

- Concept: Transfer learning and its application to graph data
  - Why needed here: The paper aims to improve the transferability of knowledge learned during pre-training to downstream tasks, so understanding transfer learning concepts is crucial.
  - Quick check question: How does transfer learning differ when applied to graph data compared to other data types like images or text?

- Concept: Uncertainty quantification in machine learning models
  - Why needed here: The paper uses predictive uncertainty as a criterion for data selection, so understanding how to quantify and interpret uncertainty in ML models is important.
  - Quick check question: What are the main approaches to quantifying uncertainty in deep learning models?

## Architecture Onboarding

- Component map: Graph selector -> Pre-training model -> Downstream task evaluation
- Critical path: Graph selection → Data pre-processing → Progressive pre-training with proximal regularization → Downstream task evaluation
- Design tradeoffs: Balancing the weight between predictive uncertainty and graph properties, choosing the appropriate proximal term strength, and deciding the number of iterations for each graph.
- Failure signatures: Poor downstream performance despite extensive pre-training, high variance in performance across different downstream tasks, and long training times without significant improvements.
- First 3 experiments:
  1. Replicate the experiments showing the curse of big data phenomenon with GCC and GraphCL models.
  2. Implement and test the graph selector component with a simple pre-training model to validate its effectiveness in choosing informative data points.
  3. Integrate the graph selector with the pre-training model and test the full APT framework on a small-scale downstream task to ensure the components work together as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the predictive uncertainty (InfoNCE loss) and the downstream cross-entropy loss, and under what conditions does this relationship hold?
- Basis in paper: [explicit] Theorem 1 provides a theoretical connection, but the proof relies on assumptions about uniform label distribution and specific graph encoder properties
- Why unresolved: The paper presents a bound rather than an exact equality, and the conditions for the bound may not always be met in practice
- What evidence would resolve it: Empirical studies showing the correlation between InfoNCE loss and downstream performance across various graph types and datasets

### Open Question 2
- Question: How does the choice of graph properties in the selector affect the final model performance, and can we determine an optimal combination of properties for different downstream tasks?
- Basis in paper: [inferred] The paper uses five properties (network entropy, density, average degree, degree variance, scale-free exponent) but doesn't explore other possible properties or optimal combinations
- Why unresolved: The selection of properties is based on intuition and preliminary experiments, but there's no systematic study of the space of possible properties or their combinations
- What evidence would resolve it: A comprehensive ablation study testing various combinations of graph properties and their impact on different downstream tasks

### Open Question 3
- Question: How does the time-adaptive parameter γ_t in the graph selector affect the model's performance, and is there a better way to adapt this parameter during training?
- Basis in paper: [explicit] The paper uses a Beta distribution for γ_t that decreases over time, but doesn't explore alternative adaptation strategies or justify this choice
- Why unresolved: The choice of decay function and its parameters appears to be heuristic, and there's no comparison with alternative strategies
- What evidence would resolve it: Experiments comparing different adaptation strategies (linear decay, step decay, etc.) and their impact on downstream performance across various tasks and datasets

## Limitations
- The lack of empirical validation of the individual components of the APT framework
- Limited evaluation on larger, more complex graph datasets
- No extensive ablation study to quantify the contribution of each component to the overall performance

## Confidence

- **High Confidence**: The identification of the "curse of big data" phenomenon in graph pre-training is well-supported by the experimental evidence presented in the paper.
- **Medium Confidence**: The proposed mechanisms for how the graph selector and progressive pre-training approach address the curse of big data are theoretically sound, but the empirical evidence for their effectiveness is limited to the specific experimental setup used in the paper.
- **Low Confidence**: The generalizability of the APT framework to other graph pre-training tasks and larger-scale datasets is uncertain, as the paper does not provide extensive validation beyond the specific experimental setup.

## Next Checks
1. Conduct an ablation study to quantify the individual contributions of the graph selector and progressive pre-training components to the overall performance of the APT framework.
2. Evaluate the APT framework on larger, more complex graph datasets to assess its scalability and effectiveness in real-world scenarios.
3. Compare the APT framework against other state-of-the-art graph pre-training methods on a wider range of downstream tasks to validate its generalizability and competitiveness.