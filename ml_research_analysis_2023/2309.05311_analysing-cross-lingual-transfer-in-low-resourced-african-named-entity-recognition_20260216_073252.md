---
ver: rpa2
title: Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity Recognition
arxiv_id: '2309.05311'
source_url: https://arxiv.org/abs/2309.05311
tags:
- language
- transfer
- data
- languages
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies cross-lingual transfer in low-resourced African
  named entity recognition. The authors investigate how much adaptive fine-tuning
  and the choice of transfer language affect zero-shot transfer performance.
---

# Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity Recognition

## Quick Facts
- **arXiv ID**: 2309.05311
- **Source URL**: https://arxiv.org/abs/2309.05311
- **Reference count**: 40
- **Key outcome**: Data overlap between source and target languages is a better predictor of transfer performance than linguistic distance in low-resourced African NER.

## Executive Summary
This paper investigates cross-lingual transfer learning for Named Entity Recognition (NER) in ten low-resourced African languages. The authors examine how language-adaptive fine-tuning (LAFT) and choice of transfer language affect zero-shot transfer performance. They find that models performing well on individual languages often generalize poorly to others, while models with better cross-linguistic generalization suffer in single-language performance. Notably, the amount of token overlap between source and target datasets emerges as a stronger predictor of transfer success than either geographical or genetic distance between languages.

## Method Summary
The study uses pre-trained multilingual models (XLM-RoBERTa) fine-tuned on NER data from one language and evaluated on another (zero-shot transfer). Language-adaptive fine-tuning is applied using unlabelled monolingual data before NER fine-tuning. The MasakhaNER dataset provides NER data for ten African languages, while unlabelled data is used for LAFT. Transfer performance is measured using F1 score, with analysis of correlations between transfer performance and features like data overlap, linguistic distance, and dataset size.

## Key Results
- Models that excel on individual languages often generalize poorly to others, while models with better cross-linguistic generalization suffer in single-language performance
- Data overlap between source and target languages is a stronger predictor of transfer performance than geographical or genetic distance (Pearson's R = 0.73)
- Language-adaptive fine-tuning can improve target language performance but may reduce cross-linguistic generalization when using large datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive fine-tuning on the target language improves performance on that language.
- Mechanism: The pre-trained model adapts its representations to the orthography, script, and linguistic patterns of the target language, improving its ability to recognize named entities in that language.
- Core assumption: The language-adaptive fine-tuning data is sufficiently large and representative to adapt the model to the target language's characteristics.
- Evidence anchors:
  - [abstract]: "We find that adaptively fine-tuning a multilingual model on unlabelled monolingual data can improve performance on the target language"
  - [section]: "The best zero-shot transfer language generally performs well, obtaining 10-20 F1 lower than training on the target language"
  - [corpus]: Weak evidence - related papers discuss language adaptation but don't specifically address zero-shot transfer performance gains.
- Break condition: If the language-adaptive fine-tuning data is too small or not representative of the target language's characteristics, the adaptation may not be effective or could even harm performance.

### Mechanism 2
- Claim: Large language-adaptive fine-tuning datasets can diminish transfer performance.
- Mechanism: Overfitting to the language-adaptive fine-tuning data reduces the model's ability to generalize to other languages, decreasing transfer performance.
- Core assumption: The language-adaptive fine-tuning data is sufficiently large to cause overfitting.
- Evidence anchors:
  - [abstract]: "We find that models that perform well on a single language often do so at the expense of generalising to others"
  - [section]: "Furthermore, we find that when the source and target dataset contain many shared tokens, then transfer performance is generally higher"
  - [corpus]: Weak evidence - related papers discuss overfitting but don't specifically address the impact on cross-lingual transfer performance.
- Break condition: If the language-adaptive fine-tuning data is not large enough to cause overfitting, or if the model is regularized to prevent overfitting, this mechanism may not hold.

### Mechanism 3
- Claim: Data overlap between source and target languages is a strong predictor of transfer performance.
- Mechanism: The model can leverage its knowledge of named entities in the source language to recognize similar entities in the target language, especially if they share tokens.
- Core assumption: The overlapping tokens are meaningful and relevant to named entity recognition.
- Evidence anchors:
  - [abstract]: "Furthermore, the amount of data overlap between the source and target datasets is a better predictor of transfer performance than either the geographical or genetic distance between the languages"
  - [section]: "We see a strong correlation (Pearson's R = 0.73) between how many tokens overlap and the performance in Fig. 2b"
  - [corpus]: Weak evidence - related papers discuss data overlap but don't specifically address its correlation with transfer performance in NER.
- Break condition: If the overlapping tokens are not meaningful or relevant to named entity recognition, or if the model relies more on other linguistic features, this mechanism may not hold.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge can be transferred from one language to another is crucial for applying pre-trained models to low-resource languages.
  - Quick check question: What is the main advantage of using cross-lingual transfer learning for low-resource languages?

- Concept: Named entity recognition (NER)
  - Why needed here: NER is the task being investigated in this paper, so understanding its basics is essential for interpreting the results.
  - Quick check question: What are the main entity types typically recognized in NER tasks?

- Concept: Language-adaptive fine-tuning (LAFT)
  - Why needed here: LAFT is a key technique used in this paper to adapt pre-trained models to specific languages, so understanding its purpose and effects is important.
  - Quick check question: What is the main goal of language-adaptive fine-tuning in the context of cross-lingual transfer learning?

## Architecture Onboarding

- Component map: Pre-trained multilingual model -> Language-adaptive fine-tuning (optional) -> NER fine-tuning -> Evaluation

- Critical path:
  1. Load pre-trained multilingual model
  2. Perform LAFT on unlabelled monolingual data (optional)
  3. Fine-tune model on NER data from source language
  4. Evaluate model on NER data from target language

- Design tradeoffs:
  - LAFT can improve performance on the target language but may reduce transfer performance
  - Larger LAFT datasets can lead to better adaptation but also more overfitting
  - Data overlap between source and target languages is a strong predictor of transfer performance

- Failure signatures:
  - Poor transfer performance: Model may be overfitting to the source language or LAFT data
  - Inconsistent results across random seeds: Model may be sensitive to initialization or training data
  - Low performance on languages with different scripts: Model may struggle with script-specific features

- First 3 experiments:
  1. Fine-tune pre-trained model on NER data from one language and evaluate on another language (zero-shot transfer)
  2. Perform LAFT on the target language and then fine-tune on NER data from that language (adapted fine-tuning)
  3. Investigate the correlation between data overlap and transfer performance across language pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the findings about data overlap and transfer performance generalize to other NLP tasks beyond Named Entity Recognition, such as machine translation or sentiment analysis?
- Basis in paper: [inferred] The paper mentions that performance in other tasks, such as machine translation, may be less influenced by the amount of data overlap, but does not investigate this further.
- Why unresolved: The paper focuses exclusively on Named Entity Recognition, so there is no direct evidence on how data overlap affects transfer in other NLP tasks.
- What evidence would resolve it: Conducting similar experiments on different NLP tasks (e.g., machine translation, sentiment analysis) to compare the correlation between data overlap and transfer performance across tasks.

### Open Question 2
- Question: What are the underlying mechanisms that allow languages with no lexical overlap, like Amharic, to still exhibit some transfer performance?
- Basis in paper: [explicit] The paper notes that Amharic, due to its different script, has no lexical overlap with other languages but still displays some transfer performance, suggesting more intricate mechanisms are at play.
- Why unresolved: The paper does not delve into the specific mechanisms that enable transfer in the absence of lexical overlap.
- What evidence would resolve it: Analyzing the internal representations of the models to understand how they manage to transfer knowledge between languages with no lexical overlap.

### Open Question 3
- Question: How would the correlation between data overlap and transfer performance change if a more sophisticated strategy for identifying similar words between related languages were used, instead of exact token matching?
- Basis in paper: [explicit] The paper mentions that using a more sophisticated strategy than only counting overlapping words when they exactly match would be promising, potentially resulting in the identification of similar, but slightly different, words between related languages.
- Why unresolved: The paper uses exact token matching for overlap calculation and does not explore more sophisticated methods.
- What evidence would resolve it: Implementing and comparing different overlap calculation methods that account for similar words between related languages and analyzing their impact on transfer performance correlation.

## Limitations

- Small dataset sizes (500-1000 sentences per language) may amplify random seed variation and make it difficult to distinguish true linguistic effects from sampling noise
- Lack of precise specification for unlabelled data sources and sizes used in language-adaptive fine-tuning makes exact replication challenging
- Focus on African languages may limit generalizability to other language families or high-resource scenarios

## Confidence

**High Confidence**: The observation that zero-shot transfer performance varies significantly across language pairs and that LAFT can improve target language performance while potentially harming cross-linguistic generalization.

**Medium Confidence**: The claim that data overlap is a better predictor of transfer performance than linguistic distance, though the underlying mechanisms require further investigation.

**Low Confidence**: The specific numerical thresholds and performance ranges reported, particularly given the high variance across random seeds and the limited dataset sizes.

## Next Checks

1. **Replication with Controlled LAFT Data**: Replicate the experiments using LAFT datasets of varying sizes (small: 10K tokens, medium: 100K tokens, large: 1M+ tokens) to systematically measure the point at which overfitting begins to harm transfer performance.

2. **Fine-grained Overlap Analysis**: Conduct a detailed analysis of which types of token overlap (proper nouns, common nouns, named entities, etc.) correlate most strongly with transfer performance.

3. **Cross-family Transfer Experiments**: Extend the study to include language pairs from different families (e.g., African-European transfers) to test whether the data overlap correlation holds beyond the primarily African language pairs studied.