---
ver: rpa2
title: Analysis of the expected $L_2$ error of an over-parametrized deep neural network
  estimate learned by gradient descent without regularization
arxiv_id: '2311.14609'
source_url: https://arxiv.org/abs/2311.14609
tags:
- proof
- neural
- page
- lemma
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the statistical properties of deep neural networks
  trained by gradient descent without a regularization term. The authors show that
  over-parameterized deep neural networks can achieve universal consistency and optimal
  rates of convergence for nonparametric regression without the need for regularization.
---

# Analysis of the expected $L_2$ error of an over-parametrized deep neural network estimate learned by gradient descent without regularization

## Quick Facts
- arXiv ID: 2311.14609
- Source URL: https://arxiv.org/abs/2311.14609
- Reference count: 40
- Key outcome: Over-parameterized deep neural networks achieve universal consistency and optimal rates of convergence for nonparametric regression without explicit regularization

## Executive Summary
This paper establishes theoretical guarantees for deep neural networks trained by gradient descent without regularization terms. The authors prove that over-parameterization enables implicit regularization, allowing networks to achieve universal consistency and optimal convergence rates for nonparametric regression. Specifically, they show that for appropriate initialization, number of gradient steps, and step size, the expected L2 error converges to zero with rate approximately n^{-1/(1+d)} for Hölder smooth regression functions, and a dimension-independent rate for interaction models.

## Method Summary
The method involves learning a regression function using a linear combination of Kn fully connected neural networks, each with L layers and r neurons per layer. The networks are trained using gradient descent on the empirical L2 risk without any explicit regularization term. The weights are initialized with specific distributions and bounds, and the final estimate is obtained by truncating the output of the trained network. The authors analyze the expected L2 error using auxiliary results on gradient descent convergence, metric entropy bounds, and approximation properties of the network architecture.

## Key Results
- Over-parameterized deep neural networks can achieve universal consistency for nonparametric regression without explicit regularization
- The expected L2 error converges to zero at rate n^{-1/(1+d)} for Hölder smooth regression functions
- A dimension-independent rate of convergence is derived for interaction models where the regression function is a sum of smooth functions depending on at most d* input components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-parameterization enables implicit regularization that leads to good generalization without explicit regularization terms.
- Mechanism: The large number of parameters allows gradient descent to find interpolating solutions that implicitly impose a regularization effect, avoiding overfitting while maintaining low L2 error.
- Core assumption: The network is sufficiently over-parameterized (Kn >> nr+2) and the initialization is appropriately chosen.
- Evidence anchors:
  - [abstract]: "over-parameterization enables the network to implicitly impose a regularization that leads to good generalization, even without explicit regularization in the empirical risk."
  - [section 1.3]: "This over-parametrization leads to benign overfitting (Bartlett, Montanari and Rakhlin (2021))."
  - [corpus]: Weak/no direct evidence - mentions related work but not this specific implicit regularization claim.
- Break condition: If Kn is not sufficiently large relative to n, or if the initialization does not have the required properties, the implicit regularization effect may not occur.

### Mechanism 2
- Claim: The combination of convexity of the empirical L2 risk in the outer weights and Lipschitz continuity of the gradient allows gradient descent to converge to a good solution.
- Mechanism: By analyzing the optimization error using both convexity and Lipschitz properties, the authors show that gradient descent can find a global minimum of the empirical L2 risk without needing a regularization term.
- Core assumption: The empirical L2 risk is convex in the outer weights and has a Lipschitz continuous gradient.
- Evidence anchors:
  - [section 1.3]: "In our work, we combine these two techniques (cf. Lemma 1 below)." (referring to combining convexity and Lipschitz continuity approaches)
  - [section 4.1]: "In this section we will present auxiliary results that are necessary for the proof of Theorem 1. The first auxiliary result enables us to analyze the gradient descent."
  - [corpus]: Weak/no direct evidence - mentions related work but not this specific combination approach.
- Break condition: If the empirical L2 risk is not convex in the outer weights, or if the gradient is not Lipschitz continuous, this convergence guarantee may fail.

### Mechanism 3
- Claim: The metric entropy bound controls the complexity of the set of over-parameterized deep neural networks, enabling generalization bounds.
- Mechanism: By bounding the metric entropy of the function class, the authors can control the generalization error and show that the expected L2 error converges to zero at the desired rate.
- Core assumption: The activation function is sufficiently smooth (k-times differentiable with bounded derivatives) and the weight bounds are appropriately chosen.
- Evidence anchors:
  - [section 4.1]: "The next auxiliary result uses a metric entropy bound to control the complexity of a set of over-parametrized deep neural networks."
  - [section 4.1]: "Lemma 4. Let α ≥ 1, β > 0 and let A, B, C ≥ 1. Let σ : R → R be k-times differentiable such that all derivatives up to order k are bounded on R."
  - [corpus]: Weak/no direct evidence - mentions related work but not this specific metric entropy approach.
- Break condition: If the activation function is not sufficiently smooth, or if the weight bounds are not appropriately chosen, the metric entropy bound may not hold.

## Foundational Learning

- Concept: Nonparametric regression and the curse of dimensionality
  - Why needed here: The paper analyzes the performance of deep neural networks in nonparametric regression, where the goal is to estimate a regression function from data without assuming a specific parametric form. The curse of dimensionality is a key challenge in this setting.
  - Quick check question: What is the optimal minimax rate of convergence for (p, C)-smooth functions in nonparametric regression, and how does it depend on the input dimension d?

- Concept: Over-parameterization and its effects
  - Why needed here: The paper focuses on over-parameterized deep neural networks, which have more parameters than the sample size. Understanding the effects of over-parameterization, including the double descent phenomenon and implicit regularization, is crucial for analyzing the results.
  - Quick check question: What is the relationship between over-parameterization, the double descent curve, and the ability of neural networks to generalize well?

- Concept: Gradient descent and its convergence properties
  - Why needed here: The paper analyzes the performance of deep neural networks learned by gradient descent, without explicit regularization. Understanding the convergence properties of gradient descent, including its ability to find global minima and the role of convexity and Lipschitz continuity, is essential for the analysis.
  - Quick check question: How do the convexity of the empirical L2 risk in the outer weights and the Lipschitz continuity of the gradient contribute to the convergence of gradient descent to a good solution?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden layers (L layers with r neurons each) -> Output layer
  - Input layer: Random vector (X,Y) with X in R^d and Y in R
  - Hidden layers: L layers with r neurons per layer, using logistic squasher activation function
  - Output layer: Linear combination of K_n fully connected neural networks
  - Weights: Initialized with specific distributions and bounds, updated via gradient descent

- Critical path:
  1. Initialize the weights of the neural network according to the specified distributions and bounds
  2. Compute the empirical L2 risk on the training data
  3. Update the weights via gradient descent for t_n steps with step size 1/t_n
  4. Truncate the output of the network to obtain the final estimate
  5. Analyze the expected L2 error of the estimate using the auxiliary results

- Design tradeoffs:
  - Over-parameterization vs. generalization: While over-parameterization enables implicit regularization, too much over-parameterization may lead to overfitting or computational challenges
  - Choice of activation function: The logistic squasher is used in this paper, but other activation functions may have different effects on the convergence and generalization properties
  - Number of gradient steps and step size: The choice of t_n and the step size 1/t_n is crucial for the convergence of gradient descent and the final error bound

- Failure signatures:
  - If the initialization does not satisfy the required properties (e.g., if the weights are not bounded appropriately), the implicit regularization effect may not occur
  - If the network is not sufficiently over-parameterized (e.g., if Kn is not large enough relative to n), the convergence guarantees may fail
  - If the activation function is not sufficiently smooth, or if the weight bounds are not appropriately chosen, the metric entropy bound may not hold

- First 3 experiments:
  1. Implement the neural network architecture and initialization as specified in the paper, and verify that the weights satisfy the required properties
  2. Compute the empirical L2 risk on synthetic data generated from a (p, C)-smooth regression function, and verify that the risk decreases with the number of gradient steps
  3. Analyze the expected L2 error of the estimate on synthetic data, and verify that it converges to zero at the desired rate (n^{-1/(1+d)}) for different values of p and d

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed deep neural network architecture without explicit regularization achieve optimal rates of convergence for all Hölder smoothness exponents p > 1/2?
- Basis in paper: [inferred] The paper derives rates close to n^{-1/(1+d)} for p in [1/2, 1], but notes these are not optimal for p > 1/2 compared to the minimax rate n^{-2p/(2p+d)}.
- Why unresolved: The proof technique relies on specific bounds for the approximation error that may not be tight for higher smoothness.
- What evidence would resolve it: Derive tighter approximation bounds for higher smoothness functions or construct counterexamples showing the derived rate is optimal for the proposed architecture.

### Open Question 2
- Question: Can the dimension-independent rate of convergence for interaction models be extended to more general compositional models beyond sums of functions depending on at most d* components?
- Basis in paper: [inferred] The paper only analyzes interaction models where the regression function is a sum of smooth functions on subsets of input components.
- Why unresolved: The proof leverages specific properties of interaction models that may not hold for general compositional structures.
- What evidence would resolve it: Analyze convergence rates for other compositional models like compositions of functions or sums of products of functions.

### Open Question 3
- Question: How does the choice of initialization scheme and network architecture (e.g. depth, width) impact the statistical performance of deep neural networks trained by gradient descent without regularization?
- Basis in paper: [explicit] The paper assumes a specific initialization scheme with carefully chosen bounds on weights.
- Why unresolved: The analysis is tailored to this specific initialization and architecture. The impact of other choices is unclear.
- What evidence would resolve it: Analyze statistical performance under different initialization schemes and architectures, possibly with matching or information-theoretic lower bounds.

## Limitations
- The analysis relies heavily on specific initialization schemes and over-parameterization conditions that may be difficult to verify in practice
- The theoretical bounds assume idealized conditions including bounded inputs and specific smoothness assumptions on the regression function
- The requirement that Kn >> nr+2 represents a strong assumption about the degree of over-parameterization needed

## Confidence
- **High confidence**: The mechanism showing that gradient descent converges to a global minimum of the empirical L2 risk when the risk is convex in outer weights
- **Medium confidence**: The implicit regularization effect through over-parameterization, as this relies on idealized conditions and specific initialization schemes
- **Medium confidence**: The metric entropy bounds controlling function class complexity, as these depend on specific smoothness assumptions

## Next Checks
1. **Numerical verification of initialization requirements**: Implement the specified initialization scheme and empirically verify that the weight distributions satisfy the theoretical bounds across different random seeds and problem instances.

2. **Empirical testing of over-parameterization threshold**: Systematically vary Kn relative to n to identify the minimum degree of over-parameterization needed to observe the implicit regularization effect and compare against theoretical predictions.

3. **Robustness to activation function choice**: Repeat the theoretical analysis with different activation functions (e.g., ReLU, tanh) to assess whether the logistic squasher is essential for the convergence guarantees or if the results extend to more commonly used alternatives.