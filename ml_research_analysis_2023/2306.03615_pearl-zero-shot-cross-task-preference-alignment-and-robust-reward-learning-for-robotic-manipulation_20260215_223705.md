---
ver: rpa2
title: 'PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning
  for Robotic Manipulation'
arxiv_id: '2306.03615'
source_url: https://arxiv.org/abs/2306.03615
tags:
- labels
- tasks
- preference
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PEARL, a method for zero-shot cross-task preference
  transfer in reinforcement learning. It addresses the problem of needing many human
  preference labels for each new task.
---

# PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation

## Quick Facts
- arXiv ID: 2306.03615
- Source URL: https://arxiv.org/abs/2306.03615
- Authors: 
- Reference count: 40
- Key outcome: PEARL achieves near-oracle performance (99.3% of full preference baseline) on robotic manipulation tasks using only 20% scripted labels through zero-shot cross-task preference transfer.

## Executive Summary
PEARL addresses the challenge of needing many human preference labels for each new task in preference-based reinforcement learning by introducing a zero-shot cross-task transfer approach. The method uses Gromov-Wasserstein distance to align trajectory distributions between source and target tasks, then transfers preference labels through optimal transport. A Robust Preference Transformer models rewards as Gaussian distributions to handle noisy transferred labels. Experiments on Meta-World and Robomimic demonstrate that PEARL significantly outperforms existing methods when few human preferences are available, achieving 99.3% of oracle performance with just 20% scripted labels.

## Method Summary
PEARL introduces a zero-shot preference-based RL approach that transfers preferences between tasks without requiring human labels for the target task. The method aligns trajectory distributions between source and target tasks using Gromov-Wasserstein distance, computes correspondence via optimal transport matrix, and transfers preference labels accordingly. A Robust Preference Transformer (RPT) models rewards as Gaussian distributions with mean and variance to handle noisy transferred labels. The trained reward model is used with offline RL (IQL) to learn policies from relabeled transitions. The approach requires only preference data from source tasks and offline trajectory data from the target task.

## Key Results
- PEARL achieves 99.3% of oracle performance (full preference baseline) using only 20% scripted labels
- Zero-shot transfer eliminates need for target task preference labels
- RPT with distributional reward modeling shows superior robustness to noisy transferred labels compared to standard PT
- Effective across multiple robotic manipulation tasks in Meta-World and Robomimic benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport alignment transfers preference labels across tasks by finding trajectory correspondences.
- Mechanism: Gromov-Wasserstein distance aligns trajectory distributions between source and target tasks, producing an optimal transport matrix that serves as correspondence between trajectories. Preference labels for target task trajectory pairs are computed as weighted sums of source task labels using this correspondence.
- Core assumption: Trajectory similarity measured by Gromov-Wasserstein distance reliably indicates preference correspondence between tasks.
- Evidence anchors:
  - [abstract]: "Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels."
  - [section]: "POT aims to identify the correspondence between two sets of trajectories and transfer the preferences based on accordingly... Each element, Tij, indicates the probability that trajectory xi matches trajectory yj... the preference labels of trajectory pairs of the target task are computed based on trajectory correspondence by (6)."
  - [corpus]: No direct evidence found; this is a novel application of optimal transport to preference transfer.
- Break condition: If trajectory distributions are too dissimilar between tasks, the alignment will be poor and transferred labels will be inaccurate.

### Mechanism 2
- Claim: Modeling rewards as Gaussian distributions with uncertainty improves robustness to noisy transferred labels.
- Mechanism: The Robust Preference Transformer (RPT) models rewards as Gaussian distributions with mean and variance. During training, sampled rewards from this distribution are used alongside the mean, and a regularization term prevents variance collapse. This distributional approach makes learning less sensitive to individual noisy labels.
- Core assumption: Reward uncertainty captured by variance provides meaningful signal for handling label noise.
- Evidence anchors:
  - [abstract]: "To this end, we introduce Robust Preference Transformer, which models the rewards as Gaussian distributions and incorporates reward uncertainty in addition to reward mean."
  - [section]: "We model the rewards as Gaussian distributions, where the mean represents the estimated reward and the variance signifies the reward uncertainty... the sampled rewards with large variance will make the second term of (7) a large value... we introduce a regularization loss to force the uncertainty level to maintain a level η."
  - [corpus]: No direct evidence found; this is a novel distributional approach for preference-based RL.
- Break condition: If uncertainty estimates are poorly calibrated, the robustness benefits may be limited or could introduce new failure modes.

### Mechanism 3
- Claim: Zero-shot transfer eliminates the need for target task preference labels by leveraging source task data.
- Mechanism: The method only requires preference data from source tasks and offline trajectory data from the target task. It computes preference labels for the target task through optimal transport alignment, enabling policy learning without any human queries for the target task.
- Core assumption: Sufficient similarity exists between source and target tasks to enable meaningful preference transfer.
- Evidence anchors:
  - [abstract]: "which learns policies from cross-task preference transfer without any human labels of the target task."
  - [section]: "POT is the first zero-shot preference-based RL approach that utilizes a small amount of preference data from similar tasks to infer pseudo labels via Optimal Transport... no preference labels from the target task."
  - [corpus]: No direct evidence found; this zero-shot transfer capability appears novel.
- Break condition: If source and target tasks are too dissimilar, transferred labels will be inaccurate and performance will degrade significantly.

## Foundational Learning

- Concept: Gromov-Wasserstein distance and optimal transport
  - Why needed here: Provides the mathematical framework for aligning trajectory distributions across different tasks and computing correspondences between trajectory pairs.
  - Quick check question: What distinguishes Gromov-Wasserstein distance from standard Wasserstein distance, and why is this distinction important for cross-task alignment?

- Concept: Distributional reward modeling
  - Why needed here: Enables robust learning from potentially noisy transferred labels by incorporating reward uncertainty into the modeling process.
  - Quick check question: How does modeling rewards as Gaussian distributions differ from traditional scalar reward modeling, and what specific advantage does this provide in the presence of label noise?

- Concept: Preference-based reinforcement learning fundamentals
  - Why needed here: Understanding how preference predictors work and how reward functions are learned from preference data is essential for implementing and debugging the system.
  - Quick check question: How does the Bradley-Terry model formulation in (1) relate to the cross-entropy loss in (2), and why is this relationship important for reward learning?

## Architecture Onboarding

- Component map: Offline trajectory data -> Optimal transport alignment -> POT label computation -> RPT training -> Reward relabeling -> IQL policy learning -> Evaluation
- Critical path: Offline trajectory data → Optimal transport alignment → POT label computation → RPT training → Reward relabeling → IQL policy learning → Evaluation
- Design tradeoffs:
  - Computational cost of optimal transport vs. accuracy of preference transfer
  - Complexity of distributional modeling vs. robustness to noise
  - Number of source task labels vs. quality of transferred labels
  - Trade-off between POT labels and scripted labels for final performance
- Failure signatures:
  - Poor optimal transport alignment indicated by low POT accuracy scores
  - Variance collapse in RPT indicated by reward variances approaching zero
  - Policy performance not improving despite successful training steps
  - High variance in success rates across different random seeds
- First 3 experiments:
  1. Run POT label computation with synthetic data where ground truth correspondences are known to verify the alignment accuracy
  2. Test RPT with controlled noise levels added to labels to measure robustness improvement over baseline PT
  3. Evaluate transfer performance across tasks with varying similarity to identify the similarity threshold for effective transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does POT perform on visual-based tasks compared to state-based tasks, given that the method may struggle with high-dimensional inputs?
- Basis in paper: [explicit] The paper mentions that POT is not well-suited for visual inputs due to potential slower processing speeds with high-dimensional inputs in Optimal Transport.
- Why unresolved: The paper does not provide experimental results or comparisons between visual-based and state-based tasks, leaving the effectiveness of POT on visual inputs unclear.
- What evidence would resolve it: Experiments comparing POT's performance on visual-based tasks versus state-based tasks, including success rates and computational efficiency, would provide insight into its effectiveness on high-dimensional inputs.

### Open Question 2
- Question: How sensitive is POT to the similarity between source and target tasks, and what is the impact on performance when tasks are less similar?
- Basis in paper: [explicit] The paper states that the efficiency of POT relies on the similarity between source and target tasks.
- Why unresolved: The paper does not quantify the impact of task similarity on POT's performance or provide a threshold for when the method becomes ineffective.
- What evidence would resolve it: Experiments varying the similarity between source and target tasks, measuring success rates and accuracy of transferred preferences, would clarify the method's sensitivity to task similarity.

### Open Question 3
- Question: What are the long-term effects of using RPT with noisy labels on policy performance and reward function accuracy?
- Basis in paper: [explicit] The paper introduces RPT to handle noisy labels by modeling rewards as Gaussian distributions, but does not explore long-term effects.
- Why unresolved: While the paper demonstrates robustness to label noise in short-term experiments, it does not investigate the cumulative impact of noisy labels over extended training periods.
- What evidence would resolve it: Longitudinal studies tracking policy performance and reward accuracy over many training iterations with varying noise levels would reveal the long-term effects of using RPT with noisy labels.

## Limitations
- Performance heavily depends on similarity between source and target tasks, with unclear thresholds for effective transfer
- Computational cost of optimal transport may limit scalability to very large datasets or high-dimensional inputs
- Method relies on availability of similar source tasks with preference data, which may not always be available

## Confidence
- High confidence: The effectiveness of distributional reward modeling for handling noisy labels is well-supported by the mathematical formulation and ablation studies showing RPT outperforming standard PT.
- Medium confidence: The zero-shot transfer capability shows strong empirical results but relies on the critical assumption that Gromov-Wasserstein distance reliably captures preference correspondence across tasks.
- Medium confidence: The computational efficiency claims are reasonable given the offline nature of the approach, though actual implementation details may affect practical runtime.

## Next Checks
1. Conduct systematic ablation studies varying task similarity (using metrics like state-action distribution divergence) to quantify the transfer performance degradation as tasks become less related.
2. Implement and test the zero-shot transfer pipeline end-to-end with synthetic datasets where ground truth correspondences are known to validate the alignment accuracy independently of RL performance.
3. Measure the impact of reward distribution entropy regularization by comparing performance with different entropy thresholds to determine optimal uncertainty modeling.