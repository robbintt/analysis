---
ver: rpa2
title: Simple Contrastive Representation Learning for Time Series Forecasting
arxiv_id: '2303.18205'
source_url: https://arxiv.org/abs/2303.18205
tags:
- time
- series
- learning
- forecasting
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SimTS, a simple representation learning approach
  for time series forecasting that learns to predict future states from historical
  contexts in latent space. Unlike existing contrastive methods that rely on negative
  pairs or specific time series characteristics, SimTS exclusively uses positive pairs
  and achieves competitive performance across multiple benchmark datasets.
---

# Simple Contrastive Representation Learning for Time Series Forecasting

## Quick Facts
- arXiv ID: 2303.18205
- Source URL: https://arxiv.org/abs/2303.18205
- Reference count: 40
- Outperforms state-of-the-art contrastive learning approaches by 11-37% in MSE and 7-16% in MAE

## Executive Summary
This paper introduces SimTS, a simple contrastive learning approach for time series forecasting that learns to predict future states from historical contexts in latent space. Unlike existing contrastive methods that rely on negative pairs or specific time series characteristics, SimTS exclusively uses positive pairs and achieves competitive performance across multiple benchmark datasets. The method demonstrates that negative pairs can degrade performance and that stop-gradient operations are crucial for optimal results, showing robust performance across diverse time series types without requiring assumptions about seasonality or trend components.

## Method Summary
SimTS is a contrastive representation learning method for time series forecasting that learns to predict future representations from historical ones in latent space. The model uses a siamese encoder to map both history and future segments to latent space, then applies a predictor network to generate future representations from historical latent vectors. The loss function only considers the similarity between predicted and actual future representations (positive pairs), avoiding negative pair construction entirely. A key innovation is the use of stop-gradient operations applied to the future encoding path, preventing the encoder from collapsing while avoiding false repulsion from negative pairs. The method employs a multi-scale convolutional encoder to capture both local and global temporal patterns essential for forecasting.

## Key Results
- Achieves 11-37% lower mean squared error and 7-16% lower mean absolute error compared to state-of-the-art contrastive learning methods
- Outperforms baseline methods across diverse datasets including ETTh1, ETTh2, ETTm1, ETTm2, Exchange, and Weather
- Demonstrates that negative pairs degrade performance in most cases while stop-gradient operations are crucial for optimal results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SimTS works by learning to predict future representations from historical ones in latent space, which directly aligns with the forecasting objective rather than instance discrimination.
- Mechanism: The model uses a siamese encoder to map both history and future segments to latent space, then applies a predictor network to generate future representations from historical latent vectors. The loss function only considers the similarity between predicted and actual future representations (positive pairs), avoiding negative pair construction entirely.
- Core assumption: The temporal dependencies between past and future segments are more informative for forecasting than discriminative features between different time series instances.
- Evidence anchors: [abstract]: "SimTS exclusively uses positive pairs and does not depend on negative pairs or specific characteristics of a given time series"; [section 3.2]: "we learn a representation such that the latent representation of the future time windows can be predicted from the latent representation of the history time windows"; [corpus]: Weak - only 5 related papers found, none directly discussing this specific mechanism

### Mechanism 2
- Claim: Stop-gradient operations prevent the encoder from collapsing while avoiding false repulsion from negative pairs.
- Mechanism: The stop-gradient is applied to the future encoding path, meaning the encoder cannot receive updates from future representations. This forces the encoder to learn representations that are predictive of the future without being influenced by the future itself.
- Core assumption: The encoder should be constrained to optimize only the history representation and its prediction, not the future representation.
- Evidence anchors: [section 3.4]: "Due to the stop-gradient operation on Zf, our encoder cannot receive updates from future representations Zf and is constrained to only optimize the history representation"; [section 5.3]: Shows ablation study demonstrating that removing stop-gradient or moving it to history path significantly degrades performance; [corpus]: Weak - no direct discussion of stop-gradient in time series forecasting context

### Mechanism 3
- Claim: Multi-scale convolutional encoders capture both local and global temporal patterns essential for forecasting.
- Mechanism: The encoder uses parallel convolutional layers with different kernel sizes (2^i for i in 0 to m) to extract patterns at multiple temporal scales simultaneously, then averages these representations.
- Core assumption: Time series forecasting benefits from capturing both short-term motifs and long-term dependencies through multi-scale feature extraction.
- Evidence anchors: [section 3.3]: "we propose to use a convolutional network with multiple filters that have various kernel sizes, which can extract both global and local patterns"; [section 5.1]: Table 2 shows that the multi-scale convolutional encoder outperforms TCN and LSTM architectures; [corpus]: Weak - no direct comparison of multi-scale convolution approaches in the found corpus

## Foundational Learning

- Concept: Temporal dependency learning
  - Why needed here: Time series forecasting fundamentally requires understanding how past patterns influence future outcomes, which is the core learning objective
  - Quick check question: Can you explain why predicting future from past is more aligned with forecasting than discriminating between different time series instances?

- Concept: Siamese network architecture
  - Why needed here: The siamese structure allows sharing parameters between history and future encoding paths while maintaining separate processing streams
  - Quick check question: What would happen if we used separate encoders for history and future instead of shared parameters?

- Concept: Stop-gradient operations in self-supervised learning
  - Why needed here: Stop-gradient prevents the model from collapsing to trivial solutions while avoiding the need for negative pairs
  - Quick check question: How does applying stop-gradient to future but not history encoding path affect the learning dynamics?

## Architecture Onboarding

- Component map: Input → History segment → Encoder → Latent representation → Predictor → Predicted future → Cosine similarity loss → Gradient update
- Critical path: History segment → Encoder → Latent representation → Predictor → Predicted future → Cosine similarity loss → Gradient update
- Design tradeoffs: No negative pairs simplifies implementation but may limit discriminative power; multi-scale convolutions capture patterns but increase parameter count; stop-gradient prevents collapse but may limit expressivity
- Failure signatures: Poor forecasting performance indicates issues with predictor network capacity, encoder architecture, or temporal pattern complexity; training instability suggests stop-gradient configuration problems
- First 3 experiments:
  1. Train with and without stop-gradient to verify its impact on performance and stability
  2. Compare different kernel size configurations in the multi-scale encoder to find optimal temporal pattern capture
  3. Test the predictor network capacity by varying its depth and width to ensure adequate modeling of temporal dependencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance gap between SimTS and baseline methods vary significantly across different time series characteristics (stationarity, seasonality, trend strength)?
- Basis in paper: [inferred] The paper mentions that seasonal disentanglement improves performance on stationary datasets like Weather but impairs performance on less stationary datasets like Exchange and ETTm2, suggesting that performance differences may be linked to time series characteristics.
- Why unresolved: The paper provides qualitative observations about dataset characteristics but does not conduct a systematic analysis of how time series properties affect the relative performance of different methods.
- What evidence would resolve it: A comprehensive study quantifying the relationship between time series characteristics (measured through metrics like ADF test statistic, seasonal strength, trend strength) and the performance differences between SimTS and each baseline method across all datasets.

### Open Question 2
- Question: What is the optimal negative sampling strategy for time series forecasting that avoids false repulsion while still preventing representation collapse?
- Basis in paper: [explicit] The paper demonstrates that adding negative pairs using InfoNCE loss degrades performance in most datasets, but does not claim negative pairs are universally ineffective. It concludes that "finding efficient augmentation techniques applicable to various time series is challenging."
- Why unresolved: The paper only tests one specific negative sampling approach (random masking and InfoNCE loss). It shows that naive negative sampling hurts performance but doesn't explore alternative strategies or prove that negative pairs cannot be beneficial with better construction methods.
- What evidence would resolve it: Systematic experimentation with different negative sampling strategies (temporal distance-based sampling, similarity-based sampling, hard negative mining) showing whether any approach can improve upon SimTS's performance without causing false repulsion.

### Open Question 3
- Question: How does the choice of kernel sizes in the multi-scale encoder affect forecasting performance for different prediction horizons?
- Basis in paper: [inferred] The paper uses kernel sizes 2^i for i in {0, 1, ..., m} where m = [log2K] + 1, noting that "for short-term forecasting, shorter local patterns are ideal, whereas, for long-term forecasting, longer sets of global patterns are preferred." However, it does not test alternative kernel size configurations.
- Why unresolved: The paper uses a fixed heuristic for kernel size selection based on sequence length but does not investigate whether this is optimal or how performance changes with different kernel size arrangements.
- What evidence would resolve it: Ablation studies varying the kernel size distribution (e.g., emphasizing smaller kernels for short horizons, larger kernels for long horizons) across different prediction horizons to determine the optimal kernel configuration for each forecasting task.

### Open Question 4
- Question: What is the minimum amount of historical data needed to achieve near-optimal forecasting performance with SimTS?
- Basis in paper: [inferred] The paper uses a fixed history length of 201 timesteps for all datasets, but does not explore how performance scales with different history lengths or whether diminishing returns occur.
- Why unresolved: The paper does not investigate the sensitivity of SimTS to the amount of historical context provided, which is crucial for understanding its practical applicability to real-world scenarios with limited historical data.
- What evidence would resolve it: Experiments varying the history length K across multiple values (e.g., 50, 100, 201, 400) for each dataset to identify the point of diminishing returns and the minimum effective history length for each forecasting task.

## Limitations
- Limited comparison scope - only benchmarks against contrastive learning methods rather than recent transformer-based state-of-the-art models
- Fixed hyperparameters - uses same kernel sizes and architecture across all datasets without exploring optimal configurations
- Short-term focus - primarily evaluated on relatively short forecast horizons without testing scalability to longer-term predictions

## Confidence
- **High confidence**: The core mechanism of using positive pairs only without negative pairs, and the ablation studies showing stop-gradient importance
- **Medium confidence**: The claim that negative pairs degrade performance (based on limited ablation studies and could be architecture-dependent)
- **Low confidence**: The claim of state-of-the-art performance across all benchmarks (due to limited comparison scope)

## Next Checks
1. Extend comparisons to non-contrastive state-of-the-art models - benchmark SimTS against recent transformer-based forecasting methods (Informer, Autoformer, etc.) on the same datasets to validate the "state-of-the-art" claim
2. Test negative pair sensitivity across architectures - systematically vary the number and quality of negative pairs across different model architectures to determine if the negative pair degradation is specific to SimTS or a general phenomenon
3. Evaluate on longer forecast horizons - test SimTS performance on longer forecast windows (beyond 201 timesteps) to assess scalability and temporal pattern capture limits