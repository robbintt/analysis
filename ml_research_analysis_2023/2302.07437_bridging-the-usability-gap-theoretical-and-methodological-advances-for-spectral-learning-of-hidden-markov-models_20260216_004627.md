---
ver: rpa2
title: 'Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral
  Learning of Hidden Markov Models'
arxiv_id: '2302.07437'
source_url: https://arxiv.org/abs/2302.07437
tags:
- shmm
- pshmm
- algorithm
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the instability and local optima issues of
  the Baum-Welch algorithm for hidden Markov models (HMMs). It proposes a novel projected
  spectral learning method (PSHMM) that leverages projection-onto-simplex regularization
  to improve robustness.
---

# Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral Learning of Hidden Markov Models

## Quick Facts
- arXiv ID: 2302.07437
- Source URL: https://arxiv.org/abs/2302.07437
- Reference count: 6
- Primary result: Proposed PSHMM method achieves highest Sharpe ratio (2.88) and annualized return (1012%) on cryptocurrency data

## Executive Summary
This paper addresses the instability and local optima issues of the Baum-Welch algorithm for hidden Markov models (HMMs) by proposing a novel projected spectral learning method (PSHMM). The method leverages projection-onto-simplex regularization to improve robustness and mitigate error propagation. The authors provide asymptotic theory for the approximation error of spectral learning and develop online learning variants for both standard and projected methods. In simulations, PSHMM significantly outperforms the standard spectral method and achieves performance close to oracle methods, especially in high-noise and heavy-tailed data scenarios.

## Method Summary
The paper introduces PSHMM, a projected spectral learning method for HMMs that combines method-of-moments estimation with projection-onto-simplex regularization. The approach first performs dimensionality reduction, then fits a GMM to estimate cluster means, and uses these to estimate first, second, and third moments. Predictions are made by solving a convex optimization problem and projecting onto the probability simplex to ensure valid probability weights. Online learning variants with forgetting factors are also developed to handle nonstationary data.

## Key Results
- PSHMM significantly outperforms standard spectral learning in high-noise and heavy-tailed scenarios
- On cryptocurrency data, PSHMM achieves Sharpe ratio of 2.88 and annualized return of 1012% with 49% maximum drawdown
- PSHMM performance approaches that of oracle methods while avoiding local optima issues of EM algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The projection-onto-simplex regularization constrains predictions within the convex hull of cluster means, mitigating error propagation.
- Mechanism: After spectral learning produces a weight vector, projection-onto-simplex maps it onto the probability simplex, ensuring valid probability weights that lie in the span of the estimated cluster means.
- Core assumption: The true hidden states correspond to convex combinations of cluster means; constraining predictions to the simplex reduces variance without severe bias.
- Evidence anchors:
  - [section] "This solution is not equivalent to solution from the projection-onto-polyhedron, because d(a,b) ≠ d(Aa,Ab) in general. But the solution set is the same, i.e. the predictions are both guaranteed to be constrained to the polyhedron."
  - [abstract] "We propose a novel algorithm called projected SHMM (PSHMM) that mitigates the problem of error propagation..."
- Break condition: If the underlying emission distributions are highly skewed or multimodal, the simplex constraint may force predictions into a region with high bias.

### Mechanism 2
- Claim: Spectral learning avoids local optima by using method-of-moments instead of maximum likelihood estimation.
- Mechanism: Estimating moments from data and solving linear equations yields a closed-form parameter estimate, bypassing iterative EM optimization prone to local minima.
- Core assumption: Third-order moments are well-estimated and the moment structure uniquely identifies the HMM parameters.
- Evidence anchors:
  - [abstract] "The Baum-Welch (B-W) algorithm... is prone to getting stuck in local optima..."
  - [section] "Although MOM gives fast approximation, the theoretical properties of SHMM estimation are less well studied."
- Break condition: If the third-order moment tensor is ill-conditioned or if the model is under-identified, spectral methods may fail to recover parameters.

### Mechanism 3
- Claim: Online learning with forgetting factor adapts to nonstationarity while controlling memory usage.
- Mechanism: Exponentially weighted updates of first, second, and third moments down-weight older observations, allowing the model to track changes in underlying dynamics.
- Core assumption: Recent observations are more relevant for current predictions than distant historical data.
- Evidence anchors:
  - [section] "When dealing with instationary data, it might help if we add a forgetting mechanism on parameter estimation... equivalent to the exponentially weighted average."
  - [abstract] "develop online learning variants of both SHMM and PSHMM that accommodate potential nonstationarity."
- Break condition: If the forgetting rate is too high, the model may overreact to noise; too low and it may not track changes.

## Foundational Learning

- Concept: Method of Moments (MoM)
  - Why needed here: SHMM relies on MoM to estimate HMM parameters directly from empirical moments rather than likelihood maximization.
  - Quick check question: How does the third-order moment tensor relate to the transition and emission structure in an HMM?

- Concept: Convex Optimization and Simplex Projection
  - Why needed here: PSHMM uses projection onto the probability simplex to regularize predictions, requiring knowledge of efficient algorithms like the one by Wang and Carreira-Perpinán.
  - Quick check question: What is the time complexity of the projection-onto-simplex algorithm and why is it preferred over polyhedron projection?

- Concept: Exponentially Weighted Averages and Forgetting
  - Why needed here: Online learning variants use forgetting factors to weight recent observations more heavily, enabling adaptation to nonstationary data.
  - Quick check question: How does the effective sample size change as the forgetting factor varies?

## Architecture Onboarding

- Component map: Data → Dimensionality Reduction (U matrix) → GMM for cluster means (M) → Moment Estimation (μ, Σ, K) → Spectral Prediction → (PSHMM) Projection onto Simplex → Back-projection to original space
- Critical path: For PSHMM, the critical steps are computing U, fitting GMM, estimating moments, recursive prediction with projection, and final reconstruction.
- Design tradeoffs: Offline vs online learning trades computational cost and model freshness; projection-onto-simplex trades some bias for reduced variance and faster computation vs projection-onto-polyhedron.
- Failure signatures: Poor R² indicates instability or model misspecification; extremely high computational time suggests inefficient matrix operations or overly large dimensionality.
- First 3 experiments:
  1. Verify that the projection-onto-simplex correctly maps arbitrary weight vectors onto the simplex by testing on synthetic examples.
  2. Compare offline vs online moment updates on a stationary synthetic HMM to confirm convergence properties.
  3. Evaluate the effect of different forgetting rates on tracking performance in a nonstationary HMM simulation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the projection dimension d when the true number of hidden states is unknown?
- Basis in paper: [explicit] The paper states that "from the theory of SHMM, d should equal the number of states in the HMM" but acknowledges that "the number of hidden states is usually unknown" and discusses choosing d using prior knowledge or tuning it.
- Why unresolved: The paper doesn't provide a definitive method for selecting d when the true number of states is unknown. It only mentions that the choice of d can be based on prior knowledge or tuned empirically.
- What evidence would resolve it: A systematic study comparing different methods for selecting d (e.g., cross-validation, information criteria, domain knowledge) across various datasets and model configurations, showing which method consistently leads to optimal performance.

### Open Question 2
- Question: How does the performance of PSHMM compare to other state-of-the-art time series forecasting methods beyond those tested (AR, HMM-EM, SHMM)?
- Basis in paper: [inferred] The paper compares PSHMM to AR, HMM-EM, and SHMM but doesn't compare it to other modern time series forecasting methods like LSTMs, Prophet, or ensemble methods.
- Why unresolved: The paper focuses on comparing PSHMM to methods within the HMM family and a simple AR model, leaving open the question of how it performs against other advanced time series forecasting techniques.
- What evidence would resolve it: Empirical studies comparing PSHMM's performance to a broader range of state-of-the-art time series forecasting methods on various datasets, including financial, natural language processing, and biological data.

### Open Question 3
- Question: What is the impact of different distance metrics in the projection-onto-polyhedron method on the performance of PSHMM?
- Basis in paper: [explicit] The paper mentions that "we can use any distance to define 'nearest point' but in our exposition we work with Euclidean distance" and notes that "this results in a convex optimization problem if the distance is convex."
- Why unresolved: While the paper uses Euclidean distance for the projection-onto-polyhedron method, it doesn't explore how different distance metrics (e.g., Manhattan, Mahalanobis) might affect the performance of PSHMM.
- What evidence would resolve it: Comparative studies using different distance metrics in the projection-onto-polyhedron method, evaluating their impact on PSHMM's performance across various datasets and model configurations.

## Limitations

- The asymptotic theory provides bounds but does not fully characterize finite-sample behavior or conditions for optimal performance
- Empirical validation is limited to one financial dataset without systematic sensitivity analysis across different data-generating processes
- Online learning extension assumes exponential forgetting is appropriate but doesn't explore alternative adaptive schemes

## Confidence

- **High confidence**: The theoretical framework for spectral learning using third-order moments and the mathematical correctness of projection-onto-simplex algorithms
- **Medium confidence**: The claim that PSHMM reduces error propagation and outperforms HMM-EM in practice, based on simulation and financial application results
- **Low confidence**: The generalizability of the cryptocurrency trading results to other domains or data types, and the robustness of the online learning variants to various forms of nonstationarity

## Next Checks

1. **Finite-sample sensitivity analysis**: Conduct systematic experiments varying sample size, dimensionality d, and noise levels to quantify the finite-sample performance gap between PSHMM and SHMM, and identify break points where asymptotic approximations fail.

2. **Alternative regularization comparison**: Implement and compare projection-onto-polyhedron (Wang and Carreira-Perpinán, 2013) against projection-onto-simplex across multiple synthetic HMMs to empirically validate the computational efficiency claims and measure the bias-variance tradeoff.

3. **Robustness to misspecification**: Test PSHMM on HMMs with non-convex emission distributions, high-dimensional observations, and structural breaks to assess failure modes beyond the controlled simulations, particularly focusing on scenarios where the simplex constraint introduces harmful bias.