---
ver: rpa2
title: Attention Is Not All You Need Anymore
arxiv_id: '2308.07661'
source_url: https://arxiv.org/abs/2308.07661
tags:
- transformer
- extractor
- sublayer
- sequence
- mechanism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new sublayer, called the Extractor, to replace
  the self-attention sublayer in the Transformer. The Extractor extracts unified features
  from the input sequence and adjusts them based on the sequence length, enabling
  the model to handle variable-length sequences in text generation tasks.
---

# Attention Is Not All You Need Anymore

## Quick Facts
- arXiv ID: 2308.07661
- Source URL: https://arxiv.org/abs/2308.07661
- Reference count: 3
- Key outcome: Extractor sublayer outperforms self-attention in training cost for text generation

## Executive Summary
This paper proposes replacing the self-attention sublayer in Transformers with a new Extractor sublayer for text generation tasks. The Extractor extracts unified features from variable-length input sequences and adjusts them based on sequence length, enabling better handling of sequence prediction problems. Experimental results on a small-scale dataset of children's books show that the Transformer with Extractor achieves better training cost (lower perplexity) than the standard Transformer, while also having the potential for faster inference due to its shorter critical path of computation.

## Method Summary
The proposed method replaces the self-attention sublayer in Transformers with an Extractor sublayer that transforms variable-length sequence prediction into a constant-length problem. The Extractor uses linear transformations to extract unified features from the entire input sequence, then adjusts these features based on sequence length before passing them to the subsequent FFN sublayer. The approach is evaluated on a text generation task using a small dataset of 100 free English children's books from Project Gutenberg, tokenized to 8.4M tokens with a vocabulary size of 5000.

## Key Results
- Transformer with Extractor outperforms standard Transformer in training cost (lower perplexity) on text generation task
- Extractor has potential for faster inference due to shorter critical path compared to self-attention
- Extractor's FFN sublayer functions as a lookup table for memorizing state transitions in the Markov chain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Extractor achieves better performance by transforming variable-length sequence prediction into constant-length through unified feature extraction
- Mechanism: Uses linear transformations to extract unified features from entire variable-length input as constant-length "index" for lookup, then adjusts based on sequence length
- Core assumption: Variable-length nature is a key bottleneck, and converting to constant-length improves model capacity
- Evidence anchors: Abstract mentions extracting unified features and adjusting based on sequence length; section describes extracting unified features as constant-length "index" for table lookup
- Break condition: If adjustment mechanism fails to properly normalize for sequence length, lookup table approach may not generalize

### Mechanism 2
- Claim: Extractor has shorter critical path enabling faster inference
- Mechanism: Critical path is "multiplication - cumulation - multiplication - multiplication - cumulation" vs self-attention's more complex path
- Core assumption: Shorter critical path directly translates to faster inference speed
- Evidence anchors: Abstract mentions potential to run faster due to shorter critical path; section compares critical paths of both mechanisms
- Break condition: If memory access patterns or parallelizability differ significantly, theoretical advantage may not manifest in practice

### Mechanism 3
- Claim: Extractor achieves better performance by allowing better "memorization" of state transitions
- Mechanism: FFN sublayer functions as lookup table that memorizes relationship between input and output; Extractor's unified feature extraction enables this to work across variable-length sequences
- Core assumption: Sequence prediction can be effectively solved through memorization rather than learned attention patterns
- Evidence anchors: Section describes FFN as implementing "lookup table" beneficial for memorizing state transitions; abstract mentions improved performance
- Break condition: If memorization approach fails to generalize to longer or more complex sequences not seen during training

## Foundational Learning

- Concept: Variable-length discrete-time Markov chains
  - Why needed here: Paper frames sequence prediction as variable-length discrete-time Markov chain where next token depends on previous sequence
  - Quick check question: In a Markov chain, what is the key property that distinguishes it from other stochastic processes?

- Concept: Attention mechanisms and self-attention
  - Why needed here: Paper proposes replacing self-attention mechanism, so understanding how it works is crucial
  - Quick check question: What is the computational complexity of self-attention with respect to sequence length, and why is this a limitation?

- Concept: Critical path analysis in neural networks
  - Why needed here: Paper claims Extractor has shorter critical path than self-attention, important for understanding potential speed advantages
  - Quick check question: How does critical path length affect inference speed in neural network architectures?

## Architecture Onboarding

- Component map: Input embedding layer (token and positional embeddings) -> Extractor sublayer (feature extraction blocks + adjustment block + optional linear transformation) -> FFN sublayer -> Output layer (softmax regression)

- Critical path: Input → Extractor sublayer (feature extraction + adjustment) → FFN sublayer → Output. The critical path is shorter than self-attention because it eliminates self-attention's attention weight computation and softmax operations.

- Design tradeoffs: More trainable parameters than self-attention (ld² + 2d² vs 4d²), higher computational complexity but shorter critical path, better performance on sequence prediction but potentially less flexible for cross-sequence relationships

- Failure signatures: Training loss plateaus or diverges (suggests adjustment mechanism issues), generated text quality degrades with longer sequences (suggests memorization limitations), no wall-clock speed improvement despite shorter critical path (suggests memory access or parallelization issues)

- First 3 experiments:
  1. Train Transformer with Extractor on small text generation dataset and compare training loss to standard Transformer
  2. Compare inference speed of both models on sequences of varying lengths to verify critical path advantage
  3. Perform ablation study: train models with only feature extraction, only adjustment, and both to determine necessity of each component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Extractor compare to other efficient Transformer variants (e.g., Hyena, SPADE) in terms of performance and computational complexity?
- Basis in paper: [explicit] Paper mentions existing works like Hyena and SPADE that aim to replace self-attention with more efficient alternatives
- Why unresolved: Paper only evaluates Extractor against standard self-attention, not other efficient variants
- What evidence would resolve it: Conducting experiments comparing Extractor's performance and computational complexity with other efficient Transformer variants on same dataset and task

### Open Question 2
- Question: Can Extractor be effectively applied to tasks beyond text generation, such as natural language understanding or computer vision?
- Basis in paper: [inferred] Paper mentions Transformer's successful application to various tasks beyond text generation, but experiments only focus on text generation
- Why unresolved: Paper provides no evidence or experiments supporting Extractor's effectiveness in tasks other than text generation
- What evidence would resolve it: Conducting experiments evaluating Transformer with Extractor on other tasks like natural language understanding or computer vision

### Open Question 3
- Question: How does Extractor's performance scale with increasing sequence length and model size?
- Basis in paper: [inferred] Paper mentions self-attention's quadratic computational complexity limits long sequence applications, but doesn't discuss Extractor's scaling behavior
- Why unresolved: Paper provides no experiments or analysis on how Extractor's performance scales with increasing sequence length and model size
- What evidence would resolve it: Conducting experiments evaluating Transformer with Extractor on tasks with varying sequence lengths and model sizes, analyzing scaling behavior

## Limitations

- Limited experimental validation on single small dataset (8.4M tokens from children's books) raises generalization concerns
- Theoretical speed advantage based on critical path analysis lacks empirical wall-clock measurements
- Claims about Extractor's superiority lack rigorous mathematical proof, particularly regarding Markov property preservation across variable lengths

## Confidence

- High confidence: Architectural description of Extractor sublayer and critical path comparison with self-attention is clearly specified and theoretically sound
- Medium confidence: Claim that Extractor improves training cost on text generation task, though experimental validation is limited to one dataset
- Low confidence: Claim that Extractor can "run faster than self-attention" based solely on critical path analysis without empirical wall-clock measurements

## Next Checks

1. **Dataset generalization study**: Reproduce experiments on multiple text generation datasets of varying sizes, domains, and complexities (WikiText, BookCorpus, large-scale web corpus). Compare training curves, final perplexity, and convergence speed to assess whether Extractor's advantages generalize beyond children's books dataset.

2. **Wall-clock speed benchmarking**: Implement both standard Transformer with self-attention and Transformer with Extractor in production-ready framework (PyTorch with CUDA optimizations). Measure actual inference latency and throughput on GPUs/TPUs for sequences of varying lengths, including both batching and autoregressive generation scenarios.

3. **Ablation analysis of adjustment mechanism**: Create controlled experiments isolating effect of adjustment block in Extractor. Train models with: (a) only feature extraction blocks, (b) only adjustment blocks applied to identity features, and (c) full Extractor. Analyze how each component contributes to performance improvements and whether adjustment mechanism is essential for handling variable-length sequences.