---
ver: rpa2
title: Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large
  Language Model
arxiv_id: '2309.11000'
source_url: https://arxiv.org/abs/2309.11000
tags:
- speech
- linguistic
- prosodic
- dialogue
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the feasibility of constructing a unified
  spoken dialogue system that can simultaneously generate dialogue responses and linguistic
  features, inspired by the human speech production process. The proposed approach
  leverages the capabilities of Large Language Models (LLMs) to jointly model dialogue
  responses and linguistic features, such as prosodic structure, pitch, duration,
  and other speech-related attributes.
---

# Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model

## Quick Facts
- arXiv ID: 2309.11000
- Source URL: https://arxiv.org/abs/2309.11000
- Reference count: 3
- Primary result: LLM-based approaches achieve competitive performance on prosodic structure prediction and demonstrate feasibility of joint dialogue response and linguistic feature generation.

## Executive Summary
This paper investigates the feasibility of constructing a unified spoken dialogue system that can simultaneously generate dialogue responses and linguistic features, inspired by the human speech production process. The proposed approach leverages the capabilities of Large Language Models (LLMs) to jointly model dialogue responses and linguistic features, such as prosodic structure, pitch, duration, and other speech-related attributes. The authors conduct two sets of experiments: first, they demonstrate the speech understanding ability of LLMs by performing prosodic structure prediction, a typical task in Text-to-Speech (TTS) front-end. Results show that LLM-based approaches, both prompting-based and fine-tuning-based, achieve competitive performance compared to traditional methods. Second, they further integrate dialogue response and a wide array of linguistic features using a unified encoding format. The model is trained to generate both dialogue response and JSON-style linguistic features simultaneously, demonstrating the feasibility of joint learning. The experiments indicate that LLM-based approaches are a promising direction for building unified spoken dialogue systems, though limitations such as high training costs, overfitting, and limited expressivity are acknowledged.

## Method Summary
The paper proposes two main approaches to investigate LLMs' capabilities for spoken dialogue systems. First, they adapt LLMs to perform prosodic structure prediction (PSP), a core TTS front-end task, using both prompting-based and fine-tuning-based methods. The prompting approach uses ChatGPT or ChatGLM with carefully crafted prompts including linguistic knowledge and few-shot demonstrations, while the fine-tuning approach adapts ChatGLM2-6B using cross-entropy loss. Second, they propose a unified model that generates both dialogue responses and JSON-style linguistic features (character, pinyin, duration, pitch, prosody hierarchy) in a single forward pass. This is achieved by formatting the output as a concatenated string and training the LLM using full-parameter fine-tuning with 4-bit quantization. The dialogue context is generated using ChatGPT, and linguistic features are automatically extracted from speech.

## Key Results
- LLM-based approaches achieve competitive performance on prosodic structure prediction, with fine-tuning-based ChatGLM outperforming prompting-based ChatGPT (82.38% vs 80.12% F-score).
- The unified model successfully generates both dialogue responses and JSON-style linguistic features, demonstrating the feasibility of joint learning.
- Experiments show that carefully crafted linguistic knowledge and selected examples enable LLMs to outperform traditional methods on PSP tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large Language Models (LLMs) possess sufficient semantic understanding to perform speech-related tasks such as prosodic structure prediction, despite being trained only on text data.
- Mechanism: LLMs learn rich linguistic representations during pretraining on text corpora, which encode syntactic and semantic patterns. These representations can be transferred to speech tasks through prompting or fine-tuning, allowing the model to predict prosodic boundaries and hierarchy.
- Core assumption: The linguistic knowledge acquired from text-only pretraining is sufficient to model speech prosody, which is closely tied to syntax and semantics.
- Evidence anchors:
  - [abstract] "We hypothesize that Large Language Models (LLMs) with billions of parameters possess significant speech understanding capabilities and can jointly model dialogue responses and linguistic features."
  - [section] "Results show that both prompting-based ChatGPT and fine-tuning based ChatGLM (Zeng et al., 2022) model achieve competitive performance against traditional methods."
  - [corpus] Weak - corpus does not contain explicit prosodic structure prediction tasks; evidence is indirect from PSP results.
- Break condition: If the LLM lacks sufficient instruction-following ability (e.g., smaller open-source models like ChatGLM in prompting mode), or if the task requires fine-grained acoustic knowledge not encoded in text representations.

### Mechanism 2
- Claim: Fine-tuning a smaller LLM can outperform prompting a larger LLM for specialized tasks like prosodic structure prediction.
- Mechanism: Fine-tuning allows the model to learn from a larger number of training examples (8k vs. max 16 in-context examples), adapting its parameters to the specific task distribution. This overcomes the context window limitation of prompting and enables deeper task-specific learning.
- Core assumption: The model's capacity is sufficient to learn from the available training data, and the task is not too data-intensive to require orders of magnitude more examples.
- Evidence anchors:
  - [section] "Experiments show that carefully crafted linguistic knowledge and selected examples (i.e., 'Knowledge + 16 Selected Examples' variation) enable ChatGPT to outperform the traditional method SpanPSP (80.12% vs. 79.80%), but such a prompting-based learning strategy failed (N/A) at smaller open-source LLM (ChatGLM) due to its limited instruction-following ability. However, it shows that fine-tuning smaller LLM can outperform prompting larger LLM (82.38% vs. 80.12%), as it can access more training samples (8k training set vs. the maximum of 16 in-context examples)."
  - [corpus] No direct corpus evidence; this is a methodological comparison.
- Break condition: If the task requires significantly more data than available, or if the model overfits due to limited data and high parameter count.

### Mechanism 3
- Claim: LLMs can jointly generate dialogue responses and JSON-style linguistic features in a single forward pass, mimicking the parallel processing of human speech production.
- Mechanism: By formatting the output as a concatenated string containing both the dialogue response and the JSON-style linguistic features, the LLM learns to generate both outputs autoregressively in a unified sequence-to-sequence framework. This allows the model to "think how to respond" and "think how to speak" simultaneously.
- Core assumption: The autoregressive generation process can handle the structured output format and maintain coherence between the dialogue response and the linguistic features.
- Evidence anchors:
  - [abstract] "Secondly, we aim to further integrate a wide array of linguistic features into the model, and maintain LLM's dialogue capability at the same time (Section 4)."
  - [section] "Experiments show that LLM learns successfully." (in the context of joint prediction of dialogue response and linguistic features)
  - [corpus] Weak - corpus does not contain dialogue context and parallel speech recordings; data is generated using ChatGPT and automatic extraction.
- Break condition: If the JSON generation is too complex for autoregressive decoding, leading to parsing errors or loss of coherence between response and features.

## Foundational Learning

- Concept: Prosodic Structure Prediction (PSP)
  - Why needed here: PSP is a core task in TTS front-end that determines the rhythmic and intonational structure of speech. Understanding how LLMs can perform PSP is key to validating their speech understanding capabilities.
  - Quick check question: What are the three levels of prosodic hierarchy in Chinese TTS, and how are they typically annotated?

- Concept: Fine-tuning vs. Prompting
  - Why needed here: The paper compares two adaptation strategies for LLMs. Understanding their trade-offs (data efficiency, task-specific performance, model capacity) is crucial for designing effective LLM-based speech systems.
  - Quick check question: What are the main limitations of prompting-based adaptation, and how does fine-tuning address them?

- Concept: Sequence-to-Sequence (Seq2seq) Modeling
  - Why needed here: The paper frames PSP and joint dialogue-linguistics prediction as seq2seq tasks. Understanding this paradigm is essential for grasping how the LLM generates structured outputs.
  - Quick check question: How does the loss function differ when treating PSP as seq2seq vs. token classification?

## Architecture Onboarding

- Component map:
  - User input -> LLM -> Dialogue response + JSON-style linguistic features -> (Future) TTS front-end -> Acoustic model -> Vocoder -> Audio

- Critical path:
  1. User input → LLM
  2. LLM generates response and linguistic features
  3. (Future) Linguistic features → TTS front-end → Acoustic model → Vocoder → Audio

- Design tradeoffs:
  - Unified vs. cascaded: Unified approach mimics human speech production but requires complex output formatting and may have higher computational cost.
  - Fine-tuning vs. prompting: Fine-tuning enables learning from more data but is more expensive; prompting is cheaper but limited by context window and instruction-following ability.
  - Output format: JSON-style encoding allows structured output but increases context length and decoding time.

- Failure signatures:
  - Parsing errors in JSON output indicate issues with autoregressive generation of structured data.
  - Mismatch between response and linguistic features suggests loss of coherence in joint generation.
  - Overfitting (high train, low test performance) indicates insufficient data or model capacity mismatch.

- First 3 experiments:
  1. Prompting-based PSP: Test LLM (ChatGPT, ChatGLM) on prosodic structure prediction with varying numbers of few-shot demonstrations and linguistic knowledge.
  2. Fine-tuning-based PSP: Fine-tune ChatGLM2-6B on PSP task and compare performance to prompting and traditional BERT-based methods.
  3. Joint dialogue-linguistics prediction: Fine-tune LLM to generate both dialogue response and JSON-style linguistic features, evaluate parsing success rate and feature accuracy on train/test splits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs trained on text-only data effectively predict diverse prosodic features beyond structure, such as duration, pitch, and intensity, directly from text input?
- Basis in paper: [explicit] The paper mentions automatic extraction of duration, pitch, and prosodic hierarchy features from speech, but only validates LLM performance on predicting prosodic structure. The discussion section acknowledges the need to explore finer-grained prosodic features like voice quality and phonation type.
- Why unresolved: The paper only evaluates LLM performance on prosodic structure prediction, not other prosodic features like duration and pitch. The JSON-style linguistic feature generation experiment uses automatically extracted pitch and duration as ground truth, rather than predicting them directly from text.
- What evidence would resolve it: Experiments comparing LLM-based direct prediction of duration, pitch, and intensity features against traditional methods or automatic extraction baselines, using metrics like correlation coefficients and error rates.

### Open Question 2
- Question: How does the expressivity of LLM-generated speech features compare to human-like conversational speech when synthesized?
- Basis in paper: [explicit] The discussion section notes that the current dataset lacks interactional resources like breath patterns, repair, and interjections, and that finer-grained annotation would increase expressivity. The paper acknowledges limitations in expressivity due to single-speaker, formal read speech data.
- Why unresolved: The paper only evaluates feature prediction accuracy, not the perceptual quality or naturalness of synthesized speech using these features. No subjective listening tests or objective speech quality metrics are reported.
- What evidence would resolve it: Perceptual evaluation studies (MOS, ABX tests) comparing synthesized speech using LLM-predicted features against natural conversational speech and traditional TTS systems, measuring naturalness, expressiveness, and speaker similarity.

### Open Question 3
- Question: What is the minimum LLM size required to achieve competitive performance on joint dialogue and speech feature prediction?
- Basis in paper: [explicit] The paper uses ChatGLM2-6B for fine-tuning experiments, achieving 82.38% average F-score on prosodic structure prediction. The discussion mentions high training costs as a limitation. Comparison is made to BERT-based methods with 0.1B parameters.
- Why unresolved: The paper only tests one LLM size (6B parameters) and doesn't explore scaling effects or efficiency-accuracy tradeoffs. No comparison is made to smaller or larger models, or parameter-efficient fine-tuning methods.
- What evidence would resolve it: Systematic experiments varying LLM size (e.g., 1B, 3B, 6B, 13B) and fine-tuning strategies, measuring performance, training time, and inference latency to identify optimal tradeoffs for joint modeling.

## Limitations

- The study relies heavily on synthetic data for the joint dialogue-linguistics task, generated using ChatGPT rather than human-annotated dialogue-linguistics pairs, raising questions about ecological validity.
- The paper does not report detailed performance metrics for the joint prediction task (only mentions "learns successfully"), making it difficult to assess the practical utility of the unified approach.
- The JSON-style output format for linguistic features is presented as a solution but its scalability and robustness for more complex feature sets remains unproven.

## Confidence

- **High Confidence**: LLM's ability to perform prosodic structure prediction through prompting and fine-tuning, as demonstrated by quantitative results on the DataBaker dataset with clear baselines.
- **Medium Confidence**: The feasibility of joint generation of dialogue and linguistic features, based on the proof-of-concept experiment showing successful parsing, but lacking detailed performance metrics.
- **Low Confidence**: Claims about the unified model's ability to "think how to speak" and mimic human speech production, as these are metaphorical and not empirically validated through perceptual or production studies.

## Next Checks

1. Conduct a human evaluation study comparing the naturalness and appropriateness of dialogue responses and associated speech features generated by the unified LLM approach versus a cascaded system (dialogue model + TTS front-end).
2. Test the model's generalization by evaluating on out-of-domain dialogue contexts and measuring performance degradation in both dialogue response quality and linguistic feature accuracy.
3. Perform an ablation study on the JSON output format to determine the impact of output complexity on generation quality and parsing success rate, particularly as more speech-related features are added.