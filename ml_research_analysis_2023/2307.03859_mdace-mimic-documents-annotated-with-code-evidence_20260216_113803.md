---
ver: rpa2
title: 'MDACE: MIMIC Documents Annotated with Code Evidence'
arxiv_id: '2307.03859'
source_url: https://arxiv.org/abs/2307.03859
tags:
- evidence
- code
- codes
- coding
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MDACE is the first publicly available dataset for evidence/rationale
  extraction in an extreme multi-label classification task over long medical documents.
  It consists of 302 Inpatient and 52 Profee MIMIC-III charts annotated by professional
  medical coders with 3,934 and 5,563 evidence spans respectively.
---

# MDACE: MIMIC Documents Annotated with Code Evidence

## Quick Facts
- arXiv ID: 2307.03859
- Source URL: https://arxiv.org/abs/2307.03859
- Reference count: 33
- Primary result: MDACE dataset enables automatic evaluation of code evidence extraction in extreme multi-label classification over long medical documents

## Executive Summary
MDACE is the first publicly available dataset for evidence/rationale extraction in extreme multi-label classification over long medical documents. The dataset consists of 302 Inpatient and 52 Profee MIMIC-III charts annotated by professional medical coders with 3,934 and 5,563 evidence spans respectively. It enables automatic evaluation of Computer-Assisted Coding (CAC) systems and interpretable deep learning models. Several baseline evidence extraction methods were implemented based on the EffectiveCAN model, with supervised attention achieving the best micro-F1 score of 0.439 for exact span match on Inpatient discharge summaries.

## Method Summary
The MDACE dataset was created by annotating MIMIC-III clinical notes with evidence spans by professional medical coders. The EffectiveCAN model was used as a baseline for evidence extraction, with three variants: supervised attention, linear tagging, and CNN tagging. Supervised attention uses a Kullback-Leibler divergence loss to encourage the model to focus on the same text spans that human coders highlighted as evidence. The dataset and evaluation metrics (token match, exact span match, position independent variants) enable automatic assessment of evidence extraction quality in multi-label classification tasks.

## Key Results
- MDACE contains 302 Inpatient charts with 3,934 evidence spans and 52 Profee charts with 5,563 evidence spans
- Supervised attention achieved the best micro-F1 score of 0.439 for exact span match on Inpatient discharge summaries
- The dataset enables automatic evaluation of code evidence extraction methods for CAC systems and interpretable deep learning models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset enables automatic evaluation of code evidence generated by CAC systems by providing ground-truth evidence spans annotated by professional medical coders.
- Mechanism: Professional coders annotate sufficient or complete evidence spans for each billing code, linking specific text offsets in clinical notes to the codes. This creates a direct mapping that allows ML models to be evaluated quantitatively rather than relying on qualitative human judgment.
- Core assumption: The annotations by professional medical coders are accurate and representative of the evidence a human coder would use to justify a code.
- Evidence anchors:
  - [abstract] "MDACE is the first publicly available dataset for evidence/rationale extraction in an extreme multi-label classification task over long medical documents."
  - [section] "The dataset – annotated by professional medical coders – consists of 302 Inpatient charts with 3,934 evidence spans and 52 Profee charts with 5,563 evidence spans."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.419, average citations=0.0. Weak corpus evidence for this specific mechanism.

### Mechanism 2
- Claim: Supervised attention improves evidence extraction performance by explicitly training the model to predict evidence spans alongside code predictions.
- Mechanism: A Kullback–Leibler divergence loss is added to the EffectiveCAN model to supervise the attention weights, encouraging the model to focus on the same text spans that human coders highlighted as evidence.
- Core assumption: The attention weights can be effectively supervised to align with human-annotated evidence spans, and this supervision improves the model's ability to extract relevant evidence.
- Evidence anchors:
  - [section] "We added a loss for evidence supervision during training... We chose Kullback–Leibler (KL) divergence loss... since it is a term in the cross-entropy loss expression."
  - [section] "Out of all the evidence extraction methods tested, Supervised Attention achieved the best micro-F1 score across all metrics."
  - [corpus] Weak corpus evidence; this mechanism is specific to the paper's implementation.

### Mechanism 3
- Claim: The dataset supports evaluation of interpretability of deep learning models for multi-label classification by providing rationales for each code.
- Mechanism: The evidence spans serve as rationales that explain why a particular code was assigned, allowing researchers to assess whether the model's predictions are based on relevant information.
- Core assumption: The presence of evidence spans allows for meaningful assessment of model interpretability, and that interpretability is important for trust and adoption of CAC systems.
- Evidence anchors:
  - [abstract] "MDACE can be used to evaluate code evidence extraction methods for CAC systems, as well as the accuracy and interpretability of deep learning models for multi-label classification."
  - [section] "Many of the above works are able to use the attention weights to identify the text snippets that justify code predictions. But there is no quantitative evaluation of the quality of the snippets mostly due to the lack of reference evidence."
  - [corpus] Weak corpus evidence for this mechanism; interpretability is a broader topic in ML.

## Foundational Learning

- Concept: Extreme multi-label classification
  - Why needed here: The task involves assigning multiple labels (billing codes) from a large set to a single document (patient encounter), which is more complex than standard multi-class classification.
  - Quick check question: In extreme multi-label classification, how many labels are typically assigned to a document compared to standard multi-label classification?

- Concept: Attention mechanisms in neural networks
  - Why needed here: Attention mechanisms are used to identify the most relevant parts of the input text for each label, which is crucial for both prediction and evidence extraction.
  - Quick check question: What is the role of attention weights in the EffectiveCAN model, and how are they used for evidence extraction?

- Concept: Inter-annotator agreement
  - Why needed here: Measuring agreement between coders ensures the quality and reliability of the annotated evidence spans, which is essential for the dataset's utility.
  - Quick check question: What agreement measure was used to assess the reliability of the annotations, and what does a high value indicate?

## Architecture Onboarding

- Component map: Clinical notes -> Res-SE encoder -> Attention module -> Evidence extraction -> Output
- Critical path: Clinical notes → Res-SE encoder → Attention module → Evidence extraction → Output
- Design tradeoffs:
  - Using supervised attention vs. unsupervised attention: Supervised attention requires labeled evidence data but may improve evidence extraction performance.
  - Position-sensitive vs. position-independent metrics: Position-sensitive metrics are more stringent but may not account for sufficient vs. complete evidence annotation.
  - Token-level vs. span-level evaluation: Span-level evaluation is more aligned with how coders annotate evidence but is more complex.
- Failure signatures:
  - Low agreement between coders: Indicates potential issues with annotation guidelines or coder expertise.
  - Poor performance on evidence extraction: Could be due to insufficient training data, model architecture limitations, or issues with the attention mechanism.
  - High variance in results: May indicate sensitivity to hyperparameters or model initialization.
- First 3 experiments:
  1. Evaluate the inter-annotator agreement on a small sample of charts to ensure annotation quality.
  2. Train the EffectiveCAN model with unsupervised attention on the code prediction task and compare to baseline results.
  3. Train the EffectiveCAN model with supervised attention on the evidence extraction task and compare to unsupervised attention results.

## Open Questions the Paper Calls Out

- Question: How does the performance of evidence extraction methods vary when applied to the complete clinical notes rather than truncated text?
  - Basis in paper: [explicit] The paper mentions that evidence results on all code-able notes could be affected by input text truncation as potentially more than half of the tokens and evidence were discarded, and more experiments and analysis should be conducted to better understand these results.
  - Why unresolved: The experiments were conducted on truncated text due to computational constraints, leaving the impact of full text on evidence extraction performance unknown.
  - What evidence would resolve it: Conducting experiments with the complete clinical notes and comparing the results to those obtained with truncated text would provide insights into the effect of text truncation on evidence extraction performance.

- Question: What is the impact of the annotation guidelines on the inter-annotator agreement and the quality of the evidence spans?
  - Basis in paper: [inferred] The paper discusses the annotation process and guidelines, mentioning that coders were instructed to annotate sufficient evidence for Inpatient coding but complete evidence for Profee coding. It also notes that there were cases of disagreement among coders, which were resolved during the review process.
  - Why unresolved: The paper does not provide a detailed analysis of how the annotation guidelines influenced the inter-annotator agreement and the quality of the evidence spans.
  - What evidence would resolve it: Conducting a thorough analysis of the annotation guidelines' impact on the inter-annotator agreement and the quality of the evidence spans, possibly through a controlled experiment with different annotation guidelines, would provide insights into their influence.

- Question: How does the performance of evidence extraction methods differ between Inpatient and Profee coding tasks?
  - Basis in paper: [explicit] The paper presents evaluation results for both Inpatient and Profee coding tasks, showing differences in performance between the two tasks.
  - Why unresolved: While the paper provides evaluation results for both tasks, it does not provide a detailed analysis of the reasons behind the differences in performance or explore potential strategies to improve performance on the task with lower results.
  - What evidence would resolve it: Conducting a detailed analysis of the differences in evidence extraction performance between Inpatient and Profee coding tasks, including an exploration of potential factors contributing to the differences and strategies to improve performance on the task with lower results, would provide insights into this question.

## Limitations
- The dataset size is relatively small (302 Inpatient charts and 52 Profee charts), which may limit the model's ability to generalize to unseen data or more complex coding scenarios.
- The evidence extraction performance, while improved by supervised attention, remains relatively low (micro-F1 of 0.439 for exact span match), indicating room for improvement in the model's ability to accurately identify relevant evidence.
- The evaluation metrics used (token match and exact span match) may not fully capture the nuances of evidence extraction, particularly for sufficient vs. complete evidence annotation.

## Confidence
- **High confidence** in the dataset creation methodology and the general approach to evidence extraction. The use of professional medical coders to annotate evidence spans is well-established and the MDACE dataset fills a clear gap in the field.
- **Medium confidence** in the effectiveness of supervised attention for evidence extraction. While the results show improvement over unsupervised attention, the difference in micro-F1 scores is relatively modest (0.439 vs. 0.436 for exact span match).
- **Low confidence** in the generalizability of the results to other medical coding tasks or datasets. The MDACE dataset is based on MIMIC-III, which may not represent all medical coding scenarios.

## Next Checks
1. **Inter-annotator agreement assessment:** Calculate and report the agreement between the two professional coders on a subset of the dataset to quantify annotation reliability and identify potential sources of disagreement.
2. **Model robustness evaluation:** Train and evaluate the EffectiveCAN model with supervised attention on different subsets of the MDACE dataset (e.g., only Inpatient charts, only Profee charts) to assess the model's ability to generalize across different types of medical encounters.
3. **Comparison with alternative evidence extraction methods:** Implement and evaluate other evidence extraction methods, such as attention-based models or rule-based approaches, on the MDACE dataset to determine if there are more effective techniques for this task.