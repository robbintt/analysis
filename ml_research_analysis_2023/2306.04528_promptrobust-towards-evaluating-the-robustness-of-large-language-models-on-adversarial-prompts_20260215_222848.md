---
ver: rpa2
title: 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on
  Adversarial Prompts'
arxiv_id: '2306.04528'
source_url: https://arxiv.org/abs/2306.04528
tags:
- prompts
- adversarial
- llms
- language
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PromptRobust, a robustness benchmark designed\
  \ to evaluate the resilience of Large Language Models (LLMs) to adversarial prompts.\
  \ The study employs various adversarial textual attacks across multiple levels\u2014\
  character, word, sentence, and semantic\u2014to assess how slight deviations in\
  \ prompts can affect LLM outcomes while maintaining semantic integrity."
---

# PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts

## Quick Facts
- arXiv ID: 2306.04528
- Source URL: https://arxiv.org/abs/2306.04528
- Reference count: 40
- Primary result: Word-level adversarial attacks cause 33% performance drop in LLMs, demonstrating significant vulnerability to prompt perturbations

## Executive Summary
This paper introduces PromptRobust, a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to adversarial prompts across multiple attack levels and tasks. The study systematically tests 4,788 adversarial prompts across 8 tasks and 13 datasets using 567,084 test samples and 4 different LLMs (T5, Vicuna, UL2, ChatGPT). The results reveal that contemporary LLMs are significantly vulnerable to adversarial prompts, with word-level attacks proving most effective at degrading performance. The research provides valuable insights into the factors affecting prompt robustness and offers practical recommendations for constructing more resilient prompts, while also raising important questions about model security and transferability of attacks.

## Method Summary
The study employs a multi-level adversarial attack framework targeting prompts at character, word, sentence, and semantic levels. Using 7 different attack approaches (TextBugger, DeepWordBug, BertAttack, TextFooler, CheckList, StressTest, Semantic), the researchers generate adversarial prompts while maintaining semantic integrity. These prompts are evaluated across diverse tasks including sentiment analysis, natural language inference, reading comprehension, translation, and mathematical problem-solving. Performance is measured using the Performance Drop Rate (PDR) metric, which quantifies the relative performance decline following prompt attacks. The evaluation includes baseline measurements on clean prompts, analysis of attention patterns through visualization techniques, and assessment of adversarial prompt transferability between different LLMs.

## Key Results
- Word-level adversarial attacks are most effective, causing a 33% performance drop across evaluated LLMs
- Vicuna-13B shows highest vulnerability to adversarial prompts with a 23% average PDR
- Attention shift patterns reveal that adversarial prompts reroute LLMs' focus from critical task-related words to adversarial content
- Transferability of adversarial prompts between models is limited, with inconsistent results across different model pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial prompts induce performance degradation by altering the model's attention focus away from critical task-related words toward adversarial content.
- Mechanism: The adversarial perturbation modifies the input prompt in a way that shifts the LLM's attention weights, causing it to prioritize the perturbed elements over the semantically important ones. This attention shift leads to misclassification or incoherent responses.
- Core assumption: The LLM's decision-making process is heavily influenced by the distribution of attention weights across input tokens.
- Evidence anchors:
  - [section]: "Our findings reveal that adversarial prompts cause LLMs to shift their focus towards adversarial elements thus response either wrong answer or meaningless sentences."
  - [section]: "Adversarial prompts can reroute LLMs' attention from integral text segments, causing misclassifications."

### Mechanism 2
- Claim: Word-level attacks are more effective than sentence-level attacks because they introduce semantic-level confusion while maintaining grammatical coherence.
- Mechanism: By replacing words with synonyms or contextually similar words, word-level attacks preserve the overall sentence structure but alter the semantic meaning enough to confuse the model's understanding. This is more effective than sentence-level attacks which simply add irrelevant content that the model might learn to ignore.
- Core assumption: The LLM's semantic understanding is sensitive to word-level substitutions but can filter out extraneous sentence-level noise.
- Evidence anchors:
  - [abstract]: "word-level attacks proving most effective, causing a 33% performance drop."
  - [section]: "Our analysis reveals that adversarial prompts can reroute LLMs' attention from integral text segments, causing misclassifications."

### Mechanism 3
- Claim: The transferability of adversarial prompts between models is limited due to differences in architecture and fine-tuning strategies.
- Mechanism: Adversarial prompts generated for one LLM may not effectively transfer to another due to variations in model architecture, pre-training data, and fine-tuning approaches. Each model may have learned different attention patterns and semantic representations.
- Core assumption: Different LLMs learn distinct representations and attention mechanisms even when trained on similar tasks.
- Evidence anchors:
  - [section]: "We observe that while adversarial prompts exhibit some degree of transferability... it is marginal compared to Table 2 and 3."
  - [corpus]: "The transferability to ChatGPT is better compared to T5 and UL2. This suggests an avenue to generate adversarial prompts to attack black-box models such as ChatGPT by training on small models like T5..."

## Foundational Learning

- Concept: Prompt Engineering
  - Why needed here: Understanding how different prompt structures affect model performance is crucial for crafting robust prompts and evaluating adversarial attacks.
  - Quick check question: What is the difference between zero-shot, few-shot, task-oriented, and role-oriented prompts?

- Concept: Adversarial Attack Techniques
  - Why needed here: Familiarity with different attack levels (character, word, sentence, semantic) is essential to understand how perturbations at different granularities affect model robustness.
  - Quick check question: How does a word-level attack differ from a character-level attack in terms of their impact on LLM performance?

- Concept: Attention Mechanisms in Transformers
  - Why needed here: Understanding how attention weights are distributed across input tokens helps explain why adversarial prompts can mislead the model.
  - Quick check question: What role do attention weights play in a transformer model's decision-making process?

## Architecture Onboarding

- Component map: PromptBench Framework -> Attack Modules (Character-level, Word-level, Sentence-level, Semantic-level) -> LLM Models (T5, Vicuna, UL2, ChatGPT) -> Datasets (GLUE, MMLU, SQuAD V2, UN Multi, IWSLT 2017, Math) -> Evaluation Metrics (Performance Drop Rate, Average PDR) -> Visualization Tools (Attention by Gradient, Attention by Deletion)

- Critical path:
  1. Generate adversarial prompts for each dataset and prompt type
  2. Evaluate clean prompts across all LLMs to establish baselines
  3. Apply adversarial attacks and measure performance drops
  4. Analyze attention patterns to understand vulnerability mechanisms
  5. Assess transferability between models
  6. Provide recommendations based on word frequency analysis

- Design tradeoffs:
  - Comprehensive evaluation vs. computational efficiency (sampling strategy)
  - Black-box attacks vs. white-box attacks (reliance on model gradients)
  - Diverse attack types vs. depth of analysis for each attack
  - Public accessibility vs. potential misuse of adversarial prompts

- Failure signatures:
  - High standard deviation in PDR across different prompts suggests inconsistent robustness
  - Models showing negative PDR indicate potential for adversarial prompts to improve performance (adversarial training effect)
  - Transferability with high standard deviation indicates unreliable cross-model attack effectiveness

- First 3 experiments:
  1. Run clean prompt evaluation on all four LLMs (T5, Vicuna, UL2, ChatGPT) across all 13 datasets to establish baseline performance.
  2. Generate adversarial prompts using TextFooler attack on SST-2 dataset and evaluate performance drop across all LLMs.
  3. Visualize attention weights for clean vs. adversarial prompts on a sample from the CoLA dataset to observe attention shift patterns.

## Open Questions the Paper Calls Out

- Question: How do different fine-tuning techniques affect the robustness of LLMs to adversarial prompts?
  - Basis in paper: [inferred] The paper mentions that models like UL2 and T5, fine-tuned on large datasets, and ChatGPT, fine-tuned via RLHF, exhibit better robustness than Vicuna.
  - Why unresolved: The paper does not provide a detailed analysis of how specific fine-tuning techniques contribute to robustness.
  - What evidence would resolve it: A comparative study of various fine-tuning techniques and their impact on robustness to adversarial prompts.

- Question: What are the most effective strategies for enhancing the robustness of LLMs against adversarial prompts?
  - Basis in paper: [explicit] The paper discusses potential countermeasures and defenses, such as input preprocessing, incorporating low-quality data in pre-training, and exploring improved fine-tuning methods.
  - Why unresolved: The paper does not provide a comprehensive evaluation of these strategies to determine their effectiveness.
  - What evidence would resolve it: Experimental results comparing the effectiveness of different robustness enhancement strategies.

- Question: How does the transferability of adversarial prompts between different LLMs vary with model architecture and size?
  - Basis in paper: [explicit] The paper mentions that adversarial prompts exhibit some degree of transferability, but the extent of transferability varies across models.
  - Why unresolved: The paper does not provide a detailed analysis of the factors influencing the transferability of adversarial prompts.
  - What evidence would resolve it: A systematic study of transferability across a diverse range of LLMs with different architectures and sizes.

## Limitations

- The evaluation relies on sampling strategies rather than exhaustive testing, which may introduce selection bias
- Transferability analysis shows inconsistent results with high standard deviations, indicating model-dependent effectiveness
- The study focuses primarily on model-level attacks without considering system-level defenses or mitigation strategies
- Public accessibility of adversarial prompts raises concerns about potential misuse despite research transparency benefits

## Confidence

**High Confidence**: The core finding that LLMs are vulnerable to adversarial prompts is well-supported by the extensive empirical evaluation across 567,084 test samples and 4,788 adversarial prompts.

**Medium Confidence**: The claim that word-level attacks are most effective (causing 33% performance drop) is supported by the data, but the relative effectiveness of different attack types may vary depending on the specific task and model architecture.

**Low Confidence**: The mechanistic explanations for why certain attacks work better than others are largely speculative, with limited causal evidence linking specific attention mechanisms to adversarial vulnerability.

## Next Checks

1. Conduct cross-model testing with models that share similar architectures but different fine-tuning strategies to determine if architectural similarity improves adversarial prompt transferability.

2. Implement and evaluate simple defense strategies (such as adversarial training or input sanitization) to assess whether the performance drops observed can be mitigated.

3. Perform a deeper analysis of which specific tasks and dataset characteristics make models more vulnerable to certain attack types, moving beyond aggregated metrics to understand granular factors contributing to adversarial vulnerability.