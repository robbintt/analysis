---
ver: rpa2
title: 'ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace
  Human Label Variation'
arxiv_id: '2310.14979'
source_url: https://arxiv.org/abs/2310.14979
tags:
- learning
- annotation
- active
- uncertainty
- multi-head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using annotator-specific classification heads
  in active learning for subjective tasks with label variation. The authors show that
  a multi-head model significantly outperforms single-head models for uncertainty
  estimation in this setting.
---

# ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation

## Quick Facts
- arXiv ID: 2310.14979
- Source URL: https://arxiv.org/abs/2310.14979
- Authors: 
- Reference count: 10
- Primary result: Multi-head active learning model achieves comparable performance to full training while saving up to 70% annotation budget

## Executive Summary
This paper addresses the challenge of label variation in subjective tasks like hate speech detection by proposing an active learning framework with annotator-specific classification heads. The authors argue that traditional approaches treating multi-annotator labels as ground truth ignore valuable information about individual annotator perspectives. Their multi-head BERT model learns unique classification heads for each annotator, allowing it to model both individual and collective uncertainty patterns during active learning.

The framework introduces five acquisition functions specifically designed for the annotator-aware setting, with group-level entropy emerging as the most robust approach across different datasets. Extensive experiments on two hate speech datasets demonstrate that the multi-head model significantly outperforms single-head baselines in uncertainty estimation while achieving comparable predictive performance with substantially reduced annotation costs. The approach effectively learns from disagreement rather than attempting to resolve it through aggregation.

## Method Summary
The method employs a multi-head BERT architecture where each annotator has a dedicated classification head sharing a common BERT encoder. The model is trained on individual (data, annotation, annotator ID) tuples rather than aggregated labels, using weighted loss functions to handle class imbalance. Active learning proceeds through iterative rounds where instances are selected using uncertainty-based acquisition functions designed for the multi-head setting. The framework includes five acquisition functions: Random, Individual Entropy, Group Entropy, Vote Variance, and Mix. Training starts with a small seed set and progressively queries the most informative instance-annotator pairs until performance plateaus.

## Key Results
- Multi-head model significantly outperforms single-head model in uncertainty estimation across both hate speech datasets
- Group-level entropy acquisition function works generally well across datasets with different annotation patterns
- The approach achieves performance comparable to full-scale training while saving up to 70% of annotation budget
- Label diversity-first approach performs better than sample diversity-first for annotator selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-head architecture improves uncertainty estimation by modeling individual annotator perspectives.
- Mechanism: Each annotator has a dedicated classification head that learns their unique labeling patterns and uncertainties. During active learning, these individual uncertainties can be aggregated to better estimate overall uncertainty.
- Core assumption: Annotator-specific classification heads can effectively capture individual annotator biases and uncertainty patterns.
- Evidence anchors:
  - [abstract] "a multi-head model performs significantly better than a single-head model in terms of uncertainty estimation"
  - [section] "the multi-head model outperforms the single-head model by a large margin" in uncertainty estimation
  - [corpus] Weak - no direct evidence in corpus
- Break condition: If annotators are too similar or the task is objective with little disagreement, the multi-head advantage disappears.

### Mechanism 2
- Claim: Group-level entropy acquisition function works generally well across datasets with different annotation patterns.
- Mechanism: Instead of looking at individual annotator uncertainty, group-level entropy aggregates all annotator-specific heads' outputs and measures the overall entropy of this aggregation. This captures both label diversity and sample uncertainty simultaneously.
- Core assumption: Aggregating individual uncertainties provides a more robust uncertainty estimate than individual-level methods.
- Evidence anchors:
  - [abstract] "we show that group-level entropy works generally well on both datasets"
  - [section] "Group performs generally well on both datasets" and "Label Diversity First shows stronger performance in general"
  - [corpus] Weak - corpus mentions related work but no direct evidence
- Break condition: If datasets have very different characteristics or if annotator disagreement is not informative for the task.

### Mechanism 3
- Claim: Active learning with multi-head models achieves comparable performance to full-scale training while saving up to 70% of annotation budget.
- Mechanism: By strategically querying the most informative instance-annotator pairs using uncertainty-based acquisition functions, the model learns more efficiently from limited annotations. The multi-head architecture allows modeling of individual perspectives without needing full annotation coverage.
- Core assumption: Uncertainty-based active learning with multi-head models can identify the most informative samples to label.
- Evidence anchors:
  - [abstract] "it achieves performance in terms of both prediction and uncertainty estimation comparable to full-scale training from disagreement, while saving up to 70% of the annotation budget"
  - [section] "we see that the F1 score slowly goes into a plateau after around 25 rounds on both datasets in Fig 3, which is around 30% usage of the overall dataset"
  - [corpus] Weak - no direct evidence in corpus
- Break condition: If the active learning acquisition function fails to identify truly informative samples, or if the multi-head model cannot effectively learn from sparse annotations.

## Foundational Learning

- Concept: Multi-task learning with shared representations
  - Why needed here: The multi-head model shares a common BERT encoder while having annotator-specific heads, allowing it to learn both shared and individual patterns.
  - Quick check question: How does the model ensure that the shared encoder learns representations useful for all annotators while the heads capture individual differences?

- Concept: Uncertainty estimation in active learning
  - Why needed here: The paper relies on uncertainty-based acquisition functions to select which samples to annotate next.
  - Quick check question: What's the difference between individual-level entropy and group-level entropy in measuring uncertainty?

- Concept: Label aggregation vs. learning from disagreement
  - Why needed here: The paper contrasts traditional approaches that aggregate labels (e.g., majority voting) with approaches that learn from raw individual annotations.
  - Quick check question: When would learning from disagreement be more beneficial than label aggregation?

## Architecture Onboarding

- Component map: BERT-base encoder -> N annotator-specific classification heads -> Loss calculation -> Uncertainty estimation -> Acquisition function -> Query selection -> Annotation -> Repeat
- Critical path: Encoder → Heads → Loss calculation → Uncertainty estimation → Acquisition function → Query selection → Annotation → Repeat
- Design tradeoffs:
  - Memory vs. Performance: More annotator heads provide better modeling but increase memory usage
  - Annotation budget vs. Model complexity: Multi-head models need more annotations per sample to train effectively
  - Individual vs. Group uncertainty: Different acquisition functions prioritize different aspects of the data
- Failure signatures:
  - Poor uncertainty estimation despite multi-head architecture (check if heads are properly modeling annotators)
  - Convergence issues (check class weights and loss function stability)
  - No improvement over single-head baseline (check if annotator IDs are properly utilized)
- First 3 experiments:
  1. Compare single-head vs multi-head on a small subset with known annotator disagreement
  2. Test individual-level vs group-level entropy acquisition on a validation set
  3. Measure annotation savings by comparing F1 curves at different annotation budget levels

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several implicit questions emerge about the broader applicability and limitations of the approach.

## Limitations
- The study focuses on hate speech detection, limiting generalizability to other subjective tasks
- Computational overhead scales linearly with the number of annotators, potentially becoming prohibitive for large annotator pools
- The framework assumes known annotator identities, which may not hold in real-world scenarios where annotator IDs are unknown or dynamic

## Confidence

*High Confidence:* The core finding that multi-head models outperform single-head models for uncertainty estimation in subjective tasks is well-supported by experimental results. The 70% annotation budget savings claim is directly demonstrated through F1 score curves.

*Medium Confidence:* The effectiveness of group-level entropy as a generally applicable acquisition function is demonstrated on two datasets but may not generalize to all subjective tasks. The assumption that learning from individual annotator perspectives is universally beneficial needs broader validation.

*Low Confidence:* The claim about computational efficiency and scalability of the multi-head approach for large numbers of annotators is not empirically validated beyond the tested datasets.

## Next Checks

1. **Generalization Test:** Apply the multi-head active learning framework to a different subjective task (e.g., sentiment analysis with diverse annotator backgrounds) to verify the general applicability of group-level entropy and the multi-head advantage.

2. **Scalability Analysis:** Systematically evaluate the computational overhead and performance degradation as the number of annotators increases from 3 to 50+ in a controlled simulation to validate scalability claims.

3. **Annotator Dynamics:** Test the model's performance when new annotators are introduced mid-training and when annotator identities are unknown, assessing robustness to real-world annotation scenarios.