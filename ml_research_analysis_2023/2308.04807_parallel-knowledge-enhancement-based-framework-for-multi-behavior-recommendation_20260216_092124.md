---
ver: rpa2
title: Parallel Knowledge Enhancement based Framework for Multi-behavior Recommendation
arxiv_id: '2308.04807'
source_url: https://arxiv.org/abs/2308.04807
tags:
- behavior
- behaviors
- information
- multi-behavior
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Parallel Knowledge Enhancement Framework
  (PKEF) for multi-behavior recommendation that addresses two key challenges: imbalanced
  data distribution across behaviors and negative transfer in multi-task learning.
  PKEF introduces a Parallel Knowledge Fusion (PKF) module that combines cascade and
  parallel paradigms to enhance behavior representations and correct information bias.'
---

# Parallel Knowledge Enhancement based Framework for Multi-behavior Recommendation

## Quick Facts
- arXiv ID: 2308.04807
- Source URL: https://arxiv.org/abs/2308.04807
- Reference count: 40
- One-line primary result: PKEF framework achieves up to 95.16% relative improvement in HR@10 and 38.58% in NDCG@10 on Beibei dataset

## Executive Summary
This paper introduces a Parallel Knowledge Enhancement Framework (PKEF) for multi-behavior recommendation that addresses two critical challenges: imbalanced data distribution across behaviors and negative transfer in multi-task learning. The framework combines a Parallel Knowledge Fusion (PKF) module that integrates cascade and parallel paradigms with a Projection Disentangling Multi-Experts (PME) network. PKF leverages parallel knowledge to enhance behavior representations while correcting information bias from imbalanced interactions, while PME treats each behavior as an independent task to generate specific expert information and uses a projection mechanism to eliminate gradient conflicts and alleviate negative transfer.

## Method Summary
PKEF is a multi-behavior recommendation framework that processes user-item interactions across multiple behavior types (e.g., View, Cart, Purchase, Collect). The framework consists of an embedding layer that converts user/item IDs to dense embeddings, followed by the PKF module which combines cascade correlation learning (using LightGCN) and parallel interaction enhancement with projection-based knowledge fusion. The PME network then generates behavior-specific expert information using separated inputs and applies a projection disentangling mechanism during aggregation to eliminate gradient conflicts. The framework is jointly optimized using parallel loss, cascade loss, and unique loss with L2 regularization. The method was evaluated on three real-world datasets (Beibei, Taobao, Tmall) and showed significant improvements over state-of-the-art methods.

## Key Results
- Achieved up to 95.16% relative improvement in HR@10 and 38.58% in NDCG@10 on Beibei dataset
- Demonstrated superior performance over state-of-the-art multi-behavior recommendation methods
- Showed effective handling of imbalanced data distribution across behaviors through PKF module
- Successfully alleviated negative transfer in multi-task learning through PME network's projection disentangling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PKF combines cascade and parallel paradigms to enhance behavior representations while correcting information bias from imbalanced behavioral interactions.
- Mechanism: The cascade stream captures hierarchical correlations between behaviors, while the parallel stream independently learns each behavior's representation. The projection-enhanced knowledge fusion explicitly extracts useful information from parallel representations to enhance cascade representations.
- Core assumption: Parallel knowledge contains complementary information that can improve cascade learning without introducing harmful noise.
- Evidence anchors:
  - [abstract]: "PKF combines the cascade and parallel paradigms, leveraging parallel knowledge for adaptive enhancement of different behaviors' representations while learning hierarchical correlation information to correct the information bias caused by imbalanced behavioral interactions."
  - [section 4.2.2]: "We devise two schemes to fuse the knowledge between the parallel and cascade streams... For simplicity, we denote eğ‘˜,ğ‘™ ğ‘ğ‘ğ‘Ÿ = Ë†Ağ‘˜ pğ‘˜,ğ‘™ and eğ‘˜,ğ‘™ ğ‘ğ‘ğ‘  = Ë†Ağ‘˜ zğ‘˜,ğ‘™"
  - [corpus]: Weak evidence - no directly comparable methods using parallel knowledge enhancement mentioned in corpus.
- Break condition: If parallel representations contain noise that correlates negatively with cascade representations, projection may amplify harmful information rather than beneficial complementary knowledge.

### Mechanism 2
- Claim: PME treats each behavior as an independent task, generating specific expert information with separate inputs and using a projection mechanism during aggregation to eliminate gradient conflicts and alleviate negative transfer.
- Mechanism: Instead of coupled inputs, PME uses separate inputs for each behavior to generate behavior-specific experts. A projection mechanism disentangles shared and unique parts from other behaviors, aggregating only shared parts while optimizing unique parts with auxiliary loss.
- Core assumption: Shared information between behaviors is beneficial for prediction while unique information is task-specific and should be handled separately.
- Evidence anchors:
  - [abstract]: "Meanwhile, in the prediction step, we decouple the representations to generate expert information and introduce a projection mechanism during aggregation to eliminate gradient conflicts and alleviate negative transfer (PME)."
  - [section 3.2.2]: "To handle the above problem, we first need to utilize the separated input of each behavior to generate the behavior-specific expert information and behavior-specific gating weight."
  - [corpus]: No directly comparable methods using projection disentangling for multi-task learning found in corpus.
- Break condition: If the assumption that shared information is always beneficial is incorrect, or if the projection mechanism fails to properly identify shared vs unique information.

### Mechanism 3
- Claim: The unique loss task takes full advantage of the mutually exclusive relationship between qğ‘˜ â€²,ğ‘˜ ğ‘¢ğ‘›ğ‘– and qğ‘˜, facilitating learning of the k-th behavior.
- Mechanism: The unique representation qğ‘˜ â€²,ğ‘˜ ğ‘¢ğ‘›ğ‘– is used to predict "k' without k" interactions, leveraging the exclusive relationship between behaviors.
- Core assumption: Behaviors contain mutually exclusive interaction patterns that can be exploited for auxiliary learning.
- Evidence anchors:
  - [section 4.4.3]: "To make full use of the unique representation qğ‘˜ â€²,ğ‘˜ ğ‘¢ğ‘›ğ‘–, we design an auxiliary prediction task... we remove from the behavioral adjacency matrix Mğ‘˜ â€² the positive items that Mğ‘˜ â€² shares with Mğ‘˜."
  - [section 4.3.2]: "qğ‘˜ â€²,ğ‘˜ ğ‘¢ğ‘›ğ‘–, which represents the unique part of the ğ‘˜â€²-th behavior, and qğ‘˜, which denotes the ğ‘˜-th behavior, are distinctive and should be as orthogonal as possible."
  - [corpus]: Weak evidence - no comparable auxiliary loss approaches for multi-behavior recommendation found in corpus.
- Break condition: If behaviors are not sufficiently mutually exclusive, or if the auxiliary task introduces noise rather than useful signal.

## Foundational Learning

- Concept: Graph Neural Networks for multi-behavior recommendation
  - Why needed here: PKF relies on GNN-based paradigms to encode information of each behavior through message passing
  - Quick check question: Can you explain how LightGCN aggregates information in a user-item bipartite graph for behavior k?

- Concept: Multi-task learning and negative transfer
  - Why needed here: PME addresses negative transfer in MTL by using separated inputs and projection mechanism
  - Quick check question: What is the difference between coupled input MTL and separated input MTL, and why does coupled input lead to gradient conflicts?

- Concept: Knowledge fusion between parallel and cascade streams
  - Why needed here: PKF fuses knowledge from both paradigms to correct information bias from imbalanced behavioral interactions
  - Quick check question: How does the projection-enhanced knowledge fusion scheme in PKF differ from simple summation or linear transformation approaches?

## Architecture Onboarding

- Component map: Embedding Layer â†’ PKF (Cascade + Parallel + Fusion) â†’ PME (Experts + Projection + Aggregation) â†’ Joint Optimization (Parallel loss + Cascade loss + Unique loss + L2 regularization)

- Critical path: Embedding â†’ PKF (Cascade + Parallel + Fusion) â†’ PME (Experts + Projection + Aggregation) â†’ Losses â†’ Optimization

- Design tradeoffs:
  - PKF vs only cascade: Better handling of imbalanced data but increased computational complexity
  - PME vs coupled input MTL: Avoids negative transfer but requires more parameters and careful tuning of projection mechanism
  - Unique loss vs no unique loss: Better utilization of exclusive information but added complexity and potential for overfitting

- Failure signatures:
  - Poor performance on target behavior despite good performance on auxiliary behaviors: Possible negative transfer not properly addressed
  - Performance degradation with increased GNN layers: Overfitting or noise accumulation
  - Inconsistent performance across datasets: Sensitivity to data distribution or parameter settings

- First 3 experiments:
  1. Ablation study: Remove PKF or PME components and measure performance impact
  2. Layer sensitivity: Vary number of GNN layers for different behaviors and observe performance changes
  3. Coefficient sensitivity: Grid search behavioral coefficients (Î») and analyze impact on recommendation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the projection disentangling mechanism in PME specifically affect gradient propagation during multi-task learning, and can this be quantified?
- Basis in paper: [explicit] The paper discusses the projection mechanism's role in alleviating negative transfer and solving gradient conflicts, particularly in Section 3.2.2 and Figure 6.
- Why unresolved: The paper demonstrates improved performance with PME but does not provide a detailed analysis of how the projection mechanism quantitatively affects gradient propagation.
- What evidence would resolve it: A detailed gradient analysis showing how the projection mechanism modifies gradient norms and directions during training, compared to baseline MTL methods.

### Open Question 2
- Question: What is the impact of different knowledge fusion schemes (Projection-enhanced, Vanilla-enhanced, Summation, Linear Trans.) on the final recommendation performance, and under what conditions does each scheme excel?
- Basis in paper: [explicit] Section 5.3.2 discusses different knowledge fusion schemes and their performance, but does not provide a comprehensive comparison of their impact under varying conditions.
- Why unresolved: The paper provides a comparison of different schemes but does not explore their performance under different dataset characteristics or model configurations.
- What evidence would resolve it: An ablation study varying dataset properties (e.g., behavior distribution, sparsity) and model configurations (e.g., number of GNN layers) to determine when each fusion scheme is most effective.

### Open Question 3
- Question: How does the number of GNN layers for different behaviors affect the model's ability to capture high-order interactions, and what is the optimal configuration for different types of behavior hierarchies?
- Basis in paper: [explicit] Section 5.4.1 discusses the impact of GNN layers on model performance, showing that different behaviors benefit from different numbers of layers.
- Why unresolved: The paper provides some insights into the optimal layer configuration but does not fully explore how this varies with different behavior hierarchies or dataset characteristics.
- What evidence would resolve it: A comprehensive study varying the number of GNN layers for each behavior and analyzing the impact on recommendation performance across different datasets with varying behavior hierarchies.

## Limitations
- Claims rely heavily on specific architectural choices without sufficient ablation studies to isolate component contributions
- PKF's knowledge fusion mechanism lacks clear justification for why parallel knowledge specifically improves cascade learning
- PME's projection disentangling assumes shared information is universally beneficial, which may not hold across all datasets
- Evaluation focuses primarily on HR@10 and NDCG@10 without exploring other relevant metrics like coverage or diversity

## Confidence
- **High Confidence**: The overall framework design for addressing imbalanced data and negative transfer in multi-behavior recommendation is sound and addresses well-documented challenges in the field.
- **Medium Confidence**: The specific implementation details of PKF and PME modules are plausible but lack sufficient empirical validation through comprehensive ablation studies.
- **Low Confidence**: Claims about the unique loss task exploiting mutually exclusive relationships between behaviors are based on theoretical assumptions rather than demonstrated empirical evidence.

## Next Checks
1. **Ablation Study**: Systematically remove PKF or PME components and measure their individual contribution to overall performance to clarify whether improvements are due to combined framework or specific components.

2. **Cross-Dataset Generalization**: Test the framework on additional datasets beyond Beibei, Taobao, and Tmall to verify that performance gains are not dataset-specific and that the model handles varying degrees of behavior imbalance.

3. **Projection Mechanism Analysis**: Conduct experiments to validate the assumption that shared information is beneficial and that the projection mechanism correctly identifies shared vs unique components, including visualization of projection weights or testing with modified projection objectives.